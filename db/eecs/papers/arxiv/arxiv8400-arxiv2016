arxiv-1410-5102 | On Bootstrapping Machine Learning Performance Predictors via Analytical Models |  http://arxiv.org/abs/1410.5102  | author:Diego Didona, Paolo Romano category:cs.PF cs.LG published:2014-10-19 summary:Performance modeling typically relies on two antithetic methodologies: whitebox models, which exploit knowledge on system's internals and capture itsdynamics using analytical approaches, and black box techniques, which inferrelations among the input and output variables of a system based on theevidences gathered during an initial training phase. In this paper weinvestigate a technique, which we name Bootstrapping, which aims at reconcilingthese two methodologies and at compensating the cons of the one with the prosof the other. We thoroughly analyze the design space of this gray box modelingtechnique, and identify a number of algorithmic and parametric trade-offs whichwe evaluate via two realistic case studies, a Key-Value Store and a Total OrderBroadcast service.
arxiv-1410-5058 | Dense 3D Face Correspondence |  http://arxiv.org/abs/1410.5058  | author:Syed Zulqarnain Gilani, Faisal Shafait, Ajmal Mian category:cs.CV published:2014-10-19 summary:We present an algorithm that automatically establishes dense correspondencesbetween a large number of 3D faces. Starting from automatically detected sparsecorrespondences on the convex hull of 3D faces, the algorithm triangulatesexisting correspondences and expands them iteratively along the triangle edges.New correspondences are established by matching keypoints on the geodesicpatches along the triangle edges and the process is repeated. After exhaustingkeypoint matches, further correspondences are established by evolving level setgeodesic curves from the centroids of large triangles. A deformable model(K3DM) is constructed from the dense corresponded faces and an algorithm isproposed for morphing the K3DM to fit unseen faces. This algorithm iteratesbetween rigid alignment of an unseen face followed by regularized morphing ofthe deformable model. We have extensively evaluated the proposed algorithms onsynthetic data and real 3D faces from the FRGCv2 and BU3DFE databases usingquantitative and qualitative benchmarks. Our algorithm achieved densecorrespondences with a mean localization error of 1.28mm on synthetic faces anddetected 18 anthropometric landmarks on unseen real faces from the FRGCv2database with 3mm precision. Furthermore, our deformable model fittingalgorithm achieved 99.8% gender classification and 98.3% face recognitionaccuracy on the FRGCv2 database.
arxiv-1410-5078 | Learning Vague Concepts for the Semantic Web |  http://arxiv.org/abs/1410.5078  | author:Paolo Pareti, Ewan Klein category:cs.AI cs.CL published:2014-10-19 summary:Ontologies can be a powerful tool for structuring knowledge, and they arecurrently the subject of extensive research. Updating the contents of anontology or improving its interoperability with other ontologies is animportant but difficult process. In this paper, we focus on the presence ofvague concepts, which are pervasive in natural language, within the frameworkof formal ontologies. We will adopt a framework in which vagueness is capturedvia numerical restrictions that can be automatically adjusted. Since updatingvague concepts, either through ontology alignment or ontology evolution, canlead to inconsistent sets of axioms, we define and implement a method todetecting and repairing such inconsistencies in a local fashion.
arxiv-1410-5784 | Optimal Feature Selection from VMware ESXi 5.1 Feature Set |  http://arxiv.org/abs/1410.5784  | author:Amartya Hatua category:cs.DC cs.LG published:2014-10-18 summary:A study of VMware ESXi 5.1 server has been carried out to find the optimalset of parameters which suggest usage of different resources of the server.Feature selection algorithms have been used to extract the optimum set ofparameters of the data obtained from VMware ESXi 5.1 server using esxtopcommand. Multiple virtual machines (VMs) are running in the mentioned server.K-means algorithm is used for clustering the VMs. The goodness of each clusteris determined by Davies Bouldin index and Dunn index respectively. The bestcluster is further identified by the determined indices. The features of thebest cluster are considered into a set of optimal parameters.
arxiv-1410-4985 | Evolvability signatures of generative encodings: beyond standard performance benchmarks |  http://arxiv.org/abs/1410.4985  | author:Danesh Tarapore, Jean-Baptiste Mouret category:cs.NE published:2014-10-18 summary:Evolutionary robotics is a promising approach to autonomously synthesizemachines with abilities that resemble those of animals, but the field suffersfrom a lack of strong foundations. In particular, evolutionary systems arecurrently assessed solely by the fitness score their evolved artifacts canachieve for a specific task, whereas such fitness-based comparisons providelimited insights about how the same system would evaluate on different tasks,and its adaptive capabilities to respond to changes in fitness (e.g., fromdamages to the machine, or in new situations). To counter these limitations, weintroduce the concept of "evolvability signatures", which picture thepost-mutation statistical distribution of both behavior diversity (howdifferent are the robot behaviors after a mutation?) and fitness values (howdifferent is the fitness after a mutation?). We tested the relevance of thisconcept by evolving controllers for hexapod robot locomotion using fivedifferent genotype-to-phenotype mappings (direct encoding, generative encodingof open-loop and closed-loop central pattern generators, generative encoding ofneural networks, and single-unit pattern generators (SUPG)). We observed apredictive relationship between the evolvability signature of each encoding andthe number of generations required by hexapods to adapt from incurred damages.Our study also reveals that, across the five investigated encodings, the SUPGscheme achieved the best evolvability signature, and was always foremost inrecovering an effective gait following robot damages. Overall, our evolvabilitysignatures neatly complement existing task-performance benchmarks, and pave theway for stronger foundations for research in evolutionary robotics.
arxiv-1410-4966 | The Visualization of Change in Word Meaning over Time using Temporal Word Embeddings |  http://arxiv.org/abs/1410.4966  | author:Chiraag Lala, Shay B. Cohen category:cs.CL published:2014-10-18 summary:We describe a visualization tool that can be used to view the change inmeaning of words over time. The tool makes use of existing (static) wordembedding datasets together with a timestamped $n$-gram corpus to create {\emtemporal} word embeddings.
arxiv-1410-4984 | Gaussian Process Models with Parallelization and GPU acceleration |  http://arxiv.org/abs/1410.4984  | author:Zhenwen Dai, Andreas Damianou, James Hensman, Neil Lawrence category:cs.DC cs.LG stat.ML published:2014-10-18 summary:In this work, we present an extension of Gaussian process (GP) models withsophisticated parallelization and GPU acceleration. The parallelization schemearises naturally from the modular computational structure w.r.t. datapoints inthe sparse Gaussian process formulation. Additionally, the computationalbottleneck is implemented with GPU acceleration for further speed up. Combiningboth techniques allows applying Gaussian process models to millions ofdatapoints. The efficiency of our algorithm is demonstrated with a syntheticdataset. Its source code has been integrated into our popular software libraryGPy.
arxiv-1410-4615 | Learning to Execute |  http://arxiv.org/abs/1410.4615  | author:Wojciech Zaremba, Ilya Sutskever category:cs.NE cs.AI cs.LG published:2014-10-17 summary:Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) arewidely used because they are expressive and are easy to train. Our interestlies in empirically evaluating the expressiveness and the learnability of LSTMsin the sequence-to-sequence regime by training them to evaluate short computerprograms, a domain that has traditionally been seen as too complex for neuralnetworks. We consider a simple class of programs that can be evaluated with asingle left-to-right pass using constant memory. Our main result is that LSTMscan learn to map the character-level representations of such programs to theircorrect outputs. Notably, it was necessary to use curriculum learning, andwhile conventional curriculum learning proved ineffective, we developed a newvariant of curriculum learning that improved our networks' performance in allexperimental conditions. The improved curriculum had a dramatic impact on anaddition problem, making it possible to train an LSTM to add two 9-digitnumbers with 99% accuracy.
arxiv-1410-4871 | Bayesian estimation of the multifractality parameter for image texture using a Whittle approximation |  http://arxiv.org/abs/1410.4871  | author:Sébastien Combrexelle, Herwig Wendt, Nicolas Dobigeon, Jean-Yves Tourneret, Steve McLaughlin, Patrice Abry category:cs.CV stat.ME published:2014-10-17 summary:Texture characterization is a central element in many image processingapplications. Multifractal analysis is a useful signal and image processingtool, yet, the accurate estimation of multifractal parameters for image textureremains a challenge. This is due in the main to the fact that currentestimation procedures consist of performing linear regressions across frequencyscales of the two-dimensional (2D) dyadic wavelet transform, for which only afew such scales are computable for images. The strongly non-Gaussian nature ofmultifractal processes, combined with their complicated dependence structure,makes it difficult to develop suitable models for parameter estimation. Here,we propose a Bayesian procedure that addresses the difficulties in theestimation of the multifractality parameter. The originality of the procedureis threefold: The construction of a generic semi-parametric statistical modelfor the logarithm of wavelet leaders; the formulation of Bayesian estimatorsthat are associated with this model and the set of parameter values admitted bymultifractal theory; the exploitation of a suitable Whittle approximationwithin the Bayesian model which enables the otherwise infeasible evaluation ofthe posterior distribution associated with the model. Performance is assessednumerically for several 2D multifractal processes, for several image sizes anda large range of process parameters. The procedure yields significant benefitsover current benchmark estimators in terms of estimation performance andability to discriminate between the two most commonly used classes ofmultifractal process models. The gains in performance are particularlypronounced for small image sizes, notably enabling for the first time theanalysis of image patches as small as 64x64 pixels.
arxiv-1410-4622 | Robust Topological Feature Extraction for Mapping of Environments using Bio-Inspired Sensor Networks |  http://arxiv.org/abs/1410.4622  | author:Alireza Dirafzoon, Edgar Lobaton category:cs.RO cs.SY math.AT stat.ML published:2014-10-17 summary:In this paper, we exploit minimal sensing information gathered frombiologically inspired sensor networks to perform exploration and mapping in anunknown environment. A probabilistic motion model of mobile sensing nodes,inspired by motion characteristics of cockroaches, is utilized to extract weakencounter information in order to build a topological representation of theenvironment. Neighbor to neighbor interactions among the nodes are exploited to buildpoint clouds representing spatial features of the manifold characterizing theenvironment based on the sampled data. To extract dominant features from sampled data, topological data analysis isused to produce persistence intervals for features, to be used for topologicalmapping. In order to improve robustness characteristics of the sampled datawith respect to outliers, density based subsampling algorithms are employed.Moreover, a robust scale-invariant classification algorithm for persistencediagrams is proposed to provide a quantitative representation of desiredfeatures in the data. Furthermore, various strategies for defining encountermetrics with different degrees of information regarding agents' motion aresuggested to enhance the precision of the estimation and classificationperformance of the topological method.
arxiv-1410-4777 | A Hierarchical Multi-Output Nearest Neighbor Model for Multi-Output Dependence Learning |  http://arxiv.org/abs/1410.4777  | author:Richard G. Morris, Tony Martinez, Michael R. Smith category:stat.ML cs.LG published:2014-10-17 summary:Multi-Output Dependence (MOD) learning is a generalization of standardclassification problems that allows for multiple outputs that are dependent oneach other. A primary issue that arises in the context of MOD learning is thatfor any given input pattern there can be multiple correct output patterns. Thischanges the learning task from function approximation to relationapproximation. Previous algorithms do not consider this problem, and thuscannot be readily applied to MOD problems. To perform MOD learning, weintroduce the Hierarchical Multi-Output Nearest Neighbor model (HMONN) thatemploys a basic learning model for each output and a modified nearest neighborapproach to refine the initial results.
arxiv-1410-4673 | KCRC-LCD: Discriminative Kernel Collaborative Representation with Locality Constrained Dictionary for Visual Categorization |  http://arxiv.org/abs/1410.4673  | author:Weiyang Liu, Zhiding Yu, Lijia Lu, Yandong Wen, Hui Li, Yuexian Zou category:cs.CV cs.LG published:2014-10-17 summary:We consider the image classification problem via kernel collaborativerepresentation classification with locality constrained dictionary (KCRC-LCD).Specifically, we propose a kernel collaborative representation classification(KCRC) approach in which kernel method is used to improve the discriminationability of collaborative representation classification (CRC). We then measurethe similarities between the query and atoms in the global dictionary in orderto construct a locality constrained dictionary (LCD) for KCRC. In addition, wediscuss several similarity measure approaches in LCD and further present asimple yet effective unified similarity measure whose superiority is validatedin experiments. There are several appealing aspects associated with LCD. First,LCD can be nicely incorporated under the framework of KCRC. The LCD similaritymeasure can be kernelized under KCRC, which theoretically links CRC and LCDunder the kernel method. Second, KCRC-LCD becomes more scalable to both thetraining set size and the feature dimension. Example shows that KCRC is able toperfectly classify data with certain distribution, while conventional CRC failscompletely. Comprehensive experiments on many public datasets also show thatKCRC-LCD is a robust discriminative classifier with both excellent performanceand good scalability, being comparable or outperforming many otherstate-of-the-art approaches.
arxiv-1410-5330 | An Overview of General Performance Metrics of Binary Classifier Systems |  http://arxiv.org/abs/1410.5330  | author:Sebastian Raschka category:cs.LG published:2014-10-17 summary:This document provides a brief overview of different metrics and terminologythat is used to measure the performance of binary classification systems.
arxiv-1410-4639 | Dependent Types for Pragmatics |  http://arxiv.org/abs/1410.4639  | author:Darryl McAdams, Jonathan Sterling category:cs.CL published:2014-10-17 summary:This paper proposes the use of dependent types for pragmatic phenomena suchas pronoun binding and presupposition resolution as a type-theoreticalternative to formalisms such as Discourse Representation Theory and DynamicSemantics.
arxiv-1410-4744 | mS2GD: Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting |  http://arxiv.org/abs/1410.4744  | author:Jakub Konečný, Jie Liu, Peter Richtárik, Martin Takáč category:cs.LG stat.ML published:2014-10-17 summary:We propose a mini-batching scheme for improving the theoretical complexityand practical performance of semi-stochastic gradient descent applied to theproblem of minimizing a strongly convex composite function represented as thesum of an average of a large number of smooth convex functions, and simplenonsmooth convex function. Our method first performs a deterministic step(computation of the gradient of the objective function at the starting point),followed by a large number of stochastic steps. The process is repeated a fewtimes with the last iterate becoming the new starting point. The novelty of ourmethod is in introduction of mini-batching into the computation of stochasticsteps. In each step, instead of choosing a single function, we sample $b$functions, compute their gradients, and compute the direction based on this. Weanalyze the complexity of the method and show that the method benefits from twospeedup effects. First, we prove that as long as $b$ is below a certainthreshold, we can reach predefined accuracy with less overall work than withoutmini-batching. Second, our mini-batching scheme admits a simple parallelimplementation, and hence is suitable for further acceleration byparallelization.
arxiv-1411-1375 | Heuristic algorithm for 1D and 2D unfolding |  http://arxiv.org/abs/1411.1375  | author:Yordan Karadzhov category:stat.ML published:2014-10-17 summary:A very simple heuristic approach to the unfolding problem will be described.An iterative algorithm starts with an empty histogram and every iteration aimsto add one entry to this histogram. The entry to be added is selected accordingto a criteria which includes a $\chi^2$ test and a regularization. After arelatively small number of iterations (500 - 1000) the growing reconstructeddistribution converges to the true distribution.
arxiv-1410-4688 | Large Vocabulary Arabic Online Handwriting Recognition System |  http://arxiv.org/abs/1410.4688  | author:Ibrahim Abdelaziz, Sherif Abdou, Hassanin Al-Barhamtoshy category:cs.CV published:2014-10-17 summary:Arabic handwriting is a consonantal and cursive writing. The analysis ofArabic script is further complicated due to obligatory dots/strokes that areplaced above or below most letters and usually written delayed in order. Due toambiguities and diversities of writing styles, recognition systems aregenerally based on a set of possible words called lexicon. When the lexicon issmall, recognition accuracy is more important as the recognition time isminimal. On the other hand, recognition speed as well as the accuracy are bothcritical when handling large lexicons. Arabic is rich in morphology and syntaxwhich makes its lexicon large. Therefore, a practical online handwritingrecognition system should be able to handle a large lexicon with reasonableperformance in terms of both accuracy and time. In this paper, we introduce afully-fledged Hidden Markov Model (HMM) based system for Arabic onlinehandwriting recognition that provides solutions for most of the difficultiesinherent in recognizing the Arabic script. A new preprocessing technique forhandling the delayed strokes is introduced. We use advanced modeling techniquesfor building our recognition system from the training data to provide moredetailed representation for the differences between the writing units, minimizethe variances between writers in the training data and have a betterrepresentation for the features space. System results are enhanced using anadditional post-processing step with a higher order language model andcross-word HMM models. The system performance is evaluated using two differentdatabases covering small and large lexicons. Our system outperforms thestate-of-art systems for the small lexicon database. Furthermore, it showspromising results (accuracy and time) when supporting large lexicon with thepossibility for adapting the models for specific writers to get even betterresults.
arxiv-1410-4792 | Variational Bayes for Merging Noisy Databases |  http://arxiv.org/abs/1410.4792  | author:Tamara Broderick, Rebecca C. Steorts category:stat.ME stat.ML published:2014-10-17 summary:Bayesian entity resolution merges together multiple, noisy databases andreturns the minimal collection of unique individuals represented, together withtheir true, latent record values. Bayesian methods allow flexible generativemodels that share power across databases as well as principled quantificationof uncertainty for queries of the final, resolved database. However, existingBayesian methods for entity resolution use Markov monte Carlo method (MCMC)approximations and are too slow to run on modern databases containing millionsor billions of records. Instead, we propose applying variational approximationsto allow scalable Bayesian inference in these models. We derive acoordinate-ascent approximation for mean-field variational Bayes, qualitativelycompare our algorithm to existing methods, note unique challenges for inferencethat arise from the expected distribution of cluster sizes in entityresolution, and discuss directions for future work in this domain.
arxiv-1410-4868 | A Modality Lexicon and its use in Automatic Tagging |  http://arxiv.org/abs/1410.4868  | author:Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Nathaniel W. Filardo, Lori Levin, Christine Piatko category:cs.CL I.2.7 published:2014-10-17 summary:This paper describes our resource-building results for an eight-week JHUHuman Language Technology Center of Excellence Summer Camp for Applied LanguageExploration (SCALE-2009) on Semantically-Informed Machine Translation.Specifically, we describe the construction of a modality annotation scheme, amodality lexicon, and two automated modality taggers that were built using thelexicon and annotation scheme. Our annotation scheme is based on identifyingthree components of modality: a trigger, a target and a holder. We describe howour modality lexicon was produced semi-automatically, expanding from an initialhand-selected list of modality trigger words and phrases. The resultingexpanded modality lexicon is being made publicly available. We demonstrate thatone tagger---a structure-based tagger---results in precision around 86%(depending on genre) for tagging of a standard LDC data set. In a machinetranslation application, using the structure-based tagger to annotate Englishmodalities on an English-Urdu training corpus improved the translation qualityscore for Urdu by 0.3 Bleu points in the face of sparse training data.
arxiv-1410-4650 | Randomized Structural Sparsity via Constrained Block Subsampling for Improved Sensitivity of Discriminative Voxel Identification |  http://arxiv.org/abs/1410.4650  | author:Yilun Wang, Junjie Zheng, Sheng Zhang, Xujun Duan, Huafu Chen category:cs.CV stat.ML G.3, I.5.2 published:2014-10-17 summary:In this paper, we consider voxel selection for functional Magnetic ResonanceImaging (fMRI) brain data with the aim of finding a more complete set ofprobably correlated discriminative voxels, thus improving interpretation of thediscovered potential biomarkers. The main difficulty in doing this is anextremely high dimensional voxel space and few training samples, resulting inunreliable feature selection. In order to deal with the difficulty, stabilityselection has received a great deal of attention lately, especially due to itsfinite sample control of false discoveries and transparent principle forchoosing a proper amount of regularization. However, it fails to make explicituse of the correlation property or structural information of thesediscriminative features and leads to large false negative rates. In otherwords, many relevant but probably correlated discriminative voxels are missed.Thus, we propose a new variant on stability selection "randomized structuralsparsity", which incorporates the idea of structural sparsity. Numericalexperiments demonstrate that our method can be superior in controlling forfalse negatives while also keeping the control of false positives inheritedfrom stability selection.
arxiv-1410-4821 | Convex Optimization in Julia |  http://arxiv.org/abs/1410.4821  | author:Madeleine Udell, Karanveer Mohan, David Zeng, Jenny Hong, Steven Diamond, Stephen Boyd category:math.OC cs.MS stat.ML published:2014-10-17 summary:This paper describes Convex, a convex optimization modeling framework inJulia. Convex translates problems from a user-friendly functional language intoan abstract syntax tree describing the problem. This concise representation ofthe global structure of the problem allows Convex to infer whether the problemcomplies with the rules of disciplined convex programming (DCP), and to passthe problem to a suitable solver. These operations are carried out in Juliausing multiple dispatch, which dramatically reduces the time required to verifyDCP compliance and to parse a problem into conic form. Convex thenautomatically chooses an appropriate backend solver to solve the conic formproblem.
arxiv-1410-4863 | Arabic Language Text Classification Using Dependency Syntax-Based Feature Selection |  http://arxiv.org/abs/1410.4863  | author:Yannis Haralambous, Yassir Elidrissi, Philippe Lenca category:cs.CL published:2014-10-17 summary:We study the performance of Arabic text classification combining varioustechniques: (a) tfidf vs. dependency syntax, for feature selection andweighting; (b) class association rules vs. support vector machines, forclassification. The Arabic text is used in two forms: rootified and lightlystemmed. The results we obtain show that lightly stemmed text leads to betterperformance than rootified text; that class association rules are better suitedfor small feature sets obtained by dependency syntax constraints; and, finally,that support vector machines are better suited for large feature sets based onmorphological feature selection criteria.
arxiv-1410-5703 | Robust Multidimensional Mean-Payoff Games are Undecidable |  http://arxiv.org/abs/1410.5703  | author:Yaron Velner category:cs.LO cs.LG published:2014-10-17 summary:Mean-payoff games play a central role in quantitative synthesis andverification. In a single-dimensional game a weight is assigned to everytransition and the objective of the protagonist is to assure a non-negativelimit-average weight. In the multidimensional setting, a weight vector isassigned to every transition and the objective of the protagonist is to satisfya boolean condition over the limit-average weight of each dimension, e.g.,$\LimAvg(x_1) \leq 0 \vee \LimAvg(x_2)\geq 0 \wedge \LimAvg(x_3) \geq 0$. Werecently proved that when one of the players is restricted to finite-memorystrategies then the decidability of determining the winner is inter-reduciblewith Hilbert's Tenth problem over rationals (a fundamental long-standing openproblem). In this work we allow arbitrary (infinite-memory) strategies for bothplayers and we show that the problem is undecidable.
arxiv-1410-4627 | Learning visual biases from human imagination |  http://arxiv.org/abs/1410.4627  | author:Carl Vondrick, Hamed Pirsiavash, Aude Oliva, Antonio Torralba category:cs.CV published:2014-10-17 summary:Although the human visual system can recognize many concepts underchallenging conditions, it still has some biases. In this paper, we investigatewhether we can extract these biases and transfer them into a machinerecognition system. We introduce a novel method that, inspired by well-knowntools in human psychophysics, estimates the biases that the human visual systemmight use for recognition, but in computer vision feature spaces. Ourexperiments are surprising, and suggest that classifiers from the human visualsystem can be transferred into a machine with some success. Since theseclassifiers seem to capture favorable biases in the human visual system, wefurther present an SVM formulation that constrains the orientation of the SVMhyperplane to agree with the bias from human visual system. Our results suggestthat transferring this human bias into machines may help object recognitionsystems generalize across datasets and perform better when very little trainingdata is available.
arxiv-1410-4812 | Inference and Mixture Modeling with the Elliptical Gamma Distribution |  http://arxiv.org/abs/1410.4812  | author:Reshad Hosseini, Suvrit Sra, Lucas Theis, Matthias Bethge category:stat.CO math.OC stat.ML published:2014-10-17 summary:We study modeling and inference with the Elliptical Gamma Distribution (EGD).We consider maximum likelihood (ML) estimation for EGD scatter matrices, a taskfor which we develop new fixed-point algorithms. Our algorithms are efficientand converge to global optima despite nonconvexity. Moreover, they turn out tobe much faster than both a well-known iterative algorithm of Kent & Tyler(1991) and sophisticated manifold optimization algorithms. Subsequently, weinvoke our ML algorithms as subroutines for estimating parameters of a mixtureof EGDs. We illustrate our methods by applying them to model natural imagestatistics---the proposed EGD mixture model yields the most parsimonious modelamong several competing approaches.
arxiv-1410-4828 | Generalized Conditional Gradient for Sparse Estimation |  http://arxiv.org/abs/1410.4828  | author:Yaoliang Yu, Xinhua Zhang, Dale Schuurmans category:math.OC cs.LG stat.ML published:2014-10-17 summary:Structured sparsity is an important modeling tool that expands theapplicability of convex formulations for data analysis, however it also createssignificant challenges for efficient algorithm design. In this paper weinvestigate the generalized conditional gradient (GCG) algorithm for solvingstructured sparse optimization problems---demonstrating that, with someenhancements, it can provide a more efficient alternative to current state ofthe art approaches. After providing a comprehensive overview of the convergenceproperties of GCG, we develop efficient methods for evaluating polar operators,a subroutine that is required in each GCG iteration. In particular, we show howthe polar operator can be efficiently evaluated in two important scenarios:dictionary learning and structured sparse estimation. A further improvement isachieved by interleaving GCG with fixed-rank local subspace optimization. Aseries of experiments on matrix completion, multi-class classification,multi-view dictionary learning and overlapping group lasso shows that theproposed method can significantly reduce the training cost of currentalternatives.
arxiv-1410-4355 | Multi-Level Anomaly Detection on Time-Varying Graph Data |  http://arxiv.org/abs/1410.4355  | author:Robert A. Bridges, John Collins, Erik M. Ferragut, Jason Laska, Blair D. Sullivan category:cs.SI cs.LG stat.ML published:2014-10-16 summary:This work presents a novel modeling and analysis framework for graphsequences which addresses the challenge of detecting and contextualizinganomalies in labelled, streaming graph data. We introduce a generalization ofthe BTER model of Seshadhri et al. by adding flexibility to communitystructure, and use this model to perform multi-scale graph anomaly detection.Specifically, probability models describing coarse subgraphs are built byaggregating probabilities at finer levels, and these closely relatedhierarchical models simultaneously detect deviations from expectation. Thistechnique provides insight into a graph's structure and internal context thatmay shed light on a detected event. Additionally, this multi-scale analysisfacilitates intuitive visualizations by allowing users to narrow focus from ananomalous graph to particular subgraphs or nodes causing the anomaly. For evaluation, two hierarchical anomaly detectors are tested against abaseline Gaussian method on a series of sampled graphs. We demonstrate that ourgraph statistics-based approach outperforms both a distribution-based detectorand the baseline in a labeled setting with community structure, and itaccurately detects anomalies in synthetic and real-world datasets at the node,subgraph, and graph levels. To illustrate the accessibility of information madepossible via this technique, the anomaly detector and an associated interactivevisualization tool are tested on NCAA football data, where teams andconferences that moved within the league are identified with perfect recall,and precision greater than 0.786.
arxiv-1410-4343 | Enhanced Multiobjective Evolutionary Algorithm based on Decomposition for Solving the Unit Commitment Problem |  http://arxiv.org/abs/1410.4343  | author:Anupam Trivedi, Kunal Pal, Chiranjib Saha, Dipti Srinivasan category:cs.NE 68T04 published:2014-10-16 summary:The unit commitment (UC) problem is a nonlinear, high-dimensional, highlyconstrained, mixed-integer power system optimization problem and is generallysolved in the literature considering minimizing the system operation cost asthe only objective. However, due to increasing environmental concerns, therecent attention has shifted to incorporating emission in the problemformulation. In this paper, a multi-objective evolutionary algorithm based ondecomposition (MOEA/D) is proposed to solve the UC problem as a multi-objectiveoptimization problem considering minimizing cost and emission as the multipleobjec- tives. Since, UC problem is a mixed-integer optimization problemconsisting of binary UC variables and continuous power dispatch variables, anovel hybridization strategy is proposed within the framework of MOEA/D suchthat genetic algorithm (GA) evolves the binary variables while differentialevolution (DE) evolves the continuous variables. Further, a novel non-uniformweight vector distribution strategy is proposed and a parallel island modelbased on combination of MOEA/D with uniform and non-uniform weight vectordistribution strategy is implemented to enhance the performance of thepresented algorithm. Extensive case studies are presented on different testsystems and the effectiveness of the proposed hybridization strategy, thenon-uniform weight vector distribution strategy and parallel island model isverified through stringent simulated results. Further, exhaustive benchmarkingagainst the algorithms proposed in the literature is presented to demonstratethe superiority of the proposed algorithm in obtaining significantly betterconverged and uniformly distributed trade-off solutions.
arxiv-1410-4281 | Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition |  http://arxiv.org/abs/1410.4281  | author:Xiangang Li, Xihong Wu category:cs.CL cs.NE published:2014-10-16 summary:Long short-term memory (LSTM) based acoustic modeling methods have recentlybeen shown to give state-of-the-art performance on some speech recognitiontasks. To achieve a further performance improvement, in this research, deepextensions on LSTM are investigated considering that deep hierarchical modelhas turned out to be more efficient than a shallow one. Motivated by previousresearch on constructing deep recurrent neural networks (RNNs), alternativedeep LSTM architectures are proposed and empirically evaluated on a largevocabulary conversational telephone speech recognition task. Meanwhile,regarding to multi-GPU devices, the training process for LSTM networks isintroduced and discussed. Experimental results demonstrate that the deep LSTMnetworks benefit from the depth and yield the state-of-the-art performance onthis task.
arxiv-1410-4604 | Domain-Independent Optimistic Initialization for Reinforcement Learning |  http://arxiv.org/abs/1410.4604  | author:Marlos C. Machado, Sriram Srinivasan, Michael Bowling category:cs.LG cs.AI published:2014-10-16 summary:In Reinforcement Learning (RL), it is common to use optimistic initializationof value functions to encourage exploration. However, such an approachgenerally depends on the domain, viz., the scale of the rewards must be known,and the feature representation must have a constant norm. We present a simpleapproach that performs optimistic initialization with less dependence on thedomain.
arxiv-1410-4573 | Learning a hyperplane regressor by minimizing an exact bound on the VC dimension |  http://arxiv.org/abs/1410.4573  | author:Jayadeva, Suresh Chandra, Siddarth Sabharwal, Sanjit S. Batra category:cs.LG I.5.1; I.5.2 published:2014-10-16 summary:The capacity of a learning machine is measured by its Vapnik-Chervonenkisdimension, and learning machines with a low VC dimension generalize better. Itis well known that the VC dimension of SVMs can be very large or unbounded,even though they generally yield state-of-the-art learning performance. In thispaper, we show how to learn a hyperplane regressor by minimizing an exact, or\boldmath{$\Theta$} bound on its VC dimension. The proposed approach, termed asthe Minimal Complexity Machine (MCM) Regressor, involves solving a simplelinear programming problem. Experimental results show, that on a number ofbenchmark datasets, the proposed approach yields regressors with error ratesmuch less than those obtained with conventional SVM regresssors, while oftenusing fewer support vectors. On some benchmark datasets, the number of supportvectors is less than one tenth the number used by SVMs, indicating that the MCMdoes indeed learn simpler representations.
arxiv-1410-4521 | Reconstructive Sparse Code Transfer for Contour Detection and Semantic Labeling |  http://arxiv.org/abs/1410.4521  | author:Michael Maire, Stella X. Yu, Pietro Perona category:cs.CV published:2014-10-16 summary:We frame the task of predicting a semantic labeling as a sparsereconstruction procedure that applies a target-specific learned transferfunction to a generic deep sparse code representation of an image. Thisstrategy partitions training into two distinct stages. First, in anunsupervised manner, we learn a set of generic dictionaries optimized forsparse coding of image patches. We train a multilayer representation viarecursive sparse dictionary learning on pooled codes output by earlier layers.Second, we encode all training images with the generic dictionaries and learn atransfer function that optimizes reconstruction of patches extracted fromannotated ground-truth given the sparse codes of their corresponding imagepatches. At test time, we encode a novel image using the generic dictionariesand then reconstruct using the transfer function. The output reconstruction isa semantic labeling of the test image. Applying this strategy to the task of contour detection, we demonstrateperformance competitive with state-of-the-art systems. Unlike almost all priorwork, our approach obviates the need for any form of hand-designed features orfilters. To illustrate general applicability, we also show initial results onsemantic part labeling of human faces. The effectiveness of our approach opens new avenues for research on deepsparse representations. Our classifiers utilize this representation in a novelmanner. Rather than acting on nodes in the deepest layer, they attach to nodesalong a slice through multiple layers of the network in order to makepredictions about local patches. Our flexible combination of a generativelylearned sparse representation with discriminatively trained transferclassifiers extends the notion of sparse reconstruction to encompass arbitrarysemantic labeling tasks.
arxiv-1410-4441 | Improve CAPTCHA's Security Using Gaussian Blur Filter |  http://arxiv.org/abs/1410.4441  | author:Ariyan Zarei category:cs.CV published:2014-10-16 summary:Providing security for webservers against unwanted and automatedregistrations has become a big concern. To prevent these kinds of falseregistrations many websites use CAPTCHAs. Among all kinds of CAPTCHAs OCR-Basedor visual CAPTCHAs are very common. Actually visual CAPTCHA is an imagecontaining a sequence of characters. So far most of visual CAPTCHAs, in orderto resist against OCR programs, use some common implementations such aswrapping the characters, random placement and rotations of characters, etc. Inthis paper we applied Gaussian Blur filter, which is an image transformation,to visual CAPTCHAs to reduce their readability by OCR programs. We concludedthat this technique made CAPTCHAs almost unreadable for OCR programs but, theirreadability by human users still remained high.
arxiv-1410-4470 | MKL-RT: Multiple Kernel Learning for Ratio-trace Problems via Convex Optimization |  http://arxiv.org/abs/1410.4470  | author:Raviteja Vemulapalli, Vinay Praneeth Boda, Rama Chellappa category:cs.CV cs.LG published:2014-10-16 summary:In the recent past, automatic selection or combination of kernels (orfeatures) based on multiple kernel learning (MKL) approaches has been receivingsignificant attention from various research communities. Though MKL has beenextensively studied in the context of support vector machines (SVM), it isrelatively less explored for ratio-trace problems. In this paper, we show thatMKL can be formulated as a convex optimization problem for a general class ofratio-trace problems that encompasses many popular algorithms used in variouscomputer vision applications. We also provide an optimization procedure that isguaranteed to converge to the global optimum of the proposed optimizationproblem. We experimentally demonstrate that the proposed MKL approach, which werefer to as MKL-RT, can be successfully used to select features fordiscriminative dimensionality reduction and cross-modal retrieval. We also showthat the proposed convex MKL-RT approach performs better than the recentlyproposed non-convex MKL-DR approach.
arxiv-1410-5329 | Naive Bayes and Text Classification I - Introduction and Theory |  http://arxiv.org/abs/1410.5329  | author:Sebastian Raschka category:cs.LG published:2014-10-16 summary:Naive Bayes classifiers, a family of classifiers that are based on thepopular Bayes' probability theorem, are known for creating simple yet wellperforming models, especially in the fields of document classification anddisease prediction. In this article, we will look at the main concepts of naiveBayes classification in the context of document categorization.
arxiv-1410-4599 | Non-parametric Bayesian Learning with Deep Learning Structure and Its Applications in Wireless Networks |  http://arxiv.org/abs/1410.4599  | author:Erte Pan, Zhu Han category:cs.LG cs.NE cs.NI stat.ML published:2014-10-16 summary:In this paper, we present an infinite hierarchical non-parametric Bayesianmodel to extract the hidden factors over observed data, where the number ofhidden factors for each layer is unknown and can be potentially infinite.Moreover, the number of layers can also be infinite. We construct the modelstructure that allows continuous values for the hidden factors and weights,which makes the model suitable for various applications. We use theMetropolis-Hastings method to infer the model structure. Then the performanceof the algorithm is evaluated by the experiments. Simulation results show thatthe model fits the underlying structure of simulated data.
arxiv-1410-7679 | Super-resolution method using sparse regularization for point-spread function recovery |  http://arxiv.org/abs/1410.7679  | author:Fred Maurice Ngolè Mboula, Jean-Luc Starck, Samuel Ronayette, Koryo Okumura, Jérôme Amiaux category:cs.CV astro-ph.IM published:2014-10-16 summary:In large-scale spatial surveys, such as the forthcoming ESA Euclid mission,images may be undersampled due to the optical sensors sizes. Therefore, one mayconsider using a super-resolution (SR) method to recover aliased frequencies,prior to further analysis. This is particularly relevant for point-sourceimages, which provide direct measurements of the instrument point-spreadfunction (PSF). We introduce SPRITE, SParse Recovery of InsTrumental rEsponse,which is an SR algorithm using a sparse analysis prior. We show that such aprior provides significant improvements over existing methods, especially onlow SNR PSFs.
arxiv-1410-4341 | Implicit segmentation of Kannada characters in offline handwriting recognition using hidden Markov models |  http://arxiv.org/abs/1410.4341  | author:Manasij Venkatesh, Vikas Majjagi, Deepu Vijayasenan category:cs.LG cs.CV published:2014-10-16 summary:We describe a method for classification of handwritten Kannada charactersusing Hidden Markov Models (HMMs). Kannada script is agglutinative, wheresimple shapes are concatenated horizontally to form a character. This resultsin a large number of characters making the task of classification difficult.Character segmentation plays a significant role in reducing the number ofclasses. Explicit segmentation techniques suffer when overlapping shapes arepresent, which is common in the case of handwritten text. We use HMMs to takeadvantage of the agglutinative nature of Kannada script, which allows us toperform implicit segmentation of characters along with recognition. All theexperiments are performed on the Chars74k dataset that consists of 657handwritten characters collected across multiple users. Gradient-based featuresare extracted from individual characters and are used to train character HMMs.The use of implicit segmentation technique at the character level resulted inan improvement of around 10%. This system also outperformed an existing systemtested on the same dataset by around 16%. Analysis based on learning curvesshowed that increasing the training data could result in better accuracy.Accordingly, we collected additional data and obtained an improvement of 4%with 6 additional samples.
arxiv-1410-4393 | The HAWKwood Database |  http://arxiv.org/abs/1410.4393  | author:Christopher Herbon category:cs.CV published:2014-10-16 summary:We present a database consisting of wood pile images, which can be used as abenchmark to evaluate the performance of wood pile detection and surveyingalgorithms. We distinguish six database cate- gories which can be used fordifferent types of algorithms. Images of real and synthetic scenes areprovided, which consist of 7655 images divided into 354 data sets. Depending onthe category the data sets either include ground truth data or forestryspecific measurements with which algorithms may be compared.
arxiv-1410-4391 | Multivariate Spearman's rho for aggregating ranks using copulas |  http://arxiv.org/abs/1410.4391  | author:Justin Bedo, Cheng Soon Ong category:stat.ML cs.LG published:2014-10-16 summary:We study the problem of rank aggregation: given a set of ranked lists, wewant to form a consensus ranking. Furthermore, we consider the case of extremelists: i.e., only the rank of the best or worst elements are known. We imputemissing ranks by the average value and generalise Spearman's \rho to extremeranks. Our main contribution is the derivation of a non-parametric estimatorfor rank aggregation based on multivariate extensions of Spearman's \rho, whichmeasures correlation between a set of ranked lists. Multivariate Spearman's\rho is defined using copulas, and we show that the geometric mean ofnormalised ranks maximises multivariate correlation. Motivated by this, wepropose a weighted geometric mean approach for learning to rank which has aclosed form least squares solution. When only the best or worst elements of aranked list are known, we impute the missing ranks by the average value,allowing us to apply Spearman's \rho. Finally, we demonstrate good performanceon the rank aggregation benchmarks MQ2007 and MQ2008.
arxiv-1410-7632 | On the Covariance of ICP-based Scan-matching Techniques |  http://arxiv.org/abs/1410.7632  | author:Silvère Bonnabel, Martin Barczyk, François Goulette category:cs.CV cs.RO cs.SY published:2014-10-16 summary:This paper considers the problem of estimating the covariance ofroto-translations computed by the Iterative Closest Point (ICP) algorithm. Theproblem is relevant for localization of mobile robots and vehicles equippedwith depth-sensing cameras (e.g., Kinect) or Lidar (e.g., Velodyne). Theclosed-form formulas for covariance proposed in previous literature generallybuild upon the fact that the solution to ICP is obtained by minimizing a linearleast-squares problem. In this paper, we show this approach needs cautionbecause the rematching step of the algorithm is not explicitly accounted for,and applying it to the point-to-point version of ICP leads to completelyerroneous covariances. We then provide a formal mathematical proof why theapproach is valid in the point-to-plane version of ICP, which validates theintuition and experimental results of practitioners.
arxiv-1410-4445 | Patterns in the English Language: Phonological Networks, Percolation and Assembly Models |  http://arxiv.org/abs/1410.4445  | author:Massimo Stella, Markus Brede category:cs.CL published:2014-10-16 summary:In this paper we provide a quantitative framework for the study ofphonological networks (PNs) for the English language by carrying out principledcomparisons to null models, either based on site percolation, randomizationtechniques, or network growth models. In contrast to previous work, we mainlyfocus on null models that reproduce lower order characteristics of theempirical data. We find that artificial networks matching connectivityproperties of the English PN are exceedingly rare: this leads to the hypothesisthat the word repertoire might have been assembled over time by preferentiallyintroducing new words which are small modifications of old words. Our nullmodels are able to explain the "power-law-like" part of the degreedistributions and generally retrieve qualitative features of the PN such ashigh clustering, high assortativity coefficient, and small-worldcharacteristics. However, the detailed comparison to expectations from nullmodels also points out significant differences, suggesting the presence ofadditional constraints in word assembly. Key constraints we identify are theavoidance of large degrees, the avoidance of triadic closure, and the avoidanceof large non-percolating clusters.
arxiv-1410-4461 | Map Matching based on Conditional Random Fields and Route Preference Mining for Uncertain Trajectories |  http://arxiv.org/abs/1410.4461  | author:Xu Ming, Du Yi-man, Wu Jian-ping, Zhou Yang category:cs.NI cs.LG published:2014-10-16 summary:In order to improve offline map matching accuracy of low-sampling-rate GPS, amap matching algorithm based on conditional random fields (CRF) and routepreference mining is proposed. In this algorithm, road offset distance and thetemporal-spatial relationship between the sampling points are used as featuresof GPS trajectory in CRF model, which can utilize the advantages of integratingthe context information into features flexibly. When the sampling rate is toolow, it is difficult to guarantee the effectiveness using temporal-spatialcontext modeled in CRF, and route preference of a driver is used asreplenishment to be superposed on the temporal-spatial transition features. Theexperimental results show that this method can improve the accuracy of thematching, especially in the case of low sampling rate.
arxiv-1410-4485 | A Gesture Recognition System for Detecting Behavioral Patterns of ADHD |  http://arxiv.org/abs/1410.4485  | author:Miguel Ángel Bautista, Antonio Hernández-Vela, Sergio Escalera, Laura Igual, Oriol Pujol, Josep Moya, Verónica Violant, María Teresa Anguera category:cs.CV published:2014-10-16 summary:We present an application of gesture recognition using an extension ofDynamic Time Warping (DTW) to recognize behavioural patterns of AttentionDeficit Hyperactivity Disorder (ADHD). We propose an extension of DTW usingone-class classifiers in order to be able to encode the variability of agesture category, and thus, perform an alignment between a gesture sample and agesture class. We model the set of gesture samples of a certain gesturecategory using either GMMs or an approximation of Convex Hulls. Thus, we add atheoretical contribution to classical warping path in DTW by including localmodeling of intra-class gesture variability. This methodology is applied in aclinical context, detecting a group of ADHD behavioural patterns defined byexperts in psychology/psychiatry, to provide support to clinicians in thediagnose procedure. The proposed methodology is tested on a novel multi-modaldataset (RGB plus Depth) of ADHD children recordings with behavioural patterns.We obtain satisfying results when compared to standard state-of-the-artapproaches in the DTW context.
arxiv-1410-4510 | Graph-Sparse LDA: A Topic Model with Structured Sparsity |  http://arxiv.org/abs/1410.4510  | author:Finale Doshi-Velez, Byron Wallace, Ryan Adams category:stat.ML cs.CL cs.LG published:2014-10-16 summary:Originally designed to model text, topic modeling has become a powerful toolfor uncovering latent structure in domains including medicine, finance, andvision. The goals for the model vary depending on the application: in somecases, the discovered topics may be used for prediction or some otherdownstream task. In other cases, the content of the topic itself may be ofintrinsic scientific interest. Unfortunately, even using modern sparse techniques, the discovered topics areoften difficult to interpret due to the high dimensionality of the underlyingspace. To improve topic interpretability, we introduce Graph-Sparse LDA, ahierarchical topic model that leverages knowledge of relationships betweenwords (e.g., as encoded by an ontology). In our model, topics are summarized bya few latent concept-words from the underlying graph that explain the observedwords. Graph-Sparse LDA recovers sparse, interpretable summaries on tworeal-world biomedical datasets while matching state-of-the-art predictionperformance.
arxiv-1410-3915 | Spotting Suspicious Link Behavior with fBox: An Adversarial Perspective |  http://arxiv.org/abs/1410.3915  | author:Neil Shah, Alex Beutel, Brian Gallagher, Christos Faloutsos category:cs.LG cs.IR cs.SI published:2014-10-15 summary:How can we detect suspicious users in large online networks? Onlinepopularity of a user or product (via follows, page-likes, etc.) can bemonetized on the premise of higher ad click-through rates or increased sales.Web services and social networks which incentivize popularity thus suffer froma major problem of fake connections from link fraudsters looking to make aquick buck. Typical methods of catching this suspicious behavior use spectraltechniques to spot large groups of often blatantly fraudulent (but sometimeshonest) users. However, small-scale, stealthy attacks may go unnoticed due tothe nature of low-rank eigenanalysis used in practice. In this work, we take an adversarial approach to find and prove claims aboutthe weaknesses of modern, state-of-the-art spectral methods and propose fBox,an algorithm designed to catch small-scale, stealth attacks that slip below theradar. Our algorithm has the following desirable properties: (a) it hastheoretical underpinnings, (b) it is shown to be highly effective on real dataand (c) it is scalable (linear on the input size). We evaluate fBox on a large,public 41.7 million node, 1.5 billion edge who-follows-whom social graph fromTwitter in 2010 and with high precision identify many suspicious accounts whichhave persisted without suspension even to this day.
arxiv-1410-3932 | Detection of Salient Regions in Crowded Scenes |  http://arxiv.org/abs/1410.3932  | author:Mei Kuan Lim, Chee Seng Chan, Dorothy Monekosso, Paolo Remagnino category:cs.CV published:2014-10-15 summary:The increasing number of cameras and a handful of human operators to monitorthe video inputs from hundreds of cameras leave the system ill equipped tofulfil the task of detecting anomalies. Thus, there is a dire need toautomatically detect regions that require immediate attention for a moreeffective and proactive surveillance. We propose a framework that utilises thetemporal variations in the flow field of a crowd scene to automatically detectsalient regions, while eliminating the need to have prior knowledge of thescene or training. We deem the flow fields to be a dynamic system and adopt thestability theory of dynamical systems, to determine the motion dynamics withina given area. In the context of this work, salient regions refer to areas withhigh motion dynamics, where points in a particular region are unstable.Experimental results on public, crowd scenes have shown the effectiveness ofthe proposed method in detecting salient regions which correspond to unstableflow, occlusions, bottlenecks, entries and exits.
arxiv-1410-4013 | A two-pass fuzzy-geno approach to pattern classification |  http://arxiv.org/abs/1410.4013  | author:Subhadip Basu, Mahantapas Kundu, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2014-10-15 summary:The work presents an extension of the fuzzy approach to 2-D shape recognition[1] through refinement of initial or coarse classification decisions under atwo pass approach. In this approach, an unknown pattern is classified byrefining possible classification decisions obtained through coarseclassification of the same. To build a fuzzy model of a pattern classhorizontal and vertical fuzzy partitions on the sample images of the class areoptimized using genetic algorithm. To make coarse classification decisionsabout an unknown pattern, the fuzzy representation of the pattern is comparedwith models of all pattern classes through a specially designed similaritymeasure. Coarse classification decisions are refined in the second pass toobtain the final classification decision of the unknown pattern. To do so,optimized horizontal and vertical fuzzy partitions are again created on certainregions of the image frame, specific to each group of similar type of patternclasses. It is observed through experiments that the technique improves theoverall recognition rate from 86.2%, in the first pass, to 90.4% after thesecond pass, with 500 training samples of handwritten digits.
arxiv-1410-3935 | A Logic-based Approach to Generatively Defined Discriminative Modeling |  http://arxiv.org/abs/1410.3935  | author:Taisuke Sato, Keiichi Kubota, Yoshitaka Kameya category:cs.LG published:2014-10-15 summary:Conditional random fields (CRFs) are usually specified by graphical modelsbut in this paper we propose to use probabilistic logic programs and specifythem generatively. Our intension is first to provide a unified approach to CRFsfor complex modeling through the use of a Turing complete language and secondto offer a convenient way of realizing generative-discriminative pairs inmachine learning to compare generative and discriminative models and choose thebest model. We implemented our approach as the D-PRISM language by modifyingPRISM, a logic-based probabilistic modeling language for generative modeling,while exploiting its dynamic programming mechanism for efficient probabilitycomputation. We tested D-PRISM with logistic regression, a linear-chain CRF anda CRF-CFG and empirically confirmed their excellent discriminative performancecompared to their generative counterparts, i.e.\ naive Bayes, an HMM and aPCFG. We also introduced new CRF models, CRF-BNCs and CRF-LCGs. They are CRFversions of Bayesian network classifiers and probabilistic left-corner grammarsrespectively and easily implementable in D-PRISM. We empirically showed thatthey outperform their generative counterparts as expected.
arxiv-1410-4017 | Online Tracking of Skin Colour Regions Against a Complex Background |  http://arxiv.org/abs/1410.4017  | author:Subhadip Basu, S. Chakraborty, K. Mukherjee, S. K. Pandit category:cs.CV published:2014-10-15 summary:Online tracking of human activity against a complex background is achallenging task for many applications. In this paper, we have developed arobust technique for localizing skin colour regions from unconstrained imageframes. A simple and fast segmentation algorithm is used to train a multiplayerperceptron (MLP) for detection of skin colours. Stepper motors are synchronizedwith the MLP to track the movement of the skin colour regions.
arxiv-1410-4062 | Complexity Issues and Randomization Strategies in Frank-Wolfe Algorithms for Machine Learning |  http://arxiv.org/abs/1410.4062  | author:Emanuele Frandi, Ricardo Nanculef, Johan Suykens category:stat.ML cs.LG cs.NA math.OC published:2014-10-15 summary:Frank-Wolfe algorithms for convex minimization have recently gainedconsiderable attention from the Optimization and Machine Learning communities,as their properties make them a suitable choice in a variety of applications.However, as each iteration requires to optimize a linear model, a cleverimplementation is crucial to make such algorithms viable on large-scaledatasets. For this purpose, approximation strategies based on a random samplinghave been proposed by several researchers. In this work, we perform anexperimental study on the effectiveness of these techniques, analyze possiblealternatives and provide some guidelines based on our results.
arxiv-1410-3910 | High Order Structure Descriptors for Scene Images |  http://arxiv.org/abs/1410.3910  | author:Wenya Zhu, Xiankai Lu, Tao Xu, Ziyi Zhao category:cs.CV published:2014-10-15 summary:Structure information is ubiquitous in natural scene images and it plays animportant role in scene representation. In this paper, third order structurestatistics (TOSS) and fourth order structure statistics (FOSS) are exploited toencode higher order structure information. Afterwards, based on the radial andnormal slice of TOSS and FOSS, we propose the high order structure feature:third order structure feature (TOSF) and fourth order structure feature (FOSF).It is well known that scene images are well characterized by particulararrangements of their local structures, we divide the scene image into thenon-overlapping sub-regions and compute the proposed higher order structuralfeatures among them. Then a scene classification is performed by using SVMclassifier with these higher order structure features. The experimental resultsshow that higher order structure statistics can deliver image structureinformation well and its spatial envelope has strong discriminative ability.
arxiv-1410-3970 | Shape and Color Object Tracking for Real-Time Robotic Navigation |  http://arxiv.org/abs/1410.3970  | author:Haythem Ghazouani category:cs.CV published:2014-10-15 summary:This paper presents a real-time approach for single-colored ball detectionand tracking. The approach consists of two main phases. In a first offlinecalibration phase, the intrinsic parameters of the camera and the radialdistortion are estimated, and a classification of colors is learned from asample image of colored balls. The second phase consists of four main steps:(1) color segmentation of the input image into several regions based on theoffline classification, (2) robust estimation of the circle parameters (3)refinement of the circle parameters, and (4) ball tracking. The experimentalresults showed that the approach presents a good compromise between suitabilityfor real-time navigation and robustness to occlusions, background congestionand colors interference in the scene.
arxiv-1410-4176 | Learning Distributed Word Representations for Natural Logic Reasoning |  http://arxiv.org/abs/1410.4176  | author:Samuel R. Bowman, Christopher Potts, Christopher D. Manning category:cs.CL published:2014-10-15 summary:Natural logic offers a powerful relational conception of meaning that is anatural counterpart to distributed semantic representations, which have provenvaluable in a wide range of sophisticated language tasks. However, it remainsan open question whether it is possible to train distributed representations tosupport the rich, diverse logical reasoning captured by natural logic. Weaddress this question using two neural network-based models for learningembeddings: plain neural networks and neural tensor networks. Our experimentsevaluate the models' ability to learn the basic algebra of natural logicrelations from simulated data and from the WordNet noun graph. The overallpositive results are promising for the future of learned distributedrepresentations in the applied modeling of logical semantics.
arxiv-1410-4210 | Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets |  http://arxiv.org/abs/1410.4210  | author:Jie Wang, Jieping Ye category:cs.LG published:2014-10-15 summary:Sparse-Group Lasso (SGL) has been shown to be a powerful regression techniquefor simultaneously discovering group and within-group sparse patterns by usinga combination of the $\ell_1$ and $\ell_2$ norms. However, in large-scaleapplications, the complexity of the regularizers entails great computationalchallenges. In this paper, we propose a novel Two-Layer Feature REductionmethod (TLFre) for SGL via a decomposition of its dual feasible set. Thetwo-layer reduction is able to quickly identify the inactive groups and theinactive features, respectively, which are guaranteed to be absent from thesparse representation and can be removed from the optimization. Existingfeature reduction methods are only applicable for sparse models with onesparsity-inducing regularizer. To our best knowledge, TLFre is the first onethat is capable of dealing with multiple sparsity-inducing regularizers.Moreover, TLFre has a very low computational cost and can be integrated withany existing solvers. We also develop a screening method---called DPC(DecomPosition of Convex set)---for the nonnegative Lasso problem. Experimentson both synthetic and real data sets show that TLFre and DPC improve theefficiency of SGL and nonnegative Lasso by several orders of magnitude.
arxiv-1410-4012 | Online interpretation of numeric sign language using 2-d skeletal model |  http://arxiv.org/abs/1410.4012  | author:Subhadip Basu, S. Dey, K. Mukherjee, T. S. Jana category:cs.CV published:2014-10-15 summary:Gesturing is one of the natural modes of human communication. Signs producedby gestures can have a basic meaning coupled with additional information thatis layered over the basic meaning of the sign. Sign language is an importantexample of communicative gestures that are highly structured and well acceptedacross the world as a communication medium for deaf and dumb. In this paper, anonline recognition scheme is proposed to interpret the standard numeric signlanguage comprising of 10 basic hand symbols. A web camera is used to capturethe real time hand movements as input to the system. The basic meaning of thehand gesture is extracted from the input data frame by analysing the shape ofthe hand, considering its orientation, movement and location to be fixed. Theinput hand shape is processed to identify the palm structure, fingertips andtheir relative positions and the presence of the extended thumb. A2-dimensional skeletal model is generated from the acquired shape informationto represent and subsequently interpret the basic meaning of the hand gesture.
arxiv-1410-4009 | Thompson sampling with the online bootstrap |  http://arxiv.org/abs/1410.4009  | author:Dean Eckles, Maurits Kaptein category:cs.LG stat.CO stat.ML 68W27, 62L05 G.3; I.2.6 published:2014-10-15 summary:Thompson sampling provides a solution to bandit problems in which newobservations are allocated to arms with the posterior probability that an armis optimal. While sometimes easy to implement and asymptotically optimal,Thompson sampling can be computationally demanding in large scale banditproblems, and its performance is dependent on the model fit to the observeddata. We introduce bootstrap Thompson sampling (BTS), a heuristic method forsolving bandit problems which modifies Thompson sampling by replacing theposterior distribution used in Thompson sampling by a bootstrap distribution.We first explain BTS and show that the performance of BTS is competitive toThompson sampling in the well-studied Bernoulli bandit case. Subsequently, wedetail why BTS using the online bootstrap is more scalable than regularThompson sampling, and we show through simulation that BTS is more robust to amisspecified error distribution. BTS is an appealing modification of Thompsonsampling, especially when samples from the posterior are otherwise notavailable or are costly.
arxiv-1410-3905 | Efficient Image Categorization with Sparse Fisher Vector |  http://arxiv.org/abs/1410.3905  | author:Xiankai Lu, Zheng Fang, Tao Xu, Haiting Zhang, Hongya Tuo category:cs.CV published:2014-10-15 summary:In object recognition, Fisher vector (FV) representation is one of thestate-of-art image representations ways at the expense of dense, highdimensional features and increased computation time. A simplification of FV isattractive, so we propose Sparse Fisher vector (SFV). By incorporating localitystrategy, we can accelerate the Fisher coding step in image categorizationwhich is implemented from a collective of local descriptors. Combining withpooling step, we explore the relationship between coding step and pooling stepto give a theoretical explanation about SFV. Experiments on benchmark datasetshave shown that SFV leads to a speedup of several-fold of magnitude compareswith FV, while maintaining the categorization performance. In addition, wedemonstrate how SFV preserves the consistence in representation of similarlocal features.
arxiv-1410-3916 | Memory Networks |  http://arxiv.org/abs/1410.3916 | author:Jason Weston, Sumit Chopra, Antoine Bordes category:cs.AI cs.CL stat.ML published:2014-10-15 summary:We describe a new class of learning models called memory networks. Memorynetworks reason with inference components combined with a long-term memorycomponent; they learn how to use these jointly. The long-term memory can beread and written to, with the goal of using it for prediction. We investigatethese models in the context of question answering (QA) where the long-termmemory effectively acts as a (dynamic) knowledge base, and the output is atextual response. We evaluate them on a large-scale QA task, and a smaller, butmore complex, toy task generated from a simulated world. In the latter, we showthe reasoning power of such models by chaining multiple supporting sentences toanswer questions that require understanding the intension of verbs.
arxiv-1410-3831 | An exact mapping between the Variational Renormalization Group and Deep Learning |  http://arxiv.org/abs/1410.3831  | author:Pankaj Mehta, David J. Schwab category:stat.ML cs.LG cs.NE published:2014-10-14 summary:Deep learning is a broad set of techniques that uses multiple layers ofrepresentation to automatically learn relevant features directly fromstructured data. Recently, such techniques have yielded record-breaking resultson a diverse set of difficult machine learning tasks in computer vision, speechrecognition, and natural language processing. Despite the enormous success ofdeep learning, relatively little is understood theoretically about why thesetechniques are so successful at feature learning and compression. Here, we showthat deep learning is intimately related to one of the most important andsuccessful techniques in theoretical physics, the renormalization group (RG).RG is an iterative coarse-graining scheme that allows for the extraction ofrelevant features (i.e. operators) as a physical system is examined atdifferent length scales. We construct an exact mapping from the variationalrenormalization group, first introduced by Kadanoff, and deep learningarchitectures based on Restricted Boltzmann Machines (RBMs). We illustratethese ideas using the nearest-neighbor Ising Model in one and two-dimensions.Our results suggests that deep learning algorithms may be employing ageneralized RG-like scheme to learn relevant features from data.
arxiv-1410-3886 | Tighter Low-rank Approximation via Sampling the Leveraged Element |  http://arxiv.org/abs/1410.3886  | author:Srinadh Bhojanapalli, Prateek Jain, Sujay Sanghavi category:cs.DS cs.LG stat.ML published:2014-10-14 summary:In this work, we propose a new randomized algorithm for computing a low-rankapproximation to a given matrix. Taking an approach different from existingliterature, our method first involves a specific biased sampling, with anelement being chosen based on the leverage scores of its row and column, andthen involves weighted alternating minimization over the factored form of theintended low-rank matrix, to minimize error only on these samples. Our methodcan leverage input sparsity, yet produce approximations in {\em spectral} (asopposed to the weaker Frobenius) norm; this combines the best aspects ofotherwise disparate current results, but with a dependence on the conditionnumber $\kappa = \sigma_1/\sigma_r$. In particular we require $O(nnz(M) +\frac{n\kappa^2 r^5}{\epsilon^2})$ computations to generate a rank-$r$approximation to $M$ in spectral norm. In contrast, the best existing methodrequires $O(nnz(M)+ \frac{nr^2}{\epsilon^4})$ time to compute an approximationin Frobenius norm. Besides the tightness in spectral norm, we have a betterdependence on the error $\epsilon$. Our method is naturally and highlyparallelizable. Our new approach enables two extensions that are interesting on their own.The first is a new method to directly compute a low-rank approximation (inefficient factored form) to the product of two given matrices; it computes asmall random set of entries of the product, and then executes weightedalternating minimization (as before) on these. The sampling strategy isdifferent because now we cannot access leverage scores of the product matrix(but instead have to work with input matrices). The second extension is animproved algorithm with smaller communication complexity for the distributedPCA setting (where each server has small set of rows of the matrix, and want tocompute low rank approximation with small amount of communication with otherservers).
arxiv-1410-3595 | A stochastic behavior analysis of stochastic restricted-gradient descent algorithm in reproducing kernel Hilbert spaces |  http://arxiv.org/abs/1410.3595  | author:Masa-aki Takizawa, Masahiro Yukawa, Cedric Richard category:cs.LG stat.ML published:2014-10-14 summary:This paper presents a stochastic behavior analysis of a kernel-basedstochastic restricted-gradient descent method. The restricted gradient gives asteepest ascent direction within the so-called dictionary subspace. Theanalysis provides the transient and steady state performance in the meansquared error criterion. It also includes stability conditions in the mean andmean-square sense. The present study is based on the analysis of the kernelnormalized least mean square (KNLMS) algorithm initially proposed by Chen etal. Simulation results validate the analysis.
arxiv-1410-3596 | Detection of cheating by decimation algorithm |  http://arxiv.org/abs/1410.3596  | author:Shogo Yamanaka, Masayuki Ohzeki, Aurelien Decelle category:stat.ML cs.LG published:2014-10-14 summary:We expand the item response theory to study the case of "cheating students"for a set of exams, trying to detect them by applying a greedy algorithm ofinference. This extended model is closely related to the Boltzmann machinelearning. In this paper we aim to infer the correct biases and interactions ofour model by considering a relatively small number of sets of training data.Nevertheless, the greedy algorithm that we employed in the present studyexhibits good performance with a few number of training data. The key point isthe sparseness of the interactions in our problem in the context of theBoltzmann machine learning: the existence of cheating students is expected tobe very rare (possibly even in real world). We compare a standard approach toinfer the sparse interactions in the Boltzmann machine learning to our greedyalgorithm and we find the latter to be superior in several aspects.
arxiv-1410-3791 | POLYGLOT-NER: Massive Multilingual Named Entity Recognition |  http://arxiv.org/abs/1410.3791  | author:Rami Al-Rfou, Vivek Kulkarni, Bryan Perozzi, Steven Skiena category:cs.CL cs.LG I.2.7; I.2.6 published:2014-10-14 summary:The increasing diversity of languages used on the web introduces a new levelof complexity to Information Retrieval (IR) systems. We can no longer assumethat textual content is written in one language or even the same languagefamily. In this paper, we demonstrate how to build massive multilingualannotators with minimal human expertise and intervention. We describe a systemthat builds Named Entity Recognition (NER) annotators for 40 major languagesusing Wikipedia and Freebase. Our approach does not require NER human annotateddatasets or language specific resources like treebanks, parallel corpora, andorthographic rules. The novelty of approach lies therein - using only languageagnostic techniques, while achieving competitive performance. Our method learns distributed word representations (word embeddings) whichencode semantic and syntactic features of words in each language. Then, weautomatically generate datasets from Wikipedia link structure and Freebaseattributes. Finally, we apply two preprocessing stages (oversampling and exactsurface form matching) which do not require any linguistic expertise. Our evaluation is two fold: First, we demonstrate the system performance onhuman annotated datasets. Second, for languages where no gold-standardbenchmarks are available, we propose a new method, distant evaluation, based onstatistical machine translation.
arxiv-1410-3699 | A graph Laplacian regularization for hyperspectral data unmixing |  http://arxiv.org/abs/1410.3699  | author:Rita Ammanouil, André Ferrari, Cédric Richard category:cs.CV published:2014-10-14 summary:This paper introduces a graph Laplacian regularization in the hyperspectralunmixing formulation. The proposed regularization relies upon the constructionof a graph representation of the hyperspectral image. Each node in the graphrepresents a pixel's spectrum, and edges connect spectrally and spatiallysimilar pixels. The proposed graph framework promotes smoothness in theestimated abundance maps and collaborative estimation between homogeneous areasof the image. The resulting convex optimization problem is solved using theAlternating Direction Method of Multipliers (ADMM). A special attention isgiven to the computational complexity of the algorithm, and Graph-cut methodsare proposed in order to reduce the computational burden. Finally, simulationsconducted on synthetic data illustrate the effectiveness of the graph Laplacianregularization with respect to other classical regularizations forhyperspectral unmixing.
arxiv-1410-3756 | Crowd Saliency Detection via Global Similarity Structure |  http://arxiv.org/abs/1410.3756  | author:Mei Kuan Lim, Ven Jyn Kok, Chen Change Loy, Chee Seng Chan category:cs.CV stat.ML published:2014-10-14 summary:It is common for CCTV operators to overlook inter- esting events taking placewithin the crowd due to large number of people in the crowded scene (i.e.marathon, rally). Thus, there is a dire need to automate the detection ofsalient crowd regions acquiring immediate attention for a more effective andproactive surveillance. This paper proposes a novel framework to identify andlocalize salient regions in a crowd scene, by transforming low-level featuresextracted from crowd motion field into a global similarity structure. Theglobal similarity structure representation allows the discovery of theintrinsic manifold of the motion dynamics, which could not be captured by thelow-level representation. Ranking is then performed on the global similaritystructure to identify a set of extrema. The proposed approach is unsupervisedso learning stage is eliminated. Experimental results on public datasetsdemonstrates the effectiveness of exploiting such extrema in identifyingsalient regions in various crowd scenarios that exhibit crowding, localirregular motion, and unique motion areas such as sources and sinks.
arxiv-1410-3726 | Scene Image is Non-Mutually Exclusive - A Fuzzy Qualitative Scene Understanding |  http://arxiv.org/abs/1410.3726  | author:Chern Hong Lim, Anhar Risnumawan, Chee Seng Chan category:cs.CV cs.AI cs.IR published:2014-10-14 summary:Ambiguity or uncertainty is a pervasive element of many real world decisionmaking processes. Variation in decisions is a norm in this situation when thesame problem is posed to different subjects. Psychological and metaphysicalresearch had proven that decision making by human is subjective. It isinfluenced by many factors such as experience, age, background, etc. Sceneunderstanding is one of the computer vision problems that fall into thiscategory. Conventional methods relax this problem by assuming scene images aremutually exclusive; and therefore, focus on developing different approaches toperform the binary classification tasks. In this paper, we show that sceneimages are non-mutually exclusive, and propose the Fuzzy Qualitative RankClassifier (FQRC) to tackle the aforementioned problems. The proposed FQRCprovides a ranking interpretation instead of binary decision. Evaluations interm of qualitative and quantitative using large numbers and challenging publicscene datasets have shown the effectiveness of our proposed method in modelingthe non-mutually exclusive scene images.
arxiv-1410-3752 | Enhanced Random Forest with Image/Patch-Level Learning for Image Understanding |  http://arxiv.org/abs/1410.3752  | author:Wai Lam Hoo, Tae-Kyun Kim, Yuru Pei, Chee Seng Chan category:cs.CV stat.ML published:2014-10-14 summary:Image understanding is an important research domain in the computer visiondue to its wide real-world applications. For an image understanding frameworkthat uses the Bag-of-Words model representation, the visual codebook is anessential part. Random forest (RF) as a tree-structure discriminative codebookhas been a popular choice. However, the performance of the RF can be degradedif the local patch labels are poorly assigned. In this paper, we tackle thisproblem by a novel way to update the RF codebook learning for a morediscriminative codebook with the introduction of the soft class labels,estimated from the pLSA model based on a feedback scheme. The feedback schemeis performed on both the image and patch levels respectively, which is incontrast to the state- of-the-art RF codebook learning that focused on eitherimage or patch level only. Experiments on 15-Scene and C-Pascal datasets hadshown the effectiveness of the proposed method in image understanding task.
arxiv-1410-3751 | A Fusion Approach for Efficient Human Skin Detection |  http://arxiv.org/abs/1410.3751  | author:Wei Ren Tan, Chee Seng Chan, Pratheepan Yogarajah, Joan Condell category:cs.CV stat.ML published:2014-10-14 summary:A reliable human skin detection method that is adaptable to different humanskin colours and illu- mination conditions is essential for better human skinsegmentation. Even though different human skin colour detection solutions havebeen successfully applied, they are prone to false skin detection and are notable to cope with the variety of human skin colours across different ethnic.Moreover, existing methods require high computational cost. In this paper, wepropose a novel human skin de- tection approach that combines a smoothed 2Dhistogram and Gaussian model, for automatic human skin detection in colourimage(s). In our approach an eye detector is used to refine the skin model fora specific person. The proposed approach reduces computational costs as notraining is required; and it improves the accuracy of skin detection despitewide variation in ethnicity and illumination. To the best of our knowledge,this is the first method to employ fusion strategy for this purpose.Qualitative and quantitative results on three standard public datasets and acomparison with state-of-the-art methods have shown the effectiveness androbustness of the proposed approach.
arxiv-1410-3748 | Zero-Shot Object Recognition System based on Topic Model |  http://arxiv.org/abs/1410.3748  | author:Wai Lam Hoo, Chee Seng Chan category:cs.CV stat.ML published:2014-10-14 summary:Object recognition systems usually require fully complete manually labeledtraining data to train the classifier. In this paper, we study the problem ofobject recognition where the training samples are missing during the classifierlearning stage, a task also known as zero-shot learning. We propose a novelzero-shot learning strategy that utilizes the topic model and hierarchicalclass concept. Our proposed method advanced where cumbersome human annotationstage (i.e. attribute-based classification) is eliminated. We achievecomparable performance with state-of-the-art algorithms in four publicdatasets: PubFig (67.09%), Cifar-100 (54.85%), Caltech-256 (52.14%), andAnimals with Attributes (49.65%) when unseen classes exist in theclassification task.
arxiv-1410-3541 | Memcomputing with membrane memcapacitive systems |  http://arxiv.org/abs/1410.3541  | author:Yuriy V. Pershin, Fabio L. Traversa, Massimiliano Di Ventra category:cs.ET cs.NE published:2014-10-14 summary:We show theoretically that networks of membrane memcapacitive systems --capacitors with memory made out of membrane materials -- can be used to performa complete set of logic gates in a massively parallel way by simply changingthe external input amplitudes, but not the topology of the network. Thispolymorphism is an important characteristic of memcomputing (computing withmemories) that closely reproduces one of the main features of the brain. Apractical realization of these membrane memcapacitive systems, using, e.g.,graphene or other 2D materials, would be a step forward towards a solid-staterealization of memcomputing with passive devices.
arxiv-1410-3864 | Multi-Agent Shape Formation and Tracking Inspired from a Social Foraging Dynamics |  http://arxiv.org/abs/1410.3864  | author:Debdipta Goswami, Chiranjib Saha, Kunal Pal, Swagatam Das category:cs.NE cs.RO 70F04 published:2014-10-14 summary:Principle of Swarm Intelligence has recently found widespread application information control and automated tracking by the automated multi-agent system.This article proposes an elegant and effective method inspired by foragingdynamics to produce geometric-patterns by the search agents. Starting from arandom initial orientation, it is investigated how the foraging dynamics can bemodified to achieve convergence of the agents on the desired pattern withalmost uniform density. Guided through the proposed dynamics, the agents canalso track a moving point by continuously circulating around the point. Ananalytical treatment supported with computer simulation results is provided tobetter understand the convergence behaviour of the system.
arxiv-1410-3744 | Refined Particle Swarm Intelligence Method for Abrupt Motion Tracking |  http://arxiv.org/abs/1410.3744  | author:Mei Kuan Lim, Chee Seng Chan, Dorothy Monekosso, Paolo Remagnino category:cs.CV cs.NE published:2014-10-14 summary:Conventional tracking solutions are not feasible in handling abrupt motion asthey are based on smooth motion assumption or an accurate motion model. Abruptmotion is not subject to motion continuity and smoothness. To assuage this, wedeem tracking as an optimisation problem and propose a novel abrupt motiontracker that based on swarm intelligence - the SwaTrack. Unlike existingswarm-based filtering methods, we first of all introduce an optimisedswarm-based sampling strategy to tradeoff between the exploration andexploitation of the search space in search for the optimal proposaldistribution. Secondly, we propose Dynamic Acceleration Parameters (DAP) allowon the fly tuning of the best mean and variance of the distribution forsampling. Such innovating idea of combining these strategies in an ingeniousway in the PSO framework to handle the abrupt motion, which so far no existingworks are found. Experimental results in both quantitative and qualitative hadshown the effectiveness of the proposed method in tracking abrupt motions.
arxiv-1410-3169 | Multi-Scale Local Shape Analysis and Feature Selection in Machine Learning Applications |  http://arxiv.org/abs/1410.3169  | author:Paul Bendich, Ellen Gasparovic, John Harer, Rauf Izmailov, Linda Ness category:cs.CG cs.LG math.AT stat.ML published:2014-10-13 summary:We introduce a method called multi-scale local shape analysis, or MLSA, forextracting features that describe the local structure of points within adataset. The method uses both geometric and topological features at multiplelevels of granularity to capture diverse types of local information forsubsequent machine learning algorithms operating on the dataset. Usingsynthetic and real dataset examples, we demonstrate significant performanceimprovement of classification algorithms constructed for these datasets withcorrespondingly augmented features.
arxiv-1410-3234 | Markov Random Fields and Mass Spectra Discrimination |  http://arxiv.org/abs/1410.3234  | author:Ao Kong, Robert Azencott category:stat.ML stat.AP stat.CO 62P10, 68T10 published:2014-10-13 summary:For mass spectra acquired from cancer patients by MALDI or SELDI techniques,automated discrimination between cancer types or stages has often beenimplemented by machine learnings. These techniques typically generate"black-box" classifiers, which are difficult to interpret biologically. Wedevelop new and efficient signature discovery algorithms leading tointerpretable signatures combining the discriminating power of explicitlyselected small groups of biomarkers, identified by their m/z ratios. Ourapproach is based on rigorous stochastic modeling of "homogeneous" datasets ofmass spectra by a versatile class of parameterized Markov Random Fields. Wepresent detailed algorithms validated by precise theoretical results. We alsooutline the successful tests of our approach to generate efficient explicitsignatures for six benchmark discrimination tasks, based on mass spectraacquired from colorectal cancer patients, as well as from ovarian cancerpatients.
arxiv-1410-3517 | Convex Modeling of Interactions with Strong Heredity |  http://arxiv.org/abs/1410.3517  | author:Asad Haris, Daniela Witten, Noah Simon category:stat.ML published:2014-10-13 summary:We consider the task of fitting a regression model involving interactionsamong a potentially large set of covariates, in which we wish to enforce strongheredity. We propose FAMILY, a very general framework for this task. Ourproposal is a generalization of several existing methods, such as VANISH[Radchenko and James, 2010], hierNet [Bien et al., 2013], the all-pairs lasso,and the lasso using only main effects. It can be formulated as the solution toa convex optimization problem, which we solve using an efficient alternatingdirections method of multipliers (ADMM) algorithm. This algorithm hasguaranteed convergence to the global optimum, can be easily specialized to anyconvex penalty function of interest, and allows for a straightforward extensionto the setting of generalized linear models. We derive an unbiased estimator ofthe degrees of freedom of FAMILY, and explore its performance in a simulationstudy and on an HIV sequence data set.
arxiv-1410-3192 | Learning without Concentration for General Loss Functions |  http://arxiv.org/abs/1410.3192  | author:Shahar Mendelson category:stat.ML K.3.2 published:2014-10-13 summary:We study prediction and estimation problems using empirical riskminimization, relative to a general convex loss function. We obtain sharp errorrates even when concentration is false or is very restricted, for example, inheavy-tailed scenarios. Our results show that the error rate depends on twoparameters: one captures the intrinsic complexity of the class, and essentiallyleads to the error rate in a noise-free (or realizable) problem; the othermeasures interactions between class members the target and the loss, and isdominant when the problem is far from realizable. We also explain how one maydeal with outliers by choosing the loss in a way that is calibrated to theintrinsic complexity of the class and to the noise-level of the problem (thelatter is measured by the distance between the target and the class).
arxiv-1410-3351 | Coarse Ricci curvature with applications to manifold learning problem |  http://arxiv.org/abs/1410.3351  | author:Antonio G. Ache, Micah W. Warren category:math.DG cs.LG math.MG stat.ML 53 published:2014-10-13 summary:Consider a sample of $n$ points taken i.i.d from a submanifold of Euclideanspace. This defines a metric measure space. We show that there is an explicitset of scales $t_{n}\rightarrow0$ such that a coarse Ricci curvature at scale$t_{n}$ on this metric measure space converges almost surely to the coarseRicci curvature of the underlying manifold.
arxiv-1410-3349 | Single Image Super Resolution via Manifold Approximation |  http://arxiv.org/abs/1410.3349  | author:Chinh Dang, Hayder Radha category:cs.CV published:2014-10-13 summary:Image super-resolution remains an important research topic to overcome thelimitations of physical acquisition systems, and to support the development ofhigh resolution displays. Previous example-based super-resolution approachesmainly focus on analyzing the co-occurrence properties of low resolution andhigh-resolution patches. Recently, we proposed a novel single imagesuper-resolution approach based on linear manifold approximation of thehigh-resolution image-patch space [1]. The image super-resolution problem isthen formulated as an optimization problem of searching for the best matchedhigh resolution patch in the manifold for a given low-resolution patch. Wedeveloped a novel technique based on the l1 norm sparse graph to learn a set oflow dimensional affine spaces or tangent subspaces of the high-resolution patchmanifold. The optimization problem is then solved based on the learned set oftangent subspaces. In this paper, we build on our recent work as follows.First, we consider and analyze each tangent subspace as one point in aGrassmann manifold, which helps to compute geodesic pairwise distances amongthese tangent subspaces. Second, we develop a min-max algorithm to select anoptimal subset of tangent subspaces. This optimal subset reduces thecomputational cost while still preserving the quality of the reconstructedhigh-resolution image. Third, and to further achieve lower computationalcomplexity, we perform hierarchical clustering on the optimal subset based onGrassmann manifold distances. Finally, we analytically prove the validity ofthe proposed Grassmann-distance based clustering. A comparison of the obtainedresults with other state-of-the-art methods clearly indicates the viability ofthe new proposed framework.
arxiv-1410-3314 | Propagation Kernels |  http://arxiv.org/abs/1410.3314  | author:Marion Neumann, Roman Garnett, Christian Bauckhage, Kristian Kersting category:stat.ML cs.LG published:2014-10-13 summary:We introduce propagation kernels, a general graph-kernel framework forefficiently measuring the similarity of structured data. Propagation kernelsare based on monitoring how information spreads through a set of given graphs.They leverage early-stage distributions from propagation schemes such as randomwalks to capture structural information encoded in node labels, attributes, andedge information. This has two benefits. First, off-the-shelf propagationschemes can be used to naturally construct kernels for many graph types,including labeled, partially labeled, unlabeled, directed, and attributedgraphs. Second, by leveraging existing efficient and informative propagationschemes, propagation kernels can be considerably faster than state-of-the-artapproaches without sacrificing predictive performance. We will also show thatif the graphs at hand have a regular structure, for instance when modelingimage or video data, one can exploit this regularity to scale the kernelcomputation to large databases of graphs with thousands of nodes. We supportour contributions by exhaustive experiments on a number of real-world graphsfrom a variety of application domains.
arxiv-1410-3426 | Computing Topology Preservation of RBF Transformations for Landmark-Based Image Registration |  http://arxiv.org/abs/1410.3426  | author:R. Cavoretto, A. De Rossi, H. Qiao, B. Quatember, W. Recheis, M. Mayr category:math.NA cs.CV published:2014-10-13 summary:In image registration, a proper transformation should be topology preserving.Especially for landmark-based image registration, if the displacement of onelandmark is larger enough than those of neighbourhood landmarks, topologyviolation will be occurred. This paper aim to analyse the topology preservationof some Radial Basis Functions (RBFs) which are used to model deformations inimage registration. Mat\'{e}rn functions are quite common in the statisticliterature (see, e.g. \cite{Matern86,Stein99}). In this paper, we use them tosolve the landmark-based image registration problem. We present the topologypreservation properties of RBFs in one landmark and four landmarks modelrespectively. Numerical results of three kinds of Mat\'{e}rn transformationsare compared with results of Gaussian, Wendland's, and Wu's functions.
arxiv-1410-3469 | Enhanced Higgs to $τ^+τ^-$ Searches with Deep Learning |  http://arxiv.org/abs/1410.3469  | author:Pierre Baldi, Peter Sadowski, Daniel Whiteson category:hep-ph cs.LG hep-ex published:2014-10-13 summary:The Higgs boson is thought to provide the interaction that imparts mass tothe fundamental fermions, but while measurements at the Large Hadron Collider(LHC) are consistent with this hypothesis, current analysis techniques lack thestatistical power to cross the traditional 5$\sigma$ significance barrierwithout more data. \emph{Deep learning} techniques have the potential toincrease the statistical power of this analysis by \emph{automatically}learning complex, high-level data representations. In this work, deep neuralnetworks are used to detect the decay of the Higgs to a pair of tau leptons. ABayesian optimization algorithm is used to tune the network architecture andtraining algorithm hyperparameters, resulting in a deep network of eightnon-linear processing layers that improves upon the performance of shallowclassifiers even without the use of features specifically engineered byphysicists for this application. The improvement in discovery significance isequivalent to an increase in the accumulated dataset of 25\%.
arxiv-1410-3386 | Testing Poisson Binomial Distributions |  http://arxiv.org/abs/1410.3386  | author:Jayadev Acharya, Constantinos Daskalakis category:cs.DS cs.IT cs.LG math.IT published:2014-10-13 summary:A Poisson Binomial distribution over $n$ variables is the distribution of thesum of $n$ independent Bernoullis. We provide a sample near-optimal algorithmfor testing whether a distribution $P$ supported on $\{0,...,n\}$ to which wehave sample access is a Poisson Binomial distribution, or far from all PoissonBinomial distributions. The sample complexity of our algorithm is $O(n^{1/4})$to which we provide a matching lower bound. We note that our sample complexityimproves quadratically upon that of the naive "learn followed by tolerant-test"approach, while instance optimal identity testing [VV14] is not applicablesince we are looking to simultaneously test against a whole family ofdistributions.
arxiv-1410-3348 | Fast Multilevel Support Vector Machines |  http://arxiv.org/abs/1410.3348  | author:Talayeh Razzaghi, Ilya Safro category:stat.ML cs.LG published:2014-10-13 summary:Solving different types of optimization models (including parameters fitting)for support vector machines on large-scale training data is often an expensivecomputational task. This paper proposes a multilevel algorithmic framework thatscales efficiently to very large data sets. Instead of solving the wholetraining set in one optimization process, the support vectors are obtained andgradually refined at multiple levels of coarseness of the data. The proposedframework includes: (a) construction of hierarchy of large-scale data coarserepresentations, and (b) a local processing of updating the hyperplanethroughout this hierarchy. Our multilevel framework substantially improves thecomputational time without loosing the quality of classifiers. The algorithmsare demonstrated for both regular and weighted support vector machines.Experimental results are presented for balanced and imbalanced classificationproblems. Quality improvement on several imbalanced data sets has beenobserved.
arxiv-1410-3462 | Tag Relevance Fusion for Social Image Retrieval |  http://arxiv.org/abs/1410.3462  | author:Xirong Li category:cs.IR cs.CV H.3.3 published:2014-10-13 summary:Due to the subjective nature of social tagging, measuring the relevance ofsocial tags with respect to the visual content is crucial for retrieving theincreasing amounts of social-networked images. Witnessing the limit of a singlemeasurement of tag relevance, we introduce in this paper tag relevance fusionas an extension to methods for tag relevance estimation. We present asystematic study, covering tag relevance fusion in early and late stages, andin supervised and unsupervised settings. Experiments on a large present-daybenchmark set show that tag relevance fusion leads to better image retrieval.Moreover, unsupervised tag relevance fusion is found to be practically aseffective as supervised tag relevance fusion, but without the need of anytraining efforts. This finding suggests the potential of tag relevance fusionfor real-world deployment.
arxiv-1410-3460 | Sentiment Analysis based on User Tag for Traditional Chinese Medicine in Weibo |  http://arxiv.org/abs/1410.3460  | author:Junhui Shen, Peiyan Zhu, Rui Fan, Wei Tan category:cs.CL cs.SI published:2014-10-13 summary:With the acceptance of Western culture and science, Traditional ChineseMedicine (TCM) has become a controversial issue in China. So, it's important tostudy the public's sentiment and opinion on TCM. The rapid development ofonline social network, such as twitter, make it convenient and efficient tosample hundreds of millions of people for the aforementioned sentiment study.To the best of our knowledge, the present work is the first attempt thatapplies sentiment analysis to the domain of TCM on Sina Weibo (a twitter-likemicroblogging service in China). In our work, firstly we collect tweets topicabout TCM from Sina Weibo, and label the tweets as supporting TCM and opposingTCM automatically based on user tag. Then, a support vector machine classifierhas been built to predict the sentiment of TCM tweets without labels. Finally,we present a method to adjust the classifier result. The performance ofF-measure attained with our method is 97%.
arxiv-1410-3463 | Mining Block I/O Traces for Cache Preloading with Sparse Temporal Non-parametric Mixture of Multivariate Poisson |  http://arxiv.org/abs/1410.3463  | author:Lavanya Sita Tekumalla, Chiranjib Bhattacharyya category:cs.OS cs.LG cs.SY published:2014-10-13 summary:Existing caching strategies, in the storage domain, though well suited toexploit short range spatio-temporal patterns, are unable to leverage long-rangemotifs for improving hitrates. Motivated by this, we investigate novel Bayesiannon-parametric modeling(BNP) techniques for count vectors, to capture longrange correlations for cache preloading, by mining Block I/O traces. Suchtraces comprise of a sequence of memory accesses that can be aggregated intohigh-dimensional sparse correlated count vector sequences. While there are several state of the art BNP algorithms for clustering andtheir temporal extensions for prediction, there has been no work on exploringthese for correlated count vectors. Our first contribution addresses this gapby proposing a DP based mixture model of Multivariate Poisson (DP-MMVP) and itstemporal extension(HMM-DP-MMVP) that captures the full covariance structure ofmultivariate count data. However, modeling full covariance structure for countvectors is computationally expensive, particularly for high dimensional data.Hence, we exploit sparsity in our count vectors, and as our main contribution,introduce the Sparse DP mixture of multivariate Poisson(Sparse-DP-MMVP),generalizing our DP-MMVP mixture model, also leading to more efficientinference. We then discuss a temporal extension to our model for cachepreloading. We take the first step towards mining historical data, to capture long rangepatterns in storage traces for cache preloading. Experimentally, we show adramatic improvement in hitrates on benchmark traces and lay the groundwork forfurther research in storage domain to reduce latencies using data miningtechniques to capture long range motifs.
arxiv-1410-3111 | Hierarchical models for neural population dynamics in the presence of non-stationarity |  http://arxiv.org/abs/1410.3111  | author:Mijung Park, Jakob H. Macke category:stat.ML q-bio.NC published:2014-10-12 summary:Neural population activity often exhibits rich variability and temporalstructure. This variability is thought to arise from single-neuronstochasticity, neural dynamics on short time-scales, as well as frommodulations of neural firing properties on long time-scales, often referred toas "non-stationarity". To better understand the nature of co-variability inneural circuits and their impact on cortical information processing, we needstatistical models that are able to capture multiple sources of variability ondifferent time-scales. Here, we introduce a hierarchical statistical model ofneural population activity which models both neural population dynamics as wellas inter-trial modulations in firing rates. In addition, we extend the model toallow us to capture non-stationarities in the population dynamics itself (i.e.,correlations across neurons). We develop variational inference methods for learning model parameters, anddemonstrate that the method can recover non-stationarities in both averagefiring rates and correlation structure. Applied to neural population recordingsfrom anesthetized macaque primary visual cortex, our models provide a betteraccount of the structure of neural firing than stationary dynamics models.
arxiv-1410-3080 | Tree-Structure Bayesian Compressive Sensing for Video |  http://arxiv.org/abs/1410.3080  | author:Xin Yuan, Patrick Llull, David J. Brady, Lawrence Carin category:cs.CV published:2014-10-12 summary:A Bayesian compressive sensing framework is developed for videoreconstruction based on the color coded aperture compressive temporal imaging(CACTI) system. By exploiting the three dimension (3D) tree structure of thewavelet and Discrete Cosine Transformation (DCT) coefficients, a Bayesiancompressive sensing inversion algorithm is derived to reconstruct (up to 22)color video frames from a single monochromatic compressive measurement. Bothsimulated and real datasets are adopted to verify the performance of theproposed algorithm.
arxiv-1410-3145 | Machine Learning Techniques in Cognitive Radio Networks |  http://arxiv.org/abs/1410.3145  | author:Peter Hossain, Adaulfo Komisarczuk, Garin Pawetczak, Sarah Van Dijk, Isabella Axelsen category:cs.LG cs.NI published:2014-10-12 summary:Cognitive radio is an intelligent radio that can be programmed and configureddynamically to fully use the frequency resources that are not used by licensedusers. It defines the radio devices that are capable of learning and adaptingto their transmission to the external radio environment, which means it hassome kind of intelligence for monitoring the radio environment, learning theenvironment and make smart decisions. In this paper, we are reviewing someexamples of the usage of machine learning techniques in cognitive radionetworks for implementing the intelligent radio.
arxiv-1410-3059 | Computabilities of Validity and Satisfiability in Probability Logics over Finite and Countable Models |  http://arxiv.org/abs/1410.3059  | author:Greg Yang category:cs.LO cs.LG math.LO math.PR published:2014-10-12 summary:The $\epsilon$-logic (which is called $\epsilon$E-logic in this paper) ofKuyper and Terwijn is a variant of first order logic with the same syntax, inwhich the models are equipped with probability measures and in which the$\forall x$ quantifier is interpreted as "there exists a set $A$ of measure$\ge 1 - \epsilon$ such that for each $x \in A$, ...." Previously, Kuyper andTerwijn proved that the general satisfiability and validity problems for thislogic are, i) for rational $\epsilon \in (0, 1)$, respectively$\Sigma^1_1$-complete and $\Pi^1_1$-hard, and ii) for $\epsilon = 0$,respectively decidable and $\Sigma^0_1$-complete. The adjective "general" heremeans "uniformly over all languages." We extend these results in the scenario of finite models. In particular, weshow that the problems of satisfiability by and validity over finite models in$\epsilon$E-logic are, i) for rational $\epsilon \in (0, 1)$, respectively$\Sigma^0_1$- and $\Pi^0_1$-complete, and ii) for $\epsilon = 0$, respectivelydecidable and $\Pi^0_1$-complete. Although partial results toward the countablecase are also achieved, the computability of $\epsilon$E-logic over countablemodels still remains largely unsolved. In addition, most of the results, ofthis paper and of Kuyper and Terwijn, do not apply to individual languages witha finite number of unary predicates. Reducing this requirement continues to bea major point of research. On the positive side, we derive the decidability of the correspondingproblems for monadic relational languages --- equality- and function-freelanguages with finitely many unary and zero other predicates. This result holdsfor all three of the unrestricted, the countable, and the finite model cases. Applications in computational learning theory, weighted graphs, and neuralnetworks are discussed in the context of these decidability and undecidabilityresults.
arxiv-1410-2954 | Q-learning for Optimal Control of Continuous-time Systems |  http://arxiv.org/abs/1410.2954  | author:Biao Luo, Derong Liu, Tingwen Huang category:cs.SY stat.ML published:2014-10-11 summary:In this paper, two Q-learning (QL) methods are proposed and their convergencetheories are established for addressing the model-free optimal control problemof general nonlinear continuous-time systems. By introducing the Q-function forcontinuous-time systems, policy iteration based QL (PIQL) and value iterationbased QL (VIQL) algorithms are proposed for learning the optimal control policyfrom real system data rather than using mathematical system model. It is provedthat both PIQL and VIQL methods generate a nonincreasing Q-function sequence,which converges to the optimal Q-function. For implementation of the QLalgorithms, the method of weighted residuals is applied to derived theparameters update rule. The developed PIQL and VIQL algorithms are essentiallyoff-policy reinforcement learning approachs, where the system data can becollected arbitrary and thus the exploration ability is increased. With thedata collected from the real system, the QL methods learn the optimal controlpolicy offline, and then the convergent control policy will be employed to realsystem. The effectiveness of the developed QL algorithms are verified throughcomputer simulation.
arxiv-1410-2959 | Direct Processing of Document Images in Compressed Domain |  http://arxiv.org/abs/1410.2959  | author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV published:2014-10-11 summary:With the rapid increase in the volume of Big data of this digital era, faxdocuments, invoices, receipts, etc are traditionally subjected to compressionfor the efficiency of data storage and transfer. However, in order to processthese documents, they need to undergo the stage of decompression which indentsadditional computing resources. This limitation induces the motivation toresearch on the possibility of directly processing of compressed images. Inthis research paper, we summarize the research work carried out to performdifferent operations straight from run-length compressed documents withoutgoing through the stage of decompression. The different operations demonstratedare feature extraction; text-line, word and character segmentation; documentblock segmentation; and font size detection, all carried out in the compressedversion of the document. Feature extraction methods demonstrate how to extractthe conventionally defined features such as projection profile, run-histogramand entropy, directly from the compressed document data. Document segmentationinvolves the extraction of compressed segments of text-lines, words andcharacters using the vertical and horizontal projection profile features.Further an attempt is made to segment randomly a block of interest from thecompressed document and subsequently facilitate absolute and relativecharacterization of the segmented block which finds real time applications inautomatic processing of Bank Cheques, Challans, etc, in compressed domain.Finally an application to detect font size at text line level is alsoinvestigated. All the proposed algorithms are validated experimentally withsufficient data set of compressed documents.
arxiv-1410-2686 | Polarization Measurement of High Dimensional Social Media Messages With Support Vector Machine Algorithm Using Mapreduce |  http://arxiv.org/abs/1410.2686  | author:Ferhat Özgür Çatak category:cs.LG cs.CL published:2014-10-10 summary:In this article, we propose a new Support Vector Machine (SVM) trainingalgorithm based on distributed MapReduce technique. In literature, there are alots of research that shows us SVM has highest generalization property amongclassification algorithms used in machine learning area. Also, SVM classifiermodel is not affected by correlations of the features. But SVM uses quadraticoptimization techniques in its training phase. The SVM algorithm is formulatedas quadratic optimization problem. Quadratic optimization problem has $O(m^3)$time and $O(m^2)$ space complexity, where m is the training set size. Thecomputation time of SVM training is quadratic in the number of traininginstances. In this reason, SVM is not a suitable classification algorithm forlarge scale dataset classification. To solve this training problem we developeda new distributed MapReduce method developed. Accordingly, (i) SVM algorithm istrained in distributed dataset individually; (ii) then merge all supportvectors of classifier model in every trained node; and (iii) iterate these twosteps until the classifier model converges to the optimal classifier function.In the implementation phase, large scale social media dataset is presented inTFxIDF matrix. The matrix is used for sentiment analysis to get polarizationvalue. Two and three class models are created for classification method.Confusion matrices of each classification model are presented in tables. Socialmedia messages corpus consists of 108 public and 66 private universitiesmessages in Turkey. Twitter is used for source of corpus. Twitter user messagesare collected using Twitter Streaming API. Results are shown in graphics andtables.
arxiv-1410-2871 | An Ontology for Comprehensive Tutoring of Euphonic Conjunctions of Sanskrit Grammar |  http://arxiv.org/abs/1410.2871  | author:S. V. Kasmir Raja, V. Rajitha, Meenakshi Lakshmanan category:cs.CL published:2014-10-10 summary:Euphonic conjunctions (sandhis) form a very important aspect of Sanskritmorphology and phonology. The traditional and modern methods of studying abouteuphonic conjunctions in Sanskrit follow different methodologies. The formerinvolves a rigorous study of the Paninian system embodied in Panini'sAshtadhyayi, while the latter usually involves the study of a few importantsandhi rules with the use of examples. The former is not suitable forbeginners, and the latter, not sufficient to gain a comprehensive understandingof the operation of sandhi rules. This is so since there are not only numeroussandhi rules and exceptions, but also complex precedence rules involved. Theneed for a new ontology for sandhi-tutoring was hence felt. This work presentsa comprehensive ontology designed to enable a student-user to learn in stagesall about euphonic conjunctions and the relevant aphorisms of Sanskrit grammarand to test and evaluate the progress of the student-user. The ontology formsthe basis of a multimedia sandhi tutor that was given to different categoriesof users including Sanskrit scholars for extensive and rigorous testing.
arxiv-1410-2910 | Riesz Logic |  http://arxiv.org/abs/1410.2910  | author:Daoud Clarke category:cs.LO cs.CL published:2014-10-10 summary:We introduce Riesz Logic, whose models are abelian lattice ordered groups,which generalise Riesz spaces (vector lattices), and show soundness andcompleteness. Our motivation is to provide a logic for distributional semanticsof natural language, where words are typically represented as elements of avector space whose dimensions correspond to contexts in which words may occur.This basis provides a lattice ordering on the space, and this ordering may beinterpreted as "distributional entailment". Several axioms of Riesz Logic arefamiliar from Basic Fuzzy Logic, and we show how the models of these two logicsmay be related; Riesz Logic may thus be considered a new fuzzy logic. Inaddition to applications in natural language processing, there is potential forapplying the theory to neuro-fuzzy systems.
arxiv-1410-2838 | Approximate False Positive Rate Control in Selection Frequency for Random Forest |  http://arxiv.org/abs/1410.2838  | author:Ender Konukoglu, Melanie Ganz category:cs.LG stat.ME published:2014-10-10 summary:Random Forest has become one of the most popular tools for feature selection.Its ability to deal with high-dimensional data makes this algorithm especiallyuseful for studies in neuroimaging and bioinformatics. Despite its popularityand wide use, feature selection in Random Forest still lacks a crucialingredient: false positive rate control. To date there is no efficient,principled and computationally light-weight solution to this shortcoming. As aresult, researchers using Random Forest for feature selection have to resort tousing heuristically set thresholds on feature rankings. This article builds anapproximate probabilistic model for the feature selection process in randomforest training, which allows us to compute an estimated false positive ratefor a given threshold on selection frequency. Hence, it presents a principledway to determine thresholds for the selection of relevant features without anyadditional computational load. Experimental analysis with synthetic datademonstrates that the proposed approach can limit false positive rates on theorder of the desired values and keep false negative rates low. Results showthat this holds even in the presence of a complex correlation structure betweenfeatures. Its good statistical properties and light-weight computational needsmake this approach widely applicable to feature selection for a wide-range ofapplications.
arxiv-1410-2786 | New SVD based initialization strategy for Non-negative Matrix Factorization |  http://arxiv.org/abs/1410.2786  | author:Hanli Qiao category:cs.LG cs.NA published:2014-10-10 summary:There are two problems need to be dealt with for Non-negative MatrixFactorization (NMF): choose a suitable rank of the factorization and provide agood initialization method for NMF algorithms. This paper aims to solve thesetwo problems using Singular Value Decomposition (SVD). At first we extract thenumber of main components as the rank, actually this method is inspired from[1, 2]. Second, we use the singular value and its vectors to initialize NMFalgorithm. In 2008, Boutsidis and Gollopoulos [3] provided the method titledNNDSVD to enhance initialization of NMF algorithms. They extracted the positivesection and respective singular triplet information of the unit matrices{C(j)}k j=1 which were obtained from singular vector pairs. This strategy aimsto use positive section to cope with negative elements of the singular vectors,but in experiments we found that even replacing negative elements by theirabsolute values could get better results than NNDSVD. Hence, we give anothermethod based SVD to fulfil initialization for NMF algorithms (SVD-NMF).Numerical experiments on two face databases ORL and YALE [16, 17] show that ourmethod is better than NNDSVD.
arxiv-1410-2663 | Challenge IEEE-ISBI/TCB : Application of Covariance matrices and wavelet marginals |  http://arxiv.org/abs/1410.2663  | author:Florian Yger category:cs.CV published:2014-10-10 summary:This short memo aims at explaining our approach for the challenge IEEE-ISBIon Bone Texture Characterization. In this work, we focus on the use ofcovariance matrices and wavelet marginals in an SVM classifier.
arxiv-1410-2724 | Compressed Sensing With Side Information: Geometrical Interpretation and Performance Bounds |  http://arxiv.org/abs/1410.2724  | author:João F. C. Mota, Nikos Deligiannis, Miguel R. D. Rodrigues category:cs.IT math.IT math.OC stat.ML published:2014-10-10 summary:We address the problem of Compressed Sensing (CS) with side information.Namely, when reconstructing a target CS signal, we assume access to a similarsignal. This additional knowledge, the side information, is integrated into CSvia L1-L1 and L1-L2 minimization. We then provide lower bounds on the number ofmeasurements that these problems require for successful reconstruction of thetarget signal. If the side information has good quality, the number ofmeasurements is significantly reduced via L1-L1 minimization, but not so muchvia L1-L2 minimization. We provide geometrical interpretations and experimentalresults illustrating our findings.
arxiv-1410-2500 | Validation of k-Nearest Neighbor Classifiers Using Inclusion and Exclusion |  http://arxiv.org/abs/1410.2500  | author:Eric Bax, Lingjie Weng, Xu Tian category:cs.LG cs.IT math.IT stat.ML published:2014-10-09 summary:This paper presents a series of PAC exponential error bounds for $k$-nearestneighbors classifiers, with O($n^{-\frac{r}{2r+1}}\sqrt{k \ln n}$) error boundrange for each integer $r>0$, where $n$ is the number of in-sample examples.This shows that $k$-nn classifiers, in spite of their famously fractureddecision boundaries, come close to having Gaussian-style exponential errorbounds with O($n^{-\frac{1}{2}}$) bound ranges.
arxiv-1410-2386 | Bayesian Robust Tensor Factorization for Incomplete Multiway Data |  http://arxiv.org/abs/1410.2386  | author:Qibin Zhao, Guoxu Zhou, Liqing Zhang, Andrzej Cichocki, Shun-ichi Amari category:cs.CV cs.LG published:2014-10-09 summary:We propose a generative model for robust tensor factorization in the presenceof both missing data and outliers. The objective is to explicitly infer theunderlying low-CP-rank tensor capturing the global information and a sparsetensor capturing the local information (also considered as outliers), thusproviding the robust predictive distribution over missing entries. Thelow-CP-rank tensor is modeled by multilinear interactions between multiplelatent factors on which the column sparsity is enforced by a hierarchicalprior, while the sparse tensor is modeled by a hierarchical view of Student-$t$distribution that associates an individual hyperparameter with each elementindependently. For model learning, we develop an efficient closed-formvariational inference under a fully Bayesian treatment, which can effectivelyprevent the overfitting problem and scales linearly with data size. In contrastto existing related works, our method can perform model selection automaticallyand implicitly without need of tuning parameters. More specifically, it candiscover the groundtruth of CP rank and automatically adapt the sparsityinducing priors to various types of outliers. In addition, the tradeoff betweenthe low-rank approximation and the sparse representation can be optimized inthe sense of maximum model evidence. The extensive experiments and comparisonswith many state-of-the-art algorithms on both synthetic and real-world datasetsdemonstrate the superiorities of our method from several perspectives.
arxiv-1410-2479 | Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy and Reverberant Environments |  http://arxiv.org/abs/1410.2479  | author:Andreas Schwarz, Christian Huemmer, Roland Maas, Walter Kellermann category:cs.CL cs.NE cs.SD stat.ML published:2014-10-09 summary:We propose a spatial diffuseness feature for deep neural network (DNN)-basedautomatic speech recognition to improve recognition accuracy in reverberant andnoisy environments. The feature is computed in real-time from multiplemicrophone signals without requiring knowledge or estimation of the directionof arrival, and represents the relative amount of diffuse noise in each timeand frequency bin. It is shown that using the diffuseness feature as anadditional input to a DNN-based acoustic model leads to a reduced word errorrate for the REVERB challenge corpus, both compared to logmelspec featuresextracted from noisy signals, and features enhanced by spectral subtraction.
arxiv-1410-2505 | Recovery of Sparse Signals Using Multiple Orthogonal Least Squares |  http://arxiv.org/abs/1410.2505  | author:Jian Wang, Ping Li category:stat.ME cs.IT cs.LG math.IT published:2014-10-09 summary:We study the problem of recovering sparse signals from compressed linearmeasurements. This problem, often referred to as sparse recovery or sparsereconstruction, has generated a great deal of interest in recent years. Torecover the sparse signals, we propose a new method called multiple orthogonalleast squares (MOLS), which extends the well-known orthogonal least squares(OLS) algorithm by allowing multiple $L$ indices to be chosen per iteration.Owing to inclusion of multiple support indices in each selection, the MOLSalgorithm converges in much fewer iterations and improves the computationalefficiency over the conventional OLS algorithm. Theoretical analysis shows thatMOLS ($L > 1$) performs exact recovery of all $K$-sparse signals within $K$iterations if the measurement matrix satisfies the restricted isometry property(RIP) with isometry constant $\delta_{LK} < \frac{\sqrt{L}}{\sqrt{K} + 2\sqrt{L}}.$ The recovery performance of MOLS in the noisy scenario is alsostudied. It is shown that stable recovery of sparse signals can be achievedwith the MOLS algorithm when the signal-to-noise ratio (SNR) scales linearlywith the sparsity level of input signals.
arxiv-1410-2653 | Distributed Estimation, Information Loss and Exponential Families |  http://arxiv.org/abs/1410.2653  | author:Qiang Liu, Alexander Ihler category:stat.ML published:2014-10-09 summary:Distributed learning of probabilistic models from multiple data repositorieswith minimum communication is increasingly important. We study a simplecommunication-efficient learning framework that first calculates the localmaximum likelihood estimates (MLE) based on the data subsets, and then combinesthe local MLEs to achieve the best possible approximation to the global MLEgiven the whole dataset. We study this framework's statistical properties,showing that the efficiency loss compared to the global setting relates to howmuch the underlying distribution families deviate from full exponentialfamilies, drawing connection to the theory of information loss by Fisher, Raoand Efron. We show that the "full-exponential-family-ness" represents the lowerbound of the error rate of arbitrary combinations of local MLEs, and isachieved by a KL-divergence-based combination method but not by a more commonlinear combination method. We also study the empirical properties of bothmethods, showing that the KL method significantly outperforms linearcombination in practical settings with issues such as model misspecification,non-convexity, and heterogeneous data partitions.
arxiv-1410-2646 | Hybrid approaches for automatic vowelization of Arabic texts |  http://arxiv.org/abs/1410.2646  | author:Mohamed Bebah, Chennoufi Amine, Mazroui Azzeddine, Lakhouaja Abdelhak category:cs.CL 68T50 published:2014-10-09 summary:Hybrid approaches for automatic vowelization of Arabic texts are presented inthis article. The process is made up of two modules. In the first one, amorphological analysis of the text words is performed using the open sourcemorphological Analyzer AlKhalil Morpho Sys. Outputs for each word analyzed outof context, are its different possible vowelizations. The integration of thisAnalyzer in our vowelization system required the addition of a lexical databasecontaining the most frequent words in Arabic language. Using a statisticalapproach based on two hidden Markov models (HMM), the second module aims toeliminate the ambiguities. Indeed, for the first HMM, the unvowelized Arabicwords are the observed states and the vowelized words are the hidden states.The observed states of the second HMM are identical to those of the first, butthe hidden states are the lists of possible diacritics of the word without itsArabic letters. Our system uses Viterbi algorithm to select the optimal pathamong the solutions proposed by Al Khalil Morpho Sys. Our approach opens animportant way to improve the performance of automatic vowelization of Arabictexts for other uses in automatic natural language processing.
arxiv-1410-2596 | Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares |  http://arxiv.org/abs/1410.2596  | author:Trevor Hastie, Rahul Mazumder, Jason Lee, Reza Zadeh category:stat.ME stat.ML published:2014-10-09 summary:The matrix-completion problem has attracted a lot of attention, largely as aresult of the celebrated Netflix competition. Two popular approaches forsolving the problem are nuclear-norm-regularized matrix approximation (Candesand Tao, 2009, Mazumder, Hastie and Tibshirani, 2010), and maximum-marginmatrix factorization (Srebro, Rennie and Jaakkola, 2005). These two proceduresare in some cases solving equivalent problems, but with quite differentalgorithms. In this article we bring the two approaches together, leading to anefficient algorithm for large matrix factorization and completion thatoutperforms both of these. We develop a software package "softImpute" in R forimplementing our approaches, and a distributed version for very large matricesusing the "Spark" cluster programming environment.
arxiv-1410-2535 | A unified approach for multi-object triangulation, tracking and camera calibration |  http://arxiv.org/abs/1410.2535  | author:Jeremie Houssineau, Daniel Clark, Spela Ivekovic, Chee Sing Lee, Jose Franco category:cs.CV stat.ME published:2014-10-09 summary:Object triangulation, 3-D object tracking, feature correspondence, and cameracalibration are key problems for estimation from camera networks. This paperaddresses these problems within a unified Bayesian framework for jointmulti-object tracking and sensor registration. Given that using standardfiltering approaches for state estimation from cameras is problematic, analternative parametrisation is exploited, called disparity space. The disparityspace-based approach for triangulation and object tracking is shown to be moreeffective than non-linear versions of the Kalman filter and particle filteringfor non-rectified cameras. The approach for feature correspondence is based onthe Probability Hypothesis Density (PHD) filter, and hence inherits the abilityto update without explicit measurement association, to initiate new targets,and to discriminate between target and clutter. The PHD filtering approach thenforms the basis of a camera calibration method from static or moving objects.Results are shown on simulated data.
arxiv-1410-2474 | Genetic Stereo Matching Algorithm with Fuzzy Fitness |  http://arxiv.org/abs/1410.2474  | author:Haythem Ghazouani category:cs.CV published:2014-10-09 summary:This paper presents a genetic stereo matching algorithm with fuzzy evaluationfunction. The proposed algorithm presents a new encoding scheme in which achromosome is represented by a disparity matrix. Evolution is controlled by afuzzy fitness function able to deal with noise and uncertain camerameasurements, and uses classical evolutionary operators. The result of thealgorithm is accurate dense disparity maps obtained in a reasonablecomputational time suitable for real-time applications as shown in experimentalresults.
arxiv-1410-2381 | Recognition of cDNA microarray image Using Feedforward artificial neural network |  http://arxiv.org/abs/1410.2381  | author:R. M. Farouk, S. Badr, M. Sayed Elahl category:cs.CV cs.NE published:2014-10-09 summary:The complementary DNA (cDNA) sequence is considered to be the magic biometrictechnique for personal identification. In this paper, we present a new methodfor cDNA recognition based on the artificial neural network (ANN). Microarrayimaging is used for the concurrent identification of thousands of genes. Wehave segmented the location of the spots in a cDNA microarray. Thus, a preciselocalization and segmenting of a spot are essential to obtain a more accurateintensity measurement, leading to a more precise expression measurement of agene. The segmented cDNA microarray image is resized and it is used as an inputfor the proposed artificial neural network. For matching and recognition, wehave trained the artificial neural network. Recognition results are given forthe galleries of cDNA sequences . The numerical results show that, the proposedmatching technique is an effective in the cDNA sequences process. We alsocompare our results with previous results and find out that, the proposedtechnique is an effective matching performance.
arxiv-1410-2455 | BilBOWA: Fast Bilingual Distributed Representations without Word Alignments |  http://arxiv.org/abs/1410.2455  | author:Stephan Gouws, Yoshua Bengio, Greg Corrado category:stat.ML cs.CL cs.LG published:2014-10-09 summary:We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simpleand computationally-efficient model for learning bilingual distributedrepresentations of words which can scale to large monolingual datasets and doesnot require word-aligned parallel training data. Instead it trains directly onmonolingual data and extracts a bilingual signal from a smaller set of raw-textsentence-aligned data. This is achieved using a novel sampled bag-of-wordscross-lingual objective, which is used to regularize two noise-contrastivelanguage models for efficient cross-lingual feature learning. We show thatbilingual embeddings learned using the proposed model outperformstate-of-the-art methods on a cross-lingual document classification task aswell as a lexical translation task on WMT11 data.
arxiv-1412-6018 | Automatic Training Data Synthesis for Handwriting Recognition Using the Structural Crossing-Over Technique |  http://arxiv.org/abs/1412.6018  | author:Sirisak Visessenee, Sanparith Marukatat, Rachada Kongkachandra category:cs.CV cs.LG published:2014-10-09 summary:The paper presents a novel technique called "Structural Crossing-Over" tosynthesize qualified data for training machine learning-based handwritingrecognition. The proposed technique can provide a greater variety of patternsof training data than the existing approaches such as elastic distortion andtangent-based affine transformation. A couple of training characters arechosen, then they are analyzed by their similar and different structures, andfinally are crossed over to generate the new characters. The experiments areset to compare the performances of tangent-based affine transformation and theproposed approach in terms of the variety of generated characters and percentof recognition errors. The standard MNIST corpus including 60,000 trainingcharacters and 10,000 test characters is employed in the experiments. Theproposed technique uses 1,000 characters to synthesize 60,000 characters, andthen uses these data to train and test the benchmark handwriting recognitionsystem that exploits Histogram of Gradient (HOG) as features and Support VectorMachine (SVM) as recognizer. The experimental result yields 8.06% of errors. Itsignificantly outperforms the tangent-based affine transformation and theoriginal MNIST training data, which are 11.74% and 16.55%, respectively.
arxiv-1410-3341 | Generalization Analysis for Game-Theoretic Machine Learning |  http://arxiv.org/abs/1410.3341  | author:Haifang Li, Fei Tian, Wei Chen, Tao Qin, Tie-Yan Liu category:cs.LG cs.GT published:2014-10-09 summary:For Internet applications like sponsored search, cautions need to be takenwhen using machine learning to optimize their mechanisms (e.g., auction) sinceself-interested agents in these applications may change their behaviors (andthus the data distribution) in response to the mechanisms. To tackle thisproblem, a framework called game-theoretic machine learning (GTML) was recentlyproposed, which first learns a Markov behavior model to characterize agents'behaviors, and then learns the optimal mechanism by simulating agents' behaviorchanges in response to the mechanism. While GTML has demonstrated practicalsuccess, its generalization analysis is challenging because the behavior dataare non-i.i.d. and dependent on the mechanism. To address this challenge,first, we decompose the generalization error for GTML into the behaviorlearning error and the mechanism learning error; second, for the behaviorlearning error, we obtain novel non-asymptotic error bounds for both parametricand non-parametric behavior learning methods; third, for the mechanism learningerror, we derive a uniform convergence bound based on a new concept callednested covering number of the mechanism space and the generalization analysistechniques developed for mixing sequences. To the best of our knowledge, thisis the first work on the generalization analysis of GTML, and we believe it hasgeneral implications to the theoretical analysis of other complicated machinelearning problems.
arxiv-1410-2265 | A Scalable, Lexicon Based Technique for Sentiment Analysis |  http://arxiv.org/abs/1410.2265  | author:Chetan Kaushik, Atul Mishra category:cs.IR cs.CL published:2014-10-08 summary:Rapid increase in the volume of sentiment rich social media on the web hasresulted in an increased interest among researchers regarding SentimentalAnalysis and opinion mining. However, with so much social media available onthe web, sentiment analysis is now considered as a big data task. Hence theconventional sentiment analysis approaches fails to efficiently handle the vastamount of sentiment data available now a days. The main focus of the researchwas to find such a technique that can efficiently perform sentiment analysis onbig data sets. A technique that can categorize the text as positive, negativeand neutral in a fast and accurate manner. In the research, sentiment analysiswas performed on a large data set of tweets using Hadoop and the performance ofthe technique was measured in form of speed and accuracy. The experimentalresults shows that the technique exhibits very good efficiency in handling bigsentiment data sets.
arxiv-1410-2046 | Bayesian tracking and parameter learning for non-linear multiple target tracking models |  http://arxiv.org/abs/1410.2046  | author:Lan Jiang, Sumeetpal S. Singh, Sinan Yıldırım category:stat.AP stat.CO stat.ML published:2014-10-08 summary:We propose a new Bayesian tracking and parameter learning algorithm fornon-linear non-Gaussian multiple target tracking (MTT) models. We design aMarkov chain Monte Carlo (MCMC) algorithm to sample from the posteriordistribution of the target states, birth and death times, and association ofobservations to targets, which constitutes the solution to the trackingproblem, as well as the model parameters. In the numerical section, we presentperformance comparisons with several competing techniques and demonstratesignificant performance improvements in all cases.
arxiv-1410-2045 | Supervised learning Methods for Bangla Web Document Categorization |  http://arxiv.org/abs/1410.2045  | author:Ashis Kumar Mandal, Rikta Sen category:cs.CL cs.LG published:2014-10-08 summary:This paper explores the use of machine learning approaches, or morespecifically, four supervised learning Methods, namely Decision Tree(C 4.5),K-Nearest Neighbour (KNN), Na\"ive Bays (NB), and Support Vector Machine (SVM)for categorization of Bangla web documents. This is a task of automaticallysorting a set of documents into categories from a predefined set. Whereas awide range of methods have been applied to English text categorization,relatively few studies have been conducted on Bangla language textcategorization. Hence, we attempt to analyze the efficiency of those fourmethods for categorization of Bangla documents. In order to validate, Banglacorpus from various websites has been developed and used as examples for theexperiment. For Bangla, empirical results support that all four methods producesatisfactory performance with SVM attaining good result in terms of highdimensional and relatively noisy document feature vectors.
arxiv-1410-2167 | Introducing SLAMBench, a performance and accuracy benchmarking methodology for SLAM |  http://arxiv.org/abs/1410.2167  | author:Luigi Nardi, Bruno Bodin, M. Zeeshan Zia, John Mawer, Andy Nisbet, Paul H. J. Kelly, Andrew J. Davison, Mikel Luján, Michael F. P. O'Boyle, Graham Riley, Nigel Topham, Steve Furber category:cs.RO cs.CV cs.DC cs.PF published:2014-10-08 summary:Real-time dense computer vision and SLAM offer great potential for a newlevel of scene modelling, tracking and real environmental interaction for manytypes of robot, but their high computational requirements mean that use on massmarket embedded platforms is challenging. Meanwhile, trends in low-cost,low-power processing are towards massive parallelism and heterogeneity, makingit difficult for robotics and vision researchers to implement their algorithmsin a performance-portable way. In this paper we introduce SLAMBench, apublicly-available software framework which represents a starting point forquantitative, comparable and validatable experimental research to investigatetrade-offs in performance, accuracy and energy consumption of a dense RGB-DSLAM system. SLAMBench provides a KinectFusion implementation in C++, OpenMP,OpenCL and CUDA, and harnesses the ICL-NUIM dataset of synthetic RGB-Dsequences with trajectory and scene ground truth for reliable accuracycomparison of different implementation and algorithms. We present an analysisand breakdown of the constituent algorithmic elements of KinectFusion, andexperimentally investigate their execution time on a variety of multicore andGPUaccelerated platforms. For a popular embedded platform, we also present ananalysis of energy efficiency for different configuration alternatives.
arxiv-1410-1980 | Deep Representations for Iris, Face, and Fingerprint Spoofing Detection |  http://arxiv.org/abs/1410.1980  | author:David Menotti, Giovani Chiachia, Allan Pinto, William Robson Schwartz, Helio Pedrini, Alexandre Xavier Falcao, Anderson Rocha category:cs.CV published:2014-10-08 summary:Biometrics systems have significantly improved person identification andauthentication, playing an important role in personal, national, and globalsecurity. However, these systems might be deceived (or "spoofed") and, despitethe recent advances in spoofing detection, current solutions often rely ondomain knowledge, specific biometric reading systems, and attack types. Weassume a very limited knowledge about biometric spoofing at the sensor toderive outstanding spoofing detection systems for iris, face, and fingerprintmodalities based on two deep learning approaches. The first approach consistsof learning suitable convolutional network architectures for each domain, whilethe second approach focuses on learning the weights of the network viaback-propagation. We consider nine biometric spoofing benchmarks --- each onecontaining real and fake samples of a given biometric modality and attack type--- and learn deep representations for each benchmark by combining andcontrasting the two learning approaches. This strategy not only provides bettercomprehension of how these approaches interplay, but also creates systems thatexceed the best known results in eight out of the nine benchmarks. The resultsstrongly indicate that spoofing detection systems based on convolutionalnetworks can be robust to attacks already known and possibly adapted, withlittle effort, to image-based attacks that are yet to come.
arxiv-1410-2082 | Contrastive Unsupervised Word Alignment with Non-Local Features |  http://arxiv.org/abs/1410.2082  | author:Yang Liu, Maosong Sun category:cs.CL published:2014-10-08 summary:Word alignment is an important natural language processing task thatindicates the correspondence between natural languages. Recently, unsupervisedlearning of log-linear models for word alignment has received considerableattention as it combines the merits of generative and discriminativeapproaches. However, a major challenge still remains: it is intractable tocalculate the expectations of non-local features that are critical forcapturing the divergence between natural languages. We propose a contrastiveapproach that aims to differentiate observed training examples from noises. Itnot only introduces prior knowledge to guide unsupervised learning but alsocancels out partition functions. Based on the observation that the probabilitymass of log-linear models for word alignment is usually highly concentrated, wepropose to use top-n alignments to approximate the expectations with respect toposterior distributions. This allows for efficient and accurate calculation ofexpectations of non-local features. Experiments show that our approach achievessignificant improvements over state-of-the-art unsupervised word alignmentmethods.
arxiv-1410-1606 | Hierarchical Sparse and Collaborative Low-Rank Representation for Emotion Recognition |  http://arxiv.org/abs/1410.1606  | author:Xiang Xiang, Minh Dao, Gregory D. Hager, Trac D. Tran category:cs.CV published:2014-10-07 summary:In this paper, we design a Collaborative-Hierarchical Sparse and Low-Rank(C-HiSLR) model that is natural for recognizing human emotion in visual data.Previous attempts require explicit expression components, which are oftenunavailable and difficult to recover. Instead, our model exploits the lowrankproperty over expressive facial frames and rescue inexact sparserepresentations by incorporating group sparsity. For the CK+ dataset, C-HiSLRon raw expressive faces performs as competitive as the Sparse Representationbased Classification (SRC) applied on manually prepared emotions. C-HiSLRperforms even better than SRC in terms of true positive rate.
arxiv-1410-1771 | PAC-Bayesian AUC classification and scoring |  http://arxiv.org/abs/1410.1771  | author:James Ridgway, Pierre Alquier, Nicolas Chopin, Feng Liang category:stat.ML stat.CO 62H30 published:2014-10-07 summary:We develop a scoring and classification procedure based on the PAC-Bayesianapproach and the AUC (Area Under Curve) criterion. We focus initially on theclass of linear score functions. We derive PAC-Bayesian non-asymptotic boundsfor two types of prior for the score parameters: a Gaussian prior, and aspike-and-slab prior; the latter makes it possible to perform featureselection. One important advantage of our approach is that it is amenable topowerful Bayesian computational tools. We derive in particular a SequentialMonte Carlo algorithm, as an efficient method which may be used as a goldstandard, and an Expectation-Propagation algorithm, as a much faster butapproximate method. We also extend our method to a class of non-linear scorefunctions, essentially leading to a nonparametric procedure, by considering aGaussian process prior.
arxiv-1410-1940 | GLAD: Group Anomaly Detection in Social Media Analysis- Extended Abstract |  http://arxiv.org/abs/1410.1940  | author:Qi, Yu, Xinran He, Yan Liu category:cs.LG cs.SI H.2.8 published:2014-10-07 summary:Traditional anomaly detection on social media mostly focuses on individualpoint anomalies while anomalous phenomena usually occur in groups. Therefore itis valuable to study the collective behavior of individuals and detect groupanomalies. Existing group anomaly detection approaches rely on the assumptionthat the groups are known, which can hardly be true in real world social mediaapplications. In this paper, we take a generative approach by proposing ahierarchical Bayes model: Group Latent Anomaly Detection (GLAD) model. GLADtakes both pair-wise and point-wise data as input, automatically infers thegroups and detects group anomalies simultaneously. To account for the dynamicproperties of the social media data, we further generalize GLAD to its dynamicextension d-GLAD. We conduct extensive experiments to evaluate our models onboth synthetic and real world datasets. The empirical results demonstrate thatour approach is effective and robust in discovering latent groups and detectinggroup anomalies.
arxiv-1410-1699 | Mumford-Shah and Potts Regularization for Manifold-Valued Data with Applications to DTI and Q-Ball Imaging |  http://arxiv.org/abs/1410.1699  | author:Andreas Weinmann, Laurent Demaret, Martin Storath category:math.NA cs.CV math.OC physics.med-ph published:2014-10-07 summary:Mumford-Shah and Potts functionals are powerful variational models forregularization which are widely used in signal and image processing; typicalapplications are edge-preserving denoising and segmentation. Being bothnon-smooth and non-convex, they are computationally challenging even for scalardata. For manifold-valued data, the problem becomes even more involved sincetypical features of vector spaces are not available. In this paper, we proposealgorithms for Mumford-Shah and for Potts regularization of manifold-valuedsignals and images. For the univariate problems, we derive solvers based ondynamic programming combined with (convex) optimization techniques formanifold-valued data. For the class of Cartan-Hadamard manifolds (whichincludes the data space in diffusion tensor imaging), we show that ouralgorithms compute global minimizers for any starting point. For themultivariate Mumford-Shah and Potts problems (for image regularization) wepropose a splitting into suitable subproblems which we can solve exactly usingthe techniques developed for the corresponding univariate problems. Our methoddoes not require any a priori restrictions on the edge set and we do not haveto discretize the data space. We apply our method to diffusion tensor imaging(DTI) as well as Q-ball imaging. Using the DTI model, we obtain a segmentationof the corpus callosum.
arxiv-1412-6069 | Annotation as a New Paradigm in Research Archiving |  http://arxiv.org/abs/1412.6069  | author:Dirk Roorda, Charles van den Heuvel category:cs.DL cs.CL published:2014-10-07 summary:We outline a paradigm to preserve results of digital scholarship, whetherthey are query results, feature values, or topic assignments. This paradigm ischaracterized by using annotations as multifunctional carriers and making themportable. The testing grounds we have chosen are two significant enterprises,one in the history of science, and one in Hebrew scholarship. The first one(CKCC) focuses on the results of a project where a Dutch consortium ofuniversities, research institutes, and cultural heritage institutionsexperimented for 4 years with language techniques and topic modeling methodswith the aim to analyze the emergence of scholarly debates. The data: a complexset of about 20.000 letters. The second one (DTHB) is a multi-year effort toexpress the linguistic features of the Hebrew bible in a text database, whichis still growing in detail and sophistication. Versions of this database arepackaged in commercial bible study software. We state that the results of theseforms of scholarship require new knowledge management and archive practices.Only when researchers can build efficiently on each other's (intermediate)results, they can achieve the aggregations of quality data by which newquestions can be answered, and hidden patterns visualized. Archives arerequired to find a balance between preserving authoritative versions of sourcesand supporting collaborative efforts in digital scholarship. Annotations arepromising vehicles for preserving and reusing research results. Keywordsannotation, portability, archiving, queries, features, topics, keywords,Republic of Letters, Hebrew text databases.
arxiv-1410-1462 | Top Rank Optimization in Linear Time |  http://arxiv.org/abs/1410.1462  | author:Nan Li, Rong Jin, Zhi-Hua Zhou category:cs.LG cs.AI cs.IR published:2014-10-06 summary:Bipartite ranking aims to learn a real-valued ranking function that orderspositive instances before negative instances. Recent efforts of bipartiteranking are focused on optimizing ranking accuracy at the top of the rankedlist. Most existing approaches are either to optimize task specific metrics orto extend the ranking loss by emphasizing more on the error associated with thetop ranked instances, leading to a high computational cost that is super-linearin the number of training instances. We propose a highly efficient approach,titled TopPush, for optimizing accuracy at the top that has computationalcomplexity linear in the number of training instances. We present a novelanalysis that bounds the generalization error for the top ranked instances forthe proposed approach. Empirical study shows that the proposed approach ishighly competitive to the state-of-the-art approaches and is 10-100 timesfaster.
arxiv-1410-2188 | An Aerial Image Recognition Framework using Discrimination and Redundancy Quality Measure |  http://arxiv.org/abs/1410.2188  | author:Yuxin Hu, Luming Zhang category:cs.CV published:2014-10-06 summary:Aerial image categorization plays an indispensable role in remote sensing andartificial intelligence. In this paper, we propose a new aerial imagecategorization framework, focusing on organizing the local patches of eachaerial image into multiple discriminative subgraphs. The subgraphs reflect boththe geometric property and the color distribution of an aerial image. First,each aerial image is decomposed into a collection of regions in terms of theircolor intensities. Thereby region connected graph (RCG), which models theconnection between the spatial neighboring regions, is constructed to encodethe spatial context of an aerial image. Second, a subgraph mining technique isadopted to discover the frequent structures in the RCGs constructed from thetraining aerial images. Thereafter, a set of refined structures are selectedamong the frequent ones toward being highly discriminative and low redundant.Lastly, given a new aerial image, its sub-RCGs corresponding to the refinedstructures are extracted. They are further quantized into a discriminativevector for SVM classification. Thorough experimental results validate thee?ectiveness of the proposed method. In addition, the visualized minedsubgraphs show that the discriminative topologies of each aerial image arediscovered.
arxiv-1412-6154 | Effective persistent homology of digital images |  http://arxiv.org/abs/1412.6154  | author:Ana Romero, Julio Rubio, Francis Sergeraert category:cs.CV published:2014-10-06 summary:In this paper, three Computational Topology methods (namely effectivehomology, persistent homology and discrete vector fields) are mixed together toproduce algorithms for homological digital image processing. The algorithmshave been implemented as extensions of the Kenzo system and have shown a goodperformance when applied on some actual images extracted from a public dataset.
arxiv-1410-1267 | Memristive Threshold Logic Circuit Design of Fast Moving Object Detection |  http://arxiv.org/abs/1410.1267  | author:Akshay Kumar Maan, Dinesh Sasi Kumar, Sherin Sugathan, Alex Pappachen James category:cs.CV cs.AR cs.ET published:2014-10-06 summary:Real-time detection of moving objects involves memorisation of features inthe template image and their comparison with those in the test image. At highsampling rates, such techniques face the problems of high algorithmiccomplexity and component delays. We present a new resistive switching basedthreshold logic cell which encodes the pixels of a template image. The cellcomprises a voltage divider circuit that programs the resistances of thememristors arranged in a single node threshold logic network and the output isencoded as a binary value using a CMOS inverter gate. When a test image isapplied to the template-programmed cell, a mismatch in the respective pixels isseen as a change in the output voltage of the cell. The proposed cell whencompared with CMOS equivalent implementation shows improved performance inarea, leakage power, power dissipation and delay.
arxiv-1410-1174 | Learning Topology and Dynamics of Large Recurrent Neural Networks |  http://arxiv.org/abs/1410.1174  | author:Yiyuan She, Yuejia He, Dapeng Wu category:stat.ML stat.CO published:2014-10-05 summary:Large-scale recurrent networks have drawn increasing attention recentlybecause of their capabilities in modeling a large variety of real-worldphenomena and physical mechanisms. This paper studies how to identify allauthentic connections and estimate system parameters of a recurrent network,given a sequence of node observations. This task becomes extremely challengingin modern network applications, because the available observations are usuallyvery noisy and limited, and the associated dynamical system is stronglynonlinear. By formulating the problem as multivariate sparse sigmoidalregression, we develop simple-to-implement network learning algorithms, withrigorous convergence guarantee in theory, for a variety of sparsity-promotingpenalty forms. A quantile variant of progressive recurrent network screening isproposed for efficient computation and allows for direct cardinality control ofnetwork topology in estimation. Moreover, we investigate recurrent networkstability conditions in Lyapunov's sense, and integrate such stabilityconstraints into sparse network learning. Experiments show excellentperformance of the proposed algorithms in network topology identification andforecasting.
arxiv-1410-1103 | Online Ranking with Top-1 Feedback |  http://arxiv.org/abs/1410.1103  | author:Sougata Chaudhuri, Ambuj Tewari category:cs.LG published:2014-10-05 summary:We consider a setting where a system learns to rank a fixed set of $m$ items.The goal is produce good item rankings for users with diverse interests whointeract online with the system for $T$ rounds. We consider a novel top-$1$feedback model: at the end of each round, the relevance score for only the topranked object is revealed. However, the performance of the system is judged onthe entire ranked list. We provide a comprehensive set of results regardinglearnability under this challenging setting. For PairwiseLoss and DCG, twopopular ranking measures, we prove that the minimax regret is$\Theta(T^{2/3})$. Moreover, the minimax regret is achievable using anefficient strategy that only spends $O(m \log m)$ time per round. The sameefficient strategy achieves $O(T^{2/3})$ regret for Precision@$k$.Surprisingly, we show that for normalized versions of these ranking measures,i.e., AUC, NDCG \& MAP, no online ranking algorithm can have sublinear regret.
arxiv-1410-1141 | On the Computational Efficiency of Training Neural Networks |  http://arxiv.org/abs/1410.1141  | author:Roi Livni, Shai Shalev-Shwartz, Ohad Shamir category:cs.LG cs.AI stat.ML published:2014-10-05 summary:It is well-known that neural networks are computationally hard to train. Onthe other hand, in practice, modern day neural networks are trained efficientlyusing SGD and a variety of tricks that include different activation functions(e.g. ReLU), over-specification (i.e., train networks which are larger thanneeded), and regularization. In this paper we revisit the computationalcomplexity of training neural networks from a modern perspective. We provideboth positive and negative results, some of them yield new provably efficientand practical algorithms for training certain types of neural networks.
arxiv-1410-1151 | Training Algorithm for Neuro-Fuzzy Network Based on Singular Spectrum Analysis |  http://arxiv.org/abs/1410.1151  | author:Yulia S. Maslennikova, Vladimir V. Bochkarev category:cs.NE stat.ME I.5.1 published:2014-10-05 summary:In this article, we propose a combination of an noise-reduction algorithmbased on Singular Spectrum Analysis (SSA) and a standard feedforward neuralprediction model. Basically, the proposed algorithm consists of two differentsteps: data preprocessing based on the SSA filtering method and step-by-steptraining procedure in which we use a simple feedforward multilayer neuralnetwork with backpropagation learning. The proposed noise-reduction proceduresuccessfully removes most of the noise. That increases long-term predictabilityof the processed dataset comparison with the raw dataset. The method wasapplied to predict the International sunspot number RZ time series. The resultsshow that our combined technique has better performances than those offered bythe same network directly applied to raw dataset.
arxiv-1410-1135 | Corpora Preparation and Stopword List Generation for Arabic data in Social Network |  http://arxiv.org/abs/1410.1135  | author:Walaa Medhat, Ahmed H. Yousef, Hoda Korashy category:cs.CL published:2014-10-05 summary:This paper proposes a methodology to prepare corpora in Arabic language fromonline social network (OSN) and review site for Sentiment Analysis (SA) task.The paper also proposes a methodology for generating a stopword list from theprepared corpora. The aim of the paper is to investigate the effect of removingstopwords on the SA task. The problem is that the stopwords lists generatedbefore were on Modern Standard Arabic (MSA) which is not the common languageused in OSN. We have generated a stopword list of Egyptian dialect and acorpus-based list to be used with the OSN corpora. We compare the efficiency oftext classification when using the generated lists along with previouslygenerated lists of MSA and combining the Egyptian dialect list with the MSAlist. The text classification was performed using Na\"ive Bayes and DecisionTree classifiers and two feature selection approaches, unigrams and bigram. Theexperiments show that the general lists containing the Egyptian dialects wordsgive better performance than using lists of MSA stopwords only.
arxiv-1410-2149 | Language-based Examples in the Statistics Classroom |  http://arxiv.org/abs/1410.2149  | author:Roger Bilisoly category:cs.CL published:2014-10-05 summary:Statistics pedagogy values using a variety of examples. Thanks to textresources on the Web, and since statistical packages have the ability toanalyze string data, it is now easy to use language-based examples in astatistics class. Three such examples are discussed here. First, many types ofwordplay (e.g., crosswords and hangman) involve finding words with letters thatsatisfy a certain pattern. Second, linguistics has shown that idiomatic pairsof words often appear together more frequently than chance. For example, in theBrown Corpus, this is true of the phrasal verb to throw up (p-value=7.92E-10.)Third, a pangram contains all the letters of the alphabet at least once. Theseare searched for in Charles Dickens' A Christmas Carol, and their lengths arecompared to the expected value given by the unequal probability couponcollector's problem as well as simulations.
arxiv-1410-1184 | Graphical LASSO Based Model Selection for Time Series |  http://arxiv.org/abs/1410.1184  | author:Alexander Jung, Gabor Hannak, Norbert Görtz category:stat.ML published:2014-10-05 summary:We propose a novel graphical model selection (GMS) scheme forhigh-dimensional stationary time series or discrete time process. The method isbased on a natural generalization of the graphical LASSO (gLASSO), introducedoriginally for GMS based on i.i.d. samples, and estimates the conditionalindependence graph (CIG) of a time series from a finite length observation. ThegLASSO for time series is defined as the solution of an l1-regularized maximum(approximate) likelihood problem. We solve this optimization problem using thealternating direction method of multipliers (ADMM). Our approach isnonparametric as we do not assume a finite dimensional (e.g., anautoregressive) parametric model for the observed process. Instead, we requirethe process to be sufficiently smooth in the spectral domain. For Gaussianprocesses, we characterize the performance of our method theoretically byderiving an upper bound on the probability that our algorithm fails tocorrectly identify the CIG. Numerical experiments demonstrate the ability ofour method to recover the correct CIG from a limited amount of samples.
arxiv-1410-1165 | Understanding Locally Competitive Networks |  http://arxiv.org/abs/1410.1165  | author:Rupesh Kumar Srivastava, Jonathan Masci, Faustino Gomez, Jürgen Schmidhuber category:cs.NE cs.LG 68T30, 68T10 I.2.6 published:2014-10-05 summary:Recently proposed neural network activation functions such as rectifiedlinear, maxout, and local winner-take-all have allowed for faster and moreeffective training of deep neural architectures on large and complex datasets.The common trait among these functions is that they implement local competitionbetween small groups of computational units within a layer, so that only partof the network is activated for any given input pattern. In this paper, weattempt to visualize and understand this self-modularization, and suggest aunified explanation for the beneficial properties of such networks. We alsoshow how our insights can be directly useful for efficiently performingretrieval over large datasets using neural networks.
arxiv-1410-1228 | Interactive Fingerprinting Codes and the Hardness of Preventing False Discovery |  http://arxiv.org/abs/1410.1228  | author:Thomas Steinke, Jonathan Ullman category:cs.CR cs.DS cs.LG published:2014-10-05 summary:We show an essentially tight bound on the number of adaptively chosenstatistical queries that a computationally efficient algorithm can answeraccurately given $n$ samples from an unknown distribution. A statistical queryasks for the expectation of a predicate over the underlying distribution, andan answer to a statistical query is accurate if it is "close" to the correctexpectation over the distribution. This question was recently studied by Dworket al., who showed how to answer $\tilde{\Omega}(n^2)$ queries efficiently, andalso by Hardt and Ullman, who showed that answering $\tilde{O}(n^3)$ queries ishard. We close the gap between the two bounds and show that, under a standardhardness assumption, there is no computationally efficient algorithm that,given $n$ samples from an unknown distribution, can give valid answers to$O(n^2)$ adaptively chosen statistical queries. An implication of our resultsis that computationally efficient algorithms for answering arbitrary,adaptively chosen statistical queries may as well be differentially private. We obtain our results using a new connection between the problem of answeringadaptively chosen statistical queries and a combinatorial object called aninteractive fingerprinting code. In order to optimize our hardness result, wegive a new Fourier-analytic approach to analyzing fingerprinting codes that issimpler, more flexible, and yields better parameters than previousconstructions.
arxiv-1410-1080 | Generating abbreviations using Google Books library |  http://arxiv.org/abs/1410.1080  | author:Valery D. Solovyev, Vladimir V. Bochkarev category:cs.CL stat.AP 91F20, 62P25 I.2.7; J.5 published:2014-10-04 summary:The article describes the original method of creating a dictionary ofabbreviations based on the Google Books Ngram Corpus. The dictionary ofabbreviations is designed for Russian, yet as its methodology is universal itcan be applied to any language. The dictionary can be used to define thefunction of the period during text segmentation in various applied systems oftext processing. The article describes difficulties encountered in the processof its construction as well as the ways to overcome them. A model of evaluatinga probability of first and second type errors (extraction accuracy andfullness) is constructed. Certain statistical data for the use of abbreviationsare provided.
arxiv-1410-1090 | Explain Images with Multimodal Recurrent Neural Networks |  http://arxiv.org/abs/1410.1090  | author:Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Alan L. Yuille category:cs.CV cs.CL cs.LG published:2014-10-04 summary:In this paper, we present a multimodal Recurrent Neural Network (m-RNN) modelfor generating novel sentence descriptions to explain the content of images. Itdirectly models the probability distribution of generating a word givenprevious words and the image. Image descriptions are generated by sampling fromthis distribution. The model consists of two sub-networks: a deep recurrentneural network for sentences and a deep convolutional network for images. Thesetwo sub-networks interact with each other in a multimodal layer to form thewhole m-RNN model. The effectiveness of our model is validated on threebenchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our modeloutperforms the state-of-the-art generative method. In addition, the m-RNNmodel can be applied to retrieval tasks for retrieving images or sentences, andachieves significant performance improvement over the state-of-the-art methodswhich directly optimize the ranking objective function for retrieval.
arxiv-1410-1068 | Gamma Processes, Stick-Breaking, and Variational Inference |  http://arxiv.org/abs/1410.1068  | author:Anirban Roychowdhury, Brian Kulis category:stat.ML cs.LG published:2014-10-04 summary:While most Bayesian nonparametric models in machine learning have focused onthe Dirichlet process, the beta process, or their variants, the gamma processhas recently emerged as a useful nonparametric prior in its own right. Currentinference schemes for models involving the gamma process are restricted toMCMC-based methods, which limits their scalability. In this paper, we present avariational inference framework for models involving gamma process priors. Ourapproach is based on a novel stick-breaking constructive definition of thegamma process. We prove correctness of this stick-breaking process by using thecharacterization of the gamma process as a completely random measure (CRM), andwe explicitly derive the rate measure of our construction using Poisson processmachinery. We also derive error bounds on the truncation of the infiniteprocess required for variational inference, similar to the truncation analysesfor other nonparametric models based on the Dirichlet and beta processes. Ourrepresentation is then used to derive a variational inference algorithm for aparticular Bayesian nonparametric latent structure formulation known as theinfinite Gamma-Poisson model, where the latent variables are drawn from a gammaprocess prior with Poisson likelihoods. Finally, we present results for ouralgorithms on nonnegative matrix factorization tasks on document corpora, andshow that we compare favorably to both sampling-based techniques andvariational approaches based on beta-Bernoulli priors.
arxiv-1410-1037 | Facial Feature Point Detection: A Comprehensive Survey |  http://arxiv.org/abs/1410.1037  | author:Nannan Wang, Xinbo Gao, Dacheng Tao, Xuelong Li category:cs.CV published:2014-10-04 summary:This paper presents a comprehensive survey of facial feature point detectionwith the assistance of abundant manually labeled images. Facial feature pointdetection favors many applications such as face recognition, animation,tracking, hallucination, expression analysis and 3D face modeling. Existingmethods can be categorized into the following four groups: constrained localmodel (CLM)-based, active appearance model (AAM)-based, regression-based, andother methods. CLM-based methods consist of a shape model and a number of localexperts, each of which is utilized to detect a facial feature point. AAM-basedmethods fit a shape model to an image by minimizing texture synthesis errors.Regression-based methods directly learn a mapping function from facial imageappearance to facial feature points. Besides the above three major categoriesof methods, there are also minor categories of methods which we classify intoother methods: graphical model-based methods, joint face alignment methods,independent facial feature point detectors, and deep learning-based methods.Though significant progress has been made, facial feature point detection islimited in its success by wild and real-world conditions: variations acrossposes, expressions, illuminations, and occlusions. A comparative illustrationand analysis of representative methods provide us a holistic understanding anddeep insight into facial feature point detection, which also motivates us toexplore promising future directions.
arxiv-1410-1035 | Learning Invariant Color Features for Person Re-Identification |  http://arxiv.org/abs/1410.1035  | author:Rahul Rama Varior, Gang Wang, Jiwen Lu category:cs.CV published:2014-10-04 summary:Matching people across multiple camera views known as personre-identification, is a challenging problem due to the change in visualappearance caused by varying lighting conditions. The perceived color of thesubject appears to be different with respect to illumination. Previous worksuse color as it is or address these challenges by designing color spacesfocusing on a specific cue. In this paper, we propose a data driven approachfor learning color patterns from pixels sampled from images across two cameraviews. The intuition behind this work is that, even though pixel values of samecolor would be different across views, they should be encoded with the samevalues. We model color feature generation as a learning problem by jointlylearning a linear transformation and a dictionary to encode pixel values. Wealso analyze different photometric invariant color spaces. Using color as theonly cue, we compare our approach with all the photometric invariant colorspaces and show superior performance over all of them. Combining with otherlearned low-level and high-level features, we obtain promising results inViPER, Person Re-ID 2011 and CAVIAR4REID datasets.
arxiv-1410-2191 | Learning manifold to regularize nonnegative matrix factorization |  http://arxiv.org/abs/1410.2191  | author:Jim Jing-Yan Wang, Xin Gao category:cs.LG published:2014-10-03 summary:Inthischapterwediscusshowtolearnanoptimalmanifoldpresentationto regularizenonegative matrix factorization (NMF) for data representation problems.NMF,whichtriestorepresentanonnegativedatamatrixasaproductoftwolowranknonnegative matrices, has been a popular method for data representation due toits ability to explore the latent part-based structure of data. Recent studyshows that lots of data distributions have manifold structures, and we shouldrespect the manifold structure when the data are represented. Recently,manifold regularized NMF used a nearest neighbor graph to regulate the learningof factorization parameter matrices and has shown its advantage overtraditional NMF methods for data representation problems. However, how toconstruct an optimal graph to present the manifold prop- erly remains adifficultproblem due to the graph modelselection, noisy features, and nonlineardistributed data. In this chapter, we introduce three effective methods tosolve these problems of graph construction for manifold regularized NMF.Multiple graph learning is proposed to solve the problem of graph modelselection, adaptive graph learning via feature selection is proposed to solvethe problem of constructing a graph from noisy features, while multi-kernellearning-based graph construction is used to solve the problem of learning agraph from nonlinearly distributed data.
arxiv-1410-0870 | BayesPy: Variational Bayesian Inference in Python |  http://arxiv.org/abs/1410.0870  | author:Jaakko Luttinen category:stat.ML stat.CO published:2014-10-03 summary:BayesPy is an open-source Python software package for performing variationalBayesian inference. It is based on the variational message passing frameworkand supports conjugate exponential family models. By removing the tedious taskof implementing the variational Bayesian update equations, the user canconstruct models faster and in a less error-prone way. Simple syntax, flexiblemodel construction and efficient inference make BayesPy suitable for bothaverage and expert Bayesian users. It also supports some advanced methods suchas stochastic and collapsed variational inference.
arxiv-1410-0925 | A Framework for the Volumetric Integration of Depth Images |  http://arxiv.org/abs/1410.0925  | author:Victor Adrian Prisacariu, Olaf Kähler, Ming Ming Cheng, Carl Yuheng Ren, Julien Valentin, Philip H. S. Torr, Ian D. Reid, David W. Murray category:cs.CV published:2014-10-03 summary:Volumetric models have become a popular representation for 3D scenes inrecent years. One of the breakthroughs leading to their popularity wasKinectFusion, where the focus is on 3D reconstruction using RGB-D sensors.However, monocular SLAM has since also been tackled with very similarapproaches. Representing the reconstruction volumetrically as a truncatedsigned distance function leads to most of the simplicity and efficiency thatcan be achieved with GPU implementations of these systems. However, thisrepresentation is also memory-intensive and limits the applicability to smallscale reconstructions. Several avenues have been explored for overcoming thislimitation. With the aim of summarizing them and providing for a fast andflexible 3D reconstruction pipeline, we propose a new, unifying frameworkcalled InfiniTAM. The core idea is that individual steps like camera tracking,scene representation and integration of new data can easily be replaced andadapted to the needs of the user. Along with the framework we also provide aset of components for scalable reconstruction: two implementations of cameratrackers, based on RGB data and on depth data, two representations of the 3Dvolumetric data, a dense volume and one based on hashes of subblocks, and anoptional module for swapping subblocks in and out of the typically limited GPUmemory.
arxiv-1410-0868 | Group Orbit Optimization: A Unified Approach to Data Normalization |  http://arxiv.org/abs/1410.0868  | author:Shuchang Zhou, Zhihua Zhang, Xiaobing Feng category:cs.NA cs.CV math.NA 15-02 G.1.3; I.5.4 published:2014-10-03 summary:In this paper we propose and study an optimization problem over a matrixgroup orbit that we call \emph{Group Orbit Optimization} (GOO). We prove thatGOO can be used to induce matrix decomposition techniques such as singularvalue decomposition (SVD), LU decomposition, QR decomposition, Schurdecomposition and Cholesky decomposition, etc. This gives rise to a unifiedframework for matrix decomposition and allows us to bridge these matrixdecomposition methods. Moreover, we generalize GOO for tensor decomposition. Asa concrete application of GOO, we devise a new data decomposition method over aspecial linear group to normalize point cloud data. Experiment results showthat our normalization method is able to obtain recovery well from distortionslike shearing, rotation and squeezing.
arxiv-1410-0949 | Tight Regret Bounds for Stochastic Combinatorial Semi-Bandits |  http://arxiv.org/abs/1410.0949  | author:Branislav Kveton, Zheng Wen, Azin Ashkan, Csaba Szepesvari category:cs.LG cs.AI stat.ML published:2014-10-03 summary:A stochastic combinatorial semi-bandit is an online learning problem where ateach step a learning agent chooses a subset of ground items subject toconstraints, and then observes stochastic weights of these items and receivestheir sum as a payoff. In this paper, we close the problem of computationallyand sample efficient learning in stochastic combinatorial semi-bandits. Inparticular, we analyze a UCB-like algorithm for solving the problem, which isknown to be computationally efficient; and prove $O(K L (1 / \Delta) \log n)$and $O(\sqrt{K L n \log n})$ upper bounds on its $n$-step regret, where $L$ isthe number of ground items, $K$ is the maximum number of chosen items, and$\Delta$ is the gap between the expected returns of the optimal and bestsuboptimal solutions. The gap-dependent bound is tight up to a constant factorand the gap-free bound is tight up to a polylogarithmic factor.
arxiv-1410-0781 | SimNets: A Generalization of Convolutional Networks |  http://arxiv.org/abs/1410.0781  | author:Nadav Cohen, Amnon Shashua category:cs.NE cs.LG published:2014-10-03 summary:We present a deep layered architecture that generalizes classicalconvolutional neural networks (ConvNets). The architecture, called SimNets, isdriven by two operators, one being a similarity function whose family containsthe convolution operator used in ConvNets, and the other is a new softmax-min-mean operator called MEX that realizes classical operators like ReLUand max pooling, but has additional capabilities that make SimNets a powerfulgeneralization of ConvNets. Three interesting properties emerge from thearchitecture: (i) the basic input to hidden layer to output machinery containsas special cases kernel machines with the Exponential and Generalized Gaussiankernels, the output units being "neurons in feature space" (ii) in its generalform, the basic machinery has a higher abstraction level than kernel machines,and (iii) initializing networks using unsupervised learning is natural.Experiments demonstrate the capability of achieving state of the art accuracywith networks that are an order of magnitude smaller than comparable ConvNets.
arxiv-1410-0908 | Probit Normal Correlated Topic Models |  http://arxiv.org/abs/1410.0908  | author:Xingchen Yu, Ernest Fokoue category:stat.ML cs.IR cs.LG 62H25, 62H30 published:2014-10-03 summary:The logistic normal distribution has recently been adapted via thetransformation of multivariate Gaus- sian variables to model the topicaldistribution of documents in the presence of correlations among topics. In thispaper, we propose a probit normal alternative approach to modelling correlatedtopical structures. Our use of the probit model in the context of topicdiscovery is novel, as many authors have so far con- centrated solely of thelogistic model partly due to the formidable inefficiency of the multinomialprobit model even in the case of very small topical spaces. We hereincircumvent the inefficiency of multinomial probit estimation by using anadaptation of the diagonal orthant multinomial probit in the topic modelscontext, resulting in the ability of our topic modelling scheme to handlecorpuses with a large number of latent topics. An additional and very importantbenefit of our method lies in the fact that unlike with the logistic normalmodel whose non-conjugacy leads to the need for sophisticated sampling schemes,our ap- proach exploits the natural conjugacy inherent in the auxiliaryformulation of the probit model to achieve greater simplicity. The applicationof our proposed scheme to a well known Associated Press corpus not only helpsdiscover a large number of meaningful topics but also reveals the capturing ofcompellingly intuitive correlations among certain topics. Besides, our proposedapproach lends itself to even further scalability thanks to various existinghigh performance algorithms and architectures capable of handling millions ofdocuments.
arxiv-1410-0948 | Contributions of natural ventilation on thermal performance of alternative floor plan designs |  http://arxiv.org/abs/1410.0948  | author:Eugénio Rodrigues, Adélio R. Gaspar, Álvaro Gomes, Manuel Gameiro da Silva category:cs.NE cs.SE D.2.2; G.1.6 published:2014-10-03 summary:During the earliest phase of architectural design process, practitionersafter analyzing the client's design program, legal requirements, topographicconstraints, and preferences synthesize these requirements into architecturalfloor plan drawings. Design decisions taken in this phase may significantlycontribute to the building performance. On account of this reason, it isimportant to estimate and compare alternative solutions, when it is stillmanageable to change the building design. The authors have been developing a prototype tool to assist architects duringthis initial design phase. It is made up of two algorithms. The first algorithmgenerates alternative floor plans according to the architect's preferences andrequirements, and the client's design program. It consists in one evolutionarystrategy approach enhanced with local search technique to allocate rooms onseveral levels in the two-dimensional space. The second algorithm evaluates,ranks, and optimizes those floor plans according to thermal performancecriteria. The prototype tool is coupled with dynamic simulation program, whichestimates the thermal behavior of each solution. A sequential variableoptimization is used to change several geometric values of differentarchitectural elements in the floor plans to explore the improvement potential. In the present communication, the two algorithms are used in an iterativeprocess to generate and optimize the thermal performance of alternative floorplans. In the building simulation specifications of EnergyPlus program, theairflow network model has been used in order to adequately model the airinfiltration and the airflows through indoor spaces. A case study of asingle-family house with three rooms in a single level is presented.
arxiv-1410-0736 | HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition |  http://arxiv.org/abs/1410.0736  | author:Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis DeCoste, Wei Di, Yizhou Yu category:cs.CV cs.LG cs.NE published:2014-10-03 summary:In image classification, visual separability between different objectcategories is highly uneven, and some categories are more difficult todistinguish than others. Such difficult categories demand more dedicatedclassifiers. However, existing deep convolutional neural networks (CNN) aretrained as flat N-way classifiers, and few efforts have been made to leveragethe hierarchical structure of categories. In this paper, we introducehierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a categoryhierarchy. An HD-CNN separates easy classes using a coarse category classifierwhile distinguishing difficult classes using fine category classifiers. DuringHD-CNN training, component-wise pretraining is followed by global finetuningwith a multinomial logistic loss regularized by a coarse category consistencyterm. In addition, conditional executions of fine category classifiers andlayer parameter compression make HD-CNNs scalable for large-scale visualrecognition. We achieve state-of-the-art results on both CIFAR100 andlarge-scale ImageNet 1000-class benchmark datasets. In our experiments, webuild up three different HD-CNNs and they lower the top-1 error of the standardCNNs by 2.65%, 3.1% and 1.1%, respectively.
arxiv-1410-0745 | Im2Fit: Fast 3D Model Fitting and Anthropometrics using Single Consumer Depth Camera and Synthetic Data |  http://arxiv.org/abs/1410.0745  | author:Qiaosong Wang, Vignesh Jagadeesh, Bryan Ressler, Robinson Piramuthu category:cs.CV published:2014-10-03 summary:Recent advances in consumer depth sensors have created many opportunities forhuman body measurement and modeling. Estimation of 3D body shape isparticularly useful for fashion e-commerce applications such as virtual try-onor fit personalization. In this paper, we propose a method for capturingaccurate human body shape and anthropometrics from a single consumer gradedepth sensor. We first generate a large dataset of synthetic 3D human bodymodels using real-world body size distributions. Next, we estimate key bodymeasurements from a single monocular depth image. We combine body measurementestimates with local geometry features around key joint positions to form arobust multi-dimensional feature vector. This allows us to conduct a fastnearest-neighbor search to every sample in the dataset and return the closestone. Compared to existing methods, our approach is able to predict accuratefull body parameters from a partial view using measurement parameters learnedfrom the synthetic dataset. Furthermore, our system is capable of generating 3Dhuman mesh models in real-time, which is significantly faster than methodswhich attempt to model shape and pose deformations. To validate the efficiencyand applicability of our system, we collected a dataset that contains frontaland back scans of 83 clothed people with ground truth height and weight.Experiments on real-world dataset show that the proposed method can achievereal-time performance with competing results achieving an average error of 1.9cm in estimated measurements.
arxiv-1410-0996 | Minimax Analysis of Active Learning |  http://arxiv.org/abs/1410.0996  | author:Steve Hanneke, Liu Yang category:cs.LG math.ST stat.ML stat.TH published:2014-10-03 summary:This work establishes distribution-free upper and lower bounds on the minimaxlabel complexity of active learning with general hypothesis classes, undervarious noise models. The results reveal a number of surprising facts. Inparticular, under the noise model of Tsybakov (2004), the minimax labelcomplexity of active learning with a VC class is always asymptotically smallerthan that of passive learning, and is typically significantly smaller than thebest previously-published upper bounds in the active learning literature. Inhigh-noise regimes, it turns out that all active learning problems of a givenVC dimension have roughly the same minimax label complexity, which contrastswith well-known results for bounded noise. In low-noise regimes, we find thatthe label complexity is well-characterized by a simple combinatorial complexitymeasure we call the star number. Interestingly, we find that almost all of thecomplexity measures previously explored in the active learning literature haveworst-case values exactly equal to the star number. We also propose new activelearning strategies that nearly achieve these minimax label complexities.
arxiv-1410-0759 | cuDNN: Efficient Primitives for Deep Learning |  http://arxiv.org/abs/1410.0759  | author:Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, Evan Shelhamer category:cs.NE cs.LG cs.MS published:2014-10-03 summary:We present a library of efficient implementations of deep learningprimitives. Deep learning workloads are computationally intensive, andoptimizing their kernels is difficult and time-consuming. As parallelarchitectures evolve, kernels must be reoptimized, which makes maintainingcodebases difficult over time. Similar issues have long been addressed in theHPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS).However, there is no analogous library for deep learning. Without such alibrary, researchers implementing deep learning workloads on parallelprocessors must create and optimize their own implementations of the maincomputational kernels, and this work must be repeated as new parallelprocessors emerge. To address this problem, we have created a library similarin intent to BLAS, with optimized routines for deep learning workloads. Ourimplementation contains routines for GPUs, although similarly to the BLASlibrary, these routines could be implemented for other platforms. The libraryis easy to integrate into existing frameworks, and provides optimizedperformance and memory usage. For example, integrating cuDNN into Caffe, apopular framework for convolutional networks, improves performance by 36% on astandard model while also reducing memory consumption.
arxiv-1410-0860 | Individualized Rank Aggregation using Nuclear Norm Regularization |  http://arxiv.org/abs/1410.0860  | author:Yu Lu, Sahand N. Negahban category:stat.ML published:2014-10-03 summary:In recent years rank aggregation has received significant attention from themachine learning community. The goal of such a problem is to combine the(partially revealed) preferences over objects of a large population into asingle, relatively consistent ordering of those objects. However, in manycases, we might not want a single ranking and instead opt for individualrankings. We study a version of the problem known as collaborative ranking. Inthis problem we assume that individual users provide us with pairwisepreferences (for example purchasing one item over another). From thosepreferences we wish to obtain rankings on items that the users have not had anopportunity to explore. The results here have a very interesting connection tothe standard matrix completion problem. We provide a theoretical justificationfor a nuclear norm regularized optimization procedure, and providehigh-dimensional scaling results that show how the error in estimating userpreferences behaves as the number of observations increase.
arxiv-1410-0741 | Generalized Laguerre Reduction of the Volterra Kernel for Practical Identification of Nonlinear Dynamic Systems |  http://arxiv.org/abs/1410.0741  | author:Brett W. Israelsen, Dale A. Smith category:cs.LG published:2014-10-03 summary:The Volterra series can be used to model a large subset of nonlinear, dynamicsystems. A major drawback is the number of coefficients required model suchsystems. In order to reduce the number of required coefficients, Laguerrepolynomials are used to estimate the Volterra kernels. Existing literatureproposes algorithms for a fixed number of Volterra kernels, and Laguerreseries. This paper presents a novel algorithm for generalized calculation ofthe finite order Volterra-Laguerre (VL) series for a MIMO system. An exampleaddresses the utility of the algorithm in practical application.
arxiv-1410-0818 | Feature Learning from Incomplete EEG with Denoising Autoencoder |  http://arxiv.org/abs/1410.0818  | author:Junhua Li, Zbigniew Struzik, Liqing Zhang, Andrzej Cichocki category:cs.CV q-bio.NC published:2014-10-03 summary:An alternative pathway for the human brain to communicate with the outsideworld is by means of a brain computer interface (BCI). A BCI can decodeelectroencephalogram (EEG) signals of brain activities, and then send a commandor an intent to an external interactive device, such as a wheelchair. Theeffectiveness of the BCI depends on the performance in decoding the EEG.Usually, the EEG is contaminated by different kinds of artefacts (e.g.,electromyogram (EMG), background activity), which leads to a low decodingperformance. A number of filtering methods can be utilized to remove or weakenthe effects of artefacts, but they generally fail when the EEG contains extremeartefacts. In such cases, the most common approach is to discard the whole datasegment containing extreme artefacts. This causes the fatal drawback that theBCI cannot output decoding results during that time. In order to solve thisproblem, we employ the Lomb-Scargle periodogram to estimate the spectral powerfrom incomplete EEG (after removing only parts contaminated by artefacts), andDenoising Autoencoder (DAE) for learning. The proposed method is evaluated withmotor imagery EEG data. The results show that our method can successfullydecode incomplete EEG to good effect.
arxiv-1410-0440 | Scalable Nonlinear Learning with Adaptive Polynomial Expansions |  http://arxiv.org/abs/1410.0440  | author:Alekh Agarwal, Alina Beygelzimer, Daniel Hsu, John Langford, Matus Telgarsky category:cs.LG stat.ML published:2014-10-02 summary:Can we effectively learn a nonlinear representation in time comparable tolinear learning? We describe a new algorithm that explicitly and adaptivelyexpands higher-order interaction features over base linear representations. Thealgorithm is designed for extreme computational efficiency, and an extensiveexperimental study shows that its computation/prediction tradeoff abilitycompares very favorably against strong baselines.
arxiv-1410-0446 | Identification of Dynamic functional brain network states Through Tensor Decomposition |  http://arxiv.org/abs/1410.0446  | author:Arash Golibagh Mahyari, Selin Aviyente category:cs.NE q-bio.NC published:2014-10-02 summary:With the advances in high resolution neuroimaging, there has been a growinginterest in the detection of functional brain connectivity. Complex networktheory has been proposed as an attractive mathematical representation offunctional brain networks. However, most of the current studies of functionalbrain networks have focused on the computation of graph theoretic indices forstatic networks, i.e. long-time averages of connectivity networks. It iswell-known that functional connectivity is a dynamic process and theconstruction and reorganization of the networks is key to understanding humancognition. Therefore, there is a growing need to track dynamic functional brainnetworks and identify time intervals over which the network isquasi-stationary. In this paper, we present a tensor decomposition based methodto identify temporally invariant 'network states' and find a common topographicrepresentation for each state. The proposed methods are applied toelectroencephalogram (EEG) data during the study of error-related negativity(ERN).
arxiv-1410-0478 | Recognition of Handwritten Bangla Basic Characters and Digits using Convex Hull based Feature Set |  http://arxiv.org/abs/1410.0478  | author:Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri category:cs.CV published:2014-10-02 summary:In dealing with the problem of recognition of handwritten character patternsof varying shapes and sizes, selection of a proper feature set is important toachieve high recognition performance. The current research aims to evaluate theperformance of the convex hull based feature set, i.e. 125 features in allcomputed over different bays attributes of the convex hull of a pattern, foreffective recognition of isolated handwritten Bangla basic characters anddigits. On experimentation with a database of 10000 samples, the maximumrecognition rate of 76.86% is observed for handwritten Bangla characters. ForBangla numerals the maximum success rate of 99.45%. is achieved on a databaseof 12000 sample. The current work validates the usefulness of a new kind offeature set for recognition of handwritten Bangla basic characters andnumerals.
arxiv-1410-0507 | Generating functionals for computational intelligence: the Fisher information as an objective function for self-limiting Hebbian learning rules |  http://arxiv.org/abs/1410.0507  | author:Rodrigo Echeveste, Claudius Gros category:q-bio.NC cs.NE published:2014-10-02 summary:Generating functionals may guide the evolution of a dynamical system andconstitute a possible route for handling the complexity of neural networks asrelevant for computational intelligence. We propose and explore a new objectivefunction, which allows to obtain plasticity rules for the afferent synapticweights. The adaption rules are Hebbian, self-limiting, and result from theminimization of the Fisher information with respect to the synaptic flux. Weperform a series of simulations examining the behavior of the new learningrules in various circumstances. The vector of synaptic weights aligns with theprincipal direction of input activities, whenever one is present. A lineardiscrimination is performed when there are two or more principal directions;directions having bimodal firing-rate distributions, being characterized by anegative excess kurtosis, are preferred. We find robust performance and fullhomeostatic adaption of the synaptic weights results as a by-product of thesynaptic flux minimization. This self-limiting behavior allows for stableonline learning for arbitrary durations. The neuron acquires new informationwhen the statistics of input activities is changed at a certain point of thesimulation, showing however, a distinct resilience to unlearn previouslyacquired knowledge. Learning is fast when starting with randomly drawn synapticweights and substantially slower when the synaptic weights are already fullyadapted.
arxiv-1410-0510 | Deep Sequential Neural Network |  http://arxiv.org/abs/1410.0510  | author:Ludovic Denoyer, Patrick Gallinari category:cs.LG cs.NE published:2014-10-02 summary:Neural Networks sequentially build high-level features through theirsuccessive layers. We propose here a new neural network model where each layeris associated with a set of candidate mappings. When an input is processed, ateach layer, one mapping among these candidates is selected according to asequential decision process. The resulting model is structured according to aDAG like architecture, so that a path from the root to a leaf node defines asequence of transformations. Instead of considering global transformations,like in classical multilayer networks, this model allows us for learning a setof local transformations. It is thus able to process data with differentcharacteristics through specific sequences of such local transformations,increasing the expression power of this model w.r.t a classical multilayerednetwork. The learning algorithm is inspired from policy gradient techniquescoming from the reinforcement learning domain and is used here instead of theclassical back-propagation based gradient descent techniques. Experiments ondifferent datasets show the relevance of this approach.
arxiv-1410-0640 | Term-Weighting Learning via Genetic Programming for Text Classification |  http://arxiv.org/abs/1410.0640  | author:Hugo Jair Escalante, Mauricio A. García-Limón, Alicia Morales-Reyes, Mario Graff, Manuel Montes-y-Gómez, Eduardo F. Morales category:cs.NE cs.LG 68T50, 68T10 published:2014-10-02 summary:This paper describes a novel approach to learning term-weighting schemes(TWSs) in the context of text classification. In text mining a TWS determinesthe way in which documents will be represented in a vector space model, beforeapplying a classifier. Whereas acceptable performance has been obtained withstandard TWSs (e.g., Boolean and term-frequency schemes), the definition ofTWSs has been traditionally an art. Further, it is still a difficult task todetermine what is the best TWS for a particular problem and it is not clearyet, whether better schemes, than those currently available, can be generatedby combining known TWS. We propose in this article a genetic program that aimsat learning effective TWSs that can improve the performance of current schemesin text classification. The genetic program learns how to combine a set ofbasic units to give rise to discriminative TWSs. We report an extensiveexperimental study comprising data sets from thematic and non-thematic textclassification as well as from image classification. Our study shows thevalidity of the proposed method; in fact, we show that TWSs learned with thegenetic program outperform traditional schemes and other TWSs proposed inrecent works. Further, we show that TWSs learned from a specific domain can beeffectively used for other tasks.
arxiv-1410-1784 | Stochastic Discriminative EM |  http://arxiv.org/abs/1410.1784  | author:Andres R. Masegosa category:cs.LG published:2014-10-02 summary:Stochastic discriminative EM (sdEM) is an online-EM-type algorithm fordiscriminative training of probabilistic generative models belonging to theexponential family. In this work, we introduce and justify this algorithm as astochastic natural gradient descent method, i.e. a method which accounts forthe information geometry in the parameter space of the statistical model. Weshow how this learning algorithm can be used to train probabilistic generativemodels by minimizing different discriminative loss functions, such as thenegative conditional log-likelihood and the Hinge loss. The resulting modelstrained by sdEM are always generative (i.e. they define a joint probabilitydistribution) and, in consequence, allows to deal with missing data and latentvariables in a principled way either when being learned or when makingpredictions. The performance of this method is illustrated by several textclassification problems for which a multinomial naive Bayes and a latentDirichlet allocation based classifier are learned using differentdiscriminative loss functions.
arxiv-1410-0576 | Mapping Energy Landscapes of Non-Convex Learning Problems |  http://arxiv.org/abs/1410.0576  | author:Maria Pavlovskaia, Kewei Tu, Song-Chun Zhu category:stat.ML cs.LG published:2014-10-02 summary:In many statistical learning problems, the target functions to be optimizedare highly non-convex in various model spaces and thus are difficult toanalyze. In this paper, we compute \emph{Energy Landscape Maps} (ELMs) whichcharacterize and visualize an energy function with a tree structure, in whicheach leaf node represents a local minimum and each non-leaf node represents thebarrier between adjacent energy basins. The ELM also associates each node withthe estimated probability mass and volume for the corresponding energy basin.We construct ELMs by adopting the generalized Wang-Landau algorithm andmulti-domain sampler that simulates a Markov chain traversing the model spaceby dynamically reweighting the energy function. We construct ELMs in the modelspace for two classic statistical learning problems: i) clustering withGaussian mixture models or Bernoulli templates; and ii) bi-clustering. Wepropose a way to measure the difficulties (or complexity) of these learningproblems and study how various conditions affect the landscape complexity, suchas separability of the clusters, the number of examples, and the level ofsupervision; and we also visualize the behaviors of different algorithms, suchas K-mean, EM, two-step EM and Swendsen-Wang cuts, in the energy landscapes.
arxiv-1410-0602 | A probabilistic evolutionary optimization approach to compute quasiparticle braids |  http://arxiv.org/abs/1410.0602  | author:Roberto Santana, Ross B. McDonald, Helmut G. Katzgraber category:quant-ph cs.NE published:2014-10-02 summary:Topological quantum computing is an alternative framework for avoiding thequantum decoherence problem in quantum computation. The problem of executing agate in this framework can be posed as the problem of braiding quasiparticles.Because these are not Abelian, the problem can be reduced to finding an optimalproduct of braid generators where the optimality is defined in terms of thegate approximation and the braid's length. In this paper we propose the use ofdifferent variants of estimation of distribution algorithms to deal with theproblem. Furthermore, we investigate how the regularities of the braidoptimization problem can be translated into statistical regularities by meansof the Boltzmann distribution. We show that our best algorithm is able toproduce many solutions that approximates the target gate with an accuracy inthe order of $10^{-6}$, and have lengths up to 9 times shorter than thoseexpected from braids of the same accuracy obtained with other methods.
arxiv-1410-0555 | Linear State-Space Model with Time-Varying Dynamics |  http://arxiv.org/abs/1410.0555  | author:Jaakko Luttinen, Tapani Raiko, Alexander Ilin category:stat.ML published:2014-10-02 summary:This paper introduces a linear state-space model with time-varying dynamics.The time dependency is obtained by forming the state dynamics matrix as atime-varying linear combination of a set of matrices. The time dependency ofthe weights in the linear combination is modelled by another linear Gaussiandynamical model allowing the model to learn how the dynamics of the processchanges. Previous approaches have used switching models which have a small setof possible state dynamics matrices and the model selects one of those matricesat each time, thus jumping between them. Our model forms the dynamics as alinear combination and the changes can be smooth and more continuous. The modelis motivated by physical processes which are described by linear partialdifferential equations whose parameters vary in time. An example of such aprocess could be a temperature field whose evolution is driven by a varyingwind direction. The posterior inference is performed using variational Bayesianapproximation. The experiments on stochastic advection-diffusion processes andreal-world weather processes show that the model with time-varying dynamics canoutperform previously introduced approaches.
arxiv-1410-0630 | Deep Directed Generative Autoencoders |  http://arxiv.org/abs/1410.0630  | author:Sherjil Ozair, Yoshua Bengio category:stat.ML cs.LG cs.NE published:2014-10-02 summary:For discrete data, the likelihood $P(x)$ can be rewritten exactly andparametrized into $P(X = x) = P(X = x H = f(x)) P(H = f(x))$ if $P(X H)$has enough capacity to put no probability mass on any $x'$ for which $f(x')\neqf(x)$, where $f(\cdot)$ is a deterministic discrete function. The log of thefirst factor gives rise to the log-likelihood reconstruction error of anautoencoder with $f(\cdot)$ as the encoder and $P(XH)$ as the (probabilistic)decoder. The log of the second term can be seen as a regularizer on the encodedactivations $h=f(x)$, e.g., as in sparse autoencoders. Both encoder and decodercan be represented by a deep neural network and trained to maximize the averageof the optimal log-likelihood $\log p(x)$. The objective is to learn an encoder$f(\cdot)$ that maps $X$ to $f(X)$ that has a much simpler distribution than$X$ itself, estimated by $P(H)$. This "flattens the manifold" or concentratesprobability mass in a smaller number of (relevant) dimensions over which thedistribution factorizes. Generating samples from the model is straightforwardusing ancestral sampling. One challenge is that regular back-propagation cannotbe used to obtain the gradient on the parameters of the encoder, but we findthat using the straight-through estimator works well here. We also find thatalthough optimizing a single level of such architecture may be difficult, muchbetter results can be obtained by pre-training and stacking them, graduallytransforming the data distribution into one that is more easily captured by asimple parametric model.
arxiv-1410-0719 | Proceedings of the second "international Traveling Workshop on Interactions between Sparse models and Technology" (iTWIST'14) |  http://arxiv.org/abs/1410.0719  | author:L. Jacques, C. De Vleeschouwer, Y. Boursier, P. Sudhakar, C. De Mol, A. Pizurica, S. Anthoine, P. Vandergheynst, P. Frossard, C. Bilen, S. Kitic, N. Bertin, R. Gribonval, N. Boumal, B. Mishra, P. -A. Absil, R. Sepulchre, S. Bundervoet, C. Schretter, A. Dooms, P. Schelkens, O. Chabiron, F. Malgouyres, J. -Y. Tourneret, N. Dobigeon, P. Chainais, C. Richard, B. Cornelis, I. Daubechies, D. Dunson, M. Dankova, P. Rajmic, K. Degraux, V. Cambareri, B. Geelen, G. Lafruit, G. Setti, J. -F. Determe, J. Louveaux, F. Horlin, A. Drémeau, P. Heas, C. Herzet, V. Duval, G. Peyré, A. Fawzi, M. Davies, N. Gillis, S. A. Vavasis, C. Soussen, L. Le Magoarou, J. Liang, J. Fadili, A. Liutkus, D. Martina, S. Gigan, L. Daudet, M. Maggioni, S. Minsker, N. Strawn, C. Mory, F. Ngole, J. -L. Starck, I. Loris, S. Vaiter, M. Golbabaee, D. Vukobratovic category:cs.NA cs.CV cs.IT cs.LG math.IT math.OC math.ST stat.TH published:2014-10-02 summary:The implicit objective of the biennial "international - Traveling Workshop onInteractions between Sparse models and Technology" (iTWIST) is to fostercollaboration between international scientific teams by disseminating ideasthrough both specific oral/poster presentations and free discussions. For itssecond edition, the iTWIST workshop took place in the medieval and picturesquetown of Namur in Belgium, from Wednesday August 27th till Friday August 29th,2014. The workshop was conveniently located in "The Arsenal" building withinwalking distance of both hotels and town center. iTWIST'14 has gathered about70 international participants and has featured 9 invited talks, 10 oralpresentations, and 14 posters on the following themes, all related to thetheory, application and generalization of the "sparsity paradigm":Sparsity-driven data sensing and processing; Union of low dimensionalsubspaces; Beyond linear and convex inverse problem; Matrix/manifold/graphsensing/processing; Blind inverse problems and dictionary learning; Sparsityand computational neuroscience; Information theory, geometry and randomness;Complexity/accuracy tradeoffs in numerical methods; Sparsity? What's next?;Sparse machine learning and inference.
arxiv-1410-0582 | Multidimensional Digital Smoothing Filters for Target Detection |  http://arxiv.org/abs/1410.0582  | author:Hugh L. Kennedy category:cs.CV published:2014-10-02 summary:Recursive, causal and non-causal, multidimensional digital filters, withinfinite impulse responses and maximally flat magnitude and delay responses inthe low-frequency region, are designed to negate correlated clutter andinterference in the background and to accumulate power due to dim targets inthe foreground of a surveillance sensor. Expressions relating meanimpulse-response duration, frequency selectivity and group delay, to low-orderlinear-difference-equation coefficients are derived using discrete Laguerrepolynomials and discounted least-squares regression, then verified throughsimulation.
arxiv-1410-0547 | Design Mining Interacting Wind Turbines |  http://arxiv.org/abs/1410.0547  | author:Richard J. Preen, Larry Bull category:cs.NE cs.AI cs.CE published:2014-10-02 summary:An initial study of surrogate-assisted evolutionary algorithms used to designvertical-axis wind turbines wherein candidate prototypes are evaluated underfan generated wind conditions after being physically instantiated by a 3Dprinter has recently been presented. Unlike other approaches, such ascomputational fluid dynamics simulations, no mathematical formulations wereused and no model assumptions were made. This paper extends that work byexploring alternative surrogate modelling and evolutionary techniques. Theaccuracy of various modelling algorithms used to estimate the fitness ofevaluated individuals from the initial experiments is compared. The effect oftemporally windowing surrogate model training samples is explored. Asurrogate-assisted approach based on an enhanced local search is introduced;and alternative coevolution collaboration schemes are examined.
arxiv-1410-0718 | Not All Neural Embeddings are Born Equal |  http://arxiv.org/abs/1410.0718  | author:Felix Hill, KyungHyun Cho, Sebastien Jean, Coline Devin, Yoshua Bengio category:cs.CL published:2014-10-02 summary:Neural language models learn word representations that capture richlinguistic and conceptual information. Here we investigate the embeddingslearned by neural machine translation models. We show that translation-basedembeddings outperform those learned by cutting-edge monolingual models atsingle-language tasks requiring knowledge of conceptual similarity and/orsyntactic role. The findings suggest that, while monolingual models learninformation about how concepts are related, neural-translation models bettercapture their true ontological status.
arxiv-1410-0633 | Deterministic Conditions for Subspace Identifiability from Incomplete Sampling |  http://arxiv.org/abs/1410.0633  | author:Daniel L. Pimentel-Alarcón, Robert D. Nowak, Nigel Boston category:stat.ML cs.LG math.CO published:2014-10-02 summary:Consider a generic $r$-dimensional subspace of $\mathbb{R}^d$, $r<d$, andsuppose that we are only given projections of this subspace onto small subsetsof the canonical coordinates. The paper establishes necessary and sufficientdeterministic conditions on the subsets for subspace identifiability.
arxiv-1410-0723 | A Lower Bound for the Optimization of Finite Sums |  http://arxiv.org/abs/1410.0723  | author:Alekh Agarwal, Leon Bottou category:stat.ML math.OC published:2014-10-02 summary:This paper presents a lower bound for optimizing a finite sum of $n$functions, where each function is $L$-smooth and the sum is $\mu$-stronglyconvex. We show that no algorithm can reach an error $\epsilon$ in minimizingall functions from this class in fewer than $\Omega(n +\sqrt{n(\kappa-1)}\log(1/\epsilon))$ iterations, where $\kappa=L/\mu$ is asurrogate condition number. We then compare this lower bound to upper boundsfor recently developed methods specializing to this setting. When the functionsinvolved in this sum are not arbitrary, but based on i.i.d. random data, thenwe further contrast these complexity results with those for optimal first-ordermethods to directly optimize the sum. The conclusion we draw is that a lot ofcaution is necessary for an accurate comparison, and identify machine learningscenarios where the new methods help computationally.
arxiv-1410-0389 | Learning to Transfer Privileged Information |  http://arxiv.org/abs/1410.0389  | author:Viktoriia Sharmanska, Novi Quadrianto, Christoph H. Lampert category:cs.CV stat.ML published:2014-10-01 summary:We introduce a learning framework called learning using privilegedinformation (LUPI) to the computer vision field. We focus on the prototypicalcomputer vision problem of teaching computers to recognize objects in images.We want the computers to be able to learn faster at the expense of providingextra information during training time. As additional information about theimage data, we look at several scenarios that have been studied in computervision before: attributes, bounding boxes and image tags. The information isprivileged as it is available at training time but not at test time. We exploretwo maximum-margin techniques that are able to make use of this additionalsource of information, for binary and multiclass object classification. Weinterpret these methods as learning easiness and hardness of the objects in theprivileged space and then transferring this knowledge to train a betterclassifier in the original space. We provide a thorough analysis and comparisonof information transfer from privileged to the original data spaces for bothLUPI methods. Our experiments show that incorporating privileged informationcan improve the classification accuracy. Finally, we conduct user studies tounderstand which samples are easy and which are hard for human learning, andexplore how this information is related to easy and hard samples when learninga classifier.
arxiv-1410-0334 | Domain adaptation of weighted majority votes via perturbed variation-based self-labeling |  http://arxiv.org/abs/1410.0334  | author:Emilie Morvant category:stat.ML cs.LG published:2014-10-01 summary:In machine learning, the domain adaptation problem arrives when the test(target) and the train (source) data are generated from differentdistributions. A key applied issue is thus the design of algorithms able togeneralize on a new distribution, for which we have no label information. Wefocus on learning classification models defined as a weighted majority voteover a set of real-val ued functions. In this context, Germain et al. (2013)have shown that a measure of disagreement between these functions is crucial tocontrol. The core of this measure is a theoretical bound--the C-bound (Lacasseet al., 2007)--which involves the disagreement and leads to a well performingmajority vote learning algorithm in usual non-adaptative supervised setting:MinCq. In this work, we propose a framework to extend MinCq to a domainadaptation scenario. This procedure takes advantage of the recent perturbedvariation divergence between distributions proposed by Harel and Mannor (2012).Justified by a theoretical bound on the target risk of the vote, we provide toMinCq a target sample labeled thanks to a perturbed variation-basedself-labeling focused on the regions where the source and target marginalsappear similar. We also study the influence of our self-labeling, from which wededuce an original process for tuning the hyperparameters. Finally, ourframework called PV-MinCq shows very promising results on a rotation andtranslation synthetic problem.
arxiv-1410-0316 | Using social network graph analysis for interest detection |  http://arxiv.org/abs/1410.0316  | author:Brian Lee Yung Rowe category:cs.SI cs.CL physics.soc-ph published:2014-10-01 summary:A person's interests exist as an internal state and are difficult to define.Since only external actions are observable, a proxy must be used thatrepresents someone's interests. Techniques like collaborative filtering,behavioral targeting, and hashtag analysis implicitly model an individual'sinterests. I argue that these models are limited to shallow, temporaryinterests, which do not reflect people's deeper interests or passions. Ipropose an alternative model of interests that takes advantage of a user'ssocial graph. The basic principle is that people only follow those thatinterest them, so the social graph is an effective and robust proxy forpeople's interests.
arxiv-1410-0286 | LAF-Fabric: a data analysis tool for Linguistic Annotation Framework with an application to the Hebrew Bible |  http://arxiv.org/abs/1410.0286  | author:Dirk Roorda, Gino Kalkman, Martijn Naaijer, Andreas van Cranenburgh category:cs.CL published:2014-10-01 summary:The Linguistic Annotation Framework (LAF) provides a general, extensiblestand-off markup system for corpora. This paper discusses LAF-Fabric, a newtool to analyse LAF resources in general with an extension to process theHebrew Bible in particular. We first walk through the history of the HebrewBible as text database in decennium-wide steps. Then we describe how LAF-Fabricmay serve as an analysis tool for this corpus. Finally, we describe threeanalytic projects/workflows that benefit from the new LAF representation: 1) the study of linguistic variation: extract cooccurrence data of commonnouns between the books of the Bible (Martijn Naaijer); 2) the study of thegrammar of Hebrew poetry in the Psalms: extract clause typology (Gino Kalkman);3) construction of a parser of classical Hebrew by Data Oriented Parsing:generate tree structures from the database (Andreas van Cranenburgh).
arxiv-1410-0243 | Pattern Encoding on the Poincare Sphere |  http://arxiv.org/abs/1410.0243  | author:Aleksandra Pizurica category:cs.CV published:2014-10-01 summary:This paper presents a convenient graphical tool for encoding visual patterns(such as image patches and image atoms) as point constellations in a spacespanned by perceptual features and with a clear geometrical interpretation.General theory and a practical pattern encoding scheme are presented, inspiredby encoding polarization states of a light wave on the Poincare sphere. Thisnew pattern encoding scheme can be useful for many applications in imageprocessing and computer vision. Here, three possible applications areillustrated, in clustering perceptually similar patterns, visualizingproperties of learned dictionaries of image atoms and generating newdictionaries of image atoms from spherical codes.
arxiv-1410-0162 | Reservoir Computing using Cellular Automata |  http://arxiv.org/abs/1410.0162  | author:Ozgur Yilmaz category:cs.NE published:2014-10-01 summary:We introduce a novel framework of reservoir computing. Cellular automaton isused as the reservoir of dynamical systems. Input is randomly projected ontothe initial conditions of automaton cells and nonlinear computation isperformed on the input via application of a rule in the automaton for a periodof time. The evolution of the automaton creates a space-time volume of theautomaton state space, and it is used as the reservoir. The proposed frameworkis capable of long short-term memory and it requires orders of magnitude lesscomputation compared to Echo State Networks. Also, for additive cellularautomaton rules, reservoir features can be combined using Boolean operations,which provides a direct way for concept building and symbolic processing, andit is much more efficient compared to state-of-the-art approaches.
arxiv-1410-0123 | Deep Tempering |  http://arxiv.org/abs/1410.0123  | author:Guillaume Desjardins, Heng Luo, Aaron Courville, Yoshua Bengio category:cs.LG stat.ML published:2014-10-01 summary:Restricted Boltzmann Machines (RBMs) are one of the fundamental buildingblocks of deep learning. Approximate maximum likelihood training of RBMstypically necessitates sampling from these models. In many training scenarios,computationally efficient Gibbs sampling procedures are crippled by poormixing. In this work we propose a novel method of sampling from Boltzmannmachines that demonstrates a computationally efficient way to promote mixing.Our approach leverages an under-appreciated property of deep generative modelssuch as the Deep Belief Network (DBN), where Gibbs sampling from deeper levelsof the latent variable hierarchy results in dramatically increased ergodicity.Our approach is thus to train an auxiliary latent hierarchical model, based onthe DBN. When used in conjunction with parallel-tempering, the method isasymptotically guaranteed to simulate samples from the target RBM. Experimentalresults confirm the effectiveness of this sampling strategy in the context ofRBM training.
arxiv-1410-0095 | Riemannian Multi-Manifold Modeling |  http://arxiv.org/abs/1410.0095  | author:Xu Wang, Konstantinos Slavakis, Gilad Lerman category:stat.ML cs.CV cs.LG published:2014-10-01 summary:This paper advocates a novel framework for segmenting a dataset in aRiemannian manifold $M$ into clusters lying around low-dimensional submanifoldsof $M$. Important examples of $M$, for which the proposed clustering algorithmis computationally efficient, are the sphere, the set of positive definitematrices, and the Grassmannian. The clustering problem with these examples of$M$ is already useful for numerous application domains such as actionidentification in video sequences, dynamic texture clustering, brain fibersegmentation in medical imaging, and clustering of deformed images. Theproposed clustering algorithm constructs a data-affinity matrix by thoroughlyexploiting the intrinsic geometry and then applies spectral clustering. Theintrinsic local geometry is encoded by local sparse coding and more importantlyby directional information of local tangent spaces and geodesics. Theoreticalguarantees are established for a simplified variant of the algorithm even whenthe clusters intersect. To avoid complication, these guarantees assume that theunderlying submanifolds are geodesic. Extensive validation on synthetic andreal data demonstrates the resiliency of the proposed method against deviationsfrom the theoretical model as well as its superior performance overstate-of-the-art techniques.
arxiv-1410-0117 | Coupling Top-down and Bottom-up Methods for 3D Human Pose and Shape Estimation from Monocular Image Sequences |  http://arxiv.org/abs/1410.0117  | author:Atul Kanaujia category:cs.CV published:2014-10-01 summary:Until recently Intelligence, Surveillance, and Reconnaissance (ISR) focusedon acquiring behavioral information of the targets and their activities.Continuous evolution of intelligence being gathered of the human centricactivities has put increased focus on the humans, especially inferring theirinnate characteristics - size, shapes and physiology. These bio-signaturesextracted from the surveillance sensors can be used to deduce age, ethnicity,gender and actions, and further characterize human actions in unseen scenarios.However, recovery of pose and shape of humans in such monocular videos isinherently an ill-posed problem, marked by frequent depth and view basedambiguities due to self-occlusion, foreshortening and misalignment. Thelikelihood function often yields a highly multimodal posterior that isdifficult to propagate even using the most advanced particle filtering(PF)algorithms. Motivated by the recent success of the discriminative approaches toefficiently predict 3D poses directly from the 2D images, we present severalprincipled approaches to integrate predictive cues using learned regressionmodels to sustain multimodality of the posterior during tracking. Additionally,these learned priors can be actively adapted to the test data using alikelihood based feedback mechanism. Estimated 3D poses are then used to fit 3Dhuman shape model to each frame independently for inferring anthropometricbio-signatures. The proposed system is fully automated, robust to noisy testdata and has ability to swiftly recover from tracking failures even afterconfronting with significant errors. We evaluate the system on a large numberof monocular human motion sequences.
arxiv-1410-0210 | A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input |  http://arxiv.org/abs/1410.0210  | author:Mateusz Malinowski, Mario Fritz category:cs.AI cs.CL cs.CV cs.LG published:2014-10-01 summary:We propose a method for automatically answering questions about images bybringing together recent advances from natural language processing and computervision. We combine discrete reasoning with uncertain predictions by amulti-world approach that represents uncertainty about the perceived world in abayesian framework. Our approach can handle human questions of high complexityabout realistic scenes and replies with range of answer like counts, objectclasses, instances and lists of them. The system is directly trained fromquestion-answer pairs. We establish a first benchmark for this task that can beseen as a modern attempt at a visual turing test.
arxiv-1410-0260 | ASKIT: Approximate Skeletonization Kernel-Independent Treecode in High Dimensions |  http://arxiv.org/abs/1410.0260  | author:William B. March, Bo Xiao, George Biros category:cs.DS cs.LG published:2014-10-01 summary:We present a fast algorithm for kernel summation problems in high-dimensions.These problems appear in computational physics, numerical approximation,non-parametric statistics, and machine learning. In our context, the sumsdepend on a kernel function that is a pair potential defined on a dataset ofpoints in a high-dimensional Euclidean space. A direct evaluation of the sumscales quadratically with the number of points. Fast kernel summation methodscan reduce this cost to linear complexity, but the constants involved do notscale well with the dimensionality of the dataset. The main algorithmic components of fast kernel summation algorithms are theseparation of the kernel sum between near and far field (which is the basis forpruning) and the efficient and accurate approximation of the far field. We introduce novel methods for pruning and approximating the far field. Ourfar field approximation requires only kernel evaluations and does not useanalytic expansions. Pruning is not done using bounding boxes but rathercombinatorially using a sparsified nearest-neighbor graph of the input. Thetime complexity of our algorithm depends linearly on the ambient dimension. Theerror in the algorithm depends on the low-rank approximability of the farfield, which in turn depends on the kernel function and on the intrinsicdimensionality of the distribution of the points. The error of the far fieldapproximation does not depend on the ambient dimension. We present the new algorithm along with experimental results that demonstrateits performance. We report results for Gaussian kernel sums for 100 millionpoints in 64 dimensions, for one million points in 1000 dimensions, and forproblems in which the Gaussian kernel has a variable bandwidth. To the best ofour knowledge, all of these experiments are impossible or prohibitivelyexpensive with existing fast kernel summation methods.
arxiv-1410-0342 | Generalized Low Rank Models |  http://arxiv.org/abs/1410.0342  | author:Madeleine Udell, Corinne Horn, Reza Zadeh, Stephen Boyd category:stat.ML cs.LG math.OC published:2014-10-01 summary:Principal components analysis (PCA) is a well-known technique forapproximating a tabular data set by a low rank matrix. Here, we extend the ideaof PCA to handle arbitrary data sets consisting of numerical, Boolean,categorical, ordinal, and other data types. This framework encompasses manywell known techniques in data analysis, such as nonnegative matrixfactorization, matrix completion, sparse and robust PCA, $k$-means, $k$-SVD,and maximum margin matrix factorization. The method handles heterogeneous datasets, and leads to coherent schemes for compressing, denoising, and imputingmissing entries across all data types simultaneously. It also admits a numberof interesting interpretations of the low rank factors, which allow clusteringof examples or of features. We propose several parallel algorithms for fittinggeneralized low rank models, and describe implementations and numericalresults.
arxiv-1410-0291 | A Morphological Analyzer for Japanese Nouns, Verbs and Adjectives |  http://arxiv.org/abs/1410.0291  | author:Yanchuan Sim category:cs.CL published:2014-10-01 summary:We present an open source morphological analyzer for Japanese nouns, verbsand adjectives. The system builds upon the morphological analyzing capabilitiesof MeCab to incorporate finer details of classification such as politeness,tense, mood and voice attributes. We implemented our analyzer in the form of afinite state transducer using the open source finite state compiler FOMAtoolkit. The source code and tool is available athttps://bitbucket.org/skylander/yc-nlplab/.
arxiv-1409-8498 | Non-Myopic Learning in Repeated Stochastic Games |  http://arxiv.org/abs/1409.8498  | author:Jacob W. Crandall category:cs.GT cs.AI cs.LG published:2014-09-30 summary:This paper addresses learning in repeated stochastic games (RSGs) playedagainst unknown associates. Learning in RSGs is extremely challenging due totheir inherently large strategy spaces. Furthermore, these games typically havemultiple (often infinite) equilibria, making attempts to solve them viaequilibrium analysis and rationality assumptions wholly insufficient. As such,previous learning algorithms for RSGs either learn very slowly or makeextremely limiting assumptions about the game structure or associates'behaviors. In this paper, we propose and evaluate the notion of gameabstraction by experts (Gabe) for two-player general-sum RSGs. Gabe reduces anRSG to a multi-armed bandit problem, which can then be solved using an expertalgorithm. Gabe maintains many aspects of the original game, including securityand Pareto optimal Nash equilibria. We demonstrate that Gabe substantiallyoutperforms existing algorithms in many scenarios.
arxiv-1409-8485 | Predicting missing links via correlation between nodes |  http://arxiv.org/abs/1409.8485  | author:Hao Liao, An Zeng, Yi-Cheng Zhang category:physics.soc-ph cs.SI stat.ML published:2014-09-30 summary:As a fundamental problem in many different fields, link prediction aims toestimate the likelihood of an existing link between two nodes based on theobserved information. Since this problem is related to many applicationsranging from uncovering missing data to predicting the evolution of networks,link prediction has been intensively investigated recently and many methodshave been proposed so far. The essential challenge of link prediction is toestimate the similarity between nodes. Most of the existing methods are basedon the common neighbor index and its variants. In this paper, we propose tocalculate the similarity between nodes by the correlation coefficient. Thismethod is found to be very effective when applied to calculate similarity basedon high order paths. We finally fuse the correlation-based method with theresource allocation method, and find that the combined method can substantiallyoutperform the existing methods, especially in sparse networks.
arxiv-1409-8484 | An agent-driven semantical identifier using radial basis neural networks and reinforcement learning |  http://arxiv.org/abs/1409.8484  | author:Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana category:cs.NE cs.AI cs.CL cs.LG cs.MA published:2014-09-30 summary:Due to the huge availability of documents in digital form, and the deceptionpossibility raise bound to the essence of digital documents and the way theyare spread, the authorship attribution problem has constantly increased itsrelevance. Nowadays, authorship attribution,for both information retrieval andanalysis, has gained great importance in the context of security, trust andcopyright preservation. This work proposes an innovative multi-agent drivenmachine learning technique that has been developed for authorship attribution.By means of a preprocessing for word-grouping and time-period related analysisof the common lexicon, we determine a bias reference level for the recurrencefrequency of the words within analysed texts, and then train a Radial BasisNeural Networks (RBPNN)-based classifier to identify the correct author. Themain advantage of the proposed approach lies in the generality of the semanticanalysis, which can be applied to different contexts and lexical domains,without requiring any modification. Moreover, the proposed system is able toincorporate an external input, meant to tune the classifier, and thenself-adjust by means of continuous learning reinforcement.
arxiv-1409-8428 | Nonstochastic Multi-Armed Bandits with Graph-Structured Feedback |  http://arxiv.org/abs/1409.8428  | author:Noga Alon, Nicolò Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, Ohad Shamir category:cs.LG stat.ML published:2014-09-30 summary:We present and study a partial-information model of online learning, where adecision maker repeatedly chooses from a finite set of actions, and observessome subset of the associated losses. This naturally models several situationswhere the losses of different actions are related, and knowing the loss of oneaction provides information on the loss of other actions. Moreover, itgeneralizes and interpolates between the well studied full-information setting(where all losses are revealed) and the bandit setting (where only the loss ofthe action chosen by the player is revealed). We provide several algorithmsaddressing different variants of our setting, and provide tight regret boundsdepending on combinatorial properties of the information feedback structure.
arxiv-1410-2488 | Computational Beauty: Aesthetic Judgment at the Intersection of Art and Science |  http://arxiv.org/abs/1410.2488  | author:Emily L. Spratt, Ahmed Elgammal category:cs.CV published:2014-09-30 summary:In part one of the Critique of Judgment, Immanuel Kant wrote that "thejudgment of taste...is not a cognitive judgment, and so not logical, but isaesthetic."\cite{Kant} While the condition of aesthetic discernment has longbeen the subject of philosophical discourse, the role of the arbiters of thatjudgment has more often been assumed than questioned. The art historian,critic, connoisseur, and curator have long held the esteemed position of theaesthetic judge, their training, instinct, and eye part of the inimitablesubjective processes that Kant described as occurring upon artistic evaluation.Although the concept of intangible knowledge in regard to aesthetic theory hasbeen much explored, little discussion has arisen in response to the developmentof new types of artificial intelligence as a challenge to the seeminglyineffable abilities of the human observer. This paper examines the developmentsin the field of computer vision analysis of paintings from canonical movementswith the history of Western art and the reaction of art historians to theapplication of this technology in the field. Through an investigation of theethical consequences of this innovative technology, the unquestioned authorityof the art expert is challenged and the subjective nature of aesthetic judgmentis brought to philosophical scrutiny once again.
arxiv-1409-8630 | Unsupervised Bump Hunting Using Principal Components |  http://arxiv.org/abs/1409.8630  | author:Daniel A Díaz-Pachón, Jean-Eudes Dazard, J. Sunil Rao category:stat.ML 65C60 published:2014-09-30 summary:Principal Components Analysis is a widely used technique for dimensionreduction and characterization of variability in multivariate populations. Ourinterest lies in studying when and why the rotation to principal components canbe used effectively within a response-predictor set relationship in the contextof mode hunting. Specifically focusing on the Patient Rule Induction Method(PRIM), we first develop a fast version of this algorithm (fastPRIM) undernormality which facilitates the theoretical studies to follow. Using basicgeometrical arguments, we then demonstrate how the PC rotation of the predictorspace alone can in fact generate improved mode estimators. Simulation resultsare used to illustrate our findings.
arxiv-1409-8558 | A Deep Learning Approach to Data-driven Parameterizations for Statistical Parametric Speech Synthesis |  http://arxiv.org/abs/1409.8558  | author:Prasanna Kumar Muthukumar, Alan W. Black category:cs.CL cs.LG cs.NE published:2014-09-30 summary:Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstralcoefficients as the vocal tract parameterization of the speech signal. MelCepstral coefficients were never intended to work in a parametric speechsynthesis framework, but as yet, there has been little success in creating abetter parameterization that is more suited to synthesis. In this paper, we usedeep learning algorithms to investigate a data-driven parameterizationtechnique that is designed for the specific requirements of synthesis. Wecreate an invertible, low-dimensional, noise-robust encoding of the Mel LogSpectrum by training a tapered Stacked Denoising Autoencoder (SDA). This SDA isthen unwrapped and used as the initialization for a Multi-Layer Perceptron(MLP). The MLP is fine-tuned by training it to reconstruct the input at theoutput layer. This MLP is then split down the middle to form encoding anddecoding networks. These networks produce a parameterization of the Mel LogSpectrum that is intended to better fulfill the requirements of synthesis.Results are reported for experiments conducted using this resultingparameterization with the ClusterGen speech synthesizer.
arxiv-1409-8576 | Data Imputation through the Identification of Local Anomalies |  http://arxiv.org/abs/1409.8576  | author:Huseyin Ozkan, Ozgun S. Pelvan, Suleyman S. Kozat category:cs.LG stat.ML published:2014-09-30 summary:We introduce a comprehensive and statistical framework in a model freesetting for a complete treatment of localized data corruptions due to severenoise sources, e.g., an occluder in the case of a visual recording. Within thisframework, we propose i) a novel algorithm to efficiently separate, i.e.,detect and localize, possible corruptions from a given suspicious data instanceand ii) a Maximum A Posteriori (MAP) estimator to impute the corrupted data. Asa generalization to Euclidean distance, we also propose a novel distancemeasure, which is based on the ranked deviations among the data attributes andempirically shown to be superior in separating the corruptions. Our algorithmfirst splits the suspicious instance into parts through a binary partitioningtree in the space of data attributes and iteratively tests those parts todetect local anomalies using the nominal statistics extracted from anuncorrupted (clean) reference data set. Once each part is labeled as anomalousvs normal, the corresponding binary patterns over this tree that characterizecorruptions are identified and the affected attributes are imputed. Under acertain conditional independency structure assumed for the binary patterns, weanalytically show that the false alarm rate of the introduced algorithm indetecting the corruptions is independent of the data and can be directly setwithout any parameter tuning. The proposed framework is tested over severalwell-known machine learning data sets with synthetically generated corruptions;and experimentally shown to produce remarkable improvements in terms ofclassification purposes with strong corruption separation capabilities. Ourexperiments also indicate that the proposed algorithms outperform the typicalapproaches and are robust to varying training phase conditions.
arxiv-1409-8606 | Distributed Detection : Finite-time Analysis and Impact of Network Topology |  http://arxiv.org/abs/1409.8606  | author:Shahin Shahrampour, Alexander Rakhlin, Ali Jadbabaie category:math.OC cs.LG cs.SI stat.ML published:2014-09-30 summary:This paper addresses the problem of distributed detection in multi-agentnetworks. Agents receive private signals about an unknown state of the world.The underlying state is globally identifiable, yet informative signals may bedispersed throughout the network. Using an optimization-based framework, wedevelop an iterative local strategy for updating individual beliefs. Incontrast to the existing literature which focuses on asymptotic learning, weprovide a finite-time analysis. Furthermore, we introduce a Kullback-Leiblercost to compare the efficiency of the algorithm to its centralized counterpart.Our bounds on the cost are expressed in terms of network size, spectral gap,centrality of each agent and relative entropy of agents' signal structures. Akey observation is that distributing more informative signals to central agentsresults in a faster learning rate. Furthermore, optimizing the weights, we canspeed up learning by improving the spectral gap. We also quantify the effect oflink failures on learning speed in symmetric networks. We finally providenumerical simulations which verify our theoretical results.
arxiv-1409-8359 | Backhaul-Constrained Multi-Cell Cooperation Leveraging Sparsity and Spectral Clustering |  http://arxiv.org/abs/1409.8359  | author:Swayambhoo Jain, Seung-Jun Kim, Georgios B. Giannakis category:cs.IT cs.NI math.IT stat.ML published:2014-09-30 summary:Multi-cell cooperative processing with limited backhaul traffic is studiedfor cellular uplinks. Aiming at reduced backhaul overhead, asparsity-regularized multi-cell receive-filter design problem is formulated.Both unstructured distributed cooperation as well as clustered cooperation, inwhich base station groups are formed for tight cooperation, are considered.Dynamic clustered cooperation, where the sparse equalizer and the cooperationclusters are jointly determined, is solved via alternating minimization basedon spectral clustering and group-sparse regression. Furthermore, decentralizedimplementations of both unstructured and clustered cooperation schemes aredeveloped for scalability, robustness and computational efficiency. Extensivenumerical tests verify the efficacy of the proposed methods.
arxiv-1409-8403 | Evaluation of Output Embeddings for Fine-Grained Image Classification |  http://arxiv.org/abs/1409.8403  | author:Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, Bernt Schiele category:cs.CV published:2014-09-30 summary:Image classification has advanced significantly in recent years with theavailability of large-scale image sets. However, fine-grained classificationremains a major challenge due to the annotation cost of large numbers offine-grained categories. This project shows that compelling classificationperformance can be achieved on such categories even without labeled trainingdata. Given image and class embeddings, we learn a compatibility function suchthat matching embeddings are assigned a higher score than mismatching ones;zero-shot classification of an image proceeds by finding the label yielding thehighest joint compatibility score. We use state-of-the-art image features andfocus on different supervised attributes and unsupervised output embeddingseither derived from hierarchies or learned from unlabeled text corpora. Weestablish a substantially improved state-of-the-art on the Animals withAttributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstratethat purely unsupervised output embeddings (learned from Wikipedia and improvedwith fine-grained text) achieve compelling results, even outperforming theprevious supervised state-of-the-art. By combining different output embeddings,we further improve results.
arxiv-1409-8437 | Fully adaptive density-based clustering |  http://arxiv.org/abs/1409.8437  | author:Ingo Steinwart category:stat.ME stat.ML published:2014-09-30 summary:The clusters of a distribution are often defined by the connected componentsof a density level set. However, this definition depends on the user-specifiedlevel. We address this issue by proposing a simple, generic algorithm, whichuses an almost arbitrary level set estimator to estimate the smallest level atwhich there are more than one connected components. In the case where thisalgorithm is fed with histogram-based level set estimates, we provide a finitesample analysis, which is then used to show that the algorithm consistentlyestimates both the smallest level and the corresponding connected components.We further establish rates of convergence for the two estimation problems, andlast but not least, we present a simple, yet adaptive strategy for determiningthe width-parameter of the involved density estimator in a data-depending way.
arxiv-1409-8500 | Hyper-Spectral Image Analysis with Partially-Latent Regression and Spatial Markov Dependencies |  http://arxiv.org/abs/1409.8500  | author:Antoine Deleforge, Florence Forbes, Sileye Ba, Radu Horaud category:stat.AP cs.CV published:2014-09-30 summary:Hyper-spectral data can be analyzed to recover physical properties at largeplanetary scales. This involves resolving inverse problems which can beaddressed within machine learning, with the advantage that, once a relationshipbetween physical parameters and spectra has been established in a data-drivenfashion, the learned relationship can be used to estimate physical parametersfor new hyper-spectral observations. Within this framework, we propose aspatially-constrained and partially-latent regression method which mapshigh-dimensional inputs (hyper-spectral images) onto low-dimensional responses(physical parameters such as the local chemical composition of the soil). Theproposed regression model comprises two key features. Firstly, it combines aGaussian mixture of locally-linear mappings (GLLiM) with a partially-latentresponse model. While the former makes high-dimensional regression tractable,the latter enables to deal with physical parameters that cannot be observed or,more generally, with data contaminated by experimental artifacts that cannot beexplained with noise models. Secondly, spatial constraints are introduced inthe model through a Markov random field (MRF) prior which provides a spatialstructure to the Gaussian-mixture hidden variables. Experiments conducted on adatabase composed of remotely sensed observations collected from the Marsplanet by the Mars Express orbiter demonstrate the effectiveness of theproposed model.
arxiv-1409-8444 | Douglas-Rachford splitting for nonconvex optimization with application to nonconvex feasibility problems |  http://arxiv.org/abs/1409.8444  | author:Guoyin Li, Ting Kei Pong category:math.OC stat.ML published:2014-09-30 summary:We adapt the Douglas-Rachford (DR) splitting method to solve nonconvexfeasibility problems by studying this method for a class of nonconvexoptimization problem. While the convergence properties of the method for convexproblems have been well studied, far less is known in the nonconvex setting. Inthis paper, for the direct adaptation of the method to minimize the sum of aproper closed function $g$ and a smooth function $f$ with a Lipschitzcontinuous gradient, we show that if the step-size parameter is smaller than acomputable threshold and the sequence generated has a cluster point, then itgives a stationary point of the optimization problem. Convergence of the wholesequence and a local convergence rate are also established under the additionalassumption that $f$ and $g$ are semi-algebraic. We also give simple sufficientconditions guaranteeing the boundedness of the sequence generated. We thenapply our nonconvex DR splitting method to finding a point in the intersectionof a closed convex set $C$ and a general closed set $D$ by minimizing thesquared distance to $C$ subject to $D$. We show that if either set is boundedand the step-size parameter is smaller than a computable threshold, then thesequence generated from the DR splitting method is actually bounded.Consequently, the sequence generated will have cluster points that arestationary for an optimization problem, and the whole sequence is convergentunder an additional assumption that $C$ and $D$ are semi-algebraic. We achievethese results based on a new merit function constructed particularly for the DRsplitting method. Our preliminary numerical results indicate that our DRsplitting method usually outperforms the alternating projection method infinding a sparse solution of a linear system, in terms of both the solutionquality and the number of iterations taken.
arxiv-1409-8581 | Improving the Performance of English-Tamil Statistical Machine Translation System using Source-Side Pre-Processing |  http://arxiv.org/abs/1409.8581  | author:M. Anand Kumar, V. Dhanalakshmi, K. P. Soman, V. Sharmiladevi category:cs.CL published:2014-09-29 summary:Machine Translation is one of the major oldest and the most active researcharea in Natural Language Processing. Currently, Statistical Machine Translation(SMT) dominates the Machine Translation research. Statistical MachineTranslation is an approach to Machine Translation which uses models to learntranslation patterns directly from data, and generalize them to translate a newunseen text. The SMT approach is largely language independent, i.e. the modelscan be applied to any language pair. Statistical Machine Translation (SMT)attempts to generate translations using statistical methods based on bilingualtext corpora. Where such corpora are available, excellent results can beattained translating similar texts, but such corpora are still not availablefor many language pairs. Statistical Machine Translation systems, in general,have difficulty in handling the morphology on the source or the target sideespecially for morphologically rich languages. Errors in morphology or syntaxin the target language can have severe consequences on meaning of the sentence.They change the grammatical function of words or the understanding of thesentence through the incorrect tense information in verb. Baseline SMT alsoknown as Phrase Based Statistical Machine Translation (PBSMT) system does notuse any linguistic information and it only operates on surface word form.Recent researches shown that adding linguistic information helps to improve theaccuracy of the translation with less amount of bilingual corpora. Addinglinguistic information can be done using the Factored Statistical MachineTranslation system through pre-processing steps. This paper investigates abouthow English side pre-processing is used to improve the accuracy ofEnglish-Tamil SMT system.
arxiv-1409-7985 | The Utility of Text: The Case of Amicus Briefs and the Supreme Court |  http://arxiv.org/abs/1409.7985  | author:Yanchuan Sim, Bryan Routledge, Noah A. Smith category:cs.CL cs.AI cs.GT cs.LG published:2014-09-29 summary:We explore the idea that authoring a piece of text is an act of maximizingone's expected utility. To make this idea concrete, we consider the societallyimportant decisions of the Supreme Court of the United States. Extensive pastwork in quantitative political science provides a framework for empiricallymodeling the decisions of justices and how they relate to text. We incorporateinto such a model texts authored by amici curiae ("friends of the court"separate from the litigants) who seek to weigh in on the decision, thenexplicitly model their goals in a random utility model. We demonstrate thebenefits of this approach in improved vote prediction and the ability toperform counterfactual analysis.
arxiv-1409-8008 | CRF-based Named Entity Recognition @ICON 2013 |  http://arxiv.org/abs/1409.8008  | author:Arjun Das, Utpal Garain category:cs.CL published:2014-09-29 summary:This paper describes performance of CRF based systems for Named EntityRecognition (NER) in Indian language as a part of ICON 2013 shared task. Inthis task we have considered a set of language independent features for all thelanguages. Only for English a language specific feature, i.e. capitalization,has been added. Next the use of gazetteer is explored for Bengali, Hindi andEnglish. The gazetteers are built from Wikipedia and other sources. Testresults show that the system achieves the highest F measure of 88% for Englishand the lowest F measure of 69% for both Tamil and Telugu. Note that for theleast performing two languages no gazetteer was used. NER in Bengali and Hindifinds accuracy (F measure) of 87% and 79%, respectively.
arxiv-1409-8276 | A Bayesian Tensor Factorization Model via Variational Inference for Link Prediction |  http://arxiv.org/abs/1409.8276  | author:Beyza Ermis, A. Taylan Cemgil category:cs.LG cs.NA stat.ML published:2014-09-29 summary:Probabilistic approaches for tensor factorization aim to extract meaningfulstructure from incomplete data by postulating low rank constraints. Recently,variational Bayesian (VB) inference techniques have successfully been appliedto large scale models. This paper presents full Bayesian inference via VB onboth single and coupled tensor factorization models. Our method can be run evenfor very large models and is easily implemented. It exhibits better predictionperformance than existing approaches based on maximum likelihood on severalreal-world datasets for missing link prediction problem.
arxiv-1409-8152 | Controversy and Sentiment in Online News |  http://arxiv.org/abs/1409.8152  | author:Yelena Mejova, Amy X. Zhang, Nicholas Diakopoulos, Carlos Castillo category:cs.CY cs.CL published:2014-09-29 summary:How do news sources tackle controversial issues? In this work, we take adata-driven approach to understand how controversy interplays with emotionalexpression and biased language in the news. We begin by introducing a newdataset of controversial and non-controversial terms collected usingcrowdsourcing. Then, focusing on 15 major U.S. news outlets, we comparemillions of articles discussing controversial and non-controversial issues overa span of 7 months. We find that in general, when it comes to controversialissues, the use of negative affect and biased language is prevalent, while theuse of strong emotion is tempered. We also observe many differences across newssources. Using these findings, we show that we can indicate to what extent anissue is controversial, by comparing it with other issues in terms of how theyare portrayed across different media.
arxiv-1409-8185 | Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models |  http://arxiv.org/abs/1409.8185  | author:Theodoros Tsiligkaridis, Keith W. Forsythe category:stat.ML cs.LG stat.ME published:2014-09-29 summary:We develop a sequential low-complexity inference procedure for Dirichletprocess mixtures of Gaussians for online clustering and parameter estimationwhen the number of clusters are unknown a-priori. We present an easilycomputable, closed form parametric expression for the conditional likelihood,in which hyperparameters are recursively updated as a function of the streamingdata assuming conjugate priors. Motivated by large-sample asymptotics, wepropose a novel adaptive low-complexity design for the Dirichlet processconcentration parameter and show that the number of classes grow at most at alogarithmic rate. We further prove that in the large-sample limit, theconditional likelihood and data predictive distribution become asymptoticallyGaussian. We demonstrate through experiments on synthetic and real data setsthat our approach is superior to other online state-of-the-art methods.
arxiv-1409-8572 | Freshness-Aware Thompson Sampling |  http://arxiv.org/abs/1409.8572  | author:Djallel Bouneffouf category:cs.IR cs.LG I.2 published:2014-09-29 summary:To follow the dynamicity of the user's content, researchers have recentlystarted to model interactions between users and the Context-Aware RecommenderSystems (CARS) as a bandit problem where the system needs to deal withexploration and exploitation dilemma. In this sense, we propose to study thefreshness of the user's content in CARS through the bandit problem. Weintroduce in this paper an algorithm named Freshness-Aware Thompson Sampling(FA-TS) that manages the recommendation of fresh document according to theuser's risk of the situation. The intensive evaluation and the detailedanalysis of the experimental results reveals several important discoveries inthe exploration/exploitation (exr/exp) behaviour.
arxiv-1409-8191 | A Neural Networks Committee for the Contextual Bandit Problem |  http://arxiv.org/abs/1409.8191  | author:Robin Allesiardo, Raphael Feraud, Djallel Bouneffouf category:cs.NE cs.LG I.2 published:2014-09-29 summary:This paper presents a new contextual bandit algorithm, NeuralBandit, whichdoes not need hypothesis on stationarity of contexts and rewards. Severalneural networks are trained to modelize the value of rewards knowing thecontext. Two variants, based on multi-experts approach, are proposed to chooseonline the parameters of multi-layer perceptrons. The proposed algorithms aresuccessfully tested on a large dataset with and without stationarity ofrewards.
arxiv-1409-8211 | Efficient multivariate sequence classification |  http://arxiv.org/abs/1409.8211  | author:Pavel P. Kuksa category:cs.LG published:2014-09-29 summary:Kernel-based approaches for sequence classification have been successfullyapplied to a variety of domains, including the text categorization, imageclassification, speech analysis, biological sequence analysis, time series andmusic classification, where they show some of the most accurate results. Typical kernel functions for sequences in these domains (e.g., bag-of-words,mismatch, or subsequence kernels) are restricted to {\em discrete univariate}(i.e. one-dimensional) string data, such as sequences of words in the textanalysis, codeword sequences in the image analysis, or nucleotide or amino acidsequences in the DNA and protein sequence analysis. However, original sequencedata are often of real-valued multivariate nature, i.e. are not univariate anddiscrete as required by typical $k$-mer based sequence kernel functions. In this work, we consider the problem of the {\em multivariate} sequenceclassification such as classification of multivariate music sequences, ormultidimensional protein sequence representations. To this end, we extend {\emunivariate} kernel functions typically used in sequence analysis and proposeefficient {\em multivariate} similarity kernel method (MVDFQ-SK) based on (1) adirect feature quantization (DFQ) of each sequence dimension in the original{\em real-valued} multivariate sequences and (2) applying novel multivariatediscrete kernel measures on these multivariate discrete DFQ sequencerepresentations to more accurately capture similarity relationships amongsequences and improve classification performance. Experiments using the proposed MVDFQ-SK kernel method show excellentclassification performance on three challenging music classification tasks aswell as protein sequence classification with significant 25-40% improvementsover univariate kernel methods and existing state-of-the-art sequenceclassification methods.
arxiv-1409-8230 | RENOIR - A Dataset for Real Low-Light Noise Image Reduction |  http://arxiv.org/abs/1409.8230  | author:Josue Anaya, Adrian Barbu category:cs.CV published:2014-09-29 summary:The application of noise reduction or image denoising is a very importanttopic in the field of computer vision and image processing. Many modern andpopular state of the art image denoising algorithms are trained and evaluatedusing images with added artificial noise. These trained algorithms and theirevaluations on synthetic data may lead to incorrect conclusions about theirperformances on real noise. In this paper we introduce a benchmark dataset ofuncompressed color images corrupted by natural noise due to low-lightconditions, together with spatially and intensity-aligned low noise images ofthe same scenes. The dataset contains over 100 scenes and more than 400 images,including both 16-bit RAW formatted images and 8-bit BMP pixel andintensity-aligned images from 2 digital cameras (Canon S90 and Canon T3i) and amobile phone (Xiaomi Mi3). We also introduce a method for estimating the truenoise level in each of our images, since even the low noise images contain asmall amount of noise. Finally, we exemplify the use of our dataset byevaluating three denoising algorithms: Active Random Field, BM3D, andMulti-Layer Perceptron. We show that while the Multi-Layer Perceptron algorithmworks as well as if not better than BM3D on synthetic data, it does not achievethe same results on our dataset.
arxiv-1409-8202 | Short-Term Predictability of Photovoltaic Production over Italy |  http://arxiv.org/abs/1409.8202  | author:Matteo De Felice, Marcello Petitta, Paolo M. Ruti category:cs.LG stat.AP published:2014-09-29 summary:Photovoltaic (PV) power production increased drastically in Europe throughoutthe last years. About the 6% of electricity in Italy comes from PV and for anefficient management of the power grid an accurate and reliable forecasting ofproduction would be needed. Starting from a dataset of electricity productionof 65 Italian solar plants for the years 2011-2012 we investigate thepossibility to forecast daily production from one to ten days of lead timewithout using on site measurements. Our study is divided in two parts: anassessment of the predictability of meteorological variables using weatherforecasts and an analysis on the application of data-driven modelling inpredicting solar power production. We calibrate a SVM model using availableobservations and then we force the same model with the predicted variables fromweather forecasts with a lead time from one to ten days. As expected, solarpower production is strongly influenced by cloudiness and clear sky, in fact weobserve that while during summer we obtain a general error under the 10%(slightly lower in south Italy), during winter the error is abundantly abovethe 20%.
arxiv-1409-8309 | Arabic Spelling Correction using Supervised Learning |  http://arxiv.org/abs/1409.8309  | author:Youssef Hassan, Mohamed Aly, Amir Atiya category:cs.LG cs.CL published:2014-09-29 summary:In this work, we address the problem of spelling correction in the Arabiclanguage utilizing the new corpus provided by QALB (Qatar Arabic Language Bank)project which is an annotated corpus of sentences with errors and theircorrections. The corpus contains edit, add before, split, merge, add after,move and other error types. We are concerned with the first four error types asthey contribute more than 90% of the spelling errors in the corpus. Theproposed system has many models to address each error type on its own and thenintegrating all the models to provide an efficient and robust system thatachieves an overall recall of 0.59, precision of 0.58 and F1 score of 0.58including all the error types on the development set. Our system participatedin the QALB 2014 shared task "Automatic Arabic Error Correction" and achievedan F1 score of 0.6, earning the sixth place out of nine participants.
arxiv-1409-8327 | Bayesian and regularization approaches to multivariable linear system identification: the role of rank penalties |  http://arxiv.org/abs/1409.8327  | author:Giulia Prando, Alessandro Chiuso, Gianluigi Pillonetto category:cs.SY cs.LG stat.ML published:2014-09-29 summary:Recent developments in linear system identification have proposed the use ofnon-parameteric methods, relying on regularization strategies, to handle theso-called bias/variance trade-off. This paper introduces an impulse responseestimator which relies on an $\ell_2$-type regularization including arank-penalty derived using the log-det heuristic as a smooth approximation tothe rank function. This allows to account for different properties of theestimated impulse response (e.g. smoothness and stability) while alsopenalizing high-complexity models. This also allows to account and enforcecoupling between different input-output channels in MIMO systems. According tothe Bayesian paradigm, the parameters defining the relative weight of the tworegularization terms as well as the structure of the rank penalty are estimatedoptimizing the marginal likelihood. Once these hyperameters have beenestimated, the impulse response estimate is available in closed form.Experiments show that the proposed method is superior to the estimator relyingon the "classic" $\ell_2$-regularization alone as well as those based in atomicand nuclear norm.
arxiv-1409-7938 | Lazier Than Lazy Greedy |  http://arxiv.org/abs/1409.7938  | author:Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrak, Andreas Krause category:cs.LG cs.DS cs.IR published:2014-09-28 summary:Is it possible to maximize a monotone submodular function faster than thewidely used lazy greedy algorithm (also known as accelerated greedy), both intheory and practice? In this paper, we develop the first linear-time algorithmfor maximizing a general monotone submodular function subject to a cardinalityconstraint. We show that our randomized algorithm, STOCHASTIC-GREEDY, canachieve a $(1-1/e-\varepsilon)$ approximation guarantee, in expectation, to theoptimum solution in time linear in the size of the data and independent of thecardinality constraint. We empirically demonstrate the effectiveness of ouralgorithm on submodular functions arising in data summarization, includingtraining large-scale kernel methods, exemplar-based clustering, and sensorplacement. We observe that STOCHASTIC-GREEDY practically achieves the sameutility value as lazy greedy but runs much faster. More surprisingly, weobserve that in many practical scenarios STOCHASTIC-GREEDY does not evaluatethe whole fraction of data points even once and still achievesindistinguishable results compared to lazy greedy.
arxiv-1409-7963 | MoDeep: A Deep Learning Framework Using Motion Features for Human Pose Estimation |  http://arxiv.org/abs/1409.7963  | author:Arjun Jain, Jonathan Tompson, Yann LeCun, Christoph Bregler category:cs.CV cs.LG cs.NE published:2014-09-28 summary:In this work, we propose a novel and efficient method for articulated humanpose estimation in videos using a convolutional network architecture, whichincorporates both color and motion features. We propose a new human body posedataset, FLIC-motion, that extends the FLIC dataset with additional motionfeatures. We apply our architecture to this dataset and report significantlybetter performance than current state-of-the-art pose detection systems.
arxiv-1409-7935 | Combining human and machine learning for morphological analysis of galaxy images |  http://arxiv.org/abs/1409.7935  | author:Evan Kuminski, Joe George, John Wallin, Lior Shamir category:astro-ph.IM astro-ph.GA cs.CV cs.LG published:2014-09-28 summary:The increasing importance of digital sky surveys collecting many millions ofgalaxy images has reinforced the need for robust methods that can performmorphological analysis of large galaxy image databases. Citizen scienceinitiatives such as Galaxy Zoo showed that large datasets of galaxy images canbe analyzed effectively by non-scientist volunteers, but since databasesgenerated by robotic telescopes grow much faster than the processing power ofany group of citizen scientists, it is clear that computer analysis isrequired. Here we propose to use citizen science data for training machinelearning systems, and show experimental results demonstrating that machinelearning systems can be trained with citizen science data. Our findings showthat the performance of machine learning depends on the quality of the data,which can be improved by using samples that have a high degree of agreementbetween the citizen scientists. The source code of the method is publiclyavailable.
arxiv-1409-7930 | Cognitive Learning of Statistical Primary Patterns via Bayesian Network |  http://arxiv.org/abs/1409.7930  | author:Weijia Han, Huiyan Sang, Min Sheng, Jiandong Li, Shuguang Cui category:cs.LG published:2014-09-28 summary:In cognitive radio (CR) technology, the trend of sensing is no longer to onlydetect the presence of active primary users. A large number of applicationsdemand for more comprehensive knowledge on primary user behaviors in spatial,temporal, and frequency domains. To satisfy such requirements, we study thestatistical relationship among primary users by introducing a Bayesian network(BN) based framework. How to learn such a BN structure is a long standingissue, not fully understood even in the statistical learning community.Besides, another key problem in this learning scenario is that the CR has toidentify how many variables are in the BN, which is usually considered as priorknowledge in statistical learning applications. To solve such two issuessimultaneously, this paper proposes a BN structure learning scheme consistingof an efficient structure learning algorithm and a blind variableidentification scheme. The proposed approach incurs significantly lowercomputational complexity compared with previous ones, and is capable ofdetermining the structure without assuming much prior knowledge aboutvariables. With this result, cognitive users could efficiently understand thestatistical pattern of primary networks, such that more efficient cognitiveprotocols could be designed across different network layers.
arxiv-1409-7758 | Combating Corrupt Messages in Sparse Clustered Associative Memories |  http://arxiv.org/abs/1409.7758  | author:Zhe Yao, Vincent Gripon, Michael Rabbat category:cs.NE published:2014-09-27 summary:In this paper we analyze and extend the neural network based associativememory proposed by Gripon and Berrou. This associative memory resembles thecelebrated Willshaw model with an added partite cluster structure. In theliterature, two retrieving schemes have been proposed for the network dynamics,namely sum-of-sum and sum-of-max. They both offer considerably betterperformance than Willshaw and Hopfield networks, when comparable retrievalscenarios are considered. Former discussions and experiments concentrate on theerasure scenario, where a partial message is used as a probe to the network, inthe hope of retrieving the full message. In this regard, sum-of-max outperformssum-of-sum in terms of retrieval rate by a large margin. However, we observethat when noise and errors are present and the network is queried by a corruptprobe, sum-of-max faces a severe limitation as its stringent activation ruleprevents a neuron from reviving back into play once deactivated. In thismanuscript, we categorize and analyze different error scenarios so that boththe erasure and the corrupt scenarios can be treated consistently. We make anamendment to the network structure to improve the retrieval rate, at the costof an extra scalar per neuron. Afterwards, five different approaches areproposed to deal with corrupt probes. As a result, we extend the networkcapability, and also increase the robustness of the retrieving procedure. Wethen experimentally compare all these proposals and discuss pros and cons ofeach approach under different types of errors. Simulation results show that ifcarefully designed, the network is able to preserve both a high retrieval rateand a low running time simultaneously, even when queried by a corrupt probe.
arxiv-1409-7787 | Audio Surveillance: a Systematic Review |  http://arxiv.org/abs/1409.7787  | author:Marco Crocco, Marco Cristani, Andrea Trucco, Vittorio Murino category:cs.SD cs.CV cs.MM published:2014-09-27 summary:Despite surveillance systems are becoming increasingly ubiquitous in ourliving environment, automated surveillance, currently based on video sensorymodality and machine intelligence, lacks most of the time the robustness andreliability required in several real applications. To tackle this issue, audiosensory devices have been taken into account, both alone or in combination withvideo, giving birth, in the last decade, to a considerable amount of research.In this paper audio-based automated surveillance methods are organized into acomprehensive survey: a general taxonomy, inspired by the more widespread videosurveillance field, is proposed in order to systematically describe the methodscovering background subtraction, event classification, object tracking andsituation analysis. For each of these tasks, all the significant works arereviewed, detailing their pros and cons and the context for which they havebeen proposed. Moreover, a specific section is devoted to audio features,discussing their expressiveness and their employment in the above describedtasks. Differently, from other surveys on audio processing and analysis, thepresent one is specifically targeted to automated surveillance, highlightingthe target applications of each described methods and providing the readertables and schemes useful to retrieve the most suited algorithms for a specificrequirement.
arxiv-1409-7780 | Maximum mutual information regularized classification |  http://arxiv.org/abs/1409.7780  | author:Jim Jing-Yan Wang, Yi Wang, Shiguang Zhao, Xin Gao category:cs.LG published:2014-09-27 summary:In this paper, a novel pattern classification approach is proposed byregularizing the classifier learning to maximize mutual information between theclassification response and the true class label. We argue that, with thelearned classifier, the uncertainty of the true class label of a data sampleshould be reduced by knowing its classification response as much as possible.The reduced uncertainty is measured by the mutual information between theclassification response and the true class label. To this end, when learning alinear classifier, we propose to maximize the mutual information betweenclassification responses and true class labels of training samples, besidesminimizing the classification error and reduc- ing the classifier complexity.An objective function is constructed by modeling mutual information withentropy estimation, and it is optimized by a gradi- ent descend method in aniterative algorithm. Experiments on two real world pattern classificationproblems show the significant improvements achieved by maximum mutualinformation regularization.
arxiv-1409-7818 | On The Power of Joint Wavelet-DCT Features for Multispectral Palmprint Recognition |  http://arxiv.org/abs/1409.7818  | author:Shervin Minaee, AmirAli Abdolrashidi category:cs.CV published:2014-09-27 summary:Biometric-based identification has drawn a lot of attention in the recentyears. Among all biometrics, palmprint is known to possess a rich set offeatures. In this paper we have proposed to use DCT-based features in parallelwith wavelet-based ones for palmprint identification. PCA is applied to thefeatures to reduce their dimensionality and the majority voting algorithm isused to perform classification. The features introduced here result in anear-perfectly accurate identification. This method is tested on a well-knownmultispectral palmprint database and an accuracy rate of 99.97-100\% isachieved, outperforming all previous methods in similar conditions.
arxiv-1409-7842 | When Darwin meets Lorenz: Evolving new chaotic attractors through genetic programming |  http://arxiv.org/abs/1409.7842  | author:Indranil Pan, Saptarshi Das category:nlin.CD cs.NE math.DS published:2014-09-27 summary:In this paper, we propose a novel methodology for automatically finding newchaotic attractors through a computational intelligence technique known asmulti-gene genetic programming (MGGP). We apply this technique to the case ofthe Lorenz attractor and evolve several new chaotic attractors based on thebasic Lorenz template. The MGGP algorithm automatically finds new nonlinearexpressions for the different state variables starting from the original Lorenzsystem. The Lyapunov exponents of each of the attractors are calculatednumerically based on the time series of the state variables using time delayembedding techniques. The MGGP algorithm tries to search the functional spaceof the attractors by aiming to maximise the largest Lyapunov exponent (LLE) ofthe evolved attractors. To demonstrate the potential of the proposedmethodology, we report over one hundred new chaotic attractor structures alongwith their parameters, which are evolved from just the Lorenz system alone.
arxiv-1409-7794 | Large-scale Online Feature Selection for Ultra-high Dimensional Sparse Data |  http://arxiv.org/abs/1409.7794  | author:Yue Wu, Steven C. H. Hoi, Tao Mei, Nenghai Yu category:cs.LG cs.CV published:2014-09-27 summary:Feature selection with large-scale high-dimensional data is important yetvery challenging in machine learning and data mining. Online feature selectionis a promising new paradigm that is more efficient and scalable than batchfeature section methods, but the existing online approaches usually fall shortin their inferior efficacy as compared with batch approaches. In this paper, wepresent a novel second-order online feature selection scheme that is simple yeteffective, very fast and extremely scalable to deal with large-scale ultra-highdimensional sparse data streams. The basic idea is to improve the existingfirst-order online feature selection methods by exploiting second-orderinformation for choosing the subset of important features with high confidenceweights. However, unlike many second-order learning methods that often sufferfrom extra high computational cost, we devise a novel smart algorithm forsecond-order online feature selection using a MaxHeap-based approach, which isnot only more effective than the existing first-order approaches, but alsosignificantly more efficient and scalable for large-scale feature selectionwith ultra-high dimensional sparse data, as validated from our extensiveexperiments. Impressively, on a billion-scale synthetic dataset (1-billiondimensions, 1-billion nonzero features, and 1-million samples), our newalgorithm took only 8 minutes on a single PC, which is orders of magnitudesfaster than traditional batch approaches. \url{http://arxiv.org/abs/1409.7794}
arxiv-1409-7474 | Extracting man-made objects from remote sensing images via fast level set evolutions |  http://arxiv.org/abs/1409.7474  | author:Zhongbin Li, Wenzhong Shi, Qunming Wang, Zelang Miao category:cs.CV 68T10, 68T45 published:2014-09-26 summary:Object extraction from remote sensing images has long been an intensiveresearch topic in the field of surveying and mapping. Most existing methods aredevoted to handling just one type of object and little attention has been paidto improving the computational efficiency. In recent years, level set evolution(LSE) has been shown to be very promising for object extraction in thecommunity of image processing and computer vision because it can handletopological changes automatically while achieving high accuracy. However, theapplication of state-of-the-art LSEs is compromised by laborious parametertuning and expensive computation. In this paper, we proposed two fast LSEs forman-made object extraction from high spatial resolution remote sensing images.The traditional mean curvature-based regularization term is replaced by aGaussian kernel and it is mathematically sound to do that. Thus a larger timestep can be used in the numerical scheme to expedite the proposed LSEs. Incontrast to existing methods, the proposed LSEs are significantly faster. Mostimportantly, they involve much fewer parameters while achieving betterperformance. The advantages of the proposed LSEs over other state-of-the-artapproaches have been verified by a range of experiments.
arxiv-1411-4614 | Using graph transformation algorithms to generate natural language equivalents of icons expressing medical concepts |  http://arxiv.org/abs/1411.4614  | author:Pascal Vaillant, Jean-Baptiste Lamy category:cs.CL published:2014-09-26 summary:A graphical language addresses the need to communicate medical information ina synthetic way. Medical concepts are expressed by icons conveying fast visualinformation about patients' current state or about the known effects of drugs.In order to increase the visual language's acceptance and usability, a naturallanguage generation interface is currently developed. In this context, thispaper describes the use of an informatics method ---graph transformation--- toprepare data consisting of concepts in an OWL-DL ontology for use in a naturallanguage generation component. The OWL concept may be considered as astar-shaped graph with a central node. The method transforms it into a graphrepresenting the deep semantic structure of a natural language phrase. Thiswork may be of future use in other contexts where ontology concepts have to bemapped to half-formalized natural language expressions.
arxiv-1409-7461 | Autoencoder Trees |  http://arxiv.org/abs/1409.7461  | author:Ozan İrsoy, Ethem Alpaydın category:cs.LG stat.ML published:2014-09-26 summary:We discuss an autoencoder model in which the encoding and decoding functionsare implemented by decision trees. We use the soft decision tree where internalnodes realize soft multivariate splits given by a gating function and theoverall output is the average of all leaves weighted by the gating values ontheir path. The encoder tree takes the input and generates a lower dimensionalrepresentation in the leaves and the decoder tree takes this and reconstructsthe original input. Exploiting the continuity of the trees, autoencoder treesare trained with stochastic gradient descent. On handwritten digit and newsdata, we see that the autoencoder trees yield good reconstruction errorcompared to traditional autoencoder perceptrons. We also see that theautoencoder tree captures hierarchical representations at differentgranularities of the data on its different levels and the leaves capture thelocalities in the input space.
arxiv-1409-7458 | Beyond Maximum Likelihood: from Theory to Practice |  http://arxiv.org/abs/1409.7458  | author:Jiantao Jiao, Kartik Venkat, Yanjun Han, Tsachy Weissman category:stat.ME cs.DS cs.IT math.IT stat.ML published:2014-09-26 summary:Maximum likelihood is the most widely used statistical estimation technique.Recent work by the authors introduced a general methodology for theconstruction of estimators for functionals in parametric models, anddemonstrated improvements - both in theory and in practice - over the maximumlikelihood estimator (MLE), particularly in high dimensional scenariosinvolving parameter dimension comparable to or larger than the number ofsamples. This approach to estimation, building on results from approximationtheory, is shown to yield minimax rate-optimal estimators for a wide class offunctionals, implementable with modest computational requirements. In anutshell, a message of this recent work is that, for a wide class offunctionals, the performance of these essentially optimal estimators with $n$samples is comparable to that of the MLE with $n \ln n$ samples. In the present paper, we highlight the applicability of the aforementionedmethodology to statistical problems beyond functional estimation, and show thatit can yield substantial gains. For example, we demonstrate that for learningtree-structured graphical models, our approach achieves a significant reductionof the required data size compared with the classical Chow--Liu algorithm,which is an implementation of the MLE, to achieve the same accuracy. The keystep in improving the Chow--Liu algorithm is to replace the empirical mutualinformation with the estimator for mutual information proposed by the authors.Further, applying the same replacement approach to classical Bayesian networkclassification, the resulting classifiers uniformly outperform the previousclassifiers on 26 widely used datasets.
arxiv-1409-7480 | Generalized Twin Gaussian Processes using Sharma-Mittal Divergence |  http://arxiv.org/abs/1409.7480  | author:Mohamed Elhoseiny, Ahmed Elgammal category:cs.LG cs.CV stat.ML published:2014-09-26 summary:There has been a growing interest in mutual information measures due to theirwide range of applications in Machine Learning and Computer Vision. In thispaper, we present a generalized structured regression framework based onShama-Mittal divergence, a relative entropy measure, which is introduced to theMachine Learning community in this work. Sharma-Mittal (SM) divergence is ageneralized mutual information measure for the widely used R\'enyi, Tsallis,Bhattacharyya, and Kullback-Leibler (KL) relative entropies. Specifically, westudy Sharma-Mittal divergence as a cost function in the context of the TwinGaussian Processes (TGP)~\citep{Bo:2010}, which generalizes over theKL-divergence without computational penalty. We show interesting properties ofSharma-Mittal TGP (SMTGP) through a theoretical analysis, which covers missinginsights in the traditional TGP formulation. However, we generalize this theorybased on SM-divergence instead of KL-divergence which is a special case.Experimentally, we evaluated the proposed SMTGP framework on several datasets.The results show that SMTGP reaches better predictions than KL-based TGP, sinceit offers a bigger class of models through its parameters that we learn fromthe data.
arxiv-1409-7450 | Two-stage Geometric Information Guided Image Reconstruction |  http://arxiv.org/abs/1409.7450  | author:Jing Qin, Weihong Guo category:math.OC cs.CV published:2014-09-26 summary:In compressive sensing, it is challenging to reconstruct image of highquality from very few noisy linear projections. Existing methods mostly workwell on piecewise constant images but not so well on piecewise smooth imagessuch as natural images, medical images that contain a lot of details. Wepropose a two-stage method called GeoCS to recover images with rich geometricinformation from very limited amount of noisy measurements. The method adoptsthe shearlet transform that is mathematically proven to be optimal in sparselyrepresenting images containing anisotropic features such as edges, corners,spikes etc. It also uses the weighted total variation (TV) sparsity withspatially variant weights to preserve sharp edges but to reduce the staircaseeffects of TV. Geometric information extracted from the results of stage Iserves as an initial prior for stage II which alternates image reconstructionand geometric information update in a mutually beneficial way. GeoCS has beentested on incomplete spectral Fourier samples. It is applicable to other typesof measurements as well. Experimental results on various complicated imagesshow that GeoCS is efficient and generates high-quality images.
arxiv-1409-7476 | Short-term solar irradiance and irradiation forecasts via different time series techniques: A preliminary study |  http://arxiv.org/abs/1409.7476  | author:Cédric Join, Cyril Voyant, Michel Fliess, Marc Muselli, Marie Laure Nivet, Christophe Paoli, Frédéric Chaxel category:cs.LG physics.ao-ph published:2014-09-26 summary:This communication is devoted to solar irradiance and irradiation short-termforecasts, which are useful for electricity production. Several different timeseries approaches are employed. Our results and the corresponding numericalsimulations show that techniques which do not need a large amount of historicaldata behave better than those which need them, especially when those data arequite noisy.
arxiv-1409-7478 | An Analysis on Selection for High-Resolution Approximations in Many-Objective Optimization |  http://arxiv.org/abs/1409.7478  | author:Hernan Aguirre, Arnaud Liefooghe, Sébastien Verel, Kiyoshi Tanaka category:cs.NE published:2014-09-26 summary:This work studies the behavior of three elitist multi- and many-objectiveevolutionary algorithms generating a high-resolution approximation of thePareto optimal set. Several search-assessment indicators are defined to tracethe dynamics of survival selection and measure the ability to simultaneouslykeep optimal solutions and discover new ones under different population sizes,set as a fraction of the size of the Pareto optimal set.
arxiv-1409-7552 | The Advantage of Cross Entropy over Entropy in Iterative Information Gathering |  http://arxiv.org/abs/1409.7552  | author:Johannes Kulick, Robert Lieck, Marc Toussaint category:stat.ML cs.LG published:2014-09-26 summary:Gathering the most information by picking the least amount of data is acommon task in experimental design or when exploring an unknown environment inreinforcement learning and robotics. A widely used measure for quantifying theinformation contained in some distribution of interest is its entropy. Greedilyminimizing the expected entropy is therefore a standard method for choosingsamples in order to gain strong beliefs about the underlying random variables.We show that this approach is prone to temporally getting stuck in local optimacorresponding to wrongly biased beliefs. We suggest instead maximizing theexpected cross entropy between old and new belief, which aims at challengingrefutable beliefs and thereby avoids these local optima. We show that bothcriteria are closely related and that their difference can be traced back tothe asymmetry of the Kullback-Leibler divergence. In illustrative examples aswell as simulated and real-world experiments we demonstrate the advantage ofcross entropy over simple entropy for practical applications.
arxiv-1409-7672 | Order-invariant prior specification in Bayesian factor analysis |  http://arxiv.org/abs/1409.7672  | author:Dennis Leung, Mathias Drton category:stat.ME stat.ML published:2014-09-26 summary:In (exploratory) factor analysis, the loading matrix is identified only up toorthogonal rotation. For identifiability, one thus often takes the loadingmatrix to be lower triangular with positive diagonal entries. In Bayesianinference, a standard practice is then to specify a prior under which theloadings are independent, the off-diagonal loadings are normally distributed,and the diagonal loadings follow a truncated normal distribution. This priorspecification, however, depends in an important way on how the variables andassociated rows of the loading matrix are ordered. We show how a minormodification of the approach allows one to compute with the identifiable lowertriangular loading matrix but maintain invariance properties under reorderingof the variables.
arxiv-1409-7556 | Location Recognition Over Large Time Lags |  http://arxiv.org/abs/1409.7556  | author:Basura Fernando, Tatiana Tommasi, Tinne Tuytelaars category:cs.CV published:2014-09-26 summary:Would it be possible to automatically associate ancient pictures to modernones and create fancy cultural heritage city maps? We introduce here the taskof recognizing the location depicted in an old photo given modern annotatedimages collected from the Internet. We present an extensive analysis ondifferent features, looking for the most discriminative and most robust to theimage variability induced by large time lags. Moreover, we show that thedescribed task benefits from domain adaptation.
arxiv-1409-7686 | How close are we to understanding image-based saliency? |  http://arxiv.org/abs/1409.7686  | author:Matthias Kümmerer, Thomas Wallis, Matthias Bethge category:cs.CV q-bio.NC stat.AP published:2014-09-26 summary:Within the set of the many complex factors driving gaze placement, theproperities of an image that are associated with fixations under free viewingconditions have been studied extensively. There is a general impression thatthe field is close to understanding this particular association. Here we framesaliency models probabilistically as point processes, allowing the calculationof log-likelihoods and bringing saliency evaluation into the domain ofinformation. We compared the information gain of state-of-the-art models to agold standard and find that only one third of the explainable spatialinformation is captured. We additionally provide a principled method to showwhere and how models fail to capture information in the fixations. Thus,contrary to previous assertions, purely spatial saliency remains a significantchallenge.
arxiv-1409-7495 | Unsupervised Domain Adaptation by Backpropagation |  http://arxiv.org/abs/1409.7495  | author:Yaroslav Ganin, Victor Lempitsky category:stat.ML cs.LG cs.NE published:2014-09-26 summary:Top-performing deep architectures are trained on massive amounts of labeleddata. In the absence of labeled data for a certain task, domain adaptationoften provides an attractive option given that labeled data of similar naturebut from a different domain (e.g. synthetic images) are available. Here, wepropose a new approach to domain adaptation in deep architectures that can betrained on large amount of labeled data from the source domain and large amountof unlabeled data from the target domain (no labeled target-domain data isnecessary). As the training progresses, the approach promotes the emergence of "deep"features that are (i) discriminative for the main learning task on the sourcedomain and (ii) invariant with respect to the shift between the domains. Weshow that this adaptation behaviour can be achieved in almost any feed-forwardmodel by augmenting it with few standard layers and a simple new gradientreversal layer. The resulting augmented architecture can be trained usingstandard backpropagation. Overall, the approach can be implemented with little effort using any of thedeep-learning packages. The method performs very well in a series of imageclassification experiments, achieving adaptation effect in the presence of bigdomain shifts and outperforming previous state-of-the-art on Office datasets.
arxiv-1409-7591 | Topic Similarity Networks: Visual Analytics for Large Document Sets |  http://arxiv.org/abs/1409.7591  | author:Arun S. Maiya, Robert M. Rolfe category:cs.CL cs.HC cs.IR cs.SI stat.ML published:2014-09-26 summary:We investigate ways in which to improve the interpretability of LDA topicmodels by better analyzing and visualizing their outputs. We focus on examiningwhat we refer to as topic similarity networks: graphs in which nodes representlatent topics in text collections and links represent similarity among topics.We describe efficient and effective approaches to both building and labelingsuch networks. Visualizations of topic models based on these networks are shownto be a powerful means of exploring, characterizing, and summarizing largecollections of unstructured text documents. They help to "tease out"non-obvious connections among different sets of documents and provide insightsinto how topics form larger themes. We demonstrate the efficacy andpracticality of these approaches through two case studies: 1) NSF grants forbasic research spanning a 14 year period and 2) the entire English portion ofWikipedia.
arxiv-1409-7489 | Recommending Investors for Crowdfunding Projects |  http://arxiv.org/abs/1409.7489  | author:Jisun An, Daniele Quercia, Jon Crowcroft category:cs.SI cs.CY cs.HC physics.soc-ph stat.ML published:2014-09-26 summary:To bring their innovative ideas to market, those embarking in new ventureshave to raise money, and, to do so, they have often resorted to banks andventure capitalists. Nowadays, they have an additional option: that ofcrowdfunding. The name refers to the idea that funds come from a network ofpeople on the Internet who are passionate about supporting others' projects.One of the most popular crowdfunding sites is Kickstarter. In it, creators postdescriptions of their projects and advertise them on social media sites (mainlyTwitter), while investors look for projects to support. The most common reasonfor project failure is the inability of founders to connect with a sufficientnumber of investors, and that is mainly because hitherto there has not been anyautomatic way of matching creators and investors. We thus set out to proposedifferent ways of recommending investors found on Twitter for specificKickstarter projects. We do so by conducting hypothesis-driven analyses ofpledging behavior and translate the corresponding findings into differentrecommendation strategies. The best strategy achieves, on average, 84% ofaccuracy in predicting a list of potential investors' Twitter accounts for anygiven project. Our findings also produced key insights about the whys andwherefores of investors deciding to support innovative efforts.
arxiv-1409-7618 | Multiple Object Tracking: A Literature Review |  http://arxiv.org/abs/1409.7618  | author:Wenhan Luo, Junliang Xing, Xiaoqin Zhang, Xiaowei Zhao, Tae-Kyun Kim category:cs.CV I.4.8 published:2014-09-26 summary:Multiple Object Tracking is an important computer vision task which hasgained increasing attention due to its academic and commercial potential.Although different approaches have been proposed to tackle it, there stillexist many issues unsolved. In order to help readers understand this topic, wecontribute a systematic and comprehensive review. In the review, we inspectrecent advances in various aspects and propose some interesting directions forfuture research. To our best knowledge, there has not been any review about this topic in thecommunity. We endeavor to provide a thorough review on the development of thisproblem in the last decades. The main contributions are fourfold: 1) Keyaspects in a multiple object tracking system, including how to formulate MOTgenerally, how to categorize MOT algorithms, what needs to be considered whendeveloping a MOT system and how to evaluate a MOT system, are discussed fromthe perspective of understanding a topic. We believe this could not onlyprovide researchers, especially new comers to the topic of MOT, a generalunderstanding of the state of the arts, but also help them to comprehend theessential components of a MOT system and the inter-component connection. 2)Instead of enumerating individual works, we discuss existing work according tothe various aspects involved in a MOT system. In each aspect, methods aredivided into different groups and each group is discussed in details for theprinciples, advances and drawbacks. 3) We examine experiments of existingpublications and give tables which list results on the popular data sets toprovide convenient comparison. We also provide some interesting discoveries byanalyzing these tables. 4) We offer some potential directions and respectivediscussions about MOT, which are still open issues and need more researchefforts. This would be helpful to identify interesting problems.
arxiv-1409-7272 | Ctrax extensions for tracking in difficult lighting conditions |  http://arxiv.org/abs/1409.7272  | author:Ulrich Stern, Chung-Hui Yang category:q-bio.QM cs.CV published:2014-09-25 summary:The fly tracking software Ctrax by Branson et al. is popular for positionaltracking of animals both within and beyond the fly community. Ctrax was notdesigned to handle tracking in difficult lighting conditions with strongshadows or recurring "on"/"off" changes in lighting - a condition that willlikely become increasingly common due to the advent of red-shiftedchannelrhodopsin. We describe Ctrax extensions we developed that address thisproblem. The extensions enabled good tracking accuracy in three types ofdifficult lighting conditions in our lab. Our technique handling shadows relieson "single animal tracking"; the other techniques should be widely applicable.
arxiv-1409-7619 | Generating Conceptual Metaphors from Proposition Stores |  http://arxiv.org/abs/1409.7619  | author:Ekaterina Ovchinnikova, Vladimir Zaytsev, Suzanne Wertheim, Ross Israel category:cs.CL published:2014-09-25 summary:Contemporary research on computational processing of linguistic metaphors isdivided into two main branches: metaphor recognition and metaphorinterpretation. We take a different line of research and present an automatedmethod for generating conceptual metaphors from linguistic data. Given thegenerated conceptual metaphors, we find corresponding linguistic metaphors incorpora. In this paper, we describe our approach and its evaluation usingEnglish and Russian data.
arxiv-1409-7287 | Identification of jump Markov linear models using particle filters |  http://arxiv.org/abs/1409.7287  | author:Andreas Svensson, Thomas B. Schön, Fredrik Lindsten category:stat.CO math.OC stat.ML published:2014-09-25 summary:Jump Markov linear models consists of a finite number of linear state spacemodels and a discrete variable encoding the jumps (or switches) between thedifferent linear models. Identifying jump Markov linear models makes for achallenging problem lacking an analytical solution. We derive a new expectationmaximization (EM) type algorithm that produce maximum likelihood estimates ofthe model parameters. Our development hinges upon recent progress in combiningparticle filters with Markov chain Monte Carlo methods in solving the nonlinearstate smoothing problem inherent in the EM formulation. Key to our developmentis that we exploit a conditionally linear Gaussian substructure in the model,allowing for an efficient algorithm.
arxiv-1409-7165 | Heterogeneous Metric Learning with Content-based Regularization for Software Artifact Retrieval |  http://arxiv.org/abs/1409.7165  | author:Liang Wu, Hui Xiong, Liang Du, Bo Liu, Guandong Xu, Yong Ge, Yanjie Fu, Yuanchun Zhou, Jianhui Li category:cs.LG cs.IR cs.SE published:2014-09-25 summary:The problem of software artifact retrieval has the goal to effectively locatesoftware artifacts, such as a piece of source code, in a large code repository.This problem has been traditionally addressed through the textual query. Inother words, information retrieval techniques will be exploited based on thetextual similarity between queries and textual representation of softwareartifacts, which is generated by collecting words from comments, identifiers,and descriptions of programs. However, in addition to these semanticinformation, there are rich information embedded in source codes themselves.These source codes, if analyzed properly, can be a rich source for enhancingthe efforts of software artifact retrieval. To this end, in this paper, wedevelop a feature extraction method on source codes. Specifically, this methodcan capture both the inherent information in the source codes and the semanticinformation hidden in the comments, descriptions, and identifiers of the sourcecodes. Moreover, we design a heterogeneous metric learning approach, whichallows to integrate code features and text features into the same latentsemantic space. This, in turn, can help to measure the artifact similarity byexploiting the joint power of both code and text features. Finally, extensiveexperiments on real-world data show that the proposed method can help toimprove the performances of software artifact retrieval with a significantmargin.
arxiv-1409-7164 | Deep Learning Representation using Autoencoder for 3D Shape Retrieval |  http://arxiv.org/abs/1409.7164  | author:Zhuotun Zhu, Xinggang Wang, Song Bai, Cong Yao, Xiang Bai category:cs.CV published:2014-09-25 summary:We study the problem of how to build a deep learning representation for 3Dshape. Deep learning has shown to be very effective in variety of visualapplications, such as image classification and object detection. However, ithas not been successfully applied to 3D shape recognition. This is because 3Dshape has complex structure in 3D space and there are limited number of 3Dshapes for feature learning. To address these problems, we project 3D shapesinto 2D space and use autoencoder for feature learning on the 2D images. Highaccuracy 3D shape retrieval performance is obtained by aggregating the featureslearned on 2D images. In addition, we show the proposed deep learning featureis complementary to conventional local image descriptors. By combing the globaldeep learning representation and the local descriptor representation, ourmethod can obtain the state-of-the-art performance on 3D shape retrievalbenchmarks.
arxiv-1409-7612 | Semi-supervised Classification for Natural Language Processing |  http://arxiv.org/abs/1409.7612  | author:Rushdi Shams category:cs.CL cs.LG published:2014-09-25 summary:Semi-supervised classification is an interesting idea where classificationmodels are learned from both labeled and unlabeled data. It has severaladvantages over supervised classification in natural language processingdomain. For instance, supervised classification exploits only labeled data thatare expensive, often difficult to get, inadequate in quantity, and requirehuman experts for annotation. On the other hand, unlabeled data are inexpensiveand abundant. Despite the fact that many factors limit the wide-spread use ofsemi-supervised classification, it has become popular since its level ofperformance is empirically as good as supervised classification. This studyexplores the possibilities and achievements as well as complexity andlimitations of semi-supervised classification for several natural langueprocessing tasks like parsing, biomedical information processing, textclassification, and summarization.
arxiv-1409-7384 | A Semidefinite Programming Based Search Strategy for Feature Selection with Mutual Information Measure |  http://arxiv.org/abs/1409.7384  | author:Tofigh Naghibi, Sarah Hoffmann, Beat Pfister category:cs.LG published:2014-09-25 summary:Feature subset selection, as a special case of the general subset selectionproblem, has been the topic of a considerable number of studies due to thegrowing importance of data-mining applications. In the feature subset selectionproblem there are two main issues that need to be addressed: (i) Finding anappropriate measure function than can be fairly fast and robustly computed forhigh-dimensional data. (ii) A search strategy to optimize the measure over thesubset space in a reasonable amount of time. In this article mutual informationbetween features and class labels is considered to be the measure function. Twoseries expansions for mutual information are proposed, and it is shown thatmost heuristic criteria suggested in the literature are truncatedapproximations of these expansions. It is well-known that searching the wholesubset space is an NP-hard problem. Here, instead of the conventionalsequential search algorithms, we suggest a parallel search strategy based onsemidefinite programming (SDP) that can search through the subset space inpolynomial time. By exploiting the similarities between the proposed algorithmand an instance of the maximum-cut problem in graph theory, the approximationratio of this algorithm is derived and is compared with the approximation ratioof the backward elimination method. The experiments show that it can bemisleading to judge the quality of a measure solely based on the classificationaccuracy, without taking the effect of the non-optimum search strategy intoaccount.
arxiv-1409-7386 | Performance of Stanford and Minipar Parser on Biomedical Texts |  http://arxiv.org/abs/1409.7386  | author:Rushdi Shams category:cs.CL published:2014-09-25 summary:In this paper, the performance of two dependency parsers, namely Stanford andMinipar, on biomedical texts has been reported. The performance of te parsersto assignm dependencies between two biomedical concepts that are already provedto be connected is not satisfying. Both Stanford and Minipar, being statisticalparsers, fail to assign dependency relation between two connected concepts ifthey are distant by at least one clause. Minipar's performance, in terms ofprecision, recall and the F-score of the attachment score (e.g., correctlyidentified head in a dependency), to parse biomedical text is also measuredtaking the Stanford's as a gold standard. The results suggest that Minipar isnot suitable yet to parse biomedical texts. In addition, a qualitativeinvestigation reveals that the difference between working principles of theparsers also play a vital role for Minipar's degraded performance.
arxiv-1409-7275 | The meaning-frequency law in Zipfian optimization models of communication |  http://arxiv.org/abs/1409.7275  | author:Ramon Ferrer-i-Cancho category:cs.CL physics.soc-ph published:2014-09-25 summary:According to Zipf's meaning-frequency law, words that are more frequent tendto have more meanings. Here it is shown that a linear dependency between thefrequency of a form and its number of meanings is found in a family of modelsof Zipf's law for word frequencies. This is evidence for a weak version of themeaning-frequency law. Interestingly, that weak law (a) is not an inevitable ofproperty of the assumptions of the family and (b) is found at least in thenarrow regime where those models exhibit Zipf's law for word frequencies.
arxiv-1409-7307 | Image Classification with A Deep Network Model based on Compressive Sensing |  http://arxiv.org/abs/1409.7307  | author:Yufei Gan, Tong Zhuo, Chu He category:cs.CV published:2014-09-25 summary:To simplify the parameter of the deep learning network, a cascadedcompressive sensing model "CSNet" is implemented for image classification.Firstly, we use cascaded compressive sensing network to learn feature from thedata. Secondly, CSNet generates the feature by binary hashing and block-wisehistograms. Finally, a linear SVM classifier is used to classify thesefeatures. The experiments on the MNIST dataset indicate that higherclassification accuracy can be obtained by this algorithm.
arxiv-1409-7193 | MIST: L0 Sparse Linear Regression with Momentum |  http://arxiv.org/abs/1409.7193  | author:Goran Marjanovic, Magnus O. Ulfarsson, Alfred O. Hero III category:stat.ML published:2014-09-25 summary:Significant attention has been given to minimizing a penalized least squarescriterion for estimating sparse solutions to large linear systems of equations.The penalty is responsible for inducing sparsity and the natural choice is theso-called $l_0$ norm. In this paper we develop a Momentumized IterativeShrinkage Thresholding (MIST) algorithm for minimizing the resulting non-convexcriterion and prove its convergence to a local minimizer. Simulations on largedata sets show superior performance of the proposed method to other methods.
arxiv-1409-7313 | A Deep Graph Embedding Network Model for Face Recognition |  http://arxiv.org/abs/1409.7313  | author:Yufei Gan, Teng Yang, Chu He category:cs.CV published:2014-09-25 summary:In this paper, we propose a new deep learning network "GENet", it combinesthe multi-layer network architec- ture and graph embedding framework. Firstly,we use simplest unsupervised learning PCA/LDA as first layer to generate thelow- level feature. Secondly, many cascaded dimensionality reduction layersbased on graph embedding framework are applied to GENet. Finally, a linear SVMclassifier is used to classify dimension-reduced features. The experimentsindicate that higher classification accuracy can be obtained by this algorithmon the CMU-PIE, ORL, Extended Yale B dataset.
arxiv-1409-7202 | A Boosting Framework on Grounds of Online Learning |  http://arxiv.org/abs/1409.7202  | author:Tofigh Naghibi, Beat Pfister category:cs.LG published:2014-09-25 summary:By exploiting the duality between boosting and online learning, we present aboosting framework which proves to be extremely powerful thanks to employingthe vast knowledge available in the online learning area. Using this framework,we develop various algorithms to address multiple practically and theoreticallyinteresting questions including sparse boosting, smooth-distribution boosting,agnostic learning and some generalization to double-projection online learningalgorithms, as a by-product.
arxiv-1409-7134 | Deconvolution of High-Dimensional Mixtures via Boosting, with Application to Diffusion-Weighted MRI of Human Brain |  http://arxiv.org/abs/1409.7134  | author:Charles Zheng, Franco Pestilli, Ariel Rokem category:stat.ML published:2014-09-25 summary:Diffusion-weighted magnetic resonance imaging (DWI) and fiber tractographyare the only methods to measure the structure of the white matter in the livinghuman brain. The diffusion signal has been modelled as the combinedcontribution from many individual fascicles of nerve fibers passing througheach location in the white matter. Typically, this is done via basis pursuit,but estimation of the exact directions is limited due to discretization. Thedifficulties inherent in modeling DWI data are shared by many other problemsinvolving fitting non-parametric mixture models. Ekanadaham et al. proposed anapproach, continuous basis pursuit, to overcome discretization error in the1-dimensional case (e.g., spike-sorting). Here, we propose a more generalalgorithm that fits mixture models of any dimensionality withoutdiscretization. Our algorithm uses the principles of L2-boost, together withrefitting of the weights and pruning of the parameters. The addition of thesesteps to L2-boost both accelerates the algorithm and assures its accuracy. Werefer to the resulting algorithm as elastic basis pursuit, or EBP, since itexpands and contracts the active set of kernels as needed. We show that incontrast to existing approaches to fitting mixtures, our boosting framework (1)enables the selection of the optimal bias-variance tradeoff along the solutionpath, and (2) scales with high-dimensional problems. In simulations of DWI, wefind that EBP yields better parameter estimates than a non-negative leastsquares (NNLS) approach, or the standard model used in DWI, the tensor model,which serves as the basis for diffusion tensor imaging (DTI). We demonstratethe utility of the method in DWI data acquired in parts of the brain containingcrossings of multiple fascicles of nerve fibers.
arxiv-1409-6813 | Histogram of Oriented Principal Components for Cross-View Action Recognition |  http://arxiv.org/abs/1409.6813  | author:Hossein Rahmani, Arif Mahmood, Du Huynh, Ajmal Mian category:cs.CV published:2014-09-24 summary:Existing techniques for 3D action recognition are sensitive to viewpointvariations because they extract features from depth images which are viewpointdependent. In contrast, we directly process pointclouds for cross-view actionrecognition from unknown and unseen views. We propose the Histogram of OrientedPrincipal Components (HOPC) descriptor that is robust to noise, viewpoint,scale and action speed variations. At a 3D point, HOPC is computed byprojecting the three scaled eigenvectors of the pointcloud within its localspatio-temporal support volume onto the vertices of a regular dodecahedron.HOPC is also used for the detection of Spatio-Temporal Keypoints (STK) in 3Dpointcloud sequences so that view-invariant STK descriptors (or Local HOPCdescriptors) at these key locations only are used for action recognition. Wealso propose a global descriptor computed from the normalized spatio-temporaldistribution of STKs in 4-D, which we refer to as STK-D. We have evaluated theperformance of our proposed descriptors against nine existing techniques on twocross-view and three single-view human action recognition datasets. TheExperimental results show that our techniques provide significant improvementover state-of-the-art methods.
arxiv-1409-6981 | Unsupervised learning of regression mixture models with unknown number of components |  http://arxiv.org/abs/1409.6981  | author:Faicel Chamroukhi category:stat.ME cs.LG stat.ML published:2014-09-24 summary:Regression mixture models are widely studied in statistics, machine learningand data analysis. Fitting regression mixtures is challenging and is usuallyperformed by maximum likelihood by using the expectation-maximization (EM)algorithm. However, it is well-known that the initialization is crucial for EM.If the initialization is inappropriately performed, the EM algorithm may leadto unsatisfactory results. The EM algorithm also requires the number ofclusters to be given a priori; the problem of selecting the number of mixturecomponents requires using model selection criteria to choose one from a set ofpre-estimated candidate models. We propose a new fully unsupervised algorithmto learn regression mixture models with unknown number of components. Thedeveloped unsupervised learning approach consists in a penalized maximumlikelihood estimation carried out by a robust expectation-maximization (EM)algorithm for fitting polynomial, spline and B-spline regressions mixtures. Theproposed learning approach is fully unsupervised: 1) it simultaneously infersthe model parameters and the optimal number of the regression mixturecomponents from the data as the learning proceeds, rather than in a two-foldscheme as in standard model-based clustering using afterward model selectioncriteria, and 2) it does not require accurate initialization unlike thestandard EM for regression mixtures. The developed approach is applied to curveclustering problems. Numerical experiments on simulated data show that theproposed robust EM algorithm performs well and provides accurate results interms of robustness with regard initialization and retrieving the optimalpartition with the actual number of clusters. An application to real data inthe framework of functional data clustering, confirms the benefit of theproposed approach for practical applications.
arxiv-1409-6833 | Quantized Estimation of Gaussian Sequence Models in Euclidean Balls |  http://arxiv.org/abs/1409.6833  | author:Yuancheng Zhu, John Lafferty category:math.ST stat.ML stat.TH published:2014-09-24 summary:A central result in statistical theory is Pinsker's theorem, whichcharacterizes the minimax rate in the normal means model of nonparametricestimation. In this paper, we present an extension to Pinsker's theorem whereestimation is carried out under storage or communication constraints. Inparticular, we place limits on the number of bits used to encode an estimator,and analyze the excess risk in terms of this constraint, the signal size, andthe noise level. We give sharp upper and lower bounds for the case of aEuclidean ball, which establishes the Pareto-optimal minimax tradeoff betweenstorage and risk in this setting.
arxiv-1409-7074 | Variational Pseudolikelihood for Regularized Ising Inference |  http://arxiv.org/abs/1409.7074  | author:Charles K. Fisher category:cs.LG stat.ML published:2014-09-24 summary:I propose a variational approach to maximum pseudolikelihood inference of theIsing model. The variational algorithm is more computationally efficient, anddoes a better job predicting out-of-sample correlations than $L_2$ regularizedmaximum pseudolikelihood inference as well as mean field and isolated spin pairapproximations with pseudocount regularization. The key to the approach is avariational energy that regularizes the inference problem by shrinking thecouplings towards zero, while still allowing some large couplings to explainstrong correlations. The utility of the variational pseudolikelihood approachis illustrated by training an Ising model to represent the letters A-J usingsamples of letters from different computer fonts.
arxiv-1409-6805 | Improving Cross-domain Recommendation through Probabilistic Cluster-level Latent Factor Model--Extended Version |  http://arxiv.org/abs/1409.6805  | author:Siting Ren, Sheng Gao category:cs.IR cs.LG stat.ML published:2014-09-24 summary:Cross-domain recommendation has been proposed to transfer user behaviorpattern by pooling together the rating data from multiple domains to alleviatethe sparsity problem appearing in single rating domains. However, previousmodels only assume that multiple domains share a latent common rating patternbased on the user-item co-clustering. To capture diversities among differentdomains, we propose a novel Probabilistic Cluster-level Latent Factor (PCLF)model to improve the cross-domain recommendation performance. Experiments onseveral real world datasets demonstrate that our proposed model outperforms thestate-of-the-art methods for the cross-domain recommendation task.
arxiv-1409-7085 | Semantically-Informed Syntactic Machine Translation: A Tree-Grafting Approach |  http://arxiv.org/abs/1409.7085  | author:Kathryn Baker, Michael Bloodgood, Chris Callison-Burch, Bonnie J. Dorr, Nathaniel W. Filardo, Lori Levin, Scott Miller, Christine Piatko category:cs.CL cs.LG stat.ML published:2014-09-24 summary:We describe a unified and coherent syntactic framework for supporting asemantically-informed syntactic approach to statistical machine translation.Semantically enriched syntactic tags assigned to the target-language trainingtexts improved translation quality. The resulting system significantlyoutperformed a linguistically naive baseline model (Hiero), and reached thehighest scores yet reported on the NIST 2009 Urdu-English translation task.This finding supports the hypothesis (posed by many researchers in the MTcommunity, e.g., in DARPA GALE) that both syntactic and semantic informationare critical for improving translation quality---and further demonstrates thatlarge gains can be achieved for low-resource languages with different wordorder than English.
arxiv-1409-6838 | Recent Progress in Image Deblurring |  http://arxiv.org/abs/1409.6838  | author:Ruxin Wang, Dacheng Tao category:cs.CV published:2014-09-24 summary:This paper comprehensively reviews the recent development of imagedeblurring, including non-blind/blind, spatially invariant/variant deblurringtechniques. Indeed, these techniques share the same objective of inferring alatent sharp image from one or several corresponding blurry images, while theblind deblurring techniques are also required to derive an accurate blurkernel. Considering the critical role of image restoration in modern imagingsystems to provide high-quality images under complex environments such asmotion, undesirable lighting conditions, and imperfect system components, imagedeblurring has attracted growing attention in recent years. From the viewpointof how to handle the ill-posedness which is a crucial issue in deblurringtasks, existing methods can be grouped into five categories: Bayesian inferenceframework, variational methods, sparse representation-based methods,homography-based modeling, and region-based methods. In spite of achieving acertain level of development, image deblurring, especially the blind case, islimited in its success by complex application conditions which make the blurkernel hard to obtain and be spatially variant. We provide a holisticunderstanding and deep insight into image deblurring in this review. Ananalysis of the empirical evidence for representative methods, practicalissues, as well as a discussion of promising future directions are alsopresented.
arxiv-1409-6911 | Do More Dropouts in Pool5 Feature Maps for Better Object Detection |  http://arxiv.org/abs/1409.6911  | author:Zhiqiang Shen, Xiangyang Xue category:cs.CV published:2014-09-24 summary:Deep Convolutional Neural Networks (CNNs) have gained great success in imageclassification and object detection. In these fields, the outputs of all layersof CNNs are usually considered as a high dimensional feature vector extractedfrom an input image and the correspondence between finer level feature vectorsand concepts that the input image contains is all-important. However, fewerstudies focus on this deserving issue. On considering the correspondence, wepropose a novel approach which generates an edited version for each originalCNN feature vector by applying the maximum entropy principle to abandonparticular vectors. These selected vectors correspond to the unfriendlyconcepts in each image category. The classifier trained from merged featuresets can significantly improve model generalization of individual categorieswhen training data is limited. The experimental results forclassification-based object detection on canonical datasets including VOC 2007(60.1%), 2010 (56.4%) and 2012 (56.3%) show obvious improvement in mean averageprecision (mAP) with simple linear support vector machines.
arxiv-1409-6440 | A non-linear learning & classification algorithm that achieves full training accuracy with stellar classification accuracy |  http://arxiv.org/abs/1409.6440  | author:Rashid Khogali category:cs.CV cs.LG published:2014-09-23 summary:A fast Non-linear and non-iterative learning and classification algorithm issynthesized and validated. This algorithm named the "Reverse RippleEffect(R.R.E)", achieves 100% learning accuracy but is computationallyexpensive upon classification. The R.R.E is a (deterministic) algorithm thatsuper imposes Gaussian weighted functions on training points. In this work, theR.R.E algorithm is compared against known learning and classificationtechniques/algorithms such as: the Perceptron Criterion algorithm, LinearSupport Vector machines, the Linear Fisher Discriminant and a simple NeuralNetwork. The classification accuracy of the R.R.E algorithm is evaluated usingsimulations conducted in MATLAB. The R.R.E algorithm's behaviour is analyzedunder linearly and non-linearly separable data sets. For the comparison withthe Neural Network, the classical XOR problem is considered.
arxiv-1409-6498 | Unified Heat Kernel Regression for Diffusion, Kernel Smoothing and Wavelets on Manifolds and Its Application to Mandible Growth Modeling in CT Images |  http://arxiv.org/abs/1409.6498  | author:Moo K. Chung, Anqi Qiu, Seongho Seo, Houri K. Vorperian category:cs.CV stat.ME published:2014-09-23 summary:We present a novel kernel regression framework for smoothing scalar surfacedata using the Laplace-Beltrami eigenfunctions. Starting with the heat kernelconstructed from the eigenfunctions, we formulate a new bivariate kernelregression framework as a weighted eigenfunction expansion with the heat kernelas the weights. The new kernel regression is mathematically equivalent toisotropic heat diffusion, kernel smoothing and recently popular diffusionwavelets. Unlike many previous partial differential equation based approachesinvolving diffusion, our approach represents the solution of diffusionanalytically, reducing numerical inaccuracy and slow convergence. The numericalimplementation is validated on a unit sphere using spherical harmonics. As anillustration, we have applied the method in characterizing the localized growthpattern of mandible surfaces obtained in CT images from subjects between ages 0and 20 years by regressing the length of displacement vectors with respect tothe template surface.
arxiv-1409-7336 | Does network complexity help organize Babel's library? |  http://arxiv.org/abs/1409.7336  | author:Juan Pablo Cárdenas, Iván González, Gerardo Vidal, Miguel Fuentes category:physics.soc-ph cs.CL nlin.AO published:2014-09-23 summary:In this work, we study properties of texts from the perspective of complexnetwork theory. Words in given texts are linked by co-occurrence andtransformed into networks, and we observe that these display topologicalproperties common to other complex systems. However, there are some propertiesthat seem to be exclusive to texts; many of these properties depend on thefrequency of words in the text, while others seem to be strictly determined bythe grammar. Precisely, these properties allow for a categorization of texts aseither with a sense and others encoded or senseless.
arxiv-1409-6745 | A Concept Learning Approach to Multisensory Object Perception |  http://arxiv.org/abs/1409.6745  | author:Ifeoma Nwogu, Goker Erdogan, Ilker Yildirim, Robert Jacobs category:cs.CV published:2014-09-23 summary:This paper presents a computational model of concept learning using Bayesianinference for a grammatically structured hypothesis space, and test the modelon multisensory (visual and haptics) recognition of 3D objects. The study isperformed on a set of artificially generated 3D objects known as fribbles,which are complex, multipart objects with categorical structures. The goal ofthis work is to develop a working multisensory representational model thatintegrates major themes on concepts and concepts learning from the cognitivescience literature. The model combines the representational power of aprobabilistic generative grammar with the inferential power of Bayesianinduction.
arxiv-1409-6448 | HSR: L1/2 Regularized Sparse Representation for Fast Face Recognition using Hierarchical Feature Selection |  http://arxiv.org/abs/1409.6448  | author:Bo Han, Bo He, Tingting Sun, Mengmeng Ma, Amaury Lendasse category:cs.CV cs.LG published:2014-09-23 summary:In this paper, we propose a novel method for fast face recognition calledL1/2 Regularized Sparse Representation using Hierarchical Feature Selection(HSR). By employing hierarchical feature selection, we can compress the scaleand dimension of global dictionary, which directly contributes to the decreaseof computational cost in sparse representation that our approach is stronglyrooted in. It consists of Gabor wavelets and Extreme Learning MachineAuto-Encoder (ELM-AE) hierarchically. For Gabor wavelets part, local featurescan be extracted at multiple scales and orientations to form Gabor-featurebased image, which in turn improves the recognition rate. Besides, in thepresence of occluded face image, the scale of Gabor-feature based globaldictionary can be compressed accordingly because redundancies exist inGabor-feature based occlusion dictionary. For ELM-AE part, the dimension ofGabor-feature based global dictionary can be compressed becausehigh-dimensional face images can be rapidly represented by low-dimensionalfeature. By introducing L1/2 regularization, our approach can produce sparserand more robust representation compared to regularized Sparse Representationbased Classification (SRC), which also contributes to the decrease of thecomputational cost in sparse representation. In comparison with related worksuch as SRC and Gabor-feature based SRC (GSRC), experimental results on avariety of face databases demonstrate the great advantage of our method forcomputational cost. Moreover, we also achieve approximate or even betterrecognition rate.
arxiv-1409-6086 | Parallel and Distributed Block-Coordinate Frank-Wolfe Algorithms |  http://arxiv.org/abs/1409.6086  | author:Yu-Xiang Wang, Veeranjaneyulu Sadhanala, Wei Dai, Willie Neiswanger, Suvrit Sra, Eric P. Xing category:stat.ML math.OC published:2014-09-22 summary:We develop parallel and distributed Frank-Wolfe algorithms; the former onshared memory machines with mini-batching, and the latter in a delayed updateframework. Whenever possible, we perform computations asynchronously, whichhelps attain speedups on multicore machines as well as in distributedenvironments. Moreover, instead of worst-case bounded delays, our methods onlydepend (mildly) on \emph{expected} delays, allowing them to be robust tostragglers and faulty worker threads. Our algorithms assume block-separableconstraints, and subsume the recent Block-Coordinate Frank-Wolfe (BCFW)method~\citep{lacoste2013block}. Our analysis reveals problem-dependentquantities that govern the speedups of our methods over BCFW. We presentexperiments on structural SVM and Group Fused Lasso, obtaining significantspeedups over competing state-of-the-art (and synchronous) methods.
arxiv-1409-5786 | Fast Low-rank Representation based Spatial Pyramid Matching for Image Classification |  http://arxiv.org/abs/1409.5786  | author:Xi Peng, Rui Yan, Bo Zhao, Huajin Tang, Zhang Yi category:cs.CV published:2014-09-22 summary:Spatial Pyramid Matching (SPM) and its variants have achieved a lot ofsuccess in image classification. The main difference among them is theirencoding schemes. For example, ScSPM incorporates Sparse Code (SC) instead ofVector Quantization (VQ) into the framework of SPM. Although the methodsachieve a higher recognition rate than the traditional SPM, they consume moretime to encode the local descriptors extracted from the image. In this paper,we propose using Low Rank Representation (LRR) to encode the descriptors underthe framework of SPM. Different from SC, LRR considers the group effect amongdata points instead of sparsity. Benefiting from this property, the proposedmethod (i.e., LrrSPM) can offer a better performance. To further improve thegeneralizability and robustness, we reformulate the rank-minimization problemas a truncated projection problem. Extensive experimental studies show thatLrrSPM is more efficient than its counterparts (e.g., ScSPM) while achievingcompetitive recognition rates on nine image data sets.
arxiv-1409-6110 | Best-Arm Identification in Linear Bandits |  http://arxiv.org/abs/1409.6110  | author:Marta Soare, Alessandro Lazaric, Rémi Munos category:cs.LG published:2014-09-22 summary:We study the best-arm identification problem in linear bandit, where therewards of the arms depend linearly on an unknown parameter $\theta^*$ and theobjective is to return the arm with the largest reward. We characterize thecomplexity of the problem and introduce sample allocation strategies that pullarms to identify the best arm with a fixed confidence, while minimizing thesample budget. In particular, we show the importance of exploiting the globallinear structure to improve the estimate of the reward of near-optimal arms. Weanalyze the proposed strategies and compare their empirical performance.Finally, as a by-product of our analysis, we point out the connection to the$G$-optimality criterion used in optimal experimental design.
arxiv-1409-6070 | Spatially-sparse convolutional neural networks |  http://arxiv.org/abs/1409.6070  | author:Benjamin Graham category:cs.CV cs.NE published:2014-09-22 summary:Convolutional neural networks (CNNs) perform well on problems such ashandwriting recognition and image classification. However, the performance ofthe networks is often limited by budget and time constraints, particularly whentrying to train deep networks. Motivated by the problem of online handwriting recognition, we developed aCNN for processing spatially-sparse inputs; a character drawn with a one-pixelwide pen on a high resolution grid looks like a sparse matrix. Taking advantageof the sparsity allowed us more efficiently to train and test large, deep CNNs.On the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a testerror of 3.82%. Although pictures are not sparse, they can be thought of as sparse by addingpadding. Applying a deep convolutional network using sparsity has resulted in asubstantial reduction in test error on the CIFAR small picture datasets: 6.28%on CIFAR-10 and 24.30% for CIFAR-100.
arxiv-1409-6075 | The Information Theoretically Efficient Model (ITEM): A model for computerized analysis of large datasets |  http://arxiv.org/abs/1409.6075  | author:Tyler Ward category:cs.LG published:2014-09-22 summary:This document discusses the Information Theoretically Efficient Model (ITEM),a computerized system to generate an information theoretically efficientmultinomial logistic regression from a general dataset. More specifically, thismodel is designed to succeed even where the logit transform of the dependentvariable is not necessarily linear in the independent variables. This researchshows that for large datasets, the resulting models can be produced on moderncomputers in a tractable amount of time. These models are also resistant tooverfitting, and as such they tend to produce interpretable models with only alimited number of features, all of which are designed to be well behaved.
arxiv-1409-6111 | Distributed Clustering and Learning Over Networks |  http://arxiv.org/abs/1409.6111  | author:Xiaochuan Zhao, Ali H. Sayed category:math.OC cs.LG cs.MA cs.SY stat.ML published:2014-09-22 summary:Distributed processing over networks relies on in-network processing andcooperation among neighboring agents. Cooperation is beneficial when agentsshare a common objective. However, in many applications agents may belong todifferent clusters that pursue different objectives. Then, indiscriminatecooperation will lead to undesired results. In this work, we propose anadaptive clustering and learning scheme that allows agents to learn whichneighbors they should cooperate with and which other neighbors they shouldignore. In doing so, the resulting algorithm enables the agents to identifytheir clusters and to attain improved learning and estimation accuracy overnetworks. We carry out a detailed mean-square analysis and assess the errorprobabilities of Types I and II, i.e., false alarm and mis-detection, for theclustering mechanism. Among other results, we establish that theseprobabilities decay exponentially with the step-sizes so that the probabilityof correct clustering can be made arbitrarily close to one.
arxiv-1409-6179 | Expectation Propagation |  http://arxiv.org/abs/1409.6179  | author:Jack Raymond, Andre Manoel, Manfred Opper category:stat.ML published:2014-09-22 summary:Variational inference is a powerful concept that underlies many iterativeapproximation algorithms; expectation propagation, mean-field methods andbelief propagations were all central themes at the school that can be perceivedfrom this unifying framework. The lectures of Manfred Opper introduce thearchetypal example of Expectation Propagation, before establishing theconnection with the other approximation methods. Corrections by expansion aboutthe expectation propagation are then explained. Finally some advanced inferencetopics and applications are explored in the final sections.
arxiv-1409-6235 | Detecting People in Cubist Art |  http://arxiv.org/abs/1409.6235  | author:Shiry Ginosar, Daniel Haas, Timothy Brown, Jitendra Malik category:cs.CV published:2014-09-22 summary:Although the human visual system is surprisingly robust to extreme distortionwhen recognizing objects, most evaluations of computer object detection methodsfocus only on robustness to natural form deformations such as people's posechanges. To determine whether algorithms truly mirror the flexibility of humanvision, they must be compared against human vision at its limits. For example,in Cubist abstract art, painted objects are distorted by object fragmentationand part-reorganization, to the point that human vision often fails torecognize them. In this paper, we evaluate existing object detection methods onthese abstract renditions of objects, comparing human annotators to fourstate-of-the-art object detectors on a corpus of Picasso paintings. Our resultsdemonstrate that while human perception significantly outperforms currentmethods, human perception and part-based models exhibit a similarly gracefuldegradation in object detection performance as the objects become increasinglyabstract and fragmented, corroborating the theory of part-based objectrepresentation in the brain.
arxiv-1409-6080 | Temporally Coherent Bayesian Models for Entity Discovery in Videos by Tracklet Clustering |  http://arxiv.org/abs/1409.6080  | author:Adway Mitra, Soma Biswas, Chiranjib Bhattacharyya category:cs.CV published:2014-09-22 summary:A video can be represented as a sequence of tracklets, each spanning 10-20frames, and associated with one entity (eg. a person). The task of \emph{EntityDiscovery} in videos can be naturally posed as tracklet clustering. We approachthis task by leveraging \emph{Temporal Coherence}(TC): the fundamental propertyof videos that each tracklet is likely to be associated with the same entity asits temporal neighbors. Our major contributions are the first Bayesiannonparametric models for TC at tracklet-level. We extend Chinese RestaurantProcess (CRP) to propose TC-CRP, and further to Temporally Coherent ChineseRestaurant Franchise (TC-CRF) to jointly model short temporal segments. On thetask of discovering persons in TV serial videos without meta-data like scripts,these methods show considerable improvement in cluster purity and personcoverage compared to state-of-the-art approaches to tracklet clustering. Werepresent entities with mixture components, and tracklets with vectors of verygeneric features, which can work for any type of entity (not necessarilyperson). The proposed methods can perform online tracklet clustering onstreaming videos with little performance deterioration unlike existingapproaches, and can automatically reject tracklets resulting from falsedetections. Finally we discuss entity-driven video summarization- where sometemporal segments of the video are selected automatically based on thediscovered entities.
arxiv-1409-6155 | 1-HKUST: Object Detection in ILSVRC 2014 |  http://arxiv.org/abs/1409.6155  | author:Cewu Lu, Hao Chen, Qifeng Chen, Hei Law, Yao Xiao, Chi-Keung Tang category:cs.CV published:2014-09-22 summary:The Imagenet Large Scale Visual Recognition Challenge (ILSVRC) is the one ofthe most important big data challenges to date. We participated in the objectdetection track of ILSVRC 2014 and received the fourth place among the 38teams. We introduce in our object detection system a number of novel techniquesin localization and recognition. For localization, initial candidate proposalsare generated using selective search, and a novel bounding boxes regressionmethod is used for better object localization. For recognition, to represent acandidate proposal, we adopt three features, namely, RCNN feature, IFV feature,and DPM feature. Given these features, category-specific combination functionsare learned to improve the object recognition rate. In addition, object contextin the form of background priors and object interaction priors are learned andapplied in our system. Our ILSVRC 2014 results are reported alongside with theresults of other participating teams.
arxiv-1409-5957 | A Global Approach for Solving Edge-Matching Puzzles |  http://arxiv.org/abs/1409.5957  | author:Shahar Z. Kovalsky, Daniel Glasner, Ronen Basri category:cs.CV published:2014-09-21 summary:We consider apictorial edge-matching puzzles, in which the goal is to arrangea collection of puzzle pieces with colored edges so that the colors match alongthe edges of adjacent pieces. We devise an algebraic representation for thisproblem and provide conditions under which it exactly characterizes a puzzle.Using the new representation, we recast the combinatorial, discrete problem ofsolving puzzles as a global, polynomial system of equations with continuousvariables. We further propose new algorithms for generating approximatesolutions to the continuous problem by solving a sequence of convexrelaxations.
arxiv-1409-6041 | Domain Adaptive Neural Networks for Object Recognition |  http://arxiv.org/abs/1409.6041  | author:Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang category:cs.CV cs.NE published:2014-09-21 summary:We propose a simple neural network model to deal with the domain adaptationproblem in object recognition. Our model incorporates the Maximum MeanDiscrepancy (MMD) measure as a regularization in the supervised learning toreduce the distribution mismatch between the source and target domains in thelatent space. From experiments, we demonstrate that the MMD regularization isan effective tool to provide good domain adaptation models on both SURFfeatures and raw image pixels of a particular image data set. We also show thatour proposed model, preceded by the denoising auto-encoder pretraining,achieves better performance than recent benchmark models on the same data sets.This work represents the first study of MMD measure in the context of neuralnetworks.
arxiv-1409-6045 | Analyzing sparse dictionaries for online learning with kernels |  http://arxiv.org/abs/1409.6045  | author:Paul Honeine category:stat.ML cs.CV cs.IT cs.LG math.IT published:2014-09-21 summary:Many signal processing and machine learning methods share essentially thesame linear-in-the-parameter model, with as many parameters as availablesamples as in kernel-based machines. Sparse approximation is essential in manydisciplines, with new challenges emerging in online learning with kernels. Tothis end, several sparsity measures have been proposed in the literature toquantify sparse dictionaries and constructing relevant ones, the most prolificones being the distance, the approximation, the coherence and the Babelmeasures. In this paper, we analyze sparse dictionaries based on thesemeasures. By conducting an eigenvalue analysis, we show that these sparsitymeasures share many properties, including the linear independence condition andinducing a well-posed optimization problem. Furthermore, we prove that thereexists a quasi-isometry between the parameter (i.e., dual) space and thedictionary's induced feature space.
arxiv-1409-6023 | A High-Level Model of Neocortical Feedback Based on an Event Window Segmentation Algorithm |  http://arxiv.org/abs/1409.6023  | author:Jerry R. Van Aken category:cs.NE I.2.7 published:2014-09-21 summary:The author previously presented an event window segmentation (EWS) algorithm[5] that uses purely statistical methods to learn to recognize recurringpatterns in an input stream of events. In the following discussion, the EWSalgorithm is first extended to make predictions about future events. Next, thisextended algorithm is used to construct a high-level, simplified model of aneocortical hierarchy. An event stream enters at the bottom of the hierarchy,and drives processing activity upward in the hierarchy. Successively higherregions in the hierarchy learn to recognize successively deeper levels ofpatterns in these events as they propagate from the bottom of the hierarchy.The lower levels in the hierarchy use the predictions from the levels above tostrengthen their own predictions. A C++ source code listing of the modelimplementation and test program is included as an appendix.
arxiv-1409-6046 | Approximation errors of online sparsification criteria |  http://arxiv.org/abs/1409.6046  | author:Paul Honeine category:stat.ML cs.CV cs.IT cs.LG cs.NE math.IT published:2014-09-21 summary:Many machine learning frameworks, such as resource-allocating networks,kernel-based methods, Gaussian processes, and radial-basis-function networks,require a sparsification scheme in order to address the online learningparadigm. For this purpose, several online sparsification criteria have beenproposed to restrict the model definition on a subset of samples. The mostknown criterion is the (linear) approximation criterion, which discards anysample that can be well represented by the already contributing samples, anoperation with excessive computational complexity. Several computationallyefficient sparsification criteria have been introduced in the literature, suchas the distance, the coherence and the Babel criteria. In this paper, weprovide a framework that connects these sparsification criteria to the issue ofapproximating samples, by deriving theoretical bounds on the approximationerrors. Moreover, we investigate the error of approximating any feature, byproposing upper-bounds on the approximation error for each of theaforementioned sparsification criteria. Two classes of features are describedin detail, the empirical mean and the principal axes in the kernel principalcomponent analysis.
arxiv-1409-5937 | Distributed Robust Learning |  http://arxiv.org/abs/1409.5937  | author:Jiashi Feng, Huan Xu, Shie Mannor category:stat.ML cs.LG published:2014-09-21 summary:We propose a framework for distributed robust statistical learning on {\embig contaminated data}. The Distributed Robust Learning (DRL) framework canreduce the computational time of traditional robust learning methods by severalorders of magnitude. We analyze the robustness property of DRL, showing thatDRL not only preserves the robustness of the base robust learning method, butalso tolerates contaminations on a constant fraction of results from computingnodes (node failures). More precisely, even in presence of the most adversarialoutlier distribution over computing nodes, DRL still achieves a breakdown pointof at least $ \lambda^*/2 $, where $ \lambda^* $ is the break down point ofcorresponding centralized algorithm. This is in stark contrast with naivedivision-and-averaging implementation, which may reduce the breakdown point bya factor of $ k $ when $ k $ computing nodes are used. We then specialize theDRL framework for two concrete cases: distributed robust principal componentanalysis and distributed robust regression. We demonstrate the efficiency andthe robustness advantages of DRL through comprehensive simulations andpredicting image tags on a large-scale image set.
arxiv-1409-5887 | Capturing "attrition intensifying" structural traits from didactic interaction sequences of MOOC learners |  http://arxiv.org/abs/1409.5887  | author:Tanmay Sinha, Nan Li, Patrick Jermann, Pierre Dillenbourg category:cs.CY cs.LG cs.SI published:2014-09-20 summary:This work is an attempt to discover hidden structural configurations inlearning activity sequences of students in Massive Open Online Courses (MOOCs).Leveraging combined representations of video clickstream interactions and forumactivities, we seek to fundamentally understand traits that are predictive ofdecreasing engagement over time. Grounded in the interdisciplinary field ofnetwork science, we follow a graph based approach to successfully extractindicators of active and passive MOOC participation that reflect persistenceand regularity in the overall interaction footprint. Using these richeducational semantics, we focus on the problem of predicting student attrition,one of the major highlights of MOOC literature in the recent years. Our resultsindicate an improvement over a baseline ngram based approach in capturing"attrition intensifying" features from the learning activities that MOOClearners engage in. Implications for some compelling future research arediscussed.
arxiv-1409-5705 | Distributed Machine Learning via Sufficient Factor Broadcasting |  http://arxiv.org/abs/1409.5705  | author:Pengtao Xie, Jin Kyu Kim, Yi Zhou, Qirong Ho, Abhimanu Kumar, Yaoliang Yu, Eric Xing category:cs.LG cs.DC published:2014-09-19 summary:Matrix-parametrized models, including multiclass logistic regression andsparse coding, are used in machine learning (ML) applications ranging fromcomputer vision to computational biology. When these models are applied tolarge-scale ML problems starting at millions of samples and tens of thousandsof classes, their parameter matrix can grow at an unexpected rate, resulting inhigh parameter synchronization costs that greatly slow down distributedlearning. To address this issue, we propose a Sufficient Factor Broadcasting(SFB) computation model for efficient distributed learning of a large family ofmatrix-parameterized models, which share the following property: the parameterupdate computed on each data sample is a rank-1 matrix, i.e., the outer productof two "sufficient factors" (SFs). By broadcasting the SFs among workermachines and reconstructing the update matrices locally at each worker, SFBimproves communication efficiency --- communication costs are linear in theparameter matrix's dimensions, rather than quadratic --- without affectingcomputational correctness. We present a theoretical convergence analysis ofSFB, and empirically corroborate its efficiency on four differentmatrix-parametrized ML models.
arxiv-1409-5616 | A Survey on Soft Subspace Clustering |  http://arxiv.org/abs/1409.5616  | author:Zhaohong Deng, Kup-Sze Choi, Yizhang Jiang, Jun Wang, Shitong Wang category:cs.LG published:2014-09-19 summary:Subspace clustering (SC) is a promising clustering technology to identifyclusters based on their associations with subspaces in high dimensional spaces.SC can be classified into hard subspace clustering (HSC) and soft subspaceclustering (SSC). While HSC algorithms have been extensively studied and wellaccepted by the scientific community, SSC algorithms are relatively new butgaining more attention in recent years due to better adaptability. In thepaper, a comprehensive survey on existing SSC algorithms and the recentdevelopment are presented. The SSC algorithms are classified systematicallyinto three main categories, namely, conventional SSC (CSSC), independent SSC(ISSC) and extended SSC (XSSC). The characteristics of these algorithms arehighlighted and the potential future development of SSC is also discussed.
arxiv-1409-5557 | Statistical Estimation: From Denoising to Sparse Regression and Hidden Cliques |  http://arxiv.org/abs/1409.5557  | author:Eric W. Tramel, Santhosh Kumar, Andrei Giurgiu, Andrea Montanari category:cs.IT math.IT stat.ML published:2014-09-19 summary:These notes review six lectures given by Prof. Andrea Montanari on the topicof statistical estimation for linear models. The first two lectures cover theprinciples of signal recovery from linear measurements in terms of minimaxrisk. Subsequent lectures demonstrate the application of these principles toseveral practical problems in science and engineering. Specifically, thesetopics include denoising of error-laden signals, recovery of compressivelysensed signals, reconstruction of low-rank matrices, and also the discovery ofhidden cliques within large networks.
arxiv-1409-5743 | Neural Hypernetwork Approach for Pulmonary Embolism diagnosis |  http://arxiv.org/abs/1409.5743  | author:Matteo Rucco, David M. S. Rodrigues, Emanuela Merelli, Jeffrey H. Johnson, Lorenzo Falsetti, Cinzia Nitti, Aldo Salvi category:physics.med-ph cs.LG q-bio.QM stat.ML published:2014-09-19 summary:This work introduces an integrative approach based on Q-analysis with machinelearning. The new approach, called Neural Hypernetwork, has been applied to acase study of pulmonary embolism diagnosis. The objective of the application ofneural hyper-network to pulmonary embolism (PE) is to improve diagnose forreducing the number of CT-angiography needed. Hypernetworks, based ontopological simplicial complex, generalize the concept of two-relation tomany-body relation. Furthermore, Hypernetworks provide a significantgeneralization of network theory, enabling the integration of relationalstructure, logic and analytic dynamics. Another important results is thatQ-analysis stays close to the data, while other approaches manipulate data,projecting them into metric spaces or applying some filtering functions tohighlight the intrinsic relations. A pulmonary embolism (PE) is a blockage ofthe main artery of the lung or one of its branches, frequently fatal. Our studyuses data on 28 diagnostic features of 1,427 people considered to be at risk ofPE. The resulting neural hypernetwork correctly recognized 94% of thosedeveloping a PE. This is better than previous results that have been obtainedwith other methods (statistical selection of features, partial least squaresregression, topological data analysis in a metric space).
arxiv-1409-5502 | Using crowdsourcing system for creating site-specific statistical machine translation engine |  http://arxiv.org/abs/1409.5502  | author:Alexander Kalinin, George Savchenko category:cs.CL published:2014-09-19 summary:A crowdsourcing translation approach is an effective tool for globalizationof site content, but it is also an important source of parallel linguisticdata. For the given site, processed with a crowdsourcing system, asentence-aligned corpus can be fetched, which covers a very narrow domain ofterminology and language patterns - a site-specific domain. These data can beused for training and estimation of site-specific statistical machinetranslation engine
arxiv-1409-5763 | Active Dictionary Learning in Sparse Representation Based Classification |  http://arxiv.org/abs/1409.5763  | author:Jin Xu, Haibo He, Hong Man category:cs.CV 68T05 published:2014-09-19 summary:Sparse representation, which uses dictionary atoms to reconstruct inputvectors, has been studied intensively in recent years. A proper dictionary is akey for the success of sparse representation. In this paper, an activedictionary learning (ADL) method is introduced, in which classification errorand reconstruction error are considered as the active learning criteria inselection of the atoms for dictionary construction. The learned dictionariesare caculated in sparse representation based classification (SRC). Theclassification accuracy and reconstruction error are used to evaluate theproposed dictionary learning method. The performance of the proposed dictionarylearning method is compared with other methods, including unsuperviseddictionary learning and whole-training-data dictionary. The experimentalresults based on the UCI data sets and face data set demonstrate the efficiencyof the proposed method.
arxiv-1409-5495 | Efficient Feature Group Sequencing for Anytime Linear Prediction |  http://arxiv.org/abs/1409.5495  | author:Hanzhang Hu, Alexander Grubb, J. Andrew Bagnell, Martial Hebert category:cs.LG published:2014-09-19 summary:We propose a regularized linear learning algorithm to sequence groups offeatures, where each group incurs test-time cost or computation. Specifically,we develop a simple extension to Orthogonal Matching Pursuit (OMP) thatrespects the structure of groups of features with variable costs, and we provethat it achieves near-optimal anytime linear prediction at each budgetthreshold where a new group is selected. Our algorithm and analysis extends togeneralized linear models with multi-dimensional responses. We demonstrate thescalability of the resulting approach on large real-world data-sets with manyfeature groups associated with test-time computational costs. Our methodimproves over Group Lasso and Group OMP in the anytime performance of linearpredictions, measured in timeliness, an anytime prediction performance metric,while providing rigorous performance guarantees.
arxiv-1409-5623 | Interactive Visual Exploration of Topic Models using Graphs |  http://arxiv.org/abs/1409.5623  | author:Samuel Rönnqvist, Xiaolu Wang, Peter Sarlin category:cs.IR cs.CL I.5.5 published:2014-09-19 summary:Probabilistic topic modeling is a popular and powerful family of tools foruncovering thematic structure in large sets of unstructured text documents.While much attention has been directed towards the modeling algorithms andtheir various extensions, comparatively few studies have concerned how topresent or visualize topic models in meaningful ways. In this paper, we presenta novel design that uses graphs to visually communicate topic structure andmeaning. By connecting topic nodes via descriptive keyterms, the graphrepresentation reveals topic similarities, topic meaning and shared, ambiguouskeyterms. At the same time, the graph can be used for information retrievalpurposes, to find documents by topic or topic subsets. To exemplify the utilityof the design, we illustrate its use for organizing and exploring corpora offinancial patents.
arxiv-1409-5729 | Hyperspectral and Multispectral Image Fusion based on a Sparse Representation |  http://arxiv.org/abs/1409.5729  | author:Qi Wei, José Bioucas-Dias, Nicolas Dobigeon, Jean-Yves Tourneret category:cs.CV published:2014-09-19 summary:This paper presents a variational based approach to fusing hyperspectral andmultispectral images. The fusion process is formulated as an inverse problemwhose solution is the target image assumed to live in a much lower dimensionalsubspace. A sparse regularization term is carefully designed, relying on adecomposition of the scene on a set of dictionaries. The dictionary atoms andthe corresponding supports of active coding coefficients are learned from theobserved images. Then, conditionally on these dictionaries and supports, thefusion problem is solved via alternating optimization with respect to thetarget image (using the alternating direction method of multipliers) and thecoding coefficients. Simulation results demonstrate the efficiency of theproposed algorithm when compared with the state-of-the-art fusion methods.
arxiv-1409-5834 | Tight Error Bounds for Structured Prediction |  http://arxiv.org/abs/1409.5834  | author:Amir Globerson, Tim Roughgarden, David Sontag, Cafer Yildirim category:cs.LG cs.DS stat.ML published:2014-09-19 summary:Structured prediction tasks in machine learning involve the simultaneousprediction of multiple labels. This is typically done by maximizing a scorefunction on the space of labels, which decomposes as a sum of pairwiseelements, each depending on two specific labels. Intuitively, the more pairwiseterms are used, the better the expected accuracy. However, there is currentlyno theoretical account of this intuition. This paper takes a significant stepin this direction. We formulate the problem as classifying the vertices of a known graph$G=(V,E)$, where the vertices and edges of the graph are labelled and correlatesemi-randomly with the ground truth. We show that the prospects for achievinglow expected Hamming error depend on the structure of the graph $G$ ininteresting ways. For example, if $G$ is a very poor expander, like a path,then large expected Hamming error is inevitable. Our main positive result showsthat, for a wide class of graphs including 2D grid graphs common in machinevision applications, there is a polynomial-time algorithm with small andinformation-theoretically near-optimal expected error. Our results provide afirst step toward a theoretical justification for the empirical success of theefficient approximate inference algorithms that are used for structuredprediction in models where exact inference is intractable.
arxiv-1409-5686 | Transfer Prototype-based Fuzzy Clustering |  http://arxiv.org/abs/1409.5686  | author:Zhaohong Deng, Yizhang Jiang, Fu-Lai Chung, Hisao Ishibuchi, Kup-Sze Choi, Shitong Wang category:cs.LG published:2014-09-19 summary:The traditional prototype based clustering methods, such as the well-knownfuzzy c-mean (FCM) algorithm, usually need sufficient data to find a goodclustering partition. If the available data is limited or scarce, most of theexisting prototype based clustering algorithms will no longer be effective.While the data for the current clustering task may be scarce, there is usuallysome useful knowledge available in the related scenes/domains. In this study,the concept of transfer learning is applied to prototype based fuzzy clustering(PFC). Specifically, the idea of leveraging knowledge from the source domain isexploited to develop a set of transfer prototype based fuzzy clustering (TPFC)algorithms. Three prototype based fuzzy clustering algorithms, namely, FCM,fuzzy k-plane clustering (FKPC) and fuzzy subspace clustering (FSC), have beenchosen to incorporate with knowledge leveraging mechanism to develop thecorresponding transfer clustering algorithms. Novel objective functions areproposed to integrate the knowledge of source domain with the data of targetdomain for clustering in the target domain. The proposed algorithms have beenvalidated on different synthetic and real-world datasets and the resultsdemonstrate their effectiveness when compared with both the original prototypebased fuzzy clustering algorithms and the related clustering algorithms likemulti-task clustering and co-clustering.
arxiv-1409-5403 | Deformable Part Models are Convolutional Neural Networks |  http://arxiv.org/abs/1409.5403  | author:Ross Girshick, Forrest Iandola, Trevor Darrell, Jitendra Malik category:cs.CV published:2014-09-18 summary:Deformable part models (DPMs) and convolutional neural networks (CNNs) aretwo widely used tools for visual recognition. They are typically viewed asdistinct approaches: DPMs are graphical models (Markov random fields), whileCNNs are "black-box" non-linear classifiers. In this paper, we show that a DPMcan be formulated as a CNN, thus providing a novel synthesis of the two ideas.Our construction involves unrolling the DPM inference algorithm and mappingeach step to an equivalent (and at times novel) CNN layer. From thisperspective, it becomes natural to replace the standard image features used inDPM with a learned feature extractor. We call the resulting model DeepPyramidDPM and experimentally validate it on PASCAL VOC. DeepPyramid DPM significantlyoutperforms DPMs based on histograms of oriented gradients features (HOG) andslightly outperforms a comparable version of the recently introduced R-CNNdetection system, while running an order of magnitude faster.
arxiv-1409-5718 | Convolutional Neural Networks over Tree Structures for Programming Language Processing |  http://arxiv.org/abs/1409.5718  | author:Lili Mou, Ge Li, Lu Zhang, Tao Wang, Zhi Jin category:cs.LG cs.NE cs.SE published:2014-09-18 summary:Programming language processing (similar to natural language processing) is ahot research topic in the field of software engineering; it has also arousedgrowing interest in the artificial intelligence community. However, differentfrom a natural language sentence, a program contains rich, explicit, andcomplicated structural information. Hence, traditional NLP models may beinappropriate for programs. In this paper, we propose a novel tree-basedconvolutional neural network (TBCNN) for programming language processing, inwhich a convolution kernel is designed over programs' abstract syntax trees tocapture structural information. TBCNN is a generic architecture for programminglanguage processing; our experiments show its effectiveness in two differentprogram analysis tasks: classifying programs according to functionality, anddetecting code snippets of certain patterns. TBCNN outperforms baselinemethods, including several neural models for NLP.
arxiv-1409-5241 | Subspace Alignment For Domain Adaptation |  http://arxiv.org/abs/1409.5241  | author:Basura Fernando, Amaury Habrard, Marc Sebban, Tinne Tuytelaars category:cs.CV published:2014-09-18 summary:In this paper, we introduce a new domain adaptation (DA) algorithm where thesource and target domains are represented by subspaces spanned by eigenvectors.Our method seeks a domain invariant feature space by learning a mappingfunction which aligns the source subspace with the target one. We show that thesolution of the corresponding optimization problem can be obtained in a simpleclosed form, leading to an extremely fast algorithm. We present two approachesto determine the only hyper-parameter in our method corresponding to the sizeof the subspaces. In the first approach we tune the size of subspaces using atheoretical bound on the stability of the obtained result. In the secondapproach, we use maximum likelihood estimation to determine the subspace size,which is particularly useful for high dimensional data. Apart from PCA, wepropose a subspace creation method that outperform partial least squares (PLS)and linear discriminant analysis (LDA) in domain adaptation. We test our methodon various datasets and show that, despite its intrinsic simplicity, itoutperforms state of the art DA methods.
arxiv-1409-5178 | Model-based Kernel Sum Rule |  http://arxiv.org/abs/1409.5178  | author:Yu Nishiyama, Motonobu Kanagawa, Arthur Gretton, Kenji Fukumizu category:stat.ML stat.ME published:2014-09-18 summary:In this study, we enrich the framework of nonparametric kernel Bayesianinference via the flexible incorporation of certain probabilistic models, suchas additive Gaussian noise models. Nonparametric inference expressed in termsof kernel means, which is called kernel Bayesian inference, has been studiedusing basic rules such as the kernel sum rule (KSR), kernel chain rule, kernelproduct rule, and kernel Bayes' rule (KBR). However, the current framework usedfor kernel Bayesian inference deals only with nonparametric inference and itcannot allow inference when combined with probabilistic models. In this study,we introduce a novel KSR, called model-based KSR (Mb-KSR), which exploits theknowledge obtained from some probabilistic models of conditional distributions.The incorporation of Mb-KSR into nonparametric kernel Bayesian inferencefacilitates more flexible kernel Bayesian inference than nonparametricinference. We focus on combinations of Mb-KSR, Non-KSR, and KBR, and we proposea filtering algorithm for state space models, which combines nonparametriclearning of the observation process using kernel means and additive Gaussiannoise models of the transition dynamics. The idea of the Mb-KSR for additiveGaussian noise models can be extended to more general noise model cases,including a conjugate pair with a positive-definite kernel and a probabilisticmodel.
arxiv-1409-5191 | Hamiltonian Monte Carlo Without Detailed Balance |  http://arxiv.org/abs/1409.5191  | author:Jascha Sohl-Dickstein, Mayur Mudigonda, Michael R. DeWeese category:stat.CO stat.ML published:2014-09-18 summary:We present a method for performing Hamiltonian Monte Carlo that largelyeliminates sample rejection for typical hyperparameters. In situations thatwould normally lead to rejection, instead a longer trajectory is computed untila new state is reached that can be accepted. This is achieved using Markovchain transitions that satisfy the fixed point equation, but do not satisfydetailed balance. The resulting algorithm significantly suppresses the randomwalk behavior and wasted function evaluations that are typically theconsequence of update rejection. We demonstrate a greater than factor of twoimprovement in mixing time on three test problems. We release the source codeas Python and MATLAB packages.
