arxiv-8100-1 | Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods | http://arxiv.org/pdf/1311.2115v7.pdf | author:Jascha Sohl-Dickstein, Ben Poole, Surya Ganguli category:cs.LG 90C26 G.1.6 published:2013-11-09 summary:We present an algorithm for minimizing a sum of functions that combines thecomputational efficiency of stochastic gradient descent (SGD) with the secondorder curvature information leveraged by quasi-Newton methods. We unify thesedisparate approaches by maintaining an independent Hessian approximation foreach contributing function in the sum. We maintain computational tractabilityand limit memory requirements even for high dimensional optimization problemsby storing and manipulating these quadratic approximations in a shared, timeevolving, low dimensional subspace. Each update step requires only a singlecontributing function or minibatch evaluation (as in SGD), and each step isscaled using an approximate inverse Hessian and little to no adjustment ofhyperparameters is required (as is typical for quasi-Newton methods). Thisalgorithm contrasts with earlier stochastic second order techniques that treatthe Hessian of each contributing function as a noisy approximation to the fullHessian, rather than as a target for direct estimation. We experimentallydemonstrate improved convergence on seven diverse optimization problems. Thealgorithm is released as open source Python and MATLAB packages.
arxiv-8100-2 | Constant Step Size Least-Mean-Square: Bias-Variance Trade-offs and Optimal Sampling Distributions | http://arxiv.org/pdf/1412.0156v1.pdf | author:Alexandre Défossez, Francis Bach category:cs.LG math.OC stat.ML published:2014-11-29 summary:We consider the least-squares regression problem and provide a detailedasymptotic analysis of the performance of averaged constant-step-sizestochastic gradient descent (a.k.a. least-mean-squares). In the strongly-convexcase, we provide an asymptotic expansion up to explicit exponentially decayingterms. Our analysis leads to new insights into stochastic approximationalgorithms: (a) it gives a tighter bound on the allowed step-size; (b) thegeneralization error may be divided into a variance term which is decaying asO(1/n), independently of the step-size $\gamma$, and a bias term that decays asO(1/$\gamma$ 2 n 2); (c) when allowing non-uniform sampling, the choice of agood sampling density depends on whether the variance or bias terms dominate.In particular, when the variance term dominates, optimal sampling densities donot lead to much gain, while when the bias term dominates, we can choose largerstep-sizes that leads to significant improvements.
arxiv-8100-3 | Latent semantics of action verbs reflect phonetic parameters of intensity and emotional content | http://arxiv.org/pdf/1405.1359v3.pdf | author:Michael Kai Petersen category:cs.CL 68T50 I.2.4; I.2.7 published:2014-05-06 summary:Conjuring up our thoughts, language reflects statistical patterns of wordco-occurrences which in turn come to describe how we perceive the world.Whether counting how frequently nouns and verbs combine in Google searchqueries, or extracting eigenvectors from term document matrices made up ofWikipedia lines and Shakespeare plots, the resulting latent semantics capturenot only the associative links which form concepts, but also spatial dimensionsembedded within the surface structure of language. As both the shape andmovements of objects have been found to be associated with phonetic contrastsalready in toddlers, this study explores whether articulatory and acousticparameters may likewise differentiate the latent semantics of action verbs.Selecting 3 x 20 emotion, face, and hand related verbs known to activatepremotor areas in the brain, their mutual cosine similarities were computedusing latent semantic analysis LSA, and the resulting adjacency matrices werecompared based on two different large scale text corpora; HAWIK and TASA.Applying hierarchical clustering to identify common structures across the twotext corpora, the verbs largely divide into combined mouth and hand movementsversus emotional expressions. Transforming the verbs into their constituentphonemes, the clustered small and large size movements appear differentiated byfront versus back vowels corresponding to increasing levels of arousal. Whereasthe clustered emotional verbs seem characterized by sequences of close versusopen jaw produced phonemes, generating up- or downwards shifts in formantfrequencies that may influence their perceived valence. Suggesting, that thelatent semantics of action verbs reflect parameters of intensity and emotionalpolarity that appear correlated with the articulatory contrasts and acousticcharacteristics of phonemes
arxiv-8100-4 | Color image quality assessment measure using multivariate generalized Gaussian distribution | http://arxiv.org/pdf/1412.0111v1.pdf | author:Mounir Omari, Abdelkaher Ait Abdelouahad, Mohammed El Hassouni, Hocine Cherifi category:cs.CV published:2014-11-29 summary:This paper deals with color image quality assessment in the reduced-referenceframework based on natural scenes statistics. In this context, we propose tomodel the statistics of the steerable pyramid coefficients by a MultivariateGeneralized Gaussian distribution (MGGD). This model allows taking into accountthe high correlation between the components of the RGB color space. For eachselected scale and orientation, we extract a parameter matrix from the threecolor components subbands. In order to quantify the visual degradation, we usea closed-form of Kullback-Leibler Divergence (KLD) between two MGGDs. Using"TID 2008" benchmark, the proposed measure has been compared with the mostinfluential methods according to the FRTV1 VQEG framework. Results demonstratesits effectiveness for a great variety of distortion type. Among other benefitsthis measure uses only very little information about the original image.
arxiv-8100-5 | Multiple Instance Reinforcement Learning for Efficient Weakly-Supervised Detection in Images | http://arxiv.org/pdf/1412.0100v1.pdf | author:Stefan Mathe, Cristian Sminchisescu category:cs.CV cs.LG published:2014-11-29 summary:State-of-the-art visual recognition and detection systems increasingly relyon large amounts of training data and complex classifiers. Therefore it becomesincreasingly expensive both to manually annotate datasets and to keep runningtimes at levels acceptable for practical applications. In this paper, wepropose two solutions to address these issues. First, we introduce a weaklysupervised, segmentation-based approach to learn accurate detectors and imageclassifiers from weak supervisory signals that provide only approximateconstraints on target localization. We illustrate our system on the problem ofaction detection in static images (Pascal VOC Actions 2012), using human visualsearch patterns as our training signal. Second, inspired from thesaccade-and-fixate operating principle of the human visual system, we usereinforcement learning techniques to train efficient search models fordetection. Our sequential method is weakly supervised and general (it does notrequire eye movements), finds optimal search strategies for any given detectionconfidence function and achieves performance similar to exhaustive slidingwindow search at a fraction of its computational cost.
arxiv-8100-6 | Multi-frame denoising of high speed optical coherence tomography data using inter-frame and intra-frame priors | http://arxiv.org/pdf/1312.1931v2.pdf | author:Liheng Bian, Jinli Suo, Feng Chen, Qionghai Dai category:cs.CV published:2013-12-06 summary:Optical coherence tomography (OCT) is an important interferometric diagnostictechnique which provides cross-sectional views of the subsurface microstructureof biological tissues. However, the imaging quality of high-speed OCT islimited due to the large speckle noise. To address this problem, this paperproposes a multi-frame algorithmic method to denoise OCT volume.Mathematically, we build an optimization model which forces the temporallyregistered frames to be low rank, and the gradient in each frame to be sparse,under logarithmic image formation and noise variance constraints. Besides, aconvex optimization algorithm based on the augmented Lagrangian method isderived to solve the above model. The results reveal that our approachoutperforms the other methods in terms of both speckle noise suppression andcrucial detail preservation.
arxiv-8100-7 | Pedestrian Detection aided by Deep Learning Semantic Tasks | http://arxiv.org/pdf/1412.0069v1.pdf | author:Yonglong Tian, Ping Luo, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2014-11-29 summary:Deep learning methods have achieved great success in pedestrian detection,owing to its ability to learn features from raw pixels. However, they mainlycapture middle-level representations, such as pose of pedestrian, but confusepositive with hard negative samples, which have large ambiguity, e.g. the shapeand appearance of `tree trunk' or `wire pole' are similar to pedestrian incertain viewpoint. This ambiguity can be distinguished by high-levelrepresentation. To this end, this work jointly optimizes pedestrian detectionwith semantic tasks, including pedestrian attributes (e.g. `carrying backpack')and scene attributes (e.g. `road', `tree', and `horizontal'). Rather thanexpensively annotating scene attributes, we transfer attributes informationfrom existing scene segmentation datasets to the pedestrian dataset, byproposing a novel deep model to learn high-level features from multiple tasksand multiple data sources. Since distinct tasks have distinct convergence ratesand data from different datasets have different distributions, a multi-taskobjective function is carefully designed to coordinate tasks and reducediscrepancies among datasets. The importance coefficients of tasks and networkparameters in this objective function can be iteratively estimated. Extensiveevaluations show that the proposed approach outperforms the state-of-the-art onthe challenging Caltech and ETH datasets, where it reduces the miss rates ofprevious deep models by 17 and 5.5 percent, respectively.
arxiv-8100-8 | 3D Hand Pose Detection in Egocentric RGB-D Images | http://arxiv.org/pdf/1412.0065v1.pdf | author:Gregory Rogez, James S. Supancic III, Maryam Khademi, Jose Maria Martinez Montiel, Deva Ramanan category:cs.CV published:2014-11-29 summary:We focus on the task of everyday hand pose estimation from egocentricviewpoints. For this task, we show that depth sensors are particularlyinformative for extracting near-field interactions of the camera wearer withhis/her environment. Despite the recent advances in full-body pose estimationusing Kinect-like sensors, reliable monocular hand pose estimation in RGB-Dimages is still an unsolved problem. The problem is considerably exacerbatedwhen analyzing hands performing daily activities from a first-person viewpoint,due to severe occlusions arising from object manipulations and a limitedfield-of-view. Our system addresses these difficulties by exploiting strongpriors over viewpoint and pose in a discriminative tracking-by-detectionframework. Our priors are operationalized through a photorealistic syntheticmodel of egocentric scenes, which is used to generate training data forlearning depth-based pose classifiers. We evaluate our approach on an annotateddataset of real egocentric object manipulation scenes and compare to bothcommercial and academic approaches. Our method provides state-of-the-artperformance for both hand detection and pose estimation in egocentric RGB-Dimages.
arxiv-8100-9 | A Bayesian Framework for Sparse Representation-Based 3D Human Pose Estimation | http://arxiv.org/pdf/1412.0062v1.pdf | author:Behnam Babagholami-Mohamadabadi, Amin Jourabloo, Ali Zarghami, Shohreh Kasaei category:cs.CV published:2014-11-29 summary:A Bayesian framework for 3D human pose estimation from monocular images basedon sparse representation (SR) is introduced. Our probabilistic approach aims atsimultaneously learning two overcomplete dictionaries (one for the visual inputspace and the other for the pose space) with a shared sparse representation.Existing SR-based pose estimation approaches only offer a point estimation ofthe dictionary and the sparse codes. Therefore, they might be unreliable whenthe number of training examples is small. Our Bayesian framework estimates aposterior distribution for the sparse codes and the dictionaries from labeledtraining data. Hence, it is robust to overfitting on small-size training data.Experimental results on various human activities show that the proposed methodis superior to the state of-the-art pose estimation algorithms.
arxiv-8100-10 | Learning graphical models from the Glauber dynamics | http://arxiv.org/pdf/1410.7659v2.pdf | author:Guy Bresler, David Gamarnik, Devavrat Shah category:cs.LG cs.IT math.IT stat.CO stat.ML published:2014-10-28 summary:In this paper we consider the problem of learning undirected graphical modelsfrom data generated according to the Glauber dynamics. The Glauber dynamics isa Markov chain that sequentially updates individual nodes (variables) in agraphical model and it is frequently used to sample from the stationarydistribution (to which it converges given sufficient time). Additionally, theGlauber dynamics is a natural dynamical model in a variety of settings. Thiswork deviates from the standard formulation of graphical model learning in theliterature, where one assumes access to i.i.d. samples from the distribution. Much of the research on graphical model learning has been directed towardsfinding algorithms with low computational cost. As the main result of thiswork, we establish that the problem of reconstructing binary pairwise graphicalmodels is computationally tractable when we observe the Glauber dynamics.Specifically, we show that a binary pairwise graphical model on $p$ nodes withmaximum degree $d$ can be learned in time $f(d)p^2\log p$, for a function$f(d)$, using nearly the information-theoretic minimum number of samples.
arxiv-8100-11 | Egocentric Pose Recognition in Four Lines of Code | http://arxiv.org/pdf/1412.0060v1.pdf | author:Gregory Rogez, James S. Supancic III, Deva Ramanan category:cs.CV published:2014-11-29 summary:We tackle the problem of estimating the 3D pose of an individual's upperlimbs (arms+hands) from a chest mounted depth-camera. Importantly, we considerpose estimation during everyday interactions with objects. Past work shows thatstrong pose+viewpoint priors and depth-based features are crucial for robustperformance. In egocentric views, hands and arms are observable within a welldefined volume in front of the camera. We call this volume an egocentricworkspace. A notable property is that hand appearance correlates with workspacelocation. To exploit this correlation, we classify arm+hand configurations in aglobal egocentric coordinate frame, rather than a local scanning window. Thisgreatly simplify the architecture and improves performance. We propose anefficient pipeline which 1) generates synthetic workspace exemplars fortraining using a virtual chest-mounted camera whose intrinsic parameters matchour physical camera, 2) computes perspective-aware depth features on thisentire volume and 3) recognizes discrete arm+hand pose classes through a sparsemulti-class SVM. Our method provides state-of-the-art hand pose recognitionperformance from egocentric RGB-D images in real-time.
arxiv-8100-12 | Online Learning and Profit Maximization from Revealed Preferences | http://arxiv.org/pdf/1407.7294v2.pdf | author:Kareem Amin, Rachel Cummings, Lili Dworkin, Michael Kearns, Aaron Roth category:cs.DS cs.GT cs.LG published:2014-07-27 summary:We consider the problem of learning from revealed preferences in an onlinesetting. In our framework, each period a consumer buys an optimal bundle ofgoods from a merchant according to her (linear) utility function and currentprices, subject to a budget constraint. The merchant observes only thepurchased goods, and seeks to adapt prices to optimize his profits. We give anefficient algorithm for the merchant's problem that consists of a learningphase in which the consumer's utility function is (perhaps partially) inferred,followed by a price optimization step. We also consider an alternative onlinelearning algorithm for the setting where prices are set exogenously, but themerchant would still like to predict the bundle that will be bought by theconsumer for purposes of inventory or supply chain management. In contrast withmost prior work on the revealed preferences problem, we demonstrate that bymaking stronger assumptions on the form of utility functions, efficientalgorithms for both learning and profit maximization are possible, even inadaptive, online settings.
arxiv-8100-13 | Bus Travel Time Predictions Using Additive Models | http://arxiv.org/pdf/1411.7973v1.pdf | author:Matthias Kormaksson, Luciano Barbosa, Marcos R. Vieira, Bianca Zadrozny category:cs.LG stat.AP published:2014-11-28 summary:Many factors can affect the predictability of public bus services such astraffic, weather and local events. Other aspects, such as day of week or hourof day, may influence bus travel times as well, either directly or inconjunction with other variables. However, the exact nature of suchrelationships between travel times and predictor variables is, in mostsituations, not known. In this paper we develop a framework that allows forflexible modeling of bus travel times through the use of Additive Models. Inparticular, we model travel times as a sum of linear as well as nonlinear termsthat are modeled as smooth functions of predictor variables. The proposed classof models provides a principled statistical framework that is highly flexiblein terms of model building. The experimental results demonstrate uniformlysuperior performance of our best model as compared to previous predictionmethods when applied to a very large GPS data set obtained from buses operatingin the city of Rio de Janeiro.
arxiv-8100-14 | Effective Face Frontalization in Unconstrained Images | http://arxiv.org/pdf/1411.7964v1.pdf | author:Tal Hassner, Shai Harel, Eran Paz, Roee Enbar category:cs.CV published:2014-11-28 summary:"Frontalization" is the process of synthesizing frontal facing views of facesappearing in single unconstrained photos. Recent reports have suggested thatthis process may substantially boost the performance of face recognitionsystems. This, by transforming the challenging problem of recognizing facesviewed from unconstrained viewpoints to the easier problem of recognizing facesin constrained, forward facing poses. Previous frontalization methods did thisby attempting to approximate 3D facial shapes for each query image. We observethat 3D face shape estimation from unconstrained photos may be a harder problemthan frontalization and can potentially introduce facial misalignments.Instead, we explore the simpler approach of using a single, unmodified, 3Dsurface as an approximation to the shape of all input faces. We show that thisleads to a straightforward, efficient and easy to implement method forfrontalization. More importantly, it produces aesthetic new frontal views andis surprisingly effective when used for face recognition and gender estimation.
arxiv-8100-15 | Adaptive Representations for Tracking Breaking News on Twitter | http://arxiv.org/pdf/1403.2923v3.pdf | author:Igor Brigadir, Derek Greene, Pádraig Cunningham category:cs.IR cs.NE published:2014-03-12 summary:Twitter is often the most up-to-date source for finding and tracking breakingnews stories. Therefore, there is considerable interest in developing filtersfor tweet streams in order to track and summarize stories. This is anon-trivial text analytics task as tweets are short, and standard retrievalmethods often fail as stories evolve over time. In this paper we examine theeffectiveness of adaptive mechanisms for tracking and summarizing breaking newsstories. We evaluate the effectiveness of these mechanisms on a number ofrecent news events for which manually curated timelines are available.Assessments based on ROUGE metrics indicate that an adaptive approaches arebest suited for tracking evolving stories on Twitter.
arxiv-8100-16 | Predicting clicks in online display advertising with latent features and side-information | http://arxiv.org/pdf/1411.7924v1.pdf | author:Bjarne Ørum Fruergaard category:stat.ML cs.LG stat.AP published:2014-11-28 summary:We review a method for click-through rate prediction based on the work ofMenon et al. [11], which combines collaborative filtering and matrixfactorization with a side-information model and fuses the outputs to properprobabilities in [0,1]. In addition we provide details, both for the modelingas well as the experimental part, that are not found elsewhere. We rigorouslytest the performance on several test data sets from consecutive days in aclick-through rate prediction setup, in a manner which reflects a real-worldpipeline. Our results confirm that performance can be increased using latentfeatures, albeit the differences in the measures are small but significant.
arxiv-8100-17 | Learning Face Representation from Scratch | http://arxiv.org/pdf/1411.7923v1.pdf | author:Dong Yi, Zhen Lei, Shengcai Liao, Stan Z. Li category:cs.CV published:2014-11-28 summary:Pushing by big data and deep convolutional neural network (CNN), theperformance of face recognition is becoming comparable to human. Using privatelarge scale training datasets, several groups achieve very high performance onLFW, i.e., 97% to 99%. While there are many open source implementations of CNN,none of large scale face dataset is publicly available. The current situationin the field of face recognition is that data is more important than algorithm.To solve this problem, this paper proposes a semi-automatical way to collectface images from Internet and builds a large scale dataset containing about10,000 subjects and 500,000 images, called CASIAWebFace. Based on the database,we use a 11-layer CNN to learn discriminative representation and obtainstate-of-theart accuracy on LFW and YTF. The publication of CASIAWebFace willattract more research groups entering this field and accelerate the developmentof face recognition in the wild.
arxiv-8100-18 | On Rendering Synthetic Images for Training an Object Detector | http://arxiv.org/pdf/1411.7911v1.pdf | author:Artem Rozantsev, Vincent Lepetit, Pascal Fua category:cs.CV published:2014-11-28 summary:We propose a novel approach to synthesizing images that are effective fortraining object detectors. Starting from a small set of real images, ouralgorithm estimates the rendering parameters required to synthesize similarimages given a coarse 3D model of the target object. These parameters can thenbe reused to generate an unlimited number of training images of the object ofinterest in arbitrary 3D poses, which can then be used to increaseclassification performances. A key insight of our approach is that the synthetically generated imagesshould be similar to real images, not in terms of image quality, but rather interms of features used during the detector training. We show in the context ofdrone, plane, and car detection that using such synthetically generated imagesyields significantly better performances than simply perturbing real images oreven synthesizing images in such way that they look very realistic, as is oftendone when only limited amounts of training data are available.
arxiv-8100-19 | A stronger null hypothesis for crossing dependencies | http://arxiv.org/pdf/1410.5485v2.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL cs.SI physics.soc-ph published:2014-10-20 summary:The syntactic structure of a sentence can be modeled as a tree where verticesare words and edges indicate syntactic dependencies between words. It iswell-known that those edges normally do not cross when drawn over the sentence.Here a new null hypothesis for the number of edge crossings of a sentence ispresented. That null hypothesis takes into account the length of the pair ofedges that may cross and predicts the relative number of crossings in randomtrees with a small error, suggesting that a ban of crossings or a principle ofminimization of crossings are not needed in general to explain the origins ofnon-crossing dependencies. Our work paves the way for more powerful nullhypotheses to investigate the origins of non-crossing dependencies in nature.
arxiv-8100-20 | Efficient inference of overlapping communities in complex networks | http://arxiv.org/pdf/1411.7864v1.pdf | author:Bjarne Ørum Fruergaard, Tue Herlau category:stat.ML cs.SI physics.soc-ph published:2014-11-28 summary:We discuss two views on extending existing methods for complex networkmodeling which we dub the communities first and the networks first view,respectively. Inspired by the networks first view that we attribute to White,Boorman, and Breiger (1976)[1], we formulate the multiple-networks stochasticblockmodel (MNSBM), which seeks to separate the observed network intosubnetworks of different types and where the problem of inferring structure ineach subnetwork becomes easier. We show how this model is specified in agenerative Bayesian framework where parameters can be inferred efficientlyusing Gibbs sampling. The result is an effective multiple-membership modelwithout the drawbacks of introducing complex definitions of "groups" and howthey interact. We demonstrate results on the recovery of planted structure insynthetic networks and show very encouraging results on link predictionperformances using multiple-networks models on a number of real-world networkdata sets.
arxiv-8100-21 | V-variable image compression | http://arxiv.org/pdf/1411.7855v1.pdf | author:Franklin Mendivil, Örjan Stenflo category:cs.CV published:2014-11-28 summary:V-variable fractals, where $V$ is a positive integer, are intuitivelyfractals with at most $V$ different "forms" or "shapes" at all levels ofmagnification. In this paper we describe how V-variable fractals can be usedfor the purpose of image compression.
arxiv-8100-22 | Lazier Than Lazy Greedy | http://arxiv.org/pdf/1409.7938v3.pdf | author:Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrak, Andreas Krause category:cs.LG cs.DS cs.IR published:2014-09-28 summary:Is it possible to maximize a monotone submodular function faster than thewidely used lazy greedy algorithm (also known as accelerated greedy), both intheory and practice? In this paper, we develop the first linear-time algorithmfor maximizing a general monotone submodular function subject to a cardinalityconstraint. We show that our randomized algorithm, STOCHASTIC-GREEDY, canachieve a $(1-1/e-\varepsilon)$ approximation guarantee, in expectation, to theoptimum solution in time linear in the size of the data and independent of thecardinality constraint. We empirically demonstrate the effectiveness of ouralgorithm on submodular functions arising in data summarization, includingtraining large-scale kernel methods, exemplar-based clustering, and sensorplacement. We observe that STOCHASTIC-GREEDY practically achieves the sameutility value as lazy greedy but runs much faster. More surprisingly, weobserve that in many practical scenarios STOCHASTIC-GREEDY does not evaluatethe whole fraction of data points even once and still achievesindistinguishable results compared to lazy greedy.
arxiv-8100-23 | Coarse-grained Cross-lingual Alignment of Comparable Texts with Topic Models and Encyclopedic Knowledge | http://arxiv.org/pdf/1411.7820v1.pdf | author:Vivi Nastase, Angela Fahrni category:cs.CL published:2014-11-28 summary:We present a method for coarse-grained cross-lingual alignment of comparabletexts: segments consisting of contiguous paragraphs that discuss the same theme(e.g. history, economy) are aligned based on induced multilingual topics. Themethod combines three ideas: a two-level LDA model that filters out words thatdo not convey themes, an HMM that models the ordering of themes in thecollection of documents, and language-independent concept annotations to serveas a cross-language bridge and to strengthen the connection between paragraphsin the same segment through concept relations. The method is evaluated onEnglish and French data previously used for monolingual alignment. The resultsshow state-of-the-art performance in both monolingual and cross-lingualsettings.
arxiv-8100-24 | Learning with Algebraic Invariances, and the Invariant Kernel Trick | http://arxiv.org/pdf/1411.7817v1.pdf | author:Franz J. Király, Andreas Ziehe, Klaus-Robert Müller category:stat.ML cs.LG math.ST stat.TH published:2014-11-28 summary:When solving data analysis problems it is important to integrate priorknowledge and/or structural invariances. This paper contributes by a novelframework for incorporating algebraic invariance structure into kernels. Inparticular, we show that algebraic properties such as sign symmetries in data,phase independence, scaling etc. can be included easily by essentiallyperforming the kernel trick twice. We demonstrate the usefulness of our theoryin simulations on selected applications such as sign-invariant spectralclustering and underdetermined ICA.
arxiv-8100-25 | Two Gaussian Approaches to Black-Box Optomization | http://arxiv.org/pdf/1411.7806v1.pdf | author:Lukáš Bajer, Martin Holeňa category:cs.NE cs.AI published:2014-11-28 summary:Outline of several strategies for using Gaussian processes as surrogatemodels for the covariance matrix adaptation evolution strategy (CMA-ES).
arxiv-8100-26 | Cross-Modal Learning via Pairwise Constraints | http://arxiv.org/pdf/1411.7798v1.pdf | author:Ran He, Man Zhang, Liang Wang, Ye Ji, Qiyue Yin category:cs.CV published:2014-11-28 summary:In multimedia applications, the text and image components in a web documentform a pairwise constraint that potentially indicates the same semanticconcept. This paper studies cross-modal learning via the pairwise constraint,and aims to find the common structure hidden in different modalities. We firstpropose a compound regularization framework to deal with the pairwiseconstraint, which can be used as a general platform for developing cross-modalalgorithms. For unsupervised learning, we propose a cross-modal subspaceclustering method to learn a common structure for different modalities. Forsupervised learning, to reduce the semantic gap and the outliers in pairwiseconstraints, we propose a cross-modal matching method based on compound ?21regularization along with an iteratively reweighted algorithm to find theglobal optimum. Extensive experiments demonstrate the benefits of joint textand image modeling with semantically induced pairwise constraints, and showthat the proposed cross-modal methods can further reduce the semantic gapbetween different modalities and improve the clustering/retrieval accuracy.
arxiv-8100-27 | Flying Objects Detection from a Single Moving Camera | http://arxiv.org/pdf/1411.7715v1.pdf | author:Artem Rozantsev, Vincent Lepetit, Pascal Fua category:cs.CV published:2014-11-27 summary:We propose an approach to detect flying objects such as UAVs and aircraftswhen they occupy a small portion of the field of view, possibly moving againstcomplex backgrounds, and are filmed by a camera that itself moves. Solving such a difficult problem requires combining both appearance andmotion cues. To this end we propose a regression-based approach to motionstabilization of local image patches that allows us to achieve effectiveclassification on spatio-temporal image cubes and outperform state-of-the-arttechniques. As the problem is relatively new, we collected two challenging datasets forUAVs and Aircrafts, which can be used as benchmarks for flying objectsdetection and vision-guided collision avoidance.
arxiv-8100-28 | Features in Concert: Discriminative Feature Selection meets Unsupervised Clustering | http://arxiv.org/pdf/1411.7714v1.pdf | author:Marius Leordeanu, Alexandra Radu, Rahul Sukthankar category:cs.CV published:2014-11-27 summary:Feature selection is an essential problem in computer vision, important forcategory learning and recognition. Along with the rapid development of a widevariety of visual features and classifiers, there is a growing need forefficient feature selection and combination methods, to construct powerfulclassifiers for more complex and higher-level recognition tasks. We propose analgorithm that efficiently discovers sparse, compact representations of inputfeatures or classifiers, from a vast sea of candidates, with importantoptimality properties, low computational cost and excellent accuracy inpractice. Different from boosting, we start with a discriminant linearclassification formulation that encourages sparse solutions. Then we obtain anequivalent unsupervised clustering problem that jointly discovers ensembles ofdiverse features. They are independently valuable but even more powerful whenunited in a cluster of classifiers. We evaluate our method on the task oflarge-scale recognition in video and show that it significantly outperformsclassical selection approaches, such as AdaBoost and greedy forward-backwardselection, and powerful classifiers such as SVMs, in speed of training andperformance, especially in the case of limited training data.
arxiv-8100-29 | A Nonparametric Bayesian Approach to Uncovering Rat Hippocampal Population Codes During Spatial Navigation | http://arxiv.org/pdf/1411.7706v1.pdf | author:Scott W. Linderman, Matthew J. Johnson, Matthew A. Wilson, Zhe Chen category:stat.ML q-bio.NC published:2014-11-27 summary:Rodent hippocampal population codes represent important spatial informationabout the environment during navigation. Several computational methods havebeen developed to uncover the neural representation of spatial topologyembedded in rodent hippocampal ensemble spike activity. Here we extend ourprevious work and propose a nonparametric Bayesian approach to infer rathippocampal population codes during spatial navigation. To tackle the modelselection problem, we leverage a nonparametric Bayesian model. Specifically, toanalyze rat hippocampal ensemble spiking activity, we apply a hierarchicalDirichlet process-hidden Markov model (HDP-HMM) using two Bayesian inferencemethods, one based on Markov chain Monte Carlo (MCMC) and the other based onvariational Bayes (VB). We demonstrate the effectiveness of our Bayesianapproaches on recordings from a freely-behaving rat navigating in an open fieldenvironment. We find that MCMC-based inference with Hamiltonian Monte Carlo(HMC) hyperparameter sampling is flexible and efficient, and outperforms VB andMCMC approaches with hyperparameters set by empirical Bayes.
arxiv-8100-30 | On color image quality assessment using natural image statistics | http://arxiv.org/pdf/1411.7682v1.pdf | author:Mounir Omari, Mohammed El Hassouni, Hocine Cherifi, Abdelkaher Ait Abdelouahad category:cs.CV published:2014-11-27 summary:Color distortion can introduce a significant damage in visual qualityperception, however, most of existing reduced-reference quality measures aredesigned for grayscale images. In this paper, we consider a basic extension ofwell-known image-statistics based quality assessment measures to color images.In order to evaluate the impact of color information on the measuresefficiency, two color spaces are investigated: RGB and CIELAB. Results of anextensive evaluation using TID 2013 benchmark demonstrates that significantimprovement can be achieved for a great number of distortion type when theCIELAB color representation is used.
arxiv-8100-31 | Complexity of Equivalence and Learning for Multiplicity Tree Automata | http://arxiv.org/pdf/1405.0514v2.pdf | author:Ines Marusic, James Worrell category:cs.LG cs.FL published:2014-05-02 summary:We consider the complexity of equivalence and learning for multiplicity treeautomata, i.e., weighted tree automata over a field. We first show that theequivalence problem is logspace equivalent to polynomial identity testing, thecomplexity of which is a longstanding open problem. Secondly, we derive lowerbounds on the number of queries needed to learn multiplicity tree automata inAngluin's exact learning model, over both arbitrary and fixed fields. Habrard and Oncina (2006) give an exact learning algorithm for multiplicitytree automata, in which the number of queries is proportional to the size ofthe target automaton and the size of a largest counterexample, represented as atree, that is returned by the Teacher. However, the smallesttree-counterexample may be exponential in the size of the target automaton.Thus the above algorithm does not run in time polynomial in the size of thetarget automaton, and has query complexity exponential in the lower bound. Assuming a Teacher that returns minimal DAG representations ofcounterexamples, we give a new exact learning algorithm whose query complexityis quadratic in the target automaton size, almost matching the lower bound, andimproving the best previously-known algorithm by an exponential factor.
arxiv-8100-32 | A statistical reduced-reference method for color image quality assessment | http://arxiv.org/pdf/1411.7655v1.pdf | author:Mounir Omari, Mohammed El Hassouni, Abdelkaher Ait Abdelouahad, Hocine Cherifi category:cs.CV published:2014-11-27 summary:Although color is a fundamental feature of human visual perception, it hasbeen largely unexplored in the reduced-reference (RR) image quality assessment(IQA) schemes. In this paper, we propose a natural scene statistic (NSS)method, which efficiently uses this information. It is based on the statisticaldeviation between the steerable pyramid coefficients of the reference colorimage and the degraded one. We propose and analyze the multivariate generalizedGaussian distribution (MGGD) to model the underlying statistics. In order toquantify the degradation, we develop and evaluate two measures basedrespectively on the Geodesic distance between two MGGDs and on the closed-formof the Kullback Leibler divergence. We performed an extensive evaluation ofboth metrics in various color spaces (RGB, HSV, CIELAB and YCrCb) using the TID2008 benchmark and the FRTV Phase I validation process. Experimental resultsdemonstrate the effectiveness of the proposed framework to achieve a goodconsistency with human visual perception. Furthermore, the best configurationis obtained with CIELAB color space associated to KLD deviation measure.
arxiv-8100-33 | The Poisson transform for unnormalised statistical models | http://arxiv.org/pdf/1406.2839v2.pdf | author:Simon Barthelmé, Nicolas Chopin category:stat.CO stat.ML 62F99 G.3 published:2014-06-11 summary:Contrary to standard statistical models, unnormalised statistical models onlyspecify the likelihood function up to a constant. While such models are naturaland popular, the lack of normalisation makes inference much more difficult.Here we show that inferring the parameters of a unnormalised model on a space$\Omega$ can be mapped onto an equivalent problem of estimating the intensityof a Poisson point process on $\Omega$. The unnormalised statistical model nowspecifies an intensity function that does not need to be normalised.Effectively, the normalisation constant may now be inferred as just anotherparameter, at no loss of information. The result can be extended to covernon-IID models, which includes for example unnormalised models for sequences ofgraphs (dynamical graphs), or for sequences of binary vectors. As aconsequence, we prove that unnormalised parameteric inference in non-IID modelscan be turned into a semi-parametric estimation problem. Moreover, we show thatthe noise-contrastive divergence of Gutmann & Hyv\"arinen (2012) can beunderstood as an approximation of the Poisson transform, and extended tonon-IID settings. We use our results to fit spatial Markov chain models of eyemovements, where the Poisson transform allows us to turn a highly non-standardmodel into vanilla semi-parametric logistic regression.
arxiv-8100-34 | A Parallel Genetic Algorithm for Generalized Vertex Cover Problem | http://arxiv.org/pdf/1411.7612v1.pdf | author:Drona Pratap Chandu category:cs.DC cs.NE published:2014-11-27 summary:This paper presents a parallel genetic algorithm for generalised vertex coverproblem (GVCP) using Hadoop Map-Reduce framework. The proposed Map-Reduceimplementation helps to run the genetic algorithm for generalized vertex coverproblem (GVCP) on multiple machines parallely and computes the solution inrelatively short time.
arxiv-8100-35 | Graph Sensitive Indices for Comparing Clusterings | http://arxiv.org/pdf/1411.7582v1.pdf | author:Zaeem Hussain, Marina Meila category:cs.LG published:2014-11-27 summary:This report discusses two new indices for comparing clusterings of a set ofpoints. The motivation for looking at new ways for comparing clusterings stemsfrom the fact that the existing clustering indices are based on set cardinalityalone and do not consider the positions of data points. The new indices,namely, the Random Walk index (RWI) and Variation of Information with Neighbors(VIN), are both inspired by the clustering metric Variation of Information(VI). VI possesses some interesting theoretical properties which are alsodesirable in a metric for comparing clusterings. We define our indices anddiscuss some of their explored properties which appear relevant for aclustering index. We also include the results of these indices on clusteringsof some example data sets.
arxiv-8100-36 | Interactive Visual Exploration of Topic Models using Graphs | http://arxiv.org/pdf/1409.5623v2.pdf | author:Samuel Rönnqvist, Xiaolu Wang, Peter Sarlin category:cs.IR cs.CL I.5.5 published:2014-09-19 summary:Probabilistic topic modeling is a popular and powerful family of tools foruncovering thematic structure in large sets of unstructured text documents.While much attention has been directed towards the modeling algorithms andtheir various extensions, comparatively few studies have concerned how topresent or visualize topic models in meaningful ways. In this paper, we presenta novel design that uses graphs to visually communicate topic structure andmeaning. By connecting topic nodes via descriptive keyterms, the graphrepresentation reveals topic similarities, topic meaning and shared, ambiguouskeyterms. At the same time, the graph can be used for information retrievalpurposes, to find documents by topic or topic subsets. To exemplify the utilityof the design, we illustrate its use for organizing and exploring corpora offinancial patents.
arxiv-8100-37 | Matrix Completion on Graphs | http://arxiv.org/pdf/1408.1717v3.pdf | author:Vassilis Kalofolias, Xavier Bresson, Michael Bronstein, Pierre Vandergheynst category:cs.LG stat.ML published:2014-08-07 summary:The problem of finding the missing values of a matrix given a few of itsentries, called matrix completion, has gathered a lot of attention in therecent years. Although the problem under the standard low rank assumption isNP-hard, Cand\`es and Recht showed that it can be exactly relaxed if the numberof observed entries is sufficiently large. In this work, we introduce a novelmatrix completion model that makes use of proximity information about rows andcolumns by assuming they form communities. This assumption makes sense inseveral real-world problems like in recommender systems, where there arecommunities of people sharing preferences, while products form clusters thatreceive similar ratings. Our main goal is thus to find a low-rank solution thatis structured by the proximities of rows and columns encoded by graphs. Weborrow ideas from manifold learning to constrain our solution to be smooth onthese graphs, in order to implicitly force row and column proximities. Ourmatrix recovery model is formulated as a convex non-smooth optimizationproblem, for which a well-posed iterative scheme is provided. We study andevaluate the proposed matrix completion on synthetic and real data, showingthat the proposed structured low-rank recovery model outperforms the standardmatrix completion model in many situations.
arxiv-8100-38 | Scalability of using Restricted Boltzmann Machines for Combinatorial Optimization | http://arxiv.org/pdf/1411.7542v1.pdf | author:Malte Probst, Franz Rothlauf, Jörn Grahl category:cs.NE I.2.6; I.2.8 published:2014-11-27 summary:Estimation of Distribution Algorithms (EDAs) require flexible probabilitymodels that can be efficiently learned and sampled. Restricted BoltzmannMachines (RBMs) are generative neural networks with these desired properties.We integrate an RBM into an EDA and evaluate the performance of this system insolving combinatorial optimization problems with a single objective. We assesshow the number of fitness evaluations and the CPU time scale with problem sizeand with problem complexity. The results are compared to the BayesianOptimization Algorithm, a state-of-the-art EDA. Although RBM-EDA requireslarger population sizes and a larger number of fitness evaluations, itoutperforms BOA in terms of CPU times, in particular if the problem is large orcomplex. RBM-EDA requires less time for model building than BOA. These resultshighlight the potential of using generative neural networks for combinatorialoptimization.
arxiv-8100-39 | Forecasting the Colorado River Discharge Using an Artificial Neural Network (ANN) Approach | http://arxiv.org/pdf/1411.7508v1.pdf | author:Amirhossein Mehrkesh, Maryam Ahmadi category:stat.ML physics.soc-ph published:2014-11-27 summary:Artificial Neural Network (ANN) based model is a computational approachcommonly used for modeling the complex relationships between input and outputparameters. Prediction of the flow rate of a river is a requisite for anysuccessful water resource management and river basin planning. In the currentsurvey, the effectiveness of an Artificial Neural Network was examined topredict the Colorado River discharge. In this modeling process, an ANN modelwas used to relate the discharge of the Colorado River to such parameters asthe amount of precipitation, ambient temperature and snowpack level at aspecific time of the year. The model was able to precisely study the impact ofclimatic parameters on the flow rate of the Colorado River.
arxiv-8100-40 | The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classification | http://arxiv.org/pdf/1411.7466v1.pdf | author:Lingqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV published:2014-11-27 summary:A number of recent studies have shown that a Deep Convolutional NeuralNetwork (DCNN) pretrained on a large dataset can be adopted as a universalimage description which leads to astounding performance in many visualclassification tasks. Most of these studies, if not all, adopt activations ofthe fully-connected layer of a DCNN as the image or region representation andit is believed that convolutional layer activations are less discriminative. This paper, however, advocates that if used appropriately convolutional layeractivations can be turned into a powerful image representation which enjoysmany advantages over fully-connected layer activations. This is achieved byadopting a new technique proposed in this paper calledcross-convolutional-layer pooling. More specifically, it extracts subarrays offeature maps of one convolutional layer as local features and pools theextracted features with the guidance of feature maps of the successiveconvolutional layer. Compared with exising methods that apply DCNNs in thelocal feature setting, the proposed method is significantly faster since itrequires much fewer times of DCNN forward computation. Moreover, it avoids thedomain mismatch issue which is usually encountered when applying fullyconnected layer activations to describe local regions. By applying our methodto four popular visual classification tasks, it is demonstrated that theproposed method can achieve comparable or in some cases significantly betterperformance than existing fully-connected layer based image representationswhile incurring much lower computational cost.
arxiv-8100-41 | Worst-Case Linear Discriminant Analysis as Scalable Semidefinite Feasibility Problems | http://arxiv.org/pdf/1411.7450v1.pdf | author:Hui Li, Chunhua Shen, Anton van den Hengel, Qinfeng Shi category:cs.LG published:2014-11-27 summary:In this paper, we propose an efficient semidefinite programming (SDP)approach to worst-case linear discriminant analysis (WLDA). Compared with thetraditional LDA, WLDA considers the dimensionality reduction problem from theworst-case viewpoint, which is in general more robust for classification.However, the original problem of WLDA is non-convex and difficult to optimize.In this paper, we reformulate the optimization problem of WLDA into a sequenceof semidefinite feasibility problems. To efficiently solve the semidefinitefeasibility problems, we design a new scalable optimization method withquasi-Newton methods and eigen-decomposition being the core components. Theproposed method is orders of magnitude faster than standard interior-pointbased SDP solvers. Experiments on a variety of classification problems demonstrate that ourapproach achieves better performance than standard LDA. Our method is also muchfaster and more scalable than standard interior-point SDP solvers based WLDA.The computational complexity for an SDP with $m$ constraints and matrices ofsize $d$ by $d$ is roughly reduced from $\mathcal{O}(m^3+md^3+m^2d^2)$ to$\mathcal{O}(d^3)$ ($m>d$ in our case).
arxiv-8100-42 | Bi-objective Optimization for Robust RGB-D Visual Odometry | http://arxiv.org/pdf/1411.7445v1.pdf | author:Tao Han, Chao Xu, Ryan Loxton, Lei Xie category:cs.RO cs.CV published:2014-11-27 summary:This paper considers a new bi-objective optimization formulation for robustRGB-D visual odometry. We investigate two methods for solving the proposedbi-objective optimization problem: the weighted sum method (in which theobjective functions are combined into a single objective function) and thebounded objective method (in which one of the objective functions is optimizedand the value of the other objective function is bounded via a constraint). Ourexperimental results for the open source TUM RGB-D dataset show that the newbi-objective optimization formulation is superior to several existing RGB-Dodometry methods. In particular, the new formulation yields more accuratemotion estimates and is more robust when textural or structural features in theimage sequence are lacking.
arxiv-8100-43 | Pattern Decomposition with Complex Combinatorial Constraints: Application to Materials Discovery | http://arxiv.org/pdf/1411.7441v1.pdf | author:Stefano Ermon, Ronan Le Bras, Santosh K. Suram, John M. Gregoire, Carla Gomes, Bart Selman, Robert B. van Dover category:cs.AI cs.LG stat.ML published:2014-11-27 summary:Identifying important components or factors in large amounts of noisy data isa key problem in machine learning and data mining. Motivated by a patterndecomposition problem in materials discovery, aimed at discovering newmaterials for renewable energy, e.g. for fuel and solar cells, we introduceCombiFD, a framework for factor based pattern decomposition that allows theincorporation of a-priori knowledge as constraints, including complexcombinatorial constraints. In addition, we propose a new pattern decompositionalgorithm, called AMIQO, based on solving a sequence of (mixed-integer)quadratic programs. Our approach considerably outperforms the state of the arton the materials discovery problem, scaling to larger datasets and recoveringmore precise and physically meaningful decompositions. We also show theeffectiveness of our approach for enforcing background knowledge on otherapplication domains.
arxiv-8100-44 | Metrics for Probabilistic Geometries | http://arxiv.org/pdf/1411.7432v1.pdf | author:Alessandra Tosi, Søren Hauberg, Alfredo Vellido, Neil D. Lawrence category:stat.ML cs.LG published:2014-11-27 summary:We investigate the geometrical structure of probabilistic generativedimensionality reduction models using the tools of Riemannian geometry. Weexplicitly define a distribution over the natural metric given by the models.We provide the necessary algorithms to compute expected metric tensors wherethe distribution over mappings is given by a Gaussian process. We treat thecorresponding latent variable model as a Riemannian manifold and we use theexpectation of the metric under the Gaussian process prior to defineinterpolating paths and measure distance between latent points. We show howdistances that respect the expected metric lead to more appropriate generationof new data.
arxiv-8100-45 | 3D-Assisted Image Feature Synthesis for Novel Views of an Object | http://arxiv.org/pdf/1412.0003v1.pdf | author:Hao Su, Fan Wang, Li Yi, Leonidas Guibas category:cs.CV published:2014-11-26 summary:Comparing two images in a view-invariant way has been a challenging problemin computer vision for a long time, as visual features are not stable underlarge view point changes. In this paper, given a single input image of anobject, we synthesize new features for other views of the same object. Toaccomplish this, we introduce an aligned set of 3D models in the same class asthe input object image. Each 3D model is represented by a set of views, and westudy the correlation of image patches between different views, seeking what wecall surrogates --- patches in one view whose feature content predicts well thefeatures of a patch in another view. In particular, for each patch in the noveldesired view, we seek surrogates from the observed view of the given image. Fora given surrogate, we predict that surrogate using linear combination of thecorresponding patches of the 3D model views, learn the coefficients, and thentransfer these coefficients on a per patch basis to synthesize the features ofthe patch in the novel view. In this way we can create feature sets for allviews of the latent object, providing us a multi-view representation of theobject. View-invariant object comparisons are achieved simply by computing the$L^2$ distances between the features of corresponding views. We providetheoretical and empirical analysis of the feature synthesis process, andevaluate the proposed view-agnostic distance (VAD) in fine-grained imageretrieval (100 object classes) and classification tasks. Experimental resultsshow that our synthesized features do enable view-independent comparisonbetween images and perform significantly better than traditional image featuresin this respect.
arxiv-8100-46 | Edge direction matrixes-based local binar patterns descriptor for shape pattern recognition | http://arxiv.org/pdf/1411.7336v1.pdf | author:Mohammed A. Talab, Siti Norul Huda Sheikh Abdullah, Bilal Bataineh category:cs.CV cs.IR published:2014-11-26 summary:Shapes and texture image recognition usage is an essential branch of patternrecognition. It is made up of techniques that aim at extracting informationfrom images via human knowledge and works. Local Binary Pattern (LBP) ensuresencoding global and local information and scaling invariance by introducing alook-up table to reflect the uniformity structure of an object. However, edgedirection matrixes (EDMS) only apply global invariant descriptor which employsfirst and secondary order relationships. The main idea behind this methodologyis the need of improved recognition capabilities, a goal achieved by thecombinative use of these descriptors. This collaboration aims to make use ofthe major advantages each one presents, by simultaneously complementing eachother, in order to elevate their weak points. By using multiple classifierapproaches such as random forest and multi-layer perceptron neural network, theproposed combinative descriptor are compared with the state of the artcombinative methods based on Gray-Level Co-occurrence matrix (GLCM with EDMS),LBP and moment invariant on four benchmark dataset MPEG-7 CE-Shape-1, KTH-TIPSimage, Enghlishfnt and Arabic calligraphy . The experiments have shown thesuperiority of the introduced descriptor over the GLCM with EDMS, LBP andmoment invariants and other well-known descriptor such as Scale InvariantFeature Transform from the literature.
arxiv-8100-47 | Understanding Deep Image Representations by Inverting Them | http://arxiv.org/pdf/1412.0035v1.pdf | author:Aravindh Mahendran, Andrea Vedaldi category:cs.CV published:2014-11-26 summary:Image representations, from SIFT and Bag of Visual Words to ConvolutionalNeural Networks (CNNs), are a crucial component of almost any imageunderstanding system. Nevertheless, our understanding of them remains limited.In this paper we conduct a direct analysis of the visual information containedin representations by asking the following question: given an encoding of animage, to which extent is it possible to reconstruct the image itself? Toanswer this question we contribute a general framework to invertrepresentations. We show that this method can invert representations such asHOG and SIFT more accurately than recent alternatives while being applicable toCNNs too. We then use this technique to study the inverse of recentstate-of-the-art CNN image representations for the first time. Among ourfindings, we show that several layers in CNNs retain photographically accurateinformation about the image, with different degrees of geometric andphotometric invariance.
arxiv-8100-48 | Open-source code for manifold-based 3D rotation recovery of X-ray scattering patterns | http://arxiv.org/pdf/1411.7889v1.pdf | author:Aliakbar Jafarpour category:physics.optics cs.CV published:2014-11-26 summary:Single particle 3D imaging with ultrashort X-ray laser pulses is based oncollecting and combining the information content of 2D scattering patterns ofan object at different orientations. Typical sample-delivery schemes leavelittle or no room for controlling the orientations. As such, the orientationassociated with a given snapshot should be estimated after the experiment. Herewe present an open-source code for the most rigorous technique having beenreported in this context. Some practical issues along with proposed solutionsare also discussed.
arxiv-8100-49 | Heuristics for Exact Nonnegative Matrix Factorization | http://arxiv.org/pdf/1411.7245v1.pdf | author:Arnaud Vandaele, Nicolas Gillis, François Glineur, Daniel Tuyttens category:math.OC cs.LG cs.NA stat.ML published:2014-11-26 summary:The exact nonnegative matrix factorization (exact NMF) problem is thefollowing: given an $m$-by-$n$ nonnegative matrix $X$ and a factorization rank$r$, find, if possible, an $m$-by-$r$ nonnegative matrix $W$ and an $r$-by-$n$nonnegative matrix $H$ such that $X = WH$. In this paper, we propose twoheuristics for exact NMF, one inspired from simulated annealing and the otherfrom the greedy randomized adaptive search procedure. We show that these twoheuristics are able to compute exact nonnegative factorizations for severalclasses of nonnegative matrices (namely, linear Euclidean distance matrices,slack matrices, unique-disjointness matrices, and randomly generated matrices)and as such demonstrate their superiority over standard multi-start strategies.We also consider a hybridization between these two heuristics that allows us tocombine the advantages of both methods. Finally, we discuss the use of theseheuristics to gain insight on the behavior of the nonnegative rank, i.e., theminimum factorization rank such that an exact NMF exists. In particular, wedisprove a conjecture on the nonnegative rank of a Kronecker product, propose anew upper bound on the extension complexity of generic $n$-gons and conjecturethe exact value of (i) the extension complexity of regular $n$-gons and (ii)the nonnegative rank of a submatrix of the slack matrix of the correlationpolytope.
arxiv-8100-50 | Localized Complexities for Transductive Learning | http://arxiv.org/pdf/1411.7200v1.pdf | author:Ilya Tolstikhin, Gilles Blanchard, Marius Kloft category:stat.ML cs.LG published:2014-11-26 summary:We show two novel concentration inequalities for suprema of empiricalprocesses when sampling without replacement, which both take the variance ofthe functions into account. While these inequalities may potentially have broadapplications in learning theory in general, we exemplify their significance bystudying the transductive setting of learning theory. For which we provide thefirst excess risk bounds based on the localized complexity of the hypothesisclass, which can yield fast rates of convergence also in the transductivelearning setting. We give a preliminary analysis of the localized complexitiesfor the prominent case of kernel classes.
arxiv-8100-51 | How Crossover Speeds Up Building-Block Assembly in Genetic Algorithms | http://arxiv.org/pdf/1403.6600v2.pdf | author:Dirk Sudholt category:cs.NE cs.DS published:2014-03-26 summary:We re-investigate a fundamental question: how effective is crossover inGenetic Algorithms in combining building blocks of good solutions? Althoughthis has been discussed controversially for decades, we are still lacking arigorous and intuitive answer. We provide such answers for royal road functionsand OneMax, where every bit is a building block. For the latter we show thatusing crossover makes every ($\mu$+$\lambda$) Genetic Algorithm at least twiceas fast as the fastest evolutionary algorithm using only standard bit mutation,up to small-order terms and for moderate $\mu$ and $\lambda$. Crossover isbeneficial because it effectively turns fitness-neutral mutations intoimprovements by combining the right building blocks at a later stage. Comparedto mutation-based evolutionary algorithms, this makes multi-bit mutations moreuseful. Introducing crossover changes the optimal mutation rate on OneMax from$1/n$ to $(1+\sqrt{5})/2 \cdot 1/n \approx 1.618/n$. This holds both foruniform crossover and $k$-point crossover. Experiments and statistical testsconfirm that our findings apply to a broad class of building-block functions.
arxiv-8100-52 | Linking GloVe with word2vec | http://arxiv.org/pdf/1411.5595v2.pdf | author:Tianze Shi, Zhiyuan Liu category:cs.CL cs.LG stat.ML published:2014-11-20 summary:The Global Vectors for word representation (GloVe), introduced by JeffreyPennington et al. is reported to be an efficient and effective method forlearning vector representations of words. State-of-the-art performance is alsoprovided by skip-gram with negative-sampling (SGNS) implemented in the word2vectool. In this note, we explain the similarities between the training objectivesof the two models, and show that the objective of SGNS is similar to theobjective of a specialized form of GloVe, though their cost functions aredefined differently.
arxiv-8100-53 | Real time Detection of Lane Markers in Urban Streets | http://arxiv.org/pdf/1411.7113v1.pdf | author:Mohamed Aly category:cs.CV cs.RO published:2014-11-26 summary:We present a robust and real time approach to lane marker detection in urbanstreets. It is based on generating a top view of the road, filtering usingselective oriented Gaussian filters, using RANSAC line fitting to give initialguesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which isthen followed by a post-processing step. Our algorithm can detect all lanes instill images of the street in various conditions, while operating at a rate of50 Hz and achieving comparable results to previous techniques.
arxiv-8100-54 | Structured Stochastic Variational Inference | http://arxiv.org/pdf/1404.4114v3.pdf | author:Matthew D. Hoffman, David M. Blei category:cs.LG published:2014-04-16 summary:Stochastic variational inference makes it possible to approximate posteriordistributions induced by large datasets quickly using stochastic optimization.The algorithm relies on the use of fully factorized variational distributions.However, this "mean-field" independence approximation limits the fidelity ofthe posterior approximation, and introduces local optima. We show how to relaxthe mean-field approximation to allow arbitrary dependencies between globalparameters and local hidden variables, producing better parameter estimates byreducing bias, sensitivity to local optima, and sensitivity to hyperparameters.
arxiv-8100-55 | A Generative Product-of-Filters Model of Audio | http://arxiv.org/pdf/1312.5857v5.pdf | author:Dawen Liang, Matthew D. Hoffman, Gautham J. Mysore category:stat.ML cs.LG published:2013-12-20 summary:We propose the product-of-filters (PoF) model, a generative model thatdecomposes audio spectra as sparse linear combinations of "filters" in thelog-spectral domain. PoF makes similar assumptions to those used in the classichomomorphic filtering approach to signal processing, but replaces hand-designeddecompositions built of basic signal processing operations with a learneddecomposition based on statistical inference. This paper formulates the PoFmodel and derives a mean-field method for posterior inference and a variationalEM algorithm to estimate the model's free parameters. We demonstrate PoF'spotential for audio processing on a bandwidth expansion task, and show that PoFcan serve as an effective unsupervised feature extractor for a speakeridentification task.
arxiv-8100-56 | Power-Law Graph Cuts | http://arxiv.org/pdf/1411.1971v2.pdf | author:Xiangyang Zhou, Jiaxin Zhang, Brian Kulis category:cs.CV cs.LG stat.ML published:2014-10-29 summary:Algorithms based on spectral graph cut objectives such as normalized cuts,ratio cuts and ratio association have become popular in recent years becausethey are widely applicable and simple to implement via standard eigenvectorcomputations. Despite strong performance for a number of clustering tasks,spectral graph cut algorithms still suffer from several limitations: first,they require the number of clusters to be known in advance, but thisinformation is often unknown a priori; second, they tend to produce clusterswith uniform sizes. In some cases, the true clusters exhibit a known sizedistribution; in image segmentation, for instance, human-segmented images tendto yield segment sizes that follow a power-law distribution. In this paper, wepropose a general framework of power-law graph cut algorithms that produceclusters whose sizes are power-law distributed, and also does not fix thenumber of clusters upfront. To achieve our goals, we treat the Pitman-Yorexchangeable partition probability function (EPPF) as a regularizer to graphcut objectives. Because the resulting objectives cannot be solved by relaxingvia eigenvectors, we derive a simple iterative algorithm to locally optimizethe objectives. Moreover, we show that our proposed algorithm can be viewed asperforming MAP inference on a particular Pitman-Yor mixture model. Ourexperiments on various data sets show the effectiveness of our algorithms.
arxiv-8100-57 | The Utility of Text: The Case of Amicus Briefs and the Supreme Court | http://arxiv.org/pdf/1409.7985v5.pdf | author:Yanchuan Sim, Bryan Routledge, Noah A. Smith category:cs.CL cs.AI cs.GT cs.LG published:2014-09-29 summary:We explore the idea that authoring a piece of text is an act of maximizingone's expected utility. To make this idea concrete, we consider the societallyimportant decisions of the Supreme Court of the United States. Extensive pastwork in quantitative political science provides a framework for empiricallymodeling the decisions of justices and how they relate to text. We incorporateinto such a model texts authored by amici curiae ("friends of the court"separate from the litigants) who seek to weigh in on the decision, thenexplicitly model their goals in a random utility model. We demonstrate thebenefits of this approach in improved vote prediction and the ability toperform counterfactual analysis.
arxiv-8100-58 | Efficient Algorithms for Bayesian Network Parameter Learning from Incomplete Data | http://arxiv.org/pdf/1411.7014v1.pdf | author:Guy Van den Broeck, Karthika Mohan, Arthur Choi, Judea Pearl category:cs.LG cs.AI published:2014-11-25 summary:We propose an efficient family of algorithms to learn the parameters of aBayesian network from incomplete data. In contrast to textbook approaches suchas EM and the gradient method, our approach is non-iterative, yields closedform parameter estimates, and eliminates the need for inference in a Bayesiannetwork. Our approach provides consistent parameter estimates for missing dataproblems that are MCAR, MAR, and in some cases, MNAR. Empirically, our approachis orders of magnitude faster than EM (as our approach requires no inference).Given sufficient data, we learn parameters that can be orders of magnitude moreaccurate.
arxiv-8100-59 | PLUTO: Penalized Unbiased Logistic Regression Trees | http://arxiv.org/pdf/1411.6948v1.pdf | author:Wenwen Zhang, Wei-Yin Loh category:stat.ML stat.ME published:2014-11-25 summary:We propose a new algorithm called PLUTO for building logistic regressiontrees to binary response data. PLUTO can capture the nonlinear and interactionpatterns in messy data by recursively partitioning the sample space. It fits asimple or a multiple linear logistic regression model in each partition. PLUTOemploys the cyclical coordinate descent method for estimation of multiplelinear logistic regression models with elastic net penalties, which allows itto deal with high-dimensional data efficiently. The tree structure comprises agraphical description of the data. Together with the logistic regressionmodels, it provides an accurate classifier as well as a piecewise smoothestimate of the probability of "success". PLUTO controls selection bias by: (1)separating split variable selection from split point selection; (2) applying anadjusted chi-squared test to find the split variable instead of exhaustivesearch. A bootstrap calibration technique is employed to further correctselection bias. Comparison on real datasets shows that on average, the multiplelinear PLUTO models predict more accurately than other algorithms.
arxiv-8100-60 | Short-Term Memory Through Persistent Activity: Evolution of Self-Stopping and Self-Sustaining Activity in Spiking Neural Networks | http://arxiv.org/pdf/1411.6912v1.pdf | author:Julien Hubert, Takashi Ikegami category:cs.NE q-bio.NC published:2014-11-25 summary:Memories in the brain are separated in two categories: short-term andlong-term memories. Long-term memories remain for a lifetime, while short-termones exist from a few milliseconds to a few minutes. Within short-term memorystudies, there is debate about what neural structure could implement it.Indeed, mechanisms responsible for long-term memories appear inadequate for thetask. Instead, it has been proposed that short-term memories could be sustainedby the persistent activity of a group of neurons. In this work, we explore whattopology could sustain short-term memories, not by designing a model fromspecific hypotheses, but through Darwinian evolution in order to obtain newinsights into its implementation. We evolved 10 networks capable of retaininginformation for a fixed duration between 2 and 11s. Our main finding has beenthat the evolution naturally created two functional modules in the network: onewhich sustains the information containing primarily excitatory neurons, whilethe other, which is responsible for forgetting, was composed mainly ofinhibitory neurons. This demonstrates how the balance between inhibition andexcitation plays an important role in cognition.
arxiv-8100-61 | Image Classification and Retrieval from User-Supplied Tags | http://arxiv.org/pdf/1411.6909v1.pdf | author:Hamid Izadinia, Ali Farhadi, Aaron Hertzmann, Matthew D. Hoffman category:cs.CV published:2014-11-25 summary:This paper proposes direct learning of image classification fromuser-supplied tags, without filtering. Each tag is supplied by the user whoshared the image online. Enormous numbers of these tags are freely availableonline, and they give insight about the image categories important to users andto image classification. Our approach is complementary to the conventionalapproach of manual annotation, which is extremely costly. We analyze of theFlickr 100 Million Image dataset, making several useful observations about thestatistics of these tags. We introduce a large-scale robust classificationalgorithm, in order to handle the inherent noise in these tags, and acalibration procedure to better predict objective annotations. We show thatfreely available, user-supplied tags can obtain similar or superior results tolarge databases of costly manual annotations.
arxiv-8100-62 | Learning Sets with Separating Kernels | http://arxiv.org/pdf/1204.3573v2.pdf | author:Ernesto De Vito, Lorenzo Rosasco, Alessandro Toigo category:stat.ML published:2012-04-16 summary:We consider the problem of learning a set from random samples. We show howrelevant geometric and topological properties of a set can be studiedanalytically using concepts from the theory of reproducing kernel Hilbertspaces. A new kind of reproducing kernel, that we call separating kernel, playsa crucial role in our study and is analyzed in detail. We prove a new analyticcharacterization of the support of a distribution, that naturally leads to afamily of provably consistent regularized learning algorithms and we discussthe stability of these methods with respect to random sampling. Numericalexperiments show that the approach is competitive, and often better, than otherstate of the art techniques.
arxiv-8100-63 | Design, Implementation and Simulation of a Cloud Computing System for Enhancing Real-time Video Services by using VANET and Onboard Navigation Systems | http://arxiv.org/pdf/1412.6149v1.pdf | author:Karim Hammoudi, Nabil Ajam, Mohamed Kasraoui, Fadi Dornaika, Karan Radhakrishnan, Karthik Bandi, Qing Cai, Sai Liu category:cs.NI cs.CV published:2014-11-25 summary:In this paper, we propose a design for novel and experimental cloud computingsystems. The proposed system aims at enhancing computational, communicationaland annalistic capabilities of road navigation services by merging severalindependent technologies, namely vision-based embedded navigation systems,prominent Cloud Computing Systems (CCSs) and Vehicular Ad-hoc NETwork (VANET).This work presents our initial investigations by describing the design of aglobal generic system. The designed system has been experimented with variousscenarios of video-based road services. Moreover, the associated architecturehas been implemented on a small-scale simulator of an in-vehicle embeddedsystem. The implemented architecture has been experimented in the case of asimulated road service to aid the police agency. The goal of this service is torecognize and track searched individuals and vehicles in a real-time monitoringsystem remotely connected to moving cars. The presented work demonstrates thepotential of our system for efficiently enhancing and diversifying real-timevideo services in road environments.
arxiv-8100-64 | Similarity- based approach for outlier detection | http://arxiv.org/pdf/1411.6850v1.pdf | author:Amina Dik, Khalid Jebari, Abdelaziz Bouroumi, Aziz Ettouhami category:cs.CV published:2014-11-25 summary:This paper presents a new approach for detecting outliers by introducing thenotion of object's proximity. The main idea is that normal point has similarcharacteristics with several neighbors. So the point in not an outlier if ithas a high degree of proximity and its neighbors are several. The performanceof this approach is illustrated through real datasets
arxiv-8100-65 | Orientation covariant aggregation of local descriptors with embeddings | http://arxiv.org/pdf/1407.2170v2.pdf | author:Giorgos Tolias, Teddy Furon, Hervé Jégou category:cs.CV published:2014-07-08 summary:Image search systems based on local descriptors typically achieve orientationinvariance by aligning the patches on their dominant orientations. Albeitsuccessful, this choice introduces too much invariance because it does notguarantee that the patches are rotated consistently. This paper introduces anaggregation strategy of local descriptors that achieves this covarianceproperty by jointly encoding the angle in the aggregation stage in a continuousmanner. It is combined with an efficient monomial embedding to provide acodebook-free method to aggregate local descriptors into a single vectorrepresentation. Our strategy is also compatible and employed with severalpopular encoding methods, in particular bag-of-words, VLAD and the Fishervector. Our geometric-aware aggregation strategy is effective for image search,as shown by experiments performed on standard benchmarks for image andparticular object retrieval, namely Holidays and Oxford buildings.
arxiv-8100-66 | Hypotheses of neural code and the information model of the neuron-detector | http://arxiv.org/pdf/1411.6768v1.pdf | author:Yuri Parzhin category:cs.NE cs.AI q-bio.NC published:2014-11-25 summary:This paper deals with the problem of neural code solving. On the basis of theformulated hypotheses the information model of a neuron-detector is suggested,the detector being one of the basic elements of an artificial neural network(ANN). The paper subjects the connectionist paradigm of ANN building tocriticism and suggests a new presentation paradigm for ANN building andneuroelements (NE) learning. The adequacy of the suggested model is proved bythe fact that is does not contradict the modern propositions of neuropsychologyand neurophysiology.
arxiv-8100-67 | Hardware-Amenable Structural Learning for Spike-based Pattern Classification using a Simple Model of Active Dendrites | http://arxiv.org/pdf/1411.5881v2.pdf | author:Shaista Hussain, Shih-Chii Liu, Arindam Basu category:cs.NE q-bio.NC published:2014-11-20 summary:This paper presents a spike-based model which employs neurons withfunctionally distinct dendritic compartments for classifying high dimensionalbinary patterns. The synaptic inputs arriving on each dendritic subunit arenonlinearly processed before being linearly integrated at the soma, giving theneuron a capacity to perform a large number of input-output mappings. The modelutilizes sparse synaptic connectivity; where each synapse takes a binary value.The optimal connection pattern of a neuron is learned by using a simplehardware-friendly, margin enhancing learning algorithm inspired by themechanism of structural plasticity in biological neurons. The learningalgorithm groups correlated synaptic inputs on the same dendritic branch. Sincethe learning results in modified connection patterns, it can be incorporatedinto current event-based neuromorphic systems with little overhead. This workalso presents a branch-specific spike-based version of this structuralplasticity rule. The proposed model is evaluated on benchmark binaryclassification problems and its performance is compared against that achievedusing Support Vector Machine (SVM) and Extreme Learning Machine (ELM)techniques. Our proposed method attains comparable performance while utilizing10 to 50% less computational resources than the other reported techniques.
arxiv-8100-68 | Accelerated Parallel Optimization Methods for Large Scale Machine Learning | http://arxiv.org/pdf/1411.6725v1.pdf | author:Haipeng Luo, Patrick Haffner, Jean-Francois Paiement category:cs.LG published:2014-11-25 summary:The growing amount of high dimensional data in different machine learningapplications requires more efficient and scalable optimization algorithms. Inthis work, we consider combining two techniques, parallelism and Nesterov'sacceleration, to design faster algorithms for L1-regularized loss. We firstsimplify BOOM, a variant of gradient descent, and study it in a unifiedframework, which allows us to not only propose a refined measurement ofsparsity to improve BOOM, but also show that BOOM is provably slower thanFISTA. Moving on to parallel coordinate descent methods, we then propose anefficient accelerated version of Shotgun, improving the convergence rate from$O(1/t)$ to $O(1/t^2)$. Our algorithm enjoys a concise form and analysiscompared to previous work, and also allows one to study several connected workin a unified way.
arxiv-8100-69 | Fast Edge Detection Using Structured Forests | http://arxiv.org/pdf/1406.5549v2.pdf | author:Piotr Dollár, C. Lawrence Zitnick category:cs.CV published:2014-06-20 summary:Edge detection is a critical component of many vision systems, includingobject detectors and image segmentation algorithms. Patches of edges exhibitwell-known forms of local structure, such as straight lines or T-junctions. Inthis paper we take advantage of the structure present in local image patches tolearn both an accurate and computationally efficient edge detector. Weformulate the problem of predicting local edge masks in a structured learningframework applied to random decision forests. Our novel approach to learningdecision trees robustly maps the structured labels to a discrete space on whichstandard information gain measures may be evaluated. The result is an approachthat obtains realtime performance that is orders of magnitude faster than manycompeting state-of-the-art approaches, while also achieving state-of-the-artedge detection results on the BSDS500 Segmentation dataset and NYU Depthdataset. Finally, we show the potential of our approach as a general purposeedge detector by showing our learned edge models generalize well acrossdatasets.
arxiv-8100-70 | One Vector is Not Enough: Entity-Augmented Distributional Semantics for Discourse Relations | http://arxiv.org/pdf/1411.6699v1.pdf | author:Yangfeng Ji, Jacob Eisenstein category:cs.CL cs.LG published:2014-11-25 summary:Discourse relations bind smaller linguistic units into coherent texts.However, automatically identifying discourse relations is difficult, because itrequires understanding the semantics of the linked arguments. A more subtlechallenge is that it is not enough to represent the meaning of each argument ofa discourse relation, because the relation may depend on links betweenlower-level components, such as entity mentions. Our solution computesdistributional meaning representations by composition up the syntactic parsetree. A key difference from previous work on compositional distributionalsemantics is that we also compute representations for entity mentions, using anovel downward compositional pass. Discourse relations are predicted from thedistributional representations of the arguments, and also of their coreferententity mentions. The resulting system obtains substantial improvements over theprevious state-of-the-art in predicting implicit discourse relations in thePenn Discourse Treebank.
arxiv-8100-71 | A Greedy, Flexible Algorithm to Learn an Optimal Bayesian Network Structure | http://arxiv.org/pdf/1411.6651v1.pdf | author:Amir Arsalan Soltani category:cs.AI stat.ML published:2014-11-24 summary:In this report paper we first present a report of the Advanced MachineLearning Course Project on the provided data set and then present a novelheuristic algorithm for exact Bayesian network (BN) structure discovery thatuses decomposable scoring functions. Our algorithm follows a different approachto solve the problem of BN structure discovery than the previously used methodssuch as Dynamic Programming (DP) and Branch and Bound to reduce the searchspace and find the global optima space for the problem. The algorithm wepropose has some degree of flexibility that can make it more or less greedy.The more the algorithm is set to be greedy, the more the speed of the algorithmwill be, and the less optimal the final structure. Our algorithm runs in a muchless time than the previously known methods and guarantees to have anoptimality of close to 99%. Therefore, it sacrifices less than one percent ofscore of an optimal structure in order to gain a much lower running time andmake the algorithm feasible for large data sets (we may note that we never usedany toolbox except for result validation)
arxiv-8100-72 | Application of the Ring Theory in the Segmentation of Digital Images | http://arxiv.org/pdf/1402.4069v2.pdf | author:Yasel Garcés, Esley Torres, Osvaldo Pereira, Roberto Rodríguez category:cs.CV published:2014-02-17 summary:Ring theory is one of the branches of the abstract algebra that has beenbroadly used in images. However, ring theory has not been very related withimage segmentation. In this paper, we propose a new index of similarity amongimages using Zn rings and the entropy function. This new index was applied as anew stopping criterion to the Mean Shift Iterative Algorithm with the goal toreach a better segmentation. An analysis on the performance of the algorithmwith this new stopping criterion is carried out. The obtained results provedthat the new index is a suitable tool to compare images.
arxiv-8100-73 | Consistency of Cheeger and Ratio Graph Cuts | http://arxiv.org/pdf/1411.6590v1.pdf | author:Nicolas Garcia Trillos, Dejan Slepcev, James von Brecht, Thomas Laurent, Xavier Bresson category:stat.ML cs.LG math.ST stat.TH published:2014-11-24 summary:This paper establishes the consistency of a family of graph-cut-basedalgorithms for clustering of data clouds. We consider point clouds obtained assamples of a ground-truth measure. We investigate approaches to clusteringbased on minimizing objective functionals defined on proximity graphs of thegiven sample. Our focus is on functionals based on graph cuts like the Cheegerand ratio cuts. We show that minimizers of the these cuts converge as thesample size increases to a minimizer of a corresponding continuum cut (whichpartitions the ground truth measure). Moreover, we obtain sharp conditions onhow the connectivity radius can be scaled with respect to the number of samplepoints for the consistency to hold. We provide results for two-way and formultiway cuts. Furthermore we provide numerical experiments that illustrate theresults and explore the optimality of scaling in dimension two.
arxiv-8100-74 | bartMachine: Machine Learning with Bayesian Additive Regression Trees | http://arxiv.org/pdf/1312.2171v3.pdf | author:Adam Kapelner, Justin Bleich category:stat.ML cs.LG published:2013-12-08 summary:We present a new package in R implementing Bayesian additive regression trees(BART). The package introduces many new features for data analysis using BARTsuch as variable selection, interaction detection, model diagnostic plots,incorporation of missing data and the ability to save trees for futureprediction. It is significantly faster than the current R implementation,parallelized, and capable of handling both large sample sizes andhigh-dimensional data.
arxiv-8100-75 | Noise Benefits in Expectation-Maximization Algorithms | http://arxiv.org/pdf/1411.6622v1.pdf | author:Osonde Adekorede Osoba category:stat.ML cs.LG math.ST stat.TH published:2014-11-24 summary:This dissertation shows that careful injection of noise into sample data cansubstantially speed up Expectation-Maximization algorithms.Expectation-Maximization algorithms are a class of iterative algorithms forextracting maximum likelihood estimates from corrupted or incomplete data. Theconvergence speed-up is an example of a noise benefit or "stochastic resonance"in statistical signal processing. The dissertation presents derivations ofsufficient conditions for such noise-benefits and demonstrates the speed-up insome ubiquitous signal-processing algorithms. These algorithms includeparameter estimation for mixture models, the $k$-means clustering algorithm,the Baum-Welch algorithm for training hidden Markov models, and backpropagationfor training feedforward artificial neural networks. This dissertation alsoanalyses the effects of data and model corruption on the more general Bayesianinference estimation framework. The main finding is a theorem guaranteeing thatuniform approximators for Bayesian model functions produce uniformapproximators for the posterior pdf via Bayes theorem. This result also appliesto hierarchical and multidimensional Bayesian models.
arxiv-8100-76 | Distributed Coordinate Descent for L1-regularized Logistic Regression | http://arxiv.org/pdf/1411.6520v1.pdf | author:Ilya Trofimov, Alexander Genkin category:stat.ML cs.LG published:2014-11-24 summary:Solving logistic regression with L1-regularization in distributed settings isan important problem. This problem arises when training dataset is very largeand cannot fit the memory of a single machine. We present d-GLMNET, a newalgorithm solving logistic regression with L1-regularization in the distributedsettings. We empirically show that it is superior over distributed onlinelearning via truncated gradient.
arxiv-8100-77 | Persistent Evidence of Local Image Properties in Generic ConvNets | http://arxiv.org/pdf/1411.6509v1.pdf | author:Ali Sharif Razavian, Hossein Azizpour, Atsuto Maki, Josephine Sullivan, Carl Henrik Ek, Stefan Carlsson category:cs.CV published:2014-11-24 summary:Supervised training of a convolutional network for object classificationshould make explicit any information related to the class of objects anddisregard any auxiliary information associated with the capture of the image orthe variation within the object class. Does this happen in practice? Althoughthis seems to pertain to the very final layers in the network, if we look atearlier layers we find that this is not the case. Surprisingly, strong spatialinformation is implicit. This paper addresses this, in particular, exploitingthe image representation at the first fully connected layer, i.e. the globalimage descriptor which has been recently shown to be most effective in a rangeof visual recognition tasks. We empirically demonstrate evidences for thefinding in the contexts of four different tasks: 2d landmark detection, 2dobject keypoints prediction, estimation of the RGB values of input image, andrecovery of semantic label of each pixel. We base our investigation on a simpleframework with ridge rigression commonly across these tasks, and show resultswhich all support our insight. Such spatial information can be used forcomputing correspondence of landmarks to a good accuracy, but shouldpotentially be useful for improving the training of the convolutional nets forclassification purposes.
arxiv-8100-78 | Solving the Periodic Timetabling Problem using a Genetic Algorithm | http://arxiv.org/pdf/1411.6998v1.pdf | author:Diego Arenas, Remy Chevirer, Said Hanafi, Joaquin Rodriguez category:cs.AI cs.NE published:2014-11-24 summary:In railway operations, a timetable is established to determine the departureand arrival times for the trains or other rolling stock at the differentstations or relevant points inside the rail network or a subset of thisnetwork. The elaboration of this timetable is done to respond to the commercialrequirements for both passenger and freight traffic, but also it must respect aset of security and capacity constraints associated with the railway network,rolling stock and legislation. Combining these requirements and constraints, aswell as the important number of trains and schedules to plan, makes thepreparation of a feasible timetable a complex and time-consuming process, thatnormally takes several months to be completed. This article addresses theproblem of generating periodic timetables, which means that the involved trainsoperate in a recurrent pattern. For instance, the trains belonging to the sametrain line, depart from some station every 15 minutes or one hour. To tacklethe problem, we present a constraint-based model suitable for this kind ofproblem. Then, we propose a genetic algorithm, allowing a rapid generation offeasible periodic timetables. Finally, two case studies are presented, thefirst, describing a sub-set of the Netherlands rail network, and the second alarge portion of the Nord-pas-de-Calais regional rail network, both of them arethen solved using our algorithm and the results are presented and discussed.
arxiv-8100-79 | The Application of Two-level Attention Models in Deep Convolutional Neural Network for Fine-grained Image Classification | http://arxiv.org/pdf/1411.6447v1.pdf | author:Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng, Zheng Zhang category:cs.CV published:2014-11-24 summary:Fine-grained classification is challenging because categories can only bediscriminated by subtle and local differences. Variances in the pose, scale orrotation usually make the problem more difficult. Most fine-grainedclassification systems follow the pipeline of finding foreground object orobject parts (where) to extract discriminative features (what). In this paper, we propose to apply visual attention to fine-grainedclassification task using deep neural network. Our pipeline integrates threetypes of attention: the bottom-up attention that propose candidate patches, theobject-level top-down attention that selects relevant patches to a certainobject, and the part-level top-down attention that localizes discriminativeparts. We combine these attentions to train domain-specific deep nets, then useit to improve both the what and where aspects. Importantly, we avoid usingexpensive annotations like bounding box or part information from end-to-end.The weak supervision constraint makes our work easier to generalize. We have verified the effectiveness of the method on the subsets of ILSVRC2012dataset and CUB200_2011 dataset. Our pipeline delivered significantimprovements and achieved the best accuracy under the weakest supervisioncondition. The performance is competitive against other methods that rely onadditional annotations.
arxiv-8100-80 | Encoding High Dimensional Local Features by Sparse Coding Based Fisher Vectors | http://arxiv.org/pdf/1411.6406v1.pdf | author:Lingqiao Liu, Chunhua Shen, Lei Wang, Anton van den Hengel, Chao Wang category:cs.CV published:2014-11-24 summary:Deriving from the gradient vector of a generative model of local features,Fisher vector coding (FVC) has been identified as an effective coding methodfor image classification. Most, if not all, % FVC implementations employ theGaussian mixture model (GMM) to characterize the generation process of localfeatures. This choice has shown to be sufficient for traditional lowdimensional local features, e.g., SIFT; and typically, good performance can beachieved with only a few hundred Gaussian distributions. However, the samenumber of Gaussians is insufficient to model the feature space spanned byhigher dimensional local features, which have become popular recently. In orderto improve the modeling capacity for high dimensional features, it turns out tobe inefficient and computationally impractical to simply increase the number ofGaussians. In this paper, we propose a model in which each local feature isdrawn from a Gaussian distribution whose mean vector is sampled from asubspace. With certain approximation, this model can be converted to a sparsecoding procedure and the learning/inference problems can be readily solved bystandard sparse coding methods. By calculating the gradient vector of theproposed model, we derive a new fisher vector encoding strategy, termed SparseCoding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recentlydeveloped Deep Convolutional Neural Network (CNN) descriptor as a highdimensional local feature and implement image classification with the proposedSCFVC. Our experimental evaluations demonstrate that our method not onlysignificantly outperforms the traditional GMM based Fisher vector encoding butalso achieves the state-of-the-art performance in generic object recognition,indoor scene, and fine-grained image classification problems.
arxiv-8100-81 | Multiclass learnability and the ERM principle | http://arxiv.org/pdf/1308.2893v2.pdf | author:Amit Daniely, Sivan Sabato, Shai Ben-David, Shai Shalev-Shwartz category:cs.LG published:2013-08-13 summary:We study the sample complexity of multiclass prediction in several learningsettings. For the PAC setting our analysis reveals a surprising phenomenon: Insharp contrast to binary classification, we show that there exist multiclasshypothesis classes for which some Empirical Risk Minimizers (ERM learners) havelower sample complexity than others. Furthermore, there are classes that arelearnable by some ERM learners, while other ERM learners will fail to learnthem. We propose a principle for designing good ERM learners, and use thisprinciple to prove tight bounds on the sample complexity of learning {\emsymmetric} multiclass hypothesis classes---classes that are invariant underpermutations of label names. We further provide a characterization of mistakeand regret bounds for multiclass learning in the online setting and the banditsetting, using new generalizations of Littlestone's dimension.
arxiv-8100-82 | Multiple object tracking with context awareness | http://arxiv.org/pdf/1411.7935v1.pdf | author:Laura Leal-Taixé category:cs.CV published:2014-11-24 summary:Multiple people tracking is a key problem for many applications such assurveillance, animation or car navigation, and a key input for tasks such asactivity recognition. In crowded environments occlusions and false detectionsare common, and although there have been substantial advances in recent years,tracking is still a challenging task. Tracking is typically divided into twosteps: detection, i.e., locating the pedestrians in the image, and dataassociation, i.e., linking detections across frames to form completetrajectories. For the data association task, approaches typically aim at developing new,more complex formulations, which in turn put the focus on the optimizationtechniques required to solve them. However, they still utilize very basicinformation such as distance between detections. In this thesis, I focus on thedata association task and argue that there is contextual information that hasnot been fully exploited yet in the tracking community, mainly social contextand spatial context coming from different views.
arxiv-8100-83 | Big Learning with Bayesian Methods | http://arxiv.org/pdf/1411.6370v1.pdf | author:Jun Zhu, Jianfei Chen, Wenbo Hu category:cs.LG stat.AP stat.CO stat.ME stat.ML F.1.2; G.3 published:2014-11-24 summary:Explosive growth in data and availability of cheap computing resources havesparked increasing interest in Big learning, an emerging subfield that studiesscalable machine learning algorithms, systems, and applications with Big Data.Bayesian methods represent one important class of statistic methods for machinelearning, with substantial recent developments on adaptive, flexible andscalable Bayesian learning. This article provides a survey of the recentadvances in Big learning with Bayesian methods, termed Big Bayesian Learning,including nonparametric Bayesian methods for adaptively inferring modelcomplexity, regularized Bayesian inference for improving the flexibility viaposterior regularization, and scalable algorithms and systems based onstochastic subsampling and distributed computing for dealing with large-scaleapplications.
arxiv-8100-84 | Scale-Invariant Convolutional Neural Networks | http://arxiv.org/pdf/1411.6369v1.pdf | author:Yichong Xu, Tianjun Xiao, Jiaxing Zhang, Kuiyuan Yang, Zheng Zhang category:cs.CV cs.LG cs.NE published:2014-11-24 summary:Even though convolutional neural networks (CNN) has achieved near-humanperformance in various computer vision tasks, its ability to tolerate scalevariations is limited. The popular practise is making the model bigger first,and then train it with data augmentation using extensive scale-jittering. Inthis paper, we propose a scaleinvariant convolutional neural network (SiCNN), amodeldesigned to incorporate multi-scale feature exaction and classificationinto the network structure. SiCNN uses a multi-column architecture, with eachcolumn focusing on a particular scale. Unlike previous multi-column strategies,these columns share the same set of filter parameters by a scale transformationamong them. This design deals with scale variation without blowing up the modelsize. Experimental results show that SiCNN detects features at various scales,and the classification result exhibits strong robustness against object scalevariations.
arxiv-8100-85 | On the mathematic modeling of non-parametric curves based on cubic Bézier curves | http://arxiv.org/pdf/1411.6365v1.pdf | author:Ha Jong Won, Choe Chun Hwa, Li Kum Song category:cs.CV published:2014-11-24 summary:B\'ezier splines are widely available in various systems with the curves andsurface designs. In general, the B\'ezier spline can be specified with theB\'ezier curve segments and a B\'ezier curve segment can be fitted to anynumber of control points. The number of control points determines the degree ofthe B\'ezier polynomial. This paper presents a method which determines controlpoints for B\'ezier curves approximating segments of obtained imageoutline(non-parametric curve) by using the properties of cubic B\'ezier curves.Proposed method is a technique to determine the control points that hasgenerality and reduces the error of the B\'ezier curve approximation. Mainadvantage of proposed method is that it has higher accuracy and compressionrate than previous methods. The cubic B\'ezier spline is obtained from cubicB\'ezier curve segments. To demonstrate the various performances of theproposed algorithm, experimental results are compared.
arxiv-8100-86 | A Hybrid Solution to improve Iteration Efficiency in the Distributed Learning | http://arxiv.org/pdf/1411.6358v1.pdf | author:Junxiong Wang, Hongzhi Wang, Chenxu Zhao category:cs.DC cs.LG 68 published:2014-11-24 summary:Currently, many machine learning algorithms contain lots of iterations. Whenit comes to existing large-scale distributed systems, some slave nodes maybreak down or have lower efficiency. Therefore traditional machine learningalgorithm may fail because of the instability of distributed system.We presentsa hybrid approach which not only own a high fault-tolerant but also achieve abalance of performance and efficiency.For each iteration, the result of slowmachines will be abandoned. Then, we discuss the relationship between accuracyand abandon rate. Next we debate the convergence speed of this process.Finally, our experiments demonstrate our idea can dramatically reducecalculation time and be used in many platforms.
arxiv-8100-87 | Iteratively Reweighted Graph Cut for Multi-label MRFs with Non-convex Priors | http://arxiv.org/pdf/1411.6340v1.pdf | author:Thalaiyasingam Ajanthan, Richard Hartley, Mathieu Salzmann, Hongdong Li category:cs.CV published:2014-11-24 summary:While widely acknowledged as highly effective in computer vision, multi-labelMRFs with non-convex priors are difficult to optimize. To tackle this, weintroduce an algorithm that iteratively approximates the original energy withan appropriately weighted surrogate energy that is easier to minimize. Ouralgorithm guarantees that the original energy decreases at each iteration. Inparticular, we consider the scenario where the global minimizer of the weightedsurrogate energy can be obtained by a multi-label graph cut algorithm, and showthat our algorithm then lets us handle of large variety of non-convex priors.We demonstrate the benefits of our method over state-of-the-art MRF energyminimization techniques on stereo and inpainting problems.
arxiv-8100-88 | Diffusion of Lexical Change in Social Media | http://arxiv.org/pdf/1210.5268v4.pdf | author:Jacob Eisenstein, Brendan O'Connor, Noah A. Smith, Eric P. Xing category:cs.CL cs.SI physics.soc-ph published:2012-10-18 summary:Computer-mediated communication is driving fundamental changes in the natureof written language. We investigate these changes by statistical analysis of adataset comprising 107 million Twitter messages (authored by 2.7 million uniqueuser accounts). Using a latent vector autoregressive model to aggregate acrossthousands of words, we identify high-level patterns in diffusion of linguisticchange over the United States. Our model is robust to unpredictable changes inTwitter's sampling rate, and provides a probabilistic characterization of therelationship of macro-scale linguistic influence to a set of demographic andgeographic predictors. The results of this analysis offer support for priorarguments that focus on geographical proximity and population size. However,demographic similarity -- especially with regard to race -- plays an even morecentral role, as cities with similar racial demographics are far more likely toshare linguistic influence. Rather than moving towards a single unified"netspeak" dialect, language evolution in computer-mediated communicationreproduces existing fault lines in spoken American English.
arxiv-8100-89 | Vision and Learning for Deliberative Monocular Cluttered Flight | http://arxiv.org/pdf/1411.6326v1.pdf | author:Debadeepta Dey, Kumar Shaurya Shankar, Sam Zeng, Rupesh Mehta, M. Talha Agcayazi, Christopher Eriksen, Shreyansh Daftry, Martial Hebert, J. Andrew Bagnell category:cs.RO cs.CV cs.LG published:2014-11-24 summary:Cameras provide a rich source of information while being passive, cheap andlightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this workwe present the first implementation of receding horizon control, which iswidely used in ground vehicles, with monocular vision as the only sensing modefor autonomous UAV flight in dense clutter. We make it feasible on UAVs via anumber of contributions: novel coupling of perception and control via relevantand diverse, multiple interpretations of the scene around the robot, leveragingrecent advances in machine learning to showcase anytime budgeted cost-sensitivefeature selection, and fast non-linear regression for monocular depthprediction. We empirically demonstrate the efficacy of our novel pipeline viareal world experiments of more than 2 kms through dense trees with a quadrotorbuilt from off-the-shelf parts. Moreover our pipeline is designed to combineinformation from other modalities like stereo and lidar as well if available.
arxiv-8100-90 | On the Decreasing Power of Kernel and Distance based Nonparametric Hypothesis Tests in High Dimensions | http://arxiv.org/pdf/1406.2083v2.pdf | author:Sashank J. Reddi, Aaditya Ramdas, Barnabás Póczos, Aarti Singh, Larry Wasserman category:stat.ML cs.IT cs.LG math.IT math.ST stat.ME stat.TH published:2014-06-09 summary:This paper is about two related decision theoretic problems, nonparametrictwo-sample testing and independence testing. There is a belief that tworecently proposed solutions, based on kernels and distances between pairs ofpoints, behave well in high-dimensional settings. We identify different sourcesof misconception that give rise to the above belief. Specifically, wedifferentiate the hardness of estimation of test statistics from the hardnessof testing whether these statistics are zero or not, and explicitly discuss anotion of "fair" alternative hypotheses for these problems as dimensionincreases. We then demonstrate that the power of these tests actually dropspolynomially with increasing dimension against fair alternatives. We end withsome theoretical insights and shed light on the \textit{median heuristic} forkernel bandwidth selection. Our work advances the current understanding of thepower of modern nonparametric hypothesis tests in high dimensions.
arxiv-8100-91 | On the High-dimensional Power of Linear-time Kernel Two-Sample Testing under Mean-difference Alternatives | http://arxiv.org/pdf/1411.6314v1.pdf | author:Aaditya Ramdas, Sashank J. Reddi, Barnabas Poczos, Aarti Singh, Larry Wasserman category:math.ST cs.AI cs.IT cs.LG math.IT stat.ML stat.TH published:2014-11-23 summary:Nonparametric two sample testing deals with the question of consistentlydeciding if two distributions are different, given samples from both, withoutmaking any parametric assumptions about the form of the distributions. Thecurrent literature is split into two kinds of tests - those which areconsistent without any assumptions about how the distributions may differ(\textit{general} alternatives), and those which are designed to specificallytest easier alternatives, like a difference in means (\textit{mean-shift}alternatives). The main contribution of this paper is to explicitly characterize the powerof a popular nonparametric two sample test, designed for general alternatives,under a mean-shift alternative in the high-dimensional setting. Specifically,we explicitly derive the power of the linear-time Maximum Mean Discrepancystatistic using the Gaussian kernel, where the dimension and sample size canboth tend to infinity at any rate, and the two distributions differ in theirmeans. As a corollary, we find that if the signal-to-noise ratio is heldconstant, then the test's power goes to one if the number of samples increasesfaster than the dimension increases. This is the first explicit powerderivation for a general nonparametric test in the high-dimensional setting,and also the first analysis of how tests designed for general alternativesperform when faced with easier ones.
arxiv-8100-92 | Optimal variable selection in multi-group sparse discriminant analysis | http://arxiv.org/pdf/1411.6311v1.pdf | author:Irina Gaynanova, Mladen Kolar category:stat.ML published:2014-11-23 summary:This article considers the problem of multi-group classification in thesetting where the number of variables $p$ is larger than the number ofobservations $n$. Several methods have been proposed in the literature thataddress this problem, however their variable selection performance is eitherunknown or suboptimal to the results known in the two-group case. In this workwe provide sharp conditions for the consistent recovery of relevant variablesin the multi-group case using the discriminant analysis proposal of Gaynanovaet al., 2014. We achieve the rates of convergence that attain the optimalscaling of the sample size $n$, number of variables $p$ and the sparsity level$s$. These rates are significantly faster than the best known results in themulti-group case. Moreover, they coincide with the optimal minimax rates forthe two-group case. We validate our theoretical results with numericalanalysis.
arxiv-8100-93 | A Convex Formulation for Spectral Shrunk Clustering | http://arxiv.org/pdf/1411.6308v1.pdf | author:Xiaojun Chang, Feiping Nie, Zhigang Ma, Yi Yang, Xiaofang Zhou category:cs.LG published:2014-11-23 summary:Spectral clustering is a fundamental technique in the field of data miningand information processing. Most existing spectral clustering algorithmsintegrate dimensionality reduction into the clustering process assisted bymanifold learning in the original space. However, the manifold inreduced-dimensional subspace is likely to exhibit altered properties incontrast with the original space. Thus, applying manifold information obtainedfrom the original space to the clustering process in a low-dimensional subspaceis prone to inferior performance. Aiming to address this issue, we propose anovel convex algorithm that mines the manifold structure in the low-dimensionalsubspace. In addition, our unified learning process makes the manifold learningparticularly tailored for the clustering. Compared with other related methods,the proposed algorithm results in more structured clustering result. Tovalidate the efficacy of the proposed algorithm, we perform extensiveexperiments on several benchmark datasets in comparison with somestate-of-the-art clustering approaches. The experimental results demonstratethat the proposed algorithm has quite promising clustering performance.
arxiv-8100-94 | Diversifying Sparsity Using Variational Determinantal Point Processes | http://arxiv.org/pdf/1411.6307v1.pdf | author:Nematollah Kayhan Batmanghelich, Gerald Quon, Alex Kulesza, Manolis Kellis, Polina Golland, Luke Bornn category:cs.LG cs.AI stat.ML published:2014-11-23 summary:We propose a novel diverse feature selection method based on determinantalpoint processes (DPPs). Our model enables one to flexibly define diversitybased on the covariance of features (similar to orthogonal matching pursuit) oralternatively based on side information. We introduce our approach in thecontext of Bayesian sparse regression, employing a DPP as a variationalapproximation to the true spike and slab posterior distribution. Wesubsequently show how this variational DPP approximation generalizes andextends mean-field approximation, and can be learned efficiently by exploitingthe fast sampling properties of DPPs. Our motivating application comes frombioinformatics, where we aim to identify a diverse set of genes whoseexpression profiles predict a tumor type where the diversity is defined withrespect to a gene-gene interaction network. We also explore an application inspatial statistics. In both cases, we demonstrate that the proposed methodyields significantly more diverse feature sets than classic sparse methods,without compromising accuracy.
arxiv-8100-95 | Revenue Optimization in Posted-Price Auctions with Strategic Buyers | http://arxiv.org/pdf/1411.6305v1.pdf | author:Mehryar Mohri, Andres Muñoz Medina category:cs.LG published:2014-11-23 summary:We study revenue optimization learning algorithms for posted-price auctionswith strategic buyers. We analyze a very broad family of monotone regretminimization algorithms for this problem, which includes the previously bestknown algorithm, and show that no algorithm in that family admits a strategicregret more favorable than $\Omega(\sqrt{T})$. We then introduce a newalgorithm that achieves a strategic regret differing from the lower bound onlyby a factor in $O(\log T)$, an exponential improvement upon the previous bestalgorithm. Our new algorithm admits a natural analysis and simpler proofs, andthe ideas behind its design are general. We also report the results ofempirical evaluations comparing our algorithm with the previous state of theart and show a consistent exponential improvement in several differentscenarios.
arxiv-8100-96 | Target Fishing: A Single-Label or Multi-Label Problem? | http://arxiv.org/pdf/1411.6285v1.pdf | author:Avid M. Afzal, Hamse Y. Mussa, Richard E. Turner, Andreas Bender, Robert C. Glen category:q-bio.BM cs.LG stat.ML published:2014-11-23 summary:According to Cobanoglu et al and Murphy, it is now widely acknowledged thatthe single target paradigm (one protein or target, one disease, one drug) thathas been the dominant premise in drug development in the recent past isuntenable. More often than not, a drug-like compound (ligand) can bepromiscuous - that is, it can interact with more than one target protein. Inrecent years, in in silico target prediction methods the promiscuity issue hasbeen approached computationally in different ways. In this study we confineattention to the so-called ligand-based target prediction machine learningapproaches, commonly referred to as target-fishing. With a few exceptions, thetarget-fishing approaches that are currently ubiquitous in cheminformaticsliterature can be essentially viewed as single-label multi-classificationschemes; these approaches inherently bank on the single target paradigmassumption that a ligand can home in on one specific target. In order toaddress the ligand promiscuity issue, one might be able to cast target-fishingas a multi-label multi-class classification problem. For illustrative andcomparison purposes, single-label and multi-label Naive Bayes classificationmodels (denoted here by SMM and MMM, respectively) for target-fishing wereimplemented. The models were constructed and tested on 65,587 compounds and 308targets retrieved from the ChEMBL17 database. SMM and MMM performeddifferently: for 16,344 test compounds, the MMM model returned recall andprecision values of 0.8058 and 0.6622, respectively; the corresponding recalland precision values yielded by the SMM model were 0.7805 and 0.7596,respectively. However, at a significance level of 0.05 and one degree offreedom McNemar test performed on the target prediction results returned by SMMand MMM for the 16,344 test ligands gave a chi-squared value of 15.656, infavour of the MMM approach.
arxiv-8100-97 | Detection of Non-Stationary Photometric Perturbations on Projection Screens | http://arxiv.org/pdf/1411.6275v1.pdf | author:Miguel Castañeda-Garay, Oscar Belmonte-Fernández, Hebert Pérez-Rosés, Antonio Diaz-Tula category:cs.CV H.5.2; I.4.1 published:2014-11-23 summary:Interfaces based on projection screens have become increasingly more popularin recent years, mainly due to the large screen size and resolution that theyprovide, as well as their stereo-vision capabilities. This work shows a localmethod for real-time detection of non-stationary photometric perturbations inprojected images by means of computer vision techniques. The method is based onthe computation of differences between the images in the projector's framebuffer and the corresponding images on the projection screen observed by thecamera. It is robust under spatial variations in the intensity of light emittedby the projector on the projection surface and also robust under stationaryphotometric perturbations caused by external factors. Moreover, we describe theexperiments carried out to show the reliability of the method.
arxiv-8100-98 | A Boosting Framework on Grounds of Online Learning | http://arxiv.org/pdf/1409.7202v3.pdf | author:Tofigh Naghibi, Beat Pfister category:cs.LG published:2014-09-25 summary:By exploiting the duality between boosting and online learning, we present aboosting framework which proves to be extremely powerful thanks to employingthe vast knowledge available in the online learning area. Using this framework,we develop various algorithms to address multiple practically and theoreticallyinteresting questions including sparse boosting, smooth-distribution boosting,agnostic learning and some generalization to double-projection online learningalgorithms, as a by-product.
arxiv-8100-99 | Balanced k-Means and Min-Cut Clustering | http://arxiv.org/pdf/1411.6235v1.pdf | author:Xiaojun Chang, Feiping Nie, Zhigang Ma, Yi Yang category:cs.LG published:2014-11-23 summary:Clustering is an effective technique in data mining to generate groups thatare the matter of interest. Among various clustering approaches, the family ofk-means algorithms and min-cut algorithms gain most popularity due to theirsimplicity and efficacy. The classical k-means algorithm partitions a number ofdata points into several subsets by iteratively updating the clustering centersand the associated data points. By contrast, a weighted undirected graph isconstructed in min-cut algorithms which partition the vertices of the graphinto two sets. However, existing clustering algorithms tend to cluster minorityof data points into a subset, which shall be avoided when the target dataset isbalanced. To achieve more accurate clustering for balanced dataset, we proposeto leverage exclusive lasso on k-means and min-cut to regulate the balancedegree of the clustering results. By optimizing our objective functions thatbuild atop the exclusive lasso, we can make the clustering result as muchbalanced as possible. Extensive experiments on several large-scale datasetsvalidate the advantage of the proposed algorithms compared to thestate-of-the-art clustering algorithms.
arxiv-8100-100 | A Convex Sparse PCA for Feature Analysis | http://arxiv.org/pdf/1411.6233v1.pdf | author:Xiaojun Chang, Feiping Nie, Yi Yang, Heng Huang category:cs.LG published:2014-11-23 summary:Principal component analysis (PCA) has been widely applied to dimensionalityreduction and data pre-processing for different applications in engineering,biology and social science. Classical PCA and its variants seek for linearprojections of the original variables to obtain a low dimensional featurerepresentation with maximal variance. One limitation is that it is verydifficult to interpret the results of PCA. In addition, the classical PCA isvulnerable to certain noisy data. In this paper, we propose a convex sparseprincipal component analysis (CSPCA) algorithm and apply it to featureanalysis. First we show that PCA can be formulated as a low-rank regressionoptimization problem. Based on the discussion, the l 2 , 1 -norm minimizationis incorporated into the objective function to make the regression coefficientssparse, thereby robust to the outliers. In addition, based on the sparse modelused in CSPCA, an optimal weight is assigned to each of the original feature,which in turn provides the output with good interpretability. With the outputof our CSPCA, we can effectively analyze the importance of each feature underthe PCA criteria. The objective function is convex, and we propose an iterativealgorithm to optimize it. We apply the CSPCA algorithm to feature selection andconduct extensive experiments on six different benchmark datasets. Experimentalresults demonstrate that the proposed algorithm outperforms state-of-the-artunsupervised feature selection algorithms.
arxiv-8100-101 | Low-Rank and Sparse Matrix Decomposition with a-priori knowledge for Dynamic 3D MRI reconstruction | http://arxiv.org/pdf/1411.6206v1.pdf | author:Dornoosh Zonoobi, Shahrooz Faghih Roohi, Ashraf A. Kassim category:cs.CV published:2014-11-23 summary:It has been recently shown that incorporating priori knowledge significantlyimproves the performance of basic compressive sensing based approaches. We havemanaged to successfully exploit this idea for recovering a matrix as asummation of a Low-rank and a Sparse component from compressive measurements.When applied to the problem of construction of 4D Cardiac MR image sequences inreal-time from highly under-sampled $k-$space data, our proposed methodachieves superior reconstruction quality compared to the other state-of-the-artmethods.
arxiv-8100-102 | Efficient Minimax Signal Detection on Graphs | http://arxiv.org/pdf/1411.6203v1.pdf | author:Jing Qian, Venkatesh Saligrama category:stat.ML published:2014-11-23 summary:Several problems such as network intrusion, community detection, and diseaseoutbreak can be described by observations attributed to nodes or edges of agraph. In these applications presence of intrusion, community or diseaseoutbreak is characterized by novel observations on some unknown connectedsubgraph. These problems can be formulated in terms of optimization of suitableobjectives on connected subgraphs, a problem which is generally computationallydifficult. We overcome the combinatorics of connectivity by embedding connectedsubgraphs into linear matrix inequalities (LMI). Computationally efficienttests are then realized by optimizing convex objective functions subject tothese LMI constraints. We prove, by means of a novel Euclidean embeddingargument, that our tests are minimax optimal for exponential family ofdistributions on 1-D and 2-D lattices. We show that internal conductance of theconnected subgraph family plays a fundamental role in characterizingdetectability.
arxiv-8100-103 | Kickback cuts Backprop's red-tape: Biologically plausible credit assignment in neural networks | http://arxiv.org/pdf/1411.6191v1.pdf | author:David Balduzzi, Hastagiri Vanchinathan, Joachim Buhmann category:cs.LG cs.NE q-bio.NC published:2014-11-23 summary:Error backpropagation is an extremely effective algorithm for assigningcredit in artificial neural networks. However, weight updates under Backpropdepend on lengthy recursive computations and require separate output and errormessages -- features not shared by biological neurons, that are perhapsunnecessary. In this paper, we revisit Backprop and the credit assignmentproblem. We first decompose Backprop into a collection of interacting learningalgorithms; provide regret bounds on the performance of these sub-algorithms;and factorize Backprop's error signals. Using these results, we derive a newcredit assignment algorithm for nonparametric regression, Kickback, that issignificantly simpler than Backprop. Finally, we provide a sufficient conditionfor Kickback to follow error gradients, and show that Kickback matchesBackprop's performance on real-world regression benchmarks.
arxiv-8100-104 | Characterization of the equivalence of robustification and regularization in linear, median, and matrix regression | http://arxiv.org/pdf/1411.6160v1.pdf | author:Dimitris Bertsimas, Martin S. Copenhaver category:math.ST cs.LG math.OC stat.ML stat.TH published:2014-11-22 summary:Sparsity is a key driver in modern statistical problems, from linearregression via the Lasso to matrix regression with nuclear norm penalties inmatrix completion and beyond. In stark contrast to sparsity motivations forsuch problems, it is known in the field of robust optimization that a varietyof vector regression problems, such as Lasso which appears as a loss functionplus a regularization penalty, can arise by simply immunizing a nominal problem(with only a loss function) to uncertainty in the data. Such a robustificationoffers an explanation for why some linear regression methods perform well inthe face of noise, even when these methods do not produce reliably sparsesolutions. In this paper we deepen and extend the understanding of theconnection between robustification and regularization in regression problems.Specifically, (a) in the context of linear regression, we characterize underwhich conditions on the model of uncertainty used and on the loss functionpenalties robustification and regularization are equivalent; (b) we show how totractably robustify median regression problems; and (c) we extend thecharacterization of robustification and regularization to matrix regressionproblems (matrix completion and Principal Component Analysis).
arxiv-8100-105 | Optimizing the CVaR via Sampling | http://arxiv.org/pdf/1404.3862v4.pdf | author:Aviv Tamar, Yonatan Glassner, Shie Mannor category:stat.ML cs.AI cs.LG published:2014-04-15 summary:Conditional Value at Risk (CVaR) is a prominent risk measure that is beingused extensively in various domains. We develop a new formula for the gradientof the CVaR in the form of a conditional expectation. Based on this formula, wepropose a novel sampling-based estimator for the CVaR gradient, in the spiritof the likelihood-ratio method. We analyze the bias of the estimator, and provethe convergence of a corresponding stochastic gradient descent algorithm to alocal CVaR optimum. Our method allows to consider CVaR optimization in newdomains. As an example, we consider a reinforcement learning application, andlearn a risk-sensitive controller for the game of Tetris.
arxiv-8100-106 | Incremental Learning of Event Definitions with Inductive Logic Programming | http://arxiv.org/pdf/1402.5988v2.pdf | author:Nikos Katzouris, Alexander Artikis, George Paliouras category:cs.LG cs.AI published:2014-02-24 summary:Event recognition systems rely on properly engineered knowledge bases ofevent definitions to infer occurrences of events in time. The manualdevelopment of such knowledge is a tedious and error-prone task, thusevent-based applications may benefit from automated knowledge constructiontechniques, such as Inductive Logic Programming (ILP), which combines machinelearning with the declarative and formal semantics of First-Order Logic.However, learning temporal logical formalisms, which are typically utilized bylogic-based Event Recognition systems is a challenging task, which most ILPsystems cannot fully undertake. In addition, event-based data is usuallymassive and collected at different times and under various circumstances.Ideally, systems that learn from temporal data should be able to operate in anincremental mode, that is, revise prior constructed knowledge in the face ofnew evidence. Most ILP systems are batch learners, in the sense that in orderto account for new evidence they have no alternative but to forget pastknowledge and learn from scratch. Given the increased inherent complexity ofILP and the volumes of real-life temporal data, this results to algorithms thatscale poorly. In this work we present an incremental method for learning andrevising event-based knowledge, in the form of Event Calculus programs. Theproposed algorithm relies on abductive-inductive learning and comprises ascalable clause refinement methodology, based on a compressive summarization ofclause coverage in a stream of examples. We present an empirical evaluation ofour approach on real and synthetic data from activity recognition and citytransport applications.
arxiv-8100-107 | From Stochastic Mixability to Fast Rates | http://arxiv.org/pdf/1406.3781v2.pdf | author:Nishant A. Mehta, Robert C. Williamson category:cs.LG stat.ML published:2014-06-14 summary:Empirical risk minimization (ERM) is a fundamental learning rule forstatistical learning problems where the data is generated according to someunknown distribution $\mathsf{P}$ and returns a hypothesis $f$ chosen from afixed class $\mathcal{F}$ with small loss $\ell$. In the parametric setting,depending upon $(\ell, \mathcal{F},\mathsf{P})$ ERM can have slow$(1/\sqrt{n})$ or fast $(1/n)$ rates of convergence of the excess risk as afunction of the sample size $n$. There exist several results that givesufficient conditions for fast rates in terms of joint properties of $\ell$,$\mathcal{F}$, and $\mathsf{P}$, such as the margin condition and the Bernsteincondition. In the non-statistical prediction with expert advice setting, thereis an analogous slow and fast rate phenomenon, and it is entirely characterizedin terms of the mixability of the loss $\ell$ (there being no role there for$\mathcal{F}$ or $\mathsf{P}$). The notion of stochastic mixability builds abridge between these two models of learning, reducing to classical mixabilityin a special case. The present paper presents a direct proof of fast rates forERM in terms of stochastic mixability of $(\ell,\mathcal{F}, \mathsf{P})$, andin so doing provides new insight into the fast-rates phenomenon. The proofexploits an old result of Kemperman on the solution to the general momentproblem. We also show a partial converse that suggests a characterization offast rates for ERM in terms of stochastic mixability is possible.
arxiv-8100-108 | Virtual View Networks for Object Reconstruction | http://arxiv.org/pdf/1411.6091v1.pdf | author:João Carreira, Abhishek Kar, Shubham Tulsiani, Jitendra Malik category:cs.CV published:2014-11-22 summary:All that structure from motion algorithms "see" are sets of 2D points. Weshow that these impoverished views of the world can be faked for the purpose ofreconstructing objects in challenging settings, such as from a single image, orfrom a few ones far apart, by recognizing the object and getting help from acollection of images of other objects from the same class. We synthesizevirtual views by computing geodesics on novel networks connecting objects withsimilar viewpoints, and introduce techniques to increase the specificity androbustness of factorization-based object reconstruction in this setting. Wereport accurate object shape reconstruction from a single image on challengingPASCAL VOC data, which suggests that the current domain of applications ofrigid structure-from-motion techniques may be significantly extended.
arxiv-8100-109 | PU Learning for Matrix Completion | http://arxiv.org/pdf/1411.6081v1.pdf | author:Cho-Jui Hsieh, Nagarajan Natarajan, Inderjit S. Dhillon category:cs.LG cs.NA stat.ML published:2014-11-22 summary:In this paper, we consider the matrix completion problem when theobservations are one-bit measurements of some underlying matrix M, and inparticular the observed samples consist only of ones and no zeros. This problemis motivated by modern applications such as recommender systems and socialnetworks where only "likes" or "friendships" are observed. The problem oflearning from only positive and unlabeled examples, called PU(positive-unlabeled) learning, has been studied in the context of binaryclassification. We consider the PU matrix completion problem, where anunderlying real-valued matrix M is first quantized to generate one-bitobservations and then a subset of positive entries is revealed. Under theassumption that M has bounded nuclear norm, we provide recovery guarantees fortwo different observation models: 1) M parameterizes a distribution thatgenerates a binary matrix, 2) M is thresholded to obtain a binary matrix. Forthe first case, we propose a "shifted matrix completion" method that recovers Musing only a subset of indices corresponding to ones, while for the secondcase, we propose a "biased matrix completion" method that recovers the(thresholded) binary matrix. Both methods yield strong error bounds --- if M isn by n, the Frobenius error is bounded as O(1/((1-rho)n), where 1-rho denotesthe fraction of ones observed. This implies a sample complexity of O(n\log n)ones to achieve a small error, when M is dense and n is large. We extend ourmethods and guarantees to the inductive matrix completion problem, where rowsand columns of M have associated features. We provide efficient and scalableoptimization procedures for both the methods and demonstrate the effectivenessof the proposed methods for link prediction (on real-world networks consistingof over 2 million nodes and 90 million links) and semi-supervised clusteringtasks.
arxiv-8100-110 | Stable Autoencoding: A Flexible Framework for Regularized Low-Rank Matrix Estimation | http://arxiv.org/pdf/1410.8275v2.pdf | author:Julie Josse, Stefan Wager category:stat.ME cs.LG stat.ML published:2014-10-30 summary:We develop a framework for low-rank matrix estimation that allows us totransform noise models into regularization schemes via a simple parametricbootstrap. Effectively, our procedure seeks an autoencoding basis for theobserved matrix that is robust with respect to the specified noise model. Inthe simplest case, with an isotropic noise model, our procedure is equivalentto a classical singular value shrinkage estimator. For non-isotropic noisemodels, however, our method does not reduce to singular value shrinkage, andinstead yields new estimators that perform well in experiments. Moreover, byiterating our stable autoencoding scheme, we can automatically generatelow-rank estimates without specifying the target rank as a tuning parameter.
arxiv-8100-111 | Finding Action Tubes | http://arxiv.org/pdf/1411.6031v1.pdf | author:Georgia Gkioxari, Jitendra Malik category:cs.CV published:2014-11-21 summary:We address the problem of action detection in videos. Driven by the latestprogress in object detection from 2D images, we build action models using richfeature hierarchies derived from shape and kinematic cues. We incorporateappearance and motion in two ways. First, starting from image region proposalswe select those that are motion salient and thus are more likely to contain theaction. This leads to a significant reduction in the number of regions beingprocessed and allows for faster computations. Second, we extractspatio-temporal feature representations to build strong classifiers usingConvolutional Neural Networks. We link our predictions to produce detectionsconsistent in time, which we call action tubes. We show that our approachoutperforms other techniques in the task of action detection.
arxiv-8100-112 | Differentially Private Algorithms for Empirical Machine Learning | http://arxiv.org/pdf/1411.5428v2.pdf | author:Ben Stoddard, Yan Chen, Ashwin Machanavajjhala category:cs.LG published:2014-11-20 summary:An important use of private data is to build machine learning classifiers.While there is a burgeoning literature on differentially private classificationalgorithms, we find that they are not practical in real applications due to tworeasons. First, existing differentially private classifiers provide pooraccuracy on real world datasets. Second, there is no known differentiallyprivate algorithm for empirically evaluating the private classifier on aprivate test dataset. In this paper, we develop differentially private algorithms that mirror realworld empirical machine learning workflows. We consider the private classifiertraining algorithm as a blackbox. We present private algorithms for selectingfeatures that are input to the classifier. Though adding a preprocessing steptakes away some of the privacy budget from the actual classification process(thus potentially making it noisier and less accurate), we show that our novelpreprocessing techniques significantly increase classifier accuracy on threereal-world datasets. We also present the first private algorithms forempirically constructing receiver operating characteristic (ROC) curves on aprivate test set.
arxiv-8100-113 | On the Impossibility of Convex Inference in Human Computation | http://arxiv.org/pdf/1411.5977v1.pdf | author:Nihar B. Shah, Dengyong Zhou category:stat.ML cs.HC cs.LG published:2014-11-21 summary:Human computation or crowdsourcing involves joint inference of theground-truth-answers and the worker-abilities by optimizing an objectivefunction, for instance, by maximizing the data likelihood based on an assumedunderlying model. A variety of methods have been proposed in the literature toaddress this inference problem. As far as we know, none of the objectivefunctions in existing methods is convex. In machine learning and appliedstatistics, a convex function such as the objective function of support vectormachines (SVMs) is generally preferred, since it can leverage thehigh-performance algorithms and rigorous guarantees established in theextensive literature on convex optimization. One may thus wonder if thereexists a meaningful convex objective function for the inference problem inhuman computation. In this paper, we investigate this convexity issue for humancomputation. We take an axiomatic approach by formulating a set of axioms thatimpose two mild and natural assumptions on the objective function for theinference. Under these axioms, we show that it is unfortunately impossible toensure convexity of the inference problem. On the other hand, we show thatinterestingly, in the absence of a requirement to model "spammers", one canconstruct reasonable objective functions for crowdsourcing that guaranteeconvex inference.
arxiv-8100-114 | Graph-Sparse LDA: A Topic Model with Structured Sparsity | http://arxiv.org/pdf/1410.4510v2.pdf | author:Finale Doshi-Velez, Byron Wallace, Ryan Adams category:stat.ML cs.CL cs.LG published:2014-10-16 summary:Originally designed to model text, topic modeling has become a powerful toolfor uncovering latent structure in domains including medicine, finance, andvision. The goals for the model vary depending on the application: in somecases, the discovered topics may be used for prediction or some otherdownstream task. In other cases, the content of the topic itself may be ofintrinsic scientific interest. Unfortunately, even using modern sparse techniques, the discovered topics areoften difficult to interpret due to the high dimensionality of the underlyingspace. To improve topic interpretability, we introduce Graph-Sparse LDA, ahierarchical topic model that leverages knowledge of relationships betweenwords (e.g., as encoded by an ontology). In our model, topics are summarized bya few latent concept-words from the underlying graph that explain the observedwords. Graph-Sparse LDA recovers sparse, interpretable summaries on tworeal-world biomedical datasets while matching state-of-the-art predictionperformance.
arxiv-8100-115 | Uncertainty And Evolutionary Optimization: A Novel Approach | http://arxiv.org/pdf/1407.4000v2.pdf | author:Maumita Bhattacharya, R. Islam, A. Mahmood category:cs.NE 68T99 F.1.1 published:2014-07-15 summary:Evolutionary algorithms (EA) have been widely accepted as efficient solversfor complex real world optimization problems, including engineeringoptimization. However, real world optimization problems often involve uncertainenvironment including noisy and/or dynamic environments, which pose majorchallenges to EA-based optimization. The presence of noise interferes with theevaluation and the selection process of EA, and thus adversely affects itsperformance. In addition, as presence of noise poses challenges to theevaluation of the fitness function, it may need to be estimated instead ofbeing evaluated. Several existing approaches attempt to address this problem,such as introduction of diversity (hyper mutation, random immigrants, specialoperators) or incorporation of memory of the past (diploidy, case basedmemory). However, these approaches fail to adequately address the problem. Inthis paper we propose a Distributed Population Switching Evolutionary Algorithm(DPSEA) method that addresses optimization of functions with noisy fitnessusing a distributed population switching architecture, to simulate adistributed self-adaptive memory of the solution space. Local regression isused in the pseudo-populations to estimate the fitness. Successful applicationsto benchmark test problems ascertain the proposed method's superior performancein terms of both robustness and accuracy.
arxiv-8100-116 | Randomized Dual Coordinate Ascent with Arbitrary Sampling | http://arxiv.org/pdf/1411.5873v1.pdf | author:Zheng Qu, Peter Richtárik, Tong Zhang category:math.OC cs.LG cs.NA math.NA published:2014-11-21 summary:We study the problem of minimizing the average of a large number of smoothconvex functions penalized with a strongly convex regularizer. We propose andanalyze a novel primal-dual method (Quartz) which at every iteration samplesand updates a random subset of the dual variables, chosen according to anarbitrary distribution. In contrast to typical analysis, we directly bound thedecrease of the primal-dual error (in expectation), without the need to firstanalyze the dual error. Depending on the choice of the sampling, we obtainefficient serial, parallel and distributed variants of the method. In theserial case, our bounds match the best known bounds for SDCA (both with uniformand importance sampling). With standard mini-batching, our bounds predictinitial data-independent speedup as well as additional data-driven speedupwhich depends on spectral and sparsity properties of the data. We calculatetheoretical speedup factors and find that they are excellent predictors ofactual speedup in practice. Moreover, we illustrate that it is possible todesign an efficient mini-batch importance sampling. The distributed variant ofQuartz is the first distributed SDCA-like method with an analysis fornon-separable data.
arxiv-8100-117 | Assessment of algorithms for mitosis detection in breast cancer histopathology images | http://arxiv.org/pdf/1411.5825v1.pdf | author:Mitko Veta, Paul J. van Diest, Stefan M. Willems, Haibo Wang, Anant Madabhushi, Angel Cruz-Roa, Fabio Gonzalez, Anders B. L. Larsen, Jacob S. Vestergaard, Anders B. Dahl, Dan C. Cireşan, Jürgen Schmidhuber, Alessandro Giusti, Luca M. Gambardella, F. Boray Tek, Thomas Walter, Ching-Wei Wang, Satoshi Kondo, Bogdan J. Matuszewski, Frederic Precioso, Violet Snell, Josef Kittler, Teofilo E. de Campos, Adnan M. Khan, Nasir M. Rajpoot, Evdokia Arkoumani, Miangela M. Lacle, Max A. Viergever, Josien P. W. Pluim category:cs.CV published:2014-11-21 summary:The proliferative activity of breast tumors, which is routinely estimated bycounting of mitotic figures in hematoxylin and eosin stained histologysections, is considered to be one of the most important prognostic markers.However, mitosis counting is laborious, subjective and may suffer from lowinter-observer agreement. With the wider acceptance of whole slide images inpathology labs, automatic image analysis has been proposed as a potentialsolution for these issues. In this paper, the results from the Assessment ofMitosis Detection Algorithms 2013 (AMIDA13) challenge are described. Thechallenge was based on a data set consisting of 12 training and 11 testingsubjects, with more than one thousand annotated mitotic figures by multipleobservers. Short descriptions and results from the evaluation of eleven methodsare presented. The top performing method has an error rate that is comparableto the inter-observer agreement among pathologists.
arxiv-8100-118 | Learning to Act Greedily: Polymatroid Semi-Bandits | http://arxiv.org/pdf/1405.7752v3.pdf | author:Branislav Kveton, Zheng Wen, Azin Ashkan, Michal Valko category:cs.LG cs.AI stat.ML published:2014-05-30 summary:Many important optimization problems, such as the minimum spanning tree andminimum-cost flow, can be solved optimally by a greedy method. In this work, westudy a learning variant of these problems, where the model of the problem isunknown and has to be learned by interacting repeatedly with the environment inthe bandit setting. We formalize our learning problem quite generally, aslearning how to maximize an unknown modular function on a known polymatroid. Wepropose a computationally efficient algorithm for solving our problem and boundits expected cumulative regret. Our gap-dependent upper bound is tight up to aconstant and our gap-free upper bound is tight up to polylogarithmic factors.Finally, we evaluate our method on three problems and demonstrate that it ispractical.
arxiv-8100-119 | Pre-processing of Domain Ontology Graph Generation System in Punjabi | http://arxiv.org/pdf/1411.5796v1.pdf | author:Rajveer Kaur, Saurabh Sharma category:cs.CL published:2014-11-21 summary:This paper describes pre-processing phase of ontology graph generation systemfrom Punjabi text documents of different domains. This research paper focuseson pre-processing of Punjabi text documents. Pre-processing is structuredrepresentation of the input text. Pre-processing of ontology graph generationincludes allowing input restrictions to the text, removal of special symbolsand punctuation marks, removal of duplicate terms, removal of stop words,extract terms by matching input terms with dictionary and gazetteer liststerms.
arxiv-8100-120 | Real-time Topic-aware Influence Maximization Using Preprocessing | http://arxiv.org/pdf/1403.0057v2.pdf | author:Wei Chen, Tian Lin, Cheng Yang category:cs.SI cs.LG F.2.2 published:2014-03-01 summary:Influence maximization is the task of finding a set of seed nodes in a socialnetwork such that the influence spread of these seed nodes based on certaininfluence diffusion model is maximized. Topic-aware influence diffusion modelshave been recently proposed to address the issue that influence between a pairof users are often topic-dependent and information, ideas, innovations etc.being propagated in networks (referred collectively as items in this paper) aretypically mixtures of topics. In this paper, we focus on the topic-awareinfluence maximization task. In particular, we study preprocessing methods forthese topics to avoid redoing influence maximization for each item fromscratch. We explore two preprocessing algorithms with theoreticaljustifications. Our empirical results on data obtained in a couple of existingstudies demonstrate that one of our algorithms stands out as a strong candidateproviding microsecond online response time and competitive influence spread,with reasonable preprocessing effort.
arxiv-8100-121 | Cascades of Regression Tree Fields for Image Restoration | http://arxiv.org/pdf/1404.2086v2.pdf | author:Uwe Schmidt, Jeremy Jancsary, Sebastian Nowozin, Stefan Roth, Carsten Rother category:cs.CV published:2014-04-08 summary:Conditional random fields (CRFs) are popular discriminative models forcomputer vision and have been successfully applied in the domain of imagerestoration, especially to image denoising. For image deblurring, however,discriminative approaches have been mostly lacking. We posit two reasons forthis: First, the blur kernel is often only known at test time, requiring anydiscriminative approach to cope with considerable variability. Second, giventhis variability it is quite difficult to construct suitable features fordiscriminative prediction. To address these challenges we first show aconnection between common half-quadratic inference for generative image priorsand Gaussian CRFs. Based on this analysis, we then propose a cascade model forimage restoration that consists of a Gaussian CRF at each stage. Each stage ofour cascade is semi-parametric, i.e. it depends on the instance-specificparameters of the restoration problem, such as the blur kernel. We train ourmodel by loss minimization with synthetically generated training data. Ourexperiments show that when applied to non-blind image deblurring, the proposedapproach is efficient and yields state-of-the-art restoration quality on imagescorrupted with synthetic and real blur. Moreover, we demonstrate itssuitability for image denoising, where we achieve competitive results forgrayscale and color images.
arxiv-8100-122 | A probabilistic framework for analysing the compositionality of conceptual combinations | http://arxiv.org/pdf/1305.5753v3.pdf | author:Peter D. Bruza, Kirsty Kitto, Brentyn J. Ramm, Laurianne Sitbon category:cs.CL published:2013-05-23 summary:Conceptual combination performs a fundamental role in creating the broadrange of compound phrases utilized in everyday language. This article providesa novel probabilistic framework for assessing whether the semantics ofconceptual combinations are compositional, and so can be considered as afunction of the semantics of the constituent concepts, or not. While thesystematicity and productivity of language provide a strong argument in favorof assuming compositionality, this very assumption is still regularlyquestioned in both cognitive science and philosophy. Additionally, theprinciple of semantic compositionality is underspecified, which means thatnotions of both "strong" and "weak" compositionality appear in the literature.Rather than adjudicating between different grades of compositionality, theframework presented here contributes formal methods for determining a cleardividing line between compositional and non-compositional semantics. Inaddition, we suggest that the distinction between these is contextuallysensitive. Utilizing formal frameworks developed for analyzing compositesystems in quantum theory, we present two methods that allow the semantics ofconceptual combinations to be classified as "compositional" or"non-compositional". Compositionality is first formalised by factorising thejoint probability distribution modeling the combination, where the terms in thefactorisation correspond to individual concepts. This leads to the necessaryand sufficient condition for the joint probability distribution to exist. Afailure to meet this condition implies that the underlying concepts cannot bemodeled in a single probability space when considering their combination, andthe combination is thus deemed "non-compositional". The formal analysis methodsare demonstrated by applying them to an empirical study of twenty-fournon-lexicalised conceptual combinations.
arxiv-8100-123 | A Joint Probabilistic Classification Model of Relevant and Irrelevant Sentences in Mathematical Word Problems | http://arxiv.org/pdf/1411.5732v1.pdf | author:Suleyman Cetintas, Luo Si, Yan Ping Xin, Dake Zhang, Joo Young Park, Ron Tzur category:cs.CL cs.IR cs.LG stat.ML published:2014-11-21 summary:Estimating the difficulty level of math word problems is an important taskfor many educational applications. Identification of relevant and irrelevantsentences in math word problems is an important step for calculating thedifficulty levels of such problems. This paper addresses a novel application oftext categorization to identify two types of sentences in mathematical wordproblems, namely relevant and irrelevant sentences. A novel joint probabilisticclassification model is proposed to estimate the joint probability ofclassification decisions for all sentences of a math word problem by utilizingthe correlation among all sentences along with the correlation between thequestion sentence and other sentences, and sentence text. The proposed model iscompared with i) a SVM classifier which makes independent classificationdecisions for individual sentences by only using the sentence text and ii) anovel SVM classifier that considers the correlation between the questionsentence and other sentences along with the sentence text. An extensive set ofexperiments demonstrates the effectiveness of the joint probabilisticclassification model for identifying relevant and irrelevant sentences as wellas the novel SVM classifier that utilizes the correlation between the questionsentence and other sentences. Furthermore, empirical results and analysis showthat i) it is highly beneficial not to remove stopwords and ii) utilizing partof speech tagging does not make a significant improvement although it has beenshown to be effective for the related task of math word problem typeclassification.
arxiv-8100-124 | Visual Sentiment Prediction with Deep Convolutional Neural Networks | http://arxiv.org/pdf/1411.5731v1.pdf | author:Can Xu, Suleyman Cetintas, Kuang-Chih Lee, Li-Jia Li category:cs.CV cs.NE stat.ML published:2014-11-21 summary:Images have become one of the most popular types of media through which usersconvey their emotions within online social networks. Although vast amount ofresearch is devoted to sentiment analysis of textual data, there has been verylimited work that focuses on analyzing sentiment of image data. In this work,we propose a novel visual sentiment prediction framework that performs imageunderstanding with Deep Convolutional Neural Networks (CNN). Specifically, theproposed sentiment prediction framework performs transfer learning from a CNNwith millions of parameters, which is pre-trained on large-scale data forobject recognition. Experiments conducted on two real-world datasets fromTwitter and Tumblr demonstrate the effectiveness of the proposed visualsentiment analysis framework.
arxiv-8100-125 | Metric recovery from directed unweighted graphs | http://arxiv.org/pdf/1411.5720v1.pdf | author:Tatsunori B. Hashimoto, Yi Sun, Tommi S. Jaakkola category:stat.ML cs.SI math.ST stat.ME stat.TH published:2014-11-20 summary:We analyze directed, unweighted graphs obtained from $x_i\in \mathbb{R}^d$ byconnecting vertex $i$ to $j$ iff $x_i - x_j < \epsilon(x_i)$. Examples ofsuch graphs include $k$-nearest neighbor graphs, where $\epsilon(x_i)$ variesfrom point to point, and, arguably, many real world graphs such asco-purchasing graphs. We ask whether we can recover the underlying Euclideanmetric $\epsilon(x_i)$ and the associated density $p(x_i)$ given only thedirected graph and $d$. We show that consistent recovery is possible up to isometric scaling when thevertex degree is at least $\omega(n^{2/(2+d)}\log(n)^{d/(d+2)})$. Our estimatoris based on a careful characterization of a random walk over the directed graphand the associated continuum limit. As an algorithm, it resembles the PageRankcentrality metric. We demonstrate empirically that the estimator performs wellon simulated examples as well as on real-world co-purchasing graphs even with asmall number of points and degree scaling as low as $\log(n)$.
arxiv-8100-126 | An algorithm for improving Non-Local Means operators via low-rank approximation | http://arxiv.org/pdf/1412.2067v1.pdf | author:Victor May, Yosi Keller, Nir Sharon, Yoel Shkolnisky category:cs.CV math.GM published:2014-11-20 summary:We present a method for improving a Non Local Means operator by computing itslow-rank approximation. The low-rank operator is constructed by applying afilter to the spectrum of the original Non Local Means operator. This resultsin an operator which is less sensitive to noise while preserving importantproperties of the original operator. The method is efficiently implementedbased on Chebyshev polynomials and is demonstrated on the application ofnatural images denoising. For this application, we provide a comprehensivecomparison of our method with leading denoising methods.
arxiv-8100-127 | Clustering evolving data using kernel-based methods | http://arxiv.org/pdf/1411.5988v1.pdf | author:Rocco Langone category:cs.SI cs.LG stat.ML published:2014-11-20 summary:In this thesis, we propose several modelling strategies to tackle evolvingdata in different contexts. In the framework of static clustering, we start byintroducing a soft kernel spectral clustering (SKSC) algorithm, which canbetter deal with overlapping clusters with respect to kernel spectralclustering (KSC) and provides more interpretable outcomes. Afterwards, a wholestrategy based upon KSC for community detection of static networks is proposed,where the extraction of a high quality training sub-graph, the choice of thekernel function, the model selection and the applicability to large-scale dataare key aspects. This paves the way for the development of a novel clusteringalgorithm for the analysis of evolving networks called kernel spectralclustering with memory effect (MKSC), where the temporal smoothness betweenclustering results in successive time steps is incorporated at the level of theprimal optimization problem, by properly modifying the KSC formulation. Lateron, an application of KSC to fault detection of an industrial machine ispresented. Here, a smart pre-processing of the data by means of a properwindowing operation is necessary to catch the ongoing degradation processaffecting the machine. In this way, in a genuinely unsupervised manner, it ispossible to raise an early warning when necessary, in an online fashion.Finally, we propose a new algorithm called incremental kernel spectralclustering (IKSC) for online learning of non-stationary data. This ambitiouschallenge is faced by taking advantage of the out-of-sample property of kernelspectral clustering (KSC) to adapt the initial model, in order to tacklemerging, splitting or drifting of clusters across time. Real-world applicationsconsidered in this thesis include image segmentation, time-series clustering,community detection of static and evolving networks.
arxiv-8100-128 | Learning a Recurrent Visual Representation for Image Caption Generation | http://arxiv.org/pdf/1411.5654v1.pdf | author:Xinlei Chen, C. Lawrence Zitnick category:cs.CV cs.AI cs.CL published:2014-11-20 summary:In this paper we explore the bi-directional mapping between images and theirsentence-based descriptions. We propose learning this mapping using a recurrentneural network. Unlike previous approaches that map both sentences and imagesto a common embedding, we enable the generation of novel sentences given animage. Using the same model, we can also reconstruct the visual featuresassociated with an image given its visual description. We use a novel recurrentvisual memory that automatically learns to remember long-term visual conceptsto aid in both sentence generation and visual feature reconstruction. Weevaluate our approach on several tasks. These include sentence generation,sentence retrieval and image retrieval. State-of-the-art results are shown forthe task of generating novel image descriptions. When compared to humangenerated captions, our automatically generated captions are preferred byhumans over $19.8\%$ of the time. Results are better than or comparable tostate-of-the-art results on the image and sentence retrieval tasks for methodsusing similar visual features.
arxiv-8100-129 | N-sphere chord length distribution | http://arxiv.org/pdf/1411.5639v1.pdf | author:Panagiotis Sidiropoulos category:math.PR stat.ML published:2014-11-20 summary:This work studies the chord length distribution, in the case where both endslie on a $N$-dimensional hypersphere ($N \geq 2$). Actually, after connectingthis distribution to the recently estimated surface of a hyperspherical cap\cite{SLi11}, closed-form expressions of both the probability density functionand the cumulative distribution function are straightforwardly extracted, whichare followed by a discussion on its basic properties, among which itsdependence from the hypersphere dimension. Additionally, the distribution ofthe dot product of unitary vectors is estimated, a problem that is related tothe chord length.
arxiv-8100-130 | Predicting the Future Behavior of a Time-Varying Probability Distribution | http://arxiv.org/pdf/1406.5362v2.pdf | author:Christoph H. Lampert category:stat.ML cs.LG published:2014-06-20 summary:We study the problem of predicting the future, though only in theprobabilistic sense of estimating a future state of a time-varying probabilitydistribution. This is not only an interesting academic problem, but solvingthis extrapolation problem also has many practical application, e.g. fortraining classifiers that have to operate under time-varying conditions. Ourmain contribution is a method for predicting the next step of the time-varyingdistribution from a given sequence of sample sets from earlier time steps. Forthis we rely on two recent machine learning techniques: embedding probabilitydistributions into a reproducing kernel Hilbert space, and learning operatorsby vector-valued regression. We illustrate the working principles and thepractical usefulness of our method by experiments on synthetic and real data.We also highlight an exemplary application: training a classifier in a domainadaptation setting without having access to examples from the test timedistribution at training time.
arxiv-8100-131 | Autonomization of Monoidal Categories | http://arxiv.org/pdf/1411.3827v2.pdf | author:Antonin Delpeuch category:math.CT cs.CL 18D10 published:2014-11-14 summary:We define the free autonomous category generated by a monoidal category andstudy some of its properties. From a linguistic perspective, this expands therange of possible models of meaning within the distributional compositionalframework, by allowing nonlinearities in maps. From a categorical point ofview, this provides a factorization of the construction in [Preller and Lambek,2007] of the free autonomous category generated by a category.
arxiv-8100-132 | Vector Autoregressions with Parsimoniously Time Varying Parameters and an Application to Monetary Policy | http://arxiv.org/pdf/1411.0877v2.pdf | author:Laurent Callot, Johannes Tang Kristensen category:math.ST stat.ML stat.TH 91G70 published:2014-11-04 summary:This paper proposes a parsimoniously time varying parameter vectorautoregressive model (with exogenous variables, VARX) and studies theproperties of the Lasso and adaptive Lasso as estimators of this model. Theparameters of the model are assumed to follow parsimonious random walks, whereparsimony stems from the assumption that increments to the parameters have anon-zero probability of being exactly equal to zero. By varying the degree ofparsimony our model can accommodate constant parameters, an unknown number ofstructural breaks, or parameters with a high degree of variation. We characterize the finite sample properties of the Lasso by deriving upperbounds on the estimation and prediction errors that are valid with highprobability; and asymptotically we show that these bounds tend to zero withprobability tending to one if the number of non zero increments grows slowerthan $\sqrt{T}$. By simulation experiments we investigate the properties of the Lasso and theadaptive Lasso in settings where the parameters are stable, experiencestructural breaks, or follow a parsimonious random walk. We use our model toinvestigate the monetary policy response to inflation and business cyclefluctuations in the US by estimating a parsimoniously time varying parameterTaylor rule. We document substantial changes in the policy response of the Fedin the 1980s and since 2008.
arxiv-8100-133 | Maximum Likelihood Directed Enumeration Method in Piecewise-Regular Object Recognition | http://arxiv.org/pdf/1411.5555v1.pdf | author:Andrey Savchenko category:cs.CV 68T10 I.5.1; I.5.4 published:2014-11-20 summary:We explore the problems of classification of composite object (images, speechsignals) with low number of models per class. We study the question ofimproving recognition performance for medium-sized database (thousands ofclasses). The key issue of fast approximate nearest-neighbor methods widelyapplied in this task is their heuristic nature. It is possible to stronglyprove their efficiency by using the theory of algorithms only for simplesimilarity measures and artificially generated tasks. On the contrary, in thispaper we propose an alternative, statistically optimal greedy algorithm. Ateach step of this algorithm joint density (likelihood) of distances topreviously checked models is estimated for each class. The next model to checkis selected from the class with the maximal likelihood. The latter is estimatedbased on the asymptotic properties of the Kullback-Leibler informationdiscrimination and mathematical model of piecewise-regular object withdistribution of each regular segment of exponential type. Experimental resultsin face recognition for FERET dataset prove that the proposed method is muchmore effective than not only brute force and the baseline (directed enumerationmethod) but also approximate nearest neighbor methods from FLANN andNonMetricSpaceLib libraries (randomized kd-tree, composite index, perm-sort).
arxiv-8100-134 | Fast gradient descent for drifting least squares regression, with application to bandits | http://arxiv.org/pdf/1307.3176v4.pdf | author:Nathaniel Korda, Prashanth L. A., Rémi Munos category:cs.LG stat.ML published:2013-07-11 summary:Online learning algorithms require to often recompute least squaresregression estimates of parameters. We study improving the computationalcomplexity of such algorithms by using stochastic gradient descent (SGD) typeschemes in place of classic regression solvers. We show that SGD schemesefficiently track the true solutions of the regression problems, even in thepresence of a drift. This finding coupled with an $O(d)$ improvement incomplexity, where $d$ is the dimension of the data, make them attractive forimplementation in the big data settings. In the case when strong convexity inthe regression problem is guaranteed, we provide bounds on the error both inexpectation and high probability (the latter is often needed to providetheoretical guarantees for higher level algorithms), despite the drifting leastsquares solution. As an example of this case we prove that the regretperformance of an SGD version of the PEGE linear bandit algorithm[Rusmevichientong and Tsitsiklis 2010] is worse that that of PEGE itself onlyby a factor of $O(\log^4 n)$. When strong convexity of the regression problemcannot be guaranteed, we investigate using an adaptive regularisation. We makean empirical study of an adaptively regularised, SGD version of LinUCB [Li etal. 2010] in a news article recommendation application, which uses the largescale news recommendation dataset from Yahoo! front page. These experimentsshow a large gain in computational complexity, with a consistently low trackingerror and click-through-rate (CTR) performance that is $75\%$ close.
arxiv-8100-135 | $l_1$-regularized Outlier Isolation and Regression | http://arxiv.org/pdf/1406.0156v2.pdf | author:Sheng Han, Suzhen Wang, Xinyu Wu category:cs.CV cs.LG stat.ML published:2014-06-01 summary:This paper proposed a new regression model called $l_1$-regularized outlierisolation and regression (LOIRE) and a fast algorithm based on block coordinatedescent to solve this model. Besides, assuming outliers are gross errorsfollowing a Bernoulli process, this paper also presented a Bernoulli estimatemodel which, in theory, should be very accurate and robust due to its completeelimination of affections caused by outliers. Though this Bernoulli estimate ishard to solve, it could be approximately achieved through a process which takesLOIRE as an important intermediate step. As a result, the approximate Bernoulliestimate is a good combination of Bernoulli estimate's accuracy and LOIREregression's efficiency with several simulations conducted to strongly verifythis point. Moreover, LOIRE can be further extended to realize robust rankfactorization which is powerful in recovering low-rank component from massivecorruptions. Extensive experimental results showed that the proposed methodoutperforms state-of-the-art methods like RPCA and GoDec in the aspect ofcomputation speed with a competitive performance.
arxiv-8100-136 | Confidence sets for persistence diagrams | http://arxiv.org/pdf/1303.7117v3.pdf | author:Brittany Terese Fasy, Fabrizio Lecci, Alessandro Rinaldo, Larry Wasserman, Sivaraman Balakrishnan, Aarti Singh category:math.ST cs.CG cs.LG stat.TH published:2013-03-28 summary:Persistent homology is a method for probing topological properties of pointclouds and functions. The method involves tracking the birth and death oftopological features (2000) as one varies a tuning parameter. Features withshort lifetimes are informally considered to be "topological noise," and thosewith a long lifetime are considered to be "topological signal." In this paper,we bring some statistical ideas to persistent homology. In particular, wederive confidence sets that allow us to separate topological signal fromtopological noise.
arxiv-8100-137 | Liquid State Machine with Dendritically Enhanced Readout for Low-power, Neuromorphic VLSI Implementations | http://arxiv.org/pdf/1411.5458v1.pdf | author:Subhrajit Roy, Amitava Banerjee, Arindam Basu category:cs.ET cs.NE published:2014-11-20 summary:In this paper, we describe a new neuro-inspired, hardware-friendly readoutstage for the liquid state machine (LSM), a popular model for reservoircomputing. Compared to the parallel perceptron architecture trained by thep-delta algorithm, which is the state of the art in terms of performance ofreadout stages, our readout architecture and learning algorithm can attainbetter performance with significantly less synaptic resources making itattractive for VLSI implementation. Inspired by the nonlinear properties ofdendrites in biological neurons, our readout stage incorporates neurons havingmultiple dendrites with a lumped nonlinearity. The number of synapticconnections on each branch is significantly lower than the total number ofconnections from the liquid neurons and the learning algorithm tries to findthe best 'combination' of input connections on each branch to reduce the error.Hence, the learning involves network rewiring (NRW) of the readout networksimilar to structural plasticity observed in its biological counterparts. Weshow that compared to a single perceptron using analog weights, thisarchitecture for the readout can attain, even by using the same number ofbinary valued synapses, up to 3.3 times less error for a two-class spike trainclassification problem and 2.4 times less error for an input rate approximationtask. Even with 60 times larger synapses, a group of 60 parallel perceptronscannot attain the performance of the proposed dendritically enhanced readout.An additional advantage of this method for hardware implementations is that the'choice' of connectivity can be easily implemented exploiting address eventrepresentation (AER) protocols commonly used in current neuromorphic systemswhere the connection matrix is stored in memory. Also, due to the use of binarysynapses, our proposed method is more robust against statistical variations.
arxiv-8100-138 | Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures | http://arxiv.org/pdf/1409.2574v4.pdf | author:John R. Hershey, Jonathan Le Roux, Felix Weninger category:cs.LG cs.NE stat.ML published:2014-09-09 summary:Model-based methods and deep neural networks have both been tremendouslysuccessful paradigms in machine learning. In model-based methods, problemdomain knowledge can be built into the constraints of the model, typically atthe expense of difficulties during inference. In contrast, deterministic deepneural networks are constructed in such a way that inference isstraightforward, but their architectures are generic and it is unclear how toincorporate knowledge. This work aims to obtain the advantages of bothapproaches. To do so, we start with a model-based approach and an associatedinference algorithm, and \emph{unfold} the inference iterations as layers in adeep network. Rather than optimizing the original model, we \emph{untie} themodel parameters across layers, in order to create a more powerful network. Theresulting architecture can be trained discriminatively to perform accurateinference within a fixed network size. We show how this framework allows us tointerpret conventional networks as mean-field inference in Markov randomfields, and to obtain new architectures by instead using belief propagation asthe inference algorithm. We then show its application to a non-negative matrixfactorization model that incorporates the problem-domain knowledge that soundsources are additive. Deep unfolding of this model yields a new kind ofnon-negative deep neural network, that can be trained using a multiplicativebackpropagation-style update algorithm. We present speech enhancementexperiments showing that our approach is competitive with conventional neuralnetworks despite using far fewer parameters.
arxiv-8100-139 | Approximate evaluation of marginal association probabilities with belief propagation | http://arxiv.org/pdf/1209.6299v2.pdf | author:Jason L. Williams, Roslyn A. Lau category:cs.AI cs.CV published:2012-09-12 summary:Data association, the problem of reasoning over correspondence betweentargets and measurements, is a fundamental problem in tracking. This paperpresents a graphical model formulation of data association and applies anapproximate inference method, belief propagation (BP), to obtain estimates ofmarginal association probabilities. We prove that BP is guaranteed to converge,and bound the number of iterations necessary. Experiments reveal a favourablecomparison to prior methods in terms of accuracy and computational complexity.
arxiv-8100-140 | Budget-Constrained Item Cold-Start Handling in Collaborative Filtering Recommenders via Optimal Design | http://arxiv.org/pdf/1406.2431v2.pdf | author:Oren Anava, Shahar Golan, Nadav Golbandi, Zohar Karnin, Ronny Lempel, Oleg Rokhlenko, Oren Somekh category:cs.IR cs.LG 62K05 published:2014-06-10 summary:It is well known that collaborative filtering (CF) based recommender systemsprovide better modeling of users and items associated with considerable ratinghistory. The lack of historical ratings results in the user and the itemcold-start problems. The latter is the main focus of this work. Most of thecurrent literature addresses this problem by integrating content-basedrecommendation techniques to model the new item. However, in many cases suchcontent is not available, and the question arises is whether this problem canbe mitigated using CF techniques only. We formalize this problem as anoptimization problem: given a new item, a pool of available users, and a budgetconstraint, select which users to assign with the task of rating the new itemin order to minimize the prediction error of our model. We show that theobjective function is monotone-supermodular, and propose efficient optimaldesign based algorithms that attain an approximation to its optimum. Ourfindings are verified by an empirical study using the Netflix dataset, wherethe proposed algorithms outperform several baselines for the problem at hand.
arxiv-8100-141 | Im2Fit: Fast 3D Model Fitting and Anthropometrics using Single Consumer Depth Camera and Synthetic Data | http://arxiv.org/pdf/1410.0745v2.pdf | author:Qiaosong Wang, Vignesh Jagadeesh, Bryan Ressler, Robinson Piramuthu category:cs.CV published:2014-10-03 summary:Recent advances in consumer depth sensors have created many opportunities forhuman body measurement and modeling. Estimation of 3D body shape isparticularly useful for fashion e-commerce applications such as virtual try-onor fit personalization. In this paper, we propose a method for capturingaccurate human body shape and anthropometrics from a single consumer gradedepth sensor. We first generate a large dataset of synthetic 3D human bodymodels using real-world body size distributions. Next, we estimate key bodymeasurements from a single monocular depth image. We combine body measurementestimates with local geometry features around key joint positions to form arobust multi-dimensional feature vector. This allows us to conduct a fastnearest-neighbor search to every sample in the dataset and return the closestone. Compared to existing methods, our approach is able to predict accuratefull body parameters from a partial view using measurement parameters learnedfrom the synthetic dataset. Furthermore, our system is capable of generating 3Dhuman mesh models in real-time, which is significantly faster than methodswhich attempt to model shape and pose deformations. To validate the efficiencyand applicability of our system, we collected a dataset that contains frontaland back scans of 83 clothed people with ground truth height and weight.Experiments on real-world dataset show that the proposed method can achievereal-time performance with competing results achieving an average error of 1.9cm in estimated measurements.
arxiv-8100-142 | Affordances Provide a Fundamental Categorization Principle for Visual Scenes | http://arxiv.org/pdf/1411.5340v1.pdf | author:Michelle R. Greene, Christopher Baldassano, Andre Esteva, Diane M. Beck, Li Fei-Fei category:q-bio.NC cs.CV cs.HC published:2014-11-19 summary:How do we know that a kitchen is a kitchen by looking? Relatively little isknown about how we conceptualize and categorize different visual environments.Traditional models of visual perception posit that scene categorization isachieved through the recognition of a scene's objects, yet these models cannotaccount for the mounting evidence that human observers are relativelyinsensitive to the local details in an image. Psychologists have long theorizedthat the affordances, or actionable possibilities of a stimulus are pivotal toits perception. To what extent are scene categories created from similaraffordances? Using a large-scale experiment using hundreds of scene categories,we show that the activities afforded by a visual scene provide a fundamentalcategorization principle. Affordance-based similarity explained the majority ofthe structure in the human scene categorization patterns, outperformingalternative similarities based on objects or visual features. We all modelswere combined, affordances provided the majority of the predictive power in thecombined model, and nearly half of the total explained variance is capturedonly by affordances. These results challenge many existing models of high-levelvisual perception, and provide immediately testable hypotheses for thefunctional organization of the human perceptual system.
arxiv-8100-143 | Visual Noise from Natural Scene Statistics Reveals Human Scene Category Representations | http://arxiv.org/pdf/1411.5331v1.pdf | author:Michelle R. Greene, Abraham P. Botros, Diane M. Beck, Li Fei-Fei category:cs.CV cs.HC published:2014-11-19 summary:Our perceptions are guided both by the bottom-up information entering oureyes, as well as our top-down expectations of what we will see. Althoughbottom-up visual processing has been extensively studied, comparatively littleis known about top-down signals. Here, we describe REVEAL (RepresentationsEnvisioned Via Evolutionary ALgorithm), a method for visualizing an observer'sinternal representation of a complex, real-world scene, allowing us to, for thefirst time, visualize the top-down information in an observer's mind. REVEALrests on two innovations for solving this high dimensional problem: visualnoise that samples from natural image statistics, and a computer algorithm thatcollaborates with human observers to efficiently obtain a solution. In thiswork, we visualize observers' internal representations of a visual scenecategory (street) using an experiment in which the observer views thenaturalistic visual noise and collaborates with the algorithm to externalizehis internal representation. As no scene information was presented, observershad to use their internal knowledge of the target, matching it with the visualfeatures in the noise. We matched reconstructed images with images ofreal-world street scenes to enhance visualization. Critically, we show that thevisualized mental images can be used to predict rapid scene detectionperformance, as each observer had faster and more accurate responses todetecting real-world images that were the most similar to his reconstructedstreet templates. These results show that it is possible to visualizepreviously unobservable mental representations of real world stimuli. Morebroadly, REVEAL provides a general method for objectively examining the contentof previously private, subjective mental experiences.
arxiv-8100-144 | ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image Collections | http://arxiv.org/pdf/1411.5328v1.pdf | author:Bolei Zhou, Vignesh Jagadeesh, Robinson Piramuthu category:cs.CV cs.AI cs.LG published:2014-11-19 summary:Discovering visual knowledge from weakly labeled data is crucial to scale upcomputer vision recognition system, since it is expensive to obtain fullylabeled data for a large number of concept categories. In this paper, wepropose ConceptLearner, which is a scalable approach to discover visualconcepts from weakly labeled image collections. Thousands of visual conceptdetectors are learned automatically, without human in the loop for additionalannotation. We show that these learned detectors could be applied to recognizeconcepts at image-level and to detect concepts at image region-levelaccurately. Under domain-specific supervision, we further evaluate the learnedconcepts for scene recognition on SUN database and for object detection onPascal VOC 2007. ConceptLearner shows promising performance compared to fullysupervised and weakly supervised methods.
arxiv-8100-145 | End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression | http://arxiv.org/pdf/1411.5309v1.pdf | author:Li Wan, David Eigen, Rob Fergus category:cs.CV published:2014-11-19 summary:Deformable Parts Models and Convolutional Networks each have achieved notableperformance in object detection. Yet these two approaches find their strengthsin complementary areas: DPMs are well-versed in object composition, modelingfine-grained spatial relationships between parts; likewise, ConvNets are adeptat producing powerful image features, having been discriminatively traineddirectly on the pixels. In this paper, we propose a new model that combinesthese two approaches, obtaining the advantages of each. We train this modelusing a new structured loss function that considers all bounding boxes withinan image, rather than isolated object instances. This enables the non-maximalsuppression (NMS) operation, previously treated as a separate post-processingstage, to be integrated into the model. This allows for discriminative trainingof our combined Convnet + DPM + NMS model in end-to-end fashion. We evaluateour system on PASCAL VOC 2007 and 2011 datasets, achieving competitive resultson both benchmarks.
arxiv-8100-146 | Efficient Media Retrieval from Non-Cooperative Queries | http://arxiv.org/pdf/1411.5307v1.pdf | author:Kevin Shih, Wei Di, Vignesh Jagadeesh, Robinson Piramuthu category:cs.IR cs.CV published:2014-11-19 summary:Text is ubiquitous in the artificial world and easily attainable when itcomes to book title and author names. Using the images from the book cover setfrom the Stanford Mobile Visual Search dataset and additional book covers andmetadata from openlibrary.org, we construct a large scale book cover retrievaldataset, complete with 100K distractor covers and title and author strings foreach. Because our query images are poorly conditioned for clean textextraction, we propose a method for extracting a matching noisy and erroneousOCR readings and matching it against clean author and book title strings in astandard document look-up problem setup. Finally, we demonstrate how to usethis text-matching as a feature in conjunction with popular retrieval featuressuch as VLAD using a simple learning setup to achieve significant improvementsin retrieval accuracy over that of either VLAD or the text alone.
arxiv-8100-147 | Sparse distributed localized gradient fused features of objects | http://arxiv.org/pdf/1411.5268v1.pdf | author:Swathikiran Sudhakarana, Alex Pappachen James category:cs.CV cs.AI published:2014-11-19 summary:The sparse, hierarchical, and modular processing of natural signals isrelated to the ability of humans to recognize objects with high accuracy. Inthis study, we report a sparse feature processing and encoding method, whichimproved the recognition performance of an automated object recognition system.Randomly distributed localized gradient enhanced features were selected beforeemploying aggregate functions for representation, where we used a modular andhierarchical approach to detect the object features. These object features werecombined with a minimum distance classifier, thereby obtaining objectrecognition system accuracies of 93% using the Amsterdam library of objectimages (ALOI) database, 92% using the Columbia object image library (COIL)-100database, and 69% using the PASCAL visual object challenge 2007 database. Theobject recognition performance was shown to be robust to variations in noise,object scaling, and object shifts. Finally, a comparison with eight existingobject recognition methods indicated that our new method improved therecognition accuracy by 10% with ALOI, 8% with the COIL-100 database, and 10%with the PASCAL visual object challenge 2007 database.
arxiv-8100-148 | Large-Margin Classification with Multiple Decision Rules | http://arxiv.org/pdf/1411.5260v1.pdf | author:Patrick K. Kimes, D. Neil Hayes, J. S. Marron, Yufeng Liu category:stat.ML cs.LG published:2014-11-19 summary:Binary classification is a common statistical learning problem in which amodel is estimated on a set of covariates for some outcome indicating themembership of one of two classes. In the literature, there exists a distinctionbetween hard and soft classification. In soft classification, the conditionalclass probability is modeled as a function of the covariates. In contrast, hardclassification methods only target the optimal prediction boundary. While hardand soft classification methods have been studied extensively, not much workhas been done to compare the actual tasks of hard and soft classification. Inthis paper we propose a spectrum of statistical learning problems which spanthe hard and soft classification tasks based on fitting multiple decision rulesto the data. By doing so, we reveal a novel collection of learning tasks ofincreasing complexity. We study the problems using the framework oflarge-margin classifiers and a class of piecewise linear convex surrogates, forwhich we derive statistical properties and a corresponding sub-gradient descentalgorithm. We conclude by applying our approach to simulation settings and amagnetic resonance imaging (MRI) dataset from the Alzheimer's DiseaseNeuroimaging Initiative (ADNI) study.
arxiv-8100-149 | Zero-Aliasing Correlation Filters for Object Recognition | http://arxiv.org/pdf/1411.2316v2.pdf | author:Joseph A. Fernandez, Vishnu Naresh Boddeti, Andres Rodriguez, B. V. K. Vijaya Kumar category:cs.CV stat.ML published:2014-11-10 summary:Correlation filters (CFs) are a class of classifiers that are attractive forobject localization and tracking applications. Traditionally, CFs have beendesigned in the frequency domain using the discrete Fourier transform (DFT),where correlation is efficiently implemented. However, existing CF designs donot account for the fact that the multiplication of two DFTs in the frequencydomain corresponds to a circular correlation in the time/spatial domain.Because this was previously unaccounted for, prior CF designs are not trulyoptimal, as their optimization criteria do not accurately quantify theiroptimization intention. In this paper, we introduce new zero-aliasingconstraints that completely eliminate this aliasing problem by ensuring thatthe optimization criterion for a given CF corresponds to a linear correlationrather than a circular correlation. This means that previous CF designs can besignificantly improved by this reformulation. We demonstrate the benefits ofthis new CF design approach with several important CFs. We present experimentalresults on diverse data sets and present solutions to the computationalchallenges associated with computing these CFs. Code for the CFs described inthis paper and their respective zero-aliasing versions is available athttp://vishnu.boddeti.net/projects/correlation-filters.html
arxiv-8100-150 | Learning nonparametric differential equations with operator-valued kernels and gradient matching | http://arxiv.org/pdf/1411.5172v1.pdf | author:Markus Heinonen, Florence d'Alché-Buc category:cs.LG stat.ML published:2014-11-19 summary:Modeling dynamical systems with ordinary differential equations implies amechanistic view of the process underlying the dynamics. However in many cases,this knowledge is not available. To overcome this issue, we introduce a generalframework for nonparametric ODE models using penalized regression inReproducing Kernel Hilbert Spaces (RKHS) based on operator-valued kernels.Moreover, we extend the scope of gradient matching approaches to nonparametricODE. A smooth estimate of the solution ODE is built to provide an approximationof the derivative of the ODE solution which is in turn used to learn thenonparametric ODE model. This approach benefits from the flexibility ofpenalized regression in RKHS allowing for ridge or (structured) sparseregression as well. Very good results are shown on 3 different ODE systems.
arxiv-8100-151 | Attentional Neural Network: Feature Selection Using Cognitive Feedback | http://arxiv.org/pdf/1411.5140v1.pdf | author:Qian Wang, Jiaxing Zhang, Sen Song, Zheng Zhang category:cs.CV cs.NE published:2014-11-19 summary:Attentional Neural Network is a new framework that integrates top-downcognitive bias and bottom-up feature extraction in one coherent architecture.The top-down influence is especially effective when dealing with high noise ordifficult segmentation problems. Our system is modular and extensible. It isalso easy to train and cheap to run, and yet can accommodate complex behaviors.We obtain classification accuracy better than or competitive with state of artresults on the MNIST variation dataset, and successfully disentangle overlaiddigits with high success rates. We view such a general purpose framework as anessential foundation for a larger system emulating the cognitive abilities ofthe whole brain.
arxiv-8100-152 | Identifying Outliers in Large Matrices via Randomized Adaptive Compressive Sampling | http://arxiv.org/pdf/1407.0312v3.pdf | author:Xingguo Li, Jarvis Haupt category:cs.IT cs.LG math.IT stat.ML published:2014-07-01 summary:This paper examines the problem of locating outlier columns in a large,otherwise low-rank, matrix. We propose a simple two-step adaptive sensing andinference approach and establish theoretical guarantees for its performance;our results show that accurate outlier identification is achievable using veryfew linear summaries of the original data matrix -- as few as the squared rankof the low-rank component plus the number of outliers, times constant andlogarithmic factors. We demonstrate the performance of our approachexperimentally in two stylized applications, one motivated by robustcollaborative filtering tasks, and the other by saliency map estimation tasksarising in computer vision and automated surveillance, and also investigateextensions to settings where the data are noisy, or possibly incomplete.
arxiv-8100-153 | Scalable Topical Phrase Mining from Text Corpora | http://arxiv.org/pdf/1406.6312v2.pdf | author:Ahmed El-Kishky, Yanglei Song, Chi Wang, Clare Voss, Jiawei Han category:cs.CL cs.IR cs.LG published:2014-06-24 summary:While most topic modeling algorithms model text corpora with unigrams, humaninterpretation often relies on inherent grouping of terms into phrases. Assuch, we consider the problem of discovering topical phrases of mixed lengths.Existing work either performs post processing to the inference results ofunigram-based topic models, or utilizes complex n-gram-discovery topic models.These methods generally produce low-quality topical phrases or suffer from poorscalability on even moderately-sized datasets. We propose a different approachthat is both computationally efficient and effective. Our solution combines anovel phrase mining framework to segment a document into single and multi-wordphrases, and a new topic model that operates on the induced document partition.Our approach discovers high quality topical phrases with negligible extra costto the bag-of-words topic model in a variety of datasets including researchpublication titles, abstracts, reviews, and news articles.
arxiv-8100-154 | Salient Object Detection: A Survey | http://arxiv.org/pdf/1411.5878v1.pdf | author:Ali Borji, Ming-Ming Cheng, Huaizu Jiang, Jia Li category:cs.CV cs.AI q-bio.NC published:2014-11-18 summary:Detecting and segmenting salient objects in natural scenes, also known assalient object detection, has attracted a lot of focused research in computervision and has resulted in many applications. However, while many such modelsexist, a deep understanding of achievements and issues is lacking. We aim toprovide a comprehensive review of the recent progress in this field. We situatesalient object detection among other closely related areas such as genericscene segmentation, object proposal generation, and saliency for fixationprediction. Covering 256 publications we survey i) roots, key concepts, andtasks, ii) core techniques and main modeling trends, and iii) datasets andevaluation metrics in salient object detection. We also discuss open problemssuch as evaluation metrics and dataset bias in model performance and suggestfuture research directions.
arxiv-8100-155 | Model of Interaction between Learning and Evolution | http://arxiv.org/pdf/1411.5053v1.pdf | author:Vladimir G. Red'ko category:cs.NE published:2014-11-18 summary:The model of interaction between learning and evolutionary optimization isdesigned and investigated. The evolving population of modeled organisms isconsidered. The mechanism of the genetic assimilation of the acquired featuresduring a number of generations of Darwinian evolution is studied. It is shownthat the genetic assimilation takes place as follows: phenotypes of modeledorganisms move towards the optimum at learning; then the selection takes place;genotypes of selected organisms also move towards the optimum. The hidingeffect is also studied; this effect means that strong learning can inhibit theevolutionary search for the optimal genotype. The mechanism of influence of thelearning load on the interaction between learning and evolution is analyzed. Itis shown that the learning load can lead to a significant acceleration ofevolution.
arxiv-8100-156 | Multi-objective Reinforcement Learning with Continuous Pareto Frontier Approximation Supplementary Material | http://arxiv.org/pdf/1406.3497v2.pdf | author:Matteo Pirotta, Simone Parisi, Marcello Restelli category:cs.AI cs.LG published:2014-06-13 summary:This document contains supplementary material for the paper "Multi-objectiveReinforcement Learning with Continuous Pareto Frontier Approximation",published at the Twenty-Ninth AAAI Conference on Artificial Intelligence(AAAI-15). The paper is about learning a continuous approximation of the Paretofrontier in Multi-Objective Markov Decision Problems (MOMDPs). We propose apolicy-based approach that exploits gradient information to generate solutionsclose to the Pareto ones. Differently from previous policy-gradientmulti-objective algorithms, where n optimization routines are use to have nsolutions, our approach performs a single gradient-ascent run that at each stepgenerates an improved continuous approximation of the Pareto frontier. The ideais to exploit a gradient-based approach to optimize the parameters of afunction that defines a manifold in the policy parameter space so that thecorresponding image in the objective space gets as close as possible to thePareto frontier. Besides deriving how to compute and estimate such gradient, wewill also discuss the non-trivial issue of defining a metric to assess thequality of the candidate Pareto frontiers. Finally, the properties of theproposed approach are empirically evaluated on two interesting MOMDPs.
arxiv-8100-157 | Nonnegative Tensor Factorization for Directional Blind Audio Source Separation | http://arxiv.org/pdf/1411.5010v1.pdf | author:Noah D. Stein category:stat.ML cs.LG published:2014-11-18 summary:We augment the nonnegative matrix factorization method for audio sourceseparation with cues about directionality of sound propagation. This improvesseparation quality greatly and removes the need for training data, but doublesthe computation.
arxiv-8100-158 | Network Motifs Analysis of Croatian Literature | http://arxiv.org/pdf/1411.4960v1.pdf | author:Hana Rizvić, Sanda Martinčić-Ipšić, Ana Meštrović category:cs.CL published:2014-11-18 summary:In this paper we analyse network motifs in the co-occurrence directednetworks constructed from five different texts (four books and one portal) inthe Croatian language. After preparing the data and network construction, weperform the network motif analysis. We analyse the motif frequencies andZ-scores in the five networks. We present the triad significance profile forfive datasets. Furthermore, we compare our results with the existing resultsfor the linguistic networks. Firstly, we show that the triad significanceprofile for the Croatian language is very similar with the other languages andall the networks belong to the same family of networks. However, there arecertain differences between the Croatian language and other analysed languages.We conclude that this is due to the free word-order of the Croatian language.
arxiv-8100-159 | Designing Deep Networks for Surface Normal Estimation | http://arxiv.org/pdf/1411.4958v1.pdf | author:Xiaolong Wang, David F. Fouhey, Abhinav Gupta category:cs.CV published:2014-11-18 summary:In the past few years, convolutional neural nets (CNN) have shown incrediblepromise for learning visual representations. In this paper, we use CNNs for thetask of predicting surface normals from a single image. But what is the rightarchitecture we should use? We propose to build upon the decades of hard workin 3D scene understanding, to design new CNN architecture for the task ofsurface normal estimation. We show by incorporating several constraints(man-made, manhattan world) and meaningful intermediate representations (roomlayout, edge labels) in the architecture leads to state of the art performanceon surface normal estimation. We also show that our network is quite robust andshow state of the art results on other datasets as well without anyfine-tuning.
arxiv-8100-160 | Linguistic Descriptions for Automatic Generation of Textual Short-Term Weather Forecasts on Real Prediction Data | http://arxiv.org/pdf/1411.4925v1.pdf | author:A. Ramos-Soto, A. Bugarín, S. Barro, J. Taboada category:cs.AI cs.CL published:2014-11-18 summary:We present in this paper an application which automatically generates textualshort-term weather forecasts for every municipality in Galicia (NW Spain),using the real data provided by the Galician Meteorology Agency (MeteoGalicia).This solution combines in an innovative way computing with perceptionstechniques and strategies for linguistic description of data together with anatural language generation (NLG) system. The application, named GALiWeather,extracts relevant information from weather forecast input data and encodes itinto intermediate descriptions using linguistic variables and temporalreferences. These descriptions are later translated into natural language textsby the natural language generation system. The obtained forecast results havebeen thoroughly validated by an expert meteorologist from MeteoGalicia using aquality assessment methodology which covers two key dimensions of a text: theaccuracy of its content and the correctness of its form. Following thisvalidation GALiWeather will be released as a real service offering customforecasts for a wide public.
arxiv-8100-161 | Do More Dropouts in Pool5 Feature Maps for Better Object Detection | http://arxiv.org/pdf/1409.6911v3.pdf | author:Zhiqiang Shen, Xiangyang Xue category:cs.CV published:2014-09-24 summary:Deep Convolutional Neural Networks (CNNs) have gained great success in imageclassification and object detection. In these fields, the outputs of all layersof CNNs are usually considered as a high dimensional feature vector extractedfrom an input image and the correspondence between finer level feature vectorsand concepts that the input image contains is all-important. However, fewerstudies focus on this deserving issue. On considering the correspondence, wepropose a novel approach which generates an edited version for each originalCNN feature vector by applying the maximum entropy principle to abandonparticular vectors. These selected vectors correspond to the unfriendlyconcepts in each image category. The classifier trained from merged featuresets can significantly improve model generalization of individual categorieswhen training data is limited. The experimental results forclassification-based object detection on canonical datasets including VOC 2007(60.1%), 2010 (56.4%) and 2012 (56.3%) show obvious improvement in mean averageprecision (mAP) with simple linear support vector machines.
arxiv-8100-162 | Towards Scene Understanding with Detailed 3D Object Representations | http://arxiv.org/pdf/1411.5935v1.pdf | author:M. Zeeshan Zia, Michael Stark, Konrad Schindler category:cs.CV published:2014-11-18 summary:Current approaches to semantic image and scene understanding typically employrather simple object representations such as 2D or 3D bounding boxes. Whilesuch coarse models are robust and allow for reliable object detection, theydiscard much of the information about objects' 3D shape and pose, and thus donot lend themselves well to higher-level reasoning. Here, we propose to basescene understanding on a high-resolution object representation. An object class- in our case cars - is modeled as a deformable 3D wireframe, which enablesfine-grained modeling at the level of individual vertices and faces. We augmentthat model to explicitly include vertex-level occlusion, and embed allinstances in a common coordinate frame, in order to infer and exploitobject-object interactions. Specifically, from a single view we jointlyestimate the shapes and poses of multiple objects in a common 3D frame. Aground plane in that frame is estimated by consensus among different objects,which significantly stabilizes monocular 3D pose estimation. The fine-grainedmodel, in conjunction with the explicit 3D scene model, further allows one toinfer part-level occlusions between the modeled objects, as well as occlusionsby other, unmodeled scene elements. To demonstrate the benefits of suchdetailed object class models in the context of scene understanding wesystematically evaluate our approach on the challenging KITTI street scenedataset. The experiments show that the model's ability to utilize imageevidence at the level of individual parts improves monocular 3D pose estimationw.r.t. both location and (continuous) viewpoint.
arxiv-8100-163 | A Multi-Plane Block-Coordinate Frank-Wolfe Algorithm for Training Structural SVMs with a Costly max-Oracle | http://arxiv.org/pdf/1408.6804v2.pdf | author:Neel Shah, Vladimir Kolmogorov, Christoph H. Lampert category:cs.LG published:2014-08-28 summary:Structural support vector machines (SSVMs) are amongst the best performingmodels for structured computer vision tasks, such as semantic imagesegmentation or human pose estimation. Training SSVMs, however, iscomputationally costly, because it requires repeated calls to a structuredprediction subroutine (called \emph{max-oracle}), which has to solve anoptimization problem itself, e.g. a graph cut. In this work, we introduce a new algorithm for SSVM training that is moreefficient than earlier techniques when the max-oracle is computationallyexpensive, as it is frequently the case in computer vision tasks. The main ideais to (i) combine the recent stochastic Block-Coordinate Frank-Wolfe algorithmwith efficient hyperplane caching, and (ii) use an automatic selection rule fordeciding whether to call the exact max-oracle or to rely on an approximate onebased on the cached hyperplanes. We show experimentally that this strategy leads to faster convergence to theoptimum with respect to the number of requires oracle calls, and that thistranslates into faster convergence with respect to the total runtime when themax-oracle is slow compared to the other steps of the algorithm. A publicly available C++ implementation is provided athttp://pub.ist.ac.at/~vnk/papers/SVM.html .
arxiv-8100-164 | Music Data Analysis: A State-of-the-art Survey | http://arxiv.org/pdf/1411.5014v1.pdf | author:Shubhanshu Gupta category:cs.DB cs.LG cs.SD 97M80 H.5.5; J.5 published:2014-11-18 summary:Music accounts for a significant chunk of interest among various onlineactivities. This is reflected by wide array of alternatives offered in musicrelated web/mobile apps, information portals, featuring millions of artists,songs and events attracting user activity at similar scale. Availability oflarge scale structured and unstructured data has attracted similar level ofattention by data science community. This paper attempts to offer currentstate-of-the-art in music related analysis. Various approaches involvingmachine learning, information theory, social network analysis, semantic web andlinked open data are represented in the form of taxonomy along with datasources and use cases addressed by the research community.
arxiv-8100-165 | Simple connectome inference from partial correlation statistics in calcium imaging | http://arxiv.org/pdf/1406.7865v4.pdf | author:Antonio Sutera, Arnaud Joly, Vincent François-Lavet, Zixiao Aaron Qiu, Gilles Louppe, Damien Ernst, Pierre Geurts category:stat.ML cs.CE cs.LG published:2014-06-30 summary:In this work, we propose a simple yet effective solution to the problem ofconnectome inference in calcium imaging data. The proposed algorithm consistsof two steps. First, processing the raw signals to detect neural peakactivities. Second, inferring the degree of association between neurons frompartial correlation statistics. This paper summarises the methodology that ledus to win the Connectomics Challenge, proposes a simplified version of ourmethod, and finally compares our results with respect to other inferencemethods.
arxiv-8100-166 | The NLMS algorithm with time-variant optimum stepsize derived from a Bayesian network perspective | http://arxiv.org/pdf/1411.4834v1.pdf | author:Christian Huemmer, Roland Maas, Walter Kellermann category:stat.ML published:2014-11-18 summary:In this article, we derive a new stepsize adaptation for the normalized leastmean square algorithm (NLMS) by describing the task of linear acoustic echocancellation from a Bayesian network perspective. Similar to the well-knownKalman filter equations, we model the acoustic wave propagation from theloudspeaker to the microphone by a latent state vector and define a linearobservation equation (to model the relation between the state vector and theobservation) as well as a linear process equation (to model the temporalprogress of the state vector). Based on additional assumptions on thestatistics of the random variables in observation and process equation, weapply the expectation-maximization (EM) algorithm to derive an NLMS-like filteradaptation. By exploiting the conditional independence rules for Bayesiannetworks, we reveal that the resulting EM-NLMS algorithm has a stepsize updateequivalent to the optimal-stepsize calculation proposed by Yamamoto andKitayama in 1982, which has been adopted in many textbooks. As main difference,the instantaneous stepsize value is estimated in the M step of the EM algorithm(instead of being approximated by artificially extending the acoustic echopath). The EM-NLMS algorithm is experimentally verified for synthesizedscenarios with both, white noise and male speech as input signal.
arxiv-8100-167 | Cognitive Systems and Question Answering | http://arxiv.org/pdf/1411.4825v1.pdf | author:Ulrich Furbach, Claudia Schon, Frieder Stolzenburg category:cs.AI cs.CL published:2014-11-18 summary:This paper briefly characterizes the field of cognitive computing. As anexemplification, the field of natural language question answering is introducedtogether with its specific challenges. A possibility to master these challengesis illustrated by a detailed presentation of the LogAnswer system, which is asuccessful representative of the field of natural language question answering.
arxiv-8100-168 | Outlier-Robust Convex Segmentation | http://arxiv.org/pdf/1411.4503v2.pdf | author:Itamar Katz, Koby Crammer category:cs.LG stat.ML published:2014-11-17 summary:We derive a convex optimization problem for the task of segmenting sequentialdata, which explicitly treats presence of outliers. We describe two algorithmsfor solving this problem, one exact and one a top-down novel approach, and wederive a consistency results for the case of two segments and no outliers.Robustness to outliers is evaluated on two real-world tasks related to speechsegmentation. Our algorithms outperform baseline segmentation algorithms.
arxiv-8100-169 | Sparse Generalized Eigenvalue Problem via Smooth Optimization | http://arxiv.org/pdf/1408.6686v2.pdf | author:Junxiao Song, Prabhu Babu, Daniel P. Palomar category:stat.ML cs.LG published:2014-08-28 summary:In this paper, we consider an $\ell_{0}$-norm penalized formulation of thegeneralized eigenvalue problem (GEP), aimed at extracting the leading sparsegeneralized eigenvector of a matrix pair. The formulation involves maximizationof a discontinuous nonconcave objective function over a nonconvex constraintset, and is therefore computationally intractable. To tackle the problem, wefirst approximate the $\ell_{0}$-norm by a continuous surrogate function. Thenan algorithm is developed via iteratively majorizing the surrogate function bya quadratic separable function, which at each iteration reduces to a regulargeneralized eigenvalue problem. A preconditioned steepest ascent algorithm forfinding the leading generalized eigenvector is provided. A systematic way basedon smoothing is proposed to deal with the "singularity issue" that arises whena quadratic function is used to majorize the nondifferentiable surrogatefunction. For sparse GEPs with special structure, algorithms that admit aclosed-form solution at every iteration are derived. Numerical experiments showthat the proposed algorithms match or outperform existing algorithms in termsof computational complexity and support recovery.
arxiv-8100-170 | Smoothed Gradients for Stochastic Variational Inference | http://arxiv.org/pdf/1406.3650v2.pdf | author:Stephan Mandt, David Blei category:stat.ML cs.LG published:2014-06-13 summary:Stochastic variational inference (SVI) lets us scale up Bayesian computationto massive data. It uses stochastic optimization to fit a variationaldistribution, following easy-to-compute noisy natural gradients. As with mosttraditional stochastic optimization methods, SVI takes precautions to useunbiased stochastic gradients whose expectations are equal to the truegradients. In this paper, we explore the idea of following biased stochasticgradients in SVI. Our method replaces the natural gradient with a similarlyconstructed vector that uses a fixed-window moving average of some of itsprevious terms. We will demonstrate the many advantages of this technique.First, its computational cost is the same as for SVI and storage requirementsonly multiply by a constant factor. Second, it enjoys significant variancereduction over the unbiased estimates, smaller bias than averaged gradients,and leads to smaller mean-squared error against the full gradient. We test ourmethod on latent Dirichlet allocation with three large corpora.
arxiv-8100-171 | Faithful Variable Screening for High-Dimensional Convex Regression | http://arxiv.org/pdf/1411.1805v2.pdf | author:Min Xu, Minhua Chen, John Lafferty category:math.ST stat.ML stat.TH published:2014-11-07 summary:We study the problem of variable selection in convex nonparametricregression. Under the assumption that the true regression function is convexand sparse, we develop a screening procedure to select a subset of variablesthat contains the relevant variables. Our approach is a two-stage quadraticprogramming method that estimates a sum of one-dimensional convex functions,followed by one-dimensional concave regression fits on the residuals. Incontrast to previous methods for sparse additive models, the optimization isfinite dimensional and requires no tuning parameters for smoothness. Underappropriate assumptions, we prove that the procedure is faithful in thepopulation setting, yielding no false negatives. We give a finite samplestatistical analysis, and introduce algorithms for efficiently carrying out therequired quadratic programs. The approach leads to computational andstatistical advantages over fitting a full model, and provides an effective,practical approach to variable screening in convex regression.
arxiv-8100-172 | Toward a Universal Cortical Algorithm: Examining Hierarchical Temporal Memory in Light of Frontal Cortical Function | http://arxiv.org/pdf/1411.4702v1.pdf | author:Michael R. Ferrier category:q-bio.NC cs.AI cs.NE I.2.6 published:2014-11-18 summary:A wide range of evidence points toward the existence of a common algorithmunderlying the processing of information throughout the cerebral cortex.Several hypothesized features of this cortical algorithm are reviewed,including sparse distributed representation, Bayesian inference, hierarchicalorganization composed of alternating template matching and pooling layers,temporal slowness and predictive coding. Hierarchical Temporal Memory (HTM) isa family of learning algorithms and corresponding theories of cortical functionthat embodies these principles. HTM has previously been applied mainly toperceptual tasks typical of posterior cortex. In order to evaluate HTM as acandidate model of cortical function, it is necessary also to investigate itscompatibility with the requirements of frontal cortical function. To this end,a variety of models of frontal cortical function are reviewed and integrated,to arrive at the hypothesis that frontal functions including attention, workingmemory and action selection depend largely upon the same basic algorithms as doposterior functions, with the notable additions of a mechanism for the activemaintenance of representations and of multiple cortico-striato-thalamo-corticalloops that allow communication between regions of frontal cortex to be gated inan adaptive manner. Computational models of this system are reviewed. Finally,there is a discussion of how HTM can contribute to the understanding of frontalcortical function, and of what the requirements of frontal cortical functionmean for the future development of HTM.
arxiv-8100-173 | Structured Hough Voting for Vision-based Highway Border Detection | http://arxiv.org/pdf/1411.4701v1.pdf | author:Zhiding Yu, Wende Zhang, B. V. K. Vijaya Kumar, Dan Levi category:cs.CV cs.RO published:2014-11-18 summary:We propose a vision-based highway border detection algorithm using structuredHough voting. Our approach takes advantage of the geometric relationshipbetween highway road borders and highway lane markings. It uses a strategywhere a number of trained road border and lane marking detectors are triggered,followed by Hough voting to generate corresponding detection of the border andlane marking. Since the initially triggered detectors usually result in largenumber of positives, conventional frame-wise Hough voting is not able to alwaysgenerate robust border and lane marking results. Therefore, we formulate thisproblem as a joint detection-and-tracking problem under the structured Houghvoting model, where tracking refers to exploiting inter-frame structuralinformation to stabilize the detection results. Both qualitative andquantitative evaluations show the superiority of the proposed structured Houghvoting model over a number of baseline methods.
arxiv-8100-174 | Feedback Solution to Optimal Switching Problems with Switching Cost | http://arxiv.org/pdf/1411.4695v1.pdf | author:Ali Heydari category:cs.SY math.OC stat.ML published:2014-11-17 summary:The problem of optimal switching between nonlinear autonomous subsystems isinvestigated in this study where the objective is not only bringing the statesto close to the desired point, but also adjusting the switching pattern, in thesense of penalizing switching occurrences and assigning different preferencesto utilization of different modes. The mode sequence is unspecified and aswitching cost term is used in the cost function for penalizing each switching.It is shown that once a switching cost is incorporated, the optimal cost-to-gofunction depends on the already active subsystem, i.e., the subsystem which wasengaged in the previous time step. Afterwards, an approximate dynamicprogramming based method is developed which provides an approximation of theoptimal solution to the problem in a feedback form and for different initialconditions. Finally, the performance of the method is analyzed throughnumerical examples.
arxiv-8100-175 | Group Regularized Estimation under Structural Hierarchy | http://arxiv.org/pdf/1411.4691v1.pdf | author:Yiyuan She, He Jiang category:math.ST stat.CO stat.ML stat.TH published:2014-11-17 summary:In high-dimensional models that involve interactions, statisticians usuallyfavor variable selection obeying certain logical hierarchical constraints. Thispaper focuses on structural hierarchy which means that the existence of aninteraction term implies that at least one or both associated main effects mustbe present. Lately this problem has attracted a lot of attentions fromstatisticians, but existing computational algorithms converge slow and cannotmeet the challenge of big data computation. More importantly, theoreticalstudies of hierarchical variable selection are extremely scarce, largely due tothe difficulty that multiple sparsity-promoting penalties are enforced on thesame subject. This work investigates a new type of estimator based on groupmulti-regularization to capture various types of structural parsimonysimultaneously. We present non-asymptotic results based on combined statisticaland computational analysis, and reveal the minimax optimal rate. Ageneral-purpose algorithm is developed with a theoretical guarantee of strictiterate convergence and global optimality. Simulations and real dataexperiments demonstrate the efficiency and efficacy of the proposed approach.
arxiv-8100-176 | Pseudo Dynamic Transitional Modeling of Building Heating Energy Demand Using Artificial Neural Network | http://arxiv.org/pdf/1411.4679v1.pdf | author:S. Paudel, M. Elmtiri, W. L. Kling, O. Le Corre, B. Lacarriere category:cs.CE cs.NE published:2014-11-17 summary:This paper presents the building heating demand prediction model withoccupancy profile and operational heating power level characteristics in shorttime horizon (a couple of days) using artificial neural network. In addition,novel pseudo dynamic transitional model is introduced, which consider timedependent attributes of operational power level characteristics and its effectin the overall model performance is outlined. Pseudo dynamic model is appliedto a case study of French Institution building and compared its results withstatic and other pseudo dynamic neural network models. The results show thecoefficients of correlation in static and pseudo dynamic neural network modelof 0.82 and 0.89 (with energy consumption error of 0.02%) during the learningphase, and 0.61 and 0.85 during the prediction phase respectively. Further,orthogonal array design is applied to the pseudo dynamic model to check theschedule of occupancy profile and operational heating power levelcharacteristics. The results show the new schedule and provide the robustdesign for pseudo dynamic model. Due to prediction in short time horizon, itfinds application for Energy Services Company (ESCOs) to manage the heatingload for dynamic control of heat production system.
arxiv-8100-177 | AlexU-Word: A New Dataset for Isolated-Word Closed-Vocabulary Offline Arabic Handwriting Recognition | http://arxiv.org/pdf/1411.4670v1.pdf | author:Mohamed E. Hussein, Marwan Torki, Ahmed Elsallamy, Mahmoud Fayyaz category:cs.CV I.5.2; I.7.5 published:2014-11-17 summary:In this paper, we introduce the first phase of a new dataset for offlineArabic handwriting recognition. The aim is to collect a very large dataset ofisolated Arabic words that covers all letters of the alphabet in all possibleshapes using a small number of simple words. The end goal is to collect a verylarge dataset of segmented letter images, which can be used to build andevaluate Arabic handwriting recognition systems that are based on segmentedletter recognition. The current version of the dataset contains $25114$ samplesof $109$ unique Arabic words that cover all possible shapes of all alphabetletters. The samples were collected from $907$ writers. In its current form,the dataset can be used for the problem of closed-vocabulary word recognition.We evaluated a number of window-based descriptors and classifiers on this taskand obtained an accuracy of $92.16\%$ using a SIFT-based descriptor and ANN.
arxiv-8100-178 | A unifying framework for relaxations of the causal assumptions in Bell's theorem | http://arxiv.org/pdf/1411.4648v1.pdf | author:Rafael Chaves, Richard Kueng, Jonatan Bohr Brask, David Gross category:quant-ph stat.ML published:2014-11-17 summary:Bell's Theorem shows that quantum mechanical correlations can violate theconstraints that the causal structure of certain experiments impose on anyclassical explanation. It is thus natural to ask to which degree the causalassumptions -- e.g. locality or measurement independence -- have to be relaxedin order to allow for a classical description of such experiments. Here, wedevelop a conceptual and computational framework for treating this problem. Weemploy the language of Bayesian networks to systematically constructalternative causal structures and bound the degree of relaxation usingquantitative measures that originate from the mathematical theory of causality.The main technical insight is that the resulting problems can often beexpressed as computationally tractable linear programs. We demonstrate theversatility of the framework by applying it to a variety of scenarios, rangingfrom relaxations of the measurement independence, locality and bilocalityassumptions, to a novel causal interpretation of CHSH inequality violations.
arxiv-8100-179 | State-of-the-Art in Retinal Optical Coherence Tomography Image Analysis | http://arxiv.org/pdf/1411.0740v2.pdf | author:Ahmadreza Baghaie, Roshan M. D'souza, Zeyun Yu category:cs.CV published:2014-11-04 summary:Optical Coherence Tomography (OCT) is one of the most emerging imagingmodalities that has been used widely in the field of biomedical imaging. Fromits emergence in 1990's, plenty of hardware and software improvements have beenmade. Its applications range from ophthalmology to dermatology to coronaryimaging etc. Here, the focus is on applications of OCT in ophthalmology andretinal imaging. OCT is able to non-invasively produce cross-sectional volumeimages of the tissues which are further used for analysis of the tissuestructure and its properties. Due to the underlying physics, OCT images usuallysuffer from a granular pattern, called speckle noise, which restricts theprocess of interpretation, hence requiring specialized noise reductiontechniques to remove the noise while preserving image details. Also, given thefact that OCT images are in the $\mu m$ -level, further analysis in needed todistinguish between the different structures in the imaged volume. Thereforethe use of different segmentation techniques are of high importance. Themovement of the tissue under imaging or the progression of disease in thetissue also imposes further implications both on the quality and the properinterpretation of the acquired images. Thus, use of image registrationtechniques can be very helpful. In this work, an overview of such imageanalysis techniques will be given.
arxiv-8100-180 | Relations World: A Possibilistic Graphical Model | http://arxiv.org/pdf/1411.4618v1.pdf | author:Christopher J. C. Burges, Erin Renshaw, Andrzej Pastusiak category:cs.CL cs.AI published:2014-11-17 summary:We explore the idea of using a "possibilistic graphical model" as the basisfor a world model that drives a dialog system. As a first step we havedeveloped a system that uses text-based dialog to derive a model of the user'sfamily relations. The system leverages its world model to infer relationaltriples, to learn to recover from upstream coreference resolution errors andambiguities, and to learn context-dependent paraphrase models. We also exploresome theoretical aspects of the underlying graphical model.
arxiv-8100-181 | Joint Association Graph Screening and Decomposition for Large-scale Linear Dynamical Systems | http://arxiv.org/pdf/1411.4598v1.pdf | author:Yiyuan She, Yuejia He, Shijie Li, Dapeng Wu category:stat.CO stat.ML published:2014-11-17 summary:This paper studies large-scale dynamical networks where the current state ofthe system is a linear transformation of the previous state, contaminated by amultivariate Gaussian noise. Examples include stock markets, human brains andgene regulatory networks. We introduce a transition matrix to describe theevolution, which can be translated to a directed Granger transition graph, anduse the concentration matrix of the Gaussian noise to capture the second-orderrelations between nodes, which can be translated to an undirected conditionaldependence graph. We propose regularizing the two graphs jointly in topologyidentification and dynamics estimation. Based on the notion of jointassociation graph (JAG), we develop a joint graphical screening and estimation(JGSE) framework for efficient network learning in big data. In particular, ourmethod can pre-determine and remove unnecessary edges based on the jointgraphical structure, referred to as JAG screening, and can decompose a largenetwork into smaller subnetworks in a robust manner, referred to as JAGdecomposition. JAG screening and decomposition can reduce the problem size andsearch space for fine estimation at a later stage. Experiments on bothsynthetic data and real-world applications show the effectiveness of theproposed framework in large-scale network topology identification and dynamicsestimation.
arxiv-8100-182 | Window-Based Descriptors for Arabic Handwritten Alphabet Recognition: A Comparative Study on a Novel Dataset | http://arxiv.org/pdf/1411.3519v2.pdf | author:Marwan Torki, Mohamed E. Hussein, Ahmed Elsallamy, Mahmoud Fayyaz, Shehab Yaser category:cs.CV I.5.2; I.7.5 published:2014-11-13 summary:This paper presents a comparative study for window-based descriptors on theapplication of Arabic handwritten alphabet recognition. We show a detailedexperimental evaluation of different descriptors with several classifiers. Theobjective of the paper is to evaluate different window-based descriptors on theproblem of Arabic letter recognition. Our experiments clearly show that theyperform very well. Moreover, we introduce a novel spatial pyramid partitioningscheme that enhances the recognition accuracy for most descriptors. Inaddition, we introduce a novel dataset for Arabic handwritten isolated alphabetletters, which can serve as a benchmark for future research.
arxiv-8100-183 | A Parallel Genetic Algorithm for Three Dimensional Bin Packing with Heterogeneous Bins | http://arxiv.org/pdf/1411.4565v1.pdf | author:Drona Pratap Chandu category:cs.DC cs.NE published:2014-11-17 summary:This paper presents a parallel genetic algorithm for three dimensional binpacking with heterogeneous bins using Hadoop Map-Reduce framework. The mostcommon three dimensional bin packing problem which packs given set of boxesinto minimum number of equal sized bins is proven to be NP Hard. The variationof three dimensional bin packing problem that allows heterogeneous bin sizesand rotation of boxes is computationally more harder than common threedimensional bin packing problem. The proposed Map-Reduce implementation helpsto run the genetic algorithm for three dimensional bin packing withheterogeneous bins on multiple machines parallely and computes the solution inrelatively short time.
arxiv-8100-184 | Stochastic Blockmodeling for Online Advertising | http://arxiv.org/pdf/1410.6714v2.pdf | author:Li Chen, Matthew Patton category:stat.ML stat.AP published:2014-10-24 summary:Online advertising is an important and huge industry. Having knowledge of thewebsite attributes can contribute greatly to business strategies forad-targeting, content display, inventory purchase or revenue prediction.Classical inferences on users and sites impose challenge, because the data isvoluminous, sparse, high-dimensional and noisy. In this paper, we introduce astochastic blockmodeling for the website relations induced by the event ofonline user visitation. We propose two clustering algorithms to discover theinstrinsic structures of websites, and compare the performance with agoodness-of-fit method and a deterministic graph partitioning method. Wedemonstrate the effectiveness of our algorithms on both simulation and AOLwebsite dataset.
arxiv-8100-185 | Implicitly Constrained Semi-Supervised Linear Discriminant Analysis | http://arxiv.org/pdf/1411.4521v1.pdf | author:Jesse H. Krijthe, Marco Loog category:stat.ML cs.LG published:2014-11-17 summary:Semi-supervised learning is an important and active topic of research inpattern recognition. For classification using linear discriminant analysisspecifically, several semi-supervised variants have been proposed. Using anyone of these methods is not guaranteed to outperform the supervised classifierwhich does not take the additional unlabeled data into account. In this work wecompare traditional Expectation Maximization type approaches forsemi-supervised linear discriminant analysis with approaches based on intrinsicconstraints and propose a new principled approach for semi-supervised lineardiscriminant analysis, using so-called implicit constraints. We explore therelationships between these methods and consider the question if and in whatsense we can expect improvement in performance over the supervised procedure.The constraint based approaches are more robust to misspecification of themodel, and may outperform alternatives that make more assumptions on the data,in terms of the log-likelihood of unseen objects.
arxiv-8100-186 | Parallel Gaussian Process Regression for Big Data: Low-Rank Representation Meets Markov Approximation | http://arxiv.org/pdf/1411.4510v1.pdf | author:Kian Hsiang Low, Jiangbo Yu, Jie Chen, Patrick Jaillet category:stat.ML cs.DC cs.LG published:2014-11-17 summary:The expressive power of a Gaussian process (GP) model comes at a cost of poorscalability in the data size. To improve its scalability, this paper presents alow-rank-cum-Markov approximation (LMA) of the GP model that is novel inleveraging the dual computational advantages stemming from complementing alow-rank approximate representation of the full-rank GP based on a support setof inputs with a Markov approximation of the resulting residual process; thelatter approximation is guaranteed to be closest in the Kullback-Leiblerdistance criterion subject to some constraint and is considerably more refinedthan that of existing sparse GP models utilizing low-rank representations dueto its more relaxed conditional independence assumption (especially with largerdata). As a result, our LMA method can trade off between the size of thesupport set and the order of the Markov property to (a) incur lowercomputational cost than such sparse GP models while achieving predictiveperformance comparable to them and (b) accurately represent features/patternsof any scale. Interestingly, varying the Markov order produces a spectrum ofLMAs with PIC approximation and full-rank GP at the two extremes. An advantageof our LMA method is that it is amenable to parallelization on multiplemachines/cores, thereby gaining greater scalability. Empirical evaluation onthree real-world datasets in clusters of up to 32 computing nodes shows thatour centralized and parallel LMA methods are significantly more time-efficientand scalable than state-of-the-art sparse and full-rank GP regression methodswhile achieving comparable predictive performances.
arxiv-8100-187 | Spectral Unmixing via Data-guided Sparsity | http://arxiv.org/pdf/1403.3155v4.pdf | author:Feiyun Zhu, Ying Wang, Bin Fan, Gaofeng Meng, Shiming Xiang, Chunhong Pan category:cs.CV published:2014-03-13 summary:Hyperspectral unmixing, the process of estimating a common set of spectralbases and their corresponding composite percentages at each pixel, is animportant task for hyperspectral analysis, visualization and understanding.From an unsupervised learning perspective, this problem is verychallenging---both the spectral bases and their composite percentages areunknown, making the solution space too large. To reduce the solution space,many approaches have been proposed by exploiting various priors. In practice,these priors would easily lead to some unsuitable solution. This is becausethey are achieved by applying an identical strength of constraints to all thefactors, which does not hold in practice. To overcome this limitation, wepropose a novel sparsity based method by learning a data-guided map to describethe individual mixed level of each pixel. Through this data-guided map, the$\ell_{p}(0<p<1)$ constraint is applied in an adaptive manner. Suchimplementation not only meets the practical situation, but also guides thespectral bases toward the pixels under highly sparse constraint. What's more,an elegant optimization scheme as well as its convergence proof have beenprovided in this paper. Extensive experiments on several datasets alsodemonstrate that the data-guided map is feasible, and high quality unmixingresults could be obtained by our method.
arxiv-8100-188 | 10,000+ Times Accelerated Robust Subset Selection (ARSS) | http://arxiv.org/pdf/1409.3660v4.pdf | author:Feiyun Zhu, Bin Fan, Xinliang Zhu, Ying Wang, Shiming Xiang, Chunhong Pan category:cs.LG cs.CV stat.ML published:2014-09-12 summary:Subset selection from massive data with noised information is increasinglypopular for various applications. This problem is still highly challenging ascurrent methods are generally slow in speed and sensitive to outliers. Toaddress the above two issues, we propose an accelerated robust subset selection(ARSS) method. Specifically in the subset selection area, this is the firstattempt to employ the $\ell_{p}(0<p\leq1)$-norm based measure for therepresentation loss, preventing large errors from dominating our objective. Asa result, the robustness against outlier elements is greatly enhanced.Actually, data size is generally much larger than feature length, i.e. $N\ggL$. Based on this observation, we propose a speedup solver (via ALM andequivalent derivations) to highly reduce the computational cost, theoreticallyfrom $O(N^{4})$ to $O(N{}^{2}L)$. Extensive experiments on ten benchmarkdatasets verify that our method not only outperforms state of the art methods,but also runs 10,000+ times faster than the most related method.
arxiv-8100-189 | Opinion mining of text documents written in Macedonian language | http://arxiv.org/pdf/1411.4472v1.pdf | author:Andrej Gajduk, Ljupco Kocarev category:cs.CL published:2014-11-17 summary:The ability to extract public opinion from web portals such as review sites,social networks and blogs will enable companies and individuals to form a view,an attitude and make decisions without having to do lengthy and costlyresearches and surveys. In this paper machine learning techniques are used fordetermining the polarity of forum posts on kajgana which are written inMacedonian language. The posts are classified as being positive, negative orneutral. We test different feature metrics and classifiers and provide detailedevaluation of their participation in improving the overall performance on amanually generated dataset. By achieving 92% accuracy, we show that theperformance of systems for automated opinion mining is comparable to a humanevaluator, thus making it a viable option for text data analysis. Finally, wepresent a few statistics derived from the forum posts using the developedsystem.
arxiv-8100-190 | Fully Convolutional Neural Networks for Crowd Segmentation | http://arxiv.org/pdf/1411.4464v1.pdf | author:Kai Kang, Xiaogang Wang category:cs.CV published:2014-11-17 summary:In this paper, we propose a fast fully convolutional neural network (FCNN)for crowd segmentation. By replacing the fully connected layers in CNN with 1by 1 convolution kernels, FCNN takes whole images as inputs and directlyoutputs segmentation maps by one pass of forward propagation. It has theproperty of translation invariance like patch-by-patch scanning but with muchlower computation cost. Once FCNN is learned, it can process input images ofany sizes without warping them to a standard size. These attractive propertiesmake it extendable to other general image segmentation problems. Based on FCNN,a multi-stage deep learning is proposed to integrate appearance and motion cuesfor crowd segmentation. Both appearance filters and motion filers arepretrained stage-by-stage and then jointly optimized. Different combinationmethods are investigated. The effectiveness of our approach and component-wiseanalysis are evaluated on two crowd segmentation datasets created by us, whichinclude image frames from 235 and 11 scenes, respectively. They are currentlythe largest crowd segmentation datasets and will be released to the public.
arxiv-8100-191 | Errata: Distant Supervision for Relation Extraction with Matrix Completion | http://arxiv.org/pdf/1411.4455v1.pdf | author:Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu, Thomas Fang Zheng, Edward Y. Chang category:cs.CL cs.LG published:2014-11-17 summary:The essence of distantly supervised relation extraction is that it is anincomplete multi-label classification problem with sparse and noisy features.To tackle the sparsity and noise challenges, we propose solving theclassification problem using matrix completion on factorized matrix ofminimized rank. We formulate relation classification as completing the unknownlabels of testing items (entity pairs) in a sparse matrix that concatenatestraining and testing textual features with training labels. Our algorithmicframework is based on the assumption that the rank of item-by-feature anditem-by-label joint matrix is low. We apply two optimization models to recoverthe underlying low-rank matrix leveraging the sparsity of feature-label matrix.The matrix completion problem is then solved by the fixed point continuation(FPC) algorithm, which can find the global optimum. Experiments on two widelyused datasets with different dimensions of textual features demonstrate thatour low-rank matrix completion approach significantly outperforms the baselineand the state-of-the-art methods.
arxiv-8100-192 | Efficient On-the-fly Category Retrieval using ConvNets and GPUs | http://arxiv.org/pdf/1407.4764v3.pdf | author:Ken Chatfield, Karen Simonyan, Andrew Zisserman category:cs.CV cs.LG cs.NE published:2014-07-17 summary:We investigate the gains in precision and speed, that can be obtained byusing Convolutional Networks (ConvNets) for on-the-fly retrieval - whereclassifiers are learnt at run time for a textual query from downloaded images,and used to rank large image or video datasets. We make three contributions: (i) we present an evaluation of state-of-the-artimage representations for object category retrieval over standard benchmarkdatasets containing 1M+ images; (ii) we show that ConvNets can be used toobtain features which are incredibly performant, and yet much lower dimensionalthan previous state-of-the-art image representations, and that theirdimensionality can be reduced further without loss in performance bycompression using product quantization or binarization. Consequently, featureswith the state-of-the-art performance on large-scale datasets of millions ofimages can fit in the memory of even a commodity GPU card; (iii) we show thatan SVM classifier can be learnt within a ConvNet framework on a GPU in parallelwith downloading the new training images, allowing for a continuous refinementof the model as more images become available, and simultaneous training andranking. The outcome is an on-the-fly system that significantly outperforms itspredecessors in terms of: precision of retrieval, memory requirements, andspeed, facilitating accurate on-the-fly learning and ranking in under a secondon a single GPU.
arxiv-8100-193 | FGPGA: An Efficient Genetic Approach for Producing Feasible Graph Partitions | http://arxiv.org/pdf/1411.4379v1.pdf | author:Md. Lisul Islam, Novia Nurain, Swakkhar Shatabda, M Sohel Rahman category:cs.NE cs.AI cs.DC published:2014-11-17 summary:Graph partitioning, a well studied problem of parallel computing has manyapplications in diversified fields such as distributed computing, socialnetwork analysis, data mining and many other domains. In this paper, weintroduce FGPGA, an efficient genetic approach for producing feasible graphpartitions. Our method takes into account the heterogeneity and capacityconstraints of the partitions to ensure balanced partitioning. Such approachhas various applications in mobile cloud computing that include feasibledeployment of software applications on the more resourceful infrastructure inthe cloud instead of mobile hand set. Our proposed approach is light weight andhence suitable for use in cloud architecture. We ensure feasibility of thepartitions generated by not allowing over-sized partitions to be generatedduring the initialization and search. Our proposed method tested on standardbenchmark datasets significantly outperforms the state-of-the-art methods interms of quality of partitions and feasibility of the solutions.
arxiv-8100-194 | Robust Kernel Density Estimation by Scaling and Projection in Hilbert Space | http://arxiv.org/pdf/1411.4378v1.pdf | author:Robert A. Vandermeulen, Clayton D. Scott category:stat.ML published:2014-11-17 summary:While robust parameter estimation has been well studied in parametric densityestimation, there has been little investigation into robust density estimationin the nonparametric setting. We present a robust version of the popular kerneldensity estimator (KDE). As with other estimators, a robust version of the KDEis useful since sample contamination is a common issue with datasets. What"robustness" means for a nonparametric density estimate is not straightforwardand is a topic we explore in this paper. To construct a robust KDE we scale thetraditional KDE and project it to its nearest weighted KDE in the $L^2$ norm.This yields a scaled and projected KDE (SPKDE). Because the squared $L^2$ normpenalizes point-wise errors superlinearly this causes the weighted KDE toallocate more weight to high density regions. We demonstrate the robustness ofthe SPKDE with numerical experiments and a consistency result which shows thatasymptotically the SPKDE recovers the uncontaminated density under sufficientconditions on the contamination.
arxiv-8100-195 | Geodesic Exponential Kernels: When Curvature and Linearity Conflict | http://arxiv.org/pdf/1411.0296v2.pdf | author:Aasa Feragen, Francois Lauze, Søren Hauberg category:cs.LG cs.CV published:2014-11-02 summary:We consider kernel methods on general geodesic metric spaces and provide bothnegative and positive results. First we show that the common Gaussian kernelcan only be generalized to a positive definite kernel on a geodesic metricspace if the space is flat. As a result, for data on a Riemannian manifold, thegeodesic Gaussian kernel is only positive definite if the Riemannian manifoldis Euclidean. This implies that any attempt to design geodesic Gaussian kernelson curved Riemannian manifolds is futile. However, we show that for spaces withconditionally negative definite distances the geodesic Laplacian kernel can begeneralized while retaining positive definiteness. This implies that geodesicLaplacian kernels can be generalized to some curved spaces, including spheresand hyperbolic spaces. Our theoretical results are verified empirically.
arxiv-8100-196 | From Manifold to Manifold: Geometry-Aware Dimensionality Reduction for SPD Matrices | http://arxiv.org/pdf/1407.1120v2.pdf | author:Mehrtash T. Harandi, Mathieu Salzmann, Richard Hartley category:cs.CV published:2014-07-04 summary:Representing images and videos with Symmetric Positive Definite (SPD)matrices and considering the Riemannian geometry of the resulting space hasproven beneficial for many recognition tasks. Unfortunately, computation on theRiemannian manifold of SPD matrices --especially of high-dimensional ones--comes at a high cost that limits the applicability of existing techniques. Inthis paper we introduce an approach that lets us handle high-dimensional SPDmatrices by constructing a lower-dimensional, more discriminative SPD manifold.To this end, we model the mapping from the high-dimensional SPD manifold to thelow-dimensional one with an orthonormal projection. In particular, we searchfor a projection that yields a low-dimensional manifold with maximumdiscriminative power encoded via an affinity-weighted similarity measure basedon metrics on the manifold. Learning can then be expressed as an optimizationproblem on a Grassmann manifold. Our evaluation on several classification tasksshows that our approach leads to a significant accuracy gain overstate-of-the-art methods.
arxiv-8100-197 | A Latent Clothing Attribute Approach for Human Pose Estimation | http://arxiv.org/pdf/1411.4331v1.pdf | author:Weipeng Zhang, Jie Shen, Guangcan Liu, Yong Yu category:cs.CV published:2014-11-16 summary:As a fundamental technique that concerns several vision tasks such as imageparsing, action recognition and clothing retrieval, human pose estimation (HPE)has been extensively investigated in recent years. To achieve accurate andreliable estimation of the human pose, it is well-recognized that the clothingattributes are useful and should be utilized properly. Most previousapproaches, however, require to manually annotate the clothing attributes andare therefore very costly. In this paper, we shall propose and explore a\emph{latent} clothing attribute approach for HPE. Unlike previous approaches,our approach models the clothing attributes as latent variables and thusrequires no explicit labeling for the clothing attributes. The inference of thelatent variables are accomplished by utilizing the framework of latentstructured support vector machines (LSSVM). We employ the strategy of\emph{alternating direction} to train the LSSVM model: In each iteration, onekind of variables (e.g., human pose or clothing attribute) are fixed and theothers are optimized. Our extensive experiments on two real-world benchmarksshow the state-of-the-art performance of our proposed approach.
arxiv-8100-198 | Empirical non-parametric estimation of the Fisher Information | http://arxiv.org/pdf/1408.1182v2.pdf | author:Visar Berisha, Alfred O. Hero category:stat.CO cs.IT math.IT stat.ML published:2014-08-06 summary:The Fisher information matrix (FIM) is a foundational concept in statisticalsignal processing. The FIM depends on the probability distribution, assumed tobelong to a smooth parametric family. Traditional approaches to estimating theFIM require estimating the probability distribution function (PDF), or itsparameters, along with its gradient or Hessian. However, in many practicalsituations the PDF of the data is not known but the statistician has access toan observation sample for any parameter value. Here we propose a method ofestimating the FIM directly from sampled data that does not require knowledgeof the underlying PDF. The method is based on non-parametric estimation of an$f$-divergence over a local neighborhood of the parameter space and a relationbetween curvature of the $f$-divergence and the FIM. Thus we obtain anempirical estimator of the FIM that does not require density estimation and isasymptotically consistent. We empirically evaluate the validity of our approachusing two experiments.
arxiv-8100-199 | Non-Standard Words as Features for Text Categorization | http://arxiv.org/pdf/1408.6746v2.pdf | author:Slobodan Beliga, Sanda Martinčić-Ipšić category:cs.CL cs.LG published:2014-08-28 summary:This paper presents categorization of Croatian texts using Non-Standard Words(NSW) as features. Non-Standard Words are: numbers, dates, acronyms,abbreviations, currency, etc. NSWs in Croatian language are determinedaccording to Croatian NSW taxonomy. For the purpose of this research, 390 textdocuments were collected and formed the SKIPEZ collection with 6 classes:official, literary, informative, popular, educational and scientific. Textcategorization experiment was conducted on three different representations ofthe SKIPEZ collection: in the first representation, the frequencies of NSWs areused as features; in the second representation, the statistic measures of NSWs(variance, coefficient of variation, standard deviation, etc.) are used asfeatures; while the third representation combines the first two feature sets.Naive Bayes, CN2, C4.5, kNN, Classification Trees and Random Forest algorithmswere used in text categorization experiments. The best categorization resultsare achieved using the first feature set (NSW frequencies) with thecategorization accuracy of 87%. This suggests that the NSWs should beconsidered as features in highly inflectional languages, such as Croatian. NSWbased features reduce the dimensionality of the feature space without standardlemmatization procedures, and therefore the bag-of-NSWs should be consideredfor further Croatian texts categorization experiments.
arxiv-8100-200 | Ten Years of Pedestrian Detection, What Have We Learned? | http://arxiv.org/pdf/1411.4304v1.pdf | author:Rodrigo Benenson, Mohamed Omran, Jan Hosang, Bernt Schiele category:cs.CV published:2014-11-16 summary:Paper-by-paper results make it easy to miss the forest for the trees.Weanalyse the remarkable progress of the last decade by discussing the main ideasexplored in the 40+ detectors currently present in the Caltech pedestriandetection benchmark. We observe that there exist three families of approaches,all currently reaching similar detection quality. Based on our analysis, westudy the complementarity of the most promising ideas by combining multiplepublished strategies. This new decision forest detector achieves the currentbest known performance on the challenging Caltech-USA dataset.
arxiv-8100-201 | Combining contextual and local edges for line segment extraction in cluttered images | http://arxiv.org/pdf/1411.4296v1.pdf | author:Rui F. C. Guerreiro category:cs.CV published:2014-11-16 summary:Automatic extraction methods typically assume that line segments arepronounced, thin, few and far between, do not cross each other, and are noiseand clutter-free. Since these assumptions often fail in realistic scenarios,many line segments are not detected or are fragmented. In more severe cases,i.e., many who use the Hough Transform, extraction can fail entirely. In thispaper, we propose a method that tackles these issues. Its key aspect is thecombination of thresholded image derivatives obtained with filters of large andsmall footprints, which we denote as contextual and local edges, respectively.Contextual edges are robust to noise and we use them to select valid localedges, i.e., local edges that are of the same type as contextual ones:dark-to-bright transition of vice-versa. If the distance between valid localedges does not exceed a maximum distance threshold, we enforce connectivity bymarking them and the pixels in between as edge points. This originatesconnected edge maps that are robust and well localized. We use a powerfultwo-sample statistical test to compute contextual edges, which we introducebriefly, as they are unfamiliar to the image processing community. Finally, wepresent experiments that illustrate, with synthetic and real images, how ourmethod is efficient in extracting complete segments of all lengths and widthsin several situations where current methods fail.
arxiv-8100-202 | HIPAD - A Hybrid Interior-Point Alternating Direction algorithm for knowledge-based SVM and feature selection | http://arxiv.org/pdf/1411.4286v1.pdf | author:Zhiwei Qin, Xiaocheng Tang, Ioannis Akrotirianakis, Amit Chakraborty category:stat.ML cs.LG published:2014-11-16 summary:We consider classification tasks in the regime of scarce labeled trainingdata in high dimensional feature space, where specific expert knowledge is alsoavailable. We propose a new hybrid optimization algorithm that solves theelastic-net support vector machine (SVM) through an alternating directionmethod of multipliers in the first phase, followed by an interior-point methodfor the classical SVM in the second phase. Both SVM formulations are adapted toknowledge incorporation. Our proposed algorithm addresses the challenges ofautomatic feature selection, high optimization accuracy, and algorithmicflexibility for taking advantage of prior knowledge. We demonstrate theeffectiveness and efficiency of our algorithm and compare it with existingmethods on a collection of synthetic and real-world data.
arxiv-8100-203 | GreMuTRRR: A Novel Genetic Algorithm to Solve Distance Geometry Problem for Protein Structures | http://arxiv.org/pdf/1411.4246v1.pdf | author:Md. Lisul Islam, Swakkhar Shatabda, M. Sohel Rahman category:cs.NE cs.CE published:2014-11-16 summary:Nuclear Magnetic Resonance (NMR) Spectroscopy is a widely used technique topredict the native structure of proteins. However, NMR machines are only ableto report approximate and partial distances between pair of atoms. To build theprotein structure one has to solve the Euclidean distance geometry problemgiven the incomplete interval distance data produced by NMR machines. In thispaper, we propose a new genetic algorithm for solving the Euclidean distancegeometry problem for protein structure prediction given sparse NMR data. Ourgenetic algorithm uses a greedy mutation operator to intensify the search, atwin removal technique for diversification in the population and a randomrestart method to recover stagnation. On a standard set of benchmark dataset,our algorithm significantly outperforms standard genetic algorithms.
arxiv-8100-204 | Efficient and Accurate Approximations of Nonlinear Convolutional Networks | http://arxiv.org/pdf/1411.4229v1.pdf | author:Xiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, Jian Sun category:cs.CV published:2014-11-16 summary:This paper aims to accelerate the test-time computation of deep convolutionalneural networks (CNNs). Unlike existing methods that are designed forapproximating linear filters or linear responses, our method takes thenonlinear units into account. We minimize the reconstruction error of thenonlinear responses, subject to a low-rank constraint which helps to reduce thecomplexity of filters. We develop an effective solution to this constrainednonlinear optimization problem. An algorithm is also presented for reducing theaccumulated error when multiple layers are approximated. A whole-model speedupratio of 4x is demonstrated on a large network trained for ImageNet, while thetop-5 error rate is only increased by 0.9%. Our accelerated model has acomparably fast speed as the "AlexNet", but is 4.7% more accurate.
arxiv-8100-205 | ECA: High Dimensional Elliptical Component Analysis in non-Gaussian Distributions | http://arxiv.org/pdf/1310.3561v3.pdf | author:Fang Han, Han Liu category:stat.ML published:2013-10-14 summary:We propose a robust alternative to principal component analysis (PCA) --named elliptical component analysis (ECA) -- for analyzing high dimensionalelliptically distributed data. ECA aims at estimating the eigenspace of thecovariance matrix of the elliptical data. To cope with the heavy-tailedelliptical distributions, a multivariate rank statistic is exploited. At themodel-level, we consider two settings that the leading eigenvectors of thecovariance matrix are non-sparse or sparse. Methodologically, we propose ECAprocedures corresponding to both non-sparse and sparse settings. Theoretically,we provide both non-asymptotic and asymptotic analysis in quantifying thetheoretical performances of ECA. Under the non-sparse setting, we show thatECA's performance is highly related to the effective rank of the covariancematrix. Under the sparse setting, the results are in two folds: (i) We showthat the sparse ECA estimator based on a combinatoric program attains theoptimal rate of convergence; (ii) Built upon some recent developments inestimating sparse leading eigenvectors, we show that a computationallyefficient sparse ECA estimator attains the optimal rate of convergence under asuboptimal scaling.
arxiv-8100-206 | Graph connection Laplacian and random matrices with random blocks | http://arxiv.org/pdf/1310.0188v2.pdf | author:Noureddine El Karoui, Hau-tieng Wu category:math.PR math.SP stat.ME stat.ML published:2013-10-01 summary:Graph connection Laplacian (GCL) is a modern data analysis technique that isstarting to be applied for the analysis of high dimensional and massivedatasets. Motivated by this technique, we study matrices that are akin to theones appearing in the null case of GCL, i.e the case where there is nostructure in the dataset under investigation. Developing this understanding isimportant in making sense of the output of the algorithms based on GCL. Wehence develop a theory explaining the behavior of the spectral distribution ofa large class of random matrices, in particular random matrices with randomblock entries of fixed size. Part of the theory covers the case where there issignificant dependence between the blocks. Numerical work shows that theagreement between our theoretical predictions and numerical simulations isgenerally very good.
arxiv-8100-207 | Revisiting Kernelized Locality-Sensitive Hashing for Improved Large-Scale Image Retrieval | http://arxiv.org/pdf/1411.4199v1.pdf | author:Ke Jiang, Qichao Que, Brian Kulis category:cs.CV cs.LG stat.ML published:2014-11-16 summary:We present a simple but powerful reinterpretation of kernelizedlocality-sensitive hashing (KLSH), a general and popular method developed inthe vision community for performing approximate nearest-neighbor searches in anarbitrary reproducing kernel Hilbert space (RKHS). Our new perspective is basedon viewing the steps of the KLSH algorithm in an appropriately projected space,and has several key theoretical and practical benefits. First, it eliminatesthe problematic conceptual difficulties that are present in the existingmotivation of KLSH. Second, it yields the first formal retrieval performancebounds for KLSH. Third, our analysis reveals two techniques for boosting theempirical performance of KLSH. We evaluate these extensions on severallarge-scale benchmark image retrieval data sets, and show that our analysisleads to improved recall performance of at least 12%, and sometimes muchhigher, over the standard KLSH method.
arxiv-8100-208 | ROSS User's Guide and Reference Manual (Version 1.0) | http://arxiv.org/pdf/1411.4194v1.pdf | author:Glenn R. Hofford category:cs.AI cs.CL published:2014-11-15 summary:The ROSS method is a new approach in the area of knowledge representationthat is useful for many artificial intelligence and natural languageunderstanding representation and reasoning tasks. (ROSS stands for"Representation", "Ontology", "Structure", "Star" language). ROSS is a physicalsymbol-based representational scheme. ROSS provides a complex model for thedeclarative representation of physical structure and for the representation ofprocesses and causality. From the metaphysical perspective, the ROSS view ofexternal reality involves a 4D model, wherein discrete single-time-pointunit-sized locations with states are the basis for all objects, processes andaspects that can be modeled. ROSS includes a language called "Star" for thespecification of ontology classes. The ROSS method also includes a formalscheme called the "instance model". Instance models are used in the area ofnatural language meaning representation to represent situations. This documentis an in-depth specification of the ROSS method.
arxiv-8100-209 | Racing to Learn: Statistical Inference and Learning in a Single Spiking Neuron with Adaptive Kernels | http://arxiv.org/pdf/1408.1245v4.pdf | author:Saeed Afshar, Libin George, Jonathan Tapson, Andre van Schaik, Tara Julia Hamilton category:cs.NE q-bio.NC published:2014-08-06 summary:This paper describes the Synapto-dendritic Kernel Adapting Neuron (SKAN), asimple spiking neuron model that performs statistical inference andunsupervised learning of spatiotemporal spike patterns. SKAN is the firstproposed neuron model to investigate the effects of dynamic synapto-dendritickernels and demonstrate their computational power even at the single neuronscale. The rule-set defining the neuron is simple there are no complexmathematical operations such as normalization, exponentiation or evenmultiplication. The functionalities of SKAN emerge from the real-timeinteraction of simple additive and binary processes. Like a biological neuron,SKAN is robust to signal and parameter noise, and can utilize both in itsoperations. At the network scale neurons are locked in a race with each otherwith the fastest neuron to spike effectively hiding its learnt pattern from itsneighbors. The robustness to noise, high speed and simple building blocks notonly make SKAN an interesting neuron model in computational neuroscience, butalso make it ideal for implementation in digital and analog neuromorphicsystems which is demonstrated through an implementation in a Field ProgrammableGate Array (FPGA).
arxiv-8100-210 | Diversity Handling In Evolutionary Landscape | http://arxiv.org/pdf/1411.4148v1.pdf | author:Maumita Bhattacharya category:cs.NE 68T05 published:2014-11-15 summary:The search ability of an Evolutionary Algorithm (EA) depends on the variationamong the individuals in the population. Maintaining an optimal level ofdiversity in the EA population is imperative to ensure that progress of the EAsearch is unhindered by premature convergence to suboptimal solutions. Clearerunderstanding of the concept of population diversity, in the context ofevolutionary search and premature convergence in particular, is the key todesigning efficient EAs. To this end, this paper first presents a comprehensiveanalysis of the EA population diversity issues. Next we present aninvestigation on a counter-niching EA technique that introduces and maintainsconstructive diversity in the population. The proposed approach uses informedgenetic operations to reach promising, but un-explored or under-explored areasof the search space, while discouraging premature local convergence. Simulationruns on a number of standard benchmark test functions with Genetic Algorithm(GA) implementation shows promising results.
arxiv-8100-211 | Investigating the Role of Prior Disambiguation in Deep-learning Compositional Models of Meaning | http://arxiv.org/pdf/1411.4116v1.pdf | author:Jianpeng Cheng, Dimitri Kartsaklis, Edward Grefenstette category:cs.CL cs.LG cs.NE published:2014-11-15 summary:This paper aims to explore the effect of prior disambiguation on neuralnetwork- based compositional models, with the hope that better semanticrepresentations for text compounds can be produced. We disambiguate the inputword vectors before they are fed into a compositional deep net. A series ofevaluations shows the positive effect of prior disambiguation for such deepmodels.
arxiv-8100-212 | Definition of Visual Speech Element and Research on a Method of Extracting Feature Vector for Korean Lip-Reading | http://arxiv.org/pdf/1411.4114v1.pdf | author:Ha Jong Won, Li Gwang Chol, Kim Hyok Chol, Li Kum Song category:cs.CL cs.CV cs.LG published:2014-11-15 summary:In this paper, we defined the viseme (visual speech element) and describedabout the method of extracting visual feature vector. We defined the 10 visemesbased on vowel by analyzing of Korean utterance and proposed the method ofextracting the 20-dimensional visual feature vector, combination of staticfeatures and dynamic features. Lastly, we took an experiment in recognizingwords based on 3-viseme HMM and evaluated the efficiency.
arxiv-8100-213 | Resolution of Difficult Pronouns Using the ROSS Method | http://arxiv.org/pdf/1411.4109v1.pdf | author:Glenn R. Hofford category:cs.CL cs.AI published:2014-11-15 summary:A new natural language understanding method for disambiguation of difficultpronouns is described. Difficult pronouns are those pronouns for which a levelof world or domain knowledge is needed in order to perform anaphoral or othertypes of resolution. Resolution of difficult pronouns may in some cases requirea prior step involving the application of inference to a situation that isrepresented by the natural language text. A general method is described: itperforms entity resolution and pronoun resolution. An extension to the generalpronoun resolution method performs inference as an embedded commonsensereasoning method. The general method and the embedded method utilize featuresof the ROSS representational scheme; in particular the methods use ROSSontology classes and the ROSS situation model. The overall method is a workingsolution that solves the following Winograd schemas: a) trophy and suitcase, b)person lifts person, c) person pays detective, and d) councilmen anddemonstrators.
arxiv-8100-214 | Anisotropic Agglomerative Adaptive Mean-Shift | http://arxiv.org/pdf/1411.4102v1.pdf | author:Rahul Sawhney, Henrik I. Christensen, Gary R. Bradski category:cs.CV cs.LG published:2014-11-15 summary:Mean Shift today, is widely used for mode detection and clustering. Thetechnique though, is challenged in practice due to assumptions of isotropicityand homoscedasticity. We present an adaptive Mean Shift methodology that allowsfor full anisotropic clustering, through unsupervised local bandwidthselection. The bandwidth matrices evolve naturally, adapting locally throughagglomeration, and in turn guiding further agglomeration. The onlinemethodology is practical and effecive for low-dimensional feature spaces,preserving better detail and clustering salience. Additionally, conventionalMean Shift either critically depends on a per instance choice of bandwidth, orrelies on offline methods which are inflexible and/or again data instancespecific. The presented approach, due to its adaptive design, also alleviatesthis issue - with a default form performing generally well. The methodologythough, allows for effective tuning of results.
arxiv-8100-215 | Deep Deconvolutional Networks for Scene Parsing | http://arxiv.org/pdf/1411.4101v1.pdf | author:Rahul Mohan category:stat.ML cs.CV cs.LG published:2014-11-15 summary:Scene parsing is an important and challenging prob- lem in computer vision.It requires labeling each pixel in an image with the category it belongs to.Tradition- ally, it has been approached with hand-engineered features fromcolor information in images. Recently convolutional neural networks (CNNs),which automatically learn hierar- chies of features, have achieved recordperformance on the task. These approaches typically include a post-processingtechnique, such as superpixels, to produce the final label- ing. In this paper,we propose a novel network architecture that combines deep deconvolutionalneural networks with CNNs. Our experiments show that deconvolutional neu- ralnetworks are capable of learning higher order image structure beyond edgeprimitives in comparison to CNNs. The new network architecture is employed formulti-patch training, introduced as part of this work. Multi-patch train- ingmakes it possible to effectively learn spatial priors from scenes. The proposedapproach yields state-of-the-art per- formance on four scene parsing datasets,namely Stanford Background, SIFT Flow, CamVid, and KITTI. In addition, oursystem has the added advantage of having a training system that can becompletely automated end-to-end with- out requiring any post-processing.
arxiv-8100-216 | Spatiotemporal Sparse Bayesian Learning with Applications to Compressed Sensing of Multichannel Physiological Signals | http://arxiv.org/pdf/1404.5122v2.pdf | author:Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Zhouyue Pi, Bhaskar D. Rao category:cs.IT cs.LG math.IT stat.ML published:2014-04-21 summary:Energy consumption is an important issue in continuous wirelesstelemonitoring of physiological signals. Compressed sensing (CS) is a promisingframework to address it, due to its energy-efficient data compressionprocedure. However, most CS algorithms have difficulty in data recovery due tonon-sparsity characteristic of many physiological signals. Block sparseBayesian learning (BSBL) is an effective approach to recover such signals withsatisfactory recovery quality. However, it is time-consuming in recoveringmultichannel signals, since its computational load almost linearly increaseswith the number of channels. This work proposes a spatiotemporal sparse Bayesian learning algorithm torecover multichannel signals simultaneously. It not only exploits temporalcorrelation within each channel signal, but also exploits inter-channelcorrelation among different channel signals. Furthermore, its computationalload is not significantly affected by the number of channels. The proposedalgorithm was applied to brain computer interface (BCI) and EEG-based driver'sdrowsiness estimation. Results showed that the algorithm had both betterrecovery performance and much higher speed than BSBL. Particularly, theproposed algorithm ensured that the BCI classification and the drowsinessestimation had little degradation even when data were compressed by 80%, makingit very suitable for continuous wireless telemonitoring of multichannelsignals.
arxiv-8100-217 | GASP : Geometric Association with Surface Patches | http://arxiv.org/pdf/1411.4098v1.pdf | author:Rahul Sawhney, Fuxin Li, Henrik I. Christensen category:cs.CV cs.GR cs.RO published:2014-11-15 summary:A fundamental challenge to sensory processing tasks in perception androbotics is the problem of obtaining data associations across views. We presenta robust solution for ascertaining potentially dense surface patch (superpixel)associations, requiring just range information. Our approach involvesdecomposition of a view into regularized surface patches. We represent them assequences expressing geometry invariantly over their superpixel neighborhoods,as uniquely consistent partial orderings. We match these representationsthrough an optimal sequence comparison metric based on the Damerau-Levenshteindistance - enabling robust association with quadratic complexity (in contrastto hitherto employed joint matching formulations which are NP-complete). Theapproach is able to perform under wide baselines, heavy rotations, partialoverlaps, significant occlusions and sensor noise. The technique does not require any priors -- motion or otherwise, and doesnot make restrictive assumptions on scene structure and sensor movement. Itdoes not require appearance -- is hence more widely applicable than appearancereliant methods, and invulnerable to related ambiguities such as textureless oraliased content. We present promising qualitative and quantitative resultsunder diverse settings, along with comparatives with popular approaches basedon range as well as RGB-D data.
arxiv-8100-218 | Error Rate Bounds and Iterative Weighted Majority Voting for Crowdsourcing | http://arxiv.org/pdf/1411.4086v1.pdf | author:Hongwei Li, Bin Yu category:stat.ML cs.HC cs.LG math.PR math.ST stat.TH published:2014-11-15 summary:Crowdsourcing has become an effective and popular tool for human-poweredcomputation to label large datasets. Since the workers can be unreliable, it iscommon in crowdsourcing to assign multiple workers to one task, and toaggregate the labels in order to obtain results of high quality. In this paper,we provide finite-sample exponential bounds on the error rate (in probabilityand in expectation) of general aggregation rules under the Dawid-Skenecrowdsourcing model. The bounds are derived for multi-class labeling, and canbe used to analyze many aggregation methods, including majority voting,weighted majority voting and the oracle Maximum A Posteriori (MAP) rule. Weshow that the oracle MAP rule approximately optimizes our upper bound on themean error rate of weighted majority voting in certain setting. We propose aniterative weighted majority voting (IWMV) method that optimizes the error ratebound and approximates the oracle MAP rule. Its one step version has a provabletheoretical guarantee on the error rate. The IWMV method is intuitive andcomputationally simple. Experimental results on simulated and real data showthat IWMV performs at least on par with the state-of-the-art methods, and ithas a much lower computational cost (around one hundred times faster) than thestate-of-the-art methods.
arxiv-8100-219 | 6 Seconds of Sound and Vision: Creativity in Micro-Videos | http://arxiv.org/pdf/1411.4080v1.pdf | author:Miriam Redi, Neil O Hare, Rossano Schifanella, Michele Trevisiol, Alejandro Jaimes category:cs.MM cs.CV cs.HC published:2014-11-14 summary:The notion of creativity, as opposed to related concepts such as beauty orinterestingness, has not been studied from the perspective of automaticanalysis of multimedia content. Meanwhile, short online videos shared on socialmedia platforms, or micro-videos, have arisen as a new medium for creativeexpression. In this paper we study creative micro-videos in an effort tounderstand the features that make a video creative, and to address the problemof automatic detection of creative content. Defining creative videos as thosethat are novel and have aesthetic value, we conduct a crowdsourcing experimentto create a dataset of over 3,800 micro-videos labelled as creative andnon-creative. We propose a set of computational features that we map to thecomponents of our definition of creativity, and conduct an analysis todetermine which of these features correlate most with creative video. Finally,we evaluate a supervised approach to automatically detect creative video, withpromising results, showing that it is necessary to model both aesthetic valueand novelty to achieve optimal classification accuracy.
arxiv-8100-220 | A framework for studying synaptic plasticity with neural spike train data | http://arxiv.org/pdf/1411.4077v1.pdf | author:Scott W. Linderman, Christopher H. Stock, Ryan P. Adams category:stat.ML q-bio.NC published:2014-11-14 summary:Learning and memory in the brain are implemented by complex, time-varyingchanges in neural circuitry. The computational rules according to whichsynaptic weights change over time are the subject of much research, and are notprecisely understood. Until recently, limitations in experimental methods havemade it challenging to test hypotheses about synaptic plasticity on a largescale. However, as such data become available and these barriers are lifted, itbecomes necessary to develop analysis techniques to validate plasticity models.Here, we present a highly extensible framework for modeling arbitrary synapticplasticity rules on spike train data in populations of interconnected neurons.We treat synaptic weights as a (potentially nonlinear) dynamical systemembedded in a fully-Bayesian generalized linear model (GLM). In addition, weprovide an algorithm for inferring synaptic weight trajectories alongside theparameters of the GLM and of the learning rules. Using this method, we performmodel comparison of two proposed variants of the well-knownspike-timing-dependent plasticity (STDP) rule, where nonlinear effects play asubstantial role. On synthetic data generated from the biophysical simulatorNEURON, we show that we can recover the weight trajectories, the pattern ofconnectivity, and the underlying learning rules.
arxiv-8100-221 | Association Rule Based Flexible Machine Learning Module for Embedded System Platforms like Android | http://arxiv.org/pdf/1411.4076v1.pdf | author:Amiraj Dhawan, Shruti Bhave, Amrita Aurora, Vishwanathan Iyer category:cs.CY cs.HC cs.LG published:2014-11-14 summary:The past few years have seen a tremendous growth in the popularity ofsmartphones. As newer features continue to be added to smartphones to increasetheir utility, their significance will only increase in future. Combiningmachine learning with mobile computing can enable smartphones to become'intelligent' devices, a feature which is hitherto unseen in them. Also, thecombination of machine learning and context aware computing can enablesmartphones to gauge user's requirements proactively, depending upon theirenvironment and context. Accordingly, necessary services can be provided tousers. In this paper, we have explored the methods and applications of integratingmachine learning and context aware computing on the Android platform, toprovide higher utility to the users. To achieve this, we define a MachineLearning (ML) module which is incorporated in the basic Android architecture.Firstly, we have outlined two major functionalities that the ML module shouldprovide. Then, we have presented three architectures, each of whichincorporates the ML module at a different level in the Android architecture.The advantages and shortcomings of each of these architectures have beenevaluated. Lastly, we have explained a few applications in which our proposedsystem can be incorporated such that their functionality is improved.
arxiv-8100-222 | Learning Multi-Relational Semantics Using Neural-Embedding Models | http://arxiv.org/pdf/1411.4072v1.pdf | author:Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, Li Deng category:cs.CL cs.LG stat.ML published:2014-11-14 summary:In this paper we present a unified framework for modeling multi-relationalrepresentations, scoring, and learning, and conduct an empirical study ofseveral recent multi-relational embedding models under the framework. Weinvestigate the different choices of relation operators based on linear andbilinear transformations, and also the effects of entity representations byincorporating unsupervised vectors pre-trained on extra textual resources. Ourresults show several interesting findings, enabling the design of a simpleembedding model that achieves the new state-of-the-art performance on a popularknowledge base completion task evaluated on Freebase.
arxiv-8100-223 | A unified view of generative models for networks: models, methods, opportunities, and challenges | http://arxiv.org/pdf/1411.4070v1.pdf | author:Abigail Z. Jacobs, Aaron Clauset category:stat.ML cs.LG cs.SI physics.soc-ph published:2014-11-14 summary:Research on probabilistic models of networks now spans a wide variety offields, including physics, sociology, biology, statistics, and machinelearning. These efforts have produced a diverse ecology of models and methods.Despite this diversity, many of these models share a common underlyingstructure: pairwise interactions (edges) are generated with probabilityconditional on latent vertex attributes. Differences between models generallystem from different philosophical choices about how to learn from data ordifferent empirically-motivated goals. The highly interdisciplinary nature ofwork on these generative models, however, has inhibited the development of aunified view of their similarities and differences. For instance, noveltheoretical models and optimization techniques developed in machine learningare largely unknown within the social and biological sciences, which haveinstead emphasized model interpretability. Here, we describe a unified view ofgenerative models for networks that draws together many of these disparatethreads and highlights the fundamental similarities and differences that spanthese fields. We then describe a number of opportunities and challenges forfuture work that are revealed by this view.
arxiv-8100-224 | Dynamic Programming for Instance Annotation in Multi-instance Multi-label Learning | http://arxiv.org/pdf/1411.4068v1.pdf | author:Anh T. Pham, Raviv Raich, Xiaoli Z. Fern category:stat.ML cs.LG published:2014-11-14 summary:Labeling data for classification requires significant human effort. To reducelabeling cost, instead of labeling every instance, a group of instances (bag)is labeled by a single bag label. Computer algorithms are then used to inferthe label for each instance in a bag, a process referred to as instanceannotation. This task is challenging due to the ambiguity regarding theinstance labels. We propose a discriminative probabilistic model for theinstance annotation problem and introduce an expectation maximization frameworkfor inference, based on the maximum likelihood approach. For many probabilisticapproaches, brute-force computation of the instance label posterior probabilitygiven its bag label is exponential in the number of instances in the bag. Ourkey contribution is a dynamic programming method for computing the posteriorthat is linear in the number of instances. We evaluate our methods using bothbenchmark and real world data sets, in the domain of bird song, imageannotation, and activity recognition. In many cases, the proposed frameworkoutperforms, sometimes significantly, the current state-of-the-art MIMLlearning methods, both in instance label prediction and bag label prediction.
arxiv-8100-225 | A Faster Method for Tracking and Scoring Videos Corresponding to Sentences | http://arxiv.org/pdf/1411.4064v1.pdf | author:Haonan Yu, Daniel P. Barrett, Jeffrey Mark Siskind category:cs.CV published:2014-11-14 summary:Prior work presented the sentence tracker, a method for scoring how well asentence describes a video clip or alternatively how well a video clip depictsa sentence. We present an improved method for optimizing the same cost functionemployed by this prior work, reducing the space complexity from exponential inthe sentence length to polynomial, as well as producing a qualitativelyidentical result in time polynomial in the sentence length instead ofexponential. Since this new method is plug-compatible with the prior method, itcan be used for the same applications: video retrieval with sentential queries,generating sentential descriptions of video clips, and focusing the attentionof a tracker with a sentence, while allowing these applications to scale withsignificantly larger numbers of object detections, word meanings modeled withHMMs with significantly larger numbers of states, and significantly longersentences, with no appreciable degradation in quality of results.
arxiv-8100-226 | Detecting change points in the large-scale structure of evolving networks | http://arxiv.org/pdf/1403.0989v2.pdf | author:Leto Peel, Aaron Clauset category:cs.SI physics.soc-ph stat.ML published:2014-03-05 summary:Interactions among people or objects are often dynamic in nature and can berepresented as a sequence of networks, each providing a snapshot of theinteractions over a brief period of time. An important task in analyzing suchevolving networks is change-point detection, in which we both identify thetimes at which the large-scale pattern of interactions changes fundamentallyand quantify how large and what kind of change occurred. Here, we formalize forthe first time the network change-point detection problem within an onlineprobabilistic learning framework and introduce a method that can reliably solveit. This method combines a generalized hierarchical random graph model with aBayesian hypothesis test to quantitatively determine if, when, and preciselyhow a change point has occurred. We analyze the detectability of our methodusing synthetic data with known change points of different types andmagnitudes, and show that this method is more accurate than several previouslyused alternatives. Applied to two high-resolution evolving social networks,this method identifies a sequence of change points that align with knownexternal "shocks" to these networks.
arxiv-8100-227 | A Discriminative CNN Video Representation for Event Detection | http://arxiv.org/pdf/1411.4006v1.pdf | author:Zhongwen Xu, Yi Yang, Alexander G. Hauptmann category:cs.CV published:2014-11-14 summary:In this paper, we propose a discriminative video representation for eventdetection over a large scale video dataset when only limited hardware resourcesare available. The focus of this paper is to effectively leverage deepConvolutional Neural Networks (CNNs) to advance event detection, where onlyframe level static descriptors can be extracted by the existing CNN toolkit.This paper makes two contributions to the inference of CNN videorepresentation. First, while average pooling and max pooling have long been thestandard approaches to aggregating frame level static features, we show thatperformance can be significantly improved by taking advantage of an appropriateencoding method. Second, we propose using a set of latent concept descriptorsas the frame descriptor, which enriches visual information while keeping itcomputationally affordable. The integration of the two contributions results ina new state-of-the-art performance in event detection over the largest videodatasets. Compared to improved Dense Trajectories, which has been recognized asthe best video representation for event detection, our new representationimproves the Mean Average Precision (mAP) from 27.6% to 36.8% for the TRECVIDMEDTest 14 dataset and from 34.0% to 44.6% for the TRECVID MEDTest 13 dataset.This work is the core part of the winning solution of our CMU-Informedia teamin TRECVID MED 2014 competition.
arxiv-8100-228 | A convex formulation for hyperspectral image superresolution via subspace-based regularization | http://arxiv.org/pdf/1411.4005v1.pdf | author:Miguel Simões, José Bioucas-Dias, Luis B. Almeida, Jocelyn Chanussot category:cs.CV stat.ML published:2014-11-14 summary:Hyperspectral remote sensing images (HSIs) usually have high spectralresolution and low spatial resolution. Conversely, multispectral images (MSIs)usually have low spectral and high spatial resolutions. The problem ofinferring images which combine the high spectral and high spatial resolutionsof HSIs and MSIs, respectively, is a data fusion problem that has been thefocus of recent active research due to the increasing availability of HSIs andMSIs retrieved from the same geographical area. We formulate this problem as the minimization of a convex objective functioncontaining two quadratic data-fitting terms and an edge-preserving regularizer.The data-fitting terms account for blur, different resolutions, and additivenoise. The regularizer, a form of vector Total Variation, promotespiecewise-smooth solutions with discontinuities aligned across thehyperspectral bands. The downsampling operator accounting for the different spatial resolutions,the non-quadratic and non-smooth nature of the regularizer, and the very largesize of the HSI to be estimated lead to a hard optimization problem. We dealwith these difficulties by exploiting the fact that HSIs generally "live" in alow-dimensional subspace and by tailoring the Split Augmented LagrangianShrinkage Algorithm (SALSA), which is an instance of the Alternating DirectionMethod of Multipliers (ADMM), to this optimization problem, by means of aconvenient variable splitting. The spatial blur and the spectral linearoperators linked, respectively, with the HSI and MSI acquisition processes arealso estimated, and we obtain an effective algorithm that outperforms thestate-of-the-art, as illustrated in a series of experiments with simulated andreal-life data.
arxiv-8100-229 | Convolutional Kernel Networks | http://arxiv.org/pdf/1406.3332v2.pdf | author:Julien Mairal, Piotr Koniusz, Zaid Harchaoui, Cordelia Schmid category:cs.CV cs.LG stat.ML published:2014-06-12 summary:An important goal in visual recognition is to devise image representationsthat are invariant to particular transformations. In this paper, we addressthis goal with a new type of convolutional neural network (CNN) whoseinvariance is encoded by a reproducing kernel. Unlike traditional approacheswhere neural networks are learned either to represent data or for solving aclassification task, our network learns to approximate the kernel feature mapon training data. Such an approach enjoys several benefits over classical ones.First, by teaching CNNs to be invariant, we obtain simple network architecturesthat achieve a similar accuracy to more complex ones, while being easy to trainand robust to overfitting. Second, we bridge a gap between the neural networkliterature and kernels, which are natural tools to model invariance. Weevaluate our methodology on visual recognition tasks where CNNs have proven toperform well, e.g., digit recognition with the MNIST dataset, and the morechallenging CIFAR-10 and STL-10 datasets, where our accuracy is competitivewith the state of the art.
arxiv-8100-230 | Deep Belief Network Training Improvement Using Elite Samples Minimizing Free Energy | http://arxiv.org/pdf/1411.4046v1.pdf | author:Mohammad Ali Keyvanrad, Mohammad Mehdi Homayounpour category:cs.LG cs.CV published:2014-11-14 summary:Nowadays this is very popular to use deep architectures in machine learning.Deep Belief Networks (DBNs) are deep architectures that use stack of RestrictedBoltzmann Machines (RBM) to create a powerful generative model using trainingdata. In this paper we present an improvement in a common method that isusually used in training of RBMs. The new method uses free energy as acriterion to obtain elite samples from generative model. We argue that thesesamples can more accurately compute gradient of log probability of trainingdata. According to the results, an error rate of 0.99% was achieved on MNISTtest set. This result shows that the proposed method outperforms the methodpresented in the first paper introducing DBN (1.25% error rate) and generalclassification methods such as SVM (1.4% error rate) and KNN (with 1.6% errorrate). In another test using ISOLET dataset, letter classification errordropped to 3.59% compared to 5.59% error rate achieved in those papers usingthis dataset. The implemented method is available online at"http://ceit.aut.ac.ir/~keyvanrad/DeeBNet Toolbox.html".
arxiv-8100-231 | Supervised mid-level features for word image representation | http://arxiv.org/pdf/1410.5224v2.pdf | author:Albert Gordo category:cs.CV published:2014-10-20 summary:This paper addresses the problem of learning word image representations:given the cropped image of a word, we are interested in finding a descriptive,robust, and compact fixed-length representation. Machine learning techniquescan then be supplied with these representations to produce models useful forword retrieval or recognition tasks. Although many works have focused on themachine learning aspect once a global representation has been produced, littlework has been devoted to the construction of those base image representations:most works use standard coding and aggregation techniques directly on top ofstandard computer vision features such as SIFT or HOG. We propose to learn local mid-level features suitable for building word imagerepresentations. These features are learnt by leveraging character bounding boxannotations on a small set of training images. However, contrary to otherapproaches that use character bounding box information, our approach does notrely on detecting the individual characters explicitly at testing time. Ourlocal mid-level features can then be aggregated to produce a global word imagesignature. When pairing these features with the recent word attributesframework of Almaz\'an et al., we obtain results comparable with or better thanthe state-of-the-art on matching and recognition tasks using global descriptorsof only 96 dimensions.
arxiv-8100-232 | A Unified Framework for Probabilistic Component Analysis | http://arxiv.org/pdf/1303.3240v2.pdf | author:Mihalis A. Nicolaou, Stefanos Zafeiriou, Maja Pantic category:cs.LG cs.CV stat.ML published:2013-03-13 summary:We present a unifying framework which reduces the construction ofprobabilistic component analysis techniques to a mere selection of the latentneighbourhood, thus providing an elegant and principled framework for creatingnovel component analysis models as well as constructing probabilisticequivalents of deterministic component analysis methods. Under our framework,we unify many very popular and well-studied component analysis algorithms, suchas Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA),Locality Preserving Projections (LPP) and Slow Feature Analysis (SFA), some ofwhich have no probabilistic equivalents in literature thus far. We firstlydefine the Markov Random Fields (MRFs) which encapsulate the latentconnectivity of the aforementioned component analysis techniques; subsequently,we show that the projection directions produced by all PCA, LDA, LPP and SFAare also produced by the Maximum Likelihood (ML) solution of a single jointprobability density function, composed by selecting one of the defined MRFpriors while utilising a simple observation model. Furthermore, we proposenovel Expectation Maximization (EM) algorithms, exploiting the proposed jointPDF, while we generalize the proposed methodologies to arbitrary connectivitiesvia parameterizable MRF products. Theoretical analysis and experiments on bothsimulated and real world data show the usefulness of the proposed framework, byderiving methods which well outperform state-of-the-art equivalents.
arxiv-8100-233 | rFerns: An Implementation of the Random Ferns Method for General-Purpose Machine Learning | http://arxiv.org/pdf/1202.1121v2.pdf | author:Miron B. Kursa category:cs.LG stat.ML published:2012-02-06 summary:In this paper I present an extended implementation of the Random fernsalgorithm contained in the R package rFerns. It differs from the original bythe ability of consuming categorical and numerical attributes instead of onlybinary ones. Also, instead of using simple attribute subspace ensemble itemploys bagging and thus produce error approximation and variable importancemeasure modelled after Random forest algorithm. I also present benchmarks'results which show that although Random ferns' accuracy is mostly smaller thanachieved by Random forest, its speed and good quality of importance measure itprovides make rFerns a reasonable choice for a specific applications.
arxiv-8100-234 | Sample-targeted clinical trial adaptation | http://arxiv.org/pdf/1411.3919v1.pdf | author:Ognjen Arandjelovic category:cs.LG published:2014-11-14 summary:Clinical trial adaptation refers to any adjustment of the trial protocolafter the onset of the trial. The main goal is to make the process ofintroducing new medical interventions to patients more efficient by reducingthe cost and the time associated with evaluating their safety and efficacy. Theprincipal question is how should adaptation be performed so as to minimize thechance of distorting the outcome of the trial. We propose a novel method forachieving this. Unlike previous work our approach focuses on trial adaptationby sample size adjustment. We adopt a recently proposed stratificationframework based on collected auxiliary data and show that this informationtogether with the primary measured variables can be used to make aprobabilistically informed choice of the particular sub-group a sample shouldbe removed from. Experiments on simulated data are used to illustrate theeffectiveness of our method and its application in practice.
arxiv-8100-235 | Learning Fuzzy Controllers in Mobile Robotics with Embedded Preprocessing | http://arxiv.org/pdf/1411.3895v1.pdf | author:I. Rodríguez-Fdez, M. Mucientes, A. Bugarín category:cs.RO cs.AI cs.LG published:2014-11-14 summary:The automatic design of controllers for mobile robots usually requires twostages. In the first stage,sensorial data are preprocessed or transformed intohigh level and meaningful values of variables whichare usually defined fromexpert knowledge. In the second stage, a machine learning technique is appliedtoobtain a controller that maps these high level variables to the controlcommands that are actually sent tothe robot. This paper describes an algorithmthat is able to embed the preprocessing stage into the learningstage in orderto get controllers directly starting from sensorial raw data with no expertknowledgeinvolved. Due to the high dimensionality of the sensorial data, thisapproach uses Quantified Fuzzy Rules(QFRs), that are able to transformlow-level input variables into high-level input variables, reducingthedimensionality through summarization. The proposed learning algorithm, calledIterative QuantifiedFuzzy Rule Learning (IQFRL), is based on geneticprogramming. IQFRL is able to learn rules with differentstructures, and canmanage linguistic variables with multiple granularities. The algorithm has beentestedwith the implementation of the wall-following behavior both in severalrealistic simulated environmentswith different complexity and on a Pioneer 3-ATrobot in two real environments. Results have beencompared with severalwell-known learning algorithms combined with different datapreprocessingtechniques, showing that IQFRL exhibits a better and statisticallysignificant performance. Moreover,three real world applications for which IQFRLplays a central role are also presented: path and objecttracking with staticand moving obstacles avoidance.
arxiv-8100-236 | Part Detector Discovery in Deep Convolutional Neural Networks | http://arxiv.org/pdf/1411.3159v2.pdf | author:Marcel Simon, Erik Rodner, Joachim Denzler category:cs.CV I.4.8 published:2014-11-12 summary:Current fine-grained classification approaches often rely on a robustlocalization of object parts to extract localized feature representationssuitable for discrimination. However, part localization is a challenging taskdue to the large variation of appearance and pose. In this paper, we show howpre-trained convolutional neural networks can be used for robust and efficientobject part discovery and localization without the necessity to actually trainthe network on the current dataset. Our approach called "part detectordiscovery" (PDD) is based on analyzing the gradient maps of the network outputsand finding activation centers spatially related to annotated semantic parts orbounding boxes. This allows us not just to obtain excellent performance on the CUB200-2011dataset, but in contrast to previous approaches also to perform detection andbird classification jointly without requiring a given bounding box annotationduring testing and ground-truth parts during training. The code is available athttp://www.inf-cv.uni-jena.de/part_discovery andhttps://github.com/cvjena/PartDetectorDisovery.
arxiv-8100-237 | Statistical Models for Degree Distributions of Networks | http://arxiv.org/pdf/1411.3825v1.pdf | author:Kayvan Sadeghi, Alessandro Rinaldo category:math.ST stat.ML stat.TH published:2014-11-14 summary:We define and study the statistical models in exponential family form whosesufficient statistics are the degree distributions and the bi-degreedistributions of undirected labelled simple graphs. Graphs that are constrainedby the joint degree distributions are called $dK$-graphs in the computerscience literature and this paper attempts to provide the first statisticallygrounded analysis of this type of models. In addition to formalizing thesemodels, we provide some preliminary results for the parameter estimation andthe asymptotic behaviour of the model for degree distribution, and discuss theparameter estimation for the model for bi-degree distribution.
arxiv-8100-238 | Integrating Fuzzy and Ant Colony System for Fuzzy Vehicle Routing Problem with Time Windows | http://arxiv.org/pdf/1411.3806v1.pdf | author:Sandhya Bansal, V. Katiyar category:cs.AI cs.CE cs.NE published:2014-11-14 summary:In this paper fuzzy VRPTW with an uncertain travel time is considered.Credibility theory is used to model the problem and specifies a preferenceindex at which it is desired that the travel times to reach the customers fallinto their time windows. We propose the integration of fuzzy and ant colonysystem based evolutionary algorithm to solve the problem while preserving theconstraints. Computational results for certain benchmark problems having shortand long time horizons are presented to show the effectiveness of thealgorithm. Comparison between different preferences indexes have been obtainedto help the user in making suitable decisions.
arxiv-8100-239 | Stochastic Compositional Gradient Descent: Algorithms for Minimizing Compositions of Expected-Value Functions | http://arxiv.org/pdf/1411.3803v1.pdf | author:Mengdi Wang, Ethan X. Fang, Han Liu category:stat.ML published:2014-11-14 summary:Classical stochastic gradient methods are well suited for minimizingexpected-value objective functions. However, they do not apply to theminimization of a nonlinear function involving expected values or a compositionof two expected-value functions, i.e., problems of the form $\min_x\mathbf{E}_v [f_v\big(\mathbf{E}_w [g_w(x)]\big)]$. In order to solve thisstochastic composition problem, we propose a class of stochastic compositionalgradient descent (SCGD) algorithms that can be viewed as stochastic versions ofquasi-gradient method. SCGD update the solutions based on noisy samplegradients of $f_v,g_{w}$ and use an auxiliary variable to track the unknownquantity $\mathbf{E}_w[g_w(x)]$. We prove that the SCGD converge almost surelyto an optimal solution for convex optimization problems, as long as such asolution exists. The convergence involves the interplay of two iterations withdifferent time scales. For nonsmooth convex problems, the SCGD achieve aconvergence rate of $O(k^{-1/4})$ in the general case and $O(k^{-2/3})$ in thestrongly convex case, after taking $k$ samples. For smooth convex problems, theSCGD can be accelerated to converge at a rate of $O(k^{-2/7})$ in the generalcase and $O(k^{-4/5})$ in the strongly convex case. For nonconvex problems, weprove that any limit point generated by SCGD is a stationary point, for whichwe also provide the convergence rate analysis. Indeed, the stochastic settingwhere one wants to optimize compositions of expected-value functions is verycommon in practice. The proposed SCGD methods find wide applications inlearning, estimation, dynamic programming, etc.
arxiv-8100-240 | Asymmetric Minwise Hashing | http://arxiv.org/pdf/1411.3787v1.pdf | author:Anshumali Shrivastava, Ping Li category:stat.ML cs.DB cs.DS cs.IR cs.LG published:2014-11-14 summary:Minwise hashing (Minhash) is a widely popular indexing scheme in practice.Minhash is designed for estimating set resemblance and is known to besuboptimal in many applications where the desired measure is set overlap (i.e.,inner product between binary vectors) or set containment. Minhash has inherentbias towards smaller sets, which adversely affects its performance inapplications where such a penalization is not desirable. In this paper, wepropose asymmetric minwise hashing (MH-ALSH), to provide a solution to thisproblem. The new scheme utilizes asymmetric transformations to cancel the biasof traditional minhash towards smaller sets, making the final "collisionprobability" monotonic in the inner product. Our theoretical comparisons showthat for the task of retrieving with binary inner products asymmetric minhashis provably better than traditional minhash and other recently proposed hashingalgorithms for general inner products. Thus, we obtain an algorithmicimprovement over existing approaches in the literature. Experimentalevaluations on four publicly available high-dimensional datasets validate ourclaims and the proposed scheme outperforms, often significantly, other hashingalgorithms on the task of near neighbor retrieval with set containment. Ourproposal is simple and easy to implement in practice.
arxiv-8100-241 | An Analysis of Random Projections in Cancelable Biometrics | http://arxiv.org/pdf/1401.4489v3.pdf | author:Devansh Arpit, Ifeoma Nwogu, Gaurav Srivastava, Venu Govindaraju category:cs.CV cs.LG stat.ML published:2014-01-17 summary:With increasing concerns about security, the need for highly secure physicalbiometrics-based authentication systems utilizing \emph{cancelable biometric}technologies is on the rise. Because the problem of cancelable templategeneration deals with the trade-off between template security and matchingperformance, many state-of-the-art algorithms successful in generating highquality cancelable biometrics all have random projection as one of their earlyprocessing steps. This paper therefore presents a formal analysis of why randomprojections is an essential step in cancelable biometrics. By formally definingthe notion of an \textit{Independent Subspace Structure} for datasets, it canbe shown that random projection preserves the subspace structure of datavectors generated from a union of independent linear subspaces. The bound onthe minimum number of random vectors required for this to hold is also derivedand is shown to depend logarithmically on the number of data samples, not onlyin independent subspaces but in disjoint subspace settings as well. Thetheoretical analysis presented is supported in detail with empirical results onreal-world face recognition datasets.
arxiv-8100-242 | Tensor decompositions for learning latent variable models | http://arxiv.org/pdf/1210.7559v4.pdf | author:Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, Matus Telgarsky category:cs.LG math.NA stat.ML published:2012-10-29 summary:This work considers a computationally and statistically efficient parameterestimation method for a wide class of latent variable models---includingGaussian mixture models, hidden Markov models, and latent Dirichletallocation---which exploits a certain tensor structure in their low-orderobservable moments (typically, of second- and third-order). Specifically,parameter estimation is reduced to the problem of extracting a certain(orthogonal) decomposition of a symmetric tensor derived from the moments; thisdecomposition can be viewed as a natural generalization of the singular valuedecomposition for matrices. Although tensor decompositions are generallyintractable to compute, the decomposition of these specially structured tensorscan be efficiently obtained by a variety of approaches, including poweriterations and maximization approaches (similar to the case of matrices). Adetailed analysis of a robust tensor power method is provided, establishing ananalogue of Wedin's perturbation theorem for the singular vectors of matrices.This implies a robust and computationally tractable estimation approach forseveral popular latent variable models.
arxiv-8100-243 | Improved Asymmetric Locality Sensitive Hashing (ALSH) for Maximum Inner Product Search (MIPS) | http://arxiv.org/pdf/1410.5410v2.pdf | author:Anshumali Shrivastava, Ping Li category:stat.ML cs.DS cs.IR cs.LG published:2014-10-20 summary:Recently it was shown that the problem of Maximum Inner Product Search (MIPS)is efficient and it admits provably sub-linear hashing algorithms. Asymmetrictransformations before hashing were the key in solving MIPS which was otherwisehard. In the prior work, the authors use asymmetric transformations whichconvert the problem of approximate MIPS into the problem of approximate nearneighbor search which can be efficiently solved using hashing. In this work, weprovide a different transformation which converts the problem of approximateMIPS into the problem of approximate cosine similarity search which can beefficiently solved using signed random projections. Theoretical analysis showthat the new scheme is significantly better than the original scheme for MIPS.Experimental evaluations strongly support the theoretical findings.
arxiv-8100-244 | Jamming Bandits | http://arxiv.org/pdf/1411.3652v1.pdf | author:SaiDhiraj Amuru, Cem Tekin, Mihaela van der Schaar, R. Michael Buehrer category:cs.IT cs.LG math.IT published:2014-11-13 summary:Can an intelligent jammer learn and adapt to unknown environments in anelectronic warfare-type scenario? In this paper, we answer this question in thepositive, by developing a cognitive jammer that adaptively and optimallydisrupts the communication between a victim transmitter-receiver pair. Weformalize the problem using a novel multi-armed bandit framework where thejammer can choose various physical layer parameters such as the signalingscheme, power level and the on-off/pulsing duration in an attempt to obtainpower efficient jamming strategies. We first present novel online learningalgorithms to maximize the jamming efficacy against static transmitter-receiverpairs and prove that our learning algorithm converges to the optimal (in termsof the error rate inflicted at the victim and the energy used) jammingstrategy. Even more importantly, we prove that the rate of convergence to theoptimal jamming strategy is sub-linear, i.e. the learning is fast in comparisonto existing reinforcement learning algorithms, which is particularly importantin dynamically changing wireless environments. Also, we characterize theperformance of the proposed bandit-based learning algorithm against multiplestatic and adaptive transmitter-receiver pairs.
arxiv-8100-245 | DUM: Diversity-Weighted Utility Maximization for Recommendations | http://arxiv.org/pdf/1411.3650v1.pdf | author:Azin Ashkan, Branislav Kveton, Shlomo Berkovsky, Zheng Wen category:cs.IR stat.ML published:2014-11-13 summary:The need for diversification of recommendation lists manifests in a number ofrecommender systems use cases. However, an increase in diversity may underminethe utility of the recommendations, as relevant items in the list may bereplaced by more diverse ones. In this work we propose a novel method formaximizing the utility of the recommended items subject to the diversity ofuser's tastes, and show that an optimal solution to this problem can be foundgreedily. We evaluate the proposed method in two online user studies as well asin an offline analysis incorporating a number of evaluation metrics. Theresults of evaluations show the superiority of our method over a number ofbaselines.
arxiv-8100-246 | Acoustic Scene Classification | http://arxiv.org/pdf/1411.3715v1.pdf | author:Daniele Barchiesi, Dimitrios Giannoulis, Dan Stowell, Mark D. Plumbley category:cs.SD cs.LG published:2014-11-13 summary:In this article we present an account of the state-of-the-art in acousticscene classification (ASC), the task of classifying environments from thesounds they produce. Starting from a historical review of previous research inthis area, we define a general framework for ASC and present different imple-mentations of its components. We then describe a range of different algorithmssubmitted for a data challenge that was held to provide a general and fairbenchmark for ASC techniques. The dataset recorded for this purpose ispresented, along with the performance metrics that are used to evaluate thealgorithms and statistical significance tests to compare the submitted methods.We use a baseline method that employs MFCCS, GMMS and a maximum likelihoodcriterion as a benchmark, and only find sufficient evidence to conclude thatthree algorithms significantly outperform it. We also evaluate the humanclassification accuracy in performing a similar classification task. The bestperforming algorithm achieves a mean accuracy that matches the median accuracyobtained by humans, and common pairs of classes are misclassified by bothcomputers and humans. However, all acoustic scenes are correctly classified byat least some individuals, while there are scenes that are misclassified by allalgorithms.
arxiv-8100-247 | Not All Neural Embeddings are Born Equal | http://arxiv.org/pdf/1410.0718v2.pdf | author:Felix Hill, KyungHyun Cho, Sebastien Jean, Coline Devin, Yoshua Bengio category:cs.CL published:2014-10-02 summary:Neural language models learn word representations that capture richlinguistic and conceptual information. Here we investigate the embeddingslearned by neural machine translation models. We show that translation-basedembeddings outperform those learned by cutting-edge monolingual models atsingle-language tasks requiring knowledge of conceptual similarity and/orsyntactic role. The findings suggest that, while monolingual models learninformation about how concepts are related, neural-translation models bettercapture their true ontological status.
arxiv-8100-248 | A Text to Speech (TTS) System with English to Punjabi Conversion | http://arxiv.org/pdf/1411.3561v1.pdf | author:Prabhsimran Singh, Amritpal Singh category:cs.CL published:2014-11-13 summary:The paper aims to show how an application can be developed that converts theEnglish language into the Punjabi Language, and the same application canconvert the Text to Speech(TTS) i.e. pronounce the text. This application canbe really beneficial for those with special needs.
arxiv-8100-249 | Greedy metrics in orthogonal greedy learning | http://arxiv.org/pdf/1411.3553v1.pdf | author:Lin Xu, Shaobo Lin, Jinshan Zeng, Zongben Xu category:cs.LG F.2.2 published:2014-11-13 summary:Orthogonal greedy learning (OGL) is a stepwise learning scheme that adds anew atom from a dictionary via the steepest gradient descent and build theestimator via orthogonal projecting the target function to the space spanned bythe selected atoms in each greedy step. Here, "greed" means choosing a new atomaccording to the steepest gradient descent principle. OGL then avoids theoverfitting/underfitting by selecting an appropriate iteration number. In thispaper, we point out that the overfitting/underfitting can also be avoided viaredefining "greed" in OGL. To this end, we introduce a new greedy metric,called $\delta$-greedy thresholds, to refine "greed" and theoretically verifiesits feasibility. Furthermore, we reveals that such a greedy metric can bring anadaptive termination rule on the premise of maintaining the prominent learningperformance of OGL. Our results show that the steepest gradient descent is notthe unique greedy metric of OGL and some other more suitable metric may lessenthe hassle of model-selection of OGL.
arxiv-8100-250 | Gaze Stabilization for Humanoid Robots: a Comprehensive Framework | http://arxiv.org/pdf/1411.3525v1.pdf | author:Alessandro Roncone, Ugo Pattacini, Giorgio Metta, Lorenzo Natale category:cs.RO cs.CV published:2014-11-13 summary:Gaze stabilization is an important requisite for humanoid robots. Previouswork on this topic has focused on the integration of inertial and visualinformation. Little attention has been given to a third component, which is theknowledge that the robot has about its own movement. In this work we propose acomprehensive framework for gaze stabilization in a humanoid robot. We focus onthe problem of compensating for disturbances induced in the cameras due toself-generated movements of the robot. In this work we employ two separatesignals for stabilization: (1) an anticipatory term obtained from the velocitycommands sent to the joints while the robot moves autonomously; (2) a feedbackterm from the on board gyroscope, which compensates unpredicted externaldisturbances. We first provide the mathematical formulation to derive theforward and the differential kinematics of the fixation point of the stereosystem. We finally test our method on the iCub robot. We show that thestabilization consistently reduces the residual optical flow during themovement of the robot and in presence of external disturbances. We alsodemonstrate that proper integration of the neck DoF is crucial to achievecorrect stabilization.
arxiv-8100-251 | Can Image-Level Labels Replace Pixel-Level Labels for Image Parsing | http://arxiv.org/pdf/1403.1626v3.pdf | author:Zhiwu Lu, Zhenyong Fu, Tao Xiang, Liwei Wang, Ji-Rong Wen category:cs.CV published:2014-03-07 summary:This paper presents a weakly supervised sparse learning approach to theproblem of noisily tagged image parsing, or segmenting all the objects within anoisily tagged image and identifying their categories (i.e. tags). Differentfrom the traditional image parsing that takes pixel-level labels as strongsupervisory information, our noisily tagged image parsing is provided withnoisy tags of all the images (i.e. image-level labels), which is a naturalsetting for social image collections (e.g. Flickr). By oversegmenting all theimages into regions, we formulate noisily tagged image parsing as a weaklysupervised sparse learning problem over all the regions, where the initiallabels of each region are inferred from image-level labels. Furthermore, wedevelop an efficient algorithm to solve such weakly supervised sparse learningproblem. The experimental results on two benchmark datasets show theeffectiveness of our approach. More notably, the reported surprising resultsshed some light on answering the question: can image-level labels replacepixel-level labels (hard to access) as supervisory information for imageparsing.
arxiv-8100-252 | Joint modeling of multiple time series via the beta process with application to motion capture segmentation | http://arxiv.org/pdf/1308.4747v3.pdf | author:Emily B. Fox, Michael C. Hughes, Erik B. Sudderth, Michael I. Jordan category:stat.ME stat.ML published:2013-08-22 summary:We propose a Bayesian nonparametric approach to the problem of jointlymodeling multiple related time series. Our model discovers a latent set ofdynamical behaviors shared among the sequences, and segments each time seriesinto regions defined by a subset of these behaviors. Using a beta processprior, the size of the behavior set and the sharing pattern are both inferredfrom data. We develop Markov chain Monte Carlo (MCMC) methods based on theIndian buffet process representation of the predictive distribution of the betaprocess. Our MCMC inference algorithm efficiently adds and removes behaviorsvia novel split-merge moves as well as data-driven birth and death proposals,avoiding the need to consider a truncated model. We demonstrate promisingresults on unsupervised segmentation of human motion capture data.
arxiv-8100-253 | Comparison three methods of clustering: k-means, spectral clustering and hierarchical clustering | http://arxiv.org/pdf/1312.6117v2.pdf | author:Kamran Kowsari category:cs.LG 68T10 H.3.3; I.5.3 published:2013-12-19 summary:Comparison of three kind of the clustering and find cost function and lossfunction and calculate them. Error rate of the clustering methods and how tocalculate the error percentage always be one on the important factor forevaluating the clustering methods, so this paper introduce one way to calculatethe error rate of clustering methods. Clustering algorithms can be divided intoseveral categories including partitioning clustering algorithms, hierarchicalalgorithms and density based algorithms. Generally speaking we should compareclustering algorithms by Scalability, Ability to work with different attribute,Clusters formed by conventional, Having minimal knowledge of the computer torecognize the input parameters, Classes for dealing with noise and extradeposition that same error rate for clustering a new data, Thus, there is noeffect on the input data, different dimensions of high levels, K-means is oneof the simplest approach to clustering that clustering is an unsupervisedproblem.
arxiv-8100-254 | SelfieBoost: A Boosting Algorithm for Deep Learning | http://arxiv.org/pdf/1411.3436v1.pdf | author:Shai Shalev-Shwartz category:stat.ML cs.LG published:2014-11-13 summary:We describe and analyze a new boosting algorithm for deep learning calledSelfieBoost. Unlike other boosting algorithms, like AdaBoost, which constructensembles of classifiers, SelfieBoost boosts the accuracy of a single network.We prove a $\log(1/\epsilon)$ convergence rate for SelfieBoost under some "SGDsuccess" assumption which seems to hold in practice.
arxiv-8100-255 | A Comparative Study of Techniques of Distant Reconstruction of Displacement Fields by using DISTRESS Simulator | http://arxiv.org/pdf/1411.3423v1.pdf | author:Ghulam Mubashar Hassan, Arcady V. Dyskin, Cara K. MacNish category:cs.CV published:2014-11-13 summary:Reconstruction and monitoring of displacement and strain fields is animportant problem in engineering. We analyze the remote and non-obtrusivemethods of strain measurement based on photogrammetry and Digital ImageCorrelation (DIC). The method is based on covering the photographed surfacewith a pattern of speckles and comparing the images taken before and after thedeformation. In this study, a comprehensive literature review and comparativeanalysis of photogrammetric solutions is presented. The analysis is based on aspecially developed Digital Image Synthesizer To Reconstruct Strain in Solids(DISTRESS) Simulator to generate synthetic images of displacement and stressfields in order to investigate the intrinsic accuracy of the existing variantsof DIC. We investigated the Basic DIC and a commercial software VIC 2D, bothbased on displacement field reconstruction with post processing straindetermination based on numerical differentiation. We also investigated what wecall the Extended DIC where the strain field is determined independently of thedisplacement field. While the Basic DIC and VIC 2D are faster, the Extended DICdelivers the best accuracy of strain reconstruction. The speckle pattern isfound to be playing a critical role in achieving high accuracy for DIC.Increase in subset size for DIC does not significantly improves the accuracy,while the smallest subset size depends on the speckle pattern and speckle size.Increase in the overall image size provides more details but does not playsignificant role in improving the accuracy, while significantly increasing thecomputation cost.
arxiv-8100-256 | Multi-view Anomaly Detection via Probabilistic Latent Variable Models | http://arxiv.org/pdf/1411.3413v1.pdf | author:Tomoharu Iwata, Makoto Yamada category:stat.ML cs.LG published:2014-11-13 summary:We propose a nonparametric Bayesian probabilistic latent variable model formulti-view anomaly detection, which is the task of finding instances that haveinconsistent views. With the proposed model, all views of a non-anomalousinstance are assumed to be generated from a single latent vector. On the otherhand, an anomalous instance is assumed to have multiple latent vectors, and itsdifferent views are generated from different latent vectors. By inferring thenumber of latent vectors used for each instance with Dirichlet process priors,we obtain multi-view anomaly scores. The proposed model can be seen as a robustextension of probabilistic canonical correlation analysis for noisy multi-viewdata. We present Bayesian inference procedures for the proposed model based ona stochastic EM algorithm. The effectiveness of the proposed model isdemonstrated in terms of performance when detecting multi-view anomalies andimputing missing values in multi-view data with anomalies.
arxiv-8100-257 | Person Re-identification Based on Color Histogram and Spatial Configuration of Dominant Color Regions | http://arxiv.org/pdf/1411.3410v1.pdf | author:Kwangchol Jang, Sokmin Han, Insong Kim category:cs.CV published:2014-11-13 summary:There is a requirement to determine whether a given person of interest hasalready been observed over a network of cameras in video surveillance systems.A human appearance obtained in one camera is usually different from the onesobtained in another camera due to difference in illumination, pose andviewpoint, camera parameters. Being related to appearance-based approaches forperson re-identification, we propose a novel method based on the dominant colorhistogram and spatial configuration of dominant color regions on human bodyparts. Dominant color histogram and spatial configuration of the dominant colorregions based on dominant color descriptor(DCD) can be considered to be robustto illumination and pose, viewpoint changes. The proposed method is evaluatedusing benchmark video datasets. Experimental results using the cumulativematching characteristic(CMC) curve demonstrate the effectiveness of ourapproach for person re-identification.
arxiv-8100-258 | A Randomized Algorithm for CCA | http://arxiv.org/pdf/1411.3409v1.pdf | author:Paul Mineiro, Nikos Karampatziakis category:stat.ML cs.LG published:2014-11-13 summary:We present RandomizedCCA, a randomized algorithm for computing canonicalanalysis, suitable for large datasets stored either out of core or on adistributed file system. Accurate results can be obtained in as few as two datapasses, which is relevant for distributed processing frameworks in whichiteration is expensive (e.g., Hadoop). The strategy also provides an excellentinitializer for standard iterative solutions.
arxiv-8100-259 | Universal Memcomputing Machines | http://arxiv.org/pdf/1405.0931v2.pdf | author:Fabio L. Traversa, Massimiliano Di Ventra category:cs.NE cs.ET cs.IT math.IT published:2014-05-05 summary:We introduce the notion of universal memcomputing machines (UMMs): a class ofbrain-inspired general-purpose computing machines based on systems with memory,whereby processing and storing of information occur on the same physicallocation. We analytically prove that the memory properties of UMMs endow themwith universal computing power - they are Turing-complete -, intrinsicparallelism, functional polymorphism, and information overhead, namely theircollective states can support exponential data compression directly in memory.We also demonstrate that a UMM has the same computational power as anon-deterministic Turing machine, namely it can solve NP--complete problems inpolynomial time. However, by virtue of its information overhead, a UMM needsonly an amount of memory cells (memprocessors) that grows polynomially with theproblem size. As an example we provide the polynomial-time solution of thesubset-sum problem and a simple hardware implementation of the same. Eventhough these results do not prove the statement NP=P within the Turingparadigm, the practical realization of these UMMs would represent a paradigmshift from present von Neumann architectures bringing us closer to brain-likeneural computation.
arxiv-8100-260 | Two-Stream Convolutional Networks for Action Recognition in Videos | http://arxiv.org/pdf/1406.2199v2.pdf | author:Karen Simonyan, Andrew Zisserman category:cs.CV published:2014-06-09 summary:We investigate architectures of discriminatively trained deep ConvolutionalNetworks (ConvNets) for action recognition in video. The challenge is tocapture the complementary information on appearance from still frames andmotion between frames. We also aim to generalise the best performinghand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNetarchitecture which incorporates spatial and temporal networks. Second, wedemonstrate that a ConvNet trained on multi-frame dense optical flow is able toachieve very good performance in spite of limited training data. Finally, weshow that multi-task learning, applied to two different action classificationdatasets, can be used to increase the amount of training data and improve theperformance on both. Our architecture is trained and evaluated on the standard video actionsbenchmarks of UCF-101 and HMDB-51, where it is competitive with the state ofthe art. It also exceeds by a large margin previous attempts to use deep netsfor video classification.
arxiv-8100-261 | Statistically Significant Detection of Linguistic Change | http://arxiv.org/pdf/1411.3315v1.pdf | author:Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, Steven Skiena category:cs.CL cs.IR cs.LG H.3.3; I.2.6 published:2014-11-12 summary:We propose a new computational approach for tracking and detectingstatistically significant linguistic shifts in the meaning and usage of words.Such linguistic shifts are especially prevalent on the Internet, where therapid exchange of ideas can quickly change a word's meaning. Our meta-analysisapproach constructs property time series of word usage, and then usesstatistically sound change point detection algorithms to identify significantlinguistic shifts. We consider and analyze three approaches of increasing complexity to generatesuch linguistic property time series, the culmination of which usesdistributional characteristics inferred from word co-occurrences. Usingrecently proposed deep neural language models, we first train vectorrepresentations of words for each time period. Second, we warp the vectorspaces into one unified coordinate system. Finally, we construct adistance-based distributional time series for each word to track it'slinguistic displacement over time. We demonstrate that our approach is scalable by tracking linguistic changeacross years of micro-blogging using Twitter, a decade of product reviews usinga corpus of movie reviews from Amazon, and a century of written books using theGoogle Book-ngrams. Our analysis reveals interesting patterns of language usagechange commensurate with each medium.
arxiv-8100-262 | Using Gaussian Measures for Efficient Constraint Based Clustering | http://arxiv.org/pdf/1411.3302v1.pdf | author:Chandrima Sarkar, Atanu Roy category:cs.LG cs.IR published:2014-11-12 summary:In this paper we present a novel iterative multiphase clustering techniquefor efficiently clustering high dimensional data points. For this purpose weimplement clustering feature (CF) tree on a real data set and a Gaussiandensity distribution constraint on the resultant CF tree. The post processingby the application of Gaussian density distribution function on themicro-clusters leads to refinement of the previously formed clusters thusimproving their quality. This algorithm also succeeds in overcoming theinherent drawbacks of conventional hierarchical methods of clustering likeinability to undo the change made to the dendogram of the data points.Moreover, the constraint measure applied in the algorithm makes this clusteringtechnique suitable for need driven data analysis. We provide veracity of ourclaim by evaluating our algorithm with other similar clustering algorithms.Introduction
arxiv-8100-263 | Using Ants as a Genetic Crossover Operator in GLS to Solve STSP | http://arxiv.org/pdf/1411.3277v1.pdf | author:Hassan Ismkhan category:cs.NE published:2014-11-12 summary:Ant Colony Algorithm (ACA) and Genetic Local Search (GLS) are twooptimization algorithms that have been successfully applied to the TravelingSalesman Problem (TSP). In this paper we define new crossover operator thenredefine ACAs ants as operate according to defined crossover operator then putforward our GLS that uses these ants to solve Symmetric TSP (STSP) instances.
arxiv-8100-264 | Identification of Helicopter Dynamics based on Flight Data using Nature Inspired Techniques | http://arxiv.org/pdf/1411.3251v1.pdf | author:S. N. Omkar, Dheevatsa Mudigere, J Senthilnath, M. Vijaya Kumar category:cs.CE cs.NE published:2014-11-12 summary:The complexity of helicopter flight dynamics makes modeling and helicoptersystem identification a very difficult task. Most of the traditional techniquesrequire a model structure to be defined apriori and in case of helicopterdynamics, this is difficult due to its complexity and the interplay betweenvarious subsystems.To overcome this difficulty, non-parametric approaches arecommonly adopted for helicopter system identification. Artificial NeuralNetwork are a widely used class of algorithms for non-parametric systemidentification, among them, the Nonlinear Auto Regressive eXogeneous inputnetwork (NARX) model is very popular, but it also necessitates some in depthknowledge regarding the system being modeled. There have been many approachesproposed to circumvent this and yet still retain the advantageouscharacteristics. In this paper we carry out an extensive study of one suchnewly proposed approach using a modified NARX model with a two tiered,externally driven recurrent neural network architecture. This is coupled withan outer optimization routine for evolving the order of the system. Thisgeneric architecture is comprehensively explored to ascertain its usability andcritically asses its potential. Different instantiations of this architecture,based on nature inspired computational techniques (Artificial Bee Colony,Artificial Immune System and Particle Swarm Optimization) are evaluated andcritically compared in this paper. Simulations have been carried out foridentifying the longitudinally uncoupled dynamics. Results of identificationindicate a quite close correlation between the actual and the predictedresponse of the helicopter for all the models.
arxiv-8100-265 | Map Matching based on Conditional Random Fields and Route Preference Mining for Uncertain Trajectories | http://arxiv.org/pdf/1410.4461v2.pdf | author:Xu Ming, Du Yi-man, Wu Jian-ping, Zhou Yang category:cs.NI cs.LG published:2014-10-16 summary:In order to improve offline map matching accuracy of low-sampling-rate GPS, amap matching algorithm based on conditional random fields (CRF) and routepreference mining is proposed. In this algorithm, road offset distance and thetemporal-spatial relationship between the sampling points are used as featuresof GPS trajectory in CRF model, which can utilize the advantages of integratingthe context information into features flexibly. When the sampling rate is toolow, it is difficult to guarantee the effectiveness using temporal-spatialcontext modeled in CRF, and route preference of a driver is used asreplenishment to be superposed on the temporal-spatial transition features. Theexperimental results show that this method can improve the accuracy of thematching, especially in the case of low sampling rate.
arxiv-8100-266 | A Semidefinite Programming Based Search Strategy for Feature Selection with Mutual Information Measure | http://arxiv.org/pdf/1409.7384v2.pdf | author:Tofigh Naghibi, Sarah Hoffmann, Beat Pfister category:cs.LG published:2014-09-25 summary:Feature subset selection, as a special case of the general subset selectionproblem, has been the topic of a considerable number of studies due to thegrowing importance of data-mining applications. In the feature subset selectionproblem there are two main issues that need to be addressed: (i) Finding anappropriate measure function than can be fairly fast and robustly computed forhigh-dimensional data. (ii) A search strategy to optimize the measure over thesubset space in a reasonable amount of time. In this article mutual informationbetween features and class labels is considered to be the measure function. Twoseries expansions for mutual information are proposed, and it is shown thatmost heuristic criteria suggested in the literature are truncatedapproximations of these expansions. It is well-known that searching the wholesubset space is an NP-hard problem. Here, instead of the conventionalsequential search algorithms, we suggest a parallel search strategy based onsemidefinite programming (SDP) that can search through the subset space inpolynomial time. By exploiting the similarities between the proposed algorithmand an instance of the maximum-cut problem in graph theory, the approximationratio of this algorithm is derived and is compared with the approximation ratioof the backward elimination method. The experiments show that it can bemisleading to judge the quality of a measure solely based on the classificationaccuracy, without taking the effect of the non-optimum search strategy intoaccount.
arxiv-8100-267 | On Coarse Graining of Information and Its Application to Pattern Recognition | http://arxiv.org/pdf/1411.3169v1.pdf | author:Ali Ghaderi category:cs.CV stat.ML published:2014-11-12 summary:We propose a method based on finite mixture models for classifying a set ofobservations into number of different categories. In order to demonstrate themethod, we show how the component densities for the mixture model can bederived by using the maximum entropy method in conjunction with conservation ofPythagorean means. Several examples of distributions belonging to thePythagorean family are derived. A discussion on estimation of model parametersand the number of categories is also given.
arxiv-8100-268 | Distributed Representations for Compositional Semantics | http://arxiv.org/pdf/1411.3146v1.pdf | author:Karl Moritz Hermann category:cs.CL published:2014-11-12 summary:The mathematical representation of semantics is a key issue for NaturalLanguage Processing (NLP). A lot of research has been devoted to finding waysof representing the semantics of individual words in vector spaces.Distributional approaches --- meaning distributed representations that exploitco-occurrence statistics of large corpora --- have proved popular andsuccessful across a number of tasks. However, natural language usually comes instructures beyond the word level, with meaning arising not only from theindividual words but also the structure they are contained in at the phrasal orsentential level. Modelling the compositional process by which the meaning ofan utterance arises from the meaning of its parts is an equally fundamentaltask of NLP. This dissertation explores methods for learning distributed semanticrepresentations and models for composing these into representations for largerlinguistic units. Our underlying hypothesis is that neural models are asuitable vehicle for learning semantically rich representations and that suchrepresentations in turn are suitable vehicles for solving important tasks innatural language processing. The contribution of this thesis is a thoroughevaluation of our hypothesis, as part of which we introduce several newapproaches to representation learning and compositional semantics, as well asmultiple state-of-the-art models which apply distributed semanticrepresentations to various tasks in NLP.
arxiv-8100-269 | Exact Estimation of Multiple Directed Acyclic Graphs | http://arxiv.org/pdf/1404.1238v3.pdf | author:Chris J. Oates, Jim Q. Smith, Sach Mukherjee, James Cussens category:stat.ML published:2014-04-04 summary:This paper considers the problem of estimating the structure of multiplerelated directed acyclic graph (DAG) models. Building on recent developments inexact estimation of DAGs using integer linear programming (ILP), we present anILP approach for joint estimation over multiple DAGs, that does not requirethat the vertices in each DAG share a common ordering. Furthermore, we allowalso for (potentially unknown) dependency structure between the DAGs. Resultsare presented on both simulated data and fMRI data obtained from multiplesubjects.
arxiv-8100-270 | Collecting Image Description Datasets using Crowdsourcing | http://arxiv.org/pdf/1411.3041v1.pdf | author:Ramakrishna Vedantam, C. Lawrence Zitnick, Devi Parikh category:cs.CV published:2014-11-12 summary:We describe our two new datasets with images described by humans. Both thedatasets were collected using Amazon Mechanical Turk, a crowdsourcing platform.The two datasets contain significantly more descriptions per image than otherexisting datasets. One is based on a popular image description dataset calledthe UIUC Pascal Sentence Dataset, whereas the other is based on the AbstractScenes dataset con- taining images made from clipart objects. In this paper wedescribe our interfaces, analyze some properties of and show exampledescriptions from our two datasets.
arxiv-8100-271 | Projecting Markov Random Field Parameters for Fast Mixing | http://arxiv.org/pdf/1411.1119v3.pdf | author:Xianghang Liu, Justin Domke category:cs.LG stat.ML published:2014-11-05 summary:Markov chain Monte Carlo (MCMC) algorithms are simple and extremely powerfultechniques to sample from almost arbitrary distributions. The flaw in practiceis that it can take a large and/or unknown amount of time to converge to thestationary distribution. This paper gives sufficient conditions to guaranteethat univariate Gibbs sampling on Markov Random Fields (MRFs) will be fastmixing, in a precise sense. Further, an algorithm is given to project onto thisset of fast-mixing parameters in the Euclidean norm. Following recent work, wegive an example use of this to project in various divergence measures,comparing univariate marginals obtained by sampling after projection to commonvariational methods and Gibbs sampling on the original parameters.
arxiv-8100-272 | A Novel Test for Additivity in Supervised Ensemble Learners | http://arxiv.org/pdf/1406.1845v2.pdf | author:Lucas Mentch, Giles Hooker category:stat.ML stat.AP published:2014-06-07 summary:Additive models remain popular statistical tools due to their ease ofinterpretation and as a result, hypothesis tests for additivity have beendeveloped to assess the appropriateness of these models. However, as data growsin size and complexity, learning algorithms continue to gain popularity due totheir exceptional predictive performance. Due to the black-box nature of theselearning methods, the increase in predictive power is assumed to come at thecost of interpretability and inference. However, recent work suggests that manypopular learning techniques, such as bagged trees and random forests, havedesirable asymptotic properties which allow for formal statistical inferencewhen base learners are built with proper subsamples. This work extendshypothesis tests previously developed and demonstrates that by enforcing a gridstructure on an appropriate test set, we may perform formal hypothesis testsfor additivity among features. We develop notions of total and partialadditivity and demonstrate that both tests can be carried out at no additionalcomputational cost. We also suggest a new testing procedure based on randomprojections that allows for testing on larger grids, even when the grid size islarger than that of the training set. Simulations and demonstrations on realdata are provided.
arxiv-8100-273 | Bounded Regret for Finite-Armed Structured Bandits | http://arxiv.org/pdf/1411.2919v1.pdf | author:Tor Lattimore, Remi Munos category:cs.LG published:2014-11-11 summary:We study a new type of K-armed bandit problem where the expected return ofone arm may depend on the returns of other arms. We present a new algorithm forthis general class of problems and show that under certain circumstances it ispossible to achieve finite expected cumulative regret. We also giveproblem-dependent lower bounds on the cumulative regret showing that at leastin special cases the new algorithm is nearly optimal.
arxiv-8100-274 | Marginal Pseudo-Likelihood Learning of Markov Network structures | http://arxiv.org/pdf/1401.4988v2.pdf | author:Johan Pensar, Henrik Nyman, Juha Niiranen, Jukka Corander category:stat.ML published:2014-01-20 summary:Undirected graphical models known as Markov networks are popular for a widevariety of applications ranging from statistical physics to computationalbiology. Traditionally, learning of the network structure has been done underthe assumption of chordality which ensures that efficient scoring methods canbe used. In general, non-chordal graphs have intractable normalizing constantswhich renders the calculation of Bayesian and other scores difficult beyondvery small-scale systems. Recently, there has been a surge of interest towardsthe use of regularized pseudo-likelihood methods for structural learning oflarge-scale Markov network models, as such an approach avoids the assumption ofchordality. The currently available methods typically necessitate the use of atuning parameter to adapt the level of regularization for a particular dataset,which can be optimized for example by cross-validation. Here we introduce aBayesian version of pseudo-likelihood scoring of Markov networks, which enablesan automatic regularization through marginalization over the nuisanceparameters in the model. We prove consistency of the resulting MPL estimatorfor the network structure via comparison with the pseudo information criterion.Identification of the MPL-optimal network on a prescanned graph space isconsidered with both greedy hill climbing and exact pseudo-Boolean optimizationalgorithms. We find that for reasonable sample sizes the hill climbing approachmost often identifies networks that are at a negligible distance from therestricted global optimum. Using synthetic and existing benchmark networks, themarginal pseudo-likelihood method is shown to generally perform favorablyagainst recent popular inference methods for Markov networks.
arxiv-8100-275 | Accelerating the ANT Colony Optimization By Smart ANTs, Using Genetic Operator | http://arxiv.org/pdf/1411.2897v1.pdf | author:Hassan Ismkhan category:cs.NE published:2014-11-11 summary:This paper research review Ant colony optimization (ACO) and GeneticAlgorithm (GA), both are two powerful meta-heuristics. This paper explains somemajor defects of these two algorithm at first then proposes a new model for ACOin which, artificial ants use a quick genetic operator and accelerate theiractions in selecting next state. Experimental results show that proposed hybridalgorithm is effective and its performance including speed and accuracy beatsother version.
arxiv-8100-276 | Detecting Figures and Part Labels in Patents: Competition-Based Development of Image Processing Algorithms | http://arxiv.org/pdf/1410.6751v3.pdf | author:Christoph Riedl, Richard Zanibbi, Marti A. Hearst, Siyu Zhu, Michael Menietti, Jason Crusan, Ivan Metelsky, Karim R. Lakhani category:cs.CV cs.IR published:2014-10-24 summary:We report the findings of a month-long online competition in whichparticipants developed algorithms for augmenting the digital version of patentdocuments published by the United States Patent and Trademark Office (USPTO).The goal was to detect figures and part labels in U.S. patent drawing pages.The challenge drew 232 teams of two, of which 70 teams (30%) submittedsolutions. Collectively, teams submitted 1,797 solutions that were compiled onthe competition servers. Participants reported spending an average of 63 hoursdeveloping their solutions, resulting in a total of 5,591 hours of developmenttime. A manually labeled dataset of 306 patents was used for training, onlinesystem tests, and evaluation. The design and performance of the top-5 systemsare presented, along with a system developed after the competition whichillustrates that winning teams produced near state-of-the-art results understrict time and computation constraints. For the 1st place system, the harmonicmean of recall and precision (f-measure) was 88.57% for figure regiondetection, 78.81% for figure regions with correctly recognized figure titles,and 70.98% for part label detection and character recognition. Data andsoftware from the competition are available through the online UCI MachineLearning repository to inspire follow-on work by the image processingcommunity.
arxiv-8100-277 | Turn Down that Noise: Synaptic Encoding of Afferent SNR in a Single Spiking Neuron | http://arxiv.org/pdf/1411.2821v1.pdf | author:Saeed Afshar, Libin George, Jonathan Tapson, Andre van Schaik, Philip de Chazal, Tara Julia Hamilton category:cs.NE q-bio.NC published:2014-11-11 summary:We have added a simplified neuromorphic model of Spike Time DependentPlasticity (STDP) to the Synapto-dendritic Kernel Adapting Neuron (SKAN). Theresulting neuron model is the first to show synaptic encoding of afferentsignal to noise ratio in addition to the unsupervised learning of spatiotemporal spike patterns. The neuron model is particularly suitable forimplementation in digital neuromorphic hardware as it does not use any complexmathematical operations and uses a novel approach to achieve synaptichomeostasis. The neurons noise compensation properties are characterized andtested on noise corrupted zeros digits of the MNIST handwritten dataset.Results show the simultaneously learning common patterns in its input datawhile dynamically weighing individual afferent channels based on their signalto noise ratio. Despite its simplicity the interesting behaviors of the neuronmodel and the resulting computational power may offer insights into biologicalsystems.
arxiv-8100-278 | Supervised Classification of Flow Cytometric Samples via the Joint Clustering and Matching (JCM) Procedure | http://arxiv.org/pdf/1411.2820v1.pdf | author:Sharon X. Lee, Geoffrey J. McLachlan, Saumyadipta Pyne category:q-bio.QM stat.ME stat.ML published:2014-11-11 summary:We consider the use of the Joint Clustering and Matching (JCM) procedure forthe supervised classification of a flow cytometric sample with respect to anumber of predefined classes of such samples. The JCM procedure has beenproposed as a method for the unsupervised classification of cells within asample into a number of clusters and in the case of multiple samples, thematching of these clusters across the samples. The two tasks of clustering andmatching of the clusters are performed simultaneously within the JCM framework.In this paper, we consider the case where there is a number of distinct classesof samples whose class of origin is known, and the problem is to classify a newsample of unknown class of origin to one of these predefined classes. Forexample, the different classes might correspond to the types of a particulardisease or to the various health outcomes of a patient subsequent to a courseof treatment. We show and demonstrate on some real datasets how the JCMprocedure can be used to carry out this supervised classification task. Amixture distribution is used to model the distribution of the expressions of afixed set of markers for each cell in a sample with the components in themixture model corresponding to the various populations of cells in thecomposition of the sample. For each class of samples, a class template isformed by the adoption of random-effects terms to model the inter-samplevariation within a class. The classification of a new unclassified sample isundertaken by assigning the unclassified sample to the class that minimizes theKullback-Leibler distance between its fitted mixture density and each classdensity provided by the class templates.
arxiv-8100-279 | Speaker Identification From Youtube Obtained Data | http://arxiv.org/pdf/1411.2795v1.pdf | author:Nitesh Kumar Chaudhary category:cs.SD cs.LG published:2014-11-11 summary:An efficient, and intuitive algorithm is presented for the identification ofspeakers from a long dataset (like YouTube long discussion, Cocktail partyrecorded audio or video).The goal of automatic speaker identification is toidentify the number of different speakers and prepare a model for that speakerby extraction, characterization and speaker-specific information contained inthe speech signal. It has many diverse application specially in the field ofSurveillance, Immigrations at Airport, cyber security, transcription inmulti-source of similar sound source, where it is difficult to assigntranscription arbitrary. The most commonly speech parametrization used inspeaker verification, K-mean, cepstral analysis, is detailed. Gaussian mixturemodeling, which is the speaker modeling technique is then explained. Gaussianmixture models (GMM), perhaps the most robust machine learning algorithm hasbeen introduced examine and judge carefully speaker identification in textindependent. The application or employment of Gaussian mixture models formonitoring & Analysing speaker identity is encouraged by the familiarity,awareness, or understanding gained through experience that Gaussian spectrumdepict the characteristics of speaker's spectral conformational pattern andremarkable ability of GMM to construct capricious densities after that weillustrate 'Expectation maximization' an iterative algorithm which takes somearbitrary value in initial estimation and carry on the iterative process untilthe convergence of value is observed,so by doing various number of experimentswe are able to obtain 79 ~ 82% of identification rate using Vector quantizationand 85 ~ 92.6% of identification rate using GMM modeling by Expectationmaximization parameter estimation depending on variation of parameter.
arxiv-8100-280 | Warranty Cost Estimation Using Bayesian Network | http://arxiv.org/pdf/1411.3197v1.pdf | author:Karamjit Singh, Puneet Agarwal, Gautam Shroff category:cs.AI cs.LG published:2014-11-11 summary:All multi-component product manufacturing companies face the problem ofwarranty cost estimation. Failure rate analysis of components plays a key rolein this problem. Data source used for failure rate analysis has traditionallybeen past failure data of components. However, failure rate analysis can beimproved by means of fusion of additional information, such as symptomsobserved during after-sale service of the product, geographical information(hilly or plains areas), and information from tele-diagnostic analytics. Inthis paper, we propose an approach, which learns dependency betweenpart-failures and symptoms gleaned from such diverse sources of information, topredict expected number of failures with better accuracy. We also indicate howthe optimum warranty period can be computed. We demonstrate, through empiricalresults, that our method can improve the warranty cost estimates significantly.
arxiv-8100-281 | The HAWKwood Database | http://arxiv.org/pdf/1410.4393v2.pdf | author:Christopher Herbon category:cs.CV published:2014-10-16 summary:We present a database consisting of wood pile images, which can be used as abenchmark to evaluate the performance of wood pile detection and surveyingalgorithms. We distinguish six database cate- gories which can be used fordifferent types of algorithms. Images of real and synthetic scenes areprovided, which consist of 7655 images divided into 354 data sets. Depending onthe category the data sets either include ground truth data or forestryspecific measurements with which algorithms may be compared.
arxiv-8100-282 | Inferring User Preferences by Probabilistic Logical Reasoning over Social Networks | http://arxiv.org/pdf/1411.2679v1.pdf | author:Jiwei Li, Alan Ritter, Dan Jurafsky category:cs.SI cs.AI cs.CL cs.LG published:2014-11-11 summary:We propose a framework for inferring the latent attitudes or preferences ofusers by performing probabilistic first-order logical reasoning over the socialnetwork graph. Our method answers questions about Twitter users like {\em Doesthis user like sushi?} or {\em Is this user a New York Knicks fan?} by buildinga probabilistic model that reasons over user attributes (the user's location orgender) and the social network (the user's friends and spouse), via inferenceslike homophily (I am more likely to like sushi if spouse or friends like sushi,I am more likely to like the Knicks if I live in New York). The algorithm usesdistant supervision, semi-supervised data harvesting and vector space models toextract user attributes (e.g. spouse, education, location) and preferences(likes and dislikes) from text. The extracted propositions are then fed into aprobabilistic reasoner (we investigate both Markov Logic and Probabilistic SoftLogic). Our experiments show that probabilistic logical reasoning significantlyimproves the performance on attribute and relation extraction, and alsoachieves an F-score of 0.791 at predicting a users likes or dislikes,significantly better than two strong baselines.
arxiv-8100-283 | Recursive Total Least-Squares Algorithm Based on Inverse Power Method and Dichotomous Coordinate-Descent Iterations | http://arxiv.org/pdf/1408.6141v2.pdf | author:Reza Arablouei, Kutluyıl Doğançay, Stefan Werner category:cs.SY cs.LG published:2014-08-25 summary:We develop a recursive total least-squares (RTLS) algorithm forerrors-in-variables system identification utilizing the inverse power methodand the dichotomous coordinate-descent (DCD) iterations. The proposedalgorithm, called DCD-RTLS, outperforms the previously-proposed RTLSalgorithms, which are based on the line-search method, with reducedcomputational complexity. We perform a comprehensive analysis of the DCD-RTLSalgorithm and show that it is asymptotically unbiased as well as being stablein the mean. We also find a lower bound for the forgetting factor that ensuresmean-square stability of the algorithm and calculate the theoreticalsteady-state mean-square deviation (MSD). We verify the effectiveness of theproposed algorithm and the accuracy of the predicted steady-state MSD viasimulations.
arxiv-8100-284 | Non-crossing dependencies: least effort, not grammar | http://arxiv.org/pdf/1411.2645v1.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL cs.SI physics.soc-ph published:2014-11-10 summary:The use of null hypotheses (in a statistical sense) is common in hardsciences but not in theoretical linguistics. Here the null hypothesis that thelow frequency of syntactic dependency crossings is expected by an arbitraryordering of words is rejected. It is shown that this would require stardependency structures, which are both unrealistic and too restrictive. Thehypothesis of the limited resources of the human brain is revisited. Strongernull hypotheses taking into account actual dependency lengths for thelikelihood of crossings are presented. Those hypotheses suggests that crossingsare likely to reduce when dependencies are shortened. A hypothesis based onpressure to reduce dependency lengths is more parsimonious than a principle ofminimization of crossings or a grammatical ban that is totally dissociated fromthe general and non-linguistic principle of economy.
arxiv-8100-285 | A chain rule for the expected suprema of Gaussian processes | http://arxiv.org/pdf/1411.2635v1.pdf | author:Andreas Maurer category:cs.LG published:2014-11-10 summary:The expected supremum of a Gaussian process indexed by the image of an indexset under a function class is bounded in terms of separate properties of theindex set and the function class. The bound is relevant to the estimation ofnonlinear transformations or the analysis of learning algorithms wheneverhypotheses are chosen from composite classes, as is the case for multi-layermodels.
arxiv-8100-286 | Deep Exponential Families | http://arxiv.org/pdf/1411.2581v1.pdf | author:Rajesh Ranganath, Linpeng Tang, Laurent Charlin, David M. Blei category:stat.ML cs.LG published:2014-11-10 summary:We describe \textit{deep exponential families} (DEFs), a class of latentvariable models that are inspired by the hidden structures used in deep neuralnetworks. DEFs capture a hierarchy of dependencies between latent variables,and are easily generalized to many settings through exponential families. Weperform inference using recent "black box" variational inference techniques. Wethen evaluate various DEFs on text and combine multiple DEFs into a model forpairwise recommendation data. In an extensive study, we show that going beyondone layer improves predictions for DEFs. We demonstrate that DEFs findinteresting exploratory structure in large data sets, and give betterpredictive performance than state-of-the-art models.
arxiv-8100-287 | Towards Optimal Algorithms for Prediction with Expert Advice | http://arxiv.org/pdf/1409.3040v4.pdf | author:Nick Gravin, Yuval Peres, Balasubramanian Sivan category:cs.LG cs.GT math.PR published:2014-09-10 summary:We study the classical problem of prediction with expert advice in theadversarial setting with a geometric stopping time. In 1965, Cover gave theoptimal algorithm for the case of $2$ experts. In this paper, we design theoptimal algorithm, adversary and regret for the case of $3$ experts. Further,we show that the optimal algorithm for $2$ and $3$ experts is a probabilitymatching algorithm (analogous to Thompson sampling) against a particularrandomized adversary. Remarkably, it turns out that this algorithm is not onlyoptimal against this adversary, but also minimax optimal against all possibleadversaries. We establish a constant factor separation between the regrets achieved by theoptimal algorithm and the widely used multiplicative weights algorithm. Alongthe way, we improve the regret lower bounds for the multiplicative weightsalgorithm for an arbitrary number of experts and show that this is tight for$2$ experts. A novel aspect of our analysis is that we develop upper and lower boundssimultaneously, analogous to the primal-dual method. The analysis of theoptimal adversary relies on delicate random walk estimates. We further use thisconnection to develop an improved regret bound for the case of $4$ experts, andprovide a general framework for designing the optimal algorithm for anarbitrary number of experts.
arxiv-8100-288 | Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models | http://arxiv.org/pdf/1411.2539v1.pdf | author:Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel category:cs.LG cs.CL cs.CV published:2014-11-10 summary:Inspired by recent advances in multimodal learning and machine translation,we introduce an encoder-decoder pipeline that learns (a): a multimodal jointembedding space with images and text and (b): a novel language model fordecoding distributed representations from our space. Our pipeline effectivelyunifies joint image-text embedding models with multimodal neural languagemodels. We introduce the structure-content neural language model thatdisentangles the structure of a sentence to its content, conditioned onrepresentations produced by the encoder. The encoder allows one to rank imagesand sentences while the decoder can generate novel descriptions from scratch.Using LSTM to encode sentences, we match the state-of-the-art performance onFlickr8K and Flickr30K without using object detections. We also set new bestresults when using the 19-layer Oxford convolutional network. Furthermore weshow that with linear encoders, the learned embedding space captures multimodalregularities in terms of vector space arithmetic e.g. *image of a blue car* -"blue" + "red" is near images of red cars. Sample captions generated for 800images are made available for comparison.
arxiv-8100-289 | Learning to Generate Networks | http://arxiv.org/pdf/1405.5868v2.pdf | author:James Atwood, Don Towsley, Krista Gile, David Jensen category:cs.LG cs.SI physics.soc-ph published:2014-05-22 summary:We investigate the problem of learning to generate complex networks fromdata. Specifically, we consider whether deep belief networks, dependencynetworks, and members of the exponential random graph family can learn togenerate networks whose complex behavior is consistent with a set of inputexamples. We find that the deep model is able to capture the complex behaviorof small networks, but that no model is able capture this behavior for networkswith more than a handful of nodes.
arxiv-8100-290 | Distributed k-means algorithm | http://arxiv.org/pdf/1312.4176v3.pdf | author:Gabriele Oliva, Roberto Setola, Christoforos N. Hadjicostis category:cs.LG cs.DC published:2013-12-15 summary:In this paper we provide a fully distributed implementation of the k-meansclustering algorithm, intended for wireless sensor networks where each agent isendowed with a possibly high-dimensional observation (e.g., position, humidity,temperature, etc.) The proposed algorithm, by means of one-hop communication,partitions the agents into measure-dependent groups that have small in-groupand large out-group "distances". Since the partitions may not have a relationwith the topology of the network--members of the same clusters may not bespatially close--the algorithm is provided with a mechanism to compute theclusters'centroids even when the clusters are disconnected in severalsub-clusters.The results of the proposed distributed algorithm coincide, interms of minimization of the objective function, with the centralized k-meansalgorithm. Some numerical examples illustrate the capabilities of the proposedsolution.
arxiv-8100-291 | Sparse Estimation with Generalized Beta Mixture and the Horseshoe Prior | http://arxiv.org/pdf/1411.2405v1.pdf | author:Zahra Sabetsarvestani, Hamidreza Amindavar category:cs.IT math.IT stat.ML published:2014-11-10 summary:In this paper, the use of the Generalized Beta Mixture (GBM) and Horseshoedistributions as priors in the Bayesian Compressive Sensing framework isproposed. The distributions are considered in a two-layer hierarchical model,making the corresponding inference problem amenable to Expectation Maximization(EM). We present an explicit, algebraic EM-update rule for the models, yieldingtwo fast and experimentally validated algorithms for signal recovery.Experimental results show that our algorithms outperform state-of-the-artmethods on a wide range of sparsity levels and amplitudes in terms ofreconstruction accuracy, convergence rate and sparsity. The largest improvementcan be observed for sparse signals with high amplitudes.
arxiv-8100-292 | Multi-Task Metric Learning on Network Data | http://arxiv.org/pdf/1411.2337v1.pdf | author:Chen Fang, Daniel N. Rockmore category:stat.ML cs.LG published:2014-11-10 summary:Multi-task learning (MTL) improves prediction performance in differentcontexts by learning models jointly on multiple different, but related tasks.Network data, which are a priori data with a rich relational structure, providean important context for applying MTL. In particular, the explicit relationalstructure implies that network data is not i.i.d. data. Network data also oftencomes with significant metadata (i.e., attributes) associated with each entity(node). Moreover, due to the diversity and variation in network data (e.g.,multi-relational links or multi-category entities), various tasks can beperformed and often a rich correlation exists between them. Learning algorithmsshould exploit all of these additional sources of information for betterperformance. In this work we take a metric-learning point of view for the MTLproblem in the network context. Our approach builds on structure preservingmetric learning (SPML). In particular SPML learns a Mahalanobis distance metricfor node attributes using network structure as supervision, so that the learneddistance function encodes the structure and can be used to predict linkpatterns from attributes. SPML is described for single-task learning on singlenetwork. Herein, we propose a multi-task version of SPML, abbreviated asMT-SPML, which is able to learn across multiple related tasks on multiplenetworks via shared intermediate parametrization. MT-SPML learns a specificmetric for each task and a common metric for all tasks. The task correlation iscarried through the common metric and the individual metrics encode taskspecific information. When combined together, they are structure-preservingwith respect to individual tasks. MT-SPML works on general networks, thus issuitable for a wide variety of problems. In experiments, we challenge MT-SPMLon two real-word problems, where MT-SPML achieves significant improvement.
arxiv-8100-293 | An Improved Tracking using IMU and Vision Fusion for Mobile Augmented Reality Applications | http://arxiv.org/pdf/1411.2335v1.pdf | author:Kriti Kumar, Ashley Varghese, Pavan K Reddy, N Narendra, Prashanth Swamy, M Girish Chandra, P Balamuralidhar category:cs.CV published:2014-11-10 summary:Mobile Augmented Reality (MAR) is becoming an important cyber-physical systemapplication given the ubiquitous availability of mobile phones. With the needto operate in unprepared environments, accurate and robust registration andtracking has become an important research problem to solve. In fact, when MARis used for tele-interactive applications involving large distances, say froman accident site to insurance office, tracking at both the ends is desirableand further it is essential to appropriately fuse inertial and vision sensorsdata. In this paper, we present results and discuss some insights gained inmarker-less tracking during the development of a prototype pertaining to anexample use case related to breakdown or damage assessment of a vehicle. Thenovelty of this paper is in bringing together different components and moduleswith appropriate enhancements towards a complete working system.
arxiv-8100-294 | Conditional Random Field Autoencoders for Unsupervised Structured Prediction | http://arxiv.org/pdf/1411.1147v2.pdf | author:Waleed Ammar, Chris Dyer, Noah A. Smith category:cs.LG cs.CL published:2014-11-05 summary:We introduce a framework for unsupervised learning of structured predictorswith overlapping, global features. Each input's latent representation ispredicted conditional on the observable data using a feature-rich conditionalrandom field. Then a reconstruction of the input is (re)generated, conditionalon the latent structure, using models for which maximum likelihood estimationhas a closed-form. Our autoencoder formulation enables efficient learningwithout making unrealistic independence assumptions or restricting the kinds offeatures that can be used. We illustrate insightful connections to traditionalautoencoders, posterior regularization and multi-view learning. We showcompetitive results with instantiations of the model for two canonical NLPtasks: part-of-speech induction and bitext word alignment, and show thattraining our model can be substantially more efficient than comparablefeature-rich baselines.
arxiv-8100-295 | N$^3$LARS: Minimum Redundancy Maximum Relevance Feature Selection for Large and High-dimensional Data | http://arxiv.org/pdf/1411.2331v1.pdf | author:Makoto Yamada, Avishek Saha, Hua Ouyang, Dawei Yin, Yi Chang category:stat.ML cs.LG published:2014-11-10 summary:We propose a feature selection method that finds non-redundant features froma large and high-dimensional data in nonlinear way. Specifically, we propose anonlinear extension of the non-negative least-angle regression (LARS) calledN${}^3$LARS, where the similarity between input and output is measured throughthe normalized version of the Hilbert-Schmidt Independence Criterion (HSIC). Anadvantage of N${}^3$LARS is that it can easily incorporate with map-reduceframeworks such as Hadoop and Spark. Thus, with the help of distributedcomputing, a set of features can be efficiently selected from a large andhigh-dimensional data. Moreover, N${}^3$LARS is a convex method and can find aglobal optimum solution. The effectiveness of the proposed method is firstdemonstrated through feature selection experiments for classification andregression with small and high-dimensional datasets. Finally, we evaluate ourproposed method over a large and high-dimensional biology dataset.
arxiv-8100-296 | Modeling Word Relatedness in Latent Dirichlet Allocation | http://arxiv.org/pdf/1411.2328v1.pdf | author:Xun Wang category:cs.CL cs.AI published:2014-11-10 summary:Standard LDA model suffers the problem that the topic assignment of each wordis independent and word correlation hence is neglected. To address thisproblem, in this paper, we propose a model called Word Related Latent DirichletAllocation (WR-LDA) by incorporating word correlation into LDA topic models.This leads to new capabilities that standard LDA model does not have such asestimating infrequently occurring words or multi-language topic modeling.Experimental results demonstrate the effectiveness of our model compared withstandard LDA.
arxiv-8100-297 | Model-Parallel Inference for Big Topic Models | http://arxiv.org/pdf/1411.2305v1.pdf | author:Xun Zheng, Jin Kyu Kim, Qirong Ho, Eric P. Xing category:cs.DC cs.LG stat.ML published:2014-11-10 summary:In real world industrial applications of topic modeling, the ability tocapture gigantic conceptual space by learning an ultra-high dimensional topicalrepresentation, i.e., the so-called "big model", is becoming the nextdesideratum after enthusiasms on "big data", especially for fine-graineddownstream tasks such as online advertising, where good performances areusually achieved by regression-based predictors built on millions if notbillions of input features. The conventional data-parallel approach fortraining gigantic topic models turns out to be rather inefficient in utilizingthe power of parallelism, due to the heavy dependency on a centralized image of"model". Big model size also poses another challenge on the storage, whereavailable model size is bounded by the smallest RAM of nodes. To address theseissues, we explore another type of parallelism, namely model-parallelism, whichenables training of disjoint blocks of a big topic model in parallel. Byintegrating data-parallelism with model-parallelism, we show that dependenciesbetween distributed elements can be handled seamlessly, achieving not onlyfaster convergence but also an ability to tackle significantly bigger modelsize. We describe an architecture for model-parallel inference of LDA, andpresent a variant of collapsed Gibbs sampling algorithm tailored for it.Experimental results demonstrate the ability of this system to handle topicmodeling with unprecedented amount of 200 billion model variables only on alow-end cluster with very limited computational resources and bandwidth.
arxiv-8100-298 | Trade-Offs in Exploiting Body Morphology for Control: from Simple Bodies and Model-Based Control to Complex Bodies with Model-Free Distributed Control Schemes | http://arxiv.org/pdf/1411.2276v1.pdf | author:Matej Hoffmann, Vincent C. Müller category:cs.RO cs.NE cs.SY published:2014-11-09 summary:Tailoring the design of robot bodies for control purposes is implicitlyperformed by engineers, however, a methodology or set of tools is largelyabsent and optimization of morphology (shape, material properties of robotbodies, etc.) is lagging behind the development of controllers. This has becomeeven more prominent with the advent of compliant, deformable or "soft" bodies.These carry substantial potential regarding their exploitation forcontrol---sometimes referred to as "morphological computation" in the sense ofoffloading computation needed for control to the body. Here, we will argue infavor of a dynamical systems rather than computational perspective on theproblem. Then, we will look at the pros and cons of simple vs. complex bodies,critically reviewing the attractive notion of "soft" bodies automaticallytaking over control tasks. We will address another key dimension of the designspace---whether model-based control should be used and to what extent it isfeasible to develop faithful models for different morphologies.
arxiv-8100-299 | Applications of sampling Kantorovich operators to thermographic images for seismic engineering | http://arxiv.org/pdf/1411.2584v1.pdf | author:Danilo Costarelli, Federico Cluni, Anna Maria Minotti, Gianluca Vinti category:cs.CV math.NA published:2014-11-09 summary:In this paper, we present some applications of the multivariate samplingKantorovich operators $S_w$ to seismic engineering. The mathematical theory ofthese operators, both in the space of continuous functions and in Orliczspaces, show how it is possible to approximate/reconstruct multivariatesignals, such as images. In particular, to obtain applications forthermographic images a mathematical algorithm is developed using MATLAB andmatrix calculus. The setting of Orlicz spaces is important since allow us toreconstruct not necessarily continuous signals by means of $S_w$. Thereconstruction of thermographic images of buildings by our sampling Kantorovichalgorithm allow us to obtain models for the simulation of the behavior ofstructures under seismic action. We analyze a real world case study in term ofstructural analysis and we compare the behavior of the building under seismicaction using various models.
arxiv-8100-300 | Abnormal Object Recognition: A Comprehensive Study | http://arxiv.org/pdf/1411.2214v1.pdf | author:Babak Saleh, Ali Farhadi, Ahmed Elgammal category:cs.CV published:2014-11-09 summary:When describing images, humans tend not to talk about the obvious, but rathermention what they find interesting. We argue that abnormalities and deviationsfrom typicalities are among the most important components that form what isworth mentioning. In this paper we introduce the abnormality detection as arecognition problem and show how to model typicalities and, consequently,meaningful deviations from prototypical properties of categories. Our model canrecognize abnormalities and report the main reasons of any recognizedabnormality. We introduce the abnormality detection dataset and showinteresting results on how to reason about abnormalities.
