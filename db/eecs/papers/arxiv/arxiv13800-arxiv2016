arxiv-1512-06009 | Face Hallucination using Linear Models of Coupled Sparse Support | http://arxiv.org/abs/1512.06009 | id:1512.06009 author:Reuben Farrugia, Christine Guillemot category:cs.CV  published:2015-12-18 summary:Most face super-resolution methods assume that low-resolution and high-resolution manifolds have similar local geometrical structure, hence learn local models on the lowresolution manifolds (e.g. sparse or locally linear embedding models), which are then applied on the high-resolution manifold. However, the low-resolution manifold is distorted by the oneto-many relationship between low- and high- resolution patches. This paper presents a method which learns linear models based on the local geometrical structure on the high-resolution manifold rather than on the low-resolution manifold. For this, in a first step, the low-resolution patch is used to derive a globally optimal estimate of the high-resolution patch. The approximated solution is shown to be close in Euclidean space to the ground-truth but is generally smooth and lacks the texture details needed by state-ofthe-art face recognizers. This first estimate allows us to find the support of the high-resolution manifold using sparse coding (SC), which are then used as support for learning a local projection (or upscaling) model between the low-resolution and the highresolution manifolds using Multivariate Ridge Regression (MRR). Experimental results show that the proposed method outperforms six face super-resolution methods in terms of both recognition and quality. These results also reveal that the recognition and quality are significantly affected by the method used for stitching all super-resolved patches together, where quilting was found to better preserve the texture details which helps to achieve higher recognition rates. version:1
arxiv-1512-05990 | Deformable Distributed Multiple Detector Fusion for Multi-Person Tracking | http://arxiv.org/abs/1512.05990 | id:1512.05990 author:Andy J Ma, Pong C Yuen, Suchi Saria category:cs.CV  published:2015-12-18 summary:This paper addresses fully automated multi-person tracking in complex environments with challenging occlusion and extensive pose variations. Our solution combines multiple detectors for a set of different regions of interest (e.g., full-body and head) for multi-person tracking. The use of multiple detectors leads to fewer miss detections as it is able to exploit the complementary strengths of the individual detectors. While the number of false positives may increase with the increased number of bounding boxes detected from multiple detectors, we propose to group the detection outputs by bounding box location and depth information. For robustness to significant pose variations, deformable spatial relationship between detectors are learnt in our multi-person tracking system. On RGBD data from a live Intensive Care Unit (ICU), we show that the proposed method significantly improves multi-person tracking performance over state-of-the-art methods. version:1
arxiv-1512-05986 | Can Pretrained Neural Networks Detect Anatomy? | http://arxiv.org/abs/1512.05986 | id:1512.05986 author:Vlado Menkovski, Zharko Aleksovski, Axel Saalbach, Hannes Nickisch category:cs.CV cs.AI cs.NE  published:2015-12-18 summary:Convolutional neural networks demonstrated outstanding empirical results in computer vision and speech recognition tasks where labeled training data is abundant. In medical imaging, there is a huge variety of possible imaging modalities and contrasts, where annotated data is usually very scarce. We present two approaches to deal with this challenge. A network pretrained in a different domain with abundant data is used as a feature extractor, while a subsequent classifier is trained on a small target dataset; and a deep architecture trained with heavy augmentation and equipped with sophisticated regularization methods. We test the approaches on a corpus of X-ray images to design an anatomy detection system. version:1
arxiv-1512-05947 | Complexity and Approximation of the Fuzzy K-Means Problem | http://arxiv.org/abs/1512.05947 | id:1512.05947 author:Johannes Blömer, Sascha Brauer, Kathrin Bujna category:cs.LG cs.DS  published:2015-12-18 summary:The fuzzy $K$-means problem is a generalization of the classical $K$-means problem to soft clusterings, i.e. clusterings where each points belongs to each cluster to some degree. Although popular in practice, prior to this work the fuzzy $K$-means problem has not been studied from a complexity theoretic or algorithmic perspective. We show that optimal solutions for fuzzy $K$-means cannot, in general, be expressed by radicals over the input points. Surprisingly, this already holds for very simple inputs in one-dimensional space. Hence, one cannot expect to compute optimal solutions exactly. We give the first $(1+\epsilon)$-approximation algorithms for the fuzzy $K$-means problem. First, we present a deterministic approximation algorithm whose runtime is polynomial in $N$ and linear in the dimension $D$ of the input set, given that $K$ is constant, i.e. a polynomial time approximation algorithm given a fixed $K$. We achieve this result by showing that for each soft clustering there exists a hard clustering with comparable properties. Second, by using techniques known from coreset constructions for the $K$-means problem, we develop a deterministic approximation algorithm that runs in time almost linear in $N$ but exponential in the dimension $D$. We complement these results with a randomized algorithm which imposes some natural restrictions on the input set and whose runtime is comparable to some of the most efficient approximation algorithms for $K$-means, i.e. linear in the number of points and the dimension, but exponential in the number of clusters. version:1
arxiv-1507-02387 | Decentralized Joint-Sparse Signal Recovery: A Sparse Bayesian Learning Approach | http://arxiv.org/abs/1507.02387 | id:1507.02387 author:Saurabh Khanna, Chandra R. Murthy category:cs.LG cs.IT math.IT  published:2015-07-09 summary:This work proposes a decentralized, iterative, Bayesian algorithm called CB-DSBL for in-network estimation of multiple jointly sparse vectors by a network of nodes, using noisy and underdetermined linear measurements. The proposed algorithm exploits the network wide joint sparsity of the un- known sparse vectors to recover them from significantly fewer number of local measurements compared to standalone sparse signal recovery schemes. To reduce the amount of inter-node communication and the associated overheads, the nodes exchange messages with only a small subset of their single hop neighbors. Under this communication scheme, we separately analyze the convergence of the underlying Alternating Directions Method of Multipliers (ADMM) iterations used in our proposed algorithm and establish its linear convergence rate. The findings from the convergence analysis of decentralized ADMM are used to accelerate the convergence of the proposed CB-DSBL algorithm. Using Monte Carlo simulations, we demonstrate the superior signal reconstruction as well as support recovery performance of our proposed algorithm compared to existing decentralized algorithms: DRL-1, DCOMP and DCSP. version:2
arxiv-1510-05407 | Bayesian Inference of Online Social Network Statistics via Lightweight Random Walk Crawls | http://arxiv.org/abs/1510.05407 | id:1510.05407 author:Konstantin Avrachenkov, Bruno Ribeiro, Jithin K. Sreedharan category:cs.SI physics.soc-ph stat.ML  published:2015-10-19 summary:Online social networks (OSN) contain extensive amount of information about the underlying society that is yet to be explored. One of the most feasible technique to fetch information from OSN, crawling through Application Programming Interface (API) requests, poses serious concerns over the the guarantees of the estimates. In this work, we focus on making reliable statistical inference with limited API crawls. Based on regenerative properties of the random walks, we propose an unbiased estimator for the aggregated sum of functions over edges and proved the connection between variance of the estimator and spectral gap. In order to facilitate Bayesian inference on the true value of the estimator, we derive the approximate posterior distribution of the estimate. Later the proposed ideas are validated with numerical experiments on inference problems in real-world networks. version:2
arxiv-1511-02986 | Experimental robustness of Fourier Ptychography phase retrieval algorithms | http://arxiv.org/abs/1511.02986 | id:1511.02986 author:Li-Hao Yeh, Jonathan Dong, Jingshan Zhong, Lei Tian, Michael Chen, Gongguo Tang, Mahdi Soltanolkotabi, Laura Waller category:physics.optics cs.CV  published:2015-11-10 summary:Fourier ptychography is a new computational microscopy technique that provides gigapixel-scale intensity and phase images with both wide field-of-view and high resolution. By capturing a stack of low-resolution images under different illumination angles, a nonlinear inverse algorithm can be used to computationally reconstruct the high-resolution complex field. Here, we compare and classify multiple proposed inverse algorithms in terms of experimental robustness. We find that the main sources of error are noise, aberrations and mis-calibration (i.e. model mis-match). Using simulations and experiments, we demonstrate that the choice of cost function plays a critical role, with amplitude-based cost functions performing better than intensity-based ones. The reason for this is that Fourier ptychography datasets consist of images from both brightfield and darkfield illumination, representing a large range of measured intensities. Both noise (e.g. Poisson noise) and model mis-match errors are shown to scale with intensity. Hence, algorithms that use an appropriate cost function will be more tolerant to both noise and model mis-match. Given these insights, we propose a global Newton's method algorithm which is robust and computationally efficient. Finally, we discuss the impact of procedures for algorithmic correction of aberrations and mis-calibration. version:2
arxiv-1512-05844 | Domain Adaptation and Transfer Learning in StochasticNets | http://arxiv.org/abs/1512.05844 | id:1512.05844 author:Mohammad Javad Shafiee, Parthipan Siva, Paul Fieguth, Alexander Wong category:cs.CV stat.ML  published:2015-12-18 summary:Transfer learning is a recent field of machine learning research that aims to resolve the challenge of dealing with insufficient training data in the domain of interest. This is a particular issue with traditional deep neural networks where a large amount of training data is needed. Recently, StochasticNets was proposed to take advantage of sparse connectivity in order to decrease the number of parameters that needs to be learned, which in turn may relax training data size requirements. In this paper, we study the efficacy of transfer learning on StochasticNet frameworks. Experimental results show ~7% improvement on StochasticNet performance when the transfer learning is applied in training step. version:1
arxiv-1506-05942 | Scene-adaptive Coded Apertures Imaging | http://arxiv.org/abs/1506.05942 | id:1506.05942 author:Xuehui Wang, Jinli Suo, Jingyi Yu, Yongdong Zhang, Qionghai Dai category:cs.CV  published:2015-06-19 summary:Coded aperture imaging systems have recently shown great success in recovering scene depth and extending the depth-of-field. The ideal pattern, however, would have to serve two conflicting purposes: 1) be broadband to ensure robust deconvolution and 2) has sufficient zero-crossings for a high depth discrepancy. This paper presents a simple but effective scene-adaptive coded aperture solution to bridge this gap. We observe that the geometric structures in a natural scene often exhibit only a few edge directions, and the successive frames are closely correlated. Therefore we adopt a spatial partitioning and temporal propagation scheme. In each frame, we address one principal direction by applying depth-discriminative codes along it and broadband codes along its orthogonal direction. Since within a frame only the regions with edge direction corresponding to its aperture code behaves well, we utilize the close among-frame correlation to propagate the high quality single frame results temporally to obtain high performance over the whole image lattice. To physically implement this scheme, we use a Liquid Crystal on Silicon (LCoS) microdisplay that permits fast changing pattern codes. Firstly, we capture the scene with a pinhole and analyze the scene content to determine primary edge orientations. Secondly, we sequentially apply the proposed coding scheme with these orientations in the following frames. Experiments on both synthetic and real scenes show that our technique is able to combine advantages of the state-of-the-art patterns for recovering better quality depth map and all-focus images. version:2
arxiv-1512-05840 | Deep Poisson Factorization Machines: factor analysis for mapping behaviors in journalist ecosystem | http://arxiv.org/abs/1512.05840 | id:1512.05840 author:Pau Perng-Hwa Kung category:cs.CY cs.LG stat.ML  published:2015-12-18 summary:Newsroom in online ecosystem is difficult to untangle. With prevalence of social media, interactions between journalists and individuals become visible, but lack of understanding to inner processing of information feedback loop in public sphere leave most journalists baffled. Can we provide an organized view to characterize journalist behaviors on individual level to know better of the ecosystem? To this end, I propose Poisson Factorization Machine (PFM), a Bayesian analogue to matrix factorization that assumes Poisson distribution for generative process. The model generalizes recent studies on Poisson Matrix Factorization to account temporal interaction which involves tensor-like structure, and label information. Two inference procedures are designed, one based on batch variational EM and another stochastic variational inference scheme that efficiently scales with data size. An important novelty in this note is that I show how to stack layers of PFM to introduce a deep architecture. This work discusses some potential results applying the model and explains how such latent factors may be useful for analyzing latent behaviors for data exploration. version:1
arxiv-1505-03159 | Monocular Object Instance Segmentation and Depth Ordering with CNNs | http://arxiv.org/abs/1505.03159 | id:1505.03159 author:Ziyu Zhang, Alexander G. Schwing, Sanja Fidler, Raquel Urtasun category:cs.CV  published:2015-05-12 summary:In this paper we tackle the problem of instance-level segmentation and depth ordering from a single monocular image. Towards this goal, we take advantage of convolutional neural nets and train them to directly predict instance-level segmentations where the instance ID encodes the depth ordering within image patches. To provide a coherent single explanation of an image we develop a Markov random field which takes as input the predictions of convolutional neural nets applied at overlapping patches of different resolutions, as well as the output of a connected component algorithm. It aims to predict accurate instance-level segmentation and depth ordering. We demonstrate the effectiveness of our approach on the challenging KITTI benchmark and show good performance on both tasks. version:2
arxiv-1512-05808 | Successive Ray Refinement and Its Application to Coordinate Descent for LASSO | http://arxiv.org/abs/1512.05808 | id:1512.05808 author:Jun Liu, Zheng Zhao, Ruiwen Zhang category:cs.LG  published:2015-12-17 summary:Coordinate descent is one of the most popular approaches for solving Lasso and its extensions due to its simplicity and efficiency. When applying coordinate descent to solving Lasso, we update one coordinate at a time while fixing the remaining coordinates. Such an update, which is usually easy to compute, greedily decreases the objective function value. In this paper, we aim to improve its computational efficiency by reducing the number of coordinate descent iterations. To this end, we propose a novel technique called Successive Ray Refinement (SRR). SRR makes use of the following ray continuation property on the successive iterations: for a particular coordinate, the value obtained in the next iteration almost always lies on a ray that starts at its previous iteration and passes through the current iteration. Motivated by this ray-continuation property, we propose that coordinate descent be performed not directly on the previous iteration but on a refined search point that has the following properties: on one hand, it lies on a ray that starts at a history solution and passes through the previous iteration, and on the other hand, it achieves the minimum objective function value among all the points on the ray. We propose two schemes for defining the search point and show that the refined search point can be efficiently obtained. Empirical results for real and synthetic data sets show that the proposed SRR can significantly reduce the number of coordinate descent iterations, especially for small Lasso regularization parameters. version:1
arxiv-1509-04610 | Macau: Scalable Bayesian Multi-relational Factorization with Side Information using MCMC | http://arxiv.org/abs/1509.04610 | id:1509.04610 author:Jaak Simm, Adam Arany, Pooya Zakeri, Tom Haber, Jörg K. Wegner, Vladimir Chupakhin, Hugo Ceulemans, Yves Moreau category:stat.ML  published:2015-09-15 summary:We propose Macau, a powerful and flexible Bayesian factorization method for heterogeneous data. Our model can factorize any set of entities and relations that can be represented by a relational model, including tensors and also multiple relations for each entity. Macau can also incorporate side information, specifically entity and relation features, which are crucial for predicting sparsely observed relations. Macau scales to millions of entity instances, hundred millions of observations, and sparse entity features with millions of dimensions. To achieve the scale up, we specially designed sampling procedure for entity and relation features that relies primarily on noise injection in linear regressions. We show performance and advanced features of Macau in a set of experiments, including challenging drug-protein activity prediction task. version:2
arxiv-1506-04089 | Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences | http://arxiv.org/abs/1506.04089 | id:1506.04089 author:Hongyuan Mei, Mohit Bansal, Matthew R. Walter category:cs.CL cs.AI cs.LG cs.NE cs.RO  published:2015-06-12 summary:We propose a neural sequence-to-sequence model for direction following, a task that is essential to realizing effective autonomous agents. Our alignment-based encoder-decoder model with long short-term memory recurrent neural networks (LSTM-RNN) translates natural language instructions to action sequences based upon a representation of the observable world state. We introduce a multi-level aligner that empowers our model to focus on sentence "regions" salient to the current world state by using multiple abstractions of the input sentence. In contrast to existing methods, our model uses no specialized linguistic resources (e.g., parsers) or task-specific annotations (e.g., seed lexicons). It is therefore generalizable, yet still achieves the best results reported to-date on a benchmark single-sentence dataset and competitive results for the limited-training multi-sentence setting. We analyze our model through a series of ablations that elucidate the contributions of the primary components of our model. version:4
arxiv-1512-05698 | Oracle inequalities for ranking and U-processes with Lasso penalty | http://arxiv.org/abs/1512.05698 | id:1512.05698 author:Wojciech Rejchel category:stat.ML  published:2015-12-17 summary:We investigate properties of estimators obtained by minimization of U-processes with the Lasso penalty in high-dimensional settings. Our attention is focused on the ranking problem that is popular in machine learning. It is related to guessing the ordering between objects on the basis of their observed predictors. We prove the oracle inequality for the excess risk of the considered estimator as well as the bound for the l1 distance between the estimator and the oracle. version:1
arxiv-1505-02865 | Asymptotic Behavior of Minimal-Exploration Allocation Policies: Almost Sure, Arbitrarily Slow Growing Regret | http://arxiv.org/abs/1505.02865 | id:1505.02865 author:Wesley Cowan, Michael N. Katehakis category:stat.ML cs.LG 62L10  published:2015-05-12 summary:The purpose of this paper is to provide further understanding into the structure of the sequential allocation ("stochastic multi-armed bandit", or MAB) problem by establishing probability one finite horizon bounds and convergence rates for the sample (or "pseudo") regret associated with two simple classes of allocation policies $\pi$. For any slowly increasing function $g$, subject to mild regularity constraints, we construct two policies (the $g$-Forcing, and the $g$-Inflated Sample Mean) that achieve a measure of regret of order $ O(g(n))$ almost surely as $n \to \infty$, bound from above and below. Additionally, almost sure upper and lower bounds on the remainder term are established. In the constructions herein, the function $g$ effectively controls the "exploration" of the classical "exploration/exploitation" tradeoff. version:2
arxiv-1511-07889 | rnn : Recurrent Library for Torch | http://arxiv.org/abs/1511.07889 | id:1511.07889 author:Nicholas Léonard, Sagar Waghmare, Yang Wang, Jin-Hwa Kim category:cs.NE  published:2015-11-24 summary:The rnn package provides components for implementing a wide range of Recurrent Neural Networks. It is built withing the framework of the Torch distribution for use with the nn package. The components have evolved from 3 iterations, each adding to the flexibility and capability of the package. All component modules inherit either the AbstractRecurrent or AbstractSequencer classes. Strong unit testing, continued backwards compatibility and access to supporting material are the principles followed during its development. The package is compared against existing implementations of two published papers. version:2
arxiv-1510-02041 | Asymptotically Optimal Sequential Experimentation Under Generalized Ranking | http://arxiv.org/abs/1510.02041 | id:1510.02041 author:Wesley Cowan, Michael N. Katehakis category:stat.ML  published:2015-10-07 summary:We consider the \mnk{classical} problem of a controller activating (or sampling) sequentially from a finite number of $N \geq 2$ populations, specified by unknown distributions. Over some time horizon, at each time $n = 1, 2, \ldots$, the controller wishes to select a population to sample, with the goal of sampling from a population that optimizes some "score" function of its distribution, e.g., maximizing the expected sum of outcomes or minimizing variability. We define a class of \textit{Uniformly Fast (UF)} sampling policies and show, under mild regularity conditions, that there is an asymptotic lower bound for the expected total number of sub-optimal population activations. Then, we provide sufficient conditions under which a UCB policy is UF and asymptotically optimal, since it attains this lower bound. Explicit solutions are provided for a number of examples of interest, including general score functionals on unconstrained Pareto distributions (of potentially infinite mean), and uniform distributions of unknown support. Additional results on bandits of Normal distributions are also provided. version:3
arxiv-1509-02857 | Asymptotically Optimal Multi-Armed Bandit Policies under a Cost Constraint | http://arxiv.org/abs/1509.02857 | id:1509.02857 author:Apostolos N. Burnetas, Odysseas Kanavetas, Michael N. Katehakis category:stat.ML math.OC  published:2015-09-09 summary:We develop asymptotically optimal policies for the multi armed bandit (MAB), problem, under a cost constraint. This model is applicable in situations where each sample (or activation) from a population (bandit) incurs a known bandit dependent cost. Successive samples from each population are iid random variables with unknown distribution. The objective is to design a feasible policy for deciding from which population to sample from, so as to maximize the expected sum of outcomes of $n$ total samples or equivalently to minimize the regret due to lack on information on sample distributions, For this problem we consider the class of feasible uniformly fast (f-UF) convergent policies, that satisfy the cost constraint sample-path wise. We first establish a necessary asymptotic lower bound for the rate of increase of the regret function of f-UF policies. Then we construct a class of f-UF policies and provide conditions under which they are asymptotically optimal within the class of f-UF policies, achieving this asymptotic lower bound. At the end we provide the explicit form of such policies for the case in which the unknown distributions are Normal with unknown means and known variances. version:3
arxiv-1512-05616 | Deep-Spying: Spying using Smartwatch and Deep Learning | http://arxiv.org/abs/1512.05616 | id:1512.05616 author:Tony Beltramelli, Sebastian Risi category:cs.CR cs.CY cs.LG  published:2015-12-17 summary:Wearable technologies are today on the rise, becoming more common and broadly available to mainstream users. In fact, wristband and armband devices such as smartwatches and fitness trackers already took an important place in the consumer electronics market and are becoming ubiquitous. By their very nature of being wearable, these devices, however, provide a new pervasive attack surface threatening users privacy, among others. In the meantime, advances in machine learning are providing unprecedented possibilities to process complex data efficiently. Allowing patterns to emerge from high dimensional unavoidably noisy data. The goal of this work is to raise awareness about the potential risks related to motion sensors built-in wearable devices and to demonstrate abuse opportunities leveraged by advanced neural network architectures. The LSTM-based implementation presented in this research can perform touchlogging and keylogging on 12-keys keypads with above-average accuracy even when confronted with raw unprocessed data. Thus demonstrating that deep neural networks are capable of making keystroke inference attacks based on motion sensors easier to achieve by removing the need for non-trivial pre-processing pipelines and carefully engineered feature extraction strategies. Our results suggest that the complete technological ecosystem of a user can be compromised when a wearable wristband device is worn. version:1
arxiv-1512-05586 | Reconstruction of Enhanced Ultrasound Images From Compressed Measurements Using Simultaneous Direction Method of Multipliers | http://arxiv.org/abs/1512.05586 | id:1512.05586 author:Zhouye Chen, Adrian Basarab, Denis Kouamé category:cs.CV  published:2015-12-17 summary:High resolution ultrasound image reconstruction from a reduced number of measurements is of great interest in ultrasound imaging, since it could enhance both the frame rate and image resolution. Compressive deconvolution, combining compressed sensing and image deconvolution, represents an interesting possibility to consider this challenging task. The model of compressive deconvolution includes, in addition to the compressive sampling matrix, a 2D convolution operator carrying the information on the system point spread function. Through this model, the resolution of reconstructed ultrasound images from compressed measurements mainly depends on three aspects: the acquisition setup, i.e. the incoherence of the sampling matrix, the image regularization, i.e. the sparsity prior, and the optimization technique. In this paper, we mainly focused on the last two aspects. We proposed a novel simultaneous direction method of multipliers-based optimization scheme to invert the linear model, including two regularization terms expressing the sparsity of the RF images in a given basis and the generalized Gaussian statistical assumption on tissue reflectivity functions. The performance of the method is evaluated on both simulated and in vivo data. version:1
arxiv-1512-05509 | An Empirical Comparison of Neural Architectures for Reinforcement Learning in Partially Observable Environments | http://arxiv.org/abs/1512.05509 | id:1512.05509 author:Denis Steckelmacher, Peter Vrancx category:cs.NE cs.AI cs.LG  published:2015-12-17 summary:This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long Short-Term Memory, Gated Recurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures. A variant of fitted Q iteration, based on Advantage values instead of Q values, is also explored. The results show that GRU performs significantly better than LSTM and MUT1 for most of the problems considered, requiring less training episodes and less CPU time before learning a very good policy. Advantage learning also tends to produce better results. version:1
arxiv-1512-05469 | Inferring the Causal Direction Privately | http://arxiv.org/abs/1512.05469 | id:1512.05469 author:Matt J. Kusner, Yu Sun, Karthik Sridharan, Kilian Q. Weinberger category:stat.ML  published:2015-12-17 summary:Causal inference deals with identifying which random variables "cause" or control other random variables. Recent advances on the topic of causal inference based on tools from statistical estimation and machine learning have resulted in practical algorithms for causal inference. Causal inference has the potential to have significant impact on medical research, prevention and control of diseases and identifying influential factors that impact society to name just a few. However, these promising applications for causal inference are often ones that involve sensitive or personal data of users that need to be kept private (e.g. medical records, personal finances). Therefore, there is a need for the development of causal inference methods that preserve data privacy. We study the problem of inferring causality using the current, popular causal inference framework, the additive noise model (ANM) while simultaneously ensuring privacy of the users. We derive a framework that provides differential privacy guarantees for a variety of ANM variants. We run extensive experiments, and demonstrate that our techniques are practical and easy to implement. version:1
arxiv-1502-04631 | Clustering and Inference From Pairwise Comparisons | http://arxiv.org/abs/1502.04631 | id:1502.04631 author:Rui Wu, Jiaming Xu, R. Srikant, Laurent Massoulié, Marc Lelarge, Bruce Hajek category:stat.ML  published:2015-02-16 summary:Given a set of pairwise comparisons, the classical ranking problem computes a single ranking that best represents the preferences of all users. In this paper, we study the problem of inferring individual preferences, arising in the context of making personalized recommendations. In particular, we assume that there are $n$ users of $r$ types; users of the same type provide similar pairwise comparisons for $m$ items according to the Bradley-Terry model. We propose an efficient algorithm that accurately estimates the individual preferences for almost all users, if there are $r \max \{m, n\}\log m \log^2 n$ pairwise comparisons per type, which is near optimal in sample complexity when $r$ only grows logarithmically with $m$ or $n$. Our algorithm has three steps: first, for each user, compute the \emph{net-win} vector which is a projection of its $\binom{m}{2}$-dimensional vector of pairwise comparisons onto an $m$-dimensional linear subspace; second, cluster the users based on the net-win vectors; third, estimate a single preference for each cluster separately. The net-win vectors are much less noisy than the high dimensional vectors of pairwise comparisons and clustering is more accurate after the projection as confirmed by numerical experiments. Moreover, we show that, when a cluster is only approximately correct, the maximum likelihood estimation for the Bradley-Terry model is still close to the true preference. version:2
arxiv-1512-05467 | Unsupervised Feature Construction for Improving Data Representation and Semantics | http://arxiv.org/abs/1512.05467 | id:1512.05467 author:Marian-Andrei Rizoiu, Julien Velcin, Stéphane Lallich category:cs.AI cs.LG  published:2015-12-17 summary:Feature-based format is the main data representation format used by machine learning algorithms. When the features do not properly describe the initial data, performance starts to degrade. Some algorithms address this problem by internally changing the representation space, but the newly-constructed features are rarely comprehensible. We seek to construct, in an unsupervised way, new features that are more appropriate for describing a given dataset and, at the same time, comprehensible for a human user. We propose two algorithms that construct the new features as conjunctions of the initial primitive features or their negations. The generated feature sets have reduced correlations between features and succeed in catching some of the hidden relations between individuals in a dataset. For example, a feature like $sky \wedge \neg building \wedge panorama$ would be true for non-urban images and is more informative than simple features expressing the presence or the absence of an object. The notion of Pareto optimality is used to evaluate feature sets and to obtain a balance between total correlation and the complexity of the resulted feature set. Statistical hypothesis testing is used in order to automatically determine the values of the parameters used for constructing a data-dependent feature set. We experimentally show that our approaches achieve the construction of informative feature sets for multiple datasets. version:1
arxiv-1505-00295 | Dense Optical Flow Prediction from a Static Image | http://arxiv.org/abs/1505.00295 | id:1505.00295 author:Jacob Walker, Abhinav Gupta, Martial Hebert category:cs.CV  published:2015-05-02 summary:Given a scene, what is going to move, and in what direction will it move? Such a question could be considered a non-semantic form of action prediction. In this work, we present a convolutional neural network (CNN) based approach for motion prediction. Given a static image, this CNN predicts the future motion of each and every pixel in the image in terms of optical flow. Our CNN model leverages the data in tens of thousands of realistic videos to train our model. Our method relies on absolutely no human labeling and is able to predict motion based on the context of the scene. Because our CNN model makes no assumptions about the underlying scene, it can predict future optical flow on a diverse set of scenarios. We outperform all previous approaches by large margins. version:2
arxiv-1507-02772 | Riemannian Dictionary Learning and Sparse Coding for Positive Definite Matrices | http://arxiv.org/abs/1507.02772 | id:1507.02772 author:Anoop Cherian, Suvrit Sra category:cs.CV  published:2015-07-10 summary:Data encoded as symmetric positive definite (SPD) matrices frequently arise in many areas of computer vision and machine learning. While these matrices form an open subset of the Euclidean space of symmetric matrices, viewing them through the lens of non-Euclidean Riemannian geometry often turns out to be better suited in capturing several desirable data properties. However, formulating classical machine learning algorithms within such a geometry is often non-trivial and computationally expensive. Inspired by the great success of dictionary learning and sparse coding for vector-valued data, our goal in this paper is to represent data in the form of SPD matrices as sparse conic combinations of SPD atoms from a learned dictionary via a Riemannian geometric approach. To that end, we formulate a novel Riemannian optimization objective for dictionary learning and sparse coding in which the representation loss is characterized via the affine invariant Riemannian metric. We also present a computationally simple algorithm for optimizing our model. Experiments on several computer vision datasets demonstrate superior classification and retrieval performance using our approach when compared to sparse coding via alternative non-Riemannian formulations. version:2
arxiv-1411-4734 | Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture | http://arxiv.org/abs/1411.4734 | id:1411.4734 author:David Eigen, Rob Fergus category:cs.CV  published:2014-11-18 summary:In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks. version:4
arxiv-1408-0145 | A convergence and asymptotic analysis of the generalized symmetric FastICA algorithm | http://arxiv.org/abs/1408.0145 | id:1408.0145 author:Tianwen Wei category:stat.ML  published:2014-08-01 summary:This contribution deals with the generalized symmetric FastICA algorithm in the domain of Independent Component Analysis (ICA). The generalized symmetric version of FastICA has been shown to have the potential to achieve the Cram\'er-Rao Bound (CRB) by allowing the usage of different nonlinearity functions in its parallel implementations of one-unit FastICA. In spite of this appealing property, a rigorous study of the asymptotic error of the generalized symmetric FastICA algorithm is still missing in the community. In fact, all the existing results exhibit certain limitations, such as ignoring the impact of data standardization on the asymptotic statistics or being based on a heuristic approach. In this work, we aim at filling this blank. The first result of this contribution is the characterization of the limits of the generalized symmetric FastICA. It is shown that the algorithm optimizes a function that is a sum of the contrast functions used by traditional one-unit FastICA with a correction of the sign. Based on this characterization, we derive a closed-form analytic expression of the asymptotic covariance matrix of the generalized symmetric FastICA estimator using the method of estimating equation and M-estimator. version:2
arxiv-1411-4738 | Cross-Modal Similarity Learning : A Low Rank Bilinear Formulation | http://arxiv.org/abs/1411.4738 | id:1411.4738 author:Cuicui Kang, Shengcai Liao, Yonghao He, Jian Wang, Wenjia Niu, Shiming Xiang, Chunhong Pan category:cs.MM cs.IR cs.LG  published:2014-11-18 summary:The cross-media retrieval problem has received much attention in recent years due to the rapid increasing of multimedia data on the Internet. A new approach to the problem has been raised which intends to match features of different modalities directly. In this research, there are two critical issues: how to get rid of the heterogeneity between different modalities and how to match the cross-modal features of different dimensions. Recently metric learning methods show a good capability in learning a distance metric to explore the relationship between data points. However, the traditional metric learning algorithms only focus on single-modal features, which suffer difficulties in addressing the cross-modal features of different dimensions. In this paper, we propose a cross-modal similarity learning algorithm for the cross-modal feature matching. The proposed method takes a bilinear formulation, and with the nuclear-norm penalization, it achieves low-rank representation. Accordingly, the accelerated proximal gradient algorithm is successfully imported to find the optimal solution with a fast convergence rate O(1/t^2). Experiments on three well known image-text cross-media retrieval databases show that the proposed method achieves the best performance compared to the state-of-the-art algorithms. version:2
arxiv-1512-05421 | Numerical Demultiplexing of Color Image Sensor Measurements via Non-linear Random Forest Modeling | http://arxiv.org/abs/1512.05421 | id:1512.05421 author:Jason Deglint, Farnoud Kazemzadeh, Daniel Cho, David A. Clausi, Alexander Wong category:cs.CV  published:2015-12-17 summary:The simultaneous capture of imaging data at multiple wavelengths across the electromagnetic spectrum is highly challenging, requiring complex and costly multispectral image sensors. In this study, we introduce a comprehensive framework for performing simultaneous multispectral imaging using conventional image sensors with color filter arrays via numerical demultiplexing of the color image sensor measurements. A numerical forward model characterizing the formation of sensor measurements from light spectra hitting the sensor is constructed based on a comprehensive spectral characterization of the sensor. A numerical demultiplexer is then learned via non-linear random forest modeling based on the forward model. Given the learned numerical demultiplexer, one can then demultiplex simultaneously-acquired measurements made by the image sensor into reflectance intensities at discrete selectable wavelengths, resulting in a higher resolution reflectance spectrum. Simulation and real-world experimental results demonstrate the efficacy of such a method for simultaneous multispectral imaging. version:1
arxiv-1408-1387 | Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing | http://arxiv.org/abs/1408.1387 | id:1408.1387 author:Nihar B. Shah, Dengyong Zhou category:cs.GT cs.HC cs.LG  published:2014-08-06 summary:Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural "no-free-lunch" requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible mechanisms (that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers. We further extend our results to a more general setting in which workers are required to provide a quantized confidence for each question. Interestingly, this unique mechanism takes a "multiplicative" form. The simplicity of the mechanism is an added benefit. In preliminary experiments involving over 900 worker-task pairs, we observe a significant drop in the error rates under this unique mechanism for the same or lower monetary expenditure. version:3
arxiv-1512-01715 | A Restricted Visual Turing Test for Deep Scene and Event Understanding | http://arxiv.org/abs/1512.01715 | id:1512.01715 author:Hang Qi, Tianfu Wu, Mun-Wai Lee, Song-Chun Zhu category:cs.CV cs.AI  published:2015-12-06 summary:This paper presents a restricted visual Turing test (VTT) for story-line based deep understanding in long-term and multi-camera captured videos. Given a set of videos of a scene (such as a multi-room office, a garden, and a parking lot.) and a sequence of story-line based queries, the task is to provide answers either simply in binary form "true/false" (to a polar query) or in an accurate natural language description (to a non-polar query). Queries, polar or non-polar, consist of view-based queries which can be answered from a particular camera view and scene-centered queries which involves joint inference across different cameras. The story lines are collected to cover spatial, temporal and causal understanding of input videos. The data and queries distinguish our VTT from recently proposed visual question answering in images and video captioning. A vision system is proposed to perform joint video and query parsing which integrates different vision modules, a knowledge base and a query engine. The system provides unified interfaces for different modules so that individual modules can be reconfigured to test a new method. We provide a benchmark dataset and a toolkit for ontology guided story-line query generation which consists of about 93.5 hours videos captured in four different locations and 3,426 queries split into 127 story lines. We also provide a baseline implementation and result analyses. version:2
arxiv-1512-05278 | Shape and Spatially-Varying Reflectance Estimation From Virtual Exemplars | http://arxiv.org/abs/1512.05278 | id:1512.05278 author:Zhuo Hui, Aswin C Sankaranarayanan category:cs.CV  published:2015-12-16 summary:This paper addresses the problem of estimating the shape of objects that exhibit spatially-varying reflectance. We assume that multiple images of the object are obtained under a fixed view-point and varying illumination, i.e., the setting of photometric stereo. At the core of our techniques is the assumption that the BRDF at each pixel lies in the non-negative span of a known BRDF dictionary.This assumption enables a per-pixel surface normal and BRDF estimation framework that is computationally tractable and requires no initialization in spite of the underlying problem being non-convex. Our estimation framework first solves for the surface normal at each pixel using a variant of example-based photometric stereo. We design an efficient multi-scale search strategy for estimating the surface normal and subsequently, refine this estimate using a gradient descent procedure. Given the surface normal estimate, we solve for the spatially-varying BRDF by constraining the BRDF at each pixel to be in the span of the BRDF dictionary, here, we use additional priors to further regularize the solution. A hallmark of our approach is that it does not require iterative optimization techniques nor the need for careful initialization, both of which are endemic to most state-of-the-art techniques. We showcase the performance of our technique on a wide range of simulated and real scenes where we outperform competing methods. version:1
arxiv-1512-01124 | Deep Reinforcement Learning with Attention for Slate Markov Decision Processes with High-Dimensional States and Actions | http://arxiv.org/abs/1512.01124 | id:1512.01124 author:Peter Sunehag, Richard Evans, Gabriel Dulac-Arnold, Yori Zwols, Daniel Visentin, Ben Coppin category:cs.AI cs.HC cs.LG  published:2015-12-03 summary:Many real-world problems come with action spaces represented as feature vectors. Although high-dimensional control is a largely unsolved problem, there has recently been progress for modest dimensionalities. Here we report on a successful attempt at addressing problems of dimensionality as high as $2000$, of a particular form. Motivated by important applications such as recommendation systems that do not fit the standard reinforcement learning frameworks, we introduce Slate Markov Decision Processes (slate-MDPs). A Slate-MDP is an MDP with a combinatorial action space consisting of slates (tuples) of primitive actions of which one is executed in an underlying MDP. The agent does not control the choice of this executed action and the action might not even be from the slate, e.g., for recommendation systems for which all recommendations can be ignored. We use deep Q-learning based on feature representations of both the state and action to learn the value of whole slates. Unlike existing methods, we optimize for both the combinatorial and sequential aspects of our tasks. The new agent's superiority over agents that either ignore the combinatorial or sequential long-term value aspect is demonstrated on a range of environments with dynamics from a real-world recommendation system. Further, we use deep deterministic policy gradients to learn a policy that for each position of the slate, guides attention towards the part of the action space in which the value is the highest and we only evaluate actions in this area. The attention is used within a sequentially greedy procedure leveraging submodularity. Finally, we show how introducing risk-seeking can dramatically improve the agents performance and ability to discover more far reaching strategies. version:2
arxiv-1512-05246 | Blockout: Dynamic Model Selection for Hierarchical Deep Networks | http://arxiv.org/abs/1512.05246 | id:1512.05246 author:Calvin Murdock, Zhen Li, Howard Zhou, Tom Duerig category:cs.CV cs.LG  published:2015-12-16 summary:Most deep architectures for image classification--even those that are trained to classify a large number of diverse categories--learn shared image representations with a single model. Intuitively, however, categories that are more similar should share more information than those that are very different. While hierarchical deep networks address this problem by learning separate features for subsets of related categories, current implementations require simplified models using fixed architectures specified via heuristic clustering methods. Instead, we propose Blockout, a method for regularization and model selection that simultaneously learns both the model architecture and parameters. A generalization of Dropout, our approach gives a novel parametrization of hierarchical architectures that allows for structure learning via back-propagation. To demonstrate its utility, we evaluate Blockout on the CIFAR and ImageNet datasets, demonstrating improved classification accuracy, better regularization performance, faster training, and the clear emergence of hierarchical network structures. version:1
arxiv-1512-05245 | Symphony from Synapses: Neocortex as a Universal Dynamical Systems Modeller using Hierarchical Temporal Memory | http://arxiv.org/abs/1512.05245 | id:1512.05245 author:Fergal Byrne category:cs.NE cs.AI q-bio.NC  published:2015-12-16 summary:Reverse engineering the brain is proving difficult, perhaps impossible. While many believe that this is just a matter of time and effort, a different approach might help. Here, we describe a very simple idea which explains the power of the brain as well as its structure, exploiting complex dynamics rather than abstracting it away. Just as a Turing Machine is a Universal Digital Computer operating in a world of symbols, we propose that the brain is a Universal Dynamical Systems Modeller, evolved bottom-up (itself using nested networks of interconnected, self-organised dynamical systems) to prosper in a world of dynamical systems. Recent progress in Applied Mathematics has produced startling evidence of what happens when abstract Dynamical Systems interact. Key latent information describing system A can be extracted by system B from very simple signals, and signals can be used by one system to control and manipulate others. Using these facts, we show how a region of the neocortex uses its dynamics to intrinsically "compute" about the external and internal world. Building on an existing "static" model of cortical computation (Hawkins' Hierarchical Temporal Memory - HTM), we describe how a region of neocortex can be viewed as a network of components which together form a Dynamical Systems modelling module, connected via sensory and motor pathways to the external world, and forming part of a larger dynamical network in the brain. Empirical modelling and simulations of Dynamical HTM are possible with simple extensions and combinations of currently existing open source software. We list a number of relevant projects. version:1
arxiv-1512-05219 | Learning a Hybrid Architecture for Sequence Regression and Annotation | http://arxiv.org/abs/1512.05219 | id:1512.05219 author:Yizhe Zhang, Ricardo Henao, Lawrence Carin, Jianling Zhong, Alexander J. Hartemink category:stat.ML  published:2015-12-16 summary:When learning a hidden Markov model (HMM), sequen- tial observations can often be complemented by real-valued summary response variables generated from the path of hid- den states. Such settings arise in numerous domains, includ- ing many applications in biology, like motif discovery and genome annotation. In this paper, we present a flexible frame- work for jointly modeling both latent sequence features and the functional mapping that relates the summary response variables to the hidden state sequence. The algorithm is com- patible with a rich set of mapping functions. Results show that the availability of additional continuous response vari- ables can simultaneously improve the annotation of the se- quential observations and yield good prediction performance in both synthetic data and real-world datasets. version:1
arxiv-1512-05135 | DNA-Level Splice Junction Prediction using Deep Recurrent Neural Networks | http://arxiv.org/abs/1512.05135 | id:1512.05135 author:Byunghan Lee, Taehoon Lee, Byunggook Na, Sungroh Yoon category:cs.LG q-bio.GN  published:2015-12-16 summary:A eukaryotic gene consists of multiple exons (protein coding regions) and introns (non-coding regions), and a splice junction refers to the boundary between a pair of exon and intron. Precise identification of spice junctions on a gene is important for deciphering its primary structure, function, and interaction. Experimental techniques for determining exon/intron boundaries include RNA-seq, which is often accompanied by computational approaches. Canonical splicing signals are known, but computational junction prediction still remains challenging because of a large number of false positives and other complications. In this paper, we exploit deep recurrent neural networks (RNNs) to model DNA sequences and to detect splice junctions thereon. We test various RNN units and architectures including long short-term memory units, gated recurrent units, and recently proposed iRNN for in-depth design space exploration. According to our experimental results, the proposed approach significantly outperforms not only conventional machine learning-based methods but also a recent state-of-the-art deep belief network-based technique in terms of prediction accuracy. version:1
arxiv-1512-05653 | Effects of GIMP Retinex Filtering Evaluated by the Image Entropy | http://arxiv.org/abs/1512.05653 | id:1512.05653 author:A. C. Sparavigna, R. Marazzato category:cs.CV  published:2015-12-16 summary:A GIMP Retinex filtering can be used for enhancing images, with good results on foggy images, as recently discussed. Since this filter has some parameters that can be adjusted to optimize the output image, several approaches can be decided according to desired results. Here, as a criterion for optimizing the filtering parameters, we consider the maximization of the image entropy. We use, besides the Shannon entropy, also a generalized entropy. version:1
arxiv-1512-05073 | A Novel Minimum Divergence Approach to Robust Speaker Identification | http://arxiv.org/abs/1512.05073 | id:1512.05073 author:Ayanendranath Basu, Smarajit Bose, Amita Pal, Anish Mukherjee, Debasmita Das category:stat.ML cs.SD stat.AP  published:2015-12-16 summary:In this work, a novel solution to the speaker identification problem is proposed through minimization of statistical divergences between the probability distribution (g). of feature vectors from the test utterance and the probability distributions of the feature vector corresponding to the speaker classes. This approach is made more robust to the presence of outliers, through the use of suitably modified versions of the standard divergence measures. The relevant solutions to the minimum distance methods are referred to as the minimum rescaled modified distance estimators (MRMDEs). Three measures were considered - the likelihood disparity, the Hellinger distance and Pearson's chi-square distance. The proposed approach is motivated by the observation that, in the case of the likelihood disparity, when the empirical distribution function is used to estimate g, it becomes equivalent to maximum likelihood classification with Gaussian Mixture Models (GMMs) for speaker classes, a highly effective approach used, for example, by Reynolds [22] based on Mel Frequency Cepstral Coefficients (MFCCs) as features. Significant improvement in classification accuracy is observed under this approach on the benchmark speech corpus NTIMIT and a new bilingual speech corpus NISIS, with MFCC features, both in isolation and in combination with delta MFCC features. Moreover, the ubiquitous principal component transformation, by itself and in conjunction with the principle of classifier combination, is found to further enhance the performance. version:1
arxiv-1511-06727 | Scalable Gradient-Based Tuning of Continuous Regularization Hyperparameters | http://arxiv.org/abs/1511.06727 | id:1511.06727 author:Jelena Luketina, Mathias Berglund, Tapani Raiko category:cs.LG  published:2015-11-20 summary:Hyperparameter selection generally relies on running multiple full training trials, with hyperparameter selection based on validation set performance. We propose a gradient-based approach for locally adjusting hyperparameters on the fly in which we adjust the hyperparameters so as to make the model parameter gradients, and hence updates, more advantageous for the validation cost. We explore the approach for tuning regularization hyperparameters and find that in experiments on MNIST the resulting regularization levels are within the optimal regions. The method is less computationally demanding compared to similar gradient-based approaches to hyperparameter selection, only requires a few trials, and consistently finds solid hyperparameter values which makes it a useful tool for training neural network models. version:2
arxiv-1512-05059 | Streaming Kernel Principal Component Analysis | http://arxiv.org/abs/1512.05059 | id:1512.05059 author:Mina Ghashami, Daniel Perry, Jeff M. Phillips category:cs.DS cs.LG stat.ML  published:2015-12-16 summary:Kernel principal component analysis (KPCA) provides a concise set of basis vectors which capture non-linear structures within large data sets, and is a central tool in data analysis and learning. To allow for non-linear relations, typically a full $n \times n$ kernel matrix is constructed over $n$ data points, but this requires too much space and time for large values of $n$. Techniques such as the Nystr\"om method and random feature maps can help towards this goal, but they do not explicitly maintain the basis vectors in a stream and take more space than desired. We propose a new approach for streaming KPCA which maintains a small set of basis elements in a stream, requiring space only logarithmic in $n$, and also improves the dependence on the error parameter. Our technique combines together random feature maps with recent advances in matrix sketching, it has guaranteed spectral norm error bounds with respect to the original kernel matrix, and it compares favorably in practice to state-of-the-art approaches. version:1
arxiv-1512-05055 | Inferring Gene Regulatory Network Using An Evolutionary Multi-Objective Method | http://arxiv.org/abs/1512.05055 | id:1512.05055 author:Yu Chen, Xiufen Zou category:cs.CE cs.NE q-bio.QM 92B99  published:2015-12-16 summary:Inference of gene regulatory networks (GRNs) based on experimental data is a challenging task in bioinformatics. In this paper, we present a bi-objective minimization model (BoMM) for inference of GRNs, where one objective is the fitting error of derivatives, and the other is the number of connections in the network. To solve the BoMM efficiently, we propose a multi-objective evolutionary algorithm (MOEA), and utilize the separable parameter estimation method (SPEM) decoupling the ordinary differential equation (ODE) system. Then, the Akaike Information Criterion (AIC) is employed to select one inference result from the obtained Pareto set. Taking the S-system as the investigated GRN model, our method can properly identify the topologies and parameter values of benchmark systems. There is no need to preset problem-dependent parameter values to obtain appropriate results, and thus, our method could be applicable to inference of various GRNs models. version:1
arxiv-1511-01804 | Wood Species Recognition Based on SIFT Keypoint Histogram | http://arxiv.org/abs/1511.01804 | id:1511.01804 author:Shuaiqi Hu, Ke Li, Xudong Bao category:cs.CV  published:2015-11-05 summary:Traditionally, only experts who are equipped with professional knowledge and rich experience are able to recognize different species of wood. Applying image processing techniques for wood species recognition can not only reduce the expense to train qualified identifiers, but also increase the recognition accuracy. In this paper, a wood species recognition technique base on Scale Invariant Feature Transformation (SIFT) keypoint histogram is proposed. We use first the SIFT algorithm to extract keypoints from wood cross section images, and then k-means and k-means++ algorithms are used for clustering. Using the clustering results, an SIFT keypoints histogram is calculated for each wood image. Furthermore, several classification models, including Artificial Neural Networks (ANN), Support Vector Machine (SVM) and K-Nearest Neighbor (KNN) are used to verify the performance of the method. Finally, through comparing with other prevalent wood recognition methods such as GLCM and LBP, results show that our scheme achieves higher accuracy. version:4
arxiv-1512-00818 | Zero-Shot Event Detection by Multimodal Distributional Semantic Embedding of Videos | http://arxiv.org/abs/1512.00818 | id:1512.00818 author:Mohamed Elhoseiny, Jingen Liu, Hui Cheng, Harpreet Sawhney, Ahmed Elgammal category:cs.CV cs.CL cs.LG  published:2015-12-02 summary:We propose a new zero-shot Event Detection method by Multi-modal Distributional Semantic embedding of videos. Our model embeds object and action concepts as well as other available modalities from videos into a distributional semantic space. To our knowledge, this is the first Zero-Shot event detection model that is built on top of distributional semantics and extends it in the following directions: (a) semantic embedding of multimodal information in videos (with focus on the visual modalities), (b) automatically determining relevance of concepts/attributes to a free text query, which could be useful for other applications, and (c) retrieving videos by free text event query (e.g., "changing a vehicle tire") based on their content. We embed videos into a distributional semantic space and then measure the similarity between videos and the event query in a free text form. We validated our method on the large TRECVID MED (Multimedia Event Detection) challenge. Using only the event title as a query, our method outperformed the state-of-the-art that uses big descriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC metric. It is also an order of magnitude faster. version:2
arxiv-1507-06350 | Admissibility of a posterior predictive decision rule | http://arxiv.org/abs/1507.06350 | id:1507.06350 author:Giri Gopalan category:stat.ML  published:2015-07-22 summary:Recent decades have seen an interest in prediction problems for which Bayesian methodology has been extremely useful. Sampling from or approximating the posterior predictive distribution in a Bayesian model allows one to make inference about potentially observable random quantities given observed data. The purpose of this note is to use statistical decision theory as a basis to justify the use of a posterior predictive distribution for making a point prediction. version:3
arxiv-1512-05010 | Multiple penalized principal curves: analysis and computation | http://arxiv.org/abs/1512.05010 | id:1512.05010 author:Slav Kirov, Dejan Slepčev category:math.AP cs.CV stat.ML  published:2015-12-15 summary:We study the problem of finding the one-dimensional structure in a given data set. In other words we consider ways to approximate a given measure (data) by curves. We consider an objective functional whose minimizers are a regularization of principal curves and introduce a new functional which allows for multiple curves. We prove the existence of minimizers and establish their basic properties. We develop an efficient algorithm for obtaining (near) minimizers of the functional. While both of the functionals used are nonconvex, we argue that enlarging the configuration space to allow for multiple curves leads to a simpler energy landscape with fewer undesirable (high-energy) local minima. Furthermore we note that the approach proposed is able to find the one-dimensional structure even for data with considerable amount of noise. version:1
arxiv-1506-01520 | An Average Classification Algorithm | http://arxiv.org/abs/1506.01520 | id:1506.01520 author:Brendan van Rooyen, Aditya Krishna Menon, Robert C. Williamson category:stat.ML cs.LG  published:2015-06-04 summary:Many classification algorithms produce a classifier that is a weighted average of kernel evaluations. When working with a high or infinite dimensional kernel, it is imperative for speed of evaluation and storage issues that as few training samples as possible are used in the kernel expansion. Popular existing approaches focus on altering standard learning algorithms, such as the Support Vector Machine, to induce sparsity, as well as post-hoc procedures for sparse approximations. Here we adopt the latter approach. We begin with a very simple classifier, given by the kernel mean $$ f(x) = \frac{1}{n} \sum\limits_{i=i}^{n} y_i K(x_i,x) $$ We then find a sparse approximation to this kernel mean via herding. The result is an accurate, easily parallelized algorithm for learning classifiers. version:3
arxiv-1510-03826 | Adopting Robustness and Optimality in Fitting and Learning | http://arxiv.org/abs/1510.03826 | id:1510.03826 author:Zhiguang Wang, Tim Oates, James Lo category:cs.LG cs.NE math.OC  published:2015-10-13 summary:We generalized a modified exponentialized estimator by pushing the robust-optimal (RO) index $\lambda$ to $-\infty$ for achieving robustness to outliers by optimizing a quasi-Minimin function. The robustness is realized and controlled adaptively by the RO index without any predefined threshold. Optimality is guaranteed by expansion of the convexity region in the Hessian matrix to largely avoid local optima. Detailed quantitative analysis on both robustness and optimality are provided. The results of proposed experiments on fitting tasks for three noisy non-convex functions and the digits recognition task on the MNIST dataset consolidate the conclusions. version:3
arxiv-1512-04973 | An Operator for Entity Extraction in MapReduce | http://arxiv.org/abs/1512.04973 | id:1512.04973 author:Ndapandula Nakashole category:cs.DB cs.CL 68T50  published:2015-12-15 summary:Dictionary-based entity extraction involves finding mentions of dictionary entities in text. Text mentions are often noisy, containing spurious or missing words. Efficient algorithms for detecting approximate entity mentions follow one of two general techniques. The first approach is to build an index on the entities and perform index lookups of document substrings. The second approach recognizes that the number of substrings generated from documents can explode to large numbers, to get around this, they use a filter to prune many such substrings which do not match any dictionary entity and then only verify the remaining substrings if they are entity mentions of dictionary entities, by means of a text join. The choice between the index-based approach and the filter & verification-based approach is a case-to-case decision as the best approach depends on the characteristics of the input entity dictionary, for example frequency of entity mentions. Choosing the right approach for the setting can make a substantial difference in execution time. Making this choice is however non-trivial as there are parameters within each of the approaches that make the space of possible approaches very large. In this paper, we present a cost-based operator for making the choice among execution plans for entity extraction. Since we need to deal with large dictionaries and even larger large datasets, our operator is developed for implementations of MapReduce distributed algorithms. version:1
arxiv-1512-04960 | A Light Touch for Heavily Constrained SGD | http://arxiv.org/abs/1512.04960 | id:1512.04960 author:Andrew Cotter, Maya Gupta, Jan Pfeifer category:cs.LG  published:2015-12-15 summary:Projected stochastic gradient descent (SGD) is often the default choice for large-scale optimization in machine learning, but requires a projection after each update. For heavily-constrained objectives, we propose an efficient extension of SGD that stays close to the feasible region while only applying constraints probabilistically at each iteration. Theoretical analysis shows a good trade-off between per-iteration work and the number of iterations needed, indicating compelling advantages on problems with a large number of constraints onto which projecting is expensive. In MATLAB experiments, our algorithm successfully handles a large-scale real-world video ranking problem with tens of thousands of linear inequality constraints that was too large for projected SGD and stochastic Frank-Wolfe. version:1
arxiv-1512-04958 | Context Driven Label Fusion for segmentation of Subcutaneous and Visceral Fat in CT Volumes | http://arxiv.org/abs/1512.04958 | id:1512.04958 author:Sarfaraz Hussein, Aileen Green, Arjun Watane, Georgios Papadakis, Medhat Osman, Ulas Bagci category:cs.CV  published:2015-12-15 summary:Quantification of adipose tissue (fat) from computed tomography (CT) scans is conducted mostly through manual or semi-automated image segmentation algorithms with limited efficacy. In this work, we propose a completely unsupervised and automatic method to identify adipose tissue, and then separate Subcutaneous Adipose Tissue (SAT) from Visceral Adipose Tissue (VAT) at the abdominal region. We offer a three-phase pipeline consisting of (1) Initial boundary estimation using gradient points, (2) boundary refinement using Geometric Median Absolute Deviation and Appearance based Local Outlier Scores (3) Context driven label fusion using Conditional Random Fields (CRF) to obtain the final boundary between SAT and VAT. We evaluate the proposed method on 151 abdominal CT scans and obtain state-of-the-art 94% and 91% dice similarity scores for SAT and VAT segmentation, as well as significant reduction in fat quantification error measure. version:1
arxiv-1512-04937 | Relative Density and Exact Recovery in Heterogeneous Stochastic Block Models | http://arxiv.org/abs/1512.04937 | id:1512.04937 author:Amin Jalali, Qiyang Han, Ioana Dumitriu, Maryam Fazel category:stat.ML  published:2015-12-15 summary:The Stochastic Block Model (SBM) is a widely used random graph model for networks with communities. Despite the recent burst of interest in recovering communities in the SBM from statistical and computational points of view, there are still gaps in understanding the fundamental information theoretic and computational limits of recovery. In this paper, we consider the SBM in its full generality, where there is no restriction on the number and sizes of communities or how they grow with the number of nodes, as well as on the connection probabilities inside or across communities. This generality allows us to move past the artifacts of homogenous SBM, and understand the right parameters (such as the relative densities of communities) that define the various recovery thresholds. We outline the implications of our generalizations via a set of illustrative examples. For instance, $\log n$ is considered to be the standard lower bound on the cluster size for exact recovery via convex methods, for homogenous SBM. We show that it is possible, in the right circumstances (when sizes are spread and the smaller the cluster, the denser), to recover very small clusters (up to $\sqrt{\log n}$ size), if there are just a few of them (at most polylogarithmic in $n$). version:1
arxiv-1512-04906 | Strategies for Training Large Vocabulary Neural Language Models | http://arxiv.org/abs/1512.04906 | id:1512.04906 author:Welin Chen, David Grangier, Michael Auli category:cs.CL cs.LG  published:2015-12-15 summary:Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We further extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney. version:1
arxiv-1412-0781 | Fast Steerable Principal Component Analysis | http://arxiv.org/abs/1412.0781 | id:1412.0781 author:Zhizhen Zhao, Yoel Shkolnisky, Amit Singer category:cs.CV  published:2014-12-02 summary:Cryo-electron microscopy nowadays often requires the analysis of hundreds of thousands of 2D images as large as a few hundred pixels in each direction. Here we introduce an algorithm that efficiently and accurately performs principal component analysis (PCA) for a large set of two-dimensional images, and, for each image, the set of its uniform rotations in the plane and their reflections. For a dataset consisting of $n$ images of size $L \times L$ pixels, the computational complexity of our algorithm is $O(nL^3 + L^4)$, while existing algorithms take $O(nL^4)$. The new algorithm computes the expansion coefficients of the images in a Fourier-Bessel basis efficiently using the non-uniform fast Fourier transform. We compare the accuracy and efficiency of the new algorithm with traditional PCA and existing algorithms for steerable PCA. version:5
arxiv-1511-01887 | Radon-Nikodym approximation in application to image analysis | http://arxiv.org/abs/1511.01887 | id:1511.01887 author:Vladislav Gennadievich Malyshkin category:cs.CV  published:2015-11-05 summary:For an image pixel information can be converted to the moments of some basis $Q_k$, e.g. Fourier-Mellin, Zernike, monomials, etc. Given sufficient number of moments pixel information can be completely recovered, for insufficient number of moments only partial information can be recovered and the image reconstruction is, at best, of interpolatory type. Standard approach is to present interpolated value as a linear combination of basis functions, what is equivalent to least squares expansion. However, recent progress in numerical stability of moments estimation allows image information to be recovered from moments in a completely different manner, applying Radon-Nikodym type of expansion, what gives the result as a ratio of two quadratic forms of basis functions. In contrast with least squares the Radon-Nikodym approach has oscillation near the boundaries very much suppressed and does not diverge outside of basis support. While least squares theory operate with vectors $<fQ_k>$, Radon-Nikodym theory operates with matrices $<fQ_jQ_k>$, what make the approach much more suitable to image transforms and statistical property estimation. version:2
arxiv-1512-03219 | Norm-Free Radon-Nikodym Approach to Machine Learning | http://arxiv.org/abs/1512.03219 | id:1512.03219 author:Vladislav Gennadievich Malyshkin category:cs.LG stat.ML  published:2015-12-10 summary:For Machine Learning (ML) classification problem, where a vector of $\mathbf{x}$--observations (values of attributes) is mapped to a single $y$ value (class label), a generalized Radon--Nikodym type of solution is proposed. Quantum--mechanics --like probability states $\psi^2(\mathbf{x})$ are considered and "Cluster Centers", corresponding to the extremums of $<y\psi^2(\mathbf{x})>/<\psi^2(\mathbf{x})>$, are found from generalized eigenvalues problem. The eigenvalues give possible $y^{[i]}$ outcomes and corresponding to them eigenvectors $\psi^{[i]}(\mathbf{x})$ define "Cluster Centers". The projection of a $\psi$ state, localized at given $\mathbf{x}$ to classify, on these eigenvectors define the probability of $y^{[i]}$ outcome, thus avoiding using a norm ($L^2$ or other types), required for "quality criteria" in a typical Machine Learning technique. A coverage of each `Cluster Center" is calculated, what potentially allows to separate system properties (described by $y^{[i]}$ outcomes) and system testing conditions (described by $C^{[i]}$ coverage). As an example of such application $y$ distribution estimator is proposed in a form of pairs $(y^{[i]},C^{[i]})$, that can be considered as Gauss quadratures generalization. This estimator allows to perform $y$ probability distribution estimation in a strongly non--Gaussian case. version:2
arxiv-1512-04860 | Increasing the Action Gap: New Operators for Reinforcement Learning | http://arxiv.org/abs/1512.04860 | id:1512.04860 author:Marc G. Bellemare, Georg Ostrovski, Arthur Guez, Philip S. Thomas, Rémi Munos category:cs.AI cs.LG  published:2015-12-15 summary:This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators. version:1
arxiv-1512-04857 | Energy-Efficient Classification for Anomaly Detection: The Wireless Channel as a Helper | http://arxiv.org/abs/1512.04857 | id:1512.04857 author:Kiril Ralinovski, Mario Goldenbaum, Sławomir Stańczak category:cs.IT cs.LG math.IT  published:2015-12-15 summary:Anomaly detection has various applications including condition monitoring and fault diagnosis. The objective is to sense the environment, learn the normal system state, and then periodically classify whether the instantaneous state deviates from the normal one or not. A flexible and cost-effective way of monitoring a system state is to use a wireless sensor network. In the traditional approach, the sensors encode their observations and transmit them to a fusion center by means of some interference avoiding channel access method. The fusion center then decodes all the data and classifies the corresponding system state. As this approach can be highly inefficient in terms of energy consumption, in this paper we propose a transmission scheme that exploits interference for carrying out the anomaly detection directly in the air. In other words, the wireless channel helps the fusion center to retrieve the sought classification outcome immediately from the channel output. To achieve this, the chosen learning model is linear support vector machines. After discussing the proposed scheme and proving its reliability, we present numerical examples demonstrating that the scheme reduces the energy consumption for anomaly detection by up to 53% compared to a strategy that uses time division multiple-access. version:1
arxiv-1512-04848 | Data Driven Resource Allocation for Distributed Learning | http://arxiv.org/abs/1512.04848 | id:1512.04848 author:Travis Dick, Mu Li, Venkata Krishna Pillutla, Colin White, Maria Florina Balcan, Alex Smola category:cs.LG cs.DS stat.ML  published:2015-12-15 summary:In distributed machine learning, data is dispatched to multiple machines for processing. Motivated by the fact that similar data points are often belonging to the same or similar classes, and more generally, classification rules of high accuracy tend to be "locally simple but globally complex", we propose data dependent dispatching that takes advantage of such structures. Our main technical contribution is to provide algorithms with provable guarantees for data-dependent dispatching, that partition the data in a way that satisfies important conditions for accurate distributed learning, including fault tolerance and balancedness. We show the effectiveness of our method over the widely used random partitioning scheme in several real world image and advertising datasets. version:1
arxiv-1512-04808 | Causal and anti-causal learning in pattern recognition for neuroimaging | http://arxiv.org/abs/1512.04808 | id:1512.04808 author:Sebastian Weichwald, Bernhard Schölkopf, Tonio Ball, Moritz Grosse-Wentrup category:stat.ML cs.LG q-bio.NC stat.ME  published:2015-12-15 summary:Pattern recognition in neuroimaging distinguishes between two types of models: encoding- and decoding models. This distinction is based on the insight that brain state features, that are found to be relevant in an experimental paradigm, carry a different meaning in encoding- than in decoding models. In this paper, we argue that this distinction is not sufficient: Relevant features in encoding- and decoding models carry a different meaning depending on whether they represent causal- or anti-causal relations. We provide a theoretical justification for this argument and conclude that causal inference is essential for interpretation in neuroimaging. version:1
arxiv-1512-04785 | On Deep Representation Learning from Noisy Web Images | http://arxiv.org/abs/1512.04785 | id:1512.04785 author:Phong D. Vo, Alexandru Ginsca, Hervé Le Borgne, Adrian Popescu category:cs.CV cs.MM  published:2015-12-15 summary:The keep-growing content of Web images may be the next important data source to scale up deep neural networks, which recently obtained a great success in the ImageNet classification challenge and related tasks. This prospect, however, has not been validated on convolutional networks (convnet) -- one of best performing deep models -- because of their supervised regime. While unsupervised alternatives are not so good as convnet in generalizing the learned model to new domains, we use convnet to leverage semi-supervised representation learning. Our approach is to use massive amounts of unlabeled and noisy Web images to train convnets as general feature detectors despite challenges coming from data such as high level of mislabeled data, outliers, and data biases. Extensive experiments are conducted at several data scales, different network architectures, and data reranking techniques. The learned representations are evaluated on nine public datasets of various topics. The best results obtained by our convnets, trained on 3.14 million Web images, outperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and is closing the gap with VGG-16. These prominent results suggest a budget solution to use deep learning in practice and motivate more research in semi-supervised representation learning. version:1
arxiv-1512-04754 | Learning optimal nonlinearities for iterative thresholding algorithms | http://arxiv.org/abs/1512.04754 | id:1512.04754 author:Ulugbek S. Kamilov, Hassan Mansour category:cs.LG stat.ML  published:2015-12-15 summary:Iterative shrinkage/thresholding algorithm (ISTA) is a well-studied method for finding sparse solutions to ill-posed inverse problems. In this letter, we present a data-driven scheme for learning optimal thresholding functions for ISTA. The proposed scheme is obtained by relating iterations of ISTA to layers of a simple deep neural network (DNN) and developing a corresponding error backpropagation algorithm that allows to fine-tune the thresholding functions. Simulations on sparse statistical signals illustrate potential gains in estimation quality due to the proposed data adaptive ISTA. version:1
arxiv-1512-04481 | Multimodal, high-dimensional, model-based, Bayesian inverse problems with applications in biomechanics | http://arxiv.org/abs/1512.04481 | id:1512.04481 author:Isabell M. Franck, P. S. Koutsourelakis category:stat.CO stat.ML  published:2015-12-14 summary:This paper is concerned with the numerical solution of model-based, Bayesian inverse problems. We are particularly interested in cases where the cost of each likelihood evaluation (forward-model call) is expensive and the number of unknown (latent) variables is high. This is the setting of many problems in computational physics where forward models with nonlinear PDEs are used and the parameters to be calibrated involve spatio-temporarily varying coefficients, which upon discretization give rise to a high-dimensional vector of unknowns. One of the consequences of the well-documented, ill-posedness of inverse problems is the possibility of multiple solutions. While such information is contained in the posterior density in Bayesian formulations, the discovery of a single mode, let alone multiple, is a formidable task. The goal of the present paper is two-fold. On one hand, we propose approximate, adaptive inference strategies using mixture densities to capture multimodal posteriors, and on the other, to extend our work in \cite{franck_sparse_2015} with regards to effective dimensionality reduction techniques that reveal low-dimensional subspaces where the posterior variance is mostly concentrated. We validate the proposed methodology in a problem in nonlinear elastography where the identification of the mechanical properties of biological materials can inform non-invasive, medical diagnosis. The discovery of multiple modes (solutions) in such problems is critical in achieving the diagnostic objectives. version:2
arxiv-1512-04701 | Joint Image-Text News Topic Detection and Tracking with And-Or Graph Representation | http://arxiv.org/abs/1512.04701 | id:1512.04701 author:Weixin Li, Jungseock Joo, Hang Qi, Song-Chun Zhu category:cs.IR cs.CL cs.SI  published:2015-12-15 summary:In this paper, we aim to develop a method for automatically detecting and tracking topics in broadcast news. We present a hierarchical And-Or graph (AOG) to jointly represent the latent structure of both texts and visuals. The AOG embeds a context sensitive grammar that can describe the hierarchical composition of news topics by semantic elements about people involved, related places and what happened, and model contextual relationships between elements in the hierarchy. We detect news topics through a cluster sampling process which groups stories about closely related events. Swendsen-Wang Cuts (SWC), an effective cluster sampling algorithm, is adopted for traversing the solution space and obtaining optimal clustering solutions by maximizing a Bayesian posterior probability. Topics are tracked to deal with the continuously updated news streams. We generate topic trajectories to show how topics emerge, evolve and disappear over time. The experimental results show that our method can explicitly describe the textual and visual data in news videos and produce meaningful topic trajectories. Our method achieves superior performance compared to state-of-the-art methods on both a public dataset Reuters-21578 and a self-collected dataset named UCLA Broadcast News Dataset. version:1
arxiv-1512-02406 | Learning Discrete Bayesian Networks from Continuous Data | http://arxiv.org/abs/1512.02406 | id:1512.02406 author:Yi-Chun Chen, Tim Allan Wheeler, Mykel John Kochenderfer category:cs.AI cs.LG  published:2015-12-08 summary:Real data often contains a mixture of discrete and continuous variables, but many Bayesian network structure learning and inference algorithms assume all random variables are discrete. Continuous variables are often discretized, but the choice of discretization policy has significant impact on the accuracy, speed, and interpretability of the resulting models. This paper introduces a principled Bayesian discretization method for continuous variables in Bayesian networks with quadratic complexity instead of the cubic complexity of other standard techniques. Empirical demonstrations show that the proposed method is superior to the state of the art. In addition, this paper shows how to incorporate existing methods into the structure learning process to discretize all continuous variables and simultaneously learn Bayesian network structures. version:2
arxiv-1512-02329 | Computational Models for Multiview Dense Depth Maps of Dynamic Scene | http://arxiv.org/abs/1512.02329 | id:1512.02329 author:Qifei Wang category:cs.CV  published:2015-12-08 summary:This paper reviews the recent progresses of the depth map generation for dynamic scene and its corresponding computational models. This paper mainly covers the homogeneous ambiguity models in depth sensing, resolution models in depth processing, and consistency models in depth optimization. We also summarize the future work in the depth map generation. version:2
arxiv-1512-02167 | Simple Baseline for Visual Question Answering | http://arxiv.org/abs/1512.02167 | id:1512.02167 author:Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, Rob Fergus category:cs.CV cs.CL  published:2015-12-07 summary:We describe a very simple bag-of-words baseline for visual question answering. This baseline concatenates the word features from the question and CNN features from the image to predict the answer. When evaluated on the challenging VQA dataset [2], it shows comparable performance to many recent approaches using recurrent neural networks. To explore the strength and weakness of the trained model, we also provide an interactive web demo and open-source code. . version:2
arxiv-1511-06443 | Neural Network Matrix Factorization | http://arxiv.org/abs/1511.06443 | id:1511.06443 author:Gintare Karolina Dziugaite, Daniel M. Roy category:cs.LG stat.ML  published:2015-11-19 summary:Data often comes in the form of an array or matrix. Matrix factorization techniques attempt to recover missing or corrupted entries by assuming that the matrix can be written as the product of two low-rank matrices. In other words, matrix factorization approximates the entries of the matrix by a simple, fixed function---namely, the inner product---acting on the latent feature vectors for the corresponding row and column. Here we consider replacing the inner product by an arbitrary function that we learn from the data at the same time as we learn the latent feature vectors. In particular, we replace the inner product by a multi-layer feed-forward neural network, and learn by alternating between optimizing the network for fixed latent features, and optimizing the latent features for a fixed network. The resulting approach---which we call neural network matrix factorization or NNMF, for short---dominates standard low-rank techniques on a suite of benchmark but is dominated by some recent proposals that take advantage of the graph features. Given the vast range of architectures, activation functions, regularizers, and optimization techniques that could be used within the NNMF framework, it seems likely the true potential of the approach has yet to be reached. version:2
arxiv-1512-04639 | Linear Models of Computation and Program Learning | http://arxiv.org/abs/1512.04639 | id:1512.04639 author:Michael Bukatin, Steve Matthews category:cs.LO cs.NE  published:2015-12-15 summary:We consider two classes of computations which admit taking linear combinations of execution runs: probabilistic sampling and generalized animation. We argue that the task of program learning should be more tractable for these architectures than for conventional deterministic programs. We look at the recent advances in the "sampling the samplers" paradigm in higher-order probabilistic programming. We also discuss connections between partial inconsistency, non-monotonic inference, and vector semantics. version:1
arxiv-1512-04636 | Noise-Compensated, Bias-Corrected Diffusion Weighted Endorectal Magnetic Resonance Imaging via a Stochastically Fully-Connected Joint Conditional Random Field Model | http://arxiv.org/abs/1512.04636 | id:1512.04636 author:Ameneh Boroomand, Mohammad Javad Shafiee, Farzad Khalvati, Masoom A. Haider, Alexander Wong category:stat.ME cs.CV physics.med-ph stat.AP  published:2015-12-15 summary:Diffusion weighted magnetic resonance imaging (DW-MRI) is a powerful tool in imaging-based prostate cancer (PCa) screening and detection. Endorectal coils are commonly used in DW-MRI to improve the signal-to-noise ratio (SNR) of the acquisition, at the expense of significant intensity inhomogeneities (bias field) that worsens as we move away from the endorectal coil. The presence of bias field can have a significant negative impact on the accuracy of different image analysis tasks, as well as the accuracy of PCa tumor localization, thus leading to increased inter- and intra-observer variability. The previously proposed bias field correction methods often suffer from undesired noise amplification that can reduce the image quality of the resulting bias-corrected DW-MRI data. Here, we propose a unified data reconstruction approach that enables joint compensation of bias field as well as data noise in diffusion weighted endorectal magnetic resonance (DW-EMR) imaging. The proposed noise-compensated, bias-corrected (NCBC) data reconstruction method takes advantage of a novel stochastically fully connected joint conditional random field (SFC-JCRF) model to mitigate the effects of data noise and bias field in the reconstructed DW-EMR prostate imaging data. The proposed NCBC reconstruction method was tested on synthetic DW-EMR data, physical DW-EMR phantom, as well as real DW-EMR imaging data. Both qualitative and quantitative analysis illustrated that the proposed NCBC method can achieve improved image quality when compared to other tested bias correction methods. As such, the proposed NCBC method can have strong potential for improving the consistency of image interpretations, thus leading to more accurate PCa diagnosis. version:1
arxiv-1504-06787 | Max-margin Deep Generative Models | http://arxiv.org/abs/1504.06787 | id:1504.06787 author:Chongxuan Li, Jun Zhu, Tianlin Shi, Bo Zhang category:cs.LG cs.CV  published:2015-04-26 summary:Deep generative models (DGMs) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability. However, little work has been done on examining or empowering the discriminative ability of DGMs on making accurate predictions. This paper presents max-margin deep generative models (mmDGMs), which explore the strongly discriminative principle of max-margin learning to improve the discriminative power of DGMs, while retaining the generative capability. We develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objective. Empirical results on MNIST and SVHN datasets demonstrate that (1) max-margin learning can significantly improve the prediction performance of DGMs and meanwhile retain the generative ability; and (2) mmDGMs are competitive to the state-of-the-art fully discriminative networks by employing deep convolutional neural networks (CNNs) as both recognition and generative models. version:4
arxiv-1512-04605 | Semantic-enriched Visual Vocabulary Construction in a Weakly Supervised Context | http://arxiv.org/abs/1512.04605 | id:1512.04605 author:Marian-Andrei Rizoiu, Julien Velcin, Stéphane Lallich category:cs.CV  published:2015-12-14 summary:One of the prevalent learning tasks involving images is content-based image classification. This is a difficult task especially because the low-level features used to digitally describe images usually capture little information about the semantics of the images. In this paper, we tackle this difficulty by enriching the semantic content of the image representation by using external knowledge. The underlying hypothesis of our work is that creating a more semantically rich representation for images would yield higher machine learning performances, without the need to modify the learning algorithms themselves. The external semantic information is presented under the form of non-positional image labels, therefore positioning our work in a weakly supervised context. Two approaches are proposed: the first one leverages the labels into the visual vocabulary construction algorithm, the result being dedicated visual vocabularies. The second approach adds a filtering phase as a pre-processing of the vocabulary construction. Known positive and known negative sets are constructed and features that are unlikely to be associated with the objects denoted by the labels are filtered. We apply our proposition to the task of content-based image classification and we show that semantically enriching the image representation yields higher classification performances than the baseline representation. version:1
arxiv-1512-04564 | Relaxed Linearized Algorithms for Faster X-Ray CT Image Reconstruction | http://arxiv.org/abs/1512.04564 | id:1512.04564 author:Hung Nien, Jeffrey A. Fessler category:math.OC cs.LG stat.ML  published:2015-12-14 summary:Statistical image reconstruction (SIR) methods are studied extensively for X-ray computed tomography (CT) due to the potential of acquiring CT scans with reduced X-ray dose while maintaining image quality. However, the longer reconstruction time of SIR methods hinders their use in X-ray CT in practice. To accelerate statistical methods, many optimization techniques have been investigated. Over-relaxation is a common technique to speed up convergence of iterative algorithms. For instance, using a relaxation parameter that is close to two in alternating direction method of multipliers (ADMM) has been shown to speed up convergence significantly. This paper proposes a relaxed linearized augmented Lagrangian (AL) method that shows theoretical faster convergence rate with over-relaxation and applies the proposed relaxed linearized AL method to X-ray CT image reconstruction problems. Experimental results with both simulated and real CT scan data show that the proposed relaxed algorithm (with ordered-subsets [OS] acceleration) is about twice as fast as the existing unrelaxed fast algorithms, with negligible computation and memory overhead. version:1
arxiv-1411-3698 | Minimal Realization Problems for Hidden Markov Models | http://arxiv.org/abs/1411.3698 | id:1411.3698 author:Qingqing Huang, Rong Ge, Sham Kakade, Munther Dahleh category:cs.LG  published:2014-11-13 summary:Consider a stationary discrete random process with alphabet size d, which is assumed to be the output process of an unknown stationary Hidden Markov Model (HMM). Given the joint probabilities of finite length strings of the process, we are interested in finding a finite state generative model to describe the entire process. In particular, we focus on two classes of models: HMMs and quasi-HMMs, which is a strictly larger class of models containing HMMs. In the main theorem, we show that if the random process is generated by an HMM of order less or equal than k, and whose transition and observation probability matrix are in general position, namely almost everywhere on the parameter space, both the minimal quasi-HMM realization and the minimal HMM realization can be efficiently computed based on the joint probabilities of all the length N strings, for N > 4 lceil log_d(k) rceil +1. In this paper, we also aim to compare and connect the two lines of literature: realization theory of HMMs, and the recent development in learning latent variable models with tensor decomposition techniques. version:2
arxiv-1511-08963 | Learning Directed Acyclic Graphs with Penalized Neighbourhood Regression | http://arxiv.org/abs/1511.08963 | id:1511.08963 author:Bryon Aragam, Arash A. Amini, Qing Zhou category:math.ST cs.LG stat.ML stat.TH  published:2015-11-29 summary:We consider the problem of estimating a directed acyclic graph (DAG) for a multivariate normal distribution from high-dimensional data with $p\gg n$. Our main results establish nonasymptotic deviation bounds on the estimation error, sparsity bounds, and model selection consistency for a penalized least squares estimator under concave regularization. The proofs rely on interpreting the graphical model as a recursive linear structural equation model, which reduces the estimation problem to a series of tractable neighbourhood regressions and allows us to avoid making any assumptions regarding faithfulness. In doing so, we provide some novel techniques for handling general nonidentifiable and nonconvex problems. These techniques are used to guarantee uniform control over a superexponential number of neighbourhood regression problems by exploiting various notions of monotonicity among them. Our results apply to a wide variety of practical situations that allow for arbitrary nondegenerate covariance structures as well as many popular regularizers including the MCP, SCAD, $\ell_{0}$ and $\ell_{1}$. version:2
arxiv-1512-04483 | Dropout Training of Matrix Factorization and Autoencoder for Link Prediction in Sparse Graphs | http://arxiv.org/abs/1512.04483 | id:1512.04483 author:Shuangfei Zhai, Zhongfei Zhang category:cs.LG  published:2015-12-14 summary:Matrix factorization (MF) and Autoencoder (AE) are among the most successful approaches of unsupervised learning. While MF based models have been extensively exploited in the graph modeling and link prediction literature, the AE family has not gained much attention. In this paper we investigate both MF and AE's application to the link prediction problem in sparse graphs. We show the connection between AE and MF from the perspective of multiview learning, and further propose MF+AE: a model training MF and AE jointly with shared parameters. We apply dropout to training both the MF and AE parts, and show that it can significantly prevent overfitting by acting as an adaptive regularization. We conduct experiments on six real world sparse graph datasets, and show that MF+AE consistently outperforms the competing methods, especially on datasets that demonstrate strong non-cohesive structures. version:1
arxiv-1512-04466 | Semisupervised Autoencoder for Sentiment Analysis | http://arxiv.org/abs/1512.04466 | id:1512.04466 author:Shuangfei Zhai, Zhongfei Zhang category:cs.LG  published:2015-12-14 summary:In this paper, we investigate the usage of autoencoders in modeling textual data. Traditional autoencoders suffer from at least two aspects: scalability with the high dimensionality of vocabulary size and dealing with task-irrelevant words. We address this problem by introducing supervision via the loss function of autoencoders. In particular, we first train a linear classifier on the labeled data, then define a loss for the autoencoder with the weights learned from the linear classifier. To reduce the bias brought by one single classifier, we define a posterior probability distribution on the weights of the classifier, and derive the marginalized loss of the autoencoder with Laplace approximation. We show that our choice of loss function can be rationalized from the perspective of Bregman Divergence, which justifies the soundness of our model. We evaluate the effectiveness of our model on six sentiment analysis datasets, and show that our model significantly outperforms all the competing methods with respect to classification accuracy. We also show that our model is able to take advantage of unlabeled dataset and get improved performance. We further show that our model successfully learns highly discriminative feature maps, which explains its superior performance. version:1
arxiv-1512-04455 | Memory-based control with recurrent neural networks | http://arxiv.org/abs/1512.04455 | id:1512.04455 author:Nicolas Heess, Jonathan J Hunt, Timothy P Lillicrap, David Silver category:cs.LG  published:2015-12-14 summary:Partially observed control problems are a challenging aspect of reinforcement learning. We extend two related, model-free algorithms for continuous control -- deterministic policy gradient and stochastic value gradient -- to solve partially observed domains using recurrent neural networks trained with backpropagation through time. We demonstrate that this approach, coupled with long-short term memory is able to solve a variety of physical control problems exhibiting an assortment of memory requirements. These include the short-term integration of information from noisy sensors and the identification of system parameters, as well as long-term memory problems that require preserving information over many time steps. We also demonstrate success on a combined exploration and memory problem in the form of a simplified version of the well-known Morris water maze task. Finally, we show that our approach can deal with high-dimensional observations by learning directly from pixels. We find that recurrent deterministic and stochastic policies are able to learn similarly good solutions to these tasks, including the water maze where the agent must learn effective search strategies. version:1
arxiv-1512-04433 | Near-Optimal Bounds for Binary Embeddings of Arbitrary Sets | http://arxiv.org/abs/1512.04433 | id:1512.04433 author:Samet Oymak, Ben Recht category:cs.LG cs.DS math.FA  published:2015-12-14 summary:We study embedding a subset $K$ of the unit sphere to the Hamming cube $\{-1,+1\}^m$. We characterize the tradeoff between distortion and sample complexity $m$ in terms of the Gaussian width $\omega(K)$ of the set. For subspaces and several structured sets we show that Gaussian maps provide the optimal tradeoff $m\sim \delta^{-2}\omega^2(K)$, in particular for $\delta$ distortion one needs $m\approx\delta^{-2}{d}$ where $d$ is the subspace dimension. For general sets, we provide sharp characterizations which reduces to $m\approx{\delta^{-4}}{\omega^2(K)}$ after simplification. We provide improved results for local embedding of points that are in close proximity of each other which is related to locality sensitive hashing. We also discuss faster binary embedding where one takes advantage of an initial sketching procedure based on Fast Johnson-Lindenstauss Transform. Finally, we list several numerical observations and discuss open problems. version:1
arxiv-1512-04419 | Sentence Entailment in Compositional Distributional Semantics | http://arxiv.org/abs/1512.04419 | id:1512.04419 author:Esma Balkir, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh category:cs.CL cs.AI math.CT I.2.7  published:2015-12-14 summary:Distributional semantic models provide vector representations for words by gathering co-occurrence frequencies from corpora of text. Compositional distributional models extend these representations from words to phrases and sentences. In categorical compositional distributional semantics these representations are built in such a manner that meanings of phrases and sentences are functions of their grammatical structure and the meanings of the words therein. These models have been applied to reasoning about phrase and sentence level similarity. In this paper, we argue for and prove that these models can also be used to reason about phrase and sentence level entailment. We provide preliminary experimental results on a toy entailment dataset. version:1
arxiv-1512-04418 | Sparse Representation of a Blur Kernel for Blind Image Restoration | http://arxiv.org/abs/1512.04418 | id:1512.04418 author:Chia-Chen Lee, Wen-Liang Hwang category:cs.CV  published:2015-12-14 summary:Blind image restoration is a non-convex problem which involves restoration of images from an unknown blur kernel. The factors affecting the performance of this restoration are how much prior information about an image and a blur kernel are provided and what algorithm is used to perform the restoration task. Prior information on images is often employed to restore the sharpness of the edges of an image. By contrast, no consensus is still present regarding what prior information to use in restoring from a blur kernel due to complex image blurring processes. In this paper, we propose modelling of a blur kernel as a sparse linear combinations of basic 2-D patterns. Our approach has a competitive edge over the existing blur kernel modelling methods because our method has the flexibility to customize the dictionary design, which makes it well-adaptive to a variety of applications. As a demonstration, we construct a dictionary formed by basic patterns derived from the Kronecker product of Gaussian sequences. We also compare our results with those derived by other state-of-the-art methods, in terms of peak signal to noise ratio (PSNR). version:1
arxiv-1512-04412 | Instance-aware Semantic Segmentation via Multi-task Network Cascades | http://arxiv.org/abs/1512.04412 | id:1512.04412 author:Jifeng Dai, Kaiming He, Jian Sun category:cs.CV  published:2015-12-14 summary:Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multi-task Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place. version:1
arxiv-1503-04127 | Image patch analysis of sunspots and active regions. I. Intrinsic dimension and correlation analysis | http://arxiv.org/abs/1503.04127 | id:1503.04127 author:Kevin R. Moon, Jimmy J. Li, Veronique Delouille, Ruben De Visscher, Fraser Watson, Alfred O. Hero III category:astro-ph.SR cs.CV  published:2015-03-13 summary:The flare-productivity of an active region is observed to be related to its spatial complexity. Mount Wilson or McIntosh sunspot classifications measure such complexity but in a categorical way, and may therefore not use all the information present in the observations. Moreover, such categorical schemes hinder a systematic study of an active region's evolution for example. We propose fine-scale quantitative descriptors for an active region's complexity and relate them to the Mount Wilson classification. We analyze the local correlation structure within continuum and magnetogram data, as well as the cross-correlation between continuum and magnetogram data. We compute the intrinsic dimension, partial correlation, and canonical correlation analysis (CCA) of image patches of continuum and magnetogram active region images taken from the SOHO-MDI instrument. We use masks of sunspots derived from continuum as well as larger masks of magnetic active regions derived from the magnetogram to analyze separately the core part of an active region from its surrounding part. We find the relationship between complexity of an active region as measured by Mount Wilson and the intrinsic dimension of its image patches. Partial correlation patterns exhibit approximately a third-order Markov structure. CCA reveals different patterns of correlation between continuum and magnetogram within the sunspots and in the region surrounding the sunspots. These results also pave the way for patch-based dictionary learning with a view towards automatic clustering of active regions. version:2
arxiv-1512-04274 | Decoding index finger position from EEG using random forests | http://arxiv.org/abs/1512.04274 | id:1512.04274 author:Sebastian Weichwald, Timm Meyer, Bernhard Schölkopf, Tonio Ball, Moritz Grosse-Wentrup category:stat.ML q-bio.NC q-bio.QM  published:2015-12-14 summary:While invasively recorded brain activity is known to provide detailed information on motor commands, it is an open question at what level of detail information about positions of body parts can be decoded from non-invasively acquired signals. In this work it is shown that index finger positions can be differentiated from non-invasive electroencephalographic (EEG) recordings in healthy human subjects. Using a leave-one-subject-out cross-validation procedure, a random forest distinguished different index finger positions on a numerical keyboard above chance-level accuracy. Among the different spectral features investigated, high $\beta$-power (20-30 Hz) over contralateral sensorimotor cortex carried most information about finger position. Thus, these findings indicate that finger position is in principle decodable from non-invasive features of brain activity that generalize across individuals. version:1
arxiv-1501-03669 | A Proximal Approach for Sparse Multiclass SVM | http://arxiv.org/abs/1501.03669 | id:1501.03669 author:G. Chierchia, Nelly Pustelnik, Jean-Christophe Pesquet, B. Pesquet-Popescu category:cs.LG  published:2015-01-15 summary:Sparsity-inducing penalties are useful tools to design multiclass support vector machines (SVMs). In this paper, we propose a convex optimization approach for efficiently and exactly solving the multiclass SVM learning problem involving a sparse regularization and the multiclass hinge loss formulated by Crammer and Singer. We provide two algorithms: the first one dealing with the hinge loss as a penalty term, and the other one addressing the case when the hinge loss is enforced through a constraint. The related convex optimization problems can be efficiently solved thanks to the flexibility offered by recent primal-dual proximal algorithms and epigraphical splitting techniques. Experiments carried out on several datasets demonstrate the interest of considering the exact expression of the hinge loss rather than a smooth approximation. The efficiency of the proposed algorithms w.r.t. several state-of-the-art methods is also assessed through comparisons of execution times. version:5
arxiv-1512-04219 | On the Relation between two Rotation Metrics | http://arxiv.org/abs/1512.04219 | id:1512.04219 author:Thomas Ruland category:cs.CV  published:2015-12-14 summary:In their work "Global Optimization through Rotation Space Search", Richard Hartley and Fredrik Kahl introduce a global optimization strategy for problems in geometric computer vision, based on rotation space search using a branch-and-bound algorithm. In its core, Lemma 2 of their publication is the important foundation for a class of global optimization algorithms, which is adopted over a wide range of problems in subsequent publications. This lemma relates a metric on rotations represented by rotation matrices with a metric on rotations in axis-angle representation. This work focuses on a proof for this relationship, which is based on Rodrigues' Rotation Theorem for the composition of rotations in axis-angle representation. version:1
arxiv-1512-04208 | Watch-Bot: Unsupervised Learning for Reminding Humans of Forgotten Actions | http://arxiv.org/abs/1512.04208 | id:1512.04208 author:Chenxia Wu, Jiemi Zhang, Bart Selman, Silvio Savarese, Ashutosh Saxena category:cs.RO cs.CV  published:2015-12-14 summary:We present a robotic system that watches a human using a Kinect v2 RGB-D sensor, detects what he forgot to do while performing an activity, and if necessary reminds the person using a laser pointer to point out the related object. Our simple setup can be easily deployed on any assistive robot. Our approach is based on a learning algorithm trained in a purely unsupervised setting, which does not require any human annotations. This makes our approach scalable and applicable to variant scenarios. Our model learns the action/object co-occurrence and action temporal relations in the activity, and uses the learned rich relationships to infer the forgotten action and the related object. We show that our approach not only improves the unsupervised action segmentation and action cluster assignment performance, but also effectively detects the forgotten actions on a challenging human activity RGB-D video dataset. In robotic experiments, we show that our robot is able to remind people of forgotten actions successfully. version:1
arxiv-1512-04205 | Compressed Dynamic Mode Decomposition for Real-Time Object Detection | http://arxiv.org/abs/1512.04205 | id:1512.04205 author:N. Benjamin Erichson, Steven L. Brunton, J. Nathan Kutz category:cs.CV  published:2015-12-14 summary:We introduce the method of compressive dynamic mode decomposition (cDMD) for robustly performing real-time foreground/background separation in high-definition video. The DMD method provides a regression technique for least-square fitting of video snapshots to a linear dynamical system. The method integrates two of the leading data analysis methods in use today: Fourier transforms and Principal Components. DMD modes with temporal Fourier frequencies near the origin (zero-modes) are interpreted as background (low-rank) portions of the given video frames, and the terms with Fourier frequencies bounded away from the origin are their foreground (sparse) counterparts. When combined with compression techniques, the resulting cDMD can process full HD video feeds in real-time on CPU computing platforms while still maintaining competitive video decomposition quality, quantified by F-measure, Recall and Precision. On a GPU architecture, the method is significantly faster than real-time, allowing for further video processing to improve the separation quality and/or enacting further computer vision processes such as object recognition. version:1
arxiv-1512-04202 | Preconditioned Stochastic Gradient Descent | http://arxiv.org/abs/1512.04202 | id:1512.04202 author:Xi-Lin Li category:stat.ML cs.LG  published:2015-12-14 summary:Stochastic gradient descent (SGD) still is the workhorse for many practical problems. However, it converges slow, and can be difficult to tune. It is possible to precondition SGD to accelerate its convergence remarkably. But many attempts in this direction either aim at solving specialized problems, or result in significantly more complicated methods than SGD. This paper proposes a new way to estimate a preconditioner by equalizing the amplitudes of parameter changes and the amplitudes of associated gradient changes. Unlike the Hessian inverse like preconditioners based on secant equation fitting as done in deterministic quasi-Newton methods, which work the best when the Hessian is positive definite, the new preconditioner works equally well for both convex and non-convex optimizations. When stochastic gradient is used, it can naturally damp the gradient noise to stabilize SGD. Efficient preconditioner estimation methods are developed, and with reasonable simplifications, they are applicable to large scaled problems. Experimental results demonstrate that equipped with the new preconditioner, without any tuning effort, preconditioned SGD can efficiently solve many challenging problems like the training of a deep neural network or a recurrent neural network requiring extremely long term memories. version:1
arxiv-1510-02675 | Controlled Experiments for Word Embeddings | http://arxiv.org/abs/1510.02675 | id:1510.02675 author:Benjamin J. Wilson, Adriaan M. J. Schakel category:cs.CL 68T50 I.2.7  published:2015-10-09 summary:An experimental approach to studying the properties of word embeddings is proposed. Controlled experiments, achieved through modifications of the training corpus, permit the demonstration of direct relations between word properties and word vector direction and length. The approach is demonstrated using the word2vec CBOW model with experiments that independently vary word frequency and word co-occurrence noise. The experiments reveal that word vector length depends more or less linearly on both word frequency and the level of noise in the co-occurrence distribution of the word. The coefficients of linearity depend upon the word. The special point in feature space, defined by the (artificial) word with pure noise in its co-occurrence distribution, is found to be small but non-zero. version:2
arxiv-1604-08164 | Understanding Human-Centric Images: From Geometry to Fashion | http://arxiv.org/abs/1604.08164 | id:1604.08164 author:Edgar Simo-Serra category:cs.CV  published:2015-12-14 summary:Understanding humans from photographs has always been a fundamental goal of computer vision. In this thesis we have developed a hierarchy of tools that cover a wide range of topics with the objective of understanding humans from monocular RGB image: from low level feature point descriptors to high level fashion-aware conditional random fields models. In order to build these high level models it is paramount to have a battery of robust and reliable low and mid level cues. Along these lines, we have proposed two low-level keypoint descriptors: one based on the theory of the heat diffusion on images, and the other that uses a convolutional neural network to learn discriminative image patch representations. We also introduce distinct low-level generative models for representing human pose: in particular we present a discrete model based on a directed acyclic graph and a continuous model that consists of poses clustered on a Riemannian manifold. As mid level cues we propose two 3D human pose estimation algorithms: one that estimates the 3D pose given a noisy 2D estimation, and an approach that simultaneously estimates both the 2D and 3D pose. Finally, we formulate higher level models built upon low and mid level cues for understanding humans from single images. Concretely, we focus on two different tasks in the context of fashion: semantic segmentation of clothing, and predicting the fashionability from images with metadata to ultimately provide fashion advice to the user. For all presented approaches we present extensive results and comparisons against the state-of-the-art and show significant improvements on the entire variety of tasks we tackle. version:1
arxiv-1503-08471 | Cross-validation of matching correlation analysis by resampling matching weights | http://arxiv.org/abs/1503.08471 | id:1503.08471 author:Hidetoshi Shimodaira category:stat.ML cs.LG  published:2015-03-29 summary:The strength of association between a pair of data vectors is represented by a nonnegative real number, called matching weight. For dimensionality reduction, we consider a linear transformation of data vectors, and define a matching error as the weighted sum of squared distances between transformed vectors with respect to the matching weights. Given data vectors and matching weights, the optimal linear transformation minimizing the matching error is solved by the spectral graph embedding of Yan et al. (2007). This method is a generalization of the canonical correlation analysis, and will be called as matching correlation analysis (MCA). In this paper, we consider a novel sampling scheme where the observed matching weights are randomly sampled from underlying true matching weights with small probability, whereas the data vectors are treated as constants. We then investigate a cross-validation by resampling the matching weights. Our asymptotic theory shows that the cross-validation, if rescaled properly, computes an unbiased estimate of the matching error with respect to the true matching weights. Existing ideas of cross-validation for resampling data vectors, instead of resampling matching weights, are not applicable here. MCA can be used for data vectors from multiple domains with different dimensions via an embarrassingly simple idea of coding the data vectors. This method will be called as cross-domain matching correlation analysis (CDMCA), and an interesting connection to the classical associative memory model of neural networks is also discussed. version:3
arxiv-1507-03032 | Spectral Smoothing via Random Matrix Perturbations | http://arxiv.org/abs/1507.03032 | id:1507.03032 author:Jacob Abernethy, Chansoo Lee, Ambuj Tewari category:cs.LG  published:2015-07-10 summary:We consider stochastic smoothing of spectral functions of matrices using perturbations commonly studied in random matrix theory. We show that a spectral function remains spectral when smoothed using a unitarily invariant perturbation distribution. We then derive state-of-the-art smoothing bounds for the maximum eigenvalue function using the Gaussian Orthogonal Ensemble (GOE). Smoothing the maximum eigenvalue function is important for applications in semidefinite optimization and online learning. As a direct consequence of our GOE smoothing results, we obtain an $O((N \log N)^{1/4} \sqrt{T})$ expected regret bound for the online variance minimization problem using an algorithm that performs only a single maximum eigenvector computation per time step. Here $T$ is the number of rounds and $N$ is the matrix dimension. Our algorithm and its analysis also extend to the more general online PCA problem where the learner has to output a rank $k$ subspace. The algorithm just requires computing $k$ maximum eigenvectors per step and enjoys an $O(k (N \log N)^{1/4} \sqrt{T})$ expected regret bound. version:2
arxiv-1512-04152 | Fighting Bandits with a New Kind of Smoothness | http://arxiv.org/abs/1512.04152 | id:1512.04152 author:Jacob Abernethy, Chansoo Lee, Ambuj Tewari category:cs.LG cs.GT stat.ML  published:2015-12-14 summary:We define a novel family of algorithms for the adversarial multi-armed bandit problem, and provide a simple analysis technique based on convex smoothing. We prove two main results. First, we show that regularization via the \emph{Tsallis entropy}, which includes EXP3 as a special case, achieves the $\Theta(\sqrt{TN})$ minimax regret. Second, we show that a wide class of perturbation methods achieve a near-optimal regret as low as $O(\sqrt{TN \log N})$ if the perturbation distribution has a bounded hazard rate. For example, the Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy this key property. version:1
arxiv-1512-04150 | Learning Deep Features for Discriminative Localization | http://arxiv.org/abs/1512.04150 | id:1512.04150 author:Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba category:cs.CV  published:2015-12-14 summary:In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them version:1
arxiv-1512-04143 | Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks | http://arxiv.org/abs/1512.04143 | id:1512.04143 author:Sean Bell, C. Lawrence Zitnick, Kavita Bala, Ross Girshick category:cs.CV  published:2015-12-14 summary:It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9% to 76.4% mAP. On the new and more challenging MS COCO dataset, we improve state-of-art-the from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection Challenge, our ION model won the Best Student Entry and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection. version:1
arxiv-1508-07902 | Maximum Persistency via Iterative Relaxed Inference with Graphical Models | http://arxiv.org/abs/1508.07902 | id:1508.07902 author:Alexander Shekhovtsov, Paul Swoboda, Bogdan Savchynskyy category:cs.CV cs.DS  published:2015-08-31 summary:We consider the NP-hard problem of MAP-inference for graphical models. We propose a polynomial time practically efficient algorithm for finding a part of its optimal solution. Specifically, our algorithm marks each label in each node of the considered graphical model either as (i) optimal, meaning that it belongs to all optimal solutions of the inference problem; (ii) non-optimal if it provably does not belong to any solution; or (iii) undefined, which means our algorithm can not make a decision regarding the label. Moreover, we prove optimality of our approach: it delivers in a certain sense the largest total number of labels marked as optimal or non-optimal. We demonstrate superiority of our approach on problems from machine learning and computer vision benchmarks. version:2
arxiv-1510-01431 | SentiCap: Generating Image Descriptions with Sentiments | http://arxiv.org/abs/1510.01431 | id:1510.01431 author:Alexander Mathews, Lexing Xie, Xuming He category:cs.CV cs.CL  published:2015-10-06 summary:The recent progress on image recognition and language modeling is making automatic description of image content a reality. However, stylized, non-factual aspects of the written description are missing from the current systems. One such style is descriptions with emotions, which is commonplace in everyday communication, and influences decision-making and interpersonal relationships. We design a system to describe an image with emotions, and present a model that automatically generates captions with positive or negative sentiments. We propose a novel switching recurrent neural network with word-level regularization, which is able to produce emotional image captions using only 2000+ training sentences containing sentiments. We evaluate the captions with different automatic and crowd-sourcing metrics. Our model compares favourably in common quality metrics for image captioning. In 84.6% of cases the generated positive captions were judged as being at least as descriptive as the factual captions. Of these positive captions 88% were confirmed by the crowd-sourced workers as having the appropriate sentiment. version:2
arxiv-1512-04134 | Evaluation of Pose Tracking Accuracy in the First and Second Generations of Microsoft Kinect | http://arxiv.org/abs/1512.04134 | id:1512.04134 author:Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy category:cs.CV cs.AI  published:2015-12-13 summary:Microsoft Kinect camera and its skeletal tracking capabilities have been embraced by many researchers and commercial developers in various applications of real-time human movement analysis. In this paper, we evaluate the accuracy of the human kinematic motion data in the first and second generation of the Kinect system, and compare the results with an optical motion capture system. We collected motion data in 12 exercises for 10 different subjects and from three different viewpoints. We report on the accuracy of the joint localization and bone length estimation of Kinect skeletons in comparison to the motion capture. We also analyze the distribution of the joint localization offsets by fitting a mixture of Gaussian and uniform distribution models to determine the outliers in the Kinect motion data. Our analysis shows that overall Kinect 2 has more robust and more accurate tracking of human pose as compared to Kinect 1. version:1
arxiv-1512-04133 | A Person Re-Identification System For Mobile Devices | http://arxiv.org/abs/1512.04133 | id:1512.04133 author:George Cushen category:cs.CV cs.CR cs.IR  published:2015-12-13 summary:Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website. version:1
arxiv-1507-07105 | Dimensionality-reduced subspace clustering | http://arxiv.org/abs/1507.07105 | id:1507.07105 author:Reinhard Heckel, Michael Tschannen, Helmut Bölcskei category:stat.ML cs.IT cs.LG math.IT  published:2015-07-25 summary:Subspace clustering refers to the problem of clustering unlabeled high-dimensional data points into a union of low-dimensional linear subspaces, whose number, orientations, and dimensions are all unknown. In practice one may have access to dimensionality-reduced observations of the data only, resulting, e.g., from undersampling due to complexity and speed constraints on the acquisition device or mechanism. More pertinently, even if the high-dimensional data set is available it is often desirable to first project the data points into a lower-dimensional space and to perform clustering there; this reduces storage requirements and computational cost. The purpose of this paper is to quantify the impact of dimensionality reduction through random projection on the performance of three subspace clustering algorithms, all of which are based on principles from sparse signal recovery. Specifically, we analyze the thresholding based subspace clustering (TSC) algorithm, the sparse subspace clustering (SSC) algorithm, and an orthogonal matching pursuit variant thereof (SSC-OMP). We find, for all three algorithms, that dimensionality reduction down to the order of the subspace dimensions is possible without incurring significant performance degradation. Moreover, these results are order-wise optimal in the sense that reducing the dimensionality further leads to a fundamentally ill-posed clustering problem. Our findings carry over to the noisy case as illustrated through analytical results for TSC and simulations for SSC and SSC-OMP. Extensive experiments on synthetic and real data complement our theoretical findings. version:2
arxiv-1512-04118 | Articulated Pose Estimation Using Hierarchical Exemplar-Based Models | http://arxiv.org/abs/1512.04118 | id:1512.04118 author:Jiongxin Liu, Yinxiao Li, Peter Allen, Peter Belhumeur category:cs.CV  published:2015-12-13 summary:Exemplar-based models have achieved great success on localizing the parts of semi-rigid objects. However, their efficacy on highly articulated objects such as humans is yet to be explored. Inspired by hierarchical object representation and recent application of Deep Convolutional Neural Networks (DCNNs) on human pose estimation, we propose a novel formulation that incorporates both hierarchical exemplar-based models and DCNNs in the spatial terms. Specifically, we obtain more expressive spatial models by assuming independence between exemplars at different levels in the hierarchy; we also obtain stronger spatial constraints by inferring the spatial relations between parts at the same level. As our method strikes a good balance between expressiveness and strength of spatial models, it is both effective and generalizable, achieving state-of-the-art results on different benchmarks: Leeds Sports Dataset and CUB-200-2011. version:1
arxiv-1512-04115 | Unsupervised Temporal Segmentation of Repetitive Human Actions Based on Kinematic Modeling and Frequency Analysis | http://arxiv.org/abs/1512.04115 | id:1512.04115 author:Qifei Wang, Gregorij Kurillo, Ferda Ofli, Ruzena Bajcsy category:cs.CV  published:2015-12-13 summary:In this paper, we propose a method for temporal segmentation of human repetitive actions based on frequency analysis of kinematic parameters, zero-velocity crossing detection, and adaptive k-means clustering. Since the human motion data may be captured with different modalities which have different temporal sampling rate and accuracy (e.g., optical motion capture systems vs. Microsoft Kinect), we first apply a generic full-body kinematic model with an unscented Kalman filter to convert the motion data into a unified representation that is robust to noise. Furthermore, we extract the most representative kinematic parameters via the primary frequency analysis. The sequences are segmented based on zero-velocity crossing of the selected parameters followed by an adaptive k-means clustering to identify the repetition segments. Experimental results demonstrate that for the motion data captured by both the motion capture system and the Microsoft Kinect, our proposed algorithm obtains robust segmentation of repetitive action sequences. version:1
arxiv-1512-04105 | Policy Gradient Methods for Off-policy Control | http://arxiv.org/abs/1512.04105 | id:1512.04105 author:Lucas Lehnert, Doina Precup category:cs.AI cs.LG  published:2015-12-13 summary:Off-policy learning refers to the problem of learning the value function of a way of behaving, or policy, while following a different policy. Gradient-based off-policy learning algorithms, such as GTD and TDC/GQ, converge even when using function approximation and incremental updates. However, they have been developed for the case of a fixed behavior policy. In control problems, one would like to adapt the behavior policy over time to become more greedy with respect to the existing value function. In this paper, we present the first gradient-based learning algorithms for this problem, which rely on the framework of policy gradient in order to modify the behavior policy. We present derivations of the algorithms, a convergence theorem, and empirical evidence showing that they compare favorably to existing approaches. version:1
arxiv-1512-04103 | Deep Relative Attributes | http://arxiv.org/abs/1512.04103 | id:1512.04103 author:Yaser Souri, Erfan Noury, Ehsan Adeli-Mosabbeb category:cs.CV  published:2015-12-13 summary:Visual attributes are great means of describing images or scenes, in a way both humans and computers understand. In order to establish a correspondence between images and to be able to compare the strength of each property between images, relative attributes were introduced. However, since their introduction, hand-crafted and engineered features were used to learn complex models for the problem of relative attributes. This limits the applicability of those methods for more realistic cases. We introduce a two part deep learning architecture for the task of relative attribute prediction. A convolutional neural network (ConvNet) architecture is adopted to learn the features with addition of an additional layer (ranking layer) that learns to rank the images based on these features. Also an appropriate ranking loss is adapted to train the whole network in an end-to-end fashion. Our proposed method outperforms the baseline and state-of-the-art methods in relative attribute prediction on various datasets. Our qualitative results also show that the network is able to learn effective features for the task. Furthermore, we use our trained models to visualize saliency maps for each attribute. version:1
arxiv-1512-04092 | Stack Exchange Tagger | http://arxiv.org/abs/1512.04092 | id:1512.04092 author:Sanket Mehta, Shagun Sodhani category:cs.CL cs.LG  published:2015-12-13 summary:The goal of our project is to develop an accurate tagger for questions posted on Stack Exchange. Our problem is an instance of the more general problem of developing accurate classifiers for large scale text datasets. We are tackling the multilabel classification problem where each item (in this case, question) can belong to multiple classes (in this case, tags). We are predicting the tags (or keywords) for a particular Stack Exchange post given only the question text and the title of the post. In the process, we compare the performance of Support Vector Classification (SVC) for different kernel functions, loss function, etc. We found linear SVC with Crammer Singer technique produces best results. version:1
arxiv-1512-04087 | True Online Temporal-Difference Learning | http://arxiv.org/abs/1512.04087 | id:1512.04087 author:Harm van Seijen, A. Rupam Mahmood, Patrick M. Pilarski, Marlos C. Machado, Richard S. Sutton category:cs.AI cs.LG  published:2015-12-13 summary:The temporal-difference methods TD($\lambda$) and Sarsa($\lambda$) form a core part of modern reinforcement learning. Their appeal comes from their good performance, low computational cost, and their simple interpretation, given by their forward view. Recently, new versions of these methods were introduced, called true online TD($\lambda$) and true online Sarsa($\lambda$), respectively (van Seijen and Sutton, 2014). Algorithmically, these true online methods only make two small changes to the update rules of the regular methods, and the extra computational cost is negligible in most cases. However, they follow the ideas underlying the forward view much more closely. In particular, they maintain an exact equivalence with the forward view at all times, whereas the traditional versions only approximate it for small step-sizes. We hypothesize that these true online methods not only have better theoretical properties, but also dominate the regular methods empirically. In this article, we put this hypothesis to the test by performing an extensive empirical comparison. Specifically, we compare the performance of true online TD($\lambda$)/Sarsa($\lambda$) with regular TD($\lambda$)/Sarsa($\lambda$) on random MRPs, a real-world myoelectric prosthetic arm, and a domain from the Arcade Learning Environment. We use linear function approximation with tabular, binary, and non-binary features. Our results suggest that the true online methods indeed dominate the regular methods. Across all domains/representations the learning speed of the true online methods are often better, but never worse than that of the regular methods. An additional advantage is that no choice between traces has to be made for the true online methods. We show that new true online temporal-difference methods can be derived by making changes to the real-time forward view and then rewriting the update equations. version:1
arxiv-1512-04065 | Cross-dimensional Weighting for Aggregated Deep Convolutional Features | http://arxiv.org/abs/1512.04065 | id:1512.04065 author:Yannis Kalantidis, Clayton Mellina, Simon Osindero category:cs.CV  published:2015-12-13 summary:We propose a simple and straightforward way of creating powerful image representations via cross-dimensional weighting and aggregation of deep convolutional neural network layer outputs. We first present a generalized framework that encompasses a broad family of approaches and includes cross-dimensional pooling and weighting steps. We then propose specific non-parametric schemes for both spatial- and channel-wise weighting, that boost the effect of highly active spatial responses and at the same time regulate burstiness effects. We experiment on four public datasets for image search and unsupervised fine-grained classification and show that our approach consistently outperforms the current state-of-the-art by a large margin. version:1
arxiv-1512-04052 | Big Data Scaling through Metric Mapping: Exploiting the Remarkable Simplicity of Very High Dimensional Spaces using Correspondence Analysis | http://arxiv.org/abs/1512.04052 | id:1512.04052 author:Fionn Murtagh category:stat.ML cs.LG 62H25  published:2015-12-13 summary:We present new findings in regard to data analysis in very high dimensional spaces. We use dimensionalities up to around one million. A particular benefit of Correspondence Analysis is its suitability for carrying out an orthonormal mapping, or scaling, of power law distributed data. Power law distributed data are found in many domains. Correspondence factor analysis provides a latent semantic or principal axes mapping. Our experiments use data from digital chemistry and finance, and other statistically generated data. version:1
arxiv-1512-04039 | Distributed Optimization with Arbitrary Local Solvers | http://arxiv.org/abs/1512.04039 | id:1512.04039 author:Chenxin Ma, Jakub Konečný, Martin Jaggi, Virginia Smith, Michael I. Jordan, Peter Richtárik, Martin Takáč category:cs.LG math.OC  published:2015-12-13 summary:With the growth of data and necessity for distributed optimization methods, solvers that work well on a single machine must be re-designed to leverage distributed computation. Recent work in this area has been limited by focusing heavily on developing highly specific methods for the distributed environment. These special-purpose methods are often unable to fully leverage the competitive performance of their well-tuned and customized single machine counterparts. Further, they are unable to easily integrate improvements that continue to be made to single machine methods. To this end, we present a framework for distributed optimization that both allows the flexibility of arbitrary solvers to be used on each (single) machine locally, and yet maintains competitive performance against other state-of-the-art special-purpose distributed methods. We give strong primal-dual convergence rate guarantees for our framework that hold for arbitrary local solvers. We demonstrate the impact of local solver selection both theoretically and in an extensive experimental comparison. Finally, we provide thorough implementation details for our framework, highlighting areas for practical performance gains. version:1
arxiv-1512-04036 | Tracking Idea Flows between Social Groups | http://arxiv.org/abs/1512.04036 | id:1512.04036 author:Yangxin Zhong, Shixia Liu, Xiting Wang, Jiannan Xiao, Yangqiu Song category:cs.SI cs.LG  published:2015-12-13 summary:In many applications, ideas that are described by a set of words often flow between different groups. To facilitate users in analyzing the flow, we present a method to model the flow behaviors that aims at identifying the lead-lag relationships between word clusters of different user groups. In particular, an improved Bayesian conditional cointegration based on dynamic time warping is employed to learn links between words in different groups. A tensor-based technique is developed to cluster these linked words into different clusters (ideas) and track the flow of ideas. The main feature of the tensor representation is that we introduce two additional dimensions to represent both time and lead-lag relationships. Experiments on both synthetic and real datasets show that our method is more effective than methods based on traditional clustering techniques and achieves better accuracy. A case study was conducted to demonstrate the usefulness of our method in helping users understand the flow of ideas between different user groups on social media version:1
arxiv-1511-06692 | Direct Prediction of 3D Body Poses from Motion Compensated Sequences | http://arxiv.org/abs/1511.06692 | id:1511.06692 author:Bugra Tekin, Artem Rozantsev, Vincent Lepetit, Pascal Fua category:cs.CV  published:2015-11-20 summary:We propose an efficient approach to exploiting motion information from consecutive frames of a video sequence to recover the 3D pose of people. Previous approaches typically compute candidate poses in individual frames and then link them in a post-processing step to resolve ambiguities. By contrast, we directly regress from a spatio-temporal volume of bounding boxes to a 3D pose in the central frame. We further show that, for this approach to achieve its full potential, it is essential to compensate for the motion in consecutive frames so that the subject remains centered. This then allows us to effectively overcome ambiguities and improve upon the state-of-the-art by a large margin on the Human3.6m, HumanEva, and KTH Multiview Football 3D human pose estimation benchmarks. version:2
arxiv-1512-03993 | Deep Tracking: Visual Tracking Using Deep Convolutional Networks | http://arxiv.org/abs/1512.03993 | id:1512.03993 author:Meera Hahn, Si Chen, Afshin Dehghan category:cs.CV  published:2015-12-13 summary:In this paper, we study a discriminatively trained deep convolutional network for the task of visual tracking. Our tracker utilizes both motion and appearance features that are extracted from a pre-trained dual stream deep convolution network. We show that the features extracted from our dual-stream network can provide rich information about the target and this leads to competitive performance against state of the art tracking methods on a visual tracking benchmark. version:1
arxiv-1512-03990 | Cloud-based Electronic Health Records for Real-time, Region-specific Influenza Surveillance | http://arxiv.org/abs/1512.03990 | id:1512.03990 author:Mauricio Santillana, Andre Nguyen, Tamara Louie, Anna Zink, Josh Gray, Iyue Sung, John S. Brownstein category:stat.AP stat.ML  published:2015-12-13 summary:Accurate real-time monitoring systems of influenza outbreaks help public health officials make informed decisions that may help save lives. We show that information extracted from cloud-based electronic health records databases, in combination with machine learning techniques and historical epidemiological information, have the potential to accurately and reliably provide near real-time regional predictions of flu outbreaks in the United States. version:1
arxiv-1512-03980 | Action Recognition with Image Based CNN Features | http://arxiv.org/abs/1512.03980 | id:1512.03980 author:Mahdyar Ravanbakhsh, Hossein Mousavi, Mohammad Rastegari, Vittorio Murino, Larry S. Davis category:cs.CV  published:2015-12-13 summary:Most of human actions consist of complex temporal compositions of more simple actions. Action recognition tasks usually relies on complex handcrafted structures as features to represent the human action model. Convolutional Neural Nets (CNN) have shown to be a powerful tool that eliminate the need for designing handcrafted features. Usually, the output of the last layer in CNN (a layer before the classification layer -known as fc7) is used as a generic feature for images. In this paper, we show that fc7 features, per se, can not get a good performance for the task of action recognition, when the network is trained only on images. We present a feature structure on top of fc7 features, which can capture the temporal variation in a video. To represent the temporal components, which is needed to capture motion information, we introduced a hierarchical structure. The hierarchical model enables to capture sub-actions from a complex action. At the higher levels of the hierarchy, it represents a coarse capture of action sequence and lower levels represent fine action elements. Furthermore, we introduce a method for extracting key-frames using binary coding of each frame in a video, which helps to improve the performance of our hierarchical model. We experimented our method on several action datasets and show that our method achieves superior results compared to other state-of-the-arts methods. version:1
arxiv-1512-03958 | RNN Fisher Vectors for Action Recognition and Image Annotation | http://arxiv.org/abs/1512.03958 | id:1512.03958 author:Guy Lev, Gil Sadeh, Benjamin Klein, Lior Wolf category:cs.CV  published:2015-12-12 summary:Recurrent Neural Networks (RNNs) have had considerable success in classifying and predicting sequences. We demonstrate that RNNs can be effectively used in order to encode sequences and provide effective representations. The methodology we use is based on Fisher Vectors, where the RNNs are the generative probabilistic models and the partial derivatives are computed using backpropagation. State of the art results are obtained in two central but distant tasks, which both rely on sequences: video action recognition and image annotation. We also show a surprising transfer learning result from the task of image annotation to the task of video action recognition. version:1
arxiv-1512-03953 | Active Distance-Based Clustering using K-medoids | http://arxiv.org/abs/1512.03953 | id:1512.03953 author:Mehrdad Ghadiri, Amin Aghaee, Mahdieh Soleymani Baghshah category:cs.LG  published:2015-12-12 summary:k-medoids algorithm is a partitional, centroid-based clustering algorithm which uses pairwise distances of data points and tries to directly decompose the dataset with $n$ points into a set of $k$ disjoint clusters. However, k-medoids itself requires all distances between data points that are not so easy to get in many applications. In this paper, we introduce a new method which requires only a small proportion of the whole set of distances and makes an effort to estimate an upper-bound for unknown distances using the inquired ones. This algorithm makes use of the triangle inequality to calculate an upper-bound estimation of the unknown distances. Our method is built upon a recursive approach to cluster objects and to choose some points actively from each bunch of data and acquire the distances between these prominent points from oracle. Experimental results show that the proposed method using only a small subset of the distances can find proper clustering on many real-world and synthetic datasets. version:1
arxiv-1503-03438 | A mathematical motivation for complex-valued convolutional networks | http://arxiv.org/abs/1503.03438 | id:1503.03438 author:Joan Bruna, Soumith Chintala, Yann LeCun, Serkan Piantino, Arthur Szlam, Mark Tygert category:cs.LG cs.NE stat.ML  published:2015-03-11 summary:A complex-valued convolutional network (convnet) implements the repeated application of the following composition of three operations, recursively applying the composition to an input vector of nonnegative real numbers: (1) convolution with complex-valued vectors followed by (2) taking the absolute value of every entry of the resulting vectors followed by (3) local averaging. For processing real-valued random vectors, complex-valued convnets can be viewed as "data-driven multiscale windowed power spectra," "data-driven multiscale windowed absolute spectra," "data-driven multiwavelet absolute values," or (in their most general configuration) "data-driven nonlinear multiwavelet packets." Indeed, complex-valued convnets can calculate multiscale windowed spectra when the convnet filters are windowed complex-valued exponentials. Standard real-valued convnets, using rectified linear units (ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max. pooling, etc., do not obviously exhibit the same exact correspondence with data-driven wavelets (whereas for complex-valued convnets, the correspondence is much more than just a vague analogy). Courtesy of the exact correspondence, the remarkably rich and rigorous body of mathematical analysis for wavelets applies directly to (complex-valued) convnets. version:3
arxiv-1512-03950 | A Hidden Markov Model Based System for Entity Extraction from Social Media English Text at FIRE 2015 | http://arxiv.org/abs/1512.03950 | id:1512.03950 author:Kamal Sarkar category:cs.CL 68T50  published:2015-12-12 summary:This paper presents the experiments carried out by us at Jadavpur University as part of the participation in FIRE 2015 task: Entity Extraction from Social Media Text - Indian Languages (ESM-IL). The tool that we have developed for the task is based on Trigram Hidden Markov Model that utilizes information like gazetteer list, POS tag and some other word level features to enhance the observation probabilities of the known tokens as well as unknown tokens. We submitted runs for English only. A statistical HMM (Hidden Markov Models) based model has been used to implement our system. The system has been trained and tested on the datasets released for FIRE 2015 task: Entity Extraction from Social Media Text - Indian Languages (ESM-IL). Our system is the best performer for English language and it obtains precision, recall and F-measures of 61.96, 39.46 and 48.21 respectively. version:1
arxiv-1510-02755 | A Novel Approach to Document Classification using WordNet | http://arxiv.org/abs/1510.02755 | id:1510.02755 author:Koushiki Sarkar, Ritwika Law category:cs.IR cs.CL  published:2015-10-04 summary:Content based Document Classification is one of the biggest challenges in the context of free text mining. Current algorithms on document classifications mostly rely on cluster analysis based on bag-of-words approach. However that method is still being applied to many modern scientific dilemmas. It has established a strong presence in fields like economics and social science to merit serious attention from the researchers. In this paper we would like to propose and explore an alternative grounded more securely on the dictionary classification and correlatedness of words and phrases. It is expected that application of our existing knowledge about the underlying classification structure may lead to improvement of the classifier's performance. version:2
arxiv-1512-03929 | Quantum assisted Gaussian process regression | http://arxiv.org/abs/1512.03929 | id:1512.03929 author:Zhikuan Zhao, Jack K. Fitzsimons, Joseph F. Fitzsimons category:quant-ph cs.LG stat.ML  published:2015-12-12 summary:Gaussian processes (GP) are a widely used model for regression problems in supervised machine learning. Implementation of GP regression typically requires $O(n^3)$ logic gates. We show that the quantum linear systems algorithm [Harrow et al., Phys. Rev. Lett. 103, 150502 (2009)] can be applied to Gaussian process regression (GPR), leading to an exponential reduction in computation time in some instances. We show that even in some cases not ideally suited to the quantum linear systems algorithm, a polynomial increase in efficiency still occurs. version:1
arxiv-1512-03880 | Active Sampler: Light-weight Accelerator for Complex Data Analytics at Scale | http://arxiv.org/abs/1512.03880 | id:1512.03880 author:Jinyang Gao, H. V. Jagadish, Beng Chin Ooi category:cs.DB cs.LG stat.ML  published:2015-12-12 summary:Recent years have witnessed amazing outcomes from "Big Models" trained by "Big Data". Most popular algorithms for model training are iterative. Due to the surging volumes of data, we can usually afford to process only a fraction of the training data in each iteration. Typically, the data are either uniformly sampled or sequentially accessed. In this paper, we study how the data access pattern can affect model training. We propose an Active Sampler algorithm, where training data with more "learning value" to the model are sampled more frequently. The goal is to focus training effort on valuable instances near the classification boundaries, rather than evident cases, noisy data or outliers. We show the correctness and optimality of Active Sampler in theory, and then develop a light-weight vectorized implementation. Active Sampler is orthogonal to most approaches optimizing the efficiency of large-scale data analytics, and can be applied to most analytics models trained by stochastic gradient descent (SGD) algorithm. Extensive experimental evaluations demonstrate that Active Sampler can speed up the training procedure of SVM, feature selection and deep learning, for comparable training quality by 1.6-2.2x. version:1
arxiv-1512-03850 | Minimal Perceptrons for Memorizing Complex Patterns | http://arxiv.org/abs/1512.03850 | id:1512.03850 author:Marissa Pastor, Juyong Song, Danh-Tai Hoang, Junghyo Jo category:q-bio.NC cs.NE  published:2015-12-12 summary:Feedforward neural networks have been investigated to understand learning and memory, as well as applied to numerous practical problems in pattern classification. It is a rule of thumb that more complex tasks require larger networks. However, the design of optimal network architectures for specific tasks is still an unsolved fundamental problem. In this study, we consider three-layered neural networks for memorizing binary patterns. We developed a new complexity measure of binary patterns, and estimated the minimal network size for memorizing them as a function of their complexity. We formulated the minimal network size for regular, random, and complex patterns. In particular, the minimal size for complex patterns, which are neither ordered nor disordered, was predicted by measuring their Hamming distances from known ordered patterns. Our predictions agreed with simulations based on the back-propagation algorithm. version:1
arxiv-1112-5997 | Multispectral Palmprint Recognition Using a Hybrid Feature | http://arxiv.org/abs/1112.5997 | id:1112.5997 author:Sina Akbari Mistani, Shervin Minaee, Emad Fatemizadeh category:cs.CV  published:2011-12-27 summary:Personal identification problem has been a major field of research in recent years. Biometrics-based technologies that exploit fingerprints, iris, face, voice and palmprints, have been in the center of attention to solve this problem. Palmprints can be used instead of fingerprints that have been of the earliest of these biometrics technologies. A palm is covered with the same skin as the fingertips but has a larger surface, giving us more information than the fingertips. The major features of the palm are palm-lines, including principal lines, wrinkles and ridges. Using these lines is one of the most popular approaches towards solving the palmprint recognition problem. Another robust feature is the wavelet energy of palms. In this paper we used a hybrid feature which combines both of these features. %Moreover, multispectral analysis is applied to improve the performance of the system. At the end, minimum distance classifier is used to match test images with one of the training samples. The proposed algorithm has been tested on a well-known multispectral palmprint dataset and achieved an average accuracy of 98.8\%. version:3
arxiv-1512-03844 | Efficient Deep Feature Learning and Extraction via StochasticNets | http://arxiv.org/abs/1512.03844 | id:1512.03844 author:Mohammad Javad Shafiee, Parthipan Siva, Paul Fieguth, Alexander Wong category:cs.LG stat.ML  published:2015-12-11 summary:Deep neural networks are a powerful tool for feature learning and extraction given their ability to model high-level abstractions in highly complex data. One area worth exploring in feature learning and extraction using deep neural networks is efficient neural connectivity formation for faster feature learning and extraction. Motivated by findings of stochastic synaptic connectivity formation in the brain as well as the brain's uncanny ability to efficiently represent information, we propose the efficient learning and extraction of features via StochasticNets, where sparsely-connected deep neural networks can be formed via stochastic connectivity between neurons. To evaluate the feasibility of such a deep neural network architecture for feature learning and extraction, we train deep convolutional StochasticNets to learn abstract features using the CIFAR-10 dataset, and extract the learned features from images to perform classification on the SVHN and STL-10 datasets. Experimental results show that features learned using deep convolutional StochasticNets, with fewer neural connections than conventional deep convolutional neural networks, can allow for better or comparable classification accuracy than conventional deep neural networks: relative test error decrease of ~4.5% for classification on the STL-10 dataset and ~1% for classification on the SVHN dataset. Furthermore, it was shown that the deep features extracted using deep convolutional StochasticNets can provide comparable classification accuracy even when only 10% of the training data is used for feature learning. Finally, it was also shown that significant gains in feature extraction speed can be achieved in embedded applications using StochasticNets. As such, StochasticNets allow for faster feature learning and extraction performance while facilitate for better or comparable accuracy performances. version:1
arxiv-1512-02728 | Distributed Training of Deep Neural Networks with Theoretical Analysis: Under SSP Setting | http://arxiv.org/abs/1512.02728 | id:1512.02728 author:Abhimanu Kumar, Pengtao Xie, Junming Yin, Eric P. Xing category:stat.ML cs.LG math.OC  published:2015-12-09 summary:We propose a distributed approach to train deep neural networks (DNNs), which has guaranteed convergence theoretically and great scalability empirically: close to 6 times faster on instance of ImageNet data set when run with 6 machines. The proposed scheme is close to optimally scalable in terms of number of machines, and guaranteed to converge to the same optima as the undistributed setting. The convergence and scalability of the distributed setting is shown empirically across di?erent datasets (TIMIT and ImageNet) and machine learning tasks (image classi?cation and phoneme extraction). The convergence analysis provides novel insights into this complex learning scheme, including: 1) layerwise convergence, and 2) convergence of the weights in probability. version:2
arxiv-1511-01064 | Color Space Transformation Network | http://arxiv.org/abs/1511.01064 | id:1511.01064 author:Alexandros Karargyris category:cs.CV  published:2015-10-31 summary:Deep networks have become very popular over the past few years. The main reason for this widespread use is their excellent ability to learn and predict knowledge in a very easy and efficient way. Convolutional neural networks and auto-encoders have become the normal in the area of imaging and computer vision achieving unprecedented accuracy levels in many applications. The most common strategy is to build and train networks with many layers by tuning their hyper-parameters. While this approach has proven to be a successful way to build robust deep learning schemes it suffers from high complexity. In this paper we introduce a module that learns color space transformations within a network. Given a large dataset of colored images the color space transformation module tries to learn color space transformations that increase overall classification accuracy. This module has shown to increase overall accuracy for the same network design and to achieve faster convergence. It is part of a broader family of image transformations (e.g. spatial transformer network). version:2
arxiv-1512-00567 | Rethinking the Inception Architecture for Computer Vision | http://arxiv.org/abs/1512.00567 | id:1512.00567 author:Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna category:cs.CV  published:2015-12-02 summary:Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set. version:3
arxiv-1512-03740 | Improving Human Activity Recognition Through Ranking and Re-ranking | http://arxiv.org/abs/1512.03740 | id:1512.03740 author:Zhenzhong Lan, Shoou-I Yu, Alexander G. Hauptmann category:cs.CV  published:2015-12-11 summary:We propose two well-motivated ranking-based methods to enhance the performance of current state-of-the-art human activity recognition systems. First, as an improvement over the classic power normalization method, we propose a parameter-free ranking technique called rank normalization (RaN). RaN normalizes each dimension of the video features to address the sparse and bursty distribution problems of Fisher Vectors and VLAD. Second, inspired by curriculum learning, we introduce a training-free re-ranking technique called multi-class iterative re-ranking (MIR). MIR captures relationships among action classes by separating easy and typical videos from difficult ones and re-ranking the prediction scores of classifiers accordingly. We demonstrate that our methods significantly improve the performance of state-of-the-art motion features on six real-world datasets. version:1
arxiv-1512-03706 | A New Approach of Gray Images Binarization with Threshold Methods | http://arxiv.org/abs/1512.03706 | id:1512.03706 author:Andrei Hossu, Daniela Andone category:cs.CV  published:2015-12-11 summary:The paper presents some aspects of the (gray level) image binarization methods used in artificial vision systems. It is introduced a new approach of gray level image binarization for artificial vision systems dedicated to industrial automation temporal thresholding. In the first part of the paper are extracted some limitations of using the global optimum thresholding in gray level image binarization. In the second part of this paper are presented some aspects of the dynamic optimum thresholding method for gray level image binarization. Starting from classic methods of global and dynamic optimal thresholding of the gray level images in the next section are introduced the concepts of temporal histogram and temporal thresholding. In the final section are presented some practical aspects of the temporal thresholding method in artificial vision applications form the moving scene in robotic automation class; pointing out the influence of the acquisition frequency on the methods results. version:1
arxiv-1508-00998 | Single and Multiple Illuminant Estimation Using Convolutional Neural Networks | http://arxiv.org/abs/1508.00998 | id:1508.00998 author:Simone Bianco, Claudio Cusano, Raimondo Schettini category:cs.CV  published:2015-08-05 summary:In this paper we present a method for the estimation of the color of the illuminant in RAW images. The method includes a Convolutional Neural Network that has been specially designed to produce multiple local estimates. A multiple illuminant detector determines whether or not the local outputs of the network must be aggregated into a single estimate. We evaluated our method on standard datasets with single and multiple illuminants, obtaining lower estimation errors with respect to those obtained by other general purpose methods in the state of the art. version:2
arxiv-1512-03622 | Deep Feature Learning with Relative Distance Comparison for Person Re-identification | http://arxiv.org/abs/1512.03622 | id:1512.03622 author:Shengyong Ding, Liang Lin, Guangrun Wang, Hongyang Chao category:cs.CV  published:2015-12-11 summary:Identifying the same individual across different scenes is an important yet difficult task in intelligent video surveillance. Its main difficulty lies in how to preserve similarity of the same person against large appearance and structure variation while discriminating different individuals. In this paper, we present a scalable distance driven feature learning framework based on the deep neural network for person re-identification, and demonstrate its effectiveness to handle the existing challenges. Specifically, given the training images with the class labels (person IDs), we first produce a large number of triplet units, each of which contains three images, i.e. one person with a matched reference and a mismatched reference. Treating the units as the input, we build the convolutional neural network to generate the layered representations, and follow with the $L2$ distance metric. By means of parameter optimization, our framework tends to maximize the relative distance between the matched pair and the mismatched pair for each triplet unit. Moreover, a nontrivial issue arising with the framework is that the triplet organization cubically enlarges the number of training triplets, as one image can be involved into several triplet units. To overcome this problem, we develop an effective triplet generation scheme and an optimized gradient descent algorithm, making the computational load mainly depends on the number of original images instead of the number of triplets. On several challenging databases, our approach achieves very promising results and outperforms other state-of-the-art approaches. version:1
arxiv-1512-03617 | Robust Dictionary based Data Representation | http://arxiv.org/abs/1512.03617 | id:1512.03617 author:Wei-Ya Ren category:cs.CV  published:2015-12-11 summary:The robustness to noise and outliers is an important issue in linear representation in real applications. We focus on the problem that samples are grossly corrupted, which is also the 'sample specific' corruptions problem. A reasonable assumption is that corrupted samples cannot be represented by the dictionary while clean samples can be well represented. This assumption is enforced in this paper by investigating the coefficients of corrupted samples. Concretely, we require the coefficients of corrupted samples be zero. In this way, the representation quality of clean data can be assured without the effect of corrupted data. At last, a robust dictionary based data representation approach and its sparse representation version are proposed, which have directive significance for future applications. version:1
arxiv-1512-00077 | Decoding Hidden Markov Models Faster Than Viterbi Via Online Matrix-Vector (max, +)-Multiplication | http://arxiv.org/abs/1512.00077 | id:1512.00077 author:Massimo Cairo, Gabriele Farina, Romeo Rizzi category:cs.LG cs.DS cs.IT math.IT  published:2015-11-30 summary:In this paper, we present a novel algorithm for the maximum a posteriori decoding (MAPD) of time-homogeneous Hidden Markov Models (HMM), improving the worst-case running time of the classical Viterbi algorithm by a logarithmic factor. In our approach, we interpret the Viterbi algorithm as a repeated computation of matrix-vector $(\max, +)$-multiplications. On time-homogeneous HMMs, this computation is online: a matrix, known in advance, has to be multiplied with several vectors revealed one at a time. Our main contribution is an algorithm solving this version of matrix-vector $(\max,+)$-multiplication in subquadratic time, by performing a polynomial preprocessing of the matrix. Employing this fast multiplication algorithm, we solve the MAPD problem in $O(mn^2/ \log n)$ time for any time-homogeneous HMM of size $n$ and observation sequence of length $m$, with an extra polynomial preprocessing cost negligible for $m > n$. To the best of our knowledge, this is the first algorithm for the MAPD problem requiring subquadratic time per observation, under the only assumption -- usually verified in practice -- that the transition probability matrix does not change with time. version:2
arxiv-1512-03549 | Words are not Equal: Graded Weighting Model for building Composite Document Vectors | http://arxiv.org/abs/1512.03549 | id:1512.03549 author:Pranjal Singh, Amitabha Mukerjee category:cs.CL cs.LG cs.NE  published:2015-12-11 summary:Despite the success of distributional semantics, composing phrases from word vectors remains an important challenge. Several methods have been tried for benchmark tasks such as sentiment classification, including word vector averaging, matrix-vector approaches based on parsing, and on-the-fly learning of paragraph vectors. Most models usually omit stop words from the composition. Instead of such an yes-no decision, we consider several graded schemes where words are weighted according to their discriminatory relevance with respect to its use in the document (e.g., idf). Some of these methods (particularly tf-idf) are seen to result in a significant improvement in performance over prior state of the art. Further, combining such approaches into an ensemble based on alternate classifiers such as the RNN model, results in an 1.6% performance improvement on the standard IMDB movie review dataset, and a 7.01% improvement on Amazon product reviews. Since these are language free models and can be obtained in an unsupervised manner, they are of interest also for under-resourced languages such as Hindi as well and many more languages. We demonstrate the language free aspects by showing a gain of 12% for two review datasets over earlier results, and also release a new larger dataset for future testing (Singh,2015). version:1
arxiv-1512-03542 | Distilling Knowledge from Deep Networks with Applications to Healthcare Domain | http://arxiv.org/abs/1512.03542 | id:1512.03542 author:Zhengping Che, Sanjay Purushotham, Robinder Khemani, Yan Liu category:stat.ML cs.LG  published:2015-12-11 summary:Exponential growth in Electronic Healthcare Records (EHR) has resulted in new opportunities and urgent needs for discovery of meaningful data-driven representations and patterns of diseases in Computational Phenotyping research. Deep Learning models have shown superior performance for robust prediction in computational phenotyping tasks, but suffer from the issue of model interpretability which is crucial for clinicians involved in decision-making. In this paper, we introduce a novel knowledge-distillation approach called Interpretable Mimic Learning, to learn interpretable phenotype features for making robust prediction while mimicking the performance of deep learning models. Our framework uses Gradient Boosting Trees to learn interpretable features from deep learning models such as Stacked Denoising Autoencoder and Long Short-Term Memory. Exhaustive experiments on a real-world clinical time-series dataset show that our method obtains similar or better performance than the deep learning models, and it provides interpretable phenotypes for clinical decision making. version:1
arxiv-1512-03526 | Randomized Low-Rank Dynamic Mode Decomposition for Motion Detection | http://arxiv.org/abs/1512.03526 | id:1512.03526 author:N. Benjamin Erichson, Carl Donovan category:cs.CV  published:2015-12-11 summary:This paper introduces a fast algorithm for randomized computation of a low-rank Dynamic Mode Decomposition (DMD) of a matrix. Here we consider this matrix to represent the development of a spatial grid through time e.g. data from a static video source. DMD was originally introduced in the fluid mechanics community, but is also suitable for motion detection in video streams and its use for background subtraction has received little previous investigation. In this study we present a comprehensive evaluation of background subtraction, using the randomized DMD and compare the results with leading robust principal component analysis algorithms. The results are convincing and show the random DMD is an efficient and powerful approach for background modeling, allowing processing of high resolution videos in real-time. Supplementary materials include implementations of the algorithms in Python. version:1
arxiv-1512-03518 | A Unified Approach to Error Bounds for Structured Convex Optimization Problems | http://arxiv.org/abs/1512.03518 | id:1512.03518 author:Zirui Zhou, Anthony Man-Cho So category:math.OC cs.LG math.NA stat.ML  published:2015-12-11 summary:Error bounds, which refer to inequalities that bound the distance of vectors in a test set to a given set by a residual function, have proven to be extremely useful in analyzing the convergence rates of a host of iterative methods for solving optimization problems. In this paper, we present a new framework for establishing error bounds for a class of structured convex optimization problems, in which the objective function is the sum of a smooth convex function and a general closed proper convex function. Such a class encapsulates not only fairly general constrained minimization problems but also various regularized loss minimization formulations in machine learning, signal processing, and statistics. Using our framework, we show that a number of existing error bound results can be recovered in a unified and transparent manner. To further demonstrate the power of our framework, we apply it to a class of nuclear-norm regularized loss minimization problems and establish a new error bound for this class under a strict complementarity-type regularity condition. We then complement this result by constructing an example to show that the said error bound could fail to hold without the regularity condition. Consequently, we obtain a rather complete answer to a question raised by Tseng. We believe that our approach will find further applications in the study of error bounds for structured convex optimization problems. version:1
arxiv-1512-00843 | Protein secondary structure prediction using deep convolutional neural fields | http://arxiv.org/abs/1512.00843 | id:1512.00843 author:Sheng Wang, Jian Peng, Jianzhu Ma, Jinbo Xu category:q-bio.BM cs.LG q-bio.QM  published:2015-12-02 summary:Protein secondary structure (SS) prediction is important for studying protein structure and function. When only the sequence (profile) information is used as input feature, currently the best predictors can obtain ~80% Q3 accuracy, which has not been improved in the past decade. Here we present DeepCNF (Deep Convolutional Neural Fields) for protein SS prediction. DeepCNF is a Deep Learning extension of Conditional Neural Fields (CNF), which is an integration of Conditional Random Fields (CRF) and shallow neural networks. DeepCNF can model not only complex sequence-structure relationship by a deep hierarchical architecture, but also interdependency between adjacent SS labels, so it is much more powerful than CNF. Experimental results show that DeepCNF can obtain ~84% Q3 accuracy, ~85% SOV score, and ~72% Q8 accuracy, respectively, on the CASP and CAMEO test proteins, greatly outperforming currently popular predictors. As a general framework, DeepCNF can be used to predict other protein structure properties such as contact number, disorder regions, and solvent accessibility. version:3
arxiv-1512-03466 | Computing factorized approximations of Pareto-fronts using mNM-landscapes and Boltzmann distributions | http://arxiv.org/abs/1512.03466 | id:1512.03466 author:Roberto Santana, Alexander Mendiburu, Jose A. Lozano category:cs.NE  published:2015-12-10 summary:NM-landscapes have been recently introduced as a class of tunable rugged models. They are a subset of the general interaction models where all the interactions are of order less or equal $M$. The Boltzmann distribution has been extensively applied in single-objective evolutionary algorithms to implement selection and study the theoretical properties of model-building algorithms. In this paper we propose the combination of the multi-objective NM-landscape model and the Boltzmann distribution to obtain Pareto-front approximations. We investigate the joint effect of the parameters of the NM-landscapes and the probabilistic factorizations in the shape of the Pareto front approximations. version:1
arxiv-1512-03465 | Measuring Semantic Relatedness using Mined Semantic Analysis | http://arxiv.org/abs/1512.03465 | id:1512.03465 author:Walid Shalaby, Wlodek Zadrozny category:cs.CL H.3.1  published:2015-12-10 summary:Mined Semantic Analysis (MSA) is a novel distributional semantics approach which employs data mining techniques. MSA embraces knowledge-driven analysis of natural languages. It uncovers implicit relations between concepts by mining for their associations in target encyclopedic corpora. MSA exploits not only target corpus content but also its knowledge graph (e.g., "See also" link graph of Wikipedia). Empirical results show competitive performance of MSA compared to prior state-of-the-art methods for measuring semantic relatedness on benchmark data sets. Additionally, we introduce the first analytical study to examine statistical significance of results reported by different semantic relatedness methods. Our study shows that, top performing results could be statistically equivalent though mathematically different. The study positions MSA as one of state-of-the-art methods for measuring semantic relatedness. version:1
arxiv-1512-03460 | Neural Self Talk: Image Understanding via Continuous Questioning and Answering | http://arxiv.org/abs/1512.03460 | id:1512.03460 author:Yezhou Yang, Yi Li, Cornelia Fermuller, Yiannis Aloimonos category:cs.CV cs.CL cs.RO I.2.10  published:2015-12-10 summary:In this paper we consider the problem of continuously discovering image contents by actively asking image based questions and subsequently answering the questions being asked. The key components include a Visual Question Generation (VQG) module and a Visual Question Answering module, in which Recurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are used. Given a dataset that contains images, questions and their answers, both modules are trained at the same time, with the difference being VQG uses the images as input and the corresponding questions as output, while VQA uses images and questions as input and the corresponding answers as output. We evaluate the self talk process subjectively using Amazon Mechanical Turk, which show effectiveness of the proposed method. version:1
arxiv-1507-00407 | Fast Convergence of Regularized Learning in Games | http://arxiv.org/abs/1507.00407 | id:1507.00407 author:Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, Robert E. Schapire category:cs.GT cs.AI cs.LG  published:2015-07-02 summary:We show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games. When each player in a game uses an algorithm from our class, their individual regret decays at $O(T^{-3/4})$, while the sum of utilities converges to an approximate optimum at $O(T^{-1})$--an improvement upon the worst case $O(T^{-1/2})$ rates. We show a black-box reduction for any algorithm in the class to achieve $\tilde{O}(T^{-1/2})$ rates against an adversary, while maintaining the faster rates against algorithms in the class. Our results extend those of [Rakhlin and Shridharan 2013] and [Daskalakis et al. 2014], who only analyzed two-player zero-sum games for specific algorithms. version:5
arxiv-1512-03444 | Cross-Validated Variable Selection in Tree-Based Methods Improves Predictive Performance | http://arxiv.org/abs/1512.03444 | id:1512.03444 author:Amichai Painsky, Saharon Rosset category:stat.ML  published:2015-12-10 summary:Recursive partitioning approaches producing tree-like models are a long standing staple of predictive modeling, in the last decade mostly as ``sub-learners'' within state of the art ensemble methods like Boosting and Random Forest. However, a fundamental flaw in the partitioning (or splitting) rule of commonly used tree building methods precludes them from treating different types of variables equally. This most clearly manifests in these methods' inability to properly utilize categorical variables with a large number of categories, which are ubiquitous in the new age of big data. Such variables can often be very informative, but current tree methods essentially leave us a choice of either not using them, or exposing our models to severe overfitting. We propose a conceptual framework to splitting using leave-one-out (LOO) cross validation for selecting the splitting variable, then performing a regular split (in our case, following CART's approach) for the selected variable. The most important consequence of our approach is that categorical variables with many categories can be safely used in tree building and are only chosen if they contribute to predictive power. We demonstrate in extensive simulation and real data analysis that our novel splitting approach significantly improves the performance of both single tree models and ensemble methods that utilize trees. Importantly, we design an algorithm for LOO splitting variable selection which under reasonable assumptions does not increase the overall computational complexity compared to CART for two-class classification. For regression tasks, our approach carries an increased computational burden, replacing a O(log(n)) factor in CART splitting rule search with an O(n) term. version:1
arxiv-1406-7349 | Convex Analysis of Mixtures for Separating Non-negative Well-grounded Sources | http://arxiv.org/abs/1406.7349 | id:1406.7349 author:Yitan Zhu, Niya Wang, David J. Miller, Yue Wang category:stat.ML q-bio.QM  published:2014-06-28 summary:Blind Source Separation (BSS) has proven to be a powerful tool for the analysis of composite patterns in engineering and science. We introduce Convex Analysis of Mixtures (CAM) for separating non-negative well-grounded sources, which learns the mixing matrix by identifying the lateral edges of the convex data scatter plot. We prove a sufficient and necessary condition for identifying the mixing matrix through edge detection, which also serves as the foundation for CAM to be applied not only to the exact-determined and over-determined cases, but also to the under-determined case. We show the optimality of the edge detection strategy, even for cases where source well-groundedness is not strictly satisfied. The CAM algorithm integrates plug-in noise filtering using sector-based clustering, an efficient geometric convex analysis scheme, and stability-based model order selection. We demonstrate the principle of CAM on simulated data and numerically mixed natural images. The superior performance of CAM against a panel of benchmark BSS techniques is demonstrated on numerically mixed gene expression data. We then apply CAM to dissect dynamic contrast-enhanced magnetic resonance imaging data taken from breast tumors and time-course microarray gene expression data derived from in-vivo muscle regeneration in mice, both producing biologically plausible decomposition results. version:3
arxiv-1512-03443 | Scalable Modeling of Conversational-role based Self-presentation Characteristics in Large Online Forums | http://arxiv.org/abs/1512.03443 | id:1512.03443 author:Abhimanu Kumar, Shriphani Palakodety, Chong Wang, Carolyn P. Rose, Eric P. Xing, Miaomiao Wen category:stat.ML cs.SI  published:2015-12-10 summary:Online discussion forums are complex webs of overlapping subcommunities (macrolevel structure, across threads) in which users enact different roles depending on which subcommunity they are participating in within a particular time point (microlevel structure, within threads). This sub-network structure is implicit in massive collections of threads. To uncover this structure, we develop a scalable algorithm based on stochastic variational inference and leverage topic models (LDA) along with mixed membership stochastic block (MMSB) models. We evaluate our model on three large-scale datasets, Cancer-ThreadStarter (22K users and 14.4K threads), Cancer-NameMention(15.1K users and 12.4K threads) and StackOverFlow (1.19 million users and 4.55 million threads). Qualitatively, we demonstrate that our model can provide useful explanations of microlevel and macrolevel user presentation characteristics in different communities using the topics discovered from posts. Quantitatively, we show that our model does better than MMSB and LDA in predicting user reply structure within threads. In addition, we demonstrate via synthetic data experiments that the proposed active sub-network discovery model is stable and recovers the original parameters of the experimental setup with high probability. version:1
arxiv-1512-03396 | Boosted Sparse Non-linear Distance Metric Learning | http://arxiv.org/abs/1512.03396 | id:1512.03396 author:Yuting Ma, Tian Zheng category:stat.ML cs.LG  published:2015-12-10 summary:This paper proposes a boosting-based solution addressing metric learning problems for high-dimensional data. Distance measures have been used as natural measures of (dis)similarity and served as the foundation of various learning methods. The efficiency of distance-based learning methods heavily depends on the chosen distance metric. With increasing dimensionality and complexity of data, however, traditional metric learning methods suffer from poor scalability and the limitation due to linearity as the true signals are usually embedded within a low-dimensional nonlinear subspace. In this paper, we propose a nonlinear sparse metric learning algorithm via boosting. We restructure a global optimization problem into a forward stage-wise learning of weak learners based on a rank-one decomposition of the weight matrix in the Mahalanobis distance metric. A gradient boosting algorithm is devised to obtain a sparse rank-one update of the weight matrix at each step. Nonlinear features are learned by a hierarchical expansion of interactions incorporated within the boosting algorithm. Meanwhile, an early stopping rule is imposed to control the overall complexity of the learned metric. As a result, our approach guarantees three desirable properties of the final metric: positive semi-definiteness, low rank and element-wise sparsity. Numerical experiments show that our learning model compares favorably with the state-of-the-art methods in the current literature of metric learning. version:1
arxiv-1512-03385 | Deep Residual Learning for Image Recognition | http://arxiv.org/abs/1512.03385 | id:1512.03385 author:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun category:cs.CV  published:2015-12-10 summary:Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. version:1
arxiv-1512-02673 | Speeding Up Distributed Machine Learning Using Codes | http://arxiv.org/abs/1512.02673 | id:1512.02673 author:Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris Papailiopoulos, Kannan Ramchandran category:cs.DC cs.IT cs.LG cs.PF math.IT  published:2015-12-08 summary:Codes are widely used in many engineering applications to offer some form of reliability and fault tolerance. The high-level idea of coding is to exploit resource redundancy to deliver higher robustness against system noise. In large-scale systems there are several types of "noise" that can affect the performance of distributed machine learning algorithms: straggler nodes, system failures, or communication bottlenecks. Moreover, redundancy is abundant: a plethora of nodes, a lot of spare storage, etc. In this work, scratching the surface of "codes for distributed computation," we provide theoretical insights on how coded solutions can achieve significant gains compared to uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms: matrix multiplication and data shuffling. For matrix multiplication, we use codes to leverage the plethora of nodes and alleviate the effects of stragglers. We show that if the number of workers is $n$, and the runtime of each subtask has an exponential tail, the optimal coded matrix multiplication is $\Theta(\log n)$ times faster than the uncoded matrix multiplication. In data shuffling, we use codes to exploit the excess in storage and reduce communication bottlenecks. We show that when $\alpha$ is the fraction of the data matrix that can be cached at each worker, and $n$ is the number of workers, coded shuffling reduces the communication cost by a factor $\Theta(\alpha \gamma(n))$ compared to uncoded shuffling, where $\gamma(n)$ is the ratio of the cost of unicasting $n$ messages to $n$ users to broadcasting a common message (of the same size) to $n$ users. Our synthetic and Open MPI experiments on Amazon EC2 show that coded distributed algorithms can achieve significant speedups of up to 40% compared to uncoded distributed algorithms. version:2
arxiv-1512-03375 | Convolutional Monte Carlo Rollouts in Go | http://arxiv.org/abs/1512.03375 | id:1512.03375 author:Peter H. Jin, Kurt Keutzer category:cs.LG cs.AI  published:2015-12-10 summary:In this work, we present a MCTS-based Go-playing program which uses convolutional networks in all parts. Our method performs MCTS in batches, explores the Monte Carlo search tree using Thompson sampling and a convolutional network, and evaluates convnet-based rollouts on the GPU. We achieve strong win rates against open source Go programs and attain competitive results against state of the art convolutional net-based Go-playing programs. version:1
arxiv-1512-02665 | Fine-grained Image Classification by Exploring Bipartite-Graph Labels | http://arxiv.org/abs/1512.02665 | id:1512.02665 author:Feng Zhou, Yuanqing Lin category:cs.CV  published:2015-12-08 summary:Given a food image, can a fine-grained object recognition engine tell "which restaurant which dish" the food belongs to? Such ultra-fine grained image recognition is the key for many applications like search by images, but it is very challenging because it needs to discern subtle difference between classes while dealing with the scarcity of training data. Fortunately, the ultra-fine granularity naturally brings rich relationships among object classes. This paper proposes a novel approach to exploit the rich relationships through bipartite-graph labels (BGL). We show how to model BGL in an overall convolutional neural networks and the resulting system can be optimized through back-propagation. We also show that it is computationally efficient in inference thanks to the bipartite structure. To facilitate the study, we construct a new food benchmark dataset, which consists of 37,885 food images collected from 6 restaurants and totally 975 menus. Experimental results on this new food and three other datasets demonstrates BGL advances previous works in fine-grained object recognition. An online demo is available at http://www.f-zhou.com/fg_demo/. version:2
arxiv-1512-03308 | Guaranteed algorithms for inference in topic models | http://arxiv.org/abs/1512.03308 | id:1512.03308 author:Khoat Than, Tung Doan category:stat.ML  published:2015-12-10 summary:One of the core problems in statistical models is the estimation of a posterior distribution. For topic models, the problem of posterior inference for individual texts is particularly important, especially when dealing with data streams, but is often intractable in the worst case \citep{SontagR11}. As a consequence, existing methods for posterior inference are approximate and do not have any guarantee on neither quality nor convergence rate. In this paper, we introduce a provably fast algorithm, namely \textit{Online Maximum a Posterior Estimation (OPE)}, for posterior inference in topic models. OPE has more attractive properties than existing inference approaches, including theoretical guarantees on quality and fast convergence rate. The discussions about OPE are very general and hence can be easily employed in a wide class of probabilistic models. Finally, we employ OPE to design three novel methods for learning Latent Dirichlet allocation from text streams or large corpora. Extensive experiments demonstrate some superior behaviors of OPE and of our new learning methods. version:1
arxiv-1504-02762 | Image patch analysis of sunspots and active regions. II. Clustering via matrix factorization | http://arxiv.org/abs/1504.02762 | id:1504.02762 author:Kevin R. Moon, Veronique Delouille, Jimmy J. Li, Ruben De Visscher, Fraser Watson, Alfred O. Hero III category:astro-ph.SR cs.CV  published:2015-04-10 summary:Separating active regions that are quiet from potentially eruptive ones is a key issue in Space Weather applications. Traditional classification schemes such as Mount Wilson and McIntosh have been effective in relating an active region large scale magnetic configuration to its ability to produce eruptive events. However, their qualitative nature prevents systematic studies of an active region's evolution for example. We introduce a new clustering of active regions that is based on the local geometry observed in Line of Sight magnetogram and continuum images. We use a reduced-dimension representation of an active region that is obtained by factoring the corresponding data matrix comprised of local image patches. Two factorizations can be compared via the definition of appropriate metrics on the resulting factors. The distances obtained from these metrics are then used to cluster the active regions. We find that these metrics result in natural clusterings of active regions. The clusterings are related to large scale descriptors of an active region such as its size, its local magnetic field distribution, and its complexity as measured by the Mount Wilson classification scheme. We also find that including data focused on the neutral line of an active region can result in an increased correspondence between our clustering results and other active region descriptors such as the Mount Wilson classifications and the $R$ value. We provide some recommendations for which metrics, matrix factorization techniques, and regions of interest to use to study active regions. version:2
arxiv-1512-03300 | Inference in topic models: sparsity and trade-off | http://arxiv.org/abs/1512.03300 | id:1512.03300 author:Khoat Than, Tu Bao Ho category:stat.ML  published:2015-12-10 summary:Topic models are popular for modeling discrete data (e.g., texts, images, videos, links), and provide an efficient way to discover hidden structures/semantics in massive data. One of the core problems in this field is the posterior inference for individual data instances. This problem is particularly important in streaming environments, but is often intractable. In this paper, we investigate the use of the Frank-Wolfe algorithm (FW) for recovering sparse solutions to posterior inference. From detailed elucidation of both theoretical and practical aspects, FW exhibits many interesting properties which are beneficial to topic modeling. We then employ FW to design fast methods, including ML-FW, for learning latent Dirichlet allocation (LDA) at large scales. Extensive experiments show that to reach the same predictiveness level, ML-FW can perform tens to thousand times faster than existing state-of-the-art methods for learning LDA from massive/streaming data. version:1
arxiv-1512-01882 | THCHS-30 : A Free Chinese Speech Corpus | http://arxiv.org/abs/1512.01882 | id:1512.01882 author:Dong Wang, Xuewei Zhang category:cs.CL cs.SD  published:2015-12-07 summary:Speech data is crucially important for speech recognition research. There are quite some speech databases that can be purchased at prices that are reasonable for most research institutes. However, for young people who just start research activities or those who just gain initial interest in this direction, the cost for data is still an annoying barrier. We support the `free data' movement in speech recognition: research institutes (particularly supported by public funds) publish their data freely so that new researchers can obtain sufficient data to kick of their career. In this paper, we follow this trend and release a free Chinese speech database THCHS-30 that can be used to build a full- edged Chinese speech recognition system. We report the baseline system established with this database, including the performance under highly noisy conditions. version:2
arxiv-1511-04192 | DISC: Deep Image Saliency Computing via Progressive Representation Learning | http://arxiv.org/abs/1511.04192 | id:1511.04192 author:Tianshui Chen, Liang Lin, Lingbo Liu, Xiaonan Luo, Xuelong Li category:cs.CV  published:2015-11-13 summary:Salient object detection increasingly receives attention as an important component or step in several pattern recognition and image processing tasks. Although a variety of powerful saliency models have been intensively proposed, they usually involve heavy feature (or model) engineering based on priors (or assumptions) about the properties of objects and backgrounds. Inspired by the effectiveness of recently developed feature learning, we provide a novel Deep Image Saliency Computing (DISC) framework for fine-grained image saliency computing. In particular, we model the image saliency from both the coarse- and fine-level observations, and utilize the deep convolutional neural network (CNN) to learn the saliency representation in a progressive manner. Specifically, our saliency model is built upon two stacked CNNs. The first CNN generates a coarse-level saliency map by taking the overall image as the input, roughly identifying saliency regions in the global context. Furthermore, we integrate superpixel-based local context information in the first CNN to refine the coarse-level saliency map. Guided by the coarse saliency map, the second CNN focuses on the local context to produce fine-grained and accurate saliency map while preserving object details. For a testing image, the two CNNs collaboratively conduct the saliency computing in one shot. Our DISC framework is capable of uniformly highlighting the objects-of-interest from complex background while preserving well object details. Extensive experiments on several standard benchmarks suggest that DISC outperforms other state-of-the-art methods and it also generalizes well across datasets without additional training. The executable version of DISC is available online: http://vision.sysu.edu.cn/projects/DISC. version:2
arxiv-1408-1381 | A Population Background for Nonparametric Density-Based Clustering | http://arxiv.org/abs/1408.1381 | id:1408.1381 author:José E. Chacón category:math.ST math.CA math.DG math.GT stat.ML stat.TH  published:2014-08-06 summary:Despite its popularity, it is widely recognized that the investigation of some theoretical aspects of clustering has been relatively sparse. One of the main reasons for this lack of theoretical results is surely the fact that, whereas for other statistical problems the theoretical population goal is clearly defined (as in regression or classification), for some of the clustering methodologies it is difficult to specify the population goal to which the data-based clustering algorithms should try to get close. This paper aims to provide some insight into the theoretical foundations of clustering by focusing on two main objectives: to provide an explicit formulation for the ideal population goal of the modal clustering methodology, which understands clusters as regions of high density; and to present two new loss functions, applicable in fact to any clustering methodology, to evaluate the performance of a data-based clustering algorithm with respect to the ideal population goal. In particular, it is shown that only mild conditions on a sequence of density estimators are needed to ensure that the sequence of modal clusterings that they induce is consistent. version:2
arxiv-1306-4032 | On Russian Roulette Estimates for Bayesian Inference with Doubly-Intractable Likelihoods | http://arxiv.org/abs/1306.4032 | id:1306.4032 author:Anne-Marie Lyne, Mark Girolami, Yves Atchadé, Heiko Strathmann, Daniel Simpson category:stat.ME stat.CO stat.ML  published:2013-06-17 summary:A large number of statistical models are "doubly-intractable": the likelihood normalising term, which is a function of the model parameters, is intractable, as well as the marginal likelihood (model evidence). This means that standard inference techniques to sample from the posterior, such as Markov chain Monte Carlo (MCMC), cannot be used. Examples include, but are not confined to, massive Gaussian Markov random fields, autologistic models and Exponential random graph models. A number of approximate schemes based on MCMC techniques, Approximate Bayesian computation (ABC) or analytic approximations to the posterior have been suggested, and these are reviewed here. Exact MCMC schemes, which can be applied to a subset of doubly-intractable distributions, have also been developed and are described in this paper. As yet, no general method exists which can be applied to all classes of models with doubly-intractable posteriors. In addition, taking inspiration from the Physics literature, we study an alternative method based on representing the intractable likelihood as an infinite series. Unbiased estimates of the likelihood can then be obtained by finite time stochastic truncation of the series via Russian Roulette sampling, although the estimates are not necessarily positive. Results from the Quantum Chromodynamics literature are exploited to allow the use of possibly negative estimates in a pseudo-marginal MCMC scheme such that expectations with respect to the posterior distribution are preserved. The methodology is reviewed on well-known examples such as the parameters in Ising models, the posterior for Fisher-Bingham distributions on the $d$-Sphere and a large-scale Gaussian Markov Random Field model describing the Ozone Column data. This leads to a critical assessment of the strengths and weaknesses of the methodology with pointers to ongoing research. version:4
arxiv-1512-03201 | Gated networks: an inventory | http://arxiv.org/abs/1512.03201 | id:1512.03201 author:Olivier Sigaud, Clément Masson, David Filliat, Freek Stulp category:cs.LG  published:2015-12-10 summary:Gated networks are networks that contain gating connections, in which the outputs of at least two neurons are multiplied. Initially, gated networks were used to learn relationships between two input sources, such as pixels from two images. More recently, they have been applied to learning activity recognition or multi-modal representations. The aims of this paper are threefold: 1) to explain the basic computations in gated networks to the non-expert, while adopting a standpoint that insists on their symmetric nature. 2) to serve as a quick reference guide to the recent literature, by providing an inventory of applications of these networks, as well as recent extensions to the basic architecture. 3) to suggest future research directions and applications. version:1
arxiv-1512-03156 | 3D Reconstruction of Crime Scenes and Design Considerations for an Interactive Investigation Tool | http://arxiv.org/abs/1512.03156 | id:1512.03156 author:Erkan Bostanci category:cs.CV  published:2015-12-10 summary:Crime Scene Investigation (CSI) is a carefully planned systematic process with the purpose of acquiring physical evidences to shed light upon the physical reality of the crime and eventually detect the identity of the criminal. Capturing images and videos of the crime scene is an important part of this process in order to conduct a deeper analysis on the digital evidence for possible hints. This work brings this idea further to use the acquired footage for generating a 3D model of the crime scene. Results show that realistic reconstructions can be obtained using sophisticated computer vision techniques. The paper also discusses a number of important design considerations describing key features that should be present in a powerful interactive CSI analysis tool. version:1
arxiv-1512-03155 | Enhanced image feature coverage: Key-point selection using genetic algorithms | http://arxiv.org/abs/1512.03155 | id:1512.03155 author:Erkan Bostanci category:cs.CV  published:2015-12-10 summary:Coverage of image features play an important role in many vision algorithms since their distribution affect the estimated homography. This paper presents a Genetic Algorithm (GA) in order to select the optimal set of features yielding maximum coverage of the image which is measured by a robust method based on spatial statistics. It is shown with statistical tests on two datasets that the metric yields better coverage and this is also confirmed by an accuracy test on the computed homography for the original set and the newly selected set of features. Results have demonstrated that the new set has similar performance in terms of the accuracy of the computed homography with the original one with an extra benefit of using fewer number of features ultimately reducing the time required for descriptor calculation and matching. version:1
arxiv-1512-03424 | Evaluation of Object Detection Proposals Under Condition Variations | http://arxiv.org/abs/1512.03424 | id:1512.03424 author:Fahimeh Rezazadegan, Sareh Shirazi, Michael Milford, Ben Upcroft category:cs.CV  published:2015-12-10 summary:Object detection is a fundamental task in many computer vision applications, therefore the importance of evaluating the quality of object detection is well acknowledged in this domain. This process gives insight into the capabilities of methods in handling environmental changes. In this paper, a new method for object detection is introduced that combines the Selective Search and EdgeBoxes. We tested these three methods under environmental variations. Our experiments demonstrate the outperformance of the combination method under illumination and view point variations. version:1
arxiv-1512-03423 | Predicting proximity with ambient mobile sensors for non-invasive health diagnostics | http://arxiv.org/abs/1512.03423 | id:1512.03423 author:Sylvester Olubolu Orimaye, Foo Chuan Leong, Chen Hui Lee, Eddy Cheng Han Ng category:cs.CY cs.LG  published:2015-12-10 summary:Modern smart phones are becoming helpful in the areas of Internet-Of-Things (IoT) and ambient health intelligence. By learning data from several mobile sensors, we detect nearness of the human body to a mobile device in a three-dimensional space with no physical contact with the device for non-invasive health diagnostics. We show that the human body generates wave patterns that interact with other naturally occurring ambient signals that could be measured by mobile sensors, such as, temperature, humidity, magnetic field, acceleration, gravity, and light. This interaction consequentially alters the patterns of the naturally occurring signals, and thus, exhibits characteristics that could be learned to predict the nearness of the human body to a mobile device, hence provide diagnostic information for medical practitioners. Our prediction technique achieved 88.75% accuracy and 88.3% specificity. version:1
arxiv-1502-06470 | Approximate Message Passing with Restricted Boltzmann Machine Priors | http://arxiv.org/abs/1502.06470 | id:1502.06470 author:Eric W. Tramel, Angélique Drémeau, Florent Krzakala category:cs.IT cond-mat.dis-nn math.IT physics.data-an stat.ML  published:2015-02-23 summary:Approximate Message Passing (AMP) has been shown to be an excellent statistical approach to signal inference and compressed sensing problem. The AMP framework provides modularity in the choice of signal prior; here we propose a hierarchical form of the Gauss-Bernouilli prior which utilizes a Restricted Boltzmann Machine (RBM) trained on the signal support to push reconstruction performance beyond that of simple iid priors for signals whose support can be well represented by a trained binary RBM. We present and analyze two methods of RBM factorization and demonstrate how these affect signal reconstruction performance within our proposed algorithm. Finally, using the MNIST handwritten digit dataset, we show experimentally that using an RBM allows AMP to approach oracle-support performance. version:3
arxiv-1511-02436 | Learning Linguistic Biomarkers for Predicting Mild Cognitive Impairment using Compound Skip-grams | http://arxiv.org/abs/1511.02436 | id:1511.02436 author:Sylvester Olubolu Orimaye, Kah Yee Tai, Jojo Sze-Meng Wong, Chee Piau Wong category:cs.CL cs.AI  published:2015-11-08 summary:Predicting Mild Cognitive Impairment (MCI) is currently a challenge as existing diagnostic criteria rely on neuropsychological examinations. Automated Machine Learning (ML) models that are trained on verbal utterances of MCI patients can aid diagnosis. Using a combination of skip-gram features, our model learned several linguistic biomarkers to distinguish between 19 patients with MCI and 19 healthy control individuals from the DementiaBank language transcript clinical dataset. Results show that a model with compound of skip-grams has better AUC and could help ML prediction on small MCI data sample. version:2
arxiv-1512-03131 | Deep Learning Algorithms with Applications to Video Analytics for A Smart City: A Survey | http://arxiv.org/abs/1512.03131 | id:1512.03131 author:Li Wang, Dennis Sng category:cs.CV  published:2015-12-10 summary:Deep learning has recently achieved very promising results in a wide range of areas such as computer vision, speech recognition and natural language processing. It aims to learn hierarchical representations of data by using deep architecture models. In a smart city, a lot of data (e.g. videos captured from many distributed sensors) need to be automatically processed and analyzed. In this paper, we review the deep learning algorithms applied to video analytics of smart city in terms of different research topics: object detection, object tracking, face recognition, image classification and scene labeling. version:1
arxiv-1407-6639 | How the Voynich Manuscript was created | http://arxiv.org/abs/1407.6639 | id:1407.6639 author:Torsten Timm category:cs.CR cs.CL  published:2014-07-24 summary:The Voynich manuscript is a medieval book written in an unknown script. This paper studies the relation between similarly spelled words in the Voynich manuscript. By means of a detailed analysis of similar spelled words it was possible to reveal the text generation method used for the Voynich manuscript. version:3
arxiv-1512-03081 | Gamma Belief Networks | http://arxiv.org/abs/1512.03081 | id:1512.03081 author:Mingyuan Zhou, Yulai Cong, Bo Chen category:stat.ML stat.ME  published:2015-12-09 summary:To infer multilayer deep representations of high-dimensional discrete and nonnegative real vectors, we propose the gamma belief network (GBN) that factorizes each of its hidden layers into the product of a sparse connection weight matrix and the nonnegative real hidden units of the next layer. The GBN's hidden layers are jointly trained with an upward-downward Gibbs sampler that solves each layer with the same subroutine. The gamma-negative binomial process combined with a layer-wise training strategy allows inferring the width of each layer given a fixed budget on the width of the first layer. Example results illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the GBN can add more layers to improve its performance in both unsupervisedly extracting features and predicting heldout data. For exploratory data analysis, we extract trees and subnetworks from the learned deep network to visualize how the very specific factors discovered at the first hidden layer and the increasingly more general factors discovered at deeper hidden layers are related to each other, and we generate synthetic data by propagating random variables through the deep network from the top hidden layer back to the bottom data layer. version:1
arxiv-1512-03025 | Partial Reinitialisation for Optimisers | http://arxiv.org/abs/1512.03025 | id:1512.03025 author:Ilia Zintchenko, Matthew Hastings, Nathan Wiebe, Ethan Brown, Matthias Troyer category:stat.ML cs.LG cs.NE math.OC  published:2015-12-09 summary:Heuristic optimisers which search for an optimal configuration of variables relative to an objective function often get stuck in local optima where the algorithm is unable to find further improvement. The standard approach to circumvent this problem involves periodically restarting the algorithm from random initial configurations when no further improvement can be found. We propose a method of partial reinitialization, whereby, in an attempt to find a better solution, only sub-sets of variables are re-initialised rather than the whole configuration. Much of the information gained from previous runs is hence retained. This leads to significant improvements in the quality of the solution found in a given time for a variety of optimisation problems in machine learning. version:1
arxiv-1512-03019 | Minimally Supervised Feature Selection for Classification (Master's Thesis, University Politehnica of Bucharest) | http://arxiv.org/abs/1512.03019 | id:1512.03019 author:Alexandra Maria Radu category:cs.CV  published:2015-12-09 summary:In the context of the highly increasing number of features that are available nowadays we design a robust and fast method for feature selection. The method tries to select the most representative features that are independent from each other, but are strong together. We propose an algorithm that requires very limited labeled data (as few as one labeled frame per class) and can accommodate as many unlabeled samples. We also present here the supervised approach from which we started. We compare our two formulations with established methods like AdaBoost, SVM, Lasso, Elastic Net and FoBa and show that our method is much faster and it has constant training time. Moreover, the unsupervised approach outperforms all the methods with which we compared and the difference might be quite prominent. The supervised approach is in most cases better than the other methods, especially when the number of training shots is very limited. All that the algorithm needs is to choose from a pool of positively correlated features. The methods are evaluated on the Youtube-Objects dataset of videos and on MNIST digits dataset, while at training time we also used features obtained on CIFAR10 dataset and others pre-trained on ImageNet dataset. Thereby, we also proved that transfer learning is useful, even though the datasets differ very much: from low-resolution centered images from 10 classes, to high-resolution images with objects from 1000 classes occurring in different regions of the images or to very difficult videos with very high intraclass variance. 7 version:1
arxiv-1512-03012 | ShapeNet: An Information-Rich 3D Model Repository | http://arxiv.org/abs/1512.03012 | id:1512.03012 author:Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, Fisher Yu category:cs.GR cs.AI cs.CG cs.CV cs.RO  published:2015-12-09 summary:We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans. version:1
arxiv-1511-06408 | Feature-based Attention in Convolutional Neural Networks | http://arxiv.org/abs/1511.06408 | id:1511.06408 author:Grace W. Lindsay category:cs.CV  published:2015-11-19 summary:Convolutional neural networks (CNNs) have proven effective for image processing tasks, such as object recognition and classification. Recently, CNNs have been enhanced with concepts of attention, similar to those found in biology. Much of this work on attention has focused on effective serial spatial processing. In this paper, I introduce a simple procedure for applying feature-based attention (FBA) to CNNs and compare multiple implementation options. FBA is a top-down signal applied globally to an input image which aides in detecting chosen objects in cluttered or noisy settings. The concept of FBA and the implementation details tested here were derived from what is known (and debated) about biological object- and feature-based attention. The implementations of FBA described here increase performance on challenging object detection tasks using a procedure that is simple, fast, and does not require additional iterative training. Furthermore, the comparisons performed here suggest that a proposed model of biological FBA (the "feature similarity gain model") is effective in increasing performance. version:2
arxiv-1512-02972 | Get More With Less: Near Real-Time Image Clustering on Mobile Phones | http://arxiv.org/abs/1512.02972 | id:1512.02972 author:Jorge Ortiz, Chien-Chin Huang, Supriyo Chakraborty category:cs.CV cs.DC cs.PF  published:2015-12-09 summary:Machine learning algorithms, in conjunction with user data, hold the promise of revolutionizing the way we interact with our phones, and indeed their widespread adoption in the design of apps bear testimony to this promise. However, currently, the computationally expensive segments of the learning pipeline, such as feature extraction and model training, are offloaded to the cloud, resulting in an over-reliance on the network and under-utilization of computing resources available on mobile platforms. In this paper, we show that by combining the computing power distributed over a number of phones, judicious optimization choices, and contextual information it is possible to execute the end-to-end pipeline entirely on the phones at the edge of the network, efficiently. We also show that by harnessing the power of this combination, it is possible to execute a computationally expensive pipeline at near real-time. To demonstrate our approach, we implement an end-to-end image-processing pipeline -- that includes feature extraction, vocabulary learning, vectorization, and image clustering -- on a set of mobile phones. Our results show a 75% improvement over the standard, full pipeline implementation running on the phones without modification -- reducing the time to one minute under certain conditions. We believe that this result is a promising indication that fully distributed, infrastructure-less computing is possible on networks of mobile phones; enabling a new class of mobile applications that are less reliant on the cloud. version:1
arxiv-1512-02970 | Scaling Up Distributed Stochastic Gradient Descent Using Variance Reduction | http://arxiv.org/abs/1512.02970 | id:1512.02970 author:Soham De, Gavin Taylor, Tom Goldstein category:cs.LG cs.DC math.OC stat.ML  published:2015-12-09 summary:Variance reduction stochastic gradient descent methods enable minimization of model fitting problems involving big datasets with low iteration complexity and fast asymptotic convergence rates. However, they scale poorly in distributed settings. In this paper, we propose a highly parallel variance reduction method, CentralVR, with performance that scales linearly with the number of worker nodes. We also propose distributed versions of popular variance reduction methods that support a high degree of parallelization. Unlike existing distributed stochastic gradient schemes, CentralVR exhibits linear performance gains up to thousands of cores for massive datasets. version:1
arxiv-1512-02949 | Video captioning with recurrent networks based on frame- and video-level features and visual content classification | http://arxiv.org/abs/1512.02949 | id:1512.02949 author:Rakshith Shetty, Jorma Laaksonen category:cs.CV  published:2015-12-09 summary:In this paper, we describe the system for generating textual descriptions of short video clips using recurrent neural networks (RNN), which we used while participating in the Large Scale Movie Description Challenge 2015 in ICCV 2015. Our work builds on static image captioning systems with RNN based language models and extends this framework to videos utilizing both static image features and video-specific features. In addition, we study the usefulness of visual content classifiers as a source of additional information for caption generation. With experimental results we show that utilizing keyframe based features, dense trajectory video features and content classifier outputs together gives better performance than any one of them individually. version:1
arxiv-1512-02930 | Stochastic Interpretation of Quasi-periodic Event-based Systems | http://arxiv.org/abs/1512.02930 | id:1512.02930 author:Hesham Mostafa, Giacomo Indiveri category:cs.NE cs.ET q-bio.NC  published:2015-12-09 summary:Many networks used in machine learning and as models of biological neural networks make use of stochastic neurons or neuron-like units. We show that stochastic artificial neurons can be realized on silicon chips by exploiting the quasi-periodic behavior of mismatched analog oscillators to approximate the neuron's stochastic activation function. We represent neurons by finite state machines (FSMs) that communicate using digital events and whose transitions are event-triggered. The event generation times of each neuron are controlled by an analog oscillator internal to that neuron/FSM and the frequencies of the oscillators in different FSMs are incommensurable. We show that within this quasi-periodic system, the transition graph of a FSM can be interpreted as the transition graph of a Markov chain and we show that by using different FSMs, we can obtain approximations of different stochastic activation functions. We investigate the quality of the stochastic interpretation of such a deterministic system and we use the system to realize and sample from a restricted Boltzmann machine. We implemented the quasi-periodic event-based system on a custom silicon chip and we show that the chip behavior can be used to closely approximate a stochastic sampling task. version:1
arxiv-1512-02914 | Yet Another Statistical Analysis of Bob Ross Paintings | http://arxiv.org/abs/1512.02914 | id:1512.02914 author:Christopher Steven Marcum category:stat.AP cs.CV  published:2015-12-09 summary:In this paper, we analyze a sample of clippings from paintings by the late artist Bob Ross. Previous work focused on the qualitative themes of his paintings (Hickey, 2014); here, we expand on that line of research by considering the colorspace and luminosity values as our data. Our results demonstrate the subtle aesthetics of the average Ross painting, the common variation shared by his paintings, and the structure of the relationships between each painting in our sample. We reveal, for the first time, renderings of the average paintings and introduce "eigenross" components to identify and evaluate shared variance. Additionally, all data and code are embedded in this document to encourage future research, and, in the spirit of Bob Ross, to teach others how to do so. version:1
arxiv-1512-02902 | MovieQA: Understanding Stories in Movies through Question-Answering | http://arxiv.org/abs/1512.02902 | id:1512.02902 author:Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, Sanja Fidler category:cs.CV cs.CL  published:2015-12-09 summary:We introduce the MovieQA dataset which aims to evaluate automatic story comprehension from both video and text. The dataset consists of 7702 questions about 294 movies with high semantic diversity. The questions range from simpler "Who" did "What" to "Whom", to "Why" and "How" certain events occurred. Each question comes with a set of five possible answers; a correct one and four deceiving answers provided by human annotators. Our dataset is unique in that it contains multiple sources of information -- full-length movies, plots, subtitles, scripts and for a subset DVS. We analyze our data through various statistics and intelligent baselines. We further extend existing QA techniques to show that question-answering with such open-ended semantics is hard. We plan to create a benchmark with an active leader board, to encourage inspiring work in this challenging domain. version:1
arxiv-1512-02896 | Where You Are Is Who You Are: User Identification by Matching Statistics | http://arxiv.org/abs/1512.02896 | id:1512.02896 author:Farid M. Naini, Jayakrishnan Unnikrishnan, Patrick Thiran, Martin Vetterli category:cs.LG cs.CR cs.SI stat.AP stat.ML  published:2015-12-09 summary:Most users of online services have unique behavioral or usage patterns. These behavioral patterns can be exploited to identify and track users by using only the observed patterns in the behavior. We study the task of identifying users from statistics of their behavioral patterns. Specifically, we focus on the setting in which we are given histograms of users' data collected during two different experiments. We assume that, in the first dataset, the users' identities are anonymized or hidden and that, in the second dataset, their identities are known. We study the task of identifying the users by matching the histograms of their data in the first dataset with the histograms from the second dataset. In recent works, the optimal algorithm for this user identification task is introduced. In this paper, we evaluate the effectiveness of this method on three different types of datasets and in multiple scenarios. Using datasets such as call data records, web browsing histories, and GPS trajectories, we show that a large fraction of users can be easily identified given only histograms of their data; hence these histograms can act as users' fingerprints. We also verify that simultaneous identification of users achieves better performance compared to one-by-one user identification. We show that using the optimal method for identification gives higher identification accuracy than heuristics-based approaches in practical scenarios. The accuracy obtained under this optimal method can thus be used to quantify the maximum level of user identification that is possible in such settings. We show that the key factors affecting the accuracy of the optimal identification algorithm are the duration of the data collection, the number of users in the anonymized dataset, and the resolution of the dataset. We analyze the effectiveness of k-anonymization in resisting user identification attacks on these datasets. version:1
arxiv-1511-04136 | DETRAC: A New Benchmark and Protocol for Multi-Object Tracking | http://arxiv.org/abs/1511.04136 | id:1511.04136 author:Longyin Wen, Dawei Du, Zhaowei Cai, Zhen Lei, Ming-Ching Chang, Honggang Qi, Jongwoo Lim, Ming-Hsuan Yang, Siwei Lyu category:cs.CV  published:2015-11-13 summary:In recent years, most effective multi-object tracking (MOT) methods are based on the tracking-by-detection framework. Existing performance evaluations of MOT methods usually separate the target association step from the object detection step by using the same object detection results for comparisons. In this work, we perform a comprehensive quantitative study on the effect of object detection accuracy to the overall MOT performance. This is based on a new large-scale DETection and tRACking (DETRAC) benchmark dataset. The DETRAC benchmark dataset consists of 100 challenging video sequences captured from real-world traffic scenes (over 140 thousand frames and 1.2 million labeled bounding boxes of objects) for both object detection and MOT. We evaluate complete MOT systems constructed from combinations of state-of-the-art target association methods and object detection schemes. Our analysis shows the complex effects of object detection accuracy on MOT performance. Based on these observations, we propose new evaluation tools and metrics for MOT systems that consider both object detection and target association for comprehensive analysis. version:2
arxiv-1512-02866 | Multi-Player Bandits -- a Musical Chairs Approach | http://arxiv.org/abs/1512.02866 | id:1512.02866 author:Jonathan Rosenski, Ohad Shamir, Liran Szlak category:cs.LG stat.ML  published:2015-12-09 summary:We consider a variant of the stochastic multi-armed bandit problem, where multiple players simultaneously choose from the same set of arms and may collide, receiving no reward. This setting has been motivated by problems arising in cognitive radio networks, and is especially challenging under the realistic assumption that communication between players is limited. We provide a communication-free algorithm (Musical Chairs) which attains constant regret with high probability, as well as a sublinear-regret, communication-free algorithm (Dynamic Musical Chairs) for the more difficult setting of players dynamically entering and leaving throughout the game. Moreover, both algorithms do not require prior knowledge of the number of players. To the best of our knowledge, these are the first communication-free algorithms with these types of formal guarantees. We also rigorously compare our algorithms to previous works, and complement our theoretical findings with experiments. version:1
arxiv-1512-02831 | Bigger Buffer k-d Trees on Multi-Many-Core Systems | http://arxiv.org/abs/1512.02831 | id:1512.02831 author:Fabian Gieseke, Cosmin Eugen Oancea, Ashish Mahabal, Christian Igel, Tom Heskes category:cs.DC cs.DS cs.LG  published:2015-12-09 summary:A buffer k-d tree is a k-d tree variant for massively-parallel nearest neighbor search. While providing valuable speed-ups on modern many-core devices in case both a large number of reference and query points are given, buffer k-d trees are limited by the amount of points that can fit on a single device. In this work, we show how to modify the original data structure and the associated workflow to make the overall approach capable of dealing with massive data sets. We further provide a simple yet efficient way of using multiple devices given in a single workstation. The applicability of the modified framework is demonstrated in the context of astronomy, a field that is faced with huge amounts of data. version:1
arxiv-1512-02033 | Risk Minimization in Structured Prediction using Orbit Loss | http://arxiv.org/abs/1512.02033 | id:1512.02033 author:Danny Karmon, Joseph Keshet category:cs.LG  published:2015-12-07 summary:We introduce a new surrogate loss function called orbit loss in the structured prediction framework, which has good theoretical and practical advantages. While the orbit loss is not convex, it has a simple analytical gradient and a simple perceptron-like learning rule. We analyze the new loss theoretically and state a PAC-Bayesian generalization bound. We also prove that the new loss is consistent in the strong sense; namely, the risk achieved by the set of the trained parameters approaches the infimum risk achievable by any linear decoder over the given features. Methods that are aimed at risk minimization, such as the structured ramp loss, the structured probit loss and the direct loss minimization require at least two inference operations per training iteration. In this sense, the orbit loss is more efficient as it requires only one inference operation per training iteration, while yields similar performance. We conclude the paper with an empirical comparison of the proposed loss function to the structured hinge loss, the structured ramp loss, the structured probit loss and the direct loss minimization method on several benchmark datasets and tasks. version:2
arxiv-1512-02766 | Sensor Fusion of Camera, GPS and IMU using Fuzzy Adaptive Multiple Motion Models | http://arxiv.org/abs/1512.02766 | id:1512.02766 author:Erkan Bostanci, Betul Bostanci, Nadia Kanwal, Adrian F. Clark category:cs.RO cs.CV  published:2015-12-09 summary:A tracking system that will be used for Augmented Reality (AR) applications has two main requirements: accuracy and frame rate. The first requirement is related to the performance of the pose estimation algorithm and how accurately the tracking system can find the position and orientation of the user in the environment. Accuracy problems of current tracking devices, considering that they are low-cost devices, cause static errors during this motion estimation process. The second requirement is related to dynamic errors (the end-to-end system delay; occurring because of the delay in estimating the motion of the user and displaying images based on this estimate. This paper investigates combining the vision-based estimates with measurements from other sensors, GPS and IMU, in order to improve the tracking accuracy in outdoor environments. The idea of using Fuzzy Adaptive Multiple Models (FAMM) was investigated using a novel fuzzy rule-based approach to decide on the model that results in improved accuracy and faster convergence for the fusion filter. Results show that the developed tracking system is more accurate than a conventional GPS-IMU fusion approach due to additional estimates from a camera and fuzzy motion models. The paper also presents an application in cultural heritage context. version:1
arxiv-1511-05547 | Return of Frustratingly Easy Domain Adaptation | http://arxiv.org/abs/1511.05547 | id:1511.05547 author:Baochen Sun, Jiashi Feng, Kate Saenko category:cs.CV cs.AI cs.LG cs.NE  published:2015-11-17 summary:Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being "frustratingly easy" to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets. version:2
arxiv-1505-04406 | Hinge-Loss Markov Random Fields and Probabilistic Soft Logic | http://arxiv.org/abs/1505.04406 | id:1505.04406 author:Stephen H. Bach, Matthias Broecheler, Bert Huang, Lise Getoor category:cs.LG cs.AI stat.ML  published:2015-05-17 summary:A fundamental challenge in developing high-impact machine learning technologies is balancing the ability to model rich, structured domains with the ability to scale to big data. Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms for modeling structured data, distinguished from previous approaches by their ability to both capture rich structure and scale to big data. The first, hinge-loss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. We then derive HL-MRFs by generalizing this unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic. We next introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization software, because it uses message passing to take advantage of sparse dependency structures. We then show how to learn the parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable. Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible. version:2
arxiv-1412-1441 | Scalable, High-Quality Object Detection | http://arxiv.org/abs/1412.1441 | id:1412.1441 author:Christian Szegedy, Scott Reed, Dumitru Erhan, Dragomir Anguelov, Sergey Ioffe category:cs.CV  published:2014-12-03 summary:Current high-quality object detection approaches use the scheme of salience-based object proposal methods followed by post-classification using deep convolutional features. This spurred recent research in improving object proposal methods. However, domain agnostic proposal generation has the principal drawback that the proposals come unranked or with very weak ranking, making it hard to trade-off quality for running time. This raises the more fundamental question of whether high-quality proposal generation requires careful engineering or can be derived just from data alone. We demonstrate that learning-based proposal methods can effectively match the performance of hand-engineered methods while allowing for very efficient runtime-quality trade-offs. Using the multi-scale convolutional MultiBox (MSC-MultiBox) approach, we substantially advance the state-of-the-art on the ILSVRC 2014 detection challenge data set, with $0.5$ mAP for a single model and $0.52$ mAP for an ensemble of two models. MSC-Multibox significantly improves the proposal quality over its predecessor MultiBox~method: AP increases from $0.42$ to $0.53$ for the ILSVRC detection challenge. Finally, we demonstrate improved bounding-box recall compared to Multiscale Combinatorial Grouping with less proposals on the Microsoft-COCO data set. version:3
arxiv-1512-02736 | Window-Object Relationship Guided Representation Learning for Generic Object Detections | http://arxiv.org/abs/1512.02736 | id:1512.02736 author:Xingyu Zeng, Wanli Ouyang, Xiaogang Wang category:cs.CV cs.LG cs.MM  published:2015-12-09 summary:In existing works that learn representation for object detection, the relationship between a candidate window and the ground truth bounding box of an object is simplified by thresholding their overlap. This paper shows information loss in this simplification and picks up the relative location/size information discarded by thresholding. We propose a representation learning pipeline to use the relationship as supervision for improving the learned representation in object detection. Such relationship is not limited to object of the target category, but also includes surrounding objects of other categories. We show that image regions with multiple contexts and multiple rotations are effective in capturing such relationship during the representation learning process and in handling the semantic and visual variation caused by different window-object configurations. Experimental results show that the representation learned by our approach can improve the object detection accuracy by 6.4% in mean average precision (mAP) on ILSVRC2014. On the challenging ILSVRC2014 test dataset, 48.6% mAP is achieved by our single model and it is the best among published results. On PASCAL VOC, it outperforms the state-of-the-art result of Fast RCNN by 3.3% in absolute mAP. version:1
arxiv-1512-02047 | Level-Based Analysis of Genetic Algorithms for Combinatorial Optimization | http://arxiv.org/abs/1512.02047 | id:1512.02047 author:Duc-Cuong Dang, Anton V. Eremeev, Per Kristian Lehre category:cs.NE  published:2015-12-07 summary:The paper is devoted to upper bounds on run-time of Non-Elitist Genetic Algorithms until some target subset of solutions is visited for the first time. In particular, we consider the sets of optimal solutions and the sets of local optima as the target subsets. Previously known upper bounds are improved by means of drift analysis. Finally, we propose conditions ensuring that a Non-Elitist Genetic Algorithm efficiently finds approximate solutions with constant approximation ratio on the class of combinatorial optimization problems with guaranteed local optima (GLO). version:2
arxiv-1512-02693 | Reinforcement Control with Hierarchical Backpropagated Adaptive Critics | http://arxiv.org/abs/1512.02693 | id:1512.02693 author:John W. Jameson category:cs.NE cs.LG cs.SY I.2.6  published:2015-12-08 summary:Present incremental learning methods are limited in the ability to achieve reliable credit assignment over a large number time steps (or events). However, this situation is typical for cases where the dynamical system to be controlled requires relatively frequent control updates in order to maintain stability or robustness yet has some action-consequences which must be established over relatively long periods of time. To address this problem, the learning capabilities of a control architecture comprised of two Backpropagated Adaptive Critics (BACs) in a two-level hierarchy with continuous actions are explored. The high-level BAC updates less frequently than the low-level BAC and controls the latter to some degree. The response of the low-level to high-level signals can either be determined a priori or it can emerge during learning. A general approach called Response Induction Learning is introduced to address the latter case. version:1
arxiv-1509-06461 | Deep Reinforcement Learning with Double Q-learning | http://arxiv.org/abs/1509.06461 | id:1509.06461 author:Hado van Hasselt, Arthur Guez, David Silver category:cs.LG  published:2015-09-22 summary:The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games. version:3
arxiv-1507-05952 | Optimal Testing for Properties of Distributions | http://arxiv.org/abs/1507.05952 | id:1507.05952 author:Jayadev Acharya, Constantinos Daskalakis, Gautam Kamath category:cs.DS cs.IT cs.LG math.IT math.ST stat.TH  published:2015-07-21 summary:Given samples from an unknown distribution $p$, is it possible to distinguish whether $p$ belongs to some class of distributions $\mathcal{C}$ versus $p$ being far from every distribution in $\mathcal{C}$? This fundamental question has received tremendous attention in statistics, focusing primarily on asymptotic analysis, and more recently in information theory and theoretical computer science, where the emphasis has been on small sample size and computational complexity. Nevertheless, even for basic properties of distributions such as monotonicity, log-concavity, unimodality, independence, and monotone-hazard rate, the optimal sample complexity is unknown. We provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem: Given samples from an unknown distribution $p$, and a known distribution $q$, are $p$ and $q$ close in $\chi^2$-distance, or far in total variation distance? The optimality of our testers is established by providing matching lower bounds with respect to both $n$ and $\varepsilon$. Finally, a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave and monotone hazard rate distributions. version:3
arxiv-1512-02595 | Deep Speech 2: End-to-End Speech Recognition in English and Mandarin | http://arxiv.org/abs/1512.02595 | id:1512.02595 author:Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, Zhenyao Zhu category:cs.CL  published:2015-12-08 summary:We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale. version:1
arxiv-1512-02567 | Distributed Adaptive LMF Algorithm for Sparse Parameter Estimation in Gaussian Mixture Noise | http://arxiv.org/abs/1512.02567 | id:1512.02567 author:Mojtaba Hajiabadi category:cs.IT cs.CL math.IT  published:2015-12-08 summary:A distributed adaptive algorithm for estimation of sparse unknown parameters in the presence of nonGaussian noise is proposed in this paper based on normalized least mean fourth (NLMF) criterion. At the first step, local adaptive NLMF algorithm is modified by zero norm in order to speed up the convergence rate and also to reduce the steady state error power in sparse conditions. Then, the proposed algorithm is extended for distributed scenario in which more improvement in estimation performance is achieved due to cooperation of local adaptive filters. Simulation results show the superiority of the proposed algorithm in comparison with conventional NLMF algorithms. version:1
arxiv-1512-02565 | Selective Sequential Model Selection | http://arxiv.org/abs/1512.02565 | id:1512.02565 author:William Fithian, Jonathan Taylor, Robert Tibshirani, Ryan Tibshirani category:stat.ME stat.ML  published:2015-12-08 summary:Many model selection algorithms produce a path of fits specifying a sequence of increasingly complex models. Given such a sequence and the data used to produce them, we consider the problem of choosing the least complex model that is not falsified by the data. Extending the selected-model tests of Fithian et al. (2014), we construct p-values for each step in the path which account for the adaptive selection of the model path using the data. In the case of linear regression, we propose two specific tests, the max-t test for forward stepwise regression (generalizing a proposal of Buja and Brown (2014)), and the next-entry test for the lasso. These tests improve on the power of the saturated-model test of Tibshirani et al. (2014), sometimes dramatically. In addition, our framework extends beyond linear regression to a much more general class of parametric and nonparametric model selection problems. To select a model, we can feed our single-step p-values as inputs into sequential stopping rules such as those proposed by G'Sell et al. (2013) and Li and Barber (2015), achieving control of the familywise error rate or false discovery rate (FDR) as desired. The FDR-controlling rules require the null p-values to be independent of each other and of the non-null p-values, a condition not satisfied by the saturated-model p-values of Tibshirani et al. (2014). We derive intuitive and general sufficient conditions for independence, and show that our proposed constructions yield independent p-values. version:1
arxiv-1512-02560 | Deep Learning for Single and Multi-Session i-Vector Speaker Recognition | http://arxiv.org/abs/1512.02560 | id:1512.02560 author:Omid Ghahabi, Javier Hernando category:cs.SD cs.LG  published:2015-12-08 summary:The promising performance of Deep Learning (DL) in speech recognition has motivated the use of DL in other speech technology applications such as speaker recognition. Given i-vectors as inputs, the authors proposed an impostor selection algorithm and a universal model adaptation process in a hybrid system based on Deep Belief Networks (DBN) and Deep Neural Networks (DNN) to discriminatively model each target speaker. In order to have more insight into the behavior of DL techniques in both single and multi-session speaker enrollment tasks, some experiments have been carried out in this paper in both scenarios. Additionally, the parameters of the global model, referred to as universal DBN (UDBN), are normalized before adaptation. UDBN normalization facilitates training DNNs specifically with more than one hidden layer. Experiments are performed on the NIST SRE 2006 corpus. It is shown that the proposed impostor selection algorithm and UDBN adaptation process enhance the performance of conventional DNNs 8-20 % and 16-20 % in terms of EER for the single and multi-session tasks, respectively. In both scenarios, the proposed architectures outperform the baseline systems obtaining up to 17 % reduction in EER. version:1
arxiv-1512-02543 | Gibbs-type Indian buffet processes | http://arxiv.org/abs/1512.02543 | id:1512.02543 author:Creighton Heaukulani, Daniel M. Roy category:stat.ML  published:2015-12-08 summary:We investigate a class of feature allocation models that generalize the Indian buffet process and are parameterized by Gibbs-type random measures. Two existing classes are contained as special cases: the original two-parameter Indian buffet process, corresponding to the Dirichlet process, and the stable (or three-parameter) Indian buffet process, corresponding to the Pitman-Yor process. Asymptotic behavior of the Gibbs-type partitions, such as power laws holding for the number of latent clusters, translates into analogous characteristics for this class of Gibbs-type feature allocation models. Despite containing several different distinct subclasses, the properties of Gibbs-type partitions allow us to develop a black-box procedure for posterior inference within any subclass of models. Through numerical experiments, we compare and contrast a few of these subclasses and highlight the utility of varying power-law behaviors in the latent features. version:1
arxiv-1401-5492 | Alternating direction method of multipliers for penalized zero-variance discriminant analysis | http://arxiv.org/abs/1401.5492 | id:1401.5492 author:Brendan P. W. Ames, Mingyi Hong category:stat.ML math.OC  published:2014-01-21 summary:We consider the task of classification in the high dimensional setting where the number of features of the given data is significantly greater than the number of observations. To accomplish this task, we propose a heuristic, called sparse zero-variance discriminant analysis (SZVD), for simultaneously performing linear discriminant analysis and feature selection on high dimensional data. This method combines classical zero-variance discriminant analysis, where discriminant vectors are identified in the null space of the sample within-class covariance matrix, with penalization applied to induce sparse structures in the resulting vectors. To approximately solve the resulting nonconvex problem, we develop a simple algorithm based on the alternating direction method of multipliers. Further, we show that this algorithm is applicable to a larger class of penalized generalized eigenvalue problems, including a particular relaxation of the sparse principal component analysis problem. Finally, we establish theoretical guarantees for convergence of our algorithm to stationary points of the original nonconvex problem, and empirically demonstrate the effectiveness of our heuristic for classifying simulated data and data drawn from applications in time-series classification. version:4
arxiv-1512-02479 | Explaining NonLinear Classification Decisions with Deep Taylor Decomposition | http://arxiv.org/abs/1512.02479 | id:1512.02479 author:Grégoire Montavon, Sebastian Bach, Alexander Binder, Wojciech Samek, Klaus-Robert Müller category:cs.LG stat.ML  published:2015-12-08 summary:Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems, e.g., image classification, natural language processing or human action recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method is based on deep Taylor decomposition and efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets. version:1
arxiv-1511-00561 | SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation | http://arxiv.org/abs/1511.00561 | id:1511.00561 author:Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla category:cs.CV cs.LG cs.NE  published:2015-11-02 summary:We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the fully convolutional network (FCN) architecture and its variants. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. The design of SegNet was primarily motivated by road scene understanding applications. Hence, it is efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than competing architectures and can be trained end-to-end using stochastic gradient descent. We also benchmark the performance of SegNet on Pascal VOC12 salient object segmentation and the recent SUN RGB-D indoor scene understanding challenge. We show that SegNet provides competitive performance although it is significantly smaller than other architectures. We also provide a Caffe implementation of SegNet and a webdemo at http://mi.eng.cam.ac.uk/projects/segnet/ version:2
arxiv-1409-5718 | Convolutional Neural Networks over Tree Structures for Programming Language Processing | http://arxiv.org/abs/1409.5718 | id:1409.5718 author:Lili Mou, Ge Li, Lu Zhang, Tao Wang, Zhi Jin category:cs.LG cs.NE cs.SE  published:2014-09-18 summary:Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the artificial intelligence community. However, different from a natural language sentence, a program contains rich, explicit, and complicated structural information. Hence, traditional NLP models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network (TBCNN) for programming language processing, in which a convolution kernel is designed over programs' abstract syntax trees to capture structural information. TBCNN is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according to functionality, and detecting code snippets of certain patterns. TBCNN outperforms baseline methods, including several neural models for NLP. version:2
arxiv-1512-01613 | A Novel Paradigm for Calculating Ramsey Number via Artificial Bee Colony Algorithm | http://arxiv.org/abs/1512.01613 | id:1512.01613 author:Wei-Hao Mao, Fei Gao, Yi-Jin Dong, Wen-Ming Li category:cs.AI cs.NE math.CO math.OC  published:2015-12-05 summary:The Ramsey number is of vital importance in Ramsey's theorem. This paper proposed a novel methodology for constructing Ramsey graphs about R(3,10), which uses Artificial Bee Colony optimization(ABC) to raise the lower bound of Ramsey number R(3,10). The r(3,10)-graph contains two limitations, that is, neither complete graphs of order 3 nor independent sets of order 10. To resolve these limitations, a special mathematical model is put in the paradigm to convert the problems into discrete optimization whose smaller minimizers are correspondent to bigger lower bound as approximation of inf R(3,10). To demonstrate the potential of the proposed method, simulations are done to to minimize the amount of these two types of graphs. For the first time, four r(3,9,39) graphs with best approximation for inf R(3,10) are reported in simulations to support the current lower bound for R(3,10). The experiments' results show that the proposed paradigm for Ramsey number's calculation driven by ABC is a successful method with the advantages of high precision and robustness. version:2
arxiv-1512-02394 | Online Gradient Descent in Function Space | http://arxiv.org/abs/1512.02394 | id:1512.02394 author:Changbo Zhu, Huan Xu category:cs.LG  published:2015-12-08 summary:In many problems in machine learning and operations research, we need to optimize a function whose input is a random variable or a probability density function, i.e. to solve optimization problems in an in?nite dimensional space. On the other hand, online learning has the advantage of dealing with streaming examples, and better model a changing environ- ment. In this paper, we extend the celebrated online gradient descent algorithm to Hilbert spaces (function spaces), and analyze the convergence guarantee of the algorithm. Finally, we demonstrate that our algorithms can be useful in several important problems. version:1
arxiv-1512-02393 | Online Crowdsourcing | http://arxiv.org/abs/1512.02393 | id:1512.02393 author:Changbo Zhu, Huan Xu, Shuicheng Yan category:cs.LG  published:2015-12-08 summary:With the success of modern internet based platform, such as Amazon Mechanical Turk, it is now normal to collect a large number of hand labeled samples from non-experts. The Dawid- Skene algorithm, which is based on Expectation- Maximization update, has been widely used for inferring the true labels from noisy crowdsourced labels. However, Dawid-Skene scheme requires all the data to perform each EM iteration, and can be infeasible for streaming data or large scale data. In this paper, we provide an online version of Dawid- Skene algorithm that only requires one data frame for each iteration. Further, we prove that under mild conditions, the online Dawid-Skene scheme with projection converges to a stationary point of the marginal log-likelihood of the observed data. Our experiments demonstrate that the online Dawid- Skene scheme achieves state of the art performance comparing with other methods based on the Dawid- Skene scheme. version:1
arxiv-1509-07761 | Sentiment of Emojis | http://arxiv.org/abs/1509.07761 | id:1509.07761 author:Petra Kralj Novak, Jasmina Smailović, Borut Sluban, Igor Mozetič category:cs.CL  published:2015-09-25 summary:There is a new generation of emoticons, called emojis, that is increasingly being used in mobile communications and social media. In the past two years, over ten billion emojis were used on Twitter. Emojis are Unicode graphic symbols, used as a shorthand to express concepts and ideas. In contrast to the small number of well-known emoticons that carry clear emotional contents, there are hundreds of emojis. But what are their emotional contents? We provide the first emoji sentiment lexicon, called the Emoji Sentiment Ranking, and draw a sentiment map of the 751 most frequently used emojis. The sentiment of the emojis is computed from the sentiment of the tweets in which they occur. We engaged 83 human annotators to label over 1.6 million tweets in 13 European languages by the sentiment polarity (negative, neutral, or positive). About 4% of the annotated tweets contain emojis. The sentiment analysis of the emojis allows us to draw several interesting conclusions. It turns out that most of the emojis are positive, especially the most popular ones. The sentiment distribution of the tweets with and without emojis is significantly different. The inter-annotator agreement on the tweets with emojis is higher. Emojis tend to occur at the end of the tweets, and their sentiment polarity increases with the distance. We observe no significant differences in the emoji rankings between the 13 languages and the Emoji Sentiment Ranking. Consequently, we propose our Emoji Sentiment Ranking as a European language-independent resource for automated sentiment analysis. Finally, the paper provides a formalization of sentiment and a novel visualization in the form of a sentiment bar. version:2
arxiv-1508-03422 | Cost Sensitive Learning of Deep Feature Representations from Imbalanced Data | http://arxiv.org/abs/1508.03422 | id:1508.03422 author:Salman H. Khan, Mohammed Bennamoun, Ferdous Sohel, Roberto Togneri category:cs.CV  published:2015-08-14 summary:Class imbalance is a common problem in the case of real-world object detection and classification tasks. Data of some classes is abundant making them an over-represented majority, and data of other classes is scarce, making them an under-represented minority. This imbalance makes it challenging for a classifier to appropriately learn the discriminating boundaries of the majority and minority classes. In this work, we propose a cost sensitive deep neural network which can automatically learn robust feature representations for both the majority and minority classes. During training, our learning procedure jointly optimizes the class dependent costs and the neural network parameters. The proposed approach is applicable to both binary and multi-class problems without any modification. Moreover, as opposed to data level approaches, we do not alter the original data distribution which results in a lower computational cost during the training process. We report the results of our experiments on six major image classification datasets and show that the proposed approach significantly outperforms the baseline algorithms. Comparisons with popular data sampling techniques and cost sensitive classifiers demonstrate the superior performance of our proposed method. version:2
arxiv-1512-02357 | Towards the Application of Linear Programming Methods For Multi-Camera Pose Estimation | http://arxiv.org/abs/1512.02357 | id:1512.02357 author:Masoud Aghamohamadian-Sharbaf, Ahmadreza Heravi, Hamidreza Pourreza category:cs.CV  published:2015-12-08 summary:We presented a separation based optimization algorithm which, rather than optimization the entire variables altogether, This would allow us to employ: 1) a class of nonlinear functions with three variables and 2) a convex quadratic multivariable polynomial, for minimization of reprojection error. Neglecting the inversion required to minimize the nonlinear functions, in this paper we demonstrate how separation allows eradication of matrix inversion. version:1
arxiv-1512-02355 | Is Hamming distance the only way for matching binary image feature descriptors? | http://arxiv.org/abs/1512.02355 | id:1512.02355 author:Erkan Bostanci category:cs.CV  published:2015-12-08 summary:Brute force matching of binary image feature descriptors is conventionally performed using the Hamming distance. This paper assesses the use of alternative metrics in order to see whether they can produce feature correspondences that yield more accurate homography matrices. Two statistical tests, namely ANOVA (Analysis of Variance) and McNemar's test were employed for evaluation. Results show that Jackard-Needham and Dice metrics can display better performance for some descriptors. Yet, these performance differences were not found to be statistically significant. version:1
arxiv-1512-02326 | Learning to Point and Count | http://arxiv.org/abs/1512.02326 | id:1512.02326 author:Jie Shao, Dequan Wang, Xiangyang Xue, Zheng Zhang category:cs.CV  published:2015-12-08 summary:This paper proposes the problem of point-and-count as a test case to break the what-and-where deadlock. Different from the traditional detection problem, the goal is to discover key salient points as a way to localize and count the number of objects simultaneously. We propose two alternatives, one that counts first and then point, and another that works the other way around. Fundamentally, they pivot around whether we solve "what" or "where" first. We evaluate their performance on dataset that contains multiple instances of the same class, demonstrating the potentials and their synergies. The experiences derive a few important insights that explains why this is a much harder problem than classification, including strong data bias and the inability to deal with object scales robustly in state-of-art convolutional neural networks. version:1
arxiv-1512-02311 | Direct Intrinsics: Learning Albedo-Shading Decomposition by Convolutional Regression | http://arxiv.org/abs/1512.02311 | id:1512.02311 author:Takuya Narihira, Michael Maire, Stella X. Yu category:cs.CV  published:2015-12-08 summary:We introduce a new approach to intrinsic image decomposition, the task of decomposing a single image into albedo and shading components. Our strategy, which we term direct intrinsics, is to learn a convolutional neural network (CNN) that directly predicts output albedo and shading channels from an input RGB image patch. Direct intrinsics is a departure from classical techniques for intrinsic image decomposition, which typically rely on physically-motivated priors and graph-based inference algorithms. The large-scale synthetic ground-truth of the MPI Sintel dataset plays a key role in training direct intrinsics. We demonstrate results on both the synthetic images of Sintel and the real images of the classic MIT intrinsic image dataset. On Sintel, direct intrinsics, using only RGB input, outperforms all prior work, including methods that rely on RGB+Depth input. Direct intrinsics also generalizes across modalities; it produces quite reasonable decompositions on the real images of the MIT dataset. Our results indicate that the marriage of CNNs with synthetic training data may be a powerful new technique for tackling classic problems in computer vision. version:1
arxiv-1512-02306 | Nonparametric Reduced-Rank Regression for Multi-SNP, Multi-Trait Association Mapping | http://arxiv.org/abs/1512.02306 | id:1512.02306 author:Ashlee Valente, Geoffrey Ginsburg, Barbara E Engelhardt category:stat.AP q-bio.GN stat.ML  published:2015-12-08 summary:Genome-wide association studies have proven to be essential for understanding the genetic basis of disease. However, many complex traits---personality traits, facial features, disease subtyping---are inherently high-dimensional, impeding simple approaches to association mapping. We developed a nonparametric Bayesian reduced rank regression model for multi-SNP, multi-trait association mapping that does not require the rank of the linear subspace to be specified. We show in simulations and real data that our model shares strength over SNPs and over correlated traits, improving statistical power to identify genetic associations with an interpretable, SNP-supervised low-dimensional linear projection of the high-dimensional phenotype. On the HapMap phase 3 gene expression QTL study data, we identify pleiotropic expression QTLs that classical univariate tests are underpowered to find and that two step approaches cannot recover. Our Python software, BERRRI, is publicly available at GitHub: https://github.com/ashlee1031/BERRRI. version:1
arxiv-1512-02271 | Optimal strategies for the control of autonomous vehicles in data assimilation | http://arxiv.org/abs/1512.02271 | id:1512.02271 author:Damon McDougall, Richard Moore category:math.OC stat.CO stat.ML  published:2015-12-07 summary:We propose a method to compute optimal control paths for autonomous vehicles deployed for the purpose of inferring a velocity field. In addition to being advected by the flow, the vehicles are able to effect a fixed relative speed with arbitrary control over direction. It is this direction that is used as the basis for the locally optimal control algorithm presented here, with objective formed from the variance trace of the expected posterior distribution. We present results for linear flows near hyperbolic fixed points. version:1
arxiv-1509-08379 | Learning FRAME Models Using CNN Filters | http://arxiv.org/abs/1509.08379 | id:1509.08379 author:Yang Lu, Song-Chun Zhu, Ying Nian Wu category:cs.CV  published:2015-09-28 summary:The convolutional neural network (ConvNet or CNN) has proven to be very successful in many tasks such as those in computer vision. In this conceptual paper, we study the generative perspective of the discriminative CNN. In particular, we propose to learn the generative FRAME (Filters, Random field, And Maximum Entropy) model using the highly expressive filters pre-learned by the CNN at the convolutional layers. We show that the learning algorithm can generate realistic and rich object and texture patterns in natural scenes. We explain that each learned model corresponds to a new CNN unit at a layer above the layer of filters employed by the model. We further show that it is possible to learn a new layer of CNN units using a generative CNN model, which is a product of experts model, and the learning algorithm admits an EM interpretation with binary latent variables. version:3
arxiv-1512-02188 | New Design Criteria for Robust PCA and a Compliant Bayesian-Inspired Algorithm | http://arxiv.org/abs/1512.02188 | id:1512.02188 author:Tae-Hyun Oh, David Wipf, Yasuyuki Matsushita, In So Kweon category:cs.CV cs.LG stat.ML  published:2015-12-07 summary:Commonly used in computer vision and other applications, robust PCA represents an algorithmic attempt to reduce the sensitivity of classical PCA to outliers. The basic idea is to learn a decomposition of some data matrix of interest into low rank and sparse components, the latter representing unwanted outliers. Although the resulting optimization problem is typically NP-hard, convex relaxations provide a computationally-expedient alternative with theoretical support. However, in practical regimes performance guarantees break down and a variety of non-convex alternatives, including Bayesian-inspired models, have been proposed to boost estimation quality. Unfortunately though, without additional a priori knowledge none of these methods can significantly expand the critical operational range such that exact principal subspace recovery is possible. Into this mix we propose a novel pseudo-Bayesian algorithm that explicitly compensates for design weaknesses in many existing non-convex approaches leading to state-of-the-art performance with a sound analytical foundation. version:1
arxiv-1512-02181 | The Teaching Dimension of Linear Learners | http://arxiv.org/abs/1512.02181 | id:1512.02181 author:Ji Liu, Xiaojin Zhu category:cs.LG  published:2015-12-07 summary:Teaching dimension is a learning theoretic quantity that specifies the minimum training set size to teach a target model to a learner. Previous studies on teaching dimension focused on version-space learners which maintain all hypotheses consistent with the training data, and cannot be applied to modern machine learners which select a specific hypothesis via optimization. This paper presents the first known teaching dimension for ridge regression, support vector machines, and logistic regression. We also exhibit optimal training sets that match these teaching dimensions. Our approach generalizes to other linear learners. version:1
arxiv-1506-02167 | Color Constancy by Learning to Predict Chromaticity from Luminance | http://arxiv.org/abs/1506.02167 | id:1506.02167 author:Ayan Chakrabarti category:cs.CV  published:2015-06-06 summary:Color constancy is the recovery of true surface color from observed color, and requires estimating the chromaticity of scene illumination to correct for the bias it induces. In this paper, we show that the per-pixel color statistics of natural scenes---without any spatial or semantic context---can by themselves be a powerful cue for color constancy. Specifically, we describe an illuminant estimation method that is built around a "classifier" for identifying the true chromaticity of a pixel given its luminance (absolute brightness across color channels). During inference, each pixel's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants, and applying the classifier over these values yields a distribution over the corresponding illuminants. A global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels. We begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images. These histograms reflect a preference for hues corresponding to smooth reflectance functions, and for achromatic colors in brighter pixels. Despite its simplicity, the resulting estimation algorithm outperforms current state-of-the-art color constancy methods. Next, we propose a method to learn the luminance-to-chromaticity classifier "end-to-end". Using stochastic gradient descent, we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set. This leads to further improvements in accuracy, most significantly in the tail of the error distribution. version:2
arxiv-1506-02515 | Fast ConvNets Using Group-wise Brain Damage | http://arxiv.org/abs/1506.02515 | id:1506.02515 author:Vadim Lebedev, Victor Lempitsky category:cs.CV  published:2015-06-08 summary:We revisit the idea of brain damage, i.e. the pruning of the coefficients of a neural network, and suggest how brain damage can be modified and used to speedup convolutional layers. The approach uses the fact that many efficient implementations reduce generalized convolutions to matrix multiplications. The suggested brain damage process prunes the convolutional kernel tensor in a group-wise fashion by adding group-sparsity regularization to the standard training process. After such group-wise pruning, convolutions can be reduced to multiplications of thinned dense matrices, which leads to speedup. In the comparison on AlexNet, the method achieves very competitive performance. version:2
arxiv-1510-04935 | Holographic Embeddings of Knowledge Graphs | http://arxiv.org/abs/1510.04935 | id:1510.04935 author:Maximilian Nickel, Lorenzo Rosasco, Tomaso Poggio category:cs.AI cs.LG stat.ML I.2.6; I.2.4  published:2015-10-16 summary:Learning embeddings of entities and relations is an efficient and versatile method to perform machine learning on relational data such as knowledge graphs. In this work, we propose holographic embeddings (HolE) to learn compositional vector space representations of entire knowledge graphs. The proposed method is related to holographic models of associative memory in that it employs circular correlation to create compositional representations. By using correlation as the compositional operator HolE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. In extensive experiments we show that holographic embeddings are able to outperform state-of-the-art methods for link prediction in knowledge graphs and relational learning benchmark datasets. version:2
arxiv-1512-02134 | A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation | http://arxiv.org/abs/1512.02134 | id:1512.02134 author:Nikolaus Mayer, Eddy Ilg, Philip Häusser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox category:cs.CV cs.LG stat.ML  published:2015-12-07 summary:Recent work has shown that optical flow estimation can be formulated as a supervised learning task and can be successfully solved with convolutional networks. Training of the so-called FlowNet was enabled by a large synthetically generated dataset. The present paper extends the concept of optical flow estimation via convolutional networks to disparity and scene flow estimation. To this end, we propose three synthetic stereo video datasets with sufficient realism, variation, and size to successfully train large networks. Our datasets are the first large-scale datasets to enable training and evaluating scene flow methods. Besides the datasets, we present a convolutional network for real-time disparity estimation that provides state-of-the-art results. By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network. version:1
arxiv-1512-02110 | In-situ multi-scattering tomography | http://arxiv.org/abs/1512.02110 | id:1512.02110 author:Vadim Holodovsky, Yoav Y. Schechner, Anat Levin, Aviad Levis, Amit Aides category:cs.CV  published:2015-12-07 summary:To recover the three dimensional (3D) volumetric distribution of matter in an object, images of the object are captured from multiple directions and locations. Using these images tomographic computations extract the distribution. In highly scattering media and constrained, natural irradiance, tomography must explicitly account for off-axis scattering. Furthermore, the tomographic model and recovery must function when imaging is done in-situ, as occurs in medical imaging and ground-based atmospheric sensing. We formulate tomography that handles arbitrary orders of scattering, using a monte-carlo model. Moreover, the model is highly parallelizable in our formulation. This enables large scale rendering and recovery of volumetric scenes having a large number of variables. We solve stability and conditioning problems that stem from radiative transfer (RT) modeling in-situ. version:1
arxiv-1512-02100 | Digital Genesis: Computers, Evolution and Artificial Life | http://arxiv.org/abs/1512.02100 | id:1512.02100 author:Tim Taylor, Alan Dorin, Kevin Korb category:cs.NE  published:2015-12-07 summary:The application of evolution in the digital realm, with the goal of creating artificial intelligence and artificial life, has a history as long as that of the digital computer itself. We illustrate the intertwined history of these ideas, starting with the early theoretical work of John von Neumann and the pioneering experimental work of Nils Aall Barricelli. We argue that evolutionary thinking and artificial life will continue to play an integral role in the future development of the digital world. version:1
arxiv-1504-02648 | Time-causal and time-recursive spatio-temporal receptive fields | http://arxiv.org/abs/1504.02648 | id:1504.02648 author:Tony Lindeberg category:cs.CV q-bio.NC  published:2015-04-10 summary:We present an improved model and theory for time-causal and time-recursive spatio-temporal receptive fields, based on a combination of Gaussian receptive fields over the spatial domain and first-order integrators or equivalently truncated exponential filters coupled in cascade over the temporal domain. Compared to previous spatio-temporal scale-space formulations in terms of non-enhancement of local extrema or scale invariance, these receptive fields are based on different scale-space axiomatics over time by ensuring non-creation of new local extrema or zero-crossings with increasing temporal scale. Specifically, extensions are presented about (i) parameterizing the intermediate temporal scale levels, (ii) analysing the resulting temporal dynamics, (iii) transferring the theory to a discrete implementation, (iv) computing scale-normalized spatio-temporal derivative expressions for spatio-temporal feature detection and (v) computational modelling of receptive fields in the lateral geniculate nucleus (LGN) and the primary visual cortex (V1) in biological vision. We show that by distributing the intermediate temporal scale levels according to a logarithmic distribution, we obtain much faster temporal response properties (shorter temporal delays) compared to a uniform distribution. Specifically, these kernels converge very rapidly to a limit kernel possessing true self-similar scale-invariant properties over temporal scales, thereby allowing for true scale invariance over variations in the temporal scale, although the underlying temporal scale-space representation is based on a discretized temporal scale parameter. We show how scale-normalized temporal derivatives can be defined for these time-causal scale-space kernels and how the composed theory can be used for computing basic types of scale-normalized spatio-temporal derivative expressions in a computationally efficient manner. version:2
arxiv-1512-02097 | Clustering by Deep Nearest Neighbor Descent (D-NND): A Density-based Parameter-Insensitive Clustering Method | http://arxiv.org/abs/1512.02097 | id:1512.02097 author:Teng Qiu, Yongjie Li category:stat.ML cs.CV cs.LG stat.CO stat.ME  published:2015-12-07 summary:Most density-based clustering methods largely rely on how well the underlying density is estimated. However, density estimation itself is also a challenging problem, especially the determination of the kernel bandwidth. A large bandwidth could lead to the over-smoothed density estimation in which the number of density peaks could be less than the true clusters, while a small bandwidth could lead to the under-smoothed density estimation in which spurious density peaks, or called the "ripple noise", would be generated in the estimated density. In this paper, we propose a density-based hierarchical clustering method, called the Deep Nearest Neighbor Descent (D-NND), which could learn the underlying density structure layer by layer and capture the cluster structure at the same time. The over-smoothed density estimation could be largely avoided and the negative effect of the under-estimated cases could be also largely reduced. Overall, D-NND presents not only the strong capability of discovering the underlying cluster structure but also the remarkable reliability due to its insensitivity to parameters. version:1
arxiv-1512-02072 | On The Continuous Steering of the Scale of Tight Wavelet Frames | http://arxiv.org/abs/1512.02072 | id:1512.02072 author:Zsuzsanna Püspöki, John Paul Ward, Daniel Sage, Michael Unser category:cs.CV  published:2015-12-07 summary:In analogy with steerable wavelets, we present a general construction of adaptable tight wavelet frames, with an emphasis on scaling operations. In particular, the derived wavelets can be "dilated" by a procedure comparable to the operation of steering steerable wavelets. The fundamental aspects of the construction are the same: an admissible collection of Fourier multipliers is used to extend a tight wavelet frame, and the "scale" of the wavelets is adapted by scaling the multipliers. As an application, the proposed wavelets can be used to improve the frequency localization. Importantly, the localized frequency bands specified by this construction can be scaled efficiently using matrix multiplication. version:1
arxiv-1511-03650 | Piecewise Linear Activation Functions For More Efficient Deep Networks | http://arxiv.org/abs/1511.03650 | id:1511.03650 author:Cheng-Yang Fu, Alexander C. Berg category:cs.CV  published:2015-11-11 summary:This submission has been withdrawn by arXiv administrators because it is intentionally incomplete, which is in violation of our policies. version:3
arxiv-1509-06004 | A Parallel Framework for Parametric Maximum Flow Problems in Image Segmentation | http://arxiv.org/abs/1509.06004 | id:1509.06004 author:Vlad Olaru, Mihai Florea, Cristian Sminchisescu category:cs.CV  published:2015-09-20 summary:This paper presents a framework that supports the implementation of parallel solutions for the widespread parametric maximum flow computational routines used in image segmentation algorithms. The framework is based on supergraphs, a special construction combining several image graphs into a larger one, and works on various architectures (multi-core or GPU), either locally or remotely in a cluster of computing nodes. The framework can also be used for performance evaluation of parallel implementations of maximum flow algorithms. We present the case study of a state-of-the-art image segmentation algorithm based on graph cuts, Constrained Parametric Min-Cut (CPMC), that uses the parallel framework to solve parametric maximum flow problems, based on a GPU implementation of the well-known push-relabel algorithm. Our results indicate that real-time implementations based on the proposed techniques are possible. version:2
arxiv-1210-2179 | Fast Online EM for Big Topic Modeling | http://arxiv.org/abs/1210.2179 | id:1210.2179 author:Jia Zeng, Zhi-Qiang Liu, Xiao-Qin Cao category:cs.LG  published:2012-10-08 summary:The expectation-maximization (EM) algorithm can compute the maximum-likelihood (ML) or maximum a posterior (MAP) point estimate of the mixture models or latent variable models such as latent Dirichlet allocation (LDA), which has been one of the most popular probabilistic topic modeling methods in the past decade. However, batch EM has high time and space complexities to learn big LDA models from big data streams. In this paper, we present a fast online EM (FOEM) algorithm that infers the topic distribution from the previously unseen documents incrementally with constant memory requirements. Within the stochastic approximation framework, we show that FOEM can converge to the local stationary point of the LDA's likelihood function. By dynamic scheduling for the fast speed and parameter streaming for the low memory usage, FOEM is more efficient for some lifelong topic modeling tasks than the state-of-the-art online LDA algorithms to handle both big data and big models (aka, big topic modeling) on just a PC. version:3
arxiv-1512-02016 | Discriminative Nonparametric Latent Feature Relational Models with Data Augmentation | http://arxiv.org/abs/1512.02016 | id:1512.02016 author:Bei Chen, Ning Chen, Jun Zhu, Jiaming Song, Bo Zhang category:cs.LG stat.ML  published:2015-12-07 summary:We present a discriminative nonparametric latent feature relational model (LFRM) for link prediction to automatically infer the dimensionality of latent features. Under the generic RegBayes (regularized Bayesian inference) framework, we handily incorporate the prediction loss with probabilistic inference of a Bayesian model; set distinct regularization parameters for different types of links to handle the imbalance issue in real networks; and unify the analysis of both the smooth logistic log-loss and the piecewise linear hinge loss. For the nonconjugate posterior inference, we present a simple Gibbs sampler via data augmentation, without making restricting assumptions as done in variational methods. We further develop an approximate sampler using stochastic gradient Langevin dynamics to handle large networks with hundreds of thousands of entities and millions of links, orders of magnitude larger than what existing LFRM models can process. Extensive studies on various real networks show promising performance. version:1
arxiv-1512-02013 | Scalable domain adaptation of convolutional neural networks | http://arxiv.org/abs/1512.02013 | id:1512.02013 author:Adrian Popescu, Etienne Gadeski, Hervé Le Borgne category:cs.CV  published:2015-12-07 summary:Convolutional neural networks (CNNs) tend to become a standard approach to solve a wide array of computer vision problems. Besides important theoretical and practical advances in their design, their success is built on the existence of manually labeled visual resources, such as ImageNet. The creation of such datasets is cumbersome and here we focus on alternatives to manual labeling. We hypothesize that new resources are of uttermost importance in domains which are not or weakly covered by ImageNet, such as tourism photographs. We first collect noisy Flickr images for tourist points of interest and apply automatic or weakly-supervised reranking techniques to reduce noise. Then, we learn domain adapted models with a standard CNN architecture and compare them to a generic model obtained from ImageNet. Experimental validation is conducted with publicly available datasets, including Oxford5k, INRIA Holidays and Div150Cred. Results show that low-cost domain adaptation improves results compared to the use of generic models but also compared to strong non-CNN baselines such as triangulation embedding. version:1
arxiv-1512-02009 | Jointly Modeling Topics and Intents with Global Order Structure | http://arxiv.org/abs/1512.02009 | id:1512.02009 author:Bei Chen, Jun Zhu, Nan Yang, Tian Tian, Ming Zhou, Bo Zhang category:cs.CL cs.IR cs.LG  published:2015-12-07 summary:Modeling document structure is of great importance for discourse analysis and related applications. The goal of this research is to capture the document intent structure by modeling documents as a mixture of topic words and rhetorical words. While the topics are relatively unchanged through one document, the rhetorical functions of sentences usually change following certain orders in discourse. We propose GMM-LDA, a topic modeling based Bayesian unsupervised model, to analyze the document intent structure cooperated with order information. Our model is flexible that has the ability to combine the annotations and do supervised learning. Additionally, entropic regularization can be introduced to model the significant divergence between topics and intents. We perform experiments in both unsupervised and supervised settings, results show the superiority of our model over several state-of-the-art baselines. version:1
arxiv-1512-01993 | A Novel Approach to Distributed Multi-Class SVM | http://arxiv.org/abs/1512.01993 | id:1512.01993 author:Aruna Govada, Shree Ranjani, Aditi Viswanathan, S. K. Sahay category:cs.LG cs.DC  published:2015-12-07 summary:With data sizes constantly expanding, and with classical machine learning algorithms that analyze such data requiring larger and larger amounts of computation time and storage space, the need to distribute computation and memory requirements among several computers has become apparent. Although substantial work has been done in developing distributed binary SVM algorithms and multi-class SVM algorithms individually, the field of multi-class distributed SVMs remains largely unexplored. This research proposes a novel algorithm that implements the Support Vector Machine over a multi-class dataset and is efficient in a distributed environment (here, Hadoop). The idea is to divide the dataset into half recursively and thus compute the optimal Support Vector Machine for this half during the training phase, much like a divide and conquer approach. While testing, this structure has been effectively exploited to significantly reduce the prediction time. Our algorithm has shown better computation time during the prediction phase than the traditional sequential SVM methods (One vs. One, One vs. Rest) and out-performs them as the size of the dataset grows. This approach also classifies the data with higher accuracy than the traditional multi-class algorithms. version:1
arxiv-1512-01979 | Hyperspectral Chemical Plume Detection Algorithms Based On Multidimensional Iterative Filtering Decomposition | http://arxiv.org/abs/1512.01979 | id:1512.01979 author:Antonio Cicone, Jingfang Liu, Haomin Zhou category:math.NA cs.CV  published:2015-12-07 summary:Chemicals released in the air can be extremely dangerous for human beings and the environment. Hyperspectral images can be used to identify chemical plumes, however the task can be extremely challenging. Assuming we know a priori that some chemical plume, with a known frequency spectrum, has been photographed using a hyperspectral sensor, we can use standard techniques like the so called matched filter or adaptive cosine estimator, plus a properly chosen threshold value, to identify the position of the chemical plume. However, due to noise and sensors fault, the accurate identification of chemical pixels is not easy even in this apparently simple situation. In this paper we present a post-processing tool that, in a completely adaptive and data driven fashion, allows to improve the performance of any classification methods in identifying the boundaries of a plume. This is done using the Multidimensional Iterative Filtering (MIF) algorithm (arXiv:1411.6051, arXiv:1507.07173), which is a non-stationary signal decomposition method like the pioneering Empirical Mode Decomposition (EMD) method. Moreover, based on the MIF technique, we propose also a pre-processing method that allows to decorrelate and mean-center a hyperspectral dataset. The Cosine Similarity measure, which often fails in practice, appears to become a successful and outperforming classifier when equipped with such pre-processing method. We show some examples of the proposed methods when applied to real life problems. version:1
arxiv-1508-03590 | Light-field Microscopy with a Consumer Light-field Camera | http://arxiv.org/abs/1508.03590 | id:1508.03590 author:Lois Mignard-Debise, Ivo Ihrke category:cs.GR cs.CV  published:2015-05-04 summary:We explore the use of inexpensive consumer light- field camera technology for the purpose of light-field mi- croscopy. Our experiments are based on the Lytro (first gen- eration) camera. Unfortunately, the optical systems of the Lytro and those of microscopes are not compatible, lead- ing to a loss of light-field information due to angular and spatial vignetting when directly recording microscopic pic- tures. We therefore consider an adaptation of the Lytro op- tical system. We demonstrate that using the Lytro directly as an oc- ular replacement, leads to unacceptable spatial vignetting. However, we also found a setting that allows the use of the Lytro camera in a virtual imaging mode which prevents the information loss to a large extent. We analyze the new vir- tual imaging mode and use it in two different setups for im- plementing light-field microscopy using a Lytro camera. As a practical result, we show that the camera can be used for low magnification work, as e.g. common in quality control, surface characterization, etc. We achieve a maximum spa- tial resolution of about 6.25{\mu}m, albeit at a limited SNR for the side views. version:2
arxiv-1512-01947 | Learning population and subject-specific brain connectivity networks via Mixed Neighborhood Selection | http://arxiv.org/abs/1512.01947 | id:1512.01947 author:Ricardo Pio Monti, Christoforos Anagnostopoulos, Giovanni Montana category:stat.ML  published:2015-12-07 summary:In neuroimaging data analysis, Gaussian graphical models are often used to model statistical dependencies across spatially remote brain regions known as functional connectivity. Typically, data is collected across a cohort of subjects and the scientific objectives consist of estimating population and subject-specific graphical models. A third objective that is often overlooked involves quantifying inter-subject variability and thus identifying regions or sub-networks that demonstrate heterogeneity across subjects. Such information is fundamental in order to thoroughly understand the human connectome. We propose Mixed Neighborhood Selection in order to simultaneously address the three aforementioned objectives. By recasting covariance selection as a neighborhood selection problem we are able to efficiently learn the topology of each node. We introduce an additional mixed effect component to neighborhood selection in order to simultaneously estimate a graphical model for the population of subjects as well as for each individual subject. The proposed method is validated empirically through a series of simulations and applied to resting state data for healthy subjects taken from the ABIDE consortium. version:1
arxiv-1512-01927 | Fast Optimization Algorithm on Riemannian Manifolds and Its Application in Low-Rank Representation | http://arxiv.org/abs/1512.01927 | id:1512.01927 author:Haoran Chen, Yanfeng Sun, Junbin Gao, Yongli Hu category:cs.NA cs.CV cs.LG  published:2015-12-07 summary:The paper addresses the problem of optimizing a class of composite functions on Riemannian manifolds and a new first order optimization algorithm (FOA) with a fast convergence rate is proposed. Through the theoretical analysis for FOA, it has been proved that the algorithm has quadratic convergence. The experiments in the matrix completion task show that FOA has better performance than other first order optimization methods on Riemannian manifolds. A fast subspace pursuit method based on FOA is proposed to solve the low-rank representation model based on augmented Lagrange method on the low rank matrix variety. Experimental results on synthetic and real data sets are presented to demonstrate that both FOA and SP-RPRG(ALM) can achieve superior performance in terms of faster convergence and higher accuracy. version:1
arxiv-1512-01926 | Thinking Required | http://arxiv.org/abs/1512.01926 | id:1512.01926 author:Kamil Rocki category:cs.LG cs.AI cs.CL  published:2015-12-07 summary:There exists a theory of a single general-purpose learning algorithm which could explain the principles its operation. It assumes the initial rough architecture, a small library of simple innate circuits which are prewired at birth. and proposes that all significant mental algorithms are learned. Given current understanding and observations, this paper reviews and lists the ingredients of such an algorithm from architectural and functional perspectives. version:1
arxiv-1512-01914 | Rademacher Complexity of the Restricted Boltzmann Machine | http://arxiv.org/abs/1512.01914 | id:1512.01914 author:Xiao Zhang category:cs.LG  published:2015-12-07 summary:Boltzmann machine, as a fundamental construction block of deep belief network and deep Boltzmann machines, is widely used in deep learning community and great success has been achieved. However, theoretical understanding of many aspects of it is still far from clear. In this paper, we studied the Rademacher complexity of both the asymptotic restricted Boltzmann machine and the practical implementation with single-step contrastive divergence (CD-1) procedure. Our results disclose the fact that practical implementation training procedure indeed increased the Rademacher complexity of restricted Boltzmann machines. A further research direction might be the investigation of the VC dimension of a compositional function used in the CD-1 procedure. version:1
arxiv-1512-01891 | Sparsifying Neural Network Connections for Face Recognition | http://arxiv.org/abs/1512.01891 | id:1512.01891 author:Yi Sun, Xiaogang Wang, Xiaoou Tang category:cs.CV  published:2015-12-07 summary:This paper proposes to learn high-performance deep ConvNets with sparse neural connections, referred to as sparse ConvNets, for face recognition. The sparse ConvNets are learned in an iterative way, each time one additional layer is sparsified and the entire model is re-trained given the initial weights learned in previous iterations. One important finding is that directly training the sparse ConvNet from scratch failed to find good solutions for face recognition, while using a previously learned denser model to properly initialize a sparser model is critical to continue learning effective features for face recognition. This paper also proposes a new neural correlation-based weight selection criterion and empirically verifies its effectiveness in selecting informative connections from previously learned models in each iteration. When taking a moderately sparse structure (26%-76% of weights in the dense model), the proposed sparse ConvNet model significantly improves the face recognition performance of the previous state-of-the-art DeepID2+ models given the same training data, while it keeps the performance of the baseline model with only 12% of the original parameters. version:1
arxiv-1601-02680 | Using SVM to pre-classify government purchases | http://arxiv.org/abs/1601.02680 | id:1601.02680 author:Thiago Marzagão category:cs.LG  published:2015-12-07 summary:The Brazilian government often misclassifies the goods it buys. That makes it hard to audit government expenditures. We cannot know whether the price paid for a ballpoint pen (code #7510) was reasonable if the pen was misclassified as a technical drawing pen (code #6675) or as any other good. This paper shows how we can use machine learning to reduce misclassification. I trained a support vector machine (SVM) classifier that takes a product description as input and returns the most likely category codes as output. I trained the classifier using 20 million goods purchased by the Brazilian government between 1999-04-01 and 2015-04-02. In 83.3% of the cases the correct category code was one of the three most likely category codes identified by the classifier. I used the trained classifier to develop a web app that might help the government reduce misclassification. I open sourced the code on GitHub; anyone can use and modify it. version:1
arxiv-1512-01858 | Fixation prediction with a combined model of bottom-up saliency and vanishing point | http://arxiv.org/abs/1512.01858 | id:1512.01858 author:Mengyang Feng, Ali Borji, Huchuan Lu category:cs.CV  published:2015-12-06 summary:By predicting where humans look in natural scenes, we can understand how they perceive complex natural scenes and prioritize information for further high-level visual processing. Several models have been proposed for this purpose, yet there is a gap between best existing saliency models and human performance. While many researchers have developed purely computational models for fixation prediction, less attempts have been made to discover cognitive factors that guide gaze. Here, we study the effect of a particular type of scene structural information, known as the vanishing point, and show that human gaze is attracted to the vanishing point regions. We record eye movements of 10 observers over 532 images, out of which 319 have vanishing points. We then construct a combined model of traditional saliency and a vanishing point channel and show that our model outperforms state of the art saliency models using three scores on our dataset. version:1
arxiv-1512-01845 | Explaining reviews and ratings with PACO: Poisson Additive Co-Clustering | http://arxiv.org/abs/1512.01845 | id:1512.01845 author:Chao-Yuan Wu, Alex Beutel, Amr Ahmed, Alexander J. Smola category:cs.LG stat.ML  published:2015-12-06 summary:Understanding a user's motivations provides valuable information beyond the ability to recommend items. Quite often this can be accomplished by perusing both ratings and review texts, since it is the latter where the reasoning for specific preferences is explicitly expressed. Unfortunately matrix factorization approaches to recommendation result in large, complex models that are difficult to interpret and give recommendations that are hard to clearly explain to users. In contrast, in this paper, we attack this problem through succinct additive co-clustering. We devise a novel Bayesian technique for summing co-clusterings of Poisson distributions. With this novel technique we propose a new Bayesian model for joint collaborative filtering of ratings and text reviews through a sum of simple co-clusterings. The simple structure of our model yields easily interpretable recommendations. Even with a simple, succinct structure, our model outperforms competitors in terms of predicting ratings with reviews. version:1
arxiv-1506-02681 | Frank-Wolfe Bayesian Quadrature: Probabilistic Integration with Theoretical Guarantees | http://arxiv.org/abs/1506.02681 | id:1506.02681 author:François-Xavier Briol, Chris J. Oates, Mark Girolami, Michael A. Osborne category:stat.ML  published:2015-06-08 summary:There is renewed interest in formulating integration as an inference problem, motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation. Current methods, such as Bayesian Quadrature, demonstrate impressive empirical performance but lack theoretical analysis. An important challenge is to reconcile these probabilistic integrators with rigorous convergence guarantees. In this paper, we present the first probabilistic integrator that admits such theoretical treatment, called Frank-Wolfe Bayesian Quadrature (FWBQ). Under FWBQ, convergence to the true value of the integral is shown to be exponential and posterior contraction rates are proven to be superexponential. In simulations, FWBQ is competitive with state-of-the-art methods and out-performs alternatives based on Frank-Wolfe optimisation. Our approach is applied to successfully quantify numerical error in the solution to a challenging model choice problem in cellular biology. version:3
arxiv-1512-01834 | Classification of Manifolds by Single-Layer Neural Networks | http://arxiv.org/abs/1512.01834 | id:1512.01834 author:SueYeon Chung, Daniel D. Lee, Haim Sompolinsky category:cond-mat.dis-nn cond-mat.stat-mech cs.NE q-bio.NC stat.ML  published:2015-12-06 summary:The neuronal representation of objects exhibit enormous variability due to changes in the object's physical features such as location, size, orientation, and intensity. How the brain copes with the variability across these manifolds of neuronal states and generates invariant perception of objects remains poorly understood. Here we present a theory of neuronal classification of manifolds, extending Gardner's replica theory of classification of isolated points by a single layer perceptron. We evaluate how the perceptron capacity depends on the dimensionality, size and shape of the classified manifolds. version:1
arxiv-1412-1523 | Information Exchange and Learning Dynamics over Weakly-Connected Adaptive Networks | http://arxiv.org/abs/1412.1523 | id:1412.1523 author:Bicheng Ying, Ali H. Sayed category:cs.MA cs.IT cs.LG math.IT  published:2014-12-04 summary:The paper examines the learning mechanism of adaptive agents over weakly-connected graphs and reveals an interesting behavior on how information flows through such topologies. The results clarify how asymmetries in the exchange of data can mask local information at certain agents and make them totally dependent on other agents. A leader-follower relationship develops with the performance of some agents being fully determined by the performance of other agents that are outside their domain of influence. This scenario can arise, for example, due to intruder attacks by malicious agents or as the result of failures by some critical links. The findings in this work help explain why strong-connectivity of the network topology, adaptation of the combination weights, and clustering of agents are important ingredients to equalize the learning abilities of all agents against such disturbances. The results also clarify how weak-connectivity can be helpful in reducing the effect of outlier data on learning performance. version:2
arxiv-1509-07107 | On The Direct Maximization of Quadratic Weighted Kappa | http://arxiv.org/abs/1509.07107 | id:1509.07107 author:David Vaughn, Derek Justice category:cs.LG  published:2015-09-23 summary:In recent years, quadratic weighted kappa has been growing in popularity in the machine learning community as an evaluation metric in domains where the target labels to be predicted are drawn from integer ratings, usually obtained from human experts. For example, it was the metric of choice in several recent, high profile machine learning contests hosted on Kaggle : https://www.kaggle.com/c/asap-aes , https://www.kaggle.com/c/asap-sas , https://www.kaggle.com/c/diabetic-retinopathy-detection . Yet, little is understood about the nature of this metric, its underlying mathematical properties, where it fits among other common evaluation metrics such as mean squared error (MSE) and correlation, or if it can be optimized analytically, and if so, how. Much of this is due to the cumbersome way that this metric is commonly defined. In this paper we first derive an equivalent but much simpler, and more useful, definition for quadratic weighted kappa, and then employ this alternate form to address the above issues. version:3
arxiv-1512-01789 | The Next Best Underwater View | http://arxiv.org/abs/1512.01789 | id:1512.01789 author:Mark Sheinin, Yoav Y. Schechner category:cs.CV  published:2015-12-06 summary:To image in high resolution large and occlusion-prone scenes, a camera must move above and around. Degradation of visibility due to geometric occlusions and distances is exacerbated by scattering, when the scene is in a participating medium. Moreover, underwater and in other media, artificial lighting is needed. Overall, data quality depends on the observed surface, medium and the time-varying poses of the camera and light source. This work proposes to optimize camera/light poses as they move, so that the surface is scanned efficiently and the descattered recovery has the highest quality. The work generalizes the next best view concept of robot vision to scattering media and cooperative movable lighting. It also extends descattering to platforms that move optimally. The optimization criterion is information gain, taken from information theory. We exploit the existence of a prior rough 3D model, since underwater such a model is routinely obtained using sonar. We demonstrate this principle in a scaled-down setup. version:1
arxiv-1512-01774 | Image reconstruction from dense binary pixels | http://arxiv.org/abs/1512.01774 | id:1512.01774 author:Or Litany, Tal Remez, Alex Bronstein category:cs.CV  published:2015-12-06 summary:Recently, the dense binary pixel Gigavision camera had been introduced, emulating a digital version of the photographic film. While seems to be a promising solution for HDR imaging, its output is not directly usable and requires an image reconstruction process. In this work, we formulate this problem as the minimization of a convex objective combining a maximum-likelihood term with a sparse synthesis prior. We present MLNet - a novel feed-forward neural network, producing acceptable output quality at a fixed complexity and is two orders of magnitude faster than iterative algorithms. We present state of the art results in the abstract. version:1
arxiv-1512-01768 | Want Answers? A Reddit Inspired Study on How to Pose Questions | http://arxiv.org/abs/1512.01768 | id:1512.01768 author:Danish, Yogesh Dahiya, Partha Talukdar category:cs.CL  published:2015-12-06 summary:Questions form an integral part of our everyday communication, both offline and online. Getting responses to our questions from others is fundamental to satisfying our information need and in extending our knowledge boundaries. A question may be represented using various factors such as social, syntactic, semantic, etc. We hypothesize that these factors contribute with varying degrees towards getting responses from others for a given question. We perform a thorough empirical study to measure effects of these factors using a novel question and answer dataset from the website Reddit.com. To the best of our knowledge, this is the first such analysis of its kind on this important topic. We also use a sparse nonnegative matrix factorization technique to automatically induce interpretable semantic factors from the question dataset. We also document various patterns on response prediction we observe during our analysis in the data. For instance, we found that preference-probing questions are scantily answered. Our method is robust to capture such latent response factors. We hope to make our code and datasets publicly available upon publication of the paper. version:1
arxiv-1508-00106 | Separated by an Un-common Language: Towards Judgment Language Informed Vector Space Modeling | http://arxiv.org/abs/1508.00106 | id:1508.00106 author:Ira Leviant, Roi Reichart category:cs.CL  published:2015-08-01 summary:A common evaluation practice in the vector space models (VSMs) literature is to measure the models' ability to predict human judgments about lexical semantic relations between word pairs. Most existing evaluation sets, however, consist of scores collected for English word pairs only, ignoring the potential impact of the judgment language in which word pairs are presented on the human scores. In this paper we translate two prominent evaluation sets, wordsim353 (association) and SimLex999 (similarity), from English to Italian, German and Russian and collect scores for each dataset from crowdworkers fluent in its language. Our analysis reveals that human judgments are strongly impacted by the judgment language. Moreover, we show that the predictions of monolingual VSMs do not necessarily best correlate with human judgments made with the language used for model training, suggesting that models and humans are affected differently by the language they use when making semantic judgments. Finally, we show that in a large number of setups, multilingual VSM combination results in improved correlations with human judgments, suggesting that multilingualism may partially compensate for the judgment language effect on human judgments. version:5
arxiv-1402-5715 | Variational Particle Approximations | http://arxiv.org/abs/1402.5715 | id:1402.5715 author:Ardavan Saeedi, Tejas D Kulkarni, Vikash Mansinghka, Samuel Gershman category:stat.ML cs.LG  published:2014-02-24 summary:Approximate inference in high-dimensional, discrete probabilistic models is a central problem in computational statistics and machine learning. This paper describes discrete particle variational inference (DPVI), a new approach that combines key strengths of Monte Carlo, variational and search-based techniques. DPVI is based on a novel family of particle-based variational approximations that can be fit using simple, fast, deterministic search techniques. Like Monte Carlo, DPVI can handle multiple modes, and yields exact results in a well-defined limit. Like unstructured mean-field, DPVI is based on optimizing a lower bound on the partition function; when this quantity is not of intrinsic interest, it facilitates convergence assessment and debugging. Like both Monte Carlo and combinatorial search, DPVI can take advantage of factorization, sequential structure, and custom search operators. This paper defines DPVI particle-based approximation family and partition function lower bounds, along with the sequential DPVI and local DPVI algorithm templates for optimizing them. DPVI is illustrated and evaluated via experiments on lattice Markov Random Fields, nonparametric Bayesian mixtures and block-models, and parametric as well as non-parametric hidden Markov models. Results include applications to real-world spike-sorting and relational modeling problems, and show that DPVI can offer appealing time/accuracy trade-offs as compared to multiple alternatives. version:3
arxiv-1512-01728 | Similarity Learning via Adaptive Regression and Its Application to Image Retrieval | http://arxiv.org/abs/1512.01728 | id:1512.01728 author:Qi Qian, Inci M. Baytas, Rong Jin, Anil Jain, Shenghuo Zhu category:cs.LG  published:2015-12-06 summary:We study the problem of similarity learning and its application to image retrieval with large-scale data. The similarity between pairs of images can be measured by the distances between their high dimensional representations, and the problem of learning the appropriate similarity is often addressed by distance metric learning. However, distance metric learning requires the learned metric to be a PSD matrix, which is computational expensive and not necessary for retrieval ranking problem. On the other hand, the bilinear model is shown to be more flexible for large-scale image retrieval task, hence, we adopt it to learn a matrix for estimating pairwise similarities under the regression framework. By adaptively updating the target matrix in regression, we can mimic the hinge loss, which is more appropriate for similarity learning problem. Although the regression problem can have the closed-form solution, the computational cost can be very expensive. The computational challenges come from two aspects: the number of images can be very large and image features have high dimensionality. We address the first challenge by compressing the data by a randomized algorithm with the theoretical guarantee. For the high dimensional issue, we address it by taking low rank assumption and applying alternating method to obtain the partial matrix, which has a global optimal solution. Empirical studies on real world image datasets (i.e., Caltech and ImageNet) demonstrate the effectiveness and efficiency of the proposed method. version:1
arxiv-1512-01712 | Generating News Headlines with Recurrent Neural Networks | http://arxiv.org/abs/1512.01712 | id:1512.01712 author:Konstantin Lopyrev category:cs.CL cs.LG cs.NE  published:2015-12-05 summary:We describe an application of an encoder-decoder recurrent neural network with LSTM units and attention to generating headlines from the text of news articles. We find that the model is quite effective at concisely paraphrasing news articles. Furthermore, we study how the neural network decides which input words to pay attention to, and specifically we identify the function of the different neurons in a simplified attention mechanism. Interestingly, our simplified attention mechanism performs better that the more complex attention mechanism on a held out set of articles. version:1
arxiv-1502-06557 | Iteratively reweighted adaptive lasso for conditional heteroscedastic time series with applications to AR-ARCH type processes | http://arxiv.org/abs/1502.06557 | id:1502.06557 author:Florian Ziel category:stat.ME q-fin.CP stat.AP stat.CO stat.ML  published:2015-02-23 summary:Shrinkage algorithms are of great importance in almost every area of statistics due to the increasing impact of big data. Especially time series analysis benefits from efficient and rapid estimation techniques such as the lasso. However, currently lasso type estimators for autoregressive time series models still focus on models with homoscedastic residuals. Therefore, an iteratively reweighted adaptive lasso algorithm for the estimation of time series models under conditional heteroscedasticity is presented in a high-dimensional setting. The asymptotic behaviour of the resulting estimator is analysed. It is found that the proposed estimation procedure performs substantially better than its homoscedastic counterpart. A special case of the algorithm is suitable to compute the estimated multivariate AR-ARCH type models efficiently. Extensions to the model like periodic AR-ARCH, threshold AR-ARCH or ARMA-GARCH are discussed. Finally, different simulation results and applications to electricity market data and returns of metal prices are shown. version:2
arxiv-1512-01708 | Variance Reduction for Distributed Stochastic Gradient Descent | http://arxiv.org/abs/1512.01708 | id:1512.01708 author:Soham De, Gavin Taylor, Tom Goldstein category:cs.LG cs.DC math.OC stat.ML  published:2015-12-05 summary:Variance reduction (VR) methods boost the performance of stochastic gradient descent (SGD) by enabling the use of larger stepsizes and preserving linear convergence rates. However, current variance reduced SGD methods require either high memory usage or require a full pass over the (large) data set at the end of each epoch to calculate the exact gradient of the objective function. This makes current VR methods impractical in distributed or parallel settings. In this paper, we propose a variance reduction method, called VR-lite, that does not require full gradient computations or extra storage. We explore distributed synchronous and asynchronous variants with both high and low communication latency. We find that our distributed algorithms scale linearly with the number of local workers and remain stable even with low communication frequency. We empirically compare both the sequential and distributed algorithms to state-of-the-art stochastic optimization methods, and find that our proposed algorithms consistently converge faster than other stochastic methods. version:1
arxiv-1501-05108 | BDgraph: An R Package for Bayesian Structure Learning in Graphical Models | http://arxiv.org/abs/1501.05108 | id:1501.05108 author:Abdolreza Mohammadi, Ernst C. Wit category:stat.ML  published:2015-01-21 summary:Graphical models provide powerful tools to uncover complicated patterns in multivariate data and are commonly used in Bayesian statistics and machine learning. In this paper, we introduce an R package BDgraph which performs Bayesian structure learning for general undirected graphical models with either continuous or discrete variables. The package efficiently implements recent improvements in the Bayesian literature. To speed up computations, the computationally intensive tasks have been implemented in C++ and interfaced with R. In addition, the package contains several functions for simulation and visualization, as well as two multivariate datasets taken from the literature and are used to describe the package capabilities. The paper includes a brief overview of the statistical methods which have been implemented in the package. The main body of the paper explains how to use the package. Furthermore, we illustrate the package's functionality in both real and artificial examples, as well as in an extensive simulation study. version:4
arxiv-1507-05444 | Canonical Correlation Forests | http://arxiv.org/abs/1507.05444 | id:1507.05444 author:Tom Rainforth, Frank Wood category:stat.ML cs.LG  published:2015-07-20 summary:We introduce canonical correlation forests (CCFs), a new decision tree ensemble method for classification. Individual canonical correlation trees are binary decision trees with hyperplane splits based on canonical correlation components. Unlike axis-aligned alternatives, the decision surfaces of CCFs are not restricted to the coordinate system of the input features and therefore more naturally represent data with correlation between the features. Additionally we introduce a novel alternative to bagging, the projection bootstrap, which maintains use of the full dataset in selecting split points. CCFs do not require parameter tuning and our experiments show that they out-perform axis-aligned random forests, other state-of-the-art tree ensemble methods and all of the 179 popular classifiers considered in a recent extensive survey. version:5
arxiv-1512-01693 | Deep Attention Recurrent Q-Network | http://arxiv.org/abs/1512.01693 | id:1512.01693 author:Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov, Anastasiia Ignateva category:cs.LG  published:2015-12-05 summary:A deep learning approach to reinforcement learning led to a general learner able to train on visual input to play a variety of arcade games at the human and superhuman levels. Its creators at the Google DeepMind's team called the approach: Deep Q-Network (DQN). We present an extension of DQN by "soft" and "hard" attention mechanisms. Tests of the proposed Deep Attention Recurrent Q-Network (DARQN) algorithm on multiple Atari 2600 games show level of performance superior to that of DQN. Moreover, built-in attention mechanisms allow a direct online monitoring of the training process by highlighting the regions of the game screen the agent is focusing on when making decisions. version:1
arxiv-1506-05173 | Feature Selection for Ridge Regression with Provable Guarantees | http://arxiv.org/abs/1506.05173 | id:1506.05173 author:Saurabh Paul, Petros Drineas category:stat.ML cs.IT cs.LG math.IT  published:2015-06-17 summary:We introduce single-set spectral sparsification as a deterministic sampling based feature selection technique for regularized least squares classification, which is the classification analogue to ridge regression. The method is unsupervised and gives worst-case guarantees of the generalization power of the classification function after feature selection with respect to the classification function obtained using all features. We also introduce leverage-score sampling as an unsupervised randomized feature selection method for ridge regression. We provide risk bounds for both single-set spectral sparsification and leverage-score sampling on ridge regression in the fixed design setting and show that the risk in the sampled space is comparable to the risk in the full-feature space. We perform experiments on synthetic and real-world datasets, namely a subset of TechTC-300 datasets, to support our theory. Experimental results indicate that the proposed methods perform better than the existing feature selection methods. version:2
arxiv-1512-01691 | Maximum Entropy Binary Encoding for Face Template Protection | http://arxiv.org/abs/1512.01691 | id:1512.01691 author:Rohit Kumar Pandey, Yingbo Zhou, Bhargava Urala Kota, Venu Govindaraju category:cs.CV  published:2015-12-05 summary:In this paper we present a framework for secure identification using deep neural networks, and apply it to the task of template protection for face authentication. We use deep convolutional neural networks (CNNs) to learn a mapping from face images to maximum entropy binary (MEB) codes. The mapping is robust enough to tackle the problem of exact matching, yielding the same code for new samples of a user as the code assigned during training. These codes are then hashed using any hash function that follows the random oracle model (like SHA-512) to generate protected face templates (similar to text based password protection). The algorithm makes no unrealistic assumptions and offers high template security, cancelability, and state-of-the-art matching performance. The efficacy of the approach is shown on CMU-PIE, Extended Yale B, and Multi-PIE face databases. We achieve high (~95%) genuine accept rates (GAR) at zero false accept rate (FAR) with up to 1024 bits of template security. version:1
arxiv-1504-08289 | Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks | http://arxiv.org/abs/1504.08289 | id:1504.08289 author:Marcel Simon, Erik Rodner category:cs.CV  published:2015-04-30 summary:Part models of object categories are essential for challenging recognition tasks, where differences in categories are subtle and only reflected in appearances of small parts of the object. We present an approach that is able to learn part models in a completely unsupervised manner, without part annotations and even without given bounding boxes during learning. The key idea is to find constellations of neural activation patterns computed using convolutional neural networks. In our experiments, we outperform existing approaches for fine-grained recognition on the CUB200-2011, NA birds, Oxford PETS, and Oxford Flowers dataset in case no part or bounding box annotations are available and achieve state-of-the-art performance for the Stanford Dog dataset. We also show the benefits of neural constellation models as a data augmentation technique for fine-tuning. Furthermore, our paper unites the areas of generic and fine-grained classification, since our approach is suitable for both scenarios. The source code of our method is available online at http://www.inf-cv.uni-jena.de/part_discovery version:3
arxiv-1512-01680 | A Shapley Value Solution to Game Theoretic-based Feature Reduction in False Alarm Detection | http://arxiv.org/abs/1512.01680 | id:1512.01680 author:Fatemeh Afghah, Abolfazl Razi, Kayvan Najarian category:cs.CV  published:2015-12-05 summary:False alarm is one of the main concerns in intensive care units and can result in care disruption, sleep deprivation, and insensitivity of care-givers to alarms. Several methods have been proposed to suppress the false alarm rate through improving the quality of physiological signals by filtering, and developing more accurate sensors. However, significant intrinsic correlation among the extracted features limits the performance of most currently available data mining techniques, as they often discard the predictors with low individual impact that may potentially have strong discriminatory power when grouped with others. We propose a model based on coalition game theory that considers the inter-features dependencies in determining the salient predictors in respect to false alarm, which results in improved classification accuracy. The superior performance of this method compared to current methods is shown in simulation results using PhysionNet's MIMIC II database. version:1
arxiv-1511-02407 | A Survey of the Trends in Facial and Expression Recognition Databases and Methods | http://arxiv.org/abs/1511.02407 | id:1511.02407 author:Sohini Roychowdhury, Michelle Emmons category:cs.CV  published:2015-11-07 summary:Automated facial identification and facial expression recognition have been topics of active research over the past few decades. Facial and expression recognition find applications in human-computer interfaces, subject tracking, real-time security surveillance systems and social networking. Several holistic and geometric methods have been developed to identify faces and expressions using public and local facial image databases. In this work we present the evolution in facial image data sets and the methodologies for facial identification and recognition of expressions such as anger, sadness, happiness, disgust, fear and surprise. We observe that most of the earlier methods for facial and expression recognition aimed at improving the recognition rates for facial feature-based methods using static images. However, the recent methodologies have shifted focus towards robust implementation of facial/expression recognition from large image databases that vary with space (gathered from the internet) and time (video recordings). The evolution trends in databases and methodologies for facial and expression recognition can be useful for assessing the next-generation topics that may have applications in security systems or personal identification systems that involve "Quantitative face" assessments. version:2
arxiv-1512-01666 | Stochastic Collapsed Variational Inference for Sequential Data | http://arxiv.org/abs/1512.01666 | id:1512.01666 author:Pengyu Wang, Phil Blunsom category:stat.ML  published:2015-12-05 summary:Stochastic variational inference for collapsed models has recently been successfully applied to large scale topic modelling. In this paper, we propose a stochastic collapsed variational inference algorithm in the sequential data setting. Our algorithm is applicable to both finite hidden Markov models and hierarchical Dirichlet process hidden Markov models, and to any datasets generated by emission distributions in the exponential family. Our experiment results on two discrete datasets show that our inference is both more efficient and more accurate than its uncollapsed version, stochastic variational inference. version:1
arxiv-1512-01665 | Stochastic Collapsed Variational Inference for Hidden Markov Models | http://arxiv.org/abs/1512.01665 | id:1512.01665 author:Pengyu Wang, Phil Blunsom category:stat.ML  published:2015-12-05 summary:Stochastic variational inference for collapsed models has recently been successfully applied to large scale topic modelling. In this paper, we propose a stochastic collapsed variational inference algorithm for hidden Markov models, in a sequential data setting. Given a collapsed hidden Markov Model, we break its long Markov chain into a set of short subchains. We propose a novel sum-product algorithm to update the posteriors of the subchains, taking into account their boundary transitions due to the sequential dependencies. Our experiments on two discrete datasets show that our collapsed algorithm is scalable to very large datasets, memory efficient and significantly more accurate than the existing uncollapsed algorithm. version:1
arxiv-1510-04601 | A Picture is Worth a Billion Bits: Real-Time Image Reconstruction from Dense Binary Pixels | http://arxiv.org/abs/1510.04601 | id:1510.04601 author:Tal Remez, Or Litany, Alex Bronstein category:cs.CV  published:2015-10-15 summary:The pursuit of smaller pixel sizes at ever increasing resolution in digital image sensors is mainly driven by the stringent price and form-factor requirements of sensors and optics in the cellular phone market. Recently, Eric Fossum proposed a novel concept of an image sensor with dense sub-diffraction limit one-bit pixels jots, which can be considered a digital emulation of silver halide photographic film. This idea has been recently embodied as the EPFL Gigavision camera. A major bottleneck in the design of such sensors is the image reconstruction process, producing a continuous high dynamic range image from oversampled binary measurements. The extreme quantization of the Poisson statistics is incompatible with the assumptions of most standard image processing and enhancement frameworks. The recently proposed maximum-likelihood (ML) approach addresses this difficulty, but suffers from image artifacts and has impractically high computational complexity. In this work, we study a variant of a sensor with binary threshold pixels and propose a reconstruction algorithm combining an ML data fitting term with a sparse synthesis prior. We also show an efficient hardware-friendly real-time approximation of this inverse operator.Promising results are shown on synthetic data as well as on HDR data emulated using multiple exposures of a regular CMOS sensor. version:2
arxiv-1511-02911 | Spatially Coherent Random Forests | http://arxiv.org/abs/1511.02911 | id:1511.02911 author:Tal Remez, Shai Avidan category:cs.CV  published:2015-11-09 summary:Spatially Coherent Random Forest (SCRF) extends Random Forest to create spatially coherent labeling. Each split function in SCRF is evaluated based on a traditional information gain measure that is regularized by a spatial coherency term. This way, SCRF is encouraged to choose split functions that cluster pixels both in appearance space and in image space. In particular, we use SCRF to detect contours in images, where contours are taken to be the boundaries between different regions. Each tree in the forest produces a segmentation of the image plane and the boundaries of the segmentations of all trees are aggregated to produce a final hierarchical contour map. We show that this modification improves the performance of regular Random Forest by about 10% on the standard Berkeley Segmentation Datasets. We believe that SCRF can be used in other settings as well. version:2
arxiv-1511-08551 | Regularized EM Algorithms: A Unified Framework and Statistical Guarantees | http://arxiv.org/abs/1511.08551 | id:1511.08551 author:Xinyang Yi, Constantine Caramanis category:cs.LG stat.ML  published:2015-11-27 summary:Latent variable models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges. The popular EM algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete. Recently, work in Balakrishnan et al. (2014) has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M-step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M-step using the state-of-the-art high-dimensional prescriptions (e.g., Wainwright (2014)) is not guaranteed to provide this balance. Our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors. We specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples. version:2
arxiv-1512-01642 | A Deep Structured Model with Radius-Margin Bound for 3D Human Activity Recognition | http://arxiv.org/abs/1512.01642 | id:1512.01642 author:Liang Lin, Keze Wang, Wangmeng Zuo, Meng Wang, Jiebo Luo, Lei Zhang category:cs.CV  published:2015-12-05 summary:Understanding human activity is very challenging even with the recently developed 3D/depth sensors. To solve this problem, this work investigates a novel deep structured model, which adaptively decomposes an activity instance into temporal parts using the convolutional neural networks (CNNs). Our model advances the traditional deep learning approaches in two aspects. First, { we incorporate latent temporal structure into the deep model, accounting for large temporal variations of diverse human activities. In particular, we utilize the latent variables to decompose the input activity into a number of temporally segmented sub-activities, and accordingly feed them into the parts (i.e. sub-networks) of the deep architecture}. Second, we incorporate a radius-margin bound as a regularization term into our deep model, which effectively improves the generalization performance for classification. For model training, we propose a principled learning algorithm that iteratively (i) discovers the optimal latent variables (i.e. the ways of activity decomposition) for all training instances, (ii) { updates the classifiers} based on the generated features, and (iii) updates the parameters of multi-layer neural networks. In the experiments, our approach is validated on several complex scenarios for human activity recognition and demonstrates superior performances over other state-of-the-art approaches. version:1
arxiv-1512-01641 | Unsupervised comparable corpora preparation and exploration for bi-lingual translation equivalents | http://arxiv.org/abs/1512.01641 | id:1512.01641 author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL stat.ML  published:2015-12-05 summary:The multilingual nature of the world makes translation a crucial requirement today. Parallel dictionaries constructed by humans are a widely-available resource, but they are limited and do not provide enough coverage for good quality translation purposes, due to out-of-vocabulary words and neologisms. This motivates the use of statistical translation systems, which are unfortunately dependent on the quantity and quality of training data. Such systems have a very limited availability especially for some languages and very narrow text domains. In this research we present our improvements to current comparable corpora mining methodologies by re- implementation of the comparison algorithms (using Needleman-Wunch algorithm), introduction of a tuning script and computation time improvement by GPU acceleration. Experiments are carried out on bilingual data extracted from the Wikipedia, on various domains. For the Wikipedia itself, additional cross-lingual comparison heuristics were introduced. The modifications made a positive impact on the quality and quantity of mined data and on the translation quality. version:1
arxiv-1512-01639 | PJAIT Systems for the IWSLT 2015 Evaluation Campaign Enhanced by Comparable Corpora | http://arxiv.org/abs/1512.01639 | id:1512.01639 author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL stat.ML  published:2015-12-05 summary:In this paper, we attempt to improve Statistical Machine Translation (SMT) systems on a very diverse set of language pairs (in both directions): Czech - English, Vietnamese - English, French - English and German - English. To accomplish this, we performed translation model training, created adaptations of training settings for each language pair, and obtained comparable corpora for our SMT systems. Innovative tools and data adaptation techniques were employed. The TED parallel text corpora for the IWSLT 2015 evaluation campaign were used to train language models, and to develop, tune, and test the system. In addition, we prepared Wikipedia-based comparable corpora for use with our SMT system. This data was specified as permissible for the IWSLT 2015 evaluation. We explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. To evaluate the effects of different preparations on translation results, we conducted experiments and used the BLEU, NIST and TER metrics. Our results indicate that our approach produced a positive impact on SMT quality. version:1
arxiv-1511-01664 | Stochastic Proximal Gradient Descent for Nuclear Norm Regularization | http://arxiv.org/abs/1511.01664 | id:1511.01664 author:Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou category:cs.LG  published:2015-11-05 summary:In this paper, we utilize stochastic optimization to reduce the space complexity of convex composite optimization with a nuclear norm regularizer, where the variable is a matrix of size $m \times n$. By constructing a low-rank estimate of the gradient, we propose an iterative algorithm based on stochastic proximal gradient descent (SPGD), and take the last iterate of SPGD as the final solution. The main advantage of the proposed algorithm is that its space complexity is $O(m+n)$, in contrast, most of previous algorithms have a $O(mn)$ space complexity. Theoretical analysis shows that it achieves $O(\log T/\sqrt{T})$ and $O(\log T/T)$ convergence rates for general convex functions and strongly convex functions, respectively. version:2
arxiv-1512-01631 | Hierarchical Sparse Modeling: A Choice of Two Regularizers | http://arxiv.org/abs/1512.01631 | id:1512.01631 author:Xiaohan Yan, Jacob Bien category:stat.ME math.ST stat.CO stat.ML stat.TH  published:2015-12-05 summary:Demanding sparsity in estimated models has become a routine practice in statistics. In many situations, we wish to demand that the sparsity patterns attained honor certain problem-specific constraints. Hierarchical sparse modeling (HSM) refers to situations in which these constraints specify that one set of parameters be set to zero whenever another is set to zero. In recent years, numerous papers have developed convex regularizers for this form of sparsity structure arising in areas including interaction modeling, time series, and covariance estimation. In this paper, we observe that these methods fall into two frameworks, which have not been systematically compared in the context of HSM. The purpose of this paper is to provide a side-by-side comparison of these two frameworks for HSM in terms of their statistical properties and computational efficiency. We call attention to a problem with the more commonly used framework and provide new insights into the other, which can greatly improve its computational performance. Finally, we compare the two methods in the context of covariance estimation, where we introduce a new sparsely-banded estimator, which we show achieves the statistical advantages of an existing method but is simpler to compute. version:1
arxiv-1512-01629 | Risk-Constrained Reinforcement Learning with Percentile Risk Criteria | http://arxiv.org/abs/1512.01629 | id:1512.01629 author:Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, Marco Pavone category:cs.AI cs.LG math.OC  published:2015-12-05 summary:In many sequential decision-making problems one is interested in minimizing an expected cumulative cost while taking into account \emph{risk}, i.e., increased awareness of events of small probability and high consequences. Accordingly, the objective of this paper is to present efficient reinforcement learning algorithms for risk-constrained Markov decision processes (MDPs), where risk is represented via a chance constraint or a constraint on the conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer to such problems as percentile risk-constrained MDPs. Specifically, we first derive a formula for computing the gradient of the Lagrangian function for percentile risk-constrained MDPs. Then, we devise policy gradient and actor-critic algorithms that (1) estimate such gradient, (2) update the policy parameters in the descent direction, and (3) update the Lagrange multiplier in the ascent direction. For these algorithms we prove convergence to locally-optimal policies. Finally, we demonstrate the effectiveness of our algorithms in an optimal stopping problem and an online marketing application. version:1
arxiv-1511-06480 | On Binary Embedding using Circulant Matrices | http://arxiv.org/abs/1511.06480 | id:1511.06480 author:Felix X. Yu, Aditya Bhaskara, Sanjiv Kumar, Yunchao Gong, Shih-Fu Chang category:cs.DS cs.LG  published:2015-11-20 summary:Binary embeddings provide efficient and powerful ways to perform operations on large scale data. However binary embedding typically requires long codes in order to preserve the discriminative power of the input space. Thus binary coding methods traditionally suffer from high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure allows us to use Fast Fourier Transform algorithms to speed up the computation. For obtaining $k$-bit binary codes from $d$-dimensional data, this improves the time complexity from $O(dk)$ to $O(d\log{d})$, and the space complexity from $O(dk)$ to $O(d)$. We study two settings, which differ in the way we choose the parameters of the circulant matrix. In the first, the parameters are chosen randomly and in the second, the parameters are learned using the data. For randomized CBE, we give a theoretical analysis comparing it with binary embedding using an unstructured random projection matrix. The challenge here is to show that the dependencies in the entries of the circulant matrix do not lead to a loss in performance. In the second setting, we design a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. In both the settings, we show by extensive experiments that the CBE approach gives much better performance than the state-of-the-art approaches if we fix a running time, and provides much faster computation with negligible performance degradation if we fix the number of bits in the embedding. version:2
arxiv-1512-01587 | Extracting Biomolecular Interactions Using Semantic Parsing of Biomedical Text | http://arxiv.org/abs/1512.01587 | id:1512.01587 author:Sahil Garg, Aram Galstyan, Ulf Hermjakob, Daniel Marcu category:cs.CL cs.AI cs.IR cs.IT cs.LG math.IT  published:2015-12-04 summary:We advance the state of the art in biomolecular interaction extraction with three contributions: (i) We show that deep, Abstract Meaning Representations (AMR) significantly improve the accuracy of a biomolecular interaction extraction system when compared to a baseline that relies solely on surface- and syntax-based features; (ii) In contrast with previous approaches that infer relations on a sentence-by-sentence basis, we expand our framework to enable consistent predictions over sets of sentences (documents); (iii) We further modify and expand a graph kernel learning framework to enable concurrent exploitation of automatically induced AMR (semantic) and dependency structure (syntactic) representations. Our experiments show that our approach yields interaction extraction systems that are more robust in environments where there is a significant mismatch between training and test conditions. version:1
arxiv-1506-02544 | Learning with Group Invariant Features: A Kernel Perspective | http://arxiv.org/abs/1506.02544 | id:1506.02544 author:Youssef Mroueh, Stephen Voinea, Tomaso Poggio category:cs.LG cs.CV stat.ML  published:2015-06-08 summary:We analyze in this paper a random feature map based on a theory of invariance I-theory introduced recently. More specifically, a group invariant signal signature is obtained through cumulative distributions of group transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of $N$ points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting. version:2
arxiv-1511-03979 | Representational Distance Learning for Deep Neural Networks | http://arxiv.org/abs/1511.03979 | id:1511.03979 author:Patrick McClure, Nikolaus Kriegeskorte category:cs.NE cs.CV  published:2015-11-12 summary:We propose representational distance learning (RDL), a technique that allows transferring knowledge from an arbitrary model with task related information to a deep neural network (DNN). This method seeks to maximize the similarity between the representational distance matrices (RDMs) of a model with desired knowledge, the teacher, and a DNN currently being trained, the student. The knowledge contained in the information transformations performed by the teacher are transferred to a student using auxiliary error functions. This allows a DNN to simultaneously learn from a teacher model and learn to perform some task within the framework of backpropagation. We test the use of RDL for knowledge distillation, also known as model compression, from a large teacher DNN to a small student DNN using the MNIST and CIFAR-10 datasets. Also, we test the use of RDL for knowledge transfer between tasks using the CIFAR-10 and CIFAR-100 datasets. For each test, RDL significantly improves performance when compared to traditional backpropagation alone and performs similarly to, or better than, recently proposed methods for model compression and knowledge transfer. version:5
arxiv-1512-01537 | Reuse of Neural Modules for General Video Game Playing | http://arxiv.org/abs/1512.01537 | id:1512.01537 author:Alexander Braylan, Mark Hollenbeck, Elliot Meyerson, Risto Miikkulainen category:cs.NE cs.AI  published:2015-12-04 summary:A general approach to knowledge transfer is introduced in which an agent controlled by a neural network adapts how it reuses existing networks as it learns in a new domain. Networks trained for a new domain can improve their performance by routing activation selectively through previously learned neural structure, regardless of how or for what it was learned. A neuroevolution implementation of this approach is presented with application to high-dimensional sequential decision-making domains. This approach is more general than previous approaches to neural transfer for reinforcement learning. It is domain-agnostic and requires no prior assumptions about the nature of task relatedness or mappings. The method is analyzed in a stochastic version of the Arcade Learning Environment, demonstrating that it improves performance in some of the more complex Atari 2600 games, and that the success of transfer can be predicted based on a high-level characterization of game dynamics. version:1
arxiv-1512-01533 | Motion trails from time-lapse video | http://arxiv.org/abs/1512.01533 | id:1512.01533 author:Camille Goudeseune category:cs.CV I.3.3; I.4.6  published:2015-12-04 summary:From an image sequence captured by a stationary camera, background subtraction can detect moving foreground objects in the scene. Distinguishing foreground from background is further improved by various heuristics. Then each object's motion can be emphasized by duplicating its positions as a motion trail. These trails clarify the objects' spatial relationships. Also, adding motion trails to a video before previewing it at high speed reduces the risk of overlooking transient events. version:1
arxiv-1511-07035 | Detecting Road Surface Wetness from Audio: A Deep Learning Approach | http://arxiv.org/abs/1511.07035 | id:1511.07035 author:Irman Abdić, Lex Fridman, Erik Marchi, Daniel E Brown, William Angell, Bryan Reimer, Björn Schuller category:cs.LG cs.NE cs.SD  published:2015-11-22 summary:We introduce a recurrent neural network architecture for automated road surface wetness detection from audio of tire-surface interaction. The robustness of our approach is evaluated on 785,826 bins of audio that span an extensive range of vehicle speeds, noises from the environment, road surface types, and pavement conditions including international roughness index (IRI) values from 25 in/mi to 1400 in/mi. The training and evaluation of the model are performed on different roads to minimize the impact of environmental and other external factors on the accuracy of the classification. We achieve an unweighted average recall (UAR) of 93.2% across all vehicle speeds including 0 mph. The classifier still works at 0 mph because the discriminating signal is present in the sound of other vehicles driving by. version:2
arxiv-1512-01525 | Learning the Semantics of Manipulation Action | http://arxiv.org/abs/1512.01525 | id:1512.01525 author:Yezhou Yang, Yiannis Aloimonos, Cornelia Fermuller, Eren Erdal Aksoy category:cs.RO cs.CL cs.CV  published:2015-12-04 summary:In this paper we present a formal computational framework for modeling manipulation actions. The introduced formalism leads to semantics of manipulation action and has applications to both observing and understanding human manipulation actions as well as executing them with a robotic mechanism (e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. The goal of the introduced framework is to: (1) represent manipulation actions with both syntax and semantic parts, where the semantic part employs $\lambda$-calculus; (2) enable a probabilistic semantic parsing schema to learn the $\lambda$-calculus representation of manipulation action from an annotated action corpus of videos; (3) use (1) and (2) to develop a system that visually observes manipulation actions and understands their meaning while it can reason beyond observations using propositional logic and axiom schemata. The experiments conducted on a public available large manipulation action dataset validate the theoretical framework and our implementation. version:1
arxiv-1512-01515 | ASIST: Automatic Semantically Invariant Scene Transformation | http://arxiv.org/abs/1512.01515 | id:1512.01515 author:Or Litany, Tal Remez, Daniel Freedman, Lior Shapira, Alex Bronstein, Ran Gal category:cs.CV  published:2015-12-04 summary:We present ASIST, a technique for transforming point clouds by replacing objects with their semantically equivalent counterparts. Transformations of this kind have applications in virtual reality, repair of fused scans, and robotics. ASIST is based on a unified formulation of semantic labeling and object replacement; both result from minimizing a single objective. We present numerical tools for the e?cient solution of this optimization problem. The method is experimentally assessed on new datasets of both synthetic and real point clouds, and is additionally compared to two recent works on object replacement on data from the corresponding papers. version:1
arxiv-1508-05565 | Necessary and Sufficient Conditions and a Provably Efficient Algorithm for Separable Topic Discovery | http://arxiv.org/abs/1508.05565 | id:1508.05565 author:Weicong Ding, Prakash Ishwar, Venkatesh Saligrama category:cs.LG cs.CL cs.IR stat.ML  published:2015-08-23 summary:We develop necessary and sufficient conditions and a novel provably consistent and efficient algorithm for discovering topics (latent factors) from observations (documents) that are realized from a probabilistic mixture of shared latent factors that have certain properties. Our focus is on the class of topic models in which each shared latent factor contains a novel word that is unique to that factor, a property that has come to be known as separability. Our algorithm is based on the key insight that the novel words correspond to the extreme points of the convex hull formed by the row-vectors of a suitably normalized word co-occurrence matrix. We leverage this geometric insight to establish polynomial computation and sample complexity bounds based on a few isotropic random projections of the rows of the normalized word co-occurrence matrix. Our proposed random-projections-based algorithm is naturally amenable to an efficient distributed implementation and is attractive for modern web-scale distributed data mining applications. version:2
arxiv-1509-05251 | Hand-held Video Deblurring via Efficient Fourier Aggregation | http://arxiv.org/abs/1509.05251 | id:1509.05251 author:Mauricio Delbracio, Guillermo Sapiro category:cs.CV  published:2015-09-17 summary:Videos captured with hand-held cameras often suffer from a significant amount of blur, mainly caused by the inevitable natural tremor of the photographer's hand. In this work, we present an algorithm that removes blur due to camera shake by combining information in the Fourier domain from nearby frames in a video. The dynamic nature of typical videos with the presence of multiple moving objects and occlusions makes this problem of camera shake removal extremely challenging, in particular when low complexity is needed. Given an input video frame, we first create a consistent registered version of temporally adjacent frames. Then, the set of consistently registered frames is block-wise fused in the Fourier domain with weights depending on the Fourier spectrum magnitude. The method is motivated from the physiological fact that camera shake blur has a random nature and therefore, nearby video frames are generally blurred differently. Experiments with numerous videos recorded in the wild, along with extensive comparisons, show that the proposed algorithm achieves state-of-the-art results while at the same time being much faster than its competitors. version:3
arxiv-1505-02731 | Removing Camera Shake via Weighted Fourier Burst Accumulation | http://arxiv.org/abs/1505.02731 | id:1505.02731 author:Mauricio Delbracio, Guillermo Sapiro category:cs.CV  published:2015-05-11 summary:Numerous recent approaches attempt to remove image blur due to camera shake, either with one or multiple input images, by explicitly solving an inverse and inherently ill-posed deconvolution problem. If the photographer takes a burst of images, a modality available in virtually all modern digital cameras, we show that it is possible to combine them to get a clean sharp version. This is done without explicitly solving any blur estimation and subsequent inverse problem. The proposed algorithm is strikingly simple: it performs a weighted average in the Fourier domain, with weights depending on the Fourier spectrum magnitude. The method can be seen as a generalization of the align and average procedure, with a weighted average, motivated by hand-shake physiology and theoretically supported, taking place in the Fourier domain. The method's rationale is that camera shake has a random nature and therefore each image in the burst is generally blurred differently. Experiments with real camera data, and extensive comparisons, show that the proposed Fourier Burst Accumulation (FBA) algorithm achieves state-of-the-art results an order of magnitude faster, with simplicity for on-board implementation on camera phones. Finally, we also present experiments in real high dynamic range (HDR) scenes, showing how the method can be straightforwardly extended to HDR photography. version:2
arxiv-1507-00136 | Compressive Deconvolution in Medical Ultrasound Imaging | http://arxiv.org/abs/1507.00136 | id:1507.00136 author:Zhouye Chen, Adrian Basarab, Denis Kouamé category:cs.CV  published:2015-07-01 summary:The interest of compressive sampling in ultrasound imaging has been recently extensively evaluated by several research teams. Following the different application setups, it has been shown that the RF data may be reconstructed from a small number of measurements and/or using a reduced number of ultrasound pulse emissions. Nevertheless, RF image spatial resolution, contrast and signal to noise ratio are affected by the limited bandwidth of the imaging transducer and the physical phenomenon related to US wave propagation. To overcome these limitations, several deconvolution-based image processing techniques have been proposed to enhance the ultrasound images. In this paper, we propose a novel framework, named compressive deconvolution, that reconstructs enhanced RF images from compressed measurements. Exploiting an unified formulation of the direct acquisition model, combining random projections and 2D convolution with a spatially invariant point spread function, the benefit of our approach is the joint data volume reduction and image quality improvement. The proposed optimization method, based on the Alternating Direction Method of Multipliers, is evaluated on both simulated and in vivo data. version:2
arxiv-1511-03249 | Stochastic Expectation Propagation for Large Scale Gaussian Process Classification | http://arxiv.org/abs/1511.03249 | id:1511.03249 author:Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Yingzhen Li, Thang Bui, Richard E. Turner category:stat.ML  published:2015-11-10 summary:A method for large scale Gaussian process classification has been recently proposed based on expectation propagation (EP). Such a method allows Gaussian process classifiers to be trained on very large datasets that were out of the reach of previous deployments of EP and has been shown to be competitive with related techniques based on stochastic variational inference. Nevertheless, the memory resources required scale linearly with the dataset size, unlike in variational methods. This is a severe limitation when the number of instances is very large. Here we show that this problem is avoided when stochastic EP is used to train the model. version:3
arxiv-1512-01413 | Computational Imaging for VLBI Image Reconstruction | http://arxiv.org/abs/1512.01413 | id:1512.01413 author:Katherine L. Bouman, Michael D. Johnson, Daniel Zoran, Vincent L. Fish, Sheperd S. Doeleman, William T. Freeman category:astro-ph.IM astro-ph.GA cs.CV  published:2015-12-04 summary:Very long baseline interferometry (VLBI) is a technique for imaging celestial radio emissions by simultaneously observing a source from telescopes distributed across Earth. The challenges in reconstructing images from fine angular resolution VLBI data are immense. The data is extremely sparse and noisy, thus requiring statistical image models such as those designed in the computer vision community. In this paper we present a novel Bayesian approach for VLBI image reconstruction. While other methods require careful tuning and parameter selection for different types of images, our method is robust and produces good results under different settings such as low SNR or extended emissions. The success of our method is demonstrated on realistic synthetic experiments as well as publicly available real data. We present this problem in a way that is accessible to members of the computer vision community, and provide a dataset website (vlbiimaging.csail.mit.edu) to allow for controlled comparisons across algorithms. This dataset can foster development of new methods by making VLBI easily approachable to computer vision researchers. version:1
arxiv-1512-01409 | What Makes it Difficult to Understand a Scientific Literature? | http://arxiv.org/abs/1512.01409 | id:1512.01409 author:Mengyun Cao, Jiao Tian, Dezhi Cheng, Jin Liu, Xiaoping Sun category:cs.CL  published:2015-12-04 summary:In the artificial intelligence area, one of the ultimate goals is to make computers understand human language and offer assistance. In order to achieve this ideal, researchers of computer science have put forward a lot of models and algorithms attempting at enabling the machine to analyze and process human natural language on different levels of semantics. Although recent progress in this field offers much hope, we still have to ask whether current research can provide assistance that people really desire in reading and comprehension. To this end, we conducted a reading comprehension test on two scientific papers which are written in different styles. We use the semantic link models to analyze the understanding obstacles that people will face in the process of reading and figure out what makes it difficult for human to understand a scientific literature. Through such analysis, we summarized some characteristics and problems which are reflected by people with different levels of knowledge on the comprehension of difficult science and technology literature, which can be modeled in semantic link network. We believe that these characteristics and problems will help us re-examine the existing machine models and are helpful in the designing of new one. version:1
arxiv-1512-01408 | Neuron's Eye View: Inferring Features of Complex Stimuli from Neural Responses | http://arxiv.org/abs/1512.01408 | id:1512.01408 author:John M Pearson, Jeffrey M Beck category:stat.ML q-bio.NC  published:2015-12-04 summary:Experiments that study neural encoding of stimuli at the level of individual neurons typically choose a small set of features present in the world --- contrast and luminance for vision, pitch and intensity for sound --- and assemble a stimulus set that systematically (and preferably exhaustively) varies along these dimensions. Neuronal responses in the form of firing rates are then examined for modulation with respect to these features via some form of regression. This approach requires that experimenters know (or guess) in advance the relevant features coded by a given population of neurons. Unfortunately, for domains as complex as social interaction or natural movement, the relevant feature space is poorly understood, and an arbitrary \emph{a priori} choice of feature sets may give rise to confirmation bias. Here, we present a Bayesian model for exploratory data analysis that is capable of automatically identifying the features present in unstructured stimuli based solely on neuronal responses. Our approach is unique within the class of latent state space models of neural activity in that it assumes that firing rates of neurons are sensitive to multiple discrete time-varying features tied to the \emph{stimulus}, each of which has Markov (or semi-Markov) dynamics. That is, we are modeling stimulus dynamics as driven by neural activity, rather than intrinsic neural dynamics. We derive a fast variational Bayesian inference algorithm and show that it correctly recovers hidden features in synthetic data, as well as ground-truth stimulus features in a prototypical neural dataset. To demonstrate the utility of the algorithm, we also apply it to an exploratory analysis of prefrontal cortex recordings performed while monkeys watched naturalistic videos of primate social activity. version:1
arxiv-1512-01401 | Model Validation for Vision Systems via Graphics Simulation | http://arxiv.org/abs/1512.01401 | id:1512.01401 author:V S R Veeravasarapu, Rudra Narayan Hota, Constantin Rothkopf, Ramesh Visvanathan category:cs.CV  published:2015-12-04 summary:Rapid advances in computation, combined with latest advances in computer graphics simulations have facilitated the development of vision systems and training them in virtual environments. One major stumbling block is in certification of the designs and tuned parameters of these systems to work in real world. In this paper, we begin to explore the fundamental question: Which type of information transfer is more analogous to real world? Inspired from the performance characterization methodology outlined in the 90's, we note that insights derived from simulations can be qualitative or quantitative depending on the degree of the fidelity of models used in simulations and the nature of the questions posed by the experimenter. We adapt the methodology in the context of current graphics simulation tools for modeling data generation processes and, for systematic performance characterization and trade-off analysis for vision system design leading to qualitative and quantitative insights. In concrete, we examine invariance assumptions used in vision algorithms for video surveillance settings as a case study and assess the degree to which those invariance assumptions deviate as a function of contextual variables on both graphics simulations and in real data. As computer graphics rendering quality improves, we believe teasing apart the degree to which model assumptions are valid via systematic graphics simulation can be a significant aid to assisting more principled ways of approaching vision system design and performance modeling. version:1
arxiv-1512-01400 | Max-Pooling Dropout for Regularization of Convolutional Neural Networks | http://arxiv.org/abs/1512.01400 | id:1512.01400 author:Haibing Wu, Xiaodong Gu category:cs.LG cs.CV cs.NE  published:2015-12-04 summary:Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage. version:1
arxiv-1512-01384 | Topic segmentation via community detection in complex networks | http://arxiv.org/abs/1512.01384 | id:1512.01384 author:Henrique F. de Arruda, Luciano da F. Costa, Diego R. Amancio category:cs.CL cs.SI  published:2015-12-04 summary:Many real systems have been modelled in terms of network concepts, and written texts are a particular example of information networks. In recent years, the use of network methods to analyze language has allowed the discovery of several interesting findings, including the proposition of novel models to explain the emergence of fundamental universal patterns. While syntactical networks, one of the most prevalent networked models of written texts, display both scale-free and small-world properties, such representation fails in capturing other textual features, such as the organization in topics or subjects. In this context, we propose a novel network representation whose main purpose is to capture the semantical relationships of words in a simple way. To do so, we link all words co-occurring in the same semantic context, which is defined in a threefold way. We show that the proposed representations favours the emergence of communities of semantically related words, and this feature may be used to identify relevant topics. The proposed methodology to detect topics was applied to segment selected Wikipedia articles. We have found that, in general, our methods outperform traditional bag-of-words representations, which suggests that a high-level textual representation may be useful to study semantical features of texts. version:1
arxiv-1512-01383 | Sublabel-Accurate Relaxation of Nonconvex Energies | http://arxiv.org/abs/1512.01383 | id:1512.01383 author:Thomas Möllenhoff, Emanuel Laude, Michael Moeller, Jan Lellmann, Daniel Cremers category:cs.CV  published:2015-12-04 summary:We propose a novel spatially continuous framework for convex relaxations based on functional lifting. Our method can be interpreted as a sublabel-accurate solution to multilabel problems. We show that previously proposed functional lifting methods optimize an energy which is linear between two labels and hence require (often infinitely) many labels for a faithful approximation. In contrast, the proposed formulation is based on a piecewise convex approximation and therefore needs far fewer labels. In comparison to recent MRF-based approaches, our method is formulated in a spatially continuous setting and shows less grid bias. Moreover, in a local sense, our formulation is the tightest possible convex relaxation. It is easy to implement and allows an efficient primal-dual optimization on GPUs. We show the effectiveness of our approach on several computer vision problems. version:1
arxiv-1507-02925 | Completely random measures for modelling block-structured networks | http://arxiv.org/abs/1507.02925 | id:1507.02925 author:Tue Herlau, Mikkel N. Schmidt, Morten Mørup category:stat.ML  published:2015-07-10 summary:Many statistical methods for network data parameterize the edge-probability by attributing latent traits to the vertices such as block structure and assume exchangeability in the sense of the Aldous-Hoover representation theorem. Empirical studies of networks indicate that many real-world networks have a power-law distribution of the vertices which in turn implies the number of edges scale slower than quadratically in the number of vertices. These assumptions are fundamentally irreconcilable as the Aldous-Hoover theorem implies quadratic scaling of the number of edges. Recently Caron and Fox (2014) proposed the use of a different notion of exchangeability due to Kallenberg (2009) and obtained a network model which admits power-law behaviour while retaining desirable statistical properties, however this model does not capture latent vertex traits such as block-structure. In this work we re-introduce the use of block-structure for network models obeying Kallenberg's notion of exchangeability and thereby obtain a model which admits the inference of block-structure and edge inhomogeneity. We derive a simple expression for the likelihood and an efficient sampling method. The obtained model is not significantly more difficult to implement than existing approaches to block-modelling and performs well on real network datasets. version:3
arxiv-1504-07027 | On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes | http://arxiv.org/abs/1504.07027 | id:1504.07027 author:Alexander G. de G. Matthews, James Hensman, Richard E. Turner, Zoubin Ghahramani category:stat.ML  published:2015-04-27 summary:The variational framework for learning inducing variables (Titsias, 2009a) has had a large impact on the Gaussian process literature. The framework may be interpreted as minimizing a rigorously defined Kullback-Leibler divergence between the approximating and posterior processes. To our knowledge this connection has thus far gone unremarked in the literature. In this paper we give a substantial generalization of the literature on this topic. We give a new proof of the result for infinite index sets which allows inducing points that are not data points and likelihoods that depend on all function values. We then discuss augmented index sets and show that, contrary to previous works, marginal consistency of augmentation is not enough to guarantee consistency of variational inference with the original model. We then characterize an extra condition where such a guarantee is obtainable. Finally we show how our framework sheds light on interdomain sparse approximations and sparse approximations for Cox processes. version:2
arxiv-1512-01370 | Locally Adaptive Translation for Knowledge Graph Embedding | http://arxiv.org/abs/1512.01370 | id:1512.01370 author:Yantao Jia, Yuanzhuo Wang, Hailun Lin, Xiaolong Jin, Xueqi Cheng category:cs.AI cs.CL I.2.4; I.2.6  published:2015-12-04 summary:Knowledge graph embedding aims to represent entities and relations in a large-scale knowledge graph as elements in a continuous vector space. Existing methods, e.g., TransE and TransH, learn embedding representation by defining a global margin-based loss function over the data. However, the optimal loss function is determined during experiments whose parameters are examined among a closed set of candidates. Moreover, embeddings over two knowledge graphs with different entities and relations share the same set of candidate loss functions, ignoring the locality of both graphs. This leads to the limited performance of embedding related applications. In this paper, we propose a locally adaptive translation method for knowledge graph embedding, called TransA, to find the optimal loss function by adaptively determining its margin over different knowledge graphs. Experiments on two benchmark data sets demonstrate the superiority of the proposed method, as compared to the-state-of-the-art ones. version:1
arxiv-1512-01362 | Proposition of a Theoretical Model for Missing Data Imputation using Deep Learning and Evolutionary Algorithms | http://arxiv.org/abs/1512.01362 | id:1512.01362 author:Collins Leke, Tshilidzi Marwala, Satyakama Paul category:cs.NE cs.LG  published:2015-12-04 summary:In the last couple of decades, there has been major advancements in the domain of missing data imputation. The techniques in the domain include amongst others: Expectation Maximization, Neural Networks with Evolutionary Algorithms or optimization techniques and K-Nearest Neighbor approaches to solve the problem. The presence of missing data entries in databases render the tasks of decision-making and data analysis nontrivial. As a result this area has attracted a lot of research interest with the aim being to yield accurate and time efficient and sensitive missing data imputation techniques especially when time sensitive applications are concerned like power plants and winding processes. In this article, considering arbitrary and monotone missing data patterns, we hypothesize that the use of deep neural networks built using autoencoders and denoising autoencoders in conjunction with genetic algorithms, swarm intelligence and maximum likelihood estimator methods as novel data imputation techniques will lead to better imputed values than existing techniques. Also considered are the missing at random, missing completely at random and missing not at random missing data mechanisms. We also intend to use fuzzy logic in tandem with deep neural networks to perform the missing data imputation tasks, as well as different building blocks for the deep neural networks like Stacked Restricted Boltzmann Machines and Deep Belief Networks to test our hypothesis. The motivation behind this article is the need for missing data imputation techniques that lead to better imputed values than existing methods with higher accuracies and lower errors. version:1
