arxiv-1301-3966 | Efficient Sample Reuse in Policy Gradients with Parameter-based Exploration | http://arxiv.org/abs/1301.3966 | id:1301.3966 author:Tingting Zhao, Hirotaka Hachiya, Voot Tangkaratt, Jun Morimoto, Masashi Sugiyama category:cs.LG stat.ML  published:2013-01-17 summary:The policy gradient approach is a flexible and powerful reinforcement learning method particularly for problems with continuous actions such as robot control. A common challenge in this scenario is how to reduce the variance of policy gradient estimates for reliable policy updates. In this paper, we combine the following three ideas and give a highly effective policy gradient method: (a) the policy gradients with parameter based exploration, which is a recently proposed policy search method with low variance of gradient estimates, (b) an importance sampling technique, which allows us to reuse previously gathered data in a consistent way, and (c) an optimal baseline, which minimizes the variance of gradient estimates with their unbiasedness being maintained. For the proposed method, we give theoretical analysis of the variance of gradient estimates and show its usefulness through extensive experiments. version:1
arxiv-1301-3964 | Multiscale Discriminant Saliency for Visual Attention | http://arxiv.org/abs/1301.3964 | id:1301.3964 author:Anh Cat Le Ngo, Kenneth Ang Li-Minn, Guoping Qiu, Jasmine Seng Kah-Phooi category:cs.CV  published:2013-01-17 summary:The bottom-up saliency, an early stage of humans' visual attention, can be considered as a binary classification problem between center and surround classes. Discriminant power of features for the classification is measured as mutual information between features and two classes distribution. The estimated discrepancy of two feature classes very much depends on considered scale levels; then, multi-scale structure and discriminant power are integrated by employing discrete wavelet features and Hidden markov tree (HMT). With wavelet coefficients and Hidden Markov Tree parameters, quad-tree like label structures are constructed and utilized in maximum a posterior probability (MAP) of hidden class variables at corresponding dyadic sub-squares. Then, saliency value for each dyadic square at each scale level is computed with discriminant power principle and the MAP. Finally, across multiple scales is integrated the final saliency map by an information maximization rule. Both standard quantitative tools such as NSS, LCC, AUC and qualitative assessments are used for evaluating the proposed multiscale discriminant saliency method (MDIS) against the well-know information-based saliency method AIM on its Bruce Database wity eye-tracking data. Simulation results are presented and analyzed to verify the validity of MDIS as well as point out its disadvantages for further research direction. version:1
arxiv-1301-3816 | Learning Output Kernels for Multi-Task Problems | http://arxiv.org/abs/1301.3816 | id:1301.3816 author:Francesco Dinuzzo category:cs.LG  published:2013-01-16 summary:Simultaneously solving multiple related learning tasks is beneficial under a variety of circumstances, but the prior knowledge necessary to correctly model task relationships is rarely available in practice. In this paper, we develop a novel kernel-based multi-task learning technique that automatically reveals structural inter-task relationships. Building over the framework of output kernel learning (OKL), we introduce a method that jointly learns multiple functions and a low-rank multi-task kernel by solving a non-convex regularization problem. Optimization is carried out via a block coordinate descent strategy, where each subproblem is solved using suitable conjugate gradient (CG) type iterative methods for linear operator equations. The effectiveness of the proposed approach is demonstrated on pharmacological and collaborative filtering data. version:1
arxiv-1207-0166 | On Multilabel Classification and Ranking with Partial Feedback | http://arxiv.org/abs/1207.0166 | id:1207.0166 author:Claudio Gentile, Francesco Orabona category:cs.LG  published:2012-06-30 summary:We present a novel multilabel/ranking algorithm working in partial information settings. The algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation. We analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models. We show O(T^{1/2} log T) regret bounds, which improve in several ways on the existing results. We test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on real-world multilabel datasets, often obtaining comparable performance. version:3
arxiv-1301-3755 | Gradient Driven Learning for Pooling in Visual Pipeline Feature Extraction Models | http://arxiv.org/abs/1301.3755 | id:1301.3755 author:Derek Rose, Itamar Arel category:cs.CV  published:2013-01-16 summary:Hyper-parameter selection remains a daunting task when building a pattern recognition architecture which performs well, particularly in recently constructed visual pipeline models for feature extraction. We re-formulate pooling in an existing pipeline as a function of adjustable pooling map weight parameters and propose the use of supervised error signals from gradient descent to tune the established maps within the model. This technique allows us to learn what would otherwise be a design choice within the model and specialize the maps to aggregate areas of invariance for the task presented. Preliminary results show moderate potential gains in classification accuracy and highlight areas of importance within the intermediate feature representation space. version:1
arxiv-1301-3901 | Variational Approximations between Mean Field Theory and the Junction Tree Algorithm | http://arxiv.org/abs/1301.3901 | id:1301.3901 author:Wim Wiegerinck category:cs.LG cs.AI stat.ML  published:2013-01-16 summary:Recently, variational approximations such as the mean field approximation have received much interest. We extend the standard mean field method by using an approximating distribution that factorises into cluster potentials. This includes undirected graphs, directed acyclic graphs and junction trees. We derive generalized mean field equations to optimize the cluster potentials. We show that the method bridges the gap between the standard mean field approximation and the exact junction tree algorithm. In addition, we address the problem of how to choose the graphical structure of the approximating distribution. From the generalised mean field equations we derive rules to simplify the structure of the approximating distribution in advance without affecting the quality of the approximation. We also show how the method fits into some other variational approximations that are currently popular. version:1
arxiv-1301-3899 | Model-Based Hierarchical Clustering | http://arxiv.org/abs/1301.3899 | id:1301.3899 author:Shivakumar Vaithyanathan, Byron E Dom category:cs.LG cs.AI stat.ML  published:2013-01-16 summary:We present an approach to model-based hierarchical clustering by formulating an objective function based on a Bayesian analysis. This model organizes the data into a cluster hierarchy while specifying a complex feature-set partitioning that is a key component of our model. Features can have either a unique distribution in every cluster or a common distribution over some (or even all) of the clusters. The cluster subsets over which these features have such a common distribution correspond to the nodes (clusters) of the tree representing the hierarchy. We apply this general model to the problem of document clustering for which we use a multinomial likelihood function and Dirichlet priors. Our algorithm consists of a two-stage process wherein we first perform a flat clustering followed by a modified hierarchical agglomerative merging process that includes determining the features that will have common distributions over the merged clusters. The regularization induced by using the marginal likelihood automatically determines the optimal model structure including number of clusters, the depth of the tree and the subset of features to be modeled as having a common distribution at each node. We present experimental results on both synthetic data and a real document collection. version:1
arxiv-1301-3897 | A Branch-and-Bound Algorithm for MDL Learning Bayesian Networks | http://arxiv.org/abs/1301.3897 | id:1301.3897 author:Jin Tian category:cs.AI cs.LG stat.ML  published:2013-01-16 summary:This paper extends the work in [Suzuki, 1996] and presents an efficient depth-first branch-and-bound algorithm for learning Bayesian network structures, based on the minimum description length (MDL) principle, for a given (consistent) variable ordering. The algorithm exhaustively searches through all network structures and guarantees to find the network with the best MDL score. Preliminary experiments show that the algorithm is efficient, and that the time complexity grows slowly with the sample size. The algorithm is useful for empirically studying both the performance of suboptimal heuristic search algorithms and the adequacy of the MDL principle in learning Bayesian networks. version:1
arxiv-1301-3896 | An Uncertainty Framework for Classification | http://arxiv.org/abs/1301.3896 | id:1301.3896 author:Loo-Nin Teow, Kia-Fock Loe category:cs.LG stat.ML  published:2013-01-16 summary:We define a generalized likelihood function based on uncertainty measures and show that maximizing such a likelihood function for different measures induces different types of classifiers. In the probabilistic framework, we obtain classifiers that optimize the cross-entropy function. In the possibilistic framework, we obtain classifiers that maximize the interclass margin. Furthermore, we show that the support vector machine is a sub-class of these maximum-margin classifiers. version:1
arxiv-1301-3895 | Dynamic Trees: A Structured Variational Method Giving Efficient Propagation Rules | http://arxiv.org/abs/1301.3895 | id:1301.3895 author:Amos J. Storkey category:cs.LG cs.AI stat.ML  published:2013-01-16 summary:Dynamic trees are mixtures of tree structured belief networks. They solve some of the problems of fixed tree networks at the cost of making exact inference intractable. For this reason approximate methods such as sampling or mean field approaches have been used. However, mean field approximations assume a factorized distribution over node states. Such a distribution seems unlickely in the posterior, as nodes are highly correlated in the prior. Here a structured variational approach is used, where the posterior distribution over the non-evidential nodes is itself approximated by a dynamic tree. It turns out that this form can be used tractably and efficiently. The result is a set of update rules which can propagate information through the network to obtain both a full variational approximation, and the relevant marginals. The progagtion rules are more efficient than the mean field approach and give noticeable quantitative and qualitative improvement in the inference. The marginals calculated give better approximations to the posterior than loopy propagation on a small toy problem. version:1
arxiv-1301-3891 | Combining Feature and Prototype Pruning by Uncertainty Minimization | http://arxiv.org/abs/1301.3891 | id:1301.3891 author:Marc Sebban, Richard Nock category:cs.LG stat.ML  published:2013-01-16 summary:We focus in this paper on dataset reduction techniques for use in k-nearest neighbor classification. In such a context, feature and prototype selections have always been independently treated by the standard storage reduction algorithms. While this certifying is theoretically justified by the fact that each subproblem is NP-hard, we assume in this paper that a joint storage reduction is in fact more intuitive and can in practice provide better results than two independent processes. Moreover, it avoids a lot of distance calculations by progressively removing useless instances during the feature pruning. While standard selection algorithms often optimize the accuracy to discriminate the set of solutions, we use in this paper a criterion based on an uncertainty measure within a nearest-neighbor graph. This choice comes from recent results that have proven that accuracy is not always the suitable criterion to optimize. In our approach, a feature or an instance is removed if its deletion improves information of the graph. Numerous experiments are presented in this paper and a statistical analysis shows the relevance of our approach, and its tolerance in the presence of noise. version:1
arxiv-1301-3890 | Monte Carlo Inference via Greedy Importance Sampling | http://arxiv.org/abs/1301.3890 | id:1301.3890 author:Dale Schuurmans, Finnegan Southey category:cs.LG stat.CO stat.ML  published:2013-01-16 summary:We present a new method for conducting Monte Carlo inference in graphical models which combines explicit search with generalized importance sampling. The idea is to reduce the variance of importance sampling by searching for significant points in the target distribution. We prove that it is possible to introduce search and still maintain unbiasedness. We then demonstrate our procedure on a few simple inference tasks and show that it can improve the inference quality of standard MCMC methods, including Gibbs sampling, Metropolis sampling, and Hybrid Monte Carlo. This paper extends previous work which showed how greedy importance sampling could be correctly realized in the one-dimensional case. version:1
arxiv-1301-3882 | Adaptive Importance Sampling for Estimation in Structured Domains | http://arxiv.org/abs/1301.3882 | id:1301.3882 author:Luis E. Ortiz, Leslie Pack Kaelbling category:cs.AI cs.LG stat.ML  published:2013-01-16 summary:Sampling is an important tool for estimating large, complex sums and integrals over high dimensional spaces. For instance, important sampling has been used as an alternative to exact methods for inference in belief networks. Ideally, we want to have a sampling distribution that provides optimal-variance estimators. In this paper, we present methods that improve the sampling distribution by systematically adapting it as we obtain information from the samples. We present a stochastic-gradient-descent method for sequentially updating the sampling distribution based on the direct minization of the variance. We also present other stochastic-gradient-descent methods based on the minimizationof typical notions of distance between the current sampling distribution and approximations of the target, optimal distribution. We finally validate and compare the different methods empirically by applying them to the problem of action evaluation in influence diagrams. version:1
arxiv-1301-3878 | PEGASUS: A Policy Search Method for Large MDPs and POMDPs | http://arxiv.org/abs/1301.3878 | id:1301.3878 author:Andrew Y. Ng, Michael I. Jordan category:cs.AI cs.LG  published:2013-01-16 summary:We propose a new approach to the problem of searching a space of policies for a Markov decision process (MDP) or a partially observable Markov decision process (POMDP), given a model. Our approach is based on the following observation: Any (PO)MDP can be transformed into an "equivalent" POMDP in which all state transitions (given the current state and action) are deterministic. This reduces the general problem of policy search to one in which we need only consider POMDPs with deterministic transitions. We give a natural way of estimating the value of all policies in these transformed POMDPs. Policy search is then simply performed by searching for a policy with high estimated value. We also establish conditions under which our value estimates will be good, recovering theoretical results similar to those of Kearns, Mansour and Ng (1999), but with "sample complexity" bounds that have only a polynomial rather than exponential dependence on the horizon time. Our method applies to arbitrary POMDPs, including ones with infinite state and action spaces. We also present empirical results for our approach on a small discrete problem, and on a complex continuous state/continuous action problem involving learning to ride a bicycle. version:1
arxiv-1301-3877 | The Anchors Hierachy: Using the triangle inequality to survive high dimensional data | http://arxiv.org/abs/1301.3877 | id:1301.3877 author:Andrew Moore category:cs.LG cs.DS stat.ML  published:2013-01-16 summary:This paper is about metric data structures in high-dimensional or non-Euclidean space that permit cached sufficient statistics accelerations of learning algorithms. It has recently been shown that for less than about 10 dimensions, decorating kd-trees with additional "cached sufficient statistics" such as first and second moments and contingency tables can provide satisfying acceleration for a very wide range of statistical learning tasks such as kernel regression, locally weighted regression, k-means clustering, mixture modeling and Bayes Net learning. In this paper, we begin by defining the anchors hierarchy - a fast data structure and algorithm for localizing data based only on a triangle-inequality-obeying distance metric. We show how this, in its own right, gives a fast and effective clustering of data. But more importantly we show how it can produce a well-balanced structure similar to a Ball-Tree (Omohundro, 1991) or a kind of metric tree (Uhlmann, 1991; Ciaccia, Patella, & Zezula, 1997) in a way that is neither "top-down" nor "bottom-up" but instead "middle-out". We then show how this structure, decorated with cached sufficient statistics, allows a wide variety of statistical learning algorithms to be accelerated even in thousands of dimensions. version:1
arxiv-1301-3875 | Tractable Bayesian Learning of Tree Belief Networks | http://arxiv.org/abs/1301.3875 | id:1301.3875 author:Marina Meila, Tommi S. Jaakkola category:cs.LG cs.AI stat.ML  published:2013-01-16 summary:In this paper we present decomposable priors, a family of priors over structure and parameters of tree belief nets for which Bayesian learning with complete observations is tractable, in the sense that the posterior is also decomposable and can be completely determined analytically in polynomial time. This follows from two main results: First, we show that factored distributions over spanning trees in a graph can be integrated in closed form. Second, we examine priors over tree parameters and show that a set of assumptions similar to (Heckerman and al. 1995) constrain the tree parameter priors to be a compactly parameterized product of Dirichlet distributions. Beside allowing for exact Bayesian learning, these results permit us to formulate a new class of tractable latent variable models in which the likelihood of a data point is computed through an ensemble average over tree structures. version:1
arxiv-1301-3865 | Feature Selection and Dualities in Maximum Entropy Discrimination | http://arxiv.org/abs/1301.3865 | id:1301.3865 author:Tony S. Jebara, Tommi S. Jaakkola category:cs.LG stat.ML  published:2013-01-16 summary:Incorporating feature selection into a classification or regression method often carries a number of advantages. In this paper we formalize feature selection specifically from a discriminative perspective of improving classification/regression accuracy. The feature selection method is developed as an extension to the recently proposed maximum entropy discrimination (MED) framework. We describe MED as a flexible (Bayesian) regularization approach that subsumes, e.g., support vector classification, regression and exponential family models. For brevity, we restrict ourselves primarily to feature selection in the context of linear classification/regression methods and demonstrate that the proposed approach indeed carries substantial improvements in practice. Moreover, we discuss and develop various extensions of feature selection, including the problem of dealing with example specific but unobserved degrees of freedom -- alignments or invariants. version:1
arxiv-1301-3862 | Dependency Networks for Collaborative Filtering and Data Visualization | http://arxiv.org/abs/1301.3862 | id:1301.3862 author:David Heckerman, David Maxwell Chickering, Christopher Meek, Robert Rounthwaite, Carl Kadie category:cs.AI cs.IR cs.LG  published:2013-01-16 summary:We describe a graphical model for probabilistic relationships---an alternative to the Bayesian network---called a dependency network. The graph of a dependency network, unlike a Bayesian network, is potentially cyclic. The probability component of a dependency network, like a Bayesian network, is a set of conditional distributions, one for each node given its parents. We identify several basic properties of this representation and describe a computationally efficient procedure for learning the graph and probability components from data. We describe the application of this representation to probabilistic inference, collaborative filtering (the task of predicting preferences), and the visualization of acausal predictive relationships. version:1
arxiv-1301-3861 | Inference for Belief Networks Using Coupling From the Past | http://arxiv.org/abs/1301.3861 | id:1301.3861 author:Michael Harvey, Radford M. Neal category:cs.AI cs.LG  published:2013-01-16 summary:Inference for belief networks using Gibbs sampling produces a distribution for unobserved variables that differs from the correct distribution by a (usually) unknown error, since convergence to the right distribution occurs only asymptotically. The method of "coupling from the past" samples from exactly the correct distribution by (conceptually) running dependent Gibbs sampling simulations from every possible starting state from a time far enough in the past that all runs reach the same state at time t=0. Explicitly considering every possible state is intractable for large networks, however. We propose a method for layered noisy-or networks that uses a compact, but often imprecise, summary of a set of states. This method samples from exactly the correct distribution, and requires only about twice the time per step as ordinary Gibbs sampling, but it may require more simulation steps than would be needed if chains were tracked exactly. version:1
arxiv-1301-3857 | Gaussian Process Networks | http://arxiv.org/abs/1301.3857 | id:1301.3857 author:Nir Friedman, Iftach Nachman category:cs.AI cs.LG stat.ML  published:2013-01-16 summary:In this paper we address the problem of learning the structure of a Bayesian network in domains with continuous variables. This task requires a procedure for comparing different candidate structures. In the Bayesian framework, this is done by evaluating the {em marginal likelihood/} of the data given a candidate structure. This term can be computed in closed-form for standard parametric families (e.g., Gaussians), and can be approximated, at some computational cost, for some semi-parametric families (e.g., mixtures of Gaussians). We present a new family of continuous variable probabilistic networks that are based on {em Gaussian Process/} priors. These priors are semi-parametric in nature and can learn almost arbitrary noisy functional relations. Using these priors, we can directly compute marginal likelihoods for structure learning. The resulting method can discover a wide range of functional dependencies in multivariate data. We develop the Bayesian score of Gaussian Process Networks and describe how to learn them from data. We present empirical results on artificial data as well as on real-life domains with non-linear dependencies. version:1
arxiv-1301-3856 | Being Bayesian about Network Structure | http://arxiv.org/abs/1301.3856 | id:1301.3856 author:Nir Friedman, Daphne Koller category:cs.LG cs.AI stat.ML  published:2013-01-16 summary:In many domains, we are interested in analyzing the structure of the underlying distribution, e.g., whether one variable is a direct parent of the other. Bayesian model-selection attempts to find the MAP model and use its structure to answer these questions. However, when the amount of available data is modest, there might be many models that have non-negligible posterior. Thus, we want compute the Bayesian posterior of a feature, i.e., the total posterior probability of all models that contain it. In this paper, we propose a new approach for this task. We first show how to efficiently compute a sum over the exponential number of networks that are consistent with a fixed ordering over network variables. This allows us to compute, for a given ordering, both the marginal probability of the data and the posterior of a feature. We then use this result as the basis for an algorithm that approximates the Bayesian posterior of a feature. Our approach uses a Markov Chain Monte Carlo (MCMC) method, but over orderings rather than over network structures. The space of orderings is much smaller and more regular than the space of structures, and has a smoother posterior `landscape'. We present empirical results on synthetic and real-life datasets that compare our approach to full model averaging (when possible), to MCMC over network structures, and to a non-Bayesian bootstrap approach. version:1
arxiv-1301-3854 | Learning Graphical Models of Images, Videos and Their Spatial Transformations | http://arxiv.org/abs/1301.3854 | id:1301.3854 author:Brendan J. Frey, Nebojsa Jojic category:cs.CV cs.LG stat.ML  published:2013-01-16 summary:Mixtures of Gaussians, factor analyzers (probabilistic PCA) and hidden Markov models are staples of static and dynamic data modeling and image and video modeling in particular. We show how topographic transformations in the input, such as translation and shearing in images, can be accounted for in these models by including a discrete transformation variable. The resulting models perform clustering, dimensionality reduction and time-series analysis in a way that is invariant to transformations in the input. Using the EM algorithm, these transformation-invariant models can be fit to static data and time series. We give results on filtering microscopy images, face and facial pose clustering, handwritten digit modeling and recognition, video clustering, object tracking, and removal of distractions from video sequences. version:1
arxiv-1301-3853 | Rao-Blackwellised Particle Filtering for Dynamic Bayesian Networks | http://arxiv.org/abs/1301.3853 | id:1301.3853 author:Arnaud Doucet, Nando de Freitas, Kevin Murphy, Stuart Russell category:cs.LG cs.AI stat.CO  published:2013-01-16 summary:Particle filters (PFs) are powerful sampling-based inference/learning algorithms for dynamic Bayesian networks (DBNs). They allow us to treat, in a principled way, any type of probability distribution, nonlinearity and non-stationarity. They have appeared in several fields under such names as "condensation", "sequential Monte Carlo" and "survival of the fittest". In this paper, we show how we can exploit the structure of the DBN to increase the efficiency of particle filtering, using a technique known as Rao-Blackwellisation. Essentially, this samples some of the variables, and marginalizes out the rest exactly, using the Kalman filter, HMM filter, junction tree algorithm, or any other finite dimensional optimal filter. We show that Rao-Blackwellised particle filters (RBPFs) lead to more accurate estimates than standard PFs. We demonstrate RBPFs on two problems, namely non-stationary online regression with radial basis function networks and robot localization and map building. We also discuss other potential application areas and provide references to some finite dimensional optimal filters. version:1
arxiv-1301-3852 | Mix-nets: Factored Mixtures of Gaussians in Bayesian Networks With Mixed Continuous And Discrete Variables | http://arxiv.org/abs/1301.3852 | id:1301.3852 author:Scott Davies, Andrew Moore category:cs.LG cs.AI stat.ML  published:2013-01-16 summary:Recently developed techniques have made it possible to quickly learn accurate probability density functions from data in low-dimensional continuous space. In particular, mixtures of Gaussians can be fitted to data very quickly using an accelerated EM algorithm that employs multiresolution kd-trees (Moore, 1999). In this paper, we propose a kind of Bayesian networks in which low-dimensional mixtures of Gaussians over different subsets of the domain's variables are combined into a coherent joint probability model over the entire domain. The network is also capable of modeling complex dependencies between discrete variables and continuous variables without requiring discretization of the continuous variables. We present efficient heuristic algorithms for automatically learning these networks from data, and perform comparative experiments illustrated how well these networks model real scientific data and synthetic data. We also briefly discuss some possible improvements to the networks, as well as possible applications. version:1
arxiv-1301-3851 | Minimum Message Length Clustering Using Gibbs Sampling | http://arxiv.org/abs/1301.3851 | id:1301.3851 author:Ian Davidson category:cs.LG stat.ML  published:2013-01-16 summary:The K-Mean and EM algorithms are popular in clustering and mixture modeling, due to their simplicity and ease of implementation. However, they have several significant limitations. Both coverage to a local optimum of their respective objective functions (ignoring the uncertainty in the model space), require the apriori specification of the number of classes/clsuters, and are inconsistent. In this work we overcome these limitations by using the Minimum Message Length (MML) principle and a variation to the K-Means/EM observation assignment and parameter calculation scheme. We maintain the simplicity of these approaches while constructing a Bayesian mixture modeling tool that samples/searches the model space using a Markov Chain Monte Carlo (MCMC) sampler known as a Gibbs sampler. Gibbs sampling allows us to visit each model according to its posterior probability. Therefore, if the model space is multi-modal we will visit all models and not get stuck in local optima. We call our approach multiple chains at equilibrium (MCE) MML sampling. version:1
arxiv-1301-3850 | A Two-round Variant of EM for Gaussian Mixtures | http://arxiv.org/abs/1301.3850 | id:1301.3850 author:Sanjoy Dasgupta, Leonard Schulman category:cs.LG stat.ML  published:2013-01-16 summary:Given a set of possible models (e.g., Bayesian network structures) and a data sample, in the unsupervised model selection problem the task is to choose the most accurate model with respect to the domain joint probability distribution. In contrast to this, in supervised model selection it is a priori known that the chosen model will be used in the future for prediction tasks involving more ``focused' predictive distributions. Although focused predictive distributions can be produced from the joint probability distribution by marginalization, in practice the best model in the unsupervised sense does not necessarily perform well in supervised domains. In particular, the standard marginal likelihood score is a criterion for the unsupervised task, and, although frequently used for supervised model selection also, does not perform well in such tasks. In this paper we study the performance of the marginal likelihood score empirically in supervised Bayesian network selection tasks by using a large number of publicly available classification data sets, and compare the results to those obtained by alternative model selection criteria, including empirical crossvalidation methods, an approximation of a supervised marginal likelihood measure, and a supervised version of Dawids prequential(predictive sequential) principle.The results demonstrate that the marginal likelihood score does NOT perform well FOR supervised model selection, WHILE the best results are obtained BY using Dawids prequential r napproach. version:1
arxiv-1301-3849 | Experiments with Random Projection | http://arxiv.org/abs/1301.3849 | id:1301.3849 author:Sanjoy Dasgupta category:cs.LG stat.ML  published:2013-01-16 summary:Recent theoretical work has identified random projection as a promising dimensionality reduction technique for learning mixtures of Gausians. Here we summarize these results and illustrate them by a wide variety of experiments on synthetic and real data. version:1
arxiv-1301-3843 | Bayesian Classification and Feature Selection from Finite Data Sets | http://arxiv.org/abs/1301.3843 | id:1301.3843 author:Frans Coetzee, Steve Lawrence, C. Lee Giles category:cs.LG stat.ML  published:2013-01-16 summary:Feature selection aims to select the smallest subset of features for a specified level of performance. The optimal achievable classification performance on a feature subset is summarized by its Receiver Operating Curve (ROC). When infinite data is available, the Neyman- Pearson (NP) design procedure provides the most efficient way of obtaining this curve. In practice the design procedure is applied to density estimates from finite data sets. We perform a detailed statistical analysis of the resulting error propagation on finite alphabets. We show that the estimated performance curve (EPC) produced by the design procedure is arbitrarily accurate given sufficient data, independent of the size of the feature set. However, the underlying likelihood ranking procedure is highly sensitive to errors that reduces the probability that the EPC is in fact the ROC. In the worst case, guaranteeing that the EPC is equal to the ROC may require data sizes exponential in the size of the feature set. These results imply that in theory the NP design approach may only be valid for characterizing relatively small feature subsets, even when the performance of any given classifier can be estimated very accurately. We discuss the practical limitations for on-line methods that ensures that the NP procedure operates in a statistically valid region. version:1
arxiv-1301-3840 | Utilities as Random Variables: Density Estimation and Structure Discovery | http://arxiv.org/abs/1301.3840 | id:1301.3840 author:Urszula Chajewska, Daphne Koller category:cs.AI cs.LG  published:2013-01-16 summary:Decision theory does not traditionally include uncertainty over utility functions. We argue that the a person's utility value for a given outcome can be treated as we treat other domain attributes: as a random variable with a density function over its possible values. We show that we can apply statistical density estimation techniques to learn such a density function from a database of partially elicited utility functions. In particular, we define a Bayesian learning framework for this problem, assuming the distribution over utilities is a mixture of Gaussians, where the mixture components represent statistically coherent subpopulations. We can also extend our techniques to the problem of discovering generalized additivity structure in the utility functions in the population. We define a Bayesian model selection criterion for utility function structure and a search procedure over structures. The factorization of the utilities in the learned model, and the generalization obtained from density estimation, allows us to provide robust estimates of utilities using a significantly smaller number of utility elicitation questions. We experiment with our technique on synthetic utility data and on a real database of utility functions in the domain of prenatal diagnosis. version:1
arxiv-1301-3838 | Variational Relevance Vector Machines | http://arxiv.org/abs/1301.3838 | id:1301.3838 author:Christopher M. Bishop, Michael Tipping category:cs.LG stat.ML  published:2013-01-16 summary:The Support Vector Machine (SVM) of Vapnik (1998) has become widely established as one of the leading approaches to pattern recognition and machine learning. It expresses predictions in terms of a linear combination of kernel functions centred on a subset of the training data, known as support vectors. Despite its widespread success, the SVM suffers from some important limitations, one of the most significant being that it makes point predictions rather than generating predictive distributions. Recently Tipping (1999) has formulated the Relevance Vector Machine (RVM), a probabilistic model whose functional form is equivalent to the SVM. It achieves comparable recognition accuracy to the SVM, yet provides a full predictive distribution, and also requires substantially fewer kernel functions. The original treatment of the RVM relied on the use of type II maximum likelihood (the `evidence framework') to provide point estimates of the hyperparameters which govern model sparsity. In this paper we show how the RVM can be formulated and solved within a completely Bayesian paradigm through the use of variational inference, thereby giving a posterior distribution over both parameters and hyperparameters. We demonstrate the practicality and performance of the variational RVM using both synthetic and real world examples. version:1
arxiv-1301-3837 | Dynamic Bayesian Multinets | http://arxiv.org/abs/1301.3837 | id:1301.3837 author:Jeff A. Bilmes category:cs.LG cs.AI stat.ML  published:2013-01-16 summary:In this work, dynamic Bayesian multinets are introduced where a Markov chain state at time t determines conditional independence patterns between random variables lying within a local time window surrounding t. It is shown how information-theoretic criterion functions can be used to induce sparse, discriminative, and class-conditional network structures that yield an optimal approximation to the class posterior probability, and therefore are useful for the classification task. Using a new structure learning heuristic, the resulting models are tested on a medium-vocabulary isolated-word speech recognition task. It is demonstrated that these discriminatively structured dynamic Bayesian multinets, when trained in a maximum likelihood setting using EM, can outperform both HMMs and other dynamic Bayesian networks with a similar number of parameters. version:1
arxiv-1301-3833 | Reversible Jump MCMC Simulated Annealing for Neural Networks | http://arxiv.org/abs/1301.3833 | id:1301.3833 author:Christophe Andrieu, Nando de Freitas, Arnaud Doucet category:cs.LG cs.NE stat.ML  published:2013-01-16 summary:We propose a novel reversible jump Markov chain Monte Carlo (MCMC) simulated annealing algorithm to optimize radial basis function (RBF) networks. This algorithm enables us to maximize the joint posterior distribution of the network parameters and the number of basis functions. It performs a global search in the joint space of the parameters and number of parameters, thereby surmounting the problem of local minima. We also show that by calibrating a Bayesian model, we can obtain the classical AIC, BIC and MDL model selection criteria within a penalized likelihood framework. Finally, we show theoretically and empirically that the algorithm converges to the modes of the full posterior distribution in an efficient way. version:1
arxiv-1301-3385 | Recurrent Online Clustering as a Spatio-Temporal Feature Extractor in DeSTIN | http://arxiv.org/abs/1301.3385 | id:1301.3385 author:Steven R. Young, Itamar Arel category:cs.CV  published:2013-01-15 summary:This paper presents a basic enhancement to the DeSTIN deep learning architecture by replacing the explicitly calculated transition tables that are used to capture temporal features with a simpler, more scalable mechanism. This mechanism uses feedback of state information to cluster over a space comprised of both the spatial input and the current state. The resulting architecture achieves state-of-the-art results on the MNIST classification benchmark. version:2
arxiv-1301-3644 | Regularized Discriminant Embedding for Visual Descriptor Learning | http://arxiv.org/abs/1301.3644 | id:1301.3644 author:Kye-Hyeon Kim, Rui Cai, Lei Zhang, Seungjin Choi category:cs.CV cs.LG  published:2013-01-16 summary:Images can vary according to changes in viewpoint, resolution, noise, and illumination. In this paper, we aim to learn representations for an image, which are robust to wide changes in such environmental conditions, using training pairs of matching and non-matching local image patches that are collected under various environmental conditions. We present a regularized discriminant analysis that emphasizes two challenging categories among the given training pairs: (1) matching, but far apart pairs and (2) non-matching, but close pairs in the original feature space (e.g., SIFT feature space). Compared to existing work on metric learning and discriminant analysis, our method can better distinguish relevant images from irrelevant, but look-alike images. version:1
arxiv-1203-0453 | Change-Point Detection in Time-Series Data by Relative Density-Ratio Estimation | http://arxiv.org/abs/1203.0453 | id:1203.0453 author:Song Liu, Makoto Yamada, Nigel Collier, Masashi Sugiyama category:stat.ML cs.LG stat.ME  published:2012-03-02 summary:The objective of change-point detection is to discover abrupt property changes lying behind time-series data. In this paper, we present a novel statistical change-point detection algorithm based on non-parametric divergence estimation between time-series samples from two retrospective segments. Our method uses the relative Pearson divergence as a divergence measure, and it is accurately and efficiently estimated by a method of direct density-ratio estimation. Through experiments on artificial and real-world datasets including human-activity sensing, speech, and Twitter messages, we demonstrate the usefulness of the proposed method. version:2
arxiv-1301-3590 | Tree structured sparse coding on cubes | http://arxiv.org/abs/1301.3590 | id:1301.3590 author:Arthur Szlam category:cs.IT cs.CV math.IT  published:2013-01-16 summary:A brief description of tree structured sparse coding on the binary cube. version:1
arxiv-1301-3575 | Kernelized Locality-Sensitive Hashing for Semi-Supervised Agglomerative Clustering | http://arxiv.org/abs/1301.3575 | id:1301.3575 author:Boyi Xie, Shuheng Zheng category:cs.LG cs.CV stat.ML  published:2013-01-16 summary:Large scale agglomerative clustering is hindered by computational burdens. We propose a novel scheme where exact inter-instance distance calculation is replaced by the Hamming distance between Kernelized Locality-Sensitive Hashing (KLSH) hashed values. This results in a method that drastically decreases computation time. Additionally, we take advantage of certain labeled data points via distance metric learning to achieve a competitive precision and recall comparing to K-Means but in much less computation time. version:1
arxiv-1301-3570 | A Nested HDP for Hierarchical Topic Models | http://arxiv.org/abs/1301.3570 | id:1301.3570 author:John Paisley, Chong Wang, David Blei, Michael I. Jordan category:stat.ML  published:2013-01-16 summary:We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP is a generalization of the nested Chinese restaurant process (nCRP) that allows each word to follow its own path to a topic node according to a document-specific distribution on a shared tree. This alleviates the rigid, single-path formulation of the nCRP, allowing a document to more easily express thematic borrowings as a random effect. We demonstrate our algorithm on 1.8 million documents from The New York Times. version:1
arxiv-1211-1482 | Gender Recognition in Walk Gait through 3D Motion by Quadratic Bezier Curve and Statistical Techniques | http://arxiv.org/abs/1211.1482 | id:1211.1482 author:Sajid Ali category:cs.CV  published:2012-11-07 summary:Motion capture is the process of recording the movement of objects or people. It is used in military, entertainment, sports, and medical applications, and for validation of computer vision[2] and robotics. In filmmaking and video game development, it refers to recording actions of human actors, and using that information to animate digital character models in 2D or 3D computer animation. When it includes face and fingers or captures subtle version:4
arxiv-1301-3560 | Complexity of Representation and Inference in Compositional Models with Part Sharing | http://arxiv.org/abs/1301.3560 | id:1301.3560 author:Alan L. Yuille, Roozbeh Mottaghi category:cs.CV  published:2013-01-16 summary:This paper describes serial and parallel compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the complexity of this model in terms of computation time (for serial computers) and numbers of nodes (e.g., "neurons") for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers. version:1
arxiv-1301-3558 | Model Selection for Gaussian Mixture Models | http://arxiv.org/abs/1301.3558 | id:1301.3558 author:Tao Huang, Heng Peng, Kun Zhang category:stat.ME math.ST stat.ML stat.TH  published:2013-01-16 summary:This paper is concerned with an important issue in finite mixture modelling, the selection of the number of mixing components. We propose a new penalized likelihood method for model selection of finite multivariate Gaussian mixture models. The proposed method is shown to be statistically consistent in determining of the number of components. A modified EM algorithm is developed to simultaneously select the number of components and to estimate the mixing weights, i.e. the mixing probabilities, and unknown parameters of Gaussian distributions. Simulations and a real data analysis are presented to illustrate the performance of the proposed method. version:1
arxiv-1301-3557 | Stochastic Pooling for Regularization of Deep Convolutional Neural Networks | http://arxiv.org/abs/1301.3557 | id:1301.3557 author:Matthew D. Zeiler, Rob Fergus category:cs.LG cs.NE stat.ML  published:2013-01-16 summary:We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation. version:1
arxiv-1301-3547 | A Rhetorical Analysis Approach to Natural Language Processing | http://arxiv.org/abs/1301.3547 | id:1301.3547 author:Benjamin Englard category:cs.CL stat.ML  published:2013-01-16 summary:The goal of this research was to find a way to extend the capabilities of computers through the processing of language in a more human way, and present applications which demonstrate the power of this method. This research presents a novel approach, Rhetorical Analysis, to solving problems in Natural Language Processing (NLP). The main benefit of Rhetorical Analysis, as opposed to previous approaches, is that it does not require the accumulation of large sets of training data, but can be used to solve a multitude of problems within the field of NLP. The NLP problems investigated with Rhetorical Analysis were the Author Identification problem - predicting the author of a piece of text based on its rhetorical strategies, Election Prediction - predicting the winner of a presidential candidate's re-election campaign based on rhetorical strategies within that president's inaugural address, Natural Language Generation - having a computer produce text containing rhetorical strategies, and Document Summarization. The results of this research indicate that an Author Identification system based on Rhetorical Analysis could predict the correct author 100% of the time, that a re-election predictor based on Rhetorical Analysis could predict the correct winner of a re-election campaign 55% of the time, that a Natural Language Generation system based on Rhetorical Analysis could output text with up to 87.3% similarity to Shakespeare in style, and that a Document Summarization system based on Rhetorical Analysis could extract highly relevant sentences. Overall, this study demonstrated that Rhetorical Analysis could be a useful approach to solving problems in NLP. version:1
arxiv-1301-3539 | Learning Features with Structure-Adapting Multi-view Exponential Family Harmoniums | http://arxiv.org/abs/1301.3539 | id:1301.3539 author:Yoonseop Kang, Seungjin Choi category:cs.LG  published:2013-01-16 summary:We proposea graphical model for multi-view feature extraction that automatically adapts its structure to achieve better representation of data distribution. The proposed model, structure-adapting multi-view harmonium (SA-MVH) has switch parameters that control the connection between hidden nodes and input views, and learn the switch parameter while training. Numerical experiments on synthetic and a real-world dataset demonstrate the useful behavior of the SA-MVH, compared to existing multi-view feature extraction methods. version:1
arxiv-1301-3528 | An Efficient Sufficient Dimension Reduction Method for Identifying Genetic Variants of Clinical Significance | http://arxiv.org/abs/1301.3528 | id:1301.3528 author:Momiao Xiong, Long Ma category:q-bio.GN cs.LG stat.ML  published:2013-01-15 summary:Fast and cheaper next generation sequencing technologies will generate unprecedentedly massive and highly-dimensional genomic and epigenomic variation data. In the near future, a routine part of medical record will include the sequenced genomes. A fundamental question is how to efficiently extract genomic and epigenomic variants of clinical utility which will provide information for optimal wellness and interference strategies. Traditional paradigm for identifying variants of clinical validity is to test association of the variants. However, significantly associated genetic variants may or may not be usefulness for diagnosis and prognosis of diseases. Alternative to association studies for finding genetic variants of predictive utility is to systematically search variants that contain sufficient information for phenotype prediction. To achieve this, we introduce concepts of sufficient dimension reduction and coordinate hypothesis which project the original high dimensional data to very low dimensional space while preserving all information on response phenotypes. We then formulate clinically significant genetic variant discovery problem into sparse SDR problem and develop algorithms that can select significant genetic variants from up to or even ten millions of predictors with the aid of dividing SDR for whole genome into a number of subSDR problems defined for genomic regions. The sparse SDR is in turn formulated as sparse optimal scoring problem, but with penalty which can remove row vectors from the basis matrix. To speed up computation, we develop the modified alternating direction method for multipliers to solve the sparse optimal scoring problem which can easily be implemented in parallel. To illustrate its application, the proposed method is applied to simulation data and the NHLBI's Exome Sequencing Project dataset version:1
arxiv-1301-3524 | How good is the Electricity benchmark for evaluating concept drift adaptation | http://arxiv.org/abs/1301.3524 | id:1301.3524 author:Indre Zliobaite category:cs.LG  published:2013-01-15 summary:In this correspondence, we will point out a problem with testing adaptive classifiers on autocorrelated data. In such a case random change alarms may boost the accuracy figures. Hence, we cannot be sure if the adaptation is working well. version:1
arxiv-1301-3514 | Anomaly Classification with the Anti-Profile Support Vector Machine | http://arxiv.org/abs/1301.3514 | id:1301.3514 author:Wikum Dinalankara, Hector Corrada Bravo category:stat.ML q-bio.GN  published:2013-01-15 summary:We introduce the anti-profile Support Vector Machine (apSVM) as a novel algorithm to address the anomaly classification problem, an extension of anomaly detection where the goal is to distinguish data samples from a number of anomalous and heterogeneous classes based on their pattern of deviation from a normal stable class. We show that under heterogeneity assumptions defined here that the apSVM can be solved as the dual of a standard SVM with an indirect kernel that measures similarity of anomalous samples through similarity to the stable normal class. We characterize this indirect kernel as the inner product in a Reproducing Kernel Hilbert Space between representers that are projected to the subspace spanned by the representers of the normal samples. We show by simulation and application to cancer genomics datasets that the anti-profile SVM produces classifiers that are more accurate and stable than the standard SVM in the anomaly classification setting. version:1
arxiv-1302-5056 | Pooling-Invariant Image Feature Learning | http://arxiv.org/abs/1302.5056 | id:1302.5056 author:Yangqing Jia, Oriol Vinyals, Trevor Darrell category:cs.CV cs.LG  published:2013-01-15 summary:Unsupervised dictionary learning has been a key component in state-of-the-art computer vision recognition architectures. While highly effective methods exist for patch-based dictionary learning, these methods may learn redundant features after the pooling stage in a given early vision architecture. In this paper, we offer a novel dictionary learning scheme to efficiently take into account the invariance of learned features after the spatial pooling stage. The algorithm is built on simple clustering, and thus enjoys efficiency and scalability. We discuss the underlying mechanism that justifies the use of clustering algorithms, and empirically show that the algorithm finds better dictionaries than patch-based methods with the same dictionary size. version:1
arxiv-1301-3347 | Multi-agent learning using Fictitious Play and Extended Kalman Filter | http://arxiv.org/abs/1301.3347 | id:1301.3347 author:Michalis Smyrnakis category:cs.MA cs.LG math.OC stat.ML  published:2013-01-15 summary:Decentralised optimisation tasks are important components of multi-agent systems. These tasks can be interpreted as n-player potential games: therefore game-theoretic learning algorithms can be used to solve decentralised optimisation tasks. Fictitious play is the canonical example of these algorithms. Nevertheless fictitious play implicitly assumes that players have stationary strategies. We present a novel variant of fictitious play where players predict their opponents' strategies using Extended Kalman filters and use their predictions to update their strategies. We show that in 2 by 2 games with at least one pure Nash equilibrium and in potential games where players have two available actions, the proposed algorithm converges to the pure Nash equilibrium. The performance of the proposed algorithm was empirically tested, in two strategic form games and an ad-hoc sensor network surveillance problem. The proposed algorithm performs better than the classic fictitious play algorithm in these games and therefore improves the performance of game-theoretical learning in decentralised optimisation. version:1
arxiv-1301-3214 | The Manifold of Human Emotions | http://arxiv.org/abs/1301.3214 | id:1301.3214 author:Seungyeon Kim, Fuxin Li, Guy Lebanon, Irfan Essa category:cs.CL  published:2013-01-15 summary:Sentiment analysis predicts the presence of positive or negative emotions in a text document. In this paper, we consider higher dimensional extensions of the sentiment concept, which represent a richer set of human emotions. Our approach goes beyond previous work in that our model contains a continuous manifold rather than a finite set of human emotions. We investigate the resulting model, compare it to psychological observations, and explore its predictive capabilities. version:1
arxiv-1301-3193 | Learning Graphical Model Parameters with Approximate Marginal Inference | http://arxiv.org/abs/1301.3193 | id:1301.3193 author:Justin Domke category:cs.LG cs.CV I.2.6; I.4.8  published:2013-01-15 summary:Likelihood based-learning of graphical models faces challenges of computational-complexity and robustness to model mis-specification. This paper studies methods that fit parameters directly to maximize a measure of the accuracy of predicted marginals, taking into account both model and inference approximations at training time. Experiments on imaging problems suggest marginalization-based learning performs better than likelihood-based approximations on difficult problems where the model being fit is approximate in nature. version:1
arxiv-1301-3192 | Matrix Approximation under Local Low-Rank Assumption | http://arxiv.org/abs/1301.3192 | id:1301.3192 author:Joonseok Lee, Seungyeon Kim, Guy Lebanon, Yoram Singer category:cs.LG stat.ML I.2.6  published:2013-01-15 summary:Matrix approximation is a common tool in machine learning for building accurate prediction models for recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is only locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy in recommendation tasks. version:1
arxiv-1212-6933 | On Automation and Medical Image Interpretation, With Applications for Laryngeal Imaging | http://arxiv.org/abs/1212.6933 | id:1212.6933 author:H. J. Moukalled category:cs.CV  published:2012-12-31 summary:Indeed, these are exciting times. We are in the heart of a digital renaissance. Automation and computer technology allow engineers and scientists to fabricate processes that amalgamate quality of life. We anticipate much growth in medical image interpretation and understanding, due to the influx of computer technologies. This work should serve as a guide to introduce the reader to core themes in theoretical computer science, as well as imaging applications for understanding vocal-fold vibrations. In this work, we motivate the use of automation, review some mathematical models of computation. We present a proof of a classical problem in image analysis that cannot be automated by means of algorithms. Furthermore, discuss some applications for processing medical images of the vocal folds, and discuss some of the exhilarating directions the art of automation will take vocal-fold image interpretation and quite possibly other areas of biomedical image analysis. version:3
arxiv-1301-3078 | Fano schemes of generic intersections and machine learning | http://arxiv.org/abs/1301.3078 | id:1301.3078 author:Franz Kirly, Paul Larsen category:math.AG stat.ML  published:2013-01-14 summary:We investigate Fano schemes of conditionally generic intersections, i.e. of hypersurfaces in projective space chosen generically up to additional conditions. Via a correspondence between generic properties of algebraic varieties and events in probability spaces that occur with probability one, we use the obtained results on Fano schemes to solve a problem in machine learning. version:1
arxiv-1212-1496 | Excess risk bounds for multitask learning with trace norm regularization | http://arxiv.org/abs/1212.1496 | id:1212.1496 author:Andreas Maurer, Massimiliano Pontil category:stat.ML cs.LG  published:2012-12-06 summary:Trace norm regularization is a popular method of multitask learning. We give excess risk bounds with explicit dependence on the number of tasks, the number of examples per task and properties of the data distribution. The bounds are independent of the dimension of the input space, which may be infinite as in the case of reproducing kernel Hilbert spaces. A byproduct of the proof are bounds on the expected norm of sums of random positive semidefinite matrices with subexponential moments. version:2
arxiv-1207-4747 | Block-Coordinate Frank-Wolfe Optimization for Structural SVMs | http://arxiv.org/abs/1207.4747 | id:1207.4747 author:Simon Lacoste-Julien, Martin Jaggi, Mark Schmidt, Patrick Pletscher category:cs.LG math.OC stat.ML G.1.6; I.2.6  published:2012-07-19 summary:We propose a randomized block-coordinate variant of the classic Frank-Wolfe algorithm for convex optimization with block-separable constraints. Despite its lower iteration cost, we show that it achieves a similar convergence rate in duality gap as the full Frank-Wolfe algorithm. We also show that, when applied to the dual structural support vector machine (SVM) objective, this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods. However, unlike stochastic subgradient methods, the block-coordinate Frank-Wolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee. Our experiments indicate that this simple algorithm outperforms competing structural SVM solvers. version:4
arxiv-1301-2884 | Wavelet-based Scale Saliency | http://arxiv.org/abs/1301.2884 | id:1301.2884 author:Anh Cat Le Ngo, Kenneth Li-Minn Ang, Jasmine Kah-Phooi Seng, Guoping Qiu category:cs.CV  published:2013-01-14 summary:Both pixel-based scale saliency (PSS) and basis project methods focus on multiscale analysis of data content and structure. Their theoretical relations and practical combination are previously discussed. However, no models have ever been proposed for calculating scale saliency on basis-projected descriptors since then. This paper extend those ideas into mathematical models and implement them in the wavelet-based scale saliency (WSS). While PSS uses pixel-value descriptors, WSS treats wavelet sub-bands as basis descriptors. The paper discusses different wavelet descriptors: discrete wavelet transform (DWT), wavelet packet transform (DWPT), quaternion wavelet transform (QWT) and best basis quaternion wavelet packet transform (QWPTBB). WSS saliency maps of different descriptors are generated and compared against other saliency methods by both quantitative and quanlitative methods. Quantitative results, ROC curves, AUC values and NSS values are collected from simulations on Bruce and Kootstra image databases with human eye-tracking data as ground-truth. Furthermore, qualitative visual results of saliency maps are analyzed and compared against each other as well as eye-tracking data inclusive in the databases. version:1
arxiv-1301-2857 | SpeedRead: A Fast Named Entity Recognition Pipeline | http://arxiv.org/abs/1301.2857 | id:1301.2857 author:Rami Al-Rfou', Steven Skiena category:cs.CL  published:2013-01-14 summary:Online content analysis employs algorithmic methods to identify entities in unstructured text. Both machine learning and knowledge-base approaches lie at the foundation of contemporary named entities extraction systems. However, the progress in deploying these approaches on web-scale has been been hampered by the computational cost of NLP over massive text corpora. We present SpeedRead (SR), a named entity recognition pipeline that runs at least 10 times faster than Stanford NLP pipeline. This pipeline consists of a high performance Penn Treebank- compliant tokenizer, close to state-of-art part-of-speech (POS) tagger and knowledge-based named entity recognizer. version:1
arxiv-1301-2785 | A comparison of SVM and RVM for Document Classification | http://arxiv.org/abs/1301.2785 | id:1301.2785 author:Muhammad Rafi, Mohammad Shahid Shaikh category:cs.IR cs.LG  published:2013-01-13 summary:Document classification is a task of assigning a new unclassified document to one of the predefined set of classes. The content based document classification uses the content of the document with some weighting criteria to assign it to one of the predefined classes. It is a major task in library science, electronic document management systems and information sciences. This paper investigates document classification by using two different classification techniques (1) Support Vector Machine (SVM) and (2) Relevance Vector Machine (RVM). SVM is a supervised machine learning technique that can be used for classification task. In its basic form, SVM represents the instances of the data into space and tries to separate the distinct classes by a maximum possible wide gap (hyper plane) that separates the classes. On the other hand RVM uses probabilistic measure to define this separation space. RVM uses Bayesian inference to obtain succinct solution, thus RVM uses significantly fewer basis functions. Experimental studies on three standard text classification datasets reveal that although RVM takes more training time, its classification is much better as compared to SVM. version:1
arxiv-1301-2725 | Robust High Dimensional Sparse Regression and Matching Pursuit | http://arxiv.org/abs/1301.2725 | id:1301.2725 author:Yudong Chen, Constantine Caramanis, Shie Mannor category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH  published:2013-01-12 summary:We consider high dimensional sparse regression, and develop strategies able to deal with arbitrary -- possibly, severe or coordinated -- errors in the covariance matrix $X$. These may come from corrupted data, persistent experimental errors, or malicious respondents in surveys/recommender systems, etc. Such non-stochastic error-in-variables problems are notoriously difficult to treat, and as we demonstrate, the problem is particularly pronounced in high-dimensional settings where the primary goal is {\em support recovery} of the sparse regressor. We develop algorithms for support recovery in sparse regression, when some number $n_1$ out of $n+n_1$ total covariate/response pairs are {\it arbitrarily (possibly maliciously) corrupted}. We are interested in understanding how many outliers, $n_1$, we can tolerate, while identifying the correct support. To the best of our knowledge, neither standard outlier rejection techniques, nor recently developed robust regression algorithms (that focus only on corrupted response variables), nor recent algorithms for dealing with stochastic noise or erasures, can provide guarantees on support recovery. Perhaps surprisingly, we also show that the natural brute force algorithm that searches over all subsets of $n$ covariate/response pairs, and all subsets of possible support coordinates in order to minimize regression error, is remarkably poor, unable to correctly identify the support with even $n_1 = O(n/k)$ corrupted points, where $k$ is the sparsity. This is true even in the basic setting we consider, where all authentic measurements and noise are independent and sub-Gaussian. In this setting, we provide a simple algorithm -- no more computationally taxing than OMP -- that gives stronger performance guarantees, recovering the support with up to $n_1 = O(n/(\sqrt{k} \log p))$ corrupted points, where $p$ is the dimension of the signal to be recovered. version:1
arxiv-1202-5130 | Support Vector Regression for Right Censored Data | http://arxiv.org/abs/1202.5130 | id:1202.5130 author:Yair Goldberg, Michael R. Kosorok category:stat.ML math.ST stat.TH  published:2012-02-23 summary:We develop a unified approach for classification and regression support vector machines for data subject to right censoring. We provide finite sample bounds on the generalization error of the algorithm, prove risk consistency for a wide class of probability measures, and study the associated learning rates. We apply the general methodology to estimation of the (truncated) mean, median, quantiles, and for classification problems. We present a simulation study that demonstrates the performance of the proposed approach. version:2
arxiv-1301-2715 | Binocular disparity as an explanation for the moon illusion | http://arxiv.org/abs/1301.2715 | id:1301.2715 author:Joseph Antonides, Toshiro Kubota category:cs.CV physics.pop-ph  published:2013-01-12 summary:We present another explanation for the moon illusion, in which the moon looks larger near the horizon than near the zenith. In our model, the sky is considered a spatially contiguous and geometrically smooth surface. When an object (like the moon) breaks the contiguity of the surface, humans perceive an occlusion of the surface rather than an object appearing through a hole. Binocular vision dictates that the moon is distant, but this perception model dictates that the moon is closer than the sky. To solve the dilemma, the brain distorts the projections of the moon to increase the binocular disparity, which results in increase of the angular size of the moon. The degree of the distortion depends upon the apparent distance to the sky, which is influenced by the surrounding objects and the condition of the sky. The closer the sky appears, the stronger the illusion. At the zenith, few distance cues are present, causing difficulty with distance estimation and weakening the illusion. version:1
arxiv-1202-6504 | Learning from Distributions via Support Measure Machines | http://arxiv.org/abs/1202.6504 | id:1202.6504 author:Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, Bernhard Schlkopf category:stat.ML cs.LG  published:2012-02-29 summary:This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion. To accomplish this, we construct a generalization of the support vector machine (SVM) called a support measure machine (SMM). Our analyses of SMMs provides several insights into their relationship to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-SVM) that places different kernel functions on each training example. Experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework. version:2
arxiv-1301-2659 | A Triclustering Approach for Time Evolving Graphs | http://arxiv.org/abs/1301.2659 | id:1301.2659 author:Romain Guigours, Marc Boull, Fabrice Rossi category:cs.LG cs.SI stat.ML  published:2013-01-12 summary:This paper introduces a novel technique to track structures in time evolving graphs. The method is based on a parameter free approach for three-dimensional co-clustering of the source vertices, the target vertices and the time. All these features are simultaneously segmented in order to build time segments and clusters of vertices whose edge distributions are similar and evolve in the same way over the time segments. The main novelty of this approach lies in that the time segments are directly inferred from the evolution of the edge distribution between the vertices, thus not requiring the user to make an a priori discretization. Experiments conducted on a synthetic dataset illustrate the good behaviour of the technique, and a study of a real-life dataset shows the potential of the proposed approach for exploratory data analysis. version:1
arxiv-1301-2656 | Multiple functional regression with both discrete and continuous covariates | http://arxiv.org/abs/1301.2656 | id:1301.2656 author:Hachem Kadri, Philippe Preux, Emmanuel Duflos, Stphane Canu category:stat.ML cs.LG  published:2013-01-12 summary:In this paper we present a nonparametric method for extending functional regression methodology to the situation where more than one functional covariate is used to predict a functional response. Borrowing the idea from Kadri et al. (2010a), the method, which support mixed discrete and continuous explanatory variables, is based on estimating a function-valued function in reproducing kernel Hilbert spaces by virtue of positive operator-valued kernels. version:1
arxiv-1301-2655 | Functional Regularized Least Squares Classi cation with Operator-valued Kernels | http://arxiv.org/abs/1301.2655 | id:1301.2655 author:Hachem Kadri, Asma Rabaoui, Philippe Preux, Emmanuel Duflos, Alain Rakotomamonjy category:cs.LG stat.ML  published:2013-01-12 summary:Although operator-valued kernels have recently received increasing interest in various machine learning and functional data analysis problems such as multi-task learning or functional regression, little attention has been paid to the understanding of their associated feature spaces. In this paper, we explore the potential of adopting an operator-valued kernel feature space perspective for the analysis of functional data. We then extend the Regularized Least Squares Classification (RLSC) algorithm to cover situations where there are multiple functions per observation. Experiments on a sound recognition problem show that the proposed method outperforms the classical RLSC algorithm. version:1
arxiv-1301-2638 | Computational Intelligence for Deepwater Reservoir Depositional Environments Interpretation | http://arxiv.org/abs/1301.2638 | id:1301.2638 author:Tina Yu, Dave Wilkinson, Julian Clark, Morgan Sullivan category:cs.NE physics.geo-ph  published:2013-01-12 summary:Predicting oil recovery efficiency of a deepwater reservoir is a challenging task. One approach to characterize a deepwater reservoir and to predict its producibility is by analyzing its depositional information. This research proposes a deposition-based stratigraphic interpretation framework for deepwater reservoir characterization. In this framework, one critical task is the identification and labeling of the stratigraphic components in the reservoir, according to their depositional environments. This interpretation process is labor intensive and can produce different results depending on the stratigrapher who performs the analysis. To relieve stratigrapher's workload and to produce more consistent results, we have developed a novel methodology to automate this process using various computational intelligence techniques. Using a well log data set, we demonstrate that the developed methodology and the designed workflow can produce finite state transducer models that interpret deepwater reservoir depositional environments adequately. version:1
arxiv-1301-2556 | Information field theory | http://arxiv.org/abs/1301.2556 | id:1301.2556 author:Torsten Enlin category:astro-ph.IM cs.IT math.IT physics.data-an stat.ML  published:2013-01-11 summary:Non-linear image reconstruction and signal analysis deal with complex inverse problems. To tackle such problems in a systematic way, I present information field theory (IFT) as a means of Bayesian, data based inference on spatially distributed signal fields. IFT is a statistical field theory, which permits the construction of optimal signal recovery algorithms even for non-linear and non-Gaussian signal inference problems. IFT algorithms exploit spatial correlations of the signal fields and benefit from techniques developed to investigate quantum and statistical field theories, such as Feynman diagrams, re-normalisation calculations, and thermodynamic potentials. The theory can be used in many areas, and applications in cosmology and numerics are presented. version:1
arxiv-1301-2466 | Determining token sequence mistakes in responses to questions with open text answer | http://arxiv.org/abs/1301.2466 | id:1301.2466 author:Oleg Sychev, Dmitry Mamontov category:cs.CL cs.CY K.3.2  published:2013-01-11 summary:When learning grammar of the new language, a teacher should routinely check student's exercises for grammatical correctness. The paper describes a method of automatically detecting and reporting grammar mistakes, regarding an order of tokens in the response. It could report extra tokens, missing tokens and misplaced tokens. The method is useful when teaching language, where order of tokens is important, which includes most formal languages and some natural ones (like English). The method was implemented in a question type plug-in CorrectWriting for the widely used learning manage system Moodle. version:1
arxiv-1301-2410 | Backward-in-Time Selection of the Order of Dynamic Regression Prediction Model | http://arxiv.org/abs/1301.2410 | id:1301.2410 author:Ioannis Vlachos, Dimitris Kugiumtzis category:stat.AP stat.ME stat.ML  published:2013-01-11 summary:We investigate the optimal structure of dynamic regression models used in multivariate time series prediction and propose a scheme to form the lagged variable structure called Backward-in-Time Selection (BTS) that takes into account feedback and multi-collinearity, often present in multivariate time series. We compare BTS to other known methods, also in conjunction with regularization techniques used for the estimation of model parameters, namely principal components, partial least squares and ridge regression estimation. The predictive efficiency of the different models is assessed by means of Monte Carlo simulations for different settings of feedback and multi-collinearity. The results show that BTS has consistently good prediction performance while other popular methods have varying and often inferior performance. The prediction performance of BTS was also found the best when tested on human electroencephalograms of an epileptic seizure, and to the prediction of returns of indices of world financial markets. version:1
arxiv-1301-2405 | Dating medieval English charters | http://arxiv.org/abs/1301.2405 | id:1301.2405 author:Gelila Tilahun, Andrey Feuerverger, Michael Gervers category:stat.AP cs.CL  published:2013-01-11 summary:Deeds, or charters, dealing with property rights, provide a continuous documentation which can be used by historians to study the evolution of social, economic and political changes. This study is concerned with charters (written in Latin) dating from the tenth through early fourteenth centuries in England. Of these, at least one million were left undated, largely due to administrative changes introduced by William the Conqueror in 1066. Correctly dating such charters is of vital importance in the study of English medieval history. This paper is concerned with computer-automated statistical methods for dating such document collections, with the goal of reducing the considerable efforts required to date them manually and of improving the accuracy of assigned dates. Proposed methods are based on such data as the variation over time of word and phrase usage, and on measures of distance between documents. The extensive (and dated) Documents of Early England Data Set (DEEDS) maintained at the University of Toronto was used for this purpose. version:1
arxiv-1110-3109 | Robust Image Analysis by L1-Norm Semi-supervised Learning | http://arxiv.org/abs/1110.3109 | id:1110.3109 author:Zhiwu Lu, Yuxin Peng category:cs.CV cs.LG  published:2011-10-14 summary:This paper presents a novel L1-norm semi-supervised learning algorithm for robust image analysis by giving new L1-norm formulation of Laplacian regularization which is the key step of graph-based semi-supervised learning. Since our L1-norm Laplacian regularization is defined directly over the eigenvectors of the normalized Laplacian matrix, we successfully formulate semi-supervised learning as an L1-norm linear reconstruction problem which can be effectively solved with sparse coding. By working with only a small subset of eigenvectors, we further develop a fast sparse coding algorithm for our L1-norm semi-supervised learning. Due to the sparsity induced by sparse coding, the proposed algorithm can deal with the noise in the data to some extent and thus has important applications to robust image analysis, such as noise-robust image classification and noise reduction for visual and textual bag-of-words (BOW) models. In particular, this paper is the first attempt to obtain robust image representation by sparse co-refinement of visual and textual BOW models. The experimental results have shown the promising performance of the proposed algorithm. version:2
arxiv-1301-2351 | Application of Hopfield Network to Saccades | http://arxiv.org/abs/1301.2351 | id:1301.2351 author:Teruyoshi Washizawa category:cs.CV q-bio.NC  published:2013-01-10 summary:Human eye movement mechanisms (saccades) are very useful for scene analysis, including object representation and pattern recognition. In this letter, a Hopfield neural network to emulate saccades is proposed. The network uses an energy function that includes location and identification tasks. Computer simulation shows that the network performs those tasks cooperatively. The result suggests that the network is applicable to shift-invariant pattern recognition. version:1
arxiv-1301-2343 | Planning by Prioritized Sweeping with Small Backups | http://arxiv.org/abs/1301.2343 | id:1301.2343 author:Harm van Seijen, Richard S. Sutton category:cs.AI cs.LG  published:2013-01-10 summary:Efficient planning plays a crucial role in model-based reinforcement learning. Traditionally, the main planning operation is a full backup based on the current estimates of the successor states. Consequently, its computation time is proportional to the number of successor states. In this paper, we introduce a new planning backup that uses only the current value of a single successor state and has a computation time independent of the number of successor states. This new backup, which we call a small backup, opens the door to a new class of model-based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods. We empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations. version:1
arxiv-1212-6846 | Maximizing a Nonnegative, Monotone, Submodular Function Constrained to Matchings | http://arxiv.org/abs/1212.6846 | id:1212.6846 author:Sagar Kale category:cs.DS cs.AI cs.CC cs.LG stat.ML  published:2012-12-31 summary:Submodular functions have many applications. Matchings have many applications. The bitext word alignment problem can be modeled as the problem of maximizing a nonnegative, monotone, submodular function constrained to matchings in a complete bipartite graph where each vertex corresponds to a word in the two input sentences and each edge represents a potential word-to-word translation. We propose a more general problem of maximizing a nonnegative, monotone, submodular function defined on the edge set of a complete graph constrained to matchings; we call this problem the CSM-Matching problem. CSM-Matching also generalizes the maximum-weight matching problem, which has a polynomial-time algorithm; however, we show that it is NP-hard to approximate CSM-Matching within a factor of e/(e-1) by reducing the max k-cover problem to it. Our main result is a simple, greedy, 3-approximation algorithm for CSM-Matching. Then we reduce CSM-Matching to maximizing a nonnegative, monotone, submodular function over two matroids, i.e., CSM-2-Matroids. CSM-2-Matroids has a (2+epsilon)-approximation algorithm - called LSV2. We show that we can find a (4+epsilon)-approximate solution to CSM-Matching using LSV2. We extend this approach to similar problems. version:2
arxiv-1207-6083 | Determinantal point processes for machine learning | http://arxiv.org/abs/1207.6083 | id:1207.6083 author:Alex Kulesza, Ben Taskar category:stat.ML cs.IR cs.LG  published:2012-07-25 summary:Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. We provide a gentle introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and show how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories. version:4
arxiv-1212-5720 | Hierarchical Graphical Models for Multigroup Shape Analysis using Expectation Maximization with Sampling in Kendall's Shape Space | http://arxiv.org/abs/1212.5720 | id:1212.5720 author:Yen-Yun Yu, P. Thomas Fletcher, Suyash P. Awate category:cs.CV  published:2012-12-22 summary:This paper proposes a novel framework for multi-group shape analysis relying on a hierarchical graphical statistical model on shapes within a population.The framework represents individual shapes as point setsmodulo translation, rotation, and scale, following the notion in Kendall shape space.While individual shapes are derived from their group shape model, each group shape model is derived from a single population shape model. The hierarchical model follows the natural organization of population data and the top level in the hierarchy provides a common frame of reference for multigroup shape analysis, e.g. classification and hypothesis testing. Unlike typical shape-modeling approaches, the proposed model is a generative model that defines a joint distribution of object-boundary data and the shape-model variables. Furthermore, it naturally enforces optimal correspondences during the process of model fitting and thereby subsumes the so-called correspondence problem. The proposed inference scheme employs an expectation maximization (EM) algorithm that treats the individual and group shape variables as hidden random variables and integrates them out before estimating the parameters (population mean and variance and the group variances). The underpinning of the EM algorithm is the sampling of pointsets, in Kendall shape space, from their posterior distribution, for which we exploit a highly-efficient scheme based on Hamiltonian Monte Carlo simulation. Experiments in this paper use the fitted hierarchical model to perform (1) hypothesis testing for comparison between pairs of groups using permutation testing and (2) classification for image retrieval. The paper validates the proposed framework on simulated data and demonstrates results on real data. version:2
arxiv-1212-1384 | Clusters and water flows: a novel approach to modal clustering through Morse theory | http://arxiv.org/abs/1212.1384 | id:1212.1384 author:Jos E. Chacn category:math.ST math.DG stat.ML stat.TH  published:2012-12-06 summary:The problem of finding groups in data (cluster analysis) has been extensively studied by researchers from the fields of Statistics and Computer Science, among others. However, despite its popularity it is widely recognized that the investigation of some theoretical aspects of clustering has been relatively sparse. One of the main reasons for this lack of theoretical results is surely the fact that, unlike the situation with other statistical problems as regression or classification, for some of the cluster methodologies it is quite difficult to specify a population goal to which the data-based clustering algorithms should try to get close. This paper aims to provide some insight into the theoretical foundations of the usual nonparametric approach to clustering, which understands clusters as regions of high density, by presenting an explicit formulation for the ideal population clustering. version:2
arxiv-1301-2194 | Network-based clustering with mixtures of L1-penalized Gaussian graphical models: an empirical investigation | http://arxiv.org/abs/1301.2194 | id:1301.2194 author:Steven M. Hill, Sach Mukherjee category:stat.ML cs.LG stat.ME  published:2013-01-10 summary:In many applications, multivariate samples may harbor previously unrecognized heterogeneity at the level of conditional independence or network structure. For example, in cancer biology, disease subtypes may differ with respect to subtype-specific interplay between molecular components. Then, both subtype discovery and estimation of subtype-specific networks present important and related challenges. To enable such analyses, we put forward a mixture model whose components are sparse Gaussian graphical models. This brings together model-based clustering and graphical modeling to permit simultaneous estimation of cluster assignments and cluster-specific networks. We carry out estimation within an L1-penalized framework, and investigate several specific penalization regimes. We present empirical results on simulated data and provide general recommendations for the formulation and use of mixtures of L1-penalized Gaussian graphical models. version:1
arxiv-1301-2320 | Using Temporal Data for Making Recommendations | http://arxiv.org/abs/1301.2320 | id:1301.2320 author:Andrew Zimdars, David Maxwell Chickering, Christopher Meek category:cs.IR cs.AI cs.LG  published:2013-01-10 summary:We treat collaborative filtering as a univariate time series estimation problem: given a user's previous votes, predict the next vote. We describe two families of methods for transforming data to encode time order in ways amenable to off-the-shelf classification and density estimation tools, and examine the results of using these approaches on several real-world data sets. The improvements in predictive accuracy we realize recommend the use of other predictive algorithms that exploit the temporal order of data. version:1
arxiv-1301-2318 | Statistical Modeling in Continuous Speech Recognition (CSR)(Invited Talk) | http://arxiv.org/abs/1301.2318 | id:1301.2318 author:Steve Young category:cs.LG cs.AI stat.ML  published:2013-01-10 summary:Automatic continuous speech recognition (CSR) is sufficiently mature that a variety of real world applications are now possible including large vocabulary transcription and interactive spoken dialogues. This paper reviews the evolution of the statistical modelling techniques which underlie current-day systems, specifically hidden Markov models (HMMs) and N-grams. Starting from a description of the speech signal and its parameterisation, the various modelling assumptions and their consequences are discussed. It then describes various techniques by which the effects of these assumptions can be mitigated. Despite the progress that has been made, the limitations of current modelling techniques are still evident. The paper therefore concludes with a brief review of some of the more fundamental modelling work now in progress. version:1
arxiv-1301-2317 | Belief Optimization for Binary Networks: A Stable Alternative to Loopy Belief Propagation | http://arxiv.org/abs/1301.2317 | id:1301.2317 author:Max Welling, Yee Whye Teh category:cs.AI cs.LG  published:2013-01-10 summary:We present a novel inference algorithm for arbitrary, binary, undirected graphs. Unlike loopy belief propagation, which iterates fixed point equations, we directly descend on the Bethe free energy. The algorithm consists of two phases, first we update the pairwise probabilities, given the marginal probabilities at each unit,using an analytic expression. Next, we update the marginal probabilities, given the pairwise probabilities by following the negative gradient of the Bethe free energy. Both steps are guaranteed to decrease the Bethe free energy, and since it is lower bounded, the algorithm is guaranteed to converge to a local minimum. We also show that the Bethe free energy is equal to the TAP free energy up to second order in the weights. In experiments we confirm that when belief propagation converges it usually finds identical solutions as our belief optimization method. However, in cases where belief propagation fails to converge, belief optimization continues to converge to reasonable beliefs. The stable nature of belief optimization makes it ideally suited for learning graphical models from data. version:1
arxiv-1301-2316 | Cross-covariance modelling via DAGs with hidden variables | http://arxiv.org/abs/1301.2316 | id:1301.2316 author:Jacob A. Wegelin, Thomas S. Richardson category:cs.LG stat.ML  published:2013-01-10 summary:DAG models with hidden variables present many difficulties that are not present when all nodes are observed. In particular, fully observed DAG models are identified and correspond to well-defined sets ofdistributions, whereas this is not true if nodes are unobserved. Inthis paper we characterize exactly the set of distributions given by a class of one-dimensional Gaussian latent variable models. These models relate two blocks of observed variables, modeling only the cross-covariance matrix. We describe the relation of this model to the singular value decomposition of the cross-covariance matrix. We show that, although the model is underidentified, useful information may be extracted. We further consider an alternative parametrization in which one latent variable is associated with each block. Our analysis leads to some novel covariance equivalence results for Gaussian hidden variable models. version:1
arxiv-1301-2315 | The Optimal Reward Baseline for Gradient-Based Reinforcement Learning | http://arxiv.org/abs/1301.2315 | id:1301.2315 author:Lex Weaver, Nigel Tao category:cs.LG cs.AI stat.ML  published:2013-01-10 summary:There exist a number of reinforcement learning algorithms which learnby climbing the gradient of expected reward. Their long-runconvergence has been proved, even in partially observableenvironments with non-deterministic actions, and without the need fora system model. However, the variance of the gradient estimator hasbeen found to be a significant practical problem. Recent approacheshave discounted future rewards, introducing a bias-variance trade-offinto the gradient estimate. We incorporate a reward baseline into thelearning system, and show that it affects variance without introducingfurther bias. In particular, as we approach the zero-bias,high-variance parameterization, the optimal (or variance minimizing)constant reward baseline is equal to the long-term average expectedreward. Modified policy-gradient algorithms are presented, and anumber of experiments demonstrate their improvement over previous work. version:1
arxiv-1301-2311 | Maximum Likelihood Bounded Tree-Width Markov Networks | http://arxiv.org/abs/1301.2311 | id:1301.2311 author:Nathan Srebro category:cs.LG cs.AI stat.ML  published:2013-01-10 summary:Chow and Liu (1968) studied the problem of learning a maximumlikelihood Markov tree. We generalize their work to more complexMarkov networks by considering the problem of learning a maximumlikelihood Markov network of bounded complexity. We discuss howtree-width is in many ways the appropriate measure of complexity andthus analyze the problem of learning a maximum likelihood Markovnetwork of bounded tree-width.Similar to the work of Chow and Liu, we are able to formalize thelearning problem as a combinatorial optimization problem on graphs. Weshow that learning a maximum likelihood Markov network of boundedtree-width is equivalent to finding a maximum weight hypertree. Thisequivalence gives rise to global, integer-programming based,approximation algorithms with provable performance guarantees, for thelearning problem. This contrasts with heuristic local-searchalgorithms which were previously suggested (e.g. by Malvestuto 1991).The equivalence also allows us to study the computational hardness ofthe learning problem. We show that learning a maximum likelihoodMarkov network of bounded tree-width is NP-hard, and discuss thehardness of approximation. version:1
arxiv-1301-2310 | Policy Improvement for POMDPs Using Normalized Importance Sampling | http://arxiv.org/abs/1301.2310 | id:1301.2310 author:Christian R. Shelton category:cs.AI cs.LG  published:2013-01-10 summary:We present a new method for estimating the expected return of a POMDP from experience. The method does not assume any knowledge of the POMDP and allows the experience to be gathered from an arbitrary sequence of policies. The return is estimated for any new policy of the POMDP. We motivate the estimator from function-approximation and importance sampling points-of-view and derive its theoretical properties. Although the estimator is biased, it has low variance and the bias is often irrelevant when the estimator is used for pair-wise comparisons. We conclude by extending the estimator to policies with memory and compare its performance in a greedy search algorithm to REINFORCE algorithms showing an order of magnitude reduction in the number of trials required. version:1
arxiv-1301-2309 | Symmetric Collaborative Filtering Using the Noisy Sensor Model | http://arxiv.org/abs/1301.2309 | id:1301.2309 author:Rita Sharma, David L Poole category:cs.IR cs.LG  published:2013-01-10 summary:Collaborative filtering is the process of making recommendations regarding the potential preference of a user, for example shopping on the Internet, based on the preference ratings of the user and a number of other users for various items. This paper considers collaborative filtering based on explicitmulti-valued ratings. To evaluate the algorithms, weconsider only {em pure} collaborative filtering, using ratings exclusively, and no other information about the people or items.Our approach is to predict a user's preferences regarding a particularitem by using other people who rated that item and other items ratedby the user as noisy sensors. The noisy sensor model uses Bayes' theorem to compute the probability distribution for the user'srating of a new item. We give two variant models: in one, we learn a{em classical normal linear regression} model of how users rate items; in another,we assume different users rate items the same, but the accuracy of thesensors needs to be learned. We compare these variant models withstate-of-the-art techniques and show how they are significantly better,whether a user has rated only two items or many. We reportempirical results using the EachMovie database footnote{http://research.compaq.com/SRC/eachmovie/} of movie ratings. Wealso show that by considering items similarity along with theusers similarity, the accuracy of the prediction increases. version:1
arxiv-1301-2303 | Probabilistic Models for Unified Collaborative and Content-Based Recommendation in Sparse-Data Environments | http://arxiv.org/abs/1301.2303 | id:1301.2303 author:Alexandrin Popescul, Lyle H. Ungar, David M Pennock, Steve Lawrence category:cs.IR cs.LG stat.ML  published:2013-01-10 summary:Recommender systems leverage product and community information to target products to consumers. Researchers have developed collaborative recommenders, content-based recommenders, and (largely ad-hoc) hybrid systems. We propose a unified probabilistic framework for merging collaborative and content-based recommendations. We extend Hofmann's [1999] aspect model to incorporate three-way co-occurrence data among users, items, and item content. The relative influence of collaboration data versus content data is not imposed as an exogenous parameter, but rather emerges naturally from the given data sources. Global probabilistic models coupled with standard Expectation Maximization (EM) learning algorithms tend to drastically overfit in sparse-data situations, as is typical in recommendation applications. We show that secondary content information can often be used to overcome sparsity. Experiments on data from the ResearchIndex library of Computer Science publications show that appropriate mixture models incorporating secondary data produce significantly better quality recommenders than k-nearest neighbors (k-NN). Global probabilistic models also allow more general inferences than local methods like k-NN. version:1
arxiv-1301-2298 | Lattice Particle Filters | http://arxiv.org/abs/1301.2298 | id:1301.2298 author:Dirk Ormoneit, Christiane Lemieux, David J. Fleet category:cs.AI cs.CV  published:2013-01-10 summary:A standard approach to approximate inference in state-space models isto apply a particle filter, e.g., the Condensation Algorithm.However, the performance of particle filters often varies significantlydue to their stochastic nature.We present a class of algorithms, called lattice particle filters, thatcircumvent this difficulty by placing the particles deterministicallyaccording to a Quasi-Monte Carlo integration rule.We describe a practical realization of this idea, discuss itstheoretical properties, and its efficiency.Experimental results with a synthetic 2D tracking problem show that thelattice particle filter is equivalent to a conventional particle filterthat has between 10 and 60% more particles, depending ontheir "sparsity" in the state-space.We also present results on inferring 3D human motion frommoving light displays. version:1
arxiv-1301-2294 | Expectation Propagation for approximate Bayesian inference | http://arxiv.org/abs/1301.2294 | id:1301.2294 author:Thomas P. Minka category:cs.AI cs.LG  published:2013-01-10 summary:This paper presents a new deterministic approximation technique in Bayesian networks. This method, "Expectation Propagation", unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. All three algorithms try to recover an approximate distribution which is close in KL divergence to the true distribution. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining certain expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Expectation Propagation also extends belief propagation in the opposite direction - it can propagate richer belief states that incorporate correlations between nodes. Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers. version:1
arxiv-1301-2292 | A Bayesian Multiresolution Independence Test for Continuous Variables | http://arxiv.org/abs/1301.2292 | id:1301.2292 author:Dimitris Margaritis, Sebastian Thrun category:cs.AI cs.LG  published:2013-01-10 summary:In this paper we present a method ofcomputing the posterior probability ofconditional independence of two or morecontinuous variables from data,examined at several resolutions. Ourapproach is motivated by theobservation that the appearance ofcontinuous data varies widely atvarious resolutions, producing verydifferent independence estimatesbetween the variablesinvolved. Therefore, it is difficultto ascertain independence withoutexamining data at several carefullyselected resolutions. In our paper, weaccomplish this using the exactcomputation of the posteriorprobability of independence, calculatedanalytically given a resolution. Ateach examined resolution, we assume amultinomial distribution with Dirichletpriors for the discretized tableparameters, and compute the posteriorusing Bayesian integration. Acrossresolutions, we use a search procedureto approximate the Bayesian integral ofprobability over an exponential numberof possible histograms. Our methodgeneralizes to an arbitrary numbervariables in a straightforward manner.The test is suitable for Bayesiannetwork learning algorithms that useindependence tests to infer the networkstructure, in domains that contain anymix of continuous, ordinal andcategorical variables. version:1
arxiv-1301-2286 | Iterative Markov Chain Monte Carlo Computation of Reference Priors and Minimax Risk | http://arxiv.org/abs/1301.2286 | id:1301.2286 author:John Lafferty, Larry A. Wasserman category:cs.LG stat.ML  published:2013-01-10 summary:We present an iterative Markov chainMonte Carlo algorithm for computingreference priors and minimax risk forgeneral parametric families. Ourapproach uses MCMC techniques based onthe Blahut-Arimoto algorithm forcomputing channel capacity ininformation theory. We give astatistical analysis of the algorithm,bounding the number of samples requiredfor the stochastic algorithm to closelyapproximate the deterministic algorithmin each iteration. Simulations arepresented for several examples fromexponential families. Although we focuson applications to reference priors andminimax risk, the methods and analysiswe develop are applicable to a muchbroader class of optimization problemsand iterative algorithms. version:1
arxiv-1301-2284 | Classifier Learning with Supervised Marginal Likelihood | http://arxiv.org/abs/1301.2284 | id:1301.2284 author:Petri Kontkanen, Petri Myllymaki, Henry Tirri category:cs.LG stat.ML  published:2013-01-10 summary:It has been argued that in supervised classification tasks, in practice it may be more sensible to perform model selection with respect to some more focused model selection score, like the supervised (conditional) marginal likelihood, than with respect to the standard marginal likelihood criterion. However, for most Bayesian network models, computing the supervised marginal likelihood score takes exponential time with respect to the amount of observed data. In this paper, we consider diagnostic Bayesian network classifiers where the significant model parameters represent conditional distributions for the class variable, given the values of the predictor variables, in which case the supervised marginal likelihood can be computed in linear time with respect to the data. As the number of model parameters grows in this case exponentially with respect to the number of predictors, we focus on simple diagnostic models where the number of relevant predictors is small, and suggest two approaches for applying this type of models in classification. The first approach is based on mixtures of simple diagnostic models, while in the second approach we apply the small predictor sets of the simple diagnostic models for augmenting the Naive Bayes classifier. version:1
arxiv-1301-2283 | Improved learning of Bayesian networks | http://arxiv.org/abs/1301.2283 | id:1301.2283 author:Tomas Kocka, Robert Castelo category:cs.LG cs.AI stat.ML  published:2013-01-10 summary:The search space of Bayesian Network structures is usually defined as Acyclic Directed Graphs (DAGs) and the search is done by local transformations of DAGs. But the space of Bayesian Networks is ordered by DAG Markov model inclusion and it is natural to consider that a good search policy should take this into account. First attempt to do this (Chickering 1996) was using equivalence classes of DAGs instead of DAGs itself. This approach produces better results but it is significantly slower. We present a compromise between these two approaches. It uses DAGs to search the space in such a way that the ordering by inclusion is taken into account. This is achieved by repetitive usage of local moves within the equivalence class of DAGs. We show that this new approach produces better results than the original DAGs approach without substantial change in time complexity. We present empirical results, within the framework of heuristic search and Markov Chain Monte Carlo, provided through the Alarm dataset. version:1
arxiv-1301-2280 | Estimating Well-Performing Bayesian Networks using Bernoulli Mixtures | http://arxiv.org/abs/1301.2280 | id:1301.2280 author:Geoff A. Jarrad category:cs.LG cs.AI stat.ML  published:2013-01-10 summary:A novel method for estimating Bayesian network (BN) parameters from data is presented which provides improved performance on test data. Previous research has shown the value of representing conditional probability distributions (CPDs) via neural networks(Neal 1992), noisy-OR gates (Neal 1992, Diez 1993)and decision trees (Friedman and Goldszmidt 1996).The Bernoulli mixture network (BMN) explicitly represents the CPDs of discrete BN nodes as mixtures of local distributions,each having a different set of parents.This increases the space of possible structures which can be considered,enabling the CPDs to have finer-grained dependencies.The resulting estimation procedure induces a modelthat is better able to emulate the underlying interactions occurring in the data than conventional conditional Bernoulli network models.The results for artificially generated data indicate that overfitting is best reduced by restricting the complexity of candidate mixture substructures local to each node. Furthermore, mixtures of very simple substructures can perform almost as well as more complex ones.The BMN is also applied to data collected from an online adventure game with an application to keyhole plan recognition. The results show that the BMN-based model brings a dramatic improvement in performance over a conventional BN model. version:1
arxiv-1301-2278 | Discovering Multiple Constraints that are Frequently Approximately Satisfied | http://arxiv.org/abs/1301.2278 | id:1301.2278 author:Geoffrey E. Hinton, Yee Whye Teh category:cs.LG stat.ML  published:2013-01-10 summary:Some high-dimensional data.sets can be modelled by assuming that there are many different linear constraints, each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations. version:1
arxiv-1301-2270 | Multivariate Information Bottleneck | http://arxiv.org/abs/1301.2270 | id:1301.2270 author:Nir Friedman, Ori Mosenzon, Noam Slonim, Naftali Tishby category:cs.LG cs.AI stat.ML  published:2013-01-10 summary:The Information bottleneck method is an unsupervised non-parametric data organization technique. Given a joint distribution P(A,B), this method constructs a new variable T that extracts partitions, or clusters, over the values of A that are informative about B. The information bottleneck has already been applied to document classification, gene expression, neural code, and spectral analysis. In this paper, we introduce a general principled framework for multivariate extensions of the information bottleneck method. This allows us to consider multiple systems of data partitions that are inter-related. Our approach utilizes Bayesian networks for specifying the systems of clusters and what information each captures. We show that this construction provides insight about bottleneck variations and enables us to characterize solutions of these variations. We also present a general framework for iterative algorithms for constructing solutions, and apply it to several examples. version:1
arxiv-1301-2269 | Learning the Dimensionality of Hidden Variables | http://arxiv.org/abs/1301.2269 | id:1301.2269 author:Gal Elidan, Nir Friedman category:cs.LG cs.AI stat.ML  published:2013-01-10 summary:A serious problem in learning probabilistic models is the presence of hidden variables. These variables are not observed, yet interact with several of the observed variables. Detecting hidden variables poses two problems: determining the relations to other variables in the model and determining the number of states of the hidden variable. In this paper, we address the latter problem in the context of Bayesian networks. We describe an approach that utilizes a score-based agglomerative state-clustering. As we show, this approach allows us to efficiently evaluate models with a range of cardinalities for the hidden variable. We show how to extend this procedure to deal with multiple interacting hidden variables. We demonstrate the effectiveness of this approach by evaluating it on synthetic and real-life data. We show that our approach learns models with hidden variables that generalize better and have better structure than previous approaches. version:1
arxiv-1301-2268 | Incorporating Expressive Graphical Models in Variational Approximations: Chain-Graphs and Hidden Variables | http://arxiv.org/abs/1301.2268 | id:1301.2268 author:Tal El-Hay, Nir Friedman category:cs.AI cs.LG  published:2013-01-10 summary:Global variational approximation methods in graphical models allow efficient approximate inference of complex posterior distributions by using a simpler model. The choice of the approximating model determines a tradeoff between the complexity of the approximation procedure and the quality of the approximation. In this paper, we consider variational approximations based on two classes of models that are richer than standard Bayesian networks, Markov networks or mixture models. As such, these classes allow to find better tradeoffs in the spectrum of approximations. The first class of models are chain graphs, which capture distributions that are partially directed. The second class of models are directed graphs (Bayesian networks) with additional latent variables. Both classes allow representation of multi-variable dependencies that cannot be easily represented within a Bayesian network. version:1
arxiv-1301-2266 | Variational MCMC | http://arxiv.org/abs/1301.2266 | id:1301.2266 author:Nando de Freitas, Pedro Hojen-Sorensen, Michael I. Jordan, Stuart Russell category:cs.LG stat.CO stat.ML  published:2013-01-10 summary:We propose a new class of learning algorithms that combines variational approximation and Markov chain Monte Carlo (MCMC) simulation. Naive algorithms that use the variational approximation as proposal distribution can perform poorly because this approximation tends to underestimate the true variance and other features of the data. We solve this problem by introducing more sophisticated MCMC algorithms. One of these algorithms is a mixture of two MCMC kernels: a random walk Metropolis kernel and a blockMetropolis-Hastings (MH) kernel with a variational approximation as proposaldistribution. The MH kernel allows one to locate regions of high probability efficiently. The Metropolis kernel allows us to explore the vicinity of these regions. This algorithm outperforms variationalapproximations because it yields slightly better estimates of the mean and considerably better estimates of higher moments, such as covariances. It also outperforms standard MCMC algorithms because it locates theregions of high probability quickly, thus speeding up convergence. We demonstrate this algorithm on the problem of Bayesian parameter estimation for logistic (sigmoid) belief networks. version:1
arxiv-1301-2262 | Conditions Under Which Conditional Independence and Scoring Methods Lead to Identical Selection of Bayesian Network Models | http://arxiv.org/abs/1301.2262 | id:1301.2262 author:Robert G. Cowell category:cs.AI cs.LG stat.ML  published:2013-01-10 summary:It is often stated in papers tackling the task of inferring Bayesian network structures from data that there are these two distinct approaches: (i) Apply conditional independence tests when testing for the presence or otherwise of edges; (ii) Search the model space using a scoring metric. Here I argue that for complete data and a given node ordering this division is a myth, by showing that cross entropy methods for checking conditional independence are mathematically identical to methods based upon discriminating between models by their overall goodness-of-fit logarithmic scores. version:1
arxiv-1301-2252 | A Factorized Variational Technique for Phase Unwrapping in Markov Random Fields | http://arxiv.org/abs/1301.2252 | id:1301.2252 author:Kannan Achan, Brendan J. Frey, Ralf Koetter category:cs.CV  published:2013-01-10 summary:Some types of medical and topographic imaging device produce images in which the pixel values are "phase-wrapped", i.e. measured modulus a known scalar. Phase unwrapping can be viewed as the problem of inferring the number of shifts between each and every pair of neighboring pixels, subject to an a priori preference for smooth surfaces, and subject to a zero curl constraint, which requires that the shifts must sum to 0 around every loop. We formulate phase unwrapping as a mean field inference problem in a Markov network, where the prior favors the zero curl constraint. We compare our mean field technique with the least squares method on a synthetic 100x100 image, and give results on a 512x512 synthetic aperture radar image from Sandia National Laboratories.<Long Text> version:1
arxiv-1301-2158 | Artificial Intelligence Framework for Simulating Clinical Decision-Making: A Markov Decision Process Approach | http://arxiv.org/abs/1301.2158 | id:1301.2158 author:Casey C. Bennett, Kris Hauser category:cs.AI stat.ML  published:2013-01-10 summary:In the modern healthcare system, rapidly expanding costs/complexity, the growing myriad of treatment options, and exploding information streams that often do not effectively reach the front lines hinder the ability to choose optimal treatment decisions over time. The goal in this paper is to develop a general purpose (non-disease-specific) computational/artificial intelligence (AI) framework to address these challenges. This serves two potential functions: 1) a simulation environment for exploring various healthcare policies, payment methodologies, etc., and 2) the basis for clinical artificial intelligence - an AI that can think like a doctor. This approach combines Markov decision processes and dynamic decision networks to learn from clinical data and develop complex plans via simulation of alternative sequential decision paths while capturing the sometimes conflicting, sometimes synergistic interactions of various components in the healthcare system. It can operate in partially observable environments (in the case of missing observations or data) by maintaining belief states about patient health status and functions as an online agent that plans and re-plans. This framework was evaluated using real patient data from an electronic health record. Such an AI framework easily outperforms the current treatment-as-usual (TAU) case-rate/fee-for-service models of healthcare (Cost per Unit Change: $189 vs. $497) while obtaining a 30-35% increase in patient outcomes. Tweaking certain model parameters further enhances this advantage, obtaining roughly 50% more improvement for roughly half the costs. Given careful design and problem formulation, an AI simulation framework can approximate optimal decisions even in complex and uncertain environments. Future work is described that outlines potential lines of research and integration of machine learning algorithms for personalized medicine. version:1
arxiv-1301-2542 | Enhancing the retrieval performance by combing the texture and edge features | http://arxiv.org/abs/1301.2542 | id:1301.2542 author:Mohamed Eisa, Amira Eletrebi, Ebrahim Elhenawy category:cs.CV cs.IR  published:2013-01-10 summary:In this paper, anew algorithm which is based on geometrical moments and local binary patterns (LBP) for content based image retrieval (CBIR) is proposed. In geometrical moments, each vector is compared with the all other vectors for edge map generation. The same concept is utilized at LBP calculation which is generating nine LBP patterns from a given 3x3 pattern. Finally, nine LBP histograms are calculated which are used as a feature vector for image retrieval. Moments are important features used in recognition of different types of images. Two experiments have been carried out for proving the worth of our algorithm. The results after being investigated shows a significant improvement in terms of their evaluation measures as compared to LBP and other existing transform domain techniques. version:1
arxiv-1301-3043 | A remark on covering | http://arxiv.org/abs/1301.3043 | id:1301.3043 author:Vladimir Temlyakov category:math.MG math.FA math.NA stat.ML  published:2013-01-10 summary:We discuss construction of coverings of the unit ball of a finite dimensional Banach space. The well known technique of comparing volumes gives upper and lower bounds on covering numbers. This technique does not provide a construction of good coverings. Here we apply incoherent dictionaries for construction of good coverings. We use the following strategy. First, we build a good covering by balls with a radius close to one. Second, we iterate this construction to obtain a good covering for any radius. We mostly concentrate on the first step of this strategy. version:1
arxiv-1301-2115 | Domain Generalization via Invariant Feature Representation | http://arxiv.org/abs/1301.2115 | id:1301.2115 author:Krikamol Muandet, David Balduzzi, Bernhard Schlkopf category:stat.ML cs.LG  published:2013-01-10 summary:This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice. version:1
arxiv-1302-3119 | Comparision and analysis of photo image forgery detection techniques | http://arxiv.org/abs/1302.3119 | id:1302.3119 author:S. Murali, Govindraj B. Chittapur, Prabhakara H. S, Basavaraj S. Anami category:cs.CV cs.CR cs.MM  published:2013-01-10 summary:Digital Photo images are everywhere, on the covers of magazines, in newspapers, in courtrooms, and all over the Internet. We are exposed to them throughout the day and most of the time. Ease with which images can be manipulated; we need to be aware that seeing does not always imply believing. We propose methodologies to identify such unbelievable photo images and succeeded to identify forged region by given only the forged image. Formats are additive tag for every file system and contents are relatively expressed with extension based on most popular digital camera uses JPEG and Other image formats like png, bmp etc. We have designed algorithm running behind with the concept of abnormal anomalies and identify the forgery regions. version:1
arxiv-1212-1819 | A fair comparison of many max-tree computation algorithms (Extended version of the paper submitted to ISMM 2013 | http://arxiv.org/abs/1212.1819 | id:1212.1819 author:Edwin Carlinet, Thierry Graud category:cs.CV  published:2012-12-08 summary:With the development of connected filters for the last decade, many algorithms have been proposed to compute the max-tree. Max-tree allows to compute the most advanced connected operators in a simple way. However, no fair comparison of algorithms has been proposed yet and the choice of an algorithm over an other depends on many parameters. Since the need of fast algorithms is obvious for production code, we present an in depth comparison of five algorithms and some variations of them in a unique framework. Finally, a decision tree will be proposed to help user in choosing the right algorithm with respect to their data. version:2
arxiv-1301-2032 | Training Effective Node Classifiers for Cascade Classification | http://arxiv.org/abs/1301.2032 | id:1301.2032 author:Chunhua Shen, Peng Wang, Sakrapee Paisitkriangkrai, Anton van den Hengel category:cs.CV cs.LG stat.ML  published:2013-01-10 summary:Cascade classifiers are widely used in real-time object detection. Different from conventional classifiers that are designed for a low overall classification error rate, a classifier in each node of the cascade is required to achieve an extremely high detection rate and moderate false positive rate. Although there are a few reported methods addressing this requirement in the context of object detection, there is no principled feature selection method that explicitly takes into account this asymmetric node learning objective. We provide such an algorithm here. We show that a special case of the biased minimax probability machine has the same formulation as the linear asymmetric classifier (LAC) of Wu et al (2005). We then design a new boosting algorithm that directly optimizes the cost function of LAC. The resulting totally-corrective boosting algorithm is implemented by the column generation technique in convex optimization. Experimental results on object detection verify the effectiveness of the proposed boosting algorithm as a node classifier in cascade object detection, and show performance better than that of the current state-of-the-art. version:1
arxiv-1301-2015 | Heteroscedastic Relevance Vector Machine | http://arxiv.org/abs/1301.2015 | id:1301.2015 author:Daniel Khashabi, Mojtaba Ziyadi, Feng Liang category:stat.ML cs.LG  published:2013-01-10 summary:In this work we propose a heteroscedastic generalization to RVM, a fast Bayesian framework for regression, based on some recent similar works. We use variational approximation and expectation propagation to tackle the problem. The work is still under progress and we are examining the results and comparing with the previous works. version:1
arxiv-1301-2012 | Error Correction in Learning using SVMs | http://arxiv.org/abs/1301.2012 | id:1301.2012 author:Srivatsan Laxman, Sushil Mittal, Ramarathnam Venkatesan category:cs.LG  published:2013-01-10 summary:This paper is concerned with learning binary classifiers under adversarial label-noise. We introduce the problem of error-correction in learning where the goal is to recover the original clean data from a label-manipulated version of it, given (i) no constraints on the adversary other than an upper-bound on the number of errors, and (ii) some regularity properties for the original data. We present a simple and practical error-correction algorithm called SubSVMs that learns individual SVMs on several small-size (log-size), class-balanced, random subsets of the data and then reclassifies the training points using a majority vote. Our analysis reveals the need for the two main ingredients of SubSVMs, namely class-balanced sampling and subsampled bagging. Experimental results on synthetic as well as benchmark UCI data demonstrate the effectiveness of our approach. In addition to noise-tolerance, log-size subsampled bagging also yields significant run-time benefits over standard SVMs. version:1
arxiv-1301-2007 | Spectral Clustering Based on Local PCA | http://arxiv.org/abs/1301.2007 | id:1301.2007 author:Ery Arias-Castro, Gilad Lerman, Teng Zhang category:stat.ML  published:2013-01-09 summary:We propose a spectral clustering method based on local principal components analysis (PCA). After performing local PCA in selected neighborhoods, the algorithm builds a nearest neighbor graph weighted according to a discrepancy between the principal subspaces in the neighborhoods, and then applies spectral clustering. As opposed to standard spectral methods based solely on pairwise distances between points, our algorithm is able to resolve intersections. We establish theoretical guarantees for simpler variants within a prototypical mathematical framework for multi-manifold clustering, and evaluate our algorithm on various simulated data sets. version:1
arxiv-1301-1950 | Syntactic Analysis Based on Morphological Characteristic Features of the Romanian Language | http://arxiv.org/abs/1301.1950 | id:1301.1950 author:Bogdan Patrut category:cs.CL cs.AI 68T50  published:2013-01-09 summary:This paper refers to the syntactic analysis of phrases in Romanian, as an important process of natural language processing. We will suggest a real-time solution, based on the idea of using some words or groups of words that indicate grammatical category; and some specific endings of some parts of sentence. Our idea is based on some characteristics of the Romanian language, where some prepositions, adverbs or some specific endings can provide a lot of information about the structure of a complex sentence. Such characteristics can be found in other languages, too, such as French. Using a special grammar, we developed a system (DIASEXP) that can perform a dialogue in natural language with assertive and interogative sentences about a "story" (a set of sentences describing some events from the real life). version:1
arxiv-1301-1936 | Risk-Aversion in Multi-armed Bandits | http://arxiv.org/abs/1301.1936 | id:1301.1936 author:Amir Sani, Alessandro Lazaric, Rmi Munos category:cs.LG  published:2013-01-09 summary:Stochastic multi-armed bandits solve the Exploration-Exploitation dilemma and ultimately maximize the expected reward. Nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective. In this paper, we introduce a novel setting based on the principle of risk-aversion where the objective is to compete against the arm with the best risk-return trade-off. This setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm. Using variance as a measure of risk, we introduce two new algorithms, investigate their theoretical guarantees, and report preliminary empirical results. version:1
arxiv-1301-1919 | Nonparametric Reduced Rank Regression | http://arxiv.org/abs/1301.1919 | id:1301.1919 author:Rina Foygel, Michael Horrell, Mathias Drton, John Lafferty category:stat.ML  published:2013-01-09 summary:We propose an approach to multivariate nonparametric regression that generalizes reduced rank regression for linear models. An additive model is estimated for each dimension of a $q$-dimensional response, with a shared $p$-dimensional predictor variable. To control the complexity of the model, we employ a functional form of the Ky-Fan or nuclear norm, resulting in a set of function estimates that have low rank. Backfitting algorithms are derived and justified using a nonparametric form of the nuclear norm subdifferential. Oracle inequalities on excess risk are derived that exhibit the scaling behavior of the procedure in the high dimensional setting. The methods are illustrated on gene expression data. version:1
arxiv-1301-1907 | Moon Search Algorithms for NASA's Dawn Mission to Asteroid Vesta | http://arxiv.org/abs/1301.1907 | id:1301.1907 author:Nargess Memarsadeghi, Lucy A. McFadden, David Skillman, Brian McLean, Max Mutchler, Uri Carsenty, Eric E. Palmer, the Dawn Mission's S category:astro-ph.IM astro-ph.EP cs.CV  published:2013-01-09 summary:A moon or natural satellite is a celestial body that orbits a planetary body such as a planet, dwarf planet, or an asteroid. Scientists seek understanding the origin and evolution of our solar system by studying moons of these bodies. Additionally, searches for satellites of planetary bodies can be important to protect the safety of a spacecraft as it approaches or orbits a planetary body. If a satellite of a celestial body is found, the mass of that body can also be calculated once its orbit is determined. Ensuring the Dawn spacecraft's safety on its mission to the asteroid (4) Vesta primarily motivated the work of Dawn's Satellite Working Group (SWG) in summer of 2011. Dawn mission scientists and engineers utilized various computational tools and techniques for Vesta's satellite search. The objectives of this paper are to 1) introduce the natural satellite search problem, 2) present the computational challenges, approaches, and tools used when addressing this problem, and 3) describe applications of various image processing and computational algorithms for performing satellite searches to the electronic imaging and computer science community. Furthermore, we hope that this communication would enable Dawn mission scientists to improve their satellite search algorithms and tools and be better prepared for performing the same investigation in 2015, when the spacecraft is scheduled to approach and orbit the dwarf planet (1) Ceres. version:1
arxiv-1301-1897 | Image Registration for Stability Testing of MEMS | http://arxiv.org/abs/1301.1897 | id:1301.1897 author:Nargess Memarsadeghi, Jacqueline Le Moigne, Peter N. Blake, Peter A. Morey, Wayne B. Landsman, Victor J. Chambers, Samuel H. Moseley category:cs.CV astro-ph.IM  published:2013-01-09 summary:Image registration, or alignment of two or more images covering the same scenes or objects, is of great interest in many disciplines such as remote sensing, medical imaging, astronomy, and computer vision. In this paper, we introduce a new application of image registration algorithms. We demonstrate how through a wavelet based image registration algorithm, engineers can evaluate stability of Micro-Electro-Mechanical Systems (MEMS). In particular, we applied image registration algorithms to assess alignment stability of the MicroShutters Subsystem (MSS) of the Near Infrared Spectrograph (NIRSpec) instrument of the James Webb Space Telescope (JWST). This work introduces a new methodology for evaluating stability of MEMS devices to engineers as well as a new application of image registration algorithms to computer scientists. version:1
arxiv-1209-0196 | Short-time homomorphic wavelet estimation | http://arxiv.org/abs/1209.0196 | id:1209.0196 author:Roberto H. Herrera, Mirko Van der Baan category:physics.geo-ph cs.CV physics.data-an  published:2012-09-02 summary:Successful wavelet estimation is an essential step for seismic methods like impedance inversion, analysis of amplitude variations with offset and full waveform inversion. Homomorphic deconvolution has long intrigued as a potentially elegant solution to the wavelet estimation problem. Yet a successful implementation has proven difficult. Associated disadvantages like phase unwrapping and restrictions of sparsity in the reflectivity function limit its application. We explore short-time homomorphic wavelet estimation as a combination of the classical homomorphic analysis and log-spectral averaging. The introduced method of log-spectral averaging using a short-term Fourier transform increases the number of sample points, thus reducing estimation variances. We apply the developed method on synthetic and real data examples and demonstrate good performance. version:3
arxiv-1301-1722 | Linear Bandits in High Dimension and Recommendation Systems | http://arxiv.org/abs/1301.1722 | id:1301.1722 author:Yash Deshpande, Andrea Montanari category:cs.LG stat.ML  published:2013-01-08 summary:A large number of online services provide automated recommendations to help users to navigate through a large collection of items. New items (products, videos, songs, advertisements) are suggested on the basis of the user's past history and --when available-- her demographic profile. Recommendations have to satisfy the dual goal of helping the user to explore the space of available items, while allowing the system to probe the user's preferences. We model this trade-off using linearly parametrized multi-armed bandits, propose a policy and prove upper and lower bounds on the cumulative "reward" that coincide up to constants in the data poor (high-dimensional) regime. Prior work on linear bandits has focused on the data rich (low-dimensional) regime and used cumulative "risk" as the figure of merit. For this data rich regime, we provide a simple modification for our policy that achieves near-optimal risk performance under more restrictive assumptions on the geometry of the problem. We test (a variation of) the scheme used for establishing achievability on the Netflix and MovieLens datasets and obtain good agreement with the qualitative predictions of the theory we develop. version:1
arxiv-1301-1671 | Causal graph-based video segmentation | http://arxiv.org/abs/1301.1671 | id:1301.1671 author:Camille Couprie, Clment Farabet, Yann LeCun category:cs.CV  published:2013-01-08 summary:Numerous approaches in image processing and computer vision are making use of super-pixels as a pre-processing step. Among the different methods producing such over-segmentation of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time. The algorithm may be trivially extended to video segmentation by considering a video as a 3D volume, however, this can not be the case for causal segmentation, when subsequent frames are unknown. We propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real time applications. version:1
arxiv-1208-3422 | Distance Metric Learning for Kernel Machines | http://arxiv.org/abs/1208.3422 | id:1208.3422 author:Zhixiang Xu, Kilian Q. Weinberger, Olivier Chapelle category:stat.ML cs.LG  published:2012-08-16 summary:Recent work in metric learning has significantly improved the state-of-the-art in k-nearest neighbor classification. Support vector machines (SVM), particularly with RBF kernels, are amongst the most popular classification algorithms that uses distance metrics to compare examples. This paper provides an empirical analysis of the efficacy of three of the most popular Mahalanobis metric learning algorithms as pre-processing for SVM training. We show that none of these algorithms generate metrics that lead to particularly satisfying improvements for SVM-RBF classification. As a remedy we introduce support vector metric learning (SVML), a novel algorithm that seamlessly combines the learning of a Mahalanobis metric with the training of the RBF-SVM parameters. We demonstrate the capabilities of SVML on nine benchmark data sets of varying sizes and difficulties. In our study, SVML outperforms all alternative state-of-the-art metric learning algorithms in terms of accuracy and establishes itself as a serious alternative to the standard Euclidean metric with model selection by cross validation. version:2
arxiv-1301-1608 | The RNA Newton Polytope and Learnability of Energy Parameters | http://arxiv.org/abs/1301.1608 | id:1301.1608 author:Elmirasadat Forouzmand, Hamidreza Chitsaz category:q-bio.BM cs.CE cs.LG  published:2013-01-08 summary:Despite nearly two scores of research on RNA secondary structure and RNA-RNA interaction prediction, the accuracy of the state-of-the-art algorithms are still far from satisfactory. Researchers have proposed increasingly complex energy models and improved parameter estimation methods in anticipation of endowing their methods with enough power to solve the problem. The output has disappointingly been only modest improvements, not matching the expectations. Even recent massively featured machine learning approaches were not able to break the barrier. In this paper, we introduce the notion of learnability of the parameters of an energy model as a measure of its inherent capability. We say that the parameters of an energy model are learnable iff there exists at least one set of such parameters that renders every known RNA structure to date the minimum free energy structure. We derive a necessary condition for the learnability and give a dynamic programming algorithm to assess it. Our algorithm computes the convex hull of the feature vectors of all feasible structures in the ensemble of a given input sequence. Interestingly, that convex hull coincides with the Newton polytope of the partition function as a polynomial in energy parameters. We demonstrated the application of our theory to a simple energy model consisting of a weighted count of A-U and C-G base pairs. Our results show that this simple energy model satisfies the necessary condition for less than one third of the input unpseudoknotted sequence-structure pairs chosen from the RNA STRAND v2.0 database. For another one third, the necessary condition is barely violated, which suggests that augmenting this simple energy model with more features such as the Turner loops may solve the problem. The necessary condition is severely violated for 8%, which provides a small set of hard cases that require further investigation. version:1
arxiv-1302-3123 | An Analysis of Gene Expression Data using Penalized Fuzzy C-Means Approach | http://arxiv.org/abs/1302.3123 | id:1302.3123 author:P. K. Nizar Banu, H. Hannah Inbarani category:cs.CV cs.CE  published:2013-01-08 summary:With the rapid advances of microarray technologies, large amounts of high-dimensional gene expression data are being generated, which poses significant computational challenges. A first step towards addressing this challenge is the use of clustering techniques, which is essential in the data mining process to reveal natural structures and identify interesting patterns in the underlying data. A robust gene expression clustering approach to minimize undesirable clustering is proposed. In this paper, Penalized Fuzzy C-Means (PFCM) Clustering algorithm is described and compared with the most representative off-line clustering techniques: K-Means Clustering, Rough K-Means Clustering and Fuzzy C-Means clustering. These techniques are implemented and tested for a Brain Tumor gene expression Dataset. Analysis of the performance of the proposed approach is presented through qualitative validation experiments. From experimental results, it can be observed that Penalized Fuzzy C-Means algorithm shows a much higher usability than the other projected clustering algorithms used in our comparison study. Significant and promising clustering results are presented using Brain Tumor Gene expression dataset. Thus patterns seen in genome-wide expression experiments can be interpreted as indications of the status of cellular processes. In these clustering results, we find that Penalized Fuzzy C-Means algorithm provides useful information as an aid to diagnosis in oncology. version:1
arxiv-1301-1590 | An Efficient Algorithm for Upper Bound on the Partition Function of Nucleic Acids | http://arxiv.org/abs/1301.1590 | id:1301.1590 author:Hamidreza Chitsaz, Elmirasadat Forouzmand, Gholamreza Haffari category:q-bio.BM cs.LG  published:2013-01-08 summary:It has been shown that minimum free energy structure for RNAs and RNA-RNA interaction is often incorrect due to inaccuracies in the energy parameters and inherent limitations of the energy model. In contrast, ensemble based quantities such as melting temperature and equilibrium concentrations can be more reliably predicted. Even structure prediction by sampling from the ensemble and clustering those structures by Sfold [7] has proven to be more reliable than minimum free energy structure prediction. The main obstacle for ensemble based approaches is the computational complexity of the partition function and base pairing probabilities. For instance, the space complexity of the partition function for RNA-RNA interaction is $O(n^4)$ and the time complexity is $O(n^6)$ which are prohibitively large [4,12]. Our goal in this paper is to give a fast algorithm, based on sparse folding, to calculate an upper bound on the partition function. Our work is based on the recent algorithm of Hazan and Jaakkola [10]. The space complexity of our algorithm is the same as that of sparse folding algorithms, and the time complexity of our algorithm is $O(MFE(n)\ell)$ for single RNA and $O(MFE(m, n)\ell)$ for RNA-RNA interaction in practice, in which $MFE$ is the running time of sparse folding and $\ell \leq n$ ($\ell \leq n + m$) is a sequence dependent parameter. version:1
arxiv-1301-1551 | A novel processing pipeline for optical multi-touch surfaces | http://arxiv.org/abs/1301.1551 | id:1301.1551 author:Philipp Ewerling category:cs.CV  published:2013-01-08 summary:In this thesis a new approach for touch detection on optical multi-touch devices is proposed that exploits the fact that the camera images reveal not only the actual touch points but also objects above the screen such as the hand or arm of a user. The touch processing relies on the Maximally Stable Extremal Regions algorithm for finding the users' fingertips in the camera image. The hierarchical structure of the generated extremal regions serves as a starting point for agglomerative clustering of the fingertips into hands. Furthermore, a heuristic is suggested that supports the identification of individual fingers as well as the distinction between left hands and right hands if all five fingers of a hand are in contact with the touch surface. The evaluation confirmed that the system is robust against detection errors resulting from non-uniform illumination and reliably assigns touch points to individual hands based on the implicitly tracked context information. The efficient multi-threaded implementation handles two-handed input from multiple users in real-time. version:1
arxiv-1301-1429 | Adaptation of fictional and online conversations to communication media | http://arxiv.org/abs/1301.1429 | id:1301.1429 author:Christian M. Alis, May T. Lim category:physics.soc-ph cs.CL physics.data-an  published:2013-01-08 summary:Conversations allow the quick transfer of short bits of information and it is reasonable to expect that changes in communication medium affect how we converse. Using conversations in works of fiction and in an online social networking platform, we show that the utterance length of conversations is slowly shortening with time but adapts more strongly to the constraints of the communication medium. This indicates that the introduction of any new medium of communication can affect the way natural language evolves. version:1
arxiv-1301-1374 | PaFiMoCS: Particle Filtered Modified-CS and Applications in Visual Tracking across Illumination Change | http://arxiv.org/abs/1301.1374 | id:1301.1374 author:R. Sarkar, S. Das, N. Vaswani category:cs.CV  published:2013-01-08 summary:We study the problem of tracking (causally estimating) a time sequence of sparse spatial signals with changing sparsity patterns, as well as other unknown states, from a sequence of nonlinear observations corrupted by (possibly) non-Gaussian noise. In many applications, particularly those in visual tracking, the unknown state can be split into a small dimensional part, e.g. global motion, and a spatial signal, e.g. illumination or shape deformation. The spatial signal is often well modeled as being sparse in some domain. For a long sequence, its sparsity pattern can change over time, although the changes are usually slow. To address the above problem, we propose a novel solution approach called Particle Filtered Modified-CS (PaFiMoCS). The key idea of PaFiMoCS is to importance sample for the small dimensional state vector, while replacing importance sampling by slow sparsity pattern change constrained posterior mode tracking for recovering the sparse spatial signal. We show that the problem of tracking moving objects across spatially varying illumination change is an example of the above problem and explain how to design PaFiMoCS for it. Experiments on both simulated data as well as on real videos with significant illumination changes demonstrate the superiority of the proposed algorithm as compared with existing particle filter based tracking algorithms. version:1
arxiv-1301-1299 | Automated Variational Inference in Probabilistic Programming | http://arxiv.org/abs/1301.1299 | id:1301.1299 author:David Wingate, Theophane Weber category:stat.ML cs.AI cs.LG  published:2013-01-07 summary:We present a new algorithm for approximate inference in probabilistic programs, based on a stochastic gradient for variational programs. This method is efficient without restrictions on the probabilistic program; it is particularly practical for distributions which are not analytically tractable, including highly structured distributions that arise in probabilistic programs. We show how to automatically derive mean-field probabilistic programs and optimize them, and demonstrate that our perspective improves inference efficiency over other algorithms. version:1
arxiv-1301-1295 | Time-Frequency Representation of Microseismic Signals using the Synchrosqueezing Transform | http://arxiv.org/abs/1301.1295 | id:1301.1295 author:Roberto H. Herrera, Jean-Baptiste Tary, Mirko van der Baan category:physics.geo-ph cs.CE cs.CV  published:2013-01-07 summary:Resonance frequencies can provide useful information on the deformation occurring during fracturing experiments or $CO_2$ management, complementary to the microseismic event distribution. An accurate time-frequency representation is of crucial importance prior to interpreting the cause of resonance frequencies during microseismic experiments. The popular methods of Short-Time Fourier Transform (STFT) and wavelet analysis have limitations in representing close frequencies and dealing with fast varying instantaneous frequencies and this is often the nature of microseismic signals. The synchrosqueezing transform (SST) is a promising tool to track these resonant frequencies and provide a detailed time-frequency representation. Here we apply the synchrosqueezing transform to microseismic signals and also show its potential to general seismic signal processing applications. version:1
arxiv-1205-0288 | A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning | http://arxiv.org/abs/1205.0288 | id:1205.0288 author:Arash Afkanpour, Andrs Gyrgy, Csaba Szepesvri, Michael Bowling category:cs.LG stat.ML  published:2012-05-01 summary:We consider the problem of simultaneously learning to linearly combine a very large number of kernels and learn a good predictor based on the learnt kernel. When the number of kernels $d$ to be combined is very large, multiple kernel learning methods whose computational cost scales linearly in $d$ are intractable. We propose a randomized version of the mirror descent algorithm to overcome this issue, under the objective of minimizing the group $p$-norm penalized empirical risk. The key to achieve the required exponential speed-up is the computationally efficient construction of low-variance estimates of the gradient. We propose importance sampling based estimates, and find that the ideal distribution samples a coordinate with a probability proportional to the magnitude of the corresponding gradient. We show the surprising result that in the case of learning the coefficients of a polynomial kernel, the combinatorial structure of the base kernels to be combined allows the implementation of sampling from this distribution to run in $O(\log(d))$ time, making the total computational cost of the method to achieve an $\epsilon$-optimal solution to be $O(\log(d)/\epsilon^2)$, thereby allowing our method to operate for very large values of $d$. Experiments with simulated and real data confirm that the new algorithm is computationally more efficient than its state-of-the-art alternatives. version:2
arxiv-1110-3741 | Multi-criteria Anomaly Detection using Pareto Depth Analysis | http://arxiv.org/abs/1110.3741 | id:1110.3741 author:Ko-Jen Hsiao, Kevin S. Xu, Jeff Calder, Alfred O. Hero III category:cs.LG cs.CV cs.DB stat.ML I.5; G.3; H.2.8  published:2011-10-17 summary:We consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection. In most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as Euclidean distance. However, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns. In such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria using a linear combination of them. If the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination. In this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using Pareto depth analysis (PDA). PDA uses the concept of Pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights. The proposed PDA approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria. version:3
arxiv-1301-1254 | Dynamical Models and Tracking Regret in Online Convex Programming | http://arxiv.org/abs/1301.1254 | id:1301.1254 author:Eric C. Hall, Rebecca M. Willett category:stat.ML cs.LG  published:2013-01-07 summary:This paper describes a new online convex optimization method which incorporates a family of candidate dynamical models and establishes novel tracking regret bounds that scale with the comparator's deviation from the best dynamical model in this family. Previous online optimization methods are designed to have a total accumulated loss comparable to that of the best comparator sequence, and existing tracking or shifting regret bounds scale with the overall variation of the comparator sequence. In many practical scenarios, however, the environment is nonstationary and comparator sequences with small variation are quite weak, resulting in large losses. The proposed Dynamic Mirror Descent method, in contrast, can yield low regret relative to highly variable comparator sequences by both tracking the best dynamical model and forming predictions based on that model. This concept is demonstrated empirically in the context of sequential compressive observations of a dynamic scene and tracking a dynamic social network. version:1
arxiv-1201-0794 | Sparse Nonparametric Graphical Models | http://arxiv.org/abs/1201.0794 | id:1201.0794 author:John Lafferty, Han Liu, Larry Wasserman category:stat.ML cs.LG stat.ME  published:2012-01-04 summary:We present some nonparametric methods for graphical modeling. In the discrete case, where the data are binary or drawn from a finite alphabet, Markov random fields are already essentially nonparametric, since the cliques can take only a finite number of values. Continuous data are different. The Gaussian graphical model is the standard parametric model for continuous data, but it makes distributional assumptions that are often unrealistic. We discuss two approaches to building more flexible graphical models. One allows arbitrary graphs and a nonparametric extension of the Gaussian; the other uses kernel density estimation and restricts the graphs to trees and forests. Examples of both methods are presented. We also discuss possible future research directions for nonparametric graphical modeling. version:2
arxiv-1301-1083 | Supervised, semi-supervised and unsupervised inference of gene regulatory networks | http://arxiv.org/abs/1301.1083 | id:1301.1083 author:Stefan R. Maetschke, Piyush B. Madhamshettiwar, Melissa J. Davis, Mark A. Ragan category:q-bio.MN q-bio.QM stat.ML  published:2013-01-07 summary:Inference of gene regulatory network from expression data is a challenging task. Many methods have been developed to this purpose but a comprehensive evaluation that covers unsupervised, semi-supervised and supervised methods, and provides guidelines for their practical application, is lacking. We performed an extensive evaluation of inference methods on simulated expression data. The results reveal very low prediction accuracies for unsupervised techniques with the notable exception of the z-score method on knock-out data. In all other cases the supervised approach achieved the highest accuracies and even in a semi-supervised setting with small numbers of only positive samples, outperformed the unsupervised techniques. version:1
arxiv-1208-0959 | Recklessly Approximate Sparse Coding | http://arxiv.org/abs/1208.0959 | id:1208.0959 author:Misha Denil, Nando de Freitas category:cs.LG cs.CV stat.ML  published:2012-08-04 summary:It has recently been observed that certain extremely simple feature encoding techniques are able to achieve state of the art performance on several standard image classification benchmarks including deep belief networks, convolutional nets, factored RBMs, mcRBMs, convolutional RBMs, sparse autoencoders and several others. Moreover, these "triangle" or "soft threshold" encodings are ex- tremely efficient to compute. Several intuitive arguments have been put forward to explain this remarkable performance, yet no mathematical justification has been offered. The main result of this report is to show that these features are realized as an approximate solution to the a non-negative sparse coding problem. Using this connection we describe several variants of the soft threshold features and demonstrate their effectiveness on two image classification benchmark tasks. version:2
arxiv-1203-5483 | Greedy Sparsity-Constrained Optimization | http://arxiv.org/abs/1203.5483 | id:1203.5483 author:Sohail Bahmani, Bhiksha Raj, Petros Boufounos category:stat.ML math.NA math.OC stat.CO 62FXX  65KXX  published:2012-03-25 summary:Sparsity-constrained optimization has wide applicability in machine learning, statistics, and signal processing problems such as feature selection and compressive Sensing. A vast body of work has studied the sparsity-constrained optimization from theoretical, algorithmic, and application aspects in the context of sparse estimation in linear models where the fidelity of the estimate is measured by the squared error. In contrast, relatively less effort has been made in the study of sparsity-constrained optimization in cases where nonlinear models are involved or the cost function is not quadratic. In this paper we propose a greedy algorithm, Gradient Support Pursuit (GraSP), to approximate sparse minima of cost functions of arbitrary form. Should a cost function have a Stable Restricted Hessian (SRH) or a Stable Restricted Linearization (SRL), both of which are introduced in this paper, our algorithm is guaranteed to produce a sparse vector within a bounded distance from the true sparse optimum. Our approach generalizes known results for quadratic cost functions that arise in sparse linear regression and Compressive Sensing. We also evaluate the performance of GraSP through numerical simulations on synthetic data, where the algorithm is employed for sparse logistic regression with and without $\ell_2$-regularization. version:3
arxiv-1301-0998 | Stratified SIFT Matching for Human Iris Recognition | http://arxiv.org/abs/1301.0998 | id:1301.0998 author:Sambit Bakshi, Hunny Mehrotra, Banshidhar Majhi category:cs.CV 68U10  published:2013-01-06 summary:This paper proposes an efficient three fold stratified SIFT matching for iris recognition. The objective is to filter wrongly paired conventional SIFT matches. In Strata I, the keypoints from gallery and probe iris images are paired using traditional SIFT approach. Due to high image similarity at different regions of iris there may be some impairments. These are detected and filtered by finding gradient of paired keypoints in Strata II. Further, the scaling factor of paired keypoints is used to remove impairments in Strata III. The pairs retained after Strata III are likely to be potential matches for iris recognition. The proposed system performs with an accuracy of 96.08% and 97.15% on publicly available CASIAV3 and BATH databases respectively. This marks significant improvement of accuracy and FAR over the existing SIFT matching for iris. version:1
arxiv-1301-0939 | Graph 3-coloring with a hybrid self-adaptive evolutionary algorithm | http://arxiv.org/abs/1301.0939 | id:1301.0939 author:Iztok Fister, Marjan Mernik, Bogdan Filipi category:cs.NE  published:2013-01-05 summary:This paper proposes a hybrid self-adaptive evolutionary algorithm for graph coloring that is hybridized with the following novel elements: heuristic genotype-phenotype mapping, a swap local search heuristic, and a neutral survivor selection operator. This algorithm was compared with the evolutionary algorithm with the SAW method of Eiben et al., the Tabucol algorithm of Hertz and de Werra, and the hybrid evolutionary algorithm of Galinier and Hao. The performance of these algorithms were tested on a test suite consisting of randomly generated 3-colorable graphs of various structural features, such as graph size, type, edge density, and variability in sizes of color classes. Furthermore, the test graphs were generated including the phase transition where the graphs are hard to color. The purpose of the extensive experimental work was threefold: to investigate the behavior of the tested algorithms in the phase transition, to identify what impact hybridization with the DSatur traditional heuristic has on the evolutionary algorithm, and to show how graph structural features influence the performance of the graph-coloring algorithms. The results indicate that the performance of the hybrid self-adaptive evolutionary algorithm is comparable with, or better than, the performance of the hybrid evolutionary algorithm which is one of the best graph-coloring algorithms today. Moreover, the fact that all the considered algorithms performed poorly on flat graphs confirms that this type of graphs is really the hardest to color. version:1
arxiv-1301-0930 | Comparative Studies on Decentralized Multiloop PID Controller Design Using Evolutionary Algorithms | http://arxiv.org/abs/1301.0930 | id:1301.0930 author:Sayan Saha, Saptarshi Das, Anindya Pakhira, Sumit Mukherjee, Indranil Pan category:cs.SY cs.NE  published:2013-01-05 summary:Decentralized PID controllers have been designed in this paper for simultaneous tracking of individual process variables in multivariable systems under step reference input. The controller design framework takes into account the minimization of a weighted sum of Integral of Time multiplied Squared Error (ITSE) and Integral of Squared Controller Output (ISCO) so as to balance the overall tracking errors for the process variables and required variation in the corresponding manipulated variables. Decentralized PID gains are tuned using three popular Evolutionary Algorithms (EAs) viz. Genetic Algorithm (GA), Evolutionary Strategy (ES) and Cultural Algorithm (CA). Credible simulation comparisons have been reported for four benchmark 2x2 multivariable processes. version:1
arxiv-1205-1765 | Chaotic multi-objective optimization based design of fractional order PID controller in AVR system | http://arxiv.org/abs/1205.1765 | id:1205.1765 author:Indranil Pan, Saptarshi Das category:cs.SY cs.NE  published:2012-05-08 summary:In this paper, a fractional order (FO) PI{\lambda}D\mu controller is designed to take care of various contradictory objective functions for an Automatic Voltage Regulator (AVR) system. An improved evolutionary Non-dominated Sorting Genetic Algorithm II (NSGA II), which is augmented with a chaotic map for greater effectiveness, is used for the multi-objective optimization problem. The Pareto fronts showing the trade-off between different design criteria are obtained for the PI{\lambda}D\mu and PID controller. A comparative analysis is done with respect to the standard PID controller to demonstrate the merits and demerits of the fractional order PI{\lambda}D\mu controller. version:2
arxiv-1301-0929 | Hybridization of Evolutionary Algorithms | http://arxiv.org/abs/1301.0929 | id:1301.0929 author:Iztok Fister, Marjan Mernik, Janez Brest category:cs.NE  published:2013-01-05 summary:Evolutionary algorithms are good general problem solver but suffer from a lack of domain specific knowledge. However, the problem specific knowledge can be added to evolutionary algorithms by hybridizing. Interestingly, all the elements of the evolutionary algorithms can be hybridized. In this chapter, the hybridization of the three elements of the evolutionary algorithms is discussed: the objective function, the survivor selection operator and the parameter settings. As an objective function, the existing heuristic function that construct the solution of the problem in traditional way is used. However, this function is embedded into the evolutionary algorithm that serves as a generator of new solutions. In addition, the objective function is improved by local search heuristics. The new neutral selection operator has been developed that is capable to deal with neutral solutions, i.e. solutions that have the different representation but expose the equal values of objective function. The aim of this operator is to directs the evolutionary search into a new undiscovered regions of the search space. To avoid of wrong setting of parameters that control the behavior of the evolutionary algorithm, the self-adaptation is used. Finally, such hybrid self-adaptive evolutionary algorithm is applied to the two real-world NP-hard problems: the graph 3-coloring and the optimization of markers in the clothing industry. Extensive experiments shown that these hybridization improves the results of the evolutionary algorithms a lot. Furthermore, the impact of the particular hybridizations is analyzed in details as well. version:1
arxiv-1112-6234 | Sparse Recovery from Nonlinear Measurements with Applications in Bad Data Detection for Power Networks | http://arxiv.org/abs/1112.6234 | id:1112.6234 author:Weiyu Xu, Meng Wang, Jianfeng Cai, Ao Tang category:cs.IT cs.LG cs.SY math.IT  published:2011-12-29 summary:In this paper, we consider the problem of sparse recovery from nonlinear measurements, which has applications in state estimation and bad data detection for power networks. An iterative mixed $\ell_1$ and $\ell_2$ convex program is used to estimate the true state by locally linearizing the nonlinear measurements. When the measurements are linear, through using the almost Euclidean property for a linear subspace, we derive a new performance bound for the state estimation error under sparse bad data and additive observation noise. As a byproduct, in this paper we provide sharp bounds on the almost Euclidean property of a linear subspace, using the "escape-through-the-mesh" theorem from geometric functional analysis. When the measurements are nonlinear, we give conditions under which the solution of the iterative algorithm converges to the true state even though the locally linearized measurements may not be the actual nonlinear measurements. We numerically evaluate our iterative convex programming approach to perform bad data detections in nonlinear electrical power networks problems. We are able to use semidefinite programming to verify the conditions for convergence of the proposed iterative sparse recovery algorithms from nonlinear measurements. version:2
arxiv-1206-1069 | Concepts and Their Dynamics: A Quantum-Theoretic Modeling of Human Thought | http://arxiv.org/abs/1206.1069 | id:1206.1069 author:Diederik Aerts, Liane Gabora, Sandro Sozzo category:cs.AI cs.CL quant-ph  published:2012-06-05 summary:We analyze different aspects of our quantum modeling approach of human concepts, and more specifically focus on the quantum effects of contextuality, interference, entanglement and emergence, illustrating how each of them makes its appearance in specific situations of the dynamics of human concepts and their combinations. We point out the relation of our approach, which is based on an ontology of a concept as an entity in a state changing under influence of a context, with the main traditional concept theories, i.e. prototype theory, exemplar theory and theory theory. We ponder about the question why quantum theory performs so well in its modeling of human concepts, and shed light on this question by analyzing the role of complex amplitudes, showing how they allow to describe interference in the statistics of measurement outcomes, while in the traditional theories statistics of outcomes originates in classical probability weights, without the possibility of interference. The relevance of complex numbers, the appearance of entanglement, and the role of Fock space in explaining contextual emergence, all as unique features of the quantum modeling, are explicitly revealed in this paper by analyzing human concepts and their dynamics. version:2
arxiv-1301-0858 | A New Geometric Approach to Latent Topic Modeling and Discovery | http://arxiv.org/abs/1301.0858 | id:1301.0858 author:Weicong Ding, Mohammad H. Rohban, Prakash Ishwar, Venkatesh Saligrama category:stat.ML  published:2013-01-05 summary:A new geometrically-motivated algorithm for nonnegative matrix factorization is developed and applied to the discovery of latent "topics" for text and image "document" corpora. The algorithm is based on robustly finding and clustering extreme points of empirical cross-document word-frequencies that correspond to novel "words" unique to each topic. In contrast to related approaches that are based on solving non-convex optimization problems using suboptimal approximations, locally-optimal methods, or heuristics, the new algorithm is convex, has polynomial complexity, and has competitive qualitative and quantitative performance compared to the current state-of-the-art approaches on synthetic and real-world datasets. version:1
arxiv-1211-4907 | Mahotas: Open source software for scriptable computer vision | http://arxiv.org/abs/1211.4907 | id:1211.4907 author:Luis Pedro Coelho category:cs.CV cs.SE  published:2012-11-21 summary:Mahotas is a computer vision library for Python. It contains traditional image processing functionality such as filtering and morphological operations as well as more modern computer vision functions for feature computation, including interest point detection and local descriptors. The interface is in Python, a dynamic programming language, which is very appropriate for fast development, but the algorithms are implemented in C++ and are tuned for speed. The library is designed to fit in with the scientific software ecosystem in this language and can leverage the existing infrastructure developed in that language. Mahotas is released under a liberal open source license (MIT License) and is available from (http://github.com/luispedro/mahotas) and from the Python Package Index (http://pypi.python.org/pypi/mahotas). version:2
arxiv-1210-5631 | Content-boosted Matrix Factorization Techniques for Recommender Systems | http://arxiv.org/abs/1210.5631 | id:1210.5631 author:Jennifer Nguyen, Mu Zhu category:stat.ML cs.LG  published:2012-10-20 summary:Many businesses are using recommender systems for marketing outreach. Recommendation algorithms can be either based on content or driven by collaborative filtering. We study different ways to incorporate content information directly into the matrix factorization approach of collaborative filtering. These content-boosted matrix factorization algorithms not only improve recommendation accuracy, but also provide useful insights about the contents, as well as make recommendations more easily interpretable. version:2
arxiv-1212-4775 | Role Mining with Probabilistic Models | http://arxiv.org/abs/1212.4775 | id:1212.4775 author:Mario Frank, Joachim M. Buhmann, David Basin category:cs.CR cs.LG stat.ML  published:2012-12-19 summary:Role mining tackles the problem of finding a role-based access control (RBAC) configuration, given an access-control matrix assigning users to access permissions as input. Most role mining approaches work by constructing a large set of candidate roles and use a greedy selection strategy to iteratively pick a small subset such that the differences between the resulting RBAC configuration and the access control matrix are minimized. In this paper, we advocate an alternative approach that recasts role mining as an inference problem rather than a lossy compression problem. Instead of using combinatorial algorithms to minimize the number of roles needed to represent the access-control matrix, we derive probabilistic models to learn the RBAC configuration that most likely underlies the given matrix. Our models are generative in that they reflect the way that permissions are assigned to users in a given RBAC configuration. We additionally model how user-permission assignments that conflict with an RBAC configuration emerge and we investigate the influence of constraints on role hierarchies and on the number of assignments. In experiments with access-control matrices from real-world enterprises, we compare our proposed models with other role mining methods. Our results show that our probabilistic models infer roles that generalize well to new system users for a wide variety of data, while other models' generalization abilities depend on the dataset given. version:3
arxiv-1301-0785 | Adaptive Intelligent Cooperative Spectrum Sensing In Cognitive Radio | http://arxiv.org/abs/1301.0785 | id:1301.0785 author:Dilip S Aldar category:cs.NE  published:2013-01-04 summary:Radio Spectrum is most precious and scarce resource and must be utilized efficiently and effectively. Cognitive radio is the promising solutions for the optimum utilization of the scared natural resource. The spectrum owned by the primary user should be shared among the secondary user, but primary user should not be interfered by the secondary user. In order to utilize the primary user spectrum, secondary user must detect accurately, the existence of primary in the band of interest. In cooperative spectrum sensing, the channel between the secondary users and the cognitive radio base station is non stationary and causes interference in the decision in decision fusion and in information in information due to multipath fading. In this paper neural network based cooperative spectrum sensing method is proposed, the performance of proposed method is evaluated and observed that, the neural network based scheme performance improve significantly over the AND,OR and Majority rule version:1
arxiv-1301-0725 | The Sum-over-Forests density index: identifying dense regions in a graph | http://arxiv.org/abs/1301.0725 | id:1301.0725 author:Mathieu Senelle, Silvia Garcia-Diez, Amin Mantrach, Masashi Shimbo, Marco Saerens, Franois Fouss category:cs.LG stat.ML  published:2013-01-04 summary:This work introduces a novel nonparametric density index defined on graphs, the Sum-over-Forests (SoF) density index. It is based on a clear and intuitive idea: high-density regions in a graph are characterized by the fact that they contain a large amount of low-cost trees with high outdegrees while low-density regions contain few ones. Therefore, a Boltzmann probability distribution on the countable set of forests in the graph is defined so that large (high-cost) forests occur with a low probability while short (low-cost) forests occur with a high probability. Then, the SoF density index of a node is defined as the expected outdegree of this node in a non-trivial tree of the forest, thus providing a measure of density around that node. Following the matrix-forest theorem, and a statistical physics framework, it is shown that the SoF density index can be easily computed in closed form through a simple matrix inversion. Experiments on artificial and real data sets show that the proposed index performs well on finding dense regions, for graphs of various origins. version:1
arxiv-1209-4277 | Multi-Level Modeling of Quotation Families Morphogenesis | http://arxiv.org/abs/1209.4277 | id:1209.4277 author:Elisa Omodei, Thierry Poibeau, Jean-Philippe Cointet category:cs.CY cs.CL cs.SI physics.soc-ph  published:2012-09-19 summary:This paper investigates cultural dynamics in social media by examining the proliferation and diversification of clearly-cut pieces of content: quoted texts. In line with the pioneering work of Leskovec et al. and Simmons et al. on memes dynamics we investigate in deep the transformations that quotations published online undergo during their diffusion. We deliberately put aside the structure of the social network as well as the dynamical patterns pertaining to the diffusion process to focus on the way quotations are changed, how often they are modified and how these changes shape more or less diverse families and sub-families of quotations. Following a biological metaphor, we try to understand in which way mutations can transform quotations at different scales and how mutation rates depend on various properties of the quotations. version:2
arxiv-1301-0435 | Investigating the performance of Correspondence Algorithms in Vision based Driver-assistance in Indoor Environment | http://arxiv.org/abs/1301.0435 | id:1301.0435 author:F. Mahmood, Syed. M. B. Haider, F. Kunwar category:cs.CV cs.RO  published:2013-01-03 summary:This paper presents the experimental comparison of fourteen stereo matching algorithms in variant illumination conditions. Different adaptations of global and local stereo matching techniques are chosen for evaluation The variant strength and weakness of the chosen correspondence algorithms are explored by employing the methodology of the prediction error strategy. The algorithms are gauged on the basis of their performance on real world data set taken in various indoor lighting conditions and at different times of the day version:1
arxiv-1301-0432 | A Self-Organizing Neural Scheme for Door Detection in Different Environments | http://arxiv.org/abs/1301.0432 | id:1301.0432 author:F. Mahmood, F. Kunwar category:cs.CV  published:2013-01-03 summary:Doors are important landmarks for indoor mobile robot navigation and also assist blind people to independently access unfamiliar buildings. Most existing algorithms of door detection are limited to work for familiar environments because of restricted assumptions about color, texture and shape. In this paper we propose a novel approach which employs feature based classification and uses the Kohonen Self-Organizing Map (SOM) for the purpose of door detection. Generic and stable features are used for the training of SOM that increase the performance significantly: concavity, bottom-edge intensity profile and door edges. To validate the robustness and generalizability of our method, we collected a large dataset of real world door images from a variety of environments and different lighting conditions. The algorithm achieves more than 95% detection which demonstrates that our door detection method is generic and robust with variations of color, texture, occlusions, lighting condition, scales, and viewpoints. version:1
arxiv-1301-0413 | A Method for Finding Structured Sparse Solutions to Non-negative Least Squares Problems with Applications | http://arxiv.org/abs/1301.0413 | id:1301.0413 author:Ernie Esser, Yifei Lou, Jack Xin category:stat.ML stat.AP stat.CO stat.ME  published:2013-01-03 summary:Demixing problems in many areas such as hyperspectral imaging and differential optical absorption spectroscopy (DOAS) often require finding sparse nonnegative linear combinations of dictionary elements that match observed data. We show how aspects of these problems, such as misalignment of DOAS references and uncertainty in hyperspectral endmembers, can be modeled by expanding the dictionary with grouped elements and imposing a structured sparsity assumption that the combinations within each group should be sparse or even 1-sparse. If the dictionary is highly coherent, it is difficult to obtain good solutions using convex or greedy methods, such as non-negative least squares (NNLS) or orthogonal matching pursuit. We use penalties related to the Hoyer measure, which is the ratio of the $l_1$ and $l_2$ norms, as sparsity penalties to be added to the objective in NNLS-type models. For solving the resulting nonconvex models, we propose a scaled gradient projection algorithm that requires solving a sequence of strongly convex quadratic programs. We discuss its close connections to convex splitting methods and difference of convex programming. We also present promising numerical results for example DOAS analysis and hyperspectral demixing problems. version:1
arxiv-1301-0339 | A Geometric Blind Source Separation Method Based on Facet Component Analysis | http://arxiv.org/abs/1301.0339 | id:1301.0339 author:P. Yin, Y. Sun, J. Xin category:math.NA stat.ML  published:2013-01-02 summary:Given a set of mixtures, blind source separation attempts to retrieve the source signals without or with very little information of the the mixing process. We present a geometric approach for blind separation of nonnegative linear mixtures termed {\em facet component analysis} (FCA). The approach is based on facet identification of the underlying cone structure of the data. Earlier works focus on recovering the cone by locating its vertices (vertex component analysis or VCA) based on a mutual sparsity condition which requires each source signal to possess a stand-alone peak in its spectrum. We formulate alternative conditions so that enough data points fall on the facets of a cone instead of accumulating around the vertices. To find a regime of unique solvability, we make use of both geometric and density properties of the data points, and develop an efficient facet identification method by combining data classification and linear regression. For noisy data, we show that denoising methods may be employed, such as the total variation technique in imaging processing, and principle component analysis. We show computational results on nuclear magnetic resonance spectroscopic data to substantiate our method. version:1
arxiv-1301-0179 | A Novel Design Specification Distance(DSD) Based K-Mean Clustering Performace Evluation on Engineering Materials Database | http://arxiv.org/abs/1301.0179 | id:1301.0179 author:Doreswamy, K. S. Hemanth category:cs.LG  published:2013-01-02 summary:Organizing data into semantically more meaningful is one of the fundamental modes of understanding and learning. Cluster analysis is a formal study of methods for understanding and algorithm for learning. K-mean clustering algorithm is one of the most fundamental and simple clustering algorithms. When there is no prior knowledge about the distribution of data sets, K-mean is the first choice for clustering with an initial number of clusters. In this paper a novel distance metric called Design Specification (DS) distance measure function is integrated with K-mean clustering algorithm to improve cluster accuracy. The K-means algorithm with proposed distance measure maximizes the cluster accuracy to 99.98% at P = 1.525, which is determined through the iterative procedure. The performance of Design Specification (DS) distance measure function with K - mean algorithm is compared with the performances of other standard distance functions such as Euclidian, squared Euclidean, City Block, and Chebshew similarity measures deployed with K-mean algorithm.The proposed method is evaluated on the engineering materials database. The experiments on cluster analysis and the outlier profiling show that these is an excellent improvement in the performance of the proposed method. version:1
arxiv-1301-0167 | Classifier Fusion Method to Recognize Handwritten Kannada Numerals | http://arxiv.org/abs/1301.0167 | id:1301.0167 author:H. R. Mamatha, S. Karthik, Murthy K. Srikanta category:cs.CV  published:2013-01-02 summary:Optical Character Recognition (OCR) is one of the important fields in image processing and pattern recognition domain. Handwritten character recognition has always been a challenging task. Only a little work can be traced towards the recognition of handwritten characters for the south Indian languages. Kannada is one such south Indian language which is also one of the official language of India. Accurate recognition of Kannada characters is a challenging task because of the high degree of similarity between the characters. Hence, good quality features are to be extracted and better classifiers are needed to improve the accuracy of the OCR for Kannada characters. This paper explores the effectiveness of feature extraction method like run length count (RLC) and directional chain code (DCC) for the recognition of handwritten Kannada numerals. In this paper, a classifier fusion method is implemented to improve the recognition rate. For the classifier fusion, we have considered K-nearest neighbour (KNN) and Linear classifier (LC). The novelty of this method is to achieve better accuracy with few features using classifier fusion approach. Proposed method achieves an average recognition rate of 96%. version:1
arxiv-1212-5841 | Data complexity measured by principal graphs | http://arxiv.org/abs/1212.5841 | id:1212.5841 author:Andrei Zinovyev, Evgeny Mirkes category:cs.LG cs.IT math.IT  published:2012-12-23 summary:How to measure the complexity of a finite set of vectors embedded in a multidimensional space? This is a non-trivial question which can be approached in many different ways. Here we suggest a set of data complexity measures using universal approximators, principal cubic complexes. Principal cubic complexes generalise the notion of principal manifolds for datasets with non-trivial topologies. The type of the principal cubic complex is determined by its dimension and a grammar of elementary graph transformations. The simplest grammar produces principal trees. We introduce three natural types of data complexity: 1) geometric (deviation of the data's approximator from some "idealized" configuration, such as deviation from harmonicity); 2) structural (how many elements of a principal graph are needed to approximate the data), and 3) construction complexity (how many applications of elementary graph transformations are needed to construct the principal object starting from the simplest one). We compute these measures for several simulated and real-life data distributions and show them in the "accuracy-complexity" plots, helping to optimize the accuracy/complexity ratio. We discuss various issues connected with measuring data complexity. Software for computing data complexity measures from principal cubic complexes is provided as well. version:2
arxiv-1301-0142 | Semi-Supervised Domain Adaptation with Non-Parametric Copulas | http://arxiv.org/abs/1301.0142 | id:1301.0142 author:David Lopez-Paz, Jos Miguel Hernndez-Lobato, Bernhard Schlkopf category:stat.ML cs.LG  published:2013-01-01 summary:A new framework based on the theory of copulas is proposed to address semi- supervised domain adaptation problems. The presented method factorizes any multivariate density into a product of marginal distributions and bivariate cop- ula functions. Therefore, changes in each of these factors can be detected and corrected to adapt a density model accross different learning domains. Impor- tantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner. Experimental results on regression problems with real-world data illustrate the efficacy of the proposed approach when compared to state-of-the-art techniques. version:1
arxiv-1301-0104 | Policy Evaluation with Variance Related Risk Criteria in Markov Decision Processes | http://arxiv.org/abs/1301.0104 | id:1301.0104 author:Aviv Tamar, Dotan Di Castro, Shie Mannor category:cs.LG stat.ML  published:2013-01-01 summary:In this paper we extend temporal difference policy evaluation algorithms to performance criteria that include the variance of the cumulative reward. Such criteria are useful for risk management, and are important in domains such as finance and process control. We propose both TD(0) and LSTD(lambda) variants with linear function approximation, prove their convergence, and demonstrate their utility in a 4-dimensional continuous state space problem. version:1
arxiv-1301-0082 | CloudSVM : Training an SVM Classifier in Cloud Computing Systems | http://arxiv.org/abs/1301.0082 | id:1301.0082 author:F. Ozgur Catak, M. Erdal Balaban category:cs.LG cs.DC  published:2013-01-01 summary:In conventional method, distributed support vector machines (SVM) algorithms are trained over pre-configured intranet/internet environments to find out an optimal classifier. These methods are very complicated and costly for large datasets. Hence, we propose a method that is referred as the Cloud SVM training mechanism (CloudSVM) in a cloud computing environment with MapReduce technique for distributed machine learning applications. Accordingly, (i) SVM algorithm is trained in distributed cloud storage servers that work concurrently; (ii) merge all support vectors in every trained cloud node; and (iii) iterate these two steps until the SVM converges to the optimal classifier function. Large scale data sets are not possible to train using SVM algorithm on a single computer. The results of this study are important for training of large scale data sets for machine learning applications. We provided that iterative training of splitted data set in cloud computing environment using SVM will converge to a global optimal classifier in finite iteration size. version:1
arxiv-1301-0048 | Generating High-Order Threshold Functions with Multiple Thresholds | http://arxiv.org/abs/1301.0048 | id:1301.0048 author:Yukihiro Kamada, Kiyonori Miyasaki category:cs.NE  published:2013-01-01 summary:In this paper, we consider situations in which a given logical function is realized by a multithreshold threshold function. In such situations, constant functions can be easily obtained from multithreshold threshold functions, and therefore, we can show that it becomes possible to optimize a class of high-order neural networks. We begin by proposing a generating method for threshold functions in which we use a vector that determines the boundary between the linearly separable function and the high-order threshold function. By applying this method to high-order threshold functions, we show that functions with the same weight as, but a different threshold than, a threshold function generated by the generation process can be easily obtained. We also show that the order of the entire network can be extended while maintaining the structure of given functions. version:1
arxiv-1301-0047 | On Distributed Online Classification in the Midst of Concept Drifts | http://arxiv.org/abs/1301.0047 | id:1301.0047 author:Zaid J. Towfic, Jianshu Chen, Ali H. Sayed category:math.OC cs.DC cs.LG cs.SI physics.soc-ph  published:2013-01-01 summary:In this work, we analyze the generalization ability of distributed online learning algorithms under stationary and non-stationary environments. We derive bounds for the excess-risk attained by each node in a connected network of learners and study the performance advantage that diffusion strategies have over individual non-cooperative processing. We conduct extensive simulations to illustrate the results. version:1
arxiv-1301-0015 | Bethe Bounds and Approximating the Global Optimum | http://arxiv.org/abs/1301.0015 | id:1301.0015 author:Adrian Weller, Tony Jebara category:cs.LG stat.ML  published:2012-12-31 summary:Inference in general Markov random fields (MRFs) is NP-hard, though identifying the maximum a posteriori (MAP) configuration of pairwise MRFs with submodular cost functions is efficiently solvable using graph cuts. Marginal inference, however, even for this restricted class, is in #P. We prove new formulations of derivatives of the Bethe free energy, provide bounds on the derivatives and bracket the locations of stationary points, introducing a new technique called Bethe bound propagation. Several results apply to pairwise models whether associative or not. Applying these to discretized pseudo-marginals in the associative case we present a polynomial time approximation scheme for global optimization provided the maximum degree is $O(\log n)$, and discuss several extensions. version:1
arxiv-1212-6958 | Fast Solutions to Projective Monotone Linear Complementarity Problems | http://arxiv.org/abs/1212.6958 | id:1212.6958 author:Geoffrey J. Gordon category:cs.LG math.OC  published:2012-12-31 summary:We present a new interior-point potential-reduction algorithm for solving monotone linear complementarity problems (LCPs) that have a particular special structure: their matrix $M\in{\mathbb R}^{n\times n}$ can be decomposed as $M=\Phi U + \Pi_0$, where the rank of $\Phi$ is $k<n$, and $\Pi_0$ denotes Euclidean projection onto the nullspace of $\Phi^\top$. We call such LCPs projective. Our algorithm solves a monotone projective LCP to relative accuracy $\epsilon$ in $O(\sqrt n \ln(1/\epsilon))$ iterations, with each iteration requiring $O(nk^2)$ flops. This complexity compares favorably with interior-point algorithms for general monotone LCPs: these algorithms also require $O(\sqrt n \ln(1/\epsilon))$ iterations, but each iteration needs to solve an $n\times n$ system of linear equations, a much higher cost than our algorithm when $k\ll n$. Our algorithm works even though the solution to a projective LCP is not restricted to lie in any low-rank subspace. version:1
arxiv-1212-6936 | Blind Analysis of EGM Signals: Sparsity-Aware Formulation | http://arxiv.org/abs/1212.6936 | id:1212.6936 author:David Luengo, Javier Via, Sandra Monzon, Tom Trigano, Antonio Artes-Rodriguez category:stat.ML  published:2012-12-31 summary:This technical note considers the problems of blind sparse learning and inference of electrogram (EGM) signals under atrial fibrillation (AF) conditions. First of all we introduce a mathematical model for the observed signals that takes into account the multiple foci typically appearing inside the heart during AF. Then we propose a reconstruction model based on a fixed dictionary and discuss several alternatives for choosing the dictionary. In order to obtain a sparse solution that takes into account the biological restrictions of the problem, a first alternative is using LASSO regularization followed by a post-processing stage that removes low amplitude coefficients violating the refractory period characteristic of cardiac cells. As an alternative we propose a novel regularization term, called cross products LASSO (CP-LASSO), that is able to incorporate the biological constraints directly into the optimization problem. Unfortunately, the resulting problem is non-convex, but we show how it can be solved efficiently in an approximated way making use of successive convex approximations (SCA). Finally, spectral analysis is performed on the clean activation sequence obtained from the sparse learning stage in order to estimate the number of latent foci and their frequencies. Simulations on synthetic and real data are provided to validate the proposed approach. version:1
arxiv-1212-6922 | Training a Functional Link Neural Network Using an Artificial Bee Colony for Solving a Classification Problems | http://arxiv.org/abs/1212.6922 | id:1212.6922 author:Yana Mazwin Mohmad Hassim, Rozaida Ghazali category:cs.NE cs.LG  published:2012-12-31 summary:Artificial Neural Networks have emerged as an important tool for classification and have been widely used to classify a non-linear separable pattern. The most popular artificial neural networks model is a Multilayer Perceptron (MLP) as it is able to perform classification task with significant success. However due to the complexity of MLP structure and also problems such as local minima trapping, over fitting and weight interference have made neural network training difficult. Thus, the easy way to avoid these problems is to remove the hidden layers. This paper presents the ability of Functional Link Neural Network (FLNN) to overcome the complexity structure of MLP by using single layer architecture and propose an Artificial Bee Colony (ABC) optimization for training the FLNN. The proposed technique is expected to provide better learning scheme for a classifier in order to get more accurate classification result version:1
arxiv-1212-6837 | Autonomously Learning to Visually Detect Where Manipulation Will Succeed | http://arxiv.org/abs/1212.6837 | id:1212.6837 author:Hai Nguyen, Charles C. Kemp category:cs.RO cs.AI cs.CV  published:2012-12-31 summary:Visual features can help predict if a manipulation behavior will succeed at a given location. For example, the success of a behavior that flips light switches depends on the location of the switch. Within this paper, we present methods that enable a mobile manipulator to autonomously learn a function that takes an RGB image and a registered 3D point cloud as input and returns a 3D location at which a manipulation behavior is likely to succeed. Given a pair of manipulation behaviors that can change the state of the world between two sets (e.g., light switch up and light switch down), classifiers that detect when each behavior has been successful, and an initial hint as to where one of the behaviors will be successful, the robot autonomously trains a pair of support vector machine (SVM) classifiers by trying out the behaviors at locations in the world and observing the results. When an image feature vector associated with a 3D location is provided as input to one of the SVMs, the SVM predicts if the associated manipulation behavior will be successful at the 3D location. To evaluate our approach, we performed experiments with a PR2 robot from Willow Garage in a simulated home using behaviors that flip a light switch, push a rocker-type light switch, and operate a drawer. By using active learning, the robot efficiently learned SVMs that enabled it to consistently succeed at these tasks. After training, the robot also continued to learn in order to adapt in the event of failure. version:1
arxiv-1205-4463 | Pilgrims Face Recognition Dataset -- HUFRD | http://arxiv.org/abs/1205.4463 | id:1205.4463 author:Salah A. Aly category:cs.CV cs.CY  published:2012-05-20 summary:In this work, we define a new pilgrims face recognition dataset, called HUFRD dataset. The new developed dataset presents various pilgrims' images taken from outside the Holy Masjid El-Harram in Makkah during the 2011-2012 Hajj and Umrah seasons. Such dataset will be used to test our developed facial recognition and detection algorithms, as well as assess in the missing and found recognition system \cite{crowdsensing}. version:2
arxiv-1210-0508 | Inference algorithms for pattern-based CRFs on sequence data | http://arxiv.org/abs/1210.0508 | id:1210.0508 author:Rustem Takhanov, Vladimir Kolmogorov category:cs.LG cs.DS  published:2012-10-01 summary:We consider Conditional Random Fields (CRFs) with pattern-based potentials defined on a chain. In this model the energy of a string (labeling) $x_1...x_n$ is the sum of terms over intervals $[i,j]$ where each term is non-zero only if the substring $x_i...x_j$ equals a prespecified pattern $\alpha$. Such CRFs can be naturally applied to many sequence tagging problems. We present efficient algorithms for the three standard inference tasks in a CRF, namely computing (i) the partition function, (ii) marginals, and (iii) computing the MAP. Their complexities are respectively $O(n L)$, $O(n L \ell_{max})$ and $O(n L \min\{ D ,\log (\ell_{max}+1)\})$ where $L$ is the combined length of input patterns, $\ell_{max}$ is the maximum length of a pattern, and $D$ is the input alphabet. This improves on the previous algorithms of (Ye et al., 2009) whose complexities are respectively $O(n L D )$, $O(n \Gamma L^2 \ell_{max}^2)$ and $O(n L D )$, where $ \Gamma $ is the number of input patterns. In addition, we give an efficient algorithm for sampling. Finally, we consider the case of non-positive weights. (Komodakis & Paragios, 2009) gave an $O(n L)$ algorithm for computing the MAP. We present a modification that has the same worst-case complexity but can beat it in the best case. version:4
arxiv-1212-6659 | Focus of Attention for Linear Predictors | http://arxiv.org/abs/1212.6659 | id:1212.6659 author:Raphael Pelossof, Zhiliang Ying category:stat.ML cs.AI cs.LG  published:2012-12-29 summary:We present a method to stop the evaluation of a prediction process when the result of the full evaluation is obvious. This trait is highly desirable in prediction tasks where a predictor evaluates all its features for every example in large datasets. We observe that some examples are easier to classify than others, a phenomenon which is characterized by the event when most of the features agree on the class of an example. By stopping the feature evaluation when encountering an easy- to-classify example, the predictor can achieve substantial gains in computation. Our method provides a natural attention mechanism for linear predictors where the predictor concentrates most of its computation on hard-to-classify examples and quickly discards easy-to-classify ones. By modifying a linear prediction algorithm such as an SVM or AdaBoost to include our attentive method we prove that the average number of features computed is O(sqrt(n log 1/sqrt(delta))) where n is the original number of features, and delta is the error rate incurred due to early stopping. We demonstrate the effectiveness of Attentive Prediction on MNIST, Real-sim, Gisette, and synthetic datasets. version:1
arxiv-1212-6527 | Discovering Basic Emotion Sets via Semantic Clustering on a Twitter Corpus | http://arxiv.org/abs/1212.6527 | id:1212.6527 author:Eugene Yuta Bann category:cs.AI cs.CL  published:2012-12-28 summary:A plethora of words are used to describe the spectrum of human emotions, but how many emotions are there really, and how do they interact? Over the past few decades, several theories of emotion have been proposed, each based around the existence of a set of 'basic emotions', and each supported by an extensive variety of research including studies in facial expression, ethology, neurology and physiology. Here we present research based on a theory that people transmit their understanding of emotions through the language they use surrounding emotion keywords. Using a labelled corpus of over 21,000 tweets, six of the basic emotion sets proposed in existing literature were analysed using Latent Semantic Clustering (LSC), evaluating the distinctiveness of the semantic meaning attached to the emotional label. We hypothesise that the more distinct the language is used to express a certain emotion, then the more distinct the perception (including proprioception) of that emotion is, and thus more 'basic'. This allows us to select the dimensions best representing the entire spectrum of emotion. We find that Ekman's set, arguably the most frequently used for classifying emotions, is in fact the most semantically distinct overall. Next, taking all analysed (that is, previously proposed) emotion terms into account, we determine the optimal semantically irreducible basic emotion set using an iterative LSC algorithm. Our newly-derived set (Accepting, Ashamed, Contempt, Interested, Joyful, Pleased, Sleepy, Stressed) generates a 6.1% increase in distinctiveness over Ekman's set (Angry, Disgusted, Joyful, Sad, Scared). We also demonstrate how using LSC data can help visualise emotions. We introduce the concept of an Emotion Profile and briefly analyse compound emotions both visually and mathematically. version:1
arxiv-1212-1824 | Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes | http://arxiv.org/abs/1212.1824 | id:1212.1824 author:Ohad Shamir, Tong Zhang category:cs.LG math.OC stat.ML  published:2012-12-08 summary:Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required non-trivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines. In this paper, we investigate the performance of SGD without such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after T rounds, the suboptimality of the last SGD iterate scales as O(log(T)/\sqrt{T}) for non-smooth convex objective functions, and O(log(T)/T) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in Rakhlin et al. (2011) is not as simple to implement). Finally, we provide some experimental illustrations. version:2
arxiv-1212-5932 | Fully scalable online-preprocessing algorithm for short oligonucleotide microarray atlases | http://arxiv.org/abs/1212.5932 | id:1212.5932 author:Leo Lahti, Aurora Torrente, Laura L. Elo, Alvis Brazma, Johan Rung category:q-bio.QM cs.CE cs.LG q-bio.GN stat.AP stat.ML  published:2012-12-24 summary:Accumulation of standardized data collections is opening up novel opportunities for holistic characterization of genome function. The limited scalability of current preprocessing techniques has, however, formed a bottleneck for full utilization of contemporary microarray collections. While short oligonucleotide arrays constitute a major source of genome-wide profiling data, scalable probe-level preprocessing algorithms have been available only for few measurement platforms based on pre-calculated model parameters from restricted reference training sets. To overcome these key limitations, we introduce a fully scalable online-learning algorithm that provides tools to process large microarray atlases including tens of thousands of arrays. Unlike the alternatives, the proposed algorithm scales up in linear time with respect to sample size and is readily applicable to all short oligonucleotide platforms. This is the only available preprocessing algorithm that can learn probe-level parameters based on sequential hyperparameter updates at small, consecutive batches of data, thus circumventing the extensive memory requirements of the standard approaches and opening up novel opportunities to take full advantage of contemporary microarray data collections. Moreover, using the most comprehensive data collections to estimate probe-level effects can assist in pinpointing individual probes affected by various biases and provide new tools to guide array design and quality control. The implementation is freely available in R/Bioconductor at http://www.bioconductor.org/packages/devel/bioc/html/RPA.html version:2
arxiv-1212-6323 | Localized Algorithm of Community Detection on Large-Scale Decentralized Social Networks | http://arxiv.org/abs/1212.6323 | id:1212.6323 author:Pili Hu, Wing Cheong Lau category:cs.SI physics.soc-ph stat.ML  published:2012-12-27 summary:Despite the overwhelming success of the existing Social Networking Services (SNS), their centralized ownership and control have led to serious concerns in user privacy, censorship vulnerability and operational robustness of these services. To overcome these limitations, Distributed Social Networks (DSN) have recently been proposed and implemented. Under these new DSN architectures, no single party possesses the full knowledge of the entire social network. While this approach solves the above problems, the lack of global knowledge for the DSN nodes makes it much more challenging to support some common but critical SNS services like friends discovery and community detection. In this paper, we tackle the problem of community detection for a given user under the constraint of limited local topology information as imposed by common DSN architectures. By considering the Personalized Page Rank (PPR) approach as an ink spilling process, we justify its applicability for decentralized community detection using limited local topology information.Our proposed PPR-based solution has a wide range of applications such as friends recommendation, targeted advertisement, automated social relationship labeling and sybil defense. Using data collected from a large-scale SNS in practice, we demonstrate our adapted version of PPR can significantly outperform the basic PR as well as two other commonly used heuristics. The inclusion of a few manually labeled friends in the Escape Vector (EV) can boost the performance considerably (64.97% relative improvement in terms of Area Under the ROC Curve (AUC)). version:1
arxiv-1212-6316 | On-line relational SOM for dissimilarity data | http://arxiv.org/abs/1212.6316 | id:1212.6316 author:Madalina Olteanu, Nathalie Villa-Vialaneix, Marie Cottrell category:stat.ML cs.LG  published:2012-12-27 summary:In some applications and in order to address real world situations better, data may be more complex than simple vectors. In some examples, they can be known through their pairwise dissimilarities only. Several variants of the Self Organizing Map algorithm were introduced to generalize the original algorithm to this framework. Whereas median SOM is based on a rough representation of the prototypes, relational SOM allows representing these prototypes by a virtual combination of all elements in the data set. However, this latter approach suffers from two main drawbacks. First, its complexity can be large. Second, only a batch version of this algorithm has been studied so far and it often provides results having a bad topographic organization. In this article, an on-line version of relational SOM is described and justified. The algorithm is tested on several datasets, including categorical data and graphs, and compared with the batch version and with other SOM algorithms for non vector data. version:1
arxiv-1212-6303 | A brief experience on journey through hardware developments for image processing and its applications on Cryptography | http://arxiv.org/abs/1212.6303 | id:1212.6303 author:Sangeet Saha, Chandrajit pal, Rourab paul, Satyabrata Maity, Suman Sau category:cs.AR cs.CR cs.CV  published:2012-12-27 summary:The importance of embedded applications on image and video processing,communication and cryptography domain has been taking a larger space in current research era. Improvement of pictorial information for betterment of human perception like deblurring, de-noising in several fields such as satellite imaging, medical imaging etc are renewed research thrust. Specifically we would like to elaborate our experience on the significance of computer vision as one of the domains where hardware implemented algorithms perform far better than those implemented through software. So far embedded design engineers have successfully implemented their designs by means of Application Specific Integrated Circuits (ASICs) and/or Digital Signal Processors (DSP), however with the advancement of VLSI technology a very powerful hardware device namely the Field Programmable Gate Array (FPGA) combining the key advantages of ASICs and DSPs was developed which have the possibility of reprogramming making them a very attractive device for rapid prototyping.Communication of image and video data in multiple FPGA is no longer far away from the thrust of secured transmission among them, and then the relevance of cryptography is indeed unavoidable. This paper shows how the Xilinx hardware development platform as well Mathworks Matlab can be used to develop hardware based computer vision algorithms and its corresponding crypto transmission channel between multiple FPGA platform from a system level approach, making it favourable for developing a hardware-software co-design environment. version:1
arxiv-1212-6276 | Echo State Queueing Network: a new reservoir computing learning tool | http://arxiv.org/abs/1212.6276 | id:1212.6276 author:Sebastin Basterrech, Gerardo Rubino category:cs.NE cs.AI cs.LG I.2; D.4.8; F.1.1  published:2012-12-26 summary:In the last decade, a new computational paradigm was introduced in the field of Machine Learning, under the name of Reservoir Computing (RC). RC models are neural networks which a recurrent part (the reservoir) that does not participate in the learning process, and the rest of the system where no recurrence (no neural circuit) occurs. This approach has grown rapidly due to its success in solving learning tasks and other computational applications. Some success was also observed with another recently proposed neural network designed using Queueing Theory, the Random Neural Network (RandNN). Both approaches have good properties and identified drawbacks. In this paper, we propose a new RC model called Echo State Queueing Network (ESQN), where we use ideas coming from RandNNs for the design of the reservoir. ESQNs consist in ESNs where the reservoir has a new dynamics inspired by recurrent RandNNs. The paper positions ESQNs in the global Machine Learning area, and provides examples of their use and performances. We show on largely used benchmarks that ESQNs are very accurate tools, and we illustrate how they compare with standard ESNs. version:1
arxiv-1212-6246 | Gaussian Process Regression with Heteroscedastic or Non-Gaussian Residuals | http://arxiv.org/abs/1212.6246 | id:1212.6246 author:Chunyi Wang, Radford M. Neal category:stat.ML cs.LG  published:2012-12-26 summary:Gaussian Process (GP) regression models typically assume that residuals are Gaussian and have the same variance for all observations. However, applications with input-dependent noise (heteroscedastic residuals) frequently arise in practice, as do applications in which the residuals do not have a Gaussian distribution. In this paper, we propose a GP Regression model with a latent variable that serves as an additional unobserved covariate for the regression. This model (which we call GPLC) allows for heteroscedasticity since it allows the function to have a changing partial derivative with respect to this unobserved covariate. With a suitable covariance function, our GPLC model can handle (a) Gaussian residuals with input-dependent variance, or (b) non-Gaussian residuals with input-dependent variance, or (c) Gaussian residuals with constant variance. We compare our model, using synthetic datasets, with a model proposed by Goldberg, Williams and Bishop (1998), which we refer to as GPLV, which only deals with case (a), as well as a standard GP model which can handle only case (c). Markov Chain Monte Carlo methods are developed for both modelsl. Experiments show that when the data is heteroscedastic, both GPLC and GPLV give better results (smaller mean squared error and negative log-probability density) than standard GP regression. In addition, when the residual are Gaussian, our GPLC model is generally nearly as good as GPLV, while when the residuals are non-Gaussian, our GPLC model is better than GPLV. version:1
arxiv-1212-6209 | Efficient Multiple Object Tracking Using Mutually Repulsive Active Membranes | http://arxiv.org/abs/1212.6209 | id:1212.6209 author:Yi Deng, Philip Coen, Mingzhai Sun, Joshua W. Shaevitz category:q-bio.QM cs.CV physics.bio-ph  published:2012-12-26 summary:Studies of social and group behavior in interacting organisms require high-throughput analysis of the motion of a large number of individual subjects. Computer vision techniques offer solutions to specific tracking problems, and allow automated and efficient tracking with minimal human intervention. In this work, we adopt the open active contour model to track the trajectories of moving objects at high density. We add repulsive interactions between open contours to the original model, treat the trajectories as an extrusion in the temporal dimension, and show applications to two tracking problems. The walking behavior of Drosophila is studied at different population density and gender composition. We demonstrate that individual male flies have distinct walking signatures, and that the social interaction between flies in a mixed gender arena is gender specific. We also apply our model to studies of trajectories of gliding Myxococcus xanthus bacteria at high density. We examine the individual gliding behavioral statistics in terms of the gliding speed distribution. Using these two examples at very distinctive spatial scales, we illustrate the use of our algorithm on tracking both short rigid bodies (Drosophila) and long flexible objects (Myxococcus xanthus). Our repulsive active membrane model reaches error rates better than $5\times 10^{-6}$ per fly per second for Drosophila tracking and comparable results for Myxococcus xanthus. version:1
arxiv-1212-6167 | Transfer Learning Using Logistic Regression in Credit Scoring | http://arxiv.org/abs/1212.6167 | id:1212.6167 author:Farid Beninel, Waad Bouaguel, Ghazi Belmufti category:cs.LG cs.CE  published:2012-12-26 summary:The credit scoring risk management is a fast growing field due to consumer's credit requests. Credit requests, of new and existing customers, are often evaluated by classical discrimination rules based on customers information. However, these kinds of strategies have serious limits and don't take into account the characteristics difference between current customers and the future ones. The aim of this paper is to measure credit worthiness for non customers borrowers and to model potential risk given a heterogeneous population formed by borrowers customers of the bank and others who are not. We hold on previous works done in generalized gaussian discrimination and transpose them into the logistic model to bring out efficient discrimination rules for non customers' subpopulation. Therefore we obtain several simple models of connection between parameters of both logistic models associated respectively to the two subpopulations. The German credit data set is selected to experiment and to compare these models. Experimental results show that the use of links between the two subpopulations improve the classification accuracy for the new loan applicants. version:1
arxiv-1212-6110 | Hyperplane Arrangements and Locality-Sensitive Hashing with Lift | http://arxiv.org/abs/1212.6110 | id:1212.6110 author:Makiko Konoshima, Yui Noma category:cs.LG cs.IR stat.ML H.3.3; H.3.m  published:2012-12-26 summary:Locality-sensitive hashing converts high-dimensional feature vectors, such as image and speech, into bit arrays and allows high-speed similarity calculation with the Hamming distance. There is a hashing scheme that maps feature vectors to bit arrays depending on the signs of the inner products between feature vectors and the normal vectors of hyperplanes placed in the feature space. This hashing can be seen as a discretization of the feature space by hyperplanes. If labels for data are given, one can determine the hyperplanes by using learning algorithms. However, many proposed learning methods do not consider the hyperplanes' offsets. Not doing so decreases the number of partitioned regions, and the correlation between Hamming distances and Euclidean distances becomes small. In this paper, we propose a lift map that converts learning algorithms without the offsets to the ones that take into account the offsets. With this method, the learning methods without the offsets give the discretizations of spaces as if it takes into account the offsets. For the proposed method, we input several high-dimensional feature data sets and studied the relationship between the statistical characteristics of data, the number of hyperplanes, and the effect of the proposed method. version:1
arxiv-1208-3716 | Improved Total Variation based Image Compressive Sensing Recovery by Nonlocal Regularization | http://arxiv.org/abs/1208.3716 | id:1208.3716 author:Jian Zhang, Shaohui Liu, Debin Zhao, Ruiqin Xiong, Siwei Ma category:cs.CV  published:2012-08-18 summary:Recently, total variation (TV) based minimization algorithms have achieved great success in compressive sensing (CS) recovery for natural images due to its virtue of preserving edges. However, the use of TV is not able to recover the fine details and textures, and often suffers from undesirable staircase artifact. To reduce these effects, this letter presents an improved TV based image CS recovery algorithm by introducing a new nonlocal regularization constraint into CS optimization problem. The nonlocal regularization is built on the well known nonlocal means (NLM) filtering and takes advantage of self-similarity in images, which helps to suppress the staircase effect and restore the fine details. Furthermore, an efficient augmented Lagrangian based algorithm is developed to solve the above combined TV and nonlocal regularization constrained problem. Experimental results demonstrate that the proposed algorithm achieves significant performance improvements over the state-of-the-art TV based algorithm in both PSNR and visual perception. version:2
arxiv-1212-6094 | Large Scale Strongly Supervised Ensemble Metric Learning, with Applications to Face Verification and Retrieval | http://arxiv.org/abs/1212.6094 | id:1212.6094 author:Chang Huang, Shenghuo Zhu, Kai Yu category:cs.CV  published:2012-12-25 summary:Learning Mahanalobis distance metrics in a high- dimensional feature space is very difficult especially when structural sparsity and low rank are enforced to improve com- putational efficiency in testing phase. This paper addresses both aspects by an ensemble metric learning approach that consists of sparse block diagonal metric ensembling and join- t metric learning as two consecutive steps. The former step pursues a highly sparse block diagonal metric by selecting effective feature groups while the latter one further exploits correlations between selected feature groups to obtain an accurate and low rank metric. Our algorithm considers all pairwise or triplet constraints generated from training samples with explicit class labels, and possesses good scala- bility with respect to increasing feature dimensionality and growing data volumes. Its applications to face verification and retrieval outperform existing state-of-the-art methods in accuracy while retaining high efficiency. version:1
arxiv-1212-6058 | High Quality Image Interpolation via Local Autoregressive and Nonlocal 3-D Sparse Regularization | http://arxiv.org/abs/1212.6058 | id:1212.6058 author:Xinwei Gao, Jian Zhang, Feng Jiang, Xiaopeng Fan, Siwei Ma, Debin Zhao category:cs.MM cs.CV  published:2012-12-25 summary:In this paper, we propose a novel image interpolation algorithm, which is formulated via combining both the local autoregressive (AR) model and the nonlocal adaptive 3-D sparse model as regularized constraints under the regularization framework. Estimating the high-resolution image by the local AR regularization is different from these conventional AR models, which weighted calculates the interpolation coefficients without considering the rough structural similarity between the low-resolution (LR) and high-resolution (HR) images. Then the nonlocal adaptive 3-D sparse model is formulated to regularize the interpolated HR image, which provides a way to modify these pixels with the problem of numerical stability caused by AR model. In addition, a new Split-Bregman based iterative algorithm is developed to solve the above optimization problem iteratively. Experiment results demonstrate that the proposed algorithm achieves significant performance improvements over the traditional algorithms in terms of both objective quality and visual perception version:1
arxiv-1212-6031 | Tangent Bundle Manifold Learning via Grassmann&Stiefel Eigenmaps | http://arxiv.org/abs/1212.6031 | id:1212.6031 author:Alexander V. Bernstein, Alexander P. Kuleshov category:cs.LG 68T05  published:2012-12-25 summary:One of the ultimate goals of Manifold Learning (ML) is to reconstruct an unknown nonlinear low-dimensional manifold embedded in a high-dimensional observation space by a given set of data points from the manifold. We derive a local lower bound for the maximum reconstruction error in a small neighborhood of an arbitrary point. The lower bound is defined in terms of the distance between tangent spaces to the original manifold and the estimated manifold at the considered point and reconstructed point, respectively. We propose an amplification of the ML, called Tangent Bundle ML, in which the proximity not only between the original manifold and its estimator but also between their tangent spaces is required. We present a new algorithm that solves this problem and gives a new solution for the ML also. version:1
arxiv-1212-6018 | Exponentially Weighted Moving Average Charts for Detecting Concept Drift | http://arxiv.org/abs/1212.6018 | id:1212.6018 author:Gordon J. Ross, Niall M. Adams, Dimitris K. Tasoulis, David J. Hand category:stat.ML cs.LG stat.AP  published:2012-12-25 summary:Classifying streaming data requires the development of methods which are computationally efficient and able to cope with changes in the underlying distribution of the stream, a phenomenon known in the literature as concept drift. We propose a new method for detecting concept drift which uses an Exponentially Weighted Moving Average (EWMA) chart to monitor the misclassification rate of an streaming classifier. Our approach is modular and can hence be run in parallel with any underlying classifier to provide an additional layer of concept drift detection. Moreover our method is computationally efficient with overhead O(1) and works in a fully online manner with no need to store data points in memory. Unlike many existing approaches to concept drift detection, our method allows the rate of false positive detections to be controlled and kept constant over time. version:1
arxiv-1301-0289 | Reconstructing Self Organizing Maps as Spider Graphs for better visual interpretation of large unstructured datasets | http://arxiv.org/abs/1301.0289 | id:1301.0289 author:Aaditya Prakash category:cs.GR stat.ML  published:2012-12-24 summary:Self-Organizing Maps (SOM) are popular unsupervised artificial neural network used to reduce dimensions and visualize data. Visual interpretation from Self-Organizing Maps (SOM) has been limited due to grid approach of data representation, which makes inter-scenario analysis impossible. The paper proposes a new way to structure SOM. This model reconstructs SOM to show strength between variables as the threads of a cobweb and illuminate inter-scenario analysis. While Radar Graphs are very crude representation of spider web, this model uses more lively and realistic cobweb representation to take into account the difference in strength and length of threads. This model allows for visualization of highly unstructured dataset with large number of dimensions, common in Bigdata sources. version:1
arxiv-1212-5921 | Distributed optimization of deeply nested systems | http://arxiv.org/abs/1212.5921 | id:1212.5921 author:Miguel . Carreira-Perpin, Weiran Wang category:cs.LG cs.NE math.OC stat.ML  published:2012-12-24 summary:In science and engineering, intelligent processing of complex signals such as images, sound or language is often performed by a parameterized hierarchy of nonlinear processing layers, sometimes biologically inspired. Hierarchical systems (or, more generally, nested systems) offer a way to generate complex mappings using simple stages. Each layer performs a different operation and achieves an ever more sophisticated representation of the input, as, for example, in an deep artificial neural network, an object recognition cascade in computer vision or a speech front-end processing. Joint estimation of the parameters of all the layers and selection of an optimal architecture is widely considered to be a difficult numerical nonconvex optimization problem, difficult to parallelize for execution in a distributed computation environment, and requiring significant human expert effort, which leads to suboptimal systems in practice. We describe a general mathematical strategy to learn the parameters and, to some extent, the architecture of nested systems, called the method of auxiliary coordinates (MAC). This replaces the original problem involving a deeply nested function with a constrained problem involving a different function in an augmented space without nesting. The constrained problem may be solved with penalty-based methods using alternating optimization over the parameters and the auxiliary coordinates. MAC has provable convergence, is easy to implement reusing existing algorithms for single layers, can be parallelized trivially and massively, applies even when parameter derivatives are not available or not desirable, and is competitive with state-of-the-art nonlinear optimizers even in the serial computation setting, often providing reasonable models within a few iterations. version:1
arxiv-1212-5860 | A short note on the tail bound of Wishart distribution | http://arxiv.org/abs/1212.5860 | id:1212.5860 author:Shenghuo Zhu category:math.ST cs.LG stat.TH  published:2012-12-24 summary:We study the tail bound of the emperical covariance of multivariate normal distribution. Following the work of (Gittens & Tropp, 2011), we provide a tail bound with a small constant. version:1
arxiv-1212-5777 | Collaborating Robotics Using Nature-Inspired Meta-Heuristics | http://arxiv.org/abs/1212.5777 | id:1212.5777 author:M. A. El-Dosuky, M. Z. Rashad, T. T. Hamza, A. H. EL-Bassiouny category:cs.NE cs.RO  published:2012-12-23 summary:This paper introduces collaborating robots which provide the possibility of enhanced task performance, high reliability and decreased. Collaborating-bots are a collection of mobile robots able to self-assemble and to self-organize in order to solve problems that cannot be solved by a single robot. These robots combine the power of swarm intelligence with the flexibility of self-reconfiguration as aggregate Collaborating-bots can dynamically change their structure to match environmental variations. Collaborating robots are more than just networks of independent agents, they are potentially reconfigurable networks of communicating agents capable of coordinated sensing and interaction with the environment. Robots are going to be an important part of the future. Collaborating robots are limited in individual capability, but robots deployed in large numbers can represent a strong force similar to a colony of ants or swarm of bees. We present a mechanism for collaborating robots based on swarm intelligence such as Ant colony optimization and Particle swarm Optimization version:1
arxiv-1212-5701 | ADADELTA: An Adaptive Learning Rate Method | http://arxiv.org/abs/1212.5701 | id:1212.5701 author:Matthew D. Zeiler category:cs.LG  published:2012-12-22 summary:We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment. version:1
arxiv-1210-0690 | Revisiting the Training of Logic Models of Protein Signaling Networks with a Formal Approach based on Answer Set Programming | http://arxiv.org/abs/1210.0690 | id:1210.0690 author:Santiago Videla, Carito Guziolowski, Federica Eduati, Sven Thiele, Niels Grabe, Julio Saez-Rodriguez, Anne Siegel category:q-bio.QM cs.AI cs.CE cs.LG  published:2012-10-02 summary:A fundamental question in systems biology is the construction and training to data of mathematical models. Logic formalisms have become very popular to model signaling networks because their simplicity allows us to model large systems encompassing hundreds of proteins. An approach to train (Boolean) logic models to high-throughput phospho-proteomics data was recently introduced and solved using optimization heuristics based on stochastic methods. Here we demonstrate how this problem can be solved using Answer Set Programming (ASP), a declarative problem solving paradigm, in which a problem is encoded as a logical program such that its answer sets represent solutions to the problem. ASP has significant improvements over heuristic methods in terms of efficiency and scalability, it guarantees global optimality of solutions as well as provides a complete set of solutions. We illustrate the application of ASP with in silico cases based on realistic networks and data. version:2
arxiv-1212-5656 | High-precision camera distortion measurements with a "calibration harp" | http://arxiv.org/abs/1212.5656 | id:1212.5656 author:Zhongwei Tang, Rafael Grompone von Gioi, Pascal Monasse, Jean-Michel Morel category:cs.CV  published:2012-12-22 summary:This paper addresses the high precision measurement of the distortion of a digital camera from photographs. Traditionally, this distortion is measured from photographs of a flat pattern which contains aligned elements. Nevertheless, it is nearly impossible to fabricate a very flat pattern and to validate its flatness. This fact limits the attainable measurable precisions. In contrast, it is much easier to obtain physically very precise straight lines by tightly stretching good quality strings on a frame. Taking literally "plumb-line methods", we built a "calibration harp" instead of the classic flat patterns to obtain a high precision measurement tool, demonstrably reaching 2/100 pixel precisions. The harp is complemented with the algorithms computing automatically from harp photographs two different and complementary lens distortion measurements. The precision of the method is evaluated on images corrected by state-of-the-art distortion correction algorithms, and by popular software. Three applications are shown: first an objective and reliable measurement of the result of any distortion correction. Second, the harp permits to control state-of-the art global camera calibration algorithms: It permits to select the right distortion model, thus avoiding internal compensation errors inherent to these methods. Third, the method replaces manual procedures in other distortion correction methods, makes them fully automatic, and increases their reliability and precision. version:1
arxiv-1212-5637 | Random Spanning Trees and the Prediction of Weighted Graphs | http://arxiv.org/abs/1212.5637 | id:1212.5637 author:Nicolo' Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella category:cs.LG stat.ML  published:2012-12-21 summary:We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world datasets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice. version:1
arxiv-1207-1727 | Mixtures of Shifted Asymmetric Laplace Distributions | http://arxiv.org/abs/1207.1727 | id:1207.1727 author:Brian C. Franczak, Ryan P. Browne, Paul D. McNicholas category:stat.ME stat.CO stat.ML  published:2012-07-06 summary:A mixture of shifted asymmetric Laplace distributions is introduced and used for clustering and classification. A variant of the EM algorithm is developed for parameter estimation by exploiting the relationship with the general inverse Gaussian distribution. This approach is mathematically elegant and relatively computationally straightforward. Our novel mixture modelling approach is demonstrated on both simulated and real data to illustrate clustering and classification applications. In these analyses, our mixture of shifted asymmetric Laplace distributions performs favourably when compared to the popular Gaussian approach. This work, which marks an important step in the non-Gaussian model-based clustering and classification direction, concludes with discussion as well as suggestions for future work. version:3
arxiv-1212-3900 | A Tutorial on Probabilistic Latent Semantic Analysis | http://arxiv.org/abs/1212.3900 | id:1212.3900 author:Liangjie Hong category:stat.ML cs.LG  published:2012-12-17 summary:In this tutorial, I will discuss the details about how Probabilistic Latent Semantic Analysis (PLSA) is formalized and how different learning algorithms are proposed to learn the model. version:2
arxiv-1212-5454 | In Vivo Quantification of Clot Formation in Extracorporeal Circuits | http://arxiv.org/abs/1212.5454 | id:1212.5454 author:Omid David, Rabin Gerrah category:cs.CV physics.med-ph  published:2012-12-21 summary:Clot formation is a common complication in extracorporeal circuits. In this paper we describe a novel method for clot formation analysis using image processing. We assembled a closed extracorporeal circuit and circulated blood at varying speeds. Blood filters were placed in downstream of the flow, and clotting agents were added to the circuit. Digital images of the filter were subsequently taken, and image analysis was applied to calculate the density of the clot. Our results show a significant correlation between the cumulative size of the clots, the density measure of the clot based on image analysis, and flow duration in the system. version:1
arxiv-1212-5594 | Black box modelling of HVAC system : improving the performances of neural networks | http://arxiv.org/abs/1212.5594 | id:1212.5594 author:Eric Fock, Thierry Alex Mara, Alfred Jean Philippe Lauret, Harry Boyer category:cs.NE cs.CE  published:2012-12-21 summary:This paper deals with neural networks modelling of HVAC systems. In order to increase the neural networks performances, a method based on sensitivity analysis is applied. The same technique is also used to compute the relevance of each input. To avoid the prediction errors in dry coil conditions, a metamodel for each capacity is derived from the neural networks. The regression coefficients of the polynomial forms are identified through the use of spectral analysis. These methods based on sensitivity and spectral analysis lead to an optimized neural network model, as regard to its architecture and predictions. version:1
arxiv-1212-5391 | Soft Set Based Feature Selection Approach for Lung Cancer Images | http://arxiv.org/abs/1212.5391 | id:1212.5391 author:G. Jothi, H. Hannah Inbarani category:cs.LG cs.CE  published:2012-12-21 summary:Lung cancer is the deadliest type of cancer for both men and women. Feature selection plays a vital role in cancer classification. This paper investigates the feature selection process in Computed Tomographic (CT) lung cancer images using soft set theory. We propose a new soft set based unsupervised feature selection algorithm. Nineteen features are extracted from the segmented lung images using gray level co-occurence matrix (GLCM) and gray level different matrix (GLDM). In this paper, an efficient Unsupervised Soft Set based Quick Reduct (SSUSQR) algorithm is presented. This method is used to select features from the data set and compared with existing rough set based unsupervised feature selection methods. Then K-Means and Self Organizing Map (SOM) clustering algorithms are used to cluster the data. The performance of the feature selection algorithms is evaluated based on performance of clustering techniques. The results show that the proposed method effectively removes redundant features. version:1
arxiv-1212-5359 | Fuzzy soft rough K-Means clustering approach for gene expression data | http://arxiv.org/abs/1212.5359 | id:1212.5359 author:K. Dhanalakshmi, H. Hannah Inbarani category:cs.LG cs.CE  published:2012-12-21 summary:Clustering is one of the widely used data mining techniques for medical diagnosis. Clustering can be considered as the most important unsupervised learning technique. Most of the clustering methods group data based on distance and few methods cluster data based on similarity. The clustering algorithms classify gene expression data into clusters and the functionally related genes are grouped together in an efficient manner. The groupings are constructed such that the degree of relationship is strong among members of the same cluster and weak among members of different clusters. In this work, we focus on a similarity relationship among genes with similar expression patterns so that a consequential and simple analytical decision can be made from the proposed Fuzzy Soft Rough K-Means algorithm. The algorithm is developed based on Fuzzy Soft sets and Rough sets. Comparative analysis of the proposed work is made with bench mark algorithms like K-Means and Rough K-Means and efficiency of the proposed algorithm is illustrated in this work by using various cluster validity measures such as DB index and Xie-Beni index. version:1
arxiv-1212-5352 | On the Adaptability of Neural Network Image Super-Resolution | http://arxiv.org/abs/1212.5352 | id:1212.5352 author:Kah Keong Chua, Yong Haur Tay category:cs.CV  published:2012-12-21 summary:In this paper, we described and developed a framework for Multilayer Perceptron (MLP) to work on low level image processing, where MLP will be used to perform image super-resolution. Meanwhile, MLP are trained with different types of images from various categories, hence analyse the behaviour and performance of the neural network. The tests are carried out using qualitative test, in which Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). The results showed that MLP trained with single image category can perform reasonably well compared to methods proposed by other researchers. version:1
arxiv-1210-2164 | ET-LDA: Joint Topic Modeling For Aligning, Analyzing and Sensemaking of Public Events and Their Twitter Feeds | http://arxiv.org/abs/1210.2164 | id:1210.2164 author:Yuheng Hu, Ajita John, Fei Wang, Doree Duncan Seligmann, Subbarao Kambhampati category:cs.LG cs.AI cs.SI physics.soc-ph  published:2012-10-08 summary:Social media channels such as Twitter have emerged as popular platforms for crowds to respond to public events such as speeches, sports and debates. While this promises tremendous opportunities to understand and make sense of the reception of an event from the social media, the promises come entwined with significant technical challenges. In particular, given an event and an associated large scale collection of tweets, we need approaches to effectively align tweets and the parts of the event they refer to. This in turn raises questions about how to segment the event into smaller yet meaningful parts, and how to figure out whether a tweet is a general one about the entire event or specific one aimed at a particular segment of the event. In this work, we present ET-LDA, an effective method for aligning an event and its tweets through joint statistical modeling of topical influences from the events and their associated tweets. The model enables the automatic segmentation of the events and the characterization of tweets into two categories: (1) episodic tweets that respond specifically to the content in the segments of the events, and (2) steady tweets that respond generally about the events. We present an efficient inference method for this model, and a comprehensive evaluation of its effectiveness over existing methods. In particular, through a user study, we demonstrate that users find the topics, the segments, the alignment, and the episodic tweets discovered by ET-LDA to be of higher quality and more interesting as compared to the state-of-the-art, with improvements in the range of 18-41%. version:3
arxiv-1212-5271 | Towards the Evolution of Novel Vertical-Axis Wind Turbines | http://arxiv.org/abs/1212.5271 | id:1212.5271 author:Richard J. Preen, Larry Bull category:cs.NE cs.AI  published:2012-12-20 summary:Renewable and sustainable energy is one of the most important challenges currently facing mankind. Wind has made an increasing contribution to the world's energy supply mix, but still remains a long way from reaching its full potential. In this paper, we investigate the use of artificial evolution to design vertical-axis wind turbine prototypes that are physically instantiated and evaluated under approximated wind tunnel conditions. An artificial neural network is used as a surrogate model to assist learning and found to reduce the number of fabrications required to reach a higher aerodynamic efficiency, resulting in an important cost reduction. Unlike in other approaches, such as computational fluid dynamics simulations, no mathematical formulations are used and no model assumptions are made. version:1
arxiv-1212-2002 | A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method | http://arxiv.org/abs/1212.2002 | id:1212.2002 author:Simon Lacoste-Julien, Mark Schmidt, Francis Bach category:cs.LG math.OC stat.ML 90C15  68T05  65K10 G.1.6; I.2.6  published:2012-12-10 summary:In this note, we present a new averaging technique for the projected stochastic subgradient method. By using a weighted average with a weight of t+1 for each iterate w_t at iteration t, we obtain the convergence rate of O(1/t) with both an easy proof and an easy implementation. The new scheme is compared empirically to existing techniques, with similar performance behavior. version:2
arxiv-1212-5238 | The Twitter of Babel: Mapping World Languages through Microblogging Platforms | http://arxiv.org/abs/1212.5238 | id:1212.5238 author:Delia Mocanu, Andrea Baronchelli, Bruno Gonalves, Nicola Perra, Alessandro Vespignani category:physics.soc-ph cs.CL cs.SI  published:2012-12-20 summary:Large scale analysis and statistics of socio-technical systems that just a few short years ago would have required the use of consistent economic and human resources can nowadays be conveniently performed by mining the enormous amount of digital data produced by human activities. Although a characterization of several aspects of our societies is emerging from the data revolution, a number of questions concerning the reliability and the biases inherent to the big data "proxies" of social life are still open. Here, we survey worldwide linguistic indicators and trends through the analysis of a large-scale dataset of microblogging posts. We show that available data allow for the study of language geography at scales ranging from country-level aggregation to specific city neighborhoods. The high resolution and coverage of the data allows us to investigate different indicators such as the linguistic homogeneity of different countries, the touristic seasonal patterns within countries and the geographical distribution of different languages in multilingual regions. This work highlights the potential of geolocalized studies of open data sources to improve current analysis and develop indicators for major social phenomena in specific communities. version:1
arxiv-1212-5217 | A Neural Network Approach to ECG Denoising | http://arxiv.org/abs/1212.5217 | id:1212.5217 author:Rui Rodrigues, Paula Couto category:cs.CE cs.NE  published:2012-12-20 summary:We propose an ECG denoising method based on a feed forward neural network with three hidden layers. Particulary useful for very noisy signals, this approach uses the available ECG channels to reconstruct a noisy channel. We tested the method, on all the records from Physionet MIT-BIH Arrhythmia Database, adding electrode motion artifact noise. This denoising method improved the perfomance of publicly available ECG analysis programs on noisy ECG signals. This is an offline method that can be used to remove noise from very corrupted Holter records. version:1
arxiv-1212-5203 | An Experiment with Hierarchical Bayesian Record Linkage | http://arxiv.org/abs/1212.5203 | id:1212.5203 author:Michael D. Larsen category:math.ST stat.AP stat.CO stat.ME stat.ML stat.TH G.3.0  published:2012-12-20 summary:In record linkage (RL), or exact file matching, the goal is to identify the links between entities with information on two or more files. RL is an important activity in areas including counting the population, enhancing survey frames and data, and conducting epidemiological and follow-up studies. RL is challenging when files are very large, no accurate personal identification (ID) number is present on all files for all units, and some information is recorded with error. Without an unique ID number one must rely on comparisons of names, addresses, dates, and other information to find the links. Latent class models can be used to automatically score the value of information for determining match status. Data for fitting models come from comparisons made within groups of units that pass initial file blocking requirements. Data distributions can vary across blocks. This article examines the use of prior information and hierarchical latent class models in the context of RL. version:1
arxiv-1212-4507 | Variational Optimization | http://arxiv.org/abs/1212.4507 | id:1212.4507 author:Joe Staines, David Barber category:stat.ML cs.LG cs.NA 65K10 G.1.6  published:2012-12-18 summary:We discuss a general technique that can be used to form a differentiable bound on the optima of non-differentiable or discrete objective functions. We form a unified description of these methods and consider under which circumstances the bound is concave. In particular we consider two concrete applications of the method, namely sparse learning and support vector classification. version:2
arxiv-1212-5101 | Hybrid Fuzzy-ART based K-Means Clustering Methodology to Cellular Manufacturing Using Operational Time | http://arxiv.org/abs/1212.5101 | id:1212.5101 author:Sourav Sengupta, Tamal Ghosh, Pranab K Dan, Manojit Chattopadhyay category:cs.LG  published:2012-12-20 summary:This paper presents a new hybrid Fuzzy-ART based K-Means Clustering technique to solve the part machine grouping problem in cellular manufacturing systems considering operational time. The performance of the proposed technique is tested with problems from open literature and the results are compared to the existing clustering models such as simple K-means algorithm and modified ART1 algorithm using an efficient modified performance measure known as modified grouping efficiency (MGE) as found in the literature. The results support the better performance of the proposed algorithm. The Novelty of this study lies in the simple and efficient methodology to produce quick solutions for shop floor managers with least computational efforts and time. version:1
arxiv-1212-4920 | Automatic landmark annotation and dense correspondence registration for 3D human facial images | http://arxiv.org/abs/1212.4920 | id:1212.4920 author:Jianya Guo, Xi Mei, Kun Tang category:cs.CV q-bio.QM  published:2012-12-20 summary:Dense surface registration of three-dimensional (3D) human facial images holds great potential for studies of human trait diversity, disease genetics, and forensics. Non-rigid registration is particularly useful for establishing dense anatomical correspondences between faces. Here we describe a novel non-rigid registration method for fully automatic 3D facial image mapping. This method comprises two steps: first, seventeen facial landmarks are automatically annotated, mainly via PCA-based feature recognition following 3D-to-2D data transformation. Second, an efficient thin-plate spline (TPS) protocol is used to establish the dense anatomical correspondence between facial images, under the guidance of the predefined landmarks. We demonstrate that this method is robust and highly accurate, even for different ethnicities. The average face is calculated for individuals of Han Chinese and Uyghur origins. While fully automatic and computationally efficient, this method enables high-throughput analysis of human facial feature variation. version:1
arxiv-1212-4906 | SMML estimators for 1-dimensional continuous data | http://arxiv.org/abs/1212.4906 | id:1212.4906 author:James G. Dowty category:cs.IT math.IT math.ST stat.ML stat.TH  published:2012-12-20 summary:A method is given for calculating the strict minimum message length (SMML) estimator for 1-dimensional exponential families with continuous sufficient statistics. A set of $n$ equations are found that the $n$ cut-points of the SMML estimator must satisfy. These equations can be solved using Newton's method and this approach is used to produce new results and to replicate results that C. S. Wallace obtained using his boundary rules for the SMML estimator. A rigorous proof is also given that, despite being composed of step functions, the posterior probability corresponding to the SMML estimator is a continuous function of the data. version:1
arxiv-1212-4871 | Automatic post-picking using MAPPOS improves particle image detection from Cryo-EM micrographs | http://arxiv.org/abs/1212.4871 | id:1212.4871 author:Ramin Norousi, Stephan Wickles, Christoph Leidig, Thomas Becker, Volker J. Schmid, Roland Beckmann, Achim Tresch category:stat.ML cs.CV  published:2012-12-19 summary:Cryo-electron microscopy (cryo-EM) studies using single particle reconstruction are extensively used to reveal structural information on macromolecular complexes. Aiming at the highest achievable resolution, state of the art electron microscopes automatically acquire thousands of high-quality micrographs. Particles are detected on and boxed out from each micrograph using fully- or semi-automated approaches. However, the obtained particles still require laborious manual post-picking classification, which is one major bottleneck for single particle analysis of large datasets. We introduce MAPPOS, a supervised post-picking strategy for the classification of boxed particle images, as additional strategy adding to the already efficient automated particle picking routines. MAPPOS employs machine learning techniques to train a robust classifier from a small number of characteristic image features. In order to accurately quantify the performance of MAPPOS we used simulated particle and non-particle images. In addition, we verified our method by applying it to an experimental cryo-EM dataset and comparing the results to the manual classification of the same dataset. Comparisons between MAPPOS and manual post-picking classification by several human experts demonstrated that merely a few hundred sample images are sufficient for MAPPOS to classify an entire dataset with a human-like performance. MAPPOS was shown to greatly accelerate the throughput of large datasets by reducing the manual workload by orders of magnitude while maintaining a reliable identification of non-particle images. version:1
arxiv-1212-4777 | A Practical Algorithm for Topic Modeling with Provable Guarantees | http://arxiv.org/abs/1212.4777 | id:1212.4777 author:Sanjeev Arora, Rong Ge, Yoni Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu, Michael Zhu category:cs.LG cs.DS stat.ML  published:2012-12-19 summary:Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model inference have been based on a maximum likelihood objective. Efficient algorithms exist that approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for topic model inference that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster. version:1
arxiv-1212-5091 | Maximally Informative Observables and Categorical Perception | http://arxiv.org/abs/1212.5091 | id:1212.5091 author:Elaine Tsiang category:cs.LG cs.SD  published:2012-12-19 summary:We formulate the problem of perception in the framework of information theory, and prove that categorical perception is equivalent to the existence of an observable that has the maximum possible information on the target of perception. We call such an observable maximally informative. Regardless whether categorical perception is real, maximally informative observables can form the basis of a theory of perception. We conclude with the implications of such a theory for the problem of speech perception. version:1
arxiv-1212-4674 | Natural Language Understanding Based on Semantic Relations between Sentences | http://arxiv.org/abs/1212.4674 | id:1212.4674 author:Hyeok Kong category:cs.CL  published:2012-12-19 summary:In this paper, we define event expression over sentences of natural language and semantic relations between events. Based on this definition, we formally consider text understanding process having events as basic unit. version:1
arxiv-1212-4608 | Perceptually Motivated Shape Context Which Uses Shape Interiors | http://arxiv.org/abs/1212.4608 | id:1212.4608 author:Vittal Premachandran, Ramakrishna Kakarala category:cs.CV  published:2012-12-19 summary:In this paper, we identify some of the limitations of current-day shape matching techniques. We provide examples of how contour-based shape matching techniques cannot provide a good match for certain visually similar shapes. To overcome this limitation, we propose a perceptually motivated variant of the well-known shape context descriptor. We identify that the interior properties of the shape play an important role in object recognition and develop a descriptor that captures these interior properties. We show that our method can easily be augmented with any other shape matching algorithm. We also show from our experiments that the use of our descriptor can significantly improve the retrieval rates. version:1
arxiv-1206-3382 | Simple Regret Optimization in Online Planning for Markov Decision Processes | http://arxiv.org/abs/1206.3382 | id:1206.3382 author:Zohar Feldman, Carmel Domshlak category:cs.AI cs.LG  published:2012-06-15 summary:We consider online planning in Markov decision processes (MDPs). In online planning, the agent focuses on its current state only, deliberates about the set of possible policies from that state onwards and, when interrupted, uses the outcome of that exploratory deliberation to choose what action to perform next. The performance of algorithms for online planning is assessed in terms of simple regret, which is the agent's expected performance loss when the chosen action, rather than an optimal one, is followed. To date, state-of-the-art algorithms for online planning in general MDPs are either best effort, or guarantee only polynomial-rate reduction of simple regret over time. Here we introduce a new Monte-Carlo tree search algorithm, BRUE, that guarantees exponential-rate reduction of simple regret and error probability. This algorithm is based on a simple yet non-standard state-space sampling scheme, MCTS2e, in which different parts of each sample are dedicated to different exploratory objectives. Our empirical evaluation shows that BRUE not only provides superior performance guarantees, but is also very effective in practice and favorably compares to state-of-the-art. We then extend BRUE with a variant of "learning by forgetting." The resulting set of algorithms, BRUE(alpha), generalizes BRUE, improves the exponential factor in the upper bound on its reduction rate, and exhibits even more attractive empirical performance. version:2
arxiv-1212-4562 | A complexity analysis of statistical learning algorithms | http://arxiv.org/abs/1212.4562 | id:1212.4562 author:Mark A. Kon category:stat.ML  published:2012-12-19 summary:We apply information-based complexity analysis to support vector machine (SVM) algorithms, with the goal of a comprehensive continuous algorithmic analysis of such algorithms. This involves complexity measures in which some higher order operations (e.g., certain optimizations) are considered primitive for the purposes of measuring complexity. We consider classes of information operators and algorithms made up of scaled families, and investigate the utility of scaling the complexities to minimize error. We look at the division of statistical learning into information and algorithmic components, at the complexities of each, and at applications to support vector machine (SVM) and more general machine learning algorithms. We give applications to SVM algorithms graded into linear and higher order components, and give an example in biomedical informatics. version:1
arxiv-1212-4527 | GMM-Based Hidden Markov Random Field for Color Image and 3D Volume Segmentation | http://arxiv.org/abs/1212.4527 | id:1212.4527 author:Quan Wang category:cs.CV  published:2012-12-18 summary:In this project, we first study the Gaussian-based hidden Markov random field (HMRF) model and its expectation-maximization (EM) algorithm. Then we generalize it to Gaussian mixture model-based hidden Markov random field. The algorithm is implemented in MATLAB. We also apply this algorithm to color image segmentation problems and 3D volume segmentation problems. version:1
arxiv-1207-3510 | HMRF-EM-image: Implementation of the Hidden Markov Random Field Model and its Expectation-Maximization Algorithm | http://arxiv.org/abs/1207.3510 | id:1207.3510 author:Quan Wang category:cs.CV  published:2012-07-15 summary:In this project, we study the hidden Markov random field (HMRF) model and its expectation-maximization (EM) algorithm. We implement a MATLAB toolbox named HMRF-EM-image for 2D image segmentation using the HMRF-EM framework. This toolbox also implements edge-prior-preserving image segmentation, and can be easily reconfigured for other problems, such as 3D image segmentation. version:2
arxiv-1212-4675 | Analysis of Large-scale Traffic Dynamics using Non-negative Tensor Factorization | http://arxiv.org/abs/1212.4675 | id:1212.4675 author:Yufei Han, Fabien Moutarde category:cs.LG  published:2012-12-18 summary:In this paper, we present our work on clustering and prediction of temporal dynamics of global congestion configurations in large-scale road networks. Instead of looking into temporal traffic state variation of individual links, or of small areas, we focus on spatial congestion configurations of the whole network. In our work, we aim at describing the typical temporal dynamic patterns of this network-level traffic state and achieving long-term prediction of the large-scale traffic dynamics, in a unified data-mining framework. To this end, we formulate this joint task using Non-negative Tensor Factorization (NTF), which has been shown to be a useful decomposition tools for multivariate data sequences. Clustering and prediction are performed based on the compact tensor factorization results. Experiments on large-scale simulated data illustrate the interest of our method with promising results for long-term forecast of traffic evolution. version:1
arxiv-1212-3225 | Identification of Nonlinear Systems From the Knowledge Around Different Operating Conditions: A Feed-Forward Multi-Layer ANN Based Approach | http://arxiv.org/abs/1212.3225 | id:1212.3225 author:Sayan Saha, Saptarshi Das, Anish Acharya, Abhishek Kumar, Sumit Mukherjee, Indranil Pan, Amitava Gupta category:cs.SY cs.NE  published:2012-12-13 summary:The paper investigates nonlinear system identification using system output data at various linearized operating points. A feed-forward multi-layer Artificial Neural Network (ANN) based approach is used for this purpose and tested for two target applications i.e. nuclear reactor power level monitoring and an AC servo position control system. Various configurations of ANN using different activation functions, number of hidden layers and neurons in each layer are trained and tested to find out the best configuration. The training is carried out multiple times to check for consistency and the mean and standard deviation of the root mean square errors (RMSE) are reported for each configuration. version:3
arxiv-1210-2613 | Measuring the Influence of Observations in HMMs through the Kullback-Leibler Distance | http://arxiv.org/abs/1210.2613 | id:1210.2613 author:Vittorio Perduca, Gregory Nuel category:cs.IT cs.LG math.IT math.PR  published:2012-10-09 summary:We measure the influence of individual observations on the sequence of the hidden states of the Hidden Markov Model (HMM) by means of the Kullback-Leibler distance (KLD). Namely, we consider the KLD between the conditional distribution of the hidden states' chain given the complete sequence of observations and the conditional distribution of the hidden chain given all the observations but the one under consideration. We introduce a linear complexity algorithm for computing the influence of all the observations. As an illustration, we investigate the application of our algorithm to the problem of detecting outliers in HMM data series. version:2
arxiv-1212-3767 | Visual Objects Classification with Sliding Spatial Pyramid Matching | http://arxiv.org/abs/1212.3767 | id:1212.3767 author:Hao Wooi Lim, Yong Haur Tay category:cs.CV  published:2012-12-16 summary:We present a method for visual object classification using only a single feature, transformed color SIFT with a variant of Spatial Pyramid Matching (SPM) that we called Sliding Spatial Pyramid Matching (SSPM), trained with an ensemble of linear regression (provided by LINEAR) to obtained state of the art result on Caltech-101 of 83.46%. SSPM is a special version of SPM where instead of dividing an image into K number of regions, a subwindow of fixed size is slide around the image with a fixed step size. For each subwindow, a histogram of visual words is generated. To obtained the visual vocabulary, instead of performing K-means clustering, we randomly pick N exemplars from the training set and encode them with a soft non-linear mapping method. We then trained 15 models, each with a different visual word size with linear regression. All 15 models are then averaged together to form a single strong model. version:2
arxiv-1212-4347 | Bayesian Group Nonnegative Matrix Factorization for EEG Analysis | http://arxiv.org/abs/1212.4347 | id:1212.4347 author:Bonggun Shin, Alice Oh category:cs.LG stat.ML  published:2012-12-18 summary:We propose a generative model of a group EEG analysis, based on appropriate kernel assumptions on EEG data. We derive the variational inference update rule using various approximation techniques. The proposed model outperforms the current state-of-the-art algorithms in terms of common pattern extraction. The validity of the proposed model is tested on the BCI competition dataset. version:1
arxiv-1212-4490 | Sketch-to-Design: Context-based Part Assembly | http://arxiv.org/abs/1212.4490 | id:1212.4490 author:Xiaohua Xie, Kai Xu, Niloy J. Mitra, Daniel Cohen-Or, Baoquan Chen category:cs.GR cs.CV  published:2012-12-18 summary:Designing 3D objects from scratch is difficult, especially when the user intent is fuzzy without a clear target form. In the spirit of modeling-by-example, we facilitate design by providing reference and inspiration from existing model contexts. We rethink model design as navigating through different possible combinations of part assemblies based on a large collection of pre-segmented 3D models. We propose an interactive sketch-to-design system, where the user sketches prominent features of parts to combine. The sketched strokes are analyzed individually and in context with the other parts to generate relevant shape suggestions via a design gallery interface. As the session progresses and more parts get selected, contextual cues becomes increasingly dominant and the system quickly converges to a final design. As a key enabler, we use pre-learned part-based contextual information to allow the user to quickly explore different combinations of parts. Our experiments demonstrate the effectiveness of our approach for efficiently designing new variations from existing shapes. version:1
arxiv-1212-4315 | Assessing Sentiment Strength in Words Prior Polarities | http://arxiv.org/abs/1212.4315 | id:1212.4315 author:Lorenzo Gatti, Marco Guerini category:cs.CL  published:2012-12-18 summary:Many approaches to sentiment analysis rely on lexica where words are tagged with their prior polarity - i.e. if a word out of context evokes something positive or something negative. In particular, broad-coverage resources like SentiWordNet provide polarities for (almost) every word. Since words can have multiple senses, we address the problem of how to compute the prior polarity of a word starting from the polarity of each sense and returning its polarity strength as an index between -1 and 1. We compare 14 such formulae that appear in the literature, and assess which one best approximates the human judgement of prior polarities, with both regression and classification models. version:1
arxiv-1212-5250 | A genetic algorithm applied to the validation of building thermal models | http://arxiv.org/abs/1212.5250 | id:1212.5250 author:Alfred Jean Philippe Lauret, Harry Boyer, Carine Riviere, Alain Bastide category:cs.NE  published:2012-12-18 summary:This paper presents the coupling of a building thermal simulation code with genetic algorithms (GAs). GAs are randomized search algorithms that are based on the mechanisms of natural selection and genetics. We show that this coupling allows the location of defective sub-models of a building thermal model i.e. parts of model that are responsible for the disagreements between measurements and model predictions. The method first of all is checked and validated on the basis of a numerical model of a building taken as reference. It is then applied to a real building case. The results show that the method could constitute an efficient tool when checking the model validity. version:1
arxiv-1207-4992 | Fast nonparametric classification based on data depth | http://arxiv.org/abs/1207.4992 | id:1207.4992 author:Tatjana Lange, Karl Mosler, Pavlo Mozharovskyi category:stat.ML cs.LG 62H30  published:2012-07-20 summary:A new procedure, called DDa-procedure, is developed to solve the problem of classifying d-dimensional objects into q >= 2 classes. The procedure is completely nonparametric; it uses q-dimensional depth plots and a very efficient algorithm for discrimination analysis in the depth space [0,1]^q. Specifically, the depth is the zonoid depth, and the algorithm is the alpha-procedure. In case of more than two classes several binary classifications are performed and a majority rule is applied. Special treatments are discussed for 'outsiders', that is, data having zero depth vector. The DDa-classifier is applied to simulated as well as real data, and the results are compared with those of similar procedures that have been recently proposed. In most cases the new procedure has comparable error rates, but is much faster than other classification approaches, including the SVM. version:2
arxiv-1211-0613 | Application of Symmetric Uncertainty and Mutual Information to Dimensionality Reduction and Classification of Hyperspectral Images | http://arxiv.org/abs/1211.0613 | id:1211.0613 author:ELkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine category:cs.CV  published:2012-11-03 summary:Remote sensing is a technology to acquire data for disatant substances, necessary to construct a model knowledge for applications as classification. Recently Hyperspectral Images (HSI) becomes a high technical tool that the main goal is to classify the point of a region. The HIS is more than a hundred bidirectional measures, called bands (or simply images), of the same region called Ground Truth Map (GT). But some bands are not relevant because they are affected by different atmospheric effects; others contain redundant information; and high dimensionality of HSI features make the accuracy of classification lower. All these bands can be important for some applications; but for the classification a small subset of these is relevant. The problematic related to HSI is the dimensionality reduction. Many studies use mutual information (MI) to select the relevant bands. Others studies use the MI normalized forms, like Symmetric Uncertainty, in medical imagery applications. In this paper we introduce an algorithm based also on MI to select relevant bands and it apply the Symmetric Uncertainty coefficient to control redundancy and increase the accuracy of classification. This algorithm is feature selection tool and a Filter strategy. We establish this study on HSI AVIRIS 92AV3C. This is an effectiveness, and fast scheme to control redundancy. version:2
arxiv-1212-4174 | Feature Clustering for Accelerating Parallel Coordinate Descent | http://arxiv.org/abs/1212.4174 | id:1212.4174 author:Chad Scherrer, Ambuj Tewari, Mahantesh Halappanavar, David Haglin category:stat.ML cs.DC cs.LG math.OC  published:2012-12-17 summary:Large-scale L1-regularized loss minimization problems arise in high-dimensional applications such as compressed sensing and high-dimensional supervised learning, including classification and regression problems. High-performance algorithms and implementations are critical to efficiently solving these problems. Building upon previous work on coordinate descent algorithms for L1-regularized problems, we introduce a novel family of algorithms called block-greedy coordinate descent that includes, as special cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and Thread-Greedy. We give a unified convergence analysis for the family of block-greedy algorithms. The analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small. Our theoretical convergence analysis is supported with experimental re- sults using data from diverse real-world applications. We hope that algorithmic approaches and convergence analysis we provide will not only advance the field, but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale L1-regularization problems. version:1
arxiv-1212-4137 | Alternating Maximization: Unifying Framework for 8 Sparse PCA Formulations and Efficient Parallel Codes | http://arxiv.org/abs/1212.4137 | id:1212.4137 author:Peter Richtrik, Martin Tak, Selin Damla Ahipaaolu category:stat.ML cs.LG math.OC  published:2012-12-17 summary:Given a multivariate data set, sparse principal component analysis (SPCA) aims to extract several linear combinations of the variables that together explain the variance in the data as much as possible, while controlling the number of nonzero loadings in these combinations. In this paper we consider 8 different optimization formulations for computing a single sparse loading vector; these are obtained by combining the following factors: we employ two norms for measuring variance (L2, L1) and two sparsity-inducing norms (L0, L1), which are used in two different ways (constraint, penalty). Three of our formulations, notably the one with L0 constraint and L1 variance, have not been considered in the literature. We give a unifying reformulation which we propose to solve via a natural alternating maximization (AM) method. We show the the AM method is nontrivially equivalent to GPower (Journ\'{e}e et al; JMLR 11:517--553, 2010) for all our formulations. Besides this, we provide 24 efficient parallel SPCA implementations: 3 codes (multi-core, GPU and cluster) for each of the 8 problems. Parallelism in the methods is aimed at i) speeding up computations (our GPU code can be 100 times faster than an efficient serial code written in C++), ii) obtaining solutions explaining more variance and iii) dealing with big data problems (our cluster code is able to solve a 357 GB problem in about a minute). version:1
arxiv-1204-1564 | Minimal model of associative learning for cross-situational lexicon acquisition | http://arxiv.org/abs/1204.1564 | id:1204.1564 author:Paulo F. C. Tilles, Jose F. Fontanari category:q-bio.NC cs.LG  published:2012-04-06 summary:An explanation for the acquisition of word-object mappings is the associative learning in a cross-situational scenario. Here we present analytical results of the performance of a simple associative learning algorithm for acquiring a one-to-one mapping between $N$ objects and $N$ words based solely on the co-occurrence between objects and words. In particular, a learning trial in our learning scenario consists of the presentation of $C + 1 < N$ objects together with a target word, which refers to one of the objects in the context. We find that the learning times are distributed exponentially and the learning rates are given by $\ln{[\frac{N(N-1)}{C + (N-1)^{2}}]}$ in the case the $N$ target words are sampled randomly and by $\frac{1}{N} \ln [\frac{N-1}{C}] $ in the case they follow a deterministic presentation sequence. This learning performance is much superior to those exhibited by humans and more realistic learning algorithms in cross-situational experiments. We show that introduction of discrimination limitations using Weber's law and forgetting reduce the performance of the associative algorithm to the human level. version:4
arxiv-1212-3493 | Sentence Compression in Spanish driven by Discourse Segmentation and Language Models | http://arxiv.org/abs/1212.3493 | id:1212.3493 author:Alejandro Molina, Juan-Manuel Torres-Moreno, Iria da Cunha, Eric SanJuan, Gerardo Sierra category:cs.CL cs.IR  published:2012-12-14 summary:Previous works demonstrated that Automatic Text Summarization (ATS) by sentences extraction may be improved using sentence compression. In this work we present a sentence compressions approach guided by level-sentence discourse segmentation and probabilistic language models (LM). The results presented here show that the proposed solution is able to generate coherent summaries with grammatical compressed sentences. The approach is simple enough to be transposed into other languages. version:2
arxiv-1212-3873 | Learning Markov Decision Processes for Model Checking | http://arxiv.org/abs/1212.3873 | id:1212.3873 author:Hua Mao, Yingke Chen, Manfred Jaeger, Thomas D. Nielsen, Kim G. Larsen, Brian Nielsen category:cs.LG cs.LO cs.SE  published:2012-12-17 summary:Constructing an accurate system model for formal model verification can be both resource demanding and time-consuming. To alleviate this shortcoming, algorithms have been proposed for automatically learning system models based on observed system behaviors. In this paper we extend the algorithm on learning probabilistic automata to reactive systems, where the observed system behavior is in the form of alternating sequences of inputs and outputs. We propose an algorithm for automatically learning a deterministic labeled Markov decision process model from the observed behavior of a reactive system. The proposed learning algorithm is adapted from algorithms for learning deterministic probabilistic finite automata, and extended to include both probabilistic and nondeterministic transitions. The algorithm is empirically analyzed and evaluated by learning system models of slot machines. The evaluation is performed by analyzing the probabilistic linear temporal logic properties of the system as well as by analyzing the schedulers, in particular the optimal schedulers, induced by the learned models. version:1
arxiv-1212-3850 | Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees | http://arxiv.org/abs/1212.3850 | id:1212.3850 author:Nima Noorshams, Martin J. Wainwright category:cs.IT cs.LG math.IT stat.ML  published:2012-12-16 summary:The sum-product or belief propagation (BP) algorithm is a widely used message-passing technique for computing approximate marginals in graphical models. We introduce a new technique, called stochastic orthogonal series message-passing (SOSMP), for computing the BP fixed point in models with continuous random variables. It is based on a deterministic approximation of the messages via orthogonal series expansion, and a stochastic approximation via Monte Carlo estimates of the integral updates of the basis coefficients. We prove that the SOSMP iterates converge to a \delta-neighborhood of the unique BP fixed point for any tree-structured graph, and for any graphs with cycles in which the BP updates satisfy a contractivity condition. In addition, we demonstrate how to choose the number of basis coefficients as a function of the desired approximation accuracy \delta and smoothness of the compatibility functions. We illustrate our theory with both simulated examples and in application to optical flow estimation. version:1
arxiv-1212-3765 | Biologically Inspired Spiking Neurons : Piecewise Linear Models and Digital Implementation | http://arxiv.org/abs/1212.3765 | id:1212.3765 author:Hamid Soleimani, Arash Ahmadi, Mohammad Bavandpour category:cs.LG cs.NE q-bio.NC  published:2012-12-16 summary:There has been a strong push recently to examine biological scale simulations of neuromorphic algorithms to achieve stronger inference capabilities. This paper presents a set of piecewise linear spiking neuron models, which can reproduce different behaviors, similar to the biological neuron, both for a single neuron as well as a network of neurons. The proposed models are investigated, in terms of digital implementation feasibility and costs, targeting large scale hardware implementation. Hardware synthesis and physical implementations on FPGA show that the proposed models can produce precise neural behaviors with higher performance and considerably lower implementation costs compared with the original model. Accordingly, a compact structure of the models which can be trained with supervised and unsupervised learning algorithms has been developed. Using this structure and based on a spike rate coding, a character recognition case study has been implemented and tested. version:1
arxiv-1212-3634 | A comparative study of root-based and stem-based approaches for measuring the similarity between arabic words for arabic text mining applications | http://arxiv.org/abs/1212.3634 | id:1212.3634 author:Hanane Froud, Abdelmonaim Lachkar, Said Alaoui Ouatik category:cs.CL cs.IR  published:2012-12-14 summary:Representation of semantic information contained in the words is needed for any Arabic Text Mining applications. More precisely, the purpose is to better take into account the semantic dependencies between words expressed by the co-occurrence frequencies of these words. There have been many proposals to compute similarities between words based on their distributions in contexts. In this paper, we compare and contrast the effect of two preprocessing techniques applied to Arabic corpus: Rootbased (Stemming), and Stem-based (Light Stemming) approaches for measuring the similarity between Arabic words with the well known abstractive model -Latent Semantic Analysis (LSA)- with a wide variety of distance functions and similarity measures, such as the Euclidean Distance, Cosine Similarity, Jaccard Coefficient, and the Pearson Correlation Coefficient. The obtained results show that, on the one hand, the variety of the corpus produces more accurate results; on the other hand, the Stem-based approach outperformed the Root-based one because this latter affects the words meanings. version:1
arxiv-1212-3631 | Learning efficient sparse and low rank models | http://arxiv.org/abs/1212.3631 | id:1212.3631 author:Pablo Sprechmann, Alex M. Bronstein, Guillermo Sapiro category:cs.LG  published:2012-12-14 summary:Parsimony, including sparsity and low rank, has been shown to successfully model data in numerous machine learning and signal processing tasks. Traditionally, such modeling approaches rely on an iterative algorithm that minimizes an objective function with parsimony-promoting terms. The inherently sequential structure and data-dependent complexity and latency of iterative optimization constitute a major limitation in many applications requiring real-time performance or involving large-scale data. Another limitation encountered by these modeling techniques is the difficulty of their inclusion in discriminative learning scenarios. In this work, we propose to move the emphasis from the model to the pursuit algorithm, and develop a process-centric view of parsimonious modeling, in which a learned deterministic fixed-complexity pursuit process is used in lieu of iterative optimization. We show a principled way to construct learnable pursuit process architectures for structured sparse and robust low rank models, derived from the iteration of proximal descent algorithms. These architectures learn to approximate the exact parsimonious representation at a fraction of the complexity of the standard optimization methods. We also show that appropriate training regimes allow to naturally extend parsimonious models to discriminative settings. State-of-the-art results are demonstrated on several challenging problems in image and audio processing with several orders of magnitude speedup compared to the exact optimization algorithms. version:1
arxiv-1111-4500 | Equivalence of History and Generator Epsilon-Machines | http://arxiv.org/abs/1111.4500 | id:1111.4500 author:Nicholas F. Travers, James P. Crutchfield category:math.PR cond-mat.stat-mech cs.IT math.IT nlin.CD stat.ML  published:2011-11-18 summary:Epsilon-machines are minimal, unifilar presentations of stationary stochastic processes. They were originally defined in the history machine sense, as hidden Markov models whose states are the equivalence classes of infinite pasts with the same probability distribution over futures. In analyzing synchronization, though, an alternative generator definition was given: unifilar, edge-emitting hidden Markov models with probabilistically distinct states. The key difference is that history epsilon-machines are defined by a process, whereas generator epsilon-machines define a process. We show here that these two definitions are equivalent in the finite-state case. version:2
arxiv-1212-3454 | Proceedings Quantities in Formal Methods | http://arxiv.org/abs/1212.3454 | id:1212.3454 author:Uli Fahrenberg, Axel Legay, Claus Thrane category:cs.LO cs.FL cs.LG cs.SE  published:2012-12-14 summary:This volume contains the proceedings of the Workshop on Quantities in Formal Methods, QFM 2012, held in Paris, France on 28 August 2012. The workshop was affiliated with the 18th Symposium on Formal Methods, FM 2012. The focus of the workshop was on quantities in modeling, verification, and synthesis. Modern applications of formal methods require to reason formally on quantities such as time, resources, or probabilities. Standard formal methods and tools have gotten very good at modeling (and verifying) qualitative properties: whether or not certain events will occur. During the last years, these methods and tools have been extended to also cover quantitative aspects, notably leading to tools like e.g. UPPAAL (for real-time systems), PRISM (for probabilistic systems), and PHAVer (for hybrid systems). A lot of work remains to be done however before these tools can be used in the industrial applications at which they are aiming. version:1
arxiv-1212-3441 | Evolution of Plastic Learning in Spiking Networks via Memristive Connections | http://arxiv.org/abs/1212.3441 | id:1212.3441 author:Gerard Howard, Ella Gale, Larry Bull, Ben de Lacy Costello, Andy Adamatzky category:cs.ET cs.NE  published:2012-12-14 summary:This article presents a spiking neuroevolutionary system which implements memristors as plastic connections, i.e. whose weights can vary during a trial. The evolutionary design process exploits parameter self-adaptation and variable topologies, allowing the number of neurons, connection weights, and inter-neural connectivity pattern to emerge. By comparing two phenomenological real-world memristor implementations with networks comprised of (i) linear resistors (ii) constant-valued connections, we demonstrate that this approach allows the evolution of networks of appropriate complexity to emerge whilst exploiting the memristive properties of the connections to reduce learning time. We extend this approach to allow for heterogeneous mixtures of memristors within the networks; our approach provides an in-depth analysis of network structure. Our networks are evaluated on simulated robotic navigation tasks; results demonstrate that memristive plasticity enables higher performance than constant-weighted connections in both static and dynamic reward scenarios, and that mixtures of memristive elements provide performance advantages when compared to homogeneous memristive networks. version:1
arxiv-1212-3390 | Know Your Personalization: Learning Topic level Personalization in Online Services | http://arxiv.org/abs/1212.3390 | id:1212.3390 author:Anirban Majumder, Nisheeth Shrivastava category:cs.LG cs.IR  published:2012-12-14 summary:Online service platforms (OSPs), such as search engines, news-websites, ad-providers, etc., serve highly pe rsonalized content to the user, based on the profile extracted from his history with the OSP. Although personalization (generally) leads to a better user experience, it also raises privacy concerns for the user---he does not know what is present in his profile and more importantly, what is being used to per sonalize content for him. In this paper, we capture OSP's personalization for an user in a new data structure called the person alization vector ($\eta$), which is a weighted vector over a set of topics, and present techniques to compute it for users of an OSP. Our approach treats OSPs as black-boxes, and extracts $\eta$ by mining only their output, specifical ly, the personalized (for an user) and vanilla (without any user information) contents served, and the differences in these content. We formulate a new model called Latent Topic Personalization (LTP) that captures the personalization vector into a learning framework and present efficient inference algorithms for it. We do extensive experiments for search result personalization using both data from real Google users and synthetic datasets. Our results show high accuracy (R-pre = 84%) of LTP in finding personalized topics. For Google data, our qualitative results show how LTP can also identifies evidences---queries for results on a topic with high $\eta$ value were re-ranked. Finally, we show how our approach can be used to build a new Privacy evaluation framework focused at end-user privacy on commercial OSPs. version:1
arxiv-1212-0901 | Advances in Optimizing Recurrent Networks | http://arxiv.org/abs/1212.0901 | id:1212.0901 author:Yoshua Bengio, Nicolas Boulanger-Lewandowski, Razvan Pascanu category:cs.LG  published:2012-12-04 summary:After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modelling sequences,their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error. version:2
arxiv-1212-3373 | A Novel Directional Weighted Minimum Deviation (DWMD) Based Filter for Removal of Random Valued Impulse Noise | http://arxiv.org/abs/1212.3373 | id:1212.3373 author:J. K. Mandal, Somnath Mukhopadhyay category:cs.CV  published:2012-12-14 summary:The most median-based de noising methods works fine for restoring the images corrupted by Randomn Valued Impulse Noise with low noise level but very poor with highly corrupted images. In this paper a directional weighted minimum deviation (DWMD) based filter has been proposed for removal of high random valued impulse noise (RVIN). The proposed approach based on Standard Deviation (SD) works in two phases. The first phase detects the contaminated pixels by differencing between the test pixel and its neighbor pixels aligned with four main directions. The second phase filters only those pixels keeping others intact. The filtering scheme is based on minimum standard deviation of the four directional pixels. Extensive simulations show that the proposed filter not only provide better performance of de noising RVIN but can preserve more details features even thin lines or dots. This technique shows better performance in terms of PSNR, Image Fidelity and Computational Cost compared to the existing filters. version:1
arxiv-1212-3171 | Multifractal analysis of sentence lengths in English literary texts | http://arxiv.org/abs/1212.3171 | id:1212.3171 author:Iwona Grabska-Gradziska, Andrzej Kulig, Jarosaw Kwapie, Pawe Owicimka, Stanisaw Drod category:physics.data-an cs.CL physics.soc-ph  published:2012-12-13 summary:This paper presents analysis of 30 literary texts written in English by different authors. For each text, there were created time series representing length of sentences in words and analyzed its fractal properties using two methods of multifractal analysis: MFDFA and WTMM. Both methods showed that there are texts which can be considered multifractal in this representation but a majority of texts are not multifractal or even not fractal at all. Out of 30 books, only a few have so-correlated lengths of consecutive sentences that the analyzed signals can be interpreted as real multifractals. An interesting direction for future investigations would be identifying what are the specific features which cause certain texts to be multifractal and other to be monofractal or even not fractal at all. version:1
arxiv-1212-3162 | Diachronic Variation in Grammatical Relations | http://arxiv.org/abs/1212.3162 | id:1212.3162 author:Aaron Gerow, Khurshid Ahmad category:cs.CL  published:2012-12-13 summary:We present a method of finding and analyzing shifts in grammatical relations found in diachronic corpora. Inspired by the econometric technique of measuring return and volatility instead of relative frequencies, we propose them as a way to better characterize changes in grammatical patterns like nominalization, modification and comparison. To exemplify the use of these techniques, we examine a corpus of NIPS papers and report trends which manifest at the token, part-of-speech and grammatical levels. Building up from frequency observations to a second-order analysis, we show that shifts in frequencies overlook deeper trends in language, even when part-of-speech information is included. Examining token, POS and grammatical levels of variation enables a summary view of diachronic text as a whole. We conclude with a discussion about how these methods can inform intuitions about specialist domains as well as changes in language use as a whole. version:1
arxiv-1212-3138 | Identifying Metaphor Hierarchies in a Corpus Analysis of Finance Articles | http://arxiv.org/abs/1212.3138 | id:1212.3138 author:Aaron Georw, Mark Keane category:cs.CL  published:2012-12-13 summary:Using a corpus of over 17,000 financial news reports (involving over 10M words), we perform an analysis of the argument-distributions of the UP- and DOWN-verbs used to describe movements of indices, stocks, and shares. Using measures of the overlap in the argument distributions of these verbs and k-means clustering of their distributions, we advance evidence for the proposal that the metaphors referred to by these verbs are organised into hierarchical structures of superordinate and subordinate groups. version:1
arxiv-1212-3034 | Multi-target tracking algorithms in 3D | http://arxiv.org/abs/1212.3034 | id:1212.3034 author:Rastislav Telgarsky category:cs.CV cs.DM 65D18  68W05  published:2012-12-13 summary:Ladars provide a unique capability for identification of objects and motions in scenes with fixed 3D field of view (FOV). This paper describes algorithms for multi-target tracking in 3D scenes including the preprocessing (mathematical morphology and Parzen windows), labeling of connected components, sorting of targets by selectable attributes (size, length of track, velocity), and handling of target states (acquired, coasting, re-acquired and tracked) in order to assemble the target trajectories. This paper is derived from working algorithms coded in Matlab, which were tested and reviewed by others, and does not speculate about usage of general formulas or frameworks. version:1
arxiv-1212-3023 | Keyword Extraction for Identifying Social Actors | http://arxiv.org/abs/1212.3023 | id:1212.3023 author:Mahyuddin K. M. Nasution, Shahrul Azman Mohd Noah category:cs.IR cs.CL  published:2012-12-13 summary:Identifying the social actor has become one of tasks in Artificial Intelligence, whereby extracting keyword from Web snippets depend on the use of web is steadily gaining ground in this research. We develop therefore an approach based on overlap principle for utilizing a collection of features in web snippets, where use of keyword will eliminate the un-relevant web pages. version:1
arxiv-1212-2991 | Accelerating Inference: towards a full Language, Compiler and Hardware stack | http://arxiv.org/abs/1212.2991 | id:1212.2991 author:Shawn Hershey, Jeff Bernstein, Bill Bradley, Andrew Schweitzer, Noah Stein, Theo Weber, Ben Vigoda category:cs.SE cs.AI stat.ML  published:2012-12-12 summary:We introduce Dimple, a fully open-source API for probabilistic modeling. Dimple allows the user to specify probabilistic models in the form of graphical models, Bayesian networks, or factor graphs, and performs inference (by automatically deriving an inference engine from a variety of algorithms) on the model. Dimple also serves as a compiler for GP5, a hardware accelerator for inference. version:1
arxiv-1212-2860 | Pituitary Adenoma Volumetry with 3D Slicer | http://arxiv.org/abs/1212.2860 | id:1212.2860 author:Jan Egger, Tina Kapur, Christopher Nimsky, Ron Kikinis category:cs.CV  published:2012-12-12 summary:In this study, we present pituitary adenoma volumetry using the free and open source medical image computing platform for biomedical research: (3D) Slicer. Volumetric changes in cerebral pathologies like pituitary adenomas are a critical factor in treatment decisions by physicians and in general the volume is acquired manually. Therefore, manual slice-by-slice segmentations in magnetic resonance imaging (MRI) data, which have been obtained at regular intervals, are performed. In contrast to this manual time consuming slice-by-slice segmentation process Slicer is an alternative which can be significantly faster and less user intensive. In this contribution, we compare pure manual segmentations of ten pituitary adenomas with semi-automatic segmentations under Slicer. Thus, physicians drew the boundaries completely manually on a slice-by-slice basis and performed a Slicer-enhanced segmentation using the competitive region-growing based module of Slicer named GrowCut. Results showed that the time and user effort required for GrowCut-based segmentations were on average about thirty percent less than the pure manual segmentations. Furthermore, we calculated the Dice Similarity Coefficient (DSC) between the manual and the Slicer-based segmentations to proof that the two are comparable yielding an average DSC of 81.97\pm3.39%. version:1
arxiv-1301-0613 | IPF for Discrete Chain Factor Graphs | http://arxiv.org/abs/1301.0613 | id:1301.0613 author:Wim Wiegerinck, Tom Heskes category:cs.LG cs.AI stat.ML  published:2012-12-12 summary:Iterative Proportional Fitting (IPF), combined with EM, is commonly used as an algorithm for likelihood maximization in undirected graphical models. In this paper, we present two iterative algorithms that generalize upon IPF. The first one is for likelihood maximization in discrete chain factor graphs, which we define as a wide class of discrete variable models including undirected graphical models and Bayesian networks, but also chain graphs and sigmoid belief networks. The second one is for conditional likelihood maximization in standard undirected models and Bayesian networks. In both algorithms, the iteration steps are expressed in closed form. Numerical simulations show that the algorithms are competitive with state of the art methods. version:1
arxiv-1301-0612 | Adaptive Foreground and Shadow Detection inImage Sequences | http://arxiv.org/abs/1301.0612 | id:1301.0612 author:Yang Wang, Tele Tan category:cs.CV  published:2012-12-12 summary:This paper presents a novel method of foreground segmentation that distinguishes moving objects from their moving cast shadows in monocular image sequences. The models of background, edge information, and shadow are set up and adaptively updated. A Bayesian belief network is proposed to describe the relationships among the segmentation label, background, intensity, and edge information. The notion of Markov random field is used to encourage the spatial connectivity of the segmented regions. The solution is obtained by maximizing the posterior possibility density of the segmentation field. version:1
arxiv-1301-0610 | A New Class of Upper Bounds on the Log Partition Function | http://arxiv.org/abs/1301.0610 | id:1301.0610 author:Martin Wainwright, Tommi S. Jaakkola, Alan Willsky category:cs.LG stat.ML  published:2012-12-12 summary:Bounds on the log partition function are important in a variety of contexts, including approximate inference, model fitting, decision theory, and large deviations analysis. We introduce a new class of upper bounds on the log partition function, based on convex combinations of distributions in the exponential domain, that is applicable to an arbitrary undirected graphical model. In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe free energy, but distinguished by the following desirable properties: i. they are cnvex, and have a unique global minimum; and ii. the global minimum gives an upper bound on the log partition function. The global minimum is defined by stationary conditions very similar to those defining fixed points of belief propagation or tree-based reparameterization Wainwright et al., 2001. As with BP fixed points, the elements of the minimizing argument can be used as approximations to the marginals of the original model. The analysis described here can be extended to structures of higher treewidth e.g., hypertrees, thereby making connections with more advanced approximations e.g., Kikuchi and variants Yedidia et al., 2001; Minka, 2001. version:1
arxiv-1301-0604 | Discriminative Probabilistic Models for Relational Data | http://arxiv.org/abs/1301.0604 | id:1301.0604 author:Ben Taskar, Pieter Abbeel, Daphne Koller category:cs.LG cs.AI stat.ML  published:2012-12-12 summary:In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies. version:1
arxiv-1301-0602 | Unsupervised Active Learning in Large Domains | http://arxiv.org/abs/1301.0602 | id:1301.0602 author:Harald Steck, Tommi S. Jaakkola category:cs.LG stat.ML  published:2012-12-12 summary:Active learning is a powerful approach to analyzing data effectively. We show that the feasibility of active learning depends crucially on the choice of measure with respect to which the query is being optimized. The standard information gain, for example, does not permit an accurate evaluation with a small committee, a representative subset of the model space. We propose a surrogate measure requiring only a small committee and discuss the properties of this new measure. We devise, in addition, a bootstrap approach for committee selection. The advantages of this approach are illustrated in the context of recovering (regulatory) network models. version:1
arxiv-1301-0601 | Reinforcement Learning with Partially Known World Dynamics | http://arxiv.org/abs/1301.0601 | id:1301.0601 author:Christian R. Shelton category:cs.LG stat.ML  published:2012-12-12 summary:Reinforcement learning would enjoy better success on real-world problems if domain knowledge could be imparted to the algorithm by the modelers. Most problems have both hidden state and unknown dynamics. Partially observable Markov decision processes (POMDPs) allow for the modeling of both. Unfortunately, they do not provide a natural framework in which to specify knowledge about the domain dynamics. The designer must either admit to knowing nothing about the dynamics or completely specify the dynamics (thereby turning it into a planning problem). We propose a new framework called a partially known Markov decision process (PKMDP) which allows the designer to specify known dynamics while still leaving portions of the environment s dynamics unknown.The model represents NOT ONLY the environment dynamics but also the agents knowledge of the dynamics. We present a reinforcement learning algorithm for this model based on importance sampling. The algorithm incorporates planning based on the known dynamics and learning about the unknown dynamics. Our results clearly demonstrate the ability to add domain knowledge and the resulting benefits for learning. version:1
arxiv-1301-0599 | Advances in Boosting (Invited Talk) | http://arxiv.org/abs/1301.0599 | id:1301.0599 author:Robert E. Schapire category:cs.LG stat.ML  published:2012-12-12 summary:Boosting is a general method of generating many simple classification rules and combining them into a single, highly accurate rule. In this talk, I will review the AdaBoost boosting algorithm and some of its underlying theory, and then look at how this theory has helped us to face some of the challenges of applying AdaBoost in two domains: In the first of these, we used boosting for predicting and modeling the uncertainty of prices in complicated, interacting auctions. The second application was to the classification of caller utterances in a telephone spoken-dialogue system where we faced two challenges: the need to incorporate prior knowledge to compensate for initially insufficient data; and a later need to filter the large stream of unlabeled examples being collected to select the ones whose labels are likely to be most informative. version:1
arxiv-1301-0598 | Asymptotic Model Selection for Naive Bayesian Networks | http://arxiv.org/abs/1301.0598 | id:1301.0598 author:Dmitry Rusakov, Dan Geiger category:cs.AI cs.LG  published:2012-12-12 summary:We develop a closed form asymptotic formula to compute the marginal likelihood of data given a naive Bayesian network model with two hidden states and binary features. This formula deviates from the standard BIC score. Our work provides a concrete example that the BIC score is generally not valid for statistical models that belong to a stratified exponential family. This stands in contrast to linear and curved exponential families, where the BIC score has been proven to provide a correct approximation for the marginal likelihood. version:1
arxiv-1301-0593 | Bayesian Network Classifiers in a High Dimensional Framework | http://arxiv.org/abs/1301.0593 | id:1301.0593 author:Tatjana Pavlenko, Dietrich von Rosen category:cs.LG stat.ML  published:2012-12-12 summary:We present a growing dimension asymptotic formalism. The perspective in this paper is classification theory and we show that it can accommodate probabilistic networks classifiers, including naive Bayes model and its augmented version. When represented as a Bayesian network these classifiers have an important advantage: The corresponding discriminant function turns out to be a specialized case of a generalized additive model, which makes it possible to get closed form expressions for the asymptotic misclassification probabilities used here as a measure of classification accuracy. Moreover, in this paper we propose a new quantity for assessing the discriminative power of a set of features which is then used to elaborate the augmented naive Bayes classifier. The result is a weighted form of the augmented naive Bayes that distributes weights among the sets of features according to their discriminative power. We derive the asymptotic distribution of the sample based discriminative power and show that it is seriously overestimated in a high dimensional case. We then apply this result to find the optimal, in a sense of minimum misclassification probability, type of weighting. version:1
arxiv-1301-0588 | Expectation-Propogation for the Generative Aspect Model | http://arxiv.org/abs/1301.0588 | id:1301.0588 author:Thomas P. Minka, John Lafferty category:cs.LG cs.IR stat.ML  published:2012-12-12 summary:The generative aspect model is an extension of the multinomial model for text that allows word probabilities to vary stochastically across documents. Previous results with aspect models have been promising, but hindered by the computational difficulty of carrying out inference and learning. This paper demonstrates that the simple variational methods of Blei et al (2001) can lead to inaccurate inferences and biased learning for the generative aspect model. We develop an alternative approach that leads to higher accuracy at comparable cost. An extension of Expectation-Propagation is used for inference and then embedded in an EM algorithm for learning. Experimental results are presented for both synthetic and real data sets. version:1
arxiv-1301-0587 | Optimal Time Bounds for Approximate Clustering | http://arxiv.org/abs/1301.0587 | id:1301.0587 author:Ramgopal Mettu, Greg Plaxton category:cs.DS cs.LG stat.ML  published:2012-12-12 summary:Clustering is a fundamental problem in unsupervised learning, and has been studied widely both as a problem of learning mixture models and as an optimization problem. In this paper, we study clustering with respect the emph{k-median} objective function, a natural formulation of clustering in which we attempt to minimize the average distance to cluster centers. One of the main contributions of this paper is a simple but powerful sampling technique that we call emph{successive sampling} that could be of independent interest. We show that our sampling procedure can rapidly identify a small set of points (of size just O(klog{n/k})) that summarize the input points for the purpose of clustering. Using successive sampling, we develop an algorithm for the k-median problem that runs in O(nk) time for a wide range of values of k and is guaranteed, with high probability, to return a solution with cost at most a constant factor times optimal. We also establish a lower bound of Omega(nk) on any randomized constant-factor approximation algorithm for the k-median problem that succeeds with even a negligible (say 1/100) probability. Thus we establish a tight time bound of Theta(nk) for the k-median problem for a wide range of values of k. The best previous upper bound for the problem was O(nk), where the O-notation hides polylogarithmic factors in n and k. The best previous lower bound of O(nk) applied only to deterministic k-median algorithms. While we focus our presentation on the k-median objective, all our upper bounds are valid for the k-means objective as well. In this context our algorithm compares favorably to the widely used k-means heuristic, which requires O(nk) time for just one iteration and provides no useful approximation guarantees. version:1
arxiv-1301-0586 | Staged Mixture Modelling and Boosting | http://arxiv.org/abs/1301.0586 | id:1301.0586 author:Christopher Meek, Bo Thiesson, David Heckerman category:cs.LG stat.ML  published:2012-12-12 summary:In this paper, we introduce and evaluate a data-driven staged mixture modeling technique for building density, regression, and classification models. Our basic approach is to sequentially add components to a finite mixture model using the structural expectation maximization (SEM) algorithm. We show that our technique is qualitatively similar to boosting. This correspondence is a natural byproduct of the fact that we use the SEM algorithm to sequentially fit the mixture model. Finally, in our experimental evaluation, we demonstrate the effectiveness of our approach on a variety of prediction and density estimation tasks using real-world data. version:1
arxiv-1301-0584 | Decayed MCMC Filtering | http://arxiv.org/abs/1301.0584 | id:1301.0584 author:Bhaskara Marthi, Hanna Pasula, Stuart Russell, Yuval Peres category:cs.AI cs.LG cs.SY  published:2012-12-12 summary:Filtering---estimating the state of a partially observable Markov process from a sequence of observations---is one of the most widely studied problems in control theory, AI, and computational statistics. Exact computation of the posterior distribution is generally intractable for large discrete systems and for nonlinear continuous systems, so a good deal of effort has gone into developing robust approximation algorithms. This paper describes a simple stochastic approximation algorithm for filtering called {em decayed MCMC}. The algorithm applies Markov chain Monte Carlo sampling to the space of state trajectories using a proposal distribution that favours flips of more recent state variables. The formal analysis of the algorithm involves a generalization of standard coupling arguments for MCMC convergence. We prove that for any ergodic underlying Markov process, the convergence time of decayed MCMC with inverse-polynomial decay remains bounded as the length of the observation sequence grows. We show experimentally that decayed MCMC is at least competitive with other approximation algorithms such as particle filtering. version:1
arxiv-1301-0579 | Almost-everywhere algorithmic stability and generalization error | http://arxiv.org/abs/1301.0579 | id:1301.0579 author:Samuel Kutin, Partha Niyogi category:cs.LG stat.ML  published:2012-12-12 summary:We explore in some detail the notion of algorithmic stability as a viable framework for analyzing the generalization error of learning algorithms. We introduce the new notion of training stability of a learning algorithm and show that, in a general setting, it is sufficient for good bounds on generalization error. In the PAC setting, training stability is both necessary and sufficient for learnability.\ The approach based on training stability makes no reference to VC dimension or VC entropy. There is no need to prove uniform convergence, and generalization error is bounded directly via an extended McDiarmid inequality. As a result it potentially allows us to deal with a broader class of learning algorithms than Empirical Risk Minimization. \ We also explore the relationships among VC dimension, generalization error, and various notions of stability. Several examples of learning algorithms are considered. version:1
arxiv-1301-0578 | Dimension Correction for Hierarchical Latent Class Models | http://arxiv.org/abs/1301.0578 | id:1301.0578 author:Tomas Kocka, Nevin Lianwen Zhang category:cs.LG stat.ML  published:2012-12-12 summary:Model complexity is an important factor to consider when selecting among graphical models. When all variables are observed, the complexity of a model can be measured by its standard dimension, i.e. the number of independent parameters. When hidden variables are present, however, standard dimension might no longer be appropriate. One should instead use effective dimension (Geiger et al. 1996). This paper is concerned with the computation of effective dimension. First we present an upper bound on the effective dimension of a latent class (LC) model. This bound is tight and its computation is easy. We then consider a generalization of LC models called hierarchical latent class (HLC) models (Zhang 2002). We show that the effective dimension of an HLC model can be obtained from the effective dimensions of some related LC models. We also demonstrate empirically that using effective dimension in place of standard dimension improves the quality of models learned from data. version:1
arxiv-1301-0570 | Reduction of Maximum Entropy Models to Hidden Markov Models | http://arxiv.org/abs/1301.0570 | id:1301.0570 author:Joshua Goodman category:cs.AI cs.CL  published:2012-12-12 summary:We show that maximum entropy (maxent) models can be modeled with certain kinds of HMMs, allowing us to construct maxent models with hidden variables, hidden state sequences, or other characteristics. The models can be trained using the forward-backward algorithm. While the results are primarily of theoretical interest, unifying apparently unrelated concepts, we also give experimental results for a maxent model with a hidden variable on a word disambiguation task; the model outperforms standard techniques. version:1
arxiv-1301-0567 | The Thing That We Tried Didn't Work Very Well : Deictic Representation in Reinforcement Learning | http://arxiv.org/abs/1301.0567 | id:1301.0567 author:Sarah Finney, Natalia Gardiol, Leslie Pack Kaelbling, Tim Oates category:cs.LG cs.AI  published:2012-12-12 summary:Most reinforcement learning methods operate on propositional representations of the world state. Such representations are often intractably large and generalize poorly. Using a deictic representation is believed to be a viable alternative: they promise generalization while allowing the use of existing reinforcement-learning methods. Yet, there are few experiments on learning with deictic representations reported in the literature. In this paper we explore the effectiveness of two forms of deictic representation and a na\"{i}ve propositional representation in a simple blocks-world domain. We find, empirically, that the deictic representations actually worsen learning performance. We conclude with a discussion of possible causes of these results and strategies for more effective learning in domains with objects. version:1
arxiv-1301-0565 | An Information-Theoretic External Cluster-Validity Measure | http://arxiv.org/abs/1301.0565 | id:1301.0565 author:Byron E Dom category:cs.LG stat.ML  published:2012-12-12 summary:In this paper we propose a measure of clustering quality or accuracy that is appropriate in situations where it is desirable to evaluate a clustering algorithm by somehow comparing the clusters it produces with ``ground truth' consisting of classes assigned to the patterns by manual means or some other means in whose veracity there is confidence. Such measures are refered to as ``external'. Our measure also has the characteristic of allowing clusterings with different numbers of clusters to be compared in a quantitative and principled way. Our evaluation scheme quantitatively measures how useful the cluster labels of the patterns are as predictors of their class labels. In cases where all clusterings to be compared have the same number of clusters, the measure is equivalent to the mutual information between the cluster labels and the class labels. In cases where the numbers of clusters are different, however, it computes the reduction in the number of bits that would be required to encode (compress) the class labels if both the encoder and decoder have free acccess to the cluster labels. To achieve this encoding the estimated conditional probabilities of the class labels given the cluster labels must also be encoded. These estimated probabilities can be seen as a model for the class labels and their associated code length as a model cost. version:1
arxiv-1301-0563 | Interpolating Conditional Density Trees | http://arxiv.org/abs/1301.0563 | id:1301.0563 author:Scott Davies, Andrew Moore category:cs.LG cs.AI stat.ML  published:2012-12-12 summary:Joint distributions over many variables are frequently modeled by decomposing them into products of simpler, lower-dimensional conditional distributions, such as in sparsely connected Bayesian networks. However, automatically learning such models can be very computationally expensive when there are many datapoints and many continuous variables with complex nonlinear relationships, particularly when no good ways of decomposing the joint distribution are known a priori. In such situations, previous research has generally focused on the use of discretization techniques in which each continuous variable has a single discretization that is used throughout the entire network. \ In this paper, we present and compare a wide variety of tree-based algorithms for learning and evaluating conditional density estimates over continuous variables. These trees can be thought of as discretizations that vary according to the particular interactions being modeled; however, the density within a given leaf of the tree need not be assumed constant, and we show that such nonuniform leaf densities lead to more accurate density estimation. We have developed Bayesian network structure-learning algorithms that employ these tree-based conditional density representations, and we show that they can be used to practically learn complex joint probability models over dozens of continuous variables from thousands of datapoints. We focus on finding models that are simultaneously accurate, fast to learn, and fast to evaluate once they are learned. version:1
arxiv-1301-0562 | Continuation Methods for Mixing Heterogenous Sources | http://arxiv.org/abs/1301.0562 | id:1301.0562 author:Adrian Corduneanu, Tommi S. Jaakkola category:cs.LG stat.ML  published:2012-12-12 summary:A number of modern learning tasks involve estimation from heterogeneous information sources. This includes classification with labeled and unlabeled data as well as other problems with analogous structure such as competitive (game theoretic) problems. The associated estimation problems can be typically reduced to solving a set of fixed point equations (consistency conditions). We introduce a general method for combining a preferred information source with another in this setting by evolving continuous paths of fixed points at intermediate allocations. We explicitly identify critical points along the unique paths to either increase the stability of estimation or to ensure a significant departure from the initial source. The homotopy continuation approach is guaranteed to terminate at the second source, and involves no combinatorial effort. We illustrate the power of these ideas both in classification tasks with labeled and unlabeled data, as well as in the context of a competitive (min-max) formulation of DNA sequence motif discovery. version:1
arxiv-1301-0556 | Learning with Scope, with Application to Information Extraction and Classification | http://arxiv.org/abs/1301.0556 | id:1301.0556 author:David Blei, J Andrew Bagnell, Andrew McCallum category:cs.LG cs.IR stat.ML  published:2012-12-12 summary:In probabilistic approaches to classification and information extraction, one typically builds a statistical model of words under the assumption that future data will exhibit the same regularities as the training data. In many data sets, however, there are scope-limited features whose predictive power is only applicable to a certain subset of the data. For example, in information extraction from web pages, word formatting may be indicative of extraction category in different ways on different web pages. The difficulty with using such features is capturing and exploiting the new regularities encountered in previously unseen data. In this paper, we propose a hierarchical probabilistic model that uses both local/scope-limited features, such as word formatting, and global features, such as word content. The local regularities are modeled as an unobserved random parameter which is drawn once for each local data set. This random parameter is estimated during the inference process and then used to perform classification with both the local and global features--- a procedure which is akin to automatically retuning the classifier to the local regularities on each newly encountered web page. Exact inference is intractable and we present approximations via point estimates and variational methods. Empirical results on large collections of web data demonstrate that this method significantly improves performance from traditional models of global features alone. version:1
arxiv-1301-0554 | Tree-dependent Component Analysis | http://arxiv.org/abs/1301.0554 | id:1301.0554 author:Francis R. Bach, Michael I. Jordan category:cs.LG stat.ML  published:2012-12-12 summary:We present a generalization of independent component analysis (ICA), where instead of looking for a linear transform that makes the data components independent, we look for a transform that makes the data components well fit by a tree-structured graphical model. Treating the problem as a semiparametric statistical problem, we show that the optimal transform is found by minimizing a contrast function based on mutual information, a function that directly extends the contrast function used for classical ICA. We provide two approximations of this contrast function, one using kernel density estimation, and another using kernel generalized variance. This tree-dependent component analysis framework leads naturally to an efficient general multivariate density estimation technique where only bivariate density estimation needs to be performed. version:1
arxiv-1301-0551 | Learning Hierarchical Object Maps Of Non-Stationary Environments with mobile robots | http://arxiv.org/abs/1301.0551 | id:1301.0551 author:Dragomir Anguelov, Rahul Biswas, Daphne Koller, Benson Limketkai, Sebastian Thrun category:cs.LG cs.RO stat.ML  published:2012-12-12 summary:Building models, or maps, of robot environments is a highly active research area; however, most existing techniques construct unstructured maps and assume static environments. In this paper, we present an algorithm for learning object models of non-stationary objects found in office-type environments. Our algorithm exploits the fact that many objects found in office environments look alike (e.g., chairs, recycling bins). It does so through a two-level hierarchical representation, which links individual objects with generic shape templates of object classes. We derive an approximate EM algorithm for learning shape parameters at both levels of the hierarchy, using local occupancy grid maps for representing shape. Additionally, we develop a Bayesian model selection algorithm that enables the robot to estimate the total number of objects and object templates in the environment. Experimental results using a real robot equipped with a laser range finder indicate that our approach performs well at learning object-based maps of simple office environments. The approach outperforms a previously developed non-hierarchical algorithm that models objects but lacks class templates. version:1
arxiv-1212-2823 | Tracking Revisited using RGBD Camera: Baseline and Benchmark | http://arxiv.org/abs/1212.2823 | id:1212.2823 author:Shuran Song, Jianxiong Xiao category:cs.CV  published:2012-12-12 summary:Although there has been significant progress in the past decade,tracking is still a very challenging computer vision task, due to problems such as occlusion and model drift.Recently, the increased popularity of depth sensors e.g. Microsoft Kinect has made it easy to obtain depth data at low cost.This may be a game changer for tracking, since depth information can be used to prevent model drift and handle occlusion.In this paper, we construct a benchmark dataset of 100 RGBD videos with high diversity, including deformable objects, various occlusion conditions and moving cameras. We propose a very simple but strong baseline model for RGBD tracking, and present a quantitative comparison of several state-of-the-art tracking algorithms.Experimental results show that including depth information and reasoning about occlusion significantly improves tracking performance. The datasets, evaluation details, source code for the baseline algorithm, and instructions for submitting new models will be made available online after acceptance. version:1
arxiv-1212-2784 | Clustering of functional boxplots for multiple streaming time series | http://arxiv.org/abs/1212.2784 | id:1212.2784 author:Elvira Romano, Antonio Balzanella category:stat.ME stat.ML  published:2012-12-12 summary:In this paper we introduce a micro-clustering strategy for Functional Boxplots. The aim is to summarize a set of streaming time series splitted in non overlapping windows. It is a two step strategy which performs at first, an on-line summarization by means of functional data structures, named Functional Boxplot micro-clusters; then it reveals the final summarization by processing, off-line, the functional data structures. Our main contribute consists in providing a new definition of micro-cluster based on Functional Boxplots and, in defining a proximity measure which allows to compare and update them. This allows to get a finer graphical summarization of the streaming time series by five functional basic statistics of data. The obtained synthesis will be able to keep track of the dynamic evolution of the multiple streams. version:1
arxiv-1212-2767 | Bayesian one-mode projection for dynamic bipartite graphs | http://arxiv.org/abs/1212.2767 | id:1212.2767 author:Ioannis Psorakis, Iead Rezek, Zach Frankel, Stephen J. Roberts category:stat.ML cond-mat.stat-mech cs.LG  published:2012-12-12 summary:We propose a Bayesian methodology for one-mode projecting a bipartite network that is being observed across a series of discrete time steps. The resulting one mode network captures the uncertainty over the presence/absence of each link and provides a probability distribution over its possible weight values. Additionally, the incorporation of prior knowledge over previous states makes the resulting network less sensitive to noise and missing observations that usually take place during the data collection process. The methodology consists of computationally inexpensive update rules and is scalable to large problems, via an appropriate distributed implementation. version:1
arxiv-1203-2507 | Deviation optimal learning using greedy Q-aggregation | http://arxiv.org/abs/1203.2507 | id:1203.2507 author:Dong Dai, Philippe Rigollet, Tong Zhang category:math.ST cs.LG stat.ML stat.TH  published:2012-03-12 summary:Given a finite family of functions, the goal of model selection aggregation is to construct a procedure that mimics the function from this family that is the closest to an unknown regression function. More precisely, we consider a general regression model with fixed design and measure the distance between functions by the mean squared error at the design points. While procedures based on exponential weights are known to solve the problem of model selection aggregation in expectation, they are, surprisingly, sub-optimal in deviation. We propose a new formulation called Q-aggregation that addresses this limitation; namely, its solution leads to sharp oracle inequalities that are optimal in a minimax sense. Moreover, based on the new formulation, we design greedy Q-aggregation procedures that produce sparse aggregation models achieving the optimal rate. The convergence and performance of these greedy procedures are illustrated and compared with other standard methods on simulated examples. version:2
arxiv-1201-1384 | A Thermodynamical Approach for Probability Estimation | http://arxiv.org/abs/1201.1384 | id:1201.1384 author:Takashi Isozaki category:cs.LG physics.data-an stat.ME  published:2012-01-06 summary:The issue of discrete probability estimation for samples of small size is addressed in this study. The maximum likelihood method often suffers over-fitting when insufficient data is available. Although the Bayesian approach can avoid over-fitting by using prior distributions, it still has problems with objective analysis. In response to these drawbacks, a new theoretical framework based on thermodynamics, where energy and temperature are introduced, was developed. Entropy and likelihood are placed at the center of this method. The key principle of inference for probability mass functions is the minimum free energy, which is shown to unify the two principles of maximum likelihood and maximum entropy. Our method can robustly estimate probability functions from small size data. version:2
arxiv-1212-2692 | Enhanced skin colour classifier using RGB Ratio model | http://arxiv.org/abs/1212.2692 | id:1212.2692 author:Ghazali Osman, Muhammad Suzuri Hitam, Mohd Nasir Ismail category:cs.CV 68T10  published:2012-12-12 summary:Skin colour detection is frequently been used for searching people, face detection, pornographic filtering and hand tracking. The presence of skin or non-skin in digital image can be determined by manipulating pixels colour or pixels texture. The main problem in skin colour detection is to represent the skin colour distribution model that is invariant or least sensitive to changes in illumination condition. Another problem comes from the fact that many objects in the real world may possess almost similar skin-tone colour such as wood, leather, skin-coloured clothing, hair and sand. Moreover, skin colour is different between races and can be different from a person to another, even with people of the same ethnicity. Finally, skin colour will appear a little different when different types of camera are used to capture the object or scene. The objective in this study is to develop a skin colour classifier based on pixel-based using RGB ratio model. The RGB ratio model is a newly proposed method that belongs under the category of an explicitly defined skin region model. This skin classifier was tested with SIdb dataset and two benchmark datasets; UChile and TDSD datasets to measure classifier performance. The performance of skin classifier was measured based on true positive (TF) and false positive (FP) indicator. This newly proposed model was compared with Kovac, Saleh and Swift models. The experimental results showed that the RGB ratio model outperformed all the other models in term of detection rate. The RGB ratio model is able to reduce FP detection that caused by reddish objects colour as well as be able to detect darkened skin and skin covered by shadow. version:1
arxiv-1212-2686 | Joint Training of Deep Boltzmann Machines | http://arxiv.org/abs/1212.2686 | id:1212.2686 author:Ian Goodfellow, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG  published:2012-12-12 summary:We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks. version:1
arxiv-1212-2676 | Mining the Web for the Voice of the Herd to Track Stock Market Bubbles | http://arxiv.org/abs/1212.2676 | id:1212.2676 author:Aaron Gerow, Mark Keane category:cs.CL cs.IR physics.soc-ph q-fin.GN  published:2012-12-11 summary:We show that power-law analyses of financial commentaries from newspaper web-sites can be used to identify stock market bubbles, supplementing traditional volatility analyses. Using a four-year corpus of 17,713 online, finance-related articles (10M+ words) from the Financial Times, the New York Times, and the BBC, we show that week-to-week changes in power-law distributions reflect market movements of the Dow Jones Industrial Average (DJI), the FTSE-100, and the NIKKEI-225. Notably, the statistical regularities in language track the 2007 stock market bubble, showing emerging structure in the language of commentators, as progressively greater agreement arose in their positive perceptions of the market. Furthermore, during the bubble period, a marked divergence in positive language occurs as revealed by a Kullback-Leibler analysis. version:1
arxiv-1212-2617 | Optimal diagnostic tests for sporadic Creutzfeldt-Jakob disease based on support vector machine classification of RT-QuIC data | http://arxiv.org/abs/1212.2617 | id:1212.2617 author:William Hulme, Peter Richtrik, Lynne McGuire, Alison Green category:q-bio.QM cs.LG stat.AP  published:2012-12-11 summary:In this work we study numerical construction of optimal clinical diagnostic tests for detecting sporadic Creutzfeldt-Jakob disease (sCJD). A cerebrospinal fluid sample (CSF) from a suspected sCJD patient is subjected to a process which initiates the aggregation of a protein present only in cases of sCJD. This aggregation is indirectly observed in real-time at regular intervals, so that a longitudinal set of data is constructed that is then analysed for evidence of this aggregation. The best existing test is based solely on the final value of this set of data, which is compared against a threshold to conclude whether or not aggregation, and thus sCJD, is present. This test criterion was decided upon by analysing data from a total of 108 sCJD and non-sCJD samples, but this was done subjectively and there is no supporting mathematical analysis declaring this criterion to be exploiting the available data optimally. This paper addresses this deficiency, seeking to validate or improve the test primarily via support vector machine (SVM) classification. Besides this, we address a number of additional issues such as i) early stopping of the measurement process, ii) the possibility of detecting the particular type of sCJD and iii) the incorporation of additional patient data such as age, sex, disease duration and timing of CSF sampling into the construction of the test. version:1
arxiv-1212-2616 | Languages cool as they expand: Allometric scaling and the decreasing need for new words | http://arxiv.org/abs/1212.2616 | id:1212.2616 author:Alexander M. Petersen, Joel N. Tenenbaum, Shlomo Havlin, H. Eugene Stanley, Matjaz Perc category:physics.soc-ph cond-mat.stat-mech cs.CL stat.AP  published:2012-12-11 summary:We analyze the occurrence frequencies of over 15 million words recorded in millions of books published during the past two centuries in seven different languages. For all languages and chronological subsets of the data we confirm that two scaling regimes characterize the word frequency distributions, with only the more common words obeying the classic Zipf law. Using corpora of unprecedented size, we test the allometric scaling relation between the corpus size and the vocabulary size of growing languages to demonstrate a decreasing marginal need for new words, a feature that is likely related to the underlying correlations between words. We calculate the annual growth fluctuations of word use which has a decreasing trend as the corpus size increases, indicating a slowdown in linguistic evolution following language expansion. This "cooling pattern" forms the basis of a third statistical regularity, which unlike the Zipf and the Heaps law, is dynamical in nature. version:1
arxiv-1212-3228 | Language Without Words: A Pointillist Model for Natural Language Processing | http://arxiv.org/abs/1212.3228 | id:1212.3228 author:Peiyou Song, Anhei Shu, David Phipps, Dan Wallach, Mohit Tiwari, Jedidiah Crandall, George Luger category:cs.CL cs.IR cs.SI I.2.7; H.2.8; H.3.1  published:2012-12-11 summary:This paper explores two separate questions: Can we perform natural language processing tasks without a lexicon?; and, Should we? Existing natural language processing techniques are either based on words as units or use units such as grams only for basic classification tasks. How close can a machine come to reasoning about the meanings of words and phrases in a corpus without using any lexicon, based only on grams? Our own motivation for posing this question is based on our efforts to find popular trends in words and phrases from online Chinese social media. This form of written Chinese uses so many neologisms, creative character placements, and combinations of writing systems that it has been dubbed the "Martian Language." Readers must often use visual queues, audible queues from reading out loud, and their knowledge and understanding of current events to understand a post. For analysis of popular trends, the specific problem is that it is difficult to build a lexicon when the invention of new ways to refer to a word or concept is easy and common. For natural language processing in general, we argue in this paper that new uses of language in social media will challenge machines' abilities to operate with words as the basic unit of understanding, not only in Chinese but potentially in other languages. version:1
arxiv-1212-2573 | Convex Relaxations for Learning Bounded Treewidth Decomposable Graphs | http://arxiv.org/abs/1212.2573 | id:1212.2573 author:K. S. Sesh Kumar, Francis Bach category:cs.LG cs.DS stat.ML  published:2012-12-11 summary:We consider the problem of learning the structure of undirected graphical models with bounded treewidth, within the maximum likelihood framework. This is an NP-hard problem and most approaches consider local search techniques. In this paper, we pose it as a combinatorial optimization problem, which is then relaxed to a convex optimization problem that involves searching over the forest and hyperforest polytopes with special structures, independently. A supergradient method is used to solve the dual problem, with a run-time complexity of $O(k^3 n^{k+2} \log n)$ for each iteration, where $n$ is the number of variables and $k$ is a bound on the treewidth. We compare our approach to state-of-the-art methods on synthetic datasets and classical benchmarks, showing the gains of the novel convex approach. version:1
arxiv-1212-2546 | A Learning Framework for Morphological Operators using Counter-Harmonic Mean | http://arxiv.org/abs/1212.2546 | id:1212.2546 author:Jonathan Masci, Jess Angulo, Jrgen Schmidhuber category:cs.CV  published:2012-12-11 summary:We present a novel framework for learning morphological operators using counter-harmonic mean. It combines concepts from morphology and convolutional neural networks. A thorough experimental validation analyzes basic morphological operators dilation and erosion, opening and closing, as well as the much more complex top-hat transform, for which we report a real-world application from the steel industry. Using online learning and stochastic gradient descent, our system learns both the structuring element and the composition of operators. It scales well to large datasets and online settings. version:1
arxiv-1212-2529 | On The Delays In Spiking Neural P Systems | http://arxiv.org/abs/1212.2529 | id:1212.2529 author:Francis George C. Cabarle, Kelvin C. Buo, Henry N. Adorna category:cs.NE cs.DC cs.ET 97P20 F.4.1  published:2012-12-11 summary:In this work we extend and improve the results done in a previous work on simulating Spiking Neural P systems (SNP systems in short) with delays using SNP systems without delays. We simulate the former with the latter over sequential, iteration, join, and split routing. Our results provide constructions so that both systems halt at exactly the same time, start with only one spike, and produce the same number of spikes to the environment after halting. version:1
arxiv-1212-2415 | Robust Face Recognition using Local Illumination Normalization and Discriminant Feature Point Selection | http://arxiv.org/abs/1212.2415 | id:1212.2415 author:Song Han, Jinsong Kim, Cholhun Kim, Jongchol Jo, Sunam Han category:cs.LG cs.CV  published:2012-12-11 summary:Face recognition systems must be robust to the variation of various factors such as facial expression, illumination, head pose and aging. Especially, the robustness against illumination variation is one of the most important problems to be solved for the practical use of face recognition systems. Gabor wavelet is widely used in face detection and recognition because it gives the possibility to simulate the function of human visual system. In this paper, we propose a method for extracting Gabor wavelet features which is stable under the variation of local illumination and show experiment results demonstrating its effectiveness. version:1
arxiv-1212-2414 | Mining Techniques in Network Security to Enhance Intrusion Detection Systems | http://arxiv.org/abs/1212.2414 | id:1212.2414 author:Maher Salem, Ulrich Buehler category:cs.CR cs.LG  published:2012-12-11 summary:In intrusion detection systems, classifiers still suffer from several drawbacks such as data dimensionality and dominance, different network feature types, and data impact on the classification. In this paper two significant enhancements are presented to solve these drawbacks. The first enhancement is an improved feature selection using sequential backward search and information gain. This, in turn, extracts valuable features that enhance positively the detection rate and reduce the false positive rate. The second enhancement is transferring nominal network features to numeric ones by exploiting the discrete random variable and the probability mass function to solve the problem of different feature types, the problem of data dominance, and data impact on the classification. The latter is combined to known normalization methods to achieve a significant hybrid normalization approach. Finally, an intensive and comparative study approves the efficiency of these enhancements and shows better performance comparing to other proposed methods. version:1
arxiv-1212-2390 | On the complexity of learning a language: An improvement of Block's algorithm | http://arxiv.org/abs/1212.2390 | id:1212.2390 author:Eric Werner category:cs.CL cs.LG  published:2012-12-11 summary:Language learning is thought to be a highly complex process. One of the hurdles in learning a language is to learn the rules of syntax of the language. Rules of syntax are often ordered in that before one rule can applied one must apply another. It has been thought that to learn the order of n rules one must go through all n! permutations. Thus to learn the order of 27 rules would require 27! steps or 1.08889x10^{28} steps. This number is much greater than the number of seconds since the beginning of the universe! In an insightful analysis the linguist Block ([Block 86], pp. 62-63, p.238) showed that with the assumption of transitivity this vast number of learning steps reduces to a mere 377 steps. We present a mathematical analysis of the complexity of Block's algorithm. The algorithm has a complexity of order n^2 given n rules. In addition, we improve Block's results exponentially, by introducing an algorithm that has complexity of order less than n log n. version:1
arxiv-1212-2340 | PAC-Bayesian Learning and Domain Adaptation | http://arxiv.org/abs/1212.2340 | id:1212.2340 author:Pascal Germain, Amaury Habrard, Franois Laviolette, Emilie Morvant category:stat.ML cs.LG  published:2012-12-11 summary:In machine learning, Domain Adaptation (DA) arises when the distribution gen- erating the test (target) data differs from the one generating the learning (source) data. It is well known that DA is an hard task even under strong assumptions, among which the covariate-shift where the source and target distributions diverge only in their marginals, i.e. they have the same labeling function. Another popular approach is to consider an hypothesis class that moves closer the two distributions while implying a low-error for both tasks. This is a VC-dim approach that restricts the complexity of an hypothesis class in order to get good generalization. Instead, we propose a PAC-Bayesian approach that seeks for suitable weights to be given to each hypothesis in order to build a majority vote. We prove a new DA bound in the PAC-Bayesian context. This leads us to design the first DA-PAC-Bayesian algorithm based on the minimization of the proposed bound. Doing so, we seek for a \rho-weighted majority vote that takes into account a trade-off between three quantities. The first two quantities being, as usual in the PAC-Bayesian approach, (a) the complexity of the majority vote (measured by a Kullback-Leibler divergence) and (b) its empirical risk (measured by the \rho-average errors on the source sample). The third quantity is (c) the capacity of the majority vote to distinguish some structural difference between the source and target samples. version:1
arxiv-1212-2262 | Bag-of-Words Representation for Biomedical Time Series Classification | http://arxiv.org/abs/1212.2262 | id:1212.2262 author:Jin Wang, Ping Liu, Mary F. H. She, Saeid Nahavandi, and Abbas Kouzani category:cs.LG cs.AI  published:2012-12-11 summary:Automatic analysis of biomedical time series such as electroencephalogram (EEG) and electrocardiographic (ECG) signals has attracted great interest in the community of biomedical engineering due to its important applications in medicine. In this work, a simple yet effective bag-of-words representation that is able to capture both local and global structure similarity information is proposed for biomedical time series representation. In particular, similar to the bag-of-words model used in text document domain, the proposed method treats a time series as a text document and extracts local segments from the time series as words. The biomedical time series is then represented as a histogram of codewords, each entry of which is the count of a codeword appeared in the time series. Although the temporal order of the local segments is ignored, the bag-of-words representation is able to capture high-level structural information because both local and global structural information are well utilized. The performance of the bag-of-words model is validated on three datasets extracted from real EEG and ECG signals. The experimental results demonstrate that the proposed method is not only insensitive to parameters of the bag-of-words model such as local segment length and codebook size, but also robust to noise. version:1
arxiv-1212-2245 | Fast and Robust Linear Motion Deblurring | http://arxiv.org/abs/1212.2245 | id:1212.2245 author:Martin Welk, Patrik Raudaschl, Thomas Schwarzbauer, Martin Erler, Martin Luter category:cs.CV I.4.4; G.1.9  published:2012-12-10 summary:We investigate efficient algorithmic realisations for robust deconvolution of grey-value images with known space-invariant point-spread function, with emphasis on 1D motion blur scenarios. The goal is to make deconvolution suitable as preprocessing step in automated image processing environments with tight time constraints. Candidate deconvolution methods are selected for their restoration quality, robustness and efficiency. Evaluation of restoration quality and robustness on synthetic and real-world test images leads us to focus on a combination of Wiener filtering with few iterations of robust and regularised Richardson-Lucy deconvolution. We discuss algorithmic optimisations for specific scenarios. In the case of uniform linear motion blur in coordinate direction, it is possible to achieve real-time performance (less than 50 ms) in single-threaded CPU computation on images of $256\times256$ pixels. For more general space-invariant blur settings, still favourable computation times are obtained. Exemplary parallel implementations demonstrate that the proposed method also achieves real-time performance for general 1D motion blurs in a multi-threaded CPU setting, and for general 2D blurs on a GPU. version:1
arxiv-1212-2145 | A Scale-Space Theory for Text | http://arxiv.org/abs/1212.2145 | id:1212.2145 author:Shuang-Hong Yang category:cs.IR cs.CL  published:2012-12-10 summary:Scale-space theory has been established primarily by the computer vision and signal processing communities as a well-founded and promising framework for multi-scale processing of signals (e.g., images). By embedding an original signal into a family of gradually coarsen signals parameterized with a continuous scale parameter, it provides a formal framework to capture the structure of a signal at different scales in a consistent way. In this paper, we present a scale space theory for text by integrating semantic and spatial filters, and demonstrate how natural language documents can be understood, processed and analyzed at multiple resolutions, and how this scale-space representation can be used to facilitate a variety of NLP and text analysis tasks. version:1
arxiv-1208-4271 | Minerva and minepy: a C engine for the MINE suite and its R, Python and MATLAB wrappers | http://arxiv.org/abs/1208.4271 | id:1208.4271 author:Davide Albanese, Michele Filosi, Roberto Visintainer, Samantha Riccadonna, Giuseppe Jurman, Cesare Furlanello category:stat.ML q-bio.QM  published:2012-08-21 summary:We introduce a novel implementation in ANSI C of the MINE family of algorithms for computing maximal information-based measures of dependence between two variables in large datasets, with the aim of a low memory footprint and ease of integration within bioinformatics pipelines. We provide the libraries minerva (with the R interface) and minepy for Python, MATLAB, Octave and C++. The C solution reduces the large memory requirement of the original Java implementation, has good upscaling properties, and offers a native parallelization for the R interface. Low memory requirements are demonstrated on the MINE benchmarks as well as on large (n=1340) microarray and Illumina GAII RNA-seq transcriptomics datasets. Availability and Implementation: Source code and binaries are freely available for download under GPL3 licence at http://minepy.sourceforge.net for minepy and through the CRAN repository http://cran.r-project.org for the R package minerva. All software is multiplatform (MS Windows, Linux and OSX). version:2
arxiv-1109-0093 | Local Component Analysis | http://arxiv.org/abs/1109.0093 | id:1109.0093 author:Nicolas Le Roux, Francis Bach category:cs.LG  published:2011-09-01 summary:Kernel density estimation, a.k.a. Parzen windows, is a popular density estimation method, which can be used for outlier detection or clustering. With multivariate data, its performance is heavily reliant on the metric used within the kernel. Most earlier work has focused on learning only the bandwidth of the kernel (i.e., a scalar multiplicative factor). In this paper, we propose to learn a full Euclidean metric through an expectation-minimization (EM) procedure, which can be seen as an unsupervised counterpart to neighbourhood component analysis (NCA). In order to avoid overfitting with a fully nonparametric density estimator in high dimensions, we also consider a semi-parametric Gaussian-Parzen density model, where some of the variables are modelled through a jointly Gaussian density, while others are modelled through Parzen windows. For these two models, EM leads to simple closed-form updates based on matrix inversions and eigenvalue decompositions. We show empirically that our method leads to density estimators with higher test-likelihoods than natural competing methods, and that the metrics may be used within most unsupervised learning techniques that rely on such metrics, such as spectral clustering or manifold learning methods. Finally, we present a stochastic approximation scheme which allows for the use of this method in a large-scale setting. version:4
arxiv-1212-1936 | High-dimensional sequence transduction | http://arxiv.org/abs/1212.1936 | id:1212.1936 author:Nicolas Boulanger-Lewandowski, Yoshua Bengio, Pascal Vincent category:cs.LG  published:2012-12-09 summary:We investigate the problem of transforming an input sequence into a high-dimensional output sequence in order to transcribe polyphonic audio music into symbolic notation. We introduce a probabilistic model based on a recurrent neural network that is able to learn realistic output distributions given the input and we devise an efficient algorithm to search for the global mode of that distribution. The resulting method produces musically plausible transcriptions even under high levels of noise and drastically outperforms previous state-of-the-art approaches on five datasets of synthesized sounds and real recordings, approximately halving the test error rate. version:1
arxiv-1109-5647 | Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization | http://arxiv.org/abs/1109.5647 | id:1109.5647 author:Alexander Rakhlin, Ohad Shamir, Karthik Sridharan category:cs.LG math.OC  published:2011-09-26 summary:Stochastic gradient descent (SGD) is a simple and popular method to solve stochastic optimization problems which arise in machine learning. For strongly convex problems, its convergence rate was known to be O(\log(T)/T), by running SGD for T iterations and returning the average point. However, recent results showed that using a different algorithm, one can get an optimal O(1/T) rate. This might lead one to believe that standard SGD is suboptimal, and maybe should even be replaced as a method of choice. In this paper, we investigate the optimality of SGD in a stochastic setting. We show that for smooth problems, the algorithm attains the optimal O(1/T) rate. However, for non-smooth problems, the convergence rate with averaging might really be \Omega(\log(T)/T), and this is not just an artifact of the analysis. On the flip side, we show that a simple modification of the averaging step suffices to recover the O(1/T) rate, and no other change of the algorithm is necessary. We also present experimental results which support our findings, and point out open problems. version:7
