arxiv-1705-09435 | Deep Learning for Lung Cancer Detection: Tackling the Kaggle Data Science Bowl 2017 Challenge | http://arxiv.org/abs/1705.09435 | id:1705.09435 author:Kingsley Kuan, Mathieu Ravaut, Gaurav Manek, Huiling Chen, Jie Lin, Babar Nazir, Cen Chen, Tse Chiang Howe, Zeng Zeng, Vijay Chandrasekhar category:cs.CV  published:2017-05-26 summary:We present a deep learning framework for computer-aided lung cancer diagnosis. Our multi-stage framework detects nodules in 3D lung CAT scans, determines if each nodule is malignant, and finally assigns a cancer probability based on these results. We discuss the challenges and advantages of our framework. In the Kaggle Data Science Bowl 2017, our framework ranked 41st out of 1972 teams. version:1
arxiv-1705-09425 | Hierarchical Cellular Automata for Visual Saliency | http://arxiv.org/abs/1705.09425 | id:1705.09425 author:Yao Qin, Mengyang Feng, Huchuan Lu, Garrison W. Cottrell category:cs.CV  published:2017-05-26 summary:Saliency detection, finding the most important parts of an image, has become increasingly popular in computer vision. In this paper, we introduce Hierarchical Cellular Automata (HCA) -- a temporally evolving model to intelligently detect salient objects. HCA consists of two main components: Single-layer Cellular Automata (SCA) and Cuboid Cellular Automata (CCA). As an unsupervised propagation mechanism, Single-layer Cellular Automata can exploit the intrinsic relevance of similar regions through interactions with neighbors. Low-level image features as well as high-level semantic information extracted from deep neural networks are incorporated into the SCA to measure the correlation between different image patches. With these hierarchical deep features, an impact factor matrix and a coherence matrix are constructed to balance the influences on each cell's next state. The saliency values of all cells are iteratively updated according to a well-defined update rule. Furthermore, we propose CCA to integrate multiple saliency maps generated by SCA at different scales in a Bayesian framework. Therefore, single-layer propagation and multi-layer integration are jointly modeled in our unified HCA. Surprisingly, we find that the SCA can improve all existing methods that we applied it to, resulting in a similar precision level regardless of the original results. The CCA can act as an efficient pixel-wise aggregation algorithm that can integrate state-of-the-art methods, resulting in even better results. Extensive experiments on four challenging datasets demonstrate that the proposed algorithm outperforms state-of-the-art conventional methods and is competitive with deep learning based approaches. version:1
arxiv-1705-09422 | Text-Independent Speaker Verification Using 3D Convolutional Neural Networks | http://arxiv.org/abs/1705.09422 | id:1705.09422 author:Amirsina Torfi, Nasser M. Nasrabadi, Jeremy Dawson category:cs.CV  published:2017-05-26 summary:In this paper, a 3D Convolutional Neural Network (3D-CNN) architecture has been utilized for text-independent speaker verification. At the development phase, a CNN is trained to classify speakers at the utterance-level. In the enrollment stage, the trained network is utilized to directly create a speaker model for each speaker based on the extracted features. Finally, in the evaluation phase, the extracted features from the test utterance will be compared to the stored speaker model to verify the claimed identity. One of the main challenges is the creation of the speaker models. Previously-reported approaches create speaker models based on averaging the extracted features from utterances of the speaker, which is known as a d-vector system. In our paper, we propose to use the 3D-CNNs for direct speaker model creation in which, for both development and enrollment phases, an identical number of speaker utterances is fed to the network for representing the speaker utterances and creation of the speaker model. This leads to simultaneously capturing the speaker-related information and building a more robust system to cope with within-speaker variation. We demonstrate that the proposed method significantly outperforms the d-vector verification system. version:1
arxiv-1705-09416 | Dual Based DSP Bidding Strategy and its Application | http://arxiv.org/abs/1705.09416 | id:1705.09416 author:Huahui Liu, Mingrui Zhu, Xiaonan Meng, Yi Hu, Hao Wang category:stat.ML cs.GT  published:2017-05-26 summary:In recent years, RTB(Real Time Bidding) becomes a popular online advertisement trading method. During the auction, each DSP is supposed to evaluate this opportunity and respond with an ad and corresponding bid price. Generally speaking, this is a kind of assginment problem for DSP. However, unlike traditional one, this assginment problem has bid price as additional variable. It's essential to find an optimal ad selection and bid price determination strategy. In this document, two major steps are taken to tackle it. First, the augmented GAP(Generalized Assignment Problem) is proposed and a general bidding strategy is correspondingly provided. Second, we show that DSP problem is a special case of the augmented GAP and the general bidding strategy applies. To the best of our knowledge, our solution is the first DSP bidding framework that is derived from strict second price auction assumption and is generally applicable to the multiple ads scenario with various objectives and constraints. Our strategy is verified through simulation and outperforms state-of-the-art strategies in real application. version:1
arxiv-1705-00995 | Fuzzy Approach Topic Discovery in Health and Medical Corpora | http://arxiv.org/abs/1705.00995 | id:1705.00995 author:Amir Karami, Aryya Gangopadhyay, Bin Zhou, Hadi Kharrazi category:stat.ML cs.CL cs.IR  published:2017-05-02 summary:The majority of medical documents and electronic health records (EHRs) are in text format that poses a challenge for data processing and finding relevant documents. Looking for ways to automatically retrieve the enormous amount of health and medical knowledge has always been an intriguing topic. Powerful methods have been developed in recent years to make the text processing automatic. One of the popular approaches to retrieve information based on discovering the themes in health & medical corpora is topic modeling, however, this approach still needs new perspectives. In this research we describe fuzzy latent semantic analysis (FLSA), a novel approach in topic modeling using fuzzy perspective. FLSA can handle health & medical corpora redundancy issue and provides a new method to estimate the number of topics. The quantitative evaluations show that FLSA produces superior performance and features to latent Dirichlet allocation (LDA), the most popular topic model. version:2
arxiv-1705-09406 | Multimodal Machine Learning: A Survey and Taxonomy | http://arxiv.org/abs/1705.09406 | id:1705.09406 author:Tadas Baltrušaitis, Chaitanya Ahuja, Louis-Philippe Morency category:cs.LG  published:2017-05-26 summary:Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research. version:1
arxiv-1705-10667 | Domain Adaptation with Randomized Multilinear Adversarial Networks | http://arxiv.org/abs/1705.10667 | id:1705.10667 author:Mingsheng Long, Zhangjie Cao, Jianmin Wang, Michael I. Jordan category:cs.LG  published:2017-05-26 summary:Adversarial learning has been successfully embedded into deep networks to learn transferable features for domain adaptation, which reduce distribution discrepancy between the source and target domains and improve generalization performance. Prior domain adversarial adaptation methods could not align complex multimode distributions since the discriminative structures and inter-layer interactions across multiple domain-specific layers have not been exploited for distribution alignment. In this paper, we present randomized multilinear adversarial networks (RMAN), which exploit multiple feature layers and the classifier layer based on a randomized multilinear adversary to enable both deep and discriminative adversarial adaptation. The learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments demonstrate that our models exceed the state-of-the-art results on standard domain adaptation datasets. version:1
arxiv-1705-09056 | Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent | http://arxiv.org/abs/1705.09056 | id:1705.09056 author:Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, Ji Liu category:math.OC cs.DC cs.LG stat.ML  published:2017-05-25 summary:Most distributed machine learning systems nowadays, including TensorFlow and CNTK, are built in a centralized fashion. One bottleneck of centralized algorithms lies on high communication cost on the central node. Motivated by this, we ask, can decentralized algorithms be faster than its centralized counterpart? Although decentralized PSGD (D-PSGD) algorithms have been studied by the control community, existing analysis and theory do not show any advantage over centralized PSGD (C-PSGD) algorithms, simply assuming the application scenario where only the decentralized network is available. In this paper, we study a D-PSGD algorithm and provide the first theoretical analysis that indicates a regime in which decentralized algorithms might outperform centralized algorithms for distributed stochastic gradient descent. This is because D-PSGD has comparable total computational complexities to C-PSGD but requires much less communication cost on the busiest node. We further conduct an empirical study to validate our theoretical analysis across multiple frameworks (CNTK and Torch), different network configurations, and computation platforms up to 112 GPUs. On network configurations with low bandwidth or high latency, D-PSGD can be up to one order of magnitude faster than its well-optimized centralized counterparts. version:2
arxiv-1705-09369 | Unsupervised Feature Learning for Writer Identification and Writer Retrieval | http://arxiv.org/abs/1705.09369 | id:1705.09369 author:Vincent Christlein, Martin Gropp, Stefan Fiel, Andreas Maier category:cs.CV  published:2017-05-25 summary:Deep Convolutional Neural Networks (CNN) have shown great success in supervised classification tasks such as character classification or dating. Deep learning methods typically need a lot of annotated training data, which is not available in many scenarios. In these cases, traditional methods are often better than or equivalent to deep learning methods. In this paper, we propose a simple, yet effective, way to learn CNN activation features in an unsupervised manner. Therefore, we train a deep residual network using surrogate classes. The surrogate classes are created by clustering the training dataset, where each cluster index represents one surrogate class. The activations from the penultimate CNN layer serve as features for subsequent classification tasks. We evaluate the feature representations on two publicly available datasets. The focus lies on the ICDAR17 competition dataset on historical document writer identification (Historical-WI). We show that the activation features we trained without supervision are superior to descriptors of state-of-the-art writer identification methods. Additionally, we achieve comparable results in the case of handwriting classification using the ICFHR16 competition dataset on historical Latin script types (CLaMM16). version:1
arxiv-1705-09367 | Stabilizing Training of Generative Adversarial Networks through Regularization | http://arxiv.org/abs/1705.09367 | id:1705.09367 author:Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, Thomas Hofmann category:cs.LG stat.ML  published:2017-05-25 summary:Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch between the model distribution and the true distribution, causing their density ratio and the associated f-divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this approach on several datasets including common benchmark image generation tasks. Our approach turns GAN models into reliable building blocks for deep learning. version:1
arxiv-1705-09359 | Time-Based Label Refinements to Discover More Precise Process Models | http://arxiv.org/abs/1705.09359 | id:1705.09359 author:Niek Tax, Emin Alasgarov, Natalia Sidorova, Wil M. P. van der Aalst, Reinder Haakma category:cs.LG cs.AI cs.DB  published:2017-05-25 summary:Process mining is a research field focused on the analysis of event data with the aim of extracting insights related to dynamic behavior. Applying process mining techniques on data from smart home environments has the potential to provide valuable insights in (un)healthy habits and to contribute to ambient assisted living solutions. Finding the right event labels to enable the application of process mining techniques is however far from trivial, as simply using the triggering sensor as the label for sensor events results in uninformative models that allow for too much behavior (overgeneralizing). Refinements of sensor level event labels suggested by domain experts have been shown to enable discovery of more precise and insightful process models. However, there exists no automated approach to generate refinements of event labels in the context of process mining. In this paper we propose a framework for the automated generation of label refinements based on the time attribute of events, allowing us to distinguish behaviourally different instances of the same event type based on their time attribute. We show on a case study with real life smart home event data that using automatically generated refined labels in process discovery, we can find more specific, and therefore more insightful, process models. We observe that one label refinement could have an effect on the usefulness of other label refinements when used together. Therefore, we explore four strategies to generate useful combinations of multiple label refinements and evaluate those on three real life smart home event logs. version:1
arxiv-1705-09353 | Predictive State Recurrent Neural Networks | http://arxiv.org/abs/1705.09353 | id:1705.09353 author:Carlton Downey, Ahmed Hefny, Boyue Li, Byron Boots, Geoffrey Gordon category:stat.ML  published:2017-05-25 summary:We present a new model, called Predictive State Recurrent Neural Networks (PSRNNs), for filtering and prediction in dynamical systems. PSRNNs draw on insights from both Recurrent Neural Networks (RNNs) and Predictive State Representations (PSRs), and inherit advantages from both types of models. Like many successful RNN architectures, PSRNNs use (potentially deeply composed) bilinear transfer functions to combine information from multiple sources, so that one source can act as a gate for another. These bilinear functions arise naturally from the connection to state updates in Bayes filters like PSRs, in which observations can be viewed as gating belief states. We show that PSRNNs can be learned effectively by combining backpropogation through time (BPTT) with an initialization based on a statistically consistent learning algorithm for PSRs called two-stage regression (2SR). We also show that PSRNNs can be can be factorized using tensor decomposition, reducing model size and suggesting interesting theoretical connections to existing multiplicative architectures such as LSTMs. We applied PSRNNs to 4 datasets, and showed that we outperform several popular alternative approaches to modeling dynamical systems in all cases. version:1
arxiv-1705-10202 | Mining Process Model Descriptions of Daily Life through Event Abstraction | http://arxiv.org/abs/1705.10202 | id:1705.10202 author:Niek Tax, Natalia Sidorova, Reinder Haakma, Wil M. P. van der Aalst category:cs.LG cs.AI cs.DB  published:2017-05-25 summary:Process mining techniques focus on extracting insight in processes from event logs. Process mining has the potential to provide valuable insights in (un)healthy habits and to contribute to ambient assisted living solutions when applied on data from smart home environments. However, events recorded in smart home environments are on the level of sensor triggers, at which process discovery algorithms produce overgeneralizing process models that allow for too much behavior and that are difficult to interpret for human experts. We show that abstracting the events to a higher-level interpretation can enable discovery of more precise and more comprehensible models. We present a framework for the extraction of features that can be used for abstraction with supervised learning methods that is based on the XES IEEE standard for event logs. This framework can automatically abstract sensor-level events to their interpretation at the human activity level, after training it on training data for which both the sensor and human activity events are known. We demonstrate our abstraction framework on three real-life smart home event logs and show that the process models that can be discovered after abstraction are more precise indeed. version:1
arxiv-1705-09339 | Real-Time Background Subtraction Using Adaptive Sampling and Cascade of Gaussians | http://arxiv.org/abs/1705.09339 | id:1705.09339 author:B Ravi Kiran, Senthil Yogamani category:stat.ML cs.CV  published:2017-05-25 summary:Background-Foreground classification is a fundamental well-studied problem in computer vision. Due to the pixel-wise nature of modeling and processing in the algorithm, it is usually difficult to satisfy real-time constraints. There is a trade-off between the speed (because of model complexity) and accuracy. Inspired by the rejection cascade of Viola-Jones classifier, we decompose the Gaussian Mixture Model (GMM) into an adaptive cascade of classifiers. This way we achieve a good improvement in speed without compensating for accuracy. In the training phase, we learn multiple KDEs for different durations to be used as strong prior distribution and detect probable oscillating pixels which usually results in misclassifications. We propose a confidence measure for the classifier based on temporal consistency and the prior distribution. The confidence measure thus derived is used to adapt the learning rate and the thresholds of the model, to improve accuracy. The confidence measure is also employed to perform temporal and spatial sampling in a principled way. We demonstrate a speed-up factor of 5x to 10x and 17 percent average improvement in accuracy over several standard videos. version:1
arxiv-1705-09322 | Convergent Tree-Backup and Retrace with Function Approximation | http://arxiv.org/abs/1705.09322 | id:1705.09322 author:Ahmed Touati, Pierre-Luc Bacon, Doina Precup, Pascal Vincent category:cs.LG  published:2017-05-25 summary:Off-policy learning is key to scaling up reinforcement learning as it allows to learn about a target policy from the experience generated by a different behavior policy. Unfortunately, it has been challenging to combine off-policy learning with function approximation and multi-step bootstrapping in a way that leads to both stable and efficient algorithms. In this paper, we show that the Tree Backup and Retrace algorithms are unstable with linear function approximation, both in theory and with specific examples. Based on our analysis, we then derive stable and efficient gradient-based algorithms, compatible with accumulating or Dutch traces, using a novel methodology based on proximal methods. In addition to convergence proofs, we provide sample-complexity bounds. version:1
arxiv-1705-09319 | Diagonal Rescaling For Neural Networks | http://arxiv.org/abs/1705.09319 | id:1705.09319 author:Jean Lafond, Nicolas Vasilache, Léon Bottou category:cs.LG stat.ML  published:2017-05-25 summary:We define a second-order neural network stochastic gradient training algorithm whose block-diagonal structure effectively amounts to normalizing the unit activations. Investigating why this algorithm lacks in robustness then reveals two interesting insights. The first insight suggests a new way to scale the stepsizes, clarifying popular algorithms such as RMSProp as well as old neural network tricks such as fanin stepsize scaling. The second insight stresses the practical importance of dealing with fast changes of the curvature of the cost. version:1
arxiv-1705-09314 | Plan3D: Viewpoint and Trajectory Optimization for Aerial Multi-View Stereo Reconstruction | http://arxiv.org/abs/1705.09314 | id:1705.09314 author:Benjamin Hepp, Matthias Nießner, Otmar Hilliges category:cs.CV  published:2017-05-25 summary:We introduce a new method that efficiently computes a set of rich viewpoints and trajectories for high-quality 3D reconstructions in outdoor environments. The input images of the reconstruction are taken with a commodity RGB camera which is mounted on an autonomously navigated quadcopter, and the obtained recordings are fed into a multi-view stereo reconstruction pipeline that produces high-quality results but is computationally expensive. Our goal is to automatically explore an unknown area, and obtain a complete 3D scan of a region of interest (e.g., a large building). In this process, the scan is constraint by the restricted flight time of quadcopters and the heavy compute costs of the subsequent 3D reconstruction -- i.e., only a small number of images can be recorded and processed. To this end, we introduce a novel optimization strategy that respects these constraints by maximizing the information gain from sparsely-sampled view points while limiting the total number of captured images. The core of this strategy is based on the concept of tri-state space classification, which is common in volumetric fusion approaches, and includes labels for unknown, free, and occupied space. Our optimization leverages a hierarchical and sparse volumetric data structure that takes advantage of the implicit representation, where its main objective is to convert unknown space into known regions. In addition to the surface geometry, we utilize the free-space information to avoid obstacles and determine feasible flight paths. A simple tool can be used to specify the region of interest and to plan trajectories. We demonstrate our method by obtaining a number of compelling 3D reconstructions, and provide a thorough quantitative evaluation for our optimization strategy. version:1
arxiv-1705-09307 | Direct Multitype Cardiac Indices Estimation via Joint Representation and Regression Learning | http://arxiv.org/abs/1705.09307 | id:1705.09307 author:Wufeng Xue, Ali Islam, Mousumi Bhaduri, Shuo Li category:cs.CV  published:2017-05-25 summary:Cardiac indices estimation is of great importance during identification and diagnosis of cardiac disease in clinical routine. However, estimation of multitype cardiac indices with consistently reliable and high accuracy is still a great challenge due to the high variability of cardiac structures and complexity of temporal dynamics in cardiac MR sequences. While efforts have been devoted into cardiac volumes estimation through feature engineering followed by a independent regression model, these methods suffer from the vulnerable feature representation and incompatible regression model. In this paper, we propose a semi-automated method for multitype cardiac indices estimation. After manual labelling of two landmarks for ROI cropping, an integrated deep neural network Indices-Net is designed to jointly learn the representation and regression models. It comprises two tightly-coupled networks: a deep convolution autoencoder (DCAE) for cardiac image representation, and a multiple output convolution neural network (CNN) for indices regression. Joint learning of the two networks effectively enhances the expressiveness of image representation with respect to cardiac indices, and the compatibility between image representation and indices regression, thus leading to accurate and reliable estimations for all the cardiac indices. When applied with five-fold cross validation on MR images of 145 subjects, Indices-Net achieves consistently low estimation error for LV wall thicknesses (1.44$\pm$0.71mm) and areas of cavity and myocardium (204$\pm$133mm$^2$). It outperforms, with significant error reductions, segmentation method (55.1% and 17.4%) and two-phase direct volume-only methods (12.7% and 14.6%) for wall thicknesses and areas, respectively. These advantages endow the proposed method a great potential in clinical cardiac function assessment. version:1
arxiv-1705-09303 | Latent Geometry and Memorization in Generative Models | http://arxiv.org/abs/1705.09303 | id:1705.09303 author:Matt Feiszli category:cs.LG stat.ML  published:2017-05-25 summary:It can be difficult to tell whether a trained generative model has learned to generate novel examples or has simply memorized a specific set of outputs. In published work, it is common to attempt to address this visually, for example by displaying a generated example and its nearest neighbor(s) in the training set (in, for example, the L2 metric). As any generative model induces a probability density on its output domain, we propose studying this density directly. We first study the geometry of the latent representation and generator, relate this to the output density, and then develop techniques to compute and inspect the output density. As an application, we demonstrate that "memorization" tends to a density made of delta functions concentrated on the memorized examples. We note that without first understanding the geometry, the measurement would be essentially impossible to make. version:1
arxiv-1705-09296 | A Neural Framework for Generalized Topic Models | http://arxiv.org/abs/1705.09296 | id:1705.09296 author:Dallas Card, Chenhao Tan, Noah A. Smith category:stat.ML cs.CL  published:2017-05-25 summary:Topic models for text corpora comprise a popular family of methods that have inspired many extensions to encode properties such as sparsity, interactions with covariates, and the gradual evolution of topics. In this paper, we combine certain motivating ideas behind variations on topic models with modern techniques for variational inference to produce a flexible framework for topic modeling that allows for rapid exploration of different models. We first discuss how our framework relates to existing models, and then demonstrate that it achieves strong performance, with the introduction of sparsity controlling the trade off between perplexity and topic coherence. version:1
arxiv-1705-09283 | Gated XNOR Networks: Deep Neural Networks with Ternary Weights and Activations under a Unified Discretization Framework | http://arxiv.org/abs/1705.09283 | id:1705.09283 author:Lei Deng, Peng Jiao, Jing Pei, Zhenzhi Wu, Guoqi Li category:cs.LG cs.CV stat.ML  published:2017-05-25 summary:There is a pressing need to build an architecture that could subsume these networks undera unified framework that achieves both higher performance and less overhead. To this end, two fundamental issues are yet to be addressed. The first one is how to implement the back propagation when neuronal activations are discrete. The second one is how to remove the full-precision hidden weights in the training phase to break the bottlenecks of memory/computation consumption. To address the first issue, we present a multistep neuronal activation discretization method and a derivative approximation technique that enable the implementing the back propagation algorithm on discrete DNNs. While for the second issue, we propose a discrete state transition (DST) methodology to constrain the weights in a discrete space without saving the hidden weights. In this way, we build a unified framework that subsumes the binary or ternary networks as its special cases.More particularly, we find that when both the weights and activations become ternary values, the DNNs can be reduced to gated XNOR networks (or sparse binary networks) since only the event of non-zero weight and non-zero activation enables the control gate to start the XNOR logic operations in the original binary networks. This promises the event-driven hardware design for efficient mobile intelligence. We achieve advanced performance compared with state-of-the-art algorithms. Furthermore,the computational sparsity and the number of states in the discrete space can be flexibly modified to make it suitable for various hardware platforms. version:1
arxiv-1705-09280 | Implicit Regularization in Matrix Factorization | http://arxiv.org/abs/1705.09280 | id:1705.09280 author:Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, Nathan Srebro category:stat.ML cs.LG  published:2017-05-25 summary:We study implicit regularization when optimizing an underdetermined quadratic objective over a matrix $X$ with gradient descent on a factorization of $X$. We conjecture and provide empirical and theoretical evidence that with small enough step sizes and initialization close enough to the origin, gradient descent on a full dimensional factorization converges to the minimum nuclear norm solution. version:1
arxiv-1705-09279 | Filtering Variational Objectives | http://arxiv.org/abs/1705.09279 | id:1705.09279 author:Chris J. Maddison, Dieterich Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, Yee Whye Teh category:cs.LG cs.AI cs.NE stat.ML  published:2017-05-25 summary:The evidence lower bound (ELBO) appears in many algorithms for maximum likelihood estimation (MLE) with latent variables because it is a sharp lower bound of the marginal log-likelihood. For neural latent variable models, optimizing the ELBO jointly in the variational posterior and model parameters produces state-of-the-art results. Inspired by the success of the ELBO as a surrogate MLE objective, we consider the extension of the ELBO to a family of lower bounds defined by a Monte Carlo estimator of the marginal likelihood. We show that the tightness of such bounds is asymptotically related to the variance of the underlying estimator. We introduce a special case, the filtering variational objectives (FIVOs), which takes the same arguments as the ELBO and passes them through a particle filter to form a tighter bound. FIVOs can be optimized tractably with stochastic gradients, and are particularly suited to MLE in sequential latent variable models. In standard sequential generative modeling tasks we present uniform improvements over models trained with ELBO, including some whole nat-per-timestep improvements. version:1
arxiv-1705-09269 | Geometric Methods for Robust Data Analysis in High Dimension | http://arxiv.org/abs/1705.09269 | id:1705.09269 author:Joseph Anderson category:cs.LG  published:2017-05-25 summary:Machine learning and data analysis now finds both scientific and industrial application in biology, chemistry, geology, medicine, and physics. These applications rely on large quantities of data gathered from automated sensors and user input. Furthermore, the dimensionality of many datasets is extreme: more details are being gathered about single user interactions or sensor readings. All of these applications encounter problems with a common theme: use observed data to make inferences about the world. Our work obtains the first provably efficient algorithms for Independent Component Analysis (ICA) in the presence of heavy-tailed data. The main tool in this result is the centroid body (a well-known topic in convex geometry), along with optimization and random walks for sampling from a convex body. This is the first algorithmic use of the centroid body and it is of independent theoretical interest, since it effectively replaces the estimation of covariance from samples, and is more generally accessible. This reduction relies on a non-linear transformation of samples from such an intersection of halfspaces (i.e. a simplex) to samples which are approximately from a linearly transformed product distribution. Through this transformation of samples, which can be done efficiently, one can then use an ICA algorithm to recover the vertices of the intersection of halfspaces. Finally, we again use ICA as an algorithmic primitive to construct an efficient solution to the widely-studied problem of learning the parameters of a Gaussian mixture model. Our algorithm again transforms samples from a Gaussian mixture model into samples which fit into the ICA model and, when processed by an ICA algorithm, result in recovery of the mixture parameters. Our algorithm is effective even when the number of Gaussians in the mixture grows polynomially with the ambient dimension version:1
arxiv-1705-08076 | Learning from partial correction | http://arxiv.org/abs/1705.08076 | id:1705.08076 author:Sanjoy Dasgupta, Michael Luby category:cs.LG  published:2017-05-23 summary:We introduce a new model of interactive learning in which an expert examines the predictions of a learner and partially fixes them if they are wrong. Although this kind of feedback is not i.i.d., we show statistical generalization bounds on the quality of the learned model. version:3
arxiv-1705-09236 | Asynchronous Parallel Bayesian Optimisation via Thompson Sampling | http://arxiv.org/abs/1705.09236 | id:1705.09236 author:Kirthevasan Kandasamy, Akshay Krishnamurthy, Jeff Schneider, Barnabas Poczos category:stat.ML cs.LG  published:2017-05-25 summary:We design and analyse variations of the classical Thompson sampling (TS) procedure for Bayesian optimisation (BO) in settings where function evaluations are expensive, but can be performed in parallel. Our theoretical analysis shows that a direct application of the sequential Thompson sampling algorithm in either synchronous or asynchronous parallel settings yields a surprisingly powerful result: making $n$ evaluations distributed among $M$ workers is essentially equivalent to performing $n$ evaluations in sequence. Further, by modeling the time taken to complete a function evaluation, we show that, under a time constraint, asynchronously parallel TS achieves asymptotically lower regret than both the synchronous and sequential versions. These results are complemented by an experimental analysis, showing that asynchronous TS outperforms a suite of existing parallel BO algorithms in simulations and in a hyper-parameter tuning application in convolutional neural networks. In addition to these, the proposed procedure is conceptually and computationally much simpler than existing work for parallel BO. version:1
arxiv-1705-09207 | Learning Structured Text Representations | http://arxiv.org/abs/1705.09207 | id:1705.09207 author:Yang Liu, Mirella Lapata category:cs.CL  published:2017-05-25 summary:In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias, we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluation across different tasks and datasets shows that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful. version:1
arxiv-1705-09199 | Towards Consistency of Adversarial Training for Generative Models | http://arxiv.org/abs/1705.09199 | id:1705.09199 author:Mathieu Sinn, Ambrish Rawat category:stat.ML  published:2017-05-25 summary:This work presents a rigorous statistical analysis of adversarial training for generative models, advancing recent work by Arjovsky and Bottou [2]. A key element is the distinction between the objective function with respect to the (unknown) data distribution, and its empirical counterpart. This yields a straight-forward explanation for common pathologies in practical adversarial training such as vanishing gradients. To overcome such issues, we pursue the idea of smoothing the Jensen-Shannon Divergence (JSD) by incorporating noise in the formulation of the discriminator. As we show, this effectively leads to an empirical version of the JSD in which the true and the generator densities are replaced by kernel density estimates. We analyze statistical consistency of this objective, and demonstrate its practical effectiveness. version:1
arxiv-1705-09193 | Classification of Quantitative Light-Induced Fluorescence Images Using Convolutional Neural Network | http://arxiv.org/abs/1705.09193 | id:1705.09193 author:Sultan Imangaliyev, Monique H. van der Veen, Catherine M. C. Volgenant, Bruno G. Loos, Bart J. F. Keijser, Wim Crielaard, Evgeni Levin category:cs.CV cs.LG  published:2017-05-25 summary:Images are an important data source for diagnosis and treatment of oral diseases. The manual classification of images may lead to misdiagnosis or mistreatment due to subjective errors. In this paper an image classification model based on Convolutional Neural Network is applied to Quantitative Light-induced Fluorescence images. The deep neural network outperforms other state of the art shallow classification models in predicting labels derived from three different dental plaque assessment scores. The model directly benefits from multi-channel representation of the images resulting in improved performance when, besides the Red colour channel, additional Green and Blue colour channels are used. version:1
arxiv-1705-09189 | Jointly Learning Sentence Embeddings and Syntax with Unsupervised Tree-LSTMs | http://arxiv.org/abs/1705.09189 | id:1705.09189 author:Jean Maillard, Stephen Clark, Dani Yogatama category:cs.CL  published:2017-05-25 summary:We introduce a neural network that represents sentences by composing their words according to induced binary parse trees. We use Tree-LSTM as our composition function, applied along a tree structure found by a fully differentiable natural language chart parser. Our model simultaneously optimises both the composition function and the parser, thus eliminating the need for externally-provided parse trees which are normally required for Tree-LSTM. It can therefore be seen as a tree-based RNN that is unsupervised with respect to the parse trees. As it is fully differentiable, our model is easily trained with an off-the-shelf gradient descent method and backpropagation. We demonstrate that it achieves better performance compared to various supervised Tree-LSTM architectures on a textual entailment task and a reverse dictionary task. version:1
arxiv-1705-09185 | Investigation of Using VAE for i-Vector Speaker Verification | http://arxiv.org/abs/1705.09185 | id:1705.09185 author:Timur Pekhovsky, Maxim Korenevsky category:cs.SD cs.LG stat.ML  published:2017-05-25 summary:New system for i-vector speaker recognition based on variational autoencoder (VAE) is investigated. VAE is a promising approach for developing accurate deep nonlinear generative models of complex data. Experiments show that VAE provides speaker embedding and can be effectively trained in an unsupervised manner. LLR estimate for VAE is developed. Experiments on NIST SRE 2010 data demonstrate its correctness. Additionally, we show that the performance of VAE-based system in the i-vectors space is close to that of the diagonal PLDA. Several interesting results are also observed in the experiments with $\beta$-VAE. In particular, we found that for $\beta\ll 1$, VAE can be trained to capture the features of complex input data distributions in an effective way, which is hard to obtain in the standard VAE ($\beta=1$). version:1
arxiv-1705-10589 | Jeffrey's prior sampling of deep sigmoidal networks | http://arxiv.org/abs/1705.10589 | id:1705.10589 author:Lorien X. Hayden, Alexander A. Alemi, Paul H. Ginsparg, James P. Sethna category:cond-mat.dis-nn cs.CV  published:2017-05-25 summary:Neural networks have been shown to have a remarkable ability to uncover low dimensional structure in data: the space of possible reconstructed images form a reduced model manifold in image space. We explore this idea directly by analyzing the manifold learned by Deep Belief Networks and Stacked Denoising Autoencoders using Monte Carlo sampling. The model manifold forms an only slightly elongated hyperball with actual reconstructed data appearing predominantly on the boundaries of the manifold. In connection with the results we present, we discuss problems of sampling high-dimensional manifolds as well as recent work [M. Transtrum, G. Hart, and P. Qiu, Submitted (2014)] discussing the relation between high dimensional geometry and model reduction. version:1
arxiv-1706-02985 | Stock Trading Using PE ratio: A Dynamic Bayesian Network Modeling on Behavioral Finance and Fundamental Investment | http://arxiv.org/abs/1706.02985 | id:1706.02985 author:Haizhen Wang, Ratthachat Chatpatanasiri, Pairote Sattayatham category:cs.CE cs.AI cs.LG q-fin.GN  published:2017-05-25 summary:On a daily investment decision in a security market, the price earnings (PE) ratio is one of the most widely applied methods being used as a firm valuation tool by investment experts. Unfortunately, recent academic developments in financial econometrics and machine learning rarely look at this tool. In practice, fundamental PE ratios are often estimated only by subjective expert opinions. The purpose of this research is to formalize a process of fundamental PE estimation by employing advanced dynamic Bayesian network (DBN) methodology. The estimated PE ratio from our model can be used either as a information support for an expert to make investment decisions, or as an automatic trading system illustrated in experiments. Forward-backward inference and EM parameter estimation algorithms are derived with respect to the proposed DBN structure. Unlike existing works in literatures, the economic interpretation of our DBN model is well-justified by behavioral finance evidences of volatility. A simple but practical trading strategy is invented based on the result of Bayesian inference. Extensive experiments show that our trading strategy equipped with the inferenced PE ratios consistently outperforms standard investment benchmarks. version:1
arxiv-1705-09620 | Discriminative Metric Learning with Deep Forest | http://arxiv.org/abs/1705.09620 | id:1705.09620 author:Lev V. Utkin, Mikhail A. Ryabinin category:stat.ML cs.LG 68T10  published:2017-05-25 summary:A Discriminative Deep Forest (DisDF) as a metric learning algorithm is proposed in the paper. It is based on the Deep Forest or gcForest proposed by Zhou and Feng and can be viewed as a gcForest modification. The case of the fully supervised learning is studied when the class labels of individual training examples are known. The main idea underlying the algorithm is to assign weights to decision trees in random forest in order to reduce distances between objects from the same class and to increase them between objects from different classes. The weights are training parameters. A specific objective function which combines Euclidean and Manhattan distances and simplifies the optimization problem for training the DisDF is proposed. The numerical experiments illustrate the proposed distance metric algorithm. version:1
arxiv-1705-09142 | Deep image representations using caption generators | http://arxiv.org/abs/1705.09142 | id:1705.09142 author:Konda Reddy Mopuri, Vishal B. Athreya, R. Venkatesh Babu category:cs.CV  published:2017-05-25 summary:Deep learning exploits large volumes of labeled data to learn powerful models. When the target dataset is small, it is a common practice to perform transfer learning using pre-trained models to learn new task specific representations. However, pre-trained CNNs for image recognition are provided with limited information about the image during training, which is label alone. Tasks such as scene retrieval suffer from features learned from this weak supervision and require stronger supervision to better understand the contents of the image. In this paper, we exploit the features learned from caption generating models to learn novel task specific image representations. In particular, we consider the state-of-the art captioning system Show and Tell~\cite{SnT-pami-2016} and the dense region description model DenseCap~\cite{densecap-cvpr-2016}. We demonstrate that, owing to richer supervision provided during the process of training, the features learned by the captioning system perform better than those of CNNs. Further, we train a siamese network with a modified pair-wise loss to fuse the features learned by~\cite{SnT-pami-2016} and~\cite{densecap-cvpr-2016} and learn image representations suitable for retrieval. Experiments show that the proposed fusion exploits the complementary nature of the individual features and yields state-of-the art retrieval results on benchmark datasets. version:1
arxiv-1705-09132 | First-spike based visual categorization using reward-modulated STDP | http://arxiv.org/abs/1705.09132 | id:1705.09132 author:Milad Mozafari, Saeed Reza Kheradpisheh, Timothée Masquelier, Abbas Nowzari-Dalini, Mohammad Ganjtabesh category:q-bio.NC cs.CV  published:2017-05-25 summary:Reinforcement learning (RL) has recently regained popularity, with major achievements such as beating the European game of Go champion. Here, for the first time, we show that RL can be used efficiently to train a spiking neural network (SNN) to perform object recognition in natural images without using an external classifier. We used a feedforward convolutional SNN and a temporal coding scheme where the most strongly activated neurons fire first, while less activated ones fire later, or not at all. In the highest layers, each neuron was assigned to an object category, and it was assumed that the stimulus category was the category of the first neuron to fire. If this assumption was correct, the neuron was rewarded, i.e. spike-timing-dependent plasticity (STDP) was applied, which reinforced the neuron's selectivity. Otherwise, anti-STDP was applied, which encouraged the neuron to learn something else. As demonstrated on various image datasets (Caltech, ETH-80, and NORB), this reward modulated STDP (R-STDP) approach extracted particularly discriminative visual features, whereas classic unsupervised STDP extracts any feature that consistently repeats. As a result, R-STDP outperformed STDP on these datasets. Furthermore, R-STDP is suitable for online learning, and can adapt to drastic changes such as label permutations. Finally, it is worth mentioning that both feature extraction and classification were done with spikes, using at most one spike per neuron. Thus the network is hardware friendly and energy efficient. version:1
arxiv-1705-09107 | SLAM based Quasi Dense Reconstruction For Minimally Invasive Surgery Scenes | http://arxiv.org/abs/1705.09107 | id:1705.09107 author:Nader Mahmoud, Alexandre Hostettler, Toby Collins, Luc Soler, Christophe Doignon, J. M. M. Montiel category:cs.CV  published:2017-05-25 summary:Recovering surgical scene structure in laparoscope surgery is crucial step for surgical guidance and augmented reality applications. In this paper, a quasi dense reconstruction algorithm of surgical scene is proposed. This is based on a state-of-the-art SLAM system, and is exploiting the initial exploration phase that is typically performed by the surgeon at the beginning of the surgery. We show how to convert the sparse SLAM map to a quasi dense scene reconstruction, using pairs of keyframe images and correlation-based featureless patch matching. We have validated the approach with a live porcine experiment using Computed Tomography as ground truth, yielding a Root Mean Squared Error of 4.9mm. version:1
arxiv-1704-08772 | Deep Face Deblurring | http://arxiv.org/abs/1704.08772 | id:1704.08772 author:Grigorios G. Chrysos, Stefanos Zafeiriou category:cs.CV cs.AI cs.LG  published:2017-04-27 summary:Blind deblurring consists a long studied task, however the outcomes of generic methods are not effective in real world blurred images. Domain-specific methods for deblurring targeted object categories, e.g. text or faces, frequently outperform their generic counterparts, hence they are attracting an increasing amount of attention. In this work, we develop such a domain-specific method to tackle deblurring of human faces, henceforth referred to as face deblurring. Studying faces is of tremendous significance in computer vision, however face deblurring has yet to demonstrate some convincing results. This can be partly attributed to the combination of i) poor texture and ii) highly structure shape that yield the contour/gradient priors (that are typically used) sub-optimal. In our work instead of making assumptions over the prior, we adopt a learning approach by inserting weak supervision that exploits the well-documented structure of the face. Namely, we utilise a deep network to perform the deblurring and employ a face alignment technique to pre-process each face. We additionally surpass the requirement of the deep network for thousands training samples, by introducing an efficient framework that allows the generation of a large dataset. We utilised this framework to create 2MF2, a dataset of over two million frames. We conducted experiments with real world blurred facial images and report that our method returns a result close to the sharp natural latent image. version:2
arxiv-1705-04379 | The Network Nullspace Property for Compressed Sensing of Big Data over Networks | http://arxiv.org/abs/1705.04379 | id:1705.04379 author:Alexander Jung category:stat.ML cs.LG  published:2017-05-11 summary:We adapt the nullspace property of compressed sensing for sparse vectors to semi-supervised learning of labels for network-structured datasets. In particular, we derive a sufficient condition, which we term the network nullspace property, for convex optimization methods to accurately learn labels which form smooth graph signals. The network nullspace property involves both the network topology and the sampling strategy and can be used to guide the design of efficient sampling strategies, i.e., the selection of those data points whose labels provide the most information for the learning task. version:2
arxiv-1705-09064 | MagNet: a Two-Pronged Defense against Adversarial Examples | http://arxiv.org/abs/1705.09064 | id:1705.09064 author:Dongyu Meng, Hao Chen category:cs.CR cs.LG  published:2017-05-25 summary:Deep learning has shown promising results on hard perceptual problems in recent years. However, deep learning systems are found to be vulnerable to small adversarial perturbations that are nearly imperceptible to human. Such specially crafted perturbations cause deep learning systems to output incorrect decisions, with potentially disastrous consequences. These vulnerabilities hinder the deployment of deep learning systems where safety or security is important. Attempts to secure deep learning systems either target specific attacks or have been shown to be ineffective. In this paper, we propose MagNet, a framework for defending neural network classifiers against adversarial examples. MagNet does not modify the protected classifier or know the process for generating adversarial examples. MagNet includes one or more separate detector networks and a reformer network. Different from previous work, MagNet learns to differentiate between normal and adversarial examples by approximating the manifold of normal examples. Since it does not rely on any process for generating adversarial examples, it has substantial generalization power. Moreover, MagNet reconstructs adversarial examples by moving them towards the manifold, which is effective for helping classify adversarial examples with small perturbation correctly. We discuss the intrinsic difficulty in defending against whitebox attack and propose a mechanism to defend against graybox attack. Inspired by the use of randomness in cryptography, we propose to use diversity to strengthen MagNet. We show empirically that MagNet is effective against most advanced state-of-the-art attacks in blackbox and graybox scenarios while keeping false positive rate on normal examples very low. version:1
arxiv-1705-09055 | The cost of fairness in classification | http://arxiv.org/abs/1705.09055 | id:1705.09055 author:Aditya Krishna Menon, Robert C. Williamson category:cs.LG  published:2017-05-25 summary:We study the problem of learning classifiers with a fairness constraint, with three main contributions towards the goal of quantifying the problem's inherent tradeoffs. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for cost-sensitive classification and fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we show how the tradeoff between accuracy and fairness is determined by the alignment between the class-probabilities for the target and sensitive features. Underpinning our analysis is a general framework that casts the problem of learning with a fairness requirement as one of minimising the difference of two statistical risks. version:1
arxiv-1705-09054 | Max-Cosine Matching Based Neural Models for Recognizing Textual Entailment | http://arxiv.org/abs/1705.09054 | id:1705.09054 author:Zhipeng Xie, Junfeng Hu category:cs.CL  published:2017-05-25 summary:Recognizing textual entailment is a fundamental task in a variety of text mining or natural language processing applications. This paper proposes a simple neural model for RTE problem. It first matches each word in the hypothesis with its most-similar word in the premise, producing an augmented representation of the hypothesis conditioned on the premise as a sequence of word pairs. The LSTM model is then used to model this augmented sequence, and the final output from the LSTM is fed into a softmax layer to make the prediction. Besides the base model, in order to enhance its performance, we also proposed three techniques: the integration of multiple word-embedding library, bi-way integration, and ensemble based on model averaging. Experimental results on the SNLI dataset have shown that the three techniques are effective in boosting the predicative accuracy and that our method outperforms several state-of-the-state ones. version:1
arxiv-1705-09052 | Weakly Supervised Semantic Segmentation Based on Co-segmentation | http://arxiv.org/abs/1705.09052 | id:1705.09052 author:Tong Shen, Guosheng Lin, Lingqiao Liu, Chunhua Shen, Ian Reid category:cs.CV  published:2017-05-25 summary:Training a Fully Convolutional Network (FCN) for semantic segmentation requires a large number of pixel-level masks, which involves a large amount of human labour and time for annotation. In contrast, image-level labels are much easier to obtain. In this work, we propose a novel method for weakly supervised semantic segmentation with only image-level labels. The method relies on a large scale co-segmentation framework that can produce object masks for a group of images containing objects belonging to the same semantic class. We first retrieve images from search engines, e.g. Flickr and Google, using semantic class names as queries, e.g. class names in PASCAL VOC 2012. We then use high quality masks produced by co-segmentation on the retrieved images as well as the target dataset images with image level labels to train segmentation networks. We obtain IoU 56.9 on test set of PASCAL VOC 2012, which reaches state of the art performance. version:1
arxiv-1705-09050 | A Clustering-based Consistency Adaptation Strategy for Distributed SDN Controllers | http://arxiv.org/abs/1705.09050 | id:1705.09050 author:Mohamed Aslan, Ashraf Matrawy category:cs.NI cs.LG  published:2017-05-25 summary:Distributed controllers are oftentimes used in large-scale SDN deployments where they run a myriad of network applications simultaneously. Such applications could have different consistency and availability preferences. These controllers need to communicate via east/west interfaces in order to synchronize their state information. The consistency and the availability of the distributed state information are governed by an underlying consistency model. Earlier, we suggested the use of adaptively-consistent controllers that can autonomously tune their consistency parameters in order to meet the performance requirements of a certain application. In this paper, we examine the feasibility of employing adaptive controllers that are built on-top of tunable consistency models similar to that of Apache Cassandra. We present an adaptation strategy that uses clustering techniques (sequential k-means and incremental k-means) in order to map a given application performance indicator into a feasible consistency level that can be used with the underlying tunable consistency model. In the cases that we modeled and tested, our results show that in the case of sequential k-means, with a reasonable number of clusters (>= 50), a plausible mapping (low RMSE) could be estimated between the application performance indicators and the consistency level indicator. In the case of incremental k-means, the results also showed that a plausible mapping (low RMSE) could be estimated using a similar number of clusters (>= 50) by using a small threshold (~$ 0.01). version:1
arxiv-1705-09048 | Convergence of Langevin MCMC in KL-divergence | http://arxiv.org/abs/1705.09048 | id:1705.09048 author:Xiang Cheng, Peter Bartlett category:stat.ML  published:2017-05-25 summary:Langevin diffusion is a commonly used tool for sampling from a given distribution. In this work, we establish that when the target density $p^*$ is such that $\log p^*$ is $L$ smooth and $m$ strongly convex, discrete Langevin diffusion produces a distribution $p$ with $KL(p p^*)\leq \epsilon$ in $\tilde{O}(\frac{d}{\epsilon})$ steps, where $d$ is the dimension of the sample space. We also study the convergence rate when the strong-convexity assumption is absent. By considering the Langevin diffusion as a gradient flow in the space of probability distributions, we obtain an elegant analysis that applies to the stronger property of convergence in KL-divergence and gives a conceptually simpler proof of the best-known convergence results in weaker metrics. version:1
arxiv-1705-07136 | Softmax Q-Distribution Estimation for Structured Prediction: A Theoretical Interpretation for RAML | http://arxiv.org/abs/1705.07136 | id:1705.07136 author:Xuezhe Ma, Pengcheng Yin, Jingzhou Liu, Graham Neubig, Eduard Hovy category:cs.LG cs.CL stat.ML  published:2017-05-19 summary:Reward augmented maximum likelihood (RAML) is a simple and effective learning framework to directly optimize towards the reward function in structured prediction tasks. RAML incorporates task-specific reward by performing maximum-likelihood updates on candidate outputs sampled according to an exponentiated payoff distribution, which gives higher probabilities to candidates that are close to the reference output. While RAML is notable for its simplicity, efficiency, and its impressive empirical successes, the theoretical properties of RAML, especially the behavior of the exponentiated payoff distribution, has not been examined thoroughly. In this work, we introduce softmax Q-distribution estimation, a novel theoretical interpretation of RAML, which reveals the relation between RAML and Bayesian decision theory. The softmax Q-distribution can be regarded as a smooth approximation of Bayes decision boundary, and the Bayes decision rule is achieved by decoding with this Q-distribution. We further show that RAML is equivalent to approximately estimating the softmax Q-distribution. Experiments on three structured prediction tasks with rewards defined on sequential (named entity recognition), tree-based (dependency parsing) and irregular (machine translation) structures show notable improvements over maximum likelihood baselines. version:2
arxiv-1705-09037 | Deriving Neural Architectures from Sequence and Graph Kernels | http://arxiv.org/abs/1705.09037 | id:1705.09037 author:Tao Lei, Wengong Jin, Regina Barzilay, Tommi Jaakkola category:cs.NE cs.CL cs.LG  published:2017-05-25 summary:The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art or competitive results across these applications. We also draw connections to existing architectures such as LSTMs. version:1
arxiv-1705-09036 | Lat-Net: Compressing Lattice Boltzmann Flow Simulations using Deep Neural Networks | http://arxiv.org/abs/1705.09036 | id:1705.09036 author:Oliver Hennigh category:stat.ML physics.comp-ph  published:2017-05-25 summary:Computational Fluid Dynamics (CFD) is a hugely important subject with applications in almost every engineering field, however, fluid simulations are extremely computationally and memory demanding. Towards this end, we present Lat-Net, a method for compressing both the computation time and memory usage of Lattice Boltzmann flow simulations using deep neural networks. Lat-Net employs convolutional autoencoders and residual connections in a fully differentiable scheme to compress the state size of a simulation and learn the dynamics on this compressed form. The result is a computationally and memory efficient neural network that can be iterated and queried to reproduce a fluid simulation. We show that once Lat-Net is trained, it can generalize to large grid sizes and complex geometries while maintaining accuracy. We also show that Lat-Net is a general method for compressing other Lattice Boltzmann based simulations such as Electromagnetism. version:1
arxiv-1705-07368 | Mixed Membership Word Embeddings for Computational Social Science | http://arxiv.org/abs/1705.07368 | id:1705.07368 author:James Foulds category:cs.CL cs.AI cs.LG  published:2017-05-20 summary:Word embeddings improve the performance of NLP systems by revealing the hidden structural relationships between words. These models have recently risen in popularity due to the performance of scalable algorithms trained in the big data setting. Despite their success, word embeddings have seen very little use in computational social science NLP tasks, presumably due to their reliance on big data, and to a lack of interpretability. I propose a probabilistic model-based word embedding method which can recover interpretable embeddings, without big data. The key insight is to leverage the notion of mixed membership modeling, in which global representations are shared, but individual entities (i.e. dictionary words) are free to use these representations to uniquely differing degrees. Leveraging connections to topic models, I show how to train these models in high dimensions using a combination of state-of-the-art techniques for word embeddings and topic modeling. Experimental results show an improvement in predictive performance of up to 63% in MRR over the skip-gram on small datasets. The models are interpretable, as embeddings of topics are used to encode embeddings for words (and hence, documents) in a model-based way. I illustrate this with two computational social science case studies, on NIPS articles and State of the Union addresses. version:2
arxiv-1705-09031 | Fast Causal Inference with Non-Random Missingness by Test-Wise Deletion | http://arxiv.org/abs/1705.09031 | id:1705.09031 author:Eric V. Strobl, Shyam Visweswaran, Peter L. Spirtes category:stat.ME stat.ML  published:2017-05-25 summary:Many real datasets contain values missing not at random (MNAR). In this scenario, investigators often perform list-wise deletion, or delete samples with any missing values, before applying causal discovery algorithms. List-wise deletion is a sound and general strategy when paired with algorithms such as FCI and RFCI, but the deletion procedure also eliminates otherwise good samples that contain only a few missing values. In this report, we show that we can more efficiently utilize the observed values with test-wise deletion while still maintaining algorithmic soundness. Here, test-wise deletion refers to the process of list-wise deleting samples only among the variables required for each conditional independence (CI) test used in constraint-based searches. Test-wise deletion therefore often saves more samples than list-wise deletion for each CI test, especially when we have a sparse underlying graph. Our theoretical results show that test-wise deletion is sound under the justifiable assumption that none of the missingness mechanisms causally affect each other in the underlying causal graph. We also find that FCI and RFCI with test-wise deletion outperform their list-wise deletion and imputation counterparts on average when MNAR holds in both synthetic and real data. version:1
arxiv-1705-09026 | Online Edge Grafting for Efficient MRF Structure Learning | http://arxiv.org/abs/1705.09026 | id:1705.09026 author:Walid Chaabene, Bert Huang category:cs.LG cs.AI stat.ML  published:2017-05-25 summary:Incremental methods for structure learning of pairwise Markov random fields (MRFs), such as grafting, improve scalability to large systems by avoiding inference over the entire feature space in each optimization step. Instead, inference is performed over an incrementally grown active set of features. In this paper, we address the computational bottlenecks that current techniques still suffer by introducing online edge grafting, an incremental, structured method that activates edges as groups of features in a streaming setting. The framework is based on reservoir sampling of edges that satisfy a necessary activation condition, approximating the search for the optimal edge to activate. Online edge grafting performs an informed edge search set reorganization using search history and structure heuristics. Experiments show a significant computational speedup for structure learning and a controllable trade-off between the speed and the quality of learning. version:1
arxiv-1705-09021 | Learning to Pour | http://arxiv.org/abs/1705.09021 | id:1705.09021 author:Yongqiang Huang, Yu Sun category:cs.RO cs.LG  published:2017-05-25 summary:Pouring is a simple task people perform daily. It is the second most frequently executed motion in cooking scenarios, after pick-and-place. We present a pouring trajectory generation approach, which uses force feedback from the cup to determine the future velocity of pouring. The approach uses recurrent neural networks as its building blocks. We collected the pouring demonstrations which we used for training. To test our approach in simulation, we also created and trained a force estimation system. The simulated experiments show that the system is able to generalize to single unseen element of the pouring characteristics. version:1
arxiv-1705-09011 | Principled Hybrids of Generative and Discriminative Domain Adaptation | http://arxiv.org/abs/1705.09011 | id:1705.09011 author:Han Zhao, Zhenyao Zhu, Junjie Hu, Adam Coates, Geoff Gordon category:cs.LG cs.AI  published:2017-05-25 summary:We propose a probabilistic framework for domain adaptation that blends both generative and discriminative modeling in a principled way. By maximizing both the marginal and the conditional log-likelihoods, models derived from this framework can use both labeled instances from the source domain as well as unlabeled instances from both source and target domains. Under this framework, we show that the popular reconstruction loss of autoencoder corresponds to an upper bound of the negative marginal log-likelihoods of unlabeled instances, where marginal distributions are given by proper kernel density estimations. This provides a way to interpret the empirical success of autoencoders in domain adaptation and semi-supervised learning. We instantiate our framework using neural networks, and build a concrete model, DAuto. Empirically, we demonstrate the effectiveness of DAuto on text, image and speech datasets, showing that it outperforms related competitors when domain adaptation is possible. version:1
arxiv-1705-07215 | How to Train Your DRAGAN | http://arxiv.org/abs/1705.07215 | id:1705.07215 author:Naveen Kodali, Jacob Abernethy, James Hays, Zsolt Kira category:cs.AI cs.CV cs.GT cs.LG cs.NE  published:2017-05-19 summary:Generative Adversarial Networks have emerged as an effective technique for estimating data distributions. The basic setup consists of two deep networks playing against each other in a zero-sum game setting. However, it is not understood if the networks reach an equilibrium eventually and what dynamics makes this possible. The current GAN training procedure, which involves simultaneous gradient descent, lacks a clear game-theoretic justification in the literature. In this paper, we introduce regret minimization as a technique to reach equilibrium in games and use this to justify the success of simultaneous GD in GANs. In addition, we present a hypothesis that mode collapse, which is a common occurrence in GAN training, happens due to the existence of spurious local equilibria in non-convex games. Motivated by these insights, we develop an algorithm called DRAGAN that is fast, simple to implement and achieves competitive performance in a stable fashion across different architectures (150 random setups), datasets (MNIST, CIFAR-10, and CelebA), and divergence measures with almost no hyperparameter tuning. We show significant improvements over the recently proposed Wasserstein GAN variants. version:3
arxiv-1705-09003 | Extraction and Classification of Diving Clips from Continuous Video Footage | http://arxiv.org/abs/1705.09003 | id:1705.09003 author:Aiden Nibali, Zhen He, Stuart Morgan, Daniel Greenwood category:cs.CV  published:2017-05-25 summary:Due to recent advances in technology, the recording and analysis of video data has become an increasingly common component of athlete training programmes. Today it is incredibly easy and affordable to set up a fixed camera and record athletes in a wide range of sports, such as diving, gymnastics, golf, tennis, etc. However, the manual analysis of the obtained footage is a time-consuming task which involves isolating actions of interest and categorizing them using domain-specific knowledge. In order to automate this kind of task, three challenging sub-problems are often encountered: 1) temporally cropping events/actions of interest from continuous video; 2) tracking the object of interest; and 3) classifying the events/actions of interest. Most previous work has focused on solving just one of the above sub-problems in isolation. In contrast, this paper provides a complete solution to the overall action monitoring task in the context of a challenging real-world exemplar. Specifically, we address the problem of diving classification. This is a challenging problem since the person (diver) of interest typically occupies fewer than 1% of the pixels in each frame. The model is required to learn the temporal boundaries of a dive, even though other divers and bystanders may be in view. Finally, the model must be sensitive to subtle changes in body pose over a large number of frames to determine the classification code. We provide effective solutions to each of the sub-problems which combine to provide a highly functional solution to the task as a whole. The techniques proposed can be easily generalized to video footage recorded from other sports. version:1
arxiv-1705-08997 | State Space Decomposition and Subgoal Creation for Transfer in Deep Reinforcement Learning | http://arxiv.org/abs/1705.08997 | id:1705.08997 author:Himanshu Sahni, Saurabh Kumar, Farhan Tejani, Yannick Schroecker, Charles Isbell category:cs.AI cs.LG stat.ML  published:2017-05-24 summary:Typical reinforcement learning (RL) agents learn to complete tasks specified by reward functions tailored to their domain. As such, the policies they learn do not generalize even to similar domains. To address this issue, we develop a framework through which a deep RL agent learns to generalize policies from smaller, simpler domains to more complex ones using a recurrent attention mechanism. The task is presented to the agent as an image and an instruction specifying the goal. This meta-controller guides the agent towards its goal by designing a sequence of smaller subtasks on the part of the state space within the attention, effectively decomposing it. As a baseline, we consider a setup without attention as well. Our experiments show that the meta-controller learns to create subgoals within the attention. version:1
arxiv-1705-08992 | Matroids Hitting Sets and Unsupervised Dependency Grammar Induction | http://arxiv.org/abs/1705.08992 | id:1705.08992 author:Nicholas Harvey, David Karger, Virginia Savova, Leonid Peshkin category:cs.DM cs.CL cs.DS  published:2017-05-24 summary:This paper formulates a novel problem on graphs: find the minimal subset of edges in a fully connected graph, such that the resulting graph contains all spanning trees for a set of specifed sub-graphs. This formulation is motivated by an un-supervised grammar induction problem from computational linguistics. We present a reduction to some known problems and algorithms from graph theory, provide computational complexity results, and describe an approximation algorithm. version:1
arxiv-1705-08991 | Approximation and Convergence Properties of Generative Adversarial Learning | http://arxiv.org/abs/1705.08991 | id:1705.08991 author:Shuang Liu, Olivier Bousquet, Kamalika Chaudhuri category:cs.LG stat.ML  published:2017-05-24 summary:Generative adversarial networks (GAN) approximate a target data distribution by jointly optimizing an objective function through a "two-player game" between a generator and a discriminator. Despite their empirical success, however, two very basic questions on how well they can approximate the target distribution remain unanswered. First, it is not known how restricting the discriminator family affects the approximation quality. Second, while a number of different objective functions have been proposed, we do not understand when convergence to the global minima of the objective function leads to convergence to the target distribution under various notions of distributional convergence. In this paper, we address these questions in a broad and unified setting by defining a notion of adversarial divergences that includes a number of recently proposed objective functions. We show that if the objective function is an adversarial divergence with some additional conditions, then using a restricted discriminator family has a moment-matching effect. Additionally, we show that for objective functions that are strict adversarial divergences, convergence in the objective function implies weak convergence, thus generalizing previous results. version:1
arxiv-1705-08983 | Plug-and-Play Unplugged: Optimization Free Reconstruction using Consensus Equilibrium | http://arxiv.org/abs/1705.08983 | id:1705.08983 author:Gregery T. Buzzard, Suhas Sreehari, Charles A. Bouman category:cs.CV math.OC 94A08  68U10  published:2017-05-24 summary:Regularized inversion methods for image reconstruction are used widely due to their tractability and their ability to combine complex physical sensor models with useful regularity criteria. Such methods were used in the recently developed Plug-and-Play prior method, which provides a framework to use advanced denoising algorithms as regularizers in inversion. However, the need to formulate regularized inversion as the solution to an optimization problem severely limits both the expressiveness of possible regularity conditions and the variety of provably convergent Plug-and-Play denoising operators. In this paper, we introduce the concept of consensus equilibrium (CE), which generalizes regularized inversion to include a much wider variety of regularity operators without the need for an optimization formulation. Consensus equilibrium is based on the solution of a set of equilibrium equations that balance data fit and regularity. In this framework, the problem of MAP estimation in regularized inversion is replaced by the problem of solving these equilibrium equations, which can be approached in multiple ways, including as a fixed point problem that generalizes the ADMM approach used in the Plug-and-Play method. We present the Douglas-Rachford (DR) algorithm for computing the CE solution as a fixed point and prove the convergence of this algorithm under conditions that include denoising operators that do not arise from optimization problems and that may not be nonexpansive. We give several examples to illustrate the idea of consensus equilibrium and the convergence properties of the DR algorithm and demonstrate this method on a sparse interpolation problem using electron microscopy data. version:1
arxiv-1705-08982 | Modeling The Intensity Function Of Point Process Via Recurrent Neural Networks | http://arxiv.org/abs/1705.08982 | id:1705.08982 author:Shuai Xiao, Junchi Yan, Stephen M. Chu, Xiaokang Yang, Hongyuan Zha category:cs.LG cs.AI stat.ML  published:2017-05-24 summary:Event sequence, asynchronously generated with random timestamp, is ubiquitous among applications. The precise and arbitrary timestamp can carry important clues about the underlying dynamics, and has lent the event data fundamentally different from the time-series whereby series is indexed with fixed and equal time interval. One expressive mathematical tool for modeling event is point process. The intensity functions of many point processes involve two components: the background and the effect by the history. Due to its inherent spontaneousness, the background can be treated as a time series while the other need to handle the history events. In this paper, we model the background by a Recurrent Neural Network (RNN) with its units aligned with time series indexes while the history effect is modeled by another RNN whose units are aligned with asynchronous events to capture the long-range dynamics. The whole model with event type and timestamp prediction output layers can be trained end-to-end. Our approach takes an RNN perspective to point process, and models its background and history effect. For utility, our method allows a black-box treatment for modeling the intensity which is often a pre-defined parametric form in point processes. Meanwhile end-to-end training opens the venue for reusing existing rich techniques in deep network for point process modeling. We apply our model to the predictive maintenance problem using a log dataset by more than 1000 ATMs from a global bank headquartered in North America. version:1
arxiv-1705-08974 | Cultural Diffusion and Trends in Facebook Photographs | http://arxiv.org/abs/1705.08974 | id:1705.08974 author:Quanzeng You, Darío García-García, Mahohar Paluri, Jiebo Luo, Jungseock Joo category:cs.CV cs.SI  published:2017-05-24 summary:Online social media is a social vehicle in which people share various moments of their lives with their friends, such as playing sports, cooking dinner or just taking a selfie for fun, via visual means, that is, photographs. Our study takes a closer look at the popular visual concepts illustrating various cultural lifestyles from aggregated, de-identified photographs. We perform analysis both at macroscopic and microscopic levels, to gain novel insights about global and local visual trends as well as the dynamics of interpersonal cultural exchange and diffusion among Facebook friends. We processed images by automatically classifying the visual content by a convolutional neural network (CNN). Through various statistical tests, we find that socially tied individuals more likely post images showing similar cultural lifestyles. To further identify the main cause of the observed social correlation, we use the Shuffle test and the Preference-based Matched Estimation (PME) test to distinguish the effects of influence and homophily. The results indicate that the visual content of each user's photographs are temporally, although not necessarily causally, correlated with the photographs of their friends, which may suggest the effect of influence. Our paper demonstrates that Facebook photographs exhibit diverse cultural lifestyles and preferences and that the social interaction mediated through the visual channel in social media can be an effective mechanism for cultural diffusion. version:1
arxiv-1705-08971 | Optimal Cooperative Inference | http://arxiv.org/abs/1705.08971 | id:1705.08971 author:Scott Cheng-Hsin Yang, Yue Yu, Arash Givchi, Pei Wang, Wai Keen Vong, Patrick Shafto category:cs.LG  published:2017-05-24 summary:Cooperative transmission of data fosters rapid accumulation of knowledge by efficiently combining experience across learners. Although well studied in human learning, there has been less attention to cooperative transmission of data in machine learning, and we consequently lack strong formal frameworks through which we may reason about the benefits and limitations of cooperative inference. We present such a framework. We introduce a novel index for measuring the effectiveness of probabilistic information transmission, and cooperative information transmission specifically. We relate our cooperative index to previous measures of teaching in deterministic settings. We prove conditions under which optimal cooperative inference can be achieved, including a representation theorem which constrains the form of inductive biases for learners optimized for cooperative inference. We conclude by demonstrating how these principles may inform the design of machine learning algorithms and discuss implications for human learning, machine learning, and human-machine learning systems. version:1
arxiv-1705-08948 | New Results for Provable Dynamic Robust PCA | http://arxiv.org/abs/1705.08948 | id:1705.08948 author:Praneeth Narayanamurthy, Namrata Vaswani category:cs.IT math.IT stat.ML  published:2017-05-24 summary:Robust PCA (RPCA) is the problem of separating a given data matrix into the sum of a sparse matrix and a low-rank matrix. Dynamic RPCA assumes that the true data vectors lie in a low-dimensional subspace that an change with time, albeit slowly. The goal is to track this changing subspace over time in the presence of sparse outliers. This work provides the first guarantee for dynamic RPCA that holds under (weakened) standard RPCA assumptions and a realistic model of slow subspace change. We analyze an existing method called ReProCS. Our result removes the strong assumptions needed by the two previous complete guarantees for ReProCS. Both these required an unrealistic model of subspace change and very specific assumptions on how the outlier support could change. Most importantly, our guarantees show that, because it exploits slow subspace change, ReProCS (and its offline counterpart) can provably tolerate much larger outlier fractions, are faster than most other provable methods, and have near-optimal storage complexity. version:1
arxiv-1705-08947 | Deep Voice 2: Multi-Speaker Neural Text-to-Speech | http://arxiv.org/abs/1705.08947 | id:1705.08947 author:Sercan Arik, Gregory Diamos, Andrew Gibiansky, John Miller, Kainan Peng, Wei Ping, Jonathan Raiman, Yanqi Zhou category:cs.CL  published:2017-05-24 summary:We introduce a technique for augmenting neural text-to-speech (TTS) with lowdimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly. version:1
arxiv-1705-08943 | Novel Deep Convolution Neural Network Applied to MRI Cardiac Segmentation | http://arxiv.org/abs/1705.08943 | id:1705.08943 author:Clement Zotti, Zhiming Luo, Alain Lalande, Olivier Humbert, Pierre-Marc Jodoin category:cs.CV  published:2017-05-24 summary:In this paper, we propose a fully automatic MRI cardiac segmentation method based on a novel deep convolutional neural network (CNN). As opposed to most cardiac segmentation methods which focus on the left ventricle (and especially the left cavity), our method segments both the left ventricular cavity, the left ventricular epicardium, and the right ventricular cavity. The novelty of our network lies in its maximum a posteriori loss function, which is specifically designed for the cardiac anatomy. Our loss function incorporates the cross-entropy of the predicted labels, the predicted contours, a cardiac shape prior, and an a priori term. Our model also includes a cardiac center-of-mass regression module which allows for an automatic shape prior registration. Also, since our method processes raw MR images without any manual preprocessing and/or image cropping, our CNN learns both high-level features (useful to distinguish the heart from other organs with a similar shape) and low-level features (useful to get accurate segmentation results). Those features are learned with a multi-resolution conv-deconv "grid" architecture which can be seen as an extension of the U-Net. We trained and tested our model on the ACDC MICCAI'17 challenge dataset of 150 patients whose diastolic and systolic images were manually outlined by 2 medical experts. Results reveal that our method can segment all three regions of a 3D MRI cardiac volume in $0.4$ second with an average Dice index of $0.90$, which is significantly better than state-of-the-art deep learning methods. version:1
arxiv-1705-08942 | Joint PoS Tagging and Stemming for Agglutinative Languages | http://arxiv.org/abs/1705.08942 | id:1705.08942 author:Necva Bölücü, Burcu Can category:cs.CL 68T50  published:2017-05-24 summary:The number of word forms in agglutinative languages is theoretically infinite and this variety in word forms introduces sparsity in many natural language processing tasks. Part-of-speech tagging (PoS tagging) is one of these tasks that often suffers from sparsity. In this paper, we present an unsupervised Bayesian model using Hidden Markov Models (HMMs) for joint PoS tagging and stemming for agglutinative languages. We use stemming to reduce sparsity in PoS tagging. Two tasks are jointly performed to provide a mutual benefit in both tasks. Our results show that joint POS tagging and stemming improves PoS tagging scores. We present results for Turkish and Finnish as agglutinative languages and English as a morphologically poor language. version:1
arxiv-1705-08933 | Doubly Stochastic Variational Inference for Deep Gaussian Processes | http://arxiv.org/abs/1705.08933 | id:1705.08933 author:Hugh Salimbeni, Marc Deisenroth category:stat.ML  published:2017-05-24 summary:Gaussian processes (GPs) are a good choice for function approximation as they are flexible, robust to over-fitting, and provide well-calibrated predictive uncertainty. Deep Gaussian processes (DGPs) are multi-layer generalisations of GPs, but inference in these models has proved challenging. Existing approaches to inference in DGP models assume approximate posteriors that force independence between the layers, and do not work well in practice. We present a doubly stochastic variational inference algorithm, which does not force independence between layers. With our method of inference we demonstrate that a DGP model can be used effectively on data ranging in size from hundreds to a billion points. We provide strong empirical evidence that our inference scheme for DGPs works well in practice in both classification and regression. version:1
arxiv-1705-08264 | Isomorphism between Differential and Moment Invariants under Affine Transform | http://arxiv.org/abs/1705.08264 | id:1705.08264 author:Erbo Li, Hua Li category:cs.CV  published:2017-05-20 summary:The invariant is one of central topics in science, technology and engineering. The differential invariant is essential in understanding or describing some important phenomena or procedures in mathematics, physics, chemistry, biology or computer science etc. The derivation of differential invariants is usually difficult or complicated. This paper reports a discovery that under the affine transform, differential invariants have similar structures with moment invariants up to a scalar function of transform parameters. If moment invariants are known, relative differential invariants can be obtained by the substitution of moments by derivatives with the same order. Whereas moment invariants can be calculated by multiple integrals, this method provides a simple way to derive differential invariants without the need to resolve any equation system. Since the definition of moments on different manifolds or in different dimension of spaces is well established, differential invariants on or in them will also be well defined. Considering that moments have a strong background in mathematics and physics, this technique offers a new view angle to the inner structure of invariants. Projective differential invariants can also be found in this way with a screening process. version:2
arxiv-1705-08931 | Proximity Variational Inference | http://arxiv.org/abs/1705.08931 | id:1705.08931 author:Jaan Altosaar, Rajesh Ranganath, David M. Blei category:stat.ML cs.LG stat.CO 68T10 G.3; I.5.0; I.5.1  published:2017-05-24 summary:Variational inference is a powerful approach for approximate posterior inference. However, it is sensitive to initialization and can be subject to poor local optima. In this paper, we develop proximity variational inference (PVI). PVI is a new method for optimizing the variational objective that constrains subsequent iterates of the variational parameters to robustify the optimization path. Consequently, PVI is less sensitive to initialization and optimization quirks and finds better local optima. We demonstrate our method on three proximity statistics. We study PVI on a Bernoulli factor model and sigmoid belief network with both real and synthetic data and compare to deterministic annealing (Katahira et al., 2008). We highlight the flexibility of PVI by designing a proximity statistic for Bayesian deep learning models such as the variational autoencoder (Kingma and Welling, 2014; Rezende et al., 2014). Empirically, we show that PVI consistently finds better local optima and gives better predictive performance. version:1
arxiv-1705-08923 | Attention-based Natural Language Person Retrieval | http://arxiv.org/abs/1705.08923 | id:1705.08923 author:Tao Zhou, Muhao Chen, Jie Yu, Demetri Terzopoulos category:cs.CV  published:2017-05-24 summary:Following the recent progress in image classification and captioning using deep learning, we develop a novel natural language person retrieval system based on an attention mechanism. More specifically, given the description of a person, the goal is to localize the person in an image. To this end, we first construct a benchmark dataset for natural language person retrieval. To do so, we generate bounding boxes for persons in a public image dataset from the segmentation masks, which are then annotated with descriptions and attributes using the Amazon Mechanical Turk. We then adopt a region proposal network in Faster R-CNN as a candidate region generator. The cropped images based on the region proposals as well as the whole images with attention weights are fed into Convolutional Neural Networks for visual feature extraction, while the natural language expression and attributes are input to Bidirectional Long Short- Term Memory (BLSTM) models for text feature extraction. The visual and text features are integrated to score region proposals, and the one with the highest score is retrieved as the output of our system. The experimental results show significant improvement over the state-of-the-art method for generic object retrieval and this line of research promises to benefit search in surveillance video footage. version:1
arxiv-1705-08918 | Unsupervised Learning Layers for Video Analysis | http://arxiv.org/abs/1705.08918 | id:1705.08918 author:Liang Zhao, Yang Wang, Yi Yang, Wei Xu category:cs.LG cs.CV stat.ML  published:2017-05-24 summary:This paper presents two unsupervised learning layers (UL layers) for label-free video analysis: one for fully connected layers, and the other for convolutional ones. The proposed UL layers can play two roles: they can be the cost function layer for providing global training signal; meanwhile they can be added to any regular neural network layers for providing local training signals and combined with the training signals backpropagated from upper layers for extracting both slow and fast changing features at layers of different depths. Therefore, the UL layers can be used in either pure unsupervised or semi-supervised settings. Both a closed-form solution and an online learning algorithm for two UL layers are provided. Experiments with unlabeled synthetic and real-world videos demonstrated that the neural networks equipped with UL layers and trained with the proposed online learning algorithm can extract shape and motion information from video sequences of moving objects. The experiments demonstrated the potential applications of UL layers and online learning algorithm to head orientation estimation and moving object localization. version:1
arxiv-1705-08868 | Flow-GAN: Bridging implicit and prescribed learning in generative models | http://arxiv.org/abs/1705.08868 | id:1705.08868 author:Aditya Grover, Manik Dhar, Stefano Ermon category:cs.LG cs.AI cs.NE stat.ML  published:2017-05-24 summary:Evaluating the performance of generative models for unsupervised learning is inherently challenging due to the lack of well-defined and tractable objectives. This is particularly difficult for implicit models such as generative adversarial networks (GANs) which perform extremely well in practice for tasks such as sample generation, but sidestep the explicit characterization of a density. We propose Flow-GANs, a generative adversarial network with the generator specified as a normalizing flow model which can perform exact likelihood evaluation. Subsequently, we learn a Flow-GAN using a hybrid objective that integrates adversarial training with maximum likelihood estimation. We show empirically the benefits of Flow-GANs on MNIST and CIFAR-10 datasets in learning generative models that can attain low generalization error based on the log-likelihoods and generate high quality samples. Finally, we show a simple, yet hard to beat baseline for GANs based on Gaussian Mixture Models. version:1
arxiv-1705-08865 | Anti-spoofing Methods for Automatic SpeakerVerification System | http://arxiv.org/abs/1705.08865 | id:1705.08865 author:Galina Lavrentyeva, Sergey Novoselov, Konstantin Simonchik category:cs.SD cs.LG stat.ML  published:2017-05-24 summary:Growing interest in automatic speaker verification (ASV)systems has lead to significant quality improvement of spoofing attackson them. Many research works confirm that despite the low equal er-ror rate (EER) ASV systems are still vulnerable to spoofing attacks. Inthis work we overview different acoustic feature spaces and classifiersto determine reliable and robust countermeasures against spoofing at-tacks. We compared several spoofing detection systems, presented so far,on the development and evaluation datasets of the Automatic SpeakerVerification Spoofing and Countermeasures (ASVspoof) Challenge 2015.Experimental results presented in this paper demonstrate that the useof magnitude and phase information combination provides a substantialinput into the efficiency of the spoofing detection systems. Also wavelet-based features show impressive results in terms of equal error rate. Inour overview we compare spoofing performance for systems based on dif-ferent classifiers. Comparison results demonstrate that the linear SVMclassifier outperforms the conventional GMM approach. However, manyresearchers inspired by the great success of deep neural networks (DNN)approaches in the automatic speech recognition, applied DNN in thespoofing detection task and obtained quite low EER for known and un-known type of spoofing attacks. version:1
arxiv-1705-08858 | Audio-replay attack detection countermeasures | http://arxiv.org/abs/1705.08858 | id:1705.08858 author:Galina Lavrentyeva, Sergey Novoselov, Egor Malykh, Alexander Kozlov, Oleg Kudashev, Vadim Shchemelinin category:cs.SD cs.LG stat.ML  published:2017-05-24 summary:This paper presents the Speech Technology Center (STC) replay attack detection systems proposed for Automatic Speaker Verification Spoofing and Countermeasures Challenge 2017. In this study we focused on comparison of different spoofing detection approaches. These were GMM based methods, high level features extraction with simple classifier and deep learning frameworks. Experiments performed on the development and evaluation parts of the challenge dataset demonstrated stable efficiency of deep learning approaches in case of changing acoustic conditions. At the same time SVM classifier with high level features provided a substantial input in the efficiency of the resulting STC systems according to the fusion systems results. version:1
arxiv-1705-08850 | Improved Semi-supervised Learning with GANs using Manifold Invariances | http://arxiv.org/abs/1705.08850 | id:1705.08850 author:Abhishek Kumar, Prasanna Sattigeri, P. Thomas Fletcher category:cs.LG cs.AI stat.ML  published:2017-05-24 summary:Semi-supervised learning methods using Generative Adversarial Networks (GANs) have shown promising empirical success recently. Most of these methods use a shared discriminator/classifier which discriminates real examples from fake while also predicting the class label. Motivated by the ability of the GANs generator to capture the data manifold well, we propose to estimate the tangent space to the data manifold using GANs and employ it to inject invariances into the classifier. In the process, we propose enhancements over existing methods for learning the inverse mapping (i.e., the encoder) which greatly improves in terms of semantic similarity of the reconstructed sample with the input sample. We observe considerable empirical gains in semi-supervised learning over baselines, particularly in the cases when the number of labeled examples is low. We also provide insights into how fake examples influence the semi-supervised learning procedure. version:1
arxiv-1705-08848 | Joint Distribution Optimal Transportation for Domain Adaptation | http://arxiv.org/abs/1705.08848 | id:1705.08848 author:Nicolas Courty, Rémi Flamary, Amaury Habrard, Alain Rakotomamonjy category:stat.ML cs.LG  published:2017-05-24 summary:This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function $f$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a non-linear transformation between the joint feature/label space distributions of the two domain $\mathcal{P}_s$ and $\mathcal{P}_t$. We propose a solution of this problem with optimal transport, that allows to recover an estimated target $\mathcal{P}^f_t=(X,f(X))$ by optimizing simultaneously the optimal coupling and $f$. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results. version:1
arxiv-1705-08844 | How a General-Purpose Commonsense Ontology can Improve Performance of Learning-Based Image Retrieval | http://arxiv.org/abs/1705.08844 | id:1705.08844 author:Rodrigo Toro Icarte, Jorge A. Baier, Cristian Ruz, Alvaro Soto category:cs.AI cs.CV cs.IR  published:2017-05-24 summary:The knowledge representation community has built general-purpose ontologies which contain large amounts of commonsense knowledge over relevant aspects of the world, including useful visual information, e.g.: "a ball is used by a football player", "a tennis player is located at a tennis court". Current state-of-the-art approaches for visual recognition do not exploit these rule-based knowledge sources. Instead, they learn recognition models directly from training examples. In this paper, we study how general-purpose ontologies---specifically, MIT's ConceptNet ontology---can improve the performance of state-of-the-art vision systems. As a testbed, we tackle the problem of sentence-based image retrieval. Our retrieval approach incorporates knowledge from ConceptNet on top of a large pool of object detectors derived from a deep learning technique. In our experiments, we show that ConceptNet can improve performance on a common benchmark dataset. Key to our performance is the use of the ESPGAME dataset to select visually relevant relations from ConceptNet. Consequently, a main conclusion of this work is that general-purpose commonsense ontologies improve performance on visual reasoning tasks when properly filtered to select meaningful visual relations. version:1
arxiv-1705-08843 | Parsing with CYK over Distributed Representations: "Classical" Syntactic Parsing in the Novel Era of Neural Networks | http://arxiv.org/abs/1705.08843 | id:1705.08843 author:Fabio Massimo Zanzotto, Giordano Cristini category:cs.CL I.2.7; I.2.6  published:2017-05-24 summary:Syntactic parsing is a key task in natural language processing which has been dominated by symbolic, grammar-based syntactic parsers. Neural networks, with their distributed representations, are challenging these methods. In this paper, we want to show that existing parsing algorithms can cross the border and be defined over distributed representations. We then define D-CYK: a version of the traditional CYK algorithm defined over distributed representations. Our D-CYK operates as the original CYK but uses matrix multiplications. These operations are compatible with traditional neural networks. Experiments show that D-CYK approximates the original CYK. By showing that CYK can be performed on distributed representations, our D-CYK opens the possibility of defining recurrent layers of CYK-informed neural networks. version:1
arxiv-1705-08841 | Multi-Level Variational Autoencoder: Learning Disentangled Representations from Grouped Observations | http://arxiv.org/abs/1705.08841 | id:1705.08841 author:Diane Bouchacourt, Ryota Tomioka, Sebastian Nowozin category:cs.LG stat.ML  published:2017-05-24 summary:We would like to learn a representation of the data which decomposes an observation into factors of variation which we can independently control. Specifically, we want to use minimal supervision to learn a latent representation that reflects the semantics behind a specific grouping of the data, where within a group the samples share a common factor of variation. For example, consider a collection of face images grouped by identity. We wish to anchor the semantics of the grouping into a relevant and disentangled representation that we can easily exploit. However, existing deep probabilistic models often assume that the observations are independent and identically distributed. We present the Multi-Level Variational Autoencoder (ML-VAE), a new deep probabilistic model for learning a disentangled representation of a set of grouped observations. The ML-VAE separates the latent representation into semantically meaningful parts by working both at the group level and the observation level, while retaining efficient test-time inference. Quantitative and qualitative evaluations show that the ML-VAE model (i) learns a semantically meaningful disentanglement of grouped data, (ii) enables manipulation of the latent representation, and (iii) generalises to unseen groups. version:1
arxiv-1705-08828 | Deep Investigation of Cross-Language Plagiarism Detection Methods | http://arxiv.org/abs/1705.08828 | id:1705.08828 author:Jeremy Ferrero, Laurent Besacier, Didier Schwab, Frederic Agnes category:cs.CL  published:2017-05-24 summary:This paper is a deep investigation of cross-language plagiarism detection methods on a new recently introduced open dataset, which contains parallel and comparable collections of documents with multiple characteristics (different genres, languages and sizes of texts). We investigate cross-language plagiarism detection methods for 6 language pairs on 2 granularities of text units in order to draw robust conclusions on the best methods while deeply analyzing correlations across document styles and languages. version:1
arxiv-1705-08826 | Learning with Average Top-k Loss | http://arxiv.org/abs/1705.08826 | id:1705.08826 author:Yanbo Fan, Siwei Lyu, Yiming Ying, Bao-Gang Hu category:stat.ML cs.LG  published:2017-05-24 summary:In this work, we introduce the average top-$k$ (AT$_k$) loss as a new ensemble loss for supervised learning, which is the average over the $k$ largest individual losses over a training dataset. We show that the AT$_k$ loss is a natural generalization of the two widely used ensemble losses, namely the average loss and the maximum loss, but can combines their advantages and mitigate their drawbacks to better adapt to different data distributions. Furthermore, it remains a convex function over all individual losses, which can lead to convex optimization problems that can be solved effectively with conventional gradient-based method. We provide an intuitive interpretation of the AT$_k$ loss based on its equivalent effect on the continuous individual loss functions, suggesting that it can reduce the penalty on correctly classified data. We further give a learning theory analysis of MAT$_k$ learning on the classification calibration of the AT$_k$ loss and the error bounds of AT$_k$-SVM. We demonstrate the applicability of minimum average top-$k$ learning for binary classification and regression using synthetic and real datasets. version:1
arxiv-1705-08824 | From source to target and back: symmetric bi-directional adaptive GAN | http://arxiv.org/abs/1705.08824 | id:1705.08824 author:Paolo Russo, Fabio Maria Carlucci, Tatiana Tommasi, Barbara Caputo category:cs.CV  published:2017-05-24 summary:The effectiveness of generative adversarial approaches in producing images according to a specific style or visual domain has recently opened new directions to solve the unsupervised domain adaptation problem. It has been shown that source labeled images can be modified to mimic target samples making it possible to train directly a classifier in the target domain, despite the original lack of annotated data. Inverse mappings from the target to the source domain have also been evaluated but only passing through adapted feature spaces, thus without new image generation. In this paper we propose to better exploit the potential of generative adversarial networks for adaptation by introducing a novel symmetric mapping among domains. We jointly optimize bi-directional image transformations combining them with target self-labeling. Moreover we define a new class consistency loss that aligns the generators in the two directions imposing to conserve the class identity of an image passing through both domain mappings. A detailed qualitative and quantitative analysis of the reconstructed images confirm the power of our approach. By integrating the two domain specific classifiers obtained with our bi-directional network we exceed previous state-of-the-art unsupervised adaptation results on four different benchmark datasets. version:1
arxiv-1705-08821 | Causal Effect Inference with Deep Latent-Variable Models | http://arxiv.org/abs/1705.08821 | id:1705.08821 author:Christos Louizos, Uri Shalit, Joris Mooij, David Sontag, Richard Zemel, Max Welling category:stat.ML cs.LG  published:2017-05-24 summary:Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modelling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects. version:1
arxiv-1705-08815 | Power Systems Data Fusion based on Belief Propagation | http://arxiv.org/abs/1705.08815 | id:1705.08815 author:Francesco Fusco, Seshu Tirupathi, Robert Gormally category:stat.ML cs.SY stat.AP  published:2017-05-24 summary:The increasing complexity of the power grid, due to higher penetration of distributed resources and the growing availability of interconnected, distributed metering devices re- quires novel tools for providing a unified and consistent view of the system. A computational framework for power systems data fusion, based on probabilistic graphical models, capable of combining heterogeneous data sources with classical state estimation nodes and other customised computational nodes, is proposed. The framework allows flexible extension of the notion of grid state beyond the view of flows and injection in bus-branch models, and an efficient, naturally distributed inference algorithm can be derived. An application of the data fusion model to the quantification of distributed solar energy is proposed through numerical examples based on semi-synthetic simulations of the standard IEEE 14-bus test case. version:1
arxiv-1705-08814 | Boundary Crossing Probabilities for General Exponential Families | http://arxiv.org/abs/1705.08814 | id:1705.08814 author:Odalric-Ambrym Maillard category:stat.ML  published:2017-05-24 summary:We consider parametric exponential families of dimension $K$ on the real line. We study a variant of \textit{boundary crossing probabilities} coming from the multi-armed bandit literature, in the case when the real-valued distributions form an exponential family of dimension $K$. Formally, our result is a concentration inequality that bounds the probability that $\mathcal{B}^\psi(\hat \theta_n,\theta^\star)\geq f(t/n)/n$, where $\theta^\star$ is the parameter of an unknown target distribution, $\hat \theta_n$ is the empirical parameter estimate built from $n$ observations, $\psi$ is the log-partition function of the exponential family and $\mathcal{B}^\psi$ is the corresponding Bregman divergence. From the perspective of stochastic multi-armed bandits, we pay special attention to the case when the boundary function $f$ is logarithmic, as it is enables to analyze the regret of the state-of-the-art \KLUCB\ and \KLUCBp\ strategies, whose analysis was left open in such generality. Indeed, previous results only hold for the case when $K=1$, while we provide results for arbitrary finite dimension $K$, thus considerably extending the existing results. Perhaps surprisingly, we highlight that the proof techniques to achieve these strong results already existed three decades ago in the work of T.L. Lai, and were apparently forgotten in the bandit community. We provide a modern rewriting of these beautiful techniques that we believe are useful beyond the application to stochastic multi-armed bandits. version:1
arxiv-1705-07877 | Block building programming for symbolic regression | http://arxiv.org/abs/1705.07877 | id:1705.07877 author:Chen Chen, Changtong Luo, Zonglin Jiang category:cs.NE  published:2017-05-22 summary:Symbolic regression that aims to detect underlying data-driven model has become increasingly important for industrial data analysis. For most of existing algorithms, such as genetic programming (GP), the convergence speed might be too slow for large scale problems with a large number of variables. This situation may become even worse with increasing problem size. The aforementioned difficulty makes symbolic regression limited in practical applications. Fortunately, in many engineering problems, the independent variables in target models are separable or partially separable. This feature inspires us to develop a new approach, block building programming (BBP), in this paper. BBP divides the original target function into several blocks, and further into factors. The factors are then optimized by an optimization engine (e.g., GP). Under such circumstance, BBP can make large reductions to the search space. The partition of separability is based on a special method, block and factor detection. Two different optimization engines are applied to test the performance of BBP on a set of symbolic regression problems. Numerical results show that BBP has a good capability of `structure and coefficient optimization' with high computational efficiency. version:3
arxiv-1705-08804 | Beyond Parity: Fairness Objectives for Collaborative Filtering | http://arxiv.org/abs/1705.08804 | id:1705.08804 author:Sirui Yao, Bert Huang category:cs.IR cs.AI cs.LG stat.ML  published:2017-05-24 summary:We study fairness in collaborative-filtering recommender systems, which are sensitive to discrimination that exists in historical data. Biased data can lead collaborative-filtering methods to make unfair predictions for users from minority groups. We identify the insufficiency of existing fairness metrics and propose four new metrics that address different forms of unfairness. These fairness metrics can be optimized by adding fairness terms to the learning objective. Experiments on synthetic and real data show that our new metrics can better measure fairness than the baseline, and that the fairness objectives effectively help reduce unfairness. version:1
arxiv-1704-07597 | Learning Agents in Black-Scholes Financial Markets: Consensus Dynamics and Volatility Smiles | http://arxiv.org/abs/1704.07597 | id:1704.07597 author:Tushar Vaidya, Carlos Murguia, Georgios Piliouras category:q-fin.MF cs.LG cs.MA  published:2017-04-25 summary:Black-Scholes (BS) is the standard mathematical model for option pricing in financial markets. Option prices are calculated using an analytical formula whose main inputs are strike (at which price to exercise) and volatility. The BS framework assumes that volatility remains constant across all strikes, however, in practice it varies. How do traders come to learn these parameters? We introduce natural models of learning agents, in which they update their beliefs about the true implied volatility based on the opinions of other traders. We prove convergence of these opinion dynamics using techniques from control theory and leader-follower models, thus providing a resolution between theory and market practices. We allow for two different models, one with feedback and one with an unknown leader. version:2
arxiv-1705-08790 | Optimization of the Jaccard index for image segmentation with the Lovász hinge | http://arxiv.org/abs/1705.08790 | id:1705.08790 author:Maxim Berman, Matthew B. Blaschko category:cs.CV  published:2017-05-24 summary:The Jaccard loss, commonly referred to as the intersection-over-union loss, is commonly employed in the evaluation of segmentation quality due to its better perceptual quality and scale invariance, which lends appropriate relevance to small objects compared with per-pixel losses. We present a method for direct optimization of the per-image intersection-over-union loss in neural networks, in the context of semantic image segmentation, based on a convex surrogate: the Lov\'asz hinge. The loss is shown to perform better with respect to the Jaccard index measure than other losses traditionally used in the context of semantic segmentation; such as cross-entropy. We develop a specialized optimization method, based on an efficient computation of the proximal operator of the Lov\'asz hinge, yielding reliably faster and more stable optimization than alternatives. We demonstrate the effectiveness of the method by showing substantially improved intersection-overunion segmentation scores on the Pascal VOC dataset using a state-of-the-art deep learning segmentation architecture. version:1
arxiv-1705-08380 | An evolutionary strategy for DeltaE - E identification | http://arxiv.org/abs/1705.08380 | id:1705.08380 author:Katarzyna Schmidt, Oskar Wyszynski category:physics.ins-det cs.NE nucl-ex  published:2017-05-23 summary:In this article we present an automatic method for charge and mass identification of charged nuclear fragments produced in heavy ion collisions at intermediate energies. The algorithm combines a generative model of DeltaE - E relation and a Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES). The CMA-ES is a stochastic and derivative-free method employed to search parameter space of the model by means of a fitness function. The article describes details of the method along with results of an application on simulated labeled data. version:2
arxiv-1705-08764 | Adaptive Detrending to Accelerate Convolutional Gated Recurrent Unit Training for Contextual Video Recognition | http://arxiv.org/abs/1705.08764 | id:1705.08764 author:Minju Jung, Haanvid Lee, Jun Tani category:cs.CV  published:2017-05-24 summary:Based on the progress of image recognition, video recognition has been extensively studied recently. However, most of the existing methods are focused on short-term but not long-term video recognition, called contextual video recognition. To address contextual video recognition, we use convolutional recurrent neural networks (ConvRNNs) having a rich spatio-temporal information processing capability, but ConvRNNs requires extensive computation that slows down training. In this paper, inspired by the normalization and detrending methods, we propose adaptive detrending (AD) for temporal normalization in order to accelerate the training of ConvRNNs, especially for convolutional gated recurrent unit (ConvGRU). AD removes internal covariate shift within a sequence of each neuron in recurrent neural networks (RNNs) by subtracting a trend. In the experiments for contextual recognition on ConvGRU, the results show that (1) ConvGRU clearly outperforms the feed-forward neural networks, (2) AD consistently offers a significant training acceleration and generalization improvement, and (3) AD is further improved by collaborating with the existing normalization methods. version:1
arxiv-1705-08759 | Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence Models for Fill-in-the-Blank Image Captioning | http://arxiv.org/abs/1705.08759 | id:1705.08759 author:Qing Sun, Stefan Lee, Dhruv Batra category:cs.CV  published:2017-05-24 summary:We develop the first approximate inference algorithm for 1-Best (and M-Best) decoding in bidirectional neural sequence models by extending Beam Search (BS) to reason about both forward and backward time dependencies. Beam Search (BS) is a widely used approximate inference algorithm for decoding sequences from unidirectional neural sequence models. Interestingly, approximate inference in bidirectional models remains an open problem, despite their significant advantage in modeling information from both the past and future. To enable the use of bidirectional models, we present Bidirectional Beam Search (BiBS), an efficient algorithm for approximate bidirectional inference.To evaluate our method and as an interesting problem in its own right, we introduce a novel Fill-in-the-Blank Image Captioning task which requires reasoning about both past and future sentence structure to reconstruct sensible image descriptions. We use this task as well as the Visual Madlibs dataset to demonstrate the effectiveness of our approach, consistently outperforming all baseline methods. version:1
arxiv-1705-08741 | Train longer, generalize better: closing the generalization gap in large batch training of neural networks | http://arxiv.org/abs/1705.08741 | id:1705.08741 author:Elad Hoffer, Itay Hubara, Daniel Soudry category:stat.ML cs.LG  published:2017-05-24 summary:Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance - known as the "generalization gap" phenomena. Identifying the origin of this gap and closing it had remained an open problem. Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization. version:1
arxiv-1705-07425 | Learning Semantic Relatedness From Human Feedback Using Metric Learning | http://arxiv.org/abs/1705.07425 | id:1705.07425 author:Thomas Niebler, Martin Becker, Christian Pölitz, Andreas Hotho category:cs.CL cs.LG  published:2017-05-21 summary:Assessing the degree of semantic relatedness between words is an important task with a variety of semantic applications, such as ontology learning for the Semantic Web, semantic search or query expansion. To accomplish this in an automated fashion, many relatedness measures have been proposed. However, most of these metrics only encode information contained in the underlying corpus and thus do not directly model human intuition. To solve this, we propose to utilize a metric learning approach to improve existing semantic relatedness measures by learning from additional information, such as explicit human feedback. For this, we argue to use word embeddings instead of traditional high-dimensional vector representations in order to leverage their semantic density and to reduce computational cost. We rigorously test our approach on several domains including tagging data as well as publicly available embeddings based on Wikipedia texts and navigation. Human feedback about semantic relatedness for learning and evaluation is extracted from publicly available datasets such as MEN or WS-353. We find that our method can significantly improve semantic relatedness measures by learning from additional information, such as explicit human feedback. For tagging data, we are the first to generate and study embeddings. Our results are of special interest for ontology and recommendation engineers, but also for any other researchers and practitioners of Semantic Web techniques. version:2
arxiv-1705-08736 | Non-Stationary Spectral Kernels | http://arxiv.org/abs/1705.08736 | id:1705.08736 author:Sami Remes, Markus Heinonen, Samuel Kaski category:stat.ML cs.LG  published:2017-05-24 summary:We propose non-stationary spectral kernels for Gaussian process regression. We propose to model the spectral density of a non-stationary kernel function as a mixture of input-dependent Gaussian process frequency density surfaces. We solve the generalised Fourier transform with such a model, and present a family of non-stationary and non-monotonic kernels that can learn input-dependent and potentially long-range, non-monotonic covariances between inputs. We derive efficient inference using model whitening and marginalized posterior, and show with case studies that these kernels are necessary when modelling even rather simple time series, image or geospatial data with non-stationary characteristics. version:1
arxiv-1705-08722 | Open-Category Classification by Adversarial Sample Generation | http://arxiv.org/abs/1705.08722 | id:1705.08722 author:Yang Yu, Wei-Yang Qu, Nan Li, Zimin Guo category:cs.LG  published:2017-05-24 summary:In real-world classification tasks, it is difficult to collect samples of all possible categories of the environment in the training stage. Therefore, the classifier should be prepared for unseen classes. When an instance of an unseen class appears in the prediction stage, a robust classifier should have the ability to tell it is unseen, instead of classifying it to be any known category. In this paper, adopting the idea of adversarial learning, we propose the ASG framework for open-category classification. ASG generates positive and negative samples of seen categories in the unsupervised manner via an adversarial learning strategy. With the generated samples, ASG then learns to tell seen from unseen in the supervised manner. Experiments performed on several datasets show the effectiveness of ASG. version:1
arxiv-1705-08716 | An experimental study of graph-based semi-supervised classification with additional node information | http://arxiv.org/abs/1705.08716 | id:1705.08716 author:Bertrand Lebichot, Marco Saerens category:stat.ML  published:2017-05-24 summary:The volume of data generated by internet and social networks is increasing every day, and there is a clear need for efficient ways of extracting useful information from them. As those data can take different forms, it is important to use all the available data representations for prediction. In this paper, we focus our attention on supervised classification using both regular plain, tabular, data and structural information coming from a network structure. 14 techniques are investigated and compared in this study and can be divided in three classes: the first one uses only the plain data to build a classification model, the second uses only the graph structure and the last uses both information sources. The relative performances in these three cases are investigated. Furthermore, the effect of using a graph embedding and well-known indicators in spatial statistics is also studied. Possible applications are automatic classification of web pages or other linked documents, of people in a social network or of proteins in a biological complex system, to name a few. Based on our comparison, we draw some general conclusions and advices to tackle this particular classification task: some datasets can be better explained by their graph structure (graph-driven), or by their feature set (features-driven). The most efficient methods are discussed in both cases. version:1
arxiv-1705-08707 | Inclusive Flavour Tagging Algorithm | http://arxiv.org/abs/1705.08707 | id:1705.08707 author:Tatiana Likhomanenko, Denis Derkach, Alex Rogozhnikov category:hep-ex stat.ML  published:2017-05-24 summary:Identifying the flavour of neutral $B$ mesons production is one of the most important components needed in the study of time-dependent $CP$ violation. The harsh environment of the Large Hadron Collider makes it particularly hard to succeed in this task. We present an inclusive flavour-tagging algorithm as an upgrade of the algorithms currently used by the LHCb experiment. Specifically, a probabilistic model which efficiently combines information from reconstructed vertices and tracks using machine learning is proposed. The algorithm does not use information about underlying physics process. It reduces the dependence on the performance of lower level identification capacities and thus increases the overall performance. The proposed inclusive flavour-tagging algorithm is applicable to tag the flavour of $B$ mesons in any proton-proton experiment. version:1
arxiv-1705-08695 | Stochastic Sequential Neural Networks with Structured Inference | http://arxiv.org/abs/1705.08695 | id:1705.08695 author:Hao Liu, Haoli Bai, Lirong He, Zenglin Xu category:cs.LG cs.CV  published:2017-05-24 summary:Unsupervised structure learning in high-dimensional time series data has attracted a lot of research interests. For example, segmenting and labelling high dimensional time series can be helpful in behavior understanding and medical diagnosis. Recent advances in generative sequential modeling have suggested to combine recurrent neural networks with state space models (e.g., Hidden Markov Models). This combination can model not only the long term dependency in sequential data, but also the uncertainty included in the hidden states. Inheriting these advantages of stochastic neural sequential models, we propose a structured and stochastic sequential neural network, which models both the long-term dependencies via recurrent neural networks and the uncertainty in the segmentation and labels via discrete random variables. For accurate and efficient inference, we present a bi-directional inference network by reparamterizing the categorical segmentation and labels with the recent proposed Gumbel-Softmax approximation and resort to the Stochastic Gradient Variational Bayes. We evaluate the proposed model in a number of tasks, including speech modeling, automatic segmentation and labeling in behavior understanding, and sequential multi-objects recognition. Experimental results have demonstrated that our proposed model can achieve significant improvement over the state-of-the-art methods. version:1
arxiv-1704-05255 | Criticality as It Could Be: organizational invariance as self-organized criticality in embodied agents | http://arxiv.org/abs/1704.05255 | id:1704.05255 author:Miguel Aguilera, Manuel G. Bedia category:nlin.AO cond-mat.dis-nn cond-mat.stat-mech cs.NE q-bio.NC  published:2017-04-18 summary:This paper outlines a methodological approach for designing adaptive agents driving themselves near points of criticality. Using a synthetic approach we construct a conceptual model that, instead of specifying mechanistic requirements to generate criticality, exploits the maintenance of an organizational structure capable of reproducing critical behavior. Our approach exploits the well-known principle of universality, which classifies critical phenomena inside a few universality classes of systems independently of their specific mechanisms or topologies. In particular, we implement an artificial embodied agent controlled by a neural network maintaining a correlation structure randomly sampled from a lattice Ising model at a critical point. We evaluate the agent in two classical reinforcement learning scenarios: the Mountain Car benchmark and the Acrobot double pendulum, finding that in both cases the neural controller reaches a point of criticality, which coincides with a transition point between two regimes of the agent's behaviour, maximizing the mutual information between neurons and sensorimotor patterns. Finally, we discuss the possible applications of this synthetic approach to the comprehension of deeper principles connected to the pervasive presence of criticality in biological and cognitive systems. version:3
arxiv-1705-08690 | Continual Learning with Deep Generative Replay | http://arxiv.org/abs/1705.08690 | id:1705.08690 author:Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim category:cs.AI cs.CV cs.LG  published:2017-05-24 summary:Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model ("solver"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks. version:1
arxiv-1705-09650 | Anomaly Detection in a Digital Video Broadcasting System Using Timed Automata | http://arxiv.org/abs/1705.09650 | id:1705.09650 author:Xiaoran Liu, Qin Lin, Sicco Verwer, Dmitri Jarnikov category:cs.LG cs.AI cs.FL cs.LO  published:2017-05-24 summary:This paper focuses on detecting anomalies in a digital video broadcasting (DVB) system from providers' perspective. We learn a probabilistic deterministic real timed automaton profiling benign behavior of encryption control in the DVB control access system. This profile is used as a one-class classifier. Anomalous items in a testing sequence are detected when the sequence is not accepted by the learned model. version:1
arxiv-1705-08664 | Towards Understanding the Invertibility of Convolutional Neural Networks | http://arxiv.org/abs/1705.08664 | id:1705.08664 author:Anna C. Gilbert, Yi Zhang, Kibok Lee, Yuting Zhang, Honglak Lee category:stat.ML cs.LG  published:2017-05-24 summary:Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable re- construction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios. version:1
arxiv-1705-00487 | Generalized orderless pooling performs implicit salient matching | http://arxiv.org/abs/1705.00487 | id:1705.00487 author:Marcel Simon, Yang Gao, Trevor Darrell, Joachim Denzler, Erik Rodner category:cs.CV  published:2017-05-01 summary:Most recent CNN architectures use average pooling as a final feature encoding step. In the field of fine-grained recognition, however, recent global representations like bilinear pooling offer improved performance. In this paper, we generalize average and bilinear pooling to "alpha-pooling", allowing for learning the pooling strategy during training. In addition, we present a novel way to visualize decisions made by these approaches. We identify parts of training images having the highest influence on the prediction of a given test image. It allows for justifying decisions to users and also for analyzing the influence of semantic parts. For example, we can show that the higher capacity VGG16 model focuses much more on the bird's head than, e.g., the lower-capacity VGG-M model when recognizing fine-grained bird categories. Both contributions allow us to analyze the difference when moving between average and bilinear pooling. In addition, experiments show that our generalized approach can outperform both across a variety of standard datasets. version:2
arxiv-1705-08369 | Her2 Challenge Contest: A Detailed Assessment of Automated Her2 Scoring Algorithms in Whole Slide Images of Breast Cancer Tissues | http://arxiv.org/abs/1705.08369 | id:1705.08369 author:Talha Qaiser, Abhik Mukherjee, Chaitanya Reddy Pb, Sai Dileep Munugoti, Vamsi Tallam, Tomi Pitkäaho, Taina Lehtimäki, Thomas Naughton, Matt Berseth, Aníbal Pedraza, Ramakrishnan Mukundan, Matthew Smith, Abhir Bhalerao, Erik Rodner, Marcel Simon, Joachim Denzler, Chao-Hui Huang, Gloria Bueno, David Snead, Ian Ellis, Mohammad Ilyas, Nasir Rajpoot category:cs.CV cs.AI q-bio.QM  published:2017-05-23 summary:Evaluating expression of the Human epidermal growth factor receptor 2 (Her2) by visual examination of immunohistochemistry (IHC) on invasive breast cancer (BCa) is a key part of the diagnostic assessment of BCa due to its recognised importance as a predictive and prognostic marker in clinical practice. However, visual scoring of Her2 is subjective and consequently prone to inter-observer variability. Given the prognostic and therapeutic implications of Her2 scoring, a more objective method is required. In this paper, we report on a recent automated Her2 scoring contest, held in conjunction with the annual PathSoc meeting held in Nottingham in June 2016, aimed at systematically comparing and advancing the state-of-the-art Artificial Intelligence (AI) based automated methods for Her2 scoring. The contest dataset comprised of digitised whole slide images (WSI) of sections from 86 cases of invasive breast carcinoma stained with both Haematoxylin & Eosin (H&E) and IHC for Her2. The contesting algorithms automatically predicted scores of the IHC slides for an unseen subset of the dataset and the predicted scores were compared with the 'ground truth' (a consensus score from at least two experts). We also report on a simple Man vs Machine contest for the scoring of Her2 and show that the automated methods could beat the pathology experts on this contest dataset. This paper presents a benchmark for comparing the performance of automated algorithms for scoring of Her2. It also demonstrates the enormous potential of automated algorithms in assisting the pathologist with objective IHC scoring. version:2
arxiv-1705-08272 | Matching neural paths: transfer from recognition to correspondence search | http://arxiv.org/abs/1705.08272 | id:1705.08272 author:Nikolay Savinov, Lubor Ladicky, Marc Pollefeys category:cs.CV cs.LG cs.NE  published:2017-05-19 summary:Many machine learning tasks require finding per-part correspondences between objects. In this work we focus on low-level correspondences - a highly ambiguous matching problem. We propose to use a hierarchical semantic representation of the objects, coming from a convolutional neural network, to solve this ambiguity. Training it for low-level correspondence prediction directly might not be an option in some domains where the ground-truth correspondences are hard to obtain. We show how transfer from recognition can be used to avoid such training. Our idea is to mark parts as "matching" if their features are close to each other at all the levels of convolutional feature hierarchy (neural paths). Although the overall number of such paths is exponential in the number of layers, we propose a polynomial algorithm for aggregating all of them in a single backward pass. The empirical validation is done on the task of stereo correspondence and demonstrates that we achieve competitive results among the methods which do not use labeled target domain data. version:2
arxiv-1705-07632 | Computer vision-based food calorie estimation: dataset, method, and experiment | http://arxiv.org/abs/1705.07632 | id:1705.07632 author:Yanchao Liang, Jianhua Li category:cs.CV  published:2017-05-22 summary:Computer vision has been introduced to estimate calories from food images. But current food image data sets don't contain volume and mass records of foods, which leads to an incomplete calorie estimation. In this paper, we present a novel food image data set with volume and mass records of foods, and a deep learning method for food detection, to make a complete calorie estimation. Our data set includes 2978 images, and every image contains corresponding each food's annotation, volume and mass records, as well as a certain calibration reference. To estimate calorie of food in the proposed data set, a deep learning method using Faster R-CNN first is put forward to detect the food. And the experiment results show our method is effective to estimate calories and our data set contains adequate information for calorie estimation. Our data set is the first released food image data set which can be used to evaluate computer vision-based calorie estimation methods. version:3
arxiv-1705-08639 | Fast-Slow Recurrent Neural Networks | http://arxiv.org/abs/1705.08639 | id:1705.08639 author:Asier Mujika, Florian Meier, Angelika Steger category:cs.NE  published:2017-05-24 summary:Processing sequential data of variable length is a major challenge in a wide range of applications, such as speech recognition, language modeling, generative image modeling and machine translation. Here, we address this challenge by proposing a novel recurrent neural network (RNN) architecture, the Fast-Slow RNN (FS-RNN). The FS-RNN incorporates the strengths of both multiscale RNNs and deep transition RNNs as it processes sequential data on different timescales and learns complex transition functions from one time step to the next. We evaluate the FS-RNN on two character level language modeling data sets, Penn Treebank and Hutter Prize Wikipedia, where we improve state of the art results to $1.19$ and $1.25$ bits-per-character (BPC), respectively. In addition, an ensemble of two FS-RNNs achieves $1.20$ BPC on Hutter Prize Wikipedia outperforming the best known compression algorithm with respect to the BPC measure. We also present an empirical investigation of the learning and network dynamics of the FS-RNN, which explains the improved performance compared to other RNN architectures. Our approach is general as any kind of RNN cell is a possible building block for the FS-RNN architecture, and thus can be flexibly applied to different tasks. version:1
arxiv-1705-08631 | Self-supervised learning of visual features through embedding images into text topic spaces | http://arxiv.org/abs/1705.08631 | id:1705.08631 author:Lluis Gomez, Yash Patel, Marçal Rusiñol, Dimosthenis Karatzas, C. V. Jawahar category:cs.CV  published:2017-05-24 summary:End-to-end training from scratch of current deep architectures for new computer vision problems would require Imagenet-scale datasets, and this is not always possible. In this paper we present a method that is able to take advantage of freely available multi-modal content to train computer vision algorithms without human supervision. We put forward the idea of performing self-supervised learning of visual features by mining a large scale corpus of multi-modal (text and image) documents. We show that discriminative visual features can be learnt efficiently by training a CNN to predict the semantic context in which a particular image is more probable to appear as an illustration. For this we leverage the hidden semantic structures discovered in the text corpus with a well-known topic modeling technique. Our experiments demonstrate state of the art performance in image classification, object detection, and multi-modal retrieval compared to recent self-supervised or natural-supervised approaches. version:1
arxiv-1705-10596 | Approximation learning methods of Harmonic Mappings in relation to Hardy Spaces | http://arxiv.org/abs/1705.10596 | id:1705.10596 author:Zhulin Liu, C. L. Philip Chen category:math.NA cs.LG  published:2017-05-24 summary:A new Hardy space Hardy space approach of Dirichlet type problem based on Tikhonov regularization and Reproducing Hilbert kernel space is discussed in this paper, which turns out to be a typical extremal problem located on the upper upper-high complex plane. If considering this in the Hardy space, the optimization operator of this problem will be highly simplified and an efficient algorithm is possible. This is mainly realized by the help of reproducing properties of the functions in the Hardy space of upper-high complex plane, and the detail algorithm is proposed. Moreover, harmonic mappings, which is a significant geometric transformation, are commonly used in many applications such as image processing, since it describes the energy minimization mappings between individual manifolds. Particularly, when we focus on the planer mappings between two Euclid planer regions, the harmonic mappings are exist and unique, which is guaranteed solidly by the existence of harmonic function. This property is attractive and simulation results are shown in this paper to ensure the capability of applications such as planer shape distortion and surface registration. version:1
arxiv-1705-07878 | TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning | http://arxiv.org/abs/1705.07878 | id:1705.07878 author:Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai Li category:cs.LG cs.DC cs.NE I.2.6; I.5.1  published:2017-05-22 summary:High network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training. In this work, we propose TernGrad that uses ternary gradients to accelerate distributed deep learning in data parallelism. Our approach requires only three numerical levels {-1,0,1} which can aggressively reduce the communication time. We mathematically prove the convergence of TernGrad under the assumption of a bound on gradients. Guided by the bound, we propose layer-wise ternarizing and gradient clipping to improve its convergence. Our experiments show that applying TernGrad on AlexNet does not incur any accuracy loss and can even improve accuracy. The accuracy loss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a performance model is proposed to study the scalability of TernGrad. Experiments show significant speed gains for various deep neural networks. version:2
arxiv-1705-08624 | VANETs Meet Autonomous Vehicles: A Multimodal 3D Environment Learning Approach | http://arxiv.org/abs/1705.08624 | id:1705.08624 author:Yassine Maalej, Sameh Sorour, Ahmed Abdel-Rahim, Mohsen Guizani category:cs.CV  published:2017-05-24 summary:In this paper, we design a multimodal framework for object detection, recognition and mapping based on the fusion of stereo camera frames, point cloud Velodyne Lidar scans, and Vehicle-to-Vehicle (V2V) Basic Safety Messages (BSMs) exchanged using Dedicated Short Range Communication (DSRC). We merge the key features of rich texture descriptions of objects from 2D images, depth and distance between objects provided by 3D point cloud and awareness of hidden vehicles from BSMs' 3D information. We present a joint pixel to point cloud and pixel to V2V correspondences of objects in frames from the Kitti Vision Benchmark Suite by using a semi-supervised manifold alignment approach to achieve camera-Lidar and camera-V2V mapping of their recognized objects that have the same underlying manifold. version:1
arxiv-1705-08623 | Deep Rotation Equivariant Network | http://arxiv.org/abs/1705.08623 | id:1705.08623 author:Junying Li, Zichen Yang, Haifeng Liu, Deng Cai category:cs.CV  published:2017-05-24 summary:Recently, learning equivariant representations has attracted considerable research attention. Dieleman et al. introduce four operations which can be inserted to CNN to learn deep representations equivariant to rotation. However, feature maps should be copied and rotated four times in each layer in their approach, which causes much running time and memory overhead. In order to address this problem, we propose Deep Rotation Equivariant Network(DREN) consisting of cycle layers, isotonic layers and decycle layers.Our proposed layers apply rotation transformation on filters rather than feature maps, achieving a speed up of more than 2 times with even less memory overhead. We evaluate DRENs on Rotated MNIST and CIFAR-10 datasets and demonstrate that it can improve the performance of state-of-the-art architectures. Our codes are released on GitHub. version:1
arxiv-1705-08621 | Nonparametric Preference Completion | http://arxiv.org/abs/1705.08621 | id:1705.08621 author:Julian Katz-Samuels, Clayton Scott category:stat.ML cs.LG  published:2017-05-24 summary:We consider the task of collaborative preference completion: given a pool of items, a pool of users and a partially observed item-user rating matrix, the goal is to recover the personalized ranking of each user over all of the items. Our approach is nonparametric: we assume that each item $i$ and each user $u$ have unobserved features $x_i$ and $y_u$, and that the associated rating is given by $g_u(f(x_i,y_u))$ where $f$ is Lipschitz and $g_u$ is a monotonic transformation that depends on the user. We propose a $k$-nearest neighbors-like algorithm and prove that it is consistent. To the best of our knowledge, this is the first consistency result for the collaborative preference completion problem in a nonparametric setting. Finally, we conduct experiments on the Netflix and Movielens datasets that suggest that our algorithm has some advantages over existing neighborhood-based methods and that its performance is comparable to some state-of-the art matrix factorization methods. version:1
arxiv-1705-08620 | Robust Data Geometric Structure Aligned Close yet Discriminative Domain Adaptation | http://arxiv.org/abs/1705.08620 | id:1705.08620 author:Lingkun Luo, Xiaofang Wang, Shiqiang Hu, Liming Chen category:cs.CV  published:2017-05-24 summary:Domain adaptation (DA) is transfer learning which aims to leverage labeled data in a related source domain to achieve informed knowledge transfer and help the classification of unlabeled data in a target domain. In this paper, we propose a novel DA method, namely Robust Data Geometric Structure Aligned, Close yet Discriminative Domain Adaptation (RSA-CDDA), which brings closer, in a latent joint subspace, both source and target data distributions, and aligns inherent hidden source and target data geometric structures while performing discriminative DA in repulsing both interclass source and target data. The proposed method performs domain adaptation between source and target in solving a unified model, which incorporates data distribution constraints, in particular via a nonparametric distance, i.e., Maximum Mean Discrepancy (MMD), as well as constraints on inherent hidden data geometric structure segmentation and alignment between source and target, through low rank and sparse representation. RSA-CDDA achieves the search of a joint subspace in solving the proposed unified model through iterative optimization, alternating Rayleigh quotient algorithm and inexact augmented Lagrange multiplier algorithm. Extensive experiments carried out on standard DA benchmarks, i.e., 16 cross-domain image classification tasks, verify the effectiveness of the proposed method, which consistently outperforms the state-of-the-art methods. version:1
arxiv-1705-08619 | Dictionary-based Monitoring of Premature Ventricular Contractions: An Ultra-Low-Cost Point-of-Care Service | http://arxiv.org/abs/1705.08619 | id:1705.08619 author:Bollepalli S. Chandra, Challa S. Sastry, Laxminarayana Anumandla, Soumya Jana category:cs.LG  published:2017-05-24 summary:While cardiovascular diseases (CVDs) are prevalent across economic strata, the economically disadvantaged population is disproportionately affected due to the high cost of traditional CVD management. Accordingly, developing an ultra-low-cost alternative, affordable even to groups at the bottom of the economic pyramid, has emerged as a societal imperative. Against this backdrop, we propose an inexpensive yet accurate home-based electrocardiogram(ECG) monitoring service. Specifically, we seek to provide point-of-care monitoring of premature ventricular contractions (PVCs), high frequency of which could indicate the onset of potentially fatal arrhythmia. Note that a traditional telecardiology system acquires the ECG, transmits it to a professional diagnostic centre without processing, and nearly achieves the diagnostic accuracy of a bedside setup, albeit at high bandwidth cost. In this context, we aim at reducing cost without significantly sacrificing reliability. To this end, we develop a dictionary-based algorithm that detects with high sensitivity the anomalous beats only which are then transmitted. We further compress those transmitted beats using class-specific dictionaries subject to suitable reconstruction/diagnostic fidelity. Such a scheme would not only reduce the overall bandwidth requirement, but also localising anomalous beats, thereby reducing physicians' burden. Finally, using Monte Carlo cross validation on MIT/BIH arrhythmia database, we evaluate the performance of the proposed system. In particular, with a sensitivity target of at most one undetected PVC in one hundred beats, and a percentage root mean squared difference less than 9% (a clinically acceptable level of fidelity), we achieved about 99.15% reduction in bandwidth cost, equivalent to 118-fold savings over traditional telecardiology. version:1
arxiv-1705-08618 | Multi-Task Learning for Contextual Bandits | http://arxiv.org/abs/1705.08618 | id:1705.08618 author:Aniket Anand Deshmukh, Urun Dogan, Clayton Scott category:stat.ML cs.LG  published:2017-05-24 summary:Contextual bandits are a form of multi-armed bandit in which the agent has access to predictive side information (known as the context) for each arm at each time step, and have been used to model personalized news recommendation, ad placement, and other applications. In this work, we propose a multi-task learning framework for contextual bandit problems. Like multi-task learning in the batch setting, the goal is to leverage similarities in contexts for different arms so as to improve the agent's ability to predict rewards from contexts. We propose an upper confidence bound-based multi-task learning algorithm for contextual bandits, establish a corresponding regret bound, and interpret this bound to quantify the advantages of learning in the presence of high task (arm) similarity. We also describe an effective scheme for estimating task similarity from data, and demonstrate our algorithm's performance on several data sets. version:1
arxiv-1705-07252 | SVM via Saddle Point Optimization: New Bounds and Distributed Algorithms | http://arxiv.org/abs/1705.07252 | id:1705.07252 author:Yifei Jin, Lingxiao Huang, Jian Li category:cs.LG cs.NA  published:2017-05-20 summary:Support Vector Machine is one of the most classical approaches for classification and regression. Despite being studied for decades, obtaining practical algorithms for SVM is still an active research problem in machine learning. In this paper, we propose a new perspective for SVM via saddle point optimization. We provide an algorithm which achieves $(1-\epsilon)$-approximations with running time $\tilde{O}(nd+n\sqrt{d / \epsilon})$ for both separable (hard margin SVM) and non-separable cases ($\nu$-SVM ), where $n$ is the number of points and $d$ is the dimensionality. To the best of our knowledge, the current best algorithm for hard margin SVM achieved by Gilbert algorithm~\cite{gartner2009coresets} requires $O(nd / \epsilon )$ time. Our algorithm improves the running time by a factor of $\sqrt{d}/\sqrt{\epsilon}$. For $\nu$-SVM, besides the well known quadratic programming approach which requires $\Omega(n^2 d)$ time~\cite{joachims1998making,platt199912}, no better algorithm is known. In the paper, we provide the first nearly linear time algorithm for $\nu$-SVM. We also consider the distributed settings and provide distributed algorithms with low communication cost via saddle point optimization. Our algorithms require $\tilde{O}(k(d +\sqrt{d/\epsilon}))$ communication cost where $k$ is the number of clients, almost matching the theoretical lower bound. version:3
arxiv-1705-07807 | Use Privacy in Data-Driven Systems: Theory and Experiments with Machine Learnt Programs | http://arxiv.org/abs/1705.07807 | id:1705.07807 author:Anupam Datta, Matthew Fredrikson, Gihyuk Ko, Piotr Mardziel, Shayak Sen category:cs.CR cs.LG  published:2017-05-22 summary:This paper presents an approach to formalizing and enforcing a class of use privacy properties in data-driven systems. In contrast to prior work, we focus on use restrictions on proxies (i.e. strong predictors) of protected information types. Our definition relates proxy use to intermediate computations that occur in a program, and identify two essential properties that characterize this behavior: 1) its result is strongly associated with the protected information type in question, and 2) it is likely to causally affect the final output of the program. For a specific instantiation of this definition, we present a program analysis technique that detects instances of proxy use in a model, and provides a witness that identifies which parts of the corresponding program exhibit the behavior. Recognizing that not all instances of proxy use of a protected information type are inappropriate, we make use of a normative judgment oracle that makes this inappropriateness determination for a given witness. Our repair algorithm uses the witness of an inappropriate proxy use to transform the model into one that provably does not exhibit proxy use, while avoiding changes that unduly affect classification accuracy. Using a corpus of social datasets, our evaluation shows that these algorithms are able to detect proxy use instances that would be difficult to find using existing techniques, and subsequently remove them while maintaining acceptable classification performance. version:2
arxiv-1705-08593 | Deep Learning Improves Template Matching by Normalized Cross Correlation | http://arxiv.org/abs/1705.08593 | id:1705.08593 author:Davit Buniatyan, Thomas Macrina, Dodam Ih, Jonathan Zung, H. Sebastian Seung category:cs.CV  published:2017-05-24 summary:Template matching by normalized cross correlation (NCC) is widely used for finding image correspondences. We improve the robustness of this algorithm by preprocessing images with "siamese" convolutional networks trained to maximize the contrast between NCC values of true and false matches. The improvement is quantified using patches of brain images from serial section electron microscopy. Relative to a parameter-tuned bandpass filter, siamese convolutional networks significantly reduce false matches. Furthermore, all false matches can be eliminated by removing a tiny fraction of all matches based on NCC values. The improved accuracy of our method could be essential for connectomics, because emerging petascale datasets may require billions of template matches to assemble 2D images of serial sections into a 3D image stack. Our method is also expected to generalize to many other computer vision applications that use NCC template matching to find image correspondences. version:1
arxiv-1705-08590 | Generative Model with Coordinate Metric Learning for Object Recognition Based on 3D Models | http://arxiv.org/abs/1705.08590 | id:1705.08590 author:Yida Wang, Weihong Deng category:cs.CV  published:2017-05-24 summary:Given large amount of real photos for training, Convolutional neural network shows excellent performance on object recognition tasks. However, the process of collecting data is so tedious and the background are also limited which makes it hard to establish a perfect database. In this paper, our generative model trained with synthetic images rendered from 3D models reduces the workload of data collection and limitation of conditions. Our structure is composed of two sub-networks: semantic foreground object reconstruction network based on Bayesian inference and classification network based on multi-triplet cost function for avoiding over-fitting problem on monotone surface and fully utilizing pose information by establishing sphere-like distribution of descriptors in each category which is helpful for recognition on regular photos according to poses, lighting condition, background and category information of rendered images. Firstly, our conjugate structure called generative model with metric learning utilizing additional foreground object channels generated from Bayesian rendering as the joint of two sub-networks. Multi-triplet cost function based on poses for object recognition are used for metric learning which makes it possible training a category classifier purely based on synthetic data. Secondly, we design a coordinate training strategy with the help of adaptive noises acting as corruption on input images to help both sub-networks benefit from each other and avoid inharmonious parameter tuning due to different convergence speed of two sub-networks. Our structure achieves the state of the art accuracy of over 50\% on ShapeNet database with data migration obstacle from synthetic images to real photos. This pipeline makes it applicable to do recognition on real images only based on 3D models. version:1
arxiv-1705-08584 | MMD GAN: Towards Deeper Understanding of Moment Matching Network | http://arxiv.org/abs/1705.08584 | id:1705.08584 author:Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, Barnabás Póczos category:cs.LG  published:2017-05-24 summary:Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD-GAN. The new distance measure in MMD-GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works. version:1
arxiv-1705-08583 | Sequence Summarization Using Order-constrained Kernelized Feature Subspaces | http://arxiv.org/abs/1705.08583 | id:1705.08583 author:Anoop Cherian, Suvrit Sra, Richard Hartley category:cs.CV  published:2017-05-24 summary:Representations that can compactly and effectively capture temporal evolution of semantic content are important to machine learning algorithms that operate on multi-variate time-series data. We investigate such representations motivated by the task of human action recognition. Here each data instance is encoded by a multivariate feature (such as via a deep CNN) where action dynamics are characterized by their variations in time. As these features are often non-linear, we propose a novel pooling method, kernelized rank pooling, that represents a given sequence compactly as the pre-image of the parameters of a hyperplane in an RKHS, projections of data onto which captures their temporal order. We develop this idea further and show that such a pooling scheme can be cast as an order-constrained kernelized PCA objective; we then propose to use the parameters of a kernelized low-rank feature subspace as the representation of the sequences. We cast our formulation as an optimization problem on generalized Grassmann manifolds and then solve it efficiently using Riemannian optimization techniques. We present experiments on several action recognition datasets using diverse feature modalities and demonstrate state-of-the-art results. version:1
arxiv-1705-08580 | Exact Recovery of Number of Blocks in Blockmodels | http://arxiv.org/abs/1705.08580 | id:1705.08580 author:Bowei Yan, Purnamrita Sarkar, Xiuyuan Cheng category:stat.ML stat.ME  published:2017-05-24 summary:Community detection is an fundamental unsupervised learning problem for unlabeled networks which has a broad range of applications. Typically, community detection algorithms assume that the number of clusters $r$ is known apriori. While provable algorithms for finding $r$ has recently garnered much attention from the theoretical statistics community, existing methods often make strong model assumptions about the separation between clusters or communities. In this paper, we propose an approach based on semi-definite relaxation, which recovers the number of clusters and the clustering matrix exactly under a broad parameter regime, with probability tending to one. Compared to existing convex relaxations, our relaxation leads to exact recovery under weaker conditions on cluster separation or cluster sizes. On a variety of simulated and real data experiments, we show that the proposed method often outperforms state-of-the-art techniques for estimating the number of clusters. version:1
arxiv-1705-08564 | Towards Interrogating Discriminative Machine Learning Models | http://arxiv.org/abs/1705.08564 | id:1705.08564 author:Wenbo Guo, Kaixuan Zhang, Lin Lin, Sui Huang, Xinyu Xing category:cs.LG stat.ML  published:2017-05-23 summary:It is oftentimes impossible to understand how machine learning models reach a decision. While recent research has proposed various technical approaches to provide some clues as to how a learning model makes individual decisions, they cannot provide users with ability to inspect a learning model as a complete entity. In this work, we propose a new technical approach that augments a Bayesian regression mixture model with multiple elastic nets. Using the enhanced mixture model, we extract explanations for a target model through global approximation. To demonstrate the utility of our approach, we evaluate it on different learning models covering the tasks of text mining and image recognition. Our results indicate that the proposed approach not only outperforms the state-of-the-art technique in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of a learning model. version:1
arxiv-1705-08562 | Hashing as Tie-Aware Learning to Rank | http://arxiv.org/abs/1705.08562 | id:1705.08562 author:Kun He, Fatih Cakir, Sarah A. Bargal, Stan Sclaroff category:stat.ML cs.CV cs.LG  published:2017-05-23 summary:We formulate the problem of supervised hashing, or learning binary embeddings of data, as a learning to rank problem. Specifically, we optimize two common ranking-based evaluation metrics, Average Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). Observing that ranking with the discrete Hamming distance naturally results in ties, we propose to use tie-aware versions of ranking metrics in both the evaluation and the learning of supervised hashing. For AP and NDCG, we derive continuous relaxations of their tie-aware versions, and optimize them using stochastic gradient ascent with deep neural networks. Our results establish the new state-of-the-art for tie-aware AP and NDCG on common hashing benchmarks. version:1
arxiv-1705-07321 | Accelerated Hierarchical Density Clustering | http://arxiv.org/abs/1705.07321 | id:1705.07321 author:Leland McInnes, John Healy category:stat.ML  published:2017-05-20 summary:We present an accelerated algorithm for hierarchical density based clustering. Our new algorithm improves upon HDBSCAN*, which itself provided a significant qualitative improvement over the popular DBSCAN algorithm. The accelerated HDBSCAN* algorithm provides comparable performance to DBSCAN, while supporting variable density clusters, and eliminating the need for the difficult to tune distance scale parameter. This makes accelerated HDBSCAN* the default choice for density based clustering. Library available at: https://github.com/scikit-learn-contrib/hdbscan version:2
arxiv-1705-08557 | Grounded Recurrent Neural Networks | http://arxiv.org/abs/1705.08557 | id:1705.08557 author:Ankit Vani, Yacine Jernite, David Sontag category:stat.ML cs.CL cs.LG cs.NE  published:2017-05-23 summary:In this work, we present the Grounded Recurrent Neural Network (GRNN), a recurrent neural network architecture for multi-label prediction which explicitly ties labels to specific dimensions of the recurrent hidden state (we call this process "grounding"). The approach is particularly well-suited for extracting large numbers of concepts from text. We apply the new model to address an important problem in healthcare of understanding what medical concepts are discussed in clinical text. Using a publicly available dataset derived from Intensive Care Units, we learn to label a patient's diagnoses and procedures from their discharge summary. Our evaluation shows a clear advantage to using our proposed architecture over a variety of strong baselines. version:1
arxiv-1705-08551 | Safe Model-based Reinforcement Learning with Stability Guarantees | http://arxiv.org/abs/1705.08551 | id:1705.08551 author:Felix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, Andreas Krause category:stat.ML cs.AI cs.LG cs.SY  published:2017-05-23 summary:Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, to find optimal policies, most reinforcement learning algorithms explore all possible actions, which may be harmful for real-world systems. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this paper, we present a learning algorithm that explicitly considers safety in terms of stability guarantees. Specifically, we extend control theoretic results on Lyapunov stability verification and show how to use statistical models of the dynamics to obtain high-performance control policies with provable stability certificates. Moreover, under additional regularity assumptions in terms of a Gaussian process prior, we prove that one can effectively and safely collect data in order to learn about the dynamics and thus both improve control performance and expand the safe region of the state space. In our experiments, we show how the resulting algorithm can safely optimize a neural network policy on a simulated inverted pendulum, without the pendulum ever falling down. version:1
arxiv-1705-08550 | Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification | http://arxiv.org/abs/1705.08550 | id:1705.08550 author:Wentao Zhu, Qi Lou, Yeeleng Scott Vang, Xiaohui Xie category:cs.CV cs.LG cs.NE  published:2017-05-23 summary:Mammogram classification is directly related to computer-aided diagnosis of breast cancer. Traditional methods rely on regions of interest (ROIs) which require great efforts to annotate. Inspired by the success of using deep convolutional features for natural image analysis and multi-instance learning (MIL) for labeling a set of instances/patches, we propose end-to-end trained deep multi-instance networks for mass classification based on whole mammogram without the aforementioned ROIs. We explore three different schemes to construct deep multi-instance networks for whole mammogram classification. Experimental results on the INbreast dataset demonstrate the robustness of proposed networks compared to previous work using segmentation and detection annotations. version:1
arxiv-1705-07356 | Structural Compression of Convolutional Neural Networks Based on Greedy Filter Pruning | http://arxiv.org/abs/1705.07356 | id:1705.07356 author:Reza Abbasi-Asl, Bin Yu category:cs.CV  published:2017-05-20 summary:Convolutional neural networks (CNNs) have state-of-the-art performance on many problems in machine vision. However, networks with superior performance often have millions of weights so that it is difficult or impossible to use CNNs on computationally limited devices or to humanly interpret them. A myriad of CNN compression approaches have been proposed and they involve pruning and compressing the weights and filters. In this article, we introduce a greedy structural compression scheme that prunes filters in a trained CNN. We define a filter importance index equal to the classification accuracy reduction (CAR) of the network after pruning that filter (similarly defined as RAR for regression). We then iteratively prune filters based on the CAR index. This algorithm achieves substantially higher classification accuracy in AlexNet compared to other structural compression schemes that prune filters. Pruning half of the filters in the first or second layer of AlexNet, our CAR algorithm achieves 26% and 20% higher classification accuracies respectively, compared to the best benchmark filter pruning scheme. Our CAR algorithm, combined with further weight pruning and compressing, reduces the size of first or second convolutional layer in AlexNet by a factor of 42, while achieving close to original classification accuracy through retraining (or fine-tuning) network. Finally, we demonstrate the interpretability of CAR-compressed CNNs by showing that our algorithm prunes filters with visually redundant functionalities. In fact, out of top 20 CAR-pruned filters in AlexNet, 17 of them in the first layer and 14 of them in the second layer are color-selective filters as opposed to shape-selective filters. To our knowledge, this is the first reported result on the connection between compression and interpretability of CNNs. version:2
arxiv-1705-08530 | Statistical Convergence Analysis of Gradient EM on General Gaussian Mixture Models | http://arxiv.org/abs/1705.08530 | id:1705.08530 author:Bowei Yan, Mingzhang Yin, Purnamrita Sarkar category:math.ST cs.LG stat.TH  published:2017-05-23 summary:In this paper, we study convergence properties of the gradient Expectation-Maximization algorithm \cite{lange1995gradient} for Gaussian Mixture Models for general number of clusters and mixing coefficients. We derive the convergence rate depending on the mixing coefficients, minimum and maximum pairwise distances between the true centers and dimensionality and number of components; and obtain a near-optimal local contraction radius. While there have been some recent notable works that derive local convergence rates for EM in the two equal mixture symmetric GMM, in the more general case, the derivations need structurally different and non-trivial arguments. We use recent tools from learning theory and empirical processes to achieve our theoretical results. version:1
arxiv-1705-08525 | Data-driven Random Fourier Features using Stein Effect | http://arxiv.org/abs/1705.08525 | id:1705.08525 author:Wei-Cheng Chang, Chun-Liang Li, Yiming Yang, Barnabas Poczos category:cs.LG stat.ML  published:2017-05-23 summary:Large-scale kernel approximation is an important problem in machine learning research. Approaches using random Fourier features have become increasingly popular [Rahimi and Recht, 2007], where kernel approximation is treated as empirical mean estimation via Monte Carlo (MC) or Quasi-Monte Carlo (QMC) integration [Yang et al., 2014]. A limitation of the current approaches is that all the features receive an equal weight summing to 1. In this paper, we propose a novel shrinkage estimator from "Stein effect", which provides a data-driven weighting strategy for random features and enjoys theoretical justifications in terms of lowering the empirical risk. We further present an efficient randomized algorithm for large-scale applications of the proposed method. Our empirical results on six benchmark data sets demonstrate the advantageous performance of this approach over representative baselines in both kernel approximation and supervised learning tasks. version:1
arxiv-1705-08520 | An effective algorithm for hyperparameter optimization of neural networks | http://arxiv.org/abs/1705.08520 | id:1705.08520 author:Gonzalo Diaz, Achille Fokoue, Giacomo Nannicini, Horst Samulowitz category:cs.AI cs.LG cs.NE  published:2017-05-23 summary:A major challenge in designing neural network (NN) systems is to determine the best structure and parameters for the network given the data for the machine learning problem at hand. Examples of parameters are the number of layers and nodes, the learning rates, and the dropout rates. Typically, these parameters are chosen based on heuristic rules and manually fine-tuned, which may be very time-consuming, because evaluating the performance of a single parametrization of the NN may require several hours. This paper addresses the problem of choosing appropriate parameters for the NN by formulating it as a box-constrained mathematical optimization problem, and applying a derivative-free optimization tool that automatically and effectively searches the parameter space. The optimization tool employs a radial basis function model of the objective function (the prediction accuracy of the NN) to accelerate the discovery of configurations yielding high accuracy. Candidate configurations explored by the algorithm are trained to a small number of epochs, and only the most promising candidates receive full training. The performance of the proposed methodology is assessed on benchmark sets and in the context of predicting drug-drug interactions, showing promising results. The optimization tool used in this paper is open-source. version:1
arxiv-1705-08504 | Interpreting Blackbox Models via Model Extraction | http://arxiv.org/abs/1705.08504 | id:1705.08504 author:Osbert Bastani, Carolyn Kim, Hamsa Bastani category:cs.LG  published:2017-05-23 summary:Interpretability has become an important issue as machine learning is increasingly used to inform consequential decisions. We propose an approach for interpreting a blackbox model by extracting a decision tree that approximates the model. Our model extraction algorithm avoids overfitting by leveraging blackbox model access to actively sample new training points. We prove that as the number of samples goes to infinity, the decision tree learned using our algorithm converges to the exact greedy decision tree. In our evaluation, we use our algorithm to interpret random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for three classical reinforcement learning problems. We show that our algorithm improves over a baseline based on CART on every problem instance. Furthermore, we show how an interpretation generated by our approach can be used to understand and debug these models. version:1
arxiv-1705-08498 | Clinical Intervention Prediction and Understanding using Deep Networks | http://arxiv.org/abs/1705.08498 | id:1705.08498 author:Harini Suresh, Nathan Hunt, Alistair Johnson, Leo Anthony Celi, Peter Szolovits, Marzyeh Ghassemi category:cs.LG  published:2017-05-23 summary:Real-time prediction of clinical interventions remains a challenge within intensive care units (ICUs). This task is complicated by data sources that are noisy, sparse, heterogeneous and outcomes that are imbalanced. In this paper, we integrate data from all available ICU sources (vitals, labs, notes, demographics) and focus on learning rich representations of this data to predict onset and weaning of multiple invasive interventions. In particular, we compare both long short-term memory networks (LSTM) and convolutional neural networks (CNN) for prediction of five intervention tasks: invasive ventilation, non-invasive ventilation, vasopressors, colloid boluses, and crystalloid boluses. Our predictions are done in a forward-facing manner to enable "real-time" performance, and predictions are made with a six hour gap time to support clinically actionable planning. We achieve state-of-the-art results on our predictive tasks using deep architectures. We explore the use of feature occlusion to interpret LSTM models, and compare this to the interpretability gained from examining inputs that maximally activate CNN outputs. We show that our models are able to significantly outperform baselines in intervention prediction, and provide insight into model learning, which is crucial for the adoption of such models in practice. version:1
arxiv-1705-08488 | Second-Order Word Embeddings from Nearest Neighbor Topological Features | http://arxiv.org/abs/1705.08488 | id:1705.08488 author:Denis Newman-Griffis, Eric Fosler-Lussier category:cs.CL cs.AI  published:2017-05-23 summary:We introduce second-order vector representations of words, induced from nearest neighborhood topological features in pre-trained contextual word embeddings. We then analyze the effects of using second-order embeddings as input features in two deep natural language processing models, for named entity recognition and recognizing textual entailment, as well as a linear model for paraphrase recognition. Surprisingly, we find that nearest neighbor information alone is sufficient to capture most of the performance benefits derived from using pre-trained word embeddings. Furthermore, second-order embeddings are able to handle highly heterogeneous data better than first-order representations, though at the cost of some specificity. Additionally, augmenting contextual embeddings with second-order information further improves model performance in some cases. Due to variance in the random initializations of word embeddings, utilizing nearest neighbor features from multiple first-order embedding samples can also contribute to downstream performance gains. Finally, we identify intriguing characteristics of second-order embedding spaces for further research, including much higher density and different semantic interpretations of cosine similarity. version:1
arxiv-1705-08481 | Bayesian Pool-based Active Learning with Abstention Feedbacks | http://arxiv.org/abs/1705.08481 | id:1705.08481 author:Cuong V. Nguyen, Lam Si Tung Ho, Huan Xu, Vu Dinh, Binh Nguyen category:stat.ML cs.LG  published:2017-05-23 summary:We study pool-based active learning with abstention feedbacks, where a labeler can abstain from labeling a queried example. We take a Bayesian approach to the problem and propose a general framework that learns both the target classification problem and the unknown abstention pattern at the same time. As specific instances of the framework, we develop two useful greedy algorithms with theoretical guarantees: they respectively achieve the ${(1-\frac{1}{e})}$ factor approximation of the optimal expected or worst-case value of a useful utility function. Our experiments show the algorithms perform well in various practical scenarios. version:1
arxiv-1705-08480 | Efficiently applying attention to sequential data with the Recurrent Discounted Attention unit | http://arxiv.org/abs/1705.08480 | id:1705.08480 author:Brendan Maginnis, Pierre H. Richemond category:cs.LG  published:2017-05-23 summary:Recurrent Neural Networks architectures excel at processing sequences by modelling dependencies over different timescales. The recently introduced Recurrent Weighted Average (RWA) unit captures long term dependencies far better than an LSTM on several challenging tasks. The RWA achieves this by applying attention to each input and computing a weighted average over the full history of its computations. Unfortunately, the RWA cannot change the attention it has assigned to previous timesteps, and so struggles with carrying out consecutive tasks or tasks with changing requirements. We present the Recurrent Discounted Attention (RDA) unit that builds on the RWA by additionally allowing the discounting of the past. We empirically compare our model to RWA, LSTM and GRU units on several challenging tasks. On tasks with a single output the RWA, RDA and GRU units learn much quicker than the LSTM and with better performance. On the multiple sequence copy task our RDA unit learns the task three times as quickly as the LSTM or GRU units while the RWA fails to learn at all. On the Wikipedia character prediction task the LSTM performs best but it followed closely by our RDA unit. Overall our RDA unit performs well and is sample efficient on a large variety of sequence tasks. version:1
arxiv-1705-08479 | Input Fast-Forwarding for Better Deep Learning | http://arxiv.org/abs/1705.08479 | id:1705.08479 author:Ahmed Ibrahim, A. Lynn Abbott, Mohamed E. Hussein category:cs.CV  published:2017-05-23 summary:This paper introduces a new architectural framework, known as input fast-forwarding, that can enhance the performance of deep networks. The main idea is to incorporate a parallel path that sends representations of input values forward to deeper network layers. This scheme is substantially different from "deep supervision" in which the loss layer is re-introduced to earlier layers. The parallel path provided by fast-forwarding enhances the training process in two ways. First, it enables the individual layers to combine higher-level information (from the standard processing path) with lower-level information (from the fast-forward path). Second, this new architecture reduces the problem of vanishing gradients substantially because the fast-forwarding path provides a shorter route for gradient backpropagation. In order to evaluate the utility of the proposed technique, a Fast-Forward Network (FFNet), with 20 convolutional layers along with parallel fast-forward paths, has been created and tested. The paper presents empirical results that demonstrate improved learning capacity of FFNet due to fast-forwarding, as compared to GoogLeNet (with deep supervision) and CaffeNet, which are 4x and 18x larger in size, respectively. All of the source code and deep learning models described in this paper will be made available to the entire research community version:1
arxiv-1705-08475 | Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation | http://arxiv.org/abs/1705.08475 | id:1705.08475 author:Matthias Hein, Maksym Andriushchenko category:cs.LG cs.AI cs.CV stat.ML  published:2017-05-23 summary:Recent work has shown that state-of-the-art classifiers are quite brittle, in the sense that a small adversarial change of an originally with high confidence correctly classified input leads to a wrong classification again with high confidence. This raises concerns that such classifiers are vulnerable to attacks and calls into question their usage in safety-critical systems. We show in this paper for the first time formal guarantees on the robustness of a classifier by giving instance-specific lower bounds on the norm of the input manipulation required to change the classifier decision. Based on this analysis we propose the Cross-Lipschitz regularization functional. We show that using this form of regularization in kernel methods resp. neural networks improves the robustness of the classifier without any loss in prediction performance. version:1
arxiv-1705-09222 | Towards a Knowledge Graph based Speech Interface | http://arxiv.org/abs/1705.09222 | id:1705.09222 author:Ashwini Jaya Kumar, Sören Auer, Christoph Schmidt, Joachim köhler category:cs.HC cs.CL  published:2017-05-23 summary:Applications which use human speech as an input require a speech interface with high recognition accuracy. The words or phrases in the recognised text are annotated with a machine-understandable meaning and linked to knowledge graphs for further processing by the target application. These semantic annotations of recognised words can be represented as a subject-predicate-object triples which collectively form a graph often referred to as a knowledge graph. This type of knowledge representation facilitates to use speech interfaces with any spoken input application, since the information is represented in logical, semantic form, retrieving and storing can be followed using any web standard query languages. In this work, we develop a methodology for linking speech input to knowledge graphs and study the impact of recognition errors in the overall process. We show that for a corpus with lower WER, the annotation and linking of entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight, a tool to interlink text documents with the linked open data is used to link the speech recognition output to the DBpedia knowledge graph. Such a knowledge-based speech recognition interface is useful for applications such as question answering or spoken dialog systems. version:1
arxiv-1705-08435 | Fast and Differentially Private Algorithms for Decentralized Collaborative Machine Learning | http://arxiv.org/abs/1705.08435 | id:1705.08435 author:Aurélien Bellet, Rachid Guerraoui, Mahsa Taziki, Marc Tommasi category:cs.LG cs.CR cs.DC cs.SY stat.ML  published:2017-05-23 summary:Consider a set of agents in a peer-to-peer communication network, where each agent has a personal dataset and a personal learning objective. The main question addressed in this paper is: how can agents collaborate to improve upon their locally learned model without leaking sensitive information about their data? Our first contribution is to reformulate this problem so that it can be solved by a block coordinate descent algorithm. We obtain an efficient and fully decentralized protocol working in an asynchronous fashion. Our second contribution is to make our algorithm differentially private to protect against the disclosure of any information about personal datasets. We prove convergence rates and exhibit the trade-off between utility and privacy. Our experiments show that our approach dramatically outperforms previous work in the non-private case, and that under privacy constraints we significantly improve over purely local models. version:1
arxiv-1705-08432 | Deep Learning of Grammatically-Interpretable Representations Through Question-Answering | http://arxiv.org/abs/1705.08432 | id:1705.08432 author:Hamid Palangi, Paul Smolensky, Xiaodong He, Li Deng category:cs.CL  published:2017-05-23 summary:We introduce an architecture in which internal representations, learned by end-to-end optimization in a deep neural network performing a textual question-answering task, can be interpreted using basic concepts from linguistic theory. This interpretability comes at a cost of only a few percentage-point reduction in accuracy relative to the original model on which the new one is based (BiDAF [1]). The internal representation that is interpreted is a Tensor Product Representation: for each input word, the model selects a symbol to encode the word, and a role in which to place the symbol, and binds the two together. The selection is via soft attention. The overall interpretation is built from interpretations of the symbols, as recruited by the trained model, and interpretations of the roles as used by the model. We find support for our initial hypothesis that symbols can be interpreted as lexical-semantic word meanings, while roles can be interpreted as approximations of grammatical roles (or categories) such as subject, wh-word, determiner, etc. Through extremely detailed, fine-grained analysis, we find specific correspondences between the learned roles and parts of speech as assigned by a standard parser [2], and find several discrepancies in the model's favor. In this sense, the model learns significant aspects of grammar, after having been exposed solely to linguistically unannotated text, questions, and answers: no prior linguistic knowledge is given to the model. What is given is the means to represent using symbols and roles and an inductive bias favoring use of these in an approximately discrete manner. version:1
arxiv-1705-08430 | Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues | http://arxiv.org/abs/1705.08430 | id:1705.08430 author:Noga Alon, Moshe Babaioff, Yannai A. Gonczarowski, Yishay Mansour, Shay Moran, Amir Yehudayoff category:cs.LG cs.GT  published:2017-05-23 summary:In this work we derive a variant of the classic Glivenko-Cantelli Theorem, which asserts uniform convergence of the empirical Cumulative Distribution Function (CDF) to the CDF of the underlying distribution. Our variant allows for tighter convergence bounds for extreme values of the CDF. We apply our bound in the context of revenue learning, which is a well-studied problem in economics and algorithmic game theory. We derive sample-complexity bounds on the uniform convergence rate of the empirical revenues to the true revenues, assuming a bound on the $k$th moment of the valuations, for any (possibly fractional) $k>1$. For uniform convergence in the limit, we give a complete characterization and a zero-one law: if the first moment of the valuations is finite, then uniform convergence almost surely occurs; conversely, if the first moment is infinite, then uniform convergence almost never occurs. version:1
arxiv-1705-08422 | Continuous State-Space Models for Optimal Sepsis Treatment - a Deep Reinforcement Learning Approach | http://arxiv.org/abs/1705.08422 | id:1705.08422 author:Aniruddh Raghu, Matthieu Komorowski, Leo Anthony Celi, Peter Szolovits, Marzyeh Ghassemi category:cs.LG  published:2017-05-23 summary:Sepsis is a leading cause of mortality in intensive care units (ICUs) and costs hospitals billions annually. Treating a septic patient is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. Understanding more about a patient's physiological state at a given time could hold the key to effective treatment policies. In this work, we propose a new approach to deduce optimal treatment policies for septic patients by using continuous state-space models and deep reinforcement learning. Learning treatment policies over continuous spaces is important, because we retain more of the patient's physiological information. Our model is able to learn clinically interpretable treatment policies, similar in important aspects to the treatment policies of physicians. Evaluating our algorithm on past ICU patient data, we find that our model could reduce patient mortality in the hospital by up to 3.6% over observed clinical policies, from a baseline mortality of 13.7%. The learned treatment policies could be used to aid intensive care clinicians in medical decision making and improve the likelihood of patient survival. version:1
arxiv-1705-08421 | AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions | http://arxiv.org/abs/1705.08421 | id:1705.08421 author:Chunhui Gu, Chen Sun, Sudheendra Vijayanarasimhan, Caroline Pantofaru, David A. Ross, George Toderici, Yeqing Li, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, Jitendra Malik category:cs.CV  published:2017-05-23 summary:This paper introduces a video dataset of spatio-temporally localized Atomic Visual Actions (AVA). The AVA dataset densely annotates 80 atomic visual actions in 64k movie clips with actions localized in space and time, resulting in 197k action labels with multiple labels per human occurring frequently. The main differences with existing video datasets are: (1) the definition of atomic visual actions, which avoids collecting data for each and every complex action; (2) precise spatio-temporal annotations with possibly multiple annotations for each human; (3) the use of diverse, realistic video material (movies). This departs from existing datasets for spatio-temporal action recognition, such as JHMDB and UCF datasets, which provide annotations for at most 24 composite actions, such as basketball dunk, captured in specific environments, i.e., basketball court. We implement a state-of-the-art approach for action localization. Despite this, the performance on our dataset remains low and underscores the need for developing new approaches for video understanding. The AVA dataset is the first step in this direction, and enables the measurement of performance and progress in realistic scenarios. version:1
arxiv-1705-08417 | Reinforcement Learning with a Corrupted Reward Channel | http://arxiv.org/abs/1705.08417 | id:1705.08417 author:Tom Everitt, Victoria Krakovna, Laurent Orseau, Marcus Hutter, Shane Legg category:cs.AI cs.LG stat.ML I.2.6; I.2.8  published:2017-05-23 summary:No real-world reward function is perfect. Sensory errors and software bugs may result in RL agents observing higher (or lower) rewards than they should. For example, a reinforcement learning agent may prefer states where a sensory error gives it the maximum reward, but where the true reward is actually small. We formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards. Two ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be completely managed. Second, by using randomisation to blunt the agent's optimisation, reward corruption can be partially managed under some assumptions. version:1
arxiv-1705-08409 | Ridesourcing Car Detection by Transfer Learning | http://arxiv.org/abs/1705.08409 | id:1705.08409 author:Leye Wang, Xu Geng, Jintao Ke, Chen Peng, Xiaojuan Ma, Daqing Zhang, Qiang Yang category:cs.LG stat.ML  published:2017-05-23 summary:Ridesourcing platforms like Uber and Didi are getting more and more popular around the world. However, unauthorized ridesourcing activities taking advantages of the sharing economy can greatly impair the healthy development of this emerging industry. As the first step to regulate on-demand ride services and eliminate black market, we design a method to detect ridesourcing cars from a pool of cars based on their trajectories. Since licensed ridesourcing car traces are not openly available and may be completely missing in some cities due to legal issues, we turn to transferring knowledge from public transport open data, i.e, taxis and buses, to ridesourcing detection among ordinary vehicles. We propose a two-stage transfer learning framework. In Stage 1, we take taxi and bus data as input to learn a random forest (RF) classifier using trajectory features shared by taxis/buses and ridesourcing/other cars. Then, we use the RF to label all the candidate cars. In Stage 2, leveraging the subset of high confident labels from the previous stage as input, we further learn a convolutional neural network (CNN) classifier for ridesourcing detection, and iteratively refine RF and CNN, as well as the feature set, via a co-training process. Finally, we use the resulting ensemble of RF and CNN to identify the ridesourcing cars in the candidate pool. Experiments on real car, taxi and bus traces show that our transfer learning framework, with no need of a pre-labeled ridesourcing dataset, can achieve similar accuracy as the supervised learning methods. version:1
arxiv-1705-08395 | Continual Learning in Generative Adversarial Nets | http://arxiv.org/abs/1705.08395 | id:1705.08395 author:Ari Seff, Alex Beatson, Daniel Suo, Han Liu category:cs.LG cs.AI stat.ML  published:2017-05-23 summary:Developments in deep generative models have allowed for tractable learning of high-dimensional data distributions. While the employed learning procedures typically assume that training data is drawn i.i.d. from the distribution of interest, it may be desirable to model distinct distributions which are observed sequentially, such as when different classes are encountered over time. Although conditional variations of deep generative models permit multiple distributions to be modeled by a single network in a disentangled fashion, they are susceptible to catastrophic forgetting when the distributions are encountered sequentially. In this paper, we adapt recent work in reducing catastrophic forgetting to the task of training generative adversarial networks on a sequence of distinct distributions, enabling continual generative modeling. version:1
arxiv-1705-07881 | Dynamic Factorization and Partition of Complex Networks | http://arxiv.org/abs/1705.07881 | id:1705.07881 author:Lin F. Yang, Vladimir Braverman, Tuo Zhao, Mengdi Wang category:cs.LG math.OC stat.ML  published:2017-05-22 summary:Finding the reduced-dimensional structure is critical to understanding complex networks. Existing approaches such as spectral clustering are applicable only when the full network is explicitly observed. In this paper, we focus on the online factorization and partition of implicit large-scale networks based on observations from an associated random walk. We propose an efficient and scalable nonconvex stochastic gradient algorithm. It is able to process dependent data dynamically generated by the underlying network and learn a low-dimensional representation for each vertex. By applying a diffusion approximation analysis, we show that the nonconvex stochastic algorithm achieves nearly optimal sample complexity. Once given the learned low-dimensional representations, we further apply clustering techniques to recover the network partition. We show that, when the associated Markov process is lumpable, one can recover the partition exactly with high probability. The proposed approach is experimented on Manhattan taxi data. version:2
arxiv-1705-08391 | Exponential error rates of SDP for block models: Beyond Grothendieck's inequality | http://arxiv.org/abs/1705.08391 | id:1705.08391 author:Yingjie Fei, Yudong Chen category:stat.ML cs.IT cs.SI math.IT math.ST stat.TH  published:2017-05-23 summary:In this paper we consider the cluster estimation problem under the Stochastic Block Model. We show that the semidefinite programming (SDP) formulation for this problem achieves an error rate that decays exponentially in the signal-to-noise ratio. The error bound implies weak recovery in the sparse graph regime with bounded expected degrees, as well as exact recovery in the dense regime. An immediate corollary of our results yields error bounds under the Censored Block Model. Moreover, these error bounds are robust, continuing to hold under heterogeneous edge probabilities and a form of the so-called monotone attack. Significantly, this error rate is achieved by the SDP solution itself without any further pre- or post-processing, and improves upon existing polynomially-decaying error bounds proved using the Grothendieck\textquoteright s inequality. Our analysis has two key ingredients: (i) showing that the graph has a well-behaved spectrum, even in the sparse regime, after discounting an exponentially small number of edges, and (ii) an order-statistics argument that governs the final error rate. Both arguments highlight the implicit regularization effect of the SDP formulation. version:1
arxiv-1705-08378 | Detecting Adversarial Examples in Deep Networks with Adaptive Noise Reduction | http://arxiv.org/abs/1705.08378 | id:1705.08378 author:Bin Liang, Hongcheng Li, Miaoqiang Su, Xirong Li, Wenchang Shi, Xiaofeng Wang category:cs.CR cs.LG  published:2017-05-23 summary:Deep neural networks (DNNs) play a key role in many applications. Unsurprisingly, they also became a potential attack target of adversaries. Some studies have demonstrated DNN classifiers can be fooled by the adversarial example, which is crafted via introducing some perturbations into an original sample. Accordingly, some powerful defense techniques were proposed against adversarial examples. However, existing defense techniques require modifying the target model or depend on the prior knowledge of attack techniques to different degrees. In this paper, we propose a straightforward method for detecting adversarial image examples. It doesn't require any prior knowledge of attack techniques and can be directly deployed into unmodified off-the-shelf DNN models. Specifically, we consider the perturbation to images as a kind of noise and introduce two classical image processing techniques, scalar quantization and smoothing spatial filter, to reduce its effect. The image two-dimensional entropy is employed as a metric to implement an adaptive noise reduction for different kinds of images. As a result, the adversarial example can be effectively detected by comparing the classification results of a given sample and its denoised version. Thousands of adversarial examples against some state-of-the-art DNN models are used to evaluate the proposed method, which are crafted with different attack techniques. The experiment shows that our detection method can achieve an overall recall of 93.73% and an overall precision of 95.45% without referring to any prior knowledge of attack techniques. version:1
arxiv-1705-08374 | Classification of Aerial Photogrammetric 3D Point Clouds | http://arxiv.org/abs/1705.08374 | id:1705.08374 author:Carlos Becker, Nicolai Häni, Elena Rosinskaya, Emmanuel d'Angelo, Christoph Strecha category:cs.CV  published:2017-05-23 summary:We present a powerful method to extract per-point semantic class labels from aerialphotogrammetry data. Labeling this kind of data is important for tasks such as environmental modelling, object classification and scene understanding. Unlike previous point cloud classification methods that rely exclusively on geometric features, we show that incorporating color information yields a significant increase in accuracy in detecting semantic classes. We test our classification method on three real-world photogrammetry datasets that were generated with Pix4Dmapper Pro, and with varying point densities. We show that off-the-shelf machine learning techniques coupled with our new features allow us to train highly accurate classifiers that generalize well to unseen data, processing point clouds containing 10 million points in less than 3 minutes on a desktop computer. version:1
arxiv-1705-05108 | Kernel Truncated Regression Representation for Robust Subspace Clustering | http://arxiv.org/abs/1705.05108 | id:1705.05108 author:Liangli Zhen, Dezhong Peng, Xin Yao category:cs.CV cs.AI  published:2017-05-15 summary:Subspace clustering aims to group data points into multiple clusters of which each corresponds to one subspace. Most existing subspace clustering methods assume that the data could be linearly represented with each other in the input space. In practice, however, this assumption is hard to be satisfied. To achieve nonlinear subspace clustering, we propose a novel method which consists of the following three steps: 1) projecting the data into a hidden space in which the data can be linearly reconstructed from each other; 2) calculating the globally linear reconstruction coefficients in the kernel space; 3) truncating the trivial coefficients to achieve robustness and block-diagonality, and then achieving clustering by solving a graph Laplacian problem. Our method has the advantages of a closed-form solution and capacity of clustering data points that lie in nonlinear subspaces. The first advantage makes our method efficient in handling large-scale data sets, and the second one enables the proposed method to address the nonlinear subspace clustering challenge. Extensive experiments on five real-world datasets demonstrate the effectiveness and the efficiency of the proposed method in comparison with ten state-of-the-art approaches regarding four evaluation metrics. version:2
arxiv-1705-08314 | Improvements to Frank-Wolfe optimization for multi-detector multi-object tracking | http://arxiv.org/abs/1705.08314 | id:1705.08314 author:Roberto Henschel, Laura Leal-Taixé, Daniel Cremers, Bodo Rosenhahn category:cs.CV  published:2017-05-23 summary:This paper proposes a novel formulation for the multi-object tracking-by-detection paradigm for two (or more) input detectors. Using full-body and heads detections, the fusion helps to recover heavily occluded persons and to reduce false positives. The assignment of the two input features to a person and the extraction of the trajectories is commonly solved from one binary quadratic program (BQP). Due to the computational complexity of the NP-hard QP, we approximate the solution using the Frank-Wolfe algorithm. We propose several improvements to this solver affecting better minimization and shorter computations, compared to off-the-shelf BQP-solvers and the standard Frank-Wolfe algorithm. Evaluation on pedestrian tracking is provided for multiple scenarios, showing improved tracking quality over single input feature trackers and standard QP-solvers. Finally we present the performance of our tracker on the challenging \MOTNEW benchmark, being comparable to state-of-the-art trackers. version:1
arxiv-1705-08292 | The Marginal Value of Adaptive Gradient Methods in Machine Learning | http://arxiv.org/abs/1705.08292 | id:1705.08292 author:Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, Benjamin Recht category:stat.ML cs.LG  published:2017-05-23 summary:Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks. version:1
arxiv-1705-07761 | VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning | http://arxiv.org/abs/1705.07761 | id:1705.07761 author:Akash Srivastava, Lazar Valkov, Chris Russell, Michael Gutmann, Charles Sutton category:stat.ML  published:2017-05-22 summary:Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples. version:2
arxiv-1705-08280 | How hard can it be? Estimating the difficulty of visual search in an image | http://arxiv.org/abs/1705.08280 | id:1705.08280 author:Radu Tudor Ionescu, Bogdan Alexe, Marius Leordeanu, Marius Popescu, Dim P. Papadopoulos, Vittorio Ferrari category:cs.CV  published:2017-05-23 summary:We address the problem of estimating image difficulty defined as the human response time for solving a visual search task. We collect human annotations of image difficulty for the PASCAL VOC 2012 data set through a crowd-sourcing platform. We then analyze what human interpretable image properties can have an impact on visual search difficulty, and how accurate are those properties for predicting difficulty. Next, we build a regression model based on deep features learned with state of the art convolutional neural networks and show better results for predicting the ground-truth visual search difficulty scores produced by human annotators. Our model is able to correctly rank about 75% image pairs according to their difficulty score. We also show that our difficulty predictor generalizes well to new classes not seen during training. Finally, we demonstrate that our predicted difficulty scores are useful for weakly supervised object localization (8% improvement) and semi-supervised object classification (1% improvement). version:1
arxiv-1705-07062 | MRI-PET Registration with Automated Algorithm in Pre-clinical Studies | http://arxiv.org/abs/1705.07062 | id:1705.07062 author:Nathanael L. Baisa, Stéphanie Bricq, Alain Lalande category:cs.CV  published:2017-05-19 summary:Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET) automatic 3-D registration is implemented and validated for small animal image volumes so that the high-resolution anatomical MRI information can be fused with the low spatial resolution of functional PET information for the localization of lesion that is currently in high demand in the study of tumor of cancer (oncology) and its corresponding preparation of pharmaceutical drugs. Though many registration algorithms are developed and applied on human brain volumes, these methods may not be as efficient on small animal datasets due to lack of intensity information and often the high anisotropy in voxel dimensions. Therefore, a fully automatic registration algorithm which can register not only assumably rigid small animal volumes such as brain but also deformable organs such as kidney, cardiac and chest is developed using a combination of global affine and local B-spline transformation models in which mutual information is used as a similarity criterion. The global affine registration uses a multi-resolution pyramid on image volumes of 3 levels whereas in local B-spline registration, a multi-resolution scheme is applied on the B-spline grid of 2 levels on the finest resolution of the image volumes in which only the transform itself is affected rather than the image volumes. Since mutual information lacks sufficient spatial information, PCA is used to inject it by estimating initial translation and rotation parameters. It is computationally efficient since it is implemented using C++ and ITK library, and is qualitatively and quantitatively shown that this PCA-initialized global registration followed by local registration is in close agreement with expert manual registration and outperforms the one without PCA initialization tested on small animal brain and kidney. version:2
arxiv-1705-07485 | Shake-Shake regularization | http://arxiv.org/abs/1705.07485 | id:1705.07485 author:Xavier Gastaldi category:cs.LG cs.CV  published:2017-05-21 summary:The method introduced in this paper aims at helping deep learning practitioners faced with an overfit problem. The idea is to replace, in a multi-branch network, the standard summation of parallel branches with a stochastic affine combination. Applied to 3-branch residual networks, shake-shake regularization improves on the best single shot published results on CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%. Experiments on architectures without skip connections or Batch Normalization show encouraging results and open the door to a large set of applications. Code is available at https://github.com/xgastaldi/shake-shake version:2
arxiv-1705-08236 | 3D Convolutional Neural Networks for Brain Tumor Segmentation: A Comparison of Multi-resolution Architectures | http://arxiv.org/abs/1705.08236 | id:1705.08236 author:Adrià Casamitjana, Santi Puch, Asier Aduriz, Verónica Vilaplana category:stat.ML  published:2017-05-23 summary:This paper analyzes the use of 3D Convolutional Neural Networks for brain tumor segmentation in MR images. We address the problem using three different architectures that combine fine and coarse features to obtain the final segmentation. We compare three different networks that use multi-resolution features in terms of both design and performance and we show that they improve their single-resolution counterparts. version:1
arxiv-1705-08214 | Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks | http://arxiv.org/abs/1705.08214 | id:1705.08214 author:Michael Gygli category:cs.CV cs.MM  published:2017-05-23 summary:Shot boundary detection (SBD) is an important component of many video analysis tasks, such as action recognition, video indexing, summarization and editing. Previous work typically used a combination of low-level features like color histograms, in conjunction with simple models such as SVMs. Instead, we propose to learn shot detection end-to-end, from pixels to final shot boundaries. For training such a model, we rely on our insight that all shot boundaries are generated. Thus, we create a dataset with one million frames and automatically generated transitions such as cuts, dissolves and fades. In order to efficiently analyze hours of videos, we propose a Convolutional Neural Network (CNN) which is fully convolutional in time, thus allowing to use a large temporal context without the need to repeatedly processing frames. With this architecture our method obtains state-of-the-art results while running at an unprecedented speed of more than 120x real-time. version:1
arxiv-1705-08209 | Unbiasing Truncated Backpropagation Through Time | http://arxiv.org/abs/1705.08209 | id:1705.08209 author:Corentin Tallec, Yann Ollivier category:cs.NE cs.LG  published:2017-05-23 summary:Truncated Backpropagation Through Time (truncated BPTT) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of Backpropagation Through Time (BPTT) while relieving the need for a complete backtrack through the whole data sequence at every step. However, truncation favors short-term dependencies: the gradient estimate of truncated BPTT is biased, so that it does not benefit from the convergence guarantees from stochastic gradient theory. We introduce Anticipated Reweighted Truncated Backpropagation (ARTBP), an algorithm that keeps the computational benefits of truncated BPTT, while providing unbiasedness. ARTBP works by using variable truncation lengths together with carefully chosen compensation factors in the backpropagation equation. We check the viability of ARTBP on two tasks. First, a simple synthetic task where careful balancing of temporal dependencies at different scales is needed: truncated BPTT displays unreliable performance, and in worst case scenarios, divergence, while ARTBP converges reliably. Second, on Penn Treebank character-level language modelling, ARTBP slightly outperforms truncated BPTT. version:1
arxiv-1705-08207 | Salient Object Detection with Semantic Priors | http://arxiv.org/abs/1705.08207 | id:1705.08207 author:Tam V. Nguyen, Luoqi Liu category:cs.CV  published:2017-05-23 summary:Salient object detection has increasingly become a popular topic in cognitive and computational sciences, including computer vision and artificial intelligence research. In this paper, we propose integrating \textit{semantic priors} into the salient object detection process. Our algorithm consists of three basic steps. Firstly, the explicit saliency map is obtained based on the semantic segmentation refined by the explicit saliency priors learned from the data. Next, the implicit saliency map is computed based on a trained model which maps the implicit saliency priors embedded into regional features with the saliency values. Finally, the explicit semantic map and the implicit map are adaptively fused to form a pixel-accurate saliency map which uniformly covers the objects of interest. We further evaluate the proposed framework on two challenging datasets, namely, ECSSD and HKUIS. The extensive experimental results demonstrate that our method outperforms other state-of-the-art methods. version:1
arxiv-1705-07556 | Boosting the accuracy of multi-spectral image pan-sharpening by learning a deep residual network | http://arxiv.org/abs/1705.07556 | id:1705.07556 author:Yancong Wei, Qiangqiang Yuan, Huanfeng Shen, Liangpei Zhang category:cs.CV  published:2017-05-22 summary:In the field of fusing multi-spectral and panchromatic images (Pan-sharpening), the impressive effectiveness of deep neural networks has been recently employed to overcome the drawbacks of traditional linear models and boost the fusing accuracy. However, to the best of our knowledge, existing research works are mainly based on simple and flat networks with relatively shallow architecture, which severely limited their performances. In this paper, the concept of residual learning has been introduced to form a very deep convolutional neural network to make a full use of the high non-linearity of deep learning models. By both quantitative and visual assessments on a large number of high quality multi-spectral images from various sources, it has been supported that our proposed model is superior to all mainstream algorithms included in the comparison, and achieved the highest spatial-spectral unified accuracy. version:2
arxiv-1705-08197 | Learning to Succeed while Teaching to Fail: Privacy in Closed Machine Learning Systems | http://arxiv.org/abs/1705.08197 | id:1705.08197 author:Jure Sokolic, Qiang Qiu, Miguel R. D. Rodrigues, Guillermo Sapiro category:stat.ML cs.LG  published:2017-05-23 summary:Security, privacy, and fairness have become critical in the era of data science and machine learning. More and more we see that achieving universally secure, private, and fair systems is practically impossible. We have seen for example how generative adversarial networks can be used to learn about the expected private training data; how the exploitation of additional data can reveal private information in the original one; and how what looks like unrelated features can teach us about each other. Confronted with this challenge, in this paper we open a new line of research, where the security, privacy, and fairness is learned and used in a closed environment. The goal is to ensure that a given entity (e.g., the company or the government), trusted to infer certain information with our data, is blocked from inferring protected information from it. For example, a hospital might be allowed to produce diagnosis on the patient (the positive task), without being able to infer the gender of the subject (negative task). Similarly, a company can guarantee that internally it is not using the provided data for any undesired task, an important goal that is not contradicting the virtually impossible challenge of blocking everybody from the undesired task. We design a system that learns to succeed on the positive task while simultaneously fail at the negative one, and illustrate this with challenging cases where the positive task is actually harder than the negative one being blocked. Fairness, to the information in the negative task, is often automatically obtained as a result of this proposed approach. The particular framework and examples open the door to security, privacy, and fairness in very important closed scenarios, ranging from private data accumulation companies like social networks to law-enforcement and hospitals. version:1
arxiv-1705-08184 | Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions | http://arxiv.org/abs/1705.08184 | id:1705.08184 author:Aryeh Kontorovich, Sivan Sabato, Roi Weiss category:cs.LG math.ST stat.TH  published:2017-05-23 summary:We examine the Bayes-consistency of a recently proposed 1-nearest-neighbor-based multiclass learning algorithm. This algorithm is derived from sample compression bounds and enjoys the statistical advantages of tight, fully empirical generalization bounds, as well as the algorithmic advantages of runtime and memory savings. We prove that this algorithm is strongly Bayes-consistent in metric spaces with finite doubling dimension --- the first consistency result for an efficient nearest-neighbor sample compression scheme. Rather surprisingly, we discover that this algorithm continues to be Bayes-consistent even in a certain infinite-dimensional setting, in which the basic measure-theoretic conditions on which classic consistency proofs hinge are violated. This is all the more surprising, since it is known that k-NN is not Bayes-consistent in this setting. We pose several challenging open problems for future research. version:1
arxiv-1705-08182 | Unmasking the abnormal events in video | http://arxiv.org/abs/1705.08182 | id:1705.08182 author:Radu Tudor Ionescu, Sorina Smeureanu, Bogdan Alexe, Marius Popescu category:cs.CV  published:2017-05-23 summary:We propose a novel framework for abnormal event detection in video that requires no training sequences. Our framework is based on unmasking, a technique previously used for authorship verification in text documents, which we adapt to our task. We iteratively train a binary classifier to distinguish between two consecutive video sequences while removing at each step the most discriminant features. Higher training accuracy rates of the intermediately obtained classifiers represent abnormal events. To the best of our knowledge, this is the first work to apply unmasking for a computer vision task. We compare our method with several state-of-the-art supervised and unsupervised methods on four benchmark data sets. The empirical results indicate that our abnormal event detection framework can achieve state-of-the-art results, while running in real-time at 20 frames per second. version:1
arxiv-1705-08180 | Correlation Alignment by Riemannian Metric for Domain Adaptation | http://arxiv.org/abs/1705.08180 | id:1705.08180 author:Pietro Morerio, Vittorio Murino category:cs.CV  published:2017-05-23 summary:Domain adaptation techniques address the problem of reducing the sensitivity of machine learning methods to the so-called domain shift, namely the difference between source (training) and target (test) data distributions. In particular, unsupervised domain adaptation assumes no labels are available in the target domain. To this end, aligning second order statistics (covariances) of target and source domains have proven to be an effective approach ti fill the gap between the domains. However, covariance matrices do not form a subspace of the Euclidean space, but live in a Riemannian manifold with non-positive curvature, making the usual Euclidean metric suboptimal to measure distances. In this paper, we extend the idea of training a neural network with a constraint on the covariances of the hidden layer features, by rigorously accounting for the curved structure of the manifold of symmetric positive definite matrices. The resulting loss function exploits a theoretically sound geodesic distance on such manifold. Results show indeed the suboptimal nature of the Euclidean distance. This makes us able to perform better than previous approaches on the standard Office dataset, a benchmark for domain adaptation techniques. version:1
arxiv-1705-08168 | Look, Listen and Learn | http://arxiv.org/abs/1705.08168 | id:1705.08168 author:Relja Arandjelović, Andrew Zisserman category:cs.CV cs.LG  published:2017-05-23 summary:We consider the question: what can be learnt by looking at and listening to a large amount of unlabelled videos? There is a valuable, but so far untapped, source of information contained in the video itself -- the correspondence between the visual and the audio streams, and we introduce a novel "Audio-Visual Correspondence" learning task that makes use of this. Training visual and audio networks from scratch, without any additional supervision other than the raw unconstrained videos themselves, is shown to successfully solve this task, and, more interestingly, result in good vision and audio representations. These features set the new state-of-the-art on two sound classification benchmarks, and perform on par with the state-of-the-art self-supervised approaches on ImageNet classification. We also demonstrate that the network is able to localize objects in both modalities, as well as perform fine-grained recognition tasks. version:1
arxiv-1705-08153 | Visualizing LSTM decisions | http://arxiv.org/abs/1705.08153 | id:1705.08153 author:Jos van der Westhuizen, Joan Lasenby category:stat.ML cs.LG  published:2017-05-23 summary:Long Short-Term Memory (LSTM) recurrent neural networks are renowned for being uninterpretable "black boxes". In the medical domain where LSTMs have shown promise, this is specifically concerning because it is imperative to understand the decisions made by machine learning models in such acute situations. This study employs techniques used in the Convolutional Neural Network domain to elucidate the operations that LSTMs perform on time series. The visualization techniques include input saliency by means of occlusion and derivatives, class mode visualization, and temporal outputs. Moreover, we demonstrate that LSTMs appear to extract features similar to those extracted by wavelets. It was found that deriving the inputs for saliency is a poor approximation and occlusion is a better approach. Moreover, analyzing LSTMs on different sets of data provide novel interpretations. version:1
arxiv-1705-08142 | Sluice networks: Learning what to share between loosely related tasks | http://arxiv.org/abs/1705.08142 | id:1705.08142 author:Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, Anders Søgaard category:stat.ML cs.AI cs.CL cs.LG cs.NE  published:2017-05-23 summary:Multi-task learning is partly motivated by the observation that humans bring to bear what they know about related problems when solving new ones. Similarly, deep neural networks can profit from related tasks by sharing parameters with other networks. However, humans do not consciously decide to transfer knowledge between tasks (and are typically not aware of the transfer). In machine learning, it is hard to estimate if sharing will lead to improvements; especially if tasks are only loosely related. To overcome this, we introduce Sluice Networks, a general framework for multi-task learning where trainable parameters control the amount of sharing -- including which parts of the models to share. Our framework goes beyond and generalizes over previous proposals in enabling hard or soft sharing of all combinations of subspaces, layers, and skip connections. We perform experiments on three task pairs from natural language processing, and across seven different domains, using data from OntoNotes 5.0, and achieve up to 15% average error reductions over common approaches to multi-task learning. We analyze when the architecture is particularly helpful, as well as its ability to fit noise. We show that a) label entropy is predictive of gains in sluice networks, confirming findings for hard parameter sharing, and b) while sluice networks easily fit noise, they are robust across domains in practice. version:1
arxiv-1705-08131 | Black-Box Attacks against RNN based Malware Detection Algorithms | http://arxiv.org/abs/1705.08131 | id:1705.08131 author:Weiwei Hu, Ying Tan category:cs.LG cs.CR  published:2017-05-23 summary:Recent researches have shown that machine learning based malware detection algorithms are very vulnerable under the attacks of adversarial examples. These works mainly focused on the detection algorithms which use features with fixed dimension, while some researchers have begun to use recurrent neural networks (RNN) to detect malware based on sequential API features. This paper proposes a novel algorithm to generate sequential adversarial examples, which are used to attack a RNN based malware detection system. It is usually hard for malicious attackers to know the exact structures and weights of the victim RNN. A substitute RNN is trained to approximate the victim RNN. Then we propose a generative RNN to output sequential adversarial examples from the original sequential malware inputs. Experimental results showed that RNN based malware detection algorithms fail to detect most of the generated malicious adversarial examples, which means the proposed model is able to effectively bypass the detection algorithms. version:1
arxiv-1705-08118 | Consistent Multitask Learning with Nonlinear Output Relations | http://arxiv.org/abs/1705.08118 | id:1705.08118 author:Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco, Massimiliano Pontil category:cs.LG stat.ML  published:2017-05-23 summary:Key to multitask learning is exploiting relationships between different tasks to improve prediction performance. If the relations are linear, regularization approaches can be used successfully. However, in practice assuming the tasks to be linearly related might be restrictive, and allowing for nonlinear structures is a challenge. In this paper, we tackle this issue by casting the problem within the framework of structured prediction. Our main contribution is a novel algorithm for learning multiple tasks which are related by a system of nonlinear equations that their joint outputs need to satisfy. We show that the algorithm is consistent and can be efficiently implemented. Experimental results show the potential of the proposed method. version:1
arxiv-1705-08110 | Semi-Bandits with Knapsacks | http://arxiv.org/abs/1705.08110 | id:1705.08110 author:Karthik Abinav Sankararaman, Aleksandrs Slivkins category:cs.LG  published:2017-05-23 summary:This paper unifies two lines of work on multi-armed bandits, Bandits with Knapsacks (BwK) and semi-bandits. The former concerns scenarios with limited "resources" consumed by the algorithm, e.g., limited inventory in a dynamic pricing problem. The latter has a huge number of actions, but there is combinatorial structure and additional feedback which makes the problem tractable. Both lines of work has received considerable recent attention, and are supported by numerous application examples. We define a common generalization, and design a general algorithm for this model. Our regret rates are comparable with those for BwK and semi-bandits in general, and essentially optimal for important special cases. version:1
arxiv-1705-00703 | Submodular Trajectory Optimization for Aerial 3D Scanning | http://arxiv.org/abs/1705.00703 | id:1705.00703 author:Mike Roberts, Debadeepta Dey, Anh Truong, Sudipta Sinha, Shital Shah, Ashish Kapoor, Pat Hanrahan, Neel Joshi category:cs.CV  published:2017-05-01 summary:Drones equipped with cameras have become a powerful tool for large-scale aerial 3D scanning, but existing automatic flight planners do not exploit all available information about the scene, and can therefore produce inaccurate and incomplete 3D models. We present an automatic method to generate drone trajectories, such that the imagery acquired during the flight will later produce a high-fidelity 3D model. Our method uses a coarse estimate of the scene geometry to plan camera trajectories that: (1) cover the scene as thoroughly as possible; (2) encourage observations of scene geometry from a diverse set of viewing angles; (3) avoid obstacles; and (4) respect a user-specified flight time budget. Our method relies on a mathematical model of scene coverage that exhibits an intuitive diminishing returns property known as submodularity. We leverage this property extensively to design a trajectory planning algorithm that reasons globally about the non-additive coverage reward obtained across a trajectory, jointly with the cost of traveling between views. We evaluate our method by using it to scan three large outdoor scenes, and we perform a quantitative evaluation using a photorealistic video game simulator. version:2
arxiv-1705-08101 | Towards seamless multi-view scene analysis from satellite to street-level | http://arxiv.org/abs/1705.08101 | id:1705.08101 author:Sébastien Lefèvre, Devis Tuia, Jan Dirk Wegner, Timothée Produit, Ahmed Samy Nassar category:cs.CV  published:2017-05-23 summary:In this paper, we discuss and review how combined multi-view imagery from satellite to street-level can benefit scene analysis. Numerous works exist that merge information from remote sensing and images acquired from the ground for tasks like land cover mapping, object detection, or scene understanding. What makes the combination of overhead and street-level images challenging, is the strongly varying viewpoint, different scale, illumination, sensor modality and time of acquisition. Direct (dense) matching of images on a per-pixel basis is thus often impossible, and one has to resort to alternative strategies that will be discussed in this paper. We review recent works that attempt to combine images taken from the ground and overhead views for purposes like scene registration, reconstruction, or classification. Three methods that represent the wide range of potential methods and applications (change detection, image orientation, and tree cataloging) are described in detail. We show that cross-fertilization between remote sensing, computer vision and machine learning is very valuable to make the best of geographic data available from Earth Observation sensors and ground imagery. Despite its challenges, we believe that integrating these complementary data sources will lead to major breakthroughs in Big GeoData. version:1
arxiv-1705-08094 | TwiInsight: Discovering Topics and Sentiments from Social Media Datasets | http://arxiv.org/abs/1705.08094 | id:1705.08094 author:Zhengkui Wang, Guangdong Bai, Soumyadeb Chowdhury, Quanqing Xu, Zhi Lin Seow category:cs.IR cs.CL  published:2017-05-23 summary:Social media platforms contain a great wealth of information which provides opportunities for us to explore hidden patterns or unknown correlations, and understand people's satisfaction with what they are discussing. As one showcase, in this paper, we present a system, TwiInsight which explores the insight of Twitter data. Different from other Twitter analysis systems, TwiInsight automatically extracts the popular topics under different categories (e.g., healthcare, food, technology, sports and transport) discussed in Twitter via topic modeling and also identifies the correlated topics across different categories. Additionally, it also discovers the people's opinions on the tweets and topics via the sentiment analysis. The system also employs an intuitive and informative visualization to show the uncovered insight. Furthermore, we also develop and compare six most popular algorithms - three for sentiment analysis and three for topic modeling. version:1
arxiv-1704-04886 | Multi-View Image Generation from a Single-View | http://arxiv.org/abs/1704.04886 | id:1704.04886 author:Bo Zhao, Xiao Wu, Zhi-Qi Cheng, Hao Liu, Jiashi Feng category:cs.CV cs.MM  published:2017-04-17 summary:This paper addresses a challenging problem -- how to generate multi-view cloth images from only a single view input. To generate realistic-looking images with different views from the input, we propose a new image generation model termed VariGANs that combines the strengths of the variational inference and the Generative Adversarial Networks (GANs). Our proposed VariGANs model generates the target image in a coarse-to-fine manner instead of a single pass which suffers from severe artifacts. It first performs variational inference to model global appearance of the object (e.g., shape and color) and produce a coarse image with a different view. Conditioned on the generated low resolution images, it then proceeds to perform adversarial learning to fill details and generate images of consistent details with the input. Extensive experiments conducted on two clothing datasets, MVC and DeepFashion, have demonstrated that images of a novel view generated by our model are more plausible than those generated by existing approaches, in terms of more consistent global appearance as well as richer and sharper details. version:2
arxiv-1705-08091 | Local Monotonic Attention Mechanism for End-to-End Speech Recognition | http://arxiv.org/abs/1705.08091 | id:1705.08091 author:Andros Tjandra, Sakriani Sakti, Satoshi Nakamura category:cs.CL  published:2017-05-23 summary:Recently, sequence-to-sequence model by using encoder-decoder neural network has gained popularity for automatic speech recognition (ASR). The architecture commonly uses an attentional mechanism which allows the model to learn alignments between source speech sequence and target text sequence. Most attentional mechanisms used today is based on a global attention property which requires a computation of a weighted summarization of the whole input sequence generated by encoder states. However, it is computationally expensive and often produces misalignment on the longer input sequence. Furthermore, it does not fit with monotonous or left-to-right nature in speech recognition task. In this paper, we propose a novel attention mechanism that has local and monotonic properties. Various ways to control those properties are also explored. Experimental results demonstrate that encoder-decoder based ASR with local monotonic attention could achieve significant performance improvements and reduce the computational complexity in comparison with the one that used the standard global attention architecture. version:1
arxiv-1705-08086 | Universal Style Transfer via Feature Transforms | http://arxiv.org/abs/1705.08086 | id:1705.08086 author:Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang category:cs.CV  published:2017-05-23 summary:Universal style transfer aims to transfer any arbitrary visual styles to content images. Existing feed-forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. In this paper, we present a simple yet effective method that tackles these limitations without training on any pre-defined styles. The key ingredient of our method is a pair of feature transforms, whitening and coloring, that are embedded to an image reconstruction network. The whitening and coloring transforms reflect a direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high-quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures via simple feature coloring. version:1
arxiv-1705-08080 | Visual Semantic Planning using Deep Successor Representations | http://arxiv.org/abs/1705.08080 | id:1705.08080 author:Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, Ali Farhadi category:cs.CV cs.LG cs.RO  published:2017-05-23 summary:A crucial capability of real-world intelligent agents is their ability to plan a sequence of actions to achieve their goals in the visual world. In this work, we address the problem of visual semantic planning: the task of predicting a sequence of actions from visual observations that transform a dynamic environment from an initial state to a goal state. Doing so entails knowledge about objects and their affordances, as well as actions and their preconditions and effects. We propose learning these through interacting with a visual and dynamic environment. Our proposed solution involves bootstrapping reinforcement learning with imitation learning. To ensure cross-task generalization, we develop a deep predictive model based on successor representations. Our experimental results show near optimal results across a wide range of tasks in the challenging THOR environment. The supplementary video can be accessed at the following link: https://goo.gl/vXsbQP version:1
arxiv-1705-08079 | Effective injury prediction in professional soccer with GPS data and machine learning | http://arxiv.org/abs/1705.08079 | id:1705.08079 author:Alessio Rossi, Luca Pappalardo, Paolo Cintia, Marcello Iaia, Javier Fernandez, Daniel Medina category:stat.ML stat.AP 62-07 H.2.8  published:2017-05-23 summary:Injuries have a great impact on professional soccer, due to their large influence on team performance and the considerable costs of rehabilitation for players. Existing studies in the literature provide just a preliminary understanding of which factors mostly affect injury risk, while an evaluation of the potential of statistical models in forecasting injuries is still missing. In this paper, we propose a multidimensional approach to injury prediction in professional soccer which is based on GPS measurements and machine learning. By using GPS tracking technology, we collect data describing the training workload of players in a professional soccer club during a season. We show that our injury predictors are both accurate and interpretable by providing a set of case studies of interest to soccer practitioners. Our approach opens a novel perspective on injury prevention, providing a set of simple and practical rules for evaluating and interpreting the complex relations between injury risk and training performance in professional soccer. version:1
arxiv-1705-08078 | Patchnet: Interpretable Neural Networks for Image Classification | http://arxiv.org/abs/1705.08078 | id:1705.08078 author:Adityanarayanan Radhakrishnan, Charles Durham, Ali Soylemezoglu, Caroline Uhler category:cs.CV  published:2017-05-23 summary:The ability to visually understand and interpret learned features from complex predictive models is crucial for their acceptance in sensitive areas such as health care. To move closer to this goal of truly interpretable complex models, we present PatchNet, a network that restricts global context for image classification tasks in order to easily provide visual representations of learned texture features on a predetermined local scale. We demonstrate how PatchNet provides visual heatmap representations of the learned features, and we mathematically analyze the behavior of the network during convergence. We also present a version of PatchNet that is particularly well suited for lowering false positive rates in image classification tasks. We apply PatchNet to the classification of textures from the Describable Textures Dataset and to the ISBI-ISIC 2016 melanoma classification challenge. version:1
arxiv-1705-08066 | Multiple Images Recovery Using a Single Affine Transformation | http://arxiv.org/abs/1705.08066 | id:1705.08066 author:Bo Jiang, Chris Ding, Bin Luo category:cs.CV  published:2017-05-23 summary:In many real-world applications, image data often come with noises, corruptions or large errors. One approach to deal with noise image data is to use data recovery techniques which aim to recover the true uncorrupted signals from the observed noise images. In this paper, we first introduce a novel corruption recovery transformation (CRT) model which aims to recover multiple (or a collection of) corrupted images using a single affine transformation. Then, we show that the introduced CRT can be efficiently constructed through learning from training data. Once CRT is learned, we can recover the true signals from the new incoming/test corrupted images explicitly. As an application, we apply our CRT to image recognition task. Experimental results on six image datasets demonstrate that the proposed CRT model is effective in recovering noise image data and thus leads to better recognition results. version:1
arxiv-1705-08063 | Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge | http://arxiv.org/abs/1705.08063 | id:1705.08063 author:Arman Cohan, Nazli Goharian category:cs.CL cs.IR  published:2017-05-23 summary:Citation texts are sometimes not very informative or in some cases inaccurate by themselves; they need the appropriate context from the referenced paper to reflect its exact contributions. To address this problem, we propose an unsupervised model that uses distributed representation of words as well as domain knowledge to extract the appropriate context from the reference paper. Evaluation results show the effectiveness of our model by significantly outperforming the state-of-the-art. We furthermore demonstrate how an effective contextualization method results in improving citation-based summarization of the scientific articles. version:1
arxiv-1705-08061 | A divide and conquer method for symbolic regression | http://arxiv.org/abs/1705.08061 | id:1705.08061 author:Changtong Luo, Chen Chen, Zonglin Jiang category:cs.NE  published:2017-05-23 summary:Symbolic regression aims to find a function that best explains the relationship between independent variables and the objective value based on a given set of sample data. Genetic programming (GP) is usually considered as an appropriate method for the problem since it can optimize functional structure and coefficients simultaneously. However, the convergence speed of GP might be too slow for large scale problems that involve a large number of variables. Fortunately, in many applications, the target function is separable or partially separable. This feature motivated us to design a new method, divide and conquer (D&C), for symbolic regression, in which the target function is divided into a number of sub-functions and the sub-functions are then determined by any GP algorithms available. The separability is probed by a new proposed technique, Bi-Correlation test (BiCT). D&C powered GP has been tested on some real-world applications, and the study shows that D&C can help GP to get the target function more rapidly and more reliably. version:1
arxiv-1705-08056 | Ambiguity set and learning via Bregman and Wasserstein | http://arxiv.org/abs/1705.08056 | id:1705.08056 author:Xin Guo, Johnny Hong, Nan Yang category:stat.ML cs.LG  published:2017-05-23 summary:Construction of ambiguity set in robust optimization relies on the choice of divergences between probability distributions. In distribution learning, choosing appropriate probability distributions based on observed data is critical for approximating the true distribution. To improve the performance of machine learning models, there has recently been interest in designing objective functions based on Lp-Wasserstein distance rather than the classical Kullback-Leibler (KL) divergence. In this paper, we derive concentration and asymptotic results using Bregman divergence. We propose a novel asymmetric statistical divergence called Wasserstein-Bregman divergence as a generalization of L2-Wasserstein distance. We discuss how these results can be applied to the construction of ambiguity set in robust optimization. version:1
arxiv-1705-08052 | Compressing Recurrent Neural Network with Tensor Train | http://arxiv.org/abs/1705.08052 | id:1705.08052 author:Andros Tjandra, Sakriani Sakti, Satoshi Nakamura category:cs.LG  published:2017-05-23 summary:Recurrent Neural Network (RNN) are a popular choice for modeling temporal and sequential tasks and achieve many state-of-the-art performance on various complex problems. However, most of the state-of-the-art RNNs have millions of parameters and require many computational resources for training and predicting new data. This paper proposes an alternative RNN model to reduce the number of parameters significantly by representing the weight parameters based on Tensor Train (TT) format. In this paper, we implement the TT-format representation for several RNN architectures such as simple RNN and Gated Recurrent Unit (GRU). We compare and evaluate our proposed RNN model with uncompressed RNN model on sequence classification and sequence prediction tasks. Our proposed RNNs with TT-format are able to preserve the performance while reducing the number of RNN parameters significantly up to 40 times smaller. version:1
arxiv-1705-08051 | Wasserstein Learning of Deep Generative Point Process Models | http://arxiv.org/abs/1705.08051 | id:1705.08051 author:Shuai Xiao, Mehrdad Farajtabar, Xiaojing Ye, Junchi Yan, Le Song, Hongyuan Zha category:cs.LG stat.ML  published:2017-05-23 summary:Point processes are becoming very popular in modeling asynchronous sequential data due to their sound mathematical foundation and strength in modeling a variety of real-world phenomena. Currently, they are often characterized via intensity function which limits model's expressiveness due to unrealistic assumptions on its parametric form used in practice. Furthermore, they are learned via maximum likelihood approach which is prone to failure in multi-modal distributions of sequences. In this paper, we propose an intensity-free approach for point processes modeling that transforms nuisance processes to a target one. Furthermore, we train the model using a likelihood-free leveraging Wasserstein distance between point processes. Experiments on various synthetic and real-world data substantiate the superiority of the proposed point process model over conventional ones. version:1
arxiv-1705-04724 | Person Re-Identification by Deep Joint Learning of Multi-Loss Classification | http://arxiv.org/abs/1705.04724 | id:1705.04724 author:Wei Li, Xiatian Zhu, Shaogang Gong category:cs.CV cs.AI  published:2017-05-12 summary:Existing person re-identification (re-id) methods rely mostly on either localised or global feature representation alone. This ignores their joint benefit and mutual complementary effects. In this work, we show the advantages of jointly learning local and global features in a Convolutional Neural Network (CNN) by aiming to discover correlated local and global features in different context. Specifically, we formulate a method for joint learning of local and global feature selection losses designed to optimise person re-id when using only generic matching metrics such as the L2 distance. We design a novel CNN architecture for Jointly Learning Multi-Loss (JLML) of local and global discriminative feature optimisation subject concurrently to the same re-id labelled information. Extensive comparative evaluations demonstrate the advantages of this new JLML model for person re-id over a wide range of state-of-the-art re-id methods on five benchmarks (VIPeR, GRID, CUHK01, CUHK03, Market-1501). version:2
arxiv-1705-08049 | Neural Network Memory Architectures for Autonomous Robot Navigation | http://arxiv.org/abs/1705.08049 | id:1705.08049 author:Steven W Chen, Nikolay Atanasov, Arbaaz Khan, Konstantinos Karydis, Daniel D. Lee, Vijay Kumar category:cs.RO cs.LG  published:2017-05-23 summary:This paper highlights the significance of including memory structures in neural networks when the latter are used to learn perception-action loops for autonomous robot navigation. Traditional navigation approaches rely on global maps of the environment to overcome cul-de-sacs and plan feasible motions. Yet, maintaining an accurate global map may be challenging in real-world settings. A possible way to mitigate this limitation is to use learning techniques that forgo hand-engineered map representations and infer appropriate control responses directly from sensed information. An important but unexplored aspect of such approaches is the effect of memory on their performance. This work is a first thorough study of memory structures for deep-neural-network-based robot navigation, and offers novel tools to train such networks from supervision and quantify their ability to generalize to unseen scenarios. We analyze the separation and generalization abilities of feedforward, long short-term memory, and differentiable neural computer networks. We introduce a new method to evaluate the generalization ability by estimating the VC-dimension of networks with a final linear readout layer. We validate that the VC estimates are good predictors of actual test performance. The reported method can be applied to deep learning problems beyond robotics. version:1
arxiv-1705-08044 | Detection Algorithms for Communication Systems Using Deep Learning | http://arxiv.org/abs/1705.08044 | id:1705.08044 author:Nariman Farsad, Andrea Goldsmith category:cs.LG cs.AI cs.ET  published:2017-05-22 summary:The design and analysis of communication systems typically rely on the development of mathematical models that describe the underlying communication channel, which dictates the relationship between the transmitted and the received signals. However, in some systems, such as molecular communication systems where chemical signals are used for transfer of information, it is not possible to accurately model this relationship. In these scenarios, because of the lack of mathematical channel models, a completely new approach to design and analysis is required. In this work, we focus on one important aspect of communication systems, the detection algorithms, and demonstrate that by borrowing tools from deep learning, it is possible to train detectors that perform well, without any knowledge of the underlying channel models. We evaluate these algorithms using experimental data that is collected by a chemical communication platform, where the channel model is unknown and difficult to model analytically. We show that deep learning algorithms perform significantly better than a simple detector that was used in previous works, which also did not assume any knowledge of the channel. version:1
arxiv-1705-08041 | Unrolled Optimization with Deep Priors | http://arxiv.org/abs/1705.08041 | id:1705.08041 author:Steven Diamond, Vincent Sitzmann, Felix Heide, Gordon Wetzstein category:cs.CV  published:2017-05-22 summary:A broad class of problems at the core of computational imaging, sensing, and low-level computer vision reduces to the inverse problem of extracting latent images that follow a prior distribution, from measurements taken under a known physical image formation model. Traditionally, hand-crafted priors along with iterative optimization methods have been used to solve such problems. In this paper we present unrolled optimization with deep priors, a principled framework for infusing knowledge of the image formation into deep networks that solve inverse problems in imaging, inspired by classical iterative methods. We show that instances of the framework outperform the state-of-the-art by a substantial margin for a wide variety of imaging problems, such as denoising, deblurring, and compressed sensing magnetic resonance imaging (MRI). Moreover, we conduct experiments that explain how the framework is best used and why it outperforms previous methods. version:1
arxiv-1705-08038 | Latent Human Traits in the Language of Social Media: An Open-Vocabulary Approach | http://arxiv.org/abs/1705.08038 | id:1705.08038 author:Vivek Kulkarni, Margaret L. Kern, David Stillwell, Michal Kosinski, Sandra Matz, Lyle Ungar, Steven Skiena, H. Andrew Schwartz category:cs.CL  published:2017-05-22 summary:Over the past century, personality theory and research has successfully identified core sets of characteristics that consistently describe and explain fundamental differences in the way people think, feel and behave. Such characteristics were derived through theory, dictionary analyses, and survey research using explicit self-reports. The availability of social media data spanning millions of users now makes it possible to automatically derive characteristics from language use -- at large scale. Taking advantage of linguistic information available through Facebook, we study the process of inferring a new set of potential human traits based on unprompted language use. We subject these new traits to a comprehensive set of evaluations and compare them with a popular five factor model of personality. We find that our language-based trait construct is often more generalizable in that it often predicts non-questionnaire-based outcomes better than questionnaire-based traits (e.g. entities someone likes, income and intelligence quotient), while the factors remain nearly as stable as traditional factors. Our approach suggests a value in new constructs of personality derived from everyday human language use. version:1
arxiv-1705-07967 | On the consistency between model selection and link prediction in networks | http://arxiv.org/abs/1705.07967 | id:1705.07967 author:Toni Vallès-Català, Tiago P. Peixoto, Roger Guimerà, Marta Sales-Pardo category:stat.ML cond-mat.dis-nn cond-mat.stat-mech  published:2017-05-22 summary:A principled approach to understand network structures is to formulate generative models. Given a collection of models, however, an outstanding key task is to determine which one provides a more accurate description of the network at hand, discounting statistical fluctuations. This problem can be approached using two principled criteria that at first may seem equivalent: selecting the most plausible model in terms of its posterior probability; or selecting the model with the highest predictive performance in terms of identifying missing links. Here we show that while these two approaches yield consistent results in most of cases, there are also notable instances where they do not, that is, where the most plausible model is not the most predictive. We show that in the latter case the improvement of predictive performance can in fact lead to overfitting both in artificial and empirical settings. Furthermore, we show that, in general, the predictive performance is higher when we average over collections of models that are individually less plausible, than when we consider only the single most plausible model. version:1
arxiv-1106-0730 | Rademacher complexity of stationary sequences | http://arxiv.org/abs/1106.0730 | id:1106.0730 author:Daniel J. McDonald, Cosma Rohilla Shalizi category:stat.ML cs.LG  published:2011-06-03 summary:We show how to control the generalization error of time series models wherein past values of the outcome are used to predict future values. The results are based on a generalization of standard i.i.d. concentration inequalities to dependent data without the mixing assumptions common in the time series setting. Our proof and the result are simpler than previous analyses with dependent data or stochastic adversaries which use sequential Rademacher complexities rather than the expected Rademacher complexity for i.i.d. processes. We also derive empirical Rademacher results without mixing assumptions resulting in fully calculable upper bounds. version:2
arxiv-1705-08030 | Parallel Stochastic Gradient Descent with Sound Combiners | http://arxiv.org/abs/1705.08030 | id:1705.08030 author:Saeed Maleki, Madanlal Musuvathi, Todd Mytkowicz category:cs.LG stat.ML  published:2017-05-22 summary:Stochastic gradient descent (SGD) is a well known method for regression and classification tasks. However, it is an inherently sequential algorithm at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing linear learners using SGD, such as HOGWILD! and ALLREDUCE, do not honor these dependencies across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SYMSGD, a parallel SGD algorithm that, to a first-order approximation, retains the sequential semantics of SGD. Each thread learns a local model in addition to a model combiner, which allows local models to be combined to produce the same result as what a sequential SGD would have produced. This paper evaluates SYMSGD's accuracy and performance on 6 datasets on a shared-memory machine shows upto 11x speedup over our heavily optimized sequential baseline on 16 cores and 2.2x, on average, faster than HOGWILD!. version:1
arxiv-1705-08018 | Use of Knowledge Graph in Rescoring the N-Best List in Automatic Speech Recognition | http://arxiv.org/abs/1705.08018 | id:1705.08018 author:Ashwini Jaya Kumar, Camilo Morales, Maria-Esther Vidal, Christoph Schmidt, Sören Auer category:cs.CL  published:2017-05-22 summary:With the evolution of neural network based methods, automatic speech recognition (ASR) field has been advanced to a level where building an application with speech interface is a reality. In spite of these advances, building a real-time speech recogniser faces several problems such as low recognition accuracy, domain constraint, and out-of-vocabulary words. The low recognition accuracy problem is addressed by improving the acoustic model, language model, decoder and by rescoring the N-best list at the output of the decoder. We are considering the N-best list rescoring approach to improve the recognition accuracy. Most of the methods in the literature use the grammatical, lexical, syntactic and semantic connection between the words in a recognised sentence as a feature to rescore. In this paper, we have tried to see the semantic relatedness between the words in a sentence to rescore the N-best list. Semantic relatedness is computed using TransE~\cite{bordes2013translating}, a method for low dimensional embedding of a triple in a knowledge graph. The novelty of the paper is the application of semantic web to automatic speech recognition. version:1
arxiv-1705-08016 | Training with Confusion for Fine-Grained Visual Classification | http://arxiv.org/abs/1705.08016 | id:1705.08016 author:Abhimanyu Dubey, Otkrist Gupta, Pei Guo, Ramesh Raskar, Ryan Farrell, Nikhil Naik category:cs.CV  published:2017-05-22 summary:Research in Fine-Grained Visual Classification has focused on tackling the variations in pose, lighting, and viewpoint using sophisticated localization and segmentation techniques, and the usage of robust texture features to improve performance. In this work, we look at the fundamental optimization of neural network training for fine-grained classification tasks with minimal inter-class variance, and attempt to learn features with increased generalization to prevent overfitting. We introduce Training-with-Confusion, an optimization procedure for fine-grained classification tasks that regularizes training by introducing confusion in activations. Our method can be generalized to any fine-tuning task; it is robust to the presence of small training sets and label noise; and adds no overhead to the prediction time. We find that Training-with-Confusion improves the state-of-the-art on all major fine-grained classification datasets. version:1
arxiv-1705-08014 | Training Deep Convolutional Neural Networks with Resistive Cross-Point Devices | http://arxiv.org/abs/1705.08014 | id:1705.08014 author:Tayfun Gokmen, O. Murat Onen, Wilfried Haensch category:cs.LG cs.NE stat.ML  published:2017-05-22 summary:In a previous work we have detailed the requirements to obtain a maximal performance benefit by implementing fully connected deep neural networks (DNN) in form of arrays of resistive devices for deep learning. This concept of Resistive Processing Unit (RPU) devices we extend here towards convolutional neural networks (CNNs). We show how to map the convolutional layers to RPU arrays such that the parallelism of the hardware can be fully utilized in all three cycles of the backpropagation algorithm. We find that the noise and bound limitations imposed due to analog nature of the computations performed on the arrays effect the training accuracy of the CNNs. Noise and bound management techniques are presented that mitigate these problems without introducing any additional complexity in the analog circuits and can be addressed by the digital circuits. In addition, we discuss digitally programmable update management and device variability reduction techniques that can be used selectively for some of the layers in a CNN. We show that combination of all those techniques enables a successful application of the RPU concept for training CNNs. The techniques discussed here are more general and can be applied beyond CNN architectures and therefore enables applicability of RPU approach for large class of neural network architectures. version:1
arxiv-1705-08011 | Convergence Analysis of Batch Normalization for Deep Neural Nets | http://arxiv.org/abs/1705.08011 | id:1705.08011 author:Yintai Ma, Diego Klabjan category:cs.LG  published:2017-05-22 summary:Batch normalization (BN) is very effective in accelerating the convergence of a neural network training phase that it has become a common practice. We propose a generalization of BN, the diminishing batch normalization (DBN) algorithm. We provide an analysis of the convergence of the DBN algorithm that converges to a stationary point with respect to trainable parameters. We analyze a two layer model with linear activation. The main challenge of the analysis is the fact that some parameters are updated by gradient while others are not. In the numerical experiments, we use models with more layers and ReLU activation. We observe that DBN outperforms the original BN algorithm on MNIST, NI and CIFAR-10 datasets with reasonable complex FNN and CNN models. version:1
arxiv-1705-08006 | Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding | http://arxiv.org/abs/1705.08006 | id:1705.08006 author:Mainak Jas, Tom Dupré La Tour, Umut Şimşekli, Alexandre Gramfort category:stat.ML q-bio.NC stat.AP  published:2017-05-22 summary:Neural time-series data contain a wide variety of prototypical signal waveforms (atoms) that are of significant importance in clinical and cognitive research. One of the goals for analyzing such data is hence to extract such 'shift-invariant' atoms. Even though some success has been reported with existing algorithms, they are limited in applicability due to their heuristic nature. Moreover, they are often vulnerable to artifacts and impulsive noise, which are typically present in raw neural recordings. In this study, we address these issues and propose a novel probabilistic convolutional sparse coding (CSC) model for learning shift-invariant atoms from raw neural signals containing potentially severe artifacts. In the core of our model, which we call $\alpha$CSC, lies a family of heavy-tailed distributions called $\alpha$-stable distributions. We develop a novel, computationally efficient Monte Carlo expectation-maximization algorithm for inference. The maximization step boils down to a weighted CSC problem, for which we develop a computationally efficient optimization algorithm. Our results show that the proposed algorithm achieves state-of-the-art convergence speeds. Besides, $\alpha$CSC is significantly more robust to artifacts when compared to three competing algorithms: it can extract spike bursts, oscillations, and even reveal more subtle phenomena such as cross-frequency coupling when applied to noisy neural time series. version:1
arxiv-1705-07999 | GP-Unet: Lesion Detection from Weak Labels with a 3D Regression Network | http://arxiv.org/abs/1705.07999 | id:1705.07999 author:Florian Dubost, Gerda Bortsova, Hieab Adams, Arfan Ikram, Wiro Niessen, Meike Vernooij, Marleen De Bruijne category:cs.CV  published:2017-05-22 summary:We propose a novel convolutional neural network for lesion detection from weak labels. Only a single, global label per image - the lesion count - is needed for training. We train a regression network with a fully convolutional architecture combined with a global pooling layer to aggregate the 3D output into a scalar indicating the lesion count. When testing on unseen images, we first run the network to estimate the number of lesions. Then we remove the global pooling layer to compute localization maps of the size of the input image. We evaluate the proposed network on the detection of enlarged perivascular spaces in the basal ganglia in MRI. Our method achieves a sensitivity of 62% with on average 1.5 false positives per image. Compared with four other approaches based on intensity thresholding, saliency and class maps, our method has a 20% higher sensitivity. version:1
arxiv-1705-07972 | Universal 3D Wearable Fingerprint Targets: Advancing Fingerprint Reader Evaluations | http://arxiv.org/abs/1705.07972 | id:1705.07972 author:Joshua J. Engelsma, Sunpreet S. Arora, Anil K. Jain, Nicholas G. Paulter Jr category:cs.CV  published:2017-05-22 summary:We present the design and manufacturing of high fidelity universal 3D fingerprint targets, which can be imaged on a variety of fingerprint sensing technologies, namely capacitive, contact-optical, and contactless-optical. Universal 3D fingerprint targets enable, for the first time, not only a repeatable and controlled evaluation of fingerprint readers, but also the ability to conduct fingerprint reader interoperability studies. Fingerprint reader interoperability refers to how robust fingerprint recognition systems are to variations in the images acquired by different types of fingerprint readers. To build universal 3D fingerprint targets, we adopt a molding and casting framework consisting of (i) digital mapping of fingerprint images to a negative mold, (ii) CAD modeling a scaffolding system to hold the negative mold, (iii) fabricating the mold and scaffolding system with a high resolution 3D printer, (iv) producing or mixing a material with similar electrical, optical, and mechanical properties to that of the human finger, and (v) fabricating a 3D fingerprint target using controlled casting. Our experiments conducted with PIV and Appendix F certified optical (contact and contactless) and capacitive fingerprint readers demonstrate the usefulness of universal 3D fingerprint targets for controlled and repeatable fingerprint reader evaluations and also fingerprint reader interoperability studies. version:1
arxiv-1705-07962 | pix2code: Generating Code from a Graphical User Interface Screenshot | http://arxiv.org/abs/1705.07962 | id:1705.07962 author:Tony Beltramelli category:cs.LG cs.AI cs.CL cs.CV cs.NE 68T45  published:2017-05-22 summary:Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites and mobile applications. In this paper, we show that Deep Learning techniques can be leveraged to automatically generate code given a graphical user interface screenshot as input. Our model is able to generate code targeting three different platforms (i.e. iOS, Android and web-based technologies) from a single input image with over 77% of accuracy. version:1
arxiv-1705-07957 | Large Scale Empirical Risk Minimization via Truncated Adaptive Newton Method | http://arxiv.org/abs/1705.07957 | id:1705.07957 author:Mark Eisen, Aryan Mokhtari, Alejandro Ribeiro category:math.OC cs.LG stat.ML  published:2017-05-22 summary:We consider large scale empirical risk minimization (ERM) problems, where both the problem dimension and variable size is large. In these cases, most second order methods are infeasible due to the high cost in both computing the Hessian over all samples and computing its inverse in high dimensions. In this paper, we propose a novel adaptive sample size second-order method, which reduces the cost of computing the Hessian by solving a sequence of ERM problems corresponding to a subset of samples and lowers the cost of computing the Hessian inverse using a truncated eigenvalue decomposition. We show that while we geometrically increase the size of the training set at each stage, a single iteration of the truncated Newton method is sufficient to solve the new ERM within its statistical accuracy. Moreover, for a large number of samples we are allowed to double the size of the training set at each stage, and the proposed method subsequently reaches the statistical accuracy of the full training set approximately after two effective passes. In addition to this theoretical result, we show empirically on a number of well known data sets that the proposed truncated adaptive sample size algorithm outperforms stochastic alternatives for solving ERM problems. version:1
arxiv-1704-07433 | Active Bias: Training More Accurate Neural Networks by Emphasizing High Variance Samples | http://arxiv.org/abs/1704.07433 | id:1704.07433 author:Haw-Shiuan Chang, Erik Learned-Miller, Andrew McCallum category:stat.ML cs.LG  published:2017-04-24 summary:Self-paced learning and hard example mining re-weight training instances to improve learning accuracy. This paper presents two improved alternatives based on lightweight estimates of sample uncertainty in stochastic gradient descent (SGD): the variance in predicted probability of the correct class across iterations of mini-batch SGD, and the proximity of the correct class probability to the decision threshold. Extensive experimental results on six datasets show that our methods reliably improve accuracy in various network architectures, including additional gains on top of other popular training techniques, such as residual learning, momentum, ADAM, batch normalization, dropout, and distillation. version:2
arxiv-1705-07904 | Semantically Decomposing the Latent Spaces of Generative Adversarial Networks | http://arxiv.org/abs/1705.07904 | id:1705.07904 author:Chris Donahue, Akshay Balsubramani, Julian McAuley, Zachary C. Lipton category:cs.LG cs.AI cs.CV cs.NE stat.ML  published:2017-05-22 summary:We propose a new algorithm for training generative adversarial networks to jointly learn latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). In practice, this means that by fixing the identity portion of latent codes, we can generate diverse images of the same subject, and by fixing the observation portion we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce images that are both photorealistic, distinct, and appear to depict the same person. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to accommodate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm's ability to generate convincing, identity-matched photographs. version:1
arxiv-1705-07884 | Facial Affect Estimation in the Wild Using Deep Residual and Convolutional Networks | http://arxiv.org/abs/1705.07884 | id:1705.07884 author:Behzad Hasani, Mohammad H. Mahoor category:cs.CV  published:2017-05-22 summary:Automated affective computing in the wild is a challenging task in the field of computer vision. This paper presents three neural network-based methods proposed for the task of facial affect estimation submitted to the First Affect-in-the-Wild challenge. These methods are based on Inception-ResNet modules redesigned specifically for the task of facial affect estimation. These methods are: Shallow Inception-ResNet, Deep Inception-ResNet, and Inception-ResNet with LSTMs. These networks extract facial features in different scales and simultaneously estimate both the valence and arousal in each frame. Root Mean Square Error (RMSE) rates of 0.4 and 0.3 are achieved for the valence and arousal respectively with corresponding Concordance Correlation Coefficient (CCC) rates of 0.04 and 0.29 using Deep Inception-ResNet method. version:1
arxiv-1705-07880 | Reducing Reparameterization Gradient Variance | http://arxiv.org/abs/1705.07880 | id:1705.07880 author:Andrew C. Miller, Nicholas J. Foti, Alexander D'Amour, Ryan P. Adams category:stat.ML stat.CO stat.ME  published:2017-05-22 summary:Optimization with noisy gradients has become ubiquitous in statistics and machine learning. Reparameterization gradients, or gradient estimates computed via the "reparameterization trick," represent a class of noisy gradients often used in Monte Carlo variational inference (MCVI). However, when these gradient estimators are too noisy, the optimization procedure can be slow or fail to converge. One way to reduce noise is to use more samples for the gradient estimate, but this can be computationally expensive. Instead, we view the noisy gradient as a random variable, and form an inexpensive approximation of the generating procedure for the gradient sample. This approximation has high correlation with the noisy gradient by construction, making it a useful control variate for variance reduction. We demonstrate our approach on non-conjugate multi-level hierarchical models and a Bayesian neural net where we observed gradient variance reductions of multiple orders of magnitude (20-2,000x). version:1
arxiv-1705-07874 | A unified approach to interpreting model predictions | http://arxiv.org/abs/1705.07874 | id:1705.07874 author:Scott Lundberg, Su-In Lee category:cs.AI cs.LG stat.ML  published:2017-05-22 summary:Understanding why a model made a certain prediction is crucial in many applications. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, such as ensemble or deep learning models. This creates a tension between accuracy and interpretability. In response, a variety of methods have recently been proposed to help users interpret the predictions of complex models. Here, we present a unified framework for interpreting predictions, namely SHAP (SHapley Additive exPlanations, which assigns each feature an importance for a particular prediction. The key novel components of the SHAP framework are the identification of a class of additive feature importance measures and theoretical results that there is a unique solution in this class with a set of desired properties. This class unifies six existing methods, and several recent methods in this class do not have these desired properties. This means that our framework can inform the development of new methods for explaining prediction models. We demonstrate that several new methods we presented in this paper based on the SHAP framework show better computational performance and better consistency with human intuition than existing methods. version:1
arxiv-1705-07871 | Facial Expression Recognition Using Enhanced Deep 3D Convolutional Neural Networks | http://arxiv.org/abs/1705.07871 | id:1705.07871 author:Behzad Hasani, Mohammad H. Mahoor category:cs.CV  published:2017-05-22 summary:Deep Neural Networks (DNNs) have shown to outperform traditional methods in various visual recognition tasks including Facial Expression Recognition (FER). In spite of efforts made to improve the accuracy of FER systems using DNN, existing methods still are not generalizable enough in practical applications. This paper proposes a 3D Convolutional Neural Network method for FER in videos. This new network architecture consists of 3D Inception-ResNet layers followed by an LSTM unit that together extracts the spatial relations within facial images as well as the temporal relations between different frames in the video. Facial landmark points are also used as inputs to our network which emphasize on the importance of facial components rather than the facial regions that may not contribute significantly to generating facial expressions. Our proposed method is evaluated using four publicly available databases in subject-independent and cross-database tasks and outperforms state-of-the-art methods. version:1
arxiv-1705-07867 | SmartPaste: Learning to Adapt Source Code | http://arxiv.org/abs/1705.07867 | id:1705.07867 author:Miltiadis Allamanis, Marc Brockschmidt category:cs.LG cs.SE  published:2017-05-22 summary:Deep Neural Networks have been shown to succeed at a range of natural language tasks such as machine translation and text summarization. While tasks on source code (ie, formal languages) have been considered recently, most work in this area does not attempt to capitalize on the unique opportunities offered by its known syntax and structure. In this work, we introduce SmartPaste, a first task that requires to use such information. The task is a variant of the program repair problem that requires to adapt a given (pasted) snippet of code to surrounding, existing source code. As first solutions, we design a set of deep neural models that learn to represent the context of each variable location and variable usage in a data flow-sensitive way. Our evaluation suggests that our models can learn to solve the SmartPaste task in many cases, achieving 58.6% accuracy, while learning meaningful representation of variable usages. version:1
arxiv-1705-07860 | On-the-fly Operation Batching in Dynamic Computation Graphs | http://arxiv.org/abs/1705.07860 | id:1705.07860 author:Graham Neubig, Yoav Goldberg, Chris Dyer category:cs.LG cs.CL stat.ML  published:2017-05-22 summary:Dynamic neural network toolkits such as PyTorch, DyNet, and Chainer offer more flexibility for implementing models that cope with data of varying dimensions and structure, relative to toolkits that operate on statically declared computations (e.g., TensorFlow, CNTK, and Theano). However, existing toolkits - both static and dynamic - require that the developer organize the computations into the batches necessary for exploiting high-performance algorithms and hardware. This batching task is generally difficult, but it becomes a major hurdle as architectures become complex. In this paper, we present an algorithm, and its implementation in the DyNet toolkit, for automatically batching operations. Developers simply write minibatch computations as aggregations of single instance computations, and the batching algorithm seamlessly executes them, on the fly, using computationally efficient batched operations. On a variety of tasks, we obtain throughput similar to that obtained with manual batches, as well as comparable speedups over single-instance learning on architectures that are impractical to batch manually. version:1
arxiv-1705-07857 | Real Time Image Saliency for Black Box Classifiers | http://arxiv.org/abs/1705.07857 | id:1705.07857 author:Piotr Dabkowski, Yarin Gal category:stat.ML  published:2017-05-22 summary:In this work we develop a fast saliency detection method that can be applied to any differentiable image classifier. We train a masking model to manipulate the scores of the classifier by masking salient parts of the input image. Our model generalises well to unseen images and requires a single forward pass to perform saliency detection, therefore suitable for use in real-time systems. We test our approach on CIFAR-10 and ImageNet datasets and show that the produced saliency maps are easily interpretable, sharp, and free of artifacts. We suggest a new metric for saliency and test our method on the ImageNet object localisation task. We achieve results outperforming other weakly supervised methods. version:1
arxiv-1705-07853 | Nonparametric Online Regression while Learning the Metric | http://arxiv.org/abs/1705.07853 | id:1705.07853 author:Ilja Kuzborskij, Nicolò Cesa-Bianchi category:cs.LG  published:2017-05-22 summary:We study algorithms for online nonparametric regression that learn the directions along which the regression function is smoother. Our algorithm learns the Mahalanobis metric based on the gradient outer product matrix $\boldsymbol{G}$ of the regression function (automatically adapting to the effective rank of this matrix), while simultaneously bounding the regret ---on the same data sequence--- in terms of the spectrum of $\boldsymbol{G}$. As a preliminary step in our analysis, we generalize a nonparametric online learning algorithm by Hazan and Megiddo by enabling it to compete against functions whose Lipschitzness is measured with respect to an arbitrary Mahalanobis metric. version:1
arxiv-1705-07837 | Size Matters: Cardinality-Constrained Clustering and Outlier Detection via Conic Optimization | http://arxiv.org/abs/1705.07837 | id:1705.07837 author:Napat Rujeerapaiboon, Kilian Schindler, Daniel Kuhn, Wolfram Wiesemann category:math.OC stat.ML 90C22  90C05  62H30  published:2017-05-22 summary:Plain vanilla K-means clustering is prone to produce unbalanced clusters and suffers from outlier sensitivity. To mitigate both shortcomings, we formulate a joint outlier-detection and clustering problem, which assigns a prescribed number of datapoints to an auxiliary outlier cluster and performs cardinality-constrained K-means clustering on the residual dataset. We cast this problem as a mixed-integer linear program (MILP) that admits tractable semidefinite and linear programming relaxations. We propose deterministic rounding schemes that transform the relaxed solutions to high quality solutions for the MILP. We prove that these solutions are optimal in the MILP if a cluster separation condition holds. To our best knowledge, we propose the first tractable solution scheme for the joint outlier-detection and clustering problem with optimality guarantees. version:1
arxiv-1705-07832 | Concrete Dropout | http://arxiv.org/abs/1705.07832 | id:1705.07832 author:Yarin Gal, Jiri Hron, Alex Kendall category:stat.ML  published:2017-05-22 summary:Dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning (RL) tasks. But to obtain well-calibrated uncertainty estimates, a grid-search over the dropout probabilities is necessary - a prohibitive operation with large models, and an impossible one with RL. We propose a new dropout variant which gives improved performance and better calibrated uncertainties. Relying on recent developments in Bayesian deep learning, we use a continuous relaxation of dropout's discrete masks. Together with a principled optimisation objective, this allows for automatic tuning of the dropout probability in large models, and as a result faster experimentation cycles. In RL this allows the agent to adapt its uncertainty dynamically as more data is observed. We analyse the proposed variant extensively on a range of tasks, and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers. version:1
arxiv-1705-07831 | Stabilizing GAN Training with Multiple Random Projections | http://arxiv.org/abs/1705.07831 | id:1705.07831 author:Behnam Neyshabur, Srinadh Bhojanapalli, Ayan Chakrabarti category:cs.LG cs.CV  published:2017-05-22 summary:Training generative adversarial networks is unstable in high-dimensions when the true data distribution lies on a lower-dimensional manifold. The discriminator is then easily able to separate nearly all generated samples leaving the generator without meaningful gradients. We propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. We show that individual discriminators then provide stable gradients to the generator, and that the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator. version:1
arxiv-1705-07830 | Ask the Right Questions: Active Question Reformulation with Reinforcement Learning | http://arxiv.org/abs/1705.07830 | id:1705.07830 author:Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Andrea Gesmundo, Neil Houlsby, Wojciech Gajewski, Wei Wang category:cs.CL cs.AI  published:2017-05-22 summary:We propose an active question answering agent that learns to reformulate questions and combine evidence to improve question answering. The agent sits between the user and a black box question-answering system and learns to optimally probe the system with natural language reformulations of the initial question and to aggregate the evidence to return the best possible answer. The system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. Our agent improves F1 by 11% over a state-of-the-art base model that uses the original question/answer pairs. version:1
arxiv-1705-07819 | Regularizing deep networks using efficient layerwise adversarial training | http://arxiv.org/abs/1705.07819 | id:1705.07819 author:Swami Sankaranarayanan, Arpit Jain, Rama Chellappa, Ser Nam Lim category:cs.CV cs.LG stat.ML  published:2017-05-22 summary:Adversarial training has been shown to regularize deep neural networks in addition to increasing their robustness to adversarial examples. However, its impact on very deep state of the art networks has not been fully investigated. In this paper, we present an efficient approach to perform adversarial training by perturbing intermediate layer activations and study the use of such perturbations as a regularizer during training. We use these perturbations to train very deep models such as ResNets and show improvement in performance both on adversarial and original test data. Our experiments highlight the benefits of perturbing intermediate layer activations compared to perturbing only the inputs. The results on CIFAR-10 and CIFAR-100 datasets show the merits of the proposed adversarial training approach. Additional results on WideResNets show that our approach provides significant improvement in classification accuracy for a given base model, outperforming dropout and other base models of larger size. version:1
arxiv-1705-07818 | TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation | http://arxiv.org/abs/1705.07818 | id:1705.07818 author:Li Ding, Chenliang Xu category:cs.CV  published:2017-05-22 summary:Action segmentation as a milestone towards building automatic systems to understand untrimmed videos has received considerable attention in the recent years. It is typically being modeled as a sequence labeling problem but contains intrinsic and sufficient differences than text parsing or speech processing. In this paper, we introduce a novel hybrid temporal convolutional and recurrent network (TricorNet), which has an encoder-decoder architecture: the encoder consists of a hierarchy of temporal convolutional kernels that capture the local motion changes of different actions; the decoder is a hierarchy of recurrent neural networks that are able to learn and memorize long-term action dependencies after the encoding stage. Our model is simple but extremely effective in terms of video sequence labeling. The experimental results on three public action segmentation datasets have shown that the proposed model achieves superior performance over the state of the art. version:1
arxiv-1705-07817 | Sparse hierarchical interaction learning with epigraphical projection | http://arxiv.org/abs/1705.07817 | id:1705.07817 author:Mingyuan Jiu, Nelly Pustelnik, Stefan Janaqi, Meriam Chebre, Philippe Ricoux category:cs.LG  published:2017-05-22 summary:This work focus on regression optimization problem with hierarchical interactions between variables, which is beyond the additive models in the traditional linear regression. We investigate two different fashions in the literature to deal with this problem: "hierNet" and structural-sparsity regularization, and study their connections, then we propose a primal-dual proximal algorithm based on epigraphical projection to optimize the learning problem. The experimental setting first highlight the improvement of the proposed procedure compare to state-of-the-art methods based on FISTA or ADMM and second we provide comparisons between the different hierarchical penalization. The experiments are conducted both on the synthetic and real data. version:1
arxiv-1705-07815 | Minimax Statistical Learning and Domain Adaptation with Wasserstein Distances | http://arxiv.org/abs/1705.07815 | id:1705.07815 author:Jaeho Lee, Maxim Raginsky category:cs.LG  published:2017-05-22 summary:As opposed to standard empirical risk minimization (ERM), distributionally robust optimization aims to minimize the worst-case risk over a larger ambiguity set containing the original empirical distribution of the training data. In this work, we describe a minimax framework for statistical learning with ambiguity sets given by balls in Wasserstein space. In particular, we prove a generalization bound that involves the covering number properties of the original ERM problem. As an illustrative example, we provide generalization guarantees for domain adaptation problems where the Wasserstein distance between the source and target domain distributions can be reliably estimated from unlabeled samples. version:1
arxiv-1705-07809 | Information-theoretic analysis of generalization capability of learning algorithms | http://arxiv.org/abs/1705.07809 | id:1705.07809 author:Aolin Xu, Maxim Raginsky category:cs.LG cs.IT math.IT stat.ML  published:2017-05-22 summary:We derive upper bounds on the generalization error of a learning algorithm in terms of the mutual information between its input and output. The upper bounds provide theoretical guidelines for striking the right balance between data fit and generalization by controlling the input-output mutual information of a learning algorithm. The results can also be used to analyze the generalization capability of learning algorithms under adaptive composition, and the bias-accuracy tradeoffs in adaptive data analytics. Our work extends and leads to nontrivial improvements on the recent results of Russo and Zou. version:1
arxiv-1705-10311 | Optimal Multi-Object Segmentation with Novel Gradient Vector Flow Based Shape Priors | http://arxiv.org/abs/1705.10311 | id:1705.10311 author:Junjie Bai, Abhay Shah, Xiaodong Wu category:cs.CV  published:2017-05-22 summary:Shape priors have been widely utilized in medical image segmentation to improve segmentation accuracy and robustness. A major way to encode such a prior shape model is to use a mesh representation, which is prone to causing self-intersection or mesh folding. Those problems require complex and expensive algorithms to mitigate. In this paper, we propose a novel shape prior directly embedded in the voxel grid space, based on gradient vector flows of a pre-segmentation. The flexible and powerful prior shape representation is ready to be extended to simultaneously segmenting multiple interacting objects with minimum separation distance constraint. The problem is formulated as a Markov random field problem whose exact solution can be efficiently computed with a single minimum s-t cut in an appropriately constructed graph. The proposed algorithm is validated on two multi-object segmentation applications: the brain tissue segmentation in MRI images, and the bladder/prostate segmentation in CT images. Both sets of experiments show superior or competitive performance of the proposed method to other state-of-the-art methods. version:1
arxiv-1705-07798 | A unified view of entropy-regularized Markov decision processes | http://arxiv.org/abs/1705.07798 | id:1705.07798 author:Gergely Neu, Anders Jonsson, Vicenç Gómez category:cs.LG cs.AI stat.ML  published:2017-05-22 summary:We propose a general framework for entropy-regularized average-reward reinforcement learning in Markov decision processes (MDPs). Our approach is based on extending the linear-programming formulation of policy optimization in MDPs to accommodate convex regularization functions. Our key result is showing that using the conditional entropy of the joint state-action distributions as regularization yields a dual optimization problem closely resembling the Bellman optimality equations. This result enables us to formalize a number of state-of-the-art entropy-regularized reinforcement learning algorithms as approximate variants of Mirror Descent or Dual Averaging, and thus to argue about the convergence properties of these methods. In particular, we show that the exact version of the TRPO algorithm of Schulman et al. (2015) actually converges to the optimal policy, while the entropy-regularized policy gradient methods of Mnih et al. (2016) may fail to converge to a fixed point. Finally, we illustrate empirically the effects of using various regularization techniques on learning performance in a simple reinforcement learning setup. version:1
arxiv-1705-07777 | Robust Localized Multi-view Subspace Clustering | http://arxiv.org/abs/1705.07777 | id:1705.07777 author:Yanbo Fan, Jian Liang, Ran He, Bao-Gang Hu, Siwei Lyu category:cs.CV  published:2017-05-22 summary:In multi-view clustering, different views may have different confidence levels when learning a consensus representation. Existing methods usually address this by assigning distinctive weights to different views. However, due to noisy nature of real-world applications, the confidence levels of samples in the same view may also vary. Thus considering a unified weight for a view may lead to suboptimal solutions. In this paper, we propose a novel localized multi-view subspace clustering model that considers the confidence levels of both views and samples. By assigning weight to each sample under each view properly, we can obtain a robust consensus representation via fusing the noiseless structures among views and samples. We further develop a regularizer on weight parameters based on the convex conjugacy theory, and samples weights are determined in an adaptive manner. An efficient iterative algorithm is developed with a convergence guarantee. Experimental results on four benchmarks demonstrate the correctness and effectiveness of the proposed model. version:1
arxiv-1705-07774 | Follow the Signs for Robust Stochastic Optimization | http://arxiv.org/abs/1705.07774 | id:1705.07774 author:Lukas Balles, Philipp Hennig category:cs.LG stat.ML  published:2017-05-22 summary:Stochastic noise on gradients is now a common feature in machine learning. It complicates the design of optimization algorithms, and its effect can be unintuitive: We show that in some settings, particularly those of low signal-to-noise ratio, it can be helpful to discard all but the signs of stochastic gradient elements. In fact, we argue that three popular existing methods already approximate this very paradigm. We devise novel stochastic optimization algorithms that explicitly follow stochastic sign estimates while appropriately accounting for their uncertainty. These methods favorably compare to the state of the art on a number of benchmark problems. version:1
arxiv-1705-07772 | Convolutional Networks with MuxOut Layers as Multi-rate Systems for Image Upscaling | http://arxiv.org/abs/1705.07772 | id:1705.07772 author:Pablo Navarrete Michelini, Hanwen Liu category:cs.CV  published:2017-05-22 summary:We interpret convolutional networks as adaptive filters and combine them with so-called MuxOut layers to efficiently upscale low resolution images. We formalize this interpretation by deriving a linear and space-variant structure of a convolutional network when its activations are fixed. We introduce general purpose algorithms to analyze a network and show its overall filter effect for each given location. We use this analysis to evaluate two types of image upscalers: deterministic upscalers that target the recovery of details from original content; and second, a new generation of upscalers that can sample the distribution of upscale aliases (images that share the same downscale version) that look like real content. version:1
arxiv-1705-07768 | Learning to Associate Words and Images Using a Large-scale Graph | http://arxiv.org/abs/1705.07768 | id:1705.07768 author:Heqing Ya, Haonan Sun, Jeffrey Helt, Tai Sing Lee category:cs.CV  published:2017-05-22 summary:We develop an approach for unsupervised learning of associations between co-occurring perceptual events using a large graph. We applied this approach to successfully solve the image captcha of China's railroad system. The approach is based on the principle of suspicious coincidence. In this particular problem, a user is presented with a deformed picture of a Chinese phrase and eight low-resolution images. They must quickly select the relevant images in order to purchase their train tickets. This problem presents several challenges: (1) the teaching labels for both the Chinese phrases and the images were not available for supervised learning, (2) no pre-trained deep convolutional neural networks are available for recognizing these Chinese phrases or the presented images, and (3) each captcha must be solved within a few seconds. We collected 2.6 million captchas, with 2.6 million deformed Chinese phrases and over 21 million images. From these data, we constructed an association graph, composed of over 6 million vertices, and linked these vertices based on co-occurrence information and feature similarity between pairs of images. We then trained a deep convolutional neural network to learn a projection of the Chinese phrases onto a 230-dimensional latent space. Using label propagation, we computed the likelihood of each of the eight images conditioned on the latent space projection of the deformed phrase for each captcha. The resulting system solved captchas with 77% accuracy in 2 seconds on average. Our work, in answering this practical challenge, illustrates the power of this class of unsupervised association learning techniques, which may be related to the brain's general strategy for associating language stimuli with visual objects on the principle of suspicious coincidence. version:1
arxiv-1705-07751 | An Asynchronous Distributed Framework for Large-scale Learning Based on Parameter Exchanges | http://arxiv.org/abs/1705.07751 | id:1705.07751 author:Bikash Joshi, Franck Iutzeler, Massih-Reza Amini category:stat.ML  published:2017-05-22 summary:In many distributed learning problems, the heterogeneous loading of computing machines may harm the overall performance of synchronous strategies. In this paper, we propose an effective asynchronous distributed framework for the minimization of a sum of smooth functions, where each machine performs iterations in parallel on its local function and updates a shared parameter asynchronously. In this way, all machines can continuously work even though they do not have the latest version of the shared parameter. We prove the convergence of the consistency of this general distributed asynchronous method for gradient iterations then show its efficiency on the matrix factorization problem for recommender systems and on binary classification. version:1
arxiv-1705-07750 | Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset | http://arxiv.org/abs/1705.07750 | id:1705.07750 author:Joao Carreira, Andrew Zisserman category:cs.CV cs.LG  published:2017-05-22 summary:The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.7% on HMDB-51 and 98.0% on UCF-101. version:1
arxiv-1705-07706 | An Out-of-the-box Full-network Embedding for Convolutional Neural Networks | http://arxiv.org/abs/1705.07706 | id:1705.07706 author:Dario Garcia-Gasulla, Armand Vilalta, Ferran Parés, Jonatan Moreno, Eduard Ayguadé, Jesus Labarta, Ulises Cortés, Toyotaro Suzumura category:cs.LG cs.NE  published:2017-05-22 summary:Transfer learning for feature extraction can be used to exploit deep representations in contexts where there is very few training data, where there are limited computational resources, or when tuning the hyper-parameters needed for training is not an option. While previous contributions to feature extraction propose embeddings based on a single layer of the network, in this paper we propose a full-network embedding which successfully integrates convolutional and fully connected features, coming from all layers of a deep convolutional neural network. To do so, the embedding normalizes features in the context of the problem, and discretizes their values to reduce noise and regularize the embedding space. Significantly, this also reduces the computational cost of processing the resultant representations. The proposed method is shown to outperform single layer embeddings on several image classification tasks, while also being more robust to the choice of the pre-trained model used for obtaining the initial features. The performance gap in classification accuracy between thoroughly tuned solutions and the full-network embedding is also reduced, which makes of the proposed approach a competitive solution for a large set of applications. version:1
arxiv-1705-07704 | A Regularized Framework for Sparse and Structured Neural Attention | http://arxiv.org/abs/1705.07704 | id:1705.07704 author:Vlad Niculae, Mathieu Blondel category:stat.ML cs.CL cs.LG  published:2017-05-22 summary:Modern neural networks are often augmented with an attention mechanism, which tells the network where to focus within the input. We propose in this paper a new framework for sparse and structured attention, building upon a max operator regularized with a strongly convex function. We show that this operator is differentiable and that its gradient defines a mapping from real values to probabilities, suitable as an attention mechanism. Our framework includes softmax and a slight generalization of the recently-proposed sparsemax as special cases. However, we also show how our framework can incorporate modern structured penalties, resulting in new attention mechanisms that focus on entire segments or groups of an input, encouraging parsimony and interpretability. We derive efficient algorithms to compute the forward and backward passes of these attention mechanisms, enabling their use in a neural network trained with backpropagation. To showcase their potential as a drop-in replacement for existing attention mechanisms, we evaluate them on three large-scale tasks: textual entailment, machine translation, and sentence summarization. Our attention mechanisms improve interpretability without sacrificing performance; notably, on textual entailment and summarization, we outperform the existing attention mechanisms based on softmax and sparsemax. version:1
arxiv-1705-07692 | Semantic Softmax Loss for Zero-Shot Learning | http://arxiv.org/abs/1705.07692 | id:1705.07692 author:Zhong Ji, Yunxin Sun, Yulong Yu, Jichang Guo, Yanwei Pang category:cs.CV  published:2017-05-22 summary:A typical pipeline for Zero-Shot Learning (ZSL) is to integrate the visual features and the class semantic descriptors into a multimodal framework with a linear or bilinear model. However, the visual features and the class semantic descriptors locate in different structural spaces, a linear or bilinear model can not capture the semantic interactions between different modalities well. In this letter, we propose a nonlinear approach to impose ZSL as a multi-class classification problem via a Semantic Softmax Loss by embedding the class semantic descriptors into the softmax layer of multi-class classification network. To narrow the structural differences between the visual features and semantic descriptors, we further use an L2 normalization constraint to the differences between the visual features and visual prototypes reconstructed with the semantic descriptors. The results on three benchmark datasets, i.e., AwA, CUB and SUN demonstrate the proposed approach can boost the performances steadily and achieve the state-of-the-art performance for both zero-shot classification and zero-shot retrieval. version:1
arxiv-1705-07687 | W2VLDA: Almost Unsupervised System for Aspect Based Sentiment Analysis | http://arxiv.org/abs/1705.07687 | id:1705.07687 author:Aitor García-Pablos, Montse Cuadros, German Rigau category:cs.CL  published:2017-05-22 summary:With the increase of online customer opinions in specialised websites and social networks, the necessity of automatic systems to help to organise and classify customer reviews by domain-specific aspect/categories and sentiment polarity is more important than ever. Supervised approaches to Aspect Based Sentiment Analysis obtain good results for the domain/language their are trained on, but having manually labelled data for training supervised systems for all domains and languages use to be very costly and time consuming. In this work we describe W2VLDA, an unsupervised system based on topic modelling, that combined with some other unsupervised methods and a minimal configuration, performs aspect/category classifiation, aspectterms/opinion-words separation and sentiment polarity classification for any given domain and language. We also evaluate the performance of the aspect and sentiment classification in the multilingual SemEval 2016 task 5 (ABSA) dataset. We show competitive results for several languages (English, Spanish, French and Dutch) and domains (hotels, restaurants, electronic-devices). version:1
arxiv-1705-03152 | Phone-aware Neural Language Identification | http://arxiv.org/abs/1705.03152 | id:1705.03152 author:Zhiyuan Tang, Dong Wang, Yixiang Chen, Ying Shi, Lantian Li category:cs.CL cs.LG cs.NE  published:2017-05-09 summary:Pure acoustic neural models, particularly the LSTM-RNN model, have shown great potential in language identification (LID). However, the phonetic information has been largely overlooked by most of existing neural LID models, although this information has been used in the conventional phonetic LID systems with a great success. We present a phone-aware neural LID architecture, which is a deep LSTM-RNN LID system but accepts output from an RNN-based ASR system. By utilizing the phonetic knowledge, the LID performance can be significantly improved. Interestingly, even if the test language is not involved in the ASR training, the phonetic knowledge still presents a large contribution. Our experiments conducted on four languages within the Babel corpus demonstrated that the phone-aware approach is highly effective. version:2
arxiv-1705-07674 | Individualized Risk Prognosis for Critical Care Patients: A Multi-task Gaussian Process Model | http://arxiv.org/abs/1705.07674 | id:1705.07674 author:Ahmed M. Alaa, Jinsung Yoon, Scott Hu, Mihaela van der Schaar category:cs.LG  published:2017-05-22 summary:We report the development and validation of a data-driven real-time risk score that provides timely assessments for the clinical acuity of ward patients based on their temporal lab tests and vital signs, which allows for timely intensive care unit (ICU) admissions. Unlike the existing risk scoring technologies, the proposed score is individualized; it uses the electronic health record (EHR) data to cluster the patients based on their static covariates into subcohorts of similar patients, and then learns a separate temporal, non-stationary multi-task Gaussian Process (GP) model that captures the physiology of every subcohort. Experiments conducted on data from a heterogeneous cohort of 6,094 patients admitted to the Ronald Reagan UCLA medical center show that our risk score significantly outperforms the state-of-the-art risk scoring technologies, such as the Rothman index and MEWS, in terms of timeliness, true positive rate (TPR), and positive predictive value (PPV). In particular, the proposed score increases the AUC with 20% and 38% as compared to Rothman index and MEWS respectively, and can predict ICU admissions 8 hours before clinicians at a PPV of 35% and a TPR of 50%. Moreover, we show that the proposed risk score allows for better decisions on when to discharge clinically stable patients from the ward, thereby improving the efficiency of hospital resource utilization. version:1
arxiv-1705-07673 | A Linear-Time Kernel Goodness-of-Fit Test | http://arxiv.org/abs/1705.07673 | id:1705.07673 author:Wittawat Jitkrittum, Wenkai Xu, Zoltan Szabo, Kenji Fukumizu, Arthur Gretton category:stat.ML cs.LG 46E22  62G10 G.3; I.2.6  published:2017-05-22 summary:We propose a novel adaptive test of goodness-of-fit, with computational cost linear in the number of samples. We learn the test features that best indicate the differences between observed samples and a reference model, by minimizing the false negative rate. These features are constructed via Stein's method, meaning that it is not necessary to compute the normalising constant of the model. We analyse the asymptotic Bahadur efficiency of the new test, and prove that under a mean-shift alternative, our test always has greater relative efficiency than a previous linear-time kernel test, regardless of the choice of parameters for that test. In experiments, the performance of our method exceeds that of the earlier linear-time test, and matches or exceeds the power of a quadratic-time kernel test. In high dimensions and where model structure may be exploited, our goodness of fit test performs far better than a quadratic-time two-sample test based on the Maximum Mean Discrepancy, with samples drawn from the model. version:1
arxiv-1705-03151 | Phonetic Temporal Neural Model for Language Identification | http://arxiv.org/abs/1705.03151 | id:1705.03151 author:Zhiyuan Tang, Dong Wang, Yixiang Chen, Lantian Li, Andrew Abel category:cs.CL cs.LG cs.NE  published:2017-05-09 summary:Deep neural models, particularly the LSTM-RNN model, have shown great potential in language identification (LID). However, the phonetic information has been largely overlooked by most of existing neural LID methods, although this information has been used in the conventional phonetic LID systems with a great success. We present a phonetic temporal neural model for LID, which is an LSTM-RNN LID system but accepts phonetic features produced by a phone-discriminative DNN as the input, rather than raw acoustic features. This new model is a reminiscence of the old phonetic LID methods, but the phonetic knowledge here is much richer: it is at the frame level and involves compacted information of all phones. Our experiments conducted on the Babel database and the AP16-OLR database demonstrate that the temporal phonetic neural approach is very effective, and significantly outperforms existing acoustic neural models. It also outperforms the conventional i-vector approach on short utterances and in noisy conditions. version:2
arxiv-1705-07664 | CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters | http://arxiv.org/abs/1705.07664 | id:1705.07664 author:Ron Levie, Federico Monti, Xavier Bresson, Michael M. Bronstein category:cs.LG  published:2017-05-22 summary:The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. In this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute localized regular filters on graphs that specialize on frequency bands of interest. Our model scales linearly with the size of the input data for sparsely-connected graphs, can handle different constructions of Laplacian operators, and typically requires less parameters than previous models. Extensive experimental results show the superior performance of our approach on various graph learning problems. version:1
arxiv-1705-07663 | LOGAN: Evaluating Privacy Leakage of Generative Models Using Generative Adversarial Networks | http://arxiv.org/abs/1705.07663 | id:1705.07663 author:Jamie Hayes, Luca Melis, George Danezis, Emiliano De Cristofaro category:cs.CR cs.LG  published:2017-05-22 summary:Recent advances in machine learning are paving the way for the artificial generation of high quality images and videos. In this paper, we investigate how generating synthetic samples through generative models can lead to information leakage, and, consequently, to privacy breaches affecting individuals' privacy that contribute their personal or sensitive data to train these models. In order to quantitatively measure privacy leakage, we train a Generative Adversarial Network (GAN), which combines a discriminative model and a generative model, to detect overfitting by relying on the discriminator capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, and show how to improve it through auxiliary knowledge of samples in the dataset. We test our attacks on several state-of-the-art models such as Deep Convolutional GAN (DCGAN), Boundary Equilibrium GAN (BEGAN), and the combination of DCGAN with a Variational Autoencoder (DCGAN+VAE), using datasets consisting of complex representations of faces (LFW) and objects (CIFAR-10). Our white-box attacks are 100% successful at inferring which samples were used to train the target model, while the best black-box attacks can infer training set membership with over 60% accuracy. version:1
arxiv-1705-07661 | Streaming Binary Sketching based on Subspace Tracking and Diagonal Uniformization | http://arxiv.org/abs/1705.07661 | id:1705.07661 author:Anne Morvan, Antoine Souloumiac, Cédric Gouy-Pailler, Jamal Atif category:cs.LG  published:2017-05-22 summary:In this paper, we address the problem of learning compact similarity-preserving embeddings for massive high-dimensional streams of data in order to perform efficient similarity search. We present a new method for computing binary compressed representations -\textit{sketches}- of high-dimensional real feature vectors. Given an expected code length $c$ and high-dimensional input data points, our algorithm provides a binary code of $c$ bits aiming at preserving the distance between the points from the original high-dimensional space. Our offline version of the algorithm outperforms the offline state-of-the-art methods regarding their computation time complexity and have a similar quality of the sketches. It also provides convergence guarantees. Moreover, our algorithm can be straightforwardly used in the streaming context by not requiring neither the storage of the whole dataset nor a chunk. We demonstrate the quality of our binary sketches through extensive experiments on real data for the nearest neighbors search task in the offline and online settings. version:1
arxiv-1705-07654 | ReFACTor: Practical Low-Rank Matrix Estimation Under Column-Sparsity | http://arxiv.org/abs/1705.07654 | id:1705.07654 author:Matan Gavish, Regev Schweiger, Elior Rahmani, Eran Halperin category:stat.ML  published:2017-05-22 summary:Various problems in data analysis and statistical genetics call for recovery of a column-sparse, low-rank matrix from noisy observations. We propose ReFACTor, a simple variation of the classical Truncated Singular Value Decomposition (TSVD) algorithm. In contrast to previous sparse principal component analysis (PCA) algorithms, our algorithm can provably reveal a low-rank signal matrix better, and often significantly better, than the widely used TSVD, making it the algorithm of choice whenever column-sparsity is suspected. Empirically, we observe that ReFACTor consistently outperforms TSVD even when the underlying signal is not sparse, suggesting that it is generally safe to use ReFACTor instead of TSVD and PCA. The algorithm is extremely simple to implement and its running time is dominated by the runtime of PCA, making it as practical as standard principal component analysis. version:1
arxiv-1705-07642 | From optimal transport to generative modeling: the VEGAN cookbook | http://arxiv.org/abs/1705.07642 | id:1705.07642 author:Olivier Bousquet, Sylvain Gelly, Ilya Tolstikhin, Carl-Johann Simon-Gabriel, Bernhard Schoelkopf category:stat.ML  published:2017-05-22 summary:We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution $P_X$ and the latent variable model distribution $P_G$. We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space. When relaxed, this constrained optimization problem leads to a penalized optimal transport (POT) objective, which can be efficiently minimized using stochastic gradient descent by sampling from $P_X$ and $P_G$. We show that POT for the 2-Wasserstein distance coincides with the objective heuristically employed in adversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the first theoretical justification for AAEs known to the authors. We also compare POT to other popular techniques like variational auto-encoders (VAE) (Kingma and Welling, 2014). Our theoretical results include (a) a better understanding of the commonly observed blurriness of images generated by VAEs, and (b) establishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and POT for the 1-Wasserstein distance. version:1
arxiv-1705-07640 | Dynamics Based 3D Skeletal Hand Tracking | http://arxiv.org/abs/1705.07640 | id:1705.07640 author:Stan Melax, Leonid Keselman, Sterling Orsten category:cs.CV cs.GR I.3.7  published:2017-05-22 summary:Tracking the full skeletal pose of the hands and fingers is a challenging problem that has a plethora of applications for user interaction. Existing techniques either require wearable hardware, add restrictions to user pose, or require significant computation resources. This research explores a new approach to tracking hands, or any articulated model, by using an augmented rigid body simulation. This allows us to phrase 3D object tracking as a linear complementarity problem with a well-defined solution. Based on a depth sensor's samples, the system generates constraints that limit motion orthogonal to the rigid body model's surface. These constraints, along with prior motion, collision/contact constraints, and joint mechanics, are resolved with a projected Gauss-Seidel solver. Due to camera noise properties and attachment errors, the numerous surface constraints are impulse capped to avoid overpowering mechanical constraints. To improve tracking accuracy, multiple simulations are spawned at each frame and fed a variety of heuristics, constraints and poses. A 3D error metric selects the best-fit simulation, helping the system handle challenging hand motions. Such an approach enables real-time, robust, and accurate 3D skeletal tracking of a user's hand on a variety of depth cameras, while only utilizing a single x86 CPU core for processing. version:1
arxiv-1705-07609 | View-Invariant Recognition of Action Style Self-Dissimilarity | http://arxiv.org/abs/1705.07609 | id:1705.07609 author:Yuping Shen, Hassan Foroosh category:cs.CV  published:2017-05-22 summary:Self-similarity was recently introduced as a measure of inter-class congruence for classification of actions. Herein, we investigate the dual problem of intra-class dissimilarity for classification of action styles. We introduce self-dissimilarity matrices that discriminate between same actions performed by different subjects regardless of viewing direction and camera parameters. We investigate two frameworks using these invariant style dissimilarity measures based on Principal Component Analysis (PCA) and Fisher Discriminant Analysis (FDA). Extensive experiments performed on IXMAS dataset indicate remarkably good discriminant characteristics for the proposed invariant measures for gender recognition from video data. version:1
arxiv-1705-07606 | Deep Reinforcement Learning with Relative Entropy Stochastic Search | http://arxiv.org/abs/1705.07606 | id:1705.07606 author:Voot Tangkaratt, Abbas Abdolmaleki, Masashi Sugiyama category:stat.ML  published:2017-05-22 summary:Many reinforcement learning methods for continuous control tasks are based on updating a policy function by maximizing an approximated action-value function or Q-function. However, the Q-function also depends on the policy and this dependency often leads to unstable policy learning. To overcome this issue, we propose a method that does not greedily exploit the Q-function. To do so, we upper-bound the Kullback-Leibler divergence of the new policy while maximizing the Q-function. Furthermore, we also lower-bound the entropy of the new policy to maintain its exploratory behavior. We show that by using a Gaussian policy and a Q-function that is quadratic in actions, we can solve the corresponding constrained optimization problem in a closed form. In addition, we show that our method can be regarded as a variant of the well-known deterministic policy gradient method. Through experiments, we evaluate the proposed method using a neural network as a function approximator and show that it gives more stable learning performance than the deep deterministic policy gradient method and the continuous Q-learning method. version:1
arxiv-1705-07603 | Multi-output Polynomial Networks and Factorization Machines | http://arxiv.org/abs/1705.07603 | id:1705.07603 author:Mathieu Blondel, Vlad Niculae, Takuma Otsuka, Naonori Ueda category:stat.ML cs.LG  published:2017-05-22 summary:Factorization machines and polynomial networks are supervised polynomial models based on an efficient low-rank decomposition. We extend these models to the multi-output setting, i.e., for learning vector-valued functions, with application to multi-class or multi-task problems. We cast this as the problem of learning a 3-way tensor whose slices share a common decomposition and propose a convex formulation of that problem. We then develop an efficient conditional gradient algorithm and prove its global convergence, despite the fact that it involves a non-convex hidden unit selection step. On classification tasks, we show that our algorithm achieves excellent accuracy with much sparser models than existing methods. On recommendation system tasks, we show how to combine our algorithm with a reduction from ordinal regression to multi-output classification and show that the resulting algorithm outperforms existing baselines in terms of ranking accuracy. version:1
arxiv-1704-02360 | Voice Conversion Using Sequence-to-Sequence Learning of Context Posterior Probabilities | http://arxiv.org/abs/1704.02360 | id:1704.02360 author:Hiroyuki Miyoshi, Yuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari category:cs.SD cs.CL cs.LG  published:2017-04-10 summary:Voice conversion (VC) using sequence-to-sequence learning of context posterior probabilities is proposed. Conventional VC using shared context posterior probabilities predicts target speech parameters from the context posterior probabilities estimated from the source speech parameters. Although conventional VC can be built from non-parallel data, it is difficult to convert speaker individuality such as phonetic property and speaking rate contained in the posterior probabilities because the source posterior probabilities are directly used for predicting target speech parameters. In this work, we assume that the training data partly include parallel speech data and propose sequence-to-sequence learning between the source and target posterior probabilities. The conversion models perform non-linear and variable-length transformation from the source probability sequence to the target one. Further, we propose a joint training algorithm for the modules. In contrast to conventional VC, which separately trains the speech recognition that estimates posterior probabilities and the speech synthesis that predicts target speech parameters, our proposed method jointly trains these modules along with the proposed probability conversion modules. Experimental results demonstrate that our approach outperforms the conventional VC. version:3
arxiv-1705-07600 | Classification Using Proximity Catch Digraphs (Technical Report) | http://arxiv.org/abs/1705.07600 | id:1705.07600 author:Artür Manukyan, Elvan Ceyhan category:cs.LG stat.ME stat.ML  published:2017-05-22 summary:We employ random geometric digraphs to construct semi-parametric classifiers. These data-random digraphs are from parametrized random digraph families called proximity catch digraphs (PCDs). A related geometric digraph family, class cover catch digraph (CCCD), has been used to solve the class cover problem by using its approximate minimum dominating set. CCCDs showed relatively good performance in the classification of imbalanced data sets, and although CCCDs have a convenient construction in $\mathbb{R}^d$, finding minimum dominating sets is NP-hard and its probabilistic behaviour is not mathematically tractable except for $d=1$. On the other hand, a particular family of PCDs, called \emph{proportional-edge} PCDs (PE-PCDs), has mathematical tractable minimum dominating sets in $\mathbb{R}^d$; however their construction in higher dimensions may be computationally demanding. More specifically, we show that the classifiers based on PE-PCDs are prototype-based classifiers such that the exact minimum number of prototypes (equivalent to minimum dominating sets) are found in polynomial time on the number of observations. We construct two types of classifiers based on PE-PCDs. One is a family of hybrid classifiers depend on the location of the points of the training data set, and another type is a family of classifiers solely based on class covers. We assess the classification performance of our PE-PCD based classifiers by extensive Monte Carlo simulations, and compare them with that of other commonly used classifiers. We also show that, similar to CCCD classifiers, our classifiers are relatively better in classification in the presence of class imbalance. version:1
arxiv-1705-07594 | Learning Robust Object Recognition Using Composed Scenes from Generative Models | http://arxiv.org/abs/1705.07594 | id:1705.07594 author:Hao Wang, Xingyu Lin, Yimeng Zhang, Tai Sing Lee category:cs.CV  published:2017-05-22 summary:Recurrent feedback connections in the mammalian visual system have been hypothesized to play a role in synthesizing input in the theoretical framework of analysis by synthesis. The comparison of internally synthesized representation with that of the input provides a validation mechanism during perceptual inference and learning. Inspired by these ideas, we proposed that the synthesis machinery can compose new, unobserved images by imagination to train the network itself so as to increase the robustness of the system in novel scenarios. As a proof of concept, we investigated whether images composed by imagination could help an object recognition system to deal with occlusion, which is challenging for the current state-of-the-art deep convolutional neural networks. We fine-tuned a network on images containing objects in various occlusion scenarios, that are imagined or self-generated through a deep generator network. Trained on imagined occluded scenarios under the object persistence constraint, our network discovered more subtle and localized image features that were neglected by the original network for object classification, obtaining better separability of different object classes in the feature space. This leads to significant improvement of object recognition under occlusion for our network relative to the original network trained only on un-occluded images. In addition to providing practical benefits in object recognition under occlusion, this work demonstrates the use of self-generated composition of visual scenes through the synthesis loop, combined with the object persistence constraint, can provide opportunities for neural networks to discover new relevant patterns in the data, and become more flexible in dealing with novel situations. version:1
arxiv-1705-07592 | Improved Clustering with Augmented k-means | http://arxiv.org/abs/1705.07592 | id:1705.07592 author:J. Andrew Howe category:stat.ML 62H30 I.5.3; G.3; G.4  published:2017-05-22 summary:Identifying a set of homogeneous clusters in a heterogeneous dataset is one of the most important classes of problems in statistical modeling. In the realm of unsupervised partitional clustering, k-means is a very important algorithm for this. In this technical report, we develop a new k-means variant called Augmented k-means, which is a hybrid of k-means and logistic regression. During each iteration, logistic regression is used to predict the current cluster labels, and the cluster belonging probabilities are used to control the subsequent re-estimation of cluster means. Observations which can't be firmly identified into clusters are excluded from the re-estimation step. This can be valuable when the data exhibit many characteristics of real datasets such as heterogeneity, non-sphericity, substantial overlap, and high scatter. Augmented k-means frequently outperforms k-means by more accurately classifying observations into known clusters and / or converging in fewer iterations. We demonstrate this on both simulated and real datasets. Our algorithm is implemented in Python and will be available with this report. version:1
arxiv-1705-07585 | Union of Intersections (UoI) for Interpretable Data Driven Discovery and Prediction | http://arxiv.org/abs/1705.07585 | id:1705.07585 author:Kristofer E. Bouchard, Alejandro F. Bujan, Farbod Roosta-Khorasani, Shashanka Ubaru, Prabhat, Antoine M. Snijders, Jian-Hua Mao, Edward F. Chang, Michael W. Mahoney, Sharmodeep Bhattacharyya category:stat.ML  published:2017-05-22 summary:The increasing size and complexity of scientific data could dramatically enhance discovery and prediction for basic scientific applications. Realizing this potential, however, requires novel statistical analysis methods that are both interpretable and predictive. We introduce Union of Intersections (UoI), a flexible, modular, and scalable framework for enhanced model selection and estimation. Methods based on UoI perform model selection and model estimation through intersection and union operations, respectively. We show that UoI-based methods achieve low-variance and nearly unbiased estimation of a small number of interpretable features, while maintaining high-quality prediction accuracy. We perform extensive numerical investigation to evaluate a UoI algorithm ($UoI_{Lasso}$) on synthetic and real data. In doing so, we demonstrate the extraction of interpretable functional networks from human electrophysiology recordings as well as accurate prediction of phenotypes from genotype-phenotype data with reduced features. We also show (with the $UoI_{L1Logistic}$ and $UoI_{CUR}$ variants of the basic framework) improved prediction parsimony for classification and matrix factorization on several benchmark biomedical data sets. These results suggest that methods based on the UoI framework could improve interpretation and prediction in data-driven discovery across scientific fields. version:1
arxiv-1704-03971 | On the Effects of Batch and Weight Normalization in Generative Adversarial Networks | http://arxiv.org/abs/1704.03971 | id:1704.03971 author:Sitao Xiang, Hao Li category:stat.ML cs.CV cs.LG  published:2017-04-13 summary:Generative adversarial networks (GANs) are highly effective unsupervised learning frameworks that can generate very sharp data, even for data such as images with complex, highly multimodal distributions. However GANs are known to be very hard to train, suffering from problems such as mode collapse and disturbing visual artifacts. Batch normalization (BN) techniques have been introduced to address the training problem. However, though BN accelerates training in the beginning, our experiments show that the use of BN can be unstable and negatively impact the quality of the trained model. The evaluation of BN and numerous other recent schemes for improving GAN training is hindered by the lack of an effective objective quality measure for GAN models. To address these issues, we first introduce a weight normalization (WN) approach for GAN training that significantly improves the stability, efficiency and the quality of the generated samples. To allow a methodical evaluation, we introduce a new objective measure based on a squared Euclidean reconstruction error metric, to assess training performance in terms of speed, stability, and quality of generated samples. Our experiments indicate that training using WN is generally superior to BN for GANs. We provide statistical evidence for commonly used datasets (CelebA, LSUN, and CIFAR-10), that WN achieves 10% lower mean squared loss for reconstruction and significantly better qualitative results than BN. version:3
arxiv-1705-07576 | Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk | http://arxiv.org/abs/1705.07576 | id:1705.07576 author:Paul Hand, Vladislav Voroninski category:cs.IT cs.LG math.IT math.OC math.PR  published:2017-05-22 summary:We examine the theoretical properties of enforcing priors provided by generative deep neural networks via empirical risk minimization. In particular we consider two models, one in which the task is to invert a generative neural network given access to its last layer and another which entails recovering a latent code in the domain of a generative neural network from compressive linear observations of its last layer. We establish that in both cases, in suitable regimes of network layer sizes and a randomness assumption on the network weights, that the non-convex objective function given by empirical risk minimization does not have any spurious stationary points. That is, we establish that with high probability, at any point away from small neighborhoods around two scalar multiples of the desired solution, there is a descent direction. These results constitute the first theoretical guarantees which establish the favorable global geometry of these non-convex optimization problems, and bridge the gap between the empirical success of deep learning and a rigorous understanding of non-linear inverse problems. version:1
arxiv-1705-08293 | An Invariant Model of the Significance of Different Body Parts in Recognizing Different Actions | http://arxiv.org/abs/1705.08293 | id:1705.08293 author:Yuping Shen, Hassan Foroosh category:cs.CV  published:2017-05-22 summary:In this paper, we show that different body parts do not play equally important roles in recognizing a human action in video data. We investigate to what extent a body part plays a role in recognition of different actions and hence propose a generic method of assigning weights to different body points. The approach is inspired by the strong evidence in the applied perception community that humans perform recognition in a foveated manner, that is they recognize events or objects by only focusing on visually significant aspects. An important contribution of our method is that the computation of the weights assigned to body parts is invariant to viewing directions and camera parameters in the input data. We have performed extensive experiments to validate the proposed approach and demonstrate its significance. In particular, results show that considerable improvement in performance is gained by taking into account the relative importance of different body parts as defined by our approach. version:1
arxiv-1705-07565 | Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon | http://arxiv.org/abs/1705.07565 | id:1705.07565 author:Xin Dong, Shangyu Chen, Sinno Jialin Pan category:cs.NE cs.CV cs.LG  published:2017-05-22 summary:How to develop slim and accurate deep neural networks has become crucial for real- world applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. Therefore, there is a guarantee that one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods. version:1
arxiv-1705-07562 | Batch Size Matters: A Diffusion Approximation Framework on Nonconvex Stochastic Gradient Descent | http://arxiv.org/abs/1705.07562 | id:1705.07562 author:Chris Junchi Li, Lei Li, Junyang Qian, Jian-Guo Liu category:stat.ML cs.LG  published:2017-05-22 summary:In this paper, we study the stochastic gradient descent method in analyzing nonconvex statistical optimization problems from a diffusion approximation point of view. Using the theory of large deviation of random dynamical system, we prove in the small stepsize regime and the presence of omnidirectional noise the following: starting from a local minimizer (resp.~saddle point) the SGD iteration escapes in a number of iteration that is exponentially (resp.~linearly) dependent on the inverse stepsize. We take the deep neural network as an example to study this phenomenon. Based on a new analysis of the mixing rate of multidimensional Ornstein-Uhlenbeck processes, our theory substantiate a very recent empirical results by \citet{keskar2016large}, suggesting that large batch sizes in training deep learning for synchronous optimization leads to poor generalization error. version:1
arxiv-1705-07543 | Building Emotional Machines: Recognizing Image Emotions through Deep Neural Networks | http://arxiv.org/abs/1705.07543 | id:1705.07543 author:Hye-Rin Kim, Seon Joo Kim, In-Kwon Lee category:cs.CV cs.MM  published:2017-05-22 summary:An image is a very effective tool for conveying emotions. Many researchers have investigated in computing the image emotions by using various features extracted from images. In this paper, we focus on two high level features, the object and the background, and assume that the semantic information of images is a good cue for emotion prediction. An object is one of the most important elements that define an image, and we find out through experiments that there is a high correlation between the object and the emotion in images. Even with the same object, there may be slight difference in emotion due to different backgrounds, and we use the semantic information of the background to improve the prediction performance. By combining the different levels of features, we build an emotion based feed forward deep neural network which produces the emotion values of a given image. The output emotion values in our framework are continuous values in the 2-dimensional space (Valence and Arousal), which are more effective than using a few number of emotion categories in describing emotions. Experiments confirm the effectiveness of our network in predicting the emotion of images. version:1
arxiv-1705-07541 | Learning from Complementary Labels | http://arxiv.org/abs/1705.07541 | id:1705.07541 author:Takashi Ishida, Gang Niu, Masashi Sugiyama category:stat.ML cs.LG  published:2017-05-22 summary:Collecting labeled data is costly and thus is a critical bottleneck in real-world classification tasks. To mitigate the problem, we consider a complementary label, which specifies a class that a pattern does not belong to. Collecting complementary labels would be less laborious than ordinary labels since users do not have to carefully choose the correct class from many candidate classes. However, complementary labels are less informative than ordinary labels and thus a suitable approach is needed to better learn from complementary labels. In this paper, we show that an unbiased estimator of the classification risk can be obtained only from complementary labels, if a loss function satisfies a particular symmetric condition. We theoretically prove the estimation error bounds for the proposed method, and experimentally demonstrate the usefulness of the proposed algorithms. version:1
arxiv-1705-06884 | A Unified Framework for Stochastic Matrix Factorization via Variance Reduction | http://arxiv.org/abs/1705.06884 | id:1705.06884 author:Renbo Zhao, William B. Haskell, Jiashi Feng category:stat.ML cs.LG math.OC  published:2017-05-19 summary:We propose a unified framework to speed up the existing stochastic matrix factorization (SMF) algorithms via variance reduction. Our framework is general and it subsumes several well-known SMF formulations in the literature. We perform a non-asymptotic convergence analysis of our framework and derive computational and sample complexities for our algorithm to converge to an $\epsilon$-stationary point in expectation. In addition, extensive experiments for a wide class of SMF formulations demonstrate that our framework consistently yields faster convergence and a more accurate output dictionary vis-\`a-vis state-of-the-art frameworks. version:2
arxiv-1705-04138 | Fast Stochastic Variance Reduced ADMM for Stochastic Composition Optimization | http://arxiv.org/abs/1705.04138 | id:1705.04138 author:Yue Yu, Longbo Huang category:cs.LG stat.ML  published:2017-05-11 summary:We consider the stochastic composition optimization problem proposed in \cite{wang2017stochastic}, which has applications ranging from estimation to statistical and machine learning. We propose the first ADMM-based algorithm named com-SVR-ADMM, and show that com-SVR-ADMM converges linearly for strongly convex and Lipschitz smooth objectives, and has a convergence rate of $O( \log S/S)$, which improves upon the $O(S^{-4/9})$ rate in \cite{wang2016accelerating} when the objective is convex and Lipschitz smooth. Moreover, com-SVR-ADMM possesses a rate of $O(1/\sqrt{S})$ when the objective is convex but without Lipschitz smoothness. We also conduct experiments and show that it outperforms existing algorithms. version:2
arxiv-1705-07522 | Classification and Retrieval of Digital Pathology Scans: A New Dataset | http://arxiv.org/abs/1705.07522 | id:1705.07522 author:Morteza Babaie, Shivam Kalra, Aditya Sriram, Christopher Mitcheltree, Shujin Zhu, Amin Khatami, Shahryar Rahnamayan, H. R. Tizhoosh category:cs.CV  published:2017-05-22 summary:In this paper, we introduce a new dataset, \textbf{Kimia Path24}, for image classification and retrieval in digital pathology. We use the whole scan images of 24 different tissue textures to generate 1,325 test patches of size 1000$\times$1000 (0.5mm$\times$0.5mm). Training data can be generated according to preferences of algorithm designer and can range from approximately 27,000 to over 50,000 patches if the preset parameters are adopted. We propose a compound patch-and-scan accuracy measurement that makes achieving high accuracies quite challenging. In addition, we set the benchmarking line by applying LBP, dictionary approach and convolutional neural nets (CNNs) and report their results. The highest accuracy was 41.80\% for CNN. version:1
arxiv-1704-04866 | Effective Warm Start for the Online Actor-Critic Reinforcement Learning based mHealth Intervention | http://arxiv.org/abs/1704.04866 | id:1704.04866 author:Feiyun Zhu, Peng Liao category:cs.LG cs.AI  published:2017-04-17 summary:Online reinforcement learning (RL) is increasingly popular for the personalized mobile health (mHealth) intervention. It is able to personalize the type and dose of interventions according to user's ongoing statuses and changing needs. However, at the beginning of online learning, there are usually too few samples to support the RL updating, which leads to poor performances. A delay in good performance of the online learning algorithms can be especially detrimental in the mHealth, where users tend to quickly disengage with the mHealth app. To address this problem, we propose a new online RL methodology that focuses on an effective warm start. The main idea is to make full use of the data accumulated and the decision rule achieved in a former study. As a result, we can greatly enrich the data size at the beginning of online learning in our method. Such case accelerates the online learning process for new users to achieve good performances not only at the beginning of online learning but also through the whole online learning process. Besides, we use the decision rules achieved in a previous study to initialize the parameter in our online RL model for new users. It provides a good initialization for the proposed online RL algorithm. Experiment results show that promising improvements have been achieved by our method compared with the state-of-the-art method. version:3
arxiv-1705-07505 | Annealed Generative Adversarial Networks | http://arxiv.org/abs/1705.07505 | id:1705.07505 author:Arash Mehrjou, Bernhard Schölkopf, Saeed Saremi category:stat.ML cs.LG  published:2017-05-21 summary:We introduce a novel framework for adversarial training where the target distribution is annealed between the uniform distribution and the data distribution. We posited a conjecture that learning under continuous annealing in the nonparametric regime is stable irrespective of the divergence measures in the objective function and proposed an algorithm, dubbed {\ss}-GAN, in corollary. In this framework, the fact that the initial support of the generative network is the whole ambient space combined with annealing are key to balancing the minimax game. In our experiments on synthetic data, MNIST, and CelebA, {\ss}-GAN with a fixed annealing schedule was stable and did not suffer from mode collapse. version:1
arxiv-1705-07492 | Parallel and in-process compilation of individuals for genetic programming on GPU | http://arxiv.org/abs/1705.07492 | id:1705.07492 author:Hakan Ayral, Songül Albayrak category:cs.NE  published:2017-05-21 summary:Three approaches to implement genetic programming on GPU hardware are compilation, interpretation and direct generation of machine code. The compiled approach is known to have a prohibitive overhead compared to other two. This paper investigates methods to accelerate compilation of individuals for genetic programming on GPU hardware. We apply in-process compilation to minimize the compilation overhead at each generation; and we investigate ways to parallelize in-process compilation. In-process compilation doesn't lend itself to trivial parallelization with threads; we propose a multiprocess parallelization using memory sharing and operating systems interprocess communication primitives. With parallelized compilation we achieve further reductions on compilation overhead. Another contribution of this work is the code framework we built in C# for the experiments. The framework makes it possible to build arbitrary grammatical genetic programming experiments that run on GPU with minimal extra coding effort, and is available as open source. version:1
arxiv-1705-07477 | Statistical inference using SGD | http://arxiv.org/abs/1705.07477 | id:1705.07477 author:Tianyang Li, Liu Liu, Anastasios Kyrillidis, Constantine Caramanis category:cs.LG cs.AI math.OC math.ST stat.ML stat.TH  published:2017-05-21 summary:We present a novel method for frequentist statistical inference in $M$-estimation problems, based on stochastic gradient descent (SGD) with a fixed step size: we demonstrate that the average of such SGD sequences can be used for statistical inference, after proper scaling. An intuitive analysis using the Ornstein-Uhlenbeck process suggests that such averages are asymptotically normal. From a practical perspective, our SGD-based inference procedure is a first order method, and is well-suited for large scale problems. To show its merits, we apply it to both synthetic and real datasets, and demonstrate that its accuracy is comparable to classical statistical methods, while requiring potentially far less computation. version:1
arxiv-1705-07474 | Nice latent variable models have log-rank | http://arxiv.org/abs/1705.07474 | id:1705.07474 author:Madeleine Udell, Alex Townsend category:cs.LG stat.ML  published:2017-05-21 summary:Matrices of low rank are pervasive in big data, appearing in recommender systems, movie preferences, topic models, medical records, and genomics. While there is a vast literature on how to exploit low rank structure in these datasets, there is less attention on explaining why the low rank structure appears in the first place. We explain the abundance of low rank matrices in big data by proving that certain latent variable models associated to piecewise analytic functions are of log-rank. A large matrix from such a latent variable model can be approximated, up to a small error, by a low rank matrix. version:1
arxiv-1705-07469 | Improved Algorithms for Matrix Recovery from Rank-One Projections | http://arxiv.org/abs/1705.07469 | id:1705.07469 author:Mohammadreza Soltani, Chinmay Hegde category:stat.ML  published:2017-05-21 summary:We consider the problem of estimation of a low-rank matrix from a limited number of noisy rank-one projections. In particular, we propose two fast, non-convex \emph{proper} algorithms for matrix recovery and support them with rigorous theoretical analysis. We show that the proposed algorithms enjoy linear convergence and that their sample complexity is independent of the condition number of the unknown true low-rank matrix. By leveraging recent advances in low-rank matrix approximation techniques, we show that our algorithms achieve computational speed-ups over existing methods. Finally, we complement our theory with some numerical experiments. version:1
arxiv-1705-07461 | Shallow Updates for Deep Reinforcement Learning | http://arxiv.org/abs/1705.07461 | id:1705.07461 author:Nir Levine, Tom Zahavy, Daniel J. Mankowitz, Aviv Tamar, Shie Mannor category:cs.AI cs.LG stat.ML  published:2017-05-21 summary:Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN) have achieved state-of-the-art results in a variety of challenging, high-dimensional domains. This success is mainly attributed to the power of deep neural networks to learn rich domain representations for approximating the value function or policy. Batch reinforcement learning methods with linear representations, on the other hand, are more stable and require less hyper parameter tuning. Yet, substantial feature engineering is necessary to achieve good results. In this work we propose a hybrid approach -- the Least Squares Deep Q-Network (LS-DQN), which combines rich feature representations learned by a DRL algorithm with the stability of a linear least squares method. We do this by periodically re-training the last hidden layer of a DRL network with a batch least squares update. Key to our approach is a Bayesian regularization term for the least squares update, which prevents over-fitting to the more recent data. We tested LS-DQN on five Atari games and demonstrate significant improvement over vanilla DQN and Double-DQN. We also investigated the reasons for the superior performance of our method. Interestingly, we found that the performance improvement can be attributed to the large batch size used by the LS method when optimizing the last layer. version:1
arxiv-1705-07450 | Image Segmentation by Iterative Inference from Conditional Score Estimation | http://arxiv.org/abs/1705.07450 | id:1705.07450 author:Adriana Romero, Michal Drozdzal, Akram Erraqabi, Simon Jégou, Yoshua Bengio category:cs.CV  published:2017-05-21 summary:Inspired by the combination of feedforward and iterative computations in the virtual cortex, and taking advantage of the ability of denoising autoencoders to estimate the score of a joint distribution, we propose a novel approach to iterative inference for capturing and exploiting the complex joint distribution of output variables conditioned on some input variables. This approach is applied to image pixel-wise segmentation, with the estimated conditional score used to perform gradient ascent towards a mode of the estimated conditional distribution. This extends previous work on score estimation by denoising autoencoders to the case of a conditional distribution, with a novel use of a corrupted feedforward predictor replacing Gaussian corruption. An advantage of this approach over more classical ways to perform iterative inference for structured outputs, like conditional random fields (CRFs), is that it is not any more necessary to define an explicit energy function linking the output variables. To keep computations tractable, such energy function parametrizations are typically fairly constrained, involving only a few neighbors of each of the output variables in each clique. We experimentally find that the proposed iterative inference from conditional score estimation by conditional denoising autoencoders performs better than comparable models based on CRFs or those not using any explicit modeling of the conditional joint distribution of outputs. version:1
arxiv-1705-07445 | Learning to Mix n-Step Returns: Generalizing lambda-Returns for Deep Reinforcement Learning | http://arxiv.org/abs/1705.07445 | id:1705.07445 author:Sahil Sharma, Srivatsan Ramesh, Girish Raguvir J, Balaraman Ravindran category:cs.LG cs.AI  published:2017-05-21 summary:Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks. A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using next state's value function. $\lambda$-returns generalize beyond 1-step returns and strike a balance between Monte Carlo and TD learning methods. While lambda-returns have been extensively studied in RL, they haven't been explored a lot in Deep RL. This paper's first contribution is an exhaustive benchmarking of lambda-returns. Although mathematically tractable, the use of exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice. Our second major contribution is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. This allows the agent to learn to decide how much it wants to weigh the n-step returns based targets. In contrast, lambda-returns restrict RL agents to use an exponentially decaying weighting scheme. Autodidactic returns can be used for improving any RL algorithm which uses TD learning. We empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns. We perform our experiments on the Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain. version:1
arxiv-1705-07443 | Parallel Streaming Wasserstein Barycenters | http://arxiv.org/abs/1705.07443 | id:1705.07443 author:Matthew Staib, Sebastian Claici, Justin Solomon, Stefanie Jegelka category:cs.LG math.OC stat.CO stat.ML  published:2017-05-21 summary:Efficiently aggregating data from different sources is a challenging problem, particularly when samples from each source are distributed differently. These differences can be inherent to the inference task or present for other reasons: sensors in a sensor network may be placed far apart, affecting their individual measurements. Conversely, it is computationally advantageous to split Bayesian inference tasks across subsets of data, but data need not be identically distributed across subsets. One principled way to fuse probability distributions is via the lens of optimal transport: the Wasserstein barycenter is a single distribution that summarizes a collection of input measures while respecting their geometry. However, computing the barycenter scales poorly and requires discretization of all input distributions and the barycenter itself. Improving on this situation, we present a scalable, communication-efficient, parallel algorithm for computing the Wasserstein barycenter of arbitrary distributions. Our algorithm can operate directly on continuous input distributions and is optimized for streaming data. Our method is even robust to nonstationary input distributions and produces a barycenter estimate that tracks the input measures over time. The algorithm is semi-discrete, needing to discretize only the barycenter estimate. To the best of our knowledge, we also provide the first bounds on the quality of the approximate barycenter as the discretization becomes finer. Finally, we demonstrate the practical effectiveness of our method, both in tracking moving distributions on a sphere, as well as in a large-scale Bayesian inference task. version:1
arxiv-1705-07426 | The Do's and Don'ts for CNN-based Face Verification | http://arxiv.org/abs/1705.07426 | id:1705.07426 author:Ankan Bansal, Carlos Castillo, Rajeev Ranjan, Rama Chellappa category:cs.CV  published:2017-05-21 summary:Convolutional neural networks (CNN) have become the most sought after tools for addressing object recognition problems. Specifically, they have produced state-of-the art results for unconstrained face recognition and verification tasks. While the research community appears to have developed a consensus on the methods of acquiring annotated data, design and training of CNNs, many questions still remain to be answered. In this paper, we explore the following questions that are critical to face recognition research: (i) Can we train on still images and expect the systems to work on videos? (ii) Are deeper datasets better than wider datasets? (iii) Does adding label noise lead to improvement in performance of deep networks? (iv) Is alignment needed for face recognition? We address these questions by training CNNs using CASIA-WebFace, UMDFaces, and a new video dataset and testing on YouTubeFaces, IJBA and a disjoint portion of UMDFaces datasets. Our new data set, which will be made publicly available, has 22,075 videos and 3,735,476 human annotated frames extracted from them. version:1
arxiv-1705-07422 | Generative Partition Networks for Multi-Person Pose Estimation | http://arxiv.org/abs/1705.07422 | id:1705.07422 author:Xuecheng Nie, Jiashi Feng, Junliang Xing, Shuicheng Yan category:cs.CV  published:2017-05-21 summary:This paper proposes a new framework, named Generative Partition Network (GPN), for addressing the challenging multi-person pose estimation problem. Different from existing pure top-down and bottom-up solutions, the proposed GPN models the multi-person partition detection as a generative process from joint candidates and infers joint configurations for person instances from each person partition locally, resulting in both low joint detection and joint partition complexities. In particular, GPN designs a generative model based on the Generalized Hough Transform framework to detect person partitions via votes from joint candidates in the Hough space, parameterized by centroids of persons. Such generative model produces joint candidates and their corresponding person partitions by performing only one pass of joint detection. In addition, GPN formulates the inference procedure for joint configurations of human poses as a graph partition problem and optimizes it locally. Inspired by recent success of deep learning techniques for human pose estimation, GPN designs a multi-stage convolutional neural network with feature pyramid branch to jointly learn joint confidence maps and Hough transformation maps. Extensive experiments on two benchmarks demonstrate the efficiency and effectiveness of the proposed GPN. version:1
arxiv-1705-07420 | Structured Image Classification from Conditional Random Field with Deep Class Embedding | http://arxiv.org/abs/1705.07420 | id:1705.07420 author:Eran Goldman, Jacob Goldberger category:cs.CV  published:2017-05-21 summary:This paper presents a novel deep learning architecture to classify structured objects in datasets with a large number of visually similar categories. Our model extends the CRF objective function to a nonlinear form, by factorizing the pairwise potential matrix, to learn neighboring-class embedding. The embedding and the classifier are jointly trained to optimize this highly nonlinear CRF objective function. The non-linear model is trained on object-level samples, which is much faster and more accurate than the standard sequence-level training of the linear model. This model overcomes the difficulties of existing CRF methods to learn the contextual relationships thoroughly when there is a large number of classes and the data is sparse. The performance of the proposed method is illustrated on a huge dataset that contains images of retail-store product displays, taken in varying settings and viewpoints, and shows significantly improved results compared to linear CRF modeling and sequence-level training. version:1
arxiv-1705-07414 | Unfolding Hidden Barriers by Active Enhanced Sampling | http://arxiv.org/abs/1705.07414 | id:1705.07414 author:Jing Zhang, Ming Chen category:physics.chem-ph cond-mat.stat-mech cs.LG  published:2017-05-21 summary:Collective variable (CV) or order parameter based enhanced sampling algorithms have achieved great success due to their ability to efficiently explore the rough potential energy landscapes of complex systems. However, the degeneracy of microscopic configurations, originating from the orthogonal space perpendicular to the CVs, is likely to shadow "hidden barriers" and greatly reduce the efficiency of CV-based sampling. Here we demonstrate that systematic machine learning CV, through enhanced sampling, can iteratively lift such degeneracies on the fly. We introduce an active learning scheme that consists of a parametric CV learner based on deep neural network and a CV-based enhanced sampler. Our active enhanced sampling (AES) algorithm is capable of identifying the least informative regions based on a historical sample, forming a positive feedback loop between the CV leaner and sampler. This approach is able to globally preserve kinetic characteristics by incrementally enhancing both sample completeness and CV quality. version:1
arxiv-1705-07404 | CrossNets : A New Approach to Complex Learning | http://arxiv.org/abs/1705.07404 | id:1705.07404 author:Chirag Agarwal, Mehdi Sharifzhadeh, Joe Klobusicky, Dan Schonfeld category:cs.CV cs.LG  published:2017-05-21 summary:We propose a novel neural network structure called CrossNets, which considers architectures on directed acyclic graphs. This structure builds on previous generalizations of feed forward models, such as ResNets, by allowing for all forward cross connections between layers (both adjacent and non-adjacent). The addition of cross connections among the network increases information flow across the whole network, leading to better training and testing performances. The superior performance of the network is tested against four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN. We conclude with a proof of convergence for Crossnets to a local minimum for error, where weights for connections are chosen through backpropagation with momentum. version:1
arxiv-1705-07393 | Recurrent Additive Networks | http://arxiv.org/abs/1705.07393 | id:1705.07393 author:Kenton Lee, Omer Levy, Luke Zettlemoyer category:cs.CL  published:2017-05-21 summary:We introduce recurrent additive networks (RANs), a new gated RNN which is distinguished by the use of purely additive latent state updates. At every time step, the new state is computed as a gated component-wise sum of the input and the previous state, without any of the non-linearities commonly used in RNN transition dynamics. We formally show that RAN states are weighted sums of the input vectors, and that the gates only contribute to computing the weights of these sums. Despite this relatively simple functional form, experiments demonstrate that RANs outperform both LSTMs and GRUs on benchmark language modeling problems. This result shows that many of the non-linear computations in LSTMs and related networks are not essential, at least for the problems we consider, and suggests that the gates are doing more of the computational work than previously understood. version:1
arxiv-1705-07386 | DeepMasterPrint: Generating Fingerprints for Presentation Attacks | http://arxiv.org/abs/1705.07386 | id:1705.07386 author:Philip Bontrager, Julian Togelius, Nasir Memon category:cs.CV cs.CR cs.LG  published:2017-05-21 summary:We present two related methods for creating MasterPrints, synthetic fingerprints that a fingerprint verification system identifies as many different people. Both methods start with training a Generative Adversarial Network (GAN) on a set of real fingerprint images. The generator network is then used to search for images that can be recognized as multiple individuals. The first method uses evolutionary optimization in the space of latent variables, and the second uses gradient-based search. Our method is able to design a MasterPrint that a commercial fingerprint system matches to 22% of all users in a strict security setting, and 75% of all users at a looser security setting. version:1
arxiv-1705-07384 | Balanced Policy Evaluation and Learning | http://arxiv.org/abs/1705.07384 | id:1705.07384 author:Nathan Kallus category:stat.ML cs.LG math.OC  published:2017-05-21 summary:We present a new approach to the problems of evaluating and learning personalized decision policies from observational data of past contexts, decisions, and outcomes. Only the outcome of the enacted decision is available and the historical policy is unknown. These problems arise in personalized medicine using electronic health records and in internet advertising. Existing approaches use inverse propensity weighting (or, doubly robust versions) to make historical outcome (or, residual) data look like it were generated by a new policy being evaluated or learned. But this relies on a plug-in approach that rejects data points with a decision that disagrees with the new policy, leading to high variance estimates and ineffective learning. We propose a new, balance-based approach that too makes the data look like the new policy but does so directly by finding weights that optimize for balance between the weighted data and the target policy in the given, finite sample, which is equivalent to minimizing worst-case or posterior conditional mean square error. Our policy learner proceeds as a two-level optimization problem over policies and weights. We demonstrate that this approach markedly outperforms existing ones both in evaluation and learning, which is unsurprising given the wider support of balance-based weights. We establish extensive theoretical consistency guarantees and regret bounds that support this empirical success. version:1
arxiv-1705-07383 | Incorporating Depth into both CNN and CRF for Indoor Semantic Segmentation | http://arxiv.org/abs/1705.07383 | id:1705.07383 author:Jindong Jiang, Zhijun Zhang, Yongqian Huang, Lunan Zheng category:cs.CV  published:2017-05-21 summary:To improve segmentation performance, a novel neural network architecture (termed DFCN-DCRF) is proposed, which combines an RGB-D fully convolutional neural network (DFCN) with a depth-sensitive fully-connected conditional random field (DCRF). First, a DFCN architecture which fuses depth information into the early layers and applies dilated convolution for later contextual reasoning is designed. Then, a depth-sensitive fully-connected conditional random field (DCRF) is proposed and combined with the previous DFCN to refine the preliminary result. Comparative experiments show that the proposed DFCN-DCRF has the best performance compared with most state-of-the-art methods. version:1
arxiv-1705-07377 | Instrument-Armed Bandits | http://arxiv.org/abs/1705.07377 | id:1705.07377 author:Nathan Kallus category:stat.ML cs.LG  published:2017-05-21 summary:We extend the classic multi-armed bandit (MAB) model to the setting of noncompliance, where the arm pull is a mere instrument and the treatment applied may differ from it, which gives rise to the instrument-armed bandit (IAB) problem. The IAB setting is relevant whenever the experimental units are human since free will, ethics, and the law may prohibit unrestricted or forced application of treatment. In particular, the setting is relevant in bandit models of dynamic clinical trials and other controlled trials on human interventions. Nonetheless, the setting has not been fully investigate in the bandit literature. We show that there are various and divergent notions of regret in this setting, all of which coincide only in the classic MAB setting. We characterize the behavior of these regrets and analyze standard MAB algorithms. We argue for a particular kind of regret that captures the causal effect of treatments but show that standard MAB algorithms cannot achieve sublinear control on this regret. Instead, we develop new algorithms for the IAB problem, prove new regret bounds for them, and compare them to standard MAB algorithms in numerical examples. version:1
arxiv-1705-07371 | Spelling Correction as a Foreign Language | http://arxiv.org/abs/1705.07371 | id:1705.07371 author:Yingbo Zhou, Utkarsh Porwal, Roberto Konow category:cs.CL  published:2017-05-21 summary:In this paper, we reformulated the spell correction problem as a machine translation task under the encoder-decoder framework. This reformulation enabled us to use a single model for solving the problem that is traditionally formulated as learning a language model and an error model. This model employs multi-layer recurrent neural networks as an encoder and a decoder. We demonstrate the effectiveness of this model using an internal dataset, where the training data is automatically obtained from user logs. The model offers competitive performance as compared to the state of the art methods but does not require any feature engineering nor hand tuning between models. version:1
arxiv-1705-07366 | Forward Thinking: Building Deep Random Forests | http://arxiv.org/abs/1705.07366 | id:1705.07366 author:Kevin Miller, Chris Hettinger, Jeffrey Humpherys, Tyler Jarvis, David Kartchner category:stat.ML cs.LG  published:2017-05-20 summary:The success of deep neural networks has inspired many to wonder whether other learners could benefit from deep, layered architectures. We present a general framework called forward thinking for deep learning that generalizes the architectural flexibility and sophistication of deep neural networks while also allowing for (i) different types of learning functions in the network, other than neurons, and (ii) the ability to adaptively deepen the network as needed to improve results. This is done by training one layer at a time, and once a layer is trained, the input data are mapped forward through the layer to create a new learning problem. The process is then repeated, transforming the data through multiple layers, one at a time, rendering a new dataset, which is expected to be better behaved, and on which a final output layer can achieve good performance. In the case where the neurons of deep neural nets are replaced with decision trees, we call the result a Forward Thinking Deep Random Forest (FTDRF). We demonstrate a proof of concept by applying FTDRF on the MNIST dataset. We also provide a general mathematical formulation that allows for other types of deep learning problems to be considered. version:1
arxiv-1705-07362 | Honey Bee Dance Modeling in Real-time using Machine Learning | http://arxiv.org/abs/1705.07362 | id:1705.07362 author:Abolfazl Saghafi, Chris P. Tsokos category:stat.ML  published:2017-05-20 summary:The waggle dance that honeybees perform is an astonishing way of communicating the location of food source. After over 60 years of its discovery, researchers still use manual labeling by watching hours of dance videos to detect different transitions between dance components thus extracting information regarding the distance and direction to the food source. We propose an automated process to monitor and segment different components of honeybee waggle dance. The process is highly accurate, runs in real-time, and can use shared information between multiple dances. version:1
arxiv-1705-07347 | Ensemble Sampling | http://arxiv.org/abs/1705.07347 | id:1705.07347 author:Xiuyuan Lu, Benjamin Van Roy category:stat.ML cs.AI cs.LG  published:2017-05-20 summary:Thompson sampling has emerged as an effective heuristic for a broad range of online decision problems. In its basic form, the algorithm requires computing and sampling from a posterior distribution over models, which is tractable only for simple special cases. This paper develops ensemble sampling, which aims to approximate Thompson sampling while maintaining tractability even in the face of complex models such as neural networks. Ensemble sampling dramatically expands on the range of applications for which Thompson sampling is viable. We establish a theoretical basis that supports the approach and present computational results that offer further insight. version:1
arxiv-1705-07340 | Phase-Shifting Separable Haar Wavelets and Applications | http://arxiv.org/abs/1705.07340 | id:1705.07340 author:Mais Alnasser, Hassan Foroosh category:cs.CV  published:2017-05-20 summary:This paper presents a new approach for tackling the shift-invariance problem in the discrete Haar domain, without trading off any of its desirable properties, such as compression, separability, orthogonality, and symmetry. The paper presents several key theoretical contributions. First, we derive closed form expressions for phase shifting in the Haar domain both in partially decimated and fully decimated transforms. Second, it is shown that the wavelet coefficients of the shifted signal can be computed solely by using the coefficients of the original transformed signal. Third, we derive closed-form expressions for non-integer shifts, which have not been previously reported in the literature. Fourth, we establish the complexity of the proposed phase shifting approach using the derived analytic expressions. As an application example of these results, we apply the new formulae to image rotation and interpolation, and evaluate its performance against standard methods. version:1
arxiv-1704-04805 | Replicator Equation: Applications Revisited | http://arxiv.org/abs/1704.04805 | id:1704.04805 author:Tinsae G. Dulecha category:cs.CV  published:2017-04-16 summary:The replicator equation is a simple model of evolution that leads to stable form of Nash Equilibrium, Evolutionary Stable Strategy (ESS). It has been studied in connection with Evolutionary Game Theory and was originally developed for symmetric games. Beyond its first emphasis in biological use, evolutionary game theory has been expanded well beyond in social studies for behavioral analysis, in machine learning, computer vision and others. Its several applications in the fields of machine learning and computer vision has drawn my attention which is the reason to write this extended abstract version:2
arxiv-1705-07329 | Critical Contours: An Invariant Linking Image Flow with Salient Surface Organization | http://arxiv.org/abs/1705.07329 | id:1705.07329 author:Benjamin S. Kunsberg, Steven W. Zucker category:cs.CV  published:2017-05-20 summary:We exploit a key result from visual psychophysics -- that individuals perceive shape qualitatively -- to develop a geometrical/topological invariant (the Morse-Smale complex) relating image structure with surface structure. Differences across individuals are minimal near certain configurations such as ridges and boundaries, and it is these configurations that are often represented in line drawings. In particular, we introduce a method for inferring qualitative 3D shape from shading patterns that link the shape-from-shading inference with shape-from-contour. For a given shape, certain shading patches become "line drawings" in a well-defined limit. Under this limit, and invariantly, these shading patterns provide a topological description of the surface. We further show that, under this model, the contours partition the surface into meaningful parts using the Morse-Smale complex. Critical contours are the (perceptually) stable parts of this complex and are invariant over a wide class of rendering models. Intuitively, our main result shows that critical contours partition smooth surfaces into bumps and valleys, in effect providing a scaffold on the image from which a full surface can be interpolated. version:1
arxiv-1705-07328 | Forecasting Hand and Object Locations in Future Frames | http://arxiv.org/abs/1705.07328 | id:1705.07328 author:Chenyou Fan, Jangwon Lee, Michael S. Ryoo category:cs.CV  published:2017-05-20 summary:This paper presents an approach to forecast future locations of human hands and objects. Given an image frame, the goal is to predict presence and location of hands and objects in the future frame (e.g., 5 seconds later), even when they are not visible in the current frame. The key idea is that (1) an intermediate representation of a convolutional object recognition model abstracts scene information in its frame and that (2) we can predict (i.e., regress) such representations corresponding to the future frames based on that of the current frame. We design a new two-stream convolutional neural network (CNN) architecture for videos by extending the state-of-the-art convolutional object detection network, and present a new fully convolutional regression network for predicting future scene representations. Our experiments confirm that combining the regressed future representation with our detection network allows reliable estimation of future hands and objects in videos version:1
arxiv-1705-07318 | Formalized Lambek Calculus in Higher Order Logic (HOL4) | http://arxiv.org/abs/1705.07318 | id:1705.07318 author:Chun Tian category:cs.CL cs.LO D.2.4  I.2.7  published:2017-05-20 summary:In this project, a rather complete proof-theoretical formalization of Lambek Calculus (non-associative with arbitrary extensions) has been ported from Coq proof assistent to HOL4 theorem prover, with some improvements and new theorems. Three deduction systems (Syntactic Calculus, Natural Deduction and Sequent Calculus) of Lambek Calculus are defined with many related theorems proved. The equivalance between these systems are formally proved. Finally, a formalization of Sequent Calculus proofs (where Coq has built-in supports) has been designed and implemented in HOL4. Some basic results including the sub-formula properties of the so-called "cut-free" proofs are formally proved. This work can be considered as the preliminary work towards a language parser based on category grammars which is not multimodal but still has ability to support context-sensitive languages through customized extensions. version:1
arxiv-1705-07312 | Lower Bound On the Computational Complexity of Discounted Markov Decision Problems | http://arxiv.org/abs/1705.07312 | id:1705.07312 author:Yichen Chen, Mengdi Wang category:cs.CC cs.LG  published:2017-05-20 summary:We study the computational complexity of the infinite-horizon discounted-reward Markov Decision Problem (MDP) with a finite state space $ \mathcal{S} $ and a finite action space $ \mathcal{A} $. We show that any randomized algorithm needs a running time at least $\Omega( \mathcal{S} ^2 \mathcal{A} )$ to compute an $\epsilon$-optimal policy with high probability. We consider two variants of the MDP where the input is given in specific data structures, including arrays of cumulative probabilities and binary trees of transition probabilities. For these cases, we show that the complexity lower bound reduces to $\Omega\left( \frac{ \mathcal{S} \mathcal{A} }{\epsilon} \right)$. These results reveal a surprising observation that the computational complexity of the MDP depends on the data structure of input. version:1
arxiv-1705-05183 | Representation learning of drug and disease terms for drug repositioning | http://arxiv.org/abs/1705.05183 | id:1705.05183 author:Sahil Manchanda, Ashish Anand category:cs.CL  published:2017-05-15 summary:Drug repositioning (DR) refers to identification of novel indications for the approved drugs. The requirement of huge investment of time as well as money and risk of failure in clinical trials have led to surge in interest in drug repositioning. DR exploits two major aspects associated with drugs and diseases: existence of similarity among drugs and among diseases due to their shared involved genes or pathways or common biological effects. Existing methods of identifying drug-disease association majorly rely on the information available in the structured databases only. On the other hand, abundant information available in form of free texts in biomedical research articles are not being fully exploited. Word-embedding or obtaining vector representation of words from a large corpora of free texts using neural network methods have been shown to give significant performance for several natural language processing tasks. In this work we propose a novel way of representation learning to obtain features of drugs and diseases by combining complementary information available in unstructured texts and structured datasets. Next we use matrix completion approach on these feature vectors to learn projection matrix between drug and disease vector spaces. The proposed method has shown competitive performance with state-of-the-art methods. Further, the case studies on Alzheimer's and Hypertension diseases have shown that the predicted associations are matching with the existing knowledge. version:2
arxiv-1705-07290 | Deep Sparse Coding Using Optimized Linear Expansion of Thresholds | http://arxiv.org/abs/1705.07290 | id:1705.07290 author:Debabrata Mahapatra, Subhadip Mukherjee, Chandra Sekhar Seelamantula category:cs.LG 68T05 I.2.6  published:2017-05-20 summary:We address the problem of reconstructing sparse signals from noisy and compressive measurements using a feed-forward deep neural network (DNN) with an architecture motivated by the iterative shrinkage-thresholding algorithm (ISTA). We maintain the weights and biases of the network links as prescribed by ISTA and model the nonlinear activation function using a linear expansion of thresholds (LET), which has been very successful in image denoising and deconvolution. The optimal set of coefficients of the parametrized activation is learned over a training dataset containing measurement-sparse signal pairs, corresponding to a fixed sensing matrix. For training, we develop an efficient second-order algorithm, which requires only matrix-vector product computations in every training epoch (Hessian-free optimization) and offers superior convergence performance than gradient-descent optimization. Subsequently, we derive an improved network architecture inspired by FISTA, a faster version of ISTA, to achieve similar signal estimation performance with about 50% of the number of layers. The resulting architecture turns out to be a deep residual network, which has recently been shown to exhibit superior performance in several visual recognition tasks. Numerical experiments demonstrate that the proposed DNN architectures lead to 3 to 4 dB improvement in the reconstruction signal-to-noise ratio (SNR), compared with the state-of-the-art sparse coding algorithms. version:1
arxiv-1705-07283 | Structured Bayesian Pruning via Log-Normal Multiplicative Noise | http://arxiv.org/abs/1705.07283 | id:1705.07283 author:Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, Dmitry Vetrov category:stat.ML  published:2017-05-20 summary:Dropout-based regularization methods can be regarded as injecting random noise with pre-defined magnitude to different parts of the neural network during training. It was recently shown that Bayesian dropout procedure not only improves generalization but also leads to extremely sparse neural architectures by automatically setting the individual noise magnitude per weight. However, this sparsity can hardly be used for acceleration since it is unstructured. In the paper, we propose a new Bayesian model that takes into account the computational structure of neural networks and provides structured sparsity, e.g. removes neurons and/or convolutional channels in CNNs. To do this, we inject noise to the neurons outputs while keeping the weights unregularized. We established the probabilistic model with a proper truncated log-uniform prior over the noise and truncated log-normal variational approximation that ensures that the KL-term in the evidence lower bound is computed in closed-form. The model leads to structured sparsity by removing elements with a low SNR from the computation graph and provides significant acceleration on a number of deep neural architectures. The model is very easy to implement as it only corresponds to the addition of one dropout-like layer in computation graph. version:1
