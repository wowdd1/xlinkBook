arxiv-1511-02796 | Bayesian Inference in Cumulative Distribution Fields | http://arxiv.org/abs/1511.02796 | id:1511.02796 author:Ricardo Silva category:stat.ML stat.ME  published:2015-11-09 summary:One approach for constructing copula functions is by multiplication. Given that products of cumulative distribution functions (CDFs) are also CDFs, an adjustment to this multiplication will result in a copula model, as discussed by Liebscher (J Mult Analysis, 2008). Parameterizing models via products of CDFs has some advantages, both from the copula perspective (e.g., it is well-defined for any dimensionality) and from general multivariate analysis (e.g., it provides models where small dimensional marginal distributions can be easily read-off from the parameters). Independently, Huang and Frey (J Mach Learn Res, 2011) showed the connection between certain sparse graphical models and products of CDFs, as well as message-passing (dynamic programming) schemes for computing the likelihood function of such models. Such schemes allows models to be estimated with likelihood-based methods. We discuss and demonstrate MCMC approaches for estimating such models in a Bayesian context, their application in copula modeling, and how message-passing can be strongly simplified. Importantly, our view of message-passing opens up possibilities to scaling up such methods, given that even dynamic programming is not a scalable solution for calculating likelihood functions in many models. version:1
arxiv-1506-05446 | Communication-Efficient False Discovery Rate Control via Knockoff Aggregation | http://arxiv.org/abs/1506.05446 | id:1506.05446 author:Weijie Su, Junyang Qian, Linxi Liu category:stat.ML stat.ME  published:2015-06-17 summary:The false discovery rate (FDR)---the expected fraction of spurious discoveries among all the discoveries---provides a popular statistical assessment of the reproducibility of scientific studies in various disciplines. In this work, we introduce a new method for controlling the FDR in meta-analysis of many decentralized linear models. Our method targets the scenario where many research groups---possibly the number of which is random---are independently testing a common set of hypotheses and then sending summary statistics to a coordinating center in an online manner. Built on the knockoffs framework introduced by Barber and Candes (2015), our procedure starts by applying the knockoff filter to each linear model and then aggregates the summary statistics via one-shot communication in a novel way. This method gives exact FDR control non-asymptotically without any knowledge of the noise variances or making any assumption about sparsity of the signal. In certain settings, it has a communication complexity that is optimal up to a logarithmic factor. version:2
arxiv-1403-3465 | A Survey of Algorithms and Analysis for Adaptive Online Learning | http://arxiv.org/abs/1403.3465 | id:1403.3465 author:H. Brendan McMahan category:cs.LG  published:2014-03-14 summary:We present tools for the analysis of Follow-The-Regularized-Leader (FTRL), Dual Averaging, and Mirror Descent algorithms when the regularizer (equivalently, prox-function or learning rate schedule) is chosen adaptively based on the data. Adaptivity can be used to prove regret bounds that hold on every round, and also allows for data-dependent regret bounds as in AdaGrad-style algorithms (e.g., Online Gradient Descent with adaptive per-coordinate learning rates). We present results from a large number of prior works in a unified manner, using a modular and tight analysis that isolates the key arguments in easily re-usable lemmas. This approach strengthens pre-viously known FTRL analysis techniques to produce bounds as tight as those achieved by potential functions or primal-dual analysis. Further, we prove a general and exact equivalence between an arbitrary adaptive Mirror Descent algorithm and a correspond- ing FTRL update, which allows us to analyze any Mirror Descent algorithm in the same framework. The key to bridging the gap between Dual Averaging and Mirror Descent algorithms lies in an analysis of the FTRL-Proximal algorithm family. Our regret bounds are proved in the most general form, holding for arbitrary norms and non-smooth regularizers with time-varying weight. version:3
arxiv-1511-02729 | PAC-Bayesian High Dimensional Bipartite Ranking | http://arxiv.org/abs/1511.02729 | id:1511.02729 author:Benjamin Guedj, Sylvain Robbiano category:stat.ML math.ST stat.TH  published:2015-11-09 summary:This paper is devoted to the bipartite ranking problem, a classical statistical learning task, in a high dimensional setting. We propose a scoring and ranking strategy based on the PAC-Bayesian approach. We consider nonlinear additive scoring functions, and we derive non-asymptotic risk bounds under a sparsity assumption. In particular, oracle inequalities in probability holding under a margin condition assess the performance of our procedure, and prove its minimax optimality. An MCMC-flavored algorithm is proposed to implement our method, along with its behavior on synthetic and real-life datasets. version:1
arxiv-1511-02722 | Learning Instrumental Variables with Non-Gaussianity Assumptions: Theoretical Limitations and Practical Algorithms | http://arxiv.org/abs/1511.02722 | id:1511.02722 author:Ricardo Silva, Shohei Shimizu category:stat.ML  published:2015-11-09 summary:Learning a causal effect from observational data is not straightforward, as this is not possible without further assumptions. If hidden common causes between treatment $X$ and outcome $Y$ cannot be blocked by other measurements, one possibility is to use an instrumental variable. In principle, it is possible under some assumptions to discover whether a variable is structurally instrumental to a target causal effect $X \rightarrow Y$, but current frameworks are somewhat lacking on how general these assumptions can be. A instrumental variable discovery problem is challenging, as no variable can be tested as an instrument in isolation but only in groups, but different variables might require different conditions to be considered an instrument. Moreover, identification constraints might be hard to detect statistically. In this paper, we give a theoretical characterization of instrumental variable discovery, highlighting identifiability problems and solutions, the need for non-Gaussianity assumptions, and how they fit within existing methods. version:1
arxiv-1511-02705 | Biologically Inspired Dynamic Textures for Probing Motion Perception | http://arxiv.org/abs/1511.02705 | id:1511.02705 author:Jonathan Vacher, Andrew Meso, Laurent U Perrinet, Gabriel Peyré category:cs.CV math.ST stat.TH  published:2015-11-09 summary:Perception is often described as a predictive process based on an optimal inference with respect to a generative model. We study here the principled construction of a generative model specifically crafted to probe motion perception. In that context, we first provide an axiomatic, biologically-driven derivation of the model. This model synthesizes random dynamic textures which are defined by stationary Gaussian distributions obtained by the random aggregation of warped patterns. Importantly, we show that this model can equivalently be described as a stochastic partial differential equation. Using this characterization of motion in images, it allows us to recast motion-energy models into a principled Bayesian inference framework. Finally, we apply these textures in order to psychophysically probe speed perception in humans. In this framework, while the likelihood is derived from the generative model, the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion. version:1
arxiv-1502-06800 | On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions | http://arxiv.org/abs/1502.06800 | id:1502.06800 author:Francis Bach category:cs.LG math.NA stat.ML  published:2015-02-24 summary:We show that kernel-based quadrature rules for computing integrals can be seen as a special case of random feature expansions for positive definite kernels, for a particular decomposition that always exists for such kernels. We provide a theoretical analysis of the number of required samples for a given approximation error, leading to both upper and lower bounds that are based solely on the eigenvalues of the associated integral operator and match up to logarithmic terms. In particular, we show that the upper bound may be obtained from independent and identically distributed samples from a specific non-uniform distribution, while the lower bound if valid for any set of points. Applying our results to kernel-based quadrature, while our results are fairly general, we recover known upper and lower bounds for the special cases of Sobolev spaces. Moreover, our results extend to the more general problem of full function approximations (beyond simply computing an integral), with results in L2- and L$\infty$-norm that match known results for special cases. Applying our results to random features, we show an improvement of the number of random features needed to preserve the generalization guarantees for learning with Lipschitz-continuous losses. version:2
arxiv-1511-02683 | A Lightened CNN for Deep Face Representation | http://arxiv.org/abs/1511.02683 | id:1511.02683 author:Xiang Wu, Ran He, Zhenan Sun category:cs.CV  published:2015-11-09 summary:Convolution neural network (CNN) has significantly pushed forward the development of face recognition techniques. To achieve ultimate accuracy, CNN models tend to be deeper or multiple local facial patch ensemble, which result in a waste of time and space. To alleviate this issue, this paper studies a lightened CNN framework to learn a compact embedding for face representation. First, we introduce the concept of maxout in the fully connected layer to the convolution layer, which leads to a new activation function, named Max-Feature-Map (MFM). Compared with widely used ReLU, MFM can simultaneously capture compact representation and competitive information. Then, one shallow CNN model is constructed by 4 convolution layers and totally contains about 4M parameters; and the other is constructed by reducing the kernel size of convolution layers and adding Network in Network (NIN) layers between convolution layers based on the previous one. These models are trained on the CASIA-WebFace dataset and evaluated on the LFW and YTF datasets. Experimental results show that the proposed models achieve state-of-the-art results. At the same time, a reduction of computational cost is reached by over 9 times in comparison with the released VGG model. version:1
arxiv-1511-02682 | Exploiting Egocentric Object Prior for 3D Saliency Detection | http://arxiv.org/abs/1511.02682 | id:1511.02682 author:Gedas Bertasius, Hyun Soo Park, Jianbo Shi category:cs.CV  published:2015-11-09 summary:On a minute-to-minute basis people undergo numerous fluid interactions with objects that barely register on a conscious level. Recent neuroscientific research demonstrates that humans have a fixed size prior for salient objects. This suggests that a salient object in 3D undergoes a consistent transformation such that people's visual system perceives it with an approximately fixed size. This finding indicates that there exists a consistent egocentric object prior that can be characterized by shape, size, depth, and location in the first person view. In this paper, we develop an EgoObject Representation, which encodes these characteristics by incorporating shape, location, size and depth features from an egocentric RGBD image. We empirically show that this representation can accurately characterize the egocentric object prior by testing it on an egocentric RGBD dataset for three tasks: the 3D saliency detection, future saliency prediction, and interaction classification. This representation is evaluated on our new Egocentric RGBD Saliency dataset that includes various activities such as cooking, dining, and shopping. By using our EgoObject representation, we outperform previously proposed models for saliency detection (relative 30% improvement for 3D saliency detection task) on our dataset. Additionally, we demonstrate that this representation allows us to predict future salient objects based on the gaze cue and classify people's interactions with objects. version:1
arxiv-1511-02680 | Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding | http://arxiv.org/abs/1511.02680 | id:1511.02680 author:Alex Kendall, Vijay Badrinarayanan, Roberto Cipolla category:cs.CV cs.NE  published:2015-11-09 summary:We present a novel deep learning framework for probabilistic pixel-wise semantic segmentation, which we term Bayesian SegNet. Pixel-wise semantic segmentation is an important step for visual scene understanding. It is a complex task requiring knowledge of support relationships and contextual information, as well as visual appearance. Our contribution is a practical system which is able to predict pixel-wise class labels with a measure of model uncertainty. We achieve this by Monte Carlo sampling with dropout at test time to generate a posterior distribution of pixel class labels. We show this Bayesian neural network provides a significant performance improvement in segmentation, with no additional parameterisation. We set a new benchmark with state-of-the-art performance on both the indoor SUN Scene Understanding and outdoor CamVid driving scenes datasets. Bayesian SegNet also performs competitively on Pascal VOC 2012 object segmentation challenge. For our web demo and source code, see http://mi.eng.cam.ac.uk/projects/segnet/ version:1
arxiv-1511-02669 | Enacting textual entailment and ontologies for automated essay grading in chemical domain | http://arxiv.org/abs/1511.02669 | id:1511.02669 author:Adrian Groza, Roxana Szabo category:cs.AI cs.CL  published:2015-11-09 summary:We propose a system for automated essay grading using ontologies and textual entailment. The process of textual entailment is guided by hypotheses, which are extracted from a domain ontology. Textual entailment checks if the truth of the hypothesis follows from a given text. We enact textual entailment to compare students answer to a model answer obtained from ontology. We validated the solution against various essays written by students in the chemistry domain. version:1
arxiv-1503-09025 | Learning Definite Horn Formulas from Closure Queries | http://arxiv.org/abs/1503.09025 | id:1503.09025 author:Marta Arias, José L. Balcázar, Cristina Tîrnăucă category:cs.LG cs.LO  published:2015-03-31 summary:A definite Horn theory is a set of n-dimensional Boolean vectors whose characteristic function is expressible as a definite Horn formula, that is, as conjunction of definite Horn clauses. The class of definite Horn theories is known to be learnable under different query learning settings, such as learning from membership and equivalence queries or learning from entailment. We propose yet a different type of query: the closure query. Closure queries are a natural extension of membership queries and also a variant, appropriate in the context of definite Horn formulas, of the so-called correction queries. We present an algorithm that learns conjunctions of definite Horn clauses in polynomial time, using closure and equivalence queries, and show how it relates to the canonical Guigues-Duquenne basis for implicational systems. We also show how the different query models mentioned relate to each other by either showing full-fledged reductions by means of query simulation (where possible), or by showing their connections in the context of particular algorithms that use them for learning definite Horn formulas. version:3
arxiv-1511-02623 | Toward Biochemical Probabilistic Computation | http://arxiv.org/abs/1511.02623 | id:1511.02623 author:Jacques Droulez, David Colliaux, Audrey Houillon, Pierre Bessière category:cs.ET cs.NE q-bio.MN  published:2015-11-09 summary:Living organisms survive and multiply even though they have uncertain and incomplete information about their environment and imperfect models to predict the consequences of their actions. Bayesian models have been proposed to face this challenge. Indeed, Bayesian inference is a way to do optimal reasoning when only uncertain and incomplete information is available. Various perceptive, sensory-motor, and cognitive functions have been successfully modeled this way. However, the biological mechanisms allowing animals and humans to represent and to compute probability distributions are not known. It has been proposed that neurons and assemblies of neurons could be the appropriate scale to search for clues to probabilistic reasoning. In contrast, in this paper, we propose that interacting populations of macromolecules and diffusible messengers can perform probabilistic computation. This suggests that probabilistic reasoning, based on cellular signaling pathways, is a fundamental skill of living organisms available to the simplest unicellular organisms as well as the most complex brains. version:1
arxiv-1511-02619 | Decomposition Bounds for Marginal MAP | http://arxiv.org/abs/1511.02619 | id:1511.02619 author:Wei Ping, Qiang Liu, Alexander Ihler category:cs.LG cs.AI cs.IT math.IT stat.ML  published:2015-11-09 summary:Marginal MAP inference involves making MAP predictions in systems defined with latent variables or missing information. It is significantly more difficult than pure marginalization and MAP tasks, for which a large class of efficient and convergent variational algorithms, such as dual decomposition, exist. In this work, we generalize dual decomposition to a generic power sum inference task, which includes marginal MAP, along with pure marginalization and MAP, as special cases. Our method is based on a block coordinate descent algorithm on a new convex decomposition bound, that is guaranteed to converge monotonically, and can be parallelized efficiently. We demonstrate our approach on marginal MAP queries defined on real-world problems from the UAI approximate inference challenge, showing that our framework is faster and more reliable than previous methods. version:1
arxiv-1511-02595 | A New Relaxation Approach to Normalized Hypergraph Cut | http://arxiv.org/abs/1511.02595 | id:1511.02595 author:Cong Xie, Wu-Jun Li, Zhihua Zhang category:cs.LG cs.DS  published:2015-11-09 summary:Normalized graph cut (NGC) has become a popular research topic due to its wide applications in a large variety of areas like machine learning and very large scale integration (VLSI) circuit design. Most of traditional NGC methods are based on pairwise relationships (similarities). However, in real-world applications relationships among the vertices (objects) may be more complex than pairwise, which are typically represented as hyperedges in hypergraphs. Thus, normalized hypergraph cut (NHC) has attracted more and more attention. Existing NHC methods cannot achieve satisfactory performance in real applications. In this paper, we propose a novel relaxation approach, which is called relaxed NHC (RNHC), to solve the NHC problem. Our model is defined as an optimization problem on the Stiefel manifold. To solve this problem, we resort to the Cayley transformation to devise a feasible learning algorithm. Experimental results on a set of large hypergraph benchmarks for clustering and partitioning in VLSI domain show that RNHC can outperform the state-of-the-art methods. version:1
arxiv-1511-02589 | Parkinson's disease patient rehabilitation using gaming platforms: lessons learnt | http://arxiv.org/abs/1511.02589 | id:1511.02589 author:Ioannis Pachoulakis, Nikolaos Papadopoulos, Cleanthe Spanaki category:cs.CY cs.CV I.6.3; I.6.8  published:2015-11-09 summary:Parkinson's disease (PD) is a progressive neurodegenerative movement disorder where motor dysfunction gradually increases as the disease progress. In addition to administering dopaminergic PD-specific drugs, attending neurologists strongly recommend regular exercise combined with physiotherapy. However, because of the long-term nature of the disease, patients following traditional rehabilitation programs may get bored, lose interest and eventually drop out as a direct result of the repeatability and predictability of the prescribed exercises. Technology supported opportunities to liven up a daily exercise schedule have appeared in the form of character-based, virtual reality games which promote physical training in a non-linear and looser fashion and provide an experience that varies from one game loop the next. Such "exergames", a word that results from the amalgamation of the words "exercise" and "game" challenge patients into performing movements of varying complexity in a playful and immersive virtual environment. Today's game consoles such as Nintendo's Wii, Sony PlayStation Eye and Microsoft's Kinect sensor present new opportunities to infuse motivation and variety to an otherwise mundane physiotherapy routine. In this paper we present some of these approaches, discuss their suitability for these PD patients, mainly on the basis of demands made on balance, agility and gesture precision, and present design principles that exergame platforms must comply with in order to be suitable for PD patients. version:1
arxiv-1511-02583 | Batch-normalized Maxout Network in Network | http://arxiv.org/abs/1511.02583 | id:1511.02583 author:Jia-Ren Chang, Yong-Sheng Chen category:cs.CV cs.LG  published:2015-11-09 summary:This paper reports a novel deep architecture referred to as Maxout network In Network (MIN), which can enhance model discriminability and facilitate the process of information abstraction within the receptive field. The proposed network adopts the framework of the recently developed Network In Network structure, which slides a universal approximator, multilayer perceptron (MLP) with rectifier units, to exact features. Instead of MLP, we employ maxout MLP to learn a variety of piecewise linear activation functions and to mediate the problem of vanishing gradients that can occur when using rectifier units. Moreover, batch normalization is applied to reduce the saturation of maxout units by pre-conditioning the model and dropout is applied to prevent overfitting. Finally, average pooling is used in all pooling layers to regularize maxout MLP in order to facilitate information abstraction in every receptive field while tolerating the change of object position. Because average pooling preserves all features in the local patch, the proposed MIN model can enforce the suppression of irrelevant information during training. Our experiments demonstrated the state-of-the-art classification performance when the MIN model was applied to MNIST, CIFAR-10, and CIFAR-100 datasets and comparable performance for SVHN dataset. version:1
arxiv-1511-02580 | How far can we go without convolution: Improving fully-connected networks | http://arxiv.org/abs/1511.02580 | id:1511.02580 author:Zhouhan Lin, Roland Memisevic, Kishore Konda category:cs.LG cs.NE  published:2015-11-09 summary:We propose ways to improve the performance of fully connected networks. We found that two approaches in particular have a strong effect on performance: linear bottleneck layers and unsupervised pre-training using autoencoders without hidden unit biases. We show how both approaches can be related to improving gradient flow and reducing sparsity in the network. We show that a fully connected network can yield approximately 70% classification accuracy on the permutation-invariant CIFAR-10 task, which is much higher than the current state-of-the-art. By adding deformations to the training data, the fully connected network achieves 78% accuracy, which is just 10% short of a decent convolutional network. version:1
arxiv-1510-08160 | Scale-aware Fast R-CNN for Pedestrian Detection | http://arxiv.org/abs/1510.08160 | id:1510.08160 author:Jianan Li, Xiaodan Liang, ShengMei Shen, Tingfa Xu, Shuicheng Yan category:cs.CV  published:2015-10-28 summary:Intuitively, instances of the same object category with different spatial scales may exhibit dramatically different features. Thus, large variance in instance scales, which results in undesirable large intra-category variance in features, may severely hurt the performance of modern object instance detection methods. We argue that this issue can be substantially alleviated by the divide-and-conquer philosophy. Taking pedestrian detection as an example, we illustrate how we can leverage this philosophy to develop a Scale-Aware Fast R-CNN (SAF R-CNN) framework. The model introduces multiple built-in sub-networks which detect pedestrians with scales from disjoint ranges. Outputs from all the sub-networks are then adaptively combined to generate the final detection results that are shown to be robust to large variance in instance scales, via a gate function defined over the sizes of object proposals. Extensive evaluations on the challenging Caltech pedestrian detection dataset~\cite{dollar2012pedestrian} well demonstrate the superiority of the proposed SAF R-CNN over the state-of-the-arts. Particularly, the miss rate is reduced to $9.68\%$, which is significantly smaller than $11.75\%$ by CompACT-Deep~\cite{compact}, $20.86\%$ by TA-CNN~\cite{ta_cnn} and $12.86\%$ by the vanilla Fast R-CNN model~\cite{girshick2015fast}. version:2
arxiv-1511-02575 | A Century of Portraits: A Visual Historical Record of American High School Yearbooks | http://arxiv.org/abs/1511.02575 | id:1511.02575 author:Shiry Ginosar, Kate Rakelly, Sarah Sachs, Brian Yin, Alexei A. Efros category:cs.CV  published:2015-11-09 summary:Many details about our world are not captured in written records because they are too mundane or too abstract to describe in words. Fortunately, since the invention of the camera, an ever-increasing number of photographs capture much of this otherwise lost information. This plethora of artifacts documenting our "visual culture" is a treasure trove of knowledge as yet untapped by historians. We present a dataset of 37,921 frontal-facing American high school yearbook photos that allow us to use computation to glimpse into the historical visual record too voluminous to be evaluated manually. The collected portraits provide a constant visual frame of reference with varying content. We can therefore use them to consider issues such as a decade's defining style elements, or trends in fashion and social norms over time. We demonstrate that our historical image dataset may be used together with weakly-supervised data-driven techniques to perform scalable historical analysis of large image corpora with minimal human effort, much in the same way that large text corpora together with natural language processing revolutionized historians' workflow. Furthermore, we demonstrate the use of our dataset in dating grayscale portraits using deep learning methods. version:1
arxiv-1506-02190 | Thresholding for Top-k Recommendation with Temporal Dynamics | http://arxiv.org/abs/1506.02190 | id:1506.02190 author:Lei Tang category:cs.IR cs.LG  published:2015-06-06 summary:This work focuses on top-k recommendation in domains where underlying data distribution shifts overtime. We propose to learn a time-dependent bias for each item over whatever existing recommendation engine. Such a bias learning process alleviates data sparsity in constructing the engine, and at the same time captures recent trend shift observed in data. We present an alternating optimization framework to resolve the bias learning problem, and develop methods to handle a variety of commonly used recommendation evaluation criteria, as well as large number of items and users in practice. The proposed algorithm is examined, both offline and online, using real world data sets collected from the largest retailer worldwide. Empirical results demonstrate that the bias learning can almost always boost recommendation performance. We encourage other practitioners to adopt it as a standard component in recommender systems where temporal dynamics is a norm. version:2
arxiv-1511-02556 | Sentiment Expression via Emoticons on Social Media | http://arxiv.org/abs/1511.02556 | id:1511.02556 author:Hao Wang, Jorge A. Castanon category:cs.CL cs.SI  published:2015-11-09 summary:Emoticons (e.g., :) and :( ) have been widely used in sentiment analysis and other NLP tasks as features to ma- chine learning algorithms or as entries of sentiment lexicons. In this paper, we argue that while emoticons are strong and common signals of sentiment expression on social media the relationship between emoticons and sentiment polarity are not always clear. Thus, any algorithm that deals with sentiment polarity should take emoticons into account but extreme cau- tion should be exercised in which emoticons to depend on. First, to demonstrate the prevalence of emoticons on social media, we analyzed the frequency of emoticons in a large re- cent Twitter data set. Then we carried out four analyses to examine the relationship between emoticons and sentiment polarity as well as the contexts in which emoticons are used. The first analysis surveyed a group of participants for their perceived sentiment polarity of the most frequent emoticons. The second analysis examined clustering of words and emoti- cons to better understand the meaning conveyed by the emoti- cons. The third analysis compared the sentiment polarity of microblog posts before and after emoticons were removed from the text. The last analysis tested the hypothesis that removing emoticons from text hurts sentiment classification by training two machine learning models with and without emoticons in the text respectively. The results confirms the arguments that: 1) a few emoticons are strong and reliable signals of sentiment polarity and one should take advantage of them in any senti- ment analysis; 2) a large group of the emoticons conveys com- plicated sentiment hence they should be treated with extreme caution. version:1
arxiv-1511-02543 | Sandwiching the marginal likelihood using bidirectional Monte Carlo | http://arxiv.org/abs/1511.02543 | id:1511.02543 author:Roger B. Grosse, Zoubin Ghahramani, Ryan P. Adams category:stat.ML cs.LG stat.CO  published:2015-11-08 summary:Computing the marginal likelihood (ML) of a model requires marginalizing out all of the parameters and latent variables, a difficult high-dimensional summation or integration problem. To make matters worse, it is often hard to measure the accuracy of one's ML estimates. We present bidirectional Monte Carlo, a technique for obtaining accurate log-ML estimates on data simulated from a model. This method obtains stochastic lower bounds on the log-ML using annealed importance sampling or sequential Monte Carlo, and obtains stochastic upper bounds by running these same algorithms in reverse starting from an exact posterior sample. The true value can be sandwiched between these two stochastic bounds with high probability. Using the ground truth log-ML estimates obtained from our method, we quantitatively evaluate a wide variety of existing ML estimators on several latent variable models: clustering, a low rank approximation, and a binary attributes model. These experiments yield insights into how to accurately estimate marginal likelihoods. version:1
arxiv-1511-02540 | Speed learning on the fly | http://arxiv.org/abs/1511.02540 | id:1511.02540 author:Pierre-Yves Massé, Yann Ollivier category:math.OC cs.LG stat.ML  published:2015-11-08 summary:The practical performance of online stochastic gradient descent algorithms is highly dependent on the chosen step size, which must be tediously hand-tuned in many applications. The same is true for more advanced variants of stochastic gradients, such as SAGA, SVRG, or AdaGrad. Here we propose to adapt the step size by performing a gradient descent on the step size itself, viewing the whole performance of the learning trajectory as a function of step size. Importantly, this adaptation can be computed online at little cost, without having to iterate backward passes over the full data. version:1
arxiv-1411-0306 | Fast Randomized Kernel Methods With Statistical Guarantees | http://arxiv.org/abs/1411.0306 | id:1411.0306 author:Ahmed El Alaoui, Michael W. Mahoney category:stat.ML cs.LG stat.CO  published:2014-11-02 summary:One approach to improving the running time of kernel-based machine learning methods is to build a small sketch of the input and use it in lieu of the full kernel matrix in the machine learning task of interest. Here, we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance. By extending the notion of \emph{statistical leverage scores} to the setting of kernel ridge regression, our main statistical result is to identify an importance sampling distribution that reduces the size of the sketch (i.e., the required number of columns to be sampled) to the \emph{effective dimensionality} of the problem. This quantity is often much smaller than previous bounds that depend on the \emph{maximal degrees of freedom}. Our main algorithmic result is to present a fast algorithm to compute approximations to these scores. This algorithm runs in time that is linear in the number of samples---more precisely, the running time is $O(np^2)$, where the parameter $p$ depends only on the trace of the kernel matrix and the regularization parameter---and it can be applied to the matrix of feature vectors, without having to form the full kernel matrix. This is obtained via a variant of length-squared sampling that we adapt to the kernel setting in a way that is of independent interest. Lastly, we provide empirical results illustrating our theory, and we discuss how this new notion of the statistical leverage of a data point captures in a fine way the difficulty of the original statistical learning problem. version:3
arxiv-1511-02513 | Algorithmic Stability for Adaptive Data Analysis | http://arxiv.org/abs/1511.02513 | id:1511.02513 author:Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, Jonathan Ullman category:cs.LG cs.CR cs.DS  published:2015-11-08 summary:Adaptivity is an important feature of data analysis---the choice of questions to ask about a dataset often depends on previous interactions with the same dataset. However, statistical validity is typically studied in a nonadaptive model, where all questions are specified before the dataset is drawn. Recent work by Dwork et al. (STOC, 2015) and Hardt and Ullman (FOCS, 2014) initiated the formal study of this problem, and gave the first upper and lower bounds on the achievable generalization error for adaptive data analysis. Specifically, suppose there is an unknown distribution $\mathbf{P}$ and a set of $n$ independent samples $\mathbf{x}$ is drawn from $\mathbf{P}$. We seek an algorithm that, given $\mathbf{x}$ as input, accurately answers a sequence of adaptively chosen queries about the unknown distribution $\mathbf{P}$. How many samples $n$ must we draw from the distribution, as a function of the type of queries, the number of queries, and the desired level of accuracy? In this work we make two new contributions: (i) We give upper bounds on the number of samples $n$ that are needed to answer statistical queries. The bounds improve and simplify the work of Dwork et al. (STOC, 2015), and have been applied in subsequent work by those authors (Science, 2015, NIPS, 2015). (ii) We prove the first upper bounds on the number of samples required to answer more general families of queries. These include arbitrary low-sensitivity queries and an important class of optimization queries. As in Dwork et al., our algorithms are based on a connection with algorithmic stability in the form of differential privacy. We extend their work by giving a quantitatively optimal, more general, and simpler proof of their main theorem that stability implies low generalization error. We also study weaker stability guarantees such as bounded KL divergence and total variation distance. version:1
arxiv-1511-02506 | Towards Structured Deep Neural Network for Automatic Speech Recognition | http://arxiv.org/abs/1511.02506 | id:1511.02506 author:Yi-Hsiu Liao, Hung-yi Lee, Lin-shan Lee category:cs.CL cs.LG cs.NE  published:2015-11-08 summary:In this paper we propose the Structured Deep Neural Network (structured DNN) as a structured and deep learning framework. This approach can learn to find the best structured object (such as a label sequence) given a structured input (such as a vector sequence) by globally considering the mapping relationships between the structures rather than item by item. When automatic speech recognition is viewed as a special case of such a structured learning problem, where we have the acoustic vector sequence as the input and the phoneme label sequence as the output, it becomes possible to comprehensively learn utterance by utterance as a whole, rather than frame by frame. Structured Support Vector Machine (structured SVM) was proposed to perform ASR with structured learning previously, but limited by the linear nature of SVM. Here we propose structured DNN to use nonlinear transformations in multi-layers as a structured and deep learning approach. This approach was shown to beat structured SVM in preliminary experiments on TIMIT. version:1
arxiv-1506-02897 | Flowing ConvNets for Human Pose Estimation in Videos | http://arxiv.org/abs/1506.02897 | id:1506.02897 author:Tomas Pfister, James Charles, Andrew Zisserman category:cs.CV  published:2015-06-09 summary:The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow. To this end we propose a network architecture with the following novelties: (i) a deeper network than previously investigated for regressing heatmaps; (ii) spatial fusion layers that learn an implicit spatial model; (iii) optical flow is used to align heatmap predictions from neighbouring frames; and (iv) a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map. We show that this architecture outperforms a number of others, including one that uses optical flow solely at the input layers, one that regresses joint coordinates directly, and one that predicts heatmaps without spatial fusion. The new architecture outperforms the state of the art by a large margin on three video pose estimation datasets, including the very challenging Poses in the Wild dataset, and outperforms other deep methods that don't use a graphical model on the single-image FLIC benchmark (and also Chen & Yuille and Tompson et al. in the high precision region). version:2
arxiv-1509-09236 | On the Complexity of Robust PCA and $\ell_1$-norm Low-Rank Matrix Approximation | http://arxiv.org/abs/1509.09236 | id:1509.09236 author:Nicolas Gillis, Stephen A. Vavasis category:cs.LG cs.CC math.NA math.OC  published:2015-09-30 summary:The low-rank matrix approximation problem with respect to the component-wise $\ell_1$-norm ($\ell_1$-LRA), which is closely related to robust principal component analysis (PCA), has become a very popular tool in data mining and machine learning. Robust PCA aims at recovering a low-rank matrix that was perturbed with sparse noise, with applications for example in foreground-background video separation. Although $\ell_1$-LRA is strongly believed to be NP-hard, there is, to the best of our knowledge, no formal proof of this fact. In this paper, we prove that $\ell_1$-LRA is NP-hard, already in the rank-one case, using a reduction from MAX CUT. Our derivations draw interesting connections between $\ell_1$-LRA and several other well-known problems, namely, robust PCA, $\ell_0$-LRA, binary matrix factorization, a particular densest bipartite subgraph problem, the computation of the cut norm of $\{-1,+1\}$ matrices, and the discrete basis problem, which we all prove to be NP-hard. version:2
arxiv-1411-7591 | An Egocentric Look at Video Photographer Identity | http://arxiv.org/abs/1411.7591 | id:1411.7591 author:Yedid Hoshen, Shmuel Peleg category:cs.CV  published:2014-11-27 summary:Egocentric cameras are being worn by an increasing number of users, among them many security forces worldwide. GoPro cameras already penetrated the mass market, reporting substantial increase in sales every year. As head-worn cameras do not capture the photographer, it may seem that the anonymity of the photographer is preserved even when the video is publicly distributed. We show that camera motion, as can be computed from the egocentric video, provides unique identity information. The photographer can be reliably recognized from a few seconds of video captured when walking. The proposed method achieves more than 90% recognition accuracy in cases where the random success rate is only 3%. Applications can include theft prevention by locking the camera when not worn by its rightful owner. Searching video sharing services (e.g. YouTube) for egocentric videos shot by a specific photographer may also become possible. An important message in this paper is that photographers should be aware that sharing egocentric video will compromise their anonymity, even when their face is not visible. version:3
arxiv-1511-02500 | Poisson Inverse Problems by the Plug-and-Play scheme | http://arxiv.org/abs/1511.02500 | id:1511.02500 author:Arie Rond, Raja Giryes, Michael Elad category:cs.CV math.OC  published:2015-11-08 summary:The Anscombe transform offers an approximate conversion of a Poisson random variable into a Gaussian one. This transform is important and appealing, as it is easy to compute, and becomes handy in various inverse problems with Poisson noise contamination. Solution to such problems can be done by first applying the Anscombe transform, then applying a Gaussian-noise-oriented restoration algorithm of choice, and finally applying an inverse Anscombe transform. The appeal in this approach is due to the abundance of high-performance restoration algorithms designed for white additive Gaussian noise (we will refer to these hereafter as "Gaussian-solvers"). This process is known to work well for high SNR images, where the Anscombe transform provides a rather accurate approximation. When the noise level is high, the above path loses much of its effectiveness, and the common practice is to replace it with a direct treatment of the Poisson distribution. Naturally, with this we lose the ability to leverage on vastly available Gaussian-solvers. In this work we suggest a novel method for coupling Gaussian denoising algorithms to Poisson noisy inverse problems, which is based on a general approach termed "Plug-and-Play". Deploying the Plug-and-Play approach to such problems leads to an iterative scheme that repeats several key steps: 1) A convex programming task of simple form that can be easily treated; 2) A powerful Gaussian denoising algorithm of choice; and 3) A simple update step. Such a modular method, just like the Anscombe transform, enables other developers to plug their own Gaussian denoising algorithms to our scheme in an easy way. While the proposed method bares some similarity to the Anscombe operation, it is in fact based on a different mathematical basis, which holds true for all SNR ranges. version:1
arxiv-1511-02492 | VideoStory Embeddings Recognize Events when Examples are Scarce | http://arxiv.org/abs/1511.02492 | id:1511.02492 author:Amirhossein Habibian, Thomas Mensink, Cees G. M. Snoek category:cs.CV cs.MM  published:2015-11-08 summary:This paper aims for event recognition when video examples are scarce or even completely absent. The key in such a challenging setting is a semantic video representation. Rather than building the representation from individual attribute detectors and their annotations, we propose to learn the entire representation from freely available web videos and their descriptions using an embedding between video features and term vectors. In our proposed embedding, which we call VideoStory, the correlations between the terms are utilized to learn a more effective representation by optimizing a joint objective balancing descriptiveness and predictability.We show how learning the VideoStory using a multimodal predictability loss, including appearance, motion and audio features, results in a better predictable representation. We also propose a variant of VideoStory to recognize an event in video from just the important terms in a text query by introducing a term sensitive descriptiveness loss. Our experiments on three challenging collections of web videos from the NIST TRECVID Multimedia Event Detection and Columbia Consumer Videos datasets demonstrate: i) the advantages of VideoStory over representations using attributes or alternative embeddings, ii) the benefit of fusing video modalities by an embedding over common strategies, iii) the complementarity of term sensitive descriptiveness and multimodal predictability for event recognition without examples. By it abilities to improve predictability upon any underlying video feature while at the same time maximizing semantic descriptiveness, VideoStory leads to state-of-the-art accuracy for both few- and zero-example recognition of events in video. version:1
arxiv-1511-02465 | A new humanlike facial attractiveness predictor with cascaded fine-tuning deep learning model | http://arxiv.org/abs/1511.02465 | id:1511.02465 author:Jie Xu, Lianwen Jin, Lingyu Liang, Ziyong Feng, Duorui Xie category:cs.CV  published:2015-11-08 summary:This paper proposes a deep leaning method to address the challenging facial attractiveness prediction problem. The method constructs a convolutional neural network of facial beauty prediction using a new deep cascaded fine-turning scheme with various face inputting channels, such as the original RGB face image, the detail layer image, and the lighting layer image. With a carefully designed CNN model of deep structure, large input size and small convolutional kernels, we have achieved a high prediction correlation of 0.88. This result convinces us that the problem of facial attractiveness prediction can be solved by deep learning approach, and it also shows the important roles of the facial smoothness, lightness, and color information that were involved in facial beauty perception, which is consistent with the result of recent psychology studies. Furthermore, we analyze the high-level features learnt by CNN through visualization of its hidden layers, and some interesting phenomena were observed. It is found that the contours and appearance of facial features, especially eyes and moth, are the most significant facial attributes for facial attractiveness prediction, which is also consistent with the visual perception intuition of human. version:1
arxiv-1511-02459 | SCUT-FBP: A Benchmark Dataset for Facial Beauty Perception | http://arxiv.org/abs/1511.02459 | id:1511.02459 author:Duorui Xie, Lingyu Liang, Lianwen Jin, Jie Xu, Mengru Li category:cs.CV  published:2015-11-08 summary:In this paper, a novel face dataset with attractiveness ratings, namely, the SCUT-FBP dataset, is developed for automatic facial beauty perception. This dataset provides a benchmark to evaluate the performance of different methods for facial attractiveness prediction, including the state-of-the-art deep learning method. The SCUT-FBP dataset contains face portraits of 500 Asian female subjects with attractiveness ratings, all of which have been verified in terms of rating distribution, standard deviation, consistency, and self-consistency. Benchmark evaluations for facial attractiveness prediction were performed with different combinations of facial geometrical features and texture features using classical statistical learning methods and the deep learning method. The best Pearson correlation (0.8187) was achieved by the CNN model. Thus, the results of our experiments indicate that the SCUT-FBP dataset provides a reliable benchmark for facial beauty perception. version:1
arxiv-1511-01258 | Learn on Source, Refine on Target:A Model Transfer Learning Framework with Random Forests | http://arxiv.org/abs/1511.01258 | id:1511.01258 author:Noam Segev, Maayan Harel, Shie Mannor, Koby Crammer, Ran El-Yaniv category:cs.LG  published:2015-11-04 summary:We propose novel model transfer-learning methods that refine a decision forest model M learned within a "source" domain using a training set sampled from a "target" domain, assumed to be a variation of the source. We present two random forest transfer algorithms. The first algorithm searches greedily for locally optimal modifications of each tree structure by trying to locally expand or reduce the tree around individual nodes. The second algorithm does not modify structure, but only the parameter (thresholds) associated with decision nodes. We also propose to combine both methods by considering an ensemble that contains the union of the two forests. The proposed methods exhibit impressive experimental results over a range of problems. version:2
arxiv-1511-02435 | A Chinese POS Decision Method Using Korean Translation Information | http://arxiv.org/abs/1511.02435 | id:1511.02435 author:Son-Il Kwak, O-Chol Kown, Chang-Sin Kim, Yong-Il Pak, Gum-Chol Son, Chol-Jun Hwang, Hyon-Chol Kim, Hyok-Chol Sin, Gyong-Il Hyon, Sok-Min Han category:cs.CL  published:2015-11-08 summary:In this paper we propose a method that imitates a translation expert using the Korean translation information and analyse the performance. Korean is good at tagging than Chinese, so we can use this property in Chinese POS tagging. version:1
arxiv-1511-02402 | Max-Sum Diversification, Monotone Submodular Functions and Semi-metric Spaces | http://arxiv.org/abs/1511.02402 | id:1511.02402 author:Sepehr Abbasi Zadeh, Mehrdad Ghadiri category:cs.LG  published:2015-11-07 summary:In many applications such as web-based search, document summarization, facility location and other applications, the results are preferable to be both representative and diversified subsets of documents. The goal of this study is to select a good "quality", bounded-size subset of a given set of items, while maintaining their diversity relative to a semi-metric distance function. This problem was first studied by Borodin et al\cite{borodin}, but a crucial property used throughout their proof is the triangle inequality. In this modified proof, we want to relax the triangle inequality and relate the approximation ratio of max-sum diversification problem to the parameter of the relaxed triangle inequality in the normal form of the problem (i.e., a uniform matroid) and also in an arbitrary matroid. version:1
arxiv-1511-01754 | Symmetry-invariant optimization in deep networks | http://arxiv.org/abs/1511.01754 | id:1511.01754 author:Vijay Badrinarayanan, Bamdev Mishra, Roberto Cipolla category:cs.LG cs.AI cs.CV  published:2015-11-05 summary:Recent works have highlighted scale invariance or symmetry that is present in the weight space of a typical deep network and the adverse effect that it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that these and other commonly used deep networks, such as those which use a max-pooling and sub-sampling layer, possess more complex forms of symmetry arising from scaling based reparameterization of the network weights. We then propose two symmetry-invariant gradient based weight updates for stochastic gradient descent based learning. Our empirical evidence based on the MNIST dataset shows that these updates improve the test performance without sacrificing the computational efficiency of the weight updates. We also show the results of training with one of the proposed weight updates on an image segmentation problem. version:2
arxiv-1511-02385 | Review-Level Sentiment Classification with Sentence-Level Polarity Correction | http://arxiv.org/abs/1511.02385 | id:1511.02385 author:Sylvester Olubolu Orimaye, Saadat M. Alhashmi, Eu-Gene Siew, Sang Jung Kang category:cs.CL cs.AI cs.LG  published:2015-11-07 summary:We propose an effective technique to solving review-level sentiment classification problem by using sentence-level polarity correction. Our polarity correction technique takes into account the consistency of the polarities (positive and negative) of sentences within each product review before performing the actual machine learning task. While sentences with inconsistent polarities are removed, sentences with consistent polarities are used to learn state-of-the-art classifiers. The technique achieved better results on different types of products reviews and outperforms baseline models without the correction technique. Experimental results show an average of 82% F-measure on four different product review domains. version:1
arxiv-1511-02319 | Review of Person Re-identification Techniques | http://arxiv.org/abs/1511.02319 | id:1511.02319 author:Mohammad Ali Saghafi, Aini Hussain, Halimah Badioze Zaman, Mohamad Hanif Md Saad category:cs.CV  published:2015-11-07 summary:Person re-identification across different surveillance cameras with disjoint fields of view has become one of the most interesting and challenging subjects in the area of intelligent video surveillance. Although several methods have been developed and proposed, certain limitations and unresolved issues remain. In all of the existing re-identification approaches, feature vectors are extracted from segmented still images or video frames. Different similarity or dissimilarity measures have been applied to these vectors. Some methods have used simple constant metrics, whereas others have utilised models to obtain optimised metrics. Some have created models based on local colour or texture information, and others have built models based on the gait of people. In general, the main objective of all these approaches is to achieve a higher-accuracy rate and lowercomputational costs. This study summarises several developments in recent literature and discusses the various available methods used in person re-identification. Specifically, their advantages and disadvantages are mentioned and compared. version:1
arxiv-1310-2627 | A Sparse and Adaptive Prior for Time-Dependent Model Parameters | http://arxiv.org/abs/1310.2627 | id:1310.2627 author:Dani Yogatama, Bryan R. Routledge, Noah A. Smith category:stat.ML cs.AI cs.LG  published:2013-10-09 summary:We consider the scenario where the parameters of a probabilistic model are expected to vary over time. We construct a novel prior distribution that promotes sparsity and adapts the strength of correlation between parameters at successive timesteps, based on the data. We derive approximate variational inference procedures for learning and prediction with this prior. We test the approach on two tasks: forecasting financial quantities from relevant text, and modeling language contingent on time-varying financial measurements. version:2
arxiv-1511-02282 | Fingertip in the Eye: A cascaded CNN pipeline for the real-time fingertip detection in egocentric videos | http://arxiv.org/abs/1511.02282 | id:1511.02282 author:Xiaorui Liu, Yichao Huang, Xin Zhang, Lianwen Jin category:cs.CV  published:2015-11-07 summary:We introduce a new pipeline for hand localization and fingertip detection. For RGB images captured from an egocentric vision mobile camera, hand and fingertip detection remains a challenging problem due to factors like background complexity and hand shape variety. To address these issues accurately and robustly, we build a large scale dataset named Ego-Fingertip and propose a bi-level cascaded pipeline of convolutional neural networks, namely, Attention-based Hand Detector as well as Multi-point Fingertip Detector. The proposed method significantly tackles challenges and achieves satisfactorily accurate prediction and real-time performance compared to previous hand and fingertip detection methods. version:1
arxiv-1511-02270 | Signed Support Recovery for Single Index Models in High-Dimensions | http://arxiv.org/abs/1511.02270 | id:1511.02270 author:Matey Neykov, Qian Lin, Jun S. Liu category:math.ST stat.ML stat.TH  published:2015-11-07 summary:In this paper we study the support recovery problem for single index models $Y=f(\boldsymbol{X}^{\intercal} \boldsymbol{\beta},\varepsilon)$, where $f$ is an unknown link function, $\boldsymbol{X}\sim N_p(0,\mathbb{I}_{p})$ and $\boldsymbol{\beta}$ is an $s$-sparse unit vector such that $\boldsymbol{\beta}_{i}\in \{\pm\frac{1}{\sqrt{s}},0\}$. In particular, we look into the performance of two computationally inexpensive algorithms: (a) the diagonal thresholding sliced inverse regression (DT-SIR) introduced by Lin et al. (2015); and (b) a semi-definite programming (SDP) approach inspired by Amini & Wainwright (2008). When $s=O(p^{1-\delta})$ for some $\delta>0$, we demonstrate that both procedures can succeed in recovering the support of $\boldsymbol{\beta}$ as long as the rescaled sample size $\kappa=\frac{n}{s\log(p-s)}$ is larger than a certain critical threshold. On the other hand, when $\kappa$ is smaller than a critical value, any algorithm fails to recover the support with probability at least $\frac{1}{2}$ asymptotically. In other words, we demonstrate that both DT-SIR and the SDP approach are optimal (up to a scalar) for recovering the support of $\boldsymbol{\beta}$ in terms of sample size. version:1
arxiv-1506-04416 | Bayesian Dark Knowledge | http://arxiv.org/abs/1506.04416 | id:1506.04416 author:Anoop Korattikara, Vivek Rathod, Kevin Murphy, Max Welling category:cs.LG stat.ML  published:2015-06-14 summary:We consider the problem of Bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/ or where we need accurate posterior predictive densities, e.g., for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time). We describe a method for "distilling" a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [Hernandez-Lobato and Adams, 2015] and an approach based on variational Bayes [Blundell et al., 2015]. Our method performs better than both of these, is much simpler to implement, and uses less computation at test time. version:3
arxiv-1511-02254 | Active Perceptual Similarity Modeling with Auxiliary Information | http://arxiv.org/abs/1511.02254 | id:1511.02254 author:Eric Heim, Matthew Berger, Lee Seversky, Milos Hauskrecht category:cs.LG stat.ML  published:2015-11-06 summary:Learning a model of perceptual similarity from a collection of objects is a fundamental task in machine learning underlying numerous applications. A common way to learn such a model is from relative comparisons in the form of triplets: responses to queries of the form "Is object a more similar to b than it is to c?". If no consideration is made in the determination of which queries to ask, existing similarity learning methods can require a prohibitively large number of responses. In this work, we consider the problem of actively learning from triplets -finding which queries are most useful for learning. Different from previous active triplet learning approaches, we incorporate auxiliary information into our similarity model and introduce an active learning scheme to find queries that are informative for quickly learning both the relevant aspects of auxiliary data and the directly-learned similarity components. Compared to prior approaches, we show that we can learn just as effectively with much fewer queries. For evaluation, we introduce a new dataset of exhaustive triplet comparisons obtained from humans and demonstrate improved performance for different types of auxiliary information. version:1
arxiv-1511-02251 | Learning Visual Features from Large Weakly Supervised Data | http://arxiv.org/abs/1511.02251 | id:1511.02251 author:Armand Joulin, Laurens van der Maaten, Allan Jabri, Nicolas Vasilache category:cs.CV  published:2015-11-06 summary:Convolutional networks trained on large supervised dataset produce visual features which form the basis for the state-of-the-art in many computer-vision problems. Further improvements of these visual features will likely require even larger manually labeled data sets, which severely limits the pace at which progress can be made. In this paper, we explore the potential of leveraging massive, weakly-labeled image collections for learning good visual features. We train convolutional networks on a dataset of 100 million Flickr photos and captions, and show that these networks produce features that perform well in a range of vision problems. We also show that the networks appropriately capture word similarity, and learn correspondences between different languages. version:1
arxiv-1511-02228 | Seven ways to improve example-based single image super resolution | http://arxiv.org/abs/1511.02228 | id:1511.02228 author:Radu Timofte, Rasmus Rothe, Luc Van Gool category:cs.CV  published:2015-11-06 summary:In this paper we present seven techniques that everybody should know to improve example-based single image super resolution (SR): 1) augmentation of data, 2) use of large dictionaries with efficient search structures, 3) cascading, 4) image self-similarities, 5) back projection refinement, 6) enhanced prediction by consistency check, and 7) context reasoning. We validate our seven techniques on standard SR benchmarks (i.e. Set5, Set14, B100) and methods (i.e. A+, SRCNN, ANR, Zeyde, Yang) and achieve substantial improvements.The techniques are widely applicable and require no changes or only minor adjustments of the SR methods. Moreover, our Improved A+ (IA) method sets new state-of-the-art results outperforming A+ by up to 0.9dB on average PSNR whilst maintaining a low time complexity. version:1
arxiv-1511-02222 | Deep Kernel Learning | http://arxiv.org/abs/1511.02222 | id:1511.02222 author:Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing category:cs.LG cs.AI stat.ME stat.ML  published:2015-11-06 summary:We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost $O(n)$ for $n$ training points, and predictions cost $O(1)$ per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures. version:1
arxiv-1510-04781 | A Survey: Time Travel in Deep Learning Space: An Introduction to Deep Learning Models and How Deep Learning Models Evolved from the Initial Ideas | http://arxiv.org/abs/1510.04781 | id:1510.04781 author:Haohan Wang, Bhiksha Raj category:cs.LG cs.NE  published:2015-10-16 summary:This report will show the history of deep learning evolves. It will trace back as far as the initial belief of connectionism modelling of brain, and come back to look at its early stage realization: neural networks. With the background of neural network, we will gradually introduce how convolutional neural network, as a representative of deep discriminative models, is developed from neural networks, together with many practical techniques that can help in optimization of neural networks. On the other hand, we will also trace back to see the evolution history of deep generative models, to see how researchers balance the representation power and computation complexity to reach Restricted Boltzmann Machine and eventually reach Deep Belief Nets. Further, we will also look into the development history of modelling time series data with neural networks. We start with Time Delay Neural Networks and move further to currently famous model named Recurrent Neural Network and its extension Long Short Term Memory. We will also briefly look into how to construct deep recurrent neural networks. Finally, we will conclude this report with some interesting open-ended questions of deep neural networks. version:2
arxiv-1511-02204 | An Extended Frank-Wolfe Method with "In-Face" Directions, and its Application to Low-Rank Matrix Completion | http://arxiv.org/abs/1511.02204 | id:1511.02204 author:Robert M. Freund, Paul Grigas, Rahul Mazumder category:math.OC stat.CO stat.ML 90C25 G.1.6  published:2015-11-06 summary:Motivated principally by the low-rank matrix completion problem, we present an extension of the Frank-Wolfe method that is designed to induce near-optimal solutions on low-dimensional faces of the feasible region. This is accomplished by a new approach to generating ``in-face" directions at each iteration, as well as through new choice rules for selecting between in-face and ``regular" Frank-Wolfe steps. Our framework for generating in-face directions generalizes the notion of away-steps introduced by Wolfe. In particular, the in-face directions always keep the next iterate within the minimal face containing the current iterate. We present computational guarantees for the new method that trade off efficiency in computing near-optimal solutions with upper bounds on the dimension of minimal faces of iterates. We apply the new method to the matrix completion problem, where low-dimensional faces correspond to low-rank matrices. We present computational results that demonstrate the effectiveness of our methodological approach at producing nearly-optimal solutions of very low rank. On both artificial and real datasets, we demonstrate significant speed-ups in computing very low-rank nearly-optimal solutions as compared to either the Frank-Wolfe method or its traditional away-step variant. version:1
arxiv-1511-02196 | Evaluating Protein-protein Interaction Predictors with a Novel 3-Dimensional Metric | http://arxiv.org/abs/1511.02196 | id:1511.02196 author:Haohan Wang, Madhavi K. Ganapathiraju category:cs.LG  published:2015-11-06 summary:In order for the predicted interactions to be directly adopted by biologists, the ma- chine learning predictions have to be of high precision, regardless of recall. This aspect cannot be evaluated or numerically represented well by traditional metrics like accuracy, ROC, or precision-recall curve. In this work, we start from the alignment in sensitivity of ROC and recall of precision-recall curve, and propose an evaluation metric focusing on the ability of a model to be adopted by biologists. This metric evaluates the ability of a machine learning algorithm to predict only new interactions, meanwhile, it eliminates the influence of test dataset. In the experiment of evaluating different classifiers with a same data set and evaluating the same predictor with different datasets, our new metric fulfills the evaluation task of our interest while two widely recognized metrics, ROC and precision-recall curve fail the tasks for different reasons. version:1
arxiv-1511-02176 | Optimal Non-Asymptotic Lower Bound on the Minimax Regret of Learning with Expert Advice | http://arxiv.org/abs/1511.02176 | id:1511.02176 author:Francesco Orabona, David Pal category:stat.ML cs.LG  published:2015-11-06 summary:We prove non-asymptotic lower bounds on the expectation of the maximum of $d$ independent Gaussian variables and the expectation of the maximum of $d$ independent symmetric random walks. Both lower bounds recover the optimal leading constant in the limit. A simple application of the lower bound for random walks is an (asymptotically optimal) non-asymptotic lower bound on the minimax regret of online learning with expert advice. version:1
arxiv-1511-02126 | Pooling the Convolutional Layers in Deep ConvNets for Action Recognition | http://arxiv.org/abs/1511.02126 | id:1511.02126 author:Shichao Zhao, Yanbin Liu, Yahong Han, Richang Hong category:cs.CV  published:2015-11-06 summary:Deep ConvNets have shown its good performance in image classification tasks. However it still remains as a problem in deep video representation for action recognition. The problem comes from two aspects: on one hand, current video ConvNets are relatively shallow compared with image ConvNets, which limits its capability of capturing the complex video action information; on the other hand, temporal information of videos is not properly utilized to pool and encode the video sequences. Towards these issues, in this paper, we utilize two state-of-the-art ConvNets, i.e., the very deep spatial net (VGGNet) and the temporal net from Two-Stream ConvNets, for action representation. The convolutional layers and the proposed new layer, called frame-diff layer, are extracted and pooled with two temporal pooling strategy: Trajectory pooling and line pooling. The pooled local descriptors are then encoded with VLAD to form the video representations. In order to verify the effectiveness of the proposed framework, we conduct experiments on UCF101 and HMDB51 datasets. It achieves the accuracy of 93.78\% on UCF101 which is the state-of-the-art and the accuracy of 65.62\% on HMDB51 which is comparable to the state-of-the-art. version:1
arxiv-1511-02117 | Introducing SKYSET - a Quintuple Approach for Improving Instructions | http://arxiv.org/abs/1511.02117 | id:1511.02117 author:Kerry Fultz, Seth Filip category:cs.CL  published:2015-11-06 summary:A new approach called SKYSET (Synthetic Knowledge Yield Social Entities Translation) is proposed to validate completeness and to reduce ambiguity from written instructional documentation. SKYSET utilizes a quintuple set of standardized categories, which differs from traditional approaches that typically use triples. The SKYSET System defines the categories required to form a standard template for representing information that is portable across different domains. It provides a standardized framework that enables sentences from written instructions to be translated into sets of category typed entities on a table or database. The SKYSET entities contain conceptual units or phrases that represent information from the original source documentation. SKYSET enables information concatenation where multiple documents from different domains can be translated and combined into a single common filterable and searchable table of entities. version:1
arxiv-1511-02086 | Hierarchical Coupled Geometry Analysis for Neuronal Structure and Activity Pattern Discovery | http://arxiv.org/abs/1511.02086 | id:1511.02086 author:Gal Mishne, Ronen Talmon, Ron Meir, Jackie Schiller, Uri Dubin, Ronald R. Coifman category:q-bio.QM q-bio.NC stat.ML  published:2015-11-06 summary:In the wake of recent advances in experimental methods in neuroscience, the ability to record in-vivo neuronal activity from awake animals has become feasible. The availability of such rich and detailed physiological measurements calls for the development of advanced data analysis tools, as commonly used techniques do not suffice to capture the spatio-temporal network complexity. In this paper, we propose a new hierarchical coupled geometry analysis, which exploits the hidden connectivity structures between neurons and the dynamic patterns at multiple time-scales. Our approach gives rise to the joint organization of neurons and dynamic patterns in data-driven hierarchical data structures. These structures provide local to global data representations, from local partitioning of the data in flexible trees through a new multiscale metric to a global manifold embedding. The application of our techniques to in-vivo neuronal recordings demonstrate the capability of extracting neuronal activity patterns and identifying temporal trends, associated with particular behavioral events and manipulations introduced in the experiments. version:1
arxiv-1505-07376 | Texture Synthesis Using Convolutional Neural Networks | http://arxiv.org/abs/1505.07376 | id:1505.07376 author:Leon A. Gatys, Alexander S. Ecker, Matthias Bethge category:cs.CV cs.NE q-bio.NC  published:2015-05-27 summary:Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks. version:3
arxiv-1511-02037 | ALOJA: A Framework for Benchmarking and Predictive Analytics in Big Data Deployments | http://arxiv.org/abs/1511.02037 | id:1511.02037 author:Josep Ll. Berral, Nicolas Poggi, David Carrera, Aaron Call, Rob Reinauer, Daron Green category:cs.LG cs.DC C.4; I.2.6  published:2015-11-06 summary:This article presents the ALOJA project and its analytics tools, which leverages machine learning to interpret Big Data benchmark performance data and tuning. ALOJA is part of a long-term collaboration between BSC and Microsoft to automate the characterization of cost-effectiveness on Big Data deployments, currently focusing on Hadoop. Hadoop presents a complex run-time environment, where costs and performance depend on a large number of configuration choices. The ALOJA project has created an open, vendor-neutral repository, featuring over 40,000 Hadoop job executions and their performance details. The repository is accompanied by a test-bed and tools to deploy and evaluate the cost-effectiveness of different hardware configurations, parameters and Cloud services. Despite early success within ALOJA, a comprehensive study requires automation of modeling procedures to allow an analysis of large and resource-constrained search spaces. The predictive analytics extension, ALOJA-ML, provides an automated system allowing knowledge discovery by modeling environments from observed executions. The resulting models can forecast execution behaviors, predicting execution times for new configurations and hardware choices. That also enables model-based anomaly detection or efficient benchmark guidance by prioritizing executions. In addition, the community can benefit from ALOJA data-sets and framework to improve the design and deployment of Big Data applications. version:1
arxiv-1511-02030 | ALOJA-ML: A Framework for Automating Characterization and Knowledge Discovery in Hadoop Deployments | http://arxiv.org/abs/1511.02030 | id:1511.02030 author:Josep Ll. Berral, Nicolas Poggi, David Carrera, Aaron Call, Rob Reinauer, Daron Green category:cs.LG cs.DC C.4; I.2.6  published:2015-11-06 summary:This article presents ALOJA-Machine Learning (ALOJA-ML) an extension to the ALOJA project that uses machine learning techniques to interpret Hadoop benchmark performance data and performance tuning; here we detail the approach, efficacy of the model and initial results. Hadoop presents a complex execution environment, where costs and performance depends on a large number of software (SW) configurations and on multiple hardware (HW) deployment choices. These results are accompanied by a test bed and tools to deploy and evaluate the cost-effectiveness of the different hardware configurations, parameter tunings, and Cloud services. Despite early success within ALOJA from expert-guided benchmarking, it became clear that a genuinely comprehensive study requires automation of modeling procedures to allow a systematic analysis of large and resource-constrained search spaces. ALOJA-ML provides such an automated system allowing knowledge discovery by modeling Hadoop executions from observed benchmarks across a broad set of configuration parameters. The resulting performance models can be used to forecast execution behavior of various workloads; they allow 'a-priori' prediction of the execution times for new configurations and HW choices and they offer a route to model-based anomaly detection. In addition, these models can guide the benchmarking exploration efficiently, by automatically prioritizing candidate future benchmark tests. Insights from ALOJA-ML's models can be used to reduce the operational time on clusters, speed-up the data acquisition and knowledge discovery process, and importantly, reduce running costs. In addition to learning from the methodology presented in this work, the community can benefit in general from ALOJA data-sets, framework, and derived insights to improve the design and deployment of Big Data applications. version:1
arxiv-1511-02024 | Towards a Better Understanding of Predict and Count Models | http://arxiv.org/abs/1511.02024 | id:1511.02024 author:S. Sathiya Keerthi, Tobias Schnabel, Rajiv Khanna category:cs.LG cs.CL  published:2015-11-06 summary:In a recent paper, Levy and Goldberg pointed out an interesting connection between prediction-based word embedding models and count models based on pointwise mutual information. Under certain conditions, they showed that both models end up optimizing equivalent objective functions. This paper explores this connection in more detail and lays out the factors leading to differences between these models. We find that the most relevant differences from an optimization perspective are (i) predict models work in a low dimensional space where embedding vectors can interact heavily; (ii) since predict models have fewer parameters, they are less prone to overfitting. Motivated by the insight of our analysis, we show how count models can be regularized in a principled manner and provide closed-form solutions for L1 and L2 regularization. Finally, we propose a new embedding model with a convex objective and the additional benefit of being intelligible. version:1
arxiv-1511-02023 | Facial Expression Recognition Using Sparse Gaussian Conditional Random Field | http://arxiv.org/abs/1511.02023 | id:1511.02023 author:Mohammadamin Abbasnejad, Mohammad Ali Masnadi-Shirazi category:cs.CV  published:2015-11-06 summary:The analysis of expression and facial Action Units (AUs) detection are very important tasks in fields of computer vision and Human Computer Interaction (HCI) due to the wide range of applications in human life. Many works has been done during the past few years which has their own advantages and disadvantages. In this work we present a new model based on Gaussian Conditional Random Field. We solve our objective problem using ADMM and we show how well the proposed model works. We train and test our work on two facial expression datasets, CK+ and RU-FACS. Experimental evaluation shows that our proposed approach outperform state of the art expression recognition. version:1
arxiv-1511-01994 | Next Generation Multicuts for Semi-Planar Graphs | http://arxiv.org/abs/1511.01994 | id:1511.01994 author:Julian Yarkony category:cs.CV cs.DS  published:2015-11-06 summary:We study the problem of multicut segmentation. We introduce modified versions of the Semi-PlanarCC based on bounding Lagrange multipliers. We apply our work to natural image segmentation. version:1
arxiv-1511-01987 | Neutralized Empirical Risk Minimization with Generalization Neutrality Bound | http://arxiv.org/abs/1511.01987 | id:1511.01987 author:Kazuto Fukuchi, Jun Sakuma category:stat.ML  published:2015-11-06 summary:Currently, machine learning plays an important role in the lives and individual activities of numerous people. Accordingly, it has become necessary to design machine learning algorithms to ensure that discrimination, biased views, or unfair treatment do not result from decision making or predictions made via machine learning. In this work, we introduce a novel empirical risk minimization (ERM) framework for supervised learning, neutralized ERM (NERM) that ensures that any classifiers obtained can be guaranteed to be neutral with respect to a viewpoint hypothesis. More specifically, given a viewpoint hypothesis, NERM works to find a target hypothesis that minimizes the empirical risk while simultaneously identifying a target hypothesis that is neutral to the viewpoint hypothesis. Within the NERM framework, we derive a theoretical bound on empirical and generalization neutrality risks. Furthermore, as a realization of NERM with linear classification, we derive a max-margin algorithm, neutral support vector machine (SVM). Experimental results show that our neutral SVM shows improved classification performance in real datasets without sacrificing the neutrality guarantee. version:1
arxiv-1511-01974 | Multi-lingual Geoparsing based on Machine Translation | http://arxiv.org/abs/1511.01974 | id:1511.01974 author:Xu Chen, Han Zhang, Judith Gelernter category:cs.CL cs.IR  published:2015-11-06 summary:Our method for multi-lingual geoparsing uses monolingual tools and resources along with machine translation and alignment to return location words in many languages. Not only does our method save the time and cost of developing geoparsers for each language separately, but also it allows the possibility of a wide range of language capabilities within a single interface. We evaluated our method in our LanguageBridge prototype on location named entities using newswire, broadcast news and telephone conversations in English, Arabic and Chinese data from the Linguistic Data Consortium (LDC). Our results for geoparsing Chinese and Arabic text using our multi-lingual geoparsing method are comparable to our results for geoparsing English text with our English tools. Furthermore, experiments using our machine translation approach results in accuracy comparable to results from the same data that was translated manually. version:1
arxiv-1502-03475 | Combinatorial Bandits Revisited | http://arxiv.org/abs/1502.03475 | id:1502.03475 author:Richard Combes, M. Sadegh Talebi, Alexandre Proutiere, Marc Lelarge category:cs.LG math.OC stat.ML  published:2015-02-11 summary:This paper investigates stochastic and adversarial combinatorial multi-armed bandit problems. In the stochastic setting under semi-bandit feedback, we derive a problem-specific regret lower bound, and discuss its scaling with the dimension of the decision space. We propose ESCB, an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret. ESCB has better performance guarantees than existing algorithms, and significantly outperforms these algorithms in practice. In the adversarial setting under bandit feedback, we propose \textsc{CombEXP}, an algorithm with the same regret scaling as state-of-the-art algorithms, but with lower computational complexity for some combinatorial problems. version:3
arxiv-1511-01954 | Recovering hard-to-find object instances by sampling context-based object proposals | http://arxiv.org/abs/1511.01954 | id:1511.01954 author:Jose Oramas M., Tinne Tuytelaars category:cs.CV  published:2015-11-05 summary:In this paper we focus on improving object detection performance in terms of recall. We propose a post-detection stage during which we explore the image with the objective of recovering missed detections. This exploration is performed by sampling object proposals in the image. We analyse four different strategies to perform this sampling, giving special attention to strategies that exploit spatial relations between objects. In addition, we propose a novel method to discover higher-order relations between groups of objects. Experiments on the challenging KITTI dataset show that our proposed relations-based proposal generation strategies can help improving recall at the cost of a relatively low amount of object proposals. version:1
arxiv-1511-01942 | Stop Wasting My Gradients: Practical SVRG | http://arxiv.org/abs/1511.01942 | id:1511.01942 author:Reza Babanezhad, Mohamed Osama Ahmed, Alim Virani, Mark Schmidt, Jakub Konečný, Scott Sallinen category:cs.LG math.OC stat.CO stat.ML  published:2015-11-05 summary:We present and analyze several strategies for improving the performance of stochastic variance-reduced gradient (SVRG) methods. We first show that the convergence rate of these methods can be preserved under a decreasing sequence of errors in the control variate, and use this to derive variants of SVRG that use growing-batch strategies to reduce the number of gradient calculations required in the early iterations. We further (i) show how to exploit support vectors to reduce the number of gradient computations in the later iterations, (ii) prove that the commonly-used regularized SVRG iteration is justified and improves the convergence rate, (iii) consider alternate mini-batch selection strategies, and (iv) consider the generalization error of the method. version:1
arxiv-1507-01784 | Rethinking LDA: moment matching for discrete ICA | http://arxiv.org/abs/1507.01784 | id:1507.01784 author:Anastasia Podosinnikova, Francis Bach, Simon Lacoste-Julien category:stat.ML cs.LG  published:2015-07-07 summary:We consider moment matching techniques for estimation in Latent Dirichlet Allocation (LDA). By drawing explicit links between LDA and discrete versions of independent component analysis (ICA), we first derive a new set of cumulant-based tensors, with an improved sample complexity. Moreover, we reuse standard ICA techniques such as joint diagonalization of tensors to improve over existing methods based on the tensor power method. In an extensive set of experiments on both synthetic and real datasets, we show that our new combination of tensors and orthogonal joint diagonalization techniques outperforms existing moment matching methods. version:2
arxiv-1511-01870 | Thoughts on Massively Scalable Gaussian Processes | http://arxiv.org/abs/1511.01870 | id:1511.01870 author:Andrew Gordon Wilson, Christoph Dann, Hannes Nickisch category:cs.LG cs.AI stat.ME stat.ML  published:2015-11-05 summary:We introduce a framework and early results for massively scalable Gaussian processes (MSGP), significantly extending the KISS-GP approach of Wilson and Nickisch (2015). The MSGP framework enables the use of Gaussian processes (GPs) on billions of datapoints, without requiring distributed inference, or severe assumptions. In particular, MSGP reduces the standard $O(n^3)$ complexity of GP learning and inference to $O(n)$, and the standard $O(n^2)$ complexity per test point prediction to $O(1)$. MSGP involves 1) decomposing covariance matrices as Kronecker products of Toeplitz matrices approximated by circulant matrices. This multi-level circulant approximation allows one to unify the orthogonal computational benefits of fast Kronecker and Toeplitz approaches, and is significantly faster than either approach in isolation; 2) local kernel interpolation and inducing points to allow for arbitrarily located data inputs, and $O(1)$ test time predictions; 3) exploiting block-Toeplitz Toeplitz-block structure (BTTB), which enables fast inference and learning when multidimensional Kronecker structure is not present; and 4) projections of the input space to flexibly model correlated inputs and high dimensional data. The ability to handle many ($m \approx n$) inducing points allows for near-exact accuracy and large scale kernel learning. version:1
arxiv-1510-08583 | Privacy Prediction of Images Shared on Social Media Sites Using Deep Features | http://arxiv.org/abs/1510.08583 | id:1510.08583 author:Ashwini Tonge, Cornelia Caragea category:cs.CV cs.CY  published:2015-10-29 summary:Online image sharing in social media sites such as Facebook, Flickr, and Instagram can lead to unwanted disclosure and privacy violations, when privacy settings are used inappropriately. With the exponential increase in the number of images that are shared online every day, the development of effective and efficient prediction methods for image privacy settings are highly needed. The performance of models critically depends on the choice of the feature representation. In this paper, we present an approach to image privacy prediction that uses deep features and deep image tags as feature representations. Specifically, we explore deep features at various neural network layers and use the top layer (probability) as an auto-annotation mechanism. The results of our experiments show that models trained on the proposed deep features and deep image tags substantially outperform baselines such as those based on SIFT and GIST as well as those that use "bag of tags" as features. version:3
arxiv-1511-01853 | Autoregressive Model for Individual Consumption Data - LASSO Selection and Significance Test | http://arxiv.org/abs/1511.01853 | id:1511.01853 author:Pan Li, Baosen Zhang, Yang Weng, Ram Rajagopal category:stat.ML cs.SY math.OC  published:2015-11-05 summary:Understanding user flexibility and behavior patterns is becoming increasingly vital to the design of robust and efficient energy saving programs. Accurate prediction of consumption is a key part to this understanding. Existing prediction methods usually have high relative errors that can be larger than 30\%. In this paper, we explore sparsity in users' past data and relationship between different users to increase prediction accuracy. We show that using LASSO and significance test techniques, prediction accuracy can be significantly compared to standard existing algorithms. We use mean absolute percentage error (MAPE) as the criteria. version:1
arxiv-1511-01846 | Sparse approximation by greedy algorithms | http://arxiv.org/abs/1511.01846 | id:1511.01846 author:Vladimir Temlyakov category:math.NA stat.ML  published:2015-11-05 summary:It is a survey on recent results in constructive sparse approximation. Three directions are discussed here: (1) Lebesgue-type inequalities for greedy algorithms with respect to a special class of dictionaries, (2) constructive sparse approximation with respect to the trigonometric system, (3) sparse approximation with respect to dictionaries with tensor product structure. In all three cases constructive ways are provided for sparse approximation. The technique used is based on fundamental results from the theory of greedy approximation. In particular, results in the direction (1) are based on deep methods developed recently in compressed sensing. We present some of these results with detailed proofs. version:1
arxiv-1507-02528 | Faster Convex Optimization: Simulated Annealing with an Efficient Universal Barrier | http://arxiv.org/abs/1507.02528 | id:1507.02528 author:Jacob Abernethy, Elad Hazan category:math.OC cs.LG  published:2015-07-09 summary:This paper explores a surprising equivalence between two seemingly-distinct convex optimization methods. We show that simulated annealing, a well-studied random walk algorithms, is directly equivalent, in a certain sense, to the central path interior point algorithm for the the entropic universal barrier function. This connection exhibits several benefits. First, we are able improve the state of the art time complexity for convex optimization under the membership oracle model. We improve the analysis of the randomized algorithm of Kalai and Vempala by utilizing tools developed by Nesterov and Nemirovskii that underly the central path following interior point algorithm. We are able to tighten the temperature schedule for simulated annealing which gives an improved running time, reducing by square root of the dimension in certain instances. Second, we get an efficient randomized interior point method with an efficiently computable universal barrier for any convex set described by a membership oracle. Previously, efficiently computable barriers were known only for particular convex sets. version:2
arxiv-1511-01776 | Computational Intractability of Dictionary Learning for Sparse Representation | http://arxiv.org/abs/1511.01776 | id:1511.01776 author:Meisam Razaviyayn, Hung-Wei Tseng, Zhi-Quan Luo category:cs.LG stat.ML  published:2015-11-05 summary:In this paper we consider the dictionary learning problem for sparse representation. We first show that this problem is NP-hard by polynomial time reduction of the densest cut problem. Then, using successive convex approximation strategies, we propose efficient dictionary learning schemes to solve several practical formulations of this problem to stationary points. Unlike many existing algorithms in the literature, such as K-SVD, our proposed dictionary learning scheme is theoretically guaranteed to converge to the set of stationary points under certain mild assumptions. For the image denoising application, the performance and the efficiency of the proposed dictionary learning scheme are comparable to that of K-SVD algorithm in simulation. version:1
arxiv-1410-6313 | Canonical Polyadic Decomposition with Auxiliary Information for Brain Computer Interface | http://arxiv.org/abs/1410.6313 | id:1410.6313 author:Junhua Li, Chao Li, Andrzej Cichocki category:cs.CV  published:2014-10-23 summary:Physiological signals are often organized in the form of multiple dimensions (e.g., channel, time, task, and 3D voxel), so it is better to preserve original organization structure when processing. Unlike vector-based methods that destroy data structure, Canonical Polyadic Decomposition (CPD) aims to process physiological signals in the form of multi-way array, which considers relationships between dimensions and preserves structure information contained by the physiological signal. Nowadays, CPD is utilized as an unsupervised method for feature extraction in a classification problem. After that, a classifier, such as support vector machine, is required to classify those features. In this manner, classification task is achieved in two isolated steps. We proposed supervised Canonical Polyadic Decomposition by directly incorporating auxiliary label information during decomposition, by which a classification task can be achieved without an extra step of classifier training. The proposed method merges the decomposition and classifier learning together, so it reduces procedure of classification task compared with that of respective decomposition and classification. In order to evaluate the performance of the proposed method, three different kinds of signals, synthetic signal, EEG signal, and MEG signal, were used. The results based on evaluations of synthetic and real signals demonstrated that the proposed method is effective and efficient. version:2
arxiv-1511-01764 | Discrete Rényi Classifiers | http://arxiv.org/abs/1511.01764 | id:1511.01764 author:Meisam Razaviyayn, Farzan Farnia, David Tse category:cs.LG  published:2015-11-05 summary:Consider the binary classification problem of predicting a target variable $Y$ from a discrete feature vector $X = (X_1,...,X_d)$. When the probability distribution $\mathbb{P}(X,Y)$ is known, the optimal classifier, leading to the minimum misclassification rate, is given by the Maximum A-posteriori Probability decision rule. However, estimating the complete joint distribution $\mathbb{P}(X,Y)$ is computationally and statistically impossible for large values of $d$. An alternative approach is to first estimate some low order marginals of $\mathbb{P}(X,Y)$ and then design the classifier based on the estimated low order marginals. This approach is also helpful when the complete training data instances are not available due to privacy concerns. In this work, we consider the problem of finding the optimum classifier based on some estimated low order marginals of $(X,Y)$. We prove that for a given set of marginals, the minimum Hirschfeld-Gebelein-Renyi (HGR) correlation principle introduced in [1] leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier. Then, under a separability condition, we show that the proposed algorithm is equivalent to a randomized linear regression approach. In addition, this method naturally results in a robust feature selection method selecting a subset of features having the maximum worst case HGR correlation with the target variable. Our theoretical upper-bound is similar to the recent Discrete Chebyshev Classifier (DCC) approach [2], while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem. Finally, we numerically compare our proposed algorithm with the DCC classifier and show that the proposed algorithm results in better misclassification rate over various datasets. version:1
arxiv-1511-01726 | Multi-Target Tracking and Occlusion Handling with Learned Variational Bayesian Clusters and a Social Force Model | http://arxiv.org/abs/1511.01726 | id:1511.01726 author:Ata-ur-Rehman, Syed Mohsen Naqvi, Lyudmila Mihaylova, Jonathon Chambers category:cs.CV  published:2015-11-05 summary:This paper considers the problem of multiple human target tracking in a sequence of video data. A solution is proposed which is able to deal with the challenges of a varying number of targets, interactions and when every target gives rise to multiple measurements. The developed novel algorithm comprises variational Bayesian clustering combined with a social force model, integrated within a particle filter with an enhanced prediction step. It performs measurement-to-target association by automatically detecting the measurement relevance. The performance of the developed algorithm is evaluated over several sequences from publicly available data sets: AV16.3, CAVIAR and PETS2006, which demonstrates that the proposed algorithm successfully initializes and tracks a variable number of targets in the presence of complex occlusions. A comparison with state-of-the-art techniques due to Khan et al., Laet et al. and Czyz et al. shows improved tracking performance. version:1
arxiv-1511-01706 | Image classification based on support vector machine and the fusion of complementary features | http://arxiv.org/abs/1511.01706 | id:1511.01706 author:Huilin Gao, Wenjie Chen, Lihua Dou category:cs.CV  published:2015-11-05 summary:Image Classification based on BOW (Bag-of-words) has broad application prospect in pattern recognition field but the shortcomings are existed because of single feature and low classification accuracy. To this end we combine three ingredients: (i) Three features with functions of mutual complementation are adopted to describe the images, including PHOW (Pyramid Histogram of Words), PHOC (Pyramid Histogram of Color) and PHOG (Pyramid Histogram of Orientated Gradients). (ii) The improvement of traditional BOW model is presented by using dense sample and an improved K-means clustering method for constructing the visual dictionary. (iii) An adaptive feature-weight adjusted image categorization algorithm based on the SVM and the fusion of multiple features is adopted. Experiments carried out on Caltech 101 database confirm the validity of the proposed approach. From the experimental results can be seen that the classification accuracy rate of the proposed method is improved by 7%-17% higher than that of the traditional BOW methods. This algorithm makes full use of global, local and spatial information and has significant improvements to the classification accuracy. version:1
arxiv-1511-01666 | Comparing Writing Styles using Word Embedding and Dynamic Time Warping | http://arxiv.org/abs/1511.01666 | id:1511.01666 author:Abhinav Tushar, Abhinav Dahiya category:cs.CL  published:2015-11-05 summary:The development of plot or story in novels is reflected in the content and the words used. The flow of sentiments, which is one aspect of writing style, can be quantified by analyzing the flow of words. This study explores literary works as signals in word embedding space and tries to compare writing styles of popular classic novels using dynamic time warping. version:1
arxiv-1511-01665 | An Empirical Study on Sentiment Classification of Chinese Review using Word Embedding | http://arxiv.org/abs/1511.01665 | id:1511.01665 author:Yiou Lin, Hang Lei, Jia Wu, Xiaoyu Li category:cs.CL  published:2015-11-05 summary:In this article, how word embeddings can be used as features in Chinese sentiment classification is presented. Firstly, a Chinese opinion corpus is built with a million comments from hotel review websites. Then the word embeddings which represent each comment are used as input in different machine learning methods for sentiment classification, including SVM, Logistic Regression, Convolutional Neural Network (CNN) and ensemble methods. These methods get better performance compared with N-gram models using Naive Bayes (NB) and Maximum Entropy (ME). Finally, a combination of machine learning methods is proposed which presents an outstanding performance in precision, recall and F1 score. After selecting the most useful methods to construct the combinational model and testing over the corpus, the final F1 score is 0.920. version:1
arxiv-1511-01644 | Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model | http://arxiv.org/abs/1511.01644 | id:1511.01644 author:Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, David Madigan category:stat.AP cs.LG stat.ML  published:2015-11-05 summary:We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if...then... statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS$_2$ score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS$_2$, but more accurate. version:1
arxiv-1504-00722 | Point Localization and Density Estimation from Ordinal kNN graphs using Synchronization | http://arxiv.org/abs/1504.00722 | id:1504.00722 author:Mihai Cucuringu, Joseph Woodworth category:stat.ML  published:2015-04-03 summary:We consider the problem of embedding unweighted, directed k-nearest neighbor graphs in low-dimensional Euclidean space. The k-nearest neighbors of each vertex provides ordinal information on the distances between points, but not the distances themselves. We use this ordinal information along with the low-dimensionality to recover the coordinates of the points up to arbitrary similarity transformations (rigid transformations and scaling). Furthermore, we also illustrate the possibility of robustly recovering the underlying density via the Total Variation Maximum Penalized Likelihood Estimation (TV-MPLE) method. We make existing approaches scalable by using an instance of a local-to-global algorithm based on group synchronization, recently proposed in the literature in the context of sensor network localization and structural biology, which we augment with a scaling synchronization step. We demonstrate the scalability of our approach on large graphs, and show how it compares to the Local Ordinal Embedding (LOE) algorithm, which was recently proposed for recovering the configuration of a cloud of points from pairwise ordinal comparisons between a sparse set of distances. version:2
arxiv-1510-08565 | Attention with Intention for a Neural Network Conversation Model | http://arxiv.org/abs/1510.08565 | id:1510.08565 author:Kaisheng Yao, Geoffrey Zweig, Baolin Peng category:cs.NE cs.AI cs.HC cs.LG  published:2015-10-29 summary:In a conversation or a dialogue process, attention and intention play intrinsic roles. This paper proposes a neural network based approach that models the attention and intention processes. It essentially consists of three recurrent networks. The encoder network is a word-level model representing source side sentences. The intention network is a recurrent network that models the dynamics of the intention process. The decoder network is a recurrent network produces responses to the input from the source side. It is a language model that is dependent on the intention and has an attention mechanism to attend to particular source side words, when predicting a symbol in the response. The model is trained end-to-end without labeling data. Experiments show that this model generates natural responses to user inputs. version:3
arxiv-1511-01631 | Background Modeling Using Adaptive Pixelwise Kernel Variances in a Hybrid Feature Space | http://arxiv.org/abs/1511.01631 | id:1511.01631 author:Manjunath Narayana, Allen Hanson, Erik Learned-Miller category:cs.CV  published:2015-11-05 summary:Recent work on background subtraction has shown developments on two major fronts. In one, there has been increasing sophistication of probabilistic models, from mixtures of Gaussians at each pixel [7], to kernel density estimates at each pixel [1], and more recently to joint domainrange density estimates that incorporate spatial information [6]. Another line of work has shown the benefits of increasingly complex feature representations, including the use of texture information, local binary patterns, and recently scale-invariant local ternary patterns [4]. In this work, we use joint domain-range based estimates for background and foreground scores and show that dynamically choosing kernel variances in our kernel estimates at each individual pixel can significantly improve results. We give a heuristic method for selectively applying the adaptive kernel calculations which is nearly as accurate as the full procedure but runs much faster. We combine these modeling improvements with recently developed complex features [4] and show significant improvements on a standard backgrounding benchmark. version:1
arxiv-1511-01627 | Background subtraction - separating the modeling and the inference | http://arxiv.org/abs/1511.01627 | id:1511.01627 author:Manjunath Narayana, Allen Hanson, Erik Learned-Miller category:cs.CV  published:2015-11-05 summary:In its early implementations, background modeling was a process of building a model for the background of a video with a stationary camera, and identifying pixels that did not conform well to this model. The pixels that were not well-described by the background model were assumed to be moving objects. Many systems today maintain models for the foreground as well as the background, and these models compete to explain the pixels in a video. In this paper, we argue that the logical endpoint of this evolution is to simply use Bayes' rule to classify pixels. In particular, it is essential to have a background likelihood, a foreground likelihood, and a prior at each pixel. A simple application of Bayes' rule then gives a posterior probability over the label. The only remaining question is the quality of the component models: the background likelihood, the foreground likelihood, and the prior. We describe a model for the likelihoods that is built by using not only the past observations at a given pixel location, but by also including observations in a spatial neighborhood around the location. This enables us to model the influence between neighboring pixels and is an improvement over earlier pixelwise models that do not allow for such influence. Although similar in spirit to the joint domain-range model, we show that our model overcomes certain deficiencies in that model. We use a spatially dependent prior for the background and foreground. The background and foreground labels from the previous frame, after spatial smoothing to account for movement of objects,are used to build the prior for the current frame. version:1
arxiv-1409-8444 | Douglas-Rachford splitting for nonconvex optimization with application to nonconvex feasibility problems | http://arxiv.org/abs/1409.8444 | id:1409.8444 author:Guoyin Li, Ting Kei Pong category:math.OC stat.ML  published:2014-09-30 summary:We adapt the Douglas-Rachford (DR) splitting method to solve nonconvex feasibility problems by studying this method for a class of nonconvex optimization problem. While the convergence properties of the method for convex problems have been well studied, far less is known in the nonconvex setting. In this paper, for the direct adaptation of the method to minimize the sum of a proper closed function $g$ and a smooth function $f$ with a Lipschitz continuous gradient, we show that if the step-size parameter is smaller than a computable threshold and the sequence generated has a cluster point, then it gives a stationary point of the optimization problem. Convergence of the whole sequence and a local convergence rate are also established under the additional assumption that $f$ and $g$ are semi-algebraic. We also give simple sufficient conditions guaranteeing the boundedness of the sequence generated. We then apply our nonconvex DR splitting method to finding a point in the intersection of a closed convex set $C$ and a general closed set $D$ by minimizing the squared distance to $C$ subject to $D$. We show that if either set is bounded and the step-size parameter is smaller than a computable threshold, then the sequence generated from the DR splitting method is actually bounded. Consequently, the sequence generated will have cluster points that are stationary for an optimization problem, and the whole sequence is convergent under an additional assumption that $C$ and $D$ are semi-algebraic. We achieve these results based on a new merit function constructed particularly for the DR splitting method. Our preliminary numerical results indicate that our DR splitting method usually outperforms the alternating projection method in finding a sparse solution of a linear system, in terms of both the solution quality and the number of iterations taken. version:5
arxiv-1511-01619 | Coherent Motion Segmentation in Moving Camera Videos using Optical Flow Orientations | http://arxiv.org/abs/1511.01619 | id:1511.01619 author:Manjunath Narayana, Allen Hanson, Erik Learned-Miller category:cs.CV  published:2015-11-05 summary:In moving camera videos, motion segmentation is commonly performed using the image plane motion of pixels, or optical flow. However, objects that are at different depths from the camera can exhibit different optical flows even if they share the same real-world motion. This can cause a depth-dependent segmentation of the scene. Our goal is to develop a segmentation algorithm that clusters pixels that have similar real-world motion irrespective of their depth in the scene. Our solution uses optical flow orientations instead of the complete vectors and exploits the well-known property that under camera translation, optical flow orientations are independent of object depth. We introduce a probabilistic model that automatically estimates the number of observed independent motions and results in a labeling that is consistent with real-world motion in the scene. The result of our system is that static objects are correctly identified as one segment, even if they are at different depths. Color features and information from previous frames in the video sequence are used to correct occasional errors due to the orientation-based segmentation. We present results on more than thirty videos from different benchmarks. The system is particularly robust on complex background scenes containing objects at significantly different depths version:1
arxiv-1511-01559 | Color Aesthetics and Social Networks in Complete Tang Poems: Explorations and Discoveries | http://arxiv.org/abs/1511.01559 | id:1511.01559 author:Chao-Lin Liu, Hongsu Wang, Wen-Huei Cheng, Chu-Ting Hsu, Wei-Yun Chiu category:cs.CL cs.DL cs.IR  published:2015-11-05 summary:The Complete Tang Poems (CTP) is the most important source to study Tang poems. We look into CTP with computational tools from specific linguistic perspectives, including distributional semantics and collocational analysis. From such quantitative viewpoints, we compare the usage of "wind" and "moon" in the poems of Li Bai and Du Fu. Colors in poems function like sounds in movies, and play a crucial role in the imageries of poems. Thus, words for colors are studied, and "white" is the main focus because it is the most frequent color in CTP. We also explore some cases of using colored words in antithesis pairs that were central for fostering the imageries of the poems. CTP also contains useful historical information, and we extract person names in CTP to study the social networks of the Tang poets. Such information can then be integrated with the China Biographical Database of Harvard University. version:1
arxiv-1412-4160 | Ripple Down Rules for Question Answering | http://arxiv.org/abs/1412.4160 | id:1412.4160 author:Dat Quoc Nguyen, Dai Quoc Nguyen, Son Bao Pham category:cs.CL cs.IR  published:2014-12-12 summary:Recent years have witnessed a new trend of building ontology-based question answering systems. These systems use semantic web information to produce more precise answers to users' queries. However, these systems are mostly designed for English. In this paper, we introduce an ontology-based question answering system named KbQAS which, to the best of our knowledge, is the first one made for Vietnamese. KbQAS employs our question analysis approach that systematically constructs a knowledge base of grammar rules to convert each input question into an intermediate representation element. KbQAS then takes the intermediate representation element with respect to a target ontology and applies concept-matching techniques to return an answer. On a wide range of Vietnamese questions, experimental results show that the performance of KbQAS is promising with accuracies of 84.1% and 82.4% for analyzing input questions and retrieving output answers, respectively. Furthermore, our question analysis approach can easily be applied to new domains and new languages, thus saving time and human effort. version:4
arxiv-1511-01556 | Mining Local Gazetteers of Literary Chinese with CRF and Pattern based Methods for Biographical Information in Chinese History | http://arxiv.org/abs/1511.01556 | id:1511.01556 author:Chao-Lin Liu, Chih-Kai Huang, Hongsu Wang, Peter K. Bol category:cs.CL cs.DL cs.IR cs.LG  published:2015-11-04 summary:Person names and location names are essential building blocks for identifying events and social networks in historical documents that were written in literary Chinese. We take the lead to explore the research on algorithmically recognizing named entities in literary Chinese for historical studies with language-model based and conditional-random-field based methods, and extend our work to mining the document structures in historical documents. Practical evaluations were conducted with texts that were extracted from more than 220 volumes of local gazetteers (Difangzhi). Difangzhi is a huge and the single most important collection that contains information about officers who served in local government in Chinese history. Our methods performed very well on these realistic tests. Thousands of names and addresses were identified from the texts. A good portion of the extracted names match the biographical information currently recorded in the China Biographical Database (CBDB) of Harvard University, and many others can be verified by historians and will become as new additions to CBDB. version:1
arxiv-1506-02155 | Optimal Rates for Random Fourier Features | http://arxiv.org/abs/1506.02155 | id:1506.02155 author:Bharath K. Sriperumbudur, Zoltan Szabo category:math.ST cs.LG math.FA stat.ML stat.TH G.3; I.2.6; F.2  published:2015-06-06 summary:Kernel methods represent one of the most powerful tools in machine learning to tackle problems expressed in terms of function values and derivatives due to their capability to represent and model complex relations. While these methods show good versatility, they are computationally intensive and have poor scalability to large data as they require operations on Gram matrices. In order to mitigate this serious computational limitation, recently randomized constructions have been proposed in the literature, which allow the application of fast linear algorithms. Random Fourier features (RFF) are among the most popular and widely applied constructions: they provide an easily computable, low-dimensional feature representation for shift-invariant kernels. Despite the popularity of RFFs, very little is understood theoretically about their approximation quality. In this paper, we provide a detailed finite-sample theoretical analysis about the approximation quality of RFFs by (i) establishing optimal (in terms of the RFF dimension, and growing set size) performance guarantees in uniform norm, and (ii) presenting guarantees in $L^r$ ($1\le r<\infty$) norms. We also propose an RFF approximation to derivatives of a kernel with a theoretical study on its approximation quality. version:2
arxiv-1511-01543 | Regularization and Bayesian Learning in Dynamical Systems: Past, Present and Future | http://arxiv.org/abs/1511.01543 | id:1511.01543 author:A. Chiuso category:cs.SY stat.ML  published:2015-11-04 summary:Regularization and Bayesian methods for system identification have been repopularized in the recent years, and proved to be competitive w.r.t. classical parametric approaches. In this paper we shall make an attempt to illustrate how the use of regularization in system identification has evolved over the years, starting from the early contributions both in the Automatic Control as well as Econometrics and Statistics literature. In particular we shall discuss some fundamental issues such as compound estimation problems and exchangeability which play and important role in regularization and Bayesian approaches, as also illustrated in early publications in Statistics. The historical and foundational issues will be given more emphasis (and space), at the expense of the more recent developments which are only briefly discussed. The main reason for such a choice is that, while the recent literature is readily available, and surveys have already been published on the subject, in the author's opinion a clear link with past work had not been completely clarified. version:1
arxiv-1511-01512 | Mean-field inference of Hawkes point processes | http://arxiv.org/abs/1511.01512 | id:1511.01512 author:Emmanuel Bacry, Stéphane Gaïffas, Iacopo Mastromatteo, Jean-François Muzy category:cs.LG cond-mat.stat-mech  published:2015-11-04 summary:We propose a fast and efficient estimation method that is able to accurately recover the parameters of a d-dimensional Hawkes point-process from a set of observations. We exploit a mean-field approximation that is valid when the fluctuations of the stochastic intensity are small. We show that this is notably the case in situations when interactions are sufficiently weak, when the dimension of the system is high or when the fluctuations are self-averaging due to the large number of past events they involve. In such a regime the estimation of a Hawkes process can be mapped on a least-squares problem for which we provide an analytic solution. Though this estimator is biased, we show that its precision can be comparable to the one of the Maximum Likelihood Estimator while its computation speed is shown to be improved considerably. We give a theoretical control on the accuracy of our new approach and illustrate its efficiency using synthetic datasets, in order to assess the statistical estimation error of the parameters. version:1
arxiv-1511-01508 | Enhancing Feature Tracking With Gyro Regularization | http://arxiv.org/abs/1511.01508 | id:1511.01508 author:Bryan Poling, Gilad Lerman category:cs.CV 68T45  published:2015-11-04 summary:We present a deeply integrated method of exploiting low-cost gyroscopes to improve general purpose feature tracking. Most previous methods use gyroscopes to initialize and bound the search for features. In contrast, we use them to regularize the tracking energy function so that they can directly assist in the tracking of ambiguous and poor-quality features. We demonstrate that our simple technique offers significant improvements in performance over conventional template-based tracking methods, and is in fact competitive with more complex and computationally expensive state-of-the-art trackers, but at a fraction of the computational cost. Additionally, we show that the practice of initializing template-based feature trackers like KLT (Kanade-Lucas-Tomasi) using gyro-predicted optical flow offers no advantage over using a careful optical-only initialization method, suggesting that some deeper level of integration, like the method we propose, is needed in order to realize a genuine improvement in tracking performance from these inertial sensors. version:1
arxiv-1507-03003 | High-Dimensional Asymptotics of Prediction: Ridge Regression and Classification | http://arxiv.org/abs/1507.03003 | id:1507.03003 author:Edgar Dobriban, Stefan Wager category:math.ST stat.ML stat.TH  published:2015-07-10 summary:We provide a unified analysis of the predictive risk of ridge regression and regularized discriminant analysis in a dense random effects model. We work in a high-dimensional asymptotic regime where $p, n \to \infty$ and $p/n \to \gamma \in (0, \, \infty)$, and allow for arbitrary covariance among the features. For both methods, we provide an explicit and efficiently computable expression for the limiting predictive risk, which depends only on the spectrum of the feature-covariance matrix, the signal strength, and the aspect ratio $\gamma$. Especially in the case of regularized discriminant analysis, we find that predictive accuracy has a nuanced dependence on the eigenvalue distribution of the covariance matrix, suggesting that analyses based on the operator norm of the covariance matrix may not be sharp. Our results also uncover several qualitative insights about both methods: for example, with ridge regression, there is an exact inverse relation between the limiting predictive risk and the limiting estimation risk given a fixed signal strength. Our analysis builds on recent advances in random matrix theory. version:2
arxiv-1511-01480 | Approximation of the truncated Zeta distribution and Zipf's law | http://arxiv.org/abs/1511.01480 | id:1511.01480 author:Maurizio Naldi category:stat.AP cs.CL cs.SI  published:2015-11-04 summary:Zipf's law appears in many application areas but does not have a closed form expression, which may make its use cumbersome. Since it coincides with the truncated version of the Zeta distribution, in this paper we propose three approximate closed form expressions for the truncated Zeta distribution, which may be employed for Zipf's law as well. The three approximations are based on the replacement of the sum occurring in Zipf's law with an integral, and are named respectively the integral approximation, the average integral approximation, and the trapezoidal approximation. While the first one is shown to be of little use, the trapezoidal approximation exhibits an error which is typically lower than 1\%, but is as low as 0.1\% for the range of values of the Zipf parameter below 1. version:1
arxiv-1511-01432 | Semi-supervised Sequence Learning | http://arxiv.org/abs/1511.01432 | id:1511.01432 author:Andrew M. Dai, Quoc V. Le category:cs.LG cs.CL  published:2015-11-04 summary:We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups. version:1
arxiv-1511-01427 | Turing Computation with Recurrent Artificial Neural Networks | http://arxiv.org/abs/1511.01427 | id:1511.01427 author:Giovanni S Carmantini, Peter beim Graben, Mathieu Desroches, Serafim Rodrigues category:cs.NE  published:2015-11-04 summary:We improve the results by Siegelmann & Sontag (1995) by providing a novel and parsimonious constructive mapping between Turing Machines and Recurrent Artificial Neural Networks, based on recent developments of Nonlinear Dynamical Automata. The architecture of the resulting R-ANNs is simple and elegant, stemming from its transparent relation with the underlying NDAs. These characteristics yield promise for developments in machine learning methods and symbolic computation with continuous time dynamical systems. A framework is provided to directly program the R-ANNs from Turing Machine descriptions, in absence of network training. At the same time, the network can potentially be trained to perform algorithmic tasks, with exciting possibilities in the integration of approaches akin to Google DeepMind's Neural Turing Machines. version:1
arxiv-1511-01017 | Consistent Parameter Estimation for LASSO and Approximate Message Passing | http://arxiv.org/abs/1511.01017 | id:1511.01017 author:Ali Mousavi, Arian Maleki, Richard G. Baraniuk category:math.ST cs.IT math.IT math.OC stat.ML stat.TH  published:2015-11-03 summary:We consider the problem of recovering a vector $\beta_o \in \mathbb{R}^p$ from $n$ random and noisy linear observations $y= X\beta_o + w$, where $X$ is the measurement matrix and $w$ is noise. The LASSO estimate is given by the solution to the optimization problem $\hat{\beta}_{\lambda} = \arg \min_{\beta} \frac{1}{2} \ y-X\beta\ _2^2 + \lambda \ \beta \ _1$. Among the iterative algorithms that have been proposed for solving this optimization problem, approximate message passing (AMP) has attracted attention for its fast convergence. Despite significant progress in the theoretical analysis of the estimates of LASSO and AMP, little is known about their behavior as a function of the regularization parameter $\lambda$, or the thereshold parameters $\tau^t$. For instance the following basic questions have not yet been studied in the literature: (i) How does the size of the active set $\ \hat{\beta}^\lambda\ _0/p$ behave as a function of $\lambda$? (ii) How does the mean square error $\ \hat{\beta}_{\lambda} - \beta_o\ _2^2/p$ behave as a function of $\lambda$? (iii) How does $\ \beta^t - \beta_o \ _2^2/p$ behave as a function of $\tau^1, \ldots, \tau^{t-1}$? Answering these questions will help in addressing practical challenges regarding the optimal tuning of $\lambda$ or $\tau^1, \tau^2, \ldots$. This paper answers these questions in the asymptotic setting and shows how these results can be employed in deriving simple and theoretically optimal approaches for tuning the parameters $\tau^1, \ldots, \tau^t$ for AMP or $\lambda$ for LASSO. It also explores the connection between the optimal tuning of the parameters of AMP and the optimal tuning of LASSO. version:2
arxiv-1505-05310 | Supervised Learning for Dynamical System Learning | http://arxiv.org/abs/1505.05310 | id:1505.05310 author:Ahmed Hefny, Carlton Downey, Geoffrey Gordon category:stat.ML cs.LG  published:2015-05-20 summary:Recently there has been substantial interest in spectral methods for learning dynamical systems. These methods are popular since they often offer a good tradeoff between computational and statistical efficiency. Unfortunately, they can be difficult to use and extend in practice: e.g., they can make it difficult to incorporate prior information such as sparsity or structure. To address this problem, we present a new view of dynamical system learning: we show how to learn dynamical systems by solving a sequence of ordinary supervised learning problems, thereby allowing users to incorporate prior knowledge via standard techniques such as L1 regularization. Many existing spectral methods are special cases of this new framework, using linear regression as the supervised learner. We demonstrate the effectiveness of our framework by showing examples where nonlinear regression or lasso let us learn better state representations than plain linear regression does; the correctness of these instances follows directly from our general analysis. version:2
arxiv-1510-08231 | Operator-valued Kernels for Learning from Functional Response Data | http://arxiv.org/abs/1510.08231 | id:1510.08231 author:Hachem Kadri, Emmanuel Duflos, Philippe Preux, Stéphane Canu, Alain Rakotomamonjy, Julien Audiffren category:cs.LG stat.ML  published:2015-10-28 summary:In this paper we consider the problems of supervised classification and regression in the case where attributes and labels are functions: a data is represented by a set of functions, and the label is also a function. We focus on the use of reproducing kernel Hilbert space theory to learn from such functional data. Basic concepts and properties of kernel-based learning are extended to include the estimation of function-valued functions. In this setting, the representer theorem is restated, a set of rigorously defined infinite-dimensional operator-valued kernels that can be valuably applied when the data are functions is described, and a learning algorithm for nonlinear functional data analysis is introduced. The methodology is illustrated through speech and audio signal processing experiments. version:2
arxiv-1511-01304 | Dictionary descent in optimization | http://arxiv.org/abs/1511.01304 | id:1511.01304 author:Vladimir Temlyakov category:stat.ML math.NA  published:2015-11-04 summary:The problem of convex optimization is studied. Usually in convex optimization the minimization is over a d-dimensional domain. Very often the convergence rate of an optimization algorithm depends on the dimension d. The algorithms studied in this paper utilize dictionaries instead of a canonical basis used in the coordinate descent algorithms. We show how this approach allows us to reduce dimensionality of the problem. Also, we investigate which properties of a dictionary are beneficial for the convergence rate of typical greedy-type algorithms. version:1
arxiv-1512-04354 | A proposal project for a blind image quality assessment by learning distortions from the full reference image quality assessments | http://arxiv.org/abs/1512.04354 | id:1512.04354 author:Stéfane Paris category:cs.MM cs.CV  published:2015-11-04 summary:This short paper presents a perspective plan to build a null reference image quality assessment. Its main goal is to deliver both the objective score and the distortion map for a given distorted image without the knowledge of its reference image. version:1
arxiv-1511-01293 | Towards a tracking algorithm based on the clustering of spatio-temporal clouds of points | http://arxiv.org/abs/1511.01293 | id:1511.01293 author:Andrea Cavagna, Chiara Creato, Lorenzo Del Castello, Stefania Melillo, Leonardo Parisi, Massimiliano Viale category:cs.CV  published:2015-11-04 summary:The interest in 3D dynamical tracking is growing in fields such as robotics, biology and fluid dynamics. Recently, a major source of progress in 3D tracking has been the study of collective behaviour in biological systems, where the trajectories of individual animals moving within large and dense groups need to be reconstructed to understand the behavioural interaction rules. Experimental data in this field are generally noisy and at low spatial resolution, so that individuals appear as small featureless objects and trajectories must be retrieved by making use of epipolar information only. Moreover, optical occlusions often occur: in a multi-camera system one or more objects become indistinguishable in one view, potentially jeopardizing the conservation of identity over long-time trajectories. The most advanced 3D tracking algorithms overcome optical occlusions making use of set-cover techniques, which however have to solve NP-hard optimization problems. Moreover, current methods are not able to cope with occlusions arising from actual physical proximity of objects in 3D space. Here, we present a new method designed to work directly in 3D space and time, creating (3D+1) clouds of points representing the full spatio-temporal evolution of the moving targets. We can then use a simple connected components labeling routine, which is linear in time, to solve optical occlusions, hence lowering from NP to P the complexity of the problem. Finally, we use normalized cut spectral clustering to tackle 3D physical proximity. version:1
arxiv-1511-01289 | Data-Driven Learning of a Union of Sparsifying Transforms Model for Blind Compressed Sensing | http://arxiv.org/abs/1511.01289 | id:1511.01289 author:Saiprasad Ravishankar, Yoram Bresler category:stat.ML cs.LG  published:2015-11-04 summary:Compressed sensing is a powerful tool in applications such as magnetic resonance imaging (MRI). It enables accurate recovery of images from highly undersampled measurements by exploiting the sparsity of the images or image patches in a transform domain or dictionary. In this work, we focus on blind compressed sensing (BCS), where the underlying sparse signal model is a priori unknown, and propose a framework to simultaneously reconstruct the underlying image as well as the unknown model from highly undersampled measurements. Specifically, our model is that the patches of the underlying image(s) are approximately sparse in a transform domain. We also extend this model to a union of transforms model that better captures the diversity of features in natural images. The proposed block coordinate descent type algorithms for blind compressed sensing are highly efficient, and are guaranteed to converge to at least the partial global and partial local minimizers of the highly non-convex BCS problems. Our numerical experiments show that the proposed framework usually leads to better quality of image reconstructions in MRI compared to several recent image reconstruction methods. Importantly, the learning of a union of sparsifying transforms leads to better image reconstructions than a single adaptive transform. version:1
arxiv-1511-01284 | Lasso based feature selection for malaria risk exposure prediction | http://arxiv.org/abs/1511.01284 | id:1511.01284 author:Bienvenue Kouwayè, Noël Fonton, Fabrice Rossi category:stat.ML  published:2015-11-04 summary:In life sciences, the experts generally use empirical knowledge to recode variables, choose interactions and perform selection by classical approach. The aim of this work is to perform automatic learning algorithm for variables selection which can lead to know if experts can be help in they decision or simply replaced by the machine and improve they knowledge and results. The Lasso method can detect the optimal subset of variables for estimation and prediction under some conditions. In this paper, we propose a novel approach which uses automatically all variables available and all interactions. By a double cross-validation combine with Lasso, we select a best subset of variables and with GLM through a simple cross-validation perform predictions. The algorithm assures the stability and the the consistency of estimators. version:1
arxiv-1511-01282 | Factorizing LambdaMART for cold start recommendations | http://arxiv.org/abs/1511.01282 | id:1511.01282 author:Phong Nguyen, Jun Wang, Alexandros Kalousis category:cs.LG cs.IR  published:2015-11-04 summary:Recommendation systems often rely on point-wise loss metrics such as the mean squared error. However, in real recommendation settings only few items are presented to a user. This observation has recently encouraged the use of rank-based metrics. LambdaMART is the state-of-the-art algorithm in learning to rank which relies on such a metric. Despite its success it does not have a principled regularization mechanism relying in empirical approaches to control model complexity leaving it thus prone to overfitting. Motivated by the fact that very often the users' and items' descriptions as well as the preference behavior can be well summarized by a small number of hidden factors, we propose a novel algorithm, LambdaMART Matrix Factorization (LambdaMART-MF), that learns a low rank latent representation of users and items using gradient boosted trees. The algorithm factorizes lambdaMART by defining relevance scores as the inner product of the learned representations of the users and items. The low rank is essentially a model complexity controller; on top of it we propose additional regularizers to constraint the learned latent representations that reflect the user and item manifolds as these are defined by their original feature based descriptors and the preference behavior. Finally we also propose to use a weighted variant of NDCG to reduce the penalty for similar items with large rating discrepancy. We experiment on two very different recommendation datasets, meta-mining and movies-users, and evaluate the performance of LambdaMART-MF, with and without regularization, in the cold start setting as well as in the simpler matrix completion setting. In both cases it outperforms in a significant manner current state of the art algorithms. version:1
arxiv-1511-01281 | Co-Clustering Network-Constrained Trajectory Data | http://arxiv.org/abs/1511.01281 | id:1511.01281 author:Mohamed Khalil El Mahrsi, Romain Guigourès, Fabrice Rossi, Marc Boullé category:stat.ML cs.DB cs.LG  published:2015-11-04 summary:Recently, clustering moving object trajectories kept gaining interest from both the data mining and machine learning communities. This problem, however, was studied mainly and extensively in the setting where moving objects can move freely on the euclidean space. In this paper, we study the problem of clustering trajectories of vehicles whose movement is restricted by the underlying road network. We model relations between these trajectories and road segments as a bipartite graph and we try to cluster its vertices. We demonstrate our approaches on synthetic data and show how it could be useful in inferring knowledge about the flow dynamics and the behavior of the drivers using the road network. version:1
arxiv-1511-01280 | Study of a bias in the offline evaluation of a recommendation algorithm | http://arxiv.org/abs/1511.01280 | id:1511.01280 author:Arnaud De Myttenaere, Boris Golden, Bénédicte Le Grand, Fabrice Rossi category:cs.IR cs.LG stat.ML  published:2015-11-04 summary:Recommendation systems have been integrated into the majority of large online systems to filter and rank information according to user profiles. It thus influences the way users interact with the system and, as a consequence, bias the evaluation of the performance of a recommendation algorithm computed using historical data (via offline evaluation). This paper describes this bias and discuss the relevance of a weighted offline evaluation to reduce this bias for different classes of recommendation algorithms. version:1
arxiv-1511-01186 | Face Aging Effect Simulation using Hidden Factor Analysis Joint Sparse Representation | http://arxiv.org/abs/1511.01186 | id:1511.01186 author:Hongyu Yang, Di Huang, Yunhong Wang, Heng Wang, Yuanyan Tang category:cs.CV  published:2015-11-04 summary:Face aging simulation has received rising investigations nowadays, whereas it still remains a challenge to generate convincing and natural age-progressed face images. In this paper, we present a novel approach to such an issue by using hidden factor analysis joint sparse representation. In contrast to the majority of tasks in the literature that handle the facial texture integrally, the proposed aging approach separately models the person-specific facial properties that tend to be stable in a relatively long period and the age-specific clues that change gradually over time. It then merely transforms the age component to a target age group via sparse reconstruction, yielding aging effects, which is finally combined with the identity component to achieve the aged face. Experiments are carried out on three aging databases, and the results achieved clearly demonstrate the effectiveness and robustness of the proposed method in rendering a face with aging effects. Additionally, a series of evaluations prove its validity with respect to identity preservation and aging effect generation. version:1
arxiv-1407-0753 | Global convergence of splitting methods for nonconvex composite optimization | http://arxiv.org/abs/1407.0753 | id:1407.0753 author:Guoyin Li, Ting Kei Pong category:math.OC cs.LG math.NA stat.ML  published:2014-07-03 summary:We consider the problem of minimizing the sum of a smooth function $h$ with a bounded Hessian, and a nonsmooth function. We assume that the latter function is a composition of a proper closed function $P$ and a surjective linear map $\cal M$, with the proximal mappings of $\tau P$, $\tau > 0$, simple to compute. This problem is nonconvex in general and encompasses many important applications in engineering and machine learning. In this paper, we examined two types of splitting methods for solving this nonconvex optimization problem: alternating direction method of multipliers and proximal gradient algorithm. For the direct adaptation of the alternating direction method of multipliers, we show that, if the penalty parameter is chosen sufficiently large and the sequence generated has a cluster point, then it gives a stationary point of the nonconvex problem. We also establish convergence of the whole sequence under an additional assumption that the functions $h$ and $P$ are semi-algebraic. Furthermore, we give simple sufficient conditions to guarantee boundedness of the sequence generated. These conditions can be satisfied for a wide range of applications including the least squares problem with the $\ell_{1/2}$ regularization. Finally, when $\cal M$ is the identity so that the proximal gradient algorithm can be efficiently applied, we show that any cluster point is stationary under a slightly more flexible constant step-size rule than what is known in the literature for a nonconvex $h$. version:6
arxiv-1511-01168 | Cell identification in whole-brain multiview images of neural activation | http://arxiv.org/abs/1511.01168 | id:1511.01168 author:Marco Paciscopi, Ludovico Silvestri, Francesco Saverio Pavone, Paolo Frasconi category:cs.CV J.3  published:2015-11-04 summary:We present a scalable method for brain cell identification in multiview confocal light sheet microscopy images. Our algorithmic pipeline includes a hierarchical registration approach and a novel multiview version of semantic deconvolution that simultaneously enhance visibility of fluorescent cell bodies, equalize their contrast, and fuses adjacent views into a single 3D images on which cell identification is performed with mean shift. We present empirical results on a whole-brain image of an adult Arc-dVenus mouse acquired at 4micron resolution. Based on an annotated test volume containing 3278 cells, our algorithm achieves an $F_1$ measure of 0.89. version:1
arxiv-1511-01161 | Image based compensation for thickness variation in microscopy section series | http://arxiv.org/abs/1511.01161 | id:1511.01161 author:Philipp Hanslovsky, John A. Bogovic, C. Shan Xu, Kenneth J. Hayworth, Zhiyuan Lu, Harald F. Hess, Stephan Saalfeld category:cs.CV  published:2015-11-03 summary:Serial block face scanning electron microscopy in combination with focused ion beam milling (FIB-SEM) has become a popular method for nanometer-resolution isotropic imaging of neural and other cellular tissue with a planar field of view of up to 100um. While FIB-SEM is particularly attractive for its high in-plane resolution, ion beam milling generates non-planar block faces and inhomogeneous z-spacing leading to distorted volume acquisitions. We extend our previous work on image-based z-spacing correction for serial section series to determine a deformation field that varies within the xy-plane to account for non-planarity. We show that our method identifies and corrects these distortions in real world FIB-SEM acquisitions and quantitatively assess its precision on virtual ground truth. Our method is available as an open source implementation that is parallelized using the Spark framework enabling rapid processing of very large volumes. version:1
arxiv-1511-01156 | Robust Large-Scale Localization in 3D Point Clouds Revisited | http://arxiv.org/abs/1511.01156 | id:1511.01156 author:Fabian Tschopp, Marco Zorzi category:cs.CV  published:2015-11-03 summary:We tackle the problem of getting a full 6-DOF pose estimation of a query image inside a given point cloud. This technical report re-evaluates the algorithms proposed by Y. Li et al. "Worldwide Pose Estimation using 3D Point Cloud". Our code computes poses from 3 or 4 points, with both known and unknown focal length. The results can easily be displayed and analyzed with Meshlab. We found both advantages and shortcomings of the methods proposed. Furthermore, additional priors and parameters for point selection, RANSAC and pose quality estimate (inlier test) are proposed and applied. version:1
arxiv-1511-01154 | Robust Registration of Calcium Images by Learned Contrast Synthesis | http://arxiv.org/abs/1511.01154 | id:1511.01154 author:John A. Bogovic, Philipp Hanslovsky, Allan Wong, Stephan Saalfeld category:cs.CV  published:2015-11-03 summary:Multi-modal image registration is a challenging task that is vital to fuse complementary signals for subsequent analyses. Despite much research into cost functions addressing this challenge, there exist cases in which these are ineffective. In this work, we show that (1) this is true for the registration of in-vivo Drosophila brain volumes visualizing genetically encoded calcium indicators to an nc82 atlas and (2) that machine learning based contrast synthesis can yield improvements. More specifically, the number of subjects for which the registration outright failed was greatly reduced (from 40% to 15%) by using a synthesized image. version:1
arxiv-1410-7326 | Neuroevolution in Games: State of the Art and Open Challenges | http://arxiv.org/abs/1410.7326 | id:1410.7326 author:Sebastian Risi, Julian Togelius category:cs.NE  published:2014-10-27 summary:This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyse the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The article also highlights important open research challenges in the field. version:3
arxiv-1511-01088 | There is no fast lunch: an examination of the running speed of evolutionary algorithms in several languages | http://arxiv.org/abs/1511.01088 | id:1511.01088 author:Juan-J. Merelo, Pablo García-Sánchez, Mario García-Valdez, Israel Blancas category:cs.NE cs.PF  published:2015-11-03 summary:It is quite usual when an evolutionary algorithm tool or library uses a language other than C, C++, Java or Matlab that a reviewer or the audience questions its usefulness based on the speed of those other languages, purportedly slower than the aforementioned ones. Despite speed being not everything needed to design a useful evolutionary algorithm application, in this paper we will measure the speed for several very basic evolutionary algorithm operations in several languages which use different virtual machines and approaches, and prove that, in fact, there is no big difference in speed between interpreted and compiled languages, and that in some cases, interpreted languages such as JavaScript or Python can be faster than compiled languages such as Scala, making them worthy of use for evolutionary algorithm experimentation. version:1
arxiv-1511-01029 | Understanding symmetries in deep networks | http://arxiv.org/abs/1511.01029 | id:1511.01029 author:Vijay Badrinarayanan, Bamdev Mishra, Roberto Cipolla category:cs.LG cs.AI cs.CV  published:2015-11-03 summary:Recent works have highlighted scale invariance or symmetry present in the weight space of a typical deep network and the adverse effect it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that a commonly used deep network, which uses convolution, batch normalization, reLU, max-pooling, and sub-sampling pipeline, possess more complex forms of symmetry arising from scaling-based reparameterization of the network weights. We propose to tackle the issue of the weight space symmetry by constraining the filters to lie on the unit-norm manifold. Consequently, training the network boils down to using stochastic gradient descent updates on the unit-norm manifold. Our empirical evidence based on the MNIST dataset shows that the proposed updates improve the test performance beyond what is achieved with batch normalization and without sacrificing the computational efficiency of the weight updates. version:1
arxiv-1510-00757 | A Survey of Online Experiment Design with the Stochastic Multi-Armed Bandit | http://arxiv.org/abs/1510.00757 | id:1510.00757 author:Giuseppe Burtini, Jason Loeppky, Ramon Lawrence category:stat.ML cs.LG  published:2015-10-02 summary:Adaptive and sequential experiment design is a well-studied area in numerous domains. We survey and synthesize the work of the online statistical learning paradigm referred to as multi-armed bandits integrating the existing research as a resource for a certain class of online experiments. We first explore the traditional stochastic model of a multi-armed bandit, then explore a taxonomic scheme of complications to that model, for each complication relating it to a specific requirement or consideration of the experiment design context. Finally, at the end of the paper, we present a table of known upper-bounds of regret for all studied algorithms providing both perspectives for future theoretical work and a decision-making tool for practitioners looking for theoretical guarantees. version:4
arxiv-1505-00387 | Highway Networks | http://arxiv.org/abs/1505.00387 | id:1505.00387 author:Rupesh Kumar Srivastava, Klaus Greff, Jürgen Schmidhuber category:cs.LG cs.NE 68T01 I.2.6; G.1.6  published:2015-05-03 summary:There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures. version:2
arxiv-1509-09292 | Convolutional Networks on Graphs for Learning Molecular Fingerprints | http://arxiv.org/abs/1509.09292 | id:1509.09292 author:David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gómez-Bombarelli, Timothy Hirzel, Alán Aspuru-Guzik, Ryan P. Adams category:cs.LG cs.NE stat.ML  published:2015-09-30 summary:We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks. version:2
arxiv-1511-00971 | Data Stream Classification using Random Feature Functions and Novel Method Combinations | http://arxiv.org/abs/1511.00971 | id:1511.00971 author:Diego Marrón, Jesse Read, Albert Bifet, Nacho Navarro category:cs.LG cs.NE  published:2015-11-03 summary:Big Data streams are being generated in a faster, bigger, and more commonplace. In this scenario, Hoeffding Trees are an established method for classification. Several extensions exist, including high-performing ensemble setups such as online and leveraging bagging. Also, $k$-nearest neighbors is a popular choice, with most extensions dealing with the inherent performance limitations over a potentially-infinite stream. At the same time, gradient descent methods are becoming increasingly popular, owing in part to the successes of deep learning. Although deep neural networks can learn incrementally, they have so far proved too sensitive to hyper-parameter options and initial conditions to be considered an effective `off-the-shelf' data-streams solution. In this work, we look at combinations of Hoeffding-trees, nearest neighbour, and gradient descent methods with a streaming preprocessing approach in the form of a random feature functions filter for additional predictive power. We further extend the investigation to implementing methods on GPUs, which we test on some large real-world datasets, and show the benefits of using GPUs for data-stream learning due to their high scalability. Our empirical evaluation yields positive results for the novel approaches that we experiment with, highlighting important issues, and shed light on promising future directions in approaches to data-stream classification. version:1
arxiv-1404-3596 | Face Detection with a 3D Model | http://arxiv.org/abs/1404.3596 | id:1404.3596 author:Adrian Barbu, Nathan Lay, Gary Gramajo category:cs.CV  published:2014-04-14 summary:This paper presents a part-based face detection approach where the spatial relationship between the face parts is represented by a hidden 3D model with six parameters. The computational complexity of the search in the six dimensional pose space is addressed by proposing meaningful 3D pose candidates by image-based regression from detected face keypoint locations. The 3D pose candidates are evaluated using a parameter sensitive classifier based on difference features relative to the 3D pose. A compatible subset of candidates is then obtained by non-maximal suppression. Experiments on two standard face detection datasets show that the proposed 3D model based approach obtains results comparable to or better than state of the art. version:7
arxiv-1511-00040 | Quantifying the Cognitive Extent of Science | http://arxiv.org/abs/1511.00040 | id:1511.00040 author:Staša Milojević category:cs.DL astro-ph.IM cs.CL physics.soc-ph  published:2015-10-30 summary:While the modern science is characterized by an exponential growth in scientific literature, the increase in publication volume clearly does not reflect the expansion of the cognitive boundaries of science. Nevertheless, most of the metrics for assessing the vitality of science or for making funding and policy decisions are based on productivity. Similarly, the increasing level of knowledge production by large science teams, whose results often enjoy greater visibility, does not necessarily mean that "big science" leads to cognitive expansion. Here we present a novel, big-data method to quantify the extents of cognitive domains of different bodies of scientific literature independently from publication volume, and apply it to 20 million articles published over 60-130 years in physics, astronomy, and biomedicine. The method is based on the lexical diversity of titles of fixed quotas of research articles. Owing to large size of quotas, the method overcomes the inherent stochasticity of article titles to achieve <1% precision. We show that the periods of cognitive growth do not necessarily coincide with the trends in publication volume. Furthermore, we show that the articles produced by larger teams cover significantly smaller cognitive territory than (the same quota of) articles from smaller teams. Our findings provide a new perspective on the role of small teams and individual researchers in expanding the cognitive boundaries of science. The proposed method of quantifying the extent of the cognitive territory can also be applied to study many other aspects of "science of science." version:2
arxiv-1506-02516 | Learning to Transduce with Unbounded Memory | http://arxiv.org/abs/1506.02516 | id:1506.02516 author:Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, Phil Blunsom category:cs.NE cs.CL cs.LG 68T05 I.5.1; I.2.6; I.2.7  published:2015-06-08 summary:Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments. version:3
arxiv-1511-00871 | Properties of the Sample Mean in Graph Spaces and the Majorize-Minimize-Mean Algorithm | http://arxiv.org/abs/1511.00871 | id:1511.00871 author:Brijnesh J. Jain category:cs.CV cs.LG stat.ML  published:2015-11-03 summary:One of the most fundamental concepts in statistics is the concept of sample mean. Properties of the sample mean that are well-defined in Euclidean spaces become unwieldy or even unclear in graph spaces. Open problems related to the sample mean of graphs include: non-existence, non-uniqueness, statistical inconsistency, lack of convergence results of mean algorithms, non-existence of midpoints, and disparity to midpoints. We present conditions to resolve all six problems and propose a Majorize-Minimize-Mean (MMM) Algorithm. Experiments on graph datasets representing images and molecules show that the MMM-Algorithm best approximates a sample mean of graphs compared to six other mean algorithms. version:1
arxiv-1511-00831 | PCA-Based Out-of-Sample Extension for Dimensionality Reduction | http://arxiv.org/abs/1511.00831 | id:1511.00831 author:Yariv Aizenbud, Amit Bermanis, Amir Averbuch category:stat.ML  published:2015-11-03 summary:Dimensionality reduction methods are very common in the field of high dimensional data analysis. Typically, algorithms for dimensionality reduction are computationally expensive. Therefore, their applications for the analysis of massive amounts of data are impractical. For example, repeated computations due to accumulated data are computationally prohibitive. In this paper, an out-of-sample extension scheme, which is used as a complementary method for dimensionality reduction, is presented. We describe an algorithm which performs an out-of-sample extension to newly-arrived data points. Unlike other extension algorithms such as Nystr\"om algorithm, the proposed algorithm uses the intrinsic geometry of the data and properties for dimensionality reduction map. We prove that the error of the proposed algorithm is bounded. Additionally to the out-of-sample extension, the algorithm provides a degree of the abnormality of any newly-arrived data point. version:1
arxiv-1506-03271 | Explore no more: Improved high-probability regret bounds for non-stochastic bandits | http://arxiv.org/abs/1506.03271 | id:1506.03271 author:Gergely Neu category:cs.LG stat.ML  published:2015-06-10 summary:This work addresses the problem of regret minimization in non-stochastic multi-armed bandit problems, focusing on performance guarantees that hold with high probability. Such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard, more intuitive algorithms that come only with guarantees that hold on expectation. One of these modifications is forcing the learner to sample arms from the uniform distribution at least $\Omega(\sqrt{T})$ times over $T$ rounds, which can adversely affect performance if many of the arms are suboptimal. While it is widely conjectured that this property is essential for proving high-probability regret bounds, we show in this paper that it is possible to achieve such strong results without this undesirable exploration component. Our result relies on a simple and intuitive loss-estimation strategy called Implicit eXploration (IX) that allows a remarkably clean analysis. To demonstrate the flexibility of our technique, we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework. Finally, we conduct a simple experiment that illustrates the robustness of our implicit exploration technique. version:3
arxiv-1511-00472 | Water Detection through Spatio-Temporal Invariant Descriptors | http://arxiv.org/abs/1511.00472 | id:1511.00472 author:Pascal Mettes, Robby T. Tan, Remco C. Veltkamp category:cs.CV  published:2015-11-02 summary:In this work, we aim to segment and detect water in videos. Water detection is beneficial for appllications such as video search, outdoor surveillance, and systems such as unmanned ground vehicles and unmanned aerial vehicles. The specific problem, however, is less discussed compared to general texture recognition. Here, we analyze several motion properties of water. First, we describe a video pre-processing step, to increase invariance against water reflections and water colours. Second, we investigate the temporal and spatial properties of water and derive corresponding local descriptors. The descriptors are used to locally classify the presence of water and a binary water detection mask is generated through spatio-temporal Markov Random Field regularization of the local classifications. Third, we introduce the Video Water Database, containing several hours of water and non-water videos, to validate our algorithm. Experimental evaluation on the Video Water Database and the DynTex database indicates the effectiveness of the proposed algorithm, outperforming multiple algorithms for dynamic texture recognition and material recognition by ca. 5% and 15% respectively. version:2
arxiv-1510-03753 | Improved Deep Learning Baselines for Ubuntu Corpus Dialogs | http://arxiv.org/abs/1510.03753 | id:1510.03753 author:Rudolf Kadlec, Martin Schmid, Jan Kleindienst category:cs.CL  published:2015-10-13 summary:This paper presents results of our experiments for the next utterance ranking on the Ubuntu Dialog Corpus -- the largest publicly available multi-turn dialog corpus. First, we use an in-house implementation of previously reported models to do an independent evaluation using the same data. Second, we evaluate the performances of various LSTMs, Bi-LSTMs and CNNs on the dataset. Third, we create an ensemble by averaging predictions of multiple models. The ensemble further improves the performance and it achieves a state-of-the-art result for the next utterance ranking on this dataset. Finally, we discuss our future plans using this corpus. version:2
arxiv-1601-03809 | Artificial neural network approach for condition-based maintenance | http://arxiv.org/abs/1601.03809 | id:1601.03809 author:Mostafa Sayyed category:cs.NE cs.CY  published:2015-11-03 summary:In this research, computerized maintenance management will be investigated. The rise of maintenance cost forced the research community to look for more effective ways to schedule maintenance operations. Using computerized models to come up with optimal maintenance policy has led to better equipment utilization and lower costs. This research adopts Condition-Based Maintenance model where the maintenance decision is generated based on equipment conditions. Artificial Neural Network technique is proposed to capture and analyze equipment condition signals which lead to higher level of knowledge gathering. This knowledge is used to accurately estimate equipment failure time. Based on these estimations, an optimal maintenance management policy can be achieved. version:1
arxiv-1506-02903 | Mixing Time Estimation in Reversible Markov Chains from a Single Sample Path | http://arxiv.org/abs/1506.02903 | id:1506.02903 author:Daniel Hsu, Aryeh Kontorovich, Csaba Szepesvári category:cs.LG stat.ML  published:2015-06-09 summary:This article provides the first procedure for computing a fully data-dependent interval that traps the mixing time $t_{\text{mix}}$ of a finite reversible ergodic Markov chain at a prescribed confidence level. The interval is computed from a single finite-length sample path from the Markov chain, and does not require the knowledge of any parameters of the chain. This stands in contrast to previous approaches, which either only provide point estimates, or require a reset mechanism, or additional prior knowledge. The interval is constructed around the relaxation time $t_{\text{relax}}$, which is strongly related to the mixing time, and the width of the interval converges to zero roughly at a $\sqrt{n}$ rate, where $n$ is the length of the sample path. Upper and lower bounds are given on the number of samples required to achieve constant-factor multiplicative accuracy. The lower bounds indicate that, unless further restrictions are placed on the chain, no procedure can achieve this accuracy level before seeing each state at least $\Omega(t_{\text{relax}})$ times on the average. Finally, future directions of research are identified. version:3
arxiv-1505-07570 | A Practical Guide to Randomized Matrix Computations with MATLAB Implementations | http://arxiv.org/abs/1505.07570 | id:1505.07570 author:Shusen Wang category:cs.MS cs.LG  published:2015-05-28 summary:Matrix operations such as matrix inversion, eigenvalue decomposition, singular value decomposition are ubiquitous in real-world applications. Unfortunately, many of these matrix operations so time and memory expensive that they are prohibitive when the scale of data is large. In real-world applications, since the data themselves are noisy, machine-precision matrix operations are not necessary at all, and one can sacrifice a reasonable amount of accuracy for computational efficiency. In recent years, a bunch of randomized algorithms have been devised to make matrix computations more scalable. Mahoney (2011) and Woodruff (2014) have written excellent but very technical reviews of the randomized algorithms. Differently, the focus of this manuscript is on intuition, algorithm derivation, and implementation. This manuscript should be accessible to people with knowledge in elementary matrix algebra but unfamiliar with randomized matrix computations. The algorithms introduced in this manuscript are all summarized in a user-friendly way, and they can be implemented in lines of MATLAB code. The readers can easily follow the implementations even if they do not understand the maths and algorithms. version:6
arxiv-1511-00754 | PAC Learning-Based Verification and Model Synthesis | http://arxiv.org/abs/1511.00754 | id:1511.00754 author:Yu-Fang Chen, Chiao Hsieh, Ondřej Lengál, Tsung-Ju Lii, Ming-Hsien Tsai, Bow-Yaw Wang, Farn Wang category:cs.SE cs.LG cs.LO  published:2015-11-03 summary:We introduce a novel technique for verification and model synthesis of sequential programs. Our technique is based on learning a regular model of the set of feasible paths in a program, and testing whether this model contains an incorrect behavior. Exact learning algorithms require checking equivalence between the model and the program, which is a difficult problem, in general undecidable. Our learning procedure is therefore based on the framework of probably approximately correct (PAC) learning, which uses sampling instead and provides correctness guarantees expressed using the terms error probability and confidence. Besides the verification result, our procedure also outputs the model with the said correctness guarantees. Obtained preliminary experiments show encouraging results, in some cases even outperforming mature software verifiers. version:1
arxiv-1506-03504 | Data Generation as Sequential Decision Making | http://arxiv.org/abs/1506.03504 | id:1506.03504 author:Philip Bachman, Doina Precup category:cs.LG stat.ML  published:2015-06-10 summary:We connect a broad class of generative models through their shared reliance on sequential decision making. Motivated by this view, we develop extensions to an existing model, and then explore the idea further in the context of data imputation -- perhaps the simplest setting in which to investigate the relation between unconditional and conditional generative modelling. We formulate data imputation as an MDP and develop models capable of representing effective policies for it. We construct the models using neural networks and train them using a form of guided policy search. Our models generate predictions through an iterative process of feedback and refinement. We show that this approach can learn effective policies for imputation problems of varying difficulty and across multiple datasets. version:3
arxiv-1511-00740 | Learning Unfair Trading: a Market Manipulation Analysis From the Reinforcement Learning Perspective | http://arxiv.org/abs/1511.00740 | id:1511.00740 author:Enrique Martínez-Miranda, Peter McBurney, Matthew J. Howard category:q-fin.TR cs.LG  published:2015-11-02 summary:Market manipulation is a strategy used by traders to alter the price of financial securities. One type of manipulation is based on the process of buying or selling assets by using several trading strategies, among them spoofing is a popular strategy and is considered illegal by market regulators. Some promising tools have been developed to detect manipulation, but cases can still be found in the markets. In this paper we model spoofing and pinging trading, two strategies that differ in the legal background but share the same elemental concept of market manipulation. We use a reinforcement learning framework within the full and partial observability of Markov decision processes and analyse the underlying behaviour of the manipulators by finding the causes of what encourages the traders to perform fraudulent activities. This reveals procedures to counter the problem that may be helpful to market regulators as our model predicts the activity of spoofers. version:1
arxiv-1505-05612 | Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering | http://arxiv.org/abs/1505.05612 | id:1505.05612 author:Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, Wei Xu category:cs.CV cs.CL cs.LG  published:2015-05-21 summary:In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long Short-Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, an LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset is evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human). The details of this work, including the FM-IQA dataset, can be found on the project page: http://idl.baidu.com/FM-IQA.html version:3
arxiv-1511-00622 | On the Number of Many-to-Many Alignments of N Sequences | http://arxiv.org/abs/1511.00622 | id:1511.00622 author:Steffen Eger category:math.CO cs.CL cs.DM  published:2015-11-02 summary:We count the number of alignments of $N \ge 1$ sequences when match-up types are from a specified set $S\subseteq \mathbb{N}^N$. Equivalently, we count the number of nonnegative integer matrices whose rows sum to a given fixed vector and each of whose columns lie in $S$. We provide a new asymptotic formula for the case $S=\{(s_1,\ldots,s_N) \: \: 1\le s_i\le 2\}$. version:1
arxiv-1511-00573 | From random walks to distances on unweighted graphs | http://arxiv.org/abs/1511.00573 | id:1511.00573 author:Tatsunori B. Hashimoto, Yi Sun, Tommi S. Jaakkola category:stat.ML cs.AI cs.SI  published:2015-11-02 summary:Large unweighted directed graphs are commonly used to capture relations between entities. A fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices. Despite the significance of this problem, statistical characterization of the proposed metrics has been limited. We introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus. Using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the Laplace transformed hitting time (LTHT). The metric serves as a natural, provably well-behaved alternative to the expected hitting time. We establish a general correspondence between hitting times of the Brownian motion and analogous hitting times on the graph. We show that the LTHT is consistent with respect to the underlying metric of a geometric graph, preserves clustering tendency, and remains robust against random addition of non-geometric edges. Tests on simulated and real-world data show that the LTHT matches theoretical predictions and outperforms alternatives. version:1
arxiv-1511-00513 | Pixel-wise Segmentation of Street with Neural Networks | http://arxiv.org/abs/1511.00513 | id:1511.00513 author:Sebastian Bittel, Vitali Kaiser, Marvin Teichmann, Martin Thoma category:cs.CV  published:2015-11-02 summary:Pixel-wise street segmentation of photographs taken from a drivers perspective is important for self-driving cars and can also support other object recognition tasks. A framework called SST was developed to examine the accuracy and execution time of different neural networks. The best neural network achieved an $F_1$-score of 89.5% with a simple feedforward neural network which trained to solve a regression task. version:1
arxiv-1508-05581 | Learning Sampling Distributions for Efficient Object Detection | http://arxiv.org/abs/1508.05581 | id:1508.05581 author:Yanwei Pang, Jiale Cao, Xuelong Li category:cs.CV cs.LG  published:2015-08-23 summary:Object detection is an important task in computer vision and learning systems. Multistage particle windows (MPW), proposed by Gualdi et al., is an algorithm of fast and accurate object detection. By sampling particle windows from a proposal distribution (PD), MPW avoids exhaustively scanning the image. Despite its success, it is unknown how to determine the number of stages and the number of particle windows in each stage. Moreover, it has to generate too many particle windows in the initialization step and it redraws unnecessary too many particle windows around object-like regions. In this paper, we attempt to solve the problems of MPW. An important fact we used is that there is large probability for a randomly generated particle window not to contain the object because the object is a sparse event relevant to the huge number of candidate windows. Therefore, we design the proposal distribution so as to efficiently reject the huge number of non-object windows. Specifically, we propose the concepts of rejection, acceptance, and ambiguity windows and regions. This contrasts to MPW which utilizes only on region of support. The PD of MPW is acceptance-oriented whereas the PD of our method (called iPW) is rejection-oriented. Experimental results on human and face detection demonstrate the efficiency and effectiveness of the iPW algorithm. The source code is publicly accessible. version:2
arxiv-1511-00461 | Circle detection using isosceles triangles sampling | http://arxiv.org/abs/1511.00461 | id:1511.00461 author:Hanqing Zhang, Krister Wiklund, Magnus Andersson category:cs.CV I.5.4  published:2015-11-02 summary:Detection of circular objects in digital images is an important problem in several vision applications. Circle detection using randomized sampling has been developed in recent years to reduce the computational intensity. Randomized sampling, however, is sensitive to noise that can lead to reduced accuracy and false-positive candidates. This paper presents a new circle detection method based upon randomized isosceles triangles sampling to improve the robustness of randomized circle detection in noisy conditions. It is shown that the geometrical property of isosceles triangles provide a robust criterion to find relevant edge pixels and thereby efficiently provide an estimation of the circle center and radii. The estimated results given by the isosceles triangles sampling from each connected component of edge map were analyzed using a simple clustering approach for efficiency. To further improve on the accuracy we applied a two-step refinement process using chords and linear error compensation with gradient information of the edge pixels. Extensive experiments using both synthetic and real images were presented and results were compared to leading state-of-the-art algorithms and showed that the proposed algorithm: are efficient in finding circles with a low number of iterations; has high rejection rate of false-positive circle candidates; and has high robustness against noise, making it adaptive and useful in many vision applications. version:1
arxiv-1511-00423 | Reading Hidden Emotions: Spontaneous Micro-expression Spotting and Recognition | http://arxiv.org/abs/1511.00423 | id:1511.00423 author:Xiaobai Li, Xiaopeng Hong, Antti Moilanen, Xiaohua Huang, Tomas Pfister, Guoying Zhao, Matti Pietikäinen category:cs.CV  published:2015-11-02 summary:Micro-expressions (MEs) are rapid, involuntary facial expressions which reveal emotions that people do not intend to show. Studying MEs is valuable as recognizing them has many important applications, particularly in forensic science and psychotherapy. However, analyzing spontaneous MEs is very challenging due to their short duration and low intensity. Automatic ME analysis includes two tasks: ME spotting and ME recognition.For ME spotting, previous studies have focused on posed rather than spontaneous videos. For ME recognition, the performance of previous studies is low. To address these challenges, we make the following contributions: (i) We propose the first method for spotting spontaneous MEs in long videos (by exploiting feature difference contrast). This method is training free and works on arbitrary unseen videos. (ii) We present an advanced ME recognition framework, which outperforms previous work by a large margin on two challenging spontaneous ME databases (SMIC and CASMEII). (iii) We propose the first automatic ME analysis system (MESR), which can spot and recognize MEs from spontaneous video data. Finally, we show that our method achieves comparable performance to humans at this very challenging task, and outperforms humans in the ME recognition task by a large margin. version:1
arxiv-1504-01575 | Bidirectional Recurrent Neural Networks as Generative Models - Reconstructing Gaps in Time Series | http://arxiv.org/abs/1504.01575 | id:1504.01575 author:Mathias Berglund, Tapani Raiko, Mikko Honkala, Leo Kärkkäinen, Akos Vetek, Juha Karhunen category:cs.LG cs.NE  published:2015-04-07 summary:Bidirectional recurrent neural networks (RNN) are trained to predict both in the positive and negative time directions simultaneously. They have not been used commonly in unsupervised tasks, because a probabilistic interpretation of the model has been difficult. Recently, two different frameworks, GSN and NADE, provide a connection between reconstruction and probabilistic modeling, which makes the interpretation possible. As far as we know, neither GSN or NADE have been studied in the context of time series before. As an example of an unsupervised task, we study the problem of filling in gaps in high-dimensional time series with complex dynamics. Although unidirectional RNNs have recently been trained successfully to model such time series, inference in the negative time direction is non-trivial. We propose two probabilistic interpretations of bidirectional RNNs that can be used to reconstruct missing gaps efficiently. Our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions, although a bit less accurate than a computationally complex bidirectional Bayesian inference on the unidirectional RNN. We also provide results on music data for which the Bayesian inference is computationally infeasible, demonstrating the scalability of the proposed methods. version:3
arxiv-1511-00360 | Automatic Prosody Prediction for Chinese Speech Synthesis using BLSTM-RNN and Embedding Features | http://arxiv.org/abs/1511.00360 | id:1511.00360 author:Chuang Ding, Lei Xie, Jie Yan, Weini Zhang, Yang Liu category:cs.CL cs.SD  published:2015-11-02 summary:Prosody affects the naturalness and intelligibility of speech. However, automatic prosody prediction from text for Chinese speech synthesis is still a great challenge and the traditional conditional random fields (CRF) based method always heavily relies on feature engineering. In this paper, we propose to use neural networks to predict prosodic boundary labels directly from Chinese characters without any feature engineering. Experimental results show that stacking feed-forward and bidirectional long short-term memory (BLSTM) recurrent network layers achieves superior performance over the CRF-based method. The embedding features learned from raw text further enhance the performance. version:1
arxiv-1511-00271 | Stochastic Top-k ListNet | http://arxiv.org/abs/1511.00271 | id:1511.00271 author:Tianyi Luo, Dong Wang, Rong Liu, Yiqiao Pan category:cs.IR cs.LG  published:2015-11-01 summary:ListNet is a well-known listwise learning to rank model and has gained much attention in recent years. A particular problem of ListNet, however, is the high computation complexity in model training, mainly due to the large number of object permutations involved in computing the gradients. This paper proposes a stochastic ListNet approach which computes the gradient within a bounded permutation subset. It significantly reduces the computation complexity of model training and allows extension to Top-k models, which is impossible with the conventional implementation based on full-set permutations. Meanwhile, the new approach utilizes partial ranking information of human labels, which helps improve model quality. Our experiments demonstrated that the stochastic ListNet method indeed leads to better ranking performance and speeds up the model training remarkably. version:1
arxiv-1504-01255 | Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding | http://arxiv.org/abs/1504.01255 | id:1504.01255 author:Rie Johnson, Tong Zhang category:stat.ML cs.CL cs.LG  published:2015-04-06 summary:This paper presents a new semi-supervised framework with convolutional neural networks (CNNs) for text categorization. Unlike the previous approaches that rely on word embeddings, our method learns embeddings of small text regions from unlabeled data for integration into a supervised CNN. The proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning, which is intended to be useful for the task of interest even though the training is done on unlabeled data. Our models achieve better results than previous approaches on sentiment classification and topic classification tasks. version:3
arxiv-1511-00221 | LM-CMA: an Alternative to L-BFGS for Large Scale Black-box Optimization | http://arxiv.org/abs/1511.00221 | id:1511.00221 author:Ilya Loshchilov category:cs.NE math.OC  published:2015-11-01 summary:The limited memory BFGS method (L-BFGS) of Liu and Nocedal (1989) is often considered to be the method of choice for continuous optimization when first- and/or second- order information is available. However, the use of L-BFGS can be complicated in a black-box scenario where gradient information is not available and therefore should be numerically estimated. The accuracy of this estimation, obtained by finite difference methods, is often problem-dependent that may lead to premature convergence of the algorithm. In this paper, we demonstrate an alternative to L-BFGS, the limited memory Covariance Matrix Adaptation Evolution Strategy (LM-CMA) proposed by Loshchilov (2014). The LM-CMA is a stochastic derivative-free algorithm for numerical optimization of non-linear, non-convex optimization problems. Inspired by the L-BFGS, the LM-CMA samples candidate solutions according to a covariance matrix reproduced from $m$ direction vectors selected during the optimization process. The decomposition of the covariance matrix into Cholesky factors allows to reduce the memory complexity to $O(mn)$, where $n$ is the number of decision variables. The time complexity of sampling one candidate solution is also $O(mn)$, but scales as only about 25 scalar-vector multiplications in practice. The algorithm has an important property of invariance w.r.t. strictly increasing transformations of the objective function, such transformations do not compromise its ability to approach the optimum. The LM-CMA outperforms the original CMA-ES and its large scale versions on non-separable ill-conditioned problems with a factor increasing with problem dimension. Invariance properties of the algorithm do not prevent it from demonstrating a comparable performance to L-BFGS on non-trivial large scale smooth and nonsmooth optimization problems. version:1
arxiv-1508-03398 | End-to-end Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture | http://arxiv.org/abs/1508.03398 | id:1508.03398 author:Jianshu Chen, Ji He, Yelong Shen, Lin Xiao, Xiaodong He, Jianfeng Gao, Xinying Song, Li Deng category:cs.LG  published:2015-08-14 summary:We develop a fully discriminative learning approach for supervised Latent Dirichlet Allocation (LDA) model using Back Propagation (i.e., BP-sLDA), which maximizes the posterior probability of the prediction variable given the input document. Different from traditional variational learning or Gibbs sampling approaches, the proposed learning method applies (i) the mirror descent algorithm for maximum a posterior inference and (ii) back propagation over a deep architecture together with stochastic gradient/mirror descent for model parameter estimation, leading to scalable and end-to-end discriminative learning of the model. As a byproduct, we also apply this technique to develop a new learning method for the traditional unsupervised LDA model (i.e., BP-LDA). Experimental results on three real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised topic models, neural networks, and is on par with deep neural networks. version:2
arxiv-1511-00215 | A Unified Tagging Solution: Bidirectional LSTM Recurrent Neural Network with Word Embedding | http://arxiv.org/abs/1511.00215 | id:1511.00215 author:Peilu Wang, Yao Qian, Frank K. Soong, Lei He, Hai Zhao category:cs.CL  published:2015-11-01 summary:Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has been shown to be very effective for modeling and predicting sequential data, e.g. speech utterances or handwritten documents. In this study, we propose to use BLSTM-RNN for a unified tagging solution that can be applied to various tagging tasks including part-of-speech tagging, chunking and named entity recognition. Instead of exploiting specific features carefully optimized for each task, our solution only uses one set of task-independent features and internal representations learnt from unlabeled text for all tasks.Requiring no task specific knowledge or sophisticated feature engineering, our approach gets nearly state-of-the-art performance in all these three tagging tasks. version:1
arxiv-1508-00330 | On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units | http://arxiv.org/abs/1508.00330 | id:1508.00330 author:Zhibin Liao, Gustavo Carneiro category:cs.CV cs.LG cs.NE  published:2015-08-03 summary:Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets. The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classification of similar input examples. During the training process, these subnetworks avoid overfitting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a significant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classification results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets. version:2
arxiv-1412-7419 | ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient | http://arxiv.org/abs/1412.7419 | id:1412.7419 author:Caglar Gulcehre, Marcin Moczulski, Yoshua Bengio category:cs.LG cs.NE stat.ML  published:2014-12-23 summary:Stochastic gradient algorithms have been the main focus of large-scale learning problems and they led to important successes in machine learning. The convergence of SGD depends on the careful choice of learning rate and the amount of the noise in stochastic estimates of the gradients. In this paper, we propose a new adaptive learning rate algorithm, which utilizes curvature information for automatically tuning the learning rates. The information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our preliminary experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms. version:5
arxiv-1506-04696 | A Complete Recipe for Stochastic Gradient MCMC | http://arxiv.org/abs/1506.04696 | id:1506.04696 author:Yi-An Ma, Tianqi Chen, Emily B. Fox category:math.ST stat.ME stat.ML stat.TH  published:2015-06-15 summary:Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial. Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing MCMC samplers--including stochastic gradient versions--based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We show how previous continuous-dynamic samplers can be trivially "reinvented" in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods. version:2
arxiv-1511-00158 | Support Vector Regression, Smooth Splines, and Time Series Prediction | http://arxiv.org/abs/1511.00158 | id:1511.00158 author:Raymundo Navarrete, Divakar Viswanath category:stat.ML cs.LG  published:2015-10-31 summary:Delay coordinates and support vector regression are among the techniques commonly used for time series prediction. We show that the combination of these two techniques leads to systematic error that obstructs convergence. A preliminary step of spline smoothing restores convergence and leads to predictions that are consistently more accurate, typically by about a factor of $2$ or so. Since the algorithm without spline smoothing is not convergent, the improvement in accuracy can even be as high as a factor of $100$. Assuming local isotropy, the systematic error in the absence of spline smoothing is estimated to be $d\sigma^{2}L/2$, where $d$ is the embedding dimension, $\sigma^{2}$ is the variance of Gaussian noise in the signal, and $L$ is a global bound on the Hessian of the exact predictor. The smooth spline, although very effective, is shown not to have even first order accuracy, unless the noise is unusually mild. The lack of order of accuracy implies that attempts to take advantage of invariance in time to enhance fidelity of learning are unlikely to be successful. version:1
arxiv-1510-07965 | Blitzkriging: Kronecker-structured Stochastic Gaussian Processes | http://arxiv.org/abs/1510.07965 | id:1510.07965 author:Thomas Nickson, Tom Gunter, Chris Lloyd, Michael A Osborne, Stephen Roberts category:stat.ML  published:2015-10-27 summary:We present Blitzkriging, a new approach to fast inference for Gaussian processes, applicable to regression, optimisation and classification. State-of-the-art (stochastic) inference for Gaussian processes on very large datasets scales cubically in the number of 'inducing inputs', variables introduced to factorise the model. Blitzkriging shares state-of-the-art scaling with data, but reduces the scaling in the number of inducing points to approximately linear. Further, in contrast to other methods, Blitzkriging: does not force the data to conform to any particular structure (including grid-like); reduces reliance on error-prone optimisation of inducing point locations; and is able to learn rich (covariance) structure from the data. We demonstrate the benefits of our approach on real data in regression, time-series prediction and signal-interpolation experiments. version:2
arxiv-1507-07073 | Efficient Face Alignment via Locality-constrained Representation for Robust Recognition | http://arxiv.org/abs/1507.07073 | id:1507.07073 author:Yandong Wen, Weiyang Liu, Meng Yang, Zhifeng Li category:cs.CV  published:2015-07-25 summary:Practical face recognition has been studied in the past decades, but still remains an open challenge. Current prevailing approaches have already achieved substantial breakthroughs in recognition accuracy. However, their performance usually drops dramatically if face samples are severely misaligned. To address this problem, we propose a highly efficient misalignment-robust locality-constrained representation (MRLR) algorithm for practical real-time face recognition. Specifically, the locality constraint that activates the most correlated atoms and suppresses the uncorrelated ones, is applied to construct the dictionary for face alignment. Then we simultaneously align the warped face and update the locality-constrained dictionary, eventually obtaining the final alignment. Moreover, we make use of the block structure to accelerate the derived analytical solution. Experimental results on public data sets show that MRLR significantly outperforms several state-of-the-art approaches in terms of efficiency and scalability with even better performance. version:2
arxiv-1511-00111 | Regional Active Contours based on Variational level sets and Machine Learning for Image Segmentation | http://arxiv.org/abs/1511.00111 | id:1511.00111 author:M. Abdelsamea category:cs.CV  published:2015-10-31 summary:Image segmentation is the problem of partitioning an image into different subsets, where each subset may have a different characterization in terms of color, intensity, texture, and/or other features. Segmentation is a fundamental component of image processing, and plays a significant role in computer vision, object recognition, and object tracking. Active Contour Models (ACMs) constitute a powerful energy-based minimization framework for image segmentation, which relies on the concept of contour evolution. Starting from an initial guess, the contour is evolved with the aim of approximating better and better the actual object boundary. Handling complex images in an efficient, effective, and robust way is a real challenge, especially in the presence of intensity inhomogeneity, overlap between the foreground/background intensity distributions, objects characterized by many different intensities, and/or additive noise. In this thesis, to deal with these challenges, we propose a number of image segmentation models relying on variational level set methods and specific kinds of neural networks, to handle complex images in both supervised and unsupervised ways. Experimental results demonstrate the high accuracy of the segmentation results, obtained by the proposed models on various benchmark synthetic and real images compared with state-of-the-art active contour models. version:1
arxiv-1511-00100 | Fast Neuromimetic Object Recognition using FPGA Outperforms GPU Implementations | http://arxiv.org/abs/1511.00100 | id:1511.00100 author:Garrick Orchard, Jacob G. Martin, R. Jacob Vogelstein, Ralph Etienne-Cummings category:cs.CV  published:2015-10-31 summary:Recognition of objects in still images has traditionally been regarded as a difficult computational problem. Although modern automated methods for visual object recognition have achieved steadily increasing recognition accuracy, even the most advanced computational vision approaches are unable to obtain performance equal to that of humans. This has led to the creation of many biologically-inspired models of visual object recognition, among them the HMAX model. HMAX is traditionally known to achieve high accuracy in visual object recognition tasks at the expense of significant computational complexity. Increasing complexity, in turn, increases computation time, reducing the number of images that can be processed per unit time. In this paper we describe how the computationally intensive, biologically inspired HMAX model for visual object recognition can be modified for implementation on a commercial Field Programmable Gate Array, specifically the Xilinx Virtex 6 ML605 evaluation board with XC6VLX240T FPGA. We show that with minor modifications to the traditional HMAX model we can perform recognition on images of size 128x128 pixels at a rate of 190 images per second with a less than 1% loss in recognition accuracy in both binary and multi-class visual object recognition tasks. version:1
arxiv-1511-00099 | Sketch-based Image Retrieval from Millions of Images under Rotation, Translation and Scale Variations | http://arxiv.org/abs/1511.00099 | id:1511.00099 author:Sarthak Parui, Anurag Mittal category:cs.CV cs.IR  published:2015-10-31 summary:Proliferation of touch-based devices has made sketch-based image retrieval practical. While many methods exist for sketch-based object detection/image retrieval on small datasets, relatively less work has been done on large (web)-scale image retrieval. In this paper, we present an efficient approach for image retrieval from millions of images based on user-drawn sketches. Unlike existing methods for this problem which are sensitive to even translation or scale variations, our method handles rotation, translation, scale (i.e. a similarity transformation) and small deformations. The object boundaries are represented as chains of connected segments and the database images are pre-processed to obtain such chains that have a high chance of containing the object. This is accomplished using two approaches in this work: a) extracting long chains in contour segment networks and b) extracting boundaries of segmented object proposals. These chains are then represented by similarity-invariant variable length descriptors. Descriptor similarities are computed by a fast Dynamic Programming-based partial matching algorithm. This matching mechanism is used to generate a hierarchical k-medoids based indexing structure for the extracted chains of all database images in an offline process which is used to efficiently retrieve a small set of possible matched images for query chains. Finally, a geometric verification step is employed to test geometric consistency of multiple chain matches to improve results. Qualitative and quantitative results clearly demonstrate superiority of the approach over existing methods. version:1
arxiv-1511-00098 | Semantic Cross-View Matching | http://arxiv.org/abs/1511.00098 | id:1511.00098 author:Francesco Castaldo, Amir Zamir, Roland Angst, Francesco Palmieri, Silvio Savarese category:cs.CV  published:2015-10-31 summary:Matching cross-view images is challenging because the appearance and viewpoints are significantly different. While low-level features based on gradient orientations or filter responses can drastically vary with such changes in viewpoint, semantic information of images however shows an invariant characteristic in this respect. Consequently, semantically labeled regions can be used for performing cross-view matching. In this paper, we therefore explore this idea and propose an automatic method for detecting and representing the semantic information of an RGB image with the goal of performing cross-view matching with a (non-RGB) geographic information system (GIS). A segmented image forms the input to our system with segments assigned to semantic concepts such as traffic signs, lakes, roads, foliage, etc. We design a descriptor to robustly capture both, the presence of semantic concepts and the spatial layout of those segments. Pairwise distances between the descriptors extracted from the GIS map and the query image are then used to generate a shortlist of the most promising locations with similar semantic concepts in a consistent spatial layout. An experimental evaluation with challenging query images and a large urban area shows promising results. version:1
arxiv-1511-00096 | Bioinspired Visual Motion Estimation | http://arxiv.org/abs/1511.00096 | id:1511.00096 author:Garrick Orchard, Ralph Etienne-Cummings category:cs.CV  published:2015-10-31 summary:Visual motion estimation is a computationally intensive, but important task for sighted animals. Replicating the robustness and efficiency of biological visual motion estimation in artificial systems would significantly enhance the capabilities of future robotic agents. 25 years ago, in this very journal, Carver Mead outlined his argument for replicating biological processing in silicon circuits. His vision served as the foundation for the field of neuromorphic engineering, which has experienced a rapid growth in interest over recent years as the ideas and technologies mature. Replicating biological visual sensing was one of the first tasks attempted in the neuromorphic field. In this paper we focus specifically on the task of visual motion estimation. We describe the task itself, present the progression of works from the early first attempts through to the modern day state-of-the-art, and provide an outlook for future directions in the field. version:1
arxiv-1506-03159 | Copula variational inference | http://arxiv.org/abs/1506.03159 | id:1506.03159 author:Dustin Tran, David M. Blei, Edoardo M. Airoldi category:stat.ML cs.LG stat.CO stat.ME  published:2015-06-10 summary:We develop a general variational inference method that preserves dependency among the latent variables. Our method uses copulas to augment the families of distributions used in mean-field and structured approximations. Copulas model the dependency that is not captured by the original variational distribution, and thus the augmented variational family guarantees better approximations to the posterior. With stochastic optimization, inference on the augmented distribution is scalable. Furthermore, our strategy is generic: it can be applied to any inference procedure that currently uses the mean-field or structured approach. Copula variational inference has many advantages: it reduces bias; it is less sensitive to local optima; it is less sensitive to hyperparameters; and it helps characterize and interpret the dependency among the latent variables. version:2
arxiv-1508-06235 | Clustering With Side Information: From a Probabilistic Model to a Deterministic Algorithm | http://arxiv.org/abs/1508.06235 | id:1508.06235 author:Daniel Khashabi, John Wieting, Jeffrey Yufei Liu, Feng Liang category:stat.ML cs.AI cs.LG stat.CO  published:2015-08-25 summary:In this paper, we propose a model-based clustering method (TVClust) that robustly incorporates noisy side information as soft-constraints and aims to seek a consensus between side information and the observed data. Our method is based on a nonparametric Bayesian hierarchical model that combines the probabilistic model for the data instance and the one for the side-information. An efficient Gibbs sampling algorithm is proposed for posterior inference. Using the small-variance asymptotics of our probabilistic model, we then derive a new deterministic clustering algorithm (RDP-means). It can be viewed as an extension of K-means that allows for the inclusion of side information and has the additional property that the number of clusters does not need to be specified a priori. Empirical studies have been carried out to compare our work with many constrained clustering algorithms from the literature on both a variety of data sets and under a variety of conditions such as using noisy side information and erroneous k values. The results of our experiments show strong results for our probabilistic and deterministic approaches under these conditions when compared to other algorithms in the literature. version:4
arxiv-1511-00054 | Gaussian Process Random Fields | http://arxiv.org/abs/1511.00054 | id:1511.00054 author:David A. Moore, Stuart J. Russell category:cs.LG stat.ML  published:2015-10-31 summary:Gaussian processes have been successful in both supervised and unsupervised machine learning tasks, but their computational complexity has constrained practical applications. We introduce a new approximation for large-scale Gaussian processes, the Gaussian Process Random Field (GPRF), in which local GPs are coupled via pairwise potentials. The GPRF likelihood is a simple, tractable, and parallelizeable approximation to the full GP marginal likelihood, enabling latent variable modeling and hyperparameter selection on large datasets. We demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location. version:1
arxiv-1511-00048 | The Pareto Regret Frontier for Bandits | http://arxiv.org/abs/1511.00048 | id:1511.00048 author:Tor Lattimore category:cs.LG  published:2015-10-30 summary:Given a multi-armed bandit problem it may be desirable to achieve a smaller-than-usual worst-case regret for some special actions. I show that the price for such unbalanced worst-case regret guarantees is rather high. Specifically, if an algorithm enjoys a worst-case regret of B with respect to some action, then there must exist another action for which the worst-case regret is at least {\Omega}(nK/B), where n is the horizon and K the number of actions. I also give upper bounds in both the stochastic and adversarial settings showing that this result cannot be improved. For the stochastic case the pareto regret frontier is characterised exactly up to constant factors. version:1
arxiv-1506-02626 | Learning both Weights and Connections for Efficient Neural Networks | http://arxiv.org/abs/1506.02626 | id:1506.02626 author:Song Han, Jeff Pool, John Tran, William J. Dally category:cs.NE cs.CV cs.LG  published:2015-06-08 summary:Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy. version:3
arxiv-1510-05198 | Learning multi-faceted representations of individuals from heterogeneous evidence using neural networks | http://arxiv.org/abs/1510.05198 | id:1510.05198 author:Jiwei Li, Alan Ritter, Dan Jurafsky category:cs.SI cs.CL  published:2015-10-18 summary:Inferring latent attributes of people online is an important social computing task, but requires integrating the many heterogeneous sources of information available on the web. We propose to learn individual representations of people using neural nets to integrate information from social media. The algorithm is able to combine any kind of cues, such as the text a person writes, the person's attributes (e.g. gender, employer, school, location) and social relations to other people (e.g., friendship, marriage), using global inference to infer missing attributes from noisy cues. The resulting latent representations capture homophily: people who have similar attributes, are related socially, or write similar text are closer in vector space. We show that these learned representations offer good performance at solving four important tasks in social media inference on Twitter: predicting (1) gender, (2) occupation, (3) location, and (4) friendships for users, and that we achieve the best performance by integrating all these signals. Our approach scales to large datasets, using parallel stochastic gradient descent for learning. The resulting representations can be used as general features in and have the potential to benefit a large number of downstream tasks like link prediction, community detection, or reasoning over social networks, discovering for example the high probability that a New York City resident is a fan of the New York Knicks, or the greater preference for iPhones by computer professionals than legal professionals. version:2
arxiv-1511-00041 | Learning Causal Graphs with Small Interventions | http://arxiv.org/abs/1511.00041 | id:1511.00041 author:Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G. Dimakis, Sriram Vishwanath category:cs.AI cs.IT cs.LG math.IT stat.ML  published:2015-10-30 summary:We consider the problem of learning causal networks with interventions, when each intervention is limited in size under Pearl's Structural Equation Model with independent errors (SEM-IE). The objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph. Previous work has focused on the use of separating systems for complete graphs for this task. We prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case. In addition, we present a novel separating system construction, whose size is close to optimal and is arguably simpler than previous work in combinatorics. We also develop a novel information theoretic lower bound on the number of interventions that applies in full generality, including for randomized adaptive learning algorithms. For general chordal graphs, we derive worst case lower bounds on the number of interventions. Building on observations about induced trees, we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely. In the worst case, our achievable scheme is an $\alpha$-approximation algorithm where $\alpha$ is the independence number of the graph. We also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound. In the other extreme, there are graph classes for which the required number of experiments is multiplicatively $\alpha$ away from our lower bound. In simulations, our algorithm almost always performs very close to the lower bound, while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs. version:1
arxiv-1506-04820 | Online Gradient Boosting | http://arxiv.org/abs/1506.04820 | id:1506.04820 author:Alina Beygelzimer, Elad Hazan, Satyen Kale, Haipeng Luo category:cs.LG  published:2015-06-16 summary:We extend the theory of boosting for regression problems to the online learning setting. Generalizing from the batch setting for boosting, the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions, while a strong learning algorithm is an online learning algorithm with convex loss functions that competes with a larger class of regression functions. Our main result is an online gradient boosting algorithm which converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class. We also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class, and prove its optimality. version:2
arxiv-1510-09219 | Submatrix localization via message passing | http://arxiv.org/abs/1510.09219 | id:1510.09219 author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.IT cs.SI math.IT math.PR math.ST stat.TH  published:2015-10-30 summary:The principal submatrix localization problem deals with recovering a $K\times K$ principal submatrix of elevated mean $\mu$ in a large $n\times n$ symmetric matrix subject to additive standard Gaussian noise. This problem serves as a prototypical example for community detection, in which the community corresponds to the support of the submatrix. The main result of this paper is that in the regime $\Omega(\sqrt{n}) \leq K \leq o(n)$, the support of the submatrix can be weakly recovered (with $o(K)$ misclassification errors on average) by an optimized message passing algorithm if $\lambda = \mu^2K^2/n$, the signal-to-noise ratio, exceeds $1/e$. This extends a result by Deshpande and Montanari previously obtained for $K=\Theta(\sqrt{n}).$ In addition, the algorithm can be extended to provide exact recovery whenever information-theoretically possible and achieve the information limit of exact recovery as long as $K \geq \frac{n}{\log n} (\frac{1}{8e} + o(1))$. The total running time of the algorithm is $O(n^2\log n)$. Another version of the submatrix localization problem, known as noisy biclustering, aims to recover a $K_1\times K_2$ submatrix of elevated mean $\mu$ in a large $n_1\times n_2$ Gaussian matrix. The optimized message passing algorithm and its analysis are adapted to the bicluster problem assuming $\Omega(\sqrt{n_i}) \leq K_i \leq o(n_i)$ and $K_1\asymp K_2.$ A sharp information-theoretic condition for the weak recovery of both clusters is also identified. version:1
arxiv-1504-05477 | Randomized Block Krylov Methods for Stronger and Faster Approximate Singular Value Decomposition | http://arxiv.org/abs/1504.05477 | id:1504.05477 author:Cameron Musco, Christopher Musco category:cs.DS cs.LG cs.NA  published:2015-04-21 summary:Since being analyzed by Rokhlin, Szlam, and Tygert and popularized by Halko, Martinsson, and Tropp, randomized Simultaneous Power Iteration has become the method of choice for approximate singular value decomposition. It is more accurate than simpler sketching algorithms, yet still converges quickly for any matrix, independently of singular value gaps. After $\tilde{O}(1/\epsilon)$ iterations, it gives a low-rank approximation within $(1+\epsilon)$ of optimal for spectral norm error. We give the first provable runtime improvement on Simultaneous Iteration: a simple randomized block Krylov method, closely related to the classic Block Lanczos algorithm, gives the same guarantees in just $\tilde{O}(1/\sqrt{\epsilon})$ iterations and performs substantially better experimentally. Despite their long history, our analysis is the first of a Krylov subspace method that does not depend on singular value gaps, which are unreliable in practice. Furthermore, while it is a simple accuracy benchmark, even $(1+\epsilon)$ error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components, a major issue for data applications. We address this problem for the first time by showing that both Block Krylov Iteration and a minor modification of Simultaneous Iteration give nearly optimal PCA for any matrix. This result further justifies their strength over non-iterative sketching methods. Finally, we give insight beyond the worst case, justifying why both algorithms can run much faster in practice than predicted. We clarify how simple techniques can take advantage of common matrix properties to significantly improve runtime. version:4
arxiv-1510-09202 | Generating Text with Deep Reinforcement Learning | http://arxiv.org/abs/1510.09202 | id:1510.09202 author:Hongyu Guo category:cs.CL cs.LG cs.NE  published:2015-10-30 summary:We introduce a novel schema for sequence to sequence learning with a Deep Q-Network (DQN), which decodes the output sequence iteratively. The aim here is to enable the decoder to first tackle easier portions of the sequences, and then turn to cope with difficult parts. Specifically, in each iteration, an encoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the input sequence, automatically create features to represent the internal states of and formulate a list of potential actions for the DQN. Take rephrasing a natural sentence as an example. This list can contain ranked potential words. Next, the DQN learns to make decision on which action (e.g., word) will be selected from the list to modify the current decoded sequence. The newly modified output sequence is subsequently used as the input to the DQN for the next decoding iteration. In each iteration, we also bias the reinforcement learning's attention to explore sequence portions which are previously difficult to be decoded. For evaluation, the proposed strategy was trained to decode ten thousands natural sentences. Our experiments indicate that, when compared to a left-to-right greedy beam search LSTM decoder, the proposed method performed competitively well when decoding sentences from the training set, but significantly outperformed the baseline when decoding unseen sentences, in terms of BLEU score obtained. version:1
arxiv-1510-09184 | Estimating Target Signatures with Diverse Density | http://arxiv.org/abs/1510.09184 | id:1510.09184 author:Taylor Glenn, Alina Zare category:cs.CV  published:2015-10-30 summary:Hyperspectral target detection algorithms rely on knowing the desired target signature in advance. However, obtaining an effective target signature can be difficult; signatures obtained from laboratory measurements or hand-spectrometers in the field may not transfer to airborne imagery effectively. One approach to dealing with this difficulty is to learn an effective target signature from training data. An approach for learning target signatures from training data is presented. The proposed approach addresses uncertainty and imprecision in groundtruth in the training data using a multiple instance learning, diverse density (DD) based objective function. After learning the target signature given data with uncertain and imprecise groundtruth, target detection can be applied on test data. Results are shown on simulated and real data. version:1
arxiv-1412-0473 | Sparse Variational Bayesian Approximations for Nonlinear Inverse Problems: applications in nonlinear elastography | http://arxiv.org/abs/1412.0473 | id:1412.0473 author:Isabell M. Franck, P. S. Koutsourelakis category:stat.AP math.NA physics.comp-ph stat.ML  published:2014-12-01 summary:This paper presents an efficient Bayesian framework for solving nonlinear, high-dimensional model calibration problems. It is based on a Variational Bayesian formulation that aims at approximating the exact posterior by means of solving an optimization problem over an appropriately selected family of distributions. The goal is two-fold. Firstly, to find lower-dimensional representations of the unknown parameter vector that capture as much as possible of the associated posterior density, and secondly to enable the computation of the approximate posterior density with as few forward calls as possible. We discuss how these objectives can be achieved by using a fully Bayesian argumentation and employing the marginal likelihood or evidence as the ultimate model validation metric for any proposed dimensionality reduction. We demonstrate the performance of the proposed methodology for problems in nonlinear elastography where the identification of the mechanical properties of biological materials can inform non-invasive, medical diagnosis. An Importance Sampling scheme is finally employed in order to validate the results and assess the efficacy of the approximations provided. version:4
arxiv-1510-09171 | Accurate Vision-based Vehicle Localization using Satellite Imagery | http://arxiv.org/abs/1510.09171 | id:1510.09171 author:Hang Chu, Hongyuan Mei, Mohit Bansal, Matthew R. Walter category:cs.RO cs.CV  published:2015-10-30 summary:We propose a method for accurately localizing ground vehicles with the aid of satellite imagery. Our approach takes a ground image as input, and outputs the location from which it was taken on a georeferenced satellite image. We perform visual localization by estimating the co-occurrence probabilities between the ground and satellite images based on a ground-satellite feature dictionary. The method is able to estimate likelihoods over arbitrary locations without the need for a dense ground image database. We present a ranking-loss based algorithm that learns location-discriminative feature projection matrices that result in further improvements in accuracy. We evaluate our method on the Malaga and KITTI public datasets and demonstrate significant improvements over a baseline that performs exhaustive search. version:1
arxiv-1510-09161 | Streaming, Distributed Variational Inference for Bayesian Nonparametrics | http://arxiv.org/abs/1510.09161 | id:1510.09161 author:Trevor Campbell, Julian Straub, John W. Fisher III, Jonathan P. How category:cs.LG stat.ML  published:2015-10-30 summary:This paper presents a methodology for creating streaming, distributed inference algorithms for Bayesian nonparametric (BNP) models. In the proposed framework, processing nodes receive a sequence of data minibatches, compute a variational posterior for each, and make asynchronous streaming updates to a central model. In contrast to previous algorithms, the proposed framework is truly streaming, distributed, asynchronous, learning-rate-free, and truncation-free. The key challenge in developing the framework, arising from the fact that BNP models do not impose an inherent ordering on their components, is finding the correspondence between minibatch and central BNP posterior components before performing each update. To address this, the paper develops a combinatorial optimization problem over component correspondences, and provides an efficient solution technique. The paper concludes with an application of the methodology to the DP mixture model, with experimental results demonstrating its practical scalability and performance. version:1
arxiv-1510-09142 | Learning Continuous Control Policies by Stochastic Value Gradients | http://arxiv.org/abs/1510.09142 | id:1510.09142 author:Nicolas Heess, Greg Wayne, David Silver, Timothy Lillicrap, Yuval Tassa, Tom Erez category:cs.LG cs.NE  published:2015-10-30 summary:We present a unified framework for learning continuous control policies using backpropagation. It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise. The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions. We use learned models but only require observations from the environment in- stead of observations from model-predicted trajectories, minimizing the impact of compounded model errors. We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation. One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains. version:1
arxiv-1510-09130 | Latent Bayesian melding for integrating individual and population models | http://arxiv.org/abs/1510.09130 | id:1510.09130 author:Mingjun Zhong, Nigel Goddard, Charles Sutton category:stat.ML cs.AI stat.AP stat.ME  published:2015-10-30 summary:In many statistical problems, a more coarse-grained model may be suitable for population-level behaviour, whereas a more detailed model is appropriate for accurate modelling of individual behaviour. This raises the question of how to integrate both types of models. Methods such as posterior regularization follow the idea of generalized moment matching, in that they allow matching expectations between two models, but sometimes both models are most conveniently expressed as latent variable models. We propose latent Bayesian melding, which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework. In a case study on electricity disaggregation, which is a type of single-channel blind source separation problem, we show that latent Bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching. version:1
arxiv-1510-09123 | Subsampling in Smoothed Range Spaces | http://arxiv.org/abs/1510.09123 | id:1510.09123 author:Jeff M. Phillips, Yan Zheng category:cs.CG cs.LG  published:2015-10-30 summary:We consider smoothed versions of geometric range spaces, so an element of the ground set (e.g. a point) can be contained in a range with a non-binary value in $[0,1]$. Similar notions have been considered for kernels; we extend them to more general types of ranges. We then consider approximations of these range spaces through $\varepsilon $-nets and $\varepsilon $-samples (aka $\varepsilon$-approximations). We characterize when size bounds for $\varepsilon $-samples on kernels can be extended to these more general smoothed range spaces. We also describe new generalizations for $\varepsilon $-nets to these range spaces and show when results from binary range spaces can carry over to these smoothed ones. version:1
arxiv-1309-6487 | A Unified Framework for Representation-based Subspace Clustering of Out-of-sample and Large-scale Data | http://arxiv.org/abs/1309.6487 | id:1309.6487 author:Xi Peng, Huajin Tang, Lei Zhang, Zhang Yi, Shijie Xiao category:cs.LG cs.CV stat.ML  published:2013-09-25 summary:Under the framework of spectral clustering, the key of subspace clustering is building a similarity graph which describes the neighborhood relations among data points. Some recent works build the graph using sparse, low-rank, and $\ell_2$-norm-based representation, and have achieved state-of-the-art performance. However, these methods have suffered from the following two limitations. First, the time complexities of these methods are at least proportional to the cube of the data size, which make those methods inefficient for solving large-scale problems. Second, they cannot cope with out-of-sample data that are not used to construct the similarity graph. To cluster each out-of-sample datum, the methods have to recalculate the similarity graph and the cluster membership of the whole data set. In this paper, we propose a unified framework which makes representation-based subspace clustering algorithms feasible to cluster both out-of-sample and large-scale data. Under our framework, the large-scale problem is tackled by converting it as out-of-sample problem in the manner of "sampling, clustering, coding, and classifying". Furthermore, we give an estimation for the error bounds by treating each subspace as a point in a hyperspace. Extensive experimental results on various benchmark data sets show that our methods outperform several recently-proposed scalable methods in clustering large-scale data set. version:2
arxiv-1510-09079 | SentiWords: Deriving a High Precision and High Coverage Lexicon for Sentiment Analysis | http://arxiv.org/abs/1510.09079 | id:1510.09079 author:Lorenzo Gatti, Marco Guerini, Marco Turchi category:cs.CL  published:2015-10-30 summary:Deriving prior polarity lexica for sentiment analysis - where positive or negative scores are associated with words out of context - is a challenging task. Usually, a trade-off between precision and coverage is hard to find, and it depends on the methodology used to build the lexicon. Manually annotated lexica provide a high precision but lack in coverage, whereas automatic derivation from pre-existing knowledge guarantees high coverage at the cost of a lower precision. Since the automatic derivation of prior polarities is less time consuming than manual annotation, there has been a great bloom of these approaches, in particular based on the SentiWordNet resource. In this paper, we compare the most frequently used techniques based on SentiWordNet with newer ones and blend them in a learning framework (a so called 'ensemble method'). By taking advantage of manually built prior polarity lexica, our ensemble method is better able to predict the prior value of unseen words and to outperform all the other SentiWordNet approaches. Using this technique we have built SentiWords, a prior polarity lexicon of approximately 155,000 words, that has both a high precision and a high coverage. We finally show that in sentiment analysis tasks, using our lexicon allows us to outperform both the single metrics derived from SentiWordNet and popular manually annotated sentiment lexica. version:1
arxiv-1510-09005 | A Study of the Spatio-Temporal Correlations in Mobile Calls Networks | http://arxiv.org/abs/1510.09005 | id:1510.09005 author:Romain Guigourès, Marc Boullé, Fabrice Rossi category:stat.ML cs.SI  published:2015-10-30 summary:For the last few years, the amount of data has significantly increased in the companies. It is the reason why data analysis methods have to evolve to meet new demands. In this article, we introduce a practical analysis of a large database from a telecommunication operator. The problem is to segment a territory and characterize the retrieved areas owing to their inhabitant behavior in terms of mobile telephony. We have call detail records collected during five months in France. We propose a two stages analysis. The first one aims at grouping source antennas which originating calls are similarly distributed on target antennas and conversely for target antenna w.r.t. source antenna. A geographic projection of the data is used to display the results on a map of France. The second stage discretizes the time into periods between which we note changes in distributions of calls emerging from the clusters of source antennas. This enables an analysis of temporal changes of inhabitants behavior in every area of the country. version:1
arxiv-1509-08992 | Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing Parameter Sets | http://arxiv.org/abs/1509.08992 | id:1509.08992 author:Justin Domke category:cs.LG stat.ML  published:2015-09-30 summary:Inference is typically intractable in high-treewidth undirected graphical models, making maximum likelihood learning a challenge. One way to overcome this is to restrict parameters to a tractable set, most typically the set of tree-structured parameters. This paper explores an alternative notion of a tractable set, namely a set of "fast-mixing parameters" where Markov chain Monte Carlo (MCMC) inference can be guaranteed to quickly converge to the stationary distribution. While it is common in practice to approximate the likelihood gradient using samples obtained from MCMC, such procedures lack theoretical guarantees. This paper proves that for any exponential family with bounded sufficient statistics, (not just graphical models) when parameters are constrained to a fast-mixing set, gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability. When unregularized, to find a solution epsilon-accurate in log-likelihood requires a total amount of effort cubic in 1/epsilon, disregarding logarithmic factors. When ridge-regularized, strong convexity allows a solution epsilon-accurate in parameter distance with effort quadratic in 1/epsilon. Both of these provide of a fully-polynomial time randomized approximation scheme. version:2
arxiv-1510-08986 | A Unified Theory of Confidence Regions and Testing for High Dimensional Estimating Equations | http://arxiv.org/abs/1510.08986 | id:1510.08986 author:Matey Neykov, Yang Ning, Jun S. Liu, Han Liu category:math.ST stat.ME stat.ML stat.TH  published:2015-10-30 summary:We propose a new inferential framework of constructing confidence regions and testing hypotheses for statistical models specified by a system of high dimensional estimating equations. The key ingredient of this framework is an influence function constructed by projecting the fitted estimating equations to a sparse direction obtained by solving a large-scale linear program. The main feature of our framework which makes it different from the existing ones is that the specification of the loglikelihood and other types of loss functions is not needed. The main theoretical contribution is to establish a unified Z-estimation theory of confidence regions for high dimensional problems. We further apply our general framework to a number of examples including noisy compressed sensing, undirected graphical models, discriminant analysis and vector autoregression models. We provide thorough numerical simulations to back up the developed theoretical results. version:1
arxiv-1510-08985 | Prediction-Adaptation-Correction Recurrent Neural Networks for Low-Resource Language Speech Recognition | http://arxiv.org/abs/1510.08985 | id:1510.08985 author:Yu Zhang, Ekapol Chuangsuwanich, James Glass, Dong Yu category:cs.CL cs.LG cs.NE  published:2015-10-30 summary:In this paper, we investigate the use of prediction-adaptation-correction recurrent neural networks (PAC-RNNs) for low-resource speech recognition. A PAC-RNN is comprised of a pair of neural networks in which a {\it correction} network uses auxiliary information given by a {\it prediction} network to help estimate the state probability. The information from the correction network is also used by the prediction network in a recurrent loop. Our model outperforms other state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks. Moreover, transfer learning from a language that is similar to the target language can help improve performance further. version:1
arxiv-1510-08974 | CONQUER: Confusion Queried Online Bandit Learning | http://arxiv.org/abs/1510.08974 | id:1510.08974 author:Daniel Barsky, Koby Crammer category:cs.LG stat.ML  published:2015-10-30 summary:We present a new recommendation setting for picking out two items from a given set to be highlighted to a user, based on contextual input. These two items are presented to a user who chooses one of them, possibly stochastically, with a bias that favours the item with the higher value. We propose a second-order algorithm framework that members of it use uses relative upper-confidence bounds to trade off exploration and exploitation, and some explore via sampling. We analyze one algorithm in this framework in an adversarial setting with only mild assumption on the data, and prove a regret bound of $O(Q_T + \sqrt{TQ_T\log T} + \sqrt{T}\log T)$, where $T$ is the number of rounds and $Q_T$ is the cumulative approximation error of item values using a linear model. Experiments with product reviews from 33 domains show the advantage of our methods over algorithms designed for related settings, and that UCB based algorithms are inferior to greed or sampling based algorithms. version:1
arxiv-1510-08973 | VISALOGY: Answering Visual Analogy Questions | http://arxiv.org/abs/1510.08973 | id:1510.08973 author:Fereshteh Sadeghi, C. Lawrence Zitnick, Ali Farhadi category:cs.CV  published:2015-10-30 summary:In this paper, we study the problem of answering visual analogy questions. These questions take the form of image A is to image B as image C is to what. Answering these questions entails discovering the mapping from image A to image B and then extending the mapping to image C and searching for the image D such that the relation from A to B holds for C to D. We pose this problem as learning an embedding that encourages pairs of analogous images with similar transformations to be close together using convolutional neural networks with a quadruple Siamese architecture. We introduce a dataset of visual analogy questions in natural images, and show first results of its kind on solving analogy questions on natural images. version:1
arxiv-1510-08971 | Robust Subspace Clustering via Tighter Rank Approximation | http://arxiv.org/abs/1510.08971 | id:1510.08971 author:Zhao Kang, Chong Peng, Qiang Cheng category:cs.CV cs.AI cs.LG stat.ML  published:2015-10-30 summary:Matrix rank minimization problem is in general NP-hard. The nuclear norm is used to substitute the rank function in many recent studies. Nevertheless, the nuclear norm approximation adds all singular values together and the approximation error may depend heavily on the magnitudes of singular values. This might restrict its capability in dealing with many practical problems. In this paper, an arctangent function is used as a tighter approximation to the rank function. We use it on the challenging subspace clustering problem. For this nonconvex minimization problem, we develop an effective optimization procedure based on a type of augmented Lagrange multipliers (ALM) method. Extensive experiments on face clustering and motion segmentation show that the proposed method is effective for rank approximation. version:1
arxiv-1510-08956 | Principal Differences Analysis: Interpretable Characterization of Differences between Distributions | http://arxiv.org/abs/1510.08956 | id:1510.08956 author:Jonas Mueller, Tommi Jaakkola category:stat.ML cs.LG stat.ME  published:2015-10-30 summary:We introduce principal differences analysis (PDA) for analyzing differences between high-dimensional distributions. The method operates by finding the projection that maximizes the Wasserstein divergence between the resulting univariate populations. Relying on the Cramer-Wold device, it requires no assumptions about the form of the underlying distributions, nor the nature of their inter-class differences. A sparse variant of the method is introduced to identify features responsible for the differences. We provide algorithms for both the original minimax formulation as well as its semidefinite relaxation. In addition to deriving some convergence results, we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our broader framework extends beyond the specific choice of Wasserstein divergence. version:1
arxiv-1510-08949 | Testing Visual Attention in Dynamic Environments | http://arxiv.org/abs/1510.08949 | id:1510.08949 author:Philip Bachman, David Krueger, Doina Precup category:cs.LG  published:2015-10-30 summary:We investigate attention as the active pursuit of useful information. This contrasts with attention as a mechanism for the attenuation of irrelevant information. We also consider the role of short-term memory, whose use is critical to any model incapable of simultaneously perceiving all information on which its output depends. We present several simple synthetic tasks, which become considerably more interesting when we impose strong constraints on how a model can interact with its input, and on how long it can take to produce its output. We develop a model with a different structure from those seen in previous work, and we train it using stochastic variational inference with a learned proposal distribution. version:1
arxiv-1510-05034 | Improving the Speed of Response of Learning Algorithms Using Multiple Models | http://arxiv.org/abs/1510.05034 | id:1510.05034 author:Kumpati S. Narendra, Snehasis Mukhopadyhay, Yu Wang category:cs.LG  published:2015-10-16 summary:This is the first of a series of papers that the authors propose to write on the subject of improving the speed of response of learning systems using multiple models. During the past two decades, the first author has worked on numerous methods for improving the stability, robustness, and performance of adaptive systems using multiple models and the other authors have collaborated with him on some of them. Independently, they have also worked on several learning methods, and have considerable experience with their advantages and limitations. In particular, they are well aware that it is common knowledge that machine learning is in general very slow. Numerous attempts have been made by researchers to improve the speed of convergence of algorithms in different contexts. In view of the success of multiple model based methods in improving the speed of convergence in adaptive systems, the authors believe that the same approach will also prove fruitful in the domain of learning. In this paper, a first attempt is made to use multiple models for improving the speed of response of the simplest learning schemes that have been studied. i.e. Learning Automata. version:2
arxiv-1510-08893 | A Deep Siamese Network for Scene Detection in Broadcast Videos | http://arxiv.org/abs/1510.08893 | id:1510.08893 author:Lorenzo Baraldi, Costantino Grana, Rita Cucchiara category:cs.CV cs.MM  published:2015-10-29 summary:We present a model that automatically divides broadcast videos into coherent scenes by learning a distance measure between shots. Experiments are performed to demonstrate the effectiveness of our approach by comparing our algorithm against recent proposals for automatic scene segmentation. We also propose an improved performance measure that aims to reduce the gap between numerical evaluation and expected results, and propose and release a new benchmark dataset. version:1
arxiv-1510-08865 | Mixed Robust/Average Submodular Partitioning: Fast Algorithms, Guarantees, and Applications | http://arxiv.org/abs/1510.08865 | id:1510.08865 author:Kai Wei, Rishabh Iyer, Shengjie Wang, Wenruo Bai, Jeff Bilmes category:cs.DS cs.DM cs.LG  published:2015-10-29 summary:We investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call \emph{Submodular Partitioning}. These problems generalize purely robust instances of the problem, namely \emph{max-min submodular fair allocation} (SFA) and \emph{min-max submodular load balancing} (SLB), and also average-case instances, that is the \emph{submodular welfare problem} (SWP) and \emph{submodular multiway partition} (SMP). While the robust versions have been studied in the theory community, existing work has focused on tight approximation guarantees, and the resultant algorithms are not generally scalable to large real-world applications. This is in contrast to the average case, where most of the algorithms are scalable. In the present paper, we bridge this gap, by proposing several new algorithms (including greedy, majorization-minimization, minorization-maximization, and relaxation algorithms) that not only scale to large datasets but that also achieve theoretical approximation guarantees comparable to the state-of-the-art. We moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives. We show that these problems have many applications in machine learning (ML), including data partitioning and load balancing for distributed ML, data clustering, and image segmentation. We empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization (of convex and deep neural network objectives), and also purely unsupervised image segmentation. version:1
arxiv-1510-08829 | Spiking Deep Networks with LIF Neurons | http://arxiv.org/abs/1510.08829 | id:1510.08829 author:Eric Hunsberger, Chris Eliasmith category:cs.LG cs.NE  published:2015-10-29 summary:We train spiking deep networks using leaky integrate-and-fire (LIF) neurons, and achieve state-of-the-art results for spiking networks on the CIFAR-10 and MNIST datasets. This demonstrates that biologically-plausible spiking LIF neurons can be integrated into deep networks can perform as well as other spiking models (e.g. integrate-and-fire). We achieved this result by softening the LIF response function, such that its derivative remains bounded, and by training the network with noise to provide robustness against the variability introduced by spikes. Our method is general and could be applied to other neuron types, including those used on modern neuromorphic hardware. Our work brings more biological realism into modern image classification models, with the hope that these models can inform how the brain performs this difficult task. It also provides new methods for training deep networks to run on neuromorphic hardware, with the aim of fast, power-efficient image classification for robotics applications. version:1
arxiv-1506-08272 | Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization | http://arxiv.org/abs/1506.08272 | id:1506.08272 author:Xiangru Lian, Yijun Huang, Yuncheng Li, Ji Liu category:math.OC cs.NA stat.ML  published:2015-06-27 summary:Asynchronous parallel implementations of stochastic gradient (SG) have been broadly used in solving deep neural network and received many successes in practice recently. However, existing theories cannot explain their convergence and speedup properties, mainly due to the nonconvexity of most deep learning formulations and the asynchronous parallel mechanism. To fill the gaps in theory and provide theoretical supports, this paper studies two asynchronous parallel implementations of SG: one is on the computer network and the other is on the shared memory system. We establish an ergodic convergence rate $O(1/\sqrt{K})$ for both algorithms and prove that the linear speedup is achievable if the number of workers is bounded by $\sqrt{K}$ ($K$ is the total number of iterations). Our results generalize and improve existing analysis for convex minimization. version:2
arxiv-1510-08692 | Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling | http://arxiv.org/abs/1510.08692 | id:1510.08692 author:Xiaocheng Shang, Zhanxing Zhu, Benedict Leimkuhler, Amos J. Storkey category:stat.ML cs.LG  published:2015-10-29 summary:Monte Carlo sampling for Bayesian posterior inference is a common approach used in machine learning. The Markov Chain Monte Carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations (SDEs). These SDEs are guaranteed to leave invariant the required posterior distribution. An area of current research addresses the computational benefits of stochastic gradient methods in this setting. Existing techniques rely on estimating the variance or covariance of the subsampling error, and typically assume constant variance. In this article, we propose a covariance-controlled adaptive Langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution. The proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications. version:1
arxiv-1510-01148 | Visual Tracking via Nonnegative Regularization Multiple Locality Coding | http://arxiv.org/abs/1510.01148 | id:1510.01148 author:Fanghui Liu, Tao Zhou, Irene Y. H. Gu, Jie Yang category:cs.CV  published:2015-10-05 summary:This paper presents a novel object tracking method based on approximated Locality-constrained Linear Coding (LLC). Rather than using a non-negativity constraint on encoding coefficients to guarantee these elements nonnegative, in this paper, the non-negativity constraint is substituted for a conventional $\ell_2$ norm regularization term in approximated LLC to obtain the similar nonnegative effect. And we provide a detailed and adequate explanation in theoretical analysis to clarify the rationality of this replacement. Instead of specifying fixed K nearest neighbors to construct the local dictionary, a series of different dictionaries with pre-defined numbers of nearest neighbors are selected. Weights of these various dictionaries are also learned from approximated LLC in the similar framework. In order to alleviate tracking drifts, we propose a simple and efficient occlusion detection method. The occlusion detection criterion mainly depends on whether negative templates are selected to represent the severe occluded target. Both qualitative and quantitative evaluations on several challenging sequences show that the proposed tracking algorithm achieves favorable performance compared with other state-of-the-art methods. version:3
arxiv-1510-08633 | Nonconvex Penalization in Sparse Estimation: An Approach Based on the Bernstein Function | http://arxiv.org/abs/1510.08633 | id:1510.08633 author:Zhihua Zhang category:stat.ML  published:2015-10-29 summary:In this paper we study nonconvex penalization using Bernstein functions whose first-order derivatives are completely monotone. The Bernstein function can induce a class of nonconvex penalty functions for high-dimensional sparse estimation problems. We derive a thresholding function based on the Bernstein penalty and discuss some important mathematical properties in sparsity modeling. We show that a coordinate descent algorithm is especially appropriate for regression problems penalized by the Bernstein function. We also consider the application of the Bernstein penalty in classification problems and devise a proximal alternating linearized minimization method. Based on theory of the Kurdyka-Lojasiewicz inequality, we conduct convergence analysis of these alternating iteration procedures. We particularly exemplify a family of Bernstein nonconvex penalties based on a generalized Gamma measure and conduct empirical analysis for this family. version:1
arxiv-1510-00112 | Higher-order asymptotics for the parametric complexity | http://arxiv.org/abs/1510.00112 | id:1510.00112 author:James G. Dowty category:cs.IT math.IT stat.ME stat.ML  published:2015-10-01 summary:The parametric complexity is the key quantity in the minimum description length (MDL) approach to statistical model selection. Rissanen and others have shown that the parametric complexity of a statistical model approaches a simple function of the Fisher information volume of the model as the sample size $n$ goes to infinity. This paper derives higher-order asymptotic expansions for the parametric complexity, in the case of exponential families and independent and identically distributed data. These higher-order approximations are calculated for some examples and are shown to have better finite-sample behaviour than Rissanen's approximation. The higher-order terms are given as expressions involving cumulants (or, more naturally, the Amari-Chentsov tensors), and these terms are likely to be interesting in themselves since they arise naturally from the general information-theoretic principles underpinning MDL. The derivation given here specializes to an alternative and arguably simpler proof of Rissanen's result (for the case considered here), proving for the first time that his approximation is $O(n^{-1})$. version:3
arxiv-1510-08532 | The Singular Value Decomposition, Applications and Beyond | http://arxiv.org/abs/1510.08532 | id:1510.08532 author:Zhihua Zhang category:cs.LG  published:2015-10-29 summary:The singular value decomposition (SVD) is not only a classical theory in matrix computation and analysis, but also is a powerful tool in machine learning and modern data analysis. In this tutorial we first study the basic notion of SVD and then show the central role of SVD in matrices. Using majorization theory, we consider variational principles of singular values and eigenvalues. Built on SVD and a theory of symmetric gauge functions, we discuss unitarily invariant norms, which are then used to formulate general results for matrix low rank approximation. We study the subdifferentials of unitarily invariant norms. These results would be potentially useful in many machine learning problems such as matrix completion and matrix data classification. Finally, we discuss matrix low rank approximation and its recent developments such as randomized SVD, approximate matrix multiplication, CUR decomposition, and Nystrom approximation. Randomized algorithms are important approaches to large scale SVD as well as fast matrix computations. version:1
arxiv-1510-08512 | Robust Gaussian Graphical Modeling with the Trimmed Graphical Lasso | http://arxiv.org/abs/1510.08512 | id:1510.08512 author:Eunho Yang, Aurélie C. Lozano category:stat.ML  published:2015-10-28 summary:Gaussian Graphical Models (GGMs) are popular tools for studying network structures. However, many modern applications such as gene network discovery and social interactions analysis often involve high-dimensional noisy data with outliers or heavier tails than the Gaussian distribution. In this paper, we propose the Trimmed Graphical Lasso for robust estimation of sparse GGMs. Our method guards against outliers by an implicit trimming mechanism akin to the popular Least Trimmed Squares method used for linear regression. We provide a rigorous statistical analysis of our estimator in the high-dimensional setting. In contrast, existing approaches for robust sparse GGMs estimation lack statistical guarantees. Our theoretical results are complemented by experiments on simulated and real gene expression data which further demonstrate the value of our approach. version:1
arxiv-1510-08480 | Emoticons vs. Emojis on Twitter: A Causal Inference Approach | http://arxiv.org/abs/1510.08480 | id:1510.08480 author:Umashanthi Pavalanathan, Jacob Eisenstein category:cs.CL  published:2015-10-28 summary:Online writing lacks the non-verbal cues present in face-to-face communication, which provide additional contextual information about the utterance, such as the speaker's intention or affective state. To fill this void, a number of orthographic features, such as emoticons, expressive lengthening, and non-standard punctuation, have become popular in social media services including Twitter and Instagram. Recently, emojis have been introduced to social media, and are increasingly popular. This raises the question of whether these predefined pictographic characters will come to replace earlier orthographic methods of paralinguistic communication. In this abstract, we attempt to shed light on this question, using a matching approach from causal inference to test whether the adoption of emojis causes individual users to employ fewer emoticons in their text on Twitter. version:1
arxiv-1510-08470 | Toward Long Distance, Sub-diffraction Imaging Using Coherent Camera Arrays | http://arxiv.org/abs/1510.08470 | id:1510.08470 author:Jason Holloway, M. Salman Asif, Manoj Kumar Sharma, Nathan Matsuda, Roarke Horstmeyer, Oliver Cossairt, Ashok Veeraraghavan category:cs.CV physics.optics  published:2015-10-28 summary:In this work, we propose using camera arrays coupled with coherent illumination as an effective method of improving spatial resolution in long distance images by a factor of ten and beyond. Recent advances in ptychography have demonstrated that one can image beyond the diffraction limit of the objective lens in a microscope. We demonstrate a similar imaging system to image beyond the diffraction limit in long range imaging. We emulate a camera array with a single camera attached to an X-Y translation stage. We show that an appropriate phase retrieval based reconstruction algorithm can be used to effectively recover the lost high resolution details from the multiple low resolution acquired images. We analyze the effects of noise, required degree of image overlap, and the effect of increasing synthetic aperture size on the reconstructed image quality. We show that coherent camera arrays have the potential to greatly improve imaging performance. Our simulations show resolution gains of 10x and more are achievable. Furthermore, experimental results from our proof-of-concept systems show resolution gains of 4x-7x for real scenes. Finally, we introduce and analyze in simulation a new strategy to capture macroscopic Fourier Ptychography images in a single snapshot, albeit using a camera array. version:1
arxiv-1510-08440 | Priors on exchangeable directed graphs | http://arxiv.org/abs/1510.08440 | id:1510.08440 author:Diana Cai, Nathanael Ackerman, Cameron Freer category:math.ST stat.ME stat.ML stat.TH  published:2015-10-28 summary:Directed graphs occur throughout statistical modeling of networks, and exchangeability is a natural assumption when the ordering of vertices does not matter. There is a deep structural theory for exchangeable undirected graphs, which extends to the directed case, but with additional complexities that arise from the need to consider the joint distribution over both edge directions on a pair of vertices. Exchangeable directed graphs are characterized by a sampling procedure given by the Aldous-Hoover theorem, which can be described in terms of a distribution on measurable objects known as digraphons. Most existing work on exchangeable graph models has focused on undirected graphs, and little attention has been placed on priors for exchangeable directed graphs. Currently, many directed network models generalize the undirected case by treating each edge direction as independent, rather than considering both edge directions jointly. By placing priors on digraphons one can capture dependence in the edge directions in exchangeable directed graphs, which we demonstrate is not captured by models that consider the edge directions independently. We construct priors on exchangeable directed graphs using digraphons, including special cases such as tournaments, linear orderings, directed acyclic graphs, and partial orderings. We also present a Bayesian nonparametric block model for exchangeable directed graphs and demonstrate inference for these models on synthetic data. version:1
arxiv-1510-08418 | Fast k-best Sentence Compression | http://arxiv.org/abs/1510.08418 | id:1510.08418 author:Katja Filippova, Enrique Alfonseca category:cs.CL  published:2015-10-28 summary:A popular approach to sentence compression is to formulate the task as a constrained optimization problem and solve it with integer linear programming (ILP) tools. Unfortunately, dependence on ILP may make the compressor prohibitively slow, and thus approximation techniques have been proposed which are often complex and offer a moderate gain in speed. As an alternative solution, we introduce a novel compression algorithm which generates k-best compressions relying on local deletion decisions. Our algorithm is two orders of magnitude faster than a recent ILP-based method while producing better compressions. Moreover, an extensive evaluation demonstrates that the quality of compressions does not degrade much as we move from single best to top-five results. version:1
arxiv-1506-01900 | Communication Complexity of Distributed Convex Learning and Optimization | http://arxiv.org/abs/1506.01900 | id:1506.01900 author:Yossi Arjevani, Ohad Shamir category:cs.LG math.OC stat.ML  published:2015-06-05 summary:We study the fundamental limits to communication-efficient distributed methods for convex learning and optimization, under different assumptions on the information available to individual machines, and the types of functions considered. We identify cases where existing algorithms are already worst-case optimal, as well as cases where room for further improvement is still possible. Among other things, our results indicate that without similarity between the local objective functions (due to statistical data similarity or otherwise) many communication rounds may be required, even if the machines have unbounded computational power. version:2
arxiv-1510-08406 | Fast Landmark Subspace Clustering | http://arxiv.org/abs/1510.08406 | id:1510.08406 author:Xu Wang, Gilad Lerman category:stat.ML  published:2015-10-28 summary:Kernel methods obtain superb performance in terms of accuracy for various machine learning tasks since they can effectively extract nonlinear relations. However, their time complexity can be rather large especially for clustering tasks. In this paper we define a general class of kernels that can be easily approximated by randomization. These kernels appear in various applications, in particular, traditional spectral clustering, landmark-based spectral clustering and landmark-based subspace clustering. We show that for $n$ data points from $K$ clusters with $D$ landmarks, the randomization procedure results in an algorithm of complexity $O(KnD)$. Furthermore, we bound the error between the original clustering scheme and its randomization. To illustrate the power of this framework, we propose a new fast landmark subspace (FLS) clustering algorithm. Experiments over synthetic and real datasets demonstrate the superior performance of FLS in accelerating subspace clustering with marginal sacrifice of accuracy. version:1
arxiv-1502-00319 | Efficient refinement of GPS-based localization in urban areas using visual information and sensor parameter | http://arxiv.org/abs/1502.00319 | id:1502.00319 author:Mahdi Salarian, Rashid Ansari category:cs.CV cs.IR  published:2015-02-01 summary:An efficient method is proposed for refining GPS-acquired location coordinates in urban areas using camera images, Google Street View (GSV) and sensor parameters. The main goal is to compensate for GPS location imprecision in dense area of cities due to proximity to walls and buildings. Avail-able methods for better localization often use visual information by using query images acquired with camera-equipped mobile devices and applying image retrieval techniques to find the closest match in a GPS-referenced image data set. The search areas required for reliable search are about 1-2 sq. Km and the accuracy is typically 25-100 meters. Here we describe a method based on image retrieval where a reliable search can be confined to areas of 0.01 sq. Km and the accuracy in our experiments is less than 10 meters. To test our procedure we created a database by acquiring all Google Street View images close to what is seen by a pedestrian in a large region of downtown Chicago and saved all coordinates and orientation data to be used for confining our search region. Prior knowledge from approximate position of query image is leveraged to address complexity and accuracy issues of our search in a large scale geo-tagged data set. One key aspect that differentiates our work is that it utilizes the sensor information of GPS SOS and the camera orientation in improving localization. Finally we demonstrate retrieval-based technique are less accurate in sparse open areas compared with purely GPS measurement. The effectiveness of our approach is discussed in detail and experimental results show improved performance when compared with regular approaches. version:3
arxiv-1510-08389 | Universal Dependency Analysis | http://arxiv.org/abs/1510.08389 | id:1510.08389 author:Hoang-Vu Nguyen, Jilles Vreeken category:stat.ML cs.LG  published:2015-10-28 summary:Most data is multi-dimensional. Discovering whether any subset of dimensions, or subspaces, of such data is significantly correlated is a core task in data mining. To do so, we require a measure that quantifies how correlated a subspace is. For practical use, such a measure should be universal in the sense that it captures correlation in subspaces of any dimensionality and allows to meaningfully compare correlation scores across different subspaces, regardless how many dimensions they have and what specific statistical properties their dimensions possess. Further, it would be nice if the measure can non-parametrically and efficiently capture both linear and non-linear correlations. In this paper, we propose UDS, a multivariate correlation measure that fulfills all of these desiderata. In short, we define \uds based on cumulative entropy and propose a principled normalization scheme to bring its scores across different subspaces to the same domain, enabling universal correlation assessment. UDS is purely non-parametric as we make no assumption on data distributions nor types of correlation. To compute it on empirical data, we introduce an efficient and non-parametric method. Extensive experiments show that UDS outperforms state of the art. version:1
arxiv-1510-08385 | Linear-time Detection of Non-linear Changes in Massively High Dimensional Time Series | http://arxiv.org/abs/1510.08385 | id:1510.08385 author:Hoang-Vu Nguyen, Jilles Vreeken category:stat.ML cs.LG  published:2015-10-28 summary:Change detection in multivariate time series has applications in many domains, including health care and network monitoring. A common approach to detect changes is to compare the divergence between the distributions of a reference window and a test window. When the number of dimensions is very large, however, the naive approach has both quality and efficiency issues: to ensure robustness the window size needs to be large, which not only leads to missed alarms but also increases runtime. To this end, we propose LIGHT, a linear-time algorithm for robustly detecting non-linear changes in massively high dimensional time series. Importantly, LIGHT provides high flexibility in choosing the window size, allowing the domain expert to fit the level of details required. To do such, we 1) perform scalable PCA to reduce dimensionality, 2) perform scalable factorization of the joint distribution, and 3) scalably compute divergences between these lower dimensional distributions. Extensive empirical evaluation on both synthetic and real-world data show that LIGHT outperforms state of the art with up to 100% improvement in both quality and efficiency. version:1
arxiv-1510-08382 | Flexibly Mining Better Subgroups | http://arxiv.org/abs/1510.08382 | id:1510.08382 author:Hoang-Vu Nguyen, Jilles Vreeken category:stat.ML cs.LG  published:2015-10-28 summary:In subgroup discovery, also known as supervised pattern mining, discovering high quality one-dimensional subgroups and refinements of these is a crucial task. For nominal attributes, this is relatively straightforward, as we can consider individual attribute values as binary features. For numerical attributes, the task is more challenging as individual numeric values are not reliable statistics. Instead, we can consider combinations of adjacent values, i.e. bins. Existing binning strategies, however, are not tailored for subgroup discovery. That is, they do not directly optimize for the quality of subgroups, therewith potentially degrading the mining result. To address this issue, we propose FLEXI. In short, with FLEXI we propose to use optimal binning to find high quality binary features for both numeric and ordinal attributes. We instantiate FLEXI with various quality measures and show how to achieve efficiency accordingly. Experiments on both synthetic and real-world data sets show that FLEXI outperforms state of the art with up to 25 times improvement in subgroup quality. version:1
arxiv-1510-08370 | Canonical Divergence Analysis | http://arxiv.org/abs/1510.08370 | id:1510.08370 author:Hoang-Vu Nguyen, Jilles Vreeken category:stat.ML cs.LG  published:2015-10-28 summary:We aim to analyze the relation between two random vectors that may potentially have both different number of attributes as well as realizations, and which may even not have a joint distribution. This problem arises in many practical domains, including biology and architecture. Existing techniques assume the vectors to have the same domain or to be jointly distributed, and hence are not applicable. To address this, we propose Canonical Divergence Analysis (CDA). We introduce three instantiations, each of which permits practical implementation. Extensive empirical evaluation shows the potential of our method. version:1
arxiv-1506-04209 | A Flexible and Efficient Algorithmic Framework for Constrained Matrix and Tensor Factorization | http://arxiv.org/abs/1506.04209 | id:1506.04209 author:Kejun Huang, Nicholas D. Sidiropoulos, Athanasios P. Liavas category:stat.ML cs.LG math.OC stat.CO  published:2015-06-13 summary:We propose a general algorithmic framework for constrained matrix and tensor factorization, which is widely used in signal processing and machine learning. The new framework is a hybrid between alternating optimization (AO) and the alternating direction method of multipliers (ADMM): each matrix factor is updated in turn, using ADMM, hence the name AO-ADMM. This combination can naturally accommodate a great variety of constraints on the factor matrices, and almost all possible loss measures for the fitting. Computation caching and warm start strategies are used to ensure that each update is evaluated efficiently, while the outer AO framework exploits recent developments in block coordinate descent (BCD)-type methods which help ensure that every limit point is a stationary point, as well as faster and more robust convergence in practice. Three special cases are studied in detail: non-negative matrix/tensor factorization, constrained matrix/tensor completion, and dictionary learning. Extensive simulations and experiments with real data are used to showcase the effectiveness and broad applicability of the proposed framework. version:2
arxiv-1506-00898 | Extreme Compressive Sampling for Covariance Estimation | http://arxiv.org/abs/1506.00898 | id:1506.00898 author:Martin Azizyan, Akshay Krishnamurthy, Aarti Singh category:stat.ML cs.IT math.IT  published:2015-06-02 summary:This paper studies the problem of estimating the covariance of a collection of vectors using only extremely compressed measurements of each vector. An estimator based on back-projections of these compressive samples is proposed and analyzed. A distribution-free analysis shows that by observing just a single compressive measurement of each vector, one can consistently estimate the covariance matrix, in both infinity and spectral norm, and this same analysis leads to precise rates of convergence in both norms. Via information-theoretic techniques, lower bounds showing that this estimator is minimax-optimal for both infinity and spectral norm estimation problems are established. These results are also specialized to give matching upper and lower bounds for estimating the population covariance of a collection of Gaussian vectors, again in the compressive measurement model. The analysis conducted in this paper shows that the effective sample complexity for this problem is scaled by a factor of $m^2/d^2$ where $m$ is the compression dimension and $d$ is the ambient dimension. Applications to subspace learning (Principal Components Analysis) and learning over distributed sensor networks are also discussed. version:2
arxiv-1501-03844 | Evaluating accuracy of community detection using the relative normalized mutual information | http://arxiv.org/abs/1501.03844 | id:1501.03844 author:Pan Zhang category:physics.soc-ph cond-mat.stat-mech cs.SI stat.ML  published:2015-01-15 summary:The Normalized Mutual Information (NMI) has been widely used to evaluate the accuracy of community detection algorithms. However in this article we show that the NMI is seriously affected by systematic errors due to finite size of networks, and may give a wrong estimate of performance of algorithms in some cases. We give a simple theory to the finite-size effect of NMI and test our theory numerically. Then we propose a new metric for the accuracy of community detection, namely the relative Normalized Mutual Information (rNMI), which considers statistical significance of the NMI by comparing it with the expected NMI of random partitions. Our numerical experiments show that the rNMI overcomes the finite-size effect of the NMI. version:2
arxiv-1510-05711 | Qualitative Projection Using Deep Neural Networks | http://arxiv.org/abs/1510.05711 | id:1510.05711 author:Andrew J. R. Simpson category:cs.NE cs.LG 68Txx  published:2015-10-19 summary:Deep neural networks (DNN) abstract by demodulating the output of linear filters. In this article, we refine this definition of abstraction to show that the inputs of a DNN are abstracted with respect to the filters. Or, to restate, the abstraction is qualified by the filters. This leads us to introduce the notion of qualitative projection. We use qualitative projection to abstract MNIST hand-written digits with respect to the various dogs, horses, planes and cars of the CIFAR dataset. We then classify the MNIST digits according to the magnitude of their dogness, horseness, planeness and carness qualities, illustrating the generality of qualitative projection. version:2
arxiv-1409-8437 | Fully adaptive density-based clustering | http://arxiv.org/abs/1409.8437 | id:1409.8437 author:Ingo Steinwart category:stat.ME stat.ML  published:2014-09-30 summary:The clusters of a distribution are often defined by the connected components of a density level set. However, this definition depends on the user-specified level. We address this issue by proposing a simple, generic algorithm, which uses an almost arbitrary level set estimator to estimate the smallest level at which there are more than one connected components. In the case where this algorithm is fed with histogram-based level set estimates, we provide a finite sample analysis, which is then used to show that the algorithm consistently estimates both the smallest level and the corresponding connected components. We further establish rates of convergence for the two estimation problems, and last but not least, we present a simple, yet adaptive strategy for determining the width-parameter of the involved density estimator in a data-depending way. version:4
arxiv-1507-02030 | Beyond Convexity: Stochastic Quasi-Convex Optimization | http://arxiv.org/abs/1507.02030 | id:1507.02030 author:Elad Hazan, Kfir Y. Levy, Shai Shalev-Shwartz category:cs.LG math.OC  published:2015-07-08 summary:Stochastic convex optimization is a basic and well studied primitive in machine learning. It is well known that convex and Lipschitz functions can be minimized efficiently using Stochastic Gradient Descent (SGD). The Normalized Gradient Descent (NGD) algorithm, is an adaptation of Gradient Descent, which updates according to the direction of the gradients, rather than the gradients themselves. In this paper we analyze a stochastic version of NGD and prove its convergence to a global minimum for a wider class of functions: we require the functions to be quasi-convex and locally-Lipschitz. Quasi-convexity broadens the con- cept of unimodality to multidimensions and allows for certain types of saddle points, which are a known hurdle for first-order optimization methods such as gradient descent. Locally-Lipschitz functions are only required to be Lipschitz in a small region around the optimum. This assumption circumvents gradient explosion, which is another known hurdle for gradient descent variants. Interestingly, unlike the vanilla SGD algorithm, the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size. version:3
arxiv-1503-09082 | Generalized Categorization Axioms | http://arxiv.org/abs/1503.09082 | id:1503.09082 author:Jian Yu category:cs.LG  published:2015-03-31 summary:Categorization axioms have been proposed to axiomatizing clustering results, which offers a hint of bridging the difference between human recognition system and machine learning through an intuitive observation: an object should be assigned to its most similar category. However, categorization axioms cannot be generalized into a general machine learning system as categorization axioms become trivial when the number of categories becomes one. In order to generalize categorization axioms into general cases, categorization input and categorization output are reinterpreted by inner and outer category representation. According to the categorization reinterpretation, two category representation axioms are presented. Category representation axioms and categorization axioms can be combined into a generalized categorization axiomatic framework, which accurately delimit the theoretical categorization constraints and overcome the shortcoming of categorization axioms. The proposed axiomatic framework not only discuses categorization test issue but also reinterprets many results in machine learning in a unified way, such as dimensionality reduction,density estimation, regression, clustering and classification. version:11
arxiv-1510-06002 | Fast and Scalable Structural SVM with Slack Rescaling | http://arxiv.org/abs/1510.06002 | id:1510.06002 author:Heejin Choi, Ofer Meshi, Nathan Srebro category:cs.LG  published:2015-10-20 summary:We present an efficient method for training slack-rescaled structural SVM. Although finding the most violating label in a margin-rescaled formulation is often easy since the target function decomposes with respect to the structure, this is not the case for a slack-rescaled formulation, and finding the most violated label might be very difficult. Our core contribution is an efficient method for finding the most-violating-label in a slack-rescaled formulation, given an oracle that returns the most-violating-label in a (slightly modified) margin-rescaled formulation. We show that our method enables accurate and scalable training for slack-rescaled SVMs, reducing runtime by an order of magnitude compared to previous approaches to slack-rescaled SVMs. version:2
arxiv-1510-08110 | Spectral Convergence Rate of Graph Laplacian | http://arxiv.org/abs/1510.08110 | id:1510.08110 author:Xu Wang category:stat.ML  published:2015-10-27 summary:Laplacian Eigenvectors of the graph constructed from a data set are used in many spectral manifold learning algorithms such as diffusion maps and spectral clustering. Given a graph constructed from a random sample of a $d$-dimensional compact submanifold $M$ in $\mathbb{R}^D$, we establish the spectral convergence rate of the graph Laplacian. It implies the consistency of the spectral clustering algorithm via a standard perturbation argument. A simple numerical study indicates the necessity of a denoising step before applying spectral algorithms. version:1
arxiv-1510-08108 | Online Learning with Gaussian Payoffs and Side Observations | http://arxiv.org/abs/1510.08108 | id:1510.08108 author:Yifan Wu, András György, Csaba Szepesvári category:stat.ML cs.LG  published:2015-10-27 summary:We consider a sequential learning problem with Gaussian payoffs and side information: after selecting an action $i$, the learner receives information about the payoff of every action $j$ in the form of Gaussian observations whose mean is the same as the mean payoff, but the variance depends on the pair $(i,j)$ (and may be infinite). The setup allows a more refined information transfer from one action to another than previous partial monitoring setups, including the recently introduced graph-structured feedback case. For the first time in the literature, we provide non-asymptotic problem-dependent lower bounds on the regret of any algorithm, which recover existing asymptotic problem-dependent lower bounds and finite-time minimax lower bounds available in the literature. We also provide algorithms that achieve the problem-dependent lower bound (up to some universal constant factor) or the minimax lower bounds (up to logarithmic factors). version:1
arxiv-1503-01243 | A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights | http://arxiv.org/abs/1503.01243 | id:1503.01243 author:Weijie Su, Stephen Boyd, Emmanuel J. Candes category:stat.ML math.CA math.OC  published:2015-03-04 summary:We derive a second-order ordinary differential equation (ODE) which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex. version:2
arxiv-1510-08039 | Hybrid One-Shot 3D Hand Pose Estimation by Exploiting Uncertainties | http://arxiv.org/abs/1510.08039 | id:1510.08039 author:Georg Poier, Konstantinos Roditakis, Samuel Schulter, Damien Michel, Horst Bischof, Antonis A. Argyros category:cs.CV  published:2015-10-27 summary:Model-based approaches to 3D hand tracking have been shown to perform well in a wide range of scenarios. However, they require initialisation and cannot recover easily from tracking failures that occur due to fast hand motions. Data-driven approaches, on the other hand, can quickly deliver a solution, but the results often suffer from lower accuracy or missing anatomical validity compared to those obtained from model-based approaches. In this work we propose a hybrid approach for hand pose estimation from a single depth image. First, a learned regressor is employed to deliver multiple initial hypotheses for the 3D position of each hand joint. Subsequently, the kinematic parameters of a 3D hand model are found by deliberately exploiting the inherent uncertainty of the inferred joint proposals. This way, the method provides anatomically valid and accurate solutions without requiring manual initialisation or suffering from track losses. Quantitative results on several standard datasets demonstrate that the proposed method outperforms state-of-the-art representatives of the model-based, data-driven and hybrid paradigms. version:1
arxiv-1510-08012 | ENFT: Efficient Non-Consecutive Feature Tracking for Robust Structure-from-Motion | http://arxiv.org/abs/1510.08012 | id:1510.08012 author:Guofeng Zhang, Haomin Liu, Zilong Dong, Jiaya Jia, Tien-Tsin Wong, Hujun Bao category:cs.CV  published:2015-10-27 summary:Structure-from-motion (SfM) largely relies on the quality of feature tracking. In image sequences, if disjointed tracks caused by objects moving in and out of the view, occasional occlusion, or image noise, are not handled well, the corresponding SfM could be significantly affected. This problem becomes more serious for accurate SfM of large-scale scenes, which typically requires to capture multiple sequences to cover the whole scene. In this paper, we propose an efficient non-consecutive feature tracking (ENFT) framework to match the interrupted tracks distributed in different subsequences or even in different videos. Our framework consists of steps of solving the feature `dropout' problem when indistinctive structures, noise or even large image distortion exist, and of rapidly recognizing and joining common features located in different subsequences. In addition, we contribute an effective segment-based coarse-to-fine SfM estimation algorithm for efficiently and robustly handling large datasets. Experimental results on several challenging and large video datasets demonstrate the effectiveness of the proposed system. version:1
arxiv-1309-7367 | Stochastic Online Shortest Path Routing: The Value of Feedback | http://arxiv.org/abs/1309.7367 | id:1309.7367 author:M. Sadegh Talebi, Zhenhua Zou, Richard Combes, Alexandre Proutiere, Mikael Johansson category:cs.NI cs.LG math.OC  published:2013-09-27 summary:This paper studies online shortest path routing over multi-hop networks. Link costs or delays are time-varying and modeled by independent and identically distributed random processes, whose parameters are initially unknown. The parameters, and hence the optimal path, can only be estimated by routing packets through the network and observing the realized delays. Our aim is to find a routing policy that minimizes the regret (the cumulative difference of expected delay) between the path chosen by the policy and the unknown optimal path. We formulate the problem as a combinatorial bandit optimization problem and consider several scenarios that differ in where routing decisions are made and in the information available when making the decisions. For each scenario, we derive a tight asymptotic lower bound on the regret that has to be satisfied by any online routing policy. These bounds help us to understand the performance improvements we can expect when (i) taking routing decisions at each hop rather than at the source only, and (ii) observing per-link delays rather than end-to-end path delays. In particular, we show that (i) is of no use while (ii) can have a spectacular impact. Three algorithms, with a trade-off between computational complexity and performance, are proposed. The regret upper bounds of these algorithms improve over those of the existing algorithms, and they significantly outperform state-of-the-art algorithms in numerical experiments. version:3
arxiv-1510-07957 | Increasing Behavioral Complexity for Evolved Virtual Creatures with the ESP Method | http://arxiv.org/abs/1510.07957 | id:1510.07957 author:Dan Lessin, Don Fussell, Risto Miikkulainen, Sebastian Risi category:cs.NE  published:2015-10-27 summary:Since their introduction in 1994 (Sims), evolved virtual creatures (EVCs) have employed the coevolution of morphology and control to produce high-impact work in multiple fields, including graphics, evolutionary computation, robotics, and artificial life. However, in contrast to fixed-morphology creatures, there has been no clear increase in the behavioral complexity of EVCs in those two decades. This paper describes a method for moving beyond this limit, making use of high-level human input in the form of a syllabus of intermediate learning tasks--along with mechanisms for preservation, reuse, and combination of previously learned tasks. This method--named ESP for its three components: encapsulation, syllabus, and pandemonium--is presented in two complementary versions: Fast ESP, which constrains later morphological changes to achieve linear growth in computation time as behavioral complexity is added, and General ESP, which allows this restriction to be removed when sufficient computational resources are available. Experiments demonstrate that the ESP method allows evolved virtual creatures to reach new levels of behavioral complexity in the co-evolution of morphology and control, approximately doubling the previous state of the art. version:1
arxiv-1510-07925 | Exclusive Sparsity Norm Minimization with Random Groups via Cone Projection | http://arxiv.org/abs/1510.07925 | id:1510.07925 author:Yijun Huang, Ji Liu category:stat.ML cs.LG  published:2015-10-27 summary:Many practical applications such as gene expression analysis, multi-task learning, image recognition, signal processing, and medical data analysis pursue a sparse solution for the feature selection purpose and particularly favor the nonzeros \emph{evenly} distributed in different groups. The exclusive sparsity norm has been widely used to serve to this purpose. However, it still lacks systematical studies for exclusive sparsity norm optimization. This paper offers two main contributions from the optimization perspective: 1) We provide several efficient algorithms to solve exclusive sparsity norm minimization with either smooth loss or hinge loss (non-smooth loss). All algorithms achieve the optimal convergence rate $O(1/k^2)$ ($k$ is the iteration number). To the best of our knowledge, this is the first time to guarantee such convergence rate for the general exclusive sparsity norm minimization; 2) When the group information is unavailable to define the exclusive sparsity norm, we propose to use the random grouping scheme to construct groups and prove that if the number of groups is appropriately chosen, the nonzeros (true features) would be grouped in the ideal way with high probability. Empirical studies validate the efficiency of proposed algorithms, and the effectiveness of random grouping scheme on the proposed exclusive SVM formulation. version:1
arxiv-1506-08052 | Automagically encoding Adverse Drug Reactions in MedDRA | http://arxiv.org/abs/1506.08052 | id:1506.08052 author:Carlo Combi, Riccardo Lora, Ugo Moretti, Marco Pagliarini, Margherita Zorzi category:cs.CL  published:2015-06-26 summary:Pharmacovigilance is the field of science devoted to the collection, analysis and prevention of Adverse Drug Reactions (ADRs). Efficient strategies for the extraction of information about ADRs from free text resources are essential to support the work of experts, employed in the crucial task of detecting and classifying unexpected pathologies possibly related to drug assumptions. Narrative ADR descriptions may be collected in several way, e.g. by monitoring social networks or through the so called spontaneous reporting, the main method pharmacovigilance adopts in order to identify ADRs. The encoding of free-text ADR descriptions according to MedDRA standard terminology is central for report analysis. It is a complex work, which has to be manually implemented by the pharmacovigilance experts. The manual encoding is expensive (in terms of time). Moreover, a problem about the accuracy of the encoding may occur, since the number of reports is growing up day by day. In this paper, we propose MagiCoder, an efficient Natural Language Processing algorithm able to automatically derive MedDRA terminologies from free-text ADR descriptions. MagiCoder is part of VigiWork, a web application for online ADR reporting and analysis. From a practical view-point, MagiCoder radically reduces the revision time of ADR reports: the pharmacologist has simply to revise and validate the automatic solution versus the hard task of choosing solutions in the 70k terms of MedDRA. This improvement of the expert work efficiency has a meaningful impact on the quality of data analysis. Moreover, our procedure is general purpose. We developed MagiCoder for the Italian pharmacovigilance language, but preliminarily analyses show that it is robust to language and dictionary changes. version:2
arxiv-1510-07851 | Standards for language resources in ISO -- Looking back at 13 fruitful years | http://arxiv.org/abs/1510.07851 | id:1510.07851 author:Laurent Romary category:cs.CL  published:2015-10-27 summary:This paper provides an overview of the various projects carried out within ISO committee TC 37/SC 4 dealing with the management of language (digital) resources. On the basis of the technical experience gained in the committee and the wider standardization landscape the paper identifies some possible trends for the future. version:1
arxiv-1303-3128 | Estimation Stability with Cross Validation (ESCV) | http://arxiv.org/abs/1303.3128 | id:1303.3128 author:Chinghway Lim, Bin Yu category:stat.ME stat.ML  published:2013-03-13 summary:Cross-validation (CV) is often used to select the regularization parameter in high dimensional problems. However, when applied to the sparse modeling method Lasso, CV leads to models that are unstable in high-dimensions, and consequently not suited for reliable interpretation. In this paper, we propose a model-free criterion ESCV based on a new estimation stability (ES) metric and CV. Our proposed ESCV finds a locally ES-optimal model smaller than the CV choice so that the it fits the data and also enjoys estimation stability property. We demonstrate that ESCV is an effective alternative to CV at a similar easily parallelizable computational cost. In particular, we compare the two approaches with respect to several performance measures when applied to the Lasso on both simulated and real data sets. For dependent predictors common in practice, our main finding is that, ESCV cuts down false positive rates often by a large margin, while sacrificing little of true positive rates. ESCV usually outperforms CV in terms of parameter estimation while giving similar performance as CV in terms of prediction. For the two real data sets from neuroscience and cell biology, the models found by ESCV are less than half of the model sizes by CV. Judged based on subject knowledge, they are more plausible than those by CV as well. We also discuss some regularization parameter alignment issues that come up in both approaches. version:2
arxiv-1502-01094 | Multimodal Task-Driven Dictionary Learning for Image Classification | http://arxiv.org/abs/1502.01094 | id:1502.01094 author:Soheil Bahrampour, Nasser M. Nasrabadi, Asok Ray, W. Kenneth Jenkins category:stat.ML cs.CV cs.LG  published:2015-02-04 summary:Dictionary learning algorithms have been successfully used for both reconstructive and discriminative tasks, where an input signal is represented with a sparse linear combination of dictionary atoms. While these methods are mostly developed for single-modality scenarios, recent studies have demonstrated the advantages of feature-level fusion based on the joint sparse representation of the multimodal inputs. In this paper, we propose a multimodal task-driven dictionary learning algorithm under the joint sparsity constraint (prior) to enforce collaborations among multiple homogeneous/heterogeneous sources of information. In this task-driven formulation, the multimodal dictionaries are learned simultaneously with their corresponding classifiers. The resulting multimodal dictionaries can generate discriminative latent features (sparse codes) from the data that are optimized for a given task such as binary or multiclass classification. Moreover, we present an extension of the proposed formulation using a mixed joint and independent sparsity prior which facilitates more flexible fusion of the modalities at feature level. The efficacy of the proposed algorithms for multimodal classification is illustrated on four different applications -- multimodal face recognition, multi-view face recognition, multi-view action recognition, and multimodal biometric recognition. It is also shown that, compared to the counterpart reconstructive-based dictionary learning algorithms, the task-driven formulations are more computationally efficient in the sense that they can be equipped with more compact dictionaries and still achieve superior performance. version:2
arxiv-1502-03436 | An exploration of parameter redundancy in deep networks with circulant projections | http://arxiv.org/abs/1502.03436 | id:1502.03436 author:Yu Cheng, Felix X. Yu, Rogerio S. Feris, Sanjiv Kumar, Alok Choudhary, Shih-Fu Chang category:cs.CV  published:2015-02-11 summary:We explore the redundancy of parameters in deep neural networks by replacing the conventional linear projection in fully-connected layers with the circulant projection. The circulant structure substantially reduces memory footprint and enables the use of the Fast Fourier Transform to speed up the computation. Considering a fully-connected neural network layer with d input nodes, and d output nodes, this method improves the time complexity from O(d^2) to O(dlogd) and space complexity from O(d^2) to O(d). The space savings are particularly important for modern deep convolutional neural network architectures, where fully-connected layers typically contain more than 90% of the network parameters. We further show that the gradient computation and optimization of the circulant projections can be performed very efficiently. Our experiments on three standard datasets show that the proposed approach achieves this significant gain in storage and efficiency with minimal increase in error rate compared to neural networks with unstructured projections. version:2
arxiv-1503-02115 | Community Detection and Classification in Hierarchical Stochastic Blockmodels | http://arxiv.org/abs/1503.02115 | id:1503.02115 author:Vince Lyzinski, Minh Tang, Avanti Athreya, Youngser Park, Carey E. Priebe category:stat.ML stat.AP  published:2015-03-07 summary:We propose a robust, scalable, integrated methodology for community detection and community comparison in graphs. In our procedure, we first embed a graph into an appropriate Euclidean space to obtain a low-dimensional representation, and then cluster the vertices into communities. We next employ nonparametric graph inference techniques to identify structural similarity among these communities. These two steps are then applied recursively on the communities, allowing us to detect more fine-grained structure. We describe a hierarchical stochastic blockmodel---namely, a stochastic blockmodel with a natural hierarchical structure---and establish conditions under which our algorithm yields consistent estimates of model parameters and motifs, which we define to be stochastically similar groups of subgraphs. Finally, we demonstrate the effectiveness of our algorithm in both simulated and real data. Specifically, we address the problem of locating similar subcommunities in a partially reconstructed Drosophila connectome and in the social network Friendster. version:3
arxiv-1510-07748 | Computational models: Bottom-up and top-down aspects | http://arxiv.org/abs/1510.07748 | id:1510.07748 author:Laurent Itti, Ali Borji category:cs.CV  published:2015-10-27 summary:Computational models of visual attention have become popular over the past decade, we believe primarily for two reasons: First, models make testable predictions that can be explored by experimentalists as well as theoreticians, second, models have practical and technological applications of interest to the applied science and engineering communities. In this chapter, we take a critical look at recent attention modeling efforts. We focus on {\em computational models of attention} as defined by Tsotsos \& Rothenstein \shortcite{Tsotsos_Rothenstein11}: Models which can process any visual stimulus (typically, an image or video clip), which can possibly also be given some task definition, and which make predictions that can be compared to human or animal behavioral or physiological responses elicited by the same stimulus and task. Thus, we here place less emphasis on abstract models, phenomenological models, purely data-driven fitting or extrapolation models, or models specifically designed for a single task or for a restricted class of stimuli. For theoretical models, we refer the reader to a number of previous reviews that address attention theories and models more generally \cite{Itti_Koch01nrn,Paletta_etal05,Frintrop_etal10,Rothenstein_Tsotsos08,Gottlieb_Balan10,Toet11,Borji_Itti12pami}. version:1
arxiv-1503-05479 | Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm | http://arxiv.org/abs/1503.05479 | id:1503.05479 author:Qinqing Zheng, Ryota Tomioka category:cs.LG cs.AI stat.ML  published:2015-03-18 summary:We consider the problem of recovering a low-rank tensor from its noisy observation. Previous work has shown a recovery guarantee with signal to noise ratio $O(n^{\lceil K/2 \rceil /2})$ for recovering a $K$th order rank one tensor of size $n\times \cdots \times n$ by recursive unfolding. In this paper, we first improve this bound to $O(n^{K/4})$ by a much simpler approach, but with a more careful analysis. Then we propose a new norm called the subspace norm, which is based on the Kronecker products of factors obtained by the proposed simple estimator. The imposed Kronecker structure allows us to show a nearly ideal $O(\sqrt{n}+\sqrt{H^{K-1}})$ bound, in which the parameter $H$ controls the blend from the non-convex estimator to mode-wise nuclear norm minimization. Furthermore, we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with $H=O(1)$. version:2
arxiv-1510-07641 | Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks | http://arxiv.org/abs/1510.07641 | id:1510.07641 author:Zachary C. Lipton, David C. Kale, Randall C. Wetzell category:cs.LG  published:2015-10-26 summary:We present a novel application of LSTM recurrent neural networks to multilabel classification of diagnoses given variable-length time series of clinical measurements. Our method outperforms a strong baseline on a variety of metrics. version:1
arxiv-1510-07609 | Efficient Learning by Directed Acyclic Graph For Resource Constrained Prediction | http://arxiv.org/abs/1510.07609 | id:1510.07609 author:Joseph Wang, Kirill Trapeznikov, Venkatesh Saligrama category:stat.ML cs.LG  published:2015-10-26 summary:We study the problem of reducing test-time acquisition costs in classification systems. Our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction. We model our system as a directed acyclic graph (DAG) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements. This problem can be naturally posed as an empirical risk minimization over training data. Rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes, we propose an efficient algorithm motivated by dynamic programming. We learn node policies in the DAG by reducing the global objective to a series of cost sensitive learning problems. Our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture. In addition, we present an extension to map other budgeted learning problems with large number of sensors to our DAG architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors. version:1
arxiv-1510-07586 | Parser for Abstract Meaning Representation using Learning to Search | http://arxiv.org/abs/1510.07586 | id:1510.07586 author:Sudha Rao, Yogarshi Vyas, Hal Daume III, Philip Resnik category:cs.CL  published:2015-10-26 summary:We develop a novel technique to parse English sentences into Abstract Meaning Representation (AMR) using SEARN, a Learning to Search approach, by modeling the concept and the relation learning in a unified framework. We evaluate our parser on multiple datasets from varied domains and show an absolute improvement of 2% to 6% over the state-of-the-art. Additionally we show that using the most frequent concept gives us a baseline that is stronger than the state-of-the-art for concept prediction. We plan to release our parser for public use. version:1
arxiv-1506-03164 | Parallelizing MCMC with Random Partition Trees | http://arxiv.org/abs/1506.03164 | id:1506.03164 author:Xiangyu Wang, Fangjian Guo, Katherine A. Heller, David B. Dunson category:stat.ML  published:2015-06-10 summary:The modern scale of data has brought new challenges to Bayesian inference. In particular, conventional MCMC algorithms are computationally very expensive for large data sets. A promising approach to solve this problem is embarrassingly parallel MCMC (EP-MCMC), which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset. The subset posterior draws are then aggregated via some combining rules to obtain the final approximation. Existing EP-MCMC algorithms are limited by approximation accuracy and difficulty in resampling. In this article, we propose a new EP-MCMC algorithm PART that solves these problems. The new algorithm applies random partition trees to combine the subset posterior draws, which is distribution-free, easy to resample from and can adapt to multiple scales. We provide theoretical justification and extensive experiments illustrating empirical performance. version:2
arxiv-1510-07573 | Generalized Regressive Motion: a Visual Cue to Collision | http://arxiv.org/abs/1510.07573 | id:1510.07573 author:Krzysztof Chalupka, Michael Dickinson, Pietro Perona category:cs.RO cs.CV cs.MA cs.SY  published:2015-10-26 summary:Brains and sensory systems evolved to guide motion. Central to this task is controlling the approach to stationary obstacles and detecting moving organisms. Looming has been proposed as the main monocular visual cue for detecting the approach of other animals and avoiding collisions with stationary obstacles. Elegant neural mechanisms for looming detection have been found in the brain of insects and vertebrates. However, looming has not been analyzed in the context of collisions between two moving animals. We propose an alternative strategy, Generalized Regressive Motion (GRM), which is consistent with recently observed behavior in fruit flies. Geometric analysis proves that GRM is a reliable cue to collision among conspecifics, whereas agent-based modeling suggests that GRM is a better cue than looming as a means to detect approach, prevent collisions and maintain mobility. version:1
arxiv-1507-06504 | Active skeleton for bacteria modeling | http://arxiv.org/abs/1507.06504 | id:1507.06504 author:Jean-Pascal Jacob, Mariella Dimiccoli, Lionel Moisan category:cs.CV  published:2015-07-23 summary:The investigation of spatio-temporal dynamics of bacterial cells and their molecular components requires automated image analysis tools to track cell shape properties and molecular component locations inside the cells. In the study of bacteria aging, the molecular components of interest are protein aggregates accumulated near bacteria boundaries. This particular location makes very ambiguous the correspondence between aggregates and cells, since computing accurately bacteria boundaries in phase-contrast time-lapse imaging is a challenging task. This paper proposes an active skeleton formulation for bacteria modeling which provides several advantages: an easy computation of shape properties (perimeter, length, thickness, orientation), an improved boundary accuracy in noisy images, and a natural bacteria-centered coordinate system that permits the intrinsic location of molecular components inside the cell. Starting from an initial skeleton estimate, the medial axis of the bacterium is obtained by minimizing an energy function which incorporates bacteria shape constraints. Experimental results on biological images and comparative evaluation of the performances validate the proposed approach for modeling cigar-shaped bacteria like Escherichia coli. The Image-J plugin of the proposed method can be found online at http://fluobactracker.inrialpes.fr. version:3
arxiv-1510-07493 | Aggregating Deep Convolutional Features for Image Retrieval | http://arxiv.org/abs/1510.07493 | id:1510.07493 author:Artem Babenko, Victor Lempitsky category:cs.CV  published:2015-10-26 summary:Several recent works have shown that image descriptors produced by deep convolutional neural networks provide state-of-the-art performance for image classification and retrieval problems. It has also been shown that the activations from the convolutional layers can be interpreted as local features describing particular image regions. These local features can be aggregated using aggregation approaches developed for local features (e.g. Fisher vectors), thus providing new powerful global descriptors. In this paper we investigate possible ways to aggregate local deep features to produce compact global descriptors for image retrieval. First, we show that deep features and traditional hand-engineered features have quite different distributions of pairwise similarities, hence existing aggregation methods have to be carefully re-evaluated. Such re-evaluation reveals that in contrast to shallow features, the simple aggregation method based on sum pooling provides arguably the best performance for deep convolutional features. This method is efficient, has few parameters, and bears little risk of overfitting when e.g. learning the PCA matrix. Overall, the new compact global descriptor improves the state-of-the-art on four common benchmarks considerably. version:1
arxiv-1510-07474 | A Markov Random Field and Active Contour Image Segmentation Model for Animal Spots Patterns | http://arxiv.org/abs/1510.07474 | id:1510.07474 author:Alexander Gómez, German Díez, Jhony Giraldo, Augusto Salazar, Juan M. Daza category:cs.CV  published:2015-10-26 summary:Non-intrusive biometrics of animals using images allows to analyze phenotypic populations and individuals with patterns like stripes and spots without affecting the studied subjects. However, non-intrusive biometrics demand a well trained subject or the development of computer vision algorithms that ease the identification task. In this work, an analysis of classic segmentation approaches that require a supervised tuning of their parameters such as threshold, adaptive threshold, histogram equalization, and saturation correction is presented. In contrast, a general unsupervised algorithm using Markov Random Fields (MRF) for segmentation of spots patterns is proposed. Active contours are used to boost results using MRF output as seeds. As study subject the Diploglossus millepunctatus lizard is used. The proposed method achieved a maximum efficiency of $91.11\%$. version:1
arxiv-1510-07471 | A Parallel algorithm for $\mathcal{X}$-Armed bandits | http://arxiv.org/abs/1510.07471 | id:1510.07471 author:Cheng Chen, Shuang Liu, Zhihua Zhang, Wu-Jun Li category:stat.ML cs.LG  published:2015-10-26 summary:The target of $\mathcal{X}$-armed bandit problem is to find the global maximum of an unknown stochastic function $f$, given a finite budget of $n$ evaluations. Recently, $\mathcal{X}$-armed bandits have been widely used in many situations. Many of these applications need to deal with large-scale data sets. To deal with these large-scale data sets, we study a distributed setting of $\mathcal{X}$-armed bandits, where $m$ players collaborate to find the maximum of the unknown function. We develop a novel anytime distributed $\mathcal{X}$-armed bandit algorithm. Compared with prior work on $\mathcal{X}$-armed bandits, our algorithm uses a quite different searching strategy so as to fit distributed learning scenarios. Our theoretical analysis shows that our distributed algorithm is $m$ times faster than the classical single-player algorithm. Moreover, the number of communication rounds of our algorithm is only logarithmic in $mn$. The numerical results show that our method can make effective use of every players to minimize the loss. Thus, our distributed approach is attractive and useful. version:1
arxiv-1510-07439 | Object Oriented Analysis using Natural Language Processing concepts: A Review | http://arxiv.org/abs/1510.07439 | id:1510.07439 author:Abinash Tripathy, Santanu Kumar Rath category:cs.SE cs.CL  published:2015-10-26 summary:The Software Development Life Cycle (SDLC) starts with eliciting requirements of the customers in the form of Software Requirement Specification (SRS). SRS document needed for software development is mostly written in Natural Language(NL) convenient for the client. From the SRS document only, the class name, its attributes and the functions incorporated in the body of the class are traced based on pre-knowledge of analyst. The paper intends to present a review on Object Oriented (OO) analysis using Natural Language Processing (NLP) techniques. This analysis can be manual where domain expert helps to generate the required diagram or automated system, where the system generates the required diagram, from the input in the form of SRS. version:1
arxiv-1510-08713 | How good is good enough? Re-evaluating the bar for energy disaggregation | http://arxiv.org/abs/1510.08713 | id:1510.08713 author:Nipun Batra, Rishi Baijal, Amarjeet Singh, Kamin Whitehouse category:cs.LG  published:2015-10-26 summary:Since the early 1980s, the research community has developed ever more sophisticated algorithms for the problem of energy disaggregation, but despite decades of research, there is still a dearth of applications with demonstrated value. In this work, we explore a question that is highly pertinent to this research community: how good does energy disaggregation need to be in order to infer characteristics of a household? We present novel techniques that use unsupervised energy disaggregation to predict both household occupancy and static properties of the household such as size of the home and number of occupants. Results show that basic disaggregation approaches performs up to 30% better at occupancy estimation than using aggregate power data alone, and are up to 10% better at estimating static household characteristics. These results show that even rudimentary energy disaggregation techniques are sufficient for improved inference of household characteristics. To conclude, we re-evaluate the bar set by the community for energy disaggregation accuracy and try to answer the question "how good is good enough?" version:1
arxiv-1510-07391 | Vehicle Color Recognition using Convolutional Neural Network | http://arxiv.org/abs/1510.07391 | id:1510.07391 author:Reza Fuad Rachmadi, I Ketut Eddy Purnama category:cs.CV  published:2015-10-26 summary:Vehicle color information is one of the important elements in ITS (Intelligent Traffic System). In this paper, we present a vehicle color recognition method using convolutional neural network (CNN). Naturally, CNN is designed to learn classification method based on shape information, but we proved that CNN can also learn classification based on color distribution. In our method, we convert the input image to two different color spaces, HSV and CIE Lab, and run it to some CNN architecture. The training process follow procedure introduce by Krizhevsky, that learning rate is decreasing by factor of 10 after some iterations. To test our method, we use publicly vehicle color recognition dataset provided by Chen. The results, our model outperform the original system provide by Chen with 2% higher overall accuracy. version:1
arxiv-1510-07390 | Pan-Tilt Camera and PIR Sensor Fusion Based Moving Object Detection for Mobile Security Robots | http://arxiv.org/abs/1510.07390 | id:1510.07390 author:YongChol Sin, MyongSong Choe, GyongIl Ryang category:cs.RO cs.CV  published:2015-10-26 summary:One of fundamental issues for security robots is to detect and track people in the surroundings. The main problems of this task are real-time constraints, a changing background, varying illumination conditions and a non-rigid shape of the person to be tracked. In this paper, we propose a solution for tracking with a pan-tilt camera and a passive infrared range (PIR) sensor to detect the moving object based on consecutive frame difference. The proposed method is excellent in real-time performance because it requires only a little memory and computation. Experiment results show that this method can detect the moving object such as human efficiently and accurately in non-stationary and complex indoor environment. version:1
arxiv-1511-02900 | Neighbourhood NILM: A Big-data Approach to Household Energy Disaggregation | http://arxiv.org/abs/1511.02900 | id:1511.02900 author:Nipun Batra, Amarjeet Singh, Kamin Whitehouse category:cs.LG  published:2015-10-26 summary:In this paper, we investigate whether "big-data" is more valuable than "precise" data for the problem of energy disaggregation: the process of breaking down aggregate energy usage on a per-appliance basis. Existing techniques for disaggregation rely on energy metering at a resolution of 1 minute or higher, but most power meters today only provide a reading once per month, and at most once every 15 minutes. In this paper, we propose a new technique called Neighbourhood NILM that leverages data from 'neighbouring' homes to disaggregate energy given only a single energy reading per month. The key intuition behind our approach is that 'similar' homes have 'similar' energy consumption on a per-appliance basis. Neighbourhood NILM matches every home with a set of 'neighbours' that have direct submetering infrastructure, i.e. power meters on individual circuits or loads. Many such homes already exist. Then, it estimates the appliance-level energy consumption of the target home to be the average of its K neighbours. We evaluate this approach using 25 homes and results show that our approach gives comparable or better disaggregation in comparison to state-of-the-art accuracy reported in the literature that depend on manual model training, high frequency power metering, or both. Results show that Neighbourhood NILM can achieve 83% and 79% accuracy disaggregating fridge and heating/cooling loads, compared to 74% and 73% for a technique called FHMM. Furthermore, it achieves up to 64% accuracy on washing machine, dryer, dishwasher, and lighting loads, which is higher than previously reported results. Many existing techniques are not able to disaggregate these loads at all. These results indicate a potentially substantial advantage to installing submetering infrastructure in a select few homes rather than installing new high-frequency smart metering infrastructure in all homes. version:1
arxiv-1510-07385 | How to merge three different methods for information filtering ? | http://arxiv.org/abs/1510.07385 | id:1510.07385 author:Jean-Valère Cossu, Ludovic Bonnefoy, Xavier Bost, Marc El Bèze category:cs.CL cs.IR  published:2015-10-26 summary:Twitter is now a gold marketing tool for entities concerned with online reputation. To automatically monitor online reputation of entities , systems have to deal with ambiguous entity names, polarity detection and topic detection. We propose three approaches to tackle the first issue: monitoring Twitter in order to find relevant tweets about a given entity. Evaluated within the framework of the RepLab-2013 Filtering task, each of them has been shown competitive with state-of-the-art approaches. Mainly we investigate on how much merging strategies may impact performances on a filtering task according to the evaluation measure. version:1
arxiv-1505-04657 | Mining User Opinions in Mobile App Reviews: A Keyword-based Approach | http://arxiv.org/abs/1505.04657 | id:1505.04657 author:Phong Minh Vu, Tam The Nguyen, Hung Viet Pham, Tung Thanh Nguyen category:cs.IR cs.CL  published:2015-05-18 summary:User reviews of mobile apps often contain complaints or suggestions which are valuable for app developers to improve user experience and satisfaction. However, due to the large volume and noisy-nature of those reviews, manually analyzing them for useful opinions is inherently challenging. To address this problem, we propose MARK, a keyword-based framework for semi-automated review analysis. MARK allows an analyst describing his interests in one or some mobile apps by a set of keywords. It then finds and lists the reviews most relevant to those keywords for further analysis. It can also draw the trends over time of those keywords and detect their sudden changes, which might indicate the occurrences of serious issues. To help analysts describe their interests more effectively, MARK can automatically extract keywords from raw reviews and rank them by their associations with negative reviews. In addition, based on a vector-based semantic representation of keywords, MARK can divide a large set of keywords into more cohesive subsets, or suggest keywords similar to the selected ones. version:2
arxiv-1510-07323 | Finding Temporally Consistent Occlusion Boundaries in Videos using Geometric Context | http://arxiv.org/abs/1510.07323 | id:1510.07323 author:S. Hussain Raza, Ahmad Humayun, Matthias Grundmann, David Anderson, Irfan Essa category:cs.CV  published:2015-10-25 summary:We present an algorithm for finding temporally consistent occlusion boundaries in videos to support segmentation of dynamic scenes. We learn occlusion boundaries in a pairwise Markov random field (MRF) framework. We first estimate the probability of an spatio-temporal edge being an occlusion boundary by using appearance, flow, and geometric features. Next, we enforce occlusion boundary continuity in a MRF model by learning pairwise occlusion probabilities using a random forest. Then, we temporally smooth boundaries to remove temporal inconsistencies in occlusion boundary estimation. Our proposed framework provides an efficient approach for finding temporally consistent occlusion boundaries in video by utilizing causality, redundancy in videos, and semantic layout of the scene. We have developed a dataset with fully annotated ground-truth occlusion boundaries of over 30 videos ($5000 frames). This dataset is used to evaluate temporal occlusion boundaries and provides a much needed baseline for future studies. We perform experiments to demonstrate the role of scene layout, and temporal information for occlusion reasoning in dynamic scenes. version:1
arxiv-1510-07320 | Geometric Context from Videos | http://arxiv.org/abs/1510.07320 | id:1510.07320 author:S. Hussain Raza, Matthias Grundmann, Irfan Essa category:cs.CV  published:2015-10-25 summary:We present a novel algorithm for estimating the broad 3D geometric structure of outdoor video scenes. Leveraging spatio-temporal video segmentation, we decompose a dynamic scene captured by a video into geometric classes, based on predictions made by region-classifiers that are trained on appearance and motion features. By examining the homogeneity of the prediction, we combine predictions across multiple segmentation hierarchy levels alleviating the need to determine the granularity a priori. We built a novel, extensive dataset on geometric context of video to evaluate our method, consisting of over 100 ground-truth annotated outdoor videos with over 20,000 frames. To further scale beyond this dataset, we propose a semi-supervised learning framework to expand the pool of labeled data with high confidence predictions obtained from unlabeled data. Our system produces an accurate prediction of geometric context of video achieving 96% accuracy across main geometric classes. version:1
arxiv-1510-07317 | Depth Extraction from Videos Using Geometric Context and Occlusion Boundaries | http://arxiv.org/abs/1510.07317 | id:1510.07317 author:S. Hussain Raza, Omar Javed, Aveek Das, Harpreet Sawhney, Hui Cheng, Irfan Essa category:cs.CV  published:2015-10-25 summary:We present an algorithm to estimate depth in dynamic video scenes. We propose to learn and infer depth in videos from appearance, motion, occlusion boundaries, and geometric context of the scene. Using our method, depth can be estimated from unconstrained videos with no requirement of camera pose estimation, and with significant background/foreground motions. We start by decomposing a video into spatio-temporal regions. For each spatio-temporal region, we learn the relationship of depth to visual appearance, motion, and geometric classes. Then we infer the depth information of new scenes using piecewise planar parametrization estimated within a Markov random field (MRF) framework by combining appearance to depth learned mappings and occlusion boundary guided smoothness constraints. Subsequently, we perform temporal smoothing to obtain temporally consistent depth maps. To evaluate our depth estimation algorithm, we provide a novel dataset with ground truth depth for outdoor video scenes. We present a thorough evaluation of our algorithm on our new dataset and the publicly available Make3d static image dataset. version:1
arxiv-1510-07303 | A Framework for Distributed Deep Learning Layer Design in Python | http://arxiv.org/abs/1510.07303 | id:1510.07303 author:Clay McLeod category:cs.LG  published:2015-10-25 summary:In this paper, a framework for testing Deep Neural Network (DNN) design in Python is presented. First, big data, machine learning (ML), and Artificial Neural Networks (ANNs) are discussed to familiarize the reader with the importance of such a system. Next, the benefits and detriments of implementing such a system in Python are presented. Lastly, the specifics of the system are explained, and some experimental results are presented to prove the effectiveness of the system. version:1
arxiv-1412-6651 | Deep learning with Elastic Averaging SGD | http://arxiv.org/abs/1412.6651 | id:1412.6651 author:Sixin Zhang, Anna Choromanska, Yann LeCun category:cs.LG stat.ML  published:2014-12-20 summary:We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient. version:8
arxiv-1510-07234 | Seam Puckering Objective Evaluation Method for Sewing Process | http://arxiv.org/abs/1510.07234 | id:1510.07234 author:Raluca Brad, Eugen HĂloiu, Remus Brad category:cs.CV cs.CE  published:2015-10-25 summary:The paper presents an automated method for the assessment and classification of puckering defects detected during the preproduction control stage of the sewing machine or product inspection. In this respect, we have presented the possible causes and remedies of the wrinkle nonconformities. Subjective factors related to the control environment and operators during the seams evaluation can be reduced using an automated system whose operation is based on image processing. Our implementation involves spectral image analysis using Fourier transform and an unsupervised neural network, the Kohonen Map, employed to classify material specimens, the input images, into five discrete degrees of quality, from grade 5 (best) to grade 1 (the worst). version:1
arxiv-1510-06664 | Random Projections through multiple optical scattering: Approximating kernels at the speed of light | http://arxiv.org/abs/1510.06664 | id:1510.06664 author:Alaa Saade, Francesco Caltagirone, Igor Carron, Laurent Daudet, Angélique Drémeau, Sylvain Gigan, Florent Krzakala category:cs.ET cs.LG physics.optics  published:2015-10-22 summary:Random projections have proven extremely useful in many signal processing and machine learning applications. However, they often require either to store a very large random matrix, or to use a different, structured matrix to reduce the computational and memory costs. Here, we overcome this difficulty by proposing an analog, optical device, that performs the random projections literally at the speed of light without having to store any matrix in memory. This is achieved using the physical properties of multiple coherent scattering of coherent light in random media. We use this device on a simple task of classification with a kernel machine, and we show that, on the MNIST database, the experimental results closely match the theoretical performance of the corresponding kernel. This framework can help make kernel methods practical for applications that have large training sets and/or require real-time prediction. We discuss possible extensions of the method in terms of a class of kernels, speed, memory consumption and different problems. version:2
arxiv-1510-07905 | Defect Detection Techniques for Airbag Production Sewing Stages | http://arxiv.org/abs/1510.07905 | id:1510.07905 author:Raluca Brad, Lavinia Barac, Remus Brad category:cs.CV  published:2015-10-25 summary:Airbags are subject to strict quality control in order to ensure passengers safety. The quality of fabric and sewing thread influence the final product and therefore, sewing defects must be early and accurately detected, in order to remove the item from production. Airbag seams assembly can take various forms, using linear and circle primitives, with threads of different colors and length densities, creating lockstitch or double threads chainstitch. The paper presents a framework for the automatic detection of defects occurring during the airbag sewing stage. Types of defects as skipped stitch, missed stitch or superimposed seam for lockstitch and two threads chainstitch are detected and marked. Using image processing methods, the proposed framework follows the seams path and determines if a color pattern of the considered stitches is valid. version:1
arxiv-1510-07211 | On End-to-End Program Generation from User Intention by Deep Neural Networks | http://arxiv.org/abs/1510.07211 | id:1510.07211 author:Lili Mou, Rui Men, Ge Li, Lu Zhang, Zhi Jin category:cs.SE cs.LG  published:2015-10-25 summary:This paper envisions an end-to-end program generation scenario using recurrent neural networks (RNNs): Users can express their intention in natural language; an RNN then automatically generates corresponding code in a characterby-by-character fashion. We demonstrate its feasibility through a case study and empirical analysis. To fully make such technique useful in practice, we also point out several cross-disciplinary challenges, including modeling user intention, providing datasets, improving model architectures, etc. Although much long-term research shall be addressed in this new field, we believe end-to-end program generation would become a reality in future decades, and we are looking forward to its practice. version:1
arxiv-1510-07208 | Vehicle Speed Prediction using Deep Learning | http://arxiv.org/abs/1510.07208 | id:1510.07208 author:Joe Lemieux, Yuan Ma category:cs.LG cs.NE  published:2015-10-25 summary:Global optimization of the energy consumption of dual power source vehicles such as hybrid electric vehicles, plug-in hybrid electric vehicles, and plug in fuel cell electric vehicles requires knowledge of the complete route characteristics at the beginning of the trip. One of the main characteristics is the vehicle speed profile across the route. The profile will translate directly into energy requirements for a given vehicle. However, the vehicle speed that a given driver chooses will vary from driver to driver and from time to time, and may be slower, equal to, or faster than the average traffic flow. If the specific driver speed profile can be predicted, the energy usage can be optimized across the route chosen. The purpose of this paper is to research the application of Deep Learning techniques to this problem to identify at the beginning of a drive cycle the driver specific vehicle speed profile for an individual driver repeated drive cycle, which can be used in an optimization algorithm to minimize the amount of fossil fuel energy used during the trip. version:1
arxiv-1510-03931 | Structured Memory for Neural Turing Machines | http://arxiv.org/abs/1510.03931 | id:1510.03931 author:Wei Zhang, Yang Yu, Bowen Zhou category:cs.AI cs.NE I.2.6  published:2015-10-14 summary:Neural Turing Machines (NTM) contain memory component that simulates "working memory" in the brain to store and retrieve information to ease simple algorithms learning. So far, only linearly organized memory is proposed, and during experiments, we observed that the model does not always converge, and overfits easily when handling certain tasks. We think memory component is key to some faulty behaviors of NTM, and better organization of memory component could help fight those problems. In this paper, we propose several different structures of memory for NTM, and we proved in experiments that two of our proposed structured-memory NTMs could lead to better convergence, in term of speed and prediction accuracy on copy task and associative recall task as in (Graves et al. 2014). version:3
arxiv-1510-07193 | Statistical Parsing by Machine Learning from a Classical Arabic Treebank | http://arxiv.org/abs/1510.07193 | id:1510.07193 author:Kais Dukes category:cs.CL  published:2015-10-25 summary:Research into statistical parsing for English has enjoyed over a decade of successful results. However, adapting these models to other languages has met with difficulties. Previous comparative work has shown that Modern Arabic is one of the most difficult languages to parse due to rich morphology and free word order. Classical Arabic is the ancient form of Arabic, and is understudied in computational linguistics, relative to its worldwide reach as the language of the Quran. The thesis is based on seven publications that make significant contributions to knowledge relating to annotating and parsing Classical Arabic. A central argument of this thesis is that using a hybrid representation closely aligned to traditional grammar leads to improved parsing for Arabic. To test this hypothesis, two approaches are compared. As a reference, a pure dependency parser is adapted using graph transformations, resulting in an 87.47% F1-score. This is compared to an integrated parsing model with an F1-score of 89.03%, demonstrating that joint dependency-constituency parsing is better suited to Classical Arabic. version:1
arxiv-1510-04195 | An Omnibus Nonparametric Test of Equality in Distribution for Unknown Functions | http://arxiv.org/abs/1510.04195 | id:1510.04195 author:Alexander R. Luedtke, Marco Carone, Mark J. van der Laan category:math.ST stat.ML stat.TH 62G10  published:2015-10-14 summary:We present a novel family of nonparametric omnibus tests of the hypothesis that two unknown but estimable functions are equal in distribution when applied to the observed data structure. We developed these tests, which represent a generalization of the maximum mean discrepancy tests described in Gretton et al. [2006], using recent developments from the higher-order pathwise differentiability literature. Despite their complex derivation, the associated test statistics can be expressed rather simply as U-statistics. We study the asymptotic behavior of the proposed tests under the null hypothesis and under both fixed and local alternatives. We provide examples to which our tests can be applied and show that they perform well in a simulation study. As an important special case, our proposed tests can be used to determine whether an unknown function, such as the conditional average treatment effect, is equal to zero almost surely. version:2
arxiv-1510-07182 | Computational models of attention | http://arxiv.org/abs/1510.07182 | id:1510.07182 author:Laurent Itti, Ali Borji category:cs.CV  published:2015-10-24 summary:This chapter reviews recent computational models of visual attention. We begin with models for the bottom-up or stimulus-driven guidance of attention to salient visual items, which we examine in seven different broad categories. We then examine more complex models which address the top-down or goal-oriented guidance of attention towards items that are more relevant to the task at hand. version:1
arxiv-1510-07169 | Fast and Scalable Lasso via Stochastic Frank-Wolfe Methods with a Convergence Guarantee | http://arxiv.org/abs/1510.07169 | id:1510.07169 author:Emanuele Frandi, Ricardo Nanculef, Stefano Lodi, Claudio Sartori, Johan A. K. Suykens category:stat.ML cs.LG math.OC  published:2015-10-24 summary:Frank-Wolfe (FW) algorithms have been often proposed over the last few years as efficient solvers for a variety of optimization problems arising in the field of Machine Learning. The ability to work with cheap projection-free iterations and the incremental nature of the method make FW a very effective choice for many large-scale problems where computing a sparse model is desirable. In this paper, we present a high-performance implementation of the FW method tailored to solve large-scale Lasso regression problems, based on a randomized iteration, and prove that the convergence guarantees of the standard FW method are preserved in the stochastic setting. We show experimentally that our algorithm outperforms several existing state of the art methods, including the Coordinate Descent algorithm by Friedman et al. (one of the fastest known Lasso solvers), on several benchmark datasets with a very large number of features, without sacrificing the accuracy of the model. Our results illustrate that the algorithm is able to generate the complete regularization path on problems of size up to four million variables in less than one minute. version:1
arxiv-1510-07163 | Evolutionary Landscape and Management of Population Diversity | http://arxiv.org/abs/1510.07163 | id:1510.07163 author:Maumita Bhattacharya category:cs.NE 68T99  published:2015-10-24 summary:The search ability of an Evolutionary Algorithm (EA) depends on the variation among the individuals in the population [3, 4, 8]. Maintaining an optimal level of diversity in the EA population is imperative to ensure that progress of the EA search is unhindered by premature convergence to suboptimal solutions. Clearer understanding of the concept of population diversity, in the context of evolutionary search and premature convergence in particular, is the key to designing efficient EAs. To this end, this paper first presents a brief analysis of the EA population diversity issues. Next we present an investigation on a counter-niching EA technique [4] that introduces and maintains constructive diversity in the population. The proposed approach uses informed genetic operations to reach promising, but unexplored or under-explored areas of the search space, while discouraging premature local convergence. Simulation runs on a suite of standard benchmark test functions with Genetic Algorithm (GA) implementation shows promising results. version:1
arxiv-1510-07146 | Data-driven detrending of nonstationary fractal time series with echo state networks | http://arxiv.org/abs/1510.07146 | id:1510.07146 author:Enrico Maiorino, Filippo Maria Bianchi, Lorenzo Livi, Antonello Rizzi, Alireza Sadeghian category:physics.data-an cs.LG cs.NE  published:2015-10-24 summary:In this paper, we propose a data-driven approach to the problem of detrending fractal and multifractal time series. We consider a time series as the measurements elaborated from a dynamical process over time. We assume that such a dynamical process is predictable to a certain degree, by means of a class of recurrent networks called echo state networks. Such networks have been shown to be able to predict the outcome of a number of dynamical processes. Here we propose to perform a data-driven detrending of nonstationary, fractal and multifractal time series by using an echo state network operating as a filter. Notably, we predict the trend component of a given input time series, which is superimposed to the (multi)fractal component of interest. Such a (estimated) trend is then removed from the original time series and the residual signal is analyzed with the Multifractal Detrended Fluctuation Analysis for a quantitative verification of the correctness of the proposed detrending procedure. In order to demonstrate the effectiveness of the proposed technique, we consider several synthetic time series having a self-similar noise component with known characteristics. Such synthetic time series contain different types of trends. We also process a real-world dataset, the sunspot time series, which is well-known for its multifractal features and it has recently gained attention in the complex systems field. Results demonstrate the validity and generality of the proposed detrending method based on echo state networks. version:1
arxiv-1410-7429 | Higher-order MRFs based image super resolution: why not MAP? | http://arxiv.org/abs/1410.7429 | id:1410.7429 author:Yunjin Chen category:cs.CV  published:2014-10-27 summary:A trainable filter-based higher-order Markov Random Fields (MRFs) model - the so called Fields of Experts (FoE), has proved a highly effective image prior model for many classic image restoration problems. Generally, two options are available to incorporate the learned FoE prior in the inference procedure: (1) sampling-based minimum mean square error (MMSE) estimate, and (2) energy minimization-based maximum a posteriori (MAP) estimate. This letter is devoted to the FoE prior based single image super resolution (SR) problem, and we suggest to make use of the MAP estimate for inference based on two facts: (I) It is well-known that the MAP inference has a remarkable advantage of high computational efficiency, while the sampling-based MMSE estimate is very time consuming. (II) Practical SR experiment results demonstrate that the MAP estimate works equally well compared to the MMSE estimate with exactly the same FoE prior model. Moreover, it can lead to even further improvements by incorporating our discriminatively trained FoE prior model. In summary, we hold that for higher-order natural image prior based SR problem, it is better to employ the MAP estimate for inference. version:4
arxiv-1510-07136 | Image Parsing with a Wide Range of Classes and Scene-Level Context | http://arxiv.org/abs/1510.07136 | id:1510.07136 author:Marian George category:cs.CV  published:2015-10-24 summary:This paper presents a nonparametric scene parsing approach that improves the overall accuracy, as well as the coverage of foreground classes in scene images. We first improve the label likelihood estimates at superpixels by merging likelihood scores from different probabilistic classifiers. This boosts the classification performance and enriches the representation of less-represented classes. Our second contribution consists of incorporating semantic context in the parsing process through global label costs. Our method does not rely on image retrieval sets but rather assigns a global likelihood estimate to each label, which is plugged into the overall energy function. We evaluate our system on two large-scale datasets, SIFTflow and LMSun. We achieve state-of-the-art performance on the SIFTflow dataset and near-record results on LMSun. version:1
arxiv-1207-4255 | On the Statistical Efficiency of $\ell_{1,p}$ Multi-Task Learning of Gaussian Graphical Models | http://arxiv.org/abs/1207.4255 | id:1207.4255 author:Jean Honorio, Tommi Jaakkola, Dimitris Samaras category:cs.LG stat.ML  published:2012-07-18 summary:In this paper, we present $\ell_{1,p}$ multi-task structure learning for Gaussian graphical models. We analyze the sufficient number of samples for the correct recovery of the support union and edge signs. We also analyze the necessary number of samples for any conceivable method by providing information-theoretic lower bounds. We compare the statistical efficiency of multi-task learning versus that of single-task learning. For experiments, we use a block coordinate descent method that is provably convergent and generates a sequence of positive definite solutions. We provide experimental validation on synthetic data as well as on two publicly available real-world data sets, including functional magnetic resonance imaging and gene expression data. version:2
arxiv-1510-07119 | Predicting Face Recognition Performance Using Image Quality | http://arxiv.org/abs/1510.07119 | id:1510.07119 author:Abhishek Dutta, Raymond Veldhuis, Luuk Spreeuwers category:cs.CV  published:2015-10-24 summary:This paper proposes a data driven model to predict the performance of a face recognition system based on image quality features. We model the relationship between image quality features (e.g. pose, illumination, etc.) and recognition performance measures using a probability density function. To address the issue of limited nature of practical training data inherent in most data driven models, we have developed a Bayesian approach to model the distribution of recognition performance measures in small regions of the quality space. Since the model is based solely on image quality features, it can predict performance even before the actual recognition has taken place. We evaluate the performance predictive capabilities of the proposed model for six face recognition systems (two commercial and four open source) operating on three independent data sets: MultiPIE, FRGC and CAS-PEAL. Our results show that the proposed model can accurately predict performance using an accurate and unbiased Image Quality Assessor (IQA). Furthermore, our experiments highlight the impact of the unaccounted quality space -- the image quality features not considered by IQA -- in contributing to performance prediction errors. version:1
arxiv-1510-07112 | Predicting Performance of a Face Recognition System Based on Image Quality | http://arxiv.org/abs/1510.07112 | id:1510.07112 author:Abhishek Dutta category:cs.CV  published:2015-10-24 summary:In this dissertation, we present a generative model to capture the relation between facial image quality features (like pose, illumination direction, etc) and face recognition performance. Such a model can be used to predict the performance of a face recognition system. Since the model is based solely on image quality features, performance predictions can be done even before the actual recognition has taken place thereby facilitating many preemptive action. A practical limitation of such a data driven generative model is the limited nature of training data set. To address this limitation, we have developed a Bayesian approach to model the distribution of recognition performance measure based on the number of match and non-match scores in small regions of the image quality space. Random samples drawn from these models provide the initial data essential for training the generative model. Experiment results based on six face recognition systems operating on three independent data sets show that the proposed performance prediction model can accurately predict face recognition performance using an accurate and unbiased Image Quality Assessor (IQA). Furthermore, our results show that variability in the unaccounted quality space -- the image quality features not considered by the IQA -- is the major factor causing inaccuracies in predicted performance. version:1
arxiv-1510-07099 | Combine CRF and MMSEG to Boost Chinese Word Segmentation in Social Media | http://arxiv.org/abs/1510.07099 | id:1510.07099 author:Yao Yushi, Huang Zheng category:cs.CL  published:2015-10-24 summary:In this paper, we propose a joint algorithm for the word segmentation on Chinese social media. Previous work mainly focus on word segmentation for plain Chinese text, in order to develop a Chinese social media processing tool, we need to take the main features of social media into account, whose grammatical structure is not rigorous, and the tendency of using colloquial and Internet terms makes the existing Chinese-processing tools inefficient to obtain good performance on social media. In our approach, we combine CRF and MMSEG algorithm and extend features of traditional CRF algorithm to train the model for word segmentation, We use Internet lexicon in order to improve the performance of our model on Chinese social media. Our experimental result on Sina Weibo shows that our approach outperforms the state-of-the-art model. version:1
arxiv-1510-06479 | Generic decoding of seen and imagined objects using hierarchical visual features | http://arxiv.org/abs/1510.06479 | id:1510.06479 author:Tomoyasu Horikawa, Yukiyasu Kamitani category:q-bio.NC cs.CV  published:2015-10-22 summary:Object recognition is a key function in both human and machine vision. While recent studies have achieved fMRI decoding of seen and imagined contents, the prediction is limited to training examples. We present a decoding approach for arbitrary objects, using the machine vision principle that an object category is represented by a set of features rendered invariant through hierarchical processing. We show that visual features including those from a convolutional neural network can be predicted from fMRI patterns and that greater accuracy is achieved for low/high-level features with lower/higher-level visual areas, respectively. Predicted features are used to identify the target object (extending beyond decoder training) from a set of computed features for numerous objects. Furthermore, we demonstrate the identification of imagined objects, suggesting the recruitment of intermediate image representations in top-down processing. Our results demonstrate a tight link between human and machine vision and its utility for brain-based information retrieval. version:2
arxiv-1502-00524 | Unsupervised Incremental Learning and Prediction of Music Signals | http://arxiv.org/abs/1502.00524 | id:1502.00524 author:Ricard Marxer, Hendrik Purwins category:cs.SD cs.IR cs.LG stat.ML 68T05 I.2.6; H.5.5  published:2015-02-02 summary:A system is presented that segments, clusters and predicts musical audio in an unsupervised manner, adjusting the number of (timbre) clusters instantaneously to the audio input. A sequence learning algorithm adapts its structure to a dynamically changing clustering tree. The flow of the system is as follows: 1) segmentation by onset detection, 2) timbre representation of each segment by Mel frequency cepstrum coefficients, 3) discretization by incremental clustering, yielding a tree of different sound classes (e.g. instruments) that can grow or shrink on the fly driven by the instantaneous sound events, resulting in a discrete symbol sequence, 4) extraction of statistical regularities of the symbol sequence, using hierarchical N-grams and the newly introduced conceptual Boltzmann machine, and 5) prediction of the next sound event in the sequence. The system's robustness is assessed with respect to complexity and noisiness of the signal. Clustering in isolation yields an adjusted Rand index (ARI) of 82.7% / 85.7% for data sets of singing voice and drums. Onset detection jointly with clustering achieve an ARI of 81.3% / 76.3% and the prediction of the entire system yields an ARI of 27.2% / 39.2%. version:2
arxiv-1510-06939 | Objects2action: Classifying and localizing actions without any video example | http://arxiv.org/abs/1510.06939 | id:1510.06939 author:Mihir Jain, Jan C. van Gemert, Thomas Mensink, Cees G. M. Snoek category:cs.CV  published:2015-10-23 summary:The goal of this paper is to recognize actions in video without the need for examples. Different from traditional zero-shot approaches we do not demand the design and specification of attribute classifiers and class-to-attribute mappings to allow for transfer from seen classes to unseen classes. Our key contribution is objects2action, a semantic word embedding that is spanned by a skip-gram model of thousands of object categories. Action labels are assigned to an object encoding of unseen video based on a convex combination of action and object affinities. Our semantic embedding has three main characteristics to accommodate for the specifics of actions. First, we propose a mechanism to exploit multiple-word descriptions of actions and objects. Second, we incorporate the automated selection of the most responsive objects per action. And finally, we demonstrate how to extend our zero-shot approach to the spatio-temporal localization of actions in video. Experiments on four action datasets demonstrate the potential of our approach. version:1
arxiv-1510-06920 | On the complexity of switching linear regression | http://arxiv.org/abs/1510.06920 | id:1510.06920 author:Fabien Lauer category:stat.ML cs.CC cs.LG  published:2015-10-23 summary:This technical note extends recent results on the computational complexity of globally minimizing the error of piecewise-affine models to the related problem of minimizing the error of switching linear regression models. In particular, we show that, on the one hand the problem is NP-hard, but on the other hand, it admits a polynomial-time algorithm with respect to the number of data for any fixed data dimension and number of modes. version:1
arxiv-1510-06915 | Semi-Automatic Segmentation of Autosomal Dominant Polycystic Kidneys using Random Forests | http://arxiv.org/abs/1510.06915 | id:1510.06915 author:Kanishka Sharma, Loic Peter, Christian Rupprecht, Anna Caroli, Lichao Wang, Andrea Remuzzi, Maximilian Baust, Nassir Navab category:cs.CV  published:2015-10-23 summary:This paper presents a method for 3D segmentation of kidneys from patients with autosomal dominant polycystic kidney disease (ADPKD) and severe renal insufficiency, using computed tomography (CT) data. ADPKD severely alters the shape of the kidneys due to non-uniform formation of cysts. As a consequence, fully automatic segmentation of such kidneys is very challenging. We present a segmentation method with minimal user interaction based on a random forest classifier. One of the major novelties of the proposed approach is the usage of geodesic distance volumes as additional source of information. These volumes contain the intensity weighted distance to a manual outline of the respective kidney in only one slice (for each kidney) of the CT volume. We evaluate our method qualitatively and quantitatively on 55 CT acquisitions using ground truth annotations from clinical experts. version:1
arxiv-1510-06895 | Nonconvex Nonsmooth Low-Rank Minimization via Iteratively Reweighted Nuclear Norm | http://arxiv.org/abs/1510.06895 | id:1510.06895 author:Canyi Lu, Jinhui Tang, Shuicheng Yan, Zhouchen Lin category:cs.LG cs.CV cs.NA  published:2015-10-23 summary:The nuclear norm is widely used as a convex surrogate of the rank function in compressive sensing for low rank matrix recovery with its applications in image recovery and signal processing. However, solving the nuclear norm based relaxed convex problem usually leads to a suboptimal solution of the original rank minimization problem. In this paper, we propose to perform a family of nonconvex surrogates of $L_0$-norm on the singular values of a matrix to approximate the rank function. This leads to a nonconvex nonsmooth minimization problem. Then we propose to solve the problem by Iteratively Reweighted Nuclear Norm (IRNN) algorithm. IRNN iteratively solves a Weighted Singular Value Thresholding (WSVT) problem, which has a closed form solution due to the special properties of the nonconvex surrogate functions. We also extend IRNN to solve the nonconvex problem with two or more blocks of variables. In theory, we prove that IRNN decreases the objective function value monotonically, and any limit point is a stationary point. Extensive experiments on both synthesized data and real images demonstrate that IRNN enhances the low-rank matrix recovery compared with state-of-the-art convex algorithms. version:1
arxiv-1509-03475 | Hessian-free Optimization for Learning Deep Multidimensional Recurrent Neural Networks | http://arxiv.org/abs/1509.03475 | id:1509.03475 author:Minhyung Cho, Chandra Shekhar Dhir, Jaehyung Lee category:cs.LG cs.NE stat.ML  published:2015-09-11 summary:Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable performance in the area of speech and handwriting recognition. The performance of an MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using Hessian-free (HF) optimization. Given that connectionist temporal classification (CTC) is utilized as an objective of learning an MDRNN for sequence labeling, the non-convexity of CTC poses a problem when applying HF to the network. As a solution, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. An MDRNN up to a depth of 15 layers is successfully trained using HF, resulting in an improved performance for sequence labeling. version:2
arxiv-1510-07035 | Fast Latent Variable Models for Inference and Visualization on Mobile Devices | http://arxiv.org/abs/1510.07035 | id:1510.07035 author:Joseph W Robinson, Aaron Q Li category:cs.LG cs.CL cs.DC cs.IR  published:2015-10-23 summary:In this project we outline Vedalia, a high performance distributed network for performing inference on latent variable models in the context of Amazon review visualization. We introduce a new model, RLDA, which extends Latent Dirichlet Allocation (LDA) [Blei et al., 2003] for the review space by incorporating auxiliary data available in online reviews to improve modeling while simultaneously remaining compatible with pre-existing fast sampling techniques such as [Yao et al., 2009; Li et al., 2014a] to achieve high performance. The network is designed such that computation is efficiently offloaded to the client devices using the Chital system [Robinson & Li, 2015], improving response times and reducing server costs. The resulting system is able to rapidly compute a large number of specialized latent variable models while requiring minimal server resources. version:1
arxiv-1510-06807 | Learning in the Rational Speech Acts Model | http://arxiv.org/abs/1510.06807 | id:1510.06807 author:Will Monroe, Christopher Potts category:cs.CL  published:2015-10-23 summary:The Rational Speech Acts (RSA) model treats language use as a recursive process in which probabilistic speaker and listener agents reason about each other's intentions to enrich the literal semantics of their language along broadly Gricean lines. RSA has been shown to capture many kinds of conversational implicature, but it has been criticized as an unrealistic model of speakers, and it has so far required the manual specification of a semantic lexicon, preventing its use in natural language processing applications that learn lexical knowledge from data. We address these concerns by showing how to define and optimize a trained statistical classifier that uses the intermediate agents of RSA as hidden layers of representation forming a non-linear activation function. This treatment opens up new application domains and new possibilities for learning effectively from data. We validate the model on a referential expression generation task, showing that the best performance is achieved by incorporating features approximating well-established insights about natural language generation into RSA. version:1
arxiv-1501-02923 | Efficient Blind Compressed Sensing Using Sparsifying Transforms with Convergence Guarantees and Application to MRI | http://arxiv.org/abs/1501.02923 | id:1501.02923 author:Saiprasad Ravishankar, Yoram Bresler category:cs.LG stat.ML  published:2015-01-13 summary:Natural signals and images are well-known to be approximately sparse in transform domains such as Wavelets and DCT. This property has been heavily exploited in various applications in image processing and medical imaging. Compressed sensing exploits the sparsity of images or image patches in a transform domain or synthesis dictionary to reconstruct images from undersampled measurements. In this work, we focus on blind compressed sensing, where the underlying sparsifying transform is a priori unknown, and propose a framework to simultaneously reconstruct the underlying image as well as the sparsifying transform from highly undersampled measurements. The proposed block coordinate descent type algorithms involve highly efficient optimal updates. Importantly, we prove that although the proposed blind compressed sensing formulations are highly nonconvex, our algorithms are globally convergent (i.e., they converge from any initialization) to the set of critical points of the objectives defining the formulations. These critical points are guaranteed to be at least partial global and partial local minimizers. The exact point(s) of convergence may depend on initialization. We illustrate the usefulness of the proposed framework for magnetic resonance image reconstruction from highly undersampled k-space measurements. As compared to previous methods involving the synthesis dictionary model, our approach is much faster, while also providing promising reconstruction quality. version:2
arxiv-1507-02801 | Adaptive Mixtures of Factor Analyzers | http://arxiv.org/abs/1507.02801 | id:1507.02801 author:Heysem Kaya, Albert Ali Salah category:stat.ML cs.IT cs.LG math.IT G.3; I.5.4  published:2015-07-10 summary:A mixture of factor analyzers is a semi-parametric density estimator that generalizes the well-known mixtures of Gaussians model by allowing each Gaussian in the mixture to be represented in a different lower-dimensional manifold. This paper presents a robust and parsimonious model selection algorithm for training a mixture of factor analyzers, carrying out simultaneous clustering and locally linear, globally nonlinear dimensionality reduction. Permitting different number of factors per mixture component, the algorithm adapts the model complexity to the data complexity. We compare the proposed algorithm with related automatic model selection algorithms on a number of benchmarks. The results indicate the effectiveness of this fast and robust approach in clustering, manifold learning and class-conditional modeling. version:2
arxiv-1510-06706 | ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional Networks on Multi-Core and Many-Core Shared Memory Machines | http://arxiv.org/abs/1510.06706 | id:1510.06706 author:Aleksandar Zlateski, Kisuk Lee, H. Sebastian Seung category:cs.NE cs.CV cs.DC cs.LG  published:2015-10-22 summary:Convolutional networks (ConvNets) have become a popular approach to computer vision. It is important to accelerate ConvNet training, which is computationally costly. We propose a novel parallel algorithm based on decomposition into a set of tasks, most of which are convolutions or FFTs. Applying Brent's theorem to the task dependency graph implies that linear speedup with the number of processors is attainable within the PRAM model of parallel computation, for wide network architectures. To attain such performance on real shared-memory machines, our algorithm computes convolutions converging on the same node of the network with temporal locality to reduce cache misses, and sums the convergent convolution outputs via an almost wait-free concurrent method to reduce time spent in critical sections. We implement the algorithm with a publicly available software package called ZNN. Benchmarking with multi-core CPUs shows that ZNN can attain speedup roughly equal to the number of physical cores. We also show that ZNN can attain over 90x speedup on a many-core CPU (Xeon Phi Knights Corner). These speedups are achieved for network architectures with widths that are in common use. The task parallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelism of previous algorithms is compatible with GPUs. Through examples, we show that ZNN can be either faster or slower than certain GPU implementations depending on specifics of the network architecture, kernel sizes, and density and size of the output patch. ZNN may be less costly to develop and maintain, due to the relative ease of general-purpose CPU programming. version:1
arxiv-1510-06688 | Partitioning Data on Features or Samples in Communication-Efficient Distributed Optimization? | http://arxiv.org/abs/1510.06688 | id:1510.06688 author:Chenxin Ma, Martin Takáč category:math.OC cs.LG  published:2015-10-22 summary:In this paper we study the effect of the way that the data is partitioned in distributed optimization. The original DiSCO algorithm [Communication-Efficient Distributed Optimization of Self-Concordant Empirical Loss, Yuchen Zhang and Lin Xiao, 2015] partitions the input data based on samples. We describe how the original algorithm has to be modified to allow partitioning on features and show its efficiency both in theory and also in practice. version:1
arxiv-1510-06684 | Dual Free SDCA for Empirical Risk Minimization with Adaptive Probabilities | http://arxiv.org/abs/1510.06684 | id:1510.06684 author:Xi He, Martin Takáč category:math.OC cs.LG  published:2015-10-22 summary:In this paper we develop dual free SDCA with adaptive probabilities for regularized empirical risk minimization. This extends recent work of Shai Shalev-Shwartz [SDCA without Duality, arXiv:1502.06177] to allow non-uniform selection of "dual" coordinate in SDCA. Moreover, the probability can change over time, making it more efficient than uniform selection. Our work focuses on generating adaptive probabilities through iterative process, preferring to choose coordinate with highest potential to decrease sub-optimality. We also propose a practical variant Algorithm adfSDCA+ which is more aggressive. The work is concluded with multiple experiments which shows efficiency of proposed algorithms. version:1
arxiv-1510-06595 | Efficient Unsupervised Temporal Segmentation of Motion Data | http://arxiv.org/abs/1510.06595 | id:1510.06595 author:Björn Krüger, Anna Vögele, Tobias Willig, Angela Yao, Reinhard Klein, Andreas Weber category:cs.CV  published:2015-10-22 summary:We introduce a method for automated temporal segmentation of human motion data into distinct actions and compositing motion primitives based on self-similar structures in the motion sequence. We use neighbourhood graphs for the partitioning and the similarity information in the graph is further exploited to cluster the motion primitives into larger entities of semantic significance. The method requires no assumptions about the motion sequences at hand and no user interaction is required for the segmentation or clustering. In addition, we introduce a feature bundling preprocessing technique to make the segmentation more robust to noise, as well as a notion of motion symmetry for more refined primitive detection. We test our method on several sensor modalities, including markered and markerless motion capture as well as on electromyograph and accelerometer recordings. The results highlight our system's capabilities for both segmentation and for analysis of the finer structures of motion data, all in a completely unsupervised manner. version:1
arxiv-1510-06582 | Collective Prediction of Individual Mobility Traces with Exponential Weights | http://arxiv.org/abs/1510.06582 | id:1510.06582 author:Bartosz Hawelka, Izabela Sitko, Pavlos Kazakopoulos, Euro Beinat category:physics.soc-ph cs.CY cs.LG stat.ML  published:2015-10-22 summary:We present and test a sequential learning algorithm for the short-term prediction of human mobility. This novel approach pairs the Exponential Weights forecaster with a very large ensemble of experts. The experts are individual sequence prediction algorithms constructed from the mobility traces of 10 million roaming mobile phone users in a European country. Average prediction accuracy is significantly higher than that of individual sequence prediction algorithms, namely constant order Markov models derived from the user's own data, that have been shown to achieve high accuracy in previous studies of human mobility prediction. The algorithm uses only time stamped location data, and accuracy depends on the completeness of the expert ensemble, which should contain redundant records of typical mobility patterns. The proposed algorithm is applicable to the prediction of any sufficiently large dataset of sequences. version:1
arxiv-1510-06567 | Generalized conditional gradient: analysis of convergence and applications | http://arxiv.org/abs/1510.06567 | id:1510.06567 author:Alain Rakotomamonjy, Rémi Flamary, Nicolas Courty category:cs.LG math.OC stat.ML  published:2015-10-22 summary:The objectives of this technical report is to provide additional results on the generalized conditional gradient methods introduced by Bredies et al. [BLM05]. Indeed , when the objective function is smooth, we provide a novel certificate of optimality and we show that the algorithm has a linear convergence rate. Applications of this algorithm are also discussed. version:1
arxiv-1510-06549 | Multi-GPU Distributed Parallel Bayesian Differential Topic Modelling | http://arxiv.org/abs/1510.06549 | id:1510.06549 author:Aaron Q Li category:cs.CL cs.DC cs.LG  published:2015-10-22 summary:There is an explosion of data, documents, and other content, and people require tools to analyze and interpret these, tools to turn the content into information and knowledge. Topic modeling have been developed to solve these problems. Topic models such as LDA [Blei et. al. 2003] allow salient patterns in data to be extracted automatically. When analyzing texts, these patterns are called topics. Among numerous extensions of LDA, few of them can reliably analyze multiple groups of documents and extract topic similarities. Recently, the introduction of differential topic modeling (SPDP) [Chen et. al. 2012] performs uniformly better than many topic models in a discriminative setting. There is also a need to improve the sampling speed for topic models. While some effort has been made for distributed algorithms, there is no work currently done using graphical processing units (GPU). Note the GPU framework has already become the most cost-efficient platform for many problems. In this thesis, I propose and implement a scalable multi-GPU distributed parallel framework which approximates SPDP. Through experiments, I have shown my algorithms have a gain in speed of about 50 times while being almost as accurate, with only one single cheap laptop GPU. Furthermore, I have shown the speed improvement is sublinearly scalable when multiple GPUs are used, while fairly maintaining the accuracy. Therefore on a medium-sized GPU cluster, the speed improvement could potentially reach a factor of a thousand. Note SPDP is just a representative of other extensions of LDA. Although my algorithm is implemented to work with SPDP, it is designed to be a general enough to work with other topic models. The speed-up on smaller collections (i.e., 1000s of documents), means that these more complex LDA extensions could now be done in real-time, thus opening up a new way of using these LDA models in industry. version:1
arxiv-1510-06507 | Modelling, Measuring and Compensating Color Weak Vision | http://arxiv.org/abs/1510.06507 | id:1510.06507 author:Satoshi Oshima, Rica Mochizuki, Reiner Lenz, Jinhui Chao category:cs.CV I.2.10; I.4.8; I.5  published:2015-10-22 summary:We use methods from Riemann geometry to investigate transformations between the color spaces of color-normal and color weak observers. The two main applications are the simulation of the perception of a color weak observer for a color normal observer and the compensation of color images in a way that a color weak observer has approximately the same perception as a color normal observer. The metrics in the color spaces of interest are characterized with the help of ellipsoids defined by the just-noticable-differences between color which are measured with the help of color-matching experiments. The constructed mappings are isometries of Riemann spaces that preserve the perceived color-differences for both observers. Among the two approaches to build such an isometry, we introduce normal coordinates in Riemann spaces as a tool to construct a global color-weak compensation map. Compared to previously used methods this method is free from approximation errors due to local linearizations and it avoids the problem of shifting locations of the origin of the local coordinate system. We analyse the variations of the Riemann metrics for different observers obtained from new color matching experiments and describe three variations of the basic method. The performance of the methods is evaluated with the help of semantic differential (SD) tests. version:1
arxiv-1510-06503 | Personalized Age Progression with Aging Dictionary | http://arxiv.org/abs/1510.06503 | id:1510.06503 author:Xiangbo Shu, Jinhui Tang, Hanjiang Lai, Luoqi Liu, Shuicheng Yan category:cs.CV  published:2015-10-22 summary:In this paper, we aim to automatically render aging faces in a personalized way. Basically, a set of age-group specific dictionaries are learned, where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups, and a linear combination of these patterns expresses a particular personalized aging process. Moreover, two factors are taken into consideration in the dictionary learning process. First, beyond the aging dictionaries, each subject may have extra personalized facial characteristics, e.g. mole, which are invariant in the aging process. Second, it is challenging or even impossible to collect faces of all age groups for a particular subject, yet much easier and more practical to get face pairs from neighboring age groups. Thus a personality-aware coupled reconstruction loss is utilized to learn the dictionaries based on face pairs from neighboring age groups. Extensive experiments well demonstrate the advantages of our proposed solution over other state-of-the-arts in term of personalized aging progression, as well as the performance gain for cross-age face verification by synthesizing aging faces. version:1
arxiv-1510-06492 | Generalized Shortest Path Kernel on Graphs | http://arxiv.org/abs/1510.06492 | id:1510.06492 author:Linus Hermansson, Fredrik D. Johansson, Osamu Watanabe category:cs.DS cs.LG  published:2015-10-22 summary:We consider the problem of classifying graphs using graph kernels. We define a new graph kernel, called the generalized shortest path kernel, based on the number and length of shortest paths between nodes. For our example classification problem, we consider the task of classifying random graphs from two well-known families, by the number of clusters they contain. We verify empirically that the generalized shortest path kernel outperforms the original shortest path kernel on a number of datasets. We give a theoretical analysis for explaining our experimental results. In particular, we estimate distributions of the expected feature vectors for the shortest path kernel and the generalized shortest path kernel, and we show some evidence explaining why our graph kernel outperforms the shortest path kernel for our graph classification problem. version:1
arxiv-1312-4664 | Filtering with State-Observation Examples via Kernel Monte Carlo Filter | http://arxiv.org/abs/1312.4664 | id:1312.4664 author:Motonobu Kanagawa, Yu Nishiyama, Arthur Gretton, Kenji Fukumizu category:stat.ML  published:2013-12-17 summary:This paper addresses the problem of filtering with a state-space model. Standard approaches for filtering assume that a probabilistic model for observations (i.e. the observation model) is given explicitly or at least parametrically. We consider a setting where this assumption is not satisfied; we assume that the knowledge of the observation model is only provided by examples of state-observation pairs. This setting is important and appears when state variables are defined as quantities that are very different from the observations. We propose Kernel Monte Carlo Filter, a novel filtering method that is focused on this setting. Our approach is based on the framework of kernel mean embeddings, which enables nonparametric posterior inference using the state-observation examples. The proposed method represents state distributions as weighted samples, propagates these samples by sampling, estimates the state posteriors by Kernel Bayes' Rule, and resamples by Kernel Herding. In particular, the sampling and resampling procedures are novel in being expressed using kernel mean embeddings, so we theoretically analyze their behaviors. We reveal the following properties, which are similar to those of corresponding procedures in particle methods: (1) the performance of sampling can degrade if the effective sample size of a weighted sample is small; (2) resampling improves the sampling performance by increasing the effective sample size. We first demonstrate these theoretical findings by synthetic experiments. Then we show the effectiveness of the proposed filter by artificial and real data experiments, which include vision-based mobile robot localization. version:4
arxiv-1510-04815 | Scalable MCMC for Mixed Membership Stochastic Blockmodels | http://arxiv.org/abs/1510.04815 | id:1510.04815 author:Wenzhe Li, Sungjin Ahn, Max Welling category:cs.LG stat.ML  published:2015-10-16 summary:We propose a stochastic gradient Markov chain Monte Carlo (SG-MCMC) algorithm for scalable inference in mixed-membership stochastic blockmodels (MMSB). Our algorithm is based on the stochastic gradient Riemannian Langevin sampler and achieves both faster speed and higher accuracy at every iteration than the current state-of-the-art algorithm based on stochastic variational inference. In addition we develop an approximation that can handle models that entertain a very large number of communities. The experimental results show that SG-MCMC strictly dominates competing algorithms in all cases. version:2
arxiv-1510-06463 | Inventory Control Involving Unknown Demand of Discrete Nonperishable Items - Analysis of a Newsvendor-based Policy | http://arxiv.org/abs/1510.06463 | id:1510.06463 author:Michael N. Katehakis, Jian Yang, Tingting Zhou category:stat.ML  published:2015-10-22 summary:Inventory control with unknown demand distribution is considered, with emphasis placed on the case involving discrete nonperishable items. We focus on an adaptive policy which in every period uses, as much as possible, the optimal newsvendor ordering quantity for the empirical distribution learned up to that period. The policy is assessed using the regret criterion, which measures the price paid for ambiguity on demand distribution over $T$ periods. When there are guarantees on the latter's separation from the critical newsvendor parameter $\beta=b/(h+b)$, a constant upper bound on regret can be found. Without any prior information on the demand distribution, we show that the regret does not grow faster than the rate $T^{1/2+\epsilon}$ for any $\epsilon>0$. In view of a known lower bound, this is almost the best one could hope for. Simulation studies involving this along with other policies are also conducted. version:1
arxiv-1505-07519 | A Bounded $p$-norm Approximation of Max-Convolution for Sub-Quadratic Bayesian Inference on Additive Factors | http://arxiv.org/abs/1505.07519 | id:1505.07519 author:Julianus Pfeuffer, Oliver Serang category:stat.CO cs.NA stat.ML  published:2015-05-28 summary:Max-convolution is an important problem closely resembling standard convolution; as such, max-convolution occurs frequently across many fields. Here we extend the method with fastest known worst-case runtime, which can be applied to nonnegative vectors by numerically approximating the Chebyshev norm $\ \cdot \ _\infty$, and use this approach to derive two numerically stable methods based on the idea of computing $p$-norms via fast convolution: The first method proposed, with runtime in $O( k \log(k) \log(\log(k)) )$ (which is less than $18 k \log(k)$ for any vectors that can be practically realized), uses the $p$-norm as a direct approximation of the Chebyshev norm. The second approach proposed, with runtime in $O( k \log(k) )$ (although in practice both perform similarly), uses a novel null space projection method, which extracts information from a sequence of $p$-norms to estimate the maximum value in the vector (this is equivalent to querying a small number of moments from a distribution of bounded support in order to estimate the maximum). The $p$-norm approaches are compared to one another and are shown to compute an approximation of the Viterbi path in a hidden Markov model where the transition matrix is a Toeplitz matrix; the runtime of approximating the Viterbi path is thus reduced from $O( n k^2 )$ steps to $O( n $k \log(k))$ steps in practice, and is demonstrated by inferring the U.S. unemployment rate from the S&P 500 stock index. version:2
arxiv-1508-05367 | Hidden Markov Models for Gene Sequence Classification: Classifying the VSG genes in the Trypanosoma brucei Genome | http://arxiv.org/abs/1508.05367 | id:1508.05367 author:Andrea Mesa, Sebastián Basterrech, Gustavo Guerberoff, Fernando Alvarez-Valin category:q-bio.GN cs.CE cs.LG  published:2015-07-31 summary:The article presents an application of Hidden Markov Models (HMMs) for pattern recognition on genome sequences. We apply HMM for identifying genes encoding the Variant Surface Glycoprotein (VSG) in the genomes of Trypanosoma brucei (T. brucei) and other African trypanosomes. These are parasitic protozoa causative agents of sleeping sickness and several diseases in domestic and wild animals. These parasites have a peculiar strategy to evade the host's immune system that consists in periodically changing their predominant cellular surface protein (VSG). The motivation for using patterns recognition methods to identify these genes, instead of traditional homology based ones, is that the levels of sequence identity (amino acid and DNA sequence) amongst these genes is often below of what is considered reliable in these methods. Among pattern recognition approaches, HMM are particularly suitable to tackle this problem because they can handle more naturally the determination of gene edges. We evaluate the performance of the model using different number of states in the Markov model, as well as several performance metrics. The model is applied using public genomic data. Our empirical results show that the VSG genes on T. brucei can be safely identified (high sensitivity and low rate of false positives) using HMM. version:2
