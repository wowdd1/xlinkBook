arxiv-2700-1 | Cost-Sensitive Tree of Classifiers | http://arxiv.org/pdf/1210.2771v3.pdf | author:Zhixiang Xu, Matt J. Kusner, Kilian Q. Weinberger, Minmin Chen category:stat.ML cs.LG published:2012-10-09 summary:Recently, machine learning algorithms have successfully entered large-scalereal-world industrial applications (e.g. search engines and email spamfilters). Here, the CPU cost during test time must be budgeted and accountedfor. In this paper, we address the challenge of balancing the test-time costand the classifier accuracy in a principled fashion. The test-time cost of aclassifier is often dominated by the computation required for featureextraction-which can vary drastically across eatures. We decrease thisextraction time by constructing a tree of classifiers, through which testinputs traverse along individual paths. Each path extracts different featuresand is optimized for a specific sub-partition of the input space. By onlycomputing features for inputs that benefit from them the most, our costsensitive tree of classifiers can match the high accuracies of the currentstate-of-the-art at a small fraction of the computational cost.
arxiv-2700-2 | Dynamic stochastic blockmodels: Statistical models for time-evolving networks | http://arxiv.org/pdf/1304.5974v1.pdf | author:Kevin S. Xu, Alfred O. Hero III category:cs.SI cs.LG physics.soc-ph stat.ME G.3; G.2.2 published:2013-04-22 summary:Significant efforts have gone into the development of statistical models foranalyzing data in the form of networks, such as social networks. Most existingwork has focused on modeling static networks, which represent either a singletime snapshot or an aggregate view over time. There has been recent interest instatistical modeling of dynamic networks, which are observed at multiple pointsin time and offer a richer representation of many complex phenomena. In thispaper, we propose a state-space model for dynamic networks that extends thewell-known stochastic blockmodel for static networks to the dynamic setting. Wethen propose a procedure to fit the model using a modification of the extendedKalman filter augmented with a local search. We apply the procedure to analyzea dynamic social network of email communication.
arxiv-2700-3 | Learning loopy graphical models with latent variables: Efficient methods and guarantees | http://arxiv.org/pdf/1203.3887v4.pdf | author:Animashree Anandkumar, Ragupathyraj Valluvan category:stat.ML cs.AI cs.LG math.ST stat.TH published:2012-03-17 summary:The problem of structure estimation in graphical models with latent variablesis considered. We characterize conditions for tractable graph estimation anddevelop efficient methods with provable guarantees. We consider models wherethe underlying Markov graph is locally tree-like, and the model is in theregime of correlation decay. For the special case of the Ising model, thenumber of samples $n$ required for structural consistency of our method scalesas $n=\Omega(\theta_{\min}^{-\delta\eta(\eta+1)-2}\log p)$, where p is thenumber of variables, $\theta_{\min}$ is the minimum edge potential, $\delta$ isthe depth (i.e., distance from a hidden node to the nearest observed nodes),and $\eta$ is a parameter which depends on the bounds on node and edgepotentials in the Ising model. Necessary conditions for structural consistencyunder any algorithm are derived and our method nearly matches the lower boundon sample requirements. Further, the proposed method is practical to implementand provides flexibility to control the number of latent variables and thecycle lengths in the output graph.
arxiv-2700-4 | Distributed User Profiling via Spectral Methods | http://arxiv.org/pdf/1109.3318v2.pdf | author:Dan-Cristian Tomozei, Laurent Massoulié category:cs.LG 60B20 G.3 published:2011-09-15 summary:User profiling is a useful primitive for constructing personalised services,such as content recommendation. In the present paper we investigate thefeasibility of user profiling in a distributed setting, with no centralauthority and only local information exchanges between users. We compute aprofile vector for each user (i.e., a low-dimensional vector that characterisesher taste) via spectral transformation of observed user-produced ratings foritems. Our two main contributions follow: i) We consider a low-rankprobabilistic model of user taste. More specifically, we consider that usersand items are partitioned in a constant number of classes, such that users anditems within the same class are statistically identical. We prove that withoutprior knowledge of the compositions of the classes, based solely on few randomobserved ratings (namely $O(N\log N)$ such ratings for $N$ users), we canpredict user preference with high probability for unrated items by running alocal vote among users with similar profile vectors. In addition, we provideempirical evaluations characterising the way in which spectral profilingperformance depends on the dimension of the profile space. Such evaluations areperformed on a data set of real user ratings provided by Netflix. ii) Wedevelop distributed algorithms which provably achieve an embedding of usersinto a low-dimensional space, based on spectral transformation. These involvesimple message passing among users, and provably converge to the desiredembedding. Our method essentially relies on a novel combination of gossipingand the algorithm proposed by Oja and Karhunen.
arxiv-2700-5 | Dealing with natural language interfaces in a geolocation context | http://arxiv.org/pdf/1304.5880v1.pdf | author:M. -A. Abchir, Isis Truck, Anna Pappa category:cs.CL published:2013-04-22 summary:In the geolocation field where high-level programs and low-level devicescoexist, it is often difficult to find a friendly user inter- face to configureall the parameters. The challenge addressed in this paper is to proposeintuitive and simple, thus natural lan- guage interfaces to interact withlow-level devices. Such inter- faces contain natural language processing andfuzzy represen- tations of words that facilitate the elicitation ofbusiness-level objectives in our context.
arxiv-2700-6 | Asymptotic Generalization Bound of Fisher's Linear Discriminant Analysis | http://arxiv.org/pdf/1208.3030v2.pdf | author:Wei Bian, Dacheng Tao category:stat.ML cs.LG published:2012-08-15 summary:Fisher's linear discriminant analysis (FLDA) is an important dimensionreduction method in statistical pattern recognition. It has been shown thatFLDA is asymptotically Bayes optimal under the homoscedastic Gaussianassumption. However, this classical result has the following two majorlimitations: 1) it holds only for a fixed dimensionality $D$, and thus does notapply when $D$ and the training sample size $N$ are proportionally large; 2) itdoes not provide a quantitative description on how the generalization abilityof FLDA is affected by $D$ and $N$. In this paper, we present an asymptoticgeneralization analysis of FLDA based on random matrix theory, in a settingwhere both $D$ and $N$ increase and $D/N\longrightarrow\gamma\in[0,1)$. Theobtained lower bound of the generalization discrimination power overcomes bothlimitations of the classical result, i.e., it is applicable when $D$ and $N$are proportionally large and provides a quantitative description of thegeneralization ability of FLDA in terms of the ratio $\gamma=D/N$ and thepopulation discrimination power. Besides, the discrimination power bound alsoleads to an upper bound on the generalization error of binary-classificationwith FLDA.
arxiv-2700-7 | Infinitely imbalanced binomial regression and deformed exponential families | http://arxiv.org/pdf/1303.7297v2.pdf | author:Tomonari Sei category:math.ST stat.ML stat.TH published:2013-03-29 summary:The logistic regression model is known to converge to a Poisson point processmodel if the binary response tends to infinitely imbalanced. In this paper, itis shown that this phenomenon is universal in a wide class of link functions onbinomial regression. The proof relies on the extreme value theory. For thelogit, probit and complementary log-log link functions, the intensity measureof the point process becomes an exponential family. For some other linkfunctions, deformed exponential families appear. A penalized maximum likelihoodestimator for the Poisson point process model is suggested.
arxiv-2700-8 | Multiple graph regularized protein domain ranking | http://arxiv.org/pdf/1208.3779v3.pdf | author:Jim Jing-Yan Wang, Halima Bensmail, Xin Gao category:cs.LG cs.CE cs.IR q-bio.QM published:2012-08-18 summary:Background Protein domain ranking is a fundamental task in structuralbiology. Most protein domain ranking methods rely on the pairwise comparison ofprotein domains while neglecting the global manifold structure of the proteindomain database. Recently, graph regularized ranking that exploits the globalstructure of the graph defined by the pairwise similarities has been proposed.However, the existing graph regularized ranking methods are very sensitive tothe choice of the graph model and parameters, and this remains a difficultproblem for most of the protein domain ranking methods. Results To tackle this problem, we have developed the Multiple Graphregularized Ranking algorithm, MultiG- Rank. Instead of using a single graph toregularize the ranking scores, MultiG-Rank approximates the intrinsic manifoldof protein domain distribution by combining multiple initial graphs for theregularization. Graph weights are learned with ranking scores jointly andautomatically, by alternately minimizing an ob- jective function in aniterative algorithm. Experimental results on a subset of the ASTRAL SCOPprotein domain database demonstrate that MultiG-Rank achieves a better rankingperformance than single graph regularized ranking methods and pairwisesimilarity based ranking methods. Conclusion The problem of graph model and parameter selection in graphregularized protein domain ranking can be solved effectively by combiningmultiple graphs. This aspect of generalization introduces a new frontier inapplying multiple graphs to solving protein domain ranking applications.
arxiv-2700-9 | Analytic Feature Selection for Support Vector Machines | http://arxiv.org/pdf/1304.5678v1.pdf | author:Carly Stambaugh, Hui Yang, Felix Breuer category:cs.LG stat.ML I.2.6; I.2.7 published:2013-04-20 summary:Support vector machines (SVMs) rely on the inherent geometry of a data set toclassify training data. Because of this, we believe SVMs are an excellentcandidate to guide the development of an analytic feature selection algorithm,as opposed to the more commonly used heuristic methods. We propose afilter-based feature selection algorithm based on the inherent geometry of afeature set. Through observation, we identified six geometric properties thatdiffer between optimal and suboptimal feature sets, and have statisticallysignificant correlations to classifier performance. Our algorithm is based onlogistic and linear regression models using these six geometric properties aspredictor variables. The proposed algorithm achieves excellent results on highdimensional text data sets, with features that can be organized into a handfulof feature types; for example, unigrams, bigrams or semantic structuralfeatures. We believe this algorithm is a novel and effective approach tosolving the feature selection problem for linear SVMs.
arxiv-2700-10 | Graph Estimation From Multi-attribute Data | http://arxiv.org/pdf/1210.7665v2.pdf | author:Mladen Kolar, Han Liu, Eric P. Xing category:stat.ML published:2012-10-29 summary:Many real world network problems often concern multivariate nodal attributessuch as image, textual, and multi-view feature vectors on nodes, rather thansimple univariate nodal attributes. The existing graph estimation methods builton Gaussian graphical models and covariance selection algorithms can not handlesuch data, neither can the theories developed around such methods be directlyapplied. In this paper, we propose a new principled framework for estimatinggraphs from multi-attribute data. Instead of estimating the partial correlationas in current literature, our method estimates the partial canonicalcorrelations that naturally accommodate complex nodal features.Computationally, we provide an efficient algorithm which utilizes themulti-attribute structure. Theoretically, we provide sufficient conditionswhich guarantee consistent graph recovery. Extensive simulation studiesdemonstrate performance of our method under various conditions. Furthermore, weprovide illustrative applications to uncovering gene regulatory networks fromgene and protein profiles, and uncovering brain connectivity graph fromfunctional magnetic resonance imaging data.
arxiv-2700-11 | A Survey on Multi-view Learning | http://arxiv.org/pdf/1304.5634v1.pdf | author:Chang Xu, Dacheng Tao, Chao Xu category:cs.LG published:2013-04-20 summary:In recent years, a great many methods of learning from multi-view data byconsidering the diversity of different views have been proposed. These viewsmay be obtained from multiple sources or different feature subsets. In tryingto organize and highlight similarities and differences between the variety ofmulti-view learning approaches, we review a number of representative multi-viewlearning algorithms in different areas and classify them into three groups: 1)co-training, 2) multiple kernel learning, and 3) subspace learning. Notably,co-training style algorithms train alternately to maximize the mutual agreementon two distinct views of the data; multiple kernel learning algorithms exploitkernels that naturally correspond to different views and combine kernels eitherlinearly or non-linearly to improve learning performance; and subspace learningalgorithms aim to obtain a latent subspace shared by multiple views by assumingthat the input views are generated from this latent subspace. Though there issignificant variance in the approaches to integrating multiple views to improvelearning performance, they mainly exploit either the consensus principle or thecomplementary principle to ensure the success of multi-view learning. Sinceaccessing multiple views is the fundament of multi-view learning, with theexception of study on learning a model from multiple views, it is also valuableto study how to construct multiple views and how to evaluate these views.Overall, by exploring the consistency and complementary properties of differentviews, multi-view learning is rendered more effective, more promising, and hasbetter generalization ability than single-view learning.
arxiv-2700-12 | Computational Capabilities of Random Automata Networks for Reservoir Computing | http://arxiv.org/pdf/1212.1744v2.pdf | author:David Snyder, Alireza Goudarzi, Christof Teuscher category:nlin.AO cs.NE published:2012-12-08 summary:This paper underscores the conjecture that intrinsic computation is maximalin systems at the "edge of chaos." We study the relationship between dynamicsand computational capability in Random Boolean Networks (RBN) for ReservoirComputing (RC). RC is a computational paradigm in which a trained readout layerinterprets the dynamics of an excitable component (called the reservoir) thatis perturbed by external input. The reservoir is often implemented as ahomogeneous recurrent neural network, but there has been little investigationinto the properties of reservoirs that are discrete and heterogeneous. RandomBoolean networks are generic and heterogeneous dynamical systems and here weuse them as the reservoir. An RBN is typically a closed system; to use it as areservoir we extend it with an input layer. As a consequence of perturbation,the RBN does not necessarily fall into an attractor. Computational capabilityin RC arises from a trade-off between separability and fading memory of inputs.We find the balance of these properties predictive of classification power andoptimal at critical connectivity. These results are relevant to theconstruction of devices which exploit the intrinsic dynamics of complexheterogeneous systems, such as biomolecular substrates.
arxiv-2700-13 | Hands-free Evolution of 3D-printable Objects via Eye Tracking | http://arxiv.org/pdf/1304.4889v3.pdf | author:Nick Cheney, Jeff Clune, Jason Yosinski, Hod Lipson category:cs.NE cs.HC published:2013-04-17 summary:Interactive evolution has shown the potential to create amazing and complexforms in both 2-D and 3-D settings. However, the algorithm is slow and usersquickly become fatigued. We propose that the use of eye tracking forinteractive evolution systems will both reduce user fatigue and improveevolutionary success. We describe a systematic method for testing thehypothesis that eye tracking driven interactive evolution will be a moresuccessful and easier-to-use design method than traditional interactiveevolution methods driven by mouse clicks. We provide preliminary results thatsupport the possibility of this proposal, and lay out future work toinvestigate these advantages in extensive clinical trials.
arxiv-2700-14 | Personalized Academic Research Paper Recommendation System | http://arxiv.org/pdf/1304.5457v1.pdf | author:Joonseok Lee, Kisung Lee, Jennifer G. Kim category:cs.IR cs.DL cs.LG published:2013-04-19 summary:A huge number of academic papers are coming out from a lot of conferences andjournals these days. In these circumstances, most researchers rely on key-basedsearch or browsing through proceedings of top conferences and journals to findtheir related work. To ease this difficulty, we propose a Personalized AcademicResearch Paper Recommendation System, which recommends related articles, foreach researcher, that may be interesting to her/him. In this paper, we firstintroduce our web crawler to retrieve research papers from the web. Then, wedefine similarity between two research papers based on the text similaritybetween them. Finally, we propose our recommender system developed usingcollaborative filtering methods. Our evaluation results demonstrate that oursystem recommends good quality research papers.
arxiv-2700-15 | Analytic Expressions for Stochastic Distances Between Relaxed Complex Wishart Distributions | http://arxiv.org/pdf/1304.5417v1.pdf | author:Alejandro C. Frery, Abraão D. C. Nascimento, Renato J. Cintra category:stat.ML published:2013-04-19 summary:The scaled complex Wishart distribution is a widely used model for multilookfull polarimetric SAR data whose adequacy has been attested in the literature.Classification, segmentation, and image analysis techniques which depend onthis model have been devised, and many of them employ some type ofdissimilarity measure. In this paper we derive analytic expressions for fourstochastic distances between relaxed scaled complex Wishart distributions intheir most general form and in important particular cases. Using thesedistances, inequalities are obtained which lead to new ways of deriving theBartlett and revised Wishart distances. The expressiveness of the four analyticdistances is assessed with respect to the variation of parameters. Suchdistances are then used for deriving new tests statistics, which are proved tohave asymptotic chi-square distribution. Adopting the test size as a comparisoncriterion, a sensitivity study is performed by means of Monte Carlo experimentssuggesting that the Bhattacharyya statistic outperforms all the others. Thepower of the tests is also assessed. Applications to actual data illustrate thediscrimination and homogeneity identification capabilities of these distances.
arxiv-2700-16 | A Joint Intensity and Depth Co-Sparse Analysis Model for Depth Map Super-Resolution | http://arxiv.org/pdf/1304.5319v1.pdf | author:Martin Kiechle, Simon Hawe, Martin Kleinsteuber category:cs.CV published:2013-04-19 summary:High-resolution depth maps can be inferred from low-resolution depthmeasurements and an additional high-resolution intensity image of the samescene. To that end, we introduce a bimodal co-sparse analysis model, which isable to capture the interdependency of registered intensity and depthinformation. This model is based on the assumption that the co-supports ofcorresponding bimodal image structures are aligned when computed by a suitablepair of analysis operators. No analytic form of such operators exist and wepropose a method for learning them from a set of registered training signals.This learning process is done offline and returns a bimodal analysis operatorthat is universally applicable to natural scenes. We use this to exploit thebimodal co-sparse analysis model as a prior for solving inverse problems, whichleads to an efficient algorithm for depth map super-resolution.
arxiv-2700-17 | Object Tracking in Videos: Approaches and Issues | http://arxiv.org/pdf/1304.5212v1.pdf | author:Duc Phu Chau, François Bremond, Monique Thonnat category:cs.CV published:2013-04-18 summary:Mobile object tracking has an important role in the computer visionapplications. In this paper, we use a tracked target-based taxonomy to presentthe object tracking algorithms. The tracked targets are divided into threecategories: points of interest, appearance and silhouette of mobile objects.Advantages and limitations of the tracking approaches are also analyzed to findthe future directions in the object tracking domain.
arxiv-2700-18 | Image Retrieval based on Bag-of-Words model | http://arxiv.org/pdf/1304.5168v1.pdf | author:Jialu Liu category:cs.IR cs.LG published:2013-04-18 summary:This article gives a survey for bag-of-words (BoW) or bag-of-features modelin image retrieval system. In recent years, large-scale image retrieval showssignificant potential in both industry applications and research problems. Aslocal descriptors like SIFT demonstrate great discriminative power in solvingvision problems like object recognition, image classification and annotation,more and more state-of-the-art large scale image retrieval systems are tryingto rely on them. A common way to achieve this is first quantizing localdescriptors into visual words, and then applying scalable textual indexing andretrieval schemes. We call this model as bag-of-words or bag-of-features model.The goal of this survey is to give an overview of this model and introducedifferent strategies when building the system based on this model.
arxiv-2700-19 | Multiobjective optimization in Gene Expression Programming for Dew Point | http://arxiv.org/pdf/1304.4055v2.pdf | author:Siddharth Shroff, Vipul Dabhi category:cs.NE published:2013-04-15 summary:The processes occurring in climatic change evolution and their variationsplay a major role in environmental engineering. Different techniques are usedto model the relationship between temperatures, dew point and relativehumidity. Gene expression programming is capable of modelling complex realitieswith great accuracy, allowing, at the same time, the extraction of knowledgefrom the evolved models compared to other learning algorithms. This researchaims to use Gene Expression Programming for modelling of dew point. Generally,accuracy of the model is the only objective used by selection mechanism of GEP.This will evolve large size models with low training error. To avoid thissituation, use of multiple objectives, like accuracy and size of the model arepreferred by Genetic Programming practitioners. Multi-objective problem finds aset of solutions satisfying the objectives given by decision maker.Multiobjective based GEP will be used to evolve simple models. Variousalgorithms widely used for multi objective optimization like NSGA II and SPEA 2are tested for different test cases. The results obtained thereafter gives ideathat SPEA 2 is better algorithm compared to NSGA II based on the features likeexecution time, number of solutions obtained and convergence rate. Thuscompared to models obtained by GEP, multi-objective algorithms fetch bettersolutions considering the dual objectives of fitness and size of the equation.These simple models can be used to predict dew point.
arxiv-2700-20 | Efficient network-guided multi-locus association mapping with graph cuts | http://arxiv.org/pdf/1211.2315v5.pdf | author:Chloé-Agathe Azencott, Dominik Grimm, Mahito Sugiyama, Yoshinobu Kawahara, Karsten M. Borgwardt category:stat.ML q-bio.QM published:2012-11-10 summary:As an increasing number of genome-wide association studies reveal thelimitations of attempting to explain phenotypic heritability by single geneticloci, there is growing interest for associating complex phenotypes with sets ofgenetic loci. While several methods for multi-locus mapping have been proposed,it is often unclear how to relate the detected loci to the growing knowledgeabout gene pathways and networks. The few methods that take biological pathwaysor networks into account are either restricted to investigating a limitednumber of predetermined sets of loci, or do not scale to genome-wide settings. We present SConES, a new efficient method to discover sets of genetic locithat are maximally associated with a phenotype, while being connected in anunderlying network. Our approach is based on a minimum cut reformulation of theproblem of selecting features under sparsity and connectivity constraints thatcan be solved exactly and rapidly. SConES outperforms state-of-the-art competitors in terms of runtime, scalesto hundreds of thousands of genetic loci, and exhibits higher power indetecting causal SNPs in simulation studies than existing methods. On floweringtime phenotypes and genotypes from Arabidopsis thaliana, SConES detects locithat enable accurate phenotype prediction and that are supported by theliterature. Matlab code for SConES is available athttp://webdav.tuebingen.mpg.de/u/karsten/Forschung/scones/
arxiv-2700-21 | On the Complexity of Trial and Error | http://arxiv.org/pdf/1205.1183v2.pdf | author:Xiaohui Bei, Ning Chen, Shengyu Zhang category:cs.CC cs.DS cs.LG published:2012-05-06 summary:Motivated by certain applications from physics, biochemistry, economics, andcomputer science, in which the objects under investigation are not accessiblebecause of various limitations, we propose a trial-and-error model to examinealgorithmic issues in such situations. Given a search problem with a hiddeninput, we are asked to find a valid solution, to find which we can proposecandidate solutions (trials), and use observed violations (errors), to preparefuture proposals. In accordance with our motivating applications, we considerthe fairly broad class of constraint satisfaction problems, and assume thaterrors are signaled by a verification oracle in the format of the index of aviolated constraint (with the content of the constraint still hidden). Our discoveries are summarized as follows. On one hand, despite the seeminglyvery little information provided by the verification oracle, efficientalgorithms do exist for a number of important problems. For the Nash, Core,Stable Matching, and SAT problems, the unknown-input versions are as hard asthe corresponding known-input versions, up to a factor of polynomial. Wefurther give almost tight bounds on the latter two problems' trialcomplexities. On the other hand, there are problems whose complexities aresubstantially increased in the unknown-input model. In particular, notime-efficient algorithms exist (under standard hardness assumptions) for GraphIsomorphism and Group Isomorphism problems. The tools used to achieve theseresults include order theory, strong ellipsoid method, and some non-standardreductions. Our model investigates the value of information, and our results demonstratethat the lack of input information can introduce various levels of extradifficulty. The model exhibits intimate connections with (and we hope can alsoserve as a useful supplement to) certain existing learning and complexitytheories.
arxiv-2700-22 | Polygon Matching and Indexing Under Affine Transformations | http://arxiv.org/pdf/1304.4994v1.pdf | author:Edgar Chávez, Ana C. Chávez-Cáliz, Jorge L. López-López category:cs.CV 51N10 published:2013-04-18 summary:Given a collection $\{Z_1,Z_2,\ldots,Z_m\}$ of $n$-sided polygons in theplane and a query polygon $W$ we give algorithms to find all $Z_\ell$ such that$W=f(Z_\ell)$ with $f$ an unknown similarity transformation in time independentof the size of the collection. If $f$ is a known affine transformation, we showhow to find all $Z_\ell$ such that $W=f(Z_\ell)$ in $O(n+\log(m))$ time. For a pair $W,W^\prime$ of polygons we can find all the pairs$Z_\ell,Z_{\ell^\prime}$ such that $W=f(Z_\ell)$ and$W^\prime=f(Z_{\ell^\prime})$ for an unknown affine transformation $f$ in$O(m+n)$ time. For the case of triangles we also give bounds for the problem of matchingtriangles with variable vertices, which is equivalent to affine matchingtriangles in noisy conditions.
arxiv-2700-23 | Learning using Local Membership Queries | http://arxiv.org/pdf/1211.0996v2.pdf | author:Pranjal Awasthi, Vitaly Feldman, Varun Kanade category:cs.LG cs.AI published:2012-11-05 summary:We introduce a new model of membership query (MQ) learning, where thelearning algorithm is restricted to query points that are \emph{close} torandom examples drawn from the underlying distribution. The learning model isintermediate between the PAC model (Valiant, 1984) and the PAC+MQ model (wherethe queries are allowed to be arbitrary points). Membership query algorithms are not popular among machine learningpractitioners. Apart from the obvious difficulty of adaptively queryinglabelers, it has also been observed that querying \emph{unnatural} points leadsto increased noise from human labelers (Lang and Baum, 1992). This motivatesour study of learning algorithms that make queries that are close to examplesgenerated from the data distribution. We restrict our attention to functions defined on the $n$-dimensional Booleanhypercube and say that a membership query is local if its Hamming distance fromsome example in the (random) training data is at most $O(\log(n))$. We show thefollowing results in this model: (i) The class of sparse polynomials (with coefficients in R) over $\{0,1\}^n$is polynomial time learnable under a large class of \emph{locally smooth}distributions using $O(\log(n))$-local queries. This class also includes theclass of $O(\log(n))$-depth decision trees. (ii) The class of polynomial-sized decision trees is polynomial timelearnable under product distributions using $O(\log(n))$-local queries. (iii) The class of polynomial size DNF formulas is learnable under theuniform distribution using $O(\log(n))$-local queries in time$n^{O(\log(\log(n)))}$. (iv) In addition we prove a number of results relating the proposed model tothe traditional PAC model and the PAC+MQ model.
arxiv-2700-24 | The Mahalanobis distance for functional data with applications to classification | http://arxiv.org/pdf/1304.4786v1.pdf | author:Esdras Joseph, Pedro Galeano, Rosa E. Lillo category:math.ST stat.CO stat.ME stat.ML stat.TH published:2013-04-17 summary:This paper presents a general notion of Mahalanobis distance for functionaldata that extends the classical multivariate concept to situations where theobserved data are points belonging to curves generated by a stochastic process.More precisely, a new semi-distance for functional observations that generalizethe usual Mahalanobis distance for multivariate datasets is introduced. Forthat, the development uses a regularized square root inverse operator inHilbert spaces. Some of the main characteristics of the functional Mahalanobissemi-distance are shown. Afterwards, new versions of several well knownfunctional classification procedures are developed using the Mahalanobisdistance for functional data as a measure of proximity between functionalobservations. The performance of several well known functional classificationprocedures are compared with those methods used in conjunction with theMahalanobis distance for functional data, with positive results, through aMonte Carlo study and the analysis of two real data examples.
arxiv-2700-25 | Robust Noise Filtering in Image Sequences | http://arxiv.org/pdf/1304.4765v1.pdf | author:Soumaya Hichri, Faouzi Benzarti, Hamid Amiri category:cs.CV published:2013-04-17 summary:Image sequences filtering have recently become a very important technicalproblem especially with the advent of new technology in multimedia and videosystems applications. Often image sequences are corrupted by some amount ofnoise introduced by the image sensor and therefore inherently present in theimaging process. The main problem in the image sequences is how to deal withspatio-temporal and non stationary signals. In this paper, we propose a robustmethod for noise removal of image sequence based on coupled spatial andtemporal anisotropic diffusion. The idea is to achieve an adaptive smoothing inboth spatial and temporal directions, by solving a nonlinear diffusionequation. This allows removing noise while preserving all spatial and temporaldiscontinuities
arxiv-2700-26 | Automated Switching System for Skin Pixel Segmentation in Varied Lighting | http://arxiv.org/pdf/1304.4711v1.pdf | author:Ankit Chaudhary, Ankur Gupta category:cs.CV published:2013-04-17 summary:In Computer Vision, colour-based spatial techniquesoften assume a static skincolour model. However, skin colour perceived by a camera can change whenlighting changes. In common real environment multiple light sources impinge onthe skin. Moreover, detection techniques may vary when the image under study istaken under different lighting condition than the one that was earlier underconsideration. Therefore, for robust skin pixel detection, a dynamic skincolour model that can cope with the changes must be employed. This paper showsthat skin pixel detection in a digital colour image can be significantlyimproved by employing automated colour space switching methods. In the root ofthe switching technique which is employed in this study, lies the statisticalmean of value of the skin pixels in the image which in turn has been derivedfrom the Value, measures as a third component of the HSV. The study is based onexperimentations on a set of images where capture time conditions varying fromhighly illuminated to almost dark.
arxiv-2700-27 | Tracking of Fingertips and Centres of Palm using KINECT | http://arxiv.org/pdf/1304.4662v1.pdf | author:J. L. Raheja, A. Chaudhary, K Singal category:cs.CV published:2013-04-17 summary:Hand Gesture is a popular way to interact or control machines and it has beenimplemented in many applications. The geometry of hand is such that it is hardto construct in virtual environment and control the joints but thefunctionality and DOF encourage researchers to make a hand like instrument.This paper presents a novel method for fingertips detection and centres ofpalms detection distinctly for both hands using MS KINECT in 3D from the inputimage. KINECT facilitates us by providing the depth information of foregroundobjects. The hands were segmented using the depth vector and centres of palmswere detected using distance transformation on inverse image. This result wouldbe used to feed the inputs to the robotic hands to emulate human handsoperation.
arxiv-2700-28 | A Health Monitoring System for Elder and Sick Persons | http://arxiv.org/pdf/1304.4652v1.pdf | author:Ankit Chaudhary, Jagdish L. Raheja category:cs.CV cs.HC published:2013-04-17 summary:This paper discusses a vision based health monitoring system which would bevery easy in use and deployment. Elder and sick people who are not able to talkor walk they are dependent on other human beings for their daily needs and needcontinuous monitoring. The developed system provides facility to the sick orelder person to describe his or her need to their caretaker in lingualdescription by showing particular hand gesture with the developed system. Thissystem uses fingertip detection technique for gesture extraction and artificialneural network for gesture classification and recognition. The system is ableto work in different light conditions and can be connected to different devicesto announce users need on a distant location.
arxiv-2700-29 | A Hash based Approach for Secure Keyless Steganography in Lossless RGB Images | http://arxiv.org/pdf/1211.5614v5.pdf | author:Ankit Chaudhary, J. Vasavada, J. L. Raheja, S. Kumar, M. Sharma category:cs.CR cs.CV cs.MM published:2012-11-23 summary:This paper proposes an improved steganography approach for hiding textmessages in lossless RGB images. The objective of this work is to increase thesecurity level and to improve the storage capacity with compression techniques.The security level is increased by randomly distributing the text message overthe entire image instead of clustering within specific image portions. Storagecapacity is increased by utilizing all the color channels for storinginformation and providing the source text message compression. The degradationof the images can be minimized by changing only one least significant bit percolor channel for hiding the message, incurring a very little change in theoriginal image. Using steganography alone with simple LSB has a potentialproblem that the secret message is easily detectable from the histogramanalysis method. To improve the security as well as the image embeddingcapacity indirectly, a compression based scheme is introduced. Various testshave been done to check the storage capacity and message distribution. Thesetestes show the superiority of the proposed approach with respect to otherexisting approaches.
arxiv-2700-30 | Easy and hard functions for the Boolean hidden shift problem | http://arxiv.org/pdf/1304.4642v1.pdf | author:Andrew M. Childs, Robin Kothari, Maris Ozols, Martin Roetteler category:quant-ph cs.CC cs.LG published:2013-04-16 summary:We study the quantum query complexity of the Boolean hidden shift problem.Given oracle access to f(x+s) for a known Boolean function f, the task is todetermine the n-bit string s. The quantum query complexity of this problemdepends strongly on f. We demonstrate that the easiest instances of thisproblem correspond to bent functions, in the sense that an exact one-queryalgorithm exists if and only if the function is bent. We partially characterizethe hardest instances, which include delta functions. Moreover, we show thatthe problem is easy for random functions, since two queries suffice. Ouralgorithm for random functions is based on performing the pretty goodmeasurement on several copies of a certain state; its analysis relies on theFourier transform. We also use this approach to improve the quantum rejectionsampling approach to the Boolean hidden shift problem.
arxiv-2700-31 | Speckle Reduction in Polarimetric SAR Imagery with Stochastic Distances and Nonlocal Means | http://arxiv.org/pdf/1304.4634v1.pdf | author:Leonardo Torres, Sidnei J. S. Sant'Anna, Corina C. Freitas, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML published:2013-04-16 summary:This paper presents a technique for reducing speckle in PolarimetricSynthetic Aperture Radar (PolSAR) imagery using Nonlocal Means and astatistical test based on stochastic divergences. The main objective is toselect homogeneous pixels in the filtering area through statistical testsbetween distributions. This proposal uses the complex Wishart model to describePolSAR data, but the technique can be extended to other models. The weights ofthe location-variant linear filter are function of the p-values of tests whichverify the hypothesis that two samples come from the same distribution and,therefore, can be used to compute a local mean. The test stems from the familyof (h-phi) divergences which originated in Information Theory. This noveltechnique was compared with the Boxcar, Refined Lee and IDAN filters. Imagequality assessment methods on simulated and real data are employed to validatethe performance of this approach. We show that the proposed filter alsoenhances the polarimetric entropy and preserves the scattering information ofthe targets.
arxiv-2700-32 | PAC Quasi-automatizability of Resolution over Restricted Distributions | http://arxiv.org/pdf/1304.4633v1.pdf | author:Brendan Juba category:cs.DS cs.LG cs.LO published:2013-04-16 summary:We consider principled alternatives to unsupervised learning in data miningby situating the learning task in the context of the subsequent analysis task.Specifically, we consider a query-answering (hypothesis-testing) task: In thecombined task, we decide whether an input query formula is satisfied over abackground distribution by using input examples directly, rather than invokinga two-stage process in which (i) rules over the distribution are learned by anunsupervised learning algorithm and (ii) a reasoning algorithm decides whetheror not the query formula follows from the learned rules. In a previous work(2013), we observed that the learning task could satisfy numerous desirablecriteria in this combined context -- effectively matching what could beachieved by agnostic learning of CNFs from partial information -- that are notknown to be achievable directly. In this work, we show that likewise, there arereasoning tasks that are achievable in such a combined context that are notknown to be achievable directly (and indeed, have been seriously conjectured tobe impossible, cf. (Alekhnovich and Razborov, 2008)). Namely, we test for aresolution proof of the query formula of a given size in quasipolynomial time(that is, "quasi-automatizing" resolution). The learning setting we consider isa partial-information, restricted-distribution setting that generalizeslearning parities over the uniform distribution from partial information,another task that is known not to be achievable directly in various models (cf.(Ben-David and Dichterman, 1998) and (Michael, 2010)).
arxiv-2700-33 | Learning Heteroscedastic Models by Convex Programming under Group Sparsity | http://arxiv.org/pdf/1304.4549v1.pdf | author:Arnak S. Dalalyan, Mohamed Hebiri, Katia Méziani, Joseph Salmon category:stat.ML published:2013-04-16 summary:Popular sparse estimation methods based on $\ell_1$-relaxation, such as theLasso and the Dantzig selector, require the knowledge of the variance of thenoise in order to properly tune the regularization parameter. This constitutesa major obstacle in applying these methods in several frameworks---such as timeseries, random fields, inverse problems---for which the noise is rarelyhomoscedastic and its level is hard to know in advance. In this paper, wepropose a new approach to the joint estimation of the conditional mean and theconditional variance in a high-dimensional (auto-) regression setting. Anattractive feature of the proposed estimator is that it is efficientlycomputable even for very large scale problems by solving a second-order coneprogram (SOCP). We present theoretical analysis and numerical results assessingthe performance of the proposed procedure.
arxiv-2700-34 | Heterogeneous patterns enhancing static and dynamic texture classification | http://arxiv.org/pdf/1304.4535v1.pdf | author:Núbia Rosa da Silva, Odemir Martinez Bruno category:cs.CV published:2013-04-16 summary:Some mixtures, such as colloids like milk, blood, and gelatin, havehomogeneous appearance when viewed with the naked eye, however, to observe themat the nanoscale is possible to understand the heterogeneity of its components.The same phenomenon can occur in pattern recognition in which it is possible tosee heterogeneous patterns in texture images. However, current methods oftexture analysis can not adequately describe such heterogeneous patterns.Common methods used by researchers analyse the image information in a globalway, taking all its features in an integrated manner. Furthermore, multi-scaleanalysis verifies the patterns at different scales, but still preserving thehomogeneous analysis. On the other hand various methods use textons torepresent the texture, breaking texture down into its smallest unit. To tacklethis problem, we propose a method to identify texture patterns not small astextons at distinct scales enhancing the separability among different types oftexture. We find sub patterns of texture according to the scale and then groupsimilar patterns for a more refined analysis. Tests were performed in fourstatic texture databases and one dynamic one. Results show that our methodprovides better classification rate compared with conventional approaches bothin static and in dynamic texture.
arxiv-2700-35 | Sentiment Analysis : A Literature Survey | http://arxiv.org/pdf/1304.4520v1.pdf | author:Subhabrata Mukherjee, Pushpak Bhattacharyya category:cs.CL published:2013-04-16 summary:Our day-to-day life has always been influenced by what people think. Ideasand opinions of others have always affected our own opinions. The explosion ofWeb 2.0 has led to increased activity in Podcasting, Blogging, Tagging,Contributing to RSS, Social Bookmarking, and Social Networking. As a resultthere has been an eruption of interest in people to mine these vast resourcesof data for opinions. Sentiment Analysis or Opinion Mining is the computationaltreatment of opinions, sentiments and subjectivity of text. In this report, wetake a look at the various challenges and applications of Sentiment Analysis.We will discuss in details various approaches to perform a computationaltreatment of sentiments and opinions. Various supervised or data-driventechniques to SA like Na\"ive Byes, Maximum Entropy, SVM, and Voted Perceptronswill be discussed and their strengths and drawbacks will be touched upon. Wewill also see a new dimension of analyzing sentiments by Cognitive Psychologymainly through the work of Janyce Wiebe, where we will see ways to detectsubjectivity, perspective in narrative and understanding the discoursestructure. We will also study some specific topics in Sentiment Analysis andthe contemporary works in those areas.
arxiv-2700-36 | A generalized risk approach to path inference based on hidden Markov models | http://arxiv.org/pdf/1007.3622v4.pdf | author:Jüri Lember, Alexey A. Koloydenko category:stat.ML cs.LG stat.CO published:2010-07-21 summary:Motivated by the unceasing interest in hidden Markov models (HMMs), thispaper re-examines hidden path inference in these models, using primarily arisk-based framework. While the most common maximum a posteriori (MAP), orViterbi, path estimator and the minimum error, or Posterior Decoder (PD), havelong been around, other path estimators, or decoders, have been either onlyhinted at or applied more recently and in dedicated applications generallyunfamiliar to the statistical learning community. Over a decade ago, however, afamily of algorithmically defined decoders aiming to hybridize the two standardones was proposed (Brushe et al., 1998). The present paper gives a carefulanalysis of this hybridization approach, identifies several problems and issueswith it and other previously proposed approaches, and proposes practicalresolutions of those. Furthermore, simple modifications of the classicalcriteria for hidden path recognition are shown to lead to a new class ofdecoders. Dynamic programming algorithms to compute these decoders in the usualforward-backward manner are presented. A particularly interesting subclass ofsuch estimators can be also viewed as hybrids of the MAP and PD estimators.Similar to previously proposed MAP-PD hybrids, the new class is parameterizedby a small number of tunable parameters. Unlike their algorithmic predecessors,the new risk-based decoders are more clearly interpretable, and, mostimportantly, work "out of the box" in practice, which is demonstrated on somereal bioinformatics tasks and data. Some further generalizations andapplications are discussed in conclusion.
arxiv-2700-37 | Sparse Coding and Dictionary Learning for Symmetric Positive Definite Matrices: A Kernel Approach | http://arxiv.org/pdf/1304.4344v1.pdf | author:Mehrtash T. Harandi, Conrad Sanderson, Richard Hartley, Brian C. Lovell category:cs.LG cs.CV stat.ML published:2013-04-16 summary:Recent advances suggest that a wide range of computer vision problems can beaddressed more appropriately by considering non-Euclidean geometry. This papertackles the problem of sparse coding and dictionary learning in the space ofsymmetric positive definite matrices, which form a Riemannian manifold. Withthe aid of the recently introduced Stein kernel (related to a symmetric versionof Bregman matrix divergence), we propose to perform sparse coding by embeddingRiemannian manifolds into reproducing kernel Hilbert spaces. This leads to aconvex and kernel version of the Lasso problem, which can be solvedefficiently. We furthermore propose an algorithm for learning a Riemanniandictionary (used for sparse coding), closely tied to the Stein kernel.Experiments on several classification tasks (face recognition, textureclassification, person re-identification) show that the proposed sparse codingapproach achieves notable improvements in discrimination accuracy, incomparison to state-of-the-art methods such as tensor sparse coding, Riemannianlocality preserving projection, and symmetry-driven accumulation of localfeatures.
arxiv-2700-38 | Why Size Matters: Feature Coding as Nystrom Sampling | http://arxiv.org/pdf/1301.5348v2.pdf | author:Oriol Vinyals, Yangqing Jia, Trevor Darrell category:cs.LG cs.CV published:2013-01-15 summary:Recently, the computer vision and machine learning community has been infavor of feature extraction pipelines that rely on a coding step followed by alinear classifier, due to their overall simplicity, well understood propertiesof linear classifiers, and their computational efficiency. In this paper wepropose a novel view of this pipeline based on kernel methods and Nystromsampling. In particular, we focus on the coding of a data point with a localrepresentation based on a dictionary with fewer elements than the number ofdata points, and view it as an approximation to the actual function that wouldcompute pair-wise similarity to all data points (often too many to compute inpractice), followed by a Nystrom sampling step to select a subset of all datapoints. Furthermore, since bounds are known on the approximation power of Nystromsampling as a function of how many samples (i.e. dictionary size) we consider,we can derive bounds on the approximation of the exact (but expensive tocompute) kernel matrix, and use it as a proxy to predict accuracy as a functionof the dictionary size, which has been observed to increase but also tosaturate as we increase its size. This model may help explaining the positiveeffect of the codebook size and justifying the need to stack more layers (oftenreferred to as deep learning), as flat models empirically saturate as we addmore complexity.
arxiv-2700-39 | Local Structure Matching Driven by Joint-Saliency-Structure Adaptive Kernel Regression | http://arxiv.org/pdf/1302.0494v4.pdf | author:Binjie Qin, Zhuangming Shen, Zien Zhou, Jiawei Zhou, Jiuai Sun, Hui Zhang, Mingxing Hu, Yisong Lv category:cs.CV published:2013-02-03 summary:For nonrigid image registration, matching the particular structures (or theoutliers) that have missing correspondence and/or local large deformations, canbe more difficult than matching the common structures with small deformationsin the two images. Most existing works depend heavily on the outliersegmentation to remove the outlier effect in the registration. Moreover, theseworks do not handle simultaneously the missing correspondences and local largedeformations. In this paper, we defined the nonrigid image registration as alocal adaptive kernel regression which locally reconstruct the moving image'sdense deformation vectors from the sparse deformation vectors in themulti-resolution block matching. The kernel function of the kernel regressionadapts its shape and orientation to the reference image's structure to gathermore deformation vector samples of the same structure for the iterativeregression computation, whereby the moving image's local deformations could becompliant with the reference image's local structures. To estimate the localdeformations around the outliers, we use joint saliency map that highlights thecorresponding saliency structures (called Joint Saliency Structures, JSSs) inthe two images to guide the dense deformation reconstruction by emphasizingthose JSSs' sparse deformation vectors in the kernel regression. Theexperimental results demonstrate that by using local JSS adaptive kernelregression, the proposed method achieves almost the best performance inalignment of all challenging image pairs with outlier structures compared withother five state-of-the-art nonrigid registration algorithms.
arxiv-2700-40 | Shadow Estimation Method for "The Episolar Constraint: Monocular Shape from Shadow Correspondence" | http://arxiv.org/pdf/1304.4112v1.pdf | author:Austin Abrams, Chris Hawley, Kylia Miskell, Adina Stoica, Nathan Jacobs, Robert Pless category:cs.CV published:2013-04-15 summary:Recovering shadows is an important step for many vision algorithms. Currentapproaches that work with time-lapse sequences are limited to simplethresholding heuristics. We show these approaches only work with very carefultuning of parameters, and do not work well for long-term time-lapse sequencestaken over the span of many months. We introduce a parameter-free expectationmaximization approach which simultaneously estimates shadows, albedo, surfacenormals, and skylight. This approach is more accurate than previous methods,works over both very short and very long sequences, and is robust to theeffects of nonlinear camera response. Finally, we demonstrate that the shadowmasks derived through this algorithm substantially improve the performance ofsun-based photometric stereo compared to earlier shadow mask estimation.
arxiv-2700-41 | Link Prediction with Social Vector Clocks | http://arxiv.org/pdf/1304.4058v1.pdf | author:Conrad Lee, Bobo Nick, Ulrik Brandes, Pádraig Cunningham category:cs.SI physics.soc-ph stat.ML published:2013-04-15 summary:State-of-the-art link prediction utilizes combinations of complex featuresderived from network panel data. We here show that computationally lessexpensive features can achieve the same performance in the common scenario inwhich the data is available as a sequence of interactions. Our features arebased on social vector clocks, an adaptation of the vector-clock conceptintroduced in distributed computing to social interaction networks. In fact,our experiments suggest that by taking into account the order and spacing ofinteractions, social vector clocks exploit different aspects of link formationso that their combination with previous approaches yields the most accuratepredictor to date.
arxiv-2700-42 | Coordinating metaheuristic agents with swarm intelligence | http://arxiv.org/pdf/1304.4051v1.pdf | author:Mehmet Emin Aydin category:cs.MA cs.NE published:2013-04-15 summary:Coordination of multi agent systems remains as a problem since there is noprominent method to completely solve this problem. Metaheuristic agents arespecific implementations of multi-agent systems, which imposes working togetherto solve optimisation problems with metaheuristic algorithms. The idea borrowedfrom swarm intelligence seems working much better than those implementationssuggested before. This paper reports the performance of swarms of simulatedannealing agents collaborating with particle swarm optimization algorithm. Theproposed approach is implemented for multidimensional knapsack problem and hasresulted much better than some other works published before.
arxiv-2700-43 | Multispectral Spatial Characterization: Application to Mitosis Detection in Breast Cancer Histopathology | http://arxiv.org/pdf/1304.4041v1.pdf | author:H. Irshad, A. Gouaillard, L. Roux, D. Racoceanu category:cs.CV published:2013-04-15 summary:Accurate detection of mitosis plays a critical role in breast cancerhistopathology. Manual detection and counting of mitosis is tedious and subjectto considerable inter- and intra-reader variations. Multispectral imaging is arecent medical imaging technology, proven successful in increasing thesegmentation accuracy in other fields. This study aims at improving theaccuracy of mitosis detection by developing a specific solution usingmultispectral and multifocal imaging of breast cancer histopathological data.We propose to enable clinical routine-compliant quality of mitosisdiscrimination from other objects. The proposed framework includescomprehensive analysis of spectral bands and z-stack focus planes, detection ofexpected mitotic regions (candidates) in selected focus planes and spectralbands, computation of multispectral spatial features for each candidate,selection of multispectral spatial features and a study of differentstate-of-the-art classification methods for candidates classification asmitotic or non mitotic figures. This framework has been evaluated on MITOSmultispectral medical dataset and achieved 60% detection rate and 57%F-Measure. Our results indicate that multispectral spatial features have moreinformation for mitosis classification in comparison with white spectral bandfeatures, being therefore a very promising exploration area to improve thequality of the diagnosis assistance in histopathology.
arxiv-2700-44 | Identifying cancer subtypes in glioblastoma by combining genomic, transcriptomic and epigenomic data | http://arxiv.org/pdf/1304.3577v2.pdf | author:Richard S. Savage, Zoubin Ghahramani, Jim E. Griffin, Paul Kirk, David L. Wild category:q-bio.GN stat.ML published:2013-04-12 summary:We present a nonparametric Bayesian method for disease subtype discovery inmulti-dimensional cancer data. Our method can simultaneously analyse a widerange of data types, allowing for both agreement and disagreement between theirunderlying clustering structure. It includes feature selection and infers themost likely number of disease subtypes, given the data. We apply the method to 277 glioblastoma samples from The Cancer Genome Atlas,for which there are gene expression, copy number variation, methylation andmicroRNA data. We identify 8 distinct consensus subtypes and study theirprognostic value for death, new tumour events, progression and recurrence. Theconsensus subtypes are prognostic of tumour recurrence (log-rank p-value of$3.6 \times 10^{-4}$ after correction for multiple hypothesis tests). This isdriven principally by the methylation data (log-rank p-value of $2.0 \times10^{-3}$) but the effect is strengthened by the other 3 data types,demonstrating the value of integrating multiple data types. Of particular note is a subtype of 47 patients characterised by very lowlevels of methylation. This subtype has very low rates of tumour recurrence andno new events in 10 years of follow up. We also identify a small geneexpression subtype of 6 patients that shows particularly poor survivaloutcomes. Additionally, we note a consensus subtype that showly a highlydistinctive data signature and suggest that it is therefore a biologicallydistinct subtype of glioblastoma. The code is available from https://sites.google.com/site/multipledatafusion/
arxiv-2700-45 | GPU Acclerated Automated Feature Extraction from Satellite Images | http://arxiv.org/pdf/1304.3992v1.pdf | author:K. Phani Tejaswi, D. Shanmukha Rao, Thara Nair, A. V. V. Prasad category:cs.DC cs.CV published:2013-04-15 summary:The availability of large volumes of remote sensing data insists on higherdegree of automation in feature extraction, making it a need of the hour.Thehuge quantum of data that needs to be processed entails accelerated processingto be enabled.GPUs, which were originally designed to provide efficientvisualization, are being massively employed for computation intensive parallelprocessing environments. Image processing in general and hence automatedfeature extraction, is highly computation intensive, where performanceimprovements have a direct impact on societal needs. In this context, analgorithm has been formulated for automated feature extraction from apanchromatic or multispectral image based on image processing techniques. TwoLaplacian of Guassian (LoG) masks were applied on the image individuallyfollowed by detection of zero crossing points and extracting the pixels basedon their standard deviation with the surrounding pixels. The two extractedimages with different LoG masks were combined together which resulted in animage with the extracted features and edges. Finally the user is at liberty toapply the image smoothing step depending on the noise content in the extractedimage. The image is passed through a hybrid median filter to remove the saltand pepper noise from the image. This paper discusses the aforesaid algorithmfor automated feature extraction, necessity of deployment of GPUs for the same;system-level challenges and quantifies the benefits of integrating GPUs in suchenvironment. The results demonstrate that substantial enhancement inperformance margin can be achieved with the best utilization of GPU resourcesand an efficient parallelization strategy. Performance results in comparisonwith the conventional computing scenario have provided a speedup of 20x, onrealization of this parallelizing strategy.
arxiv-2700-46 | Managing sparsity, time, and quality of inference in topic models | http://arxiv.org/pdf/1210.7053v2.pdf | author:Khoat Than, Tu Bao Ho category:stat.ML cs.AI cs.CV stat.ME published:2012-10-26 summary:Inference is an integral part of probabilistic topic models, but is oftennon-trivial to derive an efficient algorithm for a specific model. It is evenmuch more challenging when we want to find a fast inference algorithm whichalways yields sparse latent representations of documents. In this article, weintroduce a simple framework for inference in probabilistic topic models,denoted by FW. This framework is general and flexible enough to be easilyadapted to mixture models. It has a linear convergence rate, offers an easy wayto incorporate prior knowledge, and provides us an easy way to directly tradeoff sparsity against quality and time. We demonstrate the goodness andflexibility of FW over existing inference methods by a number of tasks.Finally, we show how inference in topic models with nonconjugate priors can bedone efficiently.
arxiv-2700-47 | Single View Depth Estimation from Examples | http://arxiv.org/pdf/1304.3915v1.pdf | author:Tal Hassner, Ronen Basri category:cs.CV 68T45 published:2013-04-14 summary:We describe a non-parametric, "example-based" method for estimating the depthof an object, viewed in a single photo. Our method consults a database ofexample 3D geometries, searching for those which look similar to the object inthe photo. The known depths of the selected database objects act as shapepriors which constrain the process of estimating the object's depth. We showhow this process can be performed by optimizing a well defined targetlikelihood function, via a hard-EM procedure. We address the problem ofrepresenting the (possibly infinite) variability of viewing conditions with afinite (and often very small) example set, by proposing an on-the-fly exampleupdate scheme. We further demonstrate the importance of non-stationarity inavoiding misleading examples when estimating structured shapes. We evaluate ourmethod and present both qualitative as well as quantitative results forchallenging object classes. Finally, we show how this same technique may bereadily applied to a number of related problems. These include the novel taskof estimating the occluded depth of an object's backside and the task oftailoring custom fitting image-maps for input depths.
arxiv-2700-48 | An accelerated CLPSO algorithm | http://arxiv.org/pdf/1304.3892v1.pdf | author:Muhammad Omer Bin Saeed, Muhammad Saqib Sohail, Syed Zeeshan Rizvi, Mobien Shoaib, Asrar Ul Haq Sheikh category:cs.NE published:2013-04-14 summary:The particle swarm approach provides a low complexity solution to theoptimization problem among various existing heuristic algorithms. Recentadvances in the algorithm resulted in improved performance at the cost ofincreased computational complexity, which is undesirable. Literature shows thatthe particle swarm optimization algorithm based on comprehensive learningprovides the best complexity-performance trade-off. We show how to reduce thecomplexity of this algorithm further, with a slight but acceptable performanceloss. This enhancement allows the application of the algorithm in time criticalapplications, such as, real-time tracking, equalization etc.
arxiv-2700-49 | Automatic case acquisition from texts for process-oriented case-based reasoning | http://arxiv.org/pdf/1304.3879v1.pdf | author:Valmi Dufour-Lussier, Florence Le Ber, Jean Lieber, Emmanuel Nauer category:cs.AI cs.CL published:2013-04-14 summary:This paper introduces a method for the automatic acquisition of a rich caserepresentation from free text for process-oriented case-based reasoning. Caseengineering is among the most complicated and costly tasks in implementing acase-based reasoning system. This is especially so for process-orientedcase-based reasoning, where more expressive case representations are generallyused and, in our opinion, actually required for satisfactory case adaptation.In this context, the ability to acquire cases automatically from proceduraltexts is a major step forward in order to reason on processes. We thereforedetail a methodology that makes case acquisition from processes described asfree text possible, with special attention given to assembly instruction texts.This methodology extends the techniques we used to extract actions from cookingrecipes. We argue that techniques taken from natural language processing arerequired for this task, and that they give satisfactory results. An evaluationbased on our implemented prototype extracting workflows from recipe texts isprovided.
arxiv-2700-50 | A New Homogeneity Inter-Clusters Measure in SemiSupervised Clustering | http://arxiv.org/pdf/1304.3840v1.pdf | author:Badreddine Meftahi, Ourida Ben Boubaker Saidi category:cs.LG published:2013-04-13 summary:Many studies in data mining have proposed a new learning calledsemi-Supervised. Such type of learning combines unlabeled and labeled datawhich are hard to obtain. However, in unsupervised methods, the only unlabeleddata are used. The problem of significance and the effectiveness ofsemi-supervised clustering results is becoming of main importance. This paperpursues the thesis that muchgreater accuracy can be achieved in such clusteringby improving the similarity computing. Hence, we introduce a new approach ofsemisupervised clustering using an innovative new homogeneity measure ofgenerated clusters. Our experimental results demonstrate significantly improvedaccuracy as a result.
arxiv-2700-51 | Solving Linear Equations Using a Jacobi Based Time-Variant Adaptive Hybrid Evolutionary Algorithm | http://arxiv.org/pdf/1304.3792v1.pdf | author:A. R. M. Jalal Uddin Jamali, M. M. A. Hashem, Md. Bazlar Rahman category:cs.NE published:2013-04-13 summary:Large set of linear equations, especially for sparse and structuredcoefficient (matrix) equations, solutions using classical methods becomearduous. And evolutionary algorithms have mostly been used to solve variousoptimization and learning problems. Recently, hybridization of classicalmethods (Jacobi method and Gauss-Seidel method) with evolutionary computationtechniques have successfully been applied in linear equation solving. In theboth above hybrid evolutionary methods, uniform adaptation (UA) techniques areused to adapt relaxation factor. In this paper, a new Jacobi Based Time-VariantAdaptive (JBTVA) hybrid evolutionary algorithm is proposed. In this algorithm,a Time-Variant Adaptive (TVA) technique of relaxation factor is introducedaiming at both improving the fine local tuning and reducing the disadvantage ofuniform adaptation of relaxation factors. This algorithm integrates the Jacobibased SR method with time variant adaptive evolutionary algorithm. Theconvergence theorems of the proposed algorithm are proved theoretically. Andthe performance of the proposed algorithm is compared with JBUA hybridevolutionary algorithm and classical methods in the experimental domain. Theproposed algorithm outperforms both the JBUA hybrid algorithm and classicalmethods in terms of convergence speed and effectiveness.
arxiv-2700-52 | Improving Generalization Ability of Genetic Programming: Comparative Study | http://arxiv.org/pdf/1304.3779v1.pdf | author:Tejashvi R. Naik, Vipul K. Dabhi category:cs.NE published:2013-04-13 summary:In the field of empirical modeling using Genetic Programming (GP), it isimportant to evolve solution with good generalization ability. Generalizationability of GP solutions get affected by two important issues: bloat andover-fitting. Bloat is uncontrolled growth of code without any gain in fitnessand important issue in GP. We surveyed and classified existing literaturerelated to different techniques used by GP research community to deal with theissue of bloat. Moreover, the classifications of different bloat controlapproaches and measures for bloat are discussed. Next, we tested four bloatcontrol methods: Tarpeian, double tournament, lexicographic parsimony pressurewith direct bucketing and ratio bucketing on six different problems andidentified where each bloat control method performs well on per problem basis.Based on the analysis of each method, we combined two methods: doubletournament (selection method) and Tarpeian method (works before evaluation) toavoid bloated solutions and compared with the results obtained from individualperformance of double tournament method. It was found that the results wereimproved with this combination of two methods.
arxiv-2700-53 | An Improved ACS Algorithm for the Solutions of Larger TSP Problems | http://arxiv.org/pdf/1304.3763v1.pdf | author:Md. Rakib Hassan, Md. Kamrul Hasan, M. M. A. Hashem category:cs.AI cs.DS cs.NE published:2013-04-13 summary:Solving large traveling salesman problem (TSP) in an efficient way is achallenging area for the researchers of computer science. This paper presents amodified version of the ant colony system (ACS) algorithm called Red-Black AntColony System (RB-ACS) for the solutions of TSP which is the most prominentmember of the combinatorial optimization problem. RB-ACS uses the concept ofant colony system together with the parallel search of genetic algorithm forobtaining the optimal solutions quickly. In this paper, it is shown that theproposed RB-ACS algorithm yields significantly better performance than theexisting best-known algorithms.
arxiv-2700-54 | Towards more accurate clustering method by using dynamic time warping | http://arxiv.org/pdf/1304.3745v1.pdf | author:Khadoudja Ghanem category:cs.LG stat.ML published:2013-04-12 summary:An intrinsic problem of classifiers based on machine learning (ML) methods isthat their learning time grows as the size and complexity of the trainingdataset increases. For this reason, it is important to have efficientcomputational methods and algorithms that can be applied on large datasets,such that it is still possible to complete the machine learning tasks inreasonable time. In this context, we present in this paper a more accuratesimple process to speed up ML methods. An unsupervised clustering algorithm iscombined with Expectation, Maximization (EM) algorithm to develop an efficientHidden Markov Model (HMM) training. The idea of the proposed process consistsof two steps. In the first step, training instances with similar inputs areclustered and a weight factor which represents the frequency of these instancesis assigned to each representative cluster. Dynamic Time Warping technique isused as a dissimilarity function to cluster similar examples. In the secondstep, all formulas in the classical HMM training algorithm (EM) associated withthe number of training instances are modified to include the weight factor inappropriate terms. This process significantly accelerates HMM training whilemaintaining the same initial, transition and emission probabilities matrixes asthose obtained with the classical HMM training algorithm. Accordingly, theclassification accuracy is preserved. Depending on the size of the trainingset, speedups of up to 2200 times is possible when the size is about 100.000instances. The proposed approach is not limited to training HMMs, but it can beemployed for a large variety of MLs methods.
arxiv-2700-55 | Advice-Efficient Prediction with Expert Advice | http://arxiv.org/pdf/1304.3708v1.pdf | author:Yevgeny Seldin, Peter Bartlett, Koby Crammer category:cs.LG stat.ML published:2013-04-12 summary:Advice-efficient prediction with expert advice (in analogy to label-efficientprediction) is a variant of prediction with expert advice game, where on eachround of the game we are allowed to ask for advice of a limited number $M$ outof $N$ experts. This setting is especially interesting when asking for adviceof every expert on every round is expensive. We present an algorithm foradvice-efficient prediction with expert advice that achieves$O(\sqrt{\frac{N}{M}T\ln N})$ regret on $T$ rounds of the game.
arxiv-2700-56 | A Novel Metaheuristics To Solve Mixed Shop Scheduling Problems | http://arxiv.org/pdf/1304.3612v1.pdf | author:V. Ravibabu category:cs.NE published:2013-04-12 summary:This paper represents the metaheuristics proposed for solving a class of ShopScheduling problem. The Bacterial Foraging Optimization algorithm is featuredwith Ant Colony Optimization algorithm and proposed as a natural inspiredcomputing approach to solve the Mixed Shop Scheduling problem. The Mixed Shopis the combination of Job Shop, Flow Shop and Open Shop scheduling problems.The sample instances for all mentioned Shop problems are used as test data andMixed Shop survive its computational complexity to minimize the makespan. Thecomputational results show that the proposed algorithm is gentler to solve andperforms better than the existing algorithms.
arxiv-2700-57 | Modified Soft Brood Crossover in Genetic Programming | http://arxiv.org/pdf/1304.3610v1.pdf | author:Hardik M. Parekh, Vipul K. Dabhi category:cs.NE published:2013-04-12 summary:Premature convergence is one of the important issues while using GeneticProgramming for data modeling. It can be avoided by improving populationdiversity. Intelligent genetic operators can help to improve the populationdiversity. Crossover is an important operator in Genetic Programming. So, wehave analyzed number of intelligent crossover operators and proposed analgorithm with the modification of soft brood crossover operator. It will helpto improve the population diversity and reduce the premature convergence. Wehave performed experiments on three different symbolic regression problems.Then we made the performance comparison of our proposed crossover (ModifiedSoft Brood Crossover) with the existing soft brood crossover and subtreecrossover operators.
arxiv-2700-58 | Sparsity regret bounds for individual sequences in online linear regression | http://arxiv.org/pdf/1101.1057v3.pdf | author:Sébastien Gerchinovitz category:stat.ML cs.LG math.ST stat.TH published:2011-01-05 summary:We consider the problem of online linear regression on arbitrarydeterministic sequences when the ambient dimension d can be much larger thanthe number of time rounds T. We introduce the notion of sparsity regret bound,which is a deterministic online counterpart of recent risk bounds derived inthe stochastic setting under a sparsity scenario. We prove such regret boundsfor an online-learning algorithm called SeqSEW and based on exponentialweighting and data-driven truncation. In a second part we apply aparameter-free version of this algorithm to the stochastic setting (regressionmodel with random design). This yields risk bounds of the same flavor as inDalalyan and Tsybakov (2011) but which solve two questions left open therein.In particular our risk bounds are adaptive (up to a logarithmic factor) to theunknown variance of the noise if the latter is Gaussian. We also address theregression model with fixed design.
arxiv-2700-59 | Astronomical Image Denoising Using Dictionary Learning | http://arxiv.org/pdf/1304.3573v1.pdf | author:Simon Beckouche, Jean-Luc Starck, Jalal Fadili category:astro-ph.IM cs.CV published:2013-04-12 summary:Astronomical images suffer a constant presence of multiple defects that areconsequences of the intrinsic properties of the acquisition equipments, andatmospheric conditions. One of the most frequent defects in astronomicalimaging is the presence of additive noise which makes a denoising stepmandatory before processing data. During the last decade, a particular modelingscheme, based on sparse representations, has drawn the attention of an evergrowing community of researchers. Sparse representations offer a promisingframework to many image and signal processing tasks, especially denoising andrestoration applications. At first, the harmonics, wavelets, and similar basesand overcomplete representations have been considered as candidate domains toseek the sparsest representation. A new generation of algorithms, based ondata-driven dictionaries, evolved rapidly and compete now with theoff-the-shelf fixed dictionaries. While designing a dictionary beforehand leanson a guess of the most appropriate representative elementary forms andfunctions, the dictionary learning framework offers to construct the dictionaryupon the data themselves, which provides us with a more flexible setup tosparse modeling and allows to build more sophisticated dictionaries. In thispaper, we introduce the Centered Dictionary Learning (CDL) method and we studyits performances for astronomical image denoising. We show how CDL outperformswavelet or classic dictionary learning denoising techniques on astronomicalimages, and we give a comparison of the effect of these different algorithms onthe photometry of the denoised images.
arxiv-2700-60 | Distributed dictionary learning over a sensor network | http://arxiv.org/pdf/1304.3568v1.pdf | author:Pierre Chainais, Cédric Richard category:stat.ML cs.LG stat.AP published:2013-04-12 summary:We consider the problem of distributed dictionary learning, where a set ofnodes is required to collectively learn a common dictionary from noisymeasurements. This approach may be useful in several contexts including sensornetworks. Diffusion cooperation schemes have been proposed to solve thedistributed linear regression problem. In this work we focus on adiffusion-based adaptive dictionary learning strategy: each node recordsobservations and cooperates with its neighbors by sharing its local dictionary.The resulting algorithm corresponds to a distributed block coordinate descent(alternate optimization). Beyond dictionary learning, this strategy could beadapted to many matrix factorization problems and generalized to varioussettings. This article presents our approach and illustrates its efficiency onsome numerical examples.
arxiv-2700-61 | Merging Satellite Measurements of Rainfall Using Multi-scale Imagery Technique | http://arxiv.org/pdf/1304.3406v1.pdf | author:Seyed Hamed Alemohammad, Dara Entekhabi category:cs.CV cs.IR published:2013-04-11 summary:Several passive microwave satellites orbit the Earth and measure rainfall.These measurements have the advantage of almost full global coverage whencompared to surface rain gauges. However, these satellites have low temporalrevisit and missing data over some regions. Image fusion is a useful techniqueto fill in the gaps of one image (one satellite measurement) using another one.The proposed algorithm uses an iterative fusion scheme to integrate informationfrom two satellite measurements. The algorithm is implemented on two datasetsfor 7 years of half-hourly data. The results show significant improvements inrain detection and rain intensity in the merged measurements.
arxiv-2700-62 | Generic Behaviour Similarity Measures for Evolutionary Swarm Robotics | http://arxiv.org/pdf/1304.3393v1.pdf | author:Jorge Gomes, Anders Lyhne Christensen category:cs.NE published:2013-04-11 summary:Novelty search has shown to be a promising approach for the evolution ofcontrollers for swarm robotics. In existing studies, however, the experimenterhad to craft a domain dependent behaviour similarity measure to use noveltysearch in swarm robotics applications. The reliance on hand-crafted similaritymeasures places an additional burden to the experimenter and introduces a biasin the evolutionary process. In this paper, we propose and compare twotask-independent, generic behaviour similarity measures: combined state countand sampled average state. The proposed measures use the values of sensors andeffectors recorded for each individual robot of the swarm. The characterisationof the group-level behaviour is then obtained by combining the sensor-effectorvalues from all the robots. We evaluate the proposed measures in an aggregationtask and in a resource sharing task. We show that the generic measures matchthe performance of domain dependent measures in terms of solution quality. Ourresults indicate that the proposed generic measures operate as effectivebehaviour similarity measures, and that it is possible to leverage the benefitsof novelty search without having to craft domain specific similarity measures.
arxiv-2700-63 | Evolution of Swarm Robotics Systems with Novelty Search | http://arxiv.org/pdf/1304.3362v1.pdf | author:Jorge Gomes, Paulo Urbano, Anders Lyhne Christensen category:cs.NE published:2013-04-11 summary:Novelty search is a recent artificial evolution technique that challengestraditional evolutionary approaches. In novelty search, solutions are rewardedbased on their novelty, rather than their quality with respect to a predefinedobjective. The lack of a predefined objective precludes premature convergencecaused by a deceptive fitness function. In this paper, we apply novelty searchcombined with NEAT to the evolution of neural controllers for homogeneousswarms of robots. Our empirical study is conducted in simulation, and we use acommon swarm robotics task - aggregation, and a more challenging task - sharingof an energy recharging station. Our results show that novelty search isunaffected by deception, is notably effective in bootstrapping the evolution,can find solutions with lower complexity than fitness-based evolution, and canfind a broad diversity of solutions for the same task. Even in non-deceptivesetups, novelty search achieves solution qualities similar to those obtained intraditional fitness-based evolution. Our study also encompasses variants ofnovelty search that work in concert with fitness-based evolution to combine theexploratory character of novelty search with the exploitatory character ofobjective-based evolution. We show that these variants can further improve theperformance of novelty search. Overall, our study shows that novelty search isa promising alternative for the evolution of controllers for robotic swarms.
arxiv-2700-64 | Probabilistic Classification using Fuzzy Support Vector Machines | http://arxiv.org/pdf/1304.3345v1.pdf | author:Marzieh Parandehgheibi category:cs.LG math.ST stat.TH published:2013-04-11 summary:In medical applications such as recognizing the type of a tumor as Malignantor Benign, a wrong diagnosis can be devastating. Methods like Fuzzy SupportVector Machines (FSVM) try to reduce the effect of misplaced training points byassigning a lower weight to the outliers. However, there are still uncertainpoints which are similar to both classes and assigning a class by the giveninformation will cause errors. In this paper, we propose a two-phaseclassification method which probabilistically assigns the uncertain points toeach of the classes. The proposed method is applied to the Breast CancerWisconsin (Diagnostic) Dataset which consists of 569 instances in 2 classes ofMalignant and Benign. This method assigns certain instances to theirappropriate classes with probability of one, and the uncertain instances toeach of the classes with associated probabilities. Therefore, based on thedegree of uncertainty, doctors can suggest further examinations before makingthe final diagnosis.
arxiv-2700-65 | Extension of hidden markov model for recognizing large vocabulary of sign language | http://arxiv.org/pdf/1304.3265v1.pdf | author:Maher Jebali, Patrice Dalle, Mohamed Jemni category:cs.CL published:2013-04-11 summary:Computers still have a long way to go before they can interact with users ina truly natural fashion. From a users perspective, the most natural way tointeract with a computer would be through a speech and gesture interface.Although speech recognition has made significant advances in the past tenyears, gesture recognition has been lagging behind. Sign Languages (SL) are themost accomplished forms of gestural communication. Therefore, their automaticanalysis is a real challenge, which is interestingly implied to their lexicaland syntactic organization levels. Statements dealing with sign language occupya significant interest in the Automatic Natural Language Processing (ANLP)domain. In this work, we are dealing with sign language recognition, inparticular of French Sign Language (FSL). FSL has its own specificities, suchas the simultaneity of several parameters, the important role of the facialexpression or movement and the use of space for the proper utteranceorganization. Unlike speech recognition, Frensh sign language (FSL) eventsoccur both sequentially and simultaneously. Thus, the computational processingof FSL is too complex than the spoken languages. We present a novel approachbased on HMM to reduce the recognition complexity.
arxiv-2700-66 | Improvement studies on neutron-gamma separation in HPGe detectors by using neural networks | http://arxiv.org/pdf/1304.3209v1.pdf | author:Serkan Akkoyun, Tuncay Bayram, S. Okan Kara category:cs.NE nucl-ex published:2013-04-11 summary:The neutrons emitted in heavy-ion fusion-evaporation (HIFE) reactionstogether with the gamma-rays cause unwanted backgrounds in gamma-ray spectra.Especially in the nuclear reactions, where relativistic ion beams (RIBs) areused, these neutrons are serious problem. They have to be rejected in order toobtain clearer gamma-ray peaks. In this study, the radiation energy and threecriteria which were previously determined for separation between neutron andgamma-rays in the HPGe detectors have been used in artificial neural network(ANN) for improving of the decomposition power. According to the preliminaryresults obtained from ANN method, the ratio of neutron rejection has beenimproved by a factor of 1.27 and the ratio of the lost in gamma-rays has beendecreased by a factor of 0.50.
arxiv-2700-67 | The Pascal Triangle of a Discrete Image: Definition, Properties and Application to Shape Analysis | http://arxiv.org/pdf/1209.4850v2.pdf | author:Mireille Boutin, Shanshan Huang category:math-ph cs.CV math.MP published:2012-09-21 summary:We define the Pascal triangle of a discrete (gray scale) image as a pyramidalarrangement of complex-valued moments and we explore its geometricsignificance. In particular, we show that the entries of row k of this trianglecorrespond to the Fourier series coefficients of the moment of order k of theRadon transform of the image. Group actions on the plane can be naturallyprolonged onto the entries of the Pascal triangle. We study the prolongation ofsome common group actions, such as rotations and reflections, and we proposesimple tests for detecting equivalences and self-equivalences under these groupactions. The motivating application of this work is the problem ofcharacterizing the geometry of objects on images, for example by detectingapproximate symmetries.
arxiv-2700-68 | An Approach to Solve Linear Equations Using a Time-Variant Adaptation Based Hybrid Evolutionary Algorithm | http://arxiv.org/pdf/1304.3200v1.pdf | author:A. R. M. Jalal Uddin Jamali, M. M. A. Hashem, Md. Bazlar Rahman category:cs.NE cs.NA published:2013-04-11 summary:For small number of equations, systems of linear (and sometimes nonlinear)equations can be solved by simple classical techniques. However, for largenumber of systems of linear (or nonlinear) equations, solutions using classicalmethod become arduous. On the other hand evolutionary algorithms have mostlybeen used to solve various optimization and learning problems. Recently,hybridization of evolutionary algorithm with classical Gauss-Seidel basedSuccessive Over Relaxation (SOR) method has successfully been used to solvelarge number of linear equations; where a uniform adaptation (UA) technique ofrelaxation factor is used. In this paper, a new hybrid algorithm is proposed inwhich a time-variant adaptation (TVA) technique of relaxation factor is usedinstead of uniform adaptation technique to solve large number of linearequations. The convergence theorems of the proposed algorithms are provedtheoretically. And the performance of the proposed TVA-based algorithm iscompared with the UA-based hybrid algorithm in the experimental domain. Theproposed algorithm outperforms the hybrid one in terms of efficiency.
arxiv-2700-69 | Rotational Projection Statistics for 3D Local Surface Description and Object Recognition | http://arxiv.org/pdf/1304.3192v1.pdf | author:Yulan Guo, Ferdous Sohel, Mohammed Bennamoun, Min Lu, Jianwei Wan category:cs.CV I.4; I.5.4 published:2013-04-11 summary:Recognizing 3D objects in the presence of noise, varying mesh resolution,occlusion and clutter is a very challenging task. This paper presents a novelmethod named Rotational Projection Statistics (RoPS). It has three majormodules: Local Reference Frame (LRF) definition, RoPS feature description and3D object recognition. We propose a novel technique to define the LRF bycalculating the scatter matrix of all points lying on the local surface. RoPSfeature descriptors are obtained by rotationally projecting the neighboringpoints of a feature point onto 2D planes and calculating a set of statistics(including low-order central moments and entropy) of the distribution of theseprojected points. Using the proposed LRF and RoPS descriptor, we present ahierarchical 3D object recognition algorithm. The performance of the proposedLRF, RoPS descriptor and object recognition algorithm was rigorously tested ona number of popular and publicly available datasets. Our proposed techniquesexhibited superior performance compared to existing techniques. We also showedthat our method is robust with respect to noise and varying mesh resolution.Our RoPS based algorithm achieved recognition rates of 100%, 98.9%, 95.4% and96.0% respectively when tested on the Bologna, UWA, Queen's and Ca' FoscariVenezia Datasets.
arxiv-2700-70 | Sustainable Cooperative Coevolution with a Multi-Armed Bandit | http://arxiv.org/pdf/1304.3138v1.pdf | author:François-Michel De Rainville, Michèle Sebag, Christian Gagné, Marc Schoenauer, Denis Laurendeau category:cs.NE I.2.8 published:2013-04-10 summary:This paper proposes a self-adaptation mechanism to manage the resourcesallocated to the different species comprising a cooperative coevolutionaryalgorithm. The proposed approach relies on a dynamic extension to thewell-known multi-armed bandit framework. At each iteration, the dynamicmulti-armed bandit makes a decision on which species to evolve for ageneration, using the history of progress made by the different species toguide the decisions. We show experimentally, on a benchmark and a real-worldproblem, that evolving the different populations at different paces allows notonly to identify solutions more rapidly, but also improves the capacity ofcooperative coevolution to solve more complex problems.
arxiv-2700-71 | A Geometric Descriptor for Cell-Division Detection | http://arxiv.org/pdf/1301.3457v2.pdf | author:Marcelo Cicconet, Italo Lima, Davi Geiger, Kris Gunsalus category:cs.CV published:2013-01-15 summary:We describe a method for cell-division detection based on a geometric-drivendescriptor that can be represented as a 5-layers processing network, basedmainly on wavelet filtering and a test for mirror symmetry between pairs ofpixels. After the centroids of the descriptors are computed for a sequence offrames, the two-steps piecewise constant function that best fits the sequenceof centroids determines the frame where the division occurs.
arxiv-2700-72 | Orientation Determination from Cryo-EM images Using Least Unsquared Deviation | http://arxiv.org/pdf/1211.7045v2.pdf | author:Lanhui Wang, Amit Singer, Zaiwen Wen category:cs.LG math.NA math.OC q-bio.BM published:2012-11-29 summary:A major challenge in single particle reconstruction from cryo-electronmicroscopy is to establish a reliable ab-initio three-dimensional model usingtwo-dimensional projection images with unknown orientations. Common-lines basedmethods estimate the orientations without additional geometric information.However, such methods fail when the detection rate of common-lines is too lowdue to the high level of noise in the images. An approximation to the leastsquares global self consistency error was obtained using convex relaxation bysemidefinite programming. In this paper we introduce a more robust global selfconsistency error and show that the corresponding optimization problem can besolved via semidefinite relaxation. In order to prevent artificial clusteringof the estimated viewing directions, we further introduce a spectral norm termthat is added as a constraint or as a regularization term to the relaxedminimization problem. The resulted problems are solved by using either thealternating direction method of multipliers or an iteratively reweighted leastsquares procedure. Numerical experiments with both simulated and real imagesdemonstrate that the proposed methods significantly reduce the orientationestimation error when the detection rate of common-lines is low.
arxiv-2700-73 | Roborobo! a Fast Robot Simulator for Swarm and Collective Robotics | http://arxiv.org/pdf/1304.2888v1.pdf | author:Nicolas Bredeche, Jean-Marc Montanier, Berend Weel, Evert Haasdijk category:cs.RO cs.AI cs.NE published:2013-04-10 summary:Roborobo! is a multi-platform, highly portable, robot simulator forlarge-scale collective robotics experiments. Roborobo! is coded in C++, andfollows the KISS guideline ("Keep it simple"). Therefore, its externaldependency is solely limited to the widely available SDL library for fast 2DGraphics. Roborobo! is based on a Khepera/ePuck model. It is targeted for fastsingle and multi-robots simulation, and has already been used in more than adozen published research mainly concerned with evolutionary swarm robotics,including environment-driven self-adaptation and distributed evolutionaryoptimization, as well as online onboard embodied evolution and embodiedmorphogenesis.
arxiv-2700-74 | Sparse projections onto the simplex | http://arxiv.org/pdf/1206.1529v5.pdf | author:Anastasios Kyrillidis, Stephen Becker, Volkan Cevher and, Christoph Koch category:cs.LG stat.ML published:2012-06-07 summary:Most learning methods with rank or sparsity constraints use convexrelaxations, which lead to optimization with the nuclear norm or the$\ell_1$-norm. However, several important learning applications cannot benefitfrom this approach as they feature these convex norms as constraints inaddition to the non-convex rank and sparsity constraints. In this setting, wederive efficient sparse projections onto the simplex and its extension, andillustrate how to use them to solve high-dimensional learning problems inquantum tomography, sparse density estimation and portfolio selection withnon-convex constraints.
arxiv-2700-75 | The BOSARIS Toolkit: Theory, Algorithms and Code for Surviving the New DCF | http://arxiv.org/pdf/1304.2865v1.pdf | author:Niko Brümmer, Edward de Villiers category:stat.AP cs.LG stat.ML published:2013-04-10 summary:The change of two orders of magnitude in the 'new DCF' of NIST's SRE'10,relative to the 'old DCF' evaluation criterion, posed a difficult challenge forparticipants and evaluator alike. Initially, participants were at a loss as tohow to calibrate their systems, while the evaluator underestimated the requirednumber of evaluation trials. After the fact, it is now obvious that bothcalibration and evaluation require very large sets of trials. This poses thechallenges of (i) how to decide what number of trials is enough, and (ii) howto process such large data sets with reasonable memory and CPU requirements.After SRE'10, at the BOSARIS Workshop, we built solutions to these problemsinto the freely available BOSARIS Toolkit. This paper explains the principlesand algorithms behind this toolkit. The main contributions of the toolkit are:1. The Normalized Bayes Error-Rate Plot, which analyses likelihood- ratiocalibration over a wide range of DCF operating points. These plots also help injudging the adequacy of the sizes of calibration and evaluation databases. 2.Efficient algorithms to compute DCF and minDCF for large score files, over therange of operating points required by these plots. 3. A new score file format,which facilitates working with very large trial lists. 4. A faster logisticregression optimizer for fusion and calibration. 5. A principled way to defineEER (equal error rate), which is of practical interest when the absolute errorcount is small.
arxiv-2700-76 | Predictive Correlation Screening: Application to Two-stage Predictor Design in High Dimension | http://arxiv.org/pdf/1303.2378v2.pdf | author:Hamed Firouzi, Bala Rajaratnam, Alfred Hero category:stat.ML published:2013-03-10 summary:We introduce a new approach to variable selection, called PredictiveCorrelation Screening, for predictor design. Predictive Correlation Screening(PCS) implements false positive control on the selected variables, is wellsuited to small sample sizes, and is scalable to high dimensions. We establishasymptotic bounds for Familywise Error Rate (FWER), and resultant mean squareerror of a linear predictor on the selected variables. We apply PredictiveCorrelation Screening to the following two-stage predictor design problem. Anexperimenter wants to learn a multivariate predictor of gene expressions basedon successive biological samples assayed on mRNA arrays. She assays the wholegenome on a few samples and from these assays she selects a small number ofvariables using Predictive Correlation Screening. To reduce assay cost, shesubsequently assays only the selected variables on the remaining samples, tolearn the predictor coefficients. We show superiority of Predictive CorrelationScreening relative to LASSO and correlation learning (sometimes popularlyreferred to in the literature as marginal regression or simple thresholding) interms of performance and computational complexity.
arxiv-2700-77 | Image Classification by Feature Dimension Reduction and Graph based Ranking | http://arxiv.org/pdf/1304.2683v1.pdf | author:Yao Nan, Qian Feng, Sun Zuolei category:cs.CV published:2013-04-09 summary:Dimensionality reduction (DR) of image features plays an important role inimage retrieval and classification tasks. Recently, two types of methods havebeen proposed to improve the both the accuracy and efficiency for thedimensionality reduction problem. One uses Non-negative matrix factorization(NMF) to describe the image distribution on the space of base matrix. Anotherone for dimension reduction trains a subspace projection matrix to projectoriginal data space into some low-dimensional subspaces which have deeparchitecture, so that the low-dimensional codes would be learned. At the sametime, the graph based similarity learning algorithm which tries to exploitcontextual information for improving the effectiveness of image rankings isalso proposed for image class and retrieval problem. In this paper, after abovetwo methods mentioned are utilized to reduce the high-dimensional features ofimages respectively, we learn the graph based similarity for the imageclassification problem. This paper compares the proposed approach with otherapproaches on an image database.
arxiv-2700-78 | Image Retrieval using Histogram Factorization and Contextual Similarity Learning | http://arxiv.org/pdf/1304.1995v2.pdf | author:Liu Liang category:cs.CV cs.DB cs.LG published:2013-04-07 summary:Image retrieval has been a top topic in the field of both computer vision andmachine learning for a long time. Content based image retrieval, which tries toretrieve images from a database visually similar to a query image, hasattracted much attention. Two most important issues of image retrieval are therepresentation and ranking of the images. Recently, bag-of-words based methodhas shown its power as a representation method. Moreover, nonnegative matrixfactorization is also a popular way to represent the data samples. In addition,contextual similarity learning has also been studied and proven to be aneffective method for the ranking problem. However, these technologies havenever been used together. In this paper, we developed an effective imageretrieval system by representing each image using the bag-of-words method ashistograms, and then apply the nonnegative matrix factorization to factorizethe histograms, and finally learn the ranking score using the contextualsimilarity learning method. The proposed novel system is evaluated on a largescale image database and the effectiveness is shown.
arxiv-2700-79 | For Solving Linear Equations Recombination is a Needless Operation in Time-Variant Adaptive Hybrid Algorithms | http://arxiv.org/pdf/1304.2545v1.pdf | author:A. R. M. Jalal Uddin Jamali, Mohammad Arif Hossain, G. M. Moniruzzaman, M. M. A. Hashem category:cs.NE cs.NA published:2013-04-09 summary:Recently hybrid evolutionary computation (EC) techniques are successfullyimplemented for solving large sets of linear equations. All the recentlydeveloped hybrid evolutionary algorithms, for solving linear equations, containboth the recombination and the mutation operations. In this paper, two modifiedhybrid evolutionary algorithms contained time-variant adaptive evolutionarytechnique are proposed for solving linear equations in which recombinationoperation is absent. The effectiveness of the recombination operator has beenstudied for the time-variant adaptive hybrid algorithms for solving large setof linear equations. Several experiments have been carried out using both theproposed modified hybrid evolutionary algorithms (in which the recombinationoperation is absent) and corresponding existing hybrid algorithms (in which therecombination operation is present) to solve large set of linear equations. Itis found that the number of generations required by the existing hybridalgorithms (i.e. the Gauss-Seidel-SR based time variant adaptive (GSBTVA)hybrid algorithm and the Jacobi-SR based time variant adaptive (JBTVA) hybridalgorithm) and modified hybrid algorithms (i.e. the modified Gauss-Seidel-SRbased time variant adaptive (MGSBTVA) hybrid algorithm and the modifiedJacobi-SR based time variant adaptive (MJBTVA) hybrid algorithm) arecomparable. Also the proposed modified algorithms require less amount ofcomputational time in comparison to the corresponding existing hybridalgorithms. As the proposed modified hybrid algorithms do not containrecombination operation, so they require less computational effort, and alsothey are more efficient, effective and easy to implement.
arxiv-2700-80 | A New Distributed Evolutionary Computation Technique for Multi-Objective Optimization | http://arxiv.org/pdf/1304.2543v1.pdf | author:Md. Asadul Islam, G. M. Mashrur-E-Elahi, M. M. A. Hashem category:cs.NE published:2013-04-09 summary:Now-a-days, it is important to find out solutions of Multi-ObjectiveOptimization Problems (MOPs). Evolutionary Strategy helps to solve such realworld problems efficiently and quickly. But sequential Evolutionary Algorithms(EAs) require an enormous computation power to solve such problems and it takesmuch time to solve large problems. To enhance the performance for solving thistype of problems, this paper presents a new Distributed Novel EvolutionaryStrategy Algorithm (DNESA) for Multi-Objective Optimization. The proposed DNESAapplies the divide-and-conquer approach to decompose population into smallersub-population and involves multiple solutions in the form of cooperativesub-populations. In DNESA, the server distributes the total computation load toall associate clients and simulation results show that the time for solvinglarge problems is much less than sequential EAs. Also DNESA shows betterperformance in convergence test when compared with other three well-known EAs.
arxiv-2700-81 | Kernel Reconstruction ICA for Sparse Representation | http://arxiv.org/pdf/1304.2490v1.pdf | author:Yanhui Xiao, Zhenfeng Zhu, Yao Zhao category:cs.CV cs.LG published:2013-04-09 summary:Independent Component Analysis (ICA) is an effective unsupervised tool tolearn statistically independent representation. However, ICA is not onlysensitive to whitening but also difficult to learn an over-complete basis.Consequently, ICA with soft Reconstruction cost(RICA) was presented to learnsparse representations with over-complete basis even on unwhitened data.Whereas RICA is infeasible to represent the data with nonlinear structure dueto its intrinsic linearity. In addition, RICA is essentially an unsupervisedmethod and can not utilize the class information. In this paper, we propose akernel ICA model with reconstruction constraint (kRICA) to capture thenonlinear features. To bring in the class information, we further extend theunsupervised kRICA to a supervised one by introducing a discriminationconstraint, namely d-kRICA. This constraint leads to learn a structured basisconsisted of basis vectors from different basis subsets corresponding todifferent class labels. Then each subset will sparsely represent well for itsown class but not for the others. Furthermore, data samples belonging to thesame class will have similar representations, and thereby the learned sparserepresentations can take more discriminative power. Experimental resultsvalidate the effectiveness of kRICA and d-kRICA for image classification.
arxiv-2700-82 | Corpus-based Web Document Summarization using Statistical and Linguistic Approach | http://arxiv.org/pdf/1304.2476v1.pdf | author:Rushdi Shams, M. M. A. Hashem, Afrina Hossain, Suraiya Rumana Akter, Monika Gope category:cs.IR cs.CL published:2013-04-09 summary:Single document summarization generates summary by extracting therepresentative sentences from the document. In this paper, we presented a noveltechnique for summarization of domain-specific text from a single web documentthat uses statistical and linguistic analysis on the text in a reference corpusand the web document. The proposed summarizer uses the combinational functionof Sentence Weight (SW) and Subject Weight (SuW) to determine the rank of asentence, where SW is the function of number of terms (t_n) and number of words(w_n) in a sentence, and term frequency (t_f) in the corpus and SuW is thefunction of t_n and w_n in a subject, and t_f in the corpus. 30 percent of theranked sentences are considered to be the summary of the web document. Wegenerated three web document summaries using our technique and compared each ofthem with the summaries developed manually from 16 different human subjects.Results showed that 68 percent of the summaries produced by our approachsatisfy the manual summaries.
arxiv-2700-83 | Evolutionary Design of Digital Circuits Using Genetic Programming | http://arxiv.org/pdf/1304.2467v1.pdf | author:S. M. Ashik Eftekhar, Sk. Mahbub Habib, M. M. A. Hashem category:cs.NE published:2013-04-09 summary:For simple digital circuits, conventional method of designing circuits caneasily be applied. But for complex digital circuits, the conventional method ofdesigning circuits is not fruitfully applicable because it is time-consuming.On the contrary, Genetic Programming is used mostly for automatic programgeneration. The modern approach for designing Arithmetic circuits, commonlydigital circuits, is based on Graphs. This graph-based evolutionary design ofarithmetic circuits is a method of optimized designing of arithmetic circuits.In this paper, a new technique for evolutionary design of digital circuits isproposed using Genetic Programming (GP) with Subtree Mutation in place ofGraph-based design. The results obtained using this technique demonstrates thepotential capability of genetic programming in digital circuit design withlimited computer algorithms. The proposed technique, helps to simplify andspeed up the process of designing digital circuits, discovers a variation inthe field of digital circuit design where optimized digital circuits can besuccessfully and effectively designed.
arxiv-2700-84 | Convergence of latent mixing measures in finite and infinite mixture models | http://arxiv.org/pdf/1109.3250v5.pdf | author:XuanLong Nguyen category:math.ST stat.ML stat.TH published:2011-09-15 summary:This paper studies convergence behavior of latent mixing measures that arisein finite and infinite mixture models, using transportation distances (i.e.,Wasserstein metrics). The relationship between Wasserstein distances on thespace of mixing measures and f-divergence functionals such as Hellinger andKullback-Leibler distances on the space of mixture distributions isinvestigated in detail using various identifiability conditions. Convergence inWasserstein metrics for discrete measures implies convergence of individualatoms that provide support for the measures, thereby providing a naturalinterpretation of convergence of clusters in clustering applications wheremixture models are typically employed. Convergence rates of posteriordistributions for latent mixing measures are established, for both finitemixtures of multivariate distributions and infinite mixtures based on theDirichlet process.
arxiv-2700-85 | Efficient Learning of Domain-invariant Image Representations | http://arxiv.org/pdf/1301.3224v5.pdf | author:Judy Hoffman, Erik Rodner, Jeff Donahue, Trevor Darrell, Kate Saenko category:cs.LG published:2013-01-15 summary:We present an algorithm that learns representations which explicitlycompensate for domain mismatch and which can be efficiently realized as linearclassifiers. Specifically, we form a linear transformation that maps featuresfrom the target (test) domain to the source (training) domain as part oftraining the classifier. We optimize both the transformation and classifierparameters jointly, and introduce an efficient cost function based onmisclassification loss. Our method combines several features previouslyunavailable in a single algorithm: multi-class adaptation throughrepresentation learning, ability to map across heterogeneous feature spaces,and scalability to large datasets. We present experiments on several imagedatasets that demonstrate improved accuracy and computational advantagescompared to previous approaches.
arxiv-2700-86 | The PAV algorithm optimizes binary proper scoring rules | http://arxiv.org/pdf/1304.2331v1.pdf | author:Niko Brummer, Johan du Preez category:stat.AP cs.LG stat.ML published:2013-04-08 summary:There has been much recent interest in application of thepool-adjacent-violators (PAV) algorithm for the purpose of calibrating theprobabilistic outputs of automatic pattern recognition and machine learningalgorithms. Special cost functions, known as proper scoring rules form naturalobjective functions to judge the goodness of such calibration. We show that forbinary pattern classifiers, the non-parametric optimization of calibration,subject to a monotonicity constraint, can be solved by PAV and that thissolution is optimal for all regular binary proper scoring rules. This extendsprevious results which were limited to convex binary proper scoring rules. Wefurther show that this result holds not only for calibration of probabilities,but also for calibration of log-likelihood-ratios, in which case optimalityholds independently of the prior probabilities of the pattern classes.
arxiv-2700-87 | ClusterCluster: Parallel Markov Chain Monte Carlo for Dirichlet Process Mixtures | http://arxiv.org/pdf/1304.2302v1.pdf | author:Dan Lovell, Jonathan Malmaud, Ryan P. Adams, Vikash K. Mansinghka category:stat.ML cs.DC cs.LG published:2013-04-08 summary:The Dirichlet process (DP) is a fundamental mathematical tool for Bayesiannonparametric modeling, and is widely used in tasks such as density estimation,natural language processing, and time series modeling. Although MCMC inferencemethods for the DP often provide a gold standard in terms asymptotic accuracy,they can be computationally expensive and are not obviously parallelizable. Wepropose a reparameterization of the Dirichlet process that induces conditionalindependencies between the atoms that form the random measure. This conditionalindependence enables many of the Markov chain transition operators for DPinference to be simulated in parallel across multiple cores. Applied to mixturemodeling, our approach enables the Dirichlet process to simultaneously learnclusters that describe the data and superclusters that define the granularityof parallelization. Unlike previous approaches, our technique does not requirealteration of the model and leaves the true posterior distribution invariant.It also naturally lends itself to a distributed software implementation interms of Map-Reduce, which we test in cluster configurations of over 50machines and 100 cores. We present experiments exploring the parallelefficiency and convergence properties of our approach on both synthetic andreal-world data, including runs on 1MM data vectors in 256 dimensions.
arxiv-2700-88 | Synaptic Scaling Balances Learning in a Spiking Model of Neocortex | http://arxiv.org/pdf/1304.2266v1.pdf | author:Mark Rowan, Samuel Neymotin category:q-bio.NC cs.NE published:2013-04-08 summary:Learning in the brain requires complementary mechanisms: potentiation andactivity-dependent homeostatic scaling. We introduce synaptic scaling to abiologically-realistic spiking model of neocortex which can learn changes inoscillatory rhythms using STDP, and show that scaling is necessary to balanceboth positive and negative changes in input from potentiation and atrophy. Wediscuss some of the issues that arise when considering synaptic scaling in sucha model, and show that scaling regulates activity whilst allowing learning toremain unaltered.
arxiv-2700-89 | Relevance As a Metric for Evaluating Machine Learning Algorithms | http://arxiv.org/pdf/1303.7093v3.pdf | author:Aravind Kota Gopalakrishna, Tanir Ozcelebi, Antonio Liotta, Johan J. Lukkien category:stat.ML cs.LG published:2013-03-28 summary:In machine learning, the choice of a learning algorithm that is suitable forthe application domain is critical. The performance metric used to comparedifferent algorithms must also reflect the concerns of users in the applicationdomain under consideration. In this work, we propose a novel probability-basedperformance metric called Relevance Score for evaluating supervised learningalgorithms. We evaluate the proposed metric through empirical analysis on adataset gathered from an intelligent lighting pilot installation. In comparisonto the commonly used Classification Accuracy metric, the Relevance Score provesto be more appropriate for a certain class of applications.
arxiv-2700-90 | Geometric tree kernels: Classification of COPD from airway tree geometry | http://arxiv.org/pdf/1303.7390v2.pdf | author:Aasa Feragen, Jens Petersen, Dominik Grimm, Asger Dirksen, Jesper Holst Pedersen, Karsten Borgwardt, Marleen de Bruijne category:cs.CV 68T10 published:2013-03-29 summary:Methodological contributions: This paper introduces a family of kernels foranalyzing (anatomical) trees endowed with vector valued measurements made alongthe tree. While state-of-the-art graph and tree kernels use combinatorialtree/graph structure with discrete node and edge labels, the kernels presentedin this paper can include geometric information such as branch shape, branchradius or other vector valued properties. In addition to being flexible intheir ability to model different types of attributes, the presented kernels arecomputationally efficient and some of them can easily be computed for largedatasets (N of the order 10.000) of trees with 30-600 branches. Combining thekernels with standard machine learning tools enables us to analyze the relationbetween disease and anatomical tree structure and geometry. Experimentalresults: The kernels are used to compare airway trees segmented from low-doseCT, endowed with branch shape descriptors and airway wall area percentagemeasurements made along the tree. Using kernelized hypothesis testing we showthat the geometric airway trees are significantly differently distributed inpatients with Chronic Obstructive Pulmonary Disease (COPD) than in healthyindividuals. The geometric tree kernels also give a significant increase in theclassification accuracy of COPD from geometric tree structure endowed withairway wall thickness measurements in comparison with state-of-the-art methods,giving further insight into the relationship between airway wall thickness andCOPD. Software: Software for computing kernels and statistical tests isavailable at http://image.diku.dk/aasa/software.php.
arxiv-2700-91 | Parsimonious module inference in large networks | http://arxiv.org/pdf/1212.4794v4.pdf | author:Tiago P. Peixoto category:physics.soc-ph stat.ML published:2012-12-19 summary:We investigate the detectability of modules in large networks when the numberof modules is not known in advance. We employ the minimum description length(MDL) principle which seeks to minimize the total amount of informationrequired to describe the network, and avoid overfitting. According to thiscriterion, we obtain general bounds on the detectability of any prescribedblock structure, given the number of nodes and edges in the sampled network. Wealso obtain that the maximum number of detectable blocks scales as $\sqrt{N}$,where $N$ is the number of nodes in the network, for a fixed average degree$<k>$. We also show that the simplicity of the MDL approach yields an efficientmultilevel Monte Carlo inference algorithm with a complexity of $O(\tau N\logN)$, if the number of blocks is unknown, and $O(\tau N)$ if it is known, where$\tau$ is the mixing time of the Markov chain. We illustrate the application ofthe method on a large network of actors and films with over $10^6$ edges, and adissortative, bipartite block structure.
arxiv-2700-92 | Dynamic Amelioration of Resolution Mismatches for Local Feature Based Identity Inference | http://arxiv.org/pdf/1304.2133v1.pdf | author:Yongkang Wong, Conrad Sanderson, Sandra Mau, Brian C. Lovell category:cs.CV cs.IR I.5.4; I.4 published:2013-04-08 summary:While existing face recognition systems based on local features are robust toissues such as misalignment, they can exhibit accuracy degradation whencomparing images of differing resolutions. This is common in surveillanceenvironments where a gallery of high resolution mugshots is compared to lowresolution CCTV probe images, or where the size of a given image is not areliable indicator of the underlying resolution (eg. poor optics). To alleviatethis degradation, we propose a compensation framework which dynamically choosesthe most appropriate face recognition system for a given pair of imageresolutions. This framework applies a novel resolution detection method whichdoes not rely on the size of the input images, but instead exploits thesensitivity of local features to resolution using a probabilistic multi-regionhistogram approach. Experiments on a resolution-modified version of the"Labeled Faces in the Wild" dataset show that the proposed resolution detectorfrontend obtains a 99% average accuracy in selecting the most appropriate facerecognition system, resulting in higher overall face discrimination accuracy(across several resolutions) compared to the individual baseline facerecognition systems.
arxiv-2700-93 | Automatic Fingerprint Recognition Using Minutiae Matching Technique for the Large Fingerprint Database | http://arxiv.org/pdf/1304.2109v1.pdf | author:S. M. Mohsen, S. M. Zamshed Farhan, M. M. A. Hashem category:cs.CV published:2013-04-08 summary:Extracting minutiae from fingerprint images is one of the most importantsteps in automatic fingerprint identification system. Because minutiae matchingare certainly the most well-known and widely used method for fingerprintmatching, minutiae are local discontinuities in the fingerprint pattern. Inthis paper a fingerprint matching algorithm is proposed using some specificfeature of the minutiae points, also the acquired fingerprint image isconsidered by minimizing its size by generating a corresponding fingerprinttemplate for a large fingerprint database. The results achieved are comparedwith those obtained through some other methods also shows some improvement inthe minutiae detection process in terms of memory and time required.
arxiv-2700-94 | A powerful and efficient set test for genetic markers that handles confounders | http://arxiv.org/pdf/1205.0793v3.pdf | author:Jennifer Listgarten, Christoph Lippert, Eun Yong Kang, Jing Xiang, Carl M. Kadie, David Heckerman category:q-bio.GN stat.AP stat.ML published:2012-05-03 summary:Approaches for testing sets of variants, such as a set of rare or commonvariants within a gene or pathway, for association with complex traits areimportant. In particular, set tests allow for aggregation of weak signal withina set, can capture interplay among variants, and reduce the burden of multiplehypothesis testing. Until now, these approaches did not address confounding byfamily relatedness and population structure, a problem that is becoming moreimportant as larger data sets are used to increase power. Results: We introduce a new approach for set tests that handles confounders.Our model is based on the linear mixed model and uses two random effects-one tocapture the set association signal and one to capture confounders. We alsointroduce a computational speedup for two-random-effects models that makes thisapproach feasible even for extremely large cohorts. Using this model with boththe likelihood ratio test and score test, we find that the former yields morepower while controlling type I error. Application of our approach to richlystructured GAW14 data demonstrates that our method successfully corrects forpopulation structure and family relatedness, while application of our method toa 15,000 individual Crohn's disease case-control cohort demonstrates that itadditionally recovers genes not recoverable by univariate analysis. Availability: A Python-based library implementing our approach is availableat http://mscompbio.codeplex.com
arxiv-2700-95 | Solving Linear Equations by Classical Jacobi-SR Based Hybrid Evolutionary Algorithm with Uniform Adaptation Technique | http://arxiv.org/pdf/1304.2097v1.pdf | author:R. M. Jalal Uddin Jamali, M. M. A. Hashem, M. Mahfuz Hasan, Md. Bazlar Rahman category:cs.NE cs.NA published:2013-04-08 summary:Solving a set of simultaneous linear equations is probably the most importanttopic in numerical methods. For solving linear equations, iterative methods arepreferred over the direct methods especially when the coefficient matrix issparse. The rate of convergence of iteration method is increased by usingSuccessive Relaxation (SR) technique. But SR technique is very much sensitiveto relaxation factor, {\omega}. Recently, hybridization of classicalGauss-Seidel based successive relaxation technique with evolutionarycomputation techniques have successfully been used to solve large set of linearequations in which relaxation factors are self-adapted. In this paper, a newhybrid algorithm is proposed in which uniform adaptive evolutionary computationtechniques and classical Jacobi based SR technique are used instead ofclassical Gauss-Seidel based SR technique. The proposed Jacobi-SR based uniformadaptive hybrid algorithm, inherently, can be implemented in parallelprocessing environment efficiently. Whereas Gauss-Seidel-SR based hybridalgorithms cannot be implemented in parallel computing environment efficiently.The convergence theorem and adaptation theorem of the proposed algorithm areproved theoretically. And the performance of the proposed Jacobi-SR baseduniform adaptive hybrid evolutionary algorithm is compared with Gauss-Seidel-SRbased uniform adaptive hybrid evolutionary algorithm as well as with bothclassical Jacobi-SR method and Gauss-Seidel-SR method in the experimentaldomain. The proposed Jacobi-SR based hybrid algorithm outperforms theGauss-Seidel-SR based hybrid algorithm as well as both classical Jacobi-SRmethod and Gauss-Seidel-SR method in terms of convergence speed andeffectiveness.
arxiv-2700-96 | Image Compression predicated on Recurrent Iterated Function Systems | http://arxiv.org/pdf/1304.2014v1.pdf | author:Chol-Hui Yun, W. Metzler, M. Barski category:math.DS cs.CV math.GT published:2013-04-07 summary:Recurrent iterated function systems (RIFSs) are improvements of iteratedfunction systems (IFSs) using elements of the theory of Marcovian stochasticprocesses which can produce more natural looking images. We construct new RIFSsconsisting substantially of a vertical contraction factor function andnonlinear transformations. These RIFSs are applied to image compression.
arxiv-2700-97 | Facial transformations of ancient portraits: the face of Caesar | http://arxiv.org/pdf/1304.1972v1.pdf | author:Amelia Carolina Sparavigna category:cs.CV published:2013-04-07 summary:Some software solutions used to obtain the facial transformations can helpinvestigating the artistic metamorphosis of the ancient portraits of the sameperson. An analysis with a freely available software of portraitures of JuliusCaesar is proposed, showing his several "morphs". The software helps enhancingthe mood the artist added to a portrait.
arxiv-2700-98 | Client-Driven Content Extraction Associated with Table | http://arxiv.org/pdf/1304.1930v1.pdf | author:K. C. Santosh, Abdel Belaïd category:cs.CV cs.IR published:2013-04-06 summary:The goal of the project is to extract content within table in document imagesbased on learnt patterns. Real-world users i.e., clients first provide a set ofkey fields within the table which they think are important. These are firstused to represent the graph where nodes are labelled with semantics includingother features and edges are attributed with relations. Attributed relationalgraph (ARG) is then employed to mine similar graphs from a document image. Eachmined graph will represent an item within the table, and hence a set of suchgraphs will compose a table. We have validated the concept by using areal-world industrial problem.
arxiv-2700-99 | RPA: Probabilistic analysis of probe performance and robust summarization | http://arxiv.org/pdf/1109.4928v2.pdf | author:Leo Lahti, Laura L. Elo, Tero Aittokallio, Samuel Kaski category:cs.CE stat.AP stat.ML published:2011-09-22 summary:Probe-level models have led to improved performance in microarray studies butthe various sources of probe-level contamination are still poorly understood.Data-driven analysis of probe performance can be used to quantify theuncertainty in individual probes and to highlight the relative contribution ofdifferent noise sources. Improved understanding of the probe-level effects canlead to improved preprocessing techniques and microarray design. We have implemented probabilistic tools for probe performance analysis andsummarization on short oligonucleotide arrays. In contrast to standardpreprocessing approaches, the methods provide quantitative estimates ofprobe-specific noise and affinity terms and tools to investigate theseparameters. Tools to incorporate prior information of the probes in theanalysis are provided as well. Comparisons to known probe-level error sourcesand spike-in data sets validate the approach. Implementation is freely available in R/BioConductor:http://www.bioconductor.org/packages/release/bioc/html/RPA.html
arxiv-2700-100 | Bug Classification: Feature Extraction and Comparison of Event Model using Naïve Bayes Approach | http://arxiv.org/pdf/1304.1677v1.pdf | author:Sunil Joy Dommati, Ruchi Agrawal, Ram Mohana Reddy G., S. Sowmya Kamath category:cs.SE cs.IR cs.LG published:2013-04-05 summary:In software industries, individuals at different levels from customer to anengineer apply diverse mechanisms to detect to which class a particular bugshould be allocated. Sometimes while a simple search in Internet might help, inmany other cases a lot of effort is spent in analyzing the bug report toclassify the bug. So there is a great need of a structured mining algorithm -where given a crash log, the existing bug database could be mined to find outthe class to which the bug should be allocated. This would involve Miningpatterns and applying different classification algorithms. This paper focuseson the feature extraction, noise reduction in data and classification ofnetwork bugs using probabilistic Na\"ive Bayes approach. Different event modelslike Bernoulli and Multinomial are applied on the extracted features. When new,unseen bugs are given as input to the algorithms, the performance comparison ofdifferent algorithms is done on the basis of accuracy and recall parameters.
arxiv-2700-101 | Joint-ViVo: Selecting and Weighting Visual Words Jointly for Bag-of-Features based Tissue Classification in Medical Images | http://arxiv.org/pdf/1208.3822v2.pdf | author:Jingyan Wang category:cs.CV stat.ML published:2012-08-19 summary:Automatically classifying the tissues types of Region of Interest (ROI) inmedical imaging has been an important application in Computer-Aided Diagnosis(CAD), such as classification of breast parenchymal tissue in the mammogram,classify lung disease patterns in High-Resolution Computed Tomography (HRCT)etc. Recently, bag-of-features method has shown its power in this field,treating each ROI as a set of local features. In this paper, we investigateusing the bag-of-features strategy to classify the tissue types in medicalimaging applications. Two important issues are considered here: the visualvocabulary learning and weighting. Although there are already plenty ofalgorithms to deal with them, all of them treat them independently, namely, thevocabulary learned first and then the histogram weighted. Inspired byAuto-Context who learns the features and classifier jointly, we try to developa novel algorithm that learns the vocabulary and weights jointly. The newalgorithm, called Joint-ViVo, works in an iterative way. In each iteration, wefirst learn the weights for each visual word by maximizing the margin of ROItriplets, and then select the most discriminate visual words based on thelearned weights for the next iteration. We test our algorithm on three tissueclassification tasks: identifying brain tissue type in magnetic resonanceimaging (MRI), classifying lung tissue in HRCT images, and classifying breasttissue density in mammograms. The results show that Joint-ViVo can performeffectively for classifying tissues.
arxiv-2700-102 | Generalization Bounds for Domain Adaptation | http://arxiv.org/pdf/1304.1574v1.pdf | author:Chao Zhang, Lei Zhang, Jieping Ye category:cs.LG math.PR published:2013-04-04 summary:In this paper, we provide a new framework to obtain the generalization boundsof the learning process for domain adaptation, and then apply the derivedbounds to analyze the asymptotical convergence of the learning process. Withoutloss of generality, we consider two kinds of representative domain adaptation:one is with multiple sources and the other is combining source and target data. In particular, we use the integral probability metric to measure thedifference between two domains. For either kind of domain adaptation, wedevelop a related Hoeffding-type deviation inequality and a symmetrizationinequality to achieve the corresponding generalization bound based on theuniform entropy number. We also generalized the classical McDiarmid'sinequality to a more general setting where independent random variables cantake values from different domains. By using this inequality, we then obtaingeneralization bounds based on the Rademacher complexity. Afterwards, weanalyze the asymptotic convergence and the rate of convergence of the learningprocess for such kind of domain adaptation. Meanwhile, we discuss the factorsthat affect the asymptotic behavior of the learning process and the numericalexperiments support our theoretical findings as well.
arxiv-2700-103 | Hiding Image in Image by Five Modulus Method for Image Steganography | http://arxiv.org/pdf/1304.1571v1.pdf | author:Firas A. Jassim category:cs.MM cs.CV published:2013-04-04 summary:This paper is to create a practical steganographic implementation to hidecolor image (stego) inside another color image (cover). The proposed techniqueuses Five Modulus Method to convert the whole pixels within both the cover andthe stego images into multiples of five. Since each pixels inside the stegoimage is divisible by five then the whole stego image could be divided by fiveto get new range of pixels 0..51. Basically, the reminder of each number thatis not divisible by five is either 1,2,3 or 4 when divided by 5. Subsequently,then a 4-by-4 window size has been implemented to accommodate the proposedtechnique. For each 4-by-4 window inside the cover image, a number from 1 to 4could be embedded secretly from the stego image. The previous discussion mustbe applied separately for each of the R, G, and B arrays. Moreover, a stego-keycould be combined with the proposed algorithm to make it difficult for anyadversary to extract the secret image from the cover image. Based on the PSNRvalue, the extracted stego image has high PSNR value. Hence this newsteganography algorithm is very efficient to hide color images.
arxiv-2700-104 | Multiscale Fractal Descriptors Applied to Texture Classification | http://arxiv.org/pdf/1304.1568v1.pdf | author:João Batista Florindo, Odemir Martinez Bruno category:cs.CV published:2013-04-04 summary:This work proposes the combination of multiscale transform with fractaldescriptors employed in the classification of gray-level texture images. Weapply the space-scale transform (derivative + Gaussian filter) over theBouligand-Minkowski fractal descriptors, followed by a threshold over thefilter response, aiming at attenuating noise effects caused by the final partof this response. The method is tested in the classification of a well-knowndata set (Brodatz) and compared with other classical texture descriptortechniques. The results demonstrate the advantage of the proposed approach,achieving a higher success rate with a reduced amount of descriptors.
arxiv-2700-105 | Integration of spatio-temporal contrast sensitivity with a multi-slice channelized Hotelling observer | http://arxiv.org/pdf/1304.1419v1.pdf | author:Ali N. Avanaki, Kathryn S. Espig, Cedric Marchessoux, Elizabeth A. Krupinski, Predrag R. Bakic, Tom R. L. Kimpe, Andrew D. A. Maidment category:cs.CV published:2013-04-04 summary:Barten's model of spatio-temporal contrast sensitivity function of humanvisual system is embedded in a multi-slice channelized Hotelling observer. Thisis done by 3D filtering of the stack of images with the spatio-temporalcontrast sensitivity function and feeding the result (i.e., the perceived imagestack) to the multi-slice channelized Hotelling observer. The proposedprocedure of considering spatio-temporal contrast sensitivity function isgeneric in the sense that it can be used with observers other than multi-slicechannelized Hotelling observer. Detection performance of the new observer indigital breast tomosynthesis is measured in a variety of browsing speeds, attwo spatial sampling rates, using computer simulations. Our results show a peakin detection performance in mid browsing speeds. We compare our results tothose of a human observer study reported earlier (I. Diaz et al. SPIE MI 2011).The effects of display luminance, contrast and spatial sampling rate, with andwithout considering foveal vision, are also studied. Reported simulations areconducted with real digital breast tomosynthesis image stacks, as well asstacks from an anthropomorphic software breast phantom (P. Bakic et al. MedPhys. 2011). Lesion cases are simulated by inserting singlemicro-calcifications or masses. Limitations of our methods and ways to improvethem are discussed.
arxiv-2700-106 | Restoration of Images Corrupted by Impulse Noise and Mixed Gaussian Impulse Noise using Blind Inpainting | http://arxiv.org/pdf/1304.1408v1.pdf | author:Ming Yan category:math.OC cs.CV math.NA published:2013-04-04 summary:This article studies the problem of image restoration of observed imagescorrupted by impulse noise and mixed Gaussian impulse noise. Since the pixelsdamaged by impulse noise contain no information about the true image, how tofind this set correctly is a very important problem. We propose two methodsbased on blind inpainting and $\ell_0$ minimization that can simultaneouslyfind the damaged pixels and restore the image. By iteratively restoring theimage and updating the set of damaged pixels, these methods have betterperformance than other methods, as shown in the experiments. In addition, weprovide convergence analysis for these methods, these algorithms will convergeto coordinatewise minimum points. In addition, they will converge to localminimum points (or with probability one) with some modifications in thealgorithms.
arxiv-2700-107 | Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback | http://arxiv.org/pdf/1204.1259v2.pdf | author:Balázs Hidasi, Domonkos Tikk category:cs.LG cs.IR cs.NA published:2012-04-05 summary:Albeit, the implicit feedback based recommendation problem - when only theuser history is available but there are no ratings - is the most typicalsetting in real-world applications, it is much less researched than theexplicit feedback case. State-of-the-art algorithms that are efficient on theexplicit case cannot be straightforwardly transformed to the implicit case ifscalability should be maintained. There are few if any implicit feedbackbenchmark datasets, therefore new ideas are usually experimented on explicitbenchmarks. In this paper, we propose a generic context-aware implicit feedbackrecommender algorithm, coined iTALS. iTALS apply a fast, ALS-based tensorfactorization learning method that scales linearly with the number of non-zeroelements in the tensor. The method also allows us to incorporate diversecontext information into the model while maintaining its computationalefficiency. In particular, we present two such context-aware implementationvariants of iTALS. The first incorporates seasonality and enables todistinguish user behavior in different time intervals. The other views the userhistory as sequential information and has the ability to recognize usagepattern typical to certain group of items, e.g. to automatically tell apartproduct types or categories that are typically purchased repetitively(collectibles, grocery goods) or once (household appliances). Experimentsperformed on three implicit datasets (two proprietary ones and an implicitvariant of the Netflix dataset) show that by integrating context-awareinformation with our factorization framework into the state-of-the-art implicitrecommender algorithm the recommendation quality improves significantly.
arxiv-2700-108 | Fast SVM training using approximate extreme points | http://arxiv.org/pdf/1304.1391v1.pdf | author:Manu Nandan, Pramod P. Khargonekar, Sachin S. Talathi category:cs.LG published:2013-04-04 summary:Applications of non-linear kernel Support Vector Machines (SVMs) to largedatasets is seriously hampered by its excessive training time. We propose amodification, called the approximate extreme points support vector machine(AESVM), that is aimed at overcoming this burden. Our approach relies onconducting the SVM optimization over a carefully selected subset, called therepresentative set, of the training dataset. We present analytical results thatindicate the similarity of AESVM and SVM solutions. A linear time algorithmbased on convex hulls and extreme points is used to compute the representativeset in kernel space. Extensive computational experiments on nine datasetscompared AESVM to LIBSVM \citep{LIBSVM}, CVM \citep{Tsang05}, BVM\citep{Tsang07}, LASVM \citep{Bordes05}, $\text{SVM}^{\text{perf}}$\citep{Joachims09}, and the random features method \citep{rahimi07}. Our AESVMimplementation was found to train much faster than the other methods, while itsclassification accuracy was similar to that of LIBSVM in all cases. Inparticular, for a seizure detection dataset, AESVM training was almost $10^3$times faster than LIBSVM and LASVM and more than forty times faster than CVMand BVM. Additionally, AESVM also gave competitively fast classification times.
arxiv-2700-109 | Stochastic model for the vocabulary growth in natural languages | http://arxiv.org/pdf/1212.1362v3.pdf | author:Martin Gerlach, Eduardo G. Altmann category:physics.soc-ph cs.CL published:2012-12-06 summary:We propose a stochastic model for the number of different words in a givendatabase which incorporates the dependence on the database size and historicalchanges. The main feature of our model is the existence of two differentclasses of words: (i) a finite number of core-words which have higher frequencyand do not affect the probability of a new word to be used; and (ii) theremaining virtually infinite number of noncore-words which have lower frequencyand once used reduce the probability of a new word to be used in the future.Our model relies on a careful analysis of the google-ngram database of bookspublished in the last centuries and its main consequence is the generalizationof Zipf's and Heaps' law to two scaling regimes. We confirm that thesegeneralizations yield the best simple description of the data among genericdescriptive models and that the two free parameters depend only on the languagebut not on the database. From the point of view of our model the main change onhistorical time scales is the composition of the specific words included in thefinite list of core-words, which we observe to decay exponentially in time witha rate of approximately 30 words per year for English.
arxiv-2700-110 | Classification of Human Epithelial Type 2 Cell Indirect Immunofluoresence Images via Codebook Based Descriptors | http://arxiv.org/pdf/1304.1262v1.pdf | author:Arnold Wiliem, Yongkang Wong, Conrad Sanderson, Peter Hobson, Shaokang Chen, Brian C. Lovell category:q-bio.CB cs.CV q-bio.QM published:2013-04-04 summary:The Anti-Nuclear Antibody (ANA) clinical pathology test is commonly used toidentify the existence of various diseases. A hallmark method for identifyingthe presence of ANAs is the Indirect Immunofluorescence method on HumanEpithelial (HEp-2) cells, due to its high sensitivity and the large range ofantigens that can be detected. However, the method suffers from numerousshortcomings, such as being subjective as well as time and labour intensive.Computer Aided Diagnostic (CAD) systems have been developed to address theseproblems, which automatically classify a HEp-2 cell image into one of its knownpatterns (eg., speckled, homogeneous). Most of the existing CAD systems usehandpicked features to represent a HEp-2 cell image, which may only work inlimited scenarios. In this paper, we propose a cell classification systemcomprised of a dual-region codebook-based descriptor, combined with the NearestConvex Hull Classifier. We evaluate the performance of several variants of thedescriptor on two publicly available datasets: ICPR HEp-2 cell classificationcontest dataset and the new SNPHEp-2 dataset. To our knowledge, this is thefirst time codebook-based descriptors are applied and studied in this domain.Experiments show that the proposed system has consistent high performance andis more robust than two recent CAD systems.
arxiv-2700-111 | Fast Approximate L_infty Minimization: Speeding Up Robust Regression | http://arxiv.org/pdf/1304.1250v1.pdf | author:Fumin Shen, Chunhua Shen, Rhys Hill, Anton van den Hengel, Zhenmin Tang category:cs.CV stat.CO published:2013-04-04 summary:Minimization of the $L_\infty$ norm, which can be viewed as approximatelysolving the non-convex least median estimation problem, is a powerful methodfor outlier removal and hence robust regression. However, current techniquesfor solving the problem at the heart of $L_\infty$ norm minimization are slow,and therefore cannot scale to large problems. A new method for the minimizationof the $L_\infty$ norm is presented here, which provides a speedup of multipleorders of magnitude for data with high dimension. This method, termed Fast$L_\infty$ Minimization, allows robust regression to be applied to a class ofproblems which were previously inaccessible. It is shown how the $L_\infty$norm minimization problem can be broken up into smaller sub-problems, which canthen be solved extremely efficiently. Experimental results demonstrate theradical reduction in computation time, along with robustness against largenumbers of outliers in a few model-fitting problems.
arxiv-2700-112 | Shadow Detection: A Survey and Comparative Evaluation of Recent Methods | http://arxiv.org/pdf/1304.1233v1.pdf | author:Andres Sanin, Conrad Sanderson, Brian C. Lovell category:cs.CV cs.RO published:2013-04-04 summary:This paper presents a survey and a comparative evaluation of recenttechniques for moving cast shadow detection. We identify shadow removal as acritical step for improving object detection and tracking. The survey coversmethods published during the last decade, and places them in a feature-basedtaxonomy comprised of four categories: chromacity, physical, geometry andtextures. A selection of prominent methods across the categories is compared interms of quantitative performance measures (shadow detection and discriminationrates, colour desaturation) as well as qualitative observations. Furthermore,we propose the use of tracking performance as an unbiased approach fordetermining the practical usefulness of shadow detection methods. Theevaluation indicates that all shadow detection approaches make differentcontributions and all have individual strength and weaknesses. Out of theselected methods, the geometry-based technique has strict assumptions and isnot generalisable to various environments, but it is a straightforward choicewhen the objects of interest are easy to model and their shadows have differentorientation. The chromacity based method is the fastest to implement and run,but it is sensitive to noise and less effective in low saturated scenes. Thephysical method improves upon the accuracy of the chromacity method by adaptingto local shadow models, but fails when the spectral properties of the objectsare similar to that of the background. The small-region texture based method isespecially robust for pixels whose neighbourhood is textured, but may takelonger to implement and is the most computationally expensive. The large-regiontexture based method produces the most accurate results, but has a significantcomputational load due to its multiple processing steps.
arxiv-2700-113 | Highly comparative time-series analysis: The empirical structure of time series and their methods | http://arxiv.org/pdf/1304.1209v1.pdf | author:Ben D. Fulcher, Max A. Little, Nick S. Jones category:cs.CV physics.bio-ph q-bio.QM stat.ML published:2013-04-03 summary:The process of collecting and organizing sets of observations represents acommon theme throughout the history of science. However, despite the ubiquityof scientists measuring, recording, and analyzing the dynamics of differentprocesses, an extensive organization of scientific time-series data andanalysis methods has never been performed. Addressing this, annotatedcollections of over 35 000 real-world and model-generated time series and over9000 time-series analysis algorithms are analyzed in this work. We introducereduced representations of both time series, in terms of their propertiesmeasured by diverse scientific methods, and of time-series analysis methods, interms of their behaviour on empirical time series, and use them to organizethese interdisciplinary resources. This new approach to comparing acrossdiverse scientific data and methods allows us to organize time-series datasetsautomatically according to their properties, retrieve alternatives toparticular analysis methods developed in other scientific disciplines, andautomate the selection of useful methods for time-series classification andregression tasks. The broad scientific utility of these tools is demonstratedon datasets of electroencephalograms, self-affine time series, heart beatintervals, speech signals, and others, in each case contributing novel analysistechniques to the existing literature. Highly comparative techniques thatcompare across an interdisciplinary literature can thus be used to guide morefocused research in time-series analysis for applications across the scientificdisciplines.
arxiv-2700-114 | Efficient Distance Metric Learning by Adaptive Sampling and Mini-Batch Stochastic Gradient Descent (SGD) | http://arxiv.org/pdf/1304.1192v1.pdf | author:Qi Qian, Rong Jin, Jinfeng Yi, Lijun Zhang, Shenghuo Zhu category:cs.LG published:2013-04-03 summary:Distance metric learning (DML) is an important task that has foundapplications in many domains. The high computational cost of DML arises fromthe large number of variables to be determined and the constraint that adistance metric has to be a positive semi-definite (PSD) matrix. Althoughstochastic gradient descent (SGD) has been successfully applied to improve theefficiency of DML, it can still be computationally expensive because in orderto ensure that the solution is a PSD matrix, it has to, at every iteration,project the updated distance metric onto the PSD cone, an expensive operation.We address this challenge by developing two strategies within SGD, i.e.mini-batch and adaptive sampling, to effectively reduce the number of updates(i.e., projections onto the PSD cone) in SGD. We also develop hybrid approachesthat combine the strength of adaptive sampling with that of mini-batch onlinelearning techniques to further improve the computational efficiency of SGD forDML. We prove the theoretical guarantees for both adaptive sampling andmini-batch based approaches for DML. We also conduct an extensive empiricalstudy to verify the effectiveness of the proposed algorithms for DML.
arxiv-2700-115 | Predator confusion is sufficient to evolve swarming behavior | http://arxiv.org/pdf/1209.3330v3.pdf | author:Randal S. Olson, Arend Hintze, Fred C. Dyer, David B. Knoester, Christoph Adami category:q-bio.PE cs.NE nlin.AO q-bio.NC published:2012-09-14 summary:Swarming behaviors in animals have been extensively studied due to theirimplications for the evolution of cooperation, social cognition, andpredator-prey dynamics. An important goal of these studies is discerning whichevolutionary pressures favor the formation of swarms. One hypothesis is thatswarms arise because the presence of multiple moving prey in swarms causesconfusion for attacking predators, but it remains unclear how important thisselective force is. Using an evolutionary model of a predator-prey system, weshow that predator confusion provides a sufficient selection pressure to evolveswarming behavior in prey. Furthermore, we demonstrate that the evolutionaryeffect of predator confusion on prey could in turn exert pressure on thestructure of the predator's visual field, favoring the frontally oriented,high-resolution visual systems commonly observed in predators that feed onswarming animals. Finally, we provide evidence that when prey evolve swarmingin response to predator confusion, there is a change in the shape of thefunctional response curve describing the predator's consumption rate as preydensity increases. Thus, we show that a relatively simple perceptualconstraint--predator confusion--could have pervasive evolutionary effects onprey behavior, predator sensory mechanisms, and the ecological interactionsbetween predators and prey.
arxiv-2700-116 | Heteroscedastic Conditional Ordinal Random Fields for Pain Intensity Estimation from Facial Images | http://arxiv.org/pdf/1301.5063v2.pdf | author:Ognjen Rudovic, Maja Pantic, Vladimir Pavlovic category:cs.CV cs.LG stat.ML published:2013-01-22 summary:We propose a novel method for automatic pain intensity estimation from facialimages based on the framework of kernel Conditional Ordinal Random Fields(KCORF). We extend this framework to account for heteroscedasticity on theoutput labels(i.e., pain intensity scores) and introduce a novel dynamicfeatures, dynamic ranks, that impose temporal ordinal constraints on the staticranks (i.e., intensity scores). Our experimental results show that the proposedapproach outperforms state-of-the art methods for sequence classification withordinal data and other ordinal regression models. The approach performssignificantly better than other models in terms of Intra-Class Correlationmeasure, which is the most accepted evaluation measure in the tasks of facialbehaviour intensity estimation.
arxiv-2700-117 | A software for aging faces applied to ancient marble busts | http://arxiv.org/pdf/1304.1022v1.pdf | author:Amelia Carolina Sparavigna category:cs.CV published:2013-04-03 summary:The study and development of software able to show the effect of aging offaces is one of the tasks of face recognition technologies. Some softwaresolutions are used for investigations, some others to show the effects of drugson healthy appearance, however some other applications can be proposed for theanalysis of visual arts. Here we use a freely available software, which isproviding interesting results, for the comparison of ancient marble busts. Ananalysis of Augustus busts is proposed.
arxiv-2700-118 | Scale Selection of Adaptive Kernel Regression by Joint Saliency Map for Nonrigid Image Registration | http://arxiv.org/pdf/1303.0479v2.pdf | author:Zhuangming Shen, Jiuai Sun, Hui Zhang, Binjie Qin category:cs.CV published:2013-03-03 summary:Joint saliency map (JSM) [1] was developed to assign high joint saliencyvalues to the corresponding saliency structures (called Joint SaliencyStructures, JSSs) but zero or low joint saliency values to the outliers (ormismatches) that are introduced by missing correspondence or local largedeformations between the reference and moving images to be registered. JSMguides the local structure matching in nonrigid registration by emphasizingthese JSSs' sparse deformation vectors in adaptive kernel regression ofhierarchical sparse deformation vectors for iterative dense deformationreconstruction. By designing an effective superpixel-based local structurescale estimator to compute the reference structure's structure scale, wefurther propose to determine the scale (the width) of kernels in the adaptivekernel regression through combining the structure scales to JSM-based scales ofmismatch between the local saliency structures. Therefore, we can adaptivelyselect the sample size of sparse deformation vectors to reconstruct the densedeformation vectors for accurately matching the every local structures in thetwo images. The experimental results demonstrate better accuracy of our methodin aligning two images with missing correspondence and local large deformationthan the state-of-the-art methods.
arxiv-2700-119 | Transferring Subspaces Between Subjects in Brain-Computer Interfacing | http://arxiv.org/pdf/1209.4115v2.pdf | author:Wojciech Samek, Frank C. Meinecke, Klaus-Robert Müller category:stat.ML cs.HC cs.LG published:2012-09-18 summary:Compensating changes between a subjects' training and testing session inBrain Computer Interfacing (BCI) is challenging but of great importance for arobust BCI operation. We show that such changes are very similar betweensubjects, thus can be reliably estimated using data from other users andutilized to construct an invariant feature space. This novel approach tolearning from other subjects aims to reduce the adverse effects of commonnon-stationarities, but does not transfer discriminative information. This isan important conceptual difference to standard multi-subject methods that e.g.improve the covariance matrix estimation by shrinking it towards the average ofother users or construct a global feature space. These methods do not reducesthe shift between training and test data and may produce poor results whensubjects have very different signal characteristics. In this paper we compareour approach to two state-of-the-art multi-subject methods on toy data and twodata sets of EEG recordings from subjects performing motor imagery. We showthat it can not only achieve a significant increase in performance, but alsothat the extracted change patterns allow for a neurophysiologically meaningfulinterpretation.
arxiv-2700-120 | Adaptive Graph via Multiple Kernel Learning for Nonnegative Matrix Factorization | http://arxiv.org/pdf/1208.3845v3.pdf | author:Jing-Yan Wang, Mustafa AbdulJabbar category:cs.LG cs.CV stat.ML published:2012-08-19 summary:Nonnegative Matrix Factorization (NMF) has been continuously evolving inseveral areas like pattern recognition and information retrieval methods. Itfactorizes a matrix into a product of 2 low-rank non-negative matrices thatwill define parts-based, and linear representation of nonnegative data.Recently, Graph regularized NMF (GrNMF) is proposed to find a compactrepresentation,which uncovers the hidden semantics and simultaneously respectsthe intrinsic geometric structure. In GNMF, an affinity graph is constructedfrom the original data space to encode the geometrical information. In thispaper, we propose a novel idea which engages a Multiple Kernel Learningapproach into refining the graph structure that reflects the factorization ofthe matrix and the new data space. The GrNMF is improved by utilizing the graphrefined by the kernel learning, and then a novel kernel learning method isintroduced under the GrNMF framework. Our approach shows encouraging results ofthe proposed algorithm in comparison to the state-of-the-art clusteringalgorithms like NMF, GrNMF, SVD etc.
arxiv-2700-121 | Discriminative Sparse Coding on Multi-Manifold for Data Representation and Classification | http://arxiv.org/pdf/1208.3839v2.pdf | author:Jing-Yan Wang category:cs.CV cs.LG stat.ML published:2012-08-19 summary:Sparse coding has been popularly used as an effective data representationmethod in various applications, such as computer vision, medical imaging andbioinformatics, etc. However, the conventional sparse coding algorithms and itsmanifold regularized variants (graph sparse coding and Laplacian sparsecoding), learn the codebook and codes in a unsupervised manner and neglect theclass information available in the training set. To address this problem, inthis paper we propose a novel discriminative sparse coding method based onmulti-manifold, by learning discriminative class-conditional codebooks andsparse codes from both data feature space and class labels. First, the entiretraining set is partitioned into multiple manifolds according to the classlabels. Then, we formulate the sparse coding as a manifold-manifold matchingproblem and learn class-conditional codebooks and codes to maximize themanifold margins of different classes. Lastly, we present a data point-manifoldmatching error based strategy to classify the unlabeled data point.Experimental results on somatic mutations identification and breast tumorsclassification in ultrasonic images tasks demonstrate the efficacy of theproposed data representation-classification approach.
arxiv-2700-122 | Improved Anomaly Detection in Crowded Scenes via Cell-based Analysis of Foreground Speed, Size and Texture | http://arxiv.org/pdf/1304.0886v1.pdf | author:Vikas Reddy, Conrad Sanderson, Brian C. Lovell category:cs.CV published:2013-04-03 summary:A robust and efficient anomaly detection technique is proposed, capable ofdealing with crowded scenes where traditional tracking based approaches tend tofail. Initial foreground segmentation of the input frames confines the analysisto foreground objects and effectively ignores irrelevant background dynamics.Input frames are split into non-overlapping cells, followed by extractingfeatures based on motion, size and texture from each cell. Each feature type isindependently analysed for the presence of an anomaly. Unlike most methods, arefined estimate of object motion is achieved by computing the optical flow ofonly the foreground pixels. The motion and size features are modelled by anapproximated version of kernel density estimation, which is computationallyefficient even for large training datasets. Texture features are modelled by anadaptively grown codebook, with the number of entries in the codebook selectedin an online fashion. Experiments on the recently published UCSD AnomalyDetection dataset show that the proposed method obtains considerably betterresults than three recent approaches: MPPCA, social force, and mixture ofdynamic textures (MDT). The proposed method is also several orders of magnitudefaster than MDT, the next best performing method.
arxiv-2700-123 | Learning DNF Expressions from Fourier Spectrum | http://arxiv.org/pdf/1203.0594v3.pdf | author:Vitaly Feldman category:cs.LG cs.CC cs.DS published:2012-03-03 summary:Since its introduction by Valiant in 1984, PAC learning of DNF expressionsremains one of the central problems in learning theory. We consider thisproblem in the setting where the underlying distribution is uniform, or moregenerally, a product distribution. Kalai, Samorodnitsky and Teng (2009) showedthat in this setting a DNF expression can be efficiently approximated from its"heavy" low-degree Fourier coefficients alone. This is in contrast to previousapproaches where boosting was used and thus Fourier coefficients of the targetfunction modified by various distributions were needed. This property iscrucial for learning of DNF expressions over smoothed product distributions, alearning model introduced by Kalai et al. (2009) and inspired by the seminalsmoothed analysis model of Spielman and Teng (2001). We introduce a new approach to learning (or approximating) a polynomialthreshold functions which is based on creating a function with range [-1,1]that approximately agrees with the unknown function on low-degree Fouriercoefficients. We then describe conditions under which this is sufficient forlearning polynomial threshold functions. Our approach yields a new, simplealgorithm for approximating any polynomial-size DNF expression from its "heavy"low-degree Fourier coefficients alone. Our algorithm greatly simplifies theproof of learnability of DNF expressions over smoothed product distributions.We also describe an application of our algorithm to learning monotone DNFexpressions over product distributions. Building on the work of Servedio(2001), we give an algorithm that runs in time $\poly((s \cdot\log{(s/\eps)})^{\log{(s/\eps)}}, n)$, where $s$ is the size of the target DNFexpression and $\eps$ is the accuracy. This improves on $\poly((s \cdot\log{(ns/\eps)})^{\log{(s/\eps)} \cdot \log{(1/\eps)}}, n)$ bound of Servedio(2001).
arxiv-2700-124 | A Fast Semidefinite Approach to Solving Binary Quadratic Problems | http://arxiv.org/pdf/1304.0840v1.pdf | author:Peng Wang, Chunhua Shen, Anton van den Hengel category:cs.CV cs.LG published:2013-04-03 summary:Many computer vision problems can be formulated as binary quadratic programs(BQPs). Two classic relaxation methods are widely used for solving BQPs,namely, spectral methods and semidefinite programming (SDP), each with theirown advantages and disadvantages. Spectral relaxation is simple and easy toimplement, but its bound is loose. Semidefinite relaxation has a tighter bound,but its computational complexity is high for large scale problems. We present anew SDP formulation for BQPs, with two desirable properties. First, it has asimilar relaxation bound to conventional SDP formulations. Second, comparedwith conventional SDP methods, the new SDP formulation leads to a significantlymore efficient and scalable dual optimization approach, which has the samedegree of complexity as spectral methods. Extensive experiments on variousapplications including clustering, image segmentation, co-segmentation andregistration demonstrate the usefulness of our SDP formulation for solvinglarge-scale BQPs.
arxiv-2700-125 | Multiscale Hybrid Non-local Means Filtering Using Modified Similarity Measure | http://arxiv.org/pdf/1304.0839v1.pdf | author:Zahid Hussain Shamsi, Dai-Gyoung Kim category:cs.CV published:2013-04-03 summary:A new multiscale implementation of non-local means filtering for imagedenoising is proposed. The proposed algorithm also introduces a modification ofsimilarity measure for patch comparison. The standard Euclidean norm isreplaced by weighted Euclidean norm for patch based comparison. Assuming thepatch as an oriented surface, notion of normal vector patch is being associatedwith each patch. The inner product of these normal vector patches is then usedin weighted Euclidean distance of photometric patches as the weight factor. Thealgorithm involves two steps: The first step is multiscale implementation of anaccelerated non-local means filtering in the stationary wavelet domain toobtain a refined version of the noisy patches for later comparison. This stepis inspired by a preselection phase of finding similar patches in variousnon-local means approaches. The next step is to apply the modified non-localmeans filtering to the noisy image using the reference patches obtained in thefirst step. These refined patches contain less noise, and consequently thecomputation of normal vectors and partial derivatives is more accurate.Experimental results indicate equivalent or better performance of proposedalgorithm as compared to various state of the art algorithms.
arxiv-2700-126 | Lie Algebrized Gaussians for Image Representation | http://arxiv.org/pdf/1304.0823v1.pdf | author:Liyu Gong, Meng Chen, Chunlong Hu category:cs.CV published:2013-04-03 summary:We present an image representation method which is derived from analyzingGaussian probability density function (\emph{pdf}) space using Lie grouptheory. In our proposed method, images are modeled by Gaussian mixture models(GMMs) which are adapted from a globally trained GMM called universalbackground model (UBM). Then we vectorize the GMMs based on two facts: (1)components of image-specific GMMs are closely grouped together around theircorresponding component of the UBM due to the characteristic of the UBMadaption procedure; (2) Gaussian \emph{pdf}s form a Lie group, which is adifferentiable manifold rather than a vector space. We map each Gaussiancomponent to the tangent vector space (named Lie algebra) of Lie group at themanifold position of UBM. The final feature vector, named Lie algebrizedGaussians (LAG) is then constructed by combining the Lie algebrized Gaussiancomponents with mixture weights. We apply LAG features to scene categoryrecognition problem and observe state-of-the-art performance on 15Scenesbenchmark.
arxiv-2700-127 | O(logT) Projections for Stochastic Optimization of Smooth and Strongly Convex Functions | http://arxiv.org/pdf/1304.0740v1.pdf | author:Lijun Zhang, Tianbao Yang, Rong Jin, Xiaofei He category:cs.LG published:2013-04-02 summary:Traditional algorithms for stochastic optimization require projecting thesolution at each iteration into a given domain to ensure its feasibility. Whenfacing complex domains, such as positive semi-definite cones, the projectionoperation can be expensive, leading to a high computational cost per iteration.In this paper, we present a novel algorithm that aims to reduce the number ofprojections for stochastic optimization. The proposed algorithm combines thestrength of several recent developments in stochastic optimization, includingmini-batch, extra-gradient, and epoch gradient descent, in order to effectivelyexplore the smoothness and strong convexity. We show, both in expectation andwith a high probability, that when the objective function is both smooth andstrongly convex, the proposed algorithm achieves the optimal $O(1/T)$ rate ofconvergence with only $O(\log T)$ projections. Our empirical study verifies thetheoretical result.
arxiv-2700-128 | Representation, Approximation and Learning of Submodular Functions Using Low-rank Decision Trees | http://arxiv.org/pdf/1304.0730v1.pdf | author:Vitaly Feldman, Pravesh Kothari, Jan Vondrak category:cs.LG cs.CC cs.DS published:2013-04-02 summary:We study the complexity of approximate representation and learning ofsubmodular functions over the uniform distribution on the Boolean hypercube$\{0,1\}^n$. Our main result is the following structural theorem: anysubmodular function is $\epsilon$-close in $\ell_2$ to a real-valued decisiontree (DT) of depth $O(1/\epsilon^2)$. This immediately implies that anysubmodular function is $\epsilon$-close to a function of at most$2^{O(1/\epsilon^2)}$ variables and has a spectral $\ell_1$ norm of$2^{O(1/\epsilon^2)}$. It also implies the closest previous result that statesthat submodular functions can be approximated by polynomials of degree$O(1/\epsilon^2)$ (Cheraghchi et al., 2012). Our result is proved byconstructing an approximation of a submodular function by a DT of rank$4/\epsilon^2$ and a proof that any rank-$r$ DT can be $\epsilon$-approximatedby a DT of depth $\frac{5}{2}(r+\log(1/\epsilon))$. We show that these structural results can be exploited to give anattribute-efficient PAC learning algorithm for submodular functions running intime $\tilde{O}(n^2) \cdot 2^{O(1/\epsilon^{4})}$. The best previous algorithmfor the problem requires $n^{O(1/\epsilon^{2})}$ time and examples (Cheraghchiet al., 2012) but works also in the agnostic setting. In addition, we giveimproved learning algorithms for a number of related settings. We also prove that our PAC and agnostic learning algorithms are essentiallyoptimal via two lower bounds: (1) an information-theoretic lower bound of$2^{\Omega(1/\epsilon^{2/3})}$ on the complexity of learning monotonesubmodular functions in any reasonable model; (2) computational lower bound of$n^{\Omega(1/\epsilon^{2/3})}$ based on a reduction to learning of sparseparities with noise, widely-believed to be intractable. These are the firstlower bounds for learning of submodular functions over the uniformdistribution.
arxiv-2700-129 | Pedestrian Detection with Unsupervised Multi-Stage Feature Learning | http://arxiv.org/pdf/1212.0142v2.pdf | author:Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, Yann LeCun category:cs.CV cs.LG published:2012-12-01 summary:Pedestrian detection is a problem of considerable practical interest. Addingto the list of successful applications of deep learning methods to vision, wereport state-of-the-art and competitive results on all major pedestriandatasets with a convolutional network model. The model uses a few new twists,such as multi-stage features, connections that skip layers to integrate globalshape information with local distinctive motif information, and an unsupervisedmethod based on convolutional sparse coding to pre-train the filters at eachstage.
arxiv-2700-130 | Sparsistent Estimation of Time-Varying Discrete Markov Random Fields | http://arxiv.org/pdf/0907.2337v2.pdf | author:Mladen Kolar, Eric P. Xing category:stat.ML published:2009-07-14 summary:Network models have been popular for modeling and representing complexrelationships and dependencies between observed variables. When data comes froma dynamic stochastic process, a single static network model cannot adequatelycapture transient dependencies, such as, gene regulatory dependenciesthroughout a developmental cycle of an organism. Kolar et al (2010b) proposed amethod based on kernel-smoothing l1-penalized logistic regression forestimating time-varying networks from nodal observations collected from atime-series of observational data. In this paper, we establish conditions underwhich the proposed method consistently recovers the structure of a time-varyingnetwork. This work complements previous empirical findings by providing soundtheoretical guarantees for the proposed estimation procedure. For completeness,we include numerical simulations in the paper.
arxiv-2700-131 | Event management for large scale event-driven digital hardware spiking neural networks | http://arxiv.org/pdf/1304.0640v1.pdf | author:Louis-Charles Caron, \and Michiel D'Haene, \and Frédéric Mailhot, \and Benjamin Schrauwen, \and Jean Rouat category:cs.NE cs.AI cs.DC published:2013-04-02 summary:The interest in brain-like computation has led to the design of a plethora ofinnovative neuromorphic systems. Individually, spiking neural networks (SNNs),event-driven simulation and digital hardware neuromorphic systems get a lot ofattention. Despite the popularity of event-driven SNNs in software, very fewdigital hardware architectures are found. This is because existing hardwaresolutions for event management scale badly with the number of events. Thispaper introduces the structured heap queue, a pipelined digital hardware datastructure, and demonstrates its suitability for event management. Thestructured heap queue scales gracefully with the number of events, allowing theefficient implementation of large scale digital hardware event-driven SNNs. Thescaling is linear for memory, logarithmic for logic resources and constant forprocessing time. The use of the structured heap queue is demonstrated onfield-programmable gate array (FPGA) with an image segmentation experiment anda SNN of 65~536 neurons and 513~184 synapses. Events can be processed at therate of 1 every 7 clock cycles and a 406$\times$158 pixel image is segmented in200 ms.
arxiv-2700-132 | A Semiparametric Bayesian Extreme Value Model Using a Dirichlet Process Mixture of Gamma Densities | http://arxiv.org/pdf/1304.0596v1.pdf | author:Jairo Fuquene category:stat.ML stat.AP stat.ME published:2013-04-02 summary:In this paper we propose a model with a Dirichlet process mixture of gammadensities in the bulk part below threshold and a generalized Pareto density inthe tail for extreme value estimation. The proposed model is simple andflexible allowing us posterior density estimation and posterior inference forhigh quantiles. The model works well even for small sample sizes and in theabsence of prior information. We evaluate the performance of the proposed modelthrough a simulation study. Finally, the proposed model is applied to a realenvironmental data.
arxiv-2700-133 | Evolution and the structure of learning agents | http://arxiv.org/pdf/1209.3818v4.pdf | author:Alok Raj category:cs.AI cs.LG I.2; I.2.6 published:2012-09-18 summary:This paper presents the thesis that all learning agents of finite informationsize are limited by their informational structure in what goals they canefficiently learn to achieve in a complex environment. Evolutionary change iscritical for creating the required structure for all learning agents in anycomplex environment. The thesis implies that there is no efficient universallearning algorithm. An agent can go past the learning limits imposed by itsstructure only by slow evolutionary change or blind search which in a verycomplex environment can only give an agent an inefficient universal learningcapability that can work only in evolutionary timescales or improbable luck.
arxiv-2700-134 | Stroke-Based Cursive Character Recognition | http://arxiv.org/pdf/1304.0421v1.pdf | author:K. C. Santosh, E. Iwata category:cs.CV published:2013-04-01 summary:Human eye can see and read what is written or displayed either in naturalhandwriting or in printed format. The same work in case the machine does iscalled handwriting recognition. Handwriting recognition can be broken down intotwo categories: off-line and on-line. ...
arxiv-2700-135 | An improved quasar detection method in EROS-2 and MACHO LMC datasets | http://arxiv.org/pdf/1304.0401v1.pdf | author:Karim Pichara, Pavlos Protopapas, Dae-Won Kim, Jean-Baptiste Marquette, Patrick Tisserand category:astro-ph.IM stat.ML published:2013-04-01 summary:We present a new classification method for quasar identification in theEROS-2 and MACHO datasets based on a boosted version of Random Forestclassifier. We use a set of variability features including parameters of acontinuous auto regressive model. We prove that continuous auto regressiveparameters are very important discriminators in the classification process. Wecreate two training sets (one for EROS-2 and one for MACHO datasets) usingknown quasars found in the LMC. Our model's accuracy in both EROS-2 and MACHOtraining sets is about 90% precision and 86% recall, improving the state of theart models accuracy in quasar detection. We apply the model on the complete,including 28 million objects, EROS-2 and MACHO LMC datasets, finding 1160 and2551 candidates respectively. To further validate our list of candidates, wecrossmatched our list with a previous 663 known strong candidates, getting 74%of matches for MACHO and 40% in EROS-2. The main difference on matching levelis because EROS-2 is a slightly shallower survey which translates tosignificantly lower signal-to-noise ratio lightcurves.
arxiv-2700-136 | On Power-law Kernels, corresponding Reproducing Kernel Hilbert Space and Applications | http://arxiv.org/pdf/1204.1800v2.pdf | author:Debarghya Ghoshdastidar, Ambedkar Dukkipati category:cs.LG cs.IT math.IT stat.ML published:2012-04-09 summary:The role of kernels is central to machine learning. Motivated by theimportance of power-law distributions in statistical modeling, in this paper,we propose the notion of power-law kernels to investigate power-laws inlearning problem. We propose two power-law kernels by generalizing Gaussian andLaplacian kernels. This generalization is based on distributions, arising outof maximization of a generalized information measure known as nonextensiveentropy that is very well studied in statistical mechanics. We prove that theproposed kernels are positive definite, and provide some insights regarding thecorresponding Reproducing Kernel Hilbert Space (RKHS). We also study practicalsignificance of both kernels in classification and regression, and present somesimulation results.
arxiv-2700-137 | Fast Feature Reduction in intrusion detection datasets | http://arxiv.org/pdf/1305.2388v1.pdf | author:Shafigh Parsazad, Ehsan Saboori, Amin Allahyar category:cs.CR cs.LG published:2013-04-01 summary:In the most intrusion detection systems (IDS), a system tries to learncharacteristics of different type of attacks by analyzing packets that sent orreceived in network. These packets have a lot of features. But not all of themis required to be analyzed to detect that specific type of attack. Detectionspeed and computational cost is another vital matter here, because in thesetypes of problems, datasets are very huge regularly. In this paper we tried topropose a very simple and fast feature selection method to eliminate featureswith no helpful information on them. Result faster learning in process ofredundant feature omission. We compared our proposed method with three mostsuccessful similarity based feature selection algorithm including CorrelationCoefficient, Least Square Regression Error and Maximal Information CompressionIndex. After that we used recommended features by each of these algorithms intwo popular classifiers including: Bayes and KNN classifier to measure thequality of the recommendations. Experimental result shows that although theproposed method can't outperform evaluated algorithms with high differences inaccuracy, but in computational cost it has huge superiority over them.
arxiv-2700-138 | A cookbook of translating English to Xapi | http://arxiv.org/pdf/1304.0715v1.pdf | author:Ladislau Bölöni category:cs.AI cs.CL published:2013-03-31 summary:The Xapagy cognitive architecture had been designed to perform narrativereasoning: to model and mimic the activities performed by humans whenwitnessing, reading, recalling, narrating and talking about stories. Xapagycommunicates with the outside world using Xapi, a simplified, "pidgin" languagewhich is strongly tied to the internal representation model (instances, scenesand verb instances) and reasoning techniques (shadows and headless shadows).While not fully a semantic equivalent of natural language, Xapi can represent awide range of complex stories. We illustrate the representation technique usedin Xapi through examples taken from folk physics, folk psychology as well assome more unusual literary examples. We argue that while the Xapi modelrepresents a conceptual shift from the English representation, the mapping islogical and consistent, and a trained knowledge engineer can translate betweenEnglish and Xapi at near-native speed.
arxiv-2700-139 | Compressive adaptive computational ghost imaging | http://arxiv.org/pdf/1304.0243v1.pdf | author:Marc Aßmann, Manfred Bayer category:physics.optics cs.CV published:2013-03-31 summary:Compressive sensing is considered a huge breakthrough in signal acquisition.It allows recording an image consisting of $N^2$ pixels using much fewer than$N^2$ measurements if it can be transformed to a basis where most pixels takeon negligibly small values. Standard compressive sensing techniques suffer fromthe computational overhead needed to reconstruct an image with typicalcomputation times between hours and days and are thus not optimal forapplications in physics and spectroscopy. We demonstrate an adaptivecompressive sampling technique that performs measurements directly in a sparsebasis. It needs much fewer than $N^2$ measurements without any computationaloverhead, so the result is available instantly.
arxiv-2700-140 | Is the Multiverse Hypothesis capable of explaining the Fine Tuning of Nature Laws and Constants? The Case of Cellular Automata | http://arxiv.org/pdf/1105.4278v3.pdf | author:Francisco José Soler Gil, Manuel Alfonseca category:nlin.CG astro-ph.CO cs.NE published:2011-05-21 summary:The objective of this paper is analyzing to which extent the multiversehypothesis provides a real explanation of the peculiarities of the laws andconstants in our universe. First we argue in favor of the thesis that allmultiverses except Tegmark's <<mathematical multiverse>> are too small toexplain the fine tuning, so that they merely shift the problem up one level.But the <<mathematical multiverse>> is surely too large. To prove thisassessment, we have performed a number of experiments with cellular automata ofcomplex behavior, which can be considered as universes in the mathematicalmultiverse. The analogy between what happens in some automata (in particularConway's <<Game of Life>>) and the real world is very strong. But if theresults of our experiments can be extrapolated to our universe, we shouldexpect to inhabit -- in the context of the multiverse -- a world in which atleast some of the laws and constants of nature should show a certain timedependence. Actually, the probability of our existence in a world such as ourswould be mathematically equal to zero. In consequence, the results presented inthis paper can be considered as an inkling that the hypothesis of themultiverse, whatever its type, does not offer an adequate explanation for thepeculiarities of the physical laws in our world. A slightly reduced version ofthis paper has been published in the Journal for General Philosophy of Science,Springer, March 2013, DOI: 10.1007/s10838-013-9215-7.
arxiv-2700-141 | Meaning-focused and Quantum-inspired Information Retrieval | http://arxiv.org/pdf/1304.0104v1.pdf | author:Diederik Aerts, Jan Broekaert, Sandro Sozzo, Tomas Veloz category:cs.IR cs.CL quant-ph published:2013-03-30 summary:In recent years, quantum-based methods have promisingly integrated thetraditional procedures in information retrieval (IR) and natural languageprocessing (NLP). Inspired by our research on the identification andapplication of quantum structures in cognition, more specifically our work onthe representation of concepts and their combinations, we put forward a'quantum meaning based' framework for structured query retrieval in textcorpora and standardized testing corpora. This scheme for IR rests onconsidering as basic notions, (i) 'entities of meaning', e.g., concepts andtheir combinations and (ii) traces of such entities of meaning, which is howdocuments are considered in this approach. The meaning content of these'entities of meaning' is reconstructed by solving an 'inverse problem' in thequantum formalism, consisting of reconstructing the full states of the entitiesof meaning from their collapsed states identified as traces in relevantdocuments. The advantages with respect to traditional approaches, such asLatent Semantic Analysis (LSA), are discussed by means of concrete examples.
arxiv-2700-142 | A Neuromorphic VLSI Design for Spike Timing and Rate Based Synaptic Plasticity | http://arxiv.org/pdf/1304.0090v1.pdf | author:Mostafa Rahimi Azghadi, Said Al-Sarawi, Derek Abbott, Nicolangelo Iannella category:cs.NE published:2013-03-30 summary:Triplet-based Spike Timing Dependent Plasticity (TSTDP) is a powerfulsynaptic plasticity rule that acts beyond conventional pair-based STDP (PSTDP).Here, the TSTDP is capable of reproducing the outcomes from a variety ofbiological experiments, while the PSTDP rule fails to reproduce them.Additionally, it has been shown that the behaviour inherent to the spikerate-based Bienenstock-Cooper-Munro (BCM) synaptic plasticity rule can alsoemerge from the TSTDP rule. This paper proposes an analog implementation of theTSTDP rule. The proposed VLSI circuit has been designed using the AMS 0.35 umCMOS process and has been simulated using design kits for Synopsys and Cadencetools. Simulation results demonstrate how well the proposed circuit can altersynaptic weights according to the timing difference amongst a set of differentpatterns of spikes. Furthermore, the circuit is shown to give rise to aBCM-like learning rule, which is a rate-based rule. To mimic implementationenvironment, a 1000 run Monte Carlo (MC) analysis was conducted on the proposedcircuit. The presented MC simulation analysis and the simulation result fromfine-tuned circuits show that, it is possible to mitigate the effect of processvariations in the proof of concept circuit, however, a practical variationaware design technique is required to promise a high circuit performance in alarge scale neural network. We believe that the proposed design can play asignificant role in future VLSI implementations of both spike timing and ratebased neuromorphic learning systems.
arxiv-2700-143 | Learning Locality-Constrained Collaborative Representation for Face Recognition | http://arxiv.org/pdf/1210.1316v2.pdf | author:Xi Peng, Lei Zhang, Zhang Yi, Kok Kiong Tan category:cs.CV published:2012-10-04 summary:The model of low-dimensional manifold and sparse representation are twowell-known concise models that suggest each data can be described by a fewcharacteristics. Manifold learning is usually investigated for dimensionreduction by preserving some expected local geometric structures from theoriginal space to a low-dimensional one. The structures are generallydetermined by using pairwise distance, e.g., Euclidean distance. Alternatively,sparse representation denotes a data point as a linear combination of thepoints from the same subspace. In practical applications, however, the nearbypoints in terms of pairwise distance may not belong to the same subspace, andvice versa. Consequently, it is interesting and important to explore how to geta better representation by integrating these two models together. To this end,this paper proposes a novel coding algorithm, called Locality-ConstrainedCollaborative Representation (LCCR), which improves the robustness anddiscrimination of data representation by introducing a kind of localconsistency. The locality term derives from a biologic observation that thesimilar inputs have similar code. The objective function of LCCR has ananalytical solution, and it does not involve local minima. The empiricalstudies based on four public facial databases, ORL, AR, Extended Yale B, andMultiple PIE, show that LCCR is promising in recognizing human faces fromfrontal views with varying expression and illumination, as well as variouscorruptions and occlusions.
arxiv-2700-144 | Anatomical Structure Segmentation in Liver MRI Images | http://arxiv.org/pdf/1207.0805v3.pdf | author:G. Geethu Lakshmi category:cs.CV published:2012-07-03 summary:Segmentation of medical images is a challenging task owing to theircomplexity. A standard segmentation problem within Magnetic Resonance Imaging(MRI) is the task of labeling voxels according to their tissue type. Imagesegmentation provides volumetric quantification of liver area and thus helps inthe diagnosis of disorders, such as Hepatitis, Cirrhosis, Jaundice,Hemochromatosis etc.This work deals with comparison of segmentation by applyingLevel Set Method,Fuzzy Level Information C-Means Clustering Algorithm andGradient Vector Flow Snake Algorithm.The results are compared using theparameters such as Number of pixels correctly classified, and percentage ofarea segmented.
arxiv-2700-145 | Registration of Images with Outliers Using Joint Saliency Map | http://arxiv.org/pdf/1304.8052v1.pdf | author:Binjie Qin, Zhijun Gu, Xianjun Sun, Yisong Lv category:cs.CV published:2013-03-29 summary:Mutual information (MI) is a popular similarity measure for imageregistration, whereby good registration can be achieved by maximizing thecompactness of the clusters in the joint histogram. However, MI is sensitive tothe "outlier" objects that appear in one image but not the other, and alsosuffers from local and biased maxima. We propose a novel joint saliency map(JSM) to highlight the corresponding salient structures in the two images, andemphatically group those salient structures into the smoothed compact clustersin the weighted joint histogram. This strategy could solve both the outlier andthe local maxima problems. Experimental results show that the JSM-MI basedalgorithm is not only accurate but also robust for registration of challengingimage pairs with outliers.
arxiv-2700-146 | Regret in Online Combinatorial Optimization | http://arxiv.org/pdf/1204.4710v2.pdf | author:Jean-Yves Audibert, Sébastien Bubeck, Gábor Lugosi category:cs.LG stat.ML published:2012-04-20 summary:We address online linear optimization problems when the possible actions ofthe decision maker are represented by binary vectors. The regret of thedecision maker is the difference between her realized loss and the best lossshe would have achieved by picking, in hindsight, the best possible action. Ourgoal is to understand the magnitude of the best possible (minimax) regret. Westudy the problem under three different assumptions for the feedback thedecision maker receives: full information, and the partial information modelsof the so-called "semi-bandit" and "bandit" problems. Combining the MirrorDescent algorithm and the INF (Implicitely Normalized Forecaster) strategy, weare able to prove optimal bounds for the semi-bandit case. We also recover theoptimal bounds for the full information setting. In the bandit case we discussexisting results in light of a new lower bound, and suggest a conjecture on theoptimal regret in that case. Finally we also prove that the standardexponentially weighted average forecaster is provably suboptimal in the settingof online combinatorial optimization.
arxiv-2700-147 | Translation-Invariant Shrinkage/Thresholding of Group Sparse Signals | http://arxiv.org/pdf/1304.0035v1.pdf | author:Po-Yu Chen, Ivan W. Selesnick category:cs.CV cs.LG cs.SD published:2013-03-29 summary:This paper addresses signal denoising when large-amplitude coefficients formclusters (groups). The L1-norm and other separable sparsity models do notcapture the tendency of coefficients to cluster (group sparsity). This workdevelops an algorithm, called 'overlapping group shrinkage' (OGS), based on theminimization of a convex cost function involving a group-sparsity promotingpenalty function. The groups are fully overlapping so the denoising method istranslation-invariant and blocking artifacts are avoided. Based on theprinciple of majorization-minimization (MM), we derive a simple iterativeminimization algorithm that reduces the cost function monotonically. Aprocedure for setting the regularization parameter, based on attenuating thenoise to a specified level, is also described. The proposed approach isillustrated on speech enhancement, wherein the OGS approach is applied in theshort-time Fourier transform (STFT) domain. The denoised speech produced by OGSdoes not suffer from musical noise.
arxiv-2700-148 | Optimal discovery with probabilistic expert advice: finite time analysis and macroscopic optimality | http://arxiv.org/pdf/1207.5259v3.pdf | author:Sebastien Bubeck, Damien Ernst, Aurelien Garivier category:cs.LG stat.ML published:2012-07-22 summary:We consider an original problem that arises from the issue of securityanalysis of a power system and that we name optimal discovery withprobabilistic expert advice. We address it with an algorithm based on theoptimistic paradigm and on the Good-Turing missing mass estimator. We prove twodifferent regret bounds on the performance of this algorithm under weakassumptions on the probabilistic experts. Under more restrictive hypotheses, wealso prove a macroscopic optimality result, comparing the algorithm both withan oracle strategy and with uniform sampling. Finally, we provide numericalexperiments illustrating these theoretical findings.
arxiv-2700-149 | Obtaining error-minimizing estimates and universal entry-wise error bounds for low-rank matrix completion | http://arxiv.org/pdf/1302.5337v2.pdf | author:Franz J. Király, Louis Theran category:stat.ML math.AG math.CO published:2013-02-21 summary:We propose a general framework for reconstructing and denoising singleentries of incomplete and noisy entries. We describe: effective algorithms fordeciding if and entry can be reconstructed and, if so, for reconstructing anddenoising it; and a priori bounds on the error of each entry, individually. Inthe noiseless case our algorithm is exact. For rank-one matrices, the newalgorithm is fast, admits a highly-parallel implementation, and produces anerror minimizing estimate that is qualitatively close to our theoretical andthe state-of-the-are Nuclear Norm and OptSpace methods.
arxiv-2700-150 | Age group and gender recognition from human facial images | http://arxiv.org/pdf/1304.0019v1.pdf | author:Tizita Nesibu Shewaye category:cs.CV published:2013-03-29 summary:This work presents an automatic human gender and age group recognition systembased on human facial images. It makes an extensive experiment with row pixelintensity valued features and Discrete Cosine Transform (DCT) coefficientfeatures with Principal Component Analysis and k-Nearest Neighborclassification to identify the best recognition approach. The final resultsshow approaches using DCT coefficient outperform their counter parts resultingin a 99% correct gender recognition rate and 68% correct age group recognitionrate (considering four distinct age groups) in unseen test images. Detailedexperimental settings and obtained results are clearly presented and explainedin this report.
arxiv-2700-151 | Independent Vector Analysis: Identification Conditions and Performance Bounds | http://arxiv.org/pdf/1303.7474v1.pdf | author:Matthew Anderson, Geng-Shen Fu, Ronald Phlypo, Tülay Adalı category:cs.LG cs.IT math.IT stat.ML published:2013-03-29 summary:Recently, an extension of independent component analysis (ICA) from one tomultiple datasets, termed independent vector analysis (IVA), has been thesubject of significant research interest. IVA has also been shown to be ageneralization of Hotelling's canonical correlation analysis. In this paper, weprovide the identification conditions for a general IVA formulation, whichaccounts for linear, nonlinear, and sample-to-sample dependencies. Theidentification conditions are a generalization of previous results for ICA andfor IVA when samples are independently and identically distributed.Furthermore, a principal aim of IVA is the identification of dependent sourcesbetween datasets. Thus, we provide the additional conditions for when thearbitrary ordering of the sources within each dataset is common. Performancebounds in terms of the Cramer-Rao lower bound are also provided for thedemixing matrices and interference to source ratio. The performance of two IVAalgorithms are compared to the theoretical bounds.
arxiv-2700-152 | Normalized Compression Distance of Multisets with Applications | http://arxiv.org/pdf/1212.5711v4.pdf | author:Andrew R. Cohen, Paul M. B. Vitanyi category:cs.CV cs.IT math.IT published:2012-12-22 summary:Normalized compression distance (NCD) is a parameter-free, feature-free,alignment-free, similarity measure between a pair of finite objects based oncompression. However, it is not sufficient for all applications. We propose anNCD of finite multisets (a.k.a. multiples) of finite objects that is also ametric. Previously, attempts to obtain such an NCD failed. We cover the entiretrajectory from theoretical underpinning to feasible practice. The new NCD formultisets is applied to retinal progenitor cell classification questions and torelated synthetically generated data that were earlier treated with thepairwise NCD. With the new method we achieved significantly better results.Similarly for questions about axonal organelle transport. We also applied thenew NCD to handwritten digit recognition and improved classification accuracysignificantly over that of pairwise NCD by incorporating both the pairwise andNCD for multisets. In the analysis we use the incomputable Kolmogorovcomplexity that for practical purposes is approximated from above by the lengthof the compressed version of the file involved, using a real-world compressionprogram. Index Terms--- Normalized compression distance, multisets or multiples,pattern recognition, data mining, similarity, classification, Kolmogorovcomplexity, retinal progenitor cells, synthetic data, organelle transport,handwritten character recognition
arxiv-2700-153 | Exploring the Role of Logically Related Non-Question Phrases for Answering Why-Questions | http://arxiv.org/pdf/1303.7310v1.pdf | author:Niraj Kumar, Rashmi Gangadharaiah, Kannan Srinathan, Vasudeva Varma category:cs.CL cs.IR H.3.m published:2013-03-29 summary:In this paper, we show that certain phrases although not present in a givenquestion/query, play a very important role in answering the question. Exploringthe role of such phrases in answering questions not only reduces the dependencyon matching question phrases for extracting answers, but also improves thequality of the extracted answers. Here matching question phrases means phraseswhich co-occur in given question and candidate answers. To achieve the abovediscussed goal, we introduce a bigram-based word graph model populated withsemantic and topical relatedness of terms in the given document. Next, we applyan improved version of ranking with a prior-based approach, which ranks allwords in the candidate document with respect to a set of root words (i.e.non-stopwords present in the question and in the candidate document). As aresult, terms logically related to the root words are scored higher than termsthat are not related to the root words. Experimental results show that ourdevised system performs better than state-of-the-art for the task of answeringWhy-questions.
arxiv-2700-154 | A problem dependent analysis of SOCP algorithms in noisy compressed sensing | http://arxiv.org/pdf/1304.0480v1.pdf | author:Mihailo Stojnic category:cs.IT math.IT stat.ML published:2013-03-29 summary:Under-determined systems of linear equations with sparse solutions have beenthe subject of an extensive research in last several years above all due toresults of \cite{CRT,CanRomTao06,DonohoPol}. In this paper we will consider\emph{noisy} under-determined linear systems. In a breakthrough\cite{CanRomTao06} it was established that in \emph{noisy} systems for anylinear level of under-determinedness there is a linear sparsity that can be\emph{approximately} recovered through an SOCP (second order cone programming)optimization algorithm so that the approximate solution vector is (in an$\ell_2$-norm sense) guaranteed to be no further from the sparse unknown vectorthan a constant times the noise. In our recent work \cite{StojnicGenSocp10} weestablished an alternative framework that can be used for statisticalperformance analysis of the SOCP algorithms. To demonstrate how the frameworkworks we then showed in \cite{StojnicGenSocp10} how one can use it to preciselycharacterize the \emph{generic} (worst-case) performance of the SOCP. In thispaper we present a different set of results that can be obtained through theframework of \cite{StojnicGenSocp10}. The results will relate to \emph{problemdependent} performance analysis of SOCP's. We will consider specific types ofunknown sparse vectors and characterize the SOCP performance when used forrecovery of such vectors. We will also show that our theoretical predictionsare in a solid agreement with the results one can get through numericalsimulations.
arxiv-2700-155 | Blinking Molecule Tracking | http://arxiv.org/pdf/1212.5877v2.pdf | author:Andreas Karrenbauer, Dominik Wöll category:cs.CV cs.DM published:2012-12-24 summary:We discuss a method for tracking individual molecules which globallyoptimizes the likelihood of the connections between molecule positions fast andwith high reliability even for high spot densities and blinking molecules. Ourmethod works with cost functions which can be freely chosen to combine costsfor distances between spots in space and time and which can account for thereliability of positioning a molecule. To this end, we describe a top-downpolyhedral approach to the problem of tracking many individual molecules. Thisimmediately yields an effective implementation using standard linearprogramming solvers. Our method can be applied to 2D and 3D tracking.
arxiv-2700-156 | Scalable Text and Link Analysis with Mixed-Topic Link Models | http://arxiv.org/pdf/1303.7264v1.pdf | author:Yaojia Zhu, Xiaoran Yan, Lise Getoor, Cristopher Moore category:cs.LG cs.IR cs.SI stat.ML published:2013-03-28 summary:Many data sets contain rich information about objects, as well as pairwiserelations between them. For instance, in networks of websites, scientificpapers, and other documents, each node has content consisting of a collectionof words, as well as hyperlinks or citations to other nodes. In order toperform inference on such data sets, and make predictions and recommendations,it is useful to have models that are able to capture the processes whichgenerate the text at each node and the links between them. In this paper, wecombine classic ideas in topic modeling with a variant of the mixed-membershipblock model recently developed in the statistical physics community. Theresulting model has the advantage that its parameters, including the mixture oftopics of each document and the resulting overlapping communities, can beinferred with a simple and scalable expectation-maximization algorithm. We testour model on three data sets, performing unsupervised topic classification andlink prediction. For both tasks, our model outperforms several existingstate-of-the-art methods, achieving higher accuracy with significantly lesscomputation, analyzing a data set with 1.3 million words and 44 thousand linksin a few minutes.
arxiv-2700-157 | Detecting Overlapping Temporal Community Structure in Time-Evolving Networks | http://arxiv.org/pdf/1303.7226v1.pdf | author:Yudong Chen, Vikas Kawadia, Rahul Urgaonkar category:cs.SI cs.LG physics.soc-ph stat.ML published:2013-03-28 summary:We present a principled approach for detecting overlapping temporal communitystructure in dynamic networks. Our method is based on the following framework:find the overlapping temporal community structure that maximizes a qualityfunction associated with each snapshot of the network subject to a temporalsmoothness constraint. A novel quality function and a smoothness constraint areproposed to handle overlaps, and a new convex relaxation is used to solve theresulting combinatorial optimization problem. We provide theoretical guaranteesas well as experimental results that reveal community structure in real andsynthetic networks. Our main insight is that certain structures can beidentified only when temporal correlation is considered and when communitiesare allowed to overlap. In general, discovering such overlapping temporalcommunity structure can enhance our understanding of real-world complexnetworks by revealing the underlying stability behind their seemingly chaoticevolution.
arxiv-2700-158 | Sparse Projections of Medical Images onto Manifolds | http://arxiv.org/pdf/1303.5508v2.pdf | author:George H. Chen, Christian Wachinger, Polina Golland category:cs.CV cs.LG stat.ML published:2013-03-22 summary:Manifold learning has been successfully applied to a variety of medicalimaging problems. Its use in real-time applications requires fast projectiononto the low-dimensional space. To this end, out-of-sample extensions areapplied by constructing an interpolation function that maps from the inputspace to the low-dimensional manifold. Commonly used approaches such as theNystr\"{o}m extension and kernel ridge regression require using all trainingpoints. We propose an interpolation function that only depends on a smallsubset of the input training data. Consequently, in the testing phase each newpoint only needs to be compared against a small number of input training datain order to project the point onto the low-dimensional space. We interpret ourmethod as an out-of-sample extension that approximates kernel ridge regression.Our method involves solving a simple convex optimization problem and has theattractive property of guaranteeing an upper bound on the approximation error,which is crucial for medical applications. Tuning this error bound controls thesparsity of the resulting interpolation function. We illustrate our method intwo clinical applications that require fast mapping of input images onto alow-dimensional space.
arxiv-2700-159 | Large-Scale Automatic Reconstruction of Neuronal Processes from Electron Microscopy Images | http://arxiv.org/pdf/1303.7186v1.pdf | author:Verena Kaynig, Amelio Vazquez-Reina, Seymour Knowles-Barley, Mike Roberts, Thouis R. Jones, Narayanan Kasthuri, Eric Miller, Jeff Lichtman, Hanspeter Pfister category:q-bio.NC cs.CV published:2013-03-28 summary:Automated sample preparation and electron microscopy enables acquisition ofvery large image data sets. These technical advances are of special importanceto the field of neuroanatomy, as 3D reconstructions of neuronal processes atthe nm scale can provide new insight into the fine grained structure of thebrain. Segmentation of large-scale electron microscopy data is the mainbottleneck in the analysis of these data sets. In this paper we present apipeline that provides state-of-the art reconstruction performance whilescaling to data sets in the GB-TB range. First, we train a random forestclassifier on interactive sparse user annotations. The classifier output iscombined with an anisotropic smoothing prior in a Conditional Random Fieldframework to generate multiple segmentation hypotheses per image. Thesesegmentations are then combined into geometrically consistent 3D objects bysegmentation fusion. We provide qualitative and quantitative evaluation of theautomatic segmentation and demonstrate large-scale 3D reconstructions ofneuronal processes from a $\mathbf{27,000}$ $\mathbf{\mu m^3}$ volume of braintissue over a cube of $\mathbf{30 \; \mu m}$ in each dimension corresponding to1000 consecutive image sections. We also introduce Mojo, a proofreading toolincluding semi-automated correction of merge errors based on sparse userscribbles.
arxiv-2700-160 | Conditional Gradient Algorithms for Norm-Regularized Smooth Convex Optimization | http://arxiv.org/pdf/1302.2325v4.pdf | author:Zaid Harchaoui, Anatoli Juditsky, Arkadi Nemirovski category:math.OC stat.CO stat.ML published:2013-02-10 summary:Motivated by some applications in signal processing and machine learning, weconsider two convex optimization problems where, given a cone $K$, a norm$\\cdot\$ and a smooth convex function $f$, we want either 1) to minimize thenorm over the intersection of the cone and a level set of $f$, or 2) tominimize over the cone the sum of $f$ and a multiple of the norm. We focus onthe case where (a) the dimension of the problem is too large to allow forinterior point algorithms, (b) $\\cdot\$ is "too complicated" to allow forcomputationally cheap Bregman projections required in the first-order proximalgradient algorithms. On the other hand, we assume that {it is relatively easyto minimize linear forms over the intersection of $K$ and the unit$\\cdot\$-ball}. Motivating examples are given by the nuclear norm with $K$being the entire space of matrices, or the positive semidefinite cone in thespace of symmetric matrices, and the Total Variation norm on the space of 2Dimages. We discuss versions of the Conditional Gradient algorithm capable tohandle our problems of interest, provide the related theoretical efficiencyestimates and outline some applications.
arxiv-2700-161 | pROST : A Smoothed Lp-norm Robust Online Subspace Tracking Method for Realtime Background Subtraction in Video | http://arxiv.org/pdf/1302.2073v2.pdf | author:Florian Seidel, Clemens Hage, Martin Kleinsteuber category:cs.CV published:2013-02-08 summary:An increasing number of methods for background subtraction use Robust PCA toidentify sparse foreground objects. While many algorithms use the L1-norm as aconvex relaxation of the ideal sparsifying function, we approach the problemwith a smoothed Lp-norm and present pROST, a method for robust online subspacetracking. The algorithm is based on alternating minimization on manifolds.Implemented on a graphics processing unit it achieves realtime performance.Experimental results on a state-of-the-art benchmark for background subtractionon real-world video data indicate that the method succeeds at a broad varietyof background subtraction scenarios, and it outperforms competing approacheswhen video quality is deteriorated by camera jitter.
arxiv-2700-162 | Inductive Hashing on Manifolds | http://arxiv.org/pdf/1303.7043v1.pdf | author:Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, Zhenmin Tang category:cs.LG published:2013-03-28 summary:Learning based hashing methods have attracted considerable attention due totheir ability to greatly increase the scale at which existing algorithms mayoperate. Most of these methods are designed to generate binary codes thatpreserve the Euclidean distance in the original space. Manifold learningtechniques, in contrast, are better able to model the intrinsic structureembedded in the original high-dimensional data. The complexity of these models,and the problems with out-of-sample data, have previously rendered themunsuitable for application to large-scale embedding, however. In this work, weconsider how to learn compact binary embeddings on their intrinsic manifolds.In order to address the above-mentioned difficulties, we describe an efficient,inductive solution to the out-of-sample data problem, and a process by whichnon-parametric manifold learning may be used as the basis of a hashing method.Our proposed approach thus allows the development of a range of new hashingtechniques exploiting the flexibility of the wide variety of manifold learningapproaches available. We particularly show that hashing on the basis of t-SNE .
arxiv-2700-163 | On Constructing the Value Function for Optimal Trajectory Problem and its Application to Image Processing | http://arxiv.org/pdf/1303.4845v2.pdf | author:Myong-Song Ho, Gwang-Hui Ju, Yong-Bom O, Gwang-Ho Jong category:cs.CV published:2013-03-20 summary:We proposed an algorithm for solving Hamilton-Jacobi equation associated toan optimal trajectory problem for a vehicle moving inside the pre-specifieddomain with the speed depending upon the direction of the motion and currentposition of the vehicle. The dynamics of the vehicle is defined by an ordinarydifferential equation, the right hand of which is given by product of control(atime dependent fuction) and a function dependent on trajectory and control. Atsome unspecified terminal time, the vehicle reaches the boundary of thepre-specified domain and incurs a terminal cost. We also associate thetraveling cost with a type of integral to the trajectory followed by vehicle.We are interested in a numerical method for finding a trajectory that minimizesthe sum of the traveling cost and terminal cost. We developed an algorithmsolving the value function for general trajectory optimization problem. Ouralgorithm is closely related to the Tsitsiklis's Fast Marching Method and J. A.Sethian's OUM and SLF-LLL[1-4] and is a generalization of them. On the basis ofthese results, We applied our algorithm to the image processing such asfingerprint verification.
arxiv-2700-164 | Information driven self-organization of complex robotic behaviors | http://arxiv.org/pdf/1301.7473v2.pdf | author:Georg Martius, Ralf Der, Nihat Ay category:cs.RO cs.IT cs.LG math.IT published:2013-01-30 summary:Information theory is a powerful tool to express principles to driveautonomous systems because it is domain invariant and allows for an intuitiveinterpretation. This paper studies the use of the predictive information (PI),also called excess entropy or effective measure complexity, of the sensorimotorprocess as a driving force to generate behavior. We study nonlinear andnonstationary systems and introduce the time-local predicting information(TiPI) which allows us to derive exact results together with explicit updaterules for the parameters of the controller in the dynamical systems framework.In this way the information principle, formulated at the level of behavior, istranslated to the dynamics of the synapses. We underpin our results with anumber of case studies with high-dimensional robotic systems. We show thespontaneous cooperativity in a complex physical system with decentralizedcontrol. Moreover, a jointly controlled humanoid robot develops a highbehavioral variety depending on its physics and the environment it isdynamically embedded into. The behavior can be decomposed into a succession oflow-dimensional modes that increasingly explore the behavior space. This is apromising way to avoid the curse of dimensionality which hinders learningsystems to scale well.
arxiv-2700-165 | Developing and Analyzing Boundary Detection Operators Using Probabilistic Models | http://arxiv.org/pdf/1304.3447v1.pdf | author:David Sher category:cs.CV published:2013-03-27 summary:Most feature detectors such as edge detectors or circle finders arestatistical, in the sense that they decide at each point in an image about thepresence of a feature, this paper describes the use of Bayesian featuredetectors.
arxiv-2700-166 | Machine Learning, Clustering, and Polymorphy | http://arxiv.org/pdf/1304.3432v1.pdf | author:Stephen Jose Hanson, Malcolm Bauer category:cs.AI cs.CL cs.LG published:2013-03-27 summary:This paper describes a machine induction program (WITT) that attempts tomodel human categorization. Properties of categories to which human subjectsare sensitive includes best or prototypical members, relative contrasts betweenputative categories, and polymorphy (neither necessary or sufficient features).This approach represents an alternative to usual Artificial Intelligenceapproaches to generalization and conceptual clustering which tend to focus onnecessary and sufficient feature rules, equivalence classes, and simple searchand match schemes. WITT is shown to be more consistent with humancategorization while potentially including results produced by more traditionalclustering schemes. Applications of this approach in the domains of expertsystems and information retrieval are also discussed.
arxiv-2700-167 | Evidential Reasoning in Parallel Hierarchical Vision Programs | http://arxiv.org/pdf/1304.3098v1.pdf | author:Ze-Nian Li, Leonard Uhr category:cs.AI cs.CV published:2013-03-27 summary:This paper presents an efficient adaptation and application of theDempster-Shafer theory of evidence, one that can be used effectively in amassively parallel hierarchical system for visual pattern perception. Itdescribes the techniques used, and shows in an extended example how they serveto improve the system's performance as it applies a multiple-level set ofprocesses.
arxiv-2700-168 | Imprecise Meanings as a Cause of Uncertainty in Medical Knowledge-Based Systems | http://arxiv.org/pdf/1304.3092v1.pdf | author:Steven J. Henkind category:cs.AI cs.CL published:2013-03-27 summary:There has been a considerable amount of work on uncertainty inknowledge-based systems. This work has generally been concerned withuncertainty arising from the strength of inferences and the weight of evidence.In this paper we discuss another type of uncertainty: that which is due toimprecision in the underlying primitives used to represent the knowledge of thesystem. In particular, a given word may denote many similar but not identicalentities. Such words are said to be lexically imprecise. Lexical imprecisionhas caused widespread problems in many areas. Unless this phenomenon isrecognized and appropriately handled, it can degrade the performance ofknowledge-based systems. In particular, it can lead to difficulties with theuser interface, and with the inferencing processes of these systems. Sometechniques are suggested for coping with this phenomenon.
arxiv-2700-169 | Evidential Reasoning in Image Understanding | http://arxiv.org/pdf/1304.2749v1.pdf | author:Minchuan Zhang, Su-shing Chen category:cs.CV cs.AI published:2013-03-27 summary:In this paper, we present some results of evidential reasoning inunderstanding multispectral images of remote sensing systems. TheDempster-Shafer approach of combination of evidences is pursued to yieldcontextual classification results, which are compared with previous results ofthe Bayesian context free classification, contextual classifications of dynamicprogramming and stochastic relaxation approaches.
arxiv-2700-170 | Comparisons of Reasoning Mechanisms for Computer Vision | http://arxiv.org/pdf/1304.2743v1.pdf | author:Ze-Nian Li category:cs.CV cs.AI published:2013-03-27 summary:An evidential reasoning mechanism based on the Dempster-Shafer theory ofevidence is introduced. Its performance in real-world image analysis iscompared with other mechanisms based on the Bayesian formalism and a simpleweight combination method.
arxiv-2700-171 | Utility-Based Control for Computer Vision | http://arxiv.org/pdf/1304.2367v1.pdf | author:Tod S. Levitt, Thomas O. Binford, Gil J. Ettinger, Patrice Gelband category:cs.CV cs.AI cs.SY published:2013-03-27 summary:Several key issues arise in implementing computer vision recognition of worldobjects in terms of Bayesian networks. Computational efficiency is a drivingforce. Perceptual networks are very deep, typically fifteen levels ofstructure. Images are wide, e.g., an unspecified-number of edges may appearanywhere in an image 512 x 512 pixels or larger. For efficiency, we dynamicallyinstantiate hypotheses of observed objects. The network is not fixed, but iscreated incrementally at runtime. Generation of hypotheses of world objects andindexing of models for recognition are important, but they are not consideredhere [4,11]. This work is aimed at near-term implementation with parallelcomputation in a radar surveillance system, ADRIES [5, 15], and a system forindustrial part recognition, SUCCESSOR [2]. For many applications, vision mustbe faster to be practical and so efficiently controlling the machine visionprocess is critical. Perceptual operators may scan megapixels and may requireminutes of computation time. It is necessary to avoid unnecessary sensoractions and computation. Parallel computation is available at several levels ofprocessor capability. The potential for parallel, distributed computation forhigh-level vision means distributing non-homogeneous computations. This paperaddresses the problem of task control in machine vision systems based onBayesian probability models. We separate control and inference to extend theprevious work [3] to maximize utility instead of probability. Maximizingutility allows adopting perceptual strategies for efficient informationgathering with sensors and analysis of sensor data. Results of controllingmachine vision via utility to recognize military situations are presented inthis paper. Future work extends this to industrial part recognition forSUCCESSOR.
arxiv-2700-172 | Multiple decision trees | http://arxiv.org/pdf/1304.2363v1.pdf | author:Suk Wah Kwok, Chris Carter category:cs.LG cs.AI stat.ML published:2013-03-27 summary:This paper describes experiments, on two domains, to investigate the effectof averaging over predictions of multiple decision trees, instead of using asingle tree. Other authors have pointed out theoretical and commonsense reasonsfor preferring the multiple tree approach. Ideally, we would like to considerpredictions from all trees, weighted by their probability. However, there is avast number of different trees, and it is difficult to estimate the probabilityof each tree. We sidestep the estimation problem by using a modified version ofthe ID3 algorithm to build good trees, and average over only these trees. Ourresults are encouraging. For each domain, we managed to produce a small numberof good trees. We find that it is best to average across sets of trees withdifferent structure; this usually gives better performance than any of theconstituent trees, including the ID3 tree.
arxiv-2700-173 | Expectation Propagation for Neural Networks with Sparsity-promoting Priors | http://arxiv.org/pdf/1303.6938v1.pdf | author:Pasi Jylänki, Aapo Nummenmaa, Aki Vehtari category:stat.ML published:2013-03-27 summary:We propose a novel approach for nonlinear regression using a two-layer neuralnetwork (NN) model structure with sparsity-favoring hierarchical priors on thenetwork weights. We present an expectation propagation (EP) approach forapproximate integration over the posterior distribution of the weights, thehierarchical scale parameters of the priors, and the residual scale. Using afactorized posterior approximation we derive a computationally efficientalgorithm, whose complexity scales similarly to an ensemble of independentsparse linear models. The approach enables flexible definition of weight priorswith different sparseness properties such as independent Laplace priors with acommon scale parameter or Gaussian automatic relevance determination (ARD)priors with different relevance parameters for all inputs. The approach can beextended beyond standard activation functions and NN model structures to formflexible nonlinear predictors from multiple sparse linear models. The effectsof the hierarchical priors and the predictive performance of the algorithm areassessed using both simulated and real-world data. Comparisons are made to twoalternative models with ARD priors: a Gaussian process with a NN covariancefunction and marginal maximum a posteriori estimates of the relevanceparameters, and a NN with Markov chain Monte Carlo integration over all theunknown model parameters.
arxiv-2700-174 | Model-based Influence Diagrams for Machine Vision | http://arxiv.org/pdf/1304.1517v1.pdf | author:Tod S. Levitt, John Mark Agosta, Thomas O. Binford category:cs.CV cs.AI published:2013-03-27 summary:We show an approach to automated control of machine vision systems based onincremental creation and evaluation of a particular family of influencediagrams that represent hypotheses of imagery interpretation and possiblesubsequent processing decisions. In our approach, model-based machine visiontechniques are integrated with hierarchical Bayesian inference to provide aframework for representing and matching instances of objects and relationshipsin imagery and for accruing probabilities to rank order conflicting sceneinterpretations. We extend a result of Tatman and Shachter to show that thesequence of processing decisions derived from evaluating the diagrams at eachstage is the same as the sequence that would have been derived by evaluatingthe final influence diagram that contains all random variables created duringthe run of the vision system.
arxiv-2700-175 | Efficiently Using Second Order Information in Large l1 Regularization Problems | http://arxiv.org/pdf/1303.6935v1.pdf | author:Xiaocheng Tang, Katya Scheinberg category:stat.ML cs.LG published:2013-03-27 summary:We propose a novel general algorithm LHAC that efficiently uses second-orderinformation to train a class of large-scale l1-regularized problems. Our methodexecutes cheap iterations while achieving fast local convergence rate byexploiting the special structure of a low-rank matrix, constructed viaquasi-Newton approximation of the Hessian of the smooth loss function. A greedyactive-set strategy, based on the largest violations in the dual constraints,is employed to maintain a working set that iteratively estimates the complementof the optimal active set. This allows for smaller size of subproblems andeventually identifies the optimal active set. Empirical comparisons confirmthat LHAC is highly competitive with several recently proposed state-of-the-artspecialized solvers for sparse logistic regression and sparse inversecovariance matrix selection.
arxiv-2700-176 | An investigation towards wavelet based optimization of automatic image registration techniques | http://arxiv.org/pdf/1303.6927v1.pdf | author:Arun P. V., Dr. S. K. Katiyar category:cs.CV published:2013-03-27 summary:Image registration is the process of transforming different sets of data intoone coordinate system and is required for various remote sensing applicationslike change detection, image fusion, and other related areas. The effect ofincreased relief displacement, requirement of more control points, andincreased data volume are the challenges associated with the registration ofhigh resolution image data. The objective of this research work is to study themost efficient techniques and to investigate the extent of improvementachievable by enhancing them with Wavelet transform. The SIFT feature basedmethod uses the Eigen value for extracting thousands of key points based onscale invariant features and these feature points when further enhanced by thewavelet transform yields the best results.
arxiv-2700-177 | A Comparative Analysis on the Applicability of Entropy in remote sensing | http://arxiv.org/pdf/1303.6926v1.pdf | author:Dr. S. K. Katiyar, Arun P. V. category:cs.CV published:2013-03-27 summary:Entropy is the measure of uncertainty in any data and is adopted formaximisation of mutual information in many remote sensing operations. Theavailability of wide entropy variations motivated us for an investigation overthe suitability preference of these versions to specific operations.Methodologies were implemented in Matlab and were enhanced with entropyvariations. Evaluation of various implementations was based on differentstatistical parameters with reference to the study area The popular availableversions like Tsalli's, Shanon's, and Renyi's entropies were analysed incontext of various remote sensing operations namely thresholding, clusteringand registration.
arxiv-2700-178 | Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients | http://arxiv.org/pdf/1301.3764v2.pdf | author:Tom Schaul, Yann LeCun category:cs.LG cs.AI stat.ML published:2013-01-16 summary:Recent work has established an empirically successful framework for adaptinglearning rates for stochastic gradient descent (SGD). This effectively removesall needs for tuning, while automatically reducing learning rates over time onstationary problems, and permitting learning rates to grow appropriately innon-stationary tasks. Here, we extend the idea in three directions, addressingproper minibatch parallelization, including reweighted updates for sparse ororthogonal gradients, improving robustness on non-smooth loss functions, in theprocess replacing the diagonal Hessian estimation procedure that may not alwaysbe available by a robust finite-difference approximation. The final algorithmintegrates all these components, has linear complexity and is hyper-parameterfree.
arxiv-2700-179 | A Note on k-support Norm Regularized Risk Minimization | http://arxiv.org/pdf/1303.6390v2.pdf | author:Matthew Blaschko category:cs.LG published:2013-03-26 summary:The k-support norm has been recently introduced to perform correlatedsparsity regularization. Although Argyriou et al. only reported experimentsusing squared loss, here we apply it to several other commonly used settingsresulting in novel machine learning algorithms with interesting and familiarlimit cases. Source code for the algorithms described here is available.
arxiv-2700-180 | Sparse approximation and recovery by greedy algorithms in Banach spaces | http://arxiv.org/pdf/1303.6811v1.pdf | author:Vladimir Temlyakov category:stat.ML math.FA 41A65 published:2013-03-27 summary:We study sparse approximation by greedy algorithms. We prove theLebesgue-type inequalities for the Weak Chebyshev Greedy Algorithm (WCGA), ageneralization of the Weak Orthogonal Matching Pursuit to the case of a Banachspace. The main novelty of these results is a Banach space setting instead of aHilbert space setting. The results are proved for redundant dictionariessatisfying certain conditions. Then we apply these general results to the caseof bases. In particular, we prove that the WCGA provides almost optimal sparseapproximation for the trigonometric system in $L_p$, $2\le p<\infty$.
arxiv-2700-181 | K-Plane Regression | http://arxiv.org/pdf/1211.1513v2.pdf | author:Naresh Manwani, P. S. Sastry category:cs.LG published:2012-11-07 summary:In this paper, we present a novel algorithm for piecewise linear regressionwhich can learn continuous as well as discontinuous piecewise linear functions.The main idea is to repeatedly partition the data and learn a liner model in ineach partition. While a simple algorithm incorporating this idea does not workwell, an interesting modification results in a good algorithm. The proposedalgorithm is similar in spirit to $k$-means clustering algorithm. We show thatour algorithm can also be viewed as an EM algorithm for maximum likelihoodestimation of parameters under a reasonable probability model. We empiricallydemonstrate the effectiveness of our approach by comparing its performance withthe state of art regression learning algorithms on some real world datasets.
arxiv-2700-182 | Sequential testing over multiple stages and performance analysis of data fusion | http://arxiv.org/pdf/1303.6750v1.pdf | author:Gaurav Thakur category:stat.ML cs.LG published:2013-03-27 summary:We describe a methodology for modeling the performance of decision-level datafusion between different sensor configurations, implemented as part of theJIEDDO Analytic Decision Engine (JADE). We first discuss a Bayesian networkformulation of classical probabilistic data fusion, which allows elementaryfusion structures to be stacked and analyzed efficiently. We then present anextension of the Wald sequential test for combining the outputs of the Bayesiannetwork over time. We discuss an algorithm to compute its performancestatistics and illustrate the approach on some examples. This variant of thesequential test involves multiple, distinct stages, where the evidenceaccumulated from each stage is carried over into the next one, and is motivatedby a need to keep certain sensors in the network inactive unless triggered byother sensors.
arxiv-2700-183 | An intelligent approach towards automatic shape modeling and object extraction from satellite images using cellular automata based algorithm | http://arxiv.org/pdf/1303.6711v1.pdf | author:P. V. Arun, S. K. Katiyar category:cs.CV published:2013-03-27 summary:Automatic feature extraction domain has witnessed the application of manyintelligent methodologies over past decade; however detection accuracy of theseapproaches were limited as object geometry and contextual knowledge were notgiven enough consideration. In this paper, we propose a frame work for accuratedetection of features along with automatic interpolation, and interpretation bymodeling feature shape as well as contextual knowledge using advancedtechniques such as SVRF, Cellular Neural Network, Core set, and MACA. Developedmethodology has been compared with contemporary methods using differentstatistical measures. Investigations over various satellite images revealedthat considerable success was achieved with the CNN approach. CNN has beeneffective in modeling different complex features effectively and complexity ofthe approach has been considerably reduced using corset optimization. Thesystem has dynamically used spectral and spatial information for representingcontextual knowledge using CNN-prolog approach. System has been also proved tobe effective in providing intelligent interpolation and interpretation ofrandom features.
arxiv-2700-184 | An N-dimensional approach towards object based classification of remotely sensed imagery | http://arxiv.org/pdf/1303.6619v1.pdf | author:Arun p V, S. K. Katiyar category:cs.CV published:2013-03-26 summary:Remote sensing techniques are widely used for land cover classification andurban analysis. The availability of high resolution remote sensing imagerylimits the level of classification accuracy attainable from pixel-basedapproach. In this paper object-based classification scheme based on ahierarchical support vector machine is introduced. By combining spatial andspectral information, the amount of overlap between classes can be decreased;thereby yielding higher classification accuracy and more accurate land covermaps. We have adopted certain automatic approaches based on the advancedtechniques as Cellular automata and Genetic Algorithm for kernel and tuningparameter selection. Performance evaluation of the proposed methodology incomparison with the existing approaches is performed with reference to theBhopal city study area.
arxiv-2700-185 | Performance Evaluation of Edge-Directed Interpolation Methods for Images | http://arxiv.org/pdf/1303.6455v1.pdf | author:Shaode Yu, Qingsong Zhu, Shibin Wu, Yaoqin Xie category:cs.CV I.4.1 published:2013-03-26 summary:Many interpolation methods have been developed for high visual quality, butfail for inability to preserve image structures. Edges carry heavy structuralinformation for detection, determination and classification. Edge-adaptiveinterpolation approaches become a center of focus. In this paper, performanceof four edge-directed interpolation methods comparing with two traditionalmethods is evaluated on two groups of images. These methods include newedge-directed interpolation (NEDI), edge-guided image interpolation (EGII),iterative curvature-based interpolation (ICBI), directional cubic convolutioninterpolation (DCCI) and two traditional approaches, bi-linear and bi-cubic.Meanwhile, no parameters are mentioned to measure edge-preserving ability ofedge-adaptive interpolation approaches and we proposed two. One evaluatesaccuracy and the other measures robustness of edge-preservation ability.Performance evaluation is based on six parameters. Objective assessment andvisual analysis are illustrated and conclusions are drawn from theoreticalbackgrounds and practical results.
arxiv-2700-186 | Analysis Operator Learning and Its Application to Image Reconstruction | http://arxiv.org/pdf/1204.5309v3.pdf | author:Simon Hawe, Martin Kleinsteuber, Klaus Diepold category:cs.LG cs.CV I.4.5 published:2012-04-24 summary:Exploiting a priori known structural information lies at the core of manyimage reconstruction methods that can be stated as inverse problems. Thesynthesis model, which assumes that images can be decomposed into a linearcombination of very few atoms of some dictionary, is now a well establishedtool for the design of image reconstruction algorithms. An interestingalternative is the analysis model, where the signal is multiplied by ananalysis operator and the outcome is assumed to be the sparse. This approachhas only recently gained increasing interest. The quality of reconstructionmethods based on an analysis model severely depends on the right choice of thesuitable operator. In this work, we present an algorithm for learning an analysis operator fromtraining images. Our method is based on an $\ell_p$-norm minimization on theset of full rank matrices with normalized columns. We carefully introduce theemployed conjugate gradient method on manifolds, and explain the underlyinggeometry of the constraints. Moreover, we compare our approach tostate-of-the-art methods for image denoising, inpainting, and single imagesuper-resolution. Our numerical results show competitive performance of ourgeneral approach in all presented applications compared to the specializedstate-of-the-art techniques.
arxiv-2700-187 | Budget-Optimal Task Allocation for Reliable Crowdsourcing Systems | http://arxiv.org/pdf/1110.3564v4.pdf | author:David R. Karger, Sewoong Oh, Devavrat Shah category:cs.LG cs.DS cs.HC stat.ML published:2011-10-17 summary:Crowdsourcing systems, in which numerous tasks are electronically distributedto numerous "information piece-workers", have emerged as an effective paradigmfor human-powered solving of large scale problems in domains such as imageclassification, data entry, optical character recognition, recommendation, andproofreading. Because these low-paid workers can be unreliable, nearly all suchsystems must devise schemes to increase confidence in their answers, typicallyby assigning each task multiple times and combining the answers in anappropriate manner, e.g. majority voting. In this paper, we consider a general model of such crowdsourcing tasks andpose the problem of minimizing the total price (i.e., number of taskassignments) that must be paid to achieve a target overall reliability. We givea new algorithm for deciding which tasks to assign to which workers and forinferring correct answers from the workers' answers. We show that ouralgorithm, inspired by belief propagation and low-rank matrix approximation,significantly outperforms majority voting and, in fact, is optimal throughcomparison to an oracle that knows the reliability of every worker. Further, wecompare our approach with a more general class of algorithms which candynamically assign tasks. By adaptively deciding which questions to ask to thenext arriving worker, one might hope to reduce uncertainty more efficiently. Weshow that, perhaps surprisingly, the minimum price necessary to achieve atarget reliability scales in the same manner under both adaptive andnon-adaptive scenarios. Hence, our non-adaptive approach is order-optimal underboth scenarios. This strongly relies on the fact that workers are fleeting andcan not be exploited. Therefore, architecturally, our results suggest thatbuilding a reliable worker-reputation system is essential to fully harnessingthe potential of adaptive designs.
arxiv-2700-188 | Simulation of Fractional Brownian Surfaces via Spectral Synthesis on Manifolds | http://arxiv.org/pdf/1303.6377v1.pdf | author:Zachary Gelbaum, Mathew Titus category:cs.CG cs.CV math.PR published:2013-03-26 summary:Using the spectral decomposition of the Laplace-Beltrami operator we simulatefractal surfaces as random series of eigenfunctions. This approach allows us togenerate random fields over smooth manifolds of arbitrary dimension,generalizing previous work with fractional Brownian motion withmulti-dimensional parameter. We give examples of surfaces with and withoutboundary and discuss implementation.
arxiv-2700-189 | Convex Tensor Decomposition via Structured Schatten Norm Regularization | http://arxiv.org/pdf/1303.6370v1.pdf | author:Ryota Tomioka, Taiji Suzuki category:stat.ML cs.LG cs.NA published:2013-03-26 summary:We discuss structured Schatten norms for tensor decomposition that includestwo recently proposed norms ("overlapped" and "latent") forconvex-optimization-based tensor decomposition, and connect tensordecomposition with wider literature on structured sparsity. Based on theproperties of the structured Schatten norms, we mathematically analyze theperformance of "latent" approach for tensor decomposition, which wasempirically found to perform better than the "overlapped" approach in somesettings. We show theoretically that this is indeed the case. In particular,when the unknown true tensor is low-rank in a specific mode, this approachperforms as good as knowing the mode with the smallest rank. Along the way, weshow a novel duality result for structures Schatten norms, establish theconsistency, and discuss the identifiability of this approach. We confirmthrough numerical simulations that our theoretical prediction can preciselypredict the scaling behavior of the mean squared error.
arxiv-2700-190 | Video Face Matching using Subset Selection and Clustering of Probabilistic Multi-Region Histograms | http://arxiv.org/pdf/1303.6361v1.pdf | author:Sandra Mau, Shaokang Chen, Conrad Sanderson, Brian C. Lovell category:cs.CV cs.IR published:2013-03-26 summary:Balancing computational efficiency with recognition accuracy is one of themajor challenges in real-world video-based face recognition. A significantdesign decision for any such system is whether to process and use all possiblefaces detected over the video frames, or whether to select only a few "best"faces. This paper presents a video face recognition system based onprobabilistic Multi-Region Histograms to characterise performance trade-offsin: (i) selecting a subset of faces compared to using all faces, and (ii)combining information from all faces via clustering. Three face selectionmetrics are evaluated for choosing a subset: face detection confidence, randomsubset, and sequential selection. Experiments on the recently introduced MOBIOdataset indicate that the usage of all faces through clustering alwaysoutperformed selecting only a subset of faces. The experiments also show thatthe face selection metric based on face detection confidence generally providesbetter recognition performance than random or sequential sampling. Moreover,the optimal number of faces varies drastically across selection metric andsubsets of MOBIO. Given the trade-offs between computational effort,recognition accuracy and robustness, it is recommended that face featureclustering would be most advantageous in batch processing (particularly forvideo-based watchlists), whereas face selection methods should be limited toapplications with significant computational restrictions.
arxiv-2700-191 | Hypothesis Testing in Feedforward Networks with Broadcast Failures | http://arxiv.org/pdf/1211.4518v3.pdf | author:Zhenliang Zhang, Edwin K. P. Chong, Ali Pezeshki, William Moran category:cs.IT cs.LG math.IT published:2012-11-19 summary:Consider a countably infinite set of nodes, which sequentially make decisionsbetween two given hypotheses. Each node takes a measurement of the underlyingtruth, observes the decisions from some immediate predecessors, and makes adecision between the given hypotheses. We consider two classes of broadcastfailures: 1) each node broadcasts a decision to the other nodes, subject torandom erasure in the form of a binary erasure channel; 2) each node broadcastsa randomly flipped decision to the other nodes in the form of a binarysymmetric channel. We are interested in whether there exists a decisionstrategy consisting of a sequence of likelihood ratio tests such that the nodedecisions converge in probability to the underlying truth. In both cases, weshow that if each node only learns from a bounded number of immediatepredecessors, then there does not exist a decision strategy such that thedecisions converge in probability to the underlying truth. However, in case 1,we show that if each node learns from an unboundedly growing number ofpredecessors, then the decisions converge in probability to the underlyingtruth, even when the erasure probabilities converge to 1. We also derive theconvergence rate of the error probability. In case 2, we show that if each nodelearns from all of its previous predecessors, then the decisions converge inprobability to the underlying truth when the flipping probabilities of thebinary symmetric channels are bounded away from 1/2. In the case where theflipping probabilities converge to 1/2, we derive a necessary condition on theconvergence rate of the flipping probabilities such that the decisions stillconverge to the underlying truth. We also explicitly characterize therelationship between the convergence rate of the error probability and theconvergence rate of the flipping probabilities.
arxiv-2700-192 | Random Intersection Trees | http://arxiv.org/pdf/1303.6223v1.pdf | author:Rajen Dinesh Shah, Nicolai Meinshausen category:stat.ML stat.CO stat.ME published:2013-03-25 summary:Finding interactions between variables in large and high-dimensional datasetsis often a serious computational challenge. Most approaches build upinteraction sets incrementally, adding variables in a greedy fashion. Thedrawback is that potentially informative high-order interactions may beoverlooked. Here, we propose at an alternative approach for classificationproblems with binary predictor variables, called Random Intersection Trees. Itworks by starting with a maximal interaction that includes all variables, andthen gradually removing variables if they fail to appear in randomly chosenobservations of a class of interest. We show that informative interactions areretained with high probability, and the computational complexity of ourprocedure is of order $p^\kappa$ for a value of $\kappa$ that can reach valuesas low as 1 for very sparse data; in many more general settings, it will stillbeat the exponent $s$ obtained when using a brute force search constrained toorder $s$ interactions. In addition, by using some new ideas based on min-wisehash schemes, we are able to further reduce the computational cost.Interactions found by our algorithm can be used for predictive modelling invarious forms, but they are also often of interest in their own right as usefulcharacterisations of what distinguishes a certain class from others.
arxiv-2700-193 | Regression for sets of polynomial equations | http://arxiv.org/pdf/1110.4531v4.pdf | author:Franz Johannes Király, Paul von Bünau, Jan Saputra Müller, Duncan Blythe, Frank Meinecke, Klaus-Robert Müller category:stat.ML published:2011-10-20 summary:We propose a method called ideal regression for approximating an arbitrarysystem of polynomial equations by a system of a particular type. Usingtechniques from approximate computational algebraic geometry, we show how wecan solve ideal regression directly without resorting to numericaloptimization. Ideal regression is useful whenever the solution to a learningproblem can be described by a system of polynomial equations. As an example, wedemonstrate how to formulate Stationary Subspace Analysis (SSA), a sourceseparation problem, in terms of ideal regression, which also yields aconsistent estimator for SSA. We then compare this estimator in simulationswith previous optimization-based approaches for SSA.
arxiv-2700-194 | Compression as a universal principle of animal behavior | http://arxiv.org/pdf/1303.6175v1.pdf | author:R. Ferrer-i-Cancho, A. Hernández-Fernández, D. Lusseau, G. Agoramoorthy, M. J. Hsu, S. Semple category:q-bio.NC cs.CL cs.IT math.IT q-bio.QM published:2013-03-25 summary:A key aim in biology and psychology is to identify fundamental principlesunderpinning the behavior of animals, including humans. Analyses of humanlanguage and the behavior of a range of non-human animal species have providedevidence for a common pattern underlying diverse behavioral phenomena: wordsfollow Zipf's law of brevity (the tendency of more frequently used words to beshorter), and conformity to this general pattern has been seen in the behaviorof a number of other animals. It has been argued that the presence of this lawis a sign of efficient coding in the information theoretic sense. However, nostrong direct connection has been demonstrated between the law and compression,the information theoretic principle of minimizing the expected length of acode. Here we show that minimizing the expected code length implies that thelength of a word cannot increase as its frequency increases. Furthermore, weshow that the mean code length or duration is significantly small in humanlanguage, and also in the behavior of other species in all cases whereagreement with the law of brevity has been found. We argue that compression isa general principle of animal behavior, that reflects selection for efficiencyof coding.
arxiv-2700-195 | On Sparsity Inducing Regularization Methods for Machine Learning | http://arxiv.org/pdf/1303.6086v1.pdf | author:Andreas Argyriou, Luca Baldassarre, Charles A. Micchelli, Massimiliano Pontil category:cs.LG stat.ML published:2013-03-25 summary:During the past years there has been an explosion of interest in learningmethods based on sparsity regularization. In this paper, we discuss a generalclass of such methods, in which the regularizer can be expressed as thecomposition of a convex function $\omega$ with a linear function. This settingincludes several methods such the group Lasso, the Fused Lasso, multi-tasklearning and many more. We present a general approach for solvingregularization problems of this kind, under the assumption that the proximityoperator of the function $\omega$ is available. Furthermore, we comment on theapplication of this approach to support vector machines, a technique pioneeredby the groundbreaking work of Vladimir Vapnik.
arxiv-2700-196 | Poisoning Attacks against Support Vector Machines | http://arxiv.org/pdf/1206.6389v3.pdf | author:Battista Biggio, Blaine Nelson, Pavel Laskov category:cs.LG cs.CR stat.ML published:2012-06-27 summary:We investigate a family of poisoning attacks against Support Vector Machines(SVM). Such attacks inject specially crafted training data that increases theSVM's test error. Central to the motivation for these attacks is the fact thatmost learning algorithms assume that their training data comes from a naturalor well-behaved distribution. However, this assumption does not generally holdin security-sensitive settings. As we demonstrate, an intelligent adversarycan, to some extent, predict the change of the SVM's decision function due tomalicious input and use this ability to construct malicious data. The proposedattack uses a gradient ascent strategy in which the gradient is computed basedon properties of the SVM's optimal solution. This method can be kernelized andenables the attack to be constructed in the input space even for non-linearkernels. We experimentally demonstrate that our gradient ascent procedurereliably identifies good local maxima of the non-convex validation errorsurface, which significantly increases the classifier's test error.
arxiv-2700-197 | Spatio-Temporal Covariance Descriptors for Action and Gesture Recognition | http://arxiv.org/pdf/1303.6021v1.pdf | author:Andres Sanin, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell category:cs.CV cs.HC published:2013-03-25 summary:We propose a new action and gesture recognition method based onspatio-temporal covariance descriptors and a weighted Riemannian localitypreserving projection approach that takes into account the curved space formedby the descriptors. The weighted projection is then exploited during boostingto create a final multiclass classification algorithm that employs the mostuseful spatio-temporal regions. We also show how the descriptors can becomputed quickly through the use of integral video representations. Experimentson the UCF sport, CK+ facial expression and Cambridge hand gesture datasetsindicate superior performance of the proposed method compared to several recentstate-of-the-art techniques. The proposed method is robust and does not requireadditional processing of the videos, such as foreground detection,interest-point detection or tracking.
arxiv-2700-198 | Generalizing k-means for an arbitrary distance matrix | http://arxiv.org/pdf/1303.6001v1.pdf | author:Balázs Szalkai category:cs.LG cs.CV stat.ML published:2013-03-24 summary:The original k-means clustering method works only if the exact vectorsrepresenting the data points are known. Therefore calculating the distancesfrom the centroids needs vector operations, since the average of abstract datapoints is undefined. Existing algorithms can be extended for those cases whenthe sole input is the distance matrix, and the exact representing vectors areunknown. This extension may be named relational k-means after a notation for asimilar algorithm invented for fuzzy clustering. A method is then proposed forgeneralizing k-means for scenarios when the data points have absolutely noconnection with a Euclidean space.
arxiv-2700-199 | Efficient Reinforcement Learning for High Dimensional Linear Quadratic Systems | http://arxiv.org/pdf/1303.5984v1.pdf | author:Morteza Ibrahimi, Adel Javanmard, Benjamin Van Roy category:stat.ML cs.LG math.OC published:2013-03-24 summary:We study the problem of adaptive control of a high dimensional linearquadratic (LQ) system. Previous work established the asymptotic convergence toan optimal controller for various adaptive control schemes. More recently, forthe average cost LQ problem, a regret bound of ${O}(\sqrt{T})$ was shown, apartform logarithmic factors. However, this bound scales exponentially with $p$,the dimension of the state space. In this work we consider the case where thematrices describing the dynamic of the LQ system are sparse and theirdimensions are large. We present an adaptive control scheme that achieves aregret bound of ${O}(p \sqrt{T})$, apart from logarithmic factors. Inparticular, our algorithm has an average cost of $(1+\eps)$ times the optimumcost after $T = \polylog(p) O(1/\eps^2)$. This is in comparison to previouswork on the dense dynamics where the algorithm requires time that scalesexponentially with dimension in order to achieve regret of $\eps$ times theoptimal cost. We believe that our result has prominent applications in the emerging area ofcomputational advertising, in particular targeted online advertising andadvertising in social networks.
arxiv-2700-200 | On Learnability, Complexity and Stability | http://arxiv.org/pdf/1303.5976v1.pdf | author:Silvia Villa, Lorenzo Rosasco, Tomaso Poggio category:stat.ML cs.LG published:2013-03-24 summary:We consider the fundamental question of learnability of a hypotheses class inthe supervised learning setting and in the general learning setting introducedby Vladimir Vapnik. We survey classic results characterizing learnability interm of suitable notions of complexity, as well as more recent results thatestablish the connection between learnability and stability of a learningalgorithm.
arxiv-2700-201 | A Diffusion Process on Riemannian Manifold for Visual Tracking | http://arxiv.org/pdf/1303.5913v1.pdf | author:Marcus Chen, Cham Tat Jen, Pang Sze Kim, Alvina Goh category:cs.CV cs.LG cs.RO stat.ML published:2013-03-24 summary:Robust visual tracking for long video sequences is a research area that hasmany important applications. The main challenges include how the target imagecan be modeled and how this model can be updated. In this paper, we model thetarget using a covariance descriptor, as this descriptor is robust to problemssuch as pixel-pixel misalignment, pose and illumination changes, that commonlyoccur in visual tracking. We model the changes in the template using agenerative process. We introduce a new dynamical model for the template updateusing a random walk on the Riemannian manifold where the covariance descriptorslie in. This is done using log-transformed space of the manifold to free theconstraints imposed inherently by positive semidefinite matrices. Modelingtemplate variations and poses kinetics together in the state space enables usto jointly quantify the uncertainties relating to the kinematic states and thetemplate in a principled way. Finally, the sequential inference of theposterior distribution of the kinematic states and the template is done using aparticle filter. Our results shows that this principled approach can be robustto changes in illumination, poses and spatial affine transformation. In theexperiments, our method outperformed the current state-of-the-art algorithm -the incremental Principal Component Analysis method, particularly when a targetunderwent fast poses changes and also maintained a comparable performance instable target tracking cases.
arxiv-2700-202 | Near-optimal compressed sensing guarantees for total variation minimization | http://arxiv.org/pdf/1210.3098v2.pdf | author:Deanna Needell, Rachel Ward category:math.NA cs.CV cs.IT math.IT published:2012-10-11 summary:Consider the problem of reconstructing a multidimensional signal from anunderdetermined set of measurements, as in the setting of compressed sensing.Without any additional assumptions, this problem is ill-posed. However, forsignals such as natural images or movies, the minimal total variation estimateconsistent with the measurements often produces a good approximation to theunderlying signal, even if the number of measurements is far smaller than theambient dimensionality. This paper extends recent reconstruction guarantees fortwo-dimensional images to signals of arbitrary dimension d>1 and to isotropictotal variation problems. To be precise, we show that a multidimensional signalx can be reconstructed from O(sd*log(N^d)) linear measurements using totalvariation minimization to within a factor of the best s-term approximation ofits gradient. The reconstruction guarantees we provide are necessarily optimalup to polynomial factors in the spatial dimension d.
arxiv-2700-203 | High quality topic extraction from business news explains abnormal financial market volatility | http://arxiv.org/pdf/1210.6321v4.pdf | author:Ryohei Hisano, Didier Sornette, Takayuki Mizuno, Takaaki Ohnishi, Tsutomu Watanabe category:stat.ML cs.LG cs.SI physics.soc-ph q-fin.ST published:2012-10-23 summary:Understanding the mutual relationships between information flows and socialactivity in society today is one of the cornerstones of the social sciences. Infinancial economics, the key issue in this regard is understanding andquantifying how news of all possible types (geopolitical, environmental,social, financial, economic, etc.) affect trading and the pricing of firms inorganized stock markets. In this article, we seek to address this issue byperforming an analysis of more than 24 million news records provided byThompson Reuters and of their relationship with trading activity for 206 majorstocks in the S&P US stock index. We show that the whole landscape of news thataffect stock price movements can be automatically summarized via simpleregularized regressions between trading activity and news information piecesdecomposed, with the help of simple topic modeling techniques, into their"thematic" features. Using these methods, we are able to estimate and quantifythe impacts of news on trading. We introduce network-based visualizationtechniques to represent the whole landscape of news information associated witha basket of stocks. The examination of the words that are representative of thetopic distributions confirms that our method is able to extract the significantpieces of information influencing the stock market. Our results show that oneof the most puzzling stylized fact in financial economies, namely that atcertain times trading volumes appear to be "abnormally large," can be partiallyexplained by the flow of news. In this sense, our results prove that there isno "excess trading," when restricting to times when news are genuinely noveland provide relevant financial information.
arxiv-2700-204 | Deep Gaussian Processes | http://arxiv.org/pdf/1211.0358v2.pdf | author:Andreas C. Damianou, Neil D. Lawrence category:stat.ML cs.LG math.PR 60G15, 58E30 published:2012-11-02 summary:In this paper we introduce deep Gaussian process (GP) models. Deep GPs are adeep belief network based on Gaussian process mappings. The data is modeled asthe output of a multivariate GP. The inputs to that Gaussian process are thengoverned by another GP. A single layer model is equivalent to a standard GP orthe GP latent variable model (GP-LVM). We perform inference in the model byapproximate variational marginalization. This results in a strict lower boundon the marginal likelihood of the model which we use for model selection(number of layers and nodes per layer). Deep belief networks are typicallyapplied to relatively large data sets using stochastic gradient descent foroptimization. Our fully Bayesian treatment allows for the application of deepmodels even when data is scarce. Model selection by our variational bound showsthat a five layer hierarchy is justified even when modelling a digit data setcontaining only 150 examples.
arxiv-2700-205 | Speech Recognition with Deep Recurrent Neural Networks | http://arxiv.org/pdf/1303.5778v1.pdf | author:Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton category:cs.NE cs.CL published:2013-03-22 summary:Recurrent neural networks (RNNs) are a powerful model for sequential data.End-to-end training methods such as Connectionist Temporal Classification makeit possible to train RNNs for sequence labelling problems where theinput-output alignment is unknown. The combination of these methods with theLong Short-term Memory RNN architecture has proved particularly fruitful,delivering state-of-the-art results in cursive handwriting recognition. HoweverRNN performance in speech recognition has so far been disappointing, withbetter results returned by deep feedforward networks. This paper investigates\emph{deep recurrent neural networks}, which combine the multiple levels ofrepresentation that have proved so effective in deep networks with the flexibleuse of long range context that empowers RNNs. When trained end-to-end withsuitable regularisation, we find that deep Long Short-term Memory RNNs achievea test set error of 17.7% on the TIMIT phoneme recognition benchmark, which toour knowledge is the best recorded score.
arxiv-2700-206 | Bayesian Compressed Regression | http://arxiv.org/pdf/1303.0642v2.pdf | author:Rajarshi Guhaniyogi, David B. Dunson category:stat.ML cs.LG published:2013-03-04 summary:As an alternative to variable selection or shrinkage in high dimensionalregression, we propose to randomly compress the predictors prior to analysis.This dramatically reduces storage and computational bottlenecks, performingwell when the predictors can be projected to a low dimensional linear subspacewith minimal loss of information about the response. As opposed to existingBayesian dimensionality reduction approaches, the exact posterior distributionconditional on the compressed data is available analytically, speeding upcomputation by many orders of magnitude while also bypassing robustness issuesdue to convergence and mixing problems with MCMC. Model averaging is used toreduce sensitivity to the random projection matrix, while accommodatinguncertainty in the subspace dimension. Strong theoretical support is providedfor the approach by showing near parametric convergence rates for thepredictive density in the large p small n asymptotic paradigm. Practicalperformance relative to competitors is illustrated in simulations and real dataapplications.
arxiv-2700-207 | Cortical Surface Co-Registration based on MRI Images and Photos | http://arxiv.org/pdf/1303.5691v1.pdf | author:Benjamin Berkels, Ivan Cabrilo, Sven Haller, Martin Rumpf, Carlo Schaller category:cs.CV published:2013-03-22 summary:Brain shift, i.e. the change in configuration of the brain after opening thedura mater, is a key problem in neuronavigation. We present an approach toco-register intra-operative microscope images with pre-operative MRI to adaptand optimize intra-operative neuronavigation. The tools are a robustclassification of sulci on MRI extracted cortical surfaces, guided user markingof most prominent sulci on a microscope image, and the actual variationalregistration method with a fidelity energy for 3D deformations of the corticalsurface combined with a higher order, linear elastica type prior energy.Furthermore, the actual registration is validated on an artificial testbed withknown ground truth deformation and on real data of a neuro clinical patient.
arxiv-2700-208 | Measuring the Complexity of Ultra-Large-Scale Adaptive Systems | http://arxiv.org/pdf/1207.6656v2.pdf | author:Michele Amoretti, Carlos Gershenson category:cs.NE cs.NI nlin.AO published:2012-07-27 summary:Ultra-large scale (ULS) systems are becoming pervasive. They are inherentlycomplex, which makes their design and control a challenge for traditionalmethods. Here we propose the design and analysis of ULS systems using measuresof complexity, emergence, self-organization, and homeostasis based oninformation theory. These measures allow the evaluation of ULS systems and thuscan be used to guide their design. We evaluate the proposal with a ULScomputing system provided with adaptation mechanisms. We show the evolution ofthe system with stable and also changing workload, using different fitnessfunctions. When the adaptive plan forces the system to converge to a predefinedperformance level, the nodes may result in highly unstable configurations, thatcorrespond to a high variance in time of the measured complexity. Conversely,if the adaptive plan is less "aggressive", the system may be more stable, butthe optimal performance may not be achieved.
arxiv-2700-209 | Network Detection Theory and Performance | http://arxiv.org/pdf/1303.5613v1.pdf | author:Steven T. Smith, Kenneth D. Senne, Scott Philips, Edward K. Kao, Garrett Bernstein category:cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH published:2013-03-22 summary:Network detection is an important capability in many areas of appliedresearch in which data can be represented as a graph of entities andrelationships. Oftentimes the object of interest is a relatively small subgraphin an enormous, potentially uninteresting background. This aspect characterizesnetwork detection as a "big data" problem. Graph partitioning and networkdiscovery have been major research areas over the last ten years, driven byinterest in internet search, cyber security, social networks, and criminal orterrorist activities. The specific problem of network discovery is addressed asa special case of graph partitioning in which membership in a small subgraph ofinterest must be determined. Algebraic graph theory is used as the basis toanalyze and compare different network detection methods. A new Bayesian networkdetection framework is introduced that partitions the graph based on priorinformation and direct observations. The new approach, called space-time threatpropagation, is proved to maximize the probability of detection and istherefore optimum in the Neyman-Pearson sense. This optimality criterion iscompared to spectral community detection approaches which divide the globalgraph into subsets or communities with optimal connectivity properties. We alsoexplore a new generative stochastic model for covert networks and analyze usingreceiver operating characteristics the detection performance of both classes ofoptimal detection techniques.
arxiv-2700-210 | Robust and Trend Following Student's t Kalman Smoothers | http://arxiv.org/pdf/1303.5588v1.pdf | author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:math.OC math.NA stat.AP stat.ML 62F35, 65K10 published:2013-03-22 summary:We present a Kalman smoothing framework based on modeling errors using theheavy tailed Student's t distribution, along with algorithms, convergencetheory, open-source general implementation, and several important applications.The computational effort per iteration grows linearly with the length of thetime series, and all smoothers allow nonlinear process and measurement models. Robust smoothers form an important subclass of smoothers within thisframework. These smoothers work in situations where measurements are highlycontaminated by noise or include data unexplained by the forward model. Highlyrobust smoothers are developed by modeling measurement errors using theStudent's t distribution, and outperform the recently proposed L1-Laplacesmoother in extreme situations with data containing 20% or more outliers. A second special application we consider in detail allows tracking suddenchanges in the state. It is developed by modeling process noise using theStudent's t distribution, and the resulting smoother can track sudden changesin the state. These features can be used separately or in tandem, and we present a generalsmoother algorithm and open source implementation, together with convergenceanalysis that covers a wide range of smoothers. A key ingredient of ourapproach is a technique to deal with the non-convexity of the Student's t lossfunction. Numerical results for linear and nonlinear models illustrate theperformance of the new smoothers for robust and tracking applications, as wellas for mixed problems that have both types of features.
arxiv-2700-211 | Genetic Algorithm for Designing a Convenient Facility Layout for a Circular Flow Path | http://arxiv.org/pdf/1211.2361v2.pdf | author:Hossein Jahandideh, Ardavan Asef-Vaziri, Mohammad Modarres category:cs.NE published:2012-11-11 summary:In this paper, we present a heuristic for designing facility layouts that areconvenient for designing a unidirectional loop for material handling. We usegenetic algorithm where the objective function and crossover and mutationoperators have all been designed specifically for this purpose. Our design ismade under flexible bay structure and comparisons are made with other layoutsfrom the literature that were designed under flexible bay structure.
arxiv-2700-212 | Adverse Conditions and ASR Techniques for Robust Speech User Interface | http://arxiv.org/pdf/1303.5515v1.pdf | author:Urmila Shrawankar, VM Thakare category:cs.CL cs.SD published:2013-03-22 summary:The main motivation for Automatic Speech Recognition (ASR) is efficientinterfaces to computers, and for the interfaces to be natural and truly useful,it should provide coverage for a large group of users. The purpose of thesetasks is to further improve man-machine communication. ASR systems exhibitunacceptable degradations in performance when the acoustical environments usedfor training and testing the system are not the same. The goal of this researchis to increase the robustness of the speech recognition systems with respect tochanges in the environment. A system can be labeled as environment-independentif the recognition accuracy for a new environment is the same or higher thanthat obtained when the system is retrained for that environment. Attaining suchperformance is the dream of the researchers. This paper elaborates some of thedifficulties with Automatic Speech Recognition (ASR). These difficulties areclassified into Speakers characteristics and environmental conditions, andtried to suggest some techniques to compensate variations in speech signal.This paper focuses on the robustness with respect to speakers variations andchanges in the acoustical environment. We discussed several different externalfactors that change the environment and physiological differences that affectthe performance of a speech recognition system followed by techniques that arehelpful to design a robust ASR system.
arxiv-2700-213 | Parameters Optimization for Improving ASR Performance in Adverse Real World Noisy Environmental Conditions | http://arxiv.org/pdf/1303.5513v1.pdf | author:Urmila Shrawankar, Vilas Thakare category:cs.CL cs.SD published:2013-03-22 summary:From the existing research it has been observed that many techniques andmethodologies are available for performing every step of Automatic SpeechRecognition (ASR) system, but the performance (Minimization of Word ErrorRecognition-WER and Maximization of Word Accuracy Rate- WAR) of the methodologyis not dependent on the only technique applied in that method. The researchwork indicates that, performance mainly depends on the category of the noise,the level of the noise and the variable size of the window, frame, frameoverlap etc is considered in the existing methods. The main aim of the workpresented in this paper is to use variable size of parameters like window size,frame size and frame overlap percentage to observe the performance ofalgorithms for various categories of noise with different levels and also trainthe system for all size of parameters and category of real world noisyenvironment to improve the performance of the speech recognition system. Thispaper presents the results of Signal-to-Noise Ratio (SNR) and Accuracy test byapplying variable size of parameters. It is observed that, it is really veryhard to evaluate test results and decide parameter size for ASR performanceimprovement for its resultant optimization. Hence, this study further suggeststhe feasible and optimum parameter size using Fuzzy Inference System (FIS) forenhancing resultant accuracy in adverse real world noisy environmentalconditions. This work will be helpful to give discriminative training ofubiquitous ASR system for better Human Computer Interaction (HCI).
arxiv-2700-214 | Warped Mixtures for Nonparametric Cluster Shapes | http://arxiv.org/pdf/1206.1846v2.pdf | author:Tomoharu Iwata, David Duvenaud, Zoubin Ghahramani category:stat.ML I.5.3 published:2012-06-08 summary:A mixture of Gaussians fit to a single curved or heavy-tailed cluster willreport that the data contains many clusters. To produce more appropriateclusterings, we introduce a model which warps a latent mixture of Gaussians toproduce nonparametric cluster shapes. The possibly low-dimensional latentmixture model allows us to summarize the properties of the high-dimensionalclusters (or density manifolds) describing the data. The number of manifolds,as well as the shape and dimension of each manifold is automatically inferred.We derive a simple inference scheme for this model which analyticallyintegrates out both the mixture parameters and the warping function. We showthat our model is effective for density estimation, performs better thaninfinite Gaussian mixture models at recovering the true number of clusters, andproduces interpretable summaries of high-dimensional datasets.
arxiv-2700-215 | A Semantic Matching Energy Function for Learning with Multi-relational Data | http://arxiv.org/pdf/1301.3485v2.pdf | author:Xavier Glorot, Antoine Bordes, Jason Weston, Yoshua Bengio category:cs.LG published:2013-01-15 summary:Large-scale relational learning becomes crucial for handling the huge amountsof structured data generated daily in many application domains ranging fromcomputational biology or information retrieval, to natural language processing.In this paper, we present a new neural network architecture designed to embedmulti-relational graphs into a flexible continuous vector space in which theoriginal data is kept and enhanced. The network is trained to encode thesemantics of these graphs in order to assign high probabilities to plausiblecomponents. We empirically show that it reaches competitive performance in linkprediction on standard datasets from the literature.
arxiv-2700-216 | Separable Dictionary Learning | http://arxiv.org/pdf/1303.5244v1.pdf | author:Simon Hawe, Matthias Seibert, Martin Kleinsteuber category:cs.CV cs.LG stat.ML published:2013-03-21 summary:Many techniques in computer vision, machine learning, and statistics rely onthe fact that a signal of interest admits a sparse representation over somedictionary. Dictionaries are either available analytically, or can be learnedfrom a suitable training set. While analytic dictionaries permit to capture theglobal structure of a signal and allow a fast implementation, learneddictionaries often perform better in applications as they are more adapted tothe considered class of signals. In imagery, unfortunately, the numericalburden for (i) learning a dictionary and for (ii) employing the dictionary forreconstruction tasks only allows to deal with relatively small image patchesthat only capture local image information. The approach presented in this paperaims at overcoming these drawbacks by allowing a separable structure on thedictionary throughout the learning process. On the one hand, this permitslarger patch-sizes for the learning phase, on the other hand, the dictionary isapplied efficiently in reconstruction tasks. The learning procedure is based onoptimizing over a product of spheres which updates the dictionary as a whole,thus enforces basic dictionary properties such as mutual coherence explicitlyduring the learning procedure. In the special case where no separable structureis enforced, our method competes with state-of-the-art dictionary learningmethods like K-SVD.
arxiv-2700-217 | Using evolutionary design to interactively sketch car silhouettes and stimulate designer's creativity | http://arxiv.org/pdf/1303.5050v2.pdf | author:François Cluzel, Bernard Yannou, Markus Dihlmann category:cs.NE cs.HC physics.med-ph published:2013-03-17 summary:An Interactive Genetic Algorithm is proposed to progressively sketch thedesired side-view of a car profile. It adopts a Fourier decomposition of a 2Dprofile as the genotype, and proposes a cross-over mechanism. In addition, aformula function of two genes' discrepancies is fitted to the perceiveddissimilarity between two car profiles. This similarity index is intensivelyused, throughout a series of user tests, to highlight the added value of theIGA compared to a systematic car shape exploration, to prove its ability tocreate superior satisfactory designs and to stimulate designer's creativity.These tests have involved six designers with a design goal defined by asemantic attribute. The results reveal that if "friendly" is diverselyinterpreted in terms of car shapes, "sportive" denotes a very conventionalrepresentation which may be a limitation for shape renewal.
arxiv-2700-218 | Estimating Confusions in the ASR Channel for Improved Topic-based Language Model Adaptation | http://arxiv.org/pdf/1303.5148v1.pdf | author:Damianos Karakos, Mark Dredze, Sanjeev Khudanpur category:cs.CL cs.LG published:2013-03-21 summary:Human language is a combination of elemental languages/domains/styles thatchange across and sometimes within discourses. Language models, which play acrucial role in speech recognizers and machine translation systems, areparticularly sensitive to such changes, unless some form of adaptation takesplace. One approach to speech language model adaptation is self-training, inwhich a language model's parameters are tuned based on automaticallytranscribed audio. However, transcription errors can misguide self-training,particularly in challenging settings such as conversational speech. In thiswork, we propose a model that considers the confusions (errors) of the ASRchannel. By modeling the likely confusions in the ASR output instead of usingjust the 1-best, we improve self-training efficacy by obtaining a more reliablereference transcription estimate. We demonstrate improved topic-based languagemodeling adaptation results over both 1-best and lattice self-training usingour ASR channel confusion estimates on telephone conversations.
arxiv-2700-219 | Behavior Pattern Recognition using A New Representation Model | http://arxiv.org/pdf/1301.3630v4.pdf | author:Qifeng Qiao, Peter A. Beling category:cs.LG published:2013-01-16 summary:We study the use of inverse reinforcement learning (IRL) as a tool for therecognition of agents' behavior on the basis of observation of their sequentialdecision behavior interacting with the environment. We model the problem facedby the agents as a Markov decision process (MDP) and model the observedbehavior of the agents in terms of forward planning for the MDP. We use IRL tolearn reward functions and then use these reward functions as the basis forclustering or classification models. Experimental studies with GridWorld, anavigation problem, and the secretary problem, an optimal stopping problem,suggest reward vectors found from IRL can be a good basis for behavior patternrecognition problems. Empirical comparisons of our method with several existingIRL algorithms and with direct methods that use feature statistics observed instate-action space suggest it may be superior for recognition problems.
arxiv-2700-220 | Saturating Auto-Encoders | http://arxiv.org/pdf/1301.3577v3.pdf | author:Rostislav Goroshin, Yann LeCun category:cs.LG published:2013-01-16 summary:We introduce a simple new regularizer for auto-encoders whose hidden-unitactivation functions contain at least one zero-gradient (saturated) region.This regularizer explicitly encourages activations in the saturated region(s)of the corresponding activation function. We call these SaturatingAuto-Encoders (SATAE). We show that the saturation regularizer explicitlylimits the SATAE's ability to reconstruct inputs which are not near the datamanifold. Furthermore, we show that a wide variety of features can be learnedwhen different activation functions are used. Finally, connections areestablished with the Contractive and Sparse Auto-Encoders.
arxiv-2700-221 | Analytic solution of a model of language competition with bilingualism and interlinguistic similarity | http://arxiv.org/pdf/1303.4959v1.pdf | author:Victoria Otero-Espinar, Luís F. Seoane, Juan J. Nieto, Jorge Mira category:physics.soc-ph cs.CL published:2013-03-20 summary:An in-depth analytic study of a model of language dynamics is presented: amodel which tackles the problem of the coexistence of two languages within aclosed community of speakers taking into account bilingualism and incorporatinga parameter to measure the distance between languages. After previous numericalsimulations, the model yielded that coexistence might lead to survival of bothlanguages within monolingual speakers along with a bilingual community or toextinction of the weakest tongue depending on different parameters. In thispaper, such study is closed with thorough analytical calculations to settle theresults in a robust way and previous results are refined with somemodifications. From the present analysis it is possible to almost completelyassay the number and nature of the equilibrium points of the model, whichdepend on its parameters, as well as to build a phase space based on them.Also, we obtain conclusions on the way the languages evolve with time. Ourrigorous considerations also suggest ways to further improve the model andfacilitate the comparison of its consequences with those from other approachesor with real data.
arxiv-2700-222 | A Robust Rapid Approach to Image Segmentation with Optimal Thresholding and Watershed Transform | http://arxiv.org/pdf/1303.4866v1.pdf | author:Ankit R. Chadha, Neha S. Satam category:cs.CV published:2013-03-20 summary:This paper describes a novel method for partitioning image into meaningfulsegments. The proposed method employs watershed transform, a well-known imagesegmentation technique. Along with that, it uses various auxiliary schemes suchas Binary Gradient Masking, dilation which segment the image in proper way. Thealgorithm proposed in this paper considers all these methods in effective wayand takes little time. It is organized in such a manner so that it operates oninput image adaptively. Its robustness and efficiency makes it more convenientand suitable for all types of images.
arxiv-2700-223 | Asynchronous Cellular Operations on Gray Images Extracting Topographic Shape Features and Their Relations | http://arxiv.org/pdf/1303.4840v1.pdf | author:Igor Polkovnikov category:cs.CV published:2013-03-20 summary:A variety of operations of cellular automata on gray images is presented. Alloperations are of a wave-front nature finishing in a stable state. They areused to extract shape descripting gray objects robust to a variety of patterndistortions. Topographic terms are used: "lakes", "dales", "dales of dales". Itis shown how mutual object relations like "above" can be presented in terms ofgray image analysis and how it can be used for character classification and forgray pattern decomposition. Algorithms can be realized with a parallelasynchronous architecture. Keywords: Pattern Recognition, MathematicalMorphology, Cellular Automata, Wave-front Algorithms, Gray Image Analysis,Topographical Shape Descriptors, Asynchronous Parallel Processors, Holes,Cavities, Concavities, Graphs.
arxiv-2700-224 | The State of the Art Recognize in Arabic Script through Combination of Online and Offline | http://arxiv.org/pdf/1303.4839v1.pdf | author:Dr. Firoj Parwej category:cs.CV published:2013-03-20 summary:Handwriting recognition refers to the identification of written characters.Handwriting recognition has become an acute research area in recent years forthe ease of access of computer science. In this paper primarily discussedOn-line and Off-line handwriting recognition methods for Arabic words which areoften used among then across the Middle East and North Africa People. Arabicword online handwriting recognition is a very challenging task due to itscursive nature. Because of the characteristic of the whole body of the Arabicscript, namely connectivity between the characters, thereby the segmentation ofAn Arabic script is very difficult. In this paper we introduced an Arabicscript multiple classifier system for recognizing notes written on a Starboard.This Arabic script multiple classifier system combines one off-line and on-linehandwriting recognition systems. The Arabic script recognizers are all based onHidden Markov Models but vary in the way of preprocessing and normalization. Tocombine the Arabic script output sequences of the recognizers, we incrementallyalign the word sequences using a norm string matching algorithm. The Arabicscript combination we could increase the system performance over the excellentcharacter recognizer by about 3%. The proposed technique is also the necessarystep towards character recognition, person identification, personalitydetermination where input data is processed from all perspectives.
arxiv-2700-225 | A Survey of Appearance Models in Visual Object Tracking | http://arxiv.org/pdf/1303.4803v1.pdf | author:Xi Li, Weiming Hu, Chunhua Shen, Zhongfei Zhang, Anthony Dick, Anton van den Hengel category:cs.CV published:2013-03-20 summary:Visual object tracking is a significant computer vision task which can beapplied to many domains such as visual surveillance, human computerinteraction, and video compression. In the literature, researchers haveproposed a variety of 2D appearance models. To help readers swiftly learn therecent advances in 2D appearance models for visual object tracking, wecontribute this survey, which provides a detailed review of the existing 2Dappearance models. In particular, this survey takes a module-based architecturethat enables readers to easily grasp the key points of visual object tracking.In this survey, we first decompose the problem of appearance modeling into twodifferent processing stages: visual representation and statistical modeling.Then, different 2D appearance models are categorized and discussed with respectto their composition modules. Finally, we address several issues of interest aswell as the remaining challenges for future research on this topic. Thecontributions of this survey are four-fold. First, we review the literature ofvisual representations according to their feature-construction mechanisms(i.e., local and global). Second, the existing statistical modeling schemes fortracking-by-detection are reviewed according to their model-constructionmechanisms: generative, discriminative, and hybrid generative-discriminative.Third, each type of visual representations or statistical modeling techniquesis analyzed and discussed from a theoretical or practical viewpoint. Fourth,the existing benchmark resources (e.g., source code and video datasets) areexamined in this survey.
arxiv-2700-226 | Zero-Shot Learning Through Cross-Modal Transfer | http://arxiv.org/pdf/1301.3666v2.pdf | author:Richard Socher, Milind Ganjoo, Hamsa Sridhar, Osbert Bastani, Christopher D. Manning, Andrew Y. Ng category:cs.CV cs.LG published:2013-01-16 summary:This work introduces a model that can recognize objects in images even if notraining data is available for the objects. The only necessary knowledge aboutthe unseen categories comes from unsupervised large text corpora. In ourzero-shot framework distributional information in language can be seen asspanning a semantic basis for understanding what objects look like. Mostprevious zero-shot learning models can only differentiate between unseenclasses. In contrast, our model can both obtain state of the art performance onclasses that have thousands of training images and obtain reasonableperformance on unseen classes. This is achieved by first using outlierdetection in the semantic space and then two separate recognition models.Furthermore, our model does not require any manually defined semantic featuresfor either words or images.
arxiv-2700-227 | On the convergence of maximum variance unfolding | http://arxiv.org/pdf/1209.0016v2.pdf | author:Ery Arias-Castro, Bruno Pelletier category:stat.ML published:2012-08-31 summary:Maximum Variance Unfolding is one of the main methods for (nonlinear)dimensionality reduction. We study its large sample limit, providing specificrates of convergence under standard assumptions. We find that it is consistentwhen the underlying submanifold is isometric to a convex subset, and we providesome simple examples where it fails to be consistent.
arxiv-2700-228 | A Method for Comparing Hedge Funds | http://arxiv.org/pdf/1303.0073v2.pdf | author:Uri Kartoun category:q-fin.ST cs.IR cs.LG stat.ML published:2013-03-01 summary:The paper presents new machine learning methods: signal composition, whichclassifies time-series regardless of length, type, and quantity; andself-labeling, a supervised-learning enhancement. The paper describes furtherthe implementation of the methods on a financial search engine system toidentify behavioral similarities among time-series representing monthly returnsof 11,312 hedge funds operated during approximately one decade (2000 - 2010).The presented approach of cross-category and cross-location classificationassists the investor to identify alternative investments.
arxiv-2700-229 | Bio-Signals-based Situation Comparison Approach to Predict Pain | http://arxiv.org/pdf/1303.0076v2.pdf | author:Uri Kartoun category:stat.AP cs.LG stat.ML published:2013-03-01 summary:This paper describes a time-series-based classification approach to identifysimilarities between bio-medical-based situations. The proposed approach allowsclassifying collections of time-series representing bio-medical measurements,i.e., situations, regardless of the type, the length and the quantity of thetime-series a situation comprised of.
arxiv-2700-230 | Inverse Signal Classification for Financial Instruments | http://arxiv.org/pdf/1303.0283v2.pdf | author:Uri Kartoun category:cs.LG cs.IR q-fin.ST stat.ML published:2013-03-01 summary:The paper presents new machine learning methods: signal composition, whichclassifies time-series regardless of length, type, and quantity; andself-labeling, a supervised-learning enhancement. The paper describes furtherthe implementation of the methods on a financial search engine system using acollection of 7,881 financial instruments traded during 2011 to identifyinverse behavior among the time-series.
arxiv-2700-231 | A proximal Newton framework for composite minimization: Graph learning without Cholesky decompositions and matrix inversions | http://arxiv.org/pdf/1301.1459v3.pdf | author:Quoc Tran Dinh, Anastasios Kyrillidis, Volkan Cevher category:stat.ML math.OC published:2013-01-08 summary:We propose an algorithmic framework for convex minimization problems of acomposite function with two terms: a self-concordant function and a possiblynonsmooth regularization term. Our method is a new proximal Newton algorithmthat features a local quadratic convergence rate. As a specific instance of ourframework, we consider the sparse inverse covariance matrix estimation in graphlearning problems. Via a careful dual formulation and a novel analyticstep-size selection procedure, our approach for graph learning avoids Choleskydecompositions and matrix inversions in its iteration making it attractive forparallel and distributed implementations.
arxiv-2700-232 | Discriminative Recurrent Sparse Auto-Encoders | http://arxiv.org/pdf/1301.3775v4.pdf | author:Jason Tyler Rolfe, Yann LeCun category:cs.LG cs.CV published:2013-01-16 summary:We present the discriminative recurrent sparse auto-encoder model, comprisinga recurrent encoder of rectified linear units, unrolled for a fixed number ofiterations, and connected to two linear decoders that reconstruct the input andpredict its supervised classification. Training viabackpropagation-through-time initially minimizes an unsupervised sparsereconstruction error; the loss function is then augmented with a discriminativeterm on the supervised classification. The depth implicit in thetemporally-unrolled form allows the system to exhibit all the power of deepnetworks, while substantially reducing the number of trainable parameters. From an initially unstructured network the hidden units differentiate intocategorical-units, each of which represents an input prototype with awell-defined class; and part-units representing deformations of theseprototypes. The learned organization of the recurrent encoder is hierarchical:part-units are driven directly by the input, whereas the activity ofcategorical-units builds up over time through interactions with the part-units.Even using a small number of hidden units per layer, discriminative recurrentsparse auto-encoders achieve excellent performance on MNIST.
arxiv-2700-233 | Large-Scale Learning with Less RAM via Randomization | http://arxiv.org/pdf/1303.4664v1.pdf | author:Daniel Golovin, D. Sculley, H. Brendan McMahan, Michael Young category:cs.LG published:2013-03-19 summary:We reduce the memory footprint of popular large-scale online learning methodsby projecting our weight vector onto a coarse discrete set using randomizedrounding. Compared to standard 32-bit float encodings, this reduces RAM usageby more than 50% during training and by up to 95% when making predictions froma fixed model, with almost no loss in accuracy. We also show that randomizedcounting can be used to implement per-coordinate learning rates, improvingmodel quality with little additional RAM. We prove these memory-saving methodsachieve regret guarantees similar to their exact variants. Empirical evaluationconfirms excellent performance, dominating standard approaches across memoryversus accuracy tradeoffs.
arxiv-2700-234 | Handwritten and Printed Text Separation in Real Document | http://arxiv.org/pdf/1303.4614v1.pdf | author:Abdel Belaïd, K. C. Santosh, Vincent Poulain D'Andecy category:cs.CV published:2013-03-19 summary:The aim of the paper is to separate handwritten and printed text from a realdocument embedded with noise, graphics including annotations. Relying onrun-length smoothing algorithm (RLSA), the extracted pseudo-lines andpseudo-words are used as basic blocks for classification. To handle this, amulti-class support vector machine (SVM) with Gaussian kernel performs a firstlabelling of each pseudo-word including the study of local neighbourhood. Itthen propagates the context between neighbours so that we can correct possiblelabelling errors. Considering running time complexity issue, we propose linearcomplexity methods where we use k-NN with constraint. When using a kd-tree, itis almost linearly proportional to the number of pseudo-words. The performanceof our system is close to 90%, even when very small learning dataset wheresamples are basically composed of complex administrative documents.
arxiv-2700-235 | Better subset regression | http://arxiv.org/pdf/1212.0634v2.pdf | author:Shifeng Xiong category:stat.ME math.ST stat.CO stat.ML stat.TH 62J07 D.2.2 published:2012-12-04 summary:To find efficient screening methods for high dimensional linear regressionmodels, this paper studies the relationship between model fitting and screeningperformance. Under a sparsity assumption, we show that a subset that includesthe true submodel always yields smaller residual sum of squares (i.e., hasbetter model fitting) than all that do not in a general asymptotic setting.This indicates that, for screening important variables, we could follow a"better fitting, better screening" rule, i.e., pick a "better" subset that hasbetter model fitting. To seek such a better subset, we consider theoptimization problem associated with best subset regression. An EM algorithm,called orthogonalizing subset screening, and its accelerating version areproposed for searching for the best subset. Although the two algorithms cannotguarantee that a subset they yield is the best, their monotonicity propertymakes the subset have better model fitting than initial subsets generated bypopular screening methods, and thus the subset can have better screeningperformance asymptotically. Simulation results show that our methods are verycompetitive in high dimensional variable screening even for finite samplesizes.
arxiv-2700-236 | Block Coordinate Descent for Sparse NMF | http://arxiv.org/pdf/1301.3527v2.pdf | author:Vamsi K. Potluru, Sergey M. Plis, Jonathan Le Roux, Barak A. Pearlmutter, Vince D. Calhoun, Thomas P. Hayes category:cs.LG cs.NA published:2013-01-15 summary:Nonnegative matrix factorization (NMF) has become a ubiquitous tool for dataanalysis. An important variant is the sparse NMF problem which arises when weexplicitly require the learnt features to be sparse. A natural measure ofsparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms,such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, basedon intuitive attributes that such measures need to satisfy. This is in contrastto computationally cheaper alternatives such as the plain L$_1$ norm. However,present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slowand other formulations for sparse NMF have been proposed such as those based onL$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed normsparsity constraints while not sacrificing computation time. We presentexperimental evidence on real-world datasets that shows our new algorithmperforms an order of magnitude faster compared to the current state-of-the-artsolvers optimizing the mixed norm and is suitable for large-scale datasets.
arxiv-2700-237 | A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems | http://arxiv.org/pdf/1303.4434v1.pdf | author:Pinghua Gong, Changshui Zhang, Zhaosong Lu, Jianhua Huang, Jieping Ye category:cs.LG cs.NA stat.CO stat.ML published:2013-03-18 summary:Non-convex sparsity-inducing penalties have recently received considerableattentions in sparse learning. Recent theoretical investigations havedemonstrated their superiority over the convex counterparts in several sparselearning settings. However, solving the non-convex optimization problemsassociated with non-convex penalties remains a big challenge. A commonly usedapproach is the Multi-Stage (MS) convex relaxation (or DC programming), whichrelaxes the original non-convex problem to a sequence of convex problems. Thisapproach is usually not very practical for large-scale problems because itscomputational cost is a multiple of solving a single convex problem. In thispaper, we propose a General Iterative Shrinkage and Thresholding (GIST)algorithm to solve the nonconvex optimization problem for a large class ofnon-convex penalties. The GIST algorithm iteratively solves a proximal operatorproblem, which in turn has a closed-form solution for many commonly usedpenalties. At each outer iteration of the algorithm, we use a line searchinitialized by the Barzilai-Borwein (BB) rule that allows finding anappropriate step size quickly. The paper also presents a detailed convergenceanalysis of the GIST algorithm. The efficiency of the proposed algorithm isdemonstrated by extensive experiments on large-scale data sets.
arxiv-2700-238 | Generalized Thompson Sampling for Sequential Decision-Making and Causal Inference | http://arxiv.org/pdf/1303.4431v1.pdf | author:Pedro A. Ortega, Daniel A. Braun category:cs.AI stat.ML published:2013-03-18 summary:Recently, it has been shown how sampling actions from the predictivedistribution over the optimal action-sometimes called Thompson sampling-can beapplied to solve sequential adaptive control problems, when the optimal policyis known for each possible environment. The predictive distribution can then beconstructed by a Bayesian superposition of the optimal policies weighted bytheir posterior probability that is updated by Bayesian inference and causalcalculus. Here we discuss three important features of this approach. First, wediscuss in how far such Thompson sampling can be regarded as a naturalconsequence of the Bayesian modeling of policy uncertainty. Second, we show howThompson sampling can be used to study interactions between multiple adaptiveagents, thus, opening up an avenue of game-theoretic analysis. Third, we showhow Thompson sampling can be applied to infer causal relationships wheninteracting with an environment in a sequential fashion. In summary, ourresults suggest that Thompson sampling might not merely be a useful heuristic,but a principled method to address problems of adaptive sequentialdecision-making and causal inference.
arxiv-2700-239 | Hierarchical Data Representation Model - Multi-layer NMF | http://arxiv.org/pdf/1301.6316v3.pdf | author:Hyun Ah Song, Soo-Young Lee category:cs.LG published:2013-01-27 summary:In this paper, we propose a data representation model that demonstrateshierarchical feature learning using nsNMF. We extend unit algorithm intoseveral layers. Experiments with document and image data successfullydiscovered feature hierarchies. We also prove that proposed method results inmuch better classification and reconstruction performance, especially for smallnumber of features. feature hierarchies.
arxiv-2700-240 | Machine learning prediction of cancer cell sensitivity to drugs based on genomic and chemical properties | http://arxiv.org/pdf/1212.0504v3.pdf | author:Michael P. Menden, Francesco Iorio, Mathew Garnett, Ultan McDermott, Cyril Benes, Pedro J. Ballester, Julio Saez-Rodriguez category:q-bio.GN cs.CE cs.LG q-bio.CB published:2012-12-03 summary:Predicting the response of a specific cancer to a therapy is a major goal inmodern oncology that should ultimately lead to a personalised treatment.High-throughput screenings of potentially active compounds against a panel ofgenomically heterogeneous cancer cell lines have unveiled multiplerelationships between genomic alterations and drug responses. Variouscomputational approaches have been proposed to predict sensitivity based ongenomic features, while others have used the chemical properties of the drugsto ascertain their effect. In an effort to integrate these complementaryapproaches, we developed machine learning models to predict the response ofcancer cell lines to drug treatment, quantified through IC50 values, based onboth the genomic features of the cell lines and the chemical properties of theconsidered drugs. Models predicted IC50 values in a 8-fold cross-validation andan independent blind test with coefficient of determination R2 of 0.72 and 0.64respectively. Furthermore, models were able to predict with comparable accuracy(R2 of 0.61) IC50s of cell lines from a tissue not used in the training stage.Our in silico models can be used to optimise the experimental design ofdrug-cell screenings by estimating a large proportion of missing IC50 valuesrather than experimentally measure them. The implications of our results gobeyond virtual drug screening design: potentially thousands of drugs could beprobed in silico to systematically test their potential efficacy as anti-tumouragents based on their structure, thus providing a computational framework toidentify new drug repositioning opportunities as well as ultimately be usefulfor personalized medicine by linking the genomic traits of patients to drugsensitivity.
arxiv-2700-241 | Topic Discovery through Data Dependent and Random Projections | http://arxiv.org/pdf/1303.3664v2.pdf | author:Weicong Ding, Mohammad H. Rohban, Prakash Ishwar, Venkatesh Saligrama category:stat.ML cs.LG published:2013-03-15 summary:We present algorithms for topic modeling based on the geometry ofcross-document word-frequency patterns. This perspective gains significanceunder the so called separability condition. This is a condition on existence ofnovel-words that are unique to each topic. We present a suite of highlyefficient algorithms based on data-dependent and random projections ofword-frequency patterns to identify novel words and associated topics. We willalso discuss the statistical guarantees of the data-dependent projectionsmethod based on two mild assumptions on the prior density of topic documentmatrix. Our key insight here is that the maximum and minimum values ofcross-document frequency patterns projected along any direction are associatedwith novel words. While our sample complexity bounds for topic recovery aresimilar to the state-of-art, the computational complexity of our randomprojection scheme scales linearly with the number of documents and the numberof words per document. We present several experiments on synthetic andreal-world datasets to demonstrate qualitative and quantitative merits of ourscheme.
arxiv-2700-242 | Genetic algorithms for finding the weight enumerator of binary linear block codes | http://arxiv.org/pdf/1303.4227v1.pdf | author:Said Nouh, Mostafa Belkasmi category:cs.IT cs.NE math.IT published:2013-03-18 summary:In this paper we present a new method for finding the weight enumerator ofbinary linear block codes by using genetic algorithms. This method consists infinding the binary weight enumerator of the code and its dual and to createfrom the famous MacWilliams identity a linear system (S) of integer variablesfor which we add all known information obtained from the structure of the code.The knowledge of some subgroups of the automorphism group, under which the coderemains invariant, permits to give powerful restrictions on the solutions of(S) and to approximate the weight enumerator. By applying this method and byusing the stability of the Extended Quadratic Residue codes (ERQ) by theProjective Special Linear group PSL2, we find a list of all possible values ofthe weight enumerators for the two ERQ codes of lengths 192 and 200. We alsomade a good approximation of the true value for these two enumerators.
arxiv-2700-243 | Towards Swarm Calculus: Urn Models of Collective Decisions and Universal Properties of Swarm Performance | http://arxiv.org/pdf/1210.6539v3.pdf | author:Heiko Hamann category:cs.NE cs.AI published:2012-10-24 summary:Methods of general applicability are searched for in swarm intelligence withthe aim of gaining new insights about natural swarms and to develop designmethodologies for artificial swarms. An ideal solution could be a `swarmcalculus' that allows to calculate key features of swarms such as expectedswarm performance and robustness based on only a few parameters. To worktowards this ideal, one needs to find methods and models with high degrees ofgenerality. In this paper, we report two models that might be examples ofexceptional generality. First, an abstract model is presented that describesswarm performance depending on swarm density based on the dichotomy betweencooperation and interference. Typical swarm experiments are given as examplesto show how the model fits to several different results. Second, we give anabstract model of collective decision making that is inspired by urn models.The effects of positive feedback probability, that is increasing over time in adecision making system, are understood by the help of a parameter that controlsthe feedback based on the swarm's current consensus. Several applicablemethods, such as the description as Markov process, calculation of splittingprobabilities, mean first passage times, and measurements of positive feedback,are discussed and applications to artificial and natural swarms are reported.
arxiv-2700-244 | The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization | http://arxiv.org/pdf/1301.3389v2.pdf | author:Hugo Van hamme category:cs.NA cs.LG published:2013-01-15 summary:Non-negative matrix factorization (NMF) has become a popular machine learningapproach to many problems in text mining, speech and image processing,bio-informatics and seismic data analysis to name a few. In NMF, a matrix ofnon-negative data is approximated by the low-rank product of two matrices withnon-negative entries. In this paper, the approximation quality is measured bythe Kullback-Leibler divergence between the data and its low-rankreconstruction. The existence of the simple multiplicative update (MU)algorithm for computing the matrix factors has contributed to the success ofNMF. Despite the availability of algorithms showing faster convergence, MUremains popular due to its simplicity. In this paper, a diagonalized Newtonalgorithm (DNA) is proposed showing faster convergence while the implementationremains simple and suitable for high-rank problems. The DNA algorithm isapplied to various publicly available data sets, showing a substantial speed-upon modern hardware.
arxiv-2700-245 | Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning | http://arxiv.org/pdf/1302.2553v2.pdf | author:Odalric-Ambrym Maillard, Phuong Nguyen, Ronald Ortner, Daniil Ryabko category:cs.LG published:2013-02-11 summary:We consider an agent interacting with an environment in a single stream ofactions, observations, and rewards, with no reset. This process is not assumedto be a Markov Decision Process (MDP). Rather, the agent has severalrepresentations (mapping histories of past interactions to a discrete statespace) of the environment with unknown dynamics, only some of which result inan MDP. The goal is to minimize the average regret criterion against an agentwho knows an MDP representation giving the highest optimal reward, and actsoptimally in it. Recent regret bounds for this setting are of order$O(T^{2/3})$ with an additive term constant yet exponential in somecharacteristics of the optimal MDP. We propose an algorithm whose regret after$T$ time steps is $O(\sqrt{T})$, with all constants reasonably small. This isoptimal in $T$ since $O(\sqrt{T})$ is the optimal regret in the setting oflearning in a (single discrete) MDP.
arxiv-2700-246 | Margins, Shrinkage, and Boosting | http://arxiv.org/pdf/1303.4172v1.pdf | author:Matus Telgarsky category:cs.LG stat.ML published:2013-03-18 summary:This manuscript shows that AdaBoost and its immediate variants can produceapproximate maximum margin classifiers simply by scaling step size choices witha fixed small constant. In this way, when the unscaled step size is an optimalchoice, these results provide guarantees for Friedman's empirically successful"shrinkage" procedure for gradient boosting (Friedman, 2000). Guarantees arealso provided for a variety of other step sizes, affirming the intuition thatincreasingly regularized line searches provide improved margin guarantees. Theresults hold for the exponential loss and similar losses, most notably thelogistic loss.
arxiv-2700-247 | Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences | http://arxiv.org/pdf/1301.3323v4.pdf | author:Sainbayar Sukhbaatar, Takaki Makino, Kazuyuki Aihara category:cs.CV cs.LG published:2013-01-15 summary:Learning invariant representations from images is one of the hardestchallenges facing computer vision. Spatial pooling is widely used to createinvariance to spatial shifting, but it is restricted to convolutional models.In this paper, we propose a novel pooling method that can learn soft clusteringof features from image sequences. It is trained to improve the temporalcoherence of features, while keeping the information loss at minimum. Ourmethod does not use spatial information, so it can be used withnon-convolutional models too. Experiments on images extracted from naturalvideos showed that our method can cluster similar features together. Whentrained by convolutional features, auto-pooling outperformed traditionalspatial pooling on an image classification task, even though it does not usethe spatial topology of features.
arxiv-2700-248 | Markov Chain Monte Carlo for Arrangement of Hyperplanes in Locality-Sensitive Hashing | http://arxiv.org/pdf/1303.4169v1.pdf | author:Yui Noma, Makiko Konoshima category:cs.LG published:2013-03-18 summary:Since Hamming distances can be calculated by bitwise computations, they canbe calculated with less computational load than L2 distances. Similaritysearches can therefore be performed faster in Hamming distance space. Theelements of Hamming distance space are bit strings. On the other hand, thearrangement of hyperplanes induce the transformation from the feature vectorsinto feature bit strings. This transformation method is a type oflocality-sensitive hashing that has been attracting attention as a way ofperforming approximate similarity searches at high speed. Supervised learningof hyperplane arrangements allows us to obtain a method that transforms theminto feature bit strings reflecting the information of labels applied tohigher-dimensional feature vectors. In this p aper, we propose a supervisedlearning method for hyperplane arrangements in feature space that uses a Markovchain Monte Carlo (MCMC) method. We consider the probability density functionsused during learning, and evaluate their performance. We also consider thesampling method for learning data pairs needed in learning, and we evaluate itsperformance. We confirm that the accuracy of this learning method when using asuitable probability density function and sampling method is greater than theaccuracy of existing learning methods.
arxiv-2700-249 | Neurally Implementable Semantic Networks | http://arxiv.org/pdf/1303.4164v1.pdf | author:Garrett N. Evans, John C. Collins category:q-bio.NC cs.NE I.2.4; I.2.6 published:2013-03-18 summary:We propose general principles for semantic networks allowing them to beimplemented as dynamical neural networks. Major features of our scheme include:(a) the interpretation that each node in a network stands for a boundintegration of the meanings of all nodes and external events the node linkswith; (b) the systematic use of nodes that stand for categories or types, withseparate nodes for instances of these types; (c) an implementation ofrelationships that does not use intrinsically typed links between nodes.
arxiv-2700-250 | Improved Foreground Detection via Block-based Classifier Cascade with Probabilistic Decision Integration | http://arxiv.org/pdf/1303.4160v1.pdf | author:Vikas Reddy, Conrad Sanderson, Brian C. Lovell category:cs.CV published:2013-03-18 summary:Background subtraction is a fundamental low-level processing task in numerouscomputer vision applications. The vast majority of algorithms process images ona pixel-by-pixel basis, where an independent decision is made for each pixel. Ageneral limitation of such processing is that rich contextual information isnot taken into account. We propose a block-based method capable of dealing withnoise, illumination variations and dynamic backgrounds, while still obtainingsmooth contours of foreground objects. Specifically, image sequences areanalysed on an overlapping block-by-block basis. A low-dimensional texturedescriptor obtained from each block is passed through an adaptive classifiercascade, where each stage handles a distinct problem. A probabilisticforeground mask generation approach then exploits block overlaps to integrateinterim block-level decisions into final pixel-level foreground segmentation.Unlike many pixel-based methods, ad-hoc post-processing of foreground masks isnot required. Experiments on the difficult Wallflower and I2R datasets showthat the proposed approach obtains on average better results (bothqualitatively and quantitatively) than several prominent methods. Wefurthermore propose the use of tracking performance as an unbiased approach forassessing the practical usefulness of foreground segmentation methods, and showthat the proposed approach leads to considerable improvements in trackingaccuracy on the CAVIAR dataset.
arxiv-2700-251 | Modeling a Sensor to Improve its Efficacy | http://arxiv.org/pdf/1303.4385v1.pdf | author:N. K. Malakar, D. Gladkov, K. H. Knuth category:astro-ph.IM stat.ML published:2013-03-18 summary:Robots rely on sensors to provide them with information about theirsurroundings. However, high-quality sensors can be extremely expensive andcost-prohibitive. Thus many robotic systems must make due with lower-qualitysensors. Here we demonstrate via a case study how modeling a sensor can improveits efficacy when employed within a Bayesian inferential framework. As a testbed we employ a robotic arm that is designed to autonomously take its ownmeasurements using an inexpensive LEGO light sensor to estimate the positionand radius of a white circle on a black field. The light sensor integrates thelight arriving from a spatially distributed region within its field of viewweighted by its Spatial Sensitivity Function (SSF). We demonstrate that byincorporating an accurate model of the light sensor SSF into the likelihoodfunction of a Bayesian inference engine, an autonomous system can make improvedinferences about its surroundings. The method presented here is data-based,fairly general, and made with plug-and play in mind so that it could beimplemented in similar problems.
arxiv-2700-252 | Generalization Bounds for Metric and Similarity Learning | http://arxiv.org/pdf/1207.5437v2.pdf | author:Qiong Cao, Zheng-Chu Guo, Yiming Ying category:cs.LG stat.ML published:2012-07-23 summary:Recently, metric learning and similarity learning have attracted a largeamount of interest. Many models and optimisation algorithms have been proposed.However, there is relatively little work on the generalization analysis of suchmethods. In this paper, we derive novel generalization bounds of metric andsimilarity learning. In particular, we first show that the generalizationanalysis reduces to the estimation of the Rademacher average over"sums-of-i.i.d." sample-blocks related to the specific matrix norm. Then, wederive generalization bounds for metric/similarity learning with differentmatrix-norm regularisers by estimating their specific Rademacher complexities.Our analysis indicates that sparse metric/similarity learning with $L^1$-normregularisation could lead to significantly better bounds than those withFrobenius-norm regularisation. Our novel generalization analysis develops andrefines the techniques of U-statistics and Rademacher complexity analysis.
arxiv-2700-253 | Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines | http://arxiv.org/pdf/1301.3545v2.pdf | author:Guillaume Desjardins, Razvan Pascanu, Aaron Courville, Yoshua Bengio category:cs.LG cs.NE stat.ML published:2013-01-16 summary:This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm fortraining Boltzmann Machines. Similar in spirit to the Hessian-Free method ofMartens [8], our algorithm belongs to the family of truncated Newton methodsand exploits an efficient matrix-vector product to avoid explicitely storingthe natural gradient metric $L$. This metric is shown to be the expected secondderivative of the log-partition function (under the model distribution), orequivalently, the variance of the vector of partial derivatives of the energyfunction. We evaluate our method on the task of joint-training a 3-layer DeepBoltzmann Machine and show that MFNG does indeed have faster per-epochconvergence compared to Stochastic Maximum Likelihood with centering, thoughwall-clock performance is currently not competitive.
arxiv-2700-254 | $l_{2,p}$ Matrix Norm and Its Application in Feature Selection | http://arxiv.org/pdf/1303.3987v1.pdf | author:Liping Wang, Songcan Chen category:cs.LG cs.CV stat.ML published:2013-03-16 summary:Recently, $l_{2,1}$ matrix norm has been widely applied to many areas such ascomputer vision, pattern recognition, biological study and etc. As an extensionof $l_1$ vector norm, the mixed $l_{2,1}$ matrix norm is often used to findjointly sparse solutions. Moreover, an efficient iterative algorithm has beendesigned to solve $l_{2,1}$-norm involved minimizations. Actually,computational studies have showed that $l_p$-regularization ($0<p<1$) issparser than $l_1$-regularization, but the extension to matrix norm has beenseldom considered. This paper presents a definition of mixed $l_{2,p}$ $(p\in(0, 1])$ matrix pseudo norm which is thought as both generalizations of $l_p$vector norm to matrix and $l_{2,1}$-norm to nonconvex cases $(0<p<1)$.Fortunately, an efficient unified algorithm is proposed to solve the induced$l_{2,p}$-norm $(p\in (0, 1])$ optimization problems. The convergence can alsobe uniformly demonstrated for all $p\in (0, 1]$. Typical $p\in (0,1]$ areapplied to select features in computational biology and the experimentalresults show that some choices of $0<p<1$ do improve the sparse pattern ofusing $p=1$.
arxiv-2700-255 | An Adaptive Methodology for Ubiquitous ASR System | http://arxiv.org/pdf/1303.3948v1.pdf | author:Urmila Shrawankar, Vilas Thakare category:cs.CL cs.HC cs.SD published:2013-03-16 summary:Achieving and maintaining the performance of ubiquitous (Automatic SpeechRecognition) ASR system is a real challenge. The main objective of this work isto develop a method that will improve and show the consistency in performanceof ubiquitous ASR system for real world noisy environment. An adaptivemethodology has been developed to achieve an objective with the help ofimplementing followings, -Cleaning speech signal as much as possible whilepreserving originality / intangibility using various modified filters andenhancement techniques. -Extracting features from speech signals using varioussizes of parameter. -Train the system for ubiquitous environment usingmulti-environmental adaptation training methods. -Optimize the word recognitionrate with appropriate variable size of parameters using fuzzy technique. Theconsistency in performance is tested using standard noise databases as well asin real world environment. A good improvement is noticed. This work will behelpful to give discriminative training of ubiquitous ASR system for betterHuman Computer Interaction (HCI) using Speech User Interface (SUI).
arxiv-2700-256 | Learning New Facts From Knowledge Bases With Neural Tensor Networks and Semantic Word Vectors | http://arxiv.org/pdf/1301.3618v2.pdf | author:Danqi Chen, Richard Socher, Christopher D. Manning, Andrew Y. Ng category:cs.CL cs.LG published:2013-01-16 summary:Knowledge bases provide applications with the benefit of easily accessible,systematic relational knowledge but often suffer in practice from theirincompleteness and lack of knowledge of new entities and relations. Much workhas focused on building or extending them by finding patterns in largeunannotated text corpora. In contrast, here we mainly aim to complete aknowledge base by predicting additional true relationships between entities,based on generalizations that can be discerned in the given knowledgebase. Weintroduce a neural tensor network (NTN) model which predicts new relationshipentries that can be added to the database. This model can be improved byinitializing entity representations with word vectors learned in anunsupervised fashion from text, and when doing this, existing relations caneven be queried for entities that were not present in the database. Our modelgeneralizes and outperforms existing models for this problem, and can classifyunseen relationships in WordNet with an accuracy of 75.8%.
arxiv-2700-257 | Herded Gibbs Sampling | http://arxiv.org/pdf/1301.4168v2.pdf | author:Luke Bornn, Yutian Chen, Nando de Freitas, Mareija Eskelin, Jing Fang, Max Welling category:cs.LG stat.CO stat.ML published:2013-01-17 summary:The Gibbs sampler is one of the most popular algorithms for inference instatistical models. In this paper, we introduce a herding variant of thisalgorithm, called herded Gibbs, that is entirely deterministic. We prove thatherded Gibbs has an $O(1/T)$ convergence rate for models with independentvariables and for fully connected probabilistic graphical models. Herded Gibbsis shown to outperform Gibbs in the tasks of image denoising with MRFs andnamed entity recognition with CRFs. However, the convergence for herded Gibbsfor sparsely connected probabilistic graphical models is still an open problem.
arxiv-2700-258 | Deep Predictive Coding Networks | http://arxiv.org/pdf/1301.3541v3.pdf | author:Rakesh Chalasani, Jose C. Principe category:cs.LG cs.CV stat.ML published:2013-01-16 summary:The quality of data representation in deep learning methods is directlyrelated to the prior model imposed on the representations; however, generallyused fixed priors are not capable of adjusting to the context in the data. Toaddress this issue, we propose deep predictive coding networks, a hierarchicalgenerative model that empirically alters priors on the latent representationsin a dynamic and context-sensitive manner. This model captures the temporaldependencies in time-varying signals and uses top-down information to modulatethe representation in lower layers. The centerpiece of our model is a novelprocedure to infer sparse states of a dynamic model which is used for featureextraction. We also extend this feature extraction block to introduce a poolingfunction that captures locally invariant representations. When applied on anatural video data, we show that our method is able to learn high-level visualfeatures. We also demonstrate the role of the top-down connections by showingthe robustness of the proposed model to structured noise.
arxiv-2700-259 | Variational Semi-blind Sparse Deconvolution with Orthogonal Kernel Bases and its Application to MRFM | http://arxiv.org/pdf/1303.3866v1.pdf | author:Se Un Park, Nicolas Dobigeon, Alfred O. Hero category:stat.ML published:2013-03-15 summary:We present a variational Bayesian method of joint image reconstruction andpoint spread function (PSF) estimation when the PSF of the imaging device isonly partially known. To solve this semi-blind deconvolution problem, priordistributions are specified for the PSF and the 3D image. Joint imagereconstruction and PSF estimation is then performed within a Bayesianframework, using a variational algorithm to estimate the posteriordistribution. The image prior distribution imposes an explicit atomic measurethat corresponds to image sparsity. Importantly, the proposed Bayesiandeconvolution algorithm does not require hand tuning. Simulation resultsclearly demonstrate that the semi-blind deconvolution algorithm comparesfavorably with previous Markov chain Monte Carlo (MCMC) version of myopicsparse reconstruction. It significantly outperforms mismatched non-blindalgorithms that rely on the assumption of the perfect knowledge of the PSF. Thealgorithm is illustrated on real data from magnetic resonance force microscopy(MRFM).
arxiv-2700-260 | Online Similarity Prediction of Networked Data from Known and Unknown Graphs | http://arxiv.org/pdf/1302.7263v3.pdf | author:Claudio Gentile, Mark Herbster, Stephen Pasteris category:cs.LG published:2013-02-28 summary:We consider online similarity prediction problems over networked data. Webegin by relating this task to the more standard class prediction problem,showing that, given an arbitrary algorithm for class prediction, we canconstruct an algorithm for similarity prediction with "nearly" the same mistakebound, and vice versa. After noticing that this general construction iscomputationally infeasible, we target our study to {\em feasible} similarityprediction algorithms on networked data. We initially assume that the networkstructure is {\em known} to the learner. Here we observe that Matrix Winnow\cite{w07} has a near-optimal mistake guarantee, at the price of cubicprediction time per round. This motivates our effort for an efficientimplementation of a Perceptron algorithm with a weaker mistake guarantee butwith only poly-logarithmic prediction time. Our focus then turns to thechallenging case of networks whose structure is initially {\em unknown} to thelearner. In this novel setting, where the network structure is onlyincrementally revealed, we obtain a mistake-bounded algorithm with a quadraticprediction time per round.
arxiv-2700-261 | A Last-Step Regression Algorithm for Non-Stationary Online Learning | http://arxiv.org/pdf/1303.3754v1.pdf | author:Edward Moroshko, Koby Crammer category:cs.LG published:2013-03-15 summary:The goal of a learner in standard online learning is to maintain an averageloss close to the loss of the best-performing single function in some class. Inmany real-world problems, such as rating or ranking items, there is no singlebest target function during the runtime of the algorithm, instead the best(local) target function is drifting over time. We develop a novel last-stepminmax optimal algorithm in context of a drift. We analyze the algorithm in theworst-case regret framework and show that it maintains an average loss close tothat of the best slowly changing sequence of linear functions, as long as thetotal of drift is sublinear. In some situations, our bound improves overexisting bounds, and additionally the algorithm suffers logarithmic regret whenthere is no drift. We also build on the H_infinity filter and its bound, anddevelop and analyze a second algorithm for drifting setting. Syntheticsimulations demonstrate the advantages of our algorithms in a worst-caseconstant drift setting.
arxiv-2700-262 | Sparse estimation via nonconcave penalized likelihood in a factor analysis model | http://arxiv.org/pdf/1205.5868v3.pdf | author:Kei Hirose, Michio Yamamoto category:stat.ME stat.CO stat.ML published:2012-05-26 summary:We consider the problem of sparse estimation in a factor analysis model. Atraditional estimation procedure in use is the following two-step approach: themodel is estimated by maximum likelihood method and then a rotation techniqueis utilized to find sparse factor loadings. However, the maximum likelihoodestimates cannot be obtained when the number of variables is much larger thanthe number of observations. Furthermore, even if the maximum likelihoodestimates are available, the rotation technique does not often produce asufficiently sparse solution. In order to handle these problems, this paperintroduces a penalized likelihood procedure that imposes a nonconvex penalty onthe factor loadings. We show that the penalized likelihood procedure can beviewed as a generalization of the traditional two-step approach, and theproposed methodology can produce sparser solutions than the rotation technique.A new algorithm via the EM algorithm along with coordinate descent isintroduced to compute the entire solution path, which permits the applicationto a wide variety of convex and nonconvex penalties. Monte Carlo simulationsare conducted to investigate the performance of our modeling strategy. A realdata example is also given to illustrate our procedure.
arxiv-2700-263 | Subspace Clustering via Thresholding and Spectral Clustering | http://arxiv.org/pdf/1303.3716v1.pdf | author:Reinhard Heckel, Helmut Bölcskei category:cs.IT cs.LG math.IT math.ST stat.ML stat.TH published:2013-03-15 summary:We consider the problem of clustering a set of high-dimensional data pointsinto sets of low-dimensional linear subspaces. The number of subspaces, theirdimensions, and their orientations are unknown. We propose a simple andlow-complexity clustering algorithm based on thresholding the correlationsbetween the data points followed by spectral clustering. A probabilisticperformance analysis shows that this algorithm succeeds even when the subspacesintersect, and when the dimensions of the subspaces scale (up to a log-factor)linearly in the ambient dimension. Moreover, we prove that the algorithm alsosucceeds for data points that are subject to erasures with the number oferasures scaling (up to a log-factor) linearly in the ambient dimension.Finally, we propose a simple scheme that provably detects outliers.
arxiv-2700-264 | Convex Hull-Based Multi-objective Genetic Programming for Maximizing ROC Performance | http://arxiv.org/pdf/1303.3145v2.pdf | author:Pu Wang, Michael Emmerich, Rui Li, Ke Tang, Thomas Baeck, Xin Yao category:cs.NE published:2013-03-13 summary:ROC is usually used to analyze the performance of classifiers in data mining.ROC convex hull (ROCCH) is the least convex major-ant (LCM) of the empiricalROC curve, and covers potential optima for the given set of classifiers.Generally, ROC performance maximization could be considered to maximize theROCCH, which also means to maximize the true positive rate (tpr) and minimizethe false positive rate (fpr) for each classifier in the ROC space. However,tpr and fpr are conflicting with each other in the ROCCH optimization process.Though ROCCH maximization problem seems like a multi-objective optimizationproblem (MOP), the special characters make it different from traditional MOP.In this work, we will discuss the difference between them and propose convexhull-based multi-objective genetic programming (CH-MOGP) to solve ROCCHmaximization problems. Convex hull-based sort is an indicator based selectionscheme that aims to maximize the area under convex hull, which serves as aunary indicator for the performance of a set of points. A selection procedureis described that can be efficiently implemented and follows similar designprinciples than classical hyper-volume based optimization algorithms. It ishypothesized that by using a tailored indicator-based selection scheme CH-MOGPgets more efficient for ROC convex hull approximation than algorithms whichcompute all Pareto optimal points. To test our hypothesis we compare the newCH-MOGP to MOGP with classical selection schemes, including NSGA-II, MOEA/D)and SMS-EMOA. Meanwhile, CH-MOGP is also compared with traditional machinelearning algorithms such as C4.5, Naive Bayes and Prie. Experimental resultsbased on 22 well-known UCI data sets show that CH-MOGP outperformssignificantly traditional EMOAs.
arxiv-2700-265 | Statistical Regression to Predict Total Cumulative CPU Usage of MapReduce Jobs | http://arxiv.org/pdf/1303.3632v1.pdf | author:Nikzad Babaii Rizvandi, Javid Taheri, Reza Moraveji, Albert Y. Zomaya category:cs.DC cs.LG cs.PF published:2013-03-14 summary:Recently, businesses have started using MapReduce as a popular computationframework for processing large amount of data, such as spam detection, anddifferent data mining tasks, in both public and private clouds. Two of thechallenging questions in such environments are (1) choosing suitable values forMapReduce configuration parameters e.g., number of mappers, number of reducers,and DFS block size, and (2) predicting the amount of resources that a usershould lease from the service provider. Currently, the tasks of both choosingconfiguration parameters and estimating required resources are solely the usersresponsibilities. In this paper, we present an approach to provision the totalCPU usage in clock cycles of jobs in MapReduce environment. For a MapReducejob, a profile of total CPU usage in clock cycles is built from the job pastexecutions with different values of two configuration parameters e.g., numberof mappers, and number of reducers. Then, a polynomial regression is used tomodel the relation between these configuration parameters and total CPU usagein clock cycles of the job. We also briefly study the influence of input datascaling on measured total CPU usage in clock cycles. This derived model alongwith the scaling result can then be used to provision the total CPU usage inclock cycles of the same jobs with different input data size. We validate theaccuracy of our models using three realistic applications (WordCount, EximMainLog parsing, and TeraSort). Results show that the predicted total CPU usagein clock cycles of generated resource provisioning options are less than 8% ofthe measured total CPU usage in clock cycles in our 20-node virtual Hadoopcluster.
arxiv-2700-266 | A survey on sensing methods and feature extraction algorithms for SLAM problem | http://arxiv.org/pdf/1303.3605v1.pdf | author:Adheen Ajay, D. Venkataraman category:cs.RO cs.CV cs.LG published:2013-03-14 summary:This paper is a survey work for a bigger project for designing a Visual SLAMrobot to generate 3D dense map of an unknown unstructured environment. A lot offactors have to be considered while designing a SLAM robot. Sensing method ofthe SLAM robot should be determined by considering the kind of environment tobe modeled. Similarly the type of environment determines the suitable featureextraction method. This paper goes through the sensing methods used in somerecently published papers. The main objective of this survey is to conduct acomparative study among the current sensing methods and feature extractionalgorithms and to extract out the best for our work.
arxiv-2700-267 | Big Neural Networks Waste Capacity | http://arxiv.org/pdf/1301.3583v4.pdf | author:Yann N. Dauphin, Yoshua Bengio category:cs.LG cs.CV published:2013-01-16 summary:This article exposes the failure of some big neural networks to leverageadded capacity to reduce underfitting. Past research suggest diminishingreturns when increasing the size of neural networks. Our experiments onImageNet LSVRC-2010 show that this may be due to the fact there are highlydiminishing returns for capacity in terms of training error, leading tounderfitting. This suggests that the optimization method - first order gradientdescent - fails at this regime. Directly attacking this problem, either throughthe optimization method or the choices of parametrization, may allow to improvethe generalization error on large datasets, for which a large capacity isrequired.
arxiv-2700-268 | Expressing Ethnicity through Behaviors of a Robot Character | http://arxiv.org/pdf/1303.3592v1.pdf | author:Maxim Makatchev, Reid Simmons, Majd Sakr, Micheline Ziadee category:cs.CL cs.CY cs.RO published:2013-03-14 summary:Achieving homophily, or association based on similarity, between a human userand a robot holds a promise of improved perception and task performance.However, no previous studies that address homophily via ethnic similarity withrobots exist. In this paper, we discuss the difficulties of evoking ethnic cuesin a robot, as opposed to a virtual agent, and an approach to overcome thosedifficulties based on using ethnically salient behaviors. We outline ourmethodology for selecting and evaluating such behaviors, and culminate with astudy that evaluates our hypotheses of the possibility of ethnic attribution ofa robot character through verbal and nonverbal behaviors and of achieving thehomophily effect.
arxiv-2700-269 | Indoor Semantic Segmentation using depth information | http://arxiv.org/pdf/1301.3572v2.pdf | author:Camille Couprie, Clément Farabet, Laurent Najman, Yann LeCun category:cs.CV published:2013-01-16 summary:This work addresses multi-class segmentation of indoor scenes with RGB-Dinputs. While this area of research has gained much attention recently, mostworks still rely on hand-crafted features. In contrast, we apply a multiscaleconvolutional network to learn features directly from the images and the depthinformation. We obtain state-of-the-art on the NYU-v2 depth dataset with anaccuracy of 64.5%. We illustrate the labeling of indoor scenes in videossequences that could be processed in real-time using appropriate hardware suchas an FPGA.
arxiv-2700-270 | Another Look at Quantum Neural Computing | http://arxiv.org/pdf/0908.3148v2.pdf | author:Subhash Kak category:cs.NE cs.AI published:2009-08-21 summary:The term quantum neural computing indicates a unity in the functioning of thebrain. It assumes that the neural structures perform classical processing andthat the virtual particles associated with the dynamical states of thestructures define the underlying quantum state. We revisit the concept and alsosummarize new arguments related to the learning modes of the brain in responseto sensory input that may be aggregated in three types: associative,reorganizational, and quantum. The associative and reorganizational types arequite apparent based on experimental findings; it is much harder to establishthat the brain as an entity exhibits quantum properties. We argue that thereorganizational behavior of the brain may be viewed as inner adjustmentcorresponding to its quantum behavior at the system level. Not only neuralstructures but their higher abstractions also may be seen as whole entities. Weconsider the dualities associated with the behavior of the brain and how thesedualities are bridged.
arxiv-2700-271 | Hybrid Evolutionary Computation for Continuous Optimization | http://arxiv.org/pdf/1303.3469v1.pdf | author:Hassan A. Bashir, Richard S. Neville category:cs.NE published:2013-03-14 summary:Hybrid optimization algorithms have gained popularity as it has becomeapparent there cannot be a universal optimization strategy which is globallymore beneficial than any other. Despite their popularity, hybridizationframeworks require more detailed categorization regarding: the nature of theproblem domain, the constituent algorithms, the coupling schema and theintended area of application. This report proposes a hybrid algorithm forsolving small to large-scale continuous global optimization problems. Itcomprises evolutionary computation (EC) algorithms and a sequential quadraticprogramming (SQP) algorithm; combined in a collaborative portfolio. The SQP isa gradient based local search method. To optimize the individual contributionsof the EC and SQP algorithms for the overall success of the proposed hybridsystem, improvements were made in key features of these algorithms. The reportproposes enhancements in: i) the evolutionary algorithm, ii) a new convergencedetection mechanism was proposed; and iii) in the methods for evaluating thesearch directions and step sizes for the SQP local search algorithm. Theproposed hybrid design aim was to ensure that the two algorithms complementeach other by exploring and exploiting the problem search space. Preliminaryresults justify that an adept hybridization of evolutionary algorithms with asuitable local search method, could yield a robust and efficient means ofsolving wide range of global optimization problems. Finally, a discussion ofthe outcomes of the initial investigation and a review of the associatedchallenges and inherent limitations of the proposed method is presented tocomplete the investigation. The report highlights extensive research,particularly, some potential case studies and application areas.
arxiv-2700-272 | Clustering Learning for Robotic Vision | http://arxiv.org/pdf/1301.2820v3.pdf | author:Eugenio Culurciello, Jordan Bates, Aysegul Dundar, Jose Carrasco, Clement Farabet category:cs.CV published:2013-01-13 summary:We present the clustering learning technique applied to multi-layerfeedforward deep neural networks. We show that this unsupervised learningtechnique can compute network filters with only a few minutes and a muchreduced set of parameters. The goal of this paper is to promote the techniquefor general-purpose robotic vision systems. We report its use in static imagedatasets and object tracking datasets. We show that networks trained withclustering learning can outperform large networks trained for many hours oncomplex datasets.
arxiv-2700-273 | EigenGP: Sparse Gaussian process models with data-dependent eigenfunctions | http://arxiv.org/pdf/1204.3972v3.pdf | author:Yuan Qi, Bo Dai, Yao Zhu category:cs.LG stat.CO stat.ML published:2012-04-18 summary:Gaussian processes (GPs) provide a nonparametric representation of functions.However, classical GP inference suffers from high computational cost and it isdifficult to design nonstationary GP priors in practice. In this paper, wepropose a sparse Gaussian process model, EigenGP, based on the Karhunen-Loeve(KL) expansion of a GP prior. We use the Nystrom approximation to obtain datadependent eigenfunctions and select these eigenfunctions by evidencemaximization. This selection reduces the number of eigenfunctions in our modeland provides a nonstationary covariance function. To handle nonlinearlikelihoods, we develop an efficient expectation propagation (EP) inferencealgorithm, and couple it with expectation maximization for eigenfunctionselection. Because the eigenfunctions of a Gaussian kernel are associated withclusters of samples - including both the labeled and unlabeled - selectingrelevant eigenfunctions enables EigenGP to conduct semi-supervised learning.Our experimental results demonstrate improved predictive performance of EigenGPover alternative state-of-the-art sparse GP and semisupervised learning methodsfor regression, classification, and semisupervised classification.
arxiv-2700-274 | Types and forgetfulness in categorical linguistics and quantum mechanics | http://arxiv.org/pdf/1303.3170v1.pdf | author:Peter Hines category:cs.CL math.CT quant-ph published:2013-03-13 summary:The role of types in categorical models of meaning is investigated. A generalscheme for how typed models of meaning may be used to compare sentences,regardless of their grammatical structure is described, and a toy example isused as an illustration. Taking as a starting point the question of whether theevaluation of such a type system 'loses information', we consider theparametrized typing associated with connectives from this viewpoint. The answer to this question implies that, within full categorical models ofmeaning, the objects associated with types must exhibit a simple but subtlecategorical property known as self-similarity. We investigate the categorytheory behind this, with explicit reference to typed systems, and theirmonoidal closed structure. We then demonstrate close connections between suchself-similar structures and dagger Frobenius algebras. In particular, wedemonstrate that the categorical structures implied by the polymorphicallytyped connectives give rise to a (lax unitless) form of the special forms ofFrobenius algebras known as classical structures, used heavily in abstractcategorical approaches to quantum mechanics.
arxiv-2700-275 | Material quality assessment of silk nanofibers based on swarm intelligence | http://arxiv.org/pdf/1303.3152v1.pdf | author:Bruno Brandoli Machado, Wesley Nunes Gonçalves, Odemir Martinez Bruno category:cs.CV published:2013-03-13 summary:In this paper, we propose a novel approach for texture analysis based onartificial crawler model. Our method assumes that each agent can interact withthe environment and each other. The evolution process converges to anequilibrium state according to the set of rules. For each textured image, thefeature vector is composed by signatures of the live agents curve at each time.Experimental results revealed that combining the minimum and maximum signaturesinto one increase the classification rate. In addition, we pioneer the use ofautonomous agents for characterizing silk fibroin scaffolds. The resultsstrongly suggest that our approach can be successfully employed for textureanalysis.
arxiv-2700-276 | An Entropy-based Learning Algorithm of Bayesian Conditional Trees | http://arxiv.org/pdf/1303.5403v1.pdf | author:Dan Geiger category:cs.LG cs.AI cs.CV published:2013-03-13 summary:This article offers a modification of Chow and Liu's learning algorithm inthe context of handwritten digit recognition. The modified algorithm directsthe user to group digits into several classes consisting of digits that arehard to distinguish and then constructing an optimal conditional treerepresentation for each class of digits instead of for each single digit asdone by Chow and Liu (1968). Advantages and extensions of the new method arediscussed. Related works of Wong and Wang (1977) and Wong and Poon (1989) whichoffer a different entropy-based learning algorithm are shown to rest oninappropriate assumptions.
arxiv-2700-277 | Egocentric vision IT technologies for Alzheimer disease assessment and studies | http://arxiv.org/pdf/1303.3134v1.pdf | author:Hugo Boujut, Vincent Buso, Guillaume Bourmaud, Jenny Benois-Pineau, Rémi Mégret, Jean-Philippe Domenger, Yann Gaëstel, Jean-François Dartigues category:cs.HC cs.CV published:2013-03-13 summary:Egocentric vision technology consists in capturing the actions of personsfrom their own visual point of view using wearable camera sensors. We applythis new paradigm to instrumental activities monitoring with the objective ofproviding new tools for the clinical evaluation of the impact of the disease onpersons with dementia. In this paper, we introduce the current state of thedevelopment of this technology and focus on two technology modules: automaticlocation estimation and visual saliency estimation for content interpretation.
arxiv-2700-278 | On Improving Energy Efficiency within Green Femtocell Networks: A Hierarchical Reinforcement Learning Approach | http://arxiv.org/pdf/1303.4638v1.pdf | author:Xianfu Chen, Honggang Zhang, Tao Chen, Mika Lasanen, Jacques Palicot category:cs.LG cs.GT published:2013-03-13 summary:One of the efficient solutions of improving coverage and increasing capacityin cellular networks is the deployment of femtocells. As the cellular networksare becoming more complex, energy consumption of whole network infrastructureis becoming important in terms of both operational costs and environmentalimpacts. This paper investigates energy efficiency of two-tier femtocellnetworks through combining game theory and stochastic learning. With theStackelberg game formulation, a hierarchical reinforcement learning frameworkis applied for studying the joint expected utility maximization of macrocellsand femtocells subject to the minimum signal-to-interference-plus-noise-ratiorequirements. In the learning procedure, the macrocells act as leaders and thefemtocells are followers. At each time step, the leaders commit to dynamicstrategies based on the best responses of the followers, while the followerscompete against each other with no further information but the leaders'transmission parameters. In this paper, we propose two reinforcement learningbased intelligent algorithms to schedule each cell's stochastic power levels.Numerical experiments are presented to validate the investigations. The resultsshow that the two learning algorithms substantially improve the energyefficiency of the femtocell networks.
arxiv-2700-279 | Statistical Texture Features based Handwritten and Printed Text Classification in South Indian Documents | http://arxiv.org/pdf/1303.3087v1.pdf | author:Mallikarjun Hangarge, K. C. Santosh, Srikanth Doddamani, Rajmohan Pardeshi category:cs.CV published:2013-03-13 summary:In this paper, we use statistical texture features for handwritten andprinted text classification. We primarily aim for word level classification insouth Indian scripts. Words are first extracted from the scanned document. Foreach extracted word, statistical texture features are computed such as mean,standard deviation, smoothness, moment, uniformity, entropy and local rangeincluding local entropy. These feature vectors are then used to classify wordsvia k-NN classifier. We have validated the approach over several differentdatasets. Scripts like Kannada, Telugu, Malayalam and Hindi i.e., Devanagariare primarily employed where an average classification rate of 99.26% isachieved. In addition, to provide an extensibility of the approach, we addressRoman script by using publicly available dataset and interesting results arereported.
arxiv-2700-280 | Iterative MapReduce for Large Scale Machine Learning | http://arxiv.org/pdf/1303.3517v1.pdf | author:Joshua Rosen, Neoklis Polyzotis, Vinayak Borkar, Yingyi Bu, Michael J. Carey, Markus Weimer, Tyson Condie, Raghu Ramakrishnan category:cs.DC cs.DB cs.LG published:2013-03-13 summary:Large datasets ("Big Data") are becoming ubiquitous because the potentialvalue in deriving insights from data, across a wide range of business andscientific applications, is increasingly recognized. In particular, machinelearning - one of the foundational disciplines for data analysis, summarizationand inference - on Big Data has become routine at most organizations thatoperate large clouds, usually based on systems such as Hadoop that support theMapReduce programming paradigm. It is now widely recognized that whileMapReduce is highly scalable, it suffers from a critical weakness for machinelearning: it does not support iteration. Consequently, one has to programaround this limitation, leading to fragile, inefficient code. Further, relianceon the programmer is inherently flawed in a multi-tenanted cloud environment,since the programmer does not have visibility into the state of the system whenhis or her program executes. Prior work has sought to address this problem byeither developing specialized systems aimed at stylized applications, or byaugmenting MapReduce with ad hoc support for saving state across iterations(driven by an external loop). In this paper, we advocate support for looping asa first-class construct, and propose an extension of the MapReduce programmingparadigm called {\em Iterative MapReduce}. We then develop an optimizer for aclass of Iterative MapReduce programs that cover most machine learningtechniques, provide theoretical justifications for the key optimization steps,and empirically demonstrate that system-optimized programs for significantmachine learning tasks are competitive with state-of-the-art specializedsolutions.
arxiv-2700-281 | Computing Motion with 3D Memristive Grid | http://arxiv.org/pdf/1303.3067v1.pdf | author:Chuan Kai Kenneth. Lim, T. Prodromakis category:cs.CV q-bio.NC published:2013-03-13 summary:Computing the relative motion of objects is an important navigation task thatwe routinely perform by relying on inherently unreliable biological cells inthe retina. The non-linear and adaptive response of memristive devices makethem excellent building blocks for realizing complex synaptic-likearchitectures that are common in the human retina. Here, we introduce a novelmemristive thresholding scheme that facilitates the detection of moving edges.In addition, a double-layered 3-D memristive network is employed for modelingthe motion computations that take place in both the Outer Plexiform Layer (OPL)and Inner Plexiform Layer (IPL) that enables the detection of on-center andoff-center transient responses. Applying the transient detection results, it isshown that it is possible to generate an estimation of the speed and directiona moving object.
arxiv-2700-282 | Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions | http://arxiv.org/pdf/1303.3055v1.pdf | author:Yasin Abbasi-Yadkori, Peter L. Bartlett, Csaba Szepesvari category:cs.LG stat.ML published:2013-03-12 summary:We study the problem of learning Markov decision processes with finite stateand action spaces when the transition probability distributions and lossfunctions are chosen adversarially and are allowed to change with time. Weintroduce an algorithm whose regret with respect to any policy in a comparisonclass grows as the square root of the number of rounds of the game, providedthe transition probabilities satisfy a uniform mixing condition. Our approachis efficient as long as the comparison class is polynomial and we can computeexpectations over sample paths for each policy. Designing an efficientalgorithm with small regret for the general case remains an open problem.
arxiv-2700-283 | Variational Inference in Nonconjugate Models | http://arxiv.org/pdf/1209.4360v4.pdf | author:Chong Wang, David M. Blei category:stat.ML published:2012-09-19 summary:Mean-field variational methods are widely used for approximate posteriorinference in many probabilistic models. In a typical application, mean-fieldmethods approximately compute the posterior with a coordinate-ascentoptimization algorithm. When the model is conditionally conjugate, thecoordinate updates are easily derived and in closed form. However, many modelsof interest---like the correlated topic model and Bayesian logisticregression---are nonconjuate. In these models, mean-field methods cannot bedirectly applied and practitioners have had to develop variational algorithmson a case-by-case basis. In this paper, we develop two generic methods fornonconjugate models, Laplace variational inference and delta method variationalinference. Our methods have several advantages: they allow for easily derivedvariational algorithms with a wide class of nonconjugate models; they extendand unify some of the existing algorithms that have been derived for specificmodels; and they work well on real-world datasets. We studied our methods onthe correlated topic model, Bayesian logistic regression, and hierarchicalBayesian logistic regression.
arxiv-2700-284 | Type-theoretical natural language semantics: on the system F for meaning assembly | http://arxiv.org/pdf/1303.3036v1.pdf | author:Christian Retoré category:cs.LO cs.CL math.LO published:2013-03-12 summary:This paper presents and extends our type theoretical framework for acompositional treatment of natural language semantics with some lexicalfeatures like coercions (e.g. of a town into a football club) and copredication(e.g. on a town as a set of people and as a location). The second order typedlambda calculus was shown to be a good framework, and here we discuss how tointroduced predefined types and coercive subtyping which are much more naturalthan internally coded similar constructs. Linguistic applications of these newfeatures are also exemplified.
arxiv-2700-285 | Stable image reconstruction using total variation minimization | http://arxiv.org/pdf/1202.6429v9.pdf | author:Deanna Needell, Rachel Ward category:cs.CV cs.IT math.IT math.NA published:2012-02-29 summary:This article presents near-optimal guarantees for accurate and robust imagerecovery from under-sampled noisy measurements using total variationminimization. In particular, we show that from O(slog(N)) nonadaptive linearmeasurements, an image can be reconstructed to within the best s-termapproximation of its gradient up to a logarithmic factor, and this factor canbe removed by taking slightly more measurements. Along the way, we prove astrengthened Sobolev inequality for functions lying in the null space ofsuitably incoherent matrices.
arxiv-2700-286 | Toward Optimal Stratification for Stratified Monte-Carlo Integration | http://arxiv.org/pdf/1303.2892v1.pdf | author:Alexandra Carpentier, Remi Munos category:stat.ML published:2013-03-12 summary:We consider the problem of adaptive stratified sampling for Monte Carlointegration of a noisy function, given a finite budget n of noisy evaluationsto the function. We tackle in this paper the problem of adapting to thefunction at the same time the number of samples into each stratum and thepartition itself. More precisely, it is interesting to refine the partition ofthe domain in area where the noise to the function, or where the variations ofthe function, are very heterogeneous. On the other hand, having a (too) refinedstratification is not optimal. Indeed, the more refined the stratification, themore difficult it is to adjust the allocation of the samples to thestratification, i.e. sample more points where the noise or variations of thefunction are larger. We provide in this paper an algorithm that selects online,among a large class of partitions, the partition that provides the optimaltrade-off, and allocates the samples almost optimally on this partition.
arxiv-2700-287 | A Stochastic Grammar for Natural Shapes | http://arxiv.org/pdf/1303.2844v1.pdf | author:Pedro F. Felzenszwalb category:cs.CV published:2013-03-12 summary:We consider object detection using a generic model for natural shapes. Acommon approach for object recognition involves matching object models directlyto images. Another approach involves building intermediate representations viaa generic grouping processes. We argue that these two processes (model-basedrecognition and grouping) may use similar computational mechanisms. By defininga generic model for shapes we can use model-based techniques to implement amid-level vision grouping process.
arxiv-2700-288 | Linear system identification using stable spline kernels and PLQ penalties | http://arxiv.org/pdf/1303.2827v1.pdf | author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:stat.ML math.OC stat.CO 47N30, 65K10 published:2013-03-12 summary:The classical approach to linear system identification is given by parametricPrediction Error Methods (PEM). In this context, model complexity is oftenunknown so that a model order selection step is needed to suitably trade-offbias and variance. Recently, a different approach to linear systemidentification has been introduced, where model order determination is avoidedby using a regularized least squares framework. In particular, the penalty termon the impulse response is defined by so called stable spline kernels. Theyembed information on regularity and BIBO stability, and depend on a smallnumber of parameters which can be estimated from data. In this paper, weprovide new nonsmooth formulations of the stable spline estimator. Inparticular, we consider linear system identification problems in a very broadcontext, where regularization functionals and data misfits can come from a richset of piecewise linear quadratic functions. Moreover, our anal- ysis includespolyhedral inequality constraints on the unknown impulse response. For anyformulation in this class, we show that interior point methods can be used tosolve the system identification problem, with complexity O(n3)+O(mn2) in eachiteration, where n and m are the number of impulse response coefficients andmeasurements, respectively. The usefulness of the framework is illustrated viaa numerical experiment where output measurements are contaminated by outliers.
arxiv-2700-289 | Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA | http://arxiv.org/pdf/1303.2826v1.pdf | author:William M. Darling, Fei Song category:cs.CL published:2013-03-12 summary:This article presents a probabilistic generative model for text based onsemantic topics and syntactic classes called Part-of-Speech LDA (POSLDA).POSLDA simultaneously uncovers short-range syntactic patterns (syntax) andlong-range semantic patterns (topics) that exist in document collections. Thisresults in word distributions that are specific to both topics (sports,education, ...) and parts-of-speech (nouns, verbs, ...). For example,multinomial distributions over words are uncovered that can be understood as"nouns about weather" or "verbs about law". We describe the model and anapproximate inference algorithm and then demonstrate the quality of the learnedtopics both qualitatively and quantitatively. Then, we discuss an NLPapplication where the output of POSLDA can lead to strong improvements inquality: unsupervised part-of-speech tagging. We describe algorithms for thistask that make use of POSLDA-learned distributions that result in improvedperformance beyond the state of the art.
arxiv-2700-290 | A Cooperative Q-learning Approach for Real-time Power Allocation in Femtocell Networks | http://arxiv.org/pdf/1303.2789v1.pdf | author:Hussein Saad, Amr Mohamed, Tamer ElBatt category:cs.MA cs.LG published:2013-03-12 summary:In this paper, we address the problem of distributed interference managementof cognitive femtocells that share the same frequency range with macrocells(primary user) using distributed multi-agent Q-learning. We formulate and solvethree problems representing three different Q-learning algorithms: namely,centralized, distributed and partially distributed power control usingQ-learning (CPC-Q, DPC-Q and PDPC-Q). CPCQ, although not of practical interest,characterizes the global optimum. Each of DPC-Q and PDPC-Q works in twodifferent learning paradigms: Independent (IL) and Cooperative (CL). The formeris considered the simplest form for applying Qlearning in multi-agentscenarios, where all the femtocells learn independently. The latter is theproposed scheme in which femtocells share partial information during thelearning process in order to strike a balance between practical relevance andperformance. In terms of performance, the simulation results showed that the CLparadigm outperforms the IL paradigm and achieves an aggregate femtocellscapacity that is very close to the optimal one. For the practical relevanceissue, we evaluate the robustness and scalability of DPC-Q, in real time, bydeploying new femtocells in the system during the learning process, where weshowed that DPC-Q in the CL paradigm is scalable to large number of femtocellsand more robust to the network dynamics compared to the IL paradigm
arxiv-2700-291 | Combined Learning of Salient Local Descriptors and Distance Metrics for Image Set Face Verification | http://arxiv.org/pdf/1303.2783v1.pdf | author:Conrad Sanderson, Mehrtash T. Harandi, Yongkang Wong, Brian C. Lovell category:cs.CV published:2013-03-12 summary:In contrast to comparing faces via single exemplars, matching sets of faceimages increases robustness and discrimination performance. Recent image setmatching approaches typically measure similarities between subspaces ormanifolds, while representing faces in a rigid and holistic manner. Suchrepresentations are easily affected by variations in terms of alignment,illumination, pose and expression. While local feature based representationsare considerably more robust to such variations, they have received littleattention within the image set matching area. We propose a novel image setmatching technique, comprised of three aspects: (i) robust descriptors of faceregions based on local features, partly inspired by the hierarchy in the humanvisual system, (ii) use of several subspace and exemplar metrics to comparecorresponding face regions, (iii) jointly learning which regions are the mostdiscriminative while finding the optimal mixing weights for combining metrics.Face recognition experiments on LFW, PIE and MOBIO face datasets show that theproposed algorithm obtains considerably better performance than several recentstate-of-the-art techniques, such as Local Principal Angle and the KernelAffine Hull Method.
arxiv-2700-292 | Gaussian Mixture Model for Handwritten Script Identification | http://arxiv.org/pdf/1303.2751v1.pdf | author:Mallikarjun Hangarge category:cs.CV published:2013-03-12 summary:This paper presents a Gaussian Mixture Model (GMM) to identify the script ofhandwritten words of Roman, Devanagari, Kannada and Telugu scripts. Itemphasizes the significance of directional energies for identification ofscript of the word. It is robust to varied image sizes and different styles ofwriting. A GMM is modeled using a set of six novel features derived fromdirectional energy distributions of the underlying image. The standarddeviation of directional energy distributions are computed by decomposing animage matrix into right and left diagonals. Furthermore, deviation ofhorizontal and vertical distributions of energies is also built-in to GMM. Adataset of 400 images out of 800 (200 of each script) are used for training GMMand the remaining is for testing. An exhaustive experimentation is carried outat bi-script, tri-script and multi-script level and achieved scriptidentification accuracies in percentage as 98.7, 98.16 and 96.91 respectively.
arxiv-2700-293 | Evolutionary Approaches to Expensive Optimisation | http://arxiv.org/pdf/1303.2745v1.pdf | author:Maumita Bhattacharya category:cs.NE 97R40 published:2013-03-12 summary:Surrogate assisted evolutionary algorithms (EA) are rapidly gainingpopularity where applications of EA in complex real world problem domains areconcerned. Although EAs are powerful global optimizers, finding optimalsolution to complex high dimensional, multimodal problems often require veryexpensive fitness function evaluations. Needless to say, this could brand anypopulation-based iterative optimization technique to be the most cripplingchoice to handle such problems. Use of approximate model or surrogates providesa much cheaper option. However, naturally this cheaper option comes with itsown price. This paper discusses some of the key issues involved with use ofapproximation in evolutionary algorithm, possible best practices and solutions.Answers to the following questions have been sought: what type of fitnessapproximation to be used; which approximation model to use; how to integratethe approximation model in EA; how much approximation to use; and how to ensurereliable approximation.
arxiv-2700-294 | Machine Learning for Bioclimatic Modelling | http://arxiv.org/pdf/1303.2739v1.pdf | author:Maumita Bhattacharya category:cs.LG stat.AP 97R30 published:2013-03-12 summary:Many machine learning (ML) approaches are widely used to generate bioclimaticmodels for prediction of geographic range of organism as a function of climate.Applications such as prediction of range shift in organism, range of invasivespecies influenced by climate change are important parameters in understandingthe impact of climate change. However, success of machine learning-basedapproaches depends on a number of factors. While it can be safely said that noparticular ML technique can be effective in all applications and success of atechnique is predominantly dependent on the application or the type of theproblem, it is useful to understand their behavior to ensure informed choice oftechniques. This paper presents a comprehensive review of machinelearning-based bioclimatic model generation and analyses the factorsinfluencing success of such models. Considering the wide use of statisticaltechniques, in our discussion we also include conventional statisticaltechniques used in bioclimatic modelling.
arxiv-2700-295 | Bilateral Filter: Graph Spectral Interpretation and Extensions | http://arxiv.org/pdf/1303.2685v1.pdf | author:Akshay Gadde, Sunil K Narang, Antonio Ortega category:cs.CV published:2013-03-11 summary:In this paper we study the bilateral filter proposed by Tomasi and Manduchi,as a spectral domain transform defined on a weighted graph. The nodes of thisgraph represent the pixels in the image and a graph signal defined on the nodesrepresents the intensity values. Edge weights in the graph correspond to thebilateral filter coefficients and hence are data adaptive. Spectrum of a graphis defined in terms of the eigenvalues and eigenvectors of the graph Laplacianmatrix. We use this spectral interpretation to generalize the bilateral filterand propose more flexible and application specific spectral designs ofbilateral-like filters. We show that these spectral filters can be implementedwith k-iterative bilateral filtering operations and do not require expensivediagonalization of the Laplacian matrix.
arxiv-2700-296 | A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets | http://arxiv.org/pdf/1202.6258v4.pdf | author:Nicolas Le Roux, Mark Schmidt, Francis Bach category:math.OC cs.LG published:2012-02-28 summary:We propose a new stochastic gradient method for optimizing the sum of afinite set of smooth functions, where the sum is strongly convex. Whilestandard stochastic gradient methods converge at sublinear rates for thisproblem, the proposed method incorporates a memory of previous gradient valuesin order to achieve a linear convergence rate. In a machine learning context,numerical experiments indicate that the new algorithm can dramaticallyoutperform standard algorithms, both in terms of optimizing the training errorand reducing the test error quickly.
arxiv-2700-297 | Revealing Cluster Structure of Graph by Path Following Replicator Dynamic | http://arxiv.org/pdf/1303.2643v1.pdf | author:Hairong Liu, Longin Jan Latecki, Shuicheng Yan category:cs.LG cs.GT published:2013-03-11 summary:In this paper, we propose a path following replicator dynamic, andinvestigate its potentials in uncovering the underlying cluster structure of agraph. The proposed dynamic is a generalization of the discrete replicatordynamic. The replicator dynamic has been successfully used to extract denseclusters of graphs; however, it is often sensitive to the degree distributionof a graph, and usually biased by vertices with large degrees, thus may fail todetect the densest cluster. To overcome this problem, we introduce a dynamicparameter, called path parameter, into the evolution process. The pathparameter can be interpreted as the maximal possible probability of a currentcluster containing a vertex, and it monotonically increases as evolutionprocess proceeds. By limiting the maximal probability, the phenomenon of somevertices dominating the early stage of evolution process is suppressed, thusmaking evolution process more robust. To solve the optimization problem with afixed path parameter, we propose an efficient fixed point algorithm. The timecomplexity of the path following replicator dynamic is only linear in thenumber of edges of a graph, thus it can analyze graphs with millions ofvertices and tens of millions of edges on a common PC in a few minutes.Besides, it can be naturally generalized to hypergraph and graph with edges ofdifferent orders. We apply it to four important problems: maximum cliqueproblem, densest k-subgraph problem, structure fitting, and discovery ofhigh-density regions. The extensive experimental results clearly demonstrateits advantages, in terms of robustness, scalability and flexility.
arxiv-2700-298 | Classification of Segments in PolSAR Imagery by Minimum Stochastic Distances Between Wishart Distributions | http://arxiv.org/pdf/1303.2108v1.pdf | author:Wagner Barreto da Silva, Corina da Costa Freitas, Sidnei João Siqueira Sant'Anna, Alejandro C. Frery category:cs.CV stat.AP published:2013-03-11 summary:A new classifier for Polarimetric SAR (PolSAR) images is proposed andassessed in this paper. Its input consists of segments, and each one isassigned the class which minimizes a stochastic distance. Assuming the complexWishart model, several stochastic distances are obtained from the h-phi familyof divergences, and they are employed to derive hypothesis test statistics thatare also used in the classification process. This article also presents, as anovelty, analytic expressions for the test statistics based on the followingstochastic distances between complex Wishart models: Kullback-Leibler,Bhattacharyya, Hellinger, R\'enyi, and Chi-Square; also, the test statisticbased on the Bhattacharyya distance between multivariate Gaussian distributionsis presented. The classifier performance is evaluated using simulated and realPolSAR data. The simulated data are based on the complex Wishart model, aimingat the analysis of the proposal well controlled data. The real data refer tothe complex L-band image, acquired during the 1994 SIR-C mission. The resultsof the proposed classifier are compared with those obtained by a Wishartper-pixel/contextual classifier, and we show the better performance of theregion-based classification. The influence of the statistical modeling isassessed by comparing the results using the Bhattacharyya distance betweenmultivariate Gaussian distributions for amplitude data. The results withsimulated data indicate that the proposed classification method has a very goodperformance when the data follow the Wishart model. The proposed classifieralso performs better than the per-pixel/contextual classifier and theBhattacharyya Gaussian distance using SIR-C PolSAR data.
arxiv-2700-299 | Kernel Sparse Models for Automated Tumor Segmentation | http://arxiv.org/pdf/1303.2610v1.pdf | author:Jayaraman J. Thiagarajan, Karthikeyan Natesan Ramamurthy, Deepta Rajan, Anup Puri, David Frakes, Andreas Spanias category:cs.CV published:2013-03-11 summary:In this paper, we propose sparse coding-based approaches for segmentation oftumor regions from MR images. Sparse coding with data-adapted dictionaries hasbeen successfully employed in several image recovery and vision problems. Theproposed approaches obtain sparse codes for each pixel in brain magneticresonance images considering their intensity values and location information.Since it is trivial to obtain pixel-wise sparse codes, and combining multiplefeatures in the sparse coding setup is not straightforward, we propose toperform sparse coding in a high-dimensional feature space where non-linearsimilarities can be effectively modeled. We use the training data fromexpert-segmented images to obtain kernel dictionaries with the kernel K-linesclustering procedure. For a test image, sparse codes are computed with thesekernel dictionaries, and they are used to identify the tumor regions. Thisapproach is completely automated, and does not require user intervention toinitialize the tumor regions in a test image. Furthermore, a low complexitysegmentation approach based on kernel sparse codes, which allows the user toinitialize the tumor region, is also presented. Results obtained with both theproposed approaches are validated against manual segmentation by an expertradiologist, and the proposed methods lead to accurate tumor identification.
arxiv-2700-300 | Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities | http://arxiv.org/pdf/1301.3476v3.pdf | author:Tommi Vatanen, Tapani Raiko, Harri Valpola, Yann LeCun category:cs.LG cs.CV stat.ML published:2013-01-15 summary:Recently, we proposed to transform the outputs of each hidden neuron in amulti-layer perceptron network to have zero output and zero slope on average,and use separate shortcut connections to model the linear dependencies instead.We continue the work by firstly introducing a third transformation to normalizethe scale of the outputs of each hidden neuron, and secondly by analyzing theconnections to second order optimization methods. We show that thetransformations make a simple stochastic gradient behave closer to second-orderoptimization methods and thus speed up learning. This is shown both in theoryand with experiments. The experiments on the third transformation show thatwhile it further increases the speed of learning, it can also hurt performanceby converging to a worse local optimum, where both the inputs and outputs ofmany hidden neurons are close to zero.
