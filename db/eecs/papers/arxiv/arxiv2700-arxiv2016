arxiv-2700-1 | PyXNAT: XNAT in Python | http://arxiv.org/pdf/1301.6952v1.pdf | author:Yannick Schwartz, Alexis Barbot, Benjamin Thyreau, Vincent Frouin, Gaël Varoquaux, Aditya Siram, Daniel Marcus, Jean-Baptiste Poline category:cs.DB cs.CV q-bio.QM published:2013-01-29 summary:As neuroimaging databases grow in size and complexity, the time researchersspend investigating and managing the data increases to the expense of dataanalysis. As a result, investigators rely more and more heavily on scriptingusing high-level languages to automate data management and processing tasks.For this, a structured and programmatic access to the data store is necessary.Web services are a first step toward this goal. They however lack infunctionality and ease of use because they provide only low level interfaces todatabases. We introduce here PyXNAT, a Python module that interacts with TheExtensible Neuroimaging Archive Toolkit (XNAT) through native Python callsacross multiple operating systems. The choice of Python enables PyXNAT toexpose the XNAT Web Services and unify their features with a higher level andmore expressive language. PyXNAT provides XNAT users direct access to all thescientific packages in Python. Finally PyXNAT aims to be efficient and easy touse, both as a backend library to build XNAT clients and as an alternativefrontend from the command line.
arxiv-2700-2 | Quadratic Basis Pursuit | http://arxiv.org/pdf/1301.7002v2.pdf | author:Henrik Ohlsson, Allen Y. Yang, Roy Dong, Michel Verhaegen, S. Shankar Sastry category:cs.IT math.IT stat.ML published:2013-01-29 summary:In many compressive sensing problems today, the relationship between themeasurements and the unknowns could be nonlinear. Traditional treatment of suchnonlinear relationships have been to approximate the nonlinearity via a linearmodel and the subsequent un-modeled dynamics as noise. The ability to moreaccurately characterize nonlinear models has the potential to improve theresults in both existing compressive sensing applications and those where alinear approximation does not suffice, e.g., phase retrieval. In this paper, weextend the classical compressive sensing framework to a second-order Taylorexpansion of the nonlinearity. Using a lifting technique and a method we callquadratic basis pursuit, we show that the sparse signal can be recoveredexactly when the sampling rate is sufficiently high. We further presentefficient numerical algorithms to recover sparse signals in second-ordernonlinear systems, which are considerably more difficult to solve than theirlinear counterparts in sparse optimization.
arxiv-2700-3 | On the Consistency of the Bootstrap Approach for Support Vector Machines and Related Kernel Based Methods | http://arxiv.org/pdf/1301.6944v1.pdf | author:Andreas Christmann, Robert Hable category:stat.ML cs.LG published:2013-01-29 summary:It is shown that bootstrap approximations of support vector machines (SVMs)based on a general convex and smooth loss function and on a general kernel areconsistent. This result is useful to approximate the unknown finite sampledistribution of SVMs by the bootstrap approach.
arxiv-2700-4 | An Impossibility Result for High Dimensional Supervised Learning | http://arxiv.org/pdf/1301.6915v2.pdf | author:Mohammad Hossein Rohban, Prakash Ishwar, Birant Orten, William C. Karl, Venkatesh Saligrama category:stat.ML published:2013-01-29 summary:We study high-dimensional asymptotic performance limits of binary supervisedclassification problems where the class conditional densities are Gaussian withunknown means and covariances and the number of signal dimensions scales fasterthan the number of labeled training samples. We show that the Bayes error,namely the minimum attainable error probability with complete distributionalknowledge and equally likely classes, can be arbitrarily close to zero and yetthe limiting minimax error probability of every supervised learning algorithmis no better than a random coin toss. In contrast to related studies where theclassification difficulty (Bayes error) is made to vanish, we hold it constantwhen taking high-dimensional limits. In contrast to VC-dimension based minimaxlower bounds that consider the worst case error probability over alldistributions that have a fixed Bayes error, our worst case is over the familyof Gaussian distributions with constant Bayes error. We also show that anontrivial asymptotic minimax error probability can only be attained forparametric subsets of zero measure (in a suitable measure space). These resultsexpose the fundamental importance of prior knowledge and suggest that unless weimpose strong structural constraints, such as sparsity, on the parametricspace, supervised learning may be ineffective in high dimensional small samplesettings.
arxiv-2700-5 | Clustering-Based Matrix Factorization | http://arxiv.org/pdf/1301.6659v4.pdf | author:Nima Mirbakhsh, Charles X. Ling category:cs.LG published:2013-01-28 summary:Recommender systems are emerging technologies that nowadays can be found inmany applications such as Amazon, Netflix, and so on. These systems help usersto find relevant information, recommendations, and their preferred items.Slightly improvement of the accuracy of these recommenders can highly affectthe quality of recommendations. Matrix Factorization is a popular method inRecommendation Systems showing promising results in accuracy and complexity. Inthis paper we propose an extension of matrix factorization which adds generalneighborhood information on the recommendation model. Users and items areclustered into different categories to see how these categories sharepreferences. We then employ these shared interests of categories in a fusion byBiased Matrix Factorization to achieve more accurate recommendations. This is acomplement for the current neighborhood aware matrix factorization models whichrely on using direct neighborhood information of users and items. The proposedmodel is tested on two well-known recommendation system datasets: Movielens100kand Netflix. Our experiment shows applying the general latent features ofcategories into factorized recommender models improves the accuracy ofrecommendations. The current neighborhood-aware models need a great number ofneighbors to acheive good accuracies. To the best of our knowledge, theproposed model is better than or comparable with the current neighborhood-awaremodels when they consider fewer number of neighbors.
arxiv-2700-6 | Generalized Bregman Divergence and Gradient of Mutual Information for Vector Poisson Channels | http://arxiv.org/pdf/1301.6648v3.pdf | author:Liming Wang, Miguel Rodrigues, Lawrence Carin category:cs.IT math.IT stat.ML published:2013-01-28 summary:We investigate connections between information-theoretic andestimation-theoretic quantities in vector Poisson channel models. Inparticular, we generalize the gradient of mutual information with respect tokey system parameters from the scalar to the vector Poisson channel model. Wealso propose, as another contribution, a generalization of the classicalBregman divergence that offers a means to encapsulate under a unifyingframework the gradient of mutual information results for scalar and vectorPoisson and Gaussian channel models. The so-called generalized Bregmandivergence is also shown to exhibit various properties akin to the propertiesof the classical version. The vector Poisson channel model is drawingconsiderable attention in view of its application in various domains: as anexample, the availability of the gradient of mutual information can be used inconjunction with gradient descent methods to effect compressive-sensingprojection designs in emerging X-ray and document classification applications.
arxiv-2700-7 | Political Disaffection: a case study on the Italian Twitter community | http://arxiv.org/pdf/1301.6630v2.pdf | author:Corrado Monti, Alessandro Rozza, Giovanni Zappella, Matteo Zignani, Adam Arvidsson, Monica Poletti category:cs.SI cs.LG physics.soc-ph published:2013-01-28 summary:In our work we analyse the political disaffection or "the subjective feelingof powerlessness, cynicism, and lack of confidence in the political process,politicians, and democratic institutions, but with no questioning of thepolitical regime" by exploiting Twitter data through machine learningtechniques. In order to validate the quality of the time-series generated bythe Twitter data, we highlight the relations of these data with politicaldisaffection as measured by means of public opinion surveys. Moreover, we showthat important political news of Italian newspapers are often correlated withthe highest peaks of the produced time-series.
arxiv-2700-8 | Image registration with sparse approximations in parametric dictionaries | http://arxiv.org/pdf/1301.6646v2.pdf | author:Alhussein Fawzi, Pascal Frossard category:cs.CV published:2013-01-28 summary:We examine in this paper the problem of image registration from the newperspective where images are given by sparse approximations in parametricdictionaries of geometric functions. We propose a registration algorithm thatlooks for an estimate of the global transformation between sparse images byexamining the set of relative geometrical transformations between therespective features. We propose a theoretical analysis of our registrationalgorithm and we derive performance guarantees based on two novel importantproperties of redundant dictionaries, namely the robust linear independence andthe transformation inconsistency. We propose several illustrations and insightsabout the importance of these dictionary properties and show that commonproperties such as coherence or restricted isometry property fail to providesufficient information in registration problems. We finally show withillustrative experiments on simple visual objects and handwritten digits imagesthat our algorithm outperforms baseline competitor methods in terms oftransformation-invariant distance computation and classification.
arxiv-2700-9 | Guarantees of Total Variation Minimization for Signal Recovery | http://arxiv.org/pdf/1301.6791v6.pdf | author:Jian-Feng Cai, Weiyu Xu category:cs.IT cs.CV cs.LG math.IT published:2013-01-28 summary:In this paper, we consider using total variation minimization to recoversignals whose gradients have a sparse support, from a small number ofmeasurements. We establish the proof for the performance guarantee of totalvariation (TV) minimization in recovering \emph{one-dimensional} signal withsparse gradient support. This partially answers the open problem of proving thefidelity of total variation minimization in such a setting \cite{TVMulti}. Inparticular, we have shown that the recoverable gradient sparsity can growlinearly with the signal dimension when TV minimization is used. Recoverablesparsity thresholds of TV minimization are explicitly computed for1-dimensional signal by using the Grassmann angle framework. We also extend ourresults to TV minimization for multidimensional signals. Stability ofrecovering signal itself using 1-D TV minimization has also been establishedthrough a property called "almost Euclidean property for 1-dimensional TVnorm". We further give a lower bound on the number of random Gaussianmeasurements for recovering 1-dimensional signal vectors with $N$ elements and$K$-sparse gradients. Interestingly, the number of needed measurements is lowerbounded by $\Omega((NK)^{\frac{1}{2}})$, rather than the $O(K\log(N/K))$ boundfrequently appearing in recovering $K$-sparse signal vectors.
arxiv-2700-10 | Discriminative Feature Selection for Uncertain Graph Classification | http://arxiv.org/pdf/1301.6626v1.pdf | author:Xiangnan Kong, Philip S. Yu, Xue Wang, Ann B. Ragin category:cs.LG cs.DB stat.ML published:2013-01-28 summary:Mining discriminative features for graph data has attracted much attention inrecent years due to its important role in constructing graph classifiers,generating graph indices, etc. Most measurement of interestingness ofdiscriminative subgraph features are defined on certain graphs, where thestructure of graph objects are certain, and the binary edges within each graphrepresent the "presence" of linkages among the nodes. In many real-worldapplications, however, the linkage structure of the graphs is inherentlyuncertain. Therefore, existing measurements of interestingness based uponcertain graphs are unable to capture the structural uncertainty in theseapplications effectively. In this paper, we study the problem of discriminativesubgraph feature selection from uncertain graphs. This problem is challengingand different from conventional subgraph mining problems because both thestructure of the graph objects and the discrimination score of each subgraphfeature are uncertain. To address these challenges, we propose a noveldiscriminative subgraph feature selection method, DUG, which can finddiscriminative subgraph features in uncertain graphs based upon differentstatistical measures including expectation, median, mode and phi-probability.We first compute the probability distribution of the discrimination scores foreach subgraph feature based on dynamic programming. Then a branch-and-boundalgorithm is proposed to search for discriminative subgraphs efficiently.Extensive experiments on various neuroimaging applications (i.e., Alzheimer'sDisease, ADHD and HIV) have been performed to analyze the gain in performanceby taking into account structural uncertainties in identifying discriminativesubgraph features for graph classification.
arxiv-2700-11 | An alternative text representation to TF-IDF and Bag-of-Words | http://arxiv.org/pdf/1301.6770v1.pdf | author:Zhixiang, Xu, Minmin Chen, Kilian Q. Weinberger, Fei Sha category:cs.IR cs.LG stat.ML published:2013-01-28 summary:In text mining, information retrieval, and machine learning, text documentsare commonly represented through variants of sparse Bag of Words (sBoW) vectors(e.g. TF-IDF). Although simple and intuitive, sBoW style representations sufferfrom their inherent over-sparsity and fail to capture word-level synonymy andpolysemy. Especially when labeled data is limited (e.g. in documentclassification), or the text documents are short (e.g. emails or abstracts),many features are rarely observed within the training corpus. This leads tooverfitting and reduced generalization accuracy. In this paper we propose DenseCohort of Terms (dCoT), an unsupervised algorithm to learn improved sBoWdocument features. dCoT explicitly models absent words by removing andreconstructing random sub-sets of words in the unlabeled corpus. With thisapproach, dCoT learns to reconstruct frequent words from co-occurringinfrequent words and maps the high dimensional sparse sBoW vectors into alow-dimensional dense representation. We show that the feature removal can bemarginalized out and that the reconstruction can be solved for in closed-form.We demonstrate empirically, on several benchmark datasets, that dCoT featuressignificantly improve the classification accuracy across several documentclassification tasks.
arxiv-2700-12 | An improvement to k-nearest neighbor classifier | http://arxiv.org/pdf/1301.6324v1.pdf | author:T. Hitendra Sarma, P. Viswanath, D. Sai Koti Reddy, S. Sri Raghava category:cs.CV cs.LG stat.ML published:2013-01-27 summary:K-Nearest neighbor classifier (k-NNC) is simple to use and has little designtime like finding k values in k-nearest neighbor classifier, hence these aresuitable to work with dynamically varying data-sets. There exists somefundamental improvements over the basic k-NNC, like weighted k-nearestneighbors classifier (where weights to nearest neighbors are given based onlinear interpolation), using artificially generated training set calledbootstrapped training set, etc. These improvements are orthogonal to spacereduction and classification time reduction techniques, hence can be coupledwith any of them. The paper proposes another improvement to the basic k-NNCwhere the weights to nearest neighbors are given based on Gaussian distribution(instead of linear interpolation as done in weighted k-NNC) which is alsoindependent of any space reduction and classification time reduction technique.We formally show that our proposed method is closely related to non-parametricdensity estimation using a Gaussian kernel. We experimentally demonstrate usingvarious standard data-sets that the proposed method is better than the existingones in most cases.
arxiv-2700-13 | An Extragradient-Based Alternating Direction Method for Convex Minimization | http://arxiv.org/pdf/1301.6308v3.pdf | author:Tianyi Lin, Shiqian Ma, Shuzhong Zhang category:math.OC stat.ML published:2013-01-27 summary:In this paper, we consider the problem of minimizing the sum of two convexfunctions subject to linear linking constraints. The classical alternatingdirection type methods usually assume that the two convex functions haverelatively easy proximal mappings. However, many problems arising fromstatistics, image processing and other fields have the structure that while oneof the two functions has easy proximal mapping, the other function is smoothlyconvex but does not have an easy proximal mapping. Therefore, the classicalalternating direction methods cannot be applied. To deal with the difficulty,we propose in this paper an alternating direction method based onextragradients. Under the assumption that the smooth function has a Lipschitzcontinuous gradient, we prove that the proposed method returns an$\epsilon$-optimal solution within $O(1/\epsilon)$ iterations. We apply theproposed method to solve a new statistical model called fused logisticregression. Our numerical experiments show that the proposed method performsvery well when solving the test problems. We also test the performance of theproposed method through solving the lasso problem arising from statistics andcompare the result with several existing efficient solvers for this problem;the results are very encouraging indeed.
arxiv-2700-14 | Hierarchical Data Representation Model - Multi-layer NMF | http://arxiv.org/pdf/1301.6316v3.pdf | author:Hyun Ah Song, Soo-Young Lee category:cs.LG published:2013-01-27 summary:In this paper, we propose a data representation model that demonstrateshierarchical feature learning using nsNMF. We extend unit algorithm intoseveral layers. Experiments with document and image data successfullydiscovered feature hierarchies. We also prove that proposed method results inmuch better classification and reconstruction performance, especially for smallnumber of features. feature hierarchies.
arxiv-2700-15 | Equitability Analysis of the Maximal Information Coefficient, with Comparisons | http://arxiv.org/pdf/1301.6314v2.pdf | author:David Reshef, Yakir Reshef, Michael Mitzenmacher, Pardis Sabeti category:cs.LG q-bio.QM stat.ML published:2013-01-27 summary:A measure of dependence is said to be equitable if it gives similar scores toequally noisy relationships of different types. Equitability is important indata exploration when the goal is to identify a relatively small set ofstrongest associations within a dataset as opposed to finding as many non-zeroassociations as possible, which often are too many to sift through. Thus anequitable statistic, such as the maximal information coefficient (MIC), can beuseful for analyzing high-dimensional data sets. Here, we explore bothequitability and the properties of MIC, and discuss several aspects of thetheory and practice of MIC. We begin by presenting an intuition behind theequitability of MIC through the exploration of the maximization andnormalization steps in its definition. We then examine the speed and optimalityof the approximation algorithm used to compute MIC, and suggest some directionsfor improving both. Finally, we demonstrate in a range of noise models andsample sizes that MIC is more equitable than natural alternatives, such asmutual information estimation and distance correlation.
arxiv-2700-16 | Neural Networks Built from Unreliable Components | http://arxiv.org/pdf/1301.6265v4.pdf | author:Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav Varshney category:cs.NE cs.IT math.IT published:2013-01-26 summary:Recent advances in associative memory design through strutured pattern setsand graph-based inference algorithms have allowed the reliable learning andretrieval of an exponential number of patterns. Both these and classicalassociative memories, however, have assumed internally noiseless computationalnodes. This paper considers the setting when internal computations are alsonoisy. Even if all components are noisy, the final error probability in recallcan often be made exceedingly small, as we characterize. There is a thresholdphenomenon. We also show how to optimize inference algorithm parameters whenknowing statistical properties of internal noise.
arxiv-2700-17 | Sample Complexity of Bayesian Optimal Dictionary Learning | http://arxiv.org/pdf/1301.6199v2.pdf | author:Ayaka Sakata, Yoshiyuki Kabashima category:cs.LG cs.IT math.IT published:2013-01-26 summary:We consider a learning problem of identifying a dictionary matrix D (M timesN dimension) from a sample set of M dimensional vectors Y = N^{-1/2} DX, whereX is a sparse matrix (N times P dimension) in which the density of non-zeroentries is 0<rho< 1. In particular, we focus on the minimum sample size P_c(sample complexity) necessary for perfectly identifying D of the optimallearning scheme when D and X are independently generated from certaindistributions. By using the replica method of statistical mechanics, we showthat P_c=O(N) holds as long as alpha = M/N >rho is satisfied in the limit of Nto infinity. Our analysis also implies that the posterior distribution given Yis condensed only at the correct dictionary D when the compression rate alphais greater than a certain critical value alpha_M(rho). This suggests thatbelief propagation may allow us to learn D with a low computational complexityusing O(N) samples.
arxiv-2700-18 | LA-LDA: A Limited Attention Topic Model for Social Recommendation | http://arxiv.org/pdf/1301.6277v1.pdf | author:Jeon-Hyung Kang, Kristina Lerman, Lise Getoor category:cs.SI cs.IR cs.LG published:2013-01-26 summary:Social media users have finite attention which limits the number of incomingmessages from friends they can process. Moreover, they pay more attention toopinions and recommendations of some friends more than others. In this paper,we propose LA-LDA, a latent topic model which incorporates limited,non-uniformly divided attention in the diffusion process by which opinions andinformation spread on the social network. We show that our proposed model isable to learn more accurate user models from users' social network and itemadoption behavior than models which do not take limited attention into account.We analyze voting on news items on the social news aggregator Digg and showthat our proposed model is better able to predict held out votes thanalternative models. Our study demonstrates that psycho-socially motivatedmodels have better ability to describe and predict observed behavior thanmodels which only consider topics.
arxiv-2700-19 | Recycling Proof Patterns in Coq: Case Studies | http://arxiv.org/pdf/1301.6039v4.pdf | author:Jónathan Heras, Ekaterina Komendantskaya category:cs.AI cs.LG cs.LO published:2013-01-25 summary:Development of Interactive Theorem Provers has led to the creation of biglibraries and varied infrastructures for formal proofs. However, despite (orperhaps due to) their sophistication, the re-use of libraries by non-experts oracross domains is a challenge. In this paper, we provide detailed case studiesand evaluate the machine-learning tool ML4PG built to interactively data-minethe electronic libraries of proofs, and to provide user guidance on the basisof proof patterns found in the existing libraries.
arxiv-2700-20 | Explorative Data Analysis for Changes in Neural Activity | http://arxiv.org/pdf/1301.6027v1.pdf | author:Duncan A. J. Blythe, Frank C. Meinecke, Paul von Buenau, Klaus-Robert Mueller category:q-bio.QM stat.ML published:2013-01-25 summary:Neural recordings are nonstationary time series, i.e. their propertiestypically change over time. Identifying specific changes, e.g. those induced bya learning task, can shed light on the underlying neural processes. However,such changes of interest are often masked by strong unrelated changes, whichcan be of physiological origin or due to measurement artifacts. We propose anovel algorithm for disentangling such different causes of non-stationarity andin this manner enable better neurophysiological interpretation for a wider setof experimental paradigms. A key ingredient is the repeated application ofStationary Subspace Analysis (SSA) using different temporal scales. Theusefulness of our explorative approach is demonstrated in simulations, theoryand EEG experiments with 80 Brain-Computer-Interfacing (BCI) subjects.
arxiv-2700-21 | Weighted Last-Step Min-Max Algorithm with Improved Sub-Logarithmic Regret | http://arxiv.org/pdf/1301.6058v1.pdf | author:Edward Moroshko, Koby Crammer category:cs.LG published:2013-01-25 summary:In online learning the performance of an algorithm is typically compared tothe performance of a fixed function from some class, with a quantity calledregret. Forster proposed a last-step min-max algorithm which was somewhatsimpler than the algorithm of Vovk, yet with the same regret. In fact thealgorithm he analyzed assumed that the choices of the adversary are bounded,yielding artificially only the two extreme cases. We fix this problem byweighing the examples in such a way that the min-max problem will be welldefined, and provide analysis with logarithmic regret that may have bettermultiplicative factor than both bounds of Forster and Vovk. We also derive anew bound that may be sub-logarithmic, as a recent bound of Orabona et.al, butmay have better multiplicative factor. Finally, we analyze the algorithm in aweak-type of non-stationary setting, and show a bound that is sub-linear if thenon-stationarity is sub-linear as well.
arxiv-2700-22 | Transfer Topic Modeling with Ease and Scalability | http://arxiv.org/pdf/1301.5686v2.pdf | author:Jeon-Hyung Kang, Jun Ma, Yan Liu category:cs.CL cs.LG stat.ML published:2013-01-24 summary:The increasing volume of short texts generated on social media sites, such asTwitter or Facebook, creates a great demand for effective and efficient topicmodeling approaches. While latent Dirichlet allocation (LDA) can be applied, itis not optimal due to its weakness in handling short texts with fast-changingtopics and scalability concerns. In this paper, we propose a transfer learningapproach that utilizes abundant labeled documents from other domains (such asYahoo! News or Wikipedia) to improve topic modeling, with better model fittingand result interpretation. Specifically, we develop Transfer Hierarchical LDA(thLDA) model, which incorporates the label information from other domains viainformative priors. In addition, we develop a parallel implementation of ourmodel for large-scale applications. We demonstrate the effectiveness of ourthLDA model on both a microblogging dataset and standard text collectionsincluding AP and RCV1 datasets.
arxiv-2700-23 | Reinforcement learning from comparisons: Three alternatives is enough, two is not | http://arxiv.org/pdf/1301.5734v1.pdf | author:Benoit Laslier, Jean-Francois Laslier category:math.OC cs.LG math.PR published:2013-01-24 summary:The paper deals with the problem of finding the best alternatives on thebasis of pairwise comparisons when these comparisons need not be transitive. Inthis setting, we study a reinforcement urn model. We prove convergence to theoptimal solution when reinforcement of a winning alternative occurs each timeafter considering three random alternatives. The simpler process, whichreinforces the winner of a random pair does not always converges: it may cycle.
arxiv-2700-24 | Phase Diagram and Approximate Message Passing for Blind Calibration and Dictionary Learning | http://arxiv.org/pdf/1301.5898v1.pdf | author:Florent Krzakala, Marc Mézard, Lenka Zdeborová category:cs.IT cs.LG math.IT published:2013-01-24 summary:We consider dictionary learning and blind calibration for signals andmatrices created from a random ensemble. We study the mean-squared error in thelimit of large signal dimension using the replica method and unveil theappearance of phase transitions delimiting impossible, possible-but-hard andpossible inference regions. We also introduce an approximate message passingalgorithm that asymptotically matches the theoretical performance, and showthrough numerical tests that it performs very well, for the calibrationproblem, for tractable system sizes.
arxiv-2700-25 | Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions | http://arxiv.org/pdf/1301.6697v2.pdf | author:Dan Geiger, David Heckerman category:cs.LG stat.ML published:2013-01-23 summary:We show that the only parameter prior for complete Gaussian DAG models thatsatisfies global parameter independence, complete model equivalence, and someweak regularity assumptions, is the normal-Wishart distribution. Our analysisis based on the following new characterization of the Wishart distribution: letW be an n x n, n >= 3, positive-definite symmetric matrix of random variablesand f(W) be a pdf of W. Then, f(W) is a Wishart distribution if and only ifW_{11}-W_{12}W_{22}^{-1}W_{12}' is independent of {W_{12}, W_{22}} for everyblock partitioning W_{11}, W_{12}, W_{12}', W_{22} of W. Similarcharacterizations of the normal and normal-Wishart distributions are providedas well. We also show how to construct a prior for every DAG model over X fromthe prior of a single regression model.
arxiv-2700-26 | Fast Learning from Sparse Data | http://arxiv.org/pdf/1301.6685v2.pdf | author:David Maxwell Chickering, David Heckerman category:cs.LG stat.ML published:2013-01-23 summary:We describe two techniques that significantly improve the running time ofseveral standard machine-learning algorithms when data is sparse. The firsttechnique is an algorithm that effeciently extracts one-way and two-waycounts--either real or expected-- from discrete data. Extracting such counts isa fundamental step in learning algorithms for constructing a variety of modelsincluding decision trees, decision graphs, Bayesian networks, and naive-Bayesclustering models. The second technique is an algorithm that efficientlyperforms the E-step of the EM algorithm (i.e. inference) when applied to anaive-Bayes clustering model. Using real-world data sets, we demonstrate adramatic decrease in running time for algorithms that incorporate thesetechniques.
arxiv-2700-27 | Improved Cheeger's Inequality: Analysis of Spectral Partitioning Algorithms through Higher Order Spectral Gap | http://arxiv.org/pdf/1301.5584v1.pdf | author:Tsz Chiu Kwok, Lap Chi Lau, Yin Tat Lee, Shayan Oveis Gharan, Luca Trevisan category:cs.DS cs.DM math.CO math.SP stat.ML published:2013-01-23 summary:Let \phi(G) be the minimum conductance of an undirected graph G, and let0=\lambda_1 <= \lambda_2 <=... <= \lambda_n <= 2 be the eigenvalues of thenormalized Laplacian matrix of G. We prove that for any graph G and any k >= 2, \phi(G) = O(k) \lambda_2 / \sqrt{\lambda_k}, and this performance guaranteeis achieved by the spectral partitioning algorithm. This improves Cheeger'sinequality, and the bound is optimal up to a constant factor for any k. Ourresult shows that the spectral partitioning algorithm is a constant factorapproximation algorithm for finding a sparse cut if \lambda_k$ is a constantfor some constant k. This provides some theoretical justification to itsempirical performance in image segmentation and clustering problems. We extendthe analysis to other graph partitioning problems, including multi-waypartition, balanced separator, and maximum cut.
arxiv-2700-28 | Multi-Class Detection and Segmentation of Objects in Depth | http://arxiv.org/pdf/1301.5582v1.pdf | author:Cheng Zhang, Hedvig Kjellstrom category:cs.CV cs.RO published:2013-01-23 summary:The quality of life of many people could be improved by autonomous humanoidrobots in the home. To function in the human world, a humanoid household robotmust be able to locate itself and perceive the environment like a human; sceneperception, object detection and segmentation, and object spatial localizationin 3D are fundamental capabilities for such humanoid robots. This paperpresents a 3D multi-class object detection and segmentation method. Thecontributions are twofold. Firstly, we present a multi-class detection method,where a minimal joint codebook is learned in a principled manner. Secondly, weincorporate depth information using RGB-D imagery, which increases therobustness of the method and gives the 3D location of objects -- necessarysince the robot reasons in 3D space. Experiments show that the multi-classextension improves the detection efficiency with respect to the number ofclasses and the depth extension improves the detection robustness and givesufficient natural 3D location of the objects.
arxiv-2700-29 | Regularization and nonlinearities for neural language models: when are they needed? | http://arxiv.org/pdf/1301.5650v2.pdf | author:Marius Pachitariu, Maneesh Sahani category:stat.ML cs.LG published:2013-01-23 summary:Neural language models (LMs) based on recurrent neural networks (RNN) aresome of the most successful word and character-level LMs. Why do they work sowell, in particular better than linear neural LMs? Possible explanations arethat RNNs have an implicitly better regularization or that RNNs have a highercapacity for storing patterns due to their nonlinearities or both. Here weargue for the first explanation in the limit of little training data and thesecond explanation for large amounts of text data. We show state-of-the-artperformance on the popular and small Penn dataset when RNN LMs are regularizedwith random dropout. Nonetheless, we show even better performance from asimplified, much less expressive linear RNN model without off-diagonal entriesin the recurrent matrix. We call this model an impulse-response LM (IRLM).Using random dropout, column normalization and annealed learning rates, IRLMsdevelop neurons that keep a memory of up to 50 words in the past and achieve aperplexity of 102.5 on the Penn dataset. On two large datasets however, thesame regularization methods are unsuccessful for both models and the RNN'sexpressivity allows it to overtake the IRLM by 10 and 20 percent perplexity,respectively. Despite the perplexity gap, IRLMs still outperform RNNs on theMicrosoft Research Sentence Completion (MRSC) task. We develop a slightlymodified IRLM that separates long-context units (LCUs) from short-context unitsand show that the LCUs alone achieve a state-of-the-art performance on the MRSCtask of 60.8%. Our analysis indicates that a fruitful direction of research forneural LMs lies in developing more accessible internal representations, andsuggests an optimization regime of very high momentum terms for effectivelytraining such models.
arxiv-2700-30 | Approximate Learning in Complex Dynamic Bayesian Networks | http://arxiv.org/pdf/1301.6738v1.pdf | author:Raffaella Settimi, Jim Q. Smith, A. S. Gargoum category:cs.LG stat.ML published:2013-01-23 summary:In this paper we extend the work of Smith and Papamichail (1999) and presentfast approximate Bayesian algorithms for learning in complex scenarios where atany time frame, the relationships between explanatory state space variables canbe described by a Bayesian network that evolve dynamically over time and theobservations taken are not necessarily Gaussian. It uses recent developments inapproximate Bayesian forecasting methods in combination with more familiarGaussian propagation algorithms on junction trees. The procedure for learningstate parameters from data is given explicitly for common samplingdistributions and the methodology is illustrated through a real application.The efficiency of the dynamic approximation is explored by using the Hellingerdivergence measure and theoretical bounds for the efficacy of such a procedureare discussed.
arxiv-2700-31 | Variational Learning in Mixed-State Dynamic Graphical Models | http://arxiv.org/pdf/1301.6731v1.pdf | author:Vladimir Pavlovic, Brendan J. Frey, Thomas S. Huang category:cs.LG stat.ML published:2013-01-23 summary:Many real-valued stochastic time-series are locally linear (Gassian), butglobally non-linear. For example, the trajectory of a human hand gesture can beviewed as a linear dynamic system driven by a nonlinear dynamic system thatrepresents muscle actions. We present a mixed-state dynamic graphical model inwhich a hidden Markov model drives a linear dynamic system. This combinationallows us to model both the discrete and continuous causes of trajectories suchas human gestures. The number of computations needed for exact inference isexponential in the sequence length, so we derive an approximate variationalinference technique that can also be used to learn the parameters of thediscrete and continuous models. We show how the mixed-state model and thevariational technique can be used to classify human hand gestures made with acomputer mouse.
arxiv-2700-32 | Accelerating EM: An Empirical Study | http://arxiv.org/pdf/1301.6730v1.pdf | author:Luis E. Ortiz, Leslie Pack Kaelbling category:cs.LG stat.ML published:2013-01-23 summary:Many applications require that we learn the parameters of a model from data.EM is a method used to learn the parameters of probabilistic models for whichthe data for some of the variables in the models is either missing or hidden.There are instances in which this method is slow to converge. Therefore,several accelerations have been proposed to improve the method. None of theproposed acceleration methods are theoretically dominant and experimentalcomparisons are lacking. In this paper, we present the different proposedaccelerations and try to compare them experimentally. From the results of theexperiments, we argue that some acceleration of EM is always possible, but thatwhich acceleration is superior depends on properties of the problem.
arxiv-2700-33 | Learning Bayesian Networks with Restricted Causal Interactions | http://arxiv.org/pdf/1301.6727v1.pdf | author:Julian R. Neil, Chris S. Wallace, Kevin B. Korb category:cs.AI cs.LG stat.ML published:2013-01-23 summary:A major problem for the learning of Bayesian networks (BNs) is theexponential number of parameters needed for conditional probability tables.Recent research reduces this complexity by modeling local structure in theprobability tables. We examine the use of log-linear local models. Whilelog-linear models in this context are not new (Whittaker, 1990; Buntine, 1991;Neal, 1992; Heckerman and Meek, 1997), for structure learning they aregenerally subsumed under a naive Bayes model. We describe an alternativeinterpretation, and use a Minimum Message Length (MML) (Wallace, 1987) metricfor structure learning of networks exhibiting causal independence, which weterm first-order networks (FONs). We also investigate local model selection ona node-by-node basis.
arxiv-2700-34 | Learning Bayesian Networks from Incomplete Data with Stochastic Search Algorithms | http://arxiv.org/pdf/1301.6726v1.pdf | author:James W. Myers, Kathryn Blackmond Laskey, Tod S. Levitt category:cs.AI cs.LG published:2013-01-23 summary:This paper describes stochastic search approaches, including a new stochasticalgorithm and an adaptive mutation operator, for learning Bayesian networksfrom incomplete data. This problem is characterized by a huge solution spacewith a highly multimodal landscape. State-of-the-art approaches all involveusing deterministic approaches such as the expectation-maximization algorithm.These approaches are guaranteed to find local maxima, but do not explore thelandscape for other modes. Our approach evolves structure and the missing data.We compare our stochastic algorithms and show they all produce accurateresults.
arxiv-2700-35 | Loopy Belief Propagation for Approximate Inference: An Empirical Study | http://arxiv.org/pdf/1301.6725v1.pdf | author:Kevin Murphy, Yair Weiss, Michael I. Jordan category:cs.AI cs.LG published:2013-01-23 summary:Recently, researchers have demonstrated that loopy belief propagation - theuse of Pearls polytree algorithm IN a Bayesian network WITH loops OF error-correcting codes.The most dramatic instance OF this IS the near Shannon - limitperformance OF Turbo Codes codes whose decoding algorithm IS equivalent TOloopy belief propagation IN a chain - structured Bayesian network. IN thispaper we ask : IS there something special about the error - correcting codecontext, OR does loopy propagation WORK AS an approximate inference schemeIN amore general setting? We compare the marginals computed using loopy propagationTO the exact ones IN four Bayesian network architectures, including two real -world networks : ALARM AND QMR.We find that the loopy beliefs often convergeAND WHEN they do, they give a good approximation TO the correctmarginals.However,ON the QMR network, the loopy beliefs oscillated AND had noobvious relationship TO the correct posteriors. We present SOME initialinvestigations INTO the cause OF these oscillations, AND show that SOME simplemethods OF preventing them lead TO the wrong results.
arxiv-2700-36 | A Variational Approximation for Bayesian Networks with Discrete and Continuous Latent Variables | http://arxiv.org/pdf/1301.6724v1.pdf | author:Kevin Murphy category:cs.AI cs.LG stat.ML published:2013-01-23 summary:We show how to use a variational approximation to the logistic function toperform approximate inference in Bayesian networks containing discrete nodeswith continuous parents. Essentially, we convert the logistic function to aGaussian, which facilitates exact inference, and then iteratively adjust thevariational parameters to improve the quality of the approximation. Wedemonstrate experimentally that this approximation is faster and potentiallymore accurate than sampling. We also introduce a simple new technique forhandling evidence, which allows us to handle arbitrary distributions onobserved nodes, as well as achieving a significant speedup in networks withdiscrete variables of large cardinality.
arxiv-2700-37 | A Bayesian Network Classifier that Combines a Finite Mixture Model and a Naive Bayes Model | http://arxiv.org/pdf/1301.6723v1.pdf | author:Stefano Monti, Gregory F. Cooper category:cs.LG cs.AI stat.ML published:2013-01-23 summary:In this paper we present a new Bayesian network model for classification thatcombines the naive-Bayes (NB) classifier and the finite-mixture (FM)classifier. The resulting classifier aims at relaxing the strong assumptions onwhich the two component models are based, in an attempt to improve on theirclassification performance, both in terms of accuracy and in terms ofcalibration of the estimated probabilities. The proposed classifier is obtainedby superimposing a finite mixture model on the set of feature variables of anaive Bayes model. We present experimental results that compare the predictiveperformance on real datasets of the new classifier with the predictiveperformance of the NB classifier and the FM classifier.
arxiv-2700-38 | On Supervised Selection of Bayesian Networks | http://arxiv.org/pdf/1301.6710v1.pdf | author:Petri Kontkanen, Petri Myllymaki, Tomi Silander, Henry Tirri category:cs.LG stat.ML published:2013-01-23 summary:Given a set of possible models (e.g., Bayesian network structures) and a datasample, in the unsupervised model selection problem the task is to choose themost accurate model with respect to the domain joint probability distribution.In contrast to this, in supervised model selection it is a priori known thatthe chosen model will be used in the future for prediction tasks involving more``focused' predictive distributions. Although focused predictive distributionscan be produced from the joint probability distribution by marginalization, inpractice the best model in the unsupervised sense does not necessarily performwell in supervised domains. In particular, the standard marginal likelihoodscore is a criterion for the unsupervised task, and, although frequently usedfor supervised model selection also, does not perform well in such tasks. Inthis paper we study the performance of the marginal likelihood scoreempirically in supervised Bayesian network selection tasks by using a largenumber of publicly available classification data sets, and compare the resultsto those obtained by alternative model selection criteria, including empiricalcrossvalidation methods, an approximation of a supervised marginal likelihoodmeasure, and a supervised version of Dawids prequential(predictive sequential)principle.The results demonstrate that the marginal likelihood score does NOTperform well FOR supervised model selection, WHILE the best results areobtained BY using Dawids prequential r napproach.
arxiv-2700-39 | Probabilistic Latent Semantic Analysis | http://arxiv.org/pdf/1301.6705v1.pdf | author:Thomas Hofmann category:cs.LG cs.IR stat.ML published:2013-01-23 summary:Probabilistic Latent Semantic Analysis is a novel statistical technique forthe analysis of two-mode and co-occurrence data, which has applications ininformation retrieval and filtering, natural language processing, machinelearning from text, and in related areas. Compared to standard Latent SemanticAnalysis which stems from linear algebra and performs a Singular ValueDecomposition of co-occurrence tables, the proposed method is based on amixture decomposition derived from a latent class model. This results in a moreprincipled approach which has a solid foundation in statistics. In order toavoid overfitting, we propose a widely applicable generalization of maximumlikelihood model fitting by tempered EM. Our approach yields substantial andconsistent improvements over Latent Semantic Analysis in a number ofexperiments.
arxiv-2700-40 | Multi-objects association in perception of dynamical situation | http://arxiv.org/pdf/1301.6701v1.pdf | author:Dominique Gruyer, Veronique Berge-Cherfaoui category:cs.AI cs.CV published:2013-01-23 summary:In current perception systems applied to the rebuilding of the environmentfor intelligent vehicles, the part reserved to object association for thetracking is increasingly significant. This allows firstly to follow the objectstemporal evolution and secondly to increase the reliability of environmentperception. We propose in this communication the development of a multi-objectsassociation algorithm with ambiguity removal entering into the design of such adynamic perception system for intelligent vehicles. This algorithm uses thebelief theory and data modelling with fuzzy mathematics in order to be able tohandle inaccurate as well as uncertain information due to imperfect sensors.These theories also allow the fusion of numerical as well as symbolic data. Wedevelop in this article the problem of matching between known and perceivedobjects. This makes it possible to update a dynamic environment map for avehicle. The belief theory will enable us to quantify the belief in theassociation of each perceived object with each known object. Conflicts canappear in the case of object appearance or disappearance, or in the case of aconfused situation or bad perception. These conflicts are removed or solvedusing an assignment algorithm, giving a solution called the " best " and soensuring the tracking of some objects present in our environment.
arxiv-2700-41 | Learning Bayesian Network Structure from Massive Datasets: The "Sparse Candidate" Algorithm | http://arxiv.org/pdf/1301.6696v1.pdf | author:Nir Friedman, Iftach Nachman, Dana Pe'er category:cs.LG cs.AI stat.ML published:2013-01-23 summary:Learning Bayesian networks is often cast as an optimization problem, wherethe computational task is to find a structure that maximizes a statisticallymotivated score. By and large, existing learning tools address thisoptimization problem using standard heuristic search techniques. Since thesearch space is extremely large, such search procedures can spend most of thetime examining candidates that are extremely unreasonable. This problem becomescritical when we deal with data sets that are large either in the number ofinstances, or the number of attributes. In this paper, we introduce analgorithm that achieves faster learning by restricting the search space. Thisiterative algorithm restricts the parents of each variable to belong to a smallsubset of candidates. We then search for a network that satisfies theseconstraints. The learned network is then used for selecting better candidatesfor the next iteration. We evaluate this algorithm both on synthetic andreal-life data. Our results show that it is significantly faster thanalternative search procedures without loss of quality in the learnedstructures.
arxiv-2700-42 | Spread spectrum compressed sensing MRI using chirp radio frequency pulses | http://arxiv.org/pdf/1301.5451v1.pdf | author:Xiaobo Qu, Ying Chen, Xiaoxing Zhuang, Zhiyu Yan, Di Guo, Zhong Chen category:cs.CV math.OC physics.med-ph published:2013-01-23 summary:Compressed sensing has shown great potential in reducing data acquisitiontime in magnetic resonance imaging (MRI). Recently, a spread spectrumcompressed sensing MRI method modulates an image with a quadratic phase. Itperforms better than the conventional compressed sensing MRI with variabledensity sampling, since the coherence between the sensing and sparsity basesare reduced. However, spread spectrum in that method is implemented via a shimcoil which limits its modulation intensity and is not convenient to operate. Inthis letter, we propose to apply chirp (linear frequency-swept) radio frequencypulses to easily control the spread spectrum. To accelerate the imagereconstruction, an alternating direction algorithm is modified by exploitingthe complex orthogonality of the quadratic phase encoding. Reconstruction onthe acquired data demonstrates that more image features are preserved using theproposed approach than those of conventional CS-MRI.
arxiv-2700-43 | Data Analysis with Bayesian Networks: A Bootstrap Approach | http://arxiv.org/pdf/1301.6695v1.pdf | author:Nir Friedman, Moises Goldszmidt, Abraham Wyner category:cs.LG cs.AI stat.ML published:2013-01-23 summary:In recent years there has been significant progress in algorithms and methodsfor inducing Bayesian networks from data. However, in complex data analysisproblems, we need to go beyond being satisfied with inducing networks with highscores. We need to provide confidence measures on features of these networks:Is the existence of an edge between two nodes warranted? Is the Markov blanketof a given node robust? Can we say something about the ordering of thevariables? We should be able to address these questions, even when the amountof data is not enough to induce a high scoring network. In this paper wepropose Efron's Bootstrap as a computationally efficient approach for answeringthese questions. In addition, we propose to use these confidence measures toinduce better structures from the data, and to detect the presence of latentvariables.
arxiv-2700-44 | Model-Based Bayesian Exploration | http://arxiv.org/pdf/1301.6690v1.pdf | author:Richard Dearden, Nir Friedman, David Andre category:cs.AI cs.LG published:2013-01-23 summary:Reinforcement learning systems are often concerned with balancing explorationof untested actions against exploitation of actions that are known to be good.The benefit of exploration can be estimated using the classical notion of Valueof Information - the expected improvement in future decision quality arisingfrom the information acquired by exploration. Estimating this quantity requiresan assessment of the agent's uncertainty about its current value estimates forstates. In this paper we investigate ways of representing and reasoning aboutthis uncertainty in algorithms where the system attempts to learn a model ofits environment. We explicitly represent uncertainty about the parameters ofthe model and build probability distributions over Q-values based on these.These distributions are used to compute a myopic approximation to the value ofinformation for each action and hence to select the action that best balancesexploration and exploitation.
arxiv-2700-45 | Learning Polytrees | http://arxiv.org/pdf/1301.6688v1.pdf | author:Sanjoy Dasgupta category:cs.AI cs.LG published:2013-01-23 summary:We consider the task of learning the maximum-likelihood polytree from data.Our first result is a performance guarantee establishing that the optimalbranching (or Chow-Liu tree), which can be computed very easily, constitutes agood approximation to the best polytree. We then show that it is not possibleto do very much better, since the learning problem is NP-hard even toapproximately solve within some constant factor.
arxiv-2700-46 | Comparing Bayesian Network Classifiers | http://arxiv.org/pdf/1301.6684v1.pdf | author:Jie Cheng, Russell Greiner category:cs.LG cs.AI stat.ML published:2013-01-23 summary:In this paper, we empirically evaluate algorithms for learning four types ofBayesian network (BN) classifiers - Naive-Bayes, tree augmented Naive-Bayes, BNaugmented Naive-Bayes and general BNs, where the latter two are learned usingtwo variants of a conditional-independence (CI) based BN-learning algorithm.Experimental results show the obtained classifiers, learned using the CI basedalgorithms, are competitive with (or superior to) the best known classifiers,based on both Bayesian networks and other formalisms; and that thecomputational time for learning and using these classifiers is relativelysmall. Moreover, these results also suggest a way to learn yet more effectiveclassifiers; we demonstrate empirically that this new algorithm does work asexpected. Collectively, these results argue that BN classifiers deserve moreattention in machine learning and data mining communities.
arxiv-2700-47 | Discovering the Hidden Structure of Complex Dynamic Systems | http://arxiv.org/pdf/1301.6683v1.pdf | author:Xavier Boyen, Nir Friedman, Daphne Koller category:cs.AI cs.LG published:2013-01-23 summary:Dynamic Bayesian networks provide a compact and natural representation forcomplex dynamic systems. However, in many cases, there is no expert availablefrom whom a model can be elicited. Learning provides an alternative approachfor constructing models of dynamic systems. In this paper, we address some ofthe crucial computational aspects of learning the structure of dynamic systems,particularly those where some relevant variables are partially observed or evenentirely unknown. Our approach is based on the Structural ExpectationMaximization (SEM) algorithm. The main computational cost of the SEM algorithmis the gathering of expected sufficient statistics. We propose a novelapproximation scheme that allows these sufficient statistics to be computedefficiently. We also investigate the fundamental problem of discovering theexistence of hidden variables without exhaustive and expensive search. Ourapproach is based on the observation that, in dynamic systems, ignoring ahidden variable typically results in a violation of the Markov property. Thus,our algorithm searches for such violations in the data, and introduces hiddenvariables to explain them. We provide empirical results showing that thealgorithm is able to learn the dynamics of complex systems in a computationallytractable way.
arxiv-2700-48 | Multi-class Generalized Binary Search for Active Inverse Reinforcement Learning | http://arxiv.org/pdf/1301.5488v1.pdf | author:Francisco Melo, Manuel Lopes category:cs.LG cs.AI stat.ML published:2013-01-23 summary:This paper addresses the problem of learning a task from demonstration. Weadopt the framework of inverse reinforcement learning, where tasks arerepresented in the form of a reward function. Our contribution is a novelactive learning algorithm that enables the learning agent to query the expertfor more informative demonstrations, thus leading to more sample-efficientlearning. For this novel algorithm (Generalized Binary Search for InverseReinforcement Learning, or GBS-IRL), we provide a theoretical bound on samplecomplexity and illustrate its applicability on several different tasks. To ourknowledge, GBS-IRL is the first active IRL algorithm with provable samplecomplexity bounds. We also discuss our method in light of other existingmethods in the literature and its general applicability in multi-classclassification problems. Finally, motivated by recent work on learning fromdemonstration in robots, we also discuss how different forms of human feedbackcan be integrated in a transparent manner in our learning framework.
arxiv-2700-49 | Relative Loss Bounds for On-line Density Estimation with the Exponential Family of Distributions | http://arxiv.org/pdf/1301.6677v1.pdf | author:Katy S. Azoury, Manfred K. Warmuth category:cs.LG stat.ML published:2013-01-23 summary:We consider on-line density estimation with a parameterized density from theexponential family. The on-line algorithm receives one example at a time andmaintains a parameter that is essentially an average of the past examples.After receiving an example the algorithm incurs a loss which is the negativelog-likelihood of the example w.r.t. the past parameter of the algorithm. Anoff-line algorithm can choose the best parameter based on all the examples. Weprove bounds on the additional total loss of the on-line algorithm over thetotal loss of the off-line algorithm. These relative loss bounds hold for anarbitrary sequence of examples. The goal is to design algorithms with the bestpossible relative loss bounds. We use a certain divergence to derive andanalyze the algorithms. This divergence is a relative entropy between twoexponential distributions.
arxiv-2700-50 | Inferring Parameters and Structure of Latent Variable Models by Variational Bayes | http://arxiv.org/pdf/1301.6676v1.pdf | author:Hagai Attias category:cs.LG stat.ML published:2013-01-23 summary:Current methods for learning graphical models with latent variables and afixed structure estimate optimal values for the model parameters. Whereas thisapproach usually produces overfitting and suboptimal generalizationperformance, carrying out the Bayesian program of computing the full posteriordistributions over the parameters remains a difficult problem. Moreover,learning the structure of models with latent variables, for which the Bayesianapproach is crucial, is yet a harder problem. In this paper I present theVariational Bayes framework, which provides a solution to these problems. Thisapproach approximates full posterior distributions over model parameters andstructures, as well as latent variables, in an analytical manner withoutresorting to sampling methods. Unlike in the Laplace approximation, theseposteriors are generally non-Gaussian and no Hessian needs to be computed. Theresulting algorithm generalizes the standard Expectation Maximizationalgorithm, and its convergence is guaranteed. I demonstrate that this algorithmcan be applied to a large class of models in several domains, includingunsupervised clustering and blind source separation.
arxiv-2700-51 | ChESS - Quick and Robust Detection of Chess-board Features | http://arxiv.org/pdf/1301.5491v1.pdf | author:Stuart Bennett, Joan Lasenby category:cs.CV published:2013-01-23 summary:Localization of chess-board vertices is a common task in computer vision,underpinning many applications, but relatively little work focusses ondesigning a specific feature detector that is fast, accurate and robust. Inthis paper the `Chess-board Extraction by Subtraction and Summation' (ChESS)feature detector, designed to exclusively respond to chess-board vertices, ispresented. The method proposed is robust against noise, poor lighting and poorcontrast, requires no prior knowledge of the extent of the chess-board pattern,is computationally very efficient, and provides a strength measure of detectedfeatures. Such a detector has significant application both in the key field ofcamera calibration, as well as in Structured Light 3D reconstruction. Evidenceis presented showing its robustness, accuracy, and efficiency in comparison toother commonly used detectors both under simulation and in experimental 3Dreconstruction of flat plate and cylindrical objects
arxiv-2700-52 | Heteroscedastic Conditional Ordinal Random Fields for Pain Intensity Estimation from Facial Images | http://arxiv.org/pdf/1301.5063v2.pdf | author:Ognjen Rudovic, Maja Pantic, Vladimir Pavlovic category:cs.CV cs.LG stat.ML published:2013-01-22 summary:We propose a novel method for automatic pain intensity estimation from facialimages based on the framework of kernel Conditional Ordinal Random Fields(KCORF). We extend this framework to account for heteroscedasticity on theoutput labels(i.e., pain intensity scores) and introduce a novel dynamicfeatures, dynamic ranks, that impose temporal ordinal constraints on the staticranks (i.e., intensity scores). Our experimental results show that the proposedapproach outperforms state-of-the art methods for sequence classification withordinal data and other ordinal regression models. The approach performssignificantly better than other models in terms of Intra-Class Correlationmeasure, which is the most accepted evaluation measure in the tasks of facialbehaviour intensity estimation.
arxiv-2700-53 | Properties of the Least Squares Temporal Difference learning algorithm | http://arxiv.org/pdf/1301.5220v2.pdf | author:Kamil Ciosek category:stat.ML cs.LG published:2013-01-22 summary:This paper presents four different ways of looking at the well-known LeastSquares Temporal Differences (LSTD) algorithm for computing the value functionof a Markov Reward Process, each of them leading to different insights: theoperator-theory approach via the Galerkin method, the statistical approach viainstrumental variables, the linear dynamical system view as well as the limitof the TD iteration. We also give a geometric view of the algorithm as anoblique projection. Furthermore, there is an extensive comparison of theoptimization problem solved by LSTD as compared to Bellman ResidualMinimization (BRM). We then review several schemes for the regularization ofthe LSTD solution. We then proceed to treat the modification of LSTD for thecase of episodic Markov Reward Processes.
arxiv-2700-54 | Online Learning with Pairwise Loss Functions | http://arxiv.org/pdf/1301.5332v1.pdf | author:Yuyang Wang, Roni Khardon, Dmitry Pechyony, Rosie Jones category:stat.ML cs.LG published:2013-01-22 summary:Efficient online learning with pairwise loss functions is a crucial componentin building large-scale learning system that maximizes the area under theReceiver Operator Characteristic (ROC) curve. In this paper we investigate thegeneralization performance of online learning algorithms with pairwise lossfunctions. We show that the existing proof techniques for generalization boundsof online algorithms with a univariate loss can not be directly applied topairwise losses. In this paper, we derive the first result providingdata-dependent bounds for the average risk of the sequence of hypothesesgenerated by an arbitrary online learner in terms of an easily computablestatistic, and show how to extract a low risk hypothesis from the sequence. Wedemonstrate the generality of our results by applying it to two importantproblems in machine learning. First, we analyze two online algorithms forbipartite ranking; one being a natural extension of the perceptron algorithmand the other using online convex optimization. Secondly, we provide ananalysis for the risk bound for an online algorithm for supervised metriclearning.
arxiv-2700-55 | Active Learning on Trees and Graphs | http://arxiv.org/pdf/1301.5112v1.pdf | author:Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella category:cs.LG stat.ML published:2013-01-22 summary:We investigate the problem of active learning on a given tree whose nodes areassigned binary labels in an adversarial way. Inspired by recent results byGuillory and Bilmes, we characterize (up to constant factors) the optimalplacement of queries so to minimize the mistakes made on the non-queried nodes.Our query selection algorithm is extremely efficient, and the optimal number ofmistakes on the non-queried nodes is achieved by a simple and efficient mincutclassifier. Through a simple modification of the query selection algorithm wealso show optimality (up to constant factors) with respect to the trade-offbetween number of queries and number of mistakes on non-queried nodes. By usingspanning trees, our algorithms can be efficiently applied to general graphs,although the problem of finding optimal and efficient active learningalgorithms for general graphs remains open. Towards this end, we provide alower bound on the number of mistakes made on arbitrary graphs by any activelearning algorithm using a number of queries which is up to a constant fractionof the graph size.
arxiv-2700-56 | Piecewise Linear Multilayer Perceptrons and Dropout | http://arxiv.org/pdf/1301.5088v1.pdf | author:Ian J. Goodfellow category:stat.ML cs.LG published:2013-01-22 summary:We propose a new type of hidden layer for a multilayer perceptron, anddemonstrate that it obtains the best reported performance for an MLP on theMNIST dataset.
arxiv-2700-57 | Efficient MRF Energy Propagation for Video Segmentation via Bilateral Filters | http://arxiv.org/pdf/1301.5356v3.pdf | author:Ozan Sener, Kemal Ugur, A. Aydin Alatan category:cs.CV published:2013-01-22 summary:Segmentation of an object from a video is a challenging task in multimediaapplications. Depending on the application, automatic or interactive methodsare desired; however, regardless of the application type, efficient computationof video object segmentation is crucial for time-critical applications;specifically, mobile and interactive applications require near real-timeefficiencies. In this paper, we address the problem of video segmentation fromthe perspective of efficiency. We initially redefine the problem of videoobject segmentation as the propagation of MRF energies along the temporaldomain. For this purpose, a novel and efficient method is proposed to propagateMRF energies throughout the frames via bilateral filters without using anyglobal texture, color or shape model. Recently presented bi-exponential filteris utilized for efficiency, whereas a novel technique is also developed todynamically solve graph-cuts for varying, non-lattice graphs in general linearfiltering scenario. These improvements are experimented for both automatic andinteractive video segmentation scenarios. Moreover, in addition to theefficiency, segmentation quality is also tested both quantitatively andqualitatively. Indeed, for some challenging examples, significant timeefficiency is observed without loss of segmentation quality.
arxiv-2700-58 | See the Tree Through the Lines: The Shazoo Algorithm -- Full Version -- | http://arxiv.org/pdf/1301.5160v2.pdf | author:Fabio Vitale, Nicolo Cesa-Bianchi, Claudio Gentile, Giovanni Zappella category:cs.LG published:2013-01-22 summary:Predicting the nodes of a given graph is a fascinating theoretical problemwith applications in several domains. Since graph sparsification via spanningtrees retains enough information while making the task much easier, trees arean important special case of this problem. Although it is known how to predictthe nodes of an unweighted tree in a nearly optimal way, in the weighted case afully satisfactory algorithm is not available yet. We fill this hole andintroduce an efficient node predictor, Shazoo, which is nearly optimal on anyweighted tree. Moreover, we show that Shazoo can be viewed as a commonnontrivial generalization of both previous approaches for unweighted trees andweighted lines. Experiments on real-world datasets confirm that Shazoo performswell in that it fully exploits the structure of the input tree, and gets veryclose to (and sometimes better than) less scalable energy minimization methods.
arxiv-2700-59 | The connection between Bayesian estimation of a Gaussian random field and RKHS | http://arxiv.org/pdf/1301.5288v3.pdf | author:Aleksandr Y. Aravkin, Bradley M. Bell, James V. Burke, Gianluigi Pillonetto category:stat.ML cs.LG math.ST stat.TH 47N30, 65K10 published:2013-01-22 summary:Reconstruction of a function from noisy data is often formulated as aregularized optimization problem over an infinite-dimensional reproducingkernel Hilbert space (RKHS). The solution describes the observed data and has asmall RKHS norm. When the data fit is measured using a quadratic loss, thisestimator has a known statistical interpretation. Given the noisy measurements,the RKHS estimate represents the posterior mean (minimum variance estimate) ofa Gaussian random field with covariance proportional to the kernel associatedwith the RKHS. In this paper, we provide a statistical interpretation when moregeneral losses are used, such as absolute value, Vapnik or Huber. Specifically,for any finite set of sampling locations (including where the data werecollected), the MAP estimate for the signal samples is given by the RKHSestimate evaluated at these locations.
arxiv-2700-60 | Pattern Matching for Self- Tuning of MapReduce Jobs | http://arxiv.org/pdf/1301.4753v1.pdf | author:Nikzad Babaii Rizvandi, Javid Taheri, Albert Y. Zomaya category:cs.DC cs.AI cs.LG published:2013-01-21 summary:In this paper, we study CPU utilization time patterns of several MapReduceapplications. After extracting running patterns of several applications, theyare saved in a reference database to be later used to tweak system parametersto efficiently execute unknown applications in future. To achieve this goal,CPU utilization patterns of new applications are compared with the alreadyknown ones in the reference database to find/predict their most probableexecution patterns. Because of different patterns lengths, the Dynamic TimeWarping (DTW) is utilized for such comparison; a correlation analysis is thenapplied to DTWs outcomes to produce feasible similarity patterns. Three realapplications (WordCount, Exim Mainlog parsing and Terasort) are used toevaluate our hypothesis in tweaking system parameters in executing similarapplications. Results were very promising and showed effectiveness of ourapproach on pseudo-distributed MapReduce platforms.
arxiv-2700-61 | Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots | http://arxiv.org/pdf/1301.4862v1.pdf | author:Adrien Baranes, Pierre-Yves Oudeyer category:cs.LG cs.AI cs.CV cs.NE cs.RO published:2013-01-21 summary:We introduce the Self-Adaptive Goal Generation - Robust Intelligent AdaptiveCuriosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goalexploration mechanism which allows active learning of inverse models inhigh-dimensional redundant robots. This allows a robot to efficiently andactively learn distributions of parameterized motor skills/policies that solvea corresponding distribution of parameterized tasks/goals. The architecturemakes the robot sample actively novel parameterized tasks in the task space,based on a measure of competence progress, each of which triggers low-levelgoal-directed learning of the motor policy pa- rameters that allow to solve it.For both learning and generalization, the system leverages regressiontechniques which allow to infer the motor policy parameters corresponding to agiven novel parameterized task, and based on the previously learntcorrespondences between policy and task parameters. We present experiments withhigh-dimensional continuous sensorimotor spaces in three different roboticsetups: 1) learning the inverse kinematics in a highly-redundant robotic arm,2) learning omnidirectional locomotion with motor primitives in a quadrupedrobot, 3) an arm learning to control a fishing rod with a flexible wire. Weshow that 1) exploration in the task space can be a lot faster than explorationin the actuator space for learning inverse models in redundant robots; 2)selecting goals maximizing competence progress creates developmentaltrajectories driving the robot to progressively focus on tasks of increasingcomplexity and is statistically significantly more efficient than selectingtasks randomly, as well as more efficient than different standard active motorbabbling methods; 3) this architecture allows the robot to actively discoverwhich parts of its task space it can learn to reach and which part it cannot.
arxiv-2700-62 | A Linear Time Active Learning Algorithm for Link Classification -- Full Version -- | http://arxiv.org/pdf/1301.4767v2.pdf | author:Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella category:cs.LG cs.SI stat.ML published:2013-01-21 summary:We present very efficient active learning algorithms for link classificationin signed networks. Our algorithms are motivated by a stochastic model in whichedge labels are obtained through perturbations of a initial sign assignmentconsistent with a two-clustering of the nodes. We provide a theoreticalanalysis within this model, showing that we can achieve an optimal (to whithina constant factor) number of mistakes on any graph G = (V,E) such that E =\Omega(V^{3/2}) by querying O(V^{3/2}) edge labels. More generally, we showan algorithm that achieves optimality to within a factor of O(k) by querying atmost order of V + (V/k)^{3/2} edge labels. The running time of thisalgorithm is at most of order E + V\logV.
arxiv-2700-63 | Dirichlet draws are sparse with high probability | http://arxiv.org/pdf/1301.4917v1.pdf | author:Matus Telgarsky category:cs.LG math.PR stat.ML published:2013-01-21 summary:This note provides an elementary proof of the folklore fact that draws from aDirichlet distribution (with parameters less than 1) are typically sparse (mostcoordinates are small).
arxiv-2700-64 | Evaluation of a Supervised Learning Approach for Stock Market Operations | http://arxiv.org/pdf/1301.4944v1.pdf | author:Marcelo S. Lauretto, Barbara B. C. Silva, Pablo M. Andrade category:stat.ML cs.LG stat.AP published:2013-01-21 summary:Data mining methods have been widely applied in financial markets, with thepurpose of providing suitable tools for prices forecasting and automatictrading. Particularly, learning methods aim to identify patterns in time seriesand, based on such patterns, to recommend buy/sell operations. The objective ofthis work is to evaluate the performance of Random Forests, a supervisedlearning method based on ensembles of decision trees, for decision support instock markets. Preliminary results indicate good rates of successful operationsand good rates of return per operation, providing a strong motivation forfurther research in this topic.
arxiv-2700-65 | A type theoretical framework for natural language semantics: the Montagovian generative lexicon | http://arxiv.org/pdf/1301.4938v3.pdf | author:Christian Retoré category:cs.LO cs.CL published:2013-01-21 summary:We present a framework, named the Montagovian generative lexicon, forcomputing the semantics of natural language sentences, expressed in many sortedhigher order logic. Word meaning is depicted by lambda terms of second orderlambda calculus (Girard's system F) with base types including a type forpropositions and many types for sorts of a many sorted logic. This framework isable to integrate a proper treatment of lexical phenomena into a Montagoviancompositional semantics, including the restriction of selection which imposesthe nature of the arguments of a predicate, and the possible adaptation of aword meaning to some contexts. Among these adaptations of a word's sense to thecontext, ontological inclusions are handled by an extension of system F withcoercive subtyping that is introduced in the present paper. The benefits ofthis framework for lexical pragmatics are illustrated on meaning transfers andcoercions, on possible and impossible copredication over different senses, ondeverbal ambiguities, and on "fictive motion". Next we show that thecompositional treatment of determiners, quantifiers, plurals,... are finergrained in our framework. We then conclude with the linguistic, logical andcomputational perspectives opened by the Montagovian generative lexicon.
arxiv-2700-66 | A Correlation Clustering Approach to Link Classification in Signed Networks -- Full Version -- | http://arxiv.org/pdf/1301.4769v2.pdf | author:Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella category:cs.LG cs.DS stat.ML published:2013-01-21 summary:Motivated by social balance theory, we develop a theory of linkclassification in signed networks using the correlation clustering index asmeasure of label regularity. We derive learning bounds in terms of correlationclustering within three fundamental transductive learning settings: online,batch and active. Our main algorithmic contribution is in the active setting,where we introduce a new family of efficient link classifiers based on coveringthe input graph with small circuits. These are the first active algorithms forlink classification with mistake bounds that hold for arbitrary signednetworks.
arxiv-2700-67 | Supervised Classification Using Sparse Fisher's LDA | http://arxiv.org/pdf/1301.4976v2.pdf | author:Irina Gaynanova, James G. Booth, Martin T. Wells category:stat.ML stat.CO published:2013-01-21 summary:It is well known that in a supervised classification setting when the numberof features is smaller than the number of observations, Fisher's lineardiscriminant rule is asymptotically Bayes. However, there are numerous modernapplications where classification is needed in the high-dimensional setting.Naive implementation of Fisher's rule in this case fails to provide goodresults because the sample covariance matrix is singular. Moreover, byconstructing a classifier that relies on all features the interpretation of theresults is challenging. Our goal is to provide robust classification thatrelies only on a small subset of important features and accounts for theunderlying correlation structure. We apply a lasso-type penalty to thediscriminant vector to ensure sparsity of the solution and use a shrinkage typeestimator for the covariance matrix. The resulting optimization problem issolved using an iterative coordinate ascent algorithm. Furthermore, we analyzethe effect of nonconvexity on the sparsity level of the solution and highlightthe difference between the penalized and the constrained versions of theproblem. The simulation results show that the proposed method performsfavorably in comparison to alternatives. The method is used to classifyleukemia patients based on DNA methylation features.
arxiv-2700-68 | Cellular Tree Classifiers | http://arxiv.org/pdf/1301.4679v2.pdf | author:Gérard Biau, Luc Devroye category:stat.ML cs.LG math.ST stat.TH published:2013-01-20 summary:The cellular tree classifier model addresses a fundamental problem in thedesign of classifiers for a parallel or distributed computing world: Given adata set, is it sufficient to apply a majority rule for classification, orshall one split the data into two or more parts and send each part to apotentially different computer (or cell) for further processing? At firstsight, it seems impossible to define with this paradigm a consistent classifieras no cell knows the "original data size", $n$. However, we show that this isnot so by exhibiting two different consistent classifiers. The consistency isuniversal but is only shown for distributions with nonatomic marginals.
arxiv-2700-69 | A Linearly Convergent Conditional Gradient Algorithm with Applications to Online and Stochastic Optimization | http://arxiv.org/pdf/1301.4666v6.pdf | author:Dan Garber, Elad Hazan category:cs.LG math.OC stat.ML published:2013-01-20 summary:Linear optimization is many times algorithmically simpler than non-linearconvex optimization. Linear optimization over matroid polytopes, matchingpolytopes and path polytopes are example of problems for which we have simpleand efficient combinatorial algorithms, but whose non-linear convex counterpartis harder and admits significantly less efficient algorithms. This motivatesthe computational model of convex optimization, including the offline, onlineand stochastic settings, using a linear optimization oracle. In thiscomputational model we give several new results that improve over the previousstate-of-the-art. Our main result is a novel conditional gradient algorithm forsmooth and strongly convex optimization over polyhedral sets that performs onlya single linear optimization step over the domain on each iteration and enjoysa linear convergence rate. This gives an exponential improvement in convergencerate over previous results. Based on this new conditional gradient algorithm we give the first algorithmsfor online convex optimization over polyhedral sets that perform only a singlelinear optimization step over the domain while having optimal regretguarantees, answering an open question of Kalai and Vempala, and Hazan andKale. Our online algorithms also imply conditional gradient algorithms fornon-smooth and stochastic convex optimization with the same convergence ratesas projected (sub)gradient methods.
arxiv-2700-70 | Recurrent Neural Network Method in Arabic Words Recognition System | http://arxiv.org/pdf/1301.4662v1.pdf | author:Yusuf Perwej category:cs.NE published:2013-01-20 summary:The recognition of unconstrained handwriting continues to be a difficult taskfor computers despite active research for several decades. This is becausehandwritten text offers great challenges such as character and wordsegmentation, character recognition, variation between handwriting styles,different character size and no font constraints as well as the backgroundclarity. In this paper primarily discussed Online Handwriting Recognitionmethods for Arabic words which being often used among then across the MiddleEast and North Africa people. Because of the characteristic of the whole bodyof the Arabic words, namely connectivity between the characters, thereby thesegmentation of An Arabic word is very difficult. We introduced a recurrentneural network to online handwriting Arabic word recognition. The keyinnovation is a recently produce recurrent neural networks objective functionknown as connectionist temporal classification. The system consists of anadvanced recurrent neural network with an output layer designed for sequencelabeling, partially combined with a probabilistic language model. Experimentalresults show that unconstrained Arabic words achieve recognition rates about79%, which is significantly higher than the about 70% using a previouslydeveloped hidden markov model based recognition system.
arxiv-2700-71 | Lip Localization and Viseme Classification for Visual Speech Recognition | http://arxiv.org/pdf/1301.4558v1.pdf | author:Salah Werda, Walid Mahdi, Abdelmajid Ben Hamadou category:cs.CV published:2013-01-19 summary:The need for an automatic lip-reading system is ever increasing. Infact,today, extraction and reliable analysis of facial movements make up animportant part in many multimedia systems such as videoconference, lowcommunication systems, lip-reading systems. In addition, visual information isimperative among people with special needs. We can imagine, for example, adependent person ordering a machine with an easy lip movement or by a simplesyllable pronunciation. Moreover, people with hearing problems compensate fortheir special needs by lip-reading as well as listening to the person withwhome they are talking.
arxiv-2700-72 | Sparse/Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory | http://arxiv.org/pdf/1301.4566v2.pdf | author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:stat.ML math.OC math.ST stat.TH 62F35, 65K10 published:2013-01-19 summary:We introduce a class of quadratic support (QS) functions, many of which playa crucial role in a variety of applications, including machine learning, robuststatistical inference, sparsity promotion, and Kalman smoothing. Well knownexamples include the l2, Huber, l1 and Vapnik losses. We build on a dualrepresentation for QS functions using convex analysis, revealing the structurenecessary for a QS function to be interpreted as the negative log of aprobability density, and providing the foundation for statisticalinterpretation and analysis of QS loss functions. For a subclass of QSfunctions called piecewise linear quadratic (PLQ) penalties, we also developefficient numerical estimation schemes. These components form a flexiblestatistical modeling framework for a variety of learning applications, togetherwith a toolbox of efficient numerical methods for inference. In particular, forPLQ densities, interior point (IP) methods can be used. IP methods solvenonsmooth optimization problems by working directly with smooth systems ofequations characterizing their optimality. The efficiency of the IP approachdepends on the structure of particular applications. We consider the class ofdynamic inverse problems using Kalman smoothing, where the aim is toreconstruct the state of a dynamical system with known process and measurementmodels starting from noisy output samples. In the classical case, Gaussianerrors are assumed in the process and measurement models. The extendedframework allows arbitrary PLQ densities to be used, and the proposed IPapproach solves the generalized Kalman smoothing problem while maintaining thelinear complexity in the size of the time series, just as in the Gaussian case.This extends the computational efficiency of classic algorithms to a muchbroader nonsmooth setting, and includes many recently proposed robust andsparse smoothers as special cases.
arxiv-2700-73 | Multiple models of Bayesian networks applied to offline recognition of Arabic handwritten city names | http://arxiv.org/pdf/1301.4377v1.pdf | author:Mohamed Ali Mahjoub, Nabil Ghanmy, Khlifia jayech, Ikram Miled category:cs.CV published:2013-01-18 summary:In this paper we address the problem of offline Arabic handwriting wordrecognition. Off-line recognition of handwritten words is a difficult task dueto the high variability and uncertainty of human writing. The majority of therecent systems are constrained by the size of the lexicon to deal with and thenumber of writers. In this paper, we propose an approach for multi-writersArabic handwritten words recognition using multiple Bayesian networks. First,we cut the image in several blocks. For each block, we compute a vector ofdescriptors. Then, we use K-means to cluster the low-level features includingZernik and Hu moments. Finally, we apply four variants of Bayesian networksclassifiers (Na\"ive Bayes, Tree Augmented Na\"ive Bayes (TAN), ForestAugmented Na\"ive Bayes (FAN) and DBN (dynamic bayesian network) to classifythe whole image of tunisian city name. The results demonstrate FAN and DBNoutperform good recognition rates
arxiv-2700-74 | Language learning from positive evidence, reconsidered: A simplicity-based approach | http://arxiv.org/pdf/1301.4432v1.pdf | author:Anne S. Hsu, Nick Chater, Paul M. B. Vitányi category:cs.CL published:2013-01-18 summary:Children learn their native language by exposure to their linguistic andcommunicative environment, but apparently without requiring that their mistakesare corrected. Such learning from positive evidence has been viewed as raisinglogical problems for language acquisition. In particular, without correction,how is the child to recover from conjecturing an over-general grammar, whichwill be consistent with any sentence that the child hears? There have been manyproposals concerning how this logical problem can be dissolved. Here, we reviewrecent formal results showing that the learner has sufficient data to learnsuccessfully from positive evidence, if it favours the simplest encoding of thelinguistic input. Results include the ability to learn a linguistic prediction,grammaticality judgements, language production, and form-meaning mappings. Thesimplicity approach can also be scaled-down to analyse the ability to learn aspecific linguistic constructions, and is amenable to empirical test as aframework for describing human language acquisition.
arxiv-2700-75 | Latent Relation Representations for Universal Schemas | http://arxiv.org/pdf/1301.4293v2.pdf | author:Sebastian Riedel, Limin Yao, Andrew McCallum category:cs.LG stat.ML published:2013-01-18 summary:Traditional relation extraction predicts relations within some fixed andfinite target schema. Machine learning approaches to this task require eithermanual annotation or, in the case of distant supervision, existing structuredsources of the same schema. The need for existing datasets can be avoided byusing a universal schema: the union of all involved schemas (surface formpredicates as in OpenIE, and relations in the schemas of pre-existingdatabases). This schema has an almost unlimited set of relations (due tosurface forms), and supports integration with existing structured data (throughthe relation types of existing databases). To populate a database of suchschema we present a family of matrix factorization models that predict affinitybetween database tuples and relations. We show that this achieves substantiallyhigher accuracy than the traditional classification approach. More importantly,by operating simultaneously on relations observed in text and in pre-existingstructured DBs such as Freebase, we are able to reason about unstructured andstructured data in mutually-supporting ways. By doing so our approachoutperforms state-of-the-art distant supervision systems.
arxiv-2700-76 | Efficient Sample Reuse in Policy Gradients with Parameter-based Exploration | http://arxiv.org/pdf/1301.3966v1.pdf | author:Tingting Zhao, Hirotaka Hachiya, Voot Tangkaratt, Jun Morimoto, Masashi Sugiyama category:cs.LG stat.ML published:2013-01-17 summary:The policy gradient approach is a flexible and powerful reinforcementlearning method particularly for problems with continuous actions such as robotcontrol. A common challenge in this scenario is how to reduce the variance ofpolicy gradient estimates for reliable policy updates. In this paper, wecombine the following three ideas and give a highly effective policy gradientmethod: (a) the policy gradients with parameter based exploration, which is arecently proposed policy search method with low variance of gradient estimates,(b) an importance sampling technique, which allows us to reuse previouslygathered data in a consistent way, and (c) an optimal baseline, which minimizesthe variance of gradient estimates with their unbiasedness being maintained.For the proposed method, we give theoretical analysis of the variance ofgradient estimates and show its usefulness through extensive experiments.
arxiv-2700-77 | Financial Portfolio Optimization: Computationally guided agents to investigate, analyse and invest!? | http://arxiv.org/pdf/1301.4194v1.pdf | author:Ankit Dangi category:q-fin.PM cs.CE cs.NE q-fin.CP stat.ML published:2013-01-17 summary:Financial portfolio optimization is a widely studied problem in mathematics,statistics, financial and computational literature. It adheres to determiningan optimal combination of weights associated with financial assets held in aportfolio. In practice, it faces challenges by virtue of varying math.formulations, parameters, business constraints and complex financialinstruments. Empirical nature of data is no longer one-sided; therebyreflecting upside and downside trends with repeated yet unidentifiable cyclicbehaviours potentially caused due to high frequency volatile movements in assettrades. Portfolio optimization under such circumstances is theoretically andcomputationally challenging. This work presents a novel mechanism to reach anoptimal solution by encoding a variety of optimal solutions in a solution bankto guide the search process for the global investment objective formulation. Itconceptualizes the role of individual solver agents that contribute optimalsolutions to a bank of solutions, a super-agent solver that learns from thesolution bank, and, thus reflects a knowledge-based computationally guidedagents approach to investigate, analyse and reach to optimal solution forinformed investment decisions. Conceptual understanding of classes of solver agents that represent varyingproblem formulations and, mathematically oriented deterministic solvers alongwith stochastic-search driven evolutionary and swarm-intelligence basedtechniques for optimal weights are discussed. Algorithmic implementation ispresented by an enhanced neighbourhood generation mechanism in SimulatedAnnealing algorithm. A framework for inclusion of heuristic knowledge and humanexpertise from financial literature related to investment decision makingprocess is reflected via introduction of controlled perturbation strategiesusing a decision matrix for neighbourhood generation.
arxiv-2700-78 | Multiscale Discriminant Saliency for Visual Attention | http://arxiv.org/pdf/1301.3964v1.pdf | author:Anh Cat Le Ngo, Kenneth Ang Li-Minn, Guoping Qiu, Jasmine Seng Kah-Phooi category:cs.CV published:2013-01-17 summary:The bottom-up saliency, an early stage of humans' visual attention, can beconsidered as a binary classification problem between center and surroundclasses. Discriminant power of features for the classification is measured asmutual information between features and two classes distribution. The estimateddiscrepancy of two feature classes very much depends on considered scalelevels; then, multi-scale structure and discriminant power are integrated byemploying discrete wavelet features and Hidden markov tree (HMT). With waveletcoefficients and Hidden Markov Tree parameters, quad-tree like label structuresare constructed and utilized in maximum a posterior probability (MAP) of hiddenclass variables at corresponding dyadic sub-squares. Then, saliency value foreach dyadic square at each scale level is computed with discriminant powerprinciple and the MAP. Finally, across multiple scales is integrated the finalsaliency map by an information maximization rule. Both standard quantitativetools such as NSS, LCC, AUC and qualitative assessments are used for evaluatingthe proposed multiscale discriminant saliency method (MDIS) against thewell-know information-based saliency method AIM on its Bruce Database wityeye-tracking data. Simulation results are presented and analyzed to verify thevalidity of MDIS as well as point out its disadvantages for further researchdirection.
arxiv-2700-79 | Evolutionary Algorithms and Dynamic Programming | http://arxiv.org/pdf/1301.4096v1.pdf | author:Benjamin Doerr, Anton Eremeev, Frank Neumann, Madeleine Theile, Christian Thyssen category:cs.NE cs.DS published:2013-01-17 summary:Recently, it has been proven that evolutionary algorithms produce goodresults for a wide range of combinatorial optimization problems. Some of theconsidered problems are tackled by evolutionary algorithms that use arepresentation which enables them to construct solutions in a dynamicprogramming fashion. We take a general approach and relate the construction ofsuch algorithms to the development of algorithms using dynamic programmingtechniques. Thereby, we give general guidelines on how to develop evolutionaryalgorithms that have the additional ability of carrying out dynamic programmingsteps. Finally, we show that for a wide class of the so-called DP-benevolentproblems (which are known to admit FPTAS) there exists a fully polynomial-timerandomized approximation scheme based on an evolutionary algorithm.
arxiv-2700-80 | Non-parametric Bayesian modelling of digital gene expression data | http://arxiv.org/pdf/1301.4144v1.pdf | author:Dimitrios V. Vavoulis, Julian Gough category:q-bio.QM q-bio.GN stat.AP stat.ML published:2013-01-17 summary:Next-generation sequencing technologies provide a revolutionary tool forgenerating gene expression data. Starting with a fixed RNA sample, theyconstruct a library of millions of differentially abundant short sequence tagsor "reads", which constitute a fundamentally discrete measure of the level ofgene expression. A common limitation in experiments using these technologies isthe low number or even absence of biological replicates, which complicates thestatistical analysis of digital gene expression data. Analysis of this type ofdata has often been based on modified tests originally devised for analysingmicroarrays; both these and even de novo methods for the analysis of RNA-seqdata are plagued by the common problem of low replication. We propose a novel,non-parametric Bayesian approach for the analysis of digital gene expressiondata. We begin with a hierarchical model for modelling over-dispersed countdata and a blocked Gibbs sampling algorithm for inferring the posteriordistribution of model parameters conditional on these counts. The algorithmcompensates for the problem of low numbers of biological replicates byclustering together genes with tag counts that are likely sampled from a commondistribution and using this augmented sample for estimating the parameters ofthis distribution. The number of clusters is not decided a priori, but it isinferred along with the remaining model parameters. We demonstrate the abilityof this approach to model biological data with high fidelity by applying thealgorithm on a public dataset obtained from cancerous and non-cancerous neuraltissues.
arxiv-2700-81 | On the Product Rule for Classification Problems | http://arxiv.org/pdf/1301.4157v1.pdf | author:Marcelo Cicconet category:cs.LG cs.CV stat.ML published:2013-01-17 summary:We discuss theoretical aspects of the product rule for classificationproblems in supervised machine learning for the case of combining classifiers.We show that (1) the product rule arises from the MAP classifier supposingequivalent priors and conditional independence given a class; (2) under someconditions, the product rule is equivalent to minimizing the sum of the squareddistances to the respective centers of the classes related with differentfeatures, such distances being weighted by the spread of the classes; (3)observing some hypothesis, the product rule is equivalent to concatenating thevectors of features.
arxiv-2700-82 | Affinity Weighted Embedding | http://arxiv.org/pdf/1301.4171v1.pdf | author:Jason Weston, Ron Weiss, Hector Yee category:cs.IR cs.LG stat.ML published:2013-01-17 summary:Supervised (linear) embedding models like Wsabie and PSI have provensuccessful at ranking, recommendation and annotation tasks. However, despitebeing scalable to large datasets they do not take full advantage of the extradata due to their linear nature, and typically underfit. We propose a new classof models which aim to provide improved performance while retaining many of thebenefits of the existing class of embedding models. Our new approach works byiteratively learning a linear embedding model where the next iteration'sfeatures and labels are reweighted as a function of the previous iteration. Wedescribe several variants of the family, and give some initial results.
arxiv-2700-83 | On Graphical Models via Univariate Exponential Family Distributions | http://arxiv.org/pdf/1301.4183v2.pdf | author:Eunho Yang, Pradeep Ravikumar, Genevera I. Allen, Zhandong Liu category:math.ST stat.ML stat.TH published:2013-01-17 summary:Undirected graphical models, or Markov networks, are a popular class ofstatistical models, used in a wide variety of applications. Popular instancesof this class include Gaussian graphical models and Ising models. In manysettings, however, it might not be clear which subclass of graphical models touse, particularly for non-Gaussian and non-categorical data. In this paper, weconsider a general sub-class of graphical models where the node-wiseconditional distributions arise from exponential families. This allows us toderive multivariate graphical model distributions from univariate exponentialfamily distributions, such as the Poisson, negative binomial, and exponentialdistributions. Our key contributions include a class of M-estimators to fitthese graphical model distributions; and rigorous statistical analysis showingthat these M-estimators recover the true graphical model structure exactly,with high probability. We provide examples of genomic and proteomic networkslearned via instances of our class of graphical models derived from Poisson andexponential distributions.
arxiv-2700-84 | Hypothesis Testing in High-Dimensional Regression under the Gaussian Random Design Model: Asymptotic Theory | http://arxiv.org/pdf/1301.4240v3.pdf | author:Adel Javanmard, Andrea Montanari category:stat.ME cs.IT math.IT math.ST stat.ML stat.TH published:2013-01-17 summary:We consider linear regression in the high-dimensional regime where the numberof observations $n$ is smaller than the number of parameters $p$. A verysuccessful approach in this setting uses $\ell_1$-penalized least squares(a.k.a. the Lasso) to search for a subset of $s_0< n$ parameters that bestexplain the data, while setting the other parameters to zero. Considerableamount of work has been devoted to characterizing the estimation and modelselection problems within this approach. In this paper we consider instead the fundamental, but far less understood,question of \emph{statistical significance}. More precisely, we address theproblem of computing p-values for single regression coefficients. On one hand, we develop a general upper bound on the minimax power of testswith a given significance level. On the other, we prove that this upper boundis (nearly) achievable through a practical procedure in the case of randomdesign matrices with independent entries. Our approach is based on a debiasingof the Lasso estimator. The analysis builds on a rigorous characterization ofthe asymptotic distribution of the Lasso estimator and its debiased version.Our result holds for optimal sample size, i.e., when $n$ is at least on theorder of $s_0 \log(p/s_0)$. We generalize our approach to random design matrices with i.i.d. Gaussianrows $x_i\sim N(0,\Sigma)$. In this case we prove that a similar distributionalcharacterization (termed `standard distributional limit') holds for $n$ muchlarger than $s_0(\log p)^2$. Finally, we show that for optimal sample size, $n$ being at least of order$s_0 \log(p/s_0)$, the standard distributional limit for general Gaussiandesigns can be derived from the replica heuristics in statistical physics.
arxiv-2700-85 | Herded Gibbs Sampling | http://arxiv.org/pdf/1301.4168v2.pdf | author:Luke Bornn, Yutian Chen, Nando de Freitas, Mareija Eskelin, Jing Fang, Max Welling category:cs.LG stat.CO stat.ML published:2013-01-17 summary:The Gibbs sampler is one of the most popular algorithms for inference instatistical models. In this paper, we introduce a herding variant of thisalgorithm, called herded Gibbs, that is entirely deterministic. We prove thatherded Gibbs has an $O(1/T)$ convergence rate for models with independentvariables and for fully connected probabilistic graphical models. Herded Gibbsis shown to outperform Gibbs in the tasks of image denoising with MRFs andnamed entity recognition with CRFs. However, the convergence for herded Gibbsfor sparsely connected probabilistic graphical models is still an open problem.
arxiv-2700-86 | Knowledge Matters: Importance of Prior Information for Optimization | http://arxiv.org/pdf/1301.4083v6.pdf | author:Çağlar Gülçehre, Yoshua Bengio category:cs.LG cs.CV cs.NE stat.ML published:2013-01-17 summary:We explore the effect of introducing prior information into the intermediatelevel of neural networks for a learning task on which all the state-of-the-artmachine learning algorithms tested failed to learn. We motivate our work fromthe hypothesis that humans learn such intermediate concepts from otherindividuals via a form of supervision or guidance using a curriculum. Theexperiments we have conducted provide positive evidence in favor of thishypothesis. In our experiments, a two-tiered MLP architecture is trained on adataset with 64x64 binary inputs images, each image with three sprites. Thefinal task is to decide whether all the sprites are the same or one of them isdifferent. Sprites are pentomino tetris shapes and they are placed in an imagewith different locations using scaling and rotation transformations. The firstpart of the two-tiered MLP is pre-trained with intermediate-level targets beingthe presence of sprites at each location, while the second part takes theoutput of the first part as input and predicts the final task's target binaryevent. The two-tiered MLP architecture, with a few tens of thousand examples,was able to learn the task perfectly, whereas all other algorithms (includeunsupervised pre-training, but also traditional algorithms like SVMs, decisiontrees and boosting) all perform no better than chance. We hypothesize that theoptimization difficulty involved when the intermediate pre-training is notperformed is due to the {\em composition} of two highly non-linear tasks. Ourfindings are also consistent with hypotheses on cultural learning inspired bythe observations of optimization problems with deep learning, presumablybecause of effective local minima.
arxiv-2700-87 | Combining Feature and Prototype Pruning by Uncertainty Minimization | http://arxiv.org/pdf/1301.3891v1.pdf | author:Marc Sebban, Richard Nock category:cs.LG stat.ML published:2013-01-16 summary:We focus in this paper on dataset reduction techniques for use in k-nearestneighbor classification. In such a context, feature and prototype selectionshave always been independently treated by the standard storage reductionalgorithms. While this certifying is theoretically justified by the fact thateach subproblem is NP-hard, we assume in this paper that a joint storagereduction is in fact more intuitive and can in practice provide better resultsthan two independent processes. Moreover, it avoids a lot of distancecalculations by progressively removing useless instances during the featurepruning. While standard selection algorithms often optimize the accuracy todiscriminate the set of solutions, we use in this paper a criterion based on anuncertainty measure within a nearest-neighbor graph. This choice comes fromrecent results that have proven that accuracy is not always the suitablecriterion to optimize. In our approach, a feature or an instance is removed ifits deletion improves information of the graph. Numerous experiments arepresented in this paper and a statistical analysis shows the relevance of ourapproach, and its tolerance in the presence of noise.
arxiv-2700-88 | Gradient Driven Learning for Pooling in Visual Pipeline Feature Extraction Models | http://arxiv.org/pdf/1301.3755v1.pdf | author:Derek Rose, Itamar Arel category:cs.CV published:2013-01-16 summary:Hyper-parameter selection remains a daunting task when building a patternrecognition architecture which performs well, particularly in recentlyconstructed visual pipeline models for feature extraction. We re-formulatepooling in an existing pipeline as a function of adjustable pooling map weightparameters and propose the use of supervised error signals from gradientdescent to tune the established maps within the model. This technique allows usto learn what would otherwise be a design choice within the model andspecialize the maps to aggregate areas of invariance for the task presented.Preliminary results show moderate potential gains in classification accuracyand highlight areas of importance within the intermediate featurerepresentation space.
arxiv-2700-89 | Monte Carlo Inference via Greedy Importance Sampling | http://arxiv.org/pdf/1301.3890v1.pdf | author:Dale Schuurmans, Finnegan Southey category:cs.LG stat.CO stat.ML published:2013-01-16 summary:We present a new method for conducting Monte Carlo inference in graphicalmodels which combines explicit search with generalized importance sampling. Theidea is to reduce the variance of importance sampling by searching forsignificant points in the target distribution. We prove that it is possible tointroduce search and still maintain unbiasedness. We then demonstrate ourprocedure on a few simple inference tasks and show that it can improve theinference quality of standard MCMC methods, including Gibbs sampling,Metropolis sampling, and Hybrid Monte Carlo. This paper extends previous workwhich showed how greedy importance sampling could be correctly realized in theone-dimensional case.
arxiv-2700-90 | Adaptive Importance Sampling for Estimation in Structured Domains | http://arxiv.org/pdf/1301.3882v1.pdf | author:Luis E. Ortiz, Leslie Pack Kaelbling category:cs.AI cs.LG stat.ML published:2013-01-16 summary:Sampling is an important tool for estimating large, complex sums andintegrals over high dimensional spaces. For instance, important sampling hasbeen used as an alternative to exact methods for inference in belief networks.Ideally, we want to have a sampling distribution that provides optimal-varianceestimators. In this paper, we present methods that improve the samplingdistribution by systematically adapting it as we obtain information from thesamples. We present a stochastic-gradient-descent method for sequentiallyupdating the sampling distribution based on the direct minization of thevariance. We also present other stochastic-gradient-descent methods based onthe minimizationof typical notions of distance between the current samplingdistribution and approximations of the target, optimal distribution. We finallyvalidate and compare the different methods empirically by applying them to theproblem of action evaluation in influence diagrams.
arxiv-2700-91 | Information Theoretic Learning with Infinitely Divisible Kernels | http://arxiv.org/pdf/1301.3551v6.pdf | author:Luis G. Sanchez Giraldo, Jose C. Principe category:cs.LG cs.CV published:2013-01-16 summary:In this paper, we develop a framework for information theoretic learningbased on infinitely divisible matrices. We formulate an entropy-like functionalon positive definite matrices based on Renyi's axiomatic definition of entropyand examine some key properties of this functional that lead to the concept ofinfinite divisibility. The proposed formulation avoids the plug in estimationof density and brings along the representation power of reproducing kernelHilbert spaces. As an application example, we derive a supervised metriclearning algorithm using a matrix based analogue to conditional entropyachieving results comparable with the state of the art.
arxiv-2700-92 | PEGASUS: A Policy Search Method for Large MDPs and POMDPs | http://arxiv.org/pdf/1301.3878v1.pdf | author:Andrew Y. Ng, Michael I. Jordan category:cs.AI cs.LG published:2013-01-16 summary:We propose a new approach to the problem of searching a space of policies fora Markov decision process (MDP) or a partially observable Markov decisionprocess (POMDP), given a model. Our approach is based on the followingobservation: Any (PO)MDP can be transformed into an "equivalent" POMDP in whichall state transitions (given the current state and action) are deterministic.This reduces the general problem of policy search to one in which we need onlyconsider POMDPs with deterministic transitions. We give a natural way ofestimating the value of all policies in these transformed POMDPs. Policy searchis then simply performed by searching for a policy with high estimated value.We also establish conditions under which our value estimates will be good,recovering theoretical results similar to those of Kearns, Mansour and Ng(1999), but with "sample complexity" bounds that have only a polynomial ratherthan exponential dependence on the horizon time. Our method applies toarbitrary POMDPs, including ones with infinite state and action spaces. We alsopresent empirical results for our approach on a small discrete problem, and ona complex continuous state/continuous action problem involving learning to ridea bicycle.
arxiv-2700-93 | The Anchors Hierachy: Using the triangle inequality to survive high dimensional data | http://arxiv.org/pdf/1301.3877v1.pdf | author:Andrew Moore category:cs.LG cs.DS stat.ML published:2013-01-16 summary:This paper is about metric data structures in high-dimensional ornon-Euclidean space that permit cached sufficient statistics accelerations oflearning algorithms. It has recently been shown that for less than about 10 dimensions, decoratingkd-trees with additional "cached sufficient statistics" such as first andsecond moments and contingency tables can provide satisfying acceleration for avery wide range of statistical learning tasks such as kernel regression,locally weighted regression, k-means clustering, mixture modeling and Bayes Netlearning. In this paper, we begin by defining the anchors hierarchy - a fast datastructure and algorithm for localizing data based only on atriangle-inequality-obeying distance metric. We show how this, in its ownright, gives a fast and effective clustering of data. But more importantly weshow how it can produce a well-balanced structure similar to a Ball-Tree(Omohundro, 1991) or a kind of metric tree (Uhlmann, 1991; Ciaccia, Patella, &Zezula, 1997) in a way that is neither "top-down" nor "bottom-up" but instead"middle-out". We then show how this structure, decorated with cached sufficientstatistics, allows a wide variety of statistical learning algorithms to beaccelerated even in thousands of dimensions.
arxiv-2700-94 | Tractable Bayesian Learning of Tree Belief Networks | http://arxiv.org/pdf/1301.3875v1.pdf | author:Marina Meila, Tommi S. Jaakkola category:cs.LG cs.AI stat.ML published:2013-01-16 summary:In this paper we present decomposable priors, a family of priors overstructure and parameters of tree belief nets for which Bayesian learning withcomplete observations is tractable, in the sense that the posterior is alsodecomposable and can be completely determined analytically in polynomial time.This follows from two main results: First, we show that factored distributionsover spanning trees in a graph can be integrated in closed form. Second, weexamine priors over tree parameters and show that a set of assumptions similarto (Heckerman and al. 1995) constrain the tree parameter priors to be acompactly parameterized product of Dirichlet distributions. Beside allowing forexact Bayesian learning, these results permit us to formulate a new class oftractable latent variable models in which the likelihood of a data point iscomputed through an ensemble average over tree structures.
arxiv-2700-95 | Convex Variational Image Restoration with Histogram Priors | http://arxiv.org/pdf/1301.3683v2.pdf | author:Paul Swoboda, Christoph Schnörr category:math.OC cs.CV G.1.6; I.4.4 published:2013-01-16 summary:We present a novel variational approach to image restoration (e.g.,denoising, inpainting, labeling) that enables to complement establishedvariational approaches with a histogram-based prior enforcing closeness of thesolution to some given empirical measure. By minimizing a single objectivefunction, the approach utilizes simultaneously two quite different sources ofinformation for restoration: spatial context in terms of some smoothness priorand non-spatial statistics in terms of the novel prior utilizing theWasserstein distance between probability measures. We study the combination ofthe functional lifting technique with two different relaxations of thehistogram prior and derive a jointly convex variational approach. Mathematicalequivalence of both relaxations is established and cases where optimality holdsare discussed. Additionally, we present an efficient algorithmic scheme for thenumerical treatment of the presented model. Experiments using the basictotal-variation based denoising approach as a case study demonstrate our novelregularization approach.
arxiv-2700-96 | Feature Selection and Dualities in Maximum Entropy Discrimination | http://arxiv.org/pdf/1301.3865v1.pdf | author:Tony S. Jebara, Tommi S. Jaakkola category:cs.LG stat.ML published:2013-01-16 summary:Incorporating feature selection into a classification or regression methodoften carries a number of advantages. In this paper we formalize featureselection specifically from a discriminative perspective of improvingclassification/regression accuracy. The feature selection method is developedas an extension to the recently proposed maximum entropy discrimination (MED)framework. We describe MED as a flexible (Bayesian) regularization approachthat subsumes, e.g., support vector classification, regression and exponentialfamily models. For brevity, we restrict ourselves primarily to featureselection in the context of linear classification/regression methods anddemonstrate that the proposed approach indeed carries substantial improvementsin practice. Moreover, we discuss and develop various extensions of featureselection, including the problem of dealing with example specific butunobserved degrees of freedom -- alignments or invariants.
arxiv-2700-97 | Dependency Networks for Collaborative Filtering and Data Visualization | http://arxiv.org/pdf/1301.3862v1.pdf | author:David Heckerman, David Maxwell Chickering, Christopher Meek, Robert Rounthwaite, Carl Kadie category:cs.AI cs.IR cs.LG published:2013-01-16 summary:We describe a graphical model for probabilistic relationships---analternative to the Bayesian network---called a dependency network. The graph ofa dependency network, unlike a Bayesian network, is potentially cyclic. Theprobability component of a dependency network, like a Bayesian network, is aset of conditional distributions, one for each node given its parents. Weidentify several basic properties of this representation and describe acomputationally efficient procedure for learning the graph and probabilitycomponents from data. We describe the application of this representation toprobabilistic inference, collaborative filtering (the task of predictingpreferences), and the visualization of acausal predictive relationships.
arxiv-2700-98 | Inference for Belief Networks Using Coupling From the Past | http://arxiv.org/pdf/1301.3861v1.pdf | author:Michael Harvey, Radford M. Neal category:cs.AI cs.LG published:2013-01-16 summary:Inference for belief networks using Gibbs sampling produces a distributionfor unobserved variables that differs from the correct distribution by a(usually) unknown error, since convergence to the right distribution occursonly asymptotically. The method of "coupling from the past" samples fromexactly the correct distribution by (conceptually) running dependent Gibbssampling simulations from every possible starting state from a time far enoughin the past that all runs reach the same state at time t=0. Explicitlyconsidering every possible state is intractable for large networks, however. Wepropose a method for layered noisy-or networks that uses a compact, but oftenimprecise, summary of a set of states. This method samples from exactly thecorrect distribution, and requires only about twice the time per step asordinary Gibbs sampling, but it may require more simulation steps than would beneeded if chains were tracked exactly.
arxiv-2700-99 | Gaussian Process Networks | http://arxiv.org/pdf/1301.3857v1.pdf | author:Nir Friedman, Iftach Nachman category:cs.AI cs.LG stat.ML published:2013-01-16 summary:In this paper we address the problem of learning the structure of a Bayesiannetwork in domains with continuous variables. This task requires a procedurefor comparing different candidate structures. In the Bayesian framework, thisis done by evaluating the {em marginal likelihood/} of the data given acandidate structure. This term can be computed in closed-form for standardparametric families (e.g., Gaussians), and can be approximated, at somecomputational cost, for some semi-parametric families (e.g., mixtures ofGaussians). We present a new family of continuous variable probabilistic networks thatare based on {em Gaussian Process/} priors. These priors are semi-parametric innature and can learn almost arbitrary noisy functional relations. Using thesepriors, we can directly compute marginal likelihoods for structure learning.The resulting method can discover a wide range of functional dependencies inmultivariate data. We develop the Bayesian score of Gaussian Process Networksand describe how to learn them from data. We present empirical results onartificial data as well as on real-life domains with non-linear dependencies.
arxiv-2700-100 | Being Bayesian about Network Structure | http://arxiv.org/pdf/1301.3856v1.pdf | author:Nir Friedman, Daphne Koller category:cs.LG cs.AI stat.ML published:2013-01-16 summary:In many domains, we are interested in analyzing the structure of theunderlying distribution, e.g., whether one variable is a direct parent of theother. Bayesian model-selection attempts to find the MAP model and use itsstructure to answer these questions. However, when the amount of available datais modest, there might be many models that have non-negligible posterior. Thus,we want compute the Bayesian posterior of a feature, i.e., the total posteriorprobability of all models that contain it. In this paper, we propose a newapproach for this task. We first show how to efficiently compute a sum over theexponential number of networks that are consistent with a fixed ordering overnetwork variables. This allows us to compute, for a given ordering, both themarginal probability of the data and the posterior of a feature. We then usethis result as the basis for an algorithm that approximates the Bayesianposterior of a feature. Our approach uses a Markov Chain Monte Carlo (MCMC)method, but over orderings rather than over network structures. The space oforderings is much smaller and more regular than the space of structures, andhas a smoother posterior `landscape'. We present empirical results on syntheticand real-life datasets that compare our approach to full model averaging (whenpossible), to MCMC over network structures, and to a non-Bayesian bootstrapapproach.
arxiv-2700-101 | Learning Graphical Models of Images, Videos and Their Spatial Transformations | http://arxiv.org/pdf/1301.3854v1.pdf | author:Brendan J. Frey, Nebojsa Jojic category:cs.CV cs.LG stat.ML published:2013-01-16 summary:Mixtures of Gaussians, factor analyzers (probabilistic PCA) and hidden Markovmodels are staples of static and dynamic data modeling and image and videomodeling in particular. We show how topographic transformations in the input,such as translation and shearing in images, can be accounted for in thesemodels by including a discrete transformation variable. The resulting modelsperform clustering, dimensionality reduction and time-series analysis in a waythat is invariant to transformations in the input. Using the EM algorithm,these transformation-invariant models can be fit to static data and timeseries. We give results on filtering microscopy images, face and facial poseclustering, handwritten digit modeling and recognition, video clustering,object tracking, and removal of distractions from video sequences.
arxiv-2700-102 | Rao-Blackwellised Particle Filtering for Dynamic Bayesian Networks | http://arxiv.org/pdf/1301.3853v1.pdf | author:Arnaud Doucet, Nando de Freitas, Kevin Murphy, Stuart Russell category:cs.LG cs.AI stat.CO published:2013-01-16 summary:Particle filters (PFs) are powerful sampling-based inference/learningalgorithms for dynamic Bayesian networks (DBNs). They allow us to treat, in aprincipled way, any type of probability distribution, nonlinearity andnon-stationarity. They have appeared in several fields under such names as"condensation", "sequential Monte Carlo" and "survival of the fittest". In thispaper, we show how we can exploit the structure of the DBN to increase theefficiency of particle filtering, using a technique known asRao-Blackwellisation. Essentially, this samples some of the variables, andmarginalizes out the rest exactly, using the Kalman filter, HMM filter,junction tree algorithm, or any other finite dimensional optimal filter. Weshow that Rao-Blackwellised particle filters (RBPFs) lead to more accurateestimates than standard PFs. We demonstrate RBPFs on two problems, namelynon-stationary online regression with radial basis function networks and robotlocalization and map building. We also discuss other potential applicationareas and provide references to some finite dimensional optimal filters.
arxiv-2700-103 | Mix-nets: Factored Mixtures of Gaussians in Bayesian Networks With Mixed Continuous And Discrete Variables | http://arxiv.org/pdf/1301.3852v1.pdf | author:Scott Davies, Andrew Moore category:cs.LG cs.AI stat.ML published:2013-01-16 summary:Recently developed techniques have made it possible to quickly learn accurateprobability density functions from data in low-dimensional continuous space. Inparticular, mixtures of Gaussians can be fitted to data very quickly using anaccelerated EM algorithm that employs multiresolution kd-trees (Moore, 1999).In this paper, we propose a kind of Bayesian networks in which low-dimensionalmixtures of Gaussians over different subsets of the domain's variables arecombined into a coherent joint probability model over the entire domain. Thenetwork is also capable of modeling complex dependencies between discretevariables and continuous variables without requiring discretization of thecontinuous variables. We present efficient heuristic algorithms forautomatically learning these networks from data, and perform comparativeexperiments illustrated how well these networks model real scientific data andsynthetic data. We also briefly discuss some possible improvements to thenetworks, as well as possible applications.
arxiv-2700-104 | Minimum Message Length Clustering Using Gibbs Sampling | http://arxiv.org/pdf/1301.3851v1.pdf | author:Ian Davidson category:cs.LG stat.ML published:2013-01-16 summary:The K-Mean and EM algorithms are popular in clustering and mixture modeling,due to their simplicity and ease of implementation. However, they have severalsignificant limitations. Both coverage to a local optimum of their respectiveobjective functions (ignoring the uncertainty in the model space), require theapriori specification of the number of classes/clsuters, and are inconsistent.In this work we overcome these limitations by using the Minimum Message Length(MML) principle and a variation to the K-Means/EM observation assignment andparameter calculation scheme. We maintain the simplicity of these approacheswhile constructing a Bayesian mixture modeling tool that samples/searches themodel space using a Markov Chain Monte Carlo (MCMC) sampler known as a Gibbssampler. Gibbs sampling allows us to visit each model according to itsposterior probability. Therefore, if the model space is multi-modal we willvisit all models and not get stuck in local optima. We call our approachmultiple chains at equilibrium (MCE) MML sampling.
arxiv-2700-105 | A Two-round Variant of EM for Gaussian Mixtures | http://arxiv.org/pdf/1301.3850v1.pdf | author:Sanjoy Dasgupta, Leonard Schulman category:cs.LG stat.ML published:2013-01-16 summary:Given a set of possible models (e.g., Bayesian network structures) and a datasample, in the unsupervised model selection problem the task is to choose themost accurate model with respect to the domain joint probability distribution.In contrast to this, in supervised model selection it is a priori known thatthe chosen model will be used in the future for prediction tasks involving more``focused' predictive distributions. Although focused predictive distributionscan be produced from the joint probability distribution by marginalization, inpractice the best model in the unsupervised sense does not necessarily performwell in supervised domains. In particular, the standard marginal likelihoodscore is a criterion for the unsupervised task, and, although frequently usedfor supervised model selection also, does not perform well in such tasks. Inthis paper we study the performance of the marginal likelihood scoreempirically in supervised Bayesian network selection tasks by using a largenumber of publicly available classification data sets, and compare the resultsto those obtained by alternative model selection criteria, including empiricalcrossvalidation methods, an approximation of a supervised marginal likelihoodmeasure, and a supervised version of Dawids prequential(predictive sequential)principle.The results demonstrate that the marginal likelihood score does NOTperform well FOR supervised model selection, WHILE the best results areobtained BY using Dawids prequential r napproach.
arxiv-2700-106 | Experiments with Random Projection | http://arxiv.org/pdf/1301.3849v1.pdf | author:Sanjoy Dasgupta category:cs.LG stat.ML published:2013-01-16 summary:Recent theoretical work has identified random projection as a promisingdimensionality reduction technique for learning mixtures of Gausians. Here wesummarize these results and illustrate them by a wide variety of experiments onsynthetic and real data.
arxiv-2700-107 | Bayesian Classification and Feature Selection from Finite Data Sets | http://arxiv.org/pdf/1301.3843v1.pdf | author:Frans Coetzee, Steve Lawrence, C. Lee Giles category:cs.LG stat.ML published:2013-01-16 summary:Feature selection aims to select the smallest subset of features for aspecified level of performance. The optimal achievable classificationperformance on a feature subset is summarized by its Receiver Operating Curve(ROC). When infinite data is available, the Neyman- Pearson (NP) designprocedure provides the most efficient way of obtaining this curve. In practicethe design procedure is applied to density estimates from finite data sets. Weperform a detailed statistical analysis of the resulting error propagation onfinite alphabets. We show that the estimated performance curve (EPC) producedby the design procedure is arbitrarily accurate given sufficient data,independent of the size of the feature set. However, the underlying likelihoodranking procedure is highly sensitive to errors that reduces the probabilitythat the EPC is in fact the ROC. In the worst case, guaranteeing that the EPCis equal to the ROC may require data sizes exponential in the size of thefeature set. These results imply that in theory the NP design approach may onlybe valid for characterizing relatively small feature subsets, even when theperformance of any given classifier can be estimated very accurately. Wediscuss the practical limitations for on-line methods that ensures that the NPprocedure operates in a statistically valid region.
arxiv-2700-108 | Switched linear encoding with rectified linear autoencoders | http://arxiv.org/pdf/1301.3753v2.pdf | author:Leif Johnson, Craig Corcoran category:cs.LG published:2013-01-16 summary:Several recent results in machine learning have established formalconnections between autoencoders---artificial neural network models thatattempt to reproduce their inputs---and other coding models like sparse codingand K-means. This paper explores in depth an autoencoder model that isconstructed using rectified linear activations on its hidden units. Ouranalysis builds on recent results to further unify the world of sparse linearcoding models. We provide an intuitive interpretation of the behavior of thesecoding models and demonstrate this intuition using small, artificial datasetswith known distributions.
arxiv-2700-109 | Utilities as Random Variables: Density Estimation and Structure Discovery | http://arxiv.org/pdf/1301.3840v1.pdf | author:Urszula Chajewska, Daphne Koller category:cs.AI cs.LG published:2013-01-16 summary:Decision theory does not traditionally include uncertainty over utilityfunctions. We argue that the a person's utility value for a given outcome canbe treated as we treat other domain attributes: as a random variable with adensity function over its possible values. We show that we can applystatistical density estimation techniques to learn such a density function froma database of partially elicited utility functions. In particular, we define aBayesian learning framework for this problem, assuming the distribution overutilities is a mixture of Gaussians, where the mixture components representstatistically coherent subpopulations. We can also extend our techniques to theproblem of discovering generalized additivity structure in the utilityfunctions in the population. We define a Bayesian model selection criterion forutility function structure and a search procedure over structures. Thefactorization of the utilities in the learned model, and the generalizationobtained from density estimation, allows us to provide robust estimates ofutilities using a significantly smaller number of utility elicitationquestions. We experiment with our technique on synthetic utility data and on areal database of utility functions in the domain of prenatal diagnosis.
arxiv-2700-110 | Variational Relevance Vector Machines | http://arxiv.org/pdf/1301.3838v1.pdf | author:Christopher M. Bishop, Michael Tipping category:cs.LG stat.ML published:2013-01-16 summary:The Support Vector Machine (SVM) of Vapnik (1998) has become widelyestablished as one of the leading approaches to pattern recognition and machinelearning. It expresses predictions in terms of a linear combination of kernelfunctions centred on a subset of the training data, known as support vectors. Despite its widespread success, the SVM suffers from some importantlimitations, one of the most significant being that it makes point predictionsrather than generating predictive distributions. Recently Tipping (1999) hasformulated the Relevance Vector Machine (RVM), a probabilistic model whosefunctional form is equivalent to the SVM. It achieves comparable recognitionaccuracy to the SVM, yet provides a full predictive distribution, and alsorequires substantially fewer kernel functions. The original treatment of the RVM relied on the use of type II maximumlikelihood (the `evidence framework') to provide point estimates of thehyperparameters which govern model sparsity. In this paper we show how the RVMcan be formulated and solved within a completely Bayesian paradigm through theuse of variational inference, thereby giving a posterior distribution over bothparameters and hyperparameters. We demonstrate the practicality and performanceof the variational RVM using both synthetic and real world examples.
arxiv-2700-111 | Dynamic Bayesian Multinets | http://arxiv.org/pdf/1301.3837v1.pdf | author:Jeff A. Bilmes category:cs.LG cs.AI stat.ML published:2013-01-16 summary:In this work, dynamic Bayesian multinets are introduced where a Markov chainstate at time t determines conditional independence patterns between randomvariables lying within a local time window surrounding t. It is shown howinformation-theoretic criterion functions can be used to induce sparse,discriminative, and class-conditional network structures that yield an optimalapproximation to the class posterior probability, and therefore are useful forthe classification task. Using a new structure learning heuristic, theresulting models are tested on a medium-vocabulary isolated-word speechrecognition task. It is demonstrated that these discriminatively structureddynamic Bayesian multinets, when trained in a maximum likelihood setting usingEM, can outperform both HMMs and other dynamic Bayesian networks with a similarnumber of parameters.
arxiv-2700-112 | Jitter-Adaptive Dictionary Learning - Application to Multi-Trial Neuroelectric Signals | http://arxiv.org/pdf/1301.3611v4.pdf | author:Sebastian Hitziger, Maureen Clerc, Alexandre Gramfort, Sandrine Saillet, Christian Bénar, Théodore Papadopoulo category:stat.ML 62-07 published:2013-01-16 summary:Dictionary Learning has proven to be a powerful tool for many imageprocessing tasks, where atoms are typically defined on small image patches. Asa drawback, the dictionary only encodes basic structures. In addition, thisapproach treats patches of different locations in one single set, which means aloss of information when features are well-aligned across signals. This is thecase, for instance, in multi-trial magneto- or electroencephalography (M/EEG).Learning the dictionary on the entire signals could make use of the alignementand reveal higher-level features. In this case, however, small missalignementsor phase variations of features would not be compensated for. In this paper, wepropose an extension to the common dictionary learning framework to overcomethese limitations by allowing atoms to adapt their position across signals. Themethod is validated on simulated and real neuroelectric data.
arxiv-2700-113 | Reversible Jump MCMC Simulated Annealing for Neural Networks | http://arxiv.org/pdf/1301.3833v1.pdf | author:Christophe Andrieu, Nando de Freitas, Arnaud Doucet category:cs.LG cs.NE stat.ML published:2013-01-16 summary:We propose a novel reversible jump Markov chain Monte Carlo (MCMC) simulatedannealing algorithm to optimize radial basis function (RBF) networks. Thisalgorithm enables us to maximize the joint posterior distribution of thenetwork parameters and the number of basis functions. It performs a globalsearch in the joint space of the parameters and number of parameters, therebysurmounting the problem of local minima. We also show that by calibrating aBayesian model, we can obtain the classical AIC, BIC and MDL model selectioncriteria within a penalized likelihood framework. Finally, we showtheoretically and empirically that the algorithm converges to the modes of thefull posterior distribution in an efficient way.
arxiv-2700-114 | Joint Space Neural Probabilistic Language Model for Statistical Machine Translation | http://arxiv.org/pdf/1301.3614v2.pdf | author:Tsuyoshi Okita category:cs.CL published:2013-01-16 summary:A neural probabilistic language model (NPLM) provides an idea to achieve thebetter perplexity than n-gram language model and their smoothed languagemodels. This paper investigates application area in bilingual NLP, specificallyStatistical Machine Translation (SMT). We focus on the perspectives that NPLMhas potential to open the possibility to complement potentially `huge'monolingual resources into the `resource-constraint' bilingual resources. Weintroduce an ngram-HMM language model as NPLM using the non-parametric Bayesianconstruction. In order to facilitate the application to various tasks, wepropose the joint space model of ngram-HMM language model. We show anexperiment of system combination in the area of SMT. One discovery was that ourtreatment of noise improved the results 0.20 BLEU points if NPLM is trained inrelatively small corpus, in our case 500,000 sentence pairs, which is often thecase due to the long training time of NPLM.
arxiv-2700-115 | Dynamic Trees: A Structured Variational Method Giving Efficient Propagation Rules | http://arxiv.org/pdf/1301.3895v1.pdf | author:Amos J. Storkey category:cs.LG cs.AI stat.ML published:2013-01-16 summary:Dynamic trees are mixtures of tree structured belief networks. They solvesome of the problems of fixed tree networks at the cost of making exactinference intractable. For this reason approximate methods such as sampling ormean field approaches have been used. However, mean field approximations assumea factorized distribution over node states. Such a distribution seems unlickelyin the posterior, as nodes are highly correlated in the prior. Here astructured variational approach is used, where the posterior distribution overthe non-evidential nodes is itself approximated by a dynamic tree. It turns outthat this form can be used tractably and efficiently. The result is a set ofupdate rules which can propagate information through the network to obtain botha full variational approximation, and the relevant marginals. The progagtionrules are more efficient than the mean field approach and give noticeablequantitative and qualitative improvement in the inference. The marginalscalculated give better approximations to the posterior than loopy propagationon a small toy problem.
arxiv-2700-116 | Regularized Discriminant Embedding for Visual Descriptor Learning | http://arxiv.org/pdf/1301.3644v1.pdf | author:Kye-Hyeon Kim, Rui Cai, Lei Zhang, Seungjin Choi category:cs.CV cs.LG published:2013-01-16 summary:Images can vary according to changes in viewpoint, resolution, noise, andillumination. In this paper, we aim to learn representations for an image,which are robust to wide changes in such environmental conditions, usingtraining pairs of matching and non-matching local image patches that arecollected under various environmental conditions. We present a regularizeddiscriminant analysis that emphasizes two challenging categories among thegiven training pairs: (1) matching, but far apart pairs and (2) non-matching,but close pairs in the original feature space (e.g., SIFT feature space).Compared to existing work on metric learning and discriminant analysis, ourmethod can better distinguish relevant images from irrelevant, but look-alikeimages.
arxiv-2700-117 | Tree structured sparse coding on cubes | http://arxiv.org/pdf/1301.3590v1.pdf | author:Arthur Szlam category:cs.IT cs.CV math.IT published:2013-01-16 summary:A brief description of tree structured sparse coding on the binary cube.
arxiv-2700-118 | Efficient Estimation of Word Representations in Vector Space | http://arxiv.org/pdf/1301.3781v3.pdf | author:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean category:cs.CL published:2013-01-16 summary:We propose two novel model architectures for computing continuous vectorrepresentations of words from very large data sets. The quality of theserepresentations is measured in a word similarity task, and the results arecompared to the previously best performing techniques based on different typesof neural networks. We observe large improvements in accuracy at much lowercomputational cost, i.e. it takes less than a day to learn high quality wordvectors from a 1.6 billion words data set. Furthermore, we show that thesevectors provide state-of-the-art performance on our test set for measuringsyntactic and semantic word similarities.
arxiv-2700-119 | Kernelized Locality-Sensitive Hashing for Semi-Supervised Agglomerative Clustering | http://arxiv.org/pdf/1301.3575v1.pdf | author:Boyi Xie, Shuheng Zheng category:cs.LG cs.CV stat.ML published:2013-01-16 summary:Large scale agglomerative clustering is hindered by computational burdens. Wepropose a novel scheme where exact inter-instance distance calculation isreplaced by the Hamming distance between Kernelized Locality-Sensitive Hashing(KLSH) hashed values. This results in a method that drastically decreasescomputation time. Additionally, we take advantage of certain labeled datapoints via distance metric learning to achieve a competitive precision andrecall comparing to K-Means but in much less computation time.
arxiv-2700-120 | A Nested HDP for Hierarchical Topic Models | http://arxiv.org/pdf/1301.3570v1.pdf | author:John Paisley, Chong Wang, David Blei, Michael I. Jordan category:stat.ML published:2013-01-16 summary:We develop a nested hierarchical Dirichlet process (nHDP) for hierarchicaltopic modeling. The nHDP is a generalization of the nested Chinese restaurantprocess (nCRP) that allows each word to follow its own path to a topic nodeaccording to a document-specific distribution on a shared tree. This alleviatesthe rigid, single-path formulation of the nCRP, allowing a document to moreeasily express thematic borrowings as a random effect. We demonstrate ouralgorithm on 1.8 million documents from The New York Times.
arxiv-2700-121 | An Uncertainty Framework for Classification | http://arxiv.org/pdf/1301.3896v1.pdf | author:Loo-Nin Teow, Kia-Fock Loe category:cs.LG stat.ML published:2013-01-16 summary:We define a generalized likelihood function based on uncertainty measures andshow that maximizing such a likelihood function for different measures inducesdifferent types of classifiers. In the probabilistic framework, we obtainclassifiers that optimize the cross-entropy function. In the possibilisticframework, we obtain classifiers that maximize the interclass margin.Furthermore, we show that the support vector machine is a sub-class of thesemaximum-margin classifiers.
arxiv-2700-122 | Complexity of Representation and Inference in Compositional Models with Part Sharing | http://arxiv.org/pdf/1301.3560v1.pdf | author:Alan L. Yuille, Roozbeh Mottaghi category:cs.CV published:2013-01-16 summary:This paper describes serial and parallel compositional models of multipleobjects with part sharing. Objects are built by part-subpart compositions andexpressed in terms of a hierarchical dictionary of object parts. These partsare represented on lattices of decreasing sizes which yield an executivesummary description. We describe inference and learning algorithms for thesemodels. We analyze the complexity of this model in terms of computation time(for serial computers) and numbers of nodes (e.g., "neurons") for parallelcomputers. In particular, we compute the complexity gains by part sharing andits dependence on how the dictionary scales with the level of the hierarchy. Weexplore three regimes of scaling behavior where the dictionary size (i)increases exponentially with the level, (ii) is determined by an unsupervisedcompositional learning algorithm applied to real data, (iii) decreasesexponentially with scale. This analysis shows that in some regimes the use ofshared parts enables algorithms which can perform inference in time linear inthe number of levels for an exponential number of objects. In other regimespart sharing has little advantage for serial computers but can give linearprocessing on parallel computers.
arxiv-2700-123 | Model Selection for Gaussian Mixture Models | http://arxiv.org/pdf/1301.3558v1.pdf | author:Tao Huang, Heng Peng, Kun Zhang category:stat.ME math.ST stat.ML stat.TH published:2013-01-16 summary:This paper is concerned with an important issue in finite mixture modelling,the selection of the number of mixing components. We propose a new penalizedlikelihood method for model selection of finite multivariate Gaussian mixturemodels. The proposed method is shown to be statistically consistent indetermining of the number of components. A modified EM algorithm is developedto simultaneously select the number of components and to estimate the mixingweights, i.e. the mixing probabilities, and unknown parameters of Gaussiandistributions. Simulations and a real data analysis are presented to illustratethe performance of the proposed method.
arxiv-2700-124 | Learning Output Kernels for Multi-Task Problems | http://arxiv.org/pdf/1301.3816v1.pdf | author:Francesco Dinuzzo category:cs.LG published:2013-01-16 summary:Simultaneously solving multiple related learning tasks is beneficial under avariety of circumstances, but the prior knowledge necessary to correctly modeltask relationships is rarely available in practice. In this paper, we develop anovel kernel-based multi-task learning technique that automatically revealsstructural inter-task relationships. Building over the framework of outputkernel learning (OKL), we introduce a method that jointly learns multiplefunctions and a low-rank multi-task kernel by solving a non-convexregularization problem. Optimization is carried out via a block coordinatedescent strategy, where each subproblem is solved using suitable conjugategradient (CG) type iterative methods for linear operator equations. Theeffectiveness of the proposed approach is demonstrated on pharmacological andcollaborative filtering data.
arxiv-2700-125 | Stochastic Pooling for Regularization of Deep Convolutional Neural Networks | http://arxiv.org/pdf/1301.3557v1.pdf | author:Matthew D. Zeiler, Rob Fergus category:cs.LG cs.NE stat.ML published:2013-01-16 summary:We introduce a simple and effective method for regularizing largeconvolutional neural networks. We replace the conventional deterministicpooling operations with a stochastic procedure, randomly picking the activationwithin each pooling region according to a multinomial distribution, given bythe activities within the pooling region. The approach is hyper-parameter freeand can be combined with other regularization approaches, such as dropout anddata augmentation. We achieve state-of-the-art performance on four imagedatasets, relative to other approaches that do not utilize data augmentation.
arxiv-2700-126 | A Rhetorical Analysis Approach to Natural Language Processing | http://arxiv.org/pdf/1301.3547v1.pdf | author:Benjamin Englard category:cs.CL stat.ML published:2013-01-16 summary:The goal of this research was to find a way to extend the capabilities ofcomputers through the processing of language in a more human way, and presentapplications which demonstrate the power of this method. This research presentsa novel approach, Rhetorical Analysis, to solving problems in Natural LanguageProcessing (NLP). The main benefit of Rhetorical Analysis, as opposed toprevious approaches, is that it does not require the accumulation of large setsof training data, but can be used to solve a multitude of problems within thefield of NLP. The NLP problems investigated with Rhetorical Analysis were theAuthor Identification problem - predicting the author of a piece of text basedon its rhetorical strategies, Election Prediction - predicting the winner of apresidential candidate's re-election campaign based on rhetorical strategieswithin that president's inaugural address, Natural Language Generation - havinga computer produce text containing rhetorical strategies, and DocumentSummarization. The results of this research indicate that an AuthorIdentification system based on Rhetorical Analysis could predict the correctauthor 100% of the time, that a re-election predictor based on RhetoricalAnalysis could predict the correct winner of a re-election campaign 55% of thetime, that a Natural Language Generation system based on Rhetorical Analysiscould output text with up to 87.3% similarity to Shakespeare in style, and thata Document Summarization system based on Rhetorical Analysis could extracthighly relevant sentences. Overall, this study demonstrated that RhetoricalAnalysis could be a useful approach to solving problems in NLP.
arxiv-2700-127 | Learning Features with Structure-Adapting Multi-view Exponential Family Harmoniums | http://arxiv.org/pdf/1301.3539v1.pdf | author:Yoonseop Kang, Seungjin Choi category:cs.LG published:2013-01-16 summary:We proposea graphical model for multi-view feature extraction thatautomatically adapts its structure to achieve better representation of datadistribution. The proposed model, structure-adapting multi-view harmonium(SA-MVH) has switch parameters that control the connection between hidden nodesand input views, and learn the switch parameter while training. Numericalexperiments on synthetic and a real-world dataset demonstrate the usefulbehavior of the SA-MVH, compared to existing multi-view feature extractionmethods.
arxiv-2700-128 | Variational Approximations between Mean Field Theory and the Junction Tree Algorithm | http://arxiv.org/pdf/1301.3901v1.pdf | author:Wim Wiegerinck category:cs.LG cs.AI stat.ML published:2013-01-16 summary:Recently, variational approximations such as the mean field approximationhave received much interest. We extend the standard mean field method by usingan approximating distribution that factorises into cluster potentials. Thisincludes undirected graphs, directed acyclic graphs and junction trees. Wederive generalized mean field equations to optimize the cluster potentials. Weshow that the method bridges the gap between the standard mean fieldapproximation and the exact junction tree algorithm. In addition, we addressthe problem of how to choose the graphical structure of the approximatingdistribution. From the generalised mean field equations we derive rules tosimplify the structure of the approximating distribution in advance withoutaffecting the quality of the approximation. We also show how the method fitsinto some other variational approximations that are currently popular.
arxiv-2700-129 | The IBMAP approach for Markov networks structure learning | http://arxiv.org/pdf/1301.3720v2.pdf | author:Federico Schlüter, Facundo Bromberg, Alejandro Edera category:cs.AI cs.LG published:2013-01-16 summary:In this work we consider the problem of learning the structure of Markovnetworks from data. We present an approach for tackling this problem calledIBMAP, together with an efficient instantiation of the approach: the IBMAP-HCalgorithm, designed for avoiding important limitations of existingindependence-based algorithms. These algorithms proceed by performingstatistical independence tests on data, trusting completely the outcome of eachtest. In practice tests may be incorrect, resulting in potential cascadingerrors and the consequent reduction in the quality of the structures learned.IBMAP contemplates this uncertainty in the outcome of the tests through aprobabilistic maximum-a-posteriori approach. The approach is instantiated inthe IBMAP-HC algorithm, a structure selection strategy that performs apolynomial heuristic local search in the space of possible structures. Wepresent an extensive empirical evaluation on synthetic and real data, showingthat our algorithm outperforms significantly the current independence-basedalgorithms, in terms of data efficiency and quality of learned structures, withequivalent computational complexities. We also show the performance of IBMAP-HCin a real-world application of knowledge discovery: EDAs, which areevolutionary algorithms that use structure learning on each generation formodeling the distribution of populations. The experiments show that whenIBMAP-HC is used to learn the structure, EDAs improve the convergence to theoptimum.
arxiv-2700-130 | Revisiting Natural Gradient for Deep Networks | http://arxiv.org/pdf/1301.3584v7.pdf | author:Razvan Pascanu, Yoshua Bengio category:cs.LG cs.NA published:2013-01-16 summary:We evaluate natural gradient, an algorithm originally proposed in Amari(1997), for learning deep models. The contributions of this paper are asfollows. We show the connection between natural gradient and three otherrecently proposed methods for training deep models: Hessian-Free (Martens,2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux etal., 2008). We describe how one can use unlabeled data to improve thegeneralization error obtained by natural gradient and empirically evaluate therobustness of the algorithm to the ordering of the training set compared tostochastic gradient descent. Finally we extend natural gradient to incorporatesecond order information alongside the manifold information and provide abenchmark of the new algorithm using a truncated Newton approach for invertingthe metric matrix instead of using a diagonal approximation of it.
arxiv-2700-131 | A Branch-and-Bound Algorithm for MDL Learning Bayesian Networks | http://arxiv.org/pdf/1301.3897v1.pdf | author:Jin Tian category:cs.AI cs.LG stat.ML published:2013-01-16 summary:This paper extends the work in [Suzuki, 1996] and presents an efficientdepth-first branch-and-bound algorithm for learning Bayesian networkstructures, based on the minimum description length (MDL) principle, for agiven (consistent) variable ordering. The algorithm exhaustively searchesthrough all network structures and guarantees to find the network with the bestMDL score. Preliminary experiments show that the algorithm is efficient, andthat the time complexity grows slowly with the sample size. The algorithm isuseful for empirically studying both the performance of suboptimal heuristicsearch algorithms and the adequacy of the MDL principle in learning Bayesiannetworks.
arxiv-2700-132 | Model-Based Hierarchical Clustering | http://arxiv.org/pdf/1301.3899v1.pdf | author:Shivakumar Vaithyanathan, Byron E Dom category:cs.LG cs.AI stat.ML published:2013-01-16 summary:We present an approach to model-based hierarchical clustering by formulatingan objective function based on a Bayesian analysis. This model organizes thedata into a cluster hierarchy while specifying a complex feature-setpartitioning that is a key component of our model. Features can have either aunique distribution in every cluster or a common distribution over some (oreven all) of the clusters. The cluster subsets over which these features havesuch a common distribution correspond to the nodes (clusters) of the treerepresenting the hierarchy. We apply this general model to the problem ofdocument clustering for which we use a multinomial likelihood function andDirichlet priors. Our algorithm consists of a two-stage process wherein wefirst perform a flat clustering followed by a modified hierarchicalagglomerative merging process that includes determining the features that willhave common distributions over the merged clusters. The regularization inducedby using the marginal likelihood automatically determines the optimal modelstructure including number of clusters, the depth of the tree and the subset offeatures to be modeled as having a common distribution at each node. We presentexperimental results on both synthetic data and a real document collection.
arxiv-2700-133 | Deep Learning for Detecting Robotic Grasps | http://arxiv.org/pdf/1301.3592v6.pdf | author:Ian Lenz, Honglak Lee, Ashutosh Saxena category:cs.LG cs.CV cs.RO published:2013-01-16 summary:We consider the problem of detecting robotic grasps in an RGB-D view of ascene containing objects. In this work, we apply a deep learning approach tosolve this problem, which avoids time-consuming hand-design of features. Thispresents two main challenges. First, we need to evaluate a huge number ofcandidate grasps. In order to make detection fast, as well as robust, wepresent a two-step cascaded structure with two deep networks, where the topdetections from the first are re-evaluated by the second. The first network hasfewer features, is faster to run, and can effectively prune out unlikelycandidate grasps. The second, with more features, is slower but has to run onlyon the top few detections. Second, we need to handle multimodal inputs well,for which we present a method to apply structured regularization on the weightsbased on multimodal group regularization. We demonstrate that our methodoutperforms the previous state-of-the-art methods in robotic grasp detection,and can be used to successfully execute grasps on two different roboticplatforms.
arxiv-2700-134 | Two SVDs produce more focal deep learning representations | http://arxiv.org/pdf/1301.3627v2.pdf | author:Hinrich Schuetze, Christian Scheible category:cs.CL cs.LG published:2013-01-16 summary:A key characteristic of work on deep learning and neural networks in generalis that it relies on representations of the input that support generalization,robust inference, domain adaptation and other desirable functionalities. Muchrecent progress in the field has focused on efficient and effective methods forcomputing representations. In this paper, we propose an alternative method thatis more efficient than prior work and produces representations that have aproperty we call focality -- a property we hypothesize to be important forneural network representations. The method consists of a simple application oftwo consecutive SVDs and is inspired by Anandkumar (2012).
arxiv-2700-135 | Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks | http://arxiv.org/pdf/1301.3605v3.pdf | author:Dong Yu, Michael L. Seltzer, Jinyu Li, Jui-Ting Huang, Frank Seide category:cs.LG cs.CL cs.NE published:2013-01-16 summary:Recent studies have shown that deep neural networks (DNNs) performsignificantly better than shallow networks and Gaussian mixture models (GMMs)on large vocabulary speech recognition tasks. In this paper, we argue that theimproved accuracy achieved by the DNNs is the result of their ability toextract discriminative internal representations that are robust to the manysources of variability in speech signals. We show that these representationsbecome increasingly insensitive to small perturbations in the input withincreasing network depth, which leads to better speech recognition performancewith deeper networks. We also show that DNNs cannot extrapolate to test samplesthat are substantially different from the training examples. If the trainingdata are sufficiently representative, however, internal features learned by theDNN are relatively stable with respect to speaker differences, bandwidthdifferences, and environment distortion. This enables DNN-based recognizers toperform as well or better than state-of-the-art systems based on GMMs orshallow networks without the need for explicit model adaptation or featurenormalization.
arxiv-2700-136 | Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients | http://arxiv.org/pdf/1301.3764v2.pdf | author:Tom Schaul, Yann LeCun category:cs.LG cs.AI stat.ML published:2013-01-16 summary:Recent work has established an empirically successful framework for adaptinglearning rates for stochastic gradient descent (SGD). This effectively removesall needs for tuning, while automatically reducing learning rates over time onstationary problems, and permitting learning rates to grow appropriately innon-stationary tasks. Here, we extend the idea in three directions, addressingproper minibatch parallelization, including reweighted updates for sparse ororthogonal gradients, improving robustness on non-smooth loss functions, in theprocess replacing the diagonal Hessian estimation procedure that may not alwaysbe available by a robust finite-difference approximation. The final algorithmintegrates all these components, has linear complexity and is hyper-parameterfree.
arxiv-2700-137 | Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint | http://arxiv.org/pdf/1301.3533v2.pdf | author:Xanadu Halkias, Sebastien Paris, Herve Glotin category:cs.NE cs.LG stat.ML published:2013-01-16 summary:Deep Belief Networks (DBN) have been successfully applied on popular machinelearning tasks. Specifically, when applied on hand-written digit recognition,DBNs have achieved approximate accuracy rates of 98.8%. In an effort tooptimize the data representation achieved by the DBN and maximize theirdescriptive power, recent advances have focused on inducing sparse constraintsat each layer of the DBN. In this paper we present a theoretical approach forsparse constraints in the DBN using the mixed norm for both non-overlapping andoverlapping groups. We explore how these constraints affect the classificationaccuracy for digit recognition in three different datasets (MNIST, USPS, RIMES)and provide initial estimations of their usefulness by altering differentparameters such as the group size and overlap percentage.
arxiv-2700-138 | Indoor Semantic Segmentation using depth information | http://arxiv.org/pdf/1301.3572v2.pdf | author:Camille Couprie, Clément Farabet, Laurent Najman, Yann LeCun category:cs.CV published:2013-01-16 summary:This work addresses multi-class segmentation of indoor scenes with RGB-Dinputs. While this area of research has gained much attention recently, mostworks still rely on hand-crafted features. In contrast, we apply a multiscaleconvolutional network to learn features directly from the images and the depthinformation. We obtain state-of-the-art on the NYU-v2 depth dataset with anaccuracy of 64.5%. We illustrate the labeling of indoor scenes in videossequences that could be processed in real-time using appropriate hardware suchas an FPGA.
arxiv-2700-139 | Big Neural Networks Waste Capacity | http://arxiv.org/pdf/1301.3583v4.pdf | author:Yann N. Dauphin, Yoshua Bengio category:cs.LG cs.CV published:2013-01-16 summary:This article exposes the failure of some big neural networks to leverageadded capacity to reduce underfitting. Past research suggest diminishingreturns when increasing the size of neural networks. Our experiments onImageNet LSVRC-2010 show that this may be due to the fact there are highlydiminishing returns for capacity in terms of training error, leading tounderfitting. This suggests that the optimization method - first order gradientdescent - fails at this regime. Directly attacking this problem, either throughthe optimization method or the choices of parametrization, may allow to improvethe generalization error on large datasets, for which a large capacity isrequired.
arxiv-2700-140 | Joint Training Deep Boltzmann Machines for Classification | http://arxiv.org/pdf/1301.3568v3.pdf | author:Ian J. Goodfellow, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG published:2013-01-16 summary:We introduce a new method for training deep Boltzmann machines jointly. Priormethods of training DBMs require an initial learning pass that trains the modelgreedily, one layer at a time, or do not perform well on classification tasks.In our approach, we train all layers of the DBM simultaneously, using a noveltraining procedure called multi-prediction training. The resulting model caneither be interpreted as a single generative model trained to maximize avariational approximation to the generalized pseudolikelihood, or as a familyof recurrent networks that share parameters and may be approximately averagedtogether using a novel technique we call the multi-inference trick. We showthat our approach performs competitively for classification and outperformsprevious methods in terms of accuracy of approximate inference andclassification with missing inputs.
arxiv-2700-141 | Training Neural Networks with Stochastic Hessian-Free Optimization | http://arxiv.org/pdf/1301.3641v3.pdf | author:Ryan Kiros category:cs.LG cs.NE stat.ML published:2013-01-16 summary:Hessian-free (HF) optimization has been successfully used for training deepautoencoders and recurrent networks. HF uses the conjugate gradient algorithmto construct update directions through curvature-vector products that can becomputed on the same order of time as gradients. In this paper we exploit thisproperty and study stochastic HF with gradient and curvature mini-batchesindependent of the dataset size. We modify Martens' HF for these settings andintegrate dropout, a method for preventing co-adaptation of feature detectors,to guard against overfitting. Stochastic Hessian-free optimization gives anintermediary between SGD and HF that achieves competitive performance on bothclassification and deep autoencoder experiments.
arxiv-2700-142 | Behavior Pattern Recognition using A New Representation Model | http://arxiv.org/pdf/1301.3630v4.pdf | author:Qifeng Qiao, Peter A. Beling category:cs.LG published:2013-01-16 summary:We study the use of inverse reinforcement learning (IRL) as a tool for therecognition of agents' behavior on the basis of observation of their sequentialdecision behavior interacting with the environment. We model the problem facedby the agents as a Markov decision process (MDP) and model the observedbehavior of the agents in terms of forward planning for the MDP. We use IRL tolearn reward functions and then use these reward functions as the basis forclustering or classification models. Experimental studies with GridWorld, anavigation problem, and the secretary problem, an optimal stopping problem,suggest reward vectors found from IRL can be a good basis for behavior patternrecognition problems. Empirical comparisons of our method with several existingIRL algorithms and with direct methods that use feature statistics observed instate-action space suggest it may be superior for recognition problems.
arxiv-2700-143 | Saturating Auto-Encoders | http://arxiv.org/pdf/1301.3577v3.pdf | author:Rostislav Goroshin, Yann LeCun category:cs.LG published:2013-01-16 summary:We introduce a simple new regularizer for auto-encoders whose hidden-unitactivation functions contain at least one zero-gradient (saturated) region.This regularizer explicitly encourages activations in the saturated region(s)of the corresponding activation function. We call these SaturatingAuto-Encoders (SATAE). We show that the saturation regularizer explicitlylimits the SATAE's ability to reconstruct inputs which are not near the datamanifold. Furthermore, we show that a wide variety of features can be learnedwhen different activation functions are used. Finally, connections areestablished with the Contractive and Sparse Auto-Encoders.
arxiv-2700-144 | Zero-Shot Learning Through Cross-Modal Transfer | http://arxiv.org/pdf/1301.3666v2.pdf | author:Richard Socher, Milind Ganjoo, Hamsa Sridhar, Osbert Bastani, Christopher D. Manning, Andrew Y. Ng category:cs.CV cs.LG published:2013-01-16 summary:This work introduces a model that can recognize objects in images even if notraining data is available for the objects. The only necessary knowledge aboutthe unseen categories comes from unsupervised large text corpora. In ourzero-shot framework distributional information in language can be seen asspanning a semantic basis for understanding what objects look like. Mostprevious zero-shot learning models can only differentiate between unseenclasses. In contrast, our model can both obtain state of the art performance onclasses that have thousands of training images and obtain reasonableperformance on unseen classes. This is achieved by first using outlierdetection in the semantic space and then two separate recognition models.Furthermore, our model does not require any manually defined semantic featuresfor either words or images.
arxiv-2700-145 | Discriminative Recurrent Sparse Auto-Encoders | http://arxiv.org/pdf/1301.3775v4.pdf | author:Jason Tyler Rolfe, Yann LeCun category:cs.LG cs.CV published:2013-01-16 summary:We present the discriminative recurrent sparse auto-encoder model, comprisinga recurrent encoder of rectified linear units, unrolled for a fixed number ofiterations, and connected to two linear decoders that reconstruct the input andpredict its supervised classification. Training viabackpropagation-through-time initially minimizes an unsupervised sparsereconstruction error; the loss function is then augmented with a discriminativeterm on the supervised classification. The depth implicit in thetemporally-unrolled form allows the system to exhibit all the power of deepnetworks, while substantially reducing the number of trainable parameters. From an initially unstructured network the hidden units differentiate intocategorical-units, each of which represents an input prototype with awell-defined class; and part-units representing deformations of theseprototypes. The learned organization of the recurrent encoder is hierarchical:part-units are driven directly by the input, whereas the activity ofcategorical-units builds up over time through interactions with the part-units.Even using a small number of hidden units per layer, discriminative recurrentsparse auto-encoders achieve excellent performance on MNIST.
arxiv-2700-146 | Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines | http://arxiv.org/pdf/1301.3545v2.pdf | author:Guillaume Desjardins, Razvan Pascanu, Aaron Courville, Yoshua Bengio category:cs.LG cs.NE stat.ML published:2013-01-16 summary:This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm fortraining Boltzmann Machines. Similar in spirit to the Hessian-Free method ofMartens [8], our algorithm belongs to the family of truncated Newton methodsand exploits an efficient matrix-vector product to avoid explicitely storingthe natural gradient metric $L$. This metric is shown to be the expected secondderivative of the log-partition function (under the model distribution), orequivalently, the variance of the vector of partial derivatives of the energyfunction. We evaluate our method on the task of joint-training a 3-layer DeepBoltzmann Machine and show that MFNG does indeed have faster per-epochconvergence compared to Stochastic Maximum Likelihood with centering, thoughwall-clock performance is currently not competitive.
arxiv-2700-147 | Learning New Facts From Knowledge Bases With Neural Tensor Networks and Semantic Word Vectors | http://arxiv.org/pdf/1301.3618v2.pdf | author:Danqi Chen, Richard Socher, Christopher D. Manning, Andrew Y. Ng category:cs.CL cs.LG published:2013-01-16 summary:Knowledge bases provide applications with the benefit of easily accessible,systematic relational knowledge but often suffer in practice from theirincompleteness and lack of knowledge of new entities and relations. Much workhas focused on building or extending them by finding patterns in largeunannotated text corpora. In contrast, here we mainly aim to complete aknowledge base by predicting additional true relationships between entities,based on generalizations that can be discerned in the given knowledgebase. Weintroduce a neural tensor network (NTN) model which predicts new relationshipentries that can be added to the database. This model can be improved byinitializing entity representations with word vectors learned in anunsupervised fashion from text, and when doing this, existing relations caneven be queried for entities that were not present in the database. Our modelgeneralizes and outperforms existing models for this problem, and can classifyunseen relationships in WordNet with an accuracy of 75.8%.
arxiv-2700-148 | Deep Predictive Coding Networks | http://arxiv.org/pdf/1301.3541v3.pdf | author:Rakesh Chalasani, Jose C. Principe category:cs.LG cs.CV stat.ML published:2013-01-16 summary:The quality of data representation in deep learning methods is directlyrelated to the prior model imposed on the representations; however, generallyused fixed priors are not capable of adjusting to the context in the data. Toaddress this issue, we propose deep predictive coding networks, a hierarchicalgenerative model that empirically alters priors on the latent representationsin a dynamic and context-sensitive manner. This model captures the temporaldependencies in time-varying signals and uses top-down information to modulatethe representation in lower layers. The centerpiece of our model is a novelprocedure to infer sparse states of a dynamic model which is used for featureextraction. We also extend this feature extraction block to introduce a poolingfunction that captures locally invariant representations. When applied on anatural video data, we show that our method is able to learn high-level visualfeatures. We also demonstrate the role of the top-down connections by showingthe robustness of the proposed model to structured noise.
arxiv-2700-149 | Matrix Approximation under Local Low-Rank Assumption | http://arxiv.org/pdf/1301.3192v1.pdf | author:Joonseok Lee, Seungyeon Kim, Guy Lebanon, Yoram Singer category:cs.LG stat.ML I.2.6 published:2013-01-15 summary:Matrix approximation is a common tool in machine learning for buildingaccurate prediction models for recommendation systems, text mining, andcomputer vision. A prevalent assumption in constructing matrix approximationsis that the partially observed matrix is of low-rank. We propose a new matrixapproximation model where we assume instead that the matrix is only locally oflow-rank, leading to a representation of the observed matrix as a weighted sumof low-rank matrices. We analyze the accuracy of the proposed local low-rankmodeling. Our experiments show improvements in prediction accuracy inrecommendation tasks.
arxiv-2700-150 | Learning Graphical Model Parameters with Approximate Marginal Inference | http://arxiv.org/pdf/1301.3193v1.pdf | author:Justin Domke category:cs.LG cs.CV I.2.6; I.4.8 published:2013-01-15 summary:Likelihood based-learning of graphical models faces challenges ofcomputational-complexity and robustness to model mis-specification. This paperstudies methods that fit parameters directly to maximize a measure of theaccuracy of predicted marginals, taking into account both model and inferenceapproximations at training time. Experiments on imaging problems suggestmarginalization-based learning performs better than likelihood-basedapproximations on difficult problems where the model being fit is approximatein nature.
arxiv-2700-151 | The Manifold of Human Emotions | http://arxiv.org/pdf/1301.3214v1.pdf | author:Seungyeon Kim, Fuxin Li, Guy Lebanon, Irfan Essa category:cs.CL published:2013-01-15 summary:Sentiment analysis predicts the presence of positive or negative emotions ina text document. In this paper, we consider higher dimensional extensions ofthe sentiment concept, which represent a richer set of human emotions. Ourapproach goes beyond previous work in that our model contains a continuousmanifold rather than a finite set of human emotions. We investigate theresulting model, compare it to psychological observations, and explore itspredictive capabilities.
arxiv-2700-152 | Multi-agent learning using Fictitious Play and Extended Kalman Filter | http://arxiv.org/pdf/1301.3347v1.pdf | author:Michalis Smyrnakis category:cs.MA cs.LG math.OC stat.ML published:2013-01-15 summary:Decentralised optimisation tasks are important components of multi-agentsystems. These tasks can be interpreted as n-player potential games: thereforegame-theoretic learning algorithms can be used to solve decentralisedoptimisation tasks. Fictitious play is the canonical example of thesealgorithms. Nevertheless fictitious play implicitly assumes that players havestationary strategies. We present a novel variant of fictitious play whereplayers predict their opponents' strategies using Extended Kalman filters anduse their predictions to update their strategies. We show that in 2 by 2 games with at least one pure Nash equilibrium and inpotential games where players have two available actions, the proposedalgorithm converges to the pure Nash equilibrium. The performance of theproposed algorithm was empirically tested, in two strategic form games and anad-hoc sensor network surveillance problem. The proposed algorithm performsbetter than the classic fictitious play algorithm in these games and thereforeimproves the performance of game-theoretical learning in decentralisedoptimisation.
arxiv-2700-153 | Pooling-Invariant Image Feature Learning | http://arxiv.org/pdf/1302.5056v1.pdf | author:Yangqing Jia, Oriol Vinyals, Trevor Darrell category:cs.CV cs.LG published:2013-01-15 summary:Unsupervised dictionary learning has been a key component in state-of-the-artcomputer vision recognition architectures. While highly effective methods existfor patch-based dictionary learning, these methods may learn redundant featuresafter the pooling stage in a given early vision architecture. In this paper, weoffer a novel dictionary learning scheme to efficiently take into account theinvariance of learned features after the spatial pooling stage. The algorithmis built on simple clustering, and thus enjoys efficiency and scalability. Wediscuss the underlying mechanism that justifies the use of clusteringalgorithms, and empirically show that the algorithm finds better dictionariesthan patch-based methods with the same dictionary size.
arxiv-2700-154 | Anomaly Classification with the Anti-Profile Support Vector Machine | http://arxiv.org/pdf/1301.3514v1.pdf | author:Wikum Dinalankara, Hector Corrada Bravo category:stat.ML q-bio.GN published:2013-01-15 summary:We introduce the anti-profile Support Vector Machine (apSVM) as a novelalgorithm to address the anomaly classification problem, an extension ofanomaly detection where the goal is to distinguish data samples from a numberof anomalous and heterogeneous classes based on their pattern of deviation froma normal stable class. We show that under heterogeneity assumptions definedhere that the apSVM can be solved as the dual of a standard SVM with anindirect kernel that measures similarity of anomalous samples throughsimilarity to the stable normal class. We characterize this indirect kernel asthe inner product in a Reproducing Kernel Hilbert Space between representersthat are projected to the subspace spanned by the representers of the normalsamples. We show by simulation and application to cancer genomics datasets thatthe anti-profile SVM produces classifiers that are more accurate and stablethan the standard SVM in the anomaly classification setting.
arxiv-2700-155 | Factorized Topic Models | http://arxiv.org/pdf/1301.3461v7.pdf | author:Cheng Zhang, Carl Henrik Ek, Andreas Damianou, Hedvig Kjellstrom category:cs.LG cs.CV cs.IR published:2013-01-15 summary:In this paper we present a modification to a latent topic model, which makesthe model exploit supervision to produce a factorized representation of theobserved data. The structured parameterization separately encodes variance thatis shared between classes from variance that is private to each class by theintroduction of a new prior over the topic space. The approach allows for amore eff{}icient inference and provides an intuitive interpretation of the datain terms of an informative signal together with structured noise. Thefactorized representation is shown to enhance inference performance for image,text, and video classification.
arxiv-2700-156 | Feature grouping from spatially constrained multiplicative interaction | http://arxiv.org/pdf/1301.3391v3.pdf | author:Felix Bauer, Roland Memisevic category:cs.LG I.2.6 published:2013-01-15 summary:We present a feature learning model that learns to encode relationshipsbetween images. The model is defined as a Gated Boltzmann Machine, which isconstrained such that hidden units that are nearby in space can gate eachother's connections. We show how frequency/orientation "columns" as well astopographic filter maps follow naturally from training the model on imagepairs. The model also helps explain why square-pooling models yield featuregroups with similar grouping properties. Experimental results on syntheticimage transformations show that spatially constrained gating is an effectiveway to reduce the number of parameters and thereby to regularize atransformation-learning model.
arxiv-2700-157 | How good is the Electricity benchmark for evaluating concept drift adaptation | http://arxiv.org/pdf/1301.3524v1.pdf | author:Indre Zliobaite category:cs.LG published:2013-01-15 summary:In this correspondence, we will point out a problem with testing adaptiveclassifiers on autocorrelated data. In such a case random change alarms mayboost the accuracy figures. Hence, we cannot be sure if the adaptation isworking well.
arxiv-2700-158 | Boltzmann Machines and Denoising Autoencoders for Image Denoising | http://arxiv.org/pdf/1301.3468v6.pdf | author:Kyunghyun Cho category:stat.ML cs.CV cs.LG published:2013-01-15 summary:Image denoising based on a probabilistic model of local image patches hasbeen employed by various researchers, and recently a deep (denoising)autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] asa good model for this. In this paper, we propose that another popular family ofmodels in the field of deep learning, called Boltzmann machines, can performimage denoising as well as, or in certain cases of high level of noise, betterthan denoising autoencoders. We empirically evaluate the two models on threedifferent sets of images with different types and levels of noise. Throughoutthe experiments we also examine the effect of the depth of the models. Theexperiments confirmed our claim and revealed that the performance can beimproved by adding more hidden layers, especially when the level of noise ishigh.
arxiv-2700-159 | An Efficient Sufficient Dimension Reduction Method for Identifying Genetic Variants of Clinical Significance | http://arxiv.org/pdf/1301.3528v1.pdf | author:Momiao Xiong, Long Ma category:q-bio.GN cs.LG stat.ML published:2013-01-15 summary:Fast and cheaper next generation sequencing technologies will generateunprecedentedly massive and highly-dimensional genomic and epigenomic variationdata. In the near future, a routine part of medical record will include thesequenced genomes. A fundamental question is how to efficiently extract genomicand epigenomic variants of clinical utility which will provide information foroptimal wellness and interference strategies. Traditional paradigm foridentifying variants of clinical validity is to test association of thevariants. However, significantly associated genetic variants may or may not beusefulness for diagnosis and prognosis of diseases. Alternative to associationstudies for finding genetic variants of predictive utility is to systematicallysearch variants that contain sufficient information for phenotype prediction.To achieve this, we introduce concepts of sufficient dimension reduction andcoordinate hypothesis which project the original high dimensional data to verylow dimensional space while preserving all information on response phenotypes.We then formulate clinically significant genetic variant discovery problem intosparse SDR problem and develop algorithms that can select significant geneticvariants from up to or even ten millions of predictors with the aid of dividingSDR for whole genome into a number of subSDR problems defined for genomicregions. The sparse SDR is in turn formulated as sparse optimal scoringproblem, but with penalty which can remove row vectors from the basis matrix.To speed up computation, we develop the modified alternating direction methodfor multipliers to solve the sparse optimal scoring problem which can easily beimplemented in parallel. To illustrate its application, the proposed method isapplied to simulation data and the NHLBI's Exome Sequencing Project dataset
arxiv-2700-160 | Barnes-Hut-SNE | http://arxiv.org/pdf/1301.3342v2.pdf | author:Laurens van der Maaten category:cs.LG cs.CV stat.ML published:2013-01-15 summary:The paper presents an O(N log N)-implementation of t-SNE -- an embeddingtechnique that is commonly used for the visualization of high-dimensional datain scatter plots and that normally runs in O(N^2). The new implementation usesvantage-point trees to compute sparse pairwise similarities between the inputdata objects, and it uses a variant of the Barnes-Hut algorithm - an algorithmused by astronomers to perform N-body simulations - to approximate the forcesbetween the corresponding points in the embedding. Our experiments show thatthe new algorithm, called Barnes-Hut-SNE, leads to substantial computationaladvantages over standard t-SNE, and that it makes it possible to learnembeddings of data sets with millions of objects.
arxiv-2700-161 | Discrete Restricted Boltzmann Machines | http://arxiv.org/pdf/1301.3529v4.pdf | author:Guido Montufar, Jason Morton category:stat.ML math.AG math.PR G.3 published:2013-01-15 summary:We describe discrete restricted Boltzmann machines: probabilistic graphicalmodels with bipartite interactions between visible and hidden discretevariables. Examples are binary restricted Boltzmann machines and discrete naiveBayes models. We detail the inference functions and distributed representationsarising in these models in terms of configurations of projected products ofsimplices and normal fans of products of simplices. We bound the number ofhidden variables, depending on the cardinalities of their state spaces, forwhich these models can approximate any probability distribution on theirvisible states to any given accuracy. In addition, we use algebraic methods andcoding theory to compute their dimension.
arxiv-2700-162 | Why Size Matters: Feature Coding as Nystrom Sampling | http://arxiv.org/pdf/1301.5348v2.pdf | author:Oriol Vinyals, Yangqing Jia, Trevor Darrell category:cs.LG cs.CV published:2013-01-15 summary:Recently, the computer vision and machine learning community has been infavor of feature extraction pipelines that rely on a coding step followed by alinear classifier, due to their overall simplicity, well understood propertiesof linear classifiers, and their computational efficiency. In this paper wepropose a novel view of this pipeline based on kernel methods and Nystromsampling. In particular, we focus on the coding of a data point with a localrepresentation based on a dictionary with fewer elements than the number ofdata points, and view it as an approximation to the actual function that wouldcompute pair-wise similarity to all data points (often too many to compute inpractice), followed by a Nystrom sampling step to select a subset of all datapoints. Furthermore, since bounds are known on the approximation power of Nystromsampling as a function of how many samples (i.e. dictionary size) we consider,we can derive bounds on the approximation of the exact (but expensive tocompute) kernel matrix, and use it as a proxy to predict accuracy as a functionof the dictionary size, which has been observed to increase but also tosaturate as we increase its size. This model may help explaining the positiveeffect of the codebook size and justifying the need to stack more layers (oftenreferred to as deep learning), as flat models empirically saturate as we addmore complexity.
arxiv-2700-163 | A Geometric Descriptor for Cell-Division Detection | http://arxiv.org/pdf/1301.3457v2.pdf | author:Marcelo Cicconet, Italo Lima, Davi Geiger, Kris Gunsalus category:cs.CV published:2013-01-15 summary:We describe a method for cell-division detection based on a geometric-drivendescriptor that can be represented as a 5-layers processing network, basedmainly on wavelet filtering and a test for mirror symmetry between pairs ofpixels. After the centroids of the descriptors are computed for a sequence offrames, the two-steps piecewise constant function that best fits the sequenceof centroids determines the frame where the division occurs.
arxiv-2700-164 | Efficient Learning of Domain-invariant Image Representations | http://arxiv.org/pdf/1301.3224v5.pdf | author:Judy Hoffman, Erik Rodner, Jeff Donahue, Trevor Darrell, Kate Saenko category:cs.LG published:2013-01-15 summary:We present an algorithm that learns representations which explicitlycompensate for domain mismatch and which can be efficiently realized as linearclassifiers. Specifically, we form a linear transformation that maps featuresfrom the target (test) domain to the source (training) domain as part oftraining the classifier. We optimize both the transformation and classifierparameters jointly, and introduce an efficient cost function based onmisclassification loss. Our method combines several features previouslyunavailable in a single algorithm: multi-class adaptation throughrepresentation learning, ability to map across heterogeneous feature spaces,and scalability to large datasets. We present experiments on several imagedatasets that demonstrate improved accuracy and computational advantagescompared to previous approaches.
arxiv-2700-165 | A Semantic Matching Energy Function for Learning with Multi-relational Data | http://arxiv.org/pdf/1301.3485v2.pdf | author:Xavier Glorot, Antoine Bordes, Jason Weston, Yoshua Bengio category:cs.LG published:2013-01-15 summary:Large-scale relational learning becomes crucial for handling the huge amountsof structured data generated daily in many application domains ranging fromcomputational biology or information retrieval, to natural language processing.In this paper, we present a new neural network architecture designed to embedmulti-relational graphs into a flexible continuous vector space in which theoriginal data is kept and enhanced. The network is trained to encode thesemantics of these graphs in order to assign high probabilities to plausiblecomponents. We empirically show that it reaches competitive performance in linkprediction on standard datasets from the literature.
arxiv-2700-166 | The Neural Representation Benchmark and its Evaluation on Brain and Machine | http://arxiv.org/pdf/1301.3530v2.pdf | author:Charles F. Cadieu, Ha Hong, Dan Yamins, Nicolas Pinto, Najib J. Majaj, James J. DiCarlo category:cs.NE cs.CV cs.LG q-bio.NC published:2013-01-15 summary:A key requirement for the development of effective learning representationsis their evaluation and comparison to representations we know to be effective.In natural sensory domains, the community has viewed the brain as a source ofinspiration and as an implicit benchmark for success. However, it has not beenpossible to directly test representational learning algorithms directly againstthe representations contained in neural systems. Here, we propose a newbenchmark for visual representations on which we have directly tested theneural representation in multiple visual cortical areas in macaque (utilizingdata from [Majaj et al., 2012]), and on which any computer vision algorithmthat produces a feature space can be tested. The benchmark measures theeffectiveness of the neural or machine representation by computing theclassification loss on the ordered eigendecomposition of a kernel matrix[Montavon et al., 2011]. In our analysis we find that the neural representationin visual area IT is superior to visual area V4. In our analysis ofrepresentational learning algorithms, we find that three-layer models approachthe representational performance of V4 and the algorithm in [Le et al., 2012]surpasses the performance of V4. Impressively, we find that a recent supervisedalgorithm [Krizhevsky et al., 2012] achieves performance comparable to that ofIT for an intermediate level of image variation difficulty, and surpasses IT ata higher difficulty level. We believe this result represents a major milestone:it is the first learning algorithm we have found that exceeds our currentestimate of IT representation performance. We hope that this benchmark willassist the community in matching the representational performance of visualcortex and will serve as an initial rallying point for further correspondencebetween representations derived in brains and machines.
arxiv-2700-167 | Audio Classical Composer Identification by Deep Neural Network | http://arxiv.org/pdf/1301.3195v7.pdf | author:Zhen Hu, Kun Fu, Changshui Zhang category:cs.NE cs.IR published:2013-01-15 summary:Audio Classical Composer Identification (ACC) is an important problem inMusic Information Retrieval (MIR) which aims at identifying the composer foraudio classical music clips. The famous annual competition, Music InformationRetrieval Evaluation eXchange (MIREX), also takes it as one of the fourtraining&testing tasks. We built a hybrid model based on Deep Belief Network(DBN) and Stacked Denoising Autoencoder (SDA) to identify the composer fromaudio signal. As a matter of copyright, sponsors of MIREX cannot publish theirdata set. We built a comparable data set to test our model. We got an accuracyof 76.26% in our data set which is better than some pure models and shallowmodels. We think our method is promising even though we test it in a differentdata set, since our data set is comparable to that in MIREX by size. We alsofound that samples from different classes become farther away from each otherwhen transformed by more layers in our model.
arxiv-2700-168 | Pushing Stochastic Gradient towards Second-Order Methods -- Backpropagation Learning with Transformations in Nonlinearities | http://arxiv.org/pdf/1301.3476v3.pdf | author:Tommi Vatanen, Tapani Raiko, Harri Valpola, Yann LeCun category:cs.LG cs.CV stat.ML published:2013-01-15 summary:Recently, we proposed to transform the outputs of each hidden neuron in amulti-layer perceptron network to have zero output and zero slope on average,and use separate shortcut connections to model the linear dependencies instead.We continue the work by firstly introducing a third transformation to normalizethe scale of the outputs of each hidden neuron, and secondly by analyzing theconnections to second order optimization methods. We show that thetransformations make a simple stochastic gradient behave closer to second-orderoptimization methods and thus speed up learning. This is shown both in theoryand with experiments. The experiments on the third transformation show thatwhile it further increases the speed of learning, it can also hurt performanceby converging to a worse local optimum, where both the inputs and outputs ofmany hidden neurons are close to zero.
arxiv-2700-169 | Recurrent Online Clustering as a Spatio-Temporal Feature Extractor in DeSTIN | http://arxiv.org/pdf/1301.3385v2.pdf | author:Steven R. Young, Itamar Arel category:cs.CV published:2013-01-15 summary:This paper presents a basic enhancement to the DeSTIN deep learningarchitecture by replacing the explicitly calculated transition tables that areused to capture temporal features with a simpler, more scalable mechanism. Thismechanism uses feedback of state information to cluster over a space comprisedof both the spatial input and the current state. The resulting architectureachieves state-of-the-art results on the MNIST classification benchmark.
arxiv-2700-170 | The Expressive Power of Word Embeddings | http://arxiv.org/pdf/1301.3226v4.pdf | author:Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, Steven Skiena category:cs.LG cs.CL stat.ML published:2013-01-15 summary:We seek to better understand the difference in quality of the severalpublicly released embeddings. We propose several tasks that help to distinguishthe characteristics of different embeddings. Our evaluation of sentimentpolarity and synonym/antonym relations shows that embeddings are able tocapture surprisingly nuanced semantics even in the absence of sentencestructure. Moreover, benchmarking the embeddings shows great variance inquality and characteristics of the semantics captured by the tested embeddings.Finally, we show the impact of varying the number of dimensions and theresolution of each dimension on the effective useful features captured by theembedding space. Our contributions highlight the importance of embeddings forNLP tasks and the effect of their quality on the final results.
arxiv-2700-171 | Block Coordinate Descent for Sparse NMF | http://arxiv.org/pdf/1301.3527v2.pdf | author:Vamsi K. Potluru, Sergey M. Plis, Jonathan Le Roux, Barak A. Pearlmutter, Vince D. Calhoun, Thomas P. Hayes category:cs.LG cs.NA published:2013-01-15 summary:Nonnegative matrix factorization (NMF) has become a ubiquitous tool for dataanalysis. An important variant is the sparse NMF problem which arises when weexplicitly require the learnt features to be sparse. A natural measure ofsparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms,such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, basedon intuitive attributes that such measures need to satisfy. This is in contrastto computationally cheaper alternatives such as the plain L$_1$ norm. However,present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slowand other formulations for sparse NMF have been proposed such as those based onL$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed normsparsity constraints while not sacrificing computation time. We presentexperimental evidence on real-world datasets that shows our new algorithmperforms an order of magnitude faster compared to the current state-of-the-artsolvers optimizing the mixed norm and is suitable for large-scale datasets.
arxiv-2700-172 | Learnable Pooling Regions for Image Classification | http://arxiv.org/pdf/1301.3516v3.pdf | author:Mateusz Malinowski, Mario Fritz category:cs.CV cs.LG published:2013-01-15 summary:Biologically inspired, from the early HMAX model to Spatial Pyramid Matching,pooling has played an important role in visual recognition pipelines. Spatialpooling, by grouping of local codes, equips these methods with a certain degreeof robustness to translation and deformation yet preserving important spatialinformation. Despite the predominance of this approach in current recognitionsystems, we have seen little progress to fully adapt the pooling strategy tothe task at hand. This paper proposes a model for learning task dependentpooling scheme -- including previously proposed hand-crafted pooling schemes asa particular instantiation. In our work, we investigate the role of differentregularization terms showing that the smooth regularization term is crucial toachieve strong performance using the presented architecture. Finally, wepropose an efficient and parallel method to train the model. Our experimentsshow improved performance over hand-crafted pooling schemes on the CIFAR-10 andCIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% onthe latter.
arxiv-2700-173 | The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization | http://arxiv.org/pdf/1301.3389v2.pdf | author:Hugo Van hamme category:cs.NA cs.LG published:2013-01-15 summary:Non-negative matrix factorization (NMF) has become a popular machine learningapproach to many problems in text mining, speech and image processing,bio-informatics and seismic data analysis to name a few. In NMF, a matrix ofnon-negative data is approximated by the low-rank product of two matrices withnon-negative entries. In this paper, the approximation quality is measured bythe Kullback-Leibler divergence between the data and its low-rankreconstruction. The existence of the simple multiplicative update (MU)algorithm for computing the matrix factors has contributed to the success ofNMF. Despite the availability of algorithms showing faster convergence, MUremains popular due to its simplicity. In this paper, a diagonalized Newtonalgorithm (DNA) is proposed showing faster convergence while the implementationremains simple and suitable for high-rank problems. The DNA algorithm isapplied to various publicly available data sets, showing a substantial speed-upon modern hardware.
arxiv-2700-174 | Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences | http://arxiv.org/pdf/1301.3323v4.pdf | author:Sainbayar Sukhbaatar, Takaki Makino, Kazuyuki Aihara category:cs.CV cs.LG published:2013-01-15 summary:Learning invariant representations from images is one of the hardestchallenges facing computer vision. Spatial pooling is widely used to createinvariance to spatial shifting, but it is restricted to convolutional models.In this paper, we propose a novel pooling method that can learn soft clusteringof features from image sequences. It is trained to improve the temporalcoherence of features, while keeping the information loss at minimum. Ourmethod does not use spatial information, so it can be used withnon-convolutional models too. Experiments on images extracted from naturalvideos showed that our method can cluster similar features together. Whentrained by convolutional features, auto-pooling outperformed traditionalspatial pooling on an image classification task, even though it does not usethe spatial topology of features.
arxiv-2700-175 | SpeedRead: A Fast Named Entity Recognition Pipeline | http://arxiv.org/pdf/1301.2857v1.pdf | author:Rami Al-Rfou', Steven Skiena category:cs.CL published:2013-01-14 summary:Online content analysis employs algorithmic methods to identify entities inunstructured text. Both machine learning and knowledge-base approaches lie atthe foundation of contemporary named entities extraction systems. However, theprogress in deploying these approaches on web-scale has been been hampered bythe computational cost of NLP over massive text corpora. We present SpeedRead(SR), a named entity recognition pipeline that runs at least 10 times fasterthan Stanford NLP pipeline. This pipeline consists of a high performance PennTreebank- compliant tokenizer, close to state-of-art part-of-speech (POS)tagger and knowledge-based named entity recognizer.
arxiv-2700-176 | Wavelet-based Scale Saliency | http://arxiv.org/pdf/1301.2884v1.pdf | author:Anh Cat Le Ngo, Kenneth Li-Minn Ang, Jasmine Kah-Phooi Seng, Guoping Qiu category:cs.CV published:2013-01-14 summary:Both pixel-based scale saliency (PSS) and basis project methods focus onmultiscale analysis of data content and structure. Their theoretical relationsand practical combination are previously discussed. However, no models haveever been proposed for calculating scale saliency on basis-projecteddescriptors since then. This paper extend those ideas into mathematical modelsand implement them in the wavelet-based scale saliency (WSS). While PSS usespixel-value descriptors, WSS treats wavelet sub-bands as basis descriptors. Thepaper discusses different wavelet descriptors: discrete wavelet transform(DWT), wavelet packet transform (DWPT), quaternion wavelet transform (QWT) andbest basis quaternion wavelet packet transform (QWPTBB). WSS saliency maps ofdifferent descriptors are generated and compared against other saliency methodsby both quantitative and quanlitative methods. Quantitative results, ROCcurves, AUC values and NSS values are collected from simulations on Bruce andKootstra image databases with human eye-tracking data as ground-truth.Furthermore, qualitative visual results of saliency maps are analyzed andcompared against each other as well as eye-tracking data inclusive in thedatabases.
arxiv-2700-177 | Fano schemes of generic intersections and machine learning | http://arxiv.org/pdf/1301.3078v1.pdf | author:Franz Király, Paul Larsen category:math.AG stat.ML published:2013-01-14 summary:We investigate Fano schemes of conditionally generic intersections, i.e. ofhypersurfaces in projective space chosen generically up to additionalconditions. Via a correspondence between generic properties of algebraicvarieties and events in probability spaces that occur with probability one, weuse the obtained results on Fano schemes to solve a problem in machinelearning.
arxiv-2700-178 | Eléments pour une théorie des réseaux en phase d'apprentissage | http://arxiv.org/pdf/1301.2959v2.pdf | author:Jean Piniello category:nlin.AO cs.NE nlin.CD published:2013-01-14 summary:This study deals with the evolution of the so called intelligent networks(insect society without leader, cells of an organism, brain...) during theirapprenticeship period. The used formalism draws one's inspiration from the oneof the Quantum field theory (Principle of stationary action, gauge fields,invariance by symmetry transformations...). After a recall of some definitions,we consider at first the free network, that is to say which does not exchangeany information with outside. Then we study the evolution of the networkconnected with its environment, that is to say immersed into an informationfield created by this environment which so dictates to it the apprenticeshipconstraints. At that time, we obtain Lagrange equations which solutionsdescribe the network evolution during the whole apprenticeship period. Finally,while proceeding with the same formalism inspiration, we suggest other studyways capable of evolving the knowledge in the considered scope.
arxiv-2700-179 | Unsupervised Feature Learning for low-level Local Image Descriptors | http://arxiv.org/pdf/1301.2840v4.pdf | author:Christian Osendorfer, Justin Bayer, Sebastian Urban, Patrick van der Smagt category:cs.CV cs.LG stat.ML published:2013-01-14 summary:Unsupervised feature learning has shown impressive results for a wide rangeof input modalities, in particular for object classification tasks in computervision. Using a large amount of unlabeled data, unsupervised feature learningmethods are utilized to construct high-level representations that arediscriminative enough for subsequently trained supervised classificationalgorithms. However, it has never been \emph{quantitatively} investigated yethow well unsupervised learning methods can find \emph{low-levelrepresentations} for image patches without any additional supervision. In thispaper we examine the performance of pure unsupervised methods on a low-levelcorrespondence task, a problem that is central to many Computer Visionapplications. We find that a special type of Restricted Boltzmann Machines(RBMs) performs comparably to hand-crafted descriptors. Additionally, a simplebinarization scheme produces compact representations that perform better thanseveral state-of-the-art descriptors.
arxiv-2700-180 | Cutting Recursive Autoencoder Trees | http://arxiv.org/pdf/1301.2811v3.pdf | author:Christian Scheible, Hinrich Schuetze category:cs.CL cs.AI published:2013-01-13 summary:Deep Learning models enjoy considerable success in Natural LanguageProcessing. While deep architectures produce useful representations that leadto improvements in various tasks, they are often difficult to interpret. Thismakes the analysis of learned structures particularly difficult. In this paper,we rely on empirical tests to see whether a particular structure makes sense.We present an analysis of the Semi-Supervised Recursive Autoencoder, awell-known model that produces structural representations of text. We show thatfor certain tasks, the structure of the autoencoder can be significantlyreduced without loss of classification accuracy and we evaluate the producedstructures using human judgment.
arxiv-2700-181 | A comparison of SVM and RVM for Document Classification | http://arxiv.org/pdf/1301.2785v1.pdf | author:Muhammad Rafi, Mohammad Shahid Shaikh category:cs.IR cs.LG published:2013-01-13 summary:Document classification is a task of assigning a new unclassified document toone of the predefined set of classes. The content based document classificationuses the content of the document with some weighting criteria to assign it toone of the predefined classes. It is a major task in library science,electronic document management systems and information sciences. This paperinvestigates document classification by using two different classificationtechniques (1) Support Vector Machine (SVM) and (2) Relevance Vector Machine(RVM). SVM is a supervised machine learning technique that can be used forclassification task. In its basic form, SVM represents the instances of thedata into space and tries to separate the distinct classes by a maximumpossible wide gap (hyper plane) that separates the classes. On the other handRVM uses probabilistic measure to define this separation space. RVM usesBayesian inference to obtain succinct solution, thus RVM uses significantlyfewer basis functions. Experimental studies on three standard textclassification datasets reveal that although RVM takes more training time, itsclassification is much better as compared to SVM.
arxiv-2700-182 | Clustering Learning for Robotic Vision | http://arxiv.org/pdf/1301.2820v3.pdf | author:Eugenio Culurciello, Jordan Bates, Aysegul Dundar, Jose Carrasco, Clement Farabet category:cs.CV published:2013-01-13 summary:We present the clustering learning technique applied to multi-layerfeedforward deep neural networks. We show that this unsupervised learningtechnique can compute network filters with only a few minutes and a muchreduced set of parameters. The goal of this paper is to promote the techniquefor general-purpose robotic vision systems. We report its use in static imagedatasets and object tracking datasets. We show that networks trained withclustering learning can outperform large networks trained for many hours oncomplex datasets.
arxiv-2700-183 | Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models | http://arxiv.org/pdf/1301.2724v2.pdf | author:Manfred Opper, Ulrich Paquet, Ole Winther category:stat.ML published:2013-01-12 summary:Expectation Propagation (EP) provides a framework for approximate inference.When the model under consideration is over a latent Gaussian field, with theapproximation being Gaussian, we show how these approximations cansystematically be corrected. A perturbative expansion is made of the exact butintractable correction, and can be applied to the model's partition functionand other moments of interest. The correction is expressed over thehigher-order cumulants which are neglected by EP's local matching of moments.Through the expansion, we see that EP is correct to first order. By consideringhigher orders, corrections of increasing polynomial complexity can be appliedto the approximation. The second order provides a correction in quadratic time,which we apply to an array of Gaussian process and Ising models. Thecorrections generalize to arbitrarily complex approximating families, which weillustrate on tree-structured Ising model approximations. Furthermore, theyprovide a polynomial-time assessment of the approximation error. We alsoprovide both theoretical and practical insights on the exactness of the EPsolution.
arxiv-2700-184 | Multiple functional regression with both discrete and continuous covariates | http://arxiv.org/pdf/1301.2656v1.pdf | author:Hachem Kadri, Philippe Preux, Emmanuel Duflos, Stéphane Canu category:stat.ML cs.LG published:2013-01-12 summary:In this paper we present a nonparametric method for extending functionalregression methodology to the situation where more than one functionalcovariate is used to predict a functional response. Borrowing the idea fromKadri et al. (2010a), the method, which support mixed discrete and continuousexplanatory variables, is based on estimating a function-valued function inreproducing kernel Hilbert spaces by virtue of positive operator-valuedkernels.
arxiv-2700-185 | A Triclustering Approach for Time Evolving Graphs | http://arxiv.org/pdf/1301.2659v1.pdf | author:Romain Guigourès, Marc Boullé, Fabrice Rossi category:cs.LG cs.SI stat.ML published:2013-01-12 summary:This paper introduces a novel technique to track structures in time evolvinggraphs. The method is based on a parameter free approach for three-dimensionalco-clustering of the source vertices, the target vertices and the time. Allthese features are simultaneously segmented in order to build time segments andclusters of vertices whose edge distributions are similar and evolve in thesame way over the time segments. The main novelty of this approach lies in thatthe time segments are directly inferred from the evolution of the edgedistribution between the vertices, thus not requiring the user to make an apriori discretization. Experiments conducted on a synthetic dataset illustratethe good behaviour of the technique, and a study of a real-life dataset showsthe potential of the proposed approach for exploratory data analysis.
arxiv-2700-186 | Functional Regularized Least Squares Classi cation with Operator-valued Kernels | http://arxiv.org/pdf/1301.2655v1.pdf | author:Hachem Kadri, Asma Rabaoui, Philippe Preux, Emmanuel Duflos, Alain Rakotomamonjy category:cs.LG stat.ML published:2013-01-12 summary:Although operator-valued kernels have recently received increasing interestin various machine learning and functional data analysis problems such asmulti-task learning or functional regression, little attention has been paid tothe understanding of their associated feature spaces. In this paper, we explorethe potential of adopting an operator-valued kernel feature space perspectivefor the analysis of functional data. We then extend the Regularized LeastSquares Classification (RLSC) algorithm to cover situations where there aremultiple functions per observation. Experiments on a sound recognition problemshow that the proposed method outperforms the classical RLSC algorithm.
arxiv-2700-187 | Binocular disparity as an explanation for the moon illusion | http://arxiv.org/pdf/1301.2715v1.pdf | author:Joseph Antonides, Toshiro Kubota category:cs.CV physics.pop-ph published:2013-01-12 summary:We present another explanation for the moon illusion, in which the moon lookslarger near the horizon than near the zenith. In our model, the sky isconsidered a spatially contiguous and geometrically smooth surface. When anobject (like the moon) breaks the contiguity of the surface, humans perceive anocclusion of the surface rather than an object appearing through a hole.Binocular vision dictates that the moon is distant, but this perception modeldictates that the moon is closer than the sky. To solve the dilemma, the braindistorts the projections of the moon to increase the binocular disparity, whichresults in increase of the angular size of the moon. The degree of thedistortion depends upon the apparent distance to the sky, which is influencedby the surrounding objects and the condition of the sky. The closer the skyappears, the stronger the illusion. At the zenith, few distance cues arepresent, causing difficulty with distance estimation and weakening theillusion.
arxiv-2700-188 | BliStr: The Blind Strategymaker | http://arxiv.org/pdf/1301.2683v2.pdf | author:Josef Urban category:cs.AI cs.LG cs.LO published:2013-01-12 summary:BliStr is a system that automatically develops strategies for E prover on alarge set of problems. The main idea is to interleave (i) iteratedlow-timelimit local search for new strategies on small sets of similar easyproblems with (ii) higher-timelimit evaluation of the new strategies on allproblems. The accumulated results of the global higher-timelimit runs are usedto define and evolve the notion of "similar easy problems", and to control theselection of the next strategy to be improved. The technique was used tosignificantly strengthen the set of E strategies used by the MaLARea, PS-E,E-MaLeS, and E systems in the CASC@Turing 2012 competition, particularly in theMizar division. Similar improvement was obtained on the problems created fromthe Flyspeck corpus.
arxiv-2700-189 | Robust High Dimensional Sparse Regression and Matching Pursuit | http://arxiv.org/pdf/1301.2725v1.pdf | author:Yudong Chen, Constantine Caramanis, Shie Mannor category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH published:2013-01-12 summary:We consider high dimensional sparse regression, and develop strategies ableto deal with arbitrary -- possibly, severe or coordinated -- errors in thecovariance matrix $X$. These may come from corrupted data, persistentexperimental errors, or malicious respondents in surveys/recommender systems,etc. Such non-stochastic error-in-variables problems are notoriously difficultto treat, and as we demonstrate, the problem is particularly pronounced inhigh-dimensional settings where the primary goal is {\em support recovery} ofthe sparse regressor. We develop algorithms for support recovery in sparseregression, when some number $n_1$ out of $n+n_1$ total covariate/responsepairs are {\it arbitrarily (possibly maliciously) corrupted}. We are interestedin understanding how many outliers, $n_1$, we can tolerate, while identifyingthe correct support. To the best of our knowledge, neither standard outlierrejection techniques, nor recently developed robust regression algorithms (thatfocus only on corrupted response variables), nor recent algorithms for dealingwith stochastic noise or erasures, can provide guarantees on support recovery.Perhaps surprisingly, we also show that the natural brute force algorithm thatsearches over all subsets of $n$ covariate/response pairs, and all subsets ofpossible support coordinates in order to minimize regression error, isremarkably poor, unable to correctly identify the support with even $n_1 =O(n/k)$ corrupted points, where $k$ is the sparsity. This is true even in thebasic setting we consider, where all authentic measurements and noise areindependent and sub-Gaussian. In this setting, we provide a simple algorithm --no more computationally taxing than OMP -- that gives stronger performanceguarantees, recovering the support with up to $n_1 = O(n/(\sqrt{k} \log p))$corrupted points, where $p$ is the dimension of the signal to be recovered.
arxiv-2700-190 | Computational Intelligence for Deepwater Reservoir Depositional Environments Interpretation | http://arxiv.org/pdf/1301.2638v1.pdf | author:Tina Yu, Dave Wilkinson, Julian Clark, Morgan Sullivan category:cs.NE physics.geo-ph published:2013-01-12 summary:Predicting oil recovery efficiency of a deepwater reservoir is a challengingtask. One approach to characterize a deepwater reservoir and to predict itsproducibility is by analyzing its depositional information. This researchproposes a deposition-based stratigraphic interpretation framework fordeepwater reservoir characterization. In this framework, one critical task isthe identification and labeling of the stratigraphic components in thereservoir, according to their depositional environments. This interpretationprocess is labor intensive and can produce different results depending on thestratigrapher who performs the analysis. To relieve stratigrapher's workloadand to produce more consistent results, we have developed a novel methodologyto automate this process using various computational intelligence techniques.Using a well log data set, we demonstrate that the developed methodology andthe designed workflow can produce finite state transducer models that interpretdeepwater reservoir depositional environments adequately.
arxiv-2700-191 | Information field theory | http://arxiv.org/pdf/1301.2556v1.pdf | author:Torsten Enßlin category:astro-ph.IM cs.IT math.IT stat.ML published:2013-01-11 summary:Non-linear image reconstruction and signal analysis deal with complex inverseproblems. To tackle such problems in a systematic way, I present informationfield theory (IFT) as a means of Bayesian, data based inference on spatiallydistributed signal fields. IFT is a statistical field theory, which permits theconstruction of optimal signal recovery algorithms even for non-linear andnon-Gaussian signal inference problems. IFT algorithms exploit spatialcorrelations of the signal fields and benefit from techniques developed toinvestigate quantum and statistical field theories, such as Feynman diagrams,re-normalisation calculations, and thermodynamic potentials. The theory can beused in many areas, and applications in cosmology and numerics are presented.
arxiv-2700-192 | Dating medieval English charters | http://arxiv.org/pdf/1301.2405v1.pdf | author:Gelila Tilahun, Andrey Feuerverger, Michael Gervers category:stat.AP cs.CL published:2013-01-11 summary:Deeds, or charters, dealing with property rights, provide a continuousdocumentation which can be used by historians to study the evolution of social,economic and political changes. This study is concerned with charters (writtenin Latin) dating from the tenth through early fourteenth centuries in England.Of these, at least one million were left undated, largely due to administrativechanges introduced by William the Conqueror in 1066. Correctly dating suchcharters is of vital importance in the study of English medieval history. Thispaper is concerned with computer-automated statistical methods for dating suchdocument collections, with the goal of reducing the considerable effortsrequired to date them manually and of improving the accuracy of assigned dates.Proposed methods are based on such data as the variation over time of word andphrase usage, and on measures of distance between documents. The extensive (anddated) Documents of Early England Data Set (DEEDS) maintained at the Universityof Toronto was used for this purpose.
arxiv-2700-193 | Backward-in-Time Selection of the Order of Dynamic Regression Prediction Model | http://arxiv.org/pdf/1301.2410v1.pdf | author:Ioannis Vlachos, Dimitris Kugiumtzis category:stat.AP stat.ME stat.ML published:2013-01-11 summary:We investigate the optimal structure of dynamic regression models used inmultivariate time series prediction and propose a scheme to form the laggedvariable structure called Backward-in-Time Selection (BTS) that takes intoaccount feedback and multi-collinearity, often present in multivariate timeseries. We compare BTS to other known methods, also in conjunction withregularization techniques used for the estimation of model parameters, namelyprincipal components, partial least squares and ridge regression estimation.The predictive efficiency of the different models is assessed by means of MonteCarlo simulations for different settings of feedback and multi-collinearity.The results show that BTS has consistently good prediction performance whileother popular methods have varying and often inferior performance. Theprediction performance of BTS was also found the best when tested on humanelectroencephalograms of an epileptic seizure, and to the prediction of returnsof indices of world financial markets.
arxiv-2700-194 | Determining token sequence mistakes in responses to questions with open text answer | http://arxiv.org/pdf/1301.2466v1.pdf | author:Oleg Sychev, Dmitry Mamontov category:cs.CL cs.CY K.3.2 published:2013-01-11 summary:When learning grammar of the new language, a teacher should routinely checkstudent's exercises for grammatical correctness. The paper describes a methodof automatically detecting and reporting grammar mistakes, regarding an orderof tokens in the response. It could report extra tokens, missing tokens andmisplaced tokens. The method is useful when teaching language, where order oftokens is important, which includes most formal languages and some natural ones(like English). The method was implemented in a question type plug-inCorrectWriting for the widely used learning manage system Moodle.
arxiv-2700-195 | TEI and LMF crosswalks | http://arxiv.org/pdf/1301.2444v3.pdf | author:Laurent Romary category:cs.CL published:2013-01-11 summary:The present paper explores various arguments in favour of making the TextEncoding Initia-tive (TEI) guidelines an appropriate serialisation for ISOstandard 24613:2008 (LMF, Lexi-cal Mark-up Framework) . It also identifies theissues that would have to be resolved in order to reach an appropriateimplementation of these ideas, in particular in terms of infor-mationalcoverage. We show how the customisation facilities offered by the TEIguidelines can provide an adequate background, not only to cover missingcomponents within the current Dictionary chapter of the TEI guidelines, butalso to allow specific lexical projects to deal with local constraints. Weexpect this proposal to be a basis for a future ISO project in the context ofthe on going revision of LMF.
arxiv-2700-196 | Robust Text Detection in Natural Scene Images | http://arxiv.org/pdf/1301.2628v3.pdf | author:Xu-Cheng Yin, Xuwang Yin, Kaizhu Huang, Hong-Wei Hao category:cs.CV cs.IR cs.LG I.5.4 published:2013-01-11 summary:Text detection in natural scene images is an important prerequisite for manycontent-based image analysis tasks. In this paper, we propose an accurate androbust method for detecting texts in natural scene images. A fast and effectivepruning algorithm is designed to extract Maximally Stable Extremal Regions(MSERs) as character candidates using the strategy of minimizing regularizedvariations. Character candidates are grouped into text candidates by theingle-link clustering algorithm, where distance weights and threshold of theclustering algorithm are learned automatically by a novel self-trainingdistance metric learning algorithm. The posterior probabilities of textcandidates corresponding to non-text are estimated with an characterclassifier; text candidates with high probabilities are then eliminated andfinally texts are identified with a text classifier. The proposed system isevaluated on the ICDAR 2011 Robust Reading Competition dataset; the f measureis over 76% and is significantly better than the state-of-the-art performanceof 71%. Experimental results on a publicly available multilingual dataset alsoshow that our proposed method can outperform the other competitive method withthe f measure increase of over 9 percent. Finally, we have setup an online demoof our proposed scene text detection system athttp://kems.ustb.edu.cn/learning/yin/dtext.
arxiv-2700-197 | Robust subspace clustering | http://arxiv.org/pdf/1301.2603v3.pdf | author:Mahdi Soltanolkotabi, Ehsan Elhamifar, Emmanuel J. Candès category:cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH published:2013-01-11 summary:Subspace clustering refers to the task of finding a multi-subspacerepresentation that best fits a collection of points taken from ahigh-dimensional space. This paper introduces an algorithm inspired by sparsesubspace clustering (SSC) [In IEEE Conference on Computer Vision and PatternRecognition, CVPR (2009) 2790-2797] to cluster noisy data, and develops somenovel theory demonstrating its correctness. In particular, the theory usesideas from geometric functional analysis to show that the algorithm canaccurately recover the underlying subspaces under minimal requirements on theirorientation, and on the number of samples per subspace. Synthetic as well asreal data experiments complement our theoretical study, illustrating ourapproach and demonstrating its effectiveness.
arxiv-2700-198 | Learning to Optimize Via Posterior Sampling | http://arxiv.org/pdf/1301.2609v5.pdf | author:Daniel Russo, Benjamin Van Roy category:cs.LG published:2013-01-11 summary:This paper considers the use of a simple posterior sampling algorithm tobalance between exploration and exploitation when learning to optimize actionssuch as in multi-armed bandit problems. The algorithm, also known as ThompsonSampling, offers significant advantages over the popular upper confidence bound(UCB) approach, and can be applied to problems with finite or infinite actionspaces and complicated relationships among action rewards. We make twotheoretical contributions. The first establishes a connection between posteriorsampling and UCB algorithms. This result lets us convert regret boundsdeveloped for UCB algorithms into Bayesian regret bounds for posteriorsampling. Our second theoretical contribution is a Bayesian regret bound forposterior sampling that applies broadly and can be specialized to many modelclasses. This bound depends on a new notion we refer to as the eluderdimension, which measures the degree of dependence among action rewards.Compared to UCB algorithm Bayesian regret bounds for specific model classes,our general bound matches the best available for linear models and is strongerthan the best available for generalized linear models. Further, our analysisprovides insight into performance advantages of posterior sampling, which arehighlighted through simulation results that demonstrate performance surpassingrecently proposed UCB algorithms.
arxiv-2700-199 | Comparision and analysis of photo image forgery detection techniques | http://arxiv.org/pdf/1302.3119v1.pdf | author:S. Murali, Govindraj B. Chittapur, Prabhakara H. S, Basavaraj S. Anami category:cs.CV cs.CR cs.MM published:2013-01-10 summary:Digital Photo images are everywhere, on the covers of magazines, innewspapers, in courtrooms, and all over the Internet. We are exposed to themthroughout the day and most of the time. Ease with which images can bemanipulated; we need to be aware that seeing does not always imply believing.We propose methodologies to identify such unbelievable photo images andsucceeded to identify forged region by given only the forged image. Formats areadditive tag for every file system and contents are relatively expressed withextension based on most popular digital camera uses JPEG and Other imageformats like png, bmp etc. We have designed algorithm running behind with theconcept of abnormal anomalies and identify the forgery regions.
arxiv-2700-200 | Error Correction in Learning using SVMs | http://arxiv.org/pdf/1301.2012v1.pdf | author:Srivatsan Laxman, Sushil Mittal, Ramarathnam Venkatesan category:cs.LG published:2013-01-10 summary:This paper is concerned with learning binary classifiers under adversariallabel-noise. We introduce the problem of error-correction in learning where thegoal is to recover the original clean data from a label-manipulated version ofit, given (i) no constraints on the adversary other than an upper-bound on thenumber of errors, and (ii) some regularity properties for the original data. Wepresent a simple and practical error-correction algorithm called SubSVMs thatlearns individual SVMs on several small-size (log-size), class-balanced, randomsubsets of the data and then reclassifies the training points using a majorityvote. Our analysis reveals the need for the two main ingredients of SubSVMs,namely class-balanced sampling and subsampled bagging. Experimental results onsynthetic as well as benchmark UCI data demonstrate the effectiveness of ourapproach. In addition to noise-tolerance, log-size subsampled bagging alsoyields significant run-time benefits over standard SVMs.
arxiv-2700-201 | Heteroscedastic Relevance Vector Machine | http://arxiv.org/pdf/1301.2015v1.pdf | author:Daniel Khashabi, Mojtaba Ziyadi, Feng Liang category:stat.ML cs.LG published:2013-01-10 summary:In this work we propose a heteroscedastic generalization to RVM, a fastBayesian framework for regression, based on some recent similar works. We usevariational approximation and expectation propagation to tackle the problem.The work is still under progress and we are examining the results and comparingwith the previous works.
arxiv-2700-202 | Training Effective Node Classifiers for Cascade Classification | http://arxiv.org/pdf/1301.2032v1.pdf | author:Chunhua Shen, Peng Wang, Sakrapee Paisitkriangkrai, Anton van den Hengel category:cs.CV cs.LG stat.ML published:2013-01-10 summary:Cascade classifiers are widely used in real-time object detection. Differentfrom conventional classifiers that are designed for a low overallclassification error rate, a classifier in each node of the cascade is requiredto achieve an extremely high detection rate and moderate false positive rate.Although there are a few reported methods addressing this requirement in thecontext of object detection, there is no principled feature selection methodthat explicitly takes into account this asymmetric node learning objective. Weprovide such an algorithm here. We show that a special case of the biasedminimax probability machine has the same formulation as the linear asymmetricclassifier (LAC) of Wu et al (2005). We then design a new boosting algorithmthat directly optimizes the cost function of LAC. The resultingtotally-corrective boosting algorithm is implemented by the column generationtechnique in convex optimization. Experimental results on object detectionverify the effectiveness of the proposed boosting algorithm as a nodeclassifier in cascade object detection, and show performance better than thatof the current state-of-the-art.
arxiv-2700-203 | Learning the Dimensionality of Hidden Variables | http://arxiv.org/pdf/1301.2269v1.pdf | author:Gal Elidan, Nir Friedman category:cs.LG cs.AI stat.ML published:2013-01-10 summary:A serious problem in learning probabilistic models is the presence of hiddenvariables. These variables are not observed, yet interact with several of theobserved variables. Detecting hidden variables poses two problems: determiningthe relations to other variables in the model and determining the number ofstates of the hidden variable. In this paper, we address the latter problem inthe context of Bayesian networks. We describe an approach that utilizes ascore-based agglomerative state-clustering. As we show, this approach allows usto efficiently evaluate models with a range of cardinalities for the hiddenvariable. We show how to extend this procedure to deal with multipleinteracting hidden variables. We demonstrate the effectiveness of this approachby evaluating it on synthetic and real-life data. We show that our approachlearns models with hidden variables that generalize better and have betterstructure than previous approaches.
arxiv-2700-204 | Application of Hopfield Network to Saccades | http://arxiv.org/pdf/1301.2351v1.pdf | author:Teruyoshi Washizawa category:cs.CV q-bio.NC published:2013-01-10 summary:Human eye movement mechanisms (saccades) are very useful for scene analysis,including object representation and pattern recognition. In this letter, aHopfield neural network to emulate saccades is proposed. The network uses anenergy function that includes location and identification tasks. Computersimulation shows that the network performs those tasks cooperatively. Theresult suggests that the network is applicable to shift-invariant patternrecognition.
arxiv-2700-205 | Incorporating Expressive Graphical Models in Variational Approximations: Chain-Graphs and Hidden Variables | http://arxiv.org/pdf/1301.2268v1.pdf | author:Tal El-Hay, Nir Friedman category:cs.AI cs.LG published:2013-01-10 summary:Global variational approximation methods in graphical models allow efficientapproximate inference of complex posterior distributions by using a simplermodel. The choice of the approximating model determines a tradeoff between thecomplexity of the approximation procedure and the quality of the approximation.In this paper, we consider variational approximations based on two classes ofmodels that are richer than standard Bayesian networks, Markov networks ormixture models. As such, these classes allow to find better tradeoffs in thespectrum of approximations. The first class of models are chain graphs, whichcapture distributions that are partially directed. The second class of modelsare directed graphs (Bayesian networks) with additional latent variables. Bothclasses allow representation of multi-variable dependencies that cannot beeasily represented within a Bayesian network.
arxiv-2700-206 | Planning by Prioritized Sweeping with Small Backups | http://arxiv.org/pdf/1301.2343v1.pdf | author:Harm van Seijen, Richard S. Sutton category:cs.AI cs.LG published:2013-01-10 summary:Efficient planning plays a crucial role in model-based reinforcementlearning. Traditionally, the main planning operation is a full backup based onthe current estimates of the successor states. Consequently, its computationtime is proportional to the number of successor states. In this paper, weintroduce a new planning backup that uses only the current value of a singlesuccessor state and has a computation time independent of the number ofsuccessor states. This new backup, which we call a small backup, opens the doorto a new class of model-based reinforcement learning methods that exhibit muchfiner control over their planning process than traditional methods. Weempirically demonstrate that this increased flexibility allows for moreefficient planning by showing that an implementation of prioritized sweepingbased on small backups achieves a substantial performance improvement overclassical implementations.
arxiv-2700-207 | Network-based clustering with mixtures of L1-penalized Gaussian graphical models: an empirical investigation | http://arxiv.org/pdf/1301.2194v1.pdf | author:Steven M. Hill, Sach Mukherjee category:stat.ML cs.LG stat.ME published:2013-01-10 summary:In many applications, multivariate samples may harbor previously unrecognizedheterogeneity at the level of conditional independence or network structure.For example, in cancer biology, disease subtypes may differ with respect tosubtype-specific interplay between molecular components. Then, both subtypediscovery and estimation of subtype-specific networks present important andrelated challenges. To enable such analyses, we put forward a mixture modelwhose components are sparse Gaussian graphical models. This brings togethermodel-based clustering and graphical modeling to permit simultaneous estimationof cluster assignments and cluster-specific networks. We carry out estimationwithin an L1-penalized framework, and investigate several specific penalizationregimes. We present empirical results on simulated data and provide generalrecommendations for the formulation and use of mixtures of L1-penalizedGaussian graphical models.
arxiv-2700-208 | Using Temporal Data for Making Recommendations | http://arxiv.org/pdf/1301.2320v1.pdf | author:Andrew Zimdars, David Maxwell Chickering, Christopher Meek category:cs.IR cs.AI cs.LG published:2013-01-10 summary:We treat collaborative filtering as a univariate time series estimationproblem: given a user's previous votes, predict the next vote. We describe twofamilies of methods for transforming data to encode time order in ways amenableto off-the-shelf classification and density estimation tools, and examine theresults of using these approaches on several real-world data sets. Theimprovements in predictive accuracy we realize recommend the use of otherpredictive algorithms that exploit the temporal order of data.
arxiv-2700-209 | Statistical Modeling in Continuous Speech Recognition (CSR)(Invited Talk) | http://arxiv.org/pdf/1301.2318v1.pdf | author:Steve Young category:cs.LG cs.AI stat.ML published:2013-01-10 summary:Automatic continuous speech recognition (CSR) is sufficiently mature that avariety of real world applications are now possible including large vocabularytranscription and interactive spoken dialogues. This paper reviews theevolution of the statistical modelling techniques which underlie current-daysystems, specifically hidden Markov models (HMMs) and N-grams. Starting from adescription of the speech signal and its parameterisation, the variousmodelling assumptions and their consequences are discussed. It then describesvarious techniques by which the effects of these assumptions can be mitigated.Despite the progress that has been made, the limitations of current modellingtechniques are still evident. The paper therefore concludes with a brief reviewof some of the more fundamental modelling work now in progress.
arxiv-2700-210 | Belief Optimization for Binary Networks: A Stable Alternative to Loopy Belief Propagation | http://arxiv.org/pdf/1301.2317v1.pdf | author:Max Welling, Yee Whye Teh category:cs.AI cs.LG published:2013-01-10 summary:We present a novel inference algorithm for arbitrary, binary, undirectedgraphs. Unlike loopy belief propagation, which iterates fixed point equations,we directly descend on the Bethe free energy. The algorithm consists of twophases, first we update the pairwise probabilities, given the marginalprobabilities at each unit,using an analytic expression. Next, we update themarginal probabilities, given the pairwise probabilities by following thenegative gradient of the Bethe free energy. Both steps are guaranteed todecrease the Bethe free energy, and since it is lower bounded, the algorithm isguaranteed to converge to a local minimum. We also show that the Bethe freeenergy is equal to the TAP free energy up to second order in the weights. Inexperiments we confirm that when belief propagation converges it usually findsidentical solutions as our belief optimization method. However, in cases wherebelief propagation fails to converge, belief optimization continues to convergeto reasonable beliefs. The stable nature of belief optimization makes itideally suited for learning graphical models from data.
arxiv-2700-211 | Cross-covariance modelling via DAGs with hidden variables | http://arxiv.org/pdf/1301.2316v1.pdf | author:Jacob A. Wegelin, Thomas S. Richardson category:cs.LG stat.ML published:2013-01-10 summary:DAG models with hidden variables present many difficulties that are notpresent when all nodes are observed. In particular, fully observed DAG modelsare identified and correspond to well-defined sets ofdistributions, whereasthis is not true if nodes are unobserved. Inthis paper we characterize exactlythe set of distributions given by a class of one-dimensional Gaussian latentvariable models. These models relate two blocks of observed variables, modelingonly the cross-covariance matrix. We describe the relation of this model to thesingular value decomposition of the cross-covariance matrix. We show that,although the model is underidentified, useful information may be extracted. Wefurther consider an alternative parametrization in which one latent variable isassociated with each block. Our analysis leads to some novel covarianceequivalence results for Gaussian hidden variable models.
arxiv-2700-212 | The Optimal Reward Baseline for Gradient-Based Reinforcement Learning | http://arxiv.org/pdf/1301.2315v1.pdf | author:Lex Weaver, Nigel Tao category:cs.LG cs.AI stat.ML published:2013-01-10 summary:There exist a number of reinforcement learning algorithms which learnbyclimbing the gradient of expected reward. Their long-runconvergence has beenproved, even in partially observableenvironments with non-deterministicactions, and without the need fora system model. However, the variance of thegradient estimator hasbeen found to be a significant practical problem. Recentapproacheshave discounted future rewards, introducing a bias-variancetrade-offinto the gradient estimate. We incorporate a reward baseline intothelearning system, and show that it affects variance withoutintroducingfurther bias. In particular, as we approach thezero-bias,high-variance parameterization, the optimal (or varianceminimizing)constant reward baseline is equal to the long-term averageexpectedreward. Modified policy-gradient algorithms are presented, and anumberof experiments demonstrate their improvement over previous work.
arxiv-2700-213 | Maximum Likelihood Bounded Tree-Width Markov Networks | http://arxiv.org/pdf/1301.2311v1.pdf | author:Nathan Srebro category:cs.LG cs.AI stat.ML published:2013-01-10 summary:Chow and Liu (1968) studied the problem of learning a maximumlikelihoodMarkov tree. We generalize their work to more complexMarkov networks byconsidering the problem of learning a maximumlikelihood Markov network ofbounded complexity. We discuss howtree-width is in many ways the appropriatemeasure of complexity andthus analyze the problem of learning a maximumlikelihood Markovnetwork of bounded tree-width.Similar to the work of Chow andLiu, we are able to formalize thelearning problem as a combinatorialoptimization problem on graphs. Weshow that learning a maximum likelihoodMarkov network of boundedtree-width is equivalent to finding a maximum weighthypertree. Thisequivalence gives rise to global, integer-programmingbased,approximation algorithms with provable performance guarantees, forthelearning problem. This contrasts with heuristic local-searchalgorithms whichwere previously suggested (e.g. by Malvestuto 1991).The equivalence also allowsus to study the computational hardness ofthe learning problem. We show thatlearning a maximum likelihoodMarkov network of bounded tree-width is NP-hard,and discuss thehardness of approximation.
arxiv-2700-214 | Policy Improvement for POMDPs Using Normalized Importance Sampling | http://arxiv.org/pdf/1301.2310v1.pdf | author:Christian R. Shelton category:cs.AI cs.LG published:2013-01-10 summary:We present a new method for estimating the expected return of a POMDP fromexperience. The method does not assume any knowledge of the POMDP and allowsthe experience to be gathered from an arbitrary sequence of policies. Thereturn is estimated for any new policy of the POMDP. We motivate the estimatorfrom function-approximation and importance sampling points-of-view and deriveits theoretical properties. Although the estimator is biased, it has lowvariance and the bias is often irrelevant when the estimator is used forpair-wise comparisons. We conclude by extending the estimator to policies withmemory and compare its performance in a greedy search algorithm to REINFORCEalgorithms showing an order of magnitude reduction in the number of trialsrequired.
arxiv-2700-215 | Symmetric Collaborative Filtering Using the Noisy Sensor Model | http://arxiv.org/pdf/1301.2309v1.pdf | author:Rita Sharma, David L Poole category:cs.IR cs.LG published:2013-01-10 summary:Collaborative filtering is the process of making recommendations regardingthe potential preference of a user, for example shopping on the Internet, basedon the preference ratings of the user and a number of other users for variousitems. This paper considers collaborative filtering based onexplicitmulti-valued ratings. To evaluate the algorithms, weconsider only {empure} collaborative filtering, using ratings exclusively, and no otherinformation about the people or items.Our approach is to predict a user'spreferences regarding a particularitem by using other people who rated thatitem and other items ratedby the user as noisy sensors. The noisy sensor modeluses Bayes' theorem to compute the probability distribution for theuser'srating of a new item. We give two variant models: in one, we learn a{emclassical normal linear regression} model of how users rate items; inanother,we assume different users rate items the same, but the accuracy ofthesensors needs to be learned. We compare these variant modelswithstate-of-the-art techniques and show how they are significantlybetter,whether a user has rated only two items or many. We reportempiricalresults using the EachMovie databasefootnote{http://research.compaq.com/SRC/eachmovie/} of movie ratings. Wealsoshow that by considering items similarity along with theusers similarity, theaccuracy of the prediction increases.
arxiv-2700-216 | Probabilistic Models for Unified Collaborative and Content-Based Recommendation in Sparse-Data Environments | http://arxiv.org/pdf/1301.2303v1.pdf | author:Alexandrin Popescul, Lyle H. Ungar, David M Pennock, Steve Lawrence category:cs.IR cs.LG stat.ML published:2013-01-10 summary:Recommender systems leverage product and community information to targetproducts to consumers. Researchers have developed collaborative recommenders,content-based recommenders, and (largely ad-hoc) hybrid systems. We propose aunified probabilistic framework for merging collaborative and content-basedrecommendations. We extend Hofmann's [1999] aspect model to incorporatethree-way co-occurrence data among users, items, and item content. The relativeinfluence of collaboration data versus content data is not imposed as anexogenous parameter, but rather emerges naturally from the given data sources.Global probabilistic models coupled with standard Expectation Maximization (EM)learning algorithms tend to drastically overfit in sparse-data situations, asis typical in recommendation applications. We show that secondary contentinformation can often be used to overcome sparsity. Experiments on data fromthe ResearchIndex library of Computer Science publications show thatappropriate mixture models incorporating secondary data produce significantlybetter quality recommenders than k-nearest neighbors (k-NN). Globalprobabilistic models also allow more general inferences than local methods likek-NN.
arxiv-2700-217 | Lattice Particle Filters | http://arxiv.org/pdf/1301.2298v1.pdf | author:Dirk Ormoneit, Christiane Lemieux, David J. Fleet category:cs.AI cs.CV published:2013-01-10 summary:A standard approach to approximate inference in state-space models isto applya particle filter, e.g., the Condensation Algorithm.However, the performance ofparticle filters often varies significantlydue to their stochastic nature.Wepresent a class of algorithms, called lattice particle filters, thatcircumventthis difficulty by placing the particles deterministicallyaccording to aQuasi-Monte Carlo integration rule.We describe a practical realization of thisidea, discuss itstheoretical properties, and its efficiency.Experimentalresults with a synthetic 2D tracking problem show that thelattice particlefilter is equivalent to a conventional particle filterthat has between 10 and60% more particles, depending ontheir "sparsity" in the state-space.We alsopresent results on inferring 3D human motion frommoving light displays.
arxiv-2700-218 | Expectation Propagation for approximate Bayesian inference | http://arxiv.org/pdf/1301.2294v1.pdf | author:Thomas P. Minka category:cs.AI cs.LG published:2013-01-10 summary:This paper presents a new deterministic approximation technique in Bayesiannetworks. This method, "Expectation Propagation", unifies two previoustechniques: assumed-density filtering, an extension of the Kalman filter, andloopy belief propagation, an extension of belief propagation in Bayesiannetworks. All three algorithms try to recover an approximate distribution whichis close in KL divergence to the true distribution. Loopy belief propagation,because it propagates exact belief states, is useful for a limited class ofbelief networks, such as those which are purely discrete. ExpectationPropagation approximates the belief states by only retaining certainexpectations, such as mean and variance, and iterates until these expectationsare consistent throughout the network. This makes it applicable to hybridnetworks with discrete and continuous nodes. Expectation Propagation alsoextends belief propagation in the opposite direction - it can propagate richerbelief states that incorporate correlations between nodes. Experiments withGaussian mixture models show Expectation Propagation to be convincingly betterthan methods with similar computational cost: Laplace's method, variationalBayes, and Monte Carlo. Expectation Propagation also provides an efficientalgorithm for training Bayes point machine classifiers.
arxiv-2700-219 | A Bayesian Multiresolution Independence Test for Continuous Variables | http://arxiv.org/pdf/1301.2292v1.pdf | author:Dimitris Margaritis, Sebastian Thrun category:cs.AI cs.LG published:2013-01-10 summary:In this paper we present a method ofcomputing the posterior probabilityofconditional independence of two or morecontinuous variables fromdata,examined at several resolutions. Ourapproach is motivated bytheobservation that the appearance ofcontinuous data varies widely atvariousresolutions, producing verydifferent independence estimatesbetween thevariablesinvolved. Therefore, it is difficultto ascertain independencewithoutexamining data at several carefullyselected resolutions. In our paper,weaccomplish this using the exactcomputation of the posteriorprobability ofindependence, calculatedanalytically given a resolution. Ateach examinedresolution, we assume amultinomial distribution with Dirichletpriors for thediscretized tableparameters, and compute the posteriorusing Bayesianintegration. Acrossresolutions, we use a search procedureto approximate theBayesian integral ofprobability over an exponential numberof possiblehistograms. Our methodgeneralizes to an arbitrary numbervariables in astraightforward manner.The test is suitable for Bayesiannetwork learningalgorithms that useindependence tests to infer the networkstructure, in domainsthat contain anymix of continuous, ordinal andcategorical variables.
arxiv-2700-220 | Iterative Markov Chain Monte Carlo Computation of Reference Priors and Minimax Risk | http://arxiv.org/pdf/1301.2286v1.pdf | author:John Lafferty, Larry A. Wasserman category:cs.LG stat.ML published:2013-01-10 summary:We present an iterative Markov chainMonte Carlo algorithm forcomputingreference priors and minimax risk forgeneral parametric families.Ourapproach uses MCMC techniques based onthe Blahut-Arimoto algorithmforcomputing channel capacity ininformation theory. We give astatisticalanalysis of the algorithm,bounding the number of samples requiredfor thestochastic algorithm to closelyapproximate the deterministic algorithmin eachiteration. Simulations arepresented for several examples fromexponentialfamilies. Although we focuson applications to reference priors andminimax risk,the methods and analysiswe develop are applicable to a muchbroader class ofoptimization problemsand iterative algorithms.
arxiv-2700-221 | Classifier Learning with Supervised Marginal Likelihood | http://arxiv.org/pdf/1301.2284v1.pdf | author:Petri Kontkanen, Petri Myllymaki, Henry Tirri category:cs.LG stat.ML published:2013-01-10 summary:It has been argued that in supervised classification tasks, in practice itmay be more sensible to perform model selection with respect to some morefocused model selection score, like the supervised (conditional) marginallikelihood, than with respect to the standard marginal likelihood criterion.However, for most Bayesian network models, computing the supervised marginallikelihood score takes exponential time with respect to the amount of observeddata. In this paper, we consider diagnostic Bayesian network classifiers wherethe significant model parameters represent conditional distributions for theclass variable, given the values of the predictor variables, in which case thesupervised marginal likelihood can be computed in linear time with respect tothe data. As the number of model parameters grows in this case exponentiallywith respect to the number of predictors, we focus on simple diagnostic modelswhere the number of relevant predictors is small, and suggest two approachesfor applying this type of models in classification. The first approach is basedon mixtures of simple diagnostic models, while in the second approach we applythe small predictor sets of the simple diagnostic models for augmenting theNaive Bayes classifier.
arxiv-2700-222 | Improved learning of Bayesian networks | http://arxiv.org/pdf/1301.2283v1.pdf | author:Tomas Kocka, Robert Castelo category:cs.LG cs.AI stat.ML published:2013-01-10 summary:The search space of Bayesian Network structures is usually defined as AcyclicDirected Graphs (DAGs) and the search is done by local transformations of DAGs.But the space of Bayesian Networks is ordered by DAG Markov model inclusion andit is natural to consider that a good search policy should take this intoaccount. First attempt to do this (Chickering 1996) was using equivalenceclasses of DAGs instead of DAGs itself. This approach produces better resultsbut it is significantly slower. We present a compromise between these twoapproaches. It uses DAGs to search the space in such a way that the ordering byinclusion is taken into account. This is achieved by repetitive usage of localmoves within the equivalence class of DAGs. We show that this new approachproduces better results than the original DAGs approach without substantialchange in time complexity. We present empirical results, within the frameworkof heuristic search and Markov Chain Monte Carlo, provided through the Alarmdataset.
arxiv-2700-223 | Estimating Well-Performing Bayesian Networks using Bernoulli Mixtures | http://arxiv.org/pdf/1301.2280v1.pdf | author:Geoff A. Jarrad category:cs.LG cs.AI stat.ML published:2013-01-10 summary:A novel method for estimating Bayesian network (BN) parameters from data ispresented which provides improved performance on test data. Previous researchhas shown the value of representing conditional probability distributions(CPDs) via neural networks(Neal 1992), noisy-OR gates (Neal 1992, Diez 1993)anddecision trees (Friedman and Goldszmidt 1996).The Bernoulli mixture network(BMN) explicitly represents the CPDs of discrete BN nodes as mixtures of localdistributions,each having a different set of parents.This increases the spaceof possible structures which can be considered,enabling the CPDs to havefiner-grained dependencies.The resulting estimation procedure induces amodelthat is better able to emulate the underlying interactions occurring inthe data than conventional conditional Bernoulli network models.The results forartificially generated data indicate that overfitting is best reduced byrestricting the complexity of candidate mixture substructures local to eachnode. Furthermore, mixtures of very simple substructures can perform almost aswell as more complex ones.The BMN is also applied to data collected from anonline adventure game with an application to keyhole plan recognition. Theresults show that the BMN-based model brings a dramatic improvement inperformance over a conventional BN model.
arxiv-2700-224 | Domain Generalization via Invariant Feature Representation | http://arxiv.org/pdf/1301.2115v1.pdf | author:Krikamol Muandet, David Balduzzi, Bernhard Schölkopf category:stat.ML cs.LG published:2013-01-10 summary:This paper investigates domain generalization: How to take knowledge acquiredfrom an arbitrary number of related domains and apply it to previously unseendomains? We propose Domain-Invariant Component Analysis (DICA), a kernel-basedoptimization algorithm that learns an invariant transformation by minimizingthe dissimilarity across domains, whilst preserving the functional relationshipbetween input and output variables. A learning-theoretic analysis shows thatreducing dissimilarity improves the expected generalization ability ofclassifiers on new domains, motivating the proposed algorithm. Experimentalresults on synthetic and real-world datasets demonstrate that DICA successfullylearns invariant features and improves classifier performance in practice.
arxiv-2700-225 | A remark on covering | http://arxiv.org/pdf/1301.3043v1.pdf | author:Vladimir Temlyakov category:math.MG math.FA math.NA stat.ML published:2013-01-10 summary:We discuss construction of coverings of the unit ball of a finite dimensionalBanach space. The well known technique of comparing volumes gives upper andlower bounds on covering numbers. This technique does not provide aconstruction of good coverings. Here we apply incoherent dictionaries forconstruction of good coverings. We use the following strategy. First, we builda good covering by balls with a radius close to one. Second, we iterate thisconstruction to obtain a good covering for any radius. We mostly concentrate onthe first step of this strategy.
arxiv-2700-226 | Enhancing the retrieval performance by combing the texture and edge features | http://arxiv.org/pdf/1301.2542v1.pdf | author:Mohamed Eisa, Amira Eletrebi, Ebrahim Elhenawy category:cs.CV cs.IR published:2013-01-10 summary:In this paper, anew algorithm which is based on geometrical moments and localbinary patterns (LBP) for content based image retrieval (CBIR) is proposed. Ingeometrical moments, each vector is compared with the all other vectors foredge map generation. The same concept is utilized at LBP calculation which isgenerating nine LBP patterns from a given 3x3 pattern. Finally, nine LBPhistograms are calculated which are used as a feature vector for imageretrieval. Moments are important features used in recognition of differenttypes of images. Two experiments have been carried out for proving the worth ofour algorithm. The results after being investigated shows a significantimprovement in terms of their evaluation measures as compared to LBP and otherexisting transform domain techniques.
arxiv-2700-227 | Artificial Intelligence Framework for Simulating Clinical Decision-Making: A Markov Decision Process Approach | http://arxiv.org/pdf/1301.2158v1.pdf | author:Casey C. Bennett, Kris Hauser category:cs.AI stat.ML published:2013-01-10 summary:In the modern healthcare system, rapidly expanding costs/complexity, thegrowing myriad of treatment options, and exploding information streams thatoften do not effectively reach the front lines hinder the ability to chooseoptimal treatment decisions over time. The goal in this paper is to develop ageneral purpose (non-disease-specific) computational/artificial intelligence(AI) framework to address these challenges. This serves two potentialfunctions: 1) a simulation environment for exploring various healthcarepolicies, payment methodologies, etc., and 2) the basis for clinical artificialintelligence - an AI that can think like a doctor. This approach combinesMarkov decision processes and dynamic decision networks to learn from clinicaldata and develop complex plans via simulation of alternative sequentialdecision paths while capturing the sometimes conflicting, sometimes synergisticinteractions of various components in the healthcare system. It can operate inpartially observable environments (in the case of missing observations or data)by maintaining belief states about patient health status and functions as anonline agent that plans and re-plans. This framework was evaluated using realpatient data from an electronic health record. Such an AI framework easilyoutperforms the current treatment-as-usual (TAU) case-rate/fee-for-servicemodels of healthcare (Cost per Unit Change: $189 vs. $497) while obtaining a30-35% increase in patient outcomes. Tweaking certain model parameters furtherenhances this advantage, obtaining roughly 50% more improvement for roughlyhalf the costs. Given careful design and problem formulation, an AI simulationframework can approximate optimal decisions even in complex and uncertainenvironments. Future work is described that outlines potential lines ofresearch and integration of machine learning algorithms for personalizedmedicine.
arxiv-2700-228 | A Factorized Variational Technique for Phase Unwrapping in Markov Random Fields | http://arxiv.org/pdf/1301.2252v1.pdf | author:Kannan Achan, Brendan J. Frey, Ralf Koetter category:cs.CV published:2013-01-10 summary:Some types of medical and topographic imaging device produce images in whichthe pixel values are "phase-wrapped", i.e. measured modulus a known scalar.Phase unwrapping can be viewed as the problem of inferring the number of shiftsbetween each and every pair of neighboring pixels, subject to an a prioripreference for smooth surfaces, and subject to a zero curl constraint, whichrequires that the shifts must sum to 0 around every loop. We formulate phaseunwrapping as a mean field inference problem in a Markov network, where theprior favors the zero curl constraint. We compare our mean field technique withthe least squares method on a synthetic 100x100 image, and give results on a512x512 synthetic aperture radar image from Sandia National Laboratories.<LongText>
arxiv-2700-229 | Variational MCMC | http://arxiv.org/pdf/1301.2266v1.pdf | author:Nando de Freitas, Pedro Hojen-Sorensen, Michael I. Jordan, Stuart Russell category:cs.LG stat.CO stat.ML published:2013-01-10 summary:We propose a new class of learning algorithms that combines variationalapproximation and Markov chain Monte Carlo (MCMC) simulation. Naive algorithmsthat use the variational approximation as proposal distribution can performpoorly because this approximation tends to underestimate the true variance andother features of the data. We solve this problem by introducing moresophisticated MCMC algorithms. One of these algorithms is a mixture of two MCMCkernels: a random walk Metropolis kernel and a blockMetropolis-Hastings (MH)kernel with a variational approximation as proposaldistribution. The MH kernelallows one to locate regions of high probability efficiently. The Metropoliskernel allows us to explore the vicinity of these regions. This algorithmoutperforms variationalapproximations because it yields slightly betterestimates of the mean and considerably better estimates of higher moments, suchas covariances. It also outperforms standard MCMC algorithms because it locatestheregions of high probability quickly, thus speeding up convergence. Wedemonstrate this algorithm on the problem of Bayesian parameter estimation forlogistic (sigmoid) belief networks.
arxiv-2700-230 | Discovering Multiple Constraints that are Frequently Approximately Satisfied | http://arxiv.org/pdf/1301.2278v1.pdf | author:Geoffrey E. Hinton, Yee Whye Teh category:cs.LG stat.ML published:2013-01-10 summary:Some high-dimensional data.sets can be modelled by assuming that there aremany different linear constraints, each of which is Frequently ApproximatelySatisfied (FAS) by the data. The probability of a data vector under the modelis then proportional to the product of the probabilities of its constraintviolations. We describe three methods of learning products of constraints usinga heavy-tailed probability distribution for the violations.
arxiv-2700-231 | Multivariate Information Bottleneck | http://arxiv.org/pdf/1301.2270v1.pdf | author:Nir Friedman, Ori Mosenzon, Noam Slonim, Naftali Tishby category:cs.LG cs.AI stat.ML published:2013-01-10 summary:The Information bottleneck method is an unsupervised non-parametric dataorganization technique. Given a joint distribution P(A,B), this methodconstructs a new variable T that extracts partitions, or clusters, over thevalues of A that are informative about B. The information bottleneck hasalready been applied to document classification, gene expression, neural code,and spectral analysis. In this paper, we introduce a general principledframework for multivariate extensions of the information bottleneck method.This allows us to consider multiple systems of data partitions that areinter-related. Our approach utilizes Bayesian networks for specifying thesystems of clusters and what information each captures. We show that thisconstruction provides insight about bottleneck variations and enables us tocharacterize solutions of these variations. We also present a general frameworkfor iterative algorithms for constructing solutions, and apply it to severalexamples.
arxiv-2700-232 | Conditions Under Which Conditional Independence and Scoring Methods Lead to Identical Selection of Bayesian Network Models | http://arxiv.org/pdf/1301.2262v1.pdf | author:Robert G. Cowell category:cs.AI cs.LG stat.ML published:2013-01-10 summary:It is often stated in papers tackling the task of inferring Bayesian networkstructures from data that there are these two distinct approaches: (i) Applyconditional independence tests when testing for the presence or otherwise ofedges; (ii) Search the model space using a scoring metric. Here I argue thatfor complete data and a given node ordering this division is a myth, by showingthat cross entropy methods for checking conditional independence aremathematically identical to methods based upon discriminating between models bytheir overall goodness-of-fit logarithmic scores.
arxiv-2700-233 | On the Incommensurability Phenomenon | http://arxiv.org/pdf/1301.1954v5.pdf | author:Donniell E. Fishkind, Cencheng Shen, Youngser Park, Carey E. Priebe category:stat.ML published:2013-01-09 summary:Suppose that two large, multi-dimensional data sets are each noisymeasurements of the same underlying random process, and principle componentsanalysis is performed separately on the data sets to reduce theirdimensionality. In some circumstances it may happen that the twolower-dimensional data sets have an inordinately large Procrusteanfitting-error between them. The purpose of this manuscript is to quantify this"incommensurability phenomenon." In particular, under specified conditions, thesquare Procrustean fitting-error of the two normalized lower-dimensional datasets is (asymptotically) a convex combination (via a correlation parameter) ofthe Hausdorff distance between the projection subspaces and the maximumpossible value of the square Procrustean fitting-error for normalized data. Weshow how this gives rise to the incommensurability phenomenon, and we employillustrative simulations as well as a real data experiment to explore how theincommensurability phenomenon may have an appreciable impact.
arxiv-2700-234 | Risk-Aversion in Multi-armed Bandits | http://arxiv.org/pdf/1301.1936v1.pdf | author:Amir Sani, Alessandro Lazaric, Rémi Munos category:cs.LG published:2013-01-09 summary:Stochastic multi-armed bandits solve the Exploration-Exploitation dilemma andultimately maximize the expected reward. Nonetheless, in many practicalproblems, maximizing the expected reward is not the most desirable objective.In this paper, we introduce a novel setting based on the principle ofrisk-aversion where the objective is to compete against the arm with the bestrisk-return trade-off. This setting proves to be intrinsically more difficultthan the standard multi-arm bandit setting due in part to an exploration riskwhich introduces a regret associated to the variability of an algorithm. Usingvariance as a measure of risk, we introduce two new algorithms, investigatetheir theoretical guarantees, and report preliminary empirical results.
arxiv-2700-235 | Nonparametric Reduced Rank Regression | http://arxiv.org/pdf/1301.1919v1.pdf | author:Rina Foygel, Michael Horrell, Mathias Drton, John Lafferty category:stat.ML published:2013-01-09 summary:We propose an approach to multivariate nonparametric regression thatgeneralizes reduced rank regression for linear models. An additive model isestimated for each dimension of a $q$-dimensional response, with a shared$p$-dimensional predictor variable. To control the complexity of the model, weemploy a functional form of the Ky-Fan or nuclear norm, resulting in a set offunction estimates that have low rank. Backfitting algorithms are derived andjustified using a nonparametric form of the nuclear norm subdifferential.Oracle inequalities on excess risk are derived that exhibit the scalingbehavior of the procedure in the high dimensional setting. The methods areillustrated on gene expression data.
arxiv-2700-236 | Image Registration for Stability Testing of MEMS | http://arxiv.org/pdf/1301.1897v1.pdf | author:Nargess Memarsadeghi, Jacqueline Le Moigne, Peter N. Blake, Peter A. Morey, Wayne B. Landsman, Victor J. Chambers, Samuel H. Moseley category:cs.CV astro-ph.IM published:2013-01-09 summary:Image registration, or alignment of two or more images covering the samescenes or objects, is of great interest in many disciplines such as remotesensing, medical imaging, astronomy, and computer vision. In this paper, weintroduce a new application of image registration algorithms. We demonstratehow through a wavelet based image registration algorithm, engineers canevaluate stability of Micro-Electro-Mechanical Systems (MEMS). In particular,we applied image registration algorithms to assess alignment stability of theMicroShutters Subsystem (MSS) of the Near Infrared Spectrograph (NIRSpec)instrument of the James Webb Space Telescope (JWST). This work introduces a newmethodology for evaluating stability of MEMS devices to engineers as well as anew application of image registration algorithms to computer scientists.
arxiv-2700-237 | Moon Search Algorithms for NASA's Dawn Mission to Asteroid Vesta | http://arxiv.org/pdf/1301.1907v1.pdf | author:Nargess Memarsadeghi, Lucy A. McFadden, David Skillman, Brian McLean, Max Mutchler, Uri Carsenty, Eric E. Palmer, the Dawn Mission's S category:astro-ph.IM astro-ph.EP cs.CV published:2013-01-09 summary:A moon or natural satellite is a celestial body that orbits a planetary bodysuch as a planet, dwarf planet, or an asteroid. Scientists seek understandingthe origin and evolution of our solar system by studying moons of these bodies.Additionally, searches for satellites of planetary bodies can be important toprotect the safety of a spacecraft as it approaches or orbits a planetary body.If a satellite of a celestial body is found, the mass of that body can also becalculated once its orbit is determined. Ensuring the Dawn spacecraft's safetyon its mission to the asteroid (4) Vesta primarily motivated the work of Dawn'sSatellite Working Group (SWG) in summer of 2011. Dawn mission scientists andengineers utilized various computational tools and techniques for Vesta'ssatellite search. The objectives of this paper are to 1) introduce the naturalsatellite search problem, 2) present the computational challenges, approaches,and tools used when addressing this problem, and 3) describe applications ofvarious image processing and computational algorithms for performing satellitesearches to the electronic imaging and computer science community. Furthermore,we hope that this communication would enable Dawn mission scientists to improvetheir satellite search algorithms and tools and be better prepared forperforming the same investigation in 2015, when the spacecraft is scheduled toapproach and orbit the dwarf planet (1) Ceres.
arxiv-2700-238 | Spectral Clustering Based on Local PCA | http://arxiv.org/pdf/1301.2007v1.pdf | author:Ery Arias-Castro, Gilad Lerman, Teng Zhang category:stat.ML published:2013-01-09 summary:We propose a spectral clustering method based on local principal componentsanalysis (PCA). After performing local PCA in selected neighborhoods, thealgorithm builds a nearest neighbor graph weighted according to a discrepancybetween the principal subspaces in the neighborhoods, and then applies spectralclustering. As opposed to standard spectral methods based solely on pairwisedistances between points, our algorithm is able to resolve intersections. Weestablish theoretical guarantees for simpler variants within a prototypicalmathematical framework for multi-manifold clustering, and evaluate ouralgorithm on various simulated data sets.
arxiv-2700-239 | Bayesian Optimization in a Billion Dimensions via Random Embeddings | http://arxiv.org/pdf/1301.1942v2.pdf | author:Ziyu Wang, Frank Hutter, Masrour Zoghi, David Matheson, Nando de Freitas category:stat.ML cs.LG published:2013-01-09 summary:Bayesian optimization techniques have been successfully applied to robotics,planning, sensor placement, recommendation, advertising, intelligent userinterfaces and automatic algorithm configuration. Despite these successes, theapproach is restricted to problems of moderate dimension, and several workshopson Bayesian optimization have identified its scaling to high-dimensions as oneof the holy grails of the field. In this paper, we introduce a novel randomembedding idea to attack this problem. The resulting Random EMbedding BayesianOptimization (REMBO) algorithm is very simple, has important invarianceproperties, and applies to domains with both categorical and continuousvariables. We present a thorough theoretical analysis of REMBO. Empiricalresults confirm that REMBO can effectively solve problems with billions ofdimensions, provided the intrinsic dimensionality is low. They also show thatREMBO achieves state-of-the-art performance in optimizing the 47 discreteparameters of a popular mixed integer linear programming solver.
arxiv-2700-240 | Syntactic Analysis Based on Morphological Characteristic Features of the Romanian Language | http://arxiv.org/pdf/1301.1950v1.pdf | author:Bogdan Patrut category:cs.CL cs.AI 68T50 published:2013-01-09 summary:This paper refers to the syntactic analysis of phrases in Romanian, as animportant process of natural language processing. We will suggest a real-timesolution, based on the idea of using some words or groups of words thatindicate grammatical category; and some specific endings of some parts ofsentence. Our idea is based on some characteristics of the Romanian language,where some prepositions, adverbs or some specific endings can provide a lot ofinformation about the structure of a complex sentence. Such characteristics canbe found in other languages, too, such as French. Using a special grammar, wedeveloped a system (DIASEXP) that can perform a dialogue in natural languagewith assertive and interogative sentences about a "story" (a set of sentencesdescribing some events from the real life).
arxiv-2700-241 | Coupled Neural Associative Memories | http://arxiv.org/pdf/1301.1555v5.pdf | author:Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi category:cs.NE cs.IT cs.LG math.IT published:2013-01-08 summary:We propose a novel architecture to design a neural associative memory that iscapable of learning a large number of patterns and recalling them later inpresence of noise. It is based on dividing the neurons into local clusters andparallel plains, very similar to the architecture of the visual cortex ofmacaque brain. The common features of our proposed architecture with those ofspatially-coupled codes enable us to show that the performance of such networksin eliminating noise is drastically better than the previous approaches whilemaintaining the ability of learning an exponentially large number of patterns.Previous work either failed in providing good performance during the recallphase or in offering large pattern retrieval (storage) capacities. We alsopresent computational experiments that lend additional support to thetheoretical analysis.
arxiv-2700-242 | The RNA Newton Polytope and Learnability of Energy Parameters | http://arxiv.org/pdf/1301.1608v1.pdf | author:Elmirasadat Forouzmand, Hamidreza Chitsaz category:q-bio.BM cs.CE cs.LG published:2013-01-08 summary:Despite nearly two scores of research on RNA secondary structure and RNA-RNAinteraction prediction, the accuracy of the state-of-the-art algorithms arestill far from satisfactory. Researchers have proposed increasingly complexenergy models and improved parameter estimation methods in anticipation ofendowing their methods with enough power to solve the problem. The output hasdisappointingly been only modest improvements, not matching the expectations.Even recent massively featured machine learning approaches were not able tobreak the barrier. In this paper, we introduce the notion of learnability ofthe parameters of an energy model as a measure of its inherent capability. Wesay that the parameters of an energy model are learnable iff there exists atleast one set of such parameters that renders every known RNA structure to datethe minimum free energy structure. We derive a necessary condition for thelearnability and give a dynamic programming algorithm to assess it. Ouralgorithm computes the convex hull of the feature vectors of all feasiblestructures in the ensemble of a given input sequence. Interestingly, thatconvex hull coincides with the Newton polytope of the partition function as apolynomial in energy parameters. We demonstrated the application of our theoryto a simple energy model consisting of a weighted count of A-U and C-G basepairs. Our results show that this simple energy model satisfies the necessarycondition for less than one third of the input unpseudoknottedsequence-structure pairs chosen from the RNA STRAND v2.0 database. For anotherone third, the necessary condition is barely violated, which suggests thataugmenting this simple energy model with more features such as the Turner loopsmay solve the problem. The necessary condition is severely violated for 8%,which provides a small set of hard cases that require further investigation.
arxiv-2700-243 | An Analysis of Gene Expression Data using Penalized Fuzzy C-Means Approach | http://arxiv.org/pdf/1302.3123v1.pdf | author:P. K. Nizar Banu, H. Hannah Inbarani category:cs.CV cs.CE published:2013-01-08 summary:With the rapid advances of microarray technologies, large amounts ofhigh-dimensional gene expression data are being generated, which posessignificant computational challenges. A first step towards addressing thischallenge is the use of clustering techniques, which is essential in the datamining process to reveal natural structures and identify interesting patternsin the underlying data. A robust gene expression clustering approach tominimize undesirable clustering is proposed. In this paper, Penalized FuzzyC-Means (PFCM) Clustering algorithm is described and compared with the mostrepresentative off-line clustering techniques: K-Means Clustering, RoughK-Means Clustering and Fuzzy C-Means clustering. These techniques areimplemented and tested for a Brain Tumor gene expression Dataset. Analysis ofthe performance of the proposed approach is presented through qualitativevalidation experiments. From experimental results, it can be observed thatPenalized Fuzzy C-Means algorithm shows a much higher usability than the otherprojected clustering algorithms used in our comparison study. Significant andpromising clustering results are presented using Brain Tumor Gene expressiondataset. Thus patterns seen in genome-wide expression experiments can beinterpreted as indications of the status of cellular processes. In theseclustering results, we find that Penalized Fuzzy C-Means algorithm providesuseful information as an aid to diagnosis in oncology.
arxiv-2700-244 | An Efficient Algorithm for Upper Bound on the Partition Function of Nucleic Acids | http://arxiv.org/pdf/1301.1590v1.pdf | author:Hamidreza Chitsaz, Elmirasadat Forouzmand, Gholamreza Haffari category:q-bio.BM cs.LG published:2013-01-08 summary:It has been shown that minimum free energy structure for RNAs and RNA-RNAinteraction is often incorrect due to inaccuracies in the energy parameters andinherent limitations of the energy model. In contrast, ensemble basedquantities such as melting temperature and equilibrium concentrations can bemore reliably predicted. Even structure prediction by sampling from theensemble and clustering those structures by Sfold [7] has proven to be morereliable than minimum free energy structure prediction. The main obstacle forensemble based approaches is the computational complexity of the partitionfunction and base pairing probabilities. For instance, the space complexity ofthe partition function for RNA-RNA interaction is $O(n^4)$ and the timecomplexity is $O(n^6)$ which are prohibitively large [4,12]. Our goal in thispaper is to give a fast algorithm, based on sparse folding, to calculate anupper bound on the partition function. Our work is based on the recentalgorithm of Hazan and Jaakkola [10]. The space complexity of our algorithm isthe same as that of sparse folding algorithms, and the time complexity of ouralgorithm is $O(MFE(n)\ell)$ for single RNA and $O(MFE(m, n)\ell)$ for RNA-RNAinteraction in practice, in which $MFE$ is the running time of sparse foldingand $\ell \leq n$ ($\ell \leq n + m$) is a sequence dependent parameter.
arxiv-2700-245 | A novel processing pipeline for optical multi-touch surfaces | http://arxiv.org/pdf/1301.1551v1.pdf | author:Philipp Ewerling category:cs.CV published:2013-01-08 summary:In this thesis a new approach for touch detection on optical multi-touchdevices is proposed that exploits the fact that the camera images reveal notonly the actual touch points but also objects above the screen such as the handor arm of a user. The touch processing relies on the Maximally Stable ExtremalRegions algorithm for finding the users' fingertips in the camera image. Thehierarchical structure of the generated extremal regions serves as a startingpoint for agglomerative clustering of the fingertips into hands. Furthermore, aheuristic is suggested that supports the identification of individual fingersas well as the distinction between left hands and right hands if all fivefingers of a hand are in contact with the touch surface. The evaluation confirmed that the system is robust against detection errorsresulting from non-uniform illumination and reliably assigns touch points toindividual hands based on the implicitly tracked context information. Theefficient multi-threaded implementation handles two-handed input from multipleusers in real-time.
arxiv-2700-246 | Linear Bandits in High Dimension and Recommendation Systems | http://arxiv.org/pdf/1301.1722v1.pdf | author:Yash Deshpande, Andrea Montanari category:cs.LG stat.ML published:2013-01-08 summary:A large number of online services provide automated recommendations to helpusers to navigate through a large collection of items. New items (products,videos, songs, advertisements) are suggested on the basis of the user's pasthistory and --when available-- her demographic profile. Recommendations have tosatisfy the dual goal of helping the user to explore the space of availableitems, while allowing the system to probe the user's preferences. We model this trade-off using linearly parametrized multi-armed bandits,propose a policy and prove upper and lower bounds on the cumulative "reward"that coincide up to constants in the data poor (high-dimensional) regime. Priorwork on linear bandits has focused on the data rich (low-dimensional) regimeand used cumulative "risk" as the figure of merit. For this data rich regime,we provide a simple modification for our policy that achieves near-optimal riskperformance under more restrictive assumptions on the geometry of the problem.We test (a variation of) the scheme used for establishing achievability on theNetflix and MovieLens datasets and obtain good agreement with the qualitativepredictions of the theory we develop.
arxiv-2700-247 | Causal graph-based video segmentation | http://arxiv.org/pdf/1301.1671v1.pdf | author:Camille Couprie, Clément Farabet, Yann LeCun category:cs.CV published:2013-01-08 summary:Numerous approaches in image processing and computer vision are making use ofsuper-pixels as a pre-processing step. Among the different methods producingsuch over-segmentation of an image, the graph-based approach of Felzenszwalband Huttenlocher is broadly employed. One of its interesting properties is thatthe regions are computed in a greedy manner in quasi-linear time. The algorithmmay be trivially extended to video segmentation by considering a video as a 3Dvolume, however, this can not be the case for causal segmentation, whensubsequent frames are unknown. We propose an efficient video segmentationapproach that computes temporally consistent pixels in a causal manner, fillingthe need for causal and real time applications.
arxiv-2700-248 | Optical Flow on Evolving Surfaces with an Application to the Analysis of 4D Microscopy Data | http://arxiv.org/pdf/1301.1576v2.pdf | author:Clemens Kirisits, Lukas F. Lang, Otmar Scherzer category:math.OC cs.CV published:2013-01-08 summary:We extend the concept of optical flow to a dynamic non-Euclidean setting.Optical flow is traditionally computed from a sequence of flat images. It isthe purpose of this paper to introduce variational motion estimation for imagesthat are defined on an evolving surface. Volumetric microscopy images depictinga live zebrafish embryo serve as both biological motivation and test data.
arxiv-2700-249 | A proximal Newton framework for composite minimization: Graph learning without Cholesky decompositions and matrix inversions | http://arxiv.org/pdf/1301.1459v3.pdf | author:Quoc Tran Dinh, Anastasios Kyrillidis, Volkan Cevher category:stat.ML math.OC published:2013-01-08 summary:We propose an algorithmic framework for convex minimization problems of acomposite function with two terms: a self-concordant function and a possiblynonsmooth regularization term. Our method is a new proximal Newton algorithmthat features a local quadratic convergence rate. As a specific instance of ourframework, we consider the sparse inverse covariance matrix estimation in graphlearning problems. Via a careful dual formulation and a novel analyticstep-size selection procedure, our approach for graph learning avoids Choleskydecompositions and matrix inversions in its iteration making it attractive forparallel and distributed implementations.
arxiv-2700-250 | Adaptation of fictional and online conversations to communication media | http://arxiv.org/pdf/1301.1429v1.pdf | author:Christian M. Alis, May T. Lim category:physics.soc-ph cs.CL published:2013-01-08 summary:Conversations allow the quick transfer of short bits of information and it isreasonable to expect that changes in communication medium affect how weconverse. Using conversations in works of fiction and in an online socialnetworking platform, we show that the utterance length of conversations isslowly shortening with time but adapts more strongly to the constraints of thecommunication medium. This indicates that the introduction of any new medium ofcommunication can affect the way natural language evolves.
arxiv-2700-251 | PaFiMoCS: Particle Filtered Modified-CS and Applications in Visual Tracking across Illumination Change | http://arxiv.org/pdf/1301.1374v1.pdf | author:R. Sarkar, S. Das, N. Vaswani category:cs.CV published:2013-01-08 summary:We study the problem of tracking (causally estimating) a time sequence ofsparse spatial signals with changing sparsity patterns, as well as otherunknown states, from a sequence of nonlinear observations corrupted by(possibly) non-Gaussian noise. In many applications, particularly those invisual tracking, the unknown state can be split into a small dimensional part,e.g. global motion, and a spatial signal, e.g. illumination or shapedeformation. The spatial signal is often well modeled as being sparse in somedomain. For a long sequence, its sparsity pattern can change over time,although the changes are usually slow. To address the above problem, we proposea novel solution approach called Particle Filtered Modified-CS (PaFiMoCS). Thekey idea of PaFiMoCS is to importance sample for the small dimensional statevector, while replacing importance sampling by slow sparsity pattern changeconstrained posterior mode tracking for recovering the sparse spatial signal.We show that the problem of tracking moving objects across spatially varyingillumination change is an example of the above problem and explain how todesign PaFiMoCS for it. Experiments on both simulated data as well as on realvideos with significant illumination changes demonstrate the superiority of theproposed algorithm as compared with existing particle filter based trackingalgorithms.
arxiv-2700-252 | Automated Variational Inference in Probabilistic Programming | http://arxiv.org/pdf/1301.1299v1.pdf | author:David Wingate, Theophane Weber category:stat.ML cs.AI cs.LG published:2013-01-07 summary:We present a new algorithm for approximate inference in probabilisticprograms, based on a stochastic gradient for variational programs. This methodis efficient without restrictions on the probabilistic program; it isparticularly practical for distributions which are not analytically tractable,including highly structured distributions that arise in probabilistic programs.We show how to automatically derive mean-field probabilistic programs andoptimize them, and demonstrate that our perspective improves inferenceefficiency over other algorithms.
arxiv-2700-253 | Time-Frequency Representation of Microseismic Signals using the Synchrosqueezing Transform | http://arxiv.org/pdf/1301.1295v1.pdf | author:Roberto H. Herrera, Jean-Baptiste Tary, Mirko van der Baan category:physics.geo-ph cs.CE cs.CV published:2013-01-07 summary:Resonance frequencies can provide useful information on the deformationoccurring during fracturing experiments or $CO_2$ management, complementary tothe microseismic event distribution. An accurate time-frequency representationis of crucial importance prior to interpreting the cause of resonancefrequencies during microseismic experiments. The popular methods of Short-TimeFourier Transform (STFT) and wavelet analysis have limitations in representingclose frequencies and dealing with fast varying instantaneous frequencies andthis is often the nature of microseismic signals. The synchrosqueezingtransform (SST) is a promising tool to track these resonant frequencies andprovide a detailed time-frequency representation. Here we apply thesynchrosqueezing transform to microseismic signals and also show its potentialto general seismic signal processing applications.
arxiv-2700-254 | Dynamical Models and Tracking Regret in Online Convex Programming | http://arxiv.org/pdf/1301.1254v1.pdf | author:Eric C. Hall, Rebecca M. Willett category:stat.ML cs.LG published:2013-01-07 summary:This paper describes a new online convex optimization method whichincorporates a family of candidate dynamical models and establishes noveltracking regret bounds that scale with the comparator's deviation from the bestdynamical model in this family. Previous online optimization methods aredesigned to have a total accumulated loss comparable to that of the bestcomparator sequence, and existing tracking or shifting regret bounds scale withthe overall variation of the comparator sequence. In many practical scenarios,however, the environment is nonstationary and comparator sequences with smallvariation are quite weak, resulting in large losses. The proposed DynamicMirror Descent method, in contrast, can yield low regret relative to highlyvariable comparator sequences by both tracking the best dynamical model andforming predictions based on that model. This concept is demonstratedempirically in the context of sequential compressive observations of a dynamicscene and tracking a dynamic social network.
arxiv-2700-255 | Supervised, semi-supervised and unsupervised inference of gene regulatory networks | http://arxiv.org/pdf/1301.1083v1.pdf | author:Stefan R. Maetschke, Piyush B. Madhamshettiwar, Melissa J. Davis, Mark A. Ragan category:q-bio.MN q-bio.QM stat.ML published:2013-01-07 summary:Inference of gene regulatory network from expression data is a challengingtask. Many methods have been developed to this purpose but a comprehensiveevaluation that covers unsupervised, semi-supervised and supervised methods,and provides guidelines for their practical application, is lacking. We performed an extensive evaluation of inference methods on simulatedexpression data. The results reveal very low prediction accuracies forunsupervised techniques with the notable exception of the z-score method onknock-out data. In all other cases the supervised approach achieved the highestaccuracies and even in a semi-supervised setting with small numbers of onlypositive samples, outperformed the unsupervised techniques.
arxiv-2700-256 | Efficient Eigen-updating for Spectral Graph Clustering | http://arxiv.org/pdf/1301.1318v4.pdf | author:Charanpal Dhanjal, Romaric Gaudel, Stéphan Clémençon category:stat.ML published:2013-01-07 summary:Partitioning a graph into groups of vertices such that those within eachgroup are more densely connected than vertices assigned to different groups,known as graph clustering, is often used to gain insight into the organisationof large scale networks and for visualisation purposes. Whereas a large numberof dedicated techniques have been recently proposed for static graphs, thedesign of on-line graph clustering methods tailored for evolving networks is achallenging problem, and much less documented in the literature. Motivated bythe broad variety of applications concerned, ranging from the study ofbiological networks to the analysis of networks of scientific referencesthrough the exploration of communications networks such as the World Wide Web,it is the main purpose of this paper to introduce a novel, computationallyefficient, approach to graph clustering in the evolutionary context. Namely,the method promoted in this article can be viewed as an incremental eigenvaluesolution for the spectral clustering method described by Ng. et al. (2001). Theincremental eigenvalue solution is a general technique for finding theapproximate eigenvectors of a symmetric matrix given a change. As well asoutlining the approach in detail, we present a theoretical bound on the qualityof the approximate eigenvectors using perturbation theory. We then derive anovel spectral clustering algorithm called Incremental Approximate SpectralClustering (IASC). The IASC algorithm is simple to implement and its efficacyis demonstrated on both synthetic and real datasets modelling the evolution ofa HIV epidemic, a citation network and the purchase history graph of ane-commerce website.
arxiv-2700-257 | Finding the True Frequent Itemsets | http://arxiv.org/pdf/1301.1218v3.pdf | author:Matteo Riondato, Fabio Vandin category:cs.LG cs.DB cs.DS stat.ML H.2.8 published:2013-01-07 summary:Frequent Itemsets (FIs) mining is a fundamental primitive in data mining. Itrequires to identify all itemsets appearing in at least a fraction $\theta$ ofa transactional dataset $\mathcal{D}$. Often though, the ultimate goal ofmining $\mathcal{D}$ is not an analysis of the dataset \emph{per se}, but theunderstanding of the underlying process that generated it. Specifically, inmany applications $\mathcal{D}$ is a collection of samples obtained from anunknown probability distribution $\pi$ on transactions, and by extracting theFIs in $\mathcal{D}$ one attempts to infer itemsets that are frequently (i.e.,with probability at least $\theta$) generated by $\pi$, which we call the TrueFrequent Itemsets (TFIs). Due to the inherently stochastic nature of thegenerative process, the set of FIs is only a rough approximation of the set ofTFIs, as it often contains a huge number of \emph{false positives}, i.e.,spurious itemsets that are not among the TFIs. In this work we design andanalyze an algorithm to identify a threshold $\hat{\theta}$ such that thecollection of itemsets with frequency at least $\hat{\theta}$ in $\mathcal{D}$contains only TFIs with probability at least $1-\delta$, for someuser-specified $\delta$. Our method uses results from statistical learningtheory involving the (empirical) VC-dimension of the problem at hand. Thisallows us to identify almost all the TFIs without including any false positive.We also experimentally compare our method with the direct mining of$\mathcal{D}$ at frequency $\theta$ and with techniques based on widely-usedstandard bounds (i.e., the Chernoff bounds) of the binomial distribution, andshow that our algorithm outperforms these methods and achieves even betterresults than what is guaranteed by the theoretical analysis.
arxiv-2700-258 | Stratified SIFT Matching for Human Iris Recognition | http://arxiv.org/pdf/1301.0998v1.pdf | author:Sambit Bakshi, Hunny Mehrotra, Banshidhar Majhi category:cs.CV 68U10 published:2013-01-06 summary:This paper proposes an efficient three fold stratified SIFT matching for irisrecognition. The objective is to filter wrongly paired conventional SIFTmatches. In Strata I, the keypoints from gallery and probe iris images arepaired using traditional SIFT approach. Due to high image similarity atdifferent regions of iris there may be some impairments. These are detected andfiltered by finding gradient of paired keypoints in Strata II. Further, thescaling factor of paired keypoints is used to remove impairments in Strata III.The pairs retained after Strata III are likely to be potential matches for irisrecognition. The proposed system performs with an accuracy of 96.08% and 97.15%on publicly available CASIAV3 and BATH databases respectively. This markssignificant improvement of accuracy and FAR over the existing SIFT matching foriris.
arxiv-2700-259 | Graph 3-coloring with a hybrid self-adaptive evolutionary algorithm | http://arxiv.org/pdf/1301.0939v1.pdf | author:Iztok Fister, Marjan Mernik, Bogdan Filipič category:cs.NE published:2013-01-05 summary:This paper proposes a hybrid self-adaptive evolutionary algorithm for graphcoloring that is hybridized with the following novel elements: heuristicgenotype-phenotype mapping, a swap local search heuristic, and a neutralsurvivor selection operator. This algorithm was compared with the evolutionaryalgorithm with the SAW method of Eiben et al., the Tabucol algorithm of Hertzand de Werra, and the hybrid evolutionary algorithm of Galinier and Hao. Theperformance of these algorithms were tested on a test suite consisting ofrandomly generated 3-colorable graphs of various structural features, such asgraph size, type, edge density, and variability in sizes of color classes.Furthermore, the test graphs were generated including the phase transitionwhere the graphs are hard to color. The purpose of the extensive experimentalwork was threefold: to investigate the behavior of the tested algorithms in thephase transition, to identify what impact hybridization with the DSaturtraditional heuristic has on the evolutionary algorithm, and to show how graphstructural features influence the performance of the graph-coloring algorithms.The results indicate that the performance of the hybrid self-adaptiveevolutionary algorithm is comparable with, or better than, the performance ofthe hybrid evolutionary algorithm which is one of the best graph-coloringalgorithms today. Moreover, the fact that all the considered algorithmsperformed poorly on flat graphs confirms that this type of graphs is really thehardest to color.
arxiv-2700-260 | Comparative Studies on Decentralized Multiloop PID Controller Design Using Evolutionary Algorithms | http://arxiv.org/pdf/1301.0930v1.pdf | author:Sayan Saha, Saptarshi Das, Anindya Pakhira, Sumit Mukherjee, Indranil Pan category:cs.SY cs.NE published:2013-01-05 summary:Decentralized PID controllers have been designed in this paper forsimultaneous tracking of individual process variables in multivariable systemsunder step reference input. The controller design framework takes into accountthe minimization of a weighted sum of Integral of Time multiplied Squared Error(ITSE) and Integral of Squared Controller Output (ISCO) so as to balance theoverall tracking errors for the process variables and required variation in thecorresponding manipulated variables. Decentralized PID gains are tuned usingthree popular Evolutionary Algorithms (EAs) viz. Genetic Algorithm (GA),Evolutionary Strategy (ES) and Cultural Algorithm (CA). Credible simulationcomparisons have been reported for four benchmark 2x2 multivariable processes.
arxiv-2700-261 | Hybridization of Evolutionary Algorithms | http://arxiv.org/pdf/1301.0929v1.pdf | author:Iztok Fister, Marjan Mernik, Janez Brest category:cs.NE published:2013-01-05 summary:Evolutionary algorithms are good general problem solver but suffer from alack of domain specific knowledge. However, the problem specific knowledge canbe added to evolutionary algorithms by hybridizing. Interestingly, all theelements of the evolutionary algorithms can be hybridized. In this chapter, thehybridization of the three elements of the evolutionary algorithms isdiscussed: the objective function, the survivor selection operator and theparameter settings. As an objective function, the existing heuristic functionthat construct the solution of the problem in traditional way is used. However,this function is embedded into the evolutionary algorithm that serves as agenerator of new solutions. In addition, the objective function is improved bylocal search heuristics. The new neutral selection operator has been developedthat is capable to deal with neutral solutions, i.e. solutions that have thedifferent representation but expose the equal values of objective function. Theaim of this operator is to directs the evolutionary search into a newundiscovered regions of the search space. To avoid of wrong setting ofparameters that control the behavior of the evolutionary algorithm, theself-adaptation is used. Finally, such hybrid self-adaptive evolutionaryalgorithm is applied to the two real-world NP-hard problems: the graph3-coloring and the optimization of markers in the clothing industry. Extensiveexperiments shown that these hybridization improves the results of theevolutionary algorithms a lot. Furthermore, the impact of the particularhybridizations is analyzed in details as well.
arxiv-2700-262 | A New Geometric Approach to Latent Topic Modeling and Discovery | http://arxiv.org/pdf/1301.0858v1.pdf | author:Weicong Ding, Mohammad H. Rohban, Prakash Ishwar, Venkatesh Saligrama category:stat.ML published:2013-01-05 summary:A new geometrically-motivated algorithm for nonnegative matrix factorizationis developed and applied to the discovery of latent "topics" for text and image"document" corpora. The algorithm is based on robustly finding and clusteringextreme points of empirical cross-document word-frequencies that correspond tonovel "words" unique to each topic. In contrast to related approaches that arebased on solving non-convex optimization problems using suboptimalapproximations, locally-optimal methods, or heuristics, the new algorithm isconvex, has polynomial complexity, and has competitive qualitative andquantitative performance compared to the current state-of-the-art approaches onsynthetic and real-world datasets.
arxiv-2700-263 | Good parts first - a new algorithm for approximate search in lexica and string databases | http://arxiv.org/pdf/1301.0722v2.pdf | author:Stefan Gerdjikov, Stoyan Mihov, Petar Mitankin, Klaus U. Schulz category:cs.CL cs.DS published:2013-01-04 summary:We present a new efficient method for approximate search in electroniclexica. Given an input string (the pattern) and a similarity threshold, thealgorithm retrieves all entries of the lexicon that are sufficiently similar tothe pattern. Search is organized in subsearches that always start with an exactpartial match where a substring of the input pattern is aligned with asubstring of a lexicon word. Afterwards this partial match is extended stepwiseto larger substrings. For aligning further parts of the pattern withcorresponding parts of lexicon entries, more errors are tolerated at eachsubsequent step. For supporting this alignment order, which may start at anypart of the pattern, the lexicon is represented as a structure that enablesimmediate access to any substring of a lexicon word and permits the extensionof such substrings in both directions. Experimental evaluations of theapproximate search procedure are given that show significant efficiencyimprovements compared to existing techniques. Since the technique can be usedfor large error bounds it offers interesting possibilities for approximatesearch in special collections of "long" strings, such as phrases, sentences, orbook ti
arxiv-2700-264 | Adaptive Intelligent Cooperative Spectrum Sensing In Cognitive Radio | http://arxiv.org/pdf/1301.0785v1.pdf | author:Dilip S Aldar category:cs.NE published:2013-01-04 summary:Radio Spectrum is most precious and scarce resource and must be utilizedefficiently and effectively. Cognitive radio is the promising solutions for theoptimum utilization of the scared natural resource. The spectrum owned by theprimary user should be shared among the secondary user, but primary user shouldnot be interfered by the secondary user. In order to utilize the primary userspectrum, secondary user must detect accurately, the existence of primary inthe band of interest. In cooperative spectrum sensing, the channel between thesecondary users and the cognitive radio base station is non stationary andcauses interference in the decision in decision fusion and in information ininformation due to multipath fading. In this paper neural network basedcooperative spectrum sensing method is proposed, the performance of proposedmethod is evaluated and observed that, the neural network based schemeperformance improve significantly over the AND,OR and Majority rule
arxiv-2700-265 | The Sum-over-Forests density index: identifying dense regions in a graph | http://arxiv.org/pdf/1301.0725v1.pdf | author:Mathieu Senelle, Silvia Garcia-Diez, Amin Mantrach, Masashi Shimbo, Marco Saerens, François Fouss category:cs.LG stat.ML published:2013-01-04 summary:This work introduces a novel nonparametric density index defined on graphs,the Sum-over-Forests (SoF) density index. It is based on a clear and intuitiveidea: high-density regions in a graph are characterized by the fact that theycontain a large amount of low-cost trees with high outdegrees while low-densityregions contain few ones. Therefore, a Boltzmann probability distribution onthe countable set of forests in the graph is defined so that large (high-cost)forests occur with a low probability while short (low-cost) forests occur witha high probability. Then, the SoF density index of a node is defined as theexpected outdegree of this node in a non-trivial tree of the forest, thusproviding a measure of density around that node. Following the matrix-foresttheorem, and a statistical physics framework, it is shown that the SoF densityindex can be easily computed in closed form through a simple matrix inversion.Experiments on artificial and real data sets show that the proposed indexperforms well on finding dense regions, for graphs of various origins.
arxiv-2700-266 | Borrowing strengh in hierarchical Bayes: Posterior concentration of the Dirichlet base measure | http://arxiv.org/pdf/1301.0802v4.pdf | author:XuanLong Nguyen category:math.ST cs.LG math.PR stat.TH published:2013-01-04 summary:This paper studies posterior concentration behavior of the base probabilitymeasure of a Dirichlet measure, given observations associated with the sampledDirichlet processes, as the number of observations tends to infinity. The basemeasure itself is endowed with another Dirichlet prior, a construction known asthe hierarchical Dirichlet processes (Teh et al. [J. Amer. Statist. Assoc. 101(2006) 1566-1581]). Convergence rates are established in transportationdistances (i.e., Wasserstein metrics) under various conditions on the geometryof the support of the true base measure. As a consequence of the theory, wedemonstrate the benefit of "borrowing strength" in the inference of multiplegroups of data - a powerful insight often invoked to motivate hierarchicalmodeling. In certain settings, the gain in efficiency due to the latenthierarchy can be dramatic, improving from a standard nonparametric rate to aparametric rate of convergence. Tools developed include transportationdistances for nonparametric Bayesian hierarchies of random measures, theexistence of tests for Dirichlet measures, and geometric properties of thesupport of Dirichlet measures.
arxiv-2700-267 | A Method for Finding Structured Sparse Solutions to Non-negative Least Squares Problems with Applications | http://arxiv.org/pdf/1301.0413v1.pdf | author:Ernie Esser, Yifei Lou, Jack Xin category:stat.ML stat.AP stat.CO stat.ME published:2013-01-03 summary:Demixing problems in many areas such as hyperspectral imaging anddifferential optical absorption spectroscopy (DOAS) often require findingsparse nonnegative linear combinations of dictionary elements that matchobserved data. We show how aspects of these problems, such as misalignment ofDOAS references and uncertainty in hyperspectral endmembers, can be modeled byexpanding the dictionary with grouped elements and imposing a structuredsparsity assumption that the combinations within each group should be sparse oreven 1-sparse. If the dictionary is highly coherent, it is difficult to obtaingood solutions using convex or greedy methods, such as non-negative leastsquares (NNLS) or orthogonal matching pursuit. We use penalties related to theHoyer measure, which is the ratio of the $l_1$ and $l_2$ norms, as sparsitypenalties to be added to the objective in NNLS-type models. For solving theresulting nonconvex models, we propose a scaled gradient projection algorithmthat requires solving a sequence of strongly convex quadratic programs. Wediscuss its close connections to convex splitting methods and difference ofconvex programming. We also present promising numerical results for exampleDOAS analysis and hyperspectral demixing problems.
arxiv-2700-268 | Follow the Leader If You Can, Hedge If You Must | http://arxiv.org/pdf/1301.0534v2.pdf | author:Steven de Rooij, Tim van Erven, Peter D. Grünwald, Wouter M. Koolen category:cs.LG stat.ML published:2013-01-03 summary:Follow-the-Leader (FTL) is an intuitive sequential prediction strategy thatguarantees constant regret in the stochastic setting, but has terribleperformance for worst-case data. Other hedging strategies have betterworst-case guarantees but may perform much worse than FTL if the data are notmaximally adversarial. We introduce the FlipFlop algorithm, which is the firstmethod that provably combines the best of both worlds. As part of our construction, we develop AdaHedge, which is a new way ofdynamically tuning the learning rate in Hedge without using the doubling trick.AdaHedge refines a method by Cesa-Bianchi, Mansour and Stoltz (2007), yieldingslightly improved worst-case guarantees. By interleaving AdaHedge and FTL, theFlipFlop algorithm achieves regret within a constant factor of the FTL regret,without sacrificing AdaHedge's worst-case guarantees. AdaHedge and FlipFlop do not need to know the range of the losses in advance;moreover, unlike earlier methods, both have the intuitive property that theissued weights are invariant under rescaling and translation of the losses. Thelosses are also allowed to be negative, in which case they may be interpretedas gains.
arxiv-2700-269 | A Self-Organizing Neural Scheme for Door Detection in Different Environments | http://arxiv.org/pdf/1301.0432v1.pdf | author:F. Mahmood, F. Kunwar category:cs.CV published:2013-01-03 summary:Doors are important landmarks for indoor mobile robot navigation and alsoassist blind people to independently access unfamiliar buildings. Most existingalgorithms of door detection are limited to work for familiar environmentsbecause of restricted assumptions about color, texture and shape. In this paperwe propose a novel approach which employs feature based classification and usesthe Kohonen Self-Organizing Map (SOM) for the purpose of door detection.Generic and stable features are used for the training of SOM that increase theperformance significantly: concavity, bottom-edge intensity profile and dooredges. To validate the robustness and generalizability of our method, wecollected a large dataset of real world door images from a variety ofenvironments and different lighting conditions. The algorithm achieves morethan 95% detection which demonstrates that our door detection method is genericand robust with variations of color, texture, occlusions, lighting condition,scales, and viewpoints.
arxiv-2700-270 | Investigating the performance of Correspondence Algorithms in Vision based Driver-assistance in Indoor Environment | http://arxiv.org/pdf/1301.0435v1.pdf | author:F. Mahmood, Syed. M. B. Haider, F. Kunwar category:cs.CV cs.RO published:2013-01-03 summary:This paper presents the experimental comparison of fourteen stereo matchingalgorithms in variant illumination conditions. Different adaptations of globaland local stereo matching techniques are chosen for evaluation The variantstrength and weakness of the chosen correspondence algorithms are explored byemploying the methodology of the prediction error strategy. The algorithms aregauged on the basis of their performance on real world data set taken invarious indoor lighting conditions and at different times of the day
arxiv-2700-271 | Validation of Soft Classification Models using Partial Class Memberships: An Extended Concept of Sensitivity & Co. applied to the Grading of Astrocytoma Tissues | http://arxiv.org/pdf/1301.0264v2.pdf | author:Claudia Beleites, Reiner Salzer, Valter Sergo category:stat.ML stat.AP published:2013-01-02 summary:We use partial class memberships in soft classification to model uncertainlabelling and mixtures of classes. Partial class memberships are not restrictedto predictions, but may also occur in reference labels (ground truth, goldstandard diagnosis) for training and validation data. Classifier performance is usually expressed as fractions of the confusionmatrix, such as sensitivity, specificity, negative and positive predictivevalues. We extend this concept to soft classification and discuss the bias andvariance properties of the extended performance measures. Ambiguity inreference labels translates to differences between best-case, expected andworst-case performance. We show a second set of measures comparing expected andideal performance which is closely related to regression performance, namelythe root mean squared error RMSE and the mean absolute error MAE. All calculations apply to classical crisp classification as well as to softclassification (partial class memberships and/or one-class classifiers). Theproposed performance measures allow to test classifiers with actual borderlinecases. In addition, hardening of e.g. posterior probabilities into class labelsis not necessary, avoiding the corresponding information loss and increase invariance. We implement the proposed performance measures in the R package"softclassval", which is available from CRAN and athttp://softclassval.r-forge.r-project.org. Our reasoning as well as the importance of partial memberships forchemometric classification is illustrated by a real-word application:astrocytoma brain tumor tissue grading (80 patients, 37000 spectra) for findingsurgical excision borders. As borderline cases are the actual target of theanalytical technique, samples which are diagnosed to be borderline cases mustbe included in the validation.
arxiv-2700-272 | A Novel Design Specification Distance(DSD) Based K-Mean Clustering Performace Evluation on Engineering Materials Database | http://arxiv.org/pdf/1301.0179v1.pdf | author:Doreswamy, K. S. Hemanth category:cs.LG published:2013-01-02 summary:Organizing data into semantically more meaningful is one of the fundamentalmodes of understanding and learning. Cluster analysis is a formal study ofmethods for understanding and algorithm for learning. K-mean clusteringalgorithm is one of the most fundamental and simple clustering algorithms. Whenthere is no prior knowledge about the distribution of data sets, K-mean is thefirst choice for clustering with an initial number of clusters. In this paper anovel distance metric called Design Specification (DS) distance measurefunction is integrated with K-mean clustering algorithm to improve clusteraccuracy. The K-means algorithm with proposed distance measure maximizes thecluster accuracy to 99.98% at P = 1.525, which is determined through theiterative procedure. The performance of Design Specification (DS) distancemeasure function with K - mean algorithm is compared with the performances ofother standard distance functions such as Euclidian, squared Euclidean, CityBlock, and Chebshew similarity measures deployed with K-mean algorithm.Theproposed method is evaluated on the engineering materials database. Theexperiments on cluster analysis and the outlier profiling show that these is anexcellent improvement in the performance of the proposed method.
arxiv-2700-273 | A Geometric Blind Source Separation Method Based on Facet Component Analysis | http://arxiv.org/pdf/1301.0339v1.pdf | author:P. Yin, Y. Sun, J. Xin category:math.NA stat.ML published:2013-01-02 summary:Given a set of mixtures, blind source separation attempts to retrieve thesource signals without or with very little information of the the mixingprocess. We present a geometric approach for blind separation of nonnegativelinear mixtures termed {\em facet component analysis} (FCA). The approach isbased on facet identification of the underlying cone structure of the data.Earlier works focus on recovering the cone by locating its vertices (vertexcomponent analysis or VCA) based on a mutual sparsity condition which requireseach source signal to possess a stand-alone peak in its spectrum. We formulatealternative conditions so that enough data points fall on the facets of a coneinstead of accumulating around the vertices. To find a regime of uniquesolvability, we make use of both geometric and density properties of the datapoints, and develop an efficient facet identification method by combining dataclassification and linear regression. For noisy data, we show that denoisingmethods may be employed, such as the total variation technique in imagingprocessing, and principle component analysis. We show computational results onnuclear magnetic resonance spectroscopic data to substantiate our method.
arxiv-2700-274 | Classifier Fusion Method to Recognize Handwritten Kannada Numerals | http://arxiv.org/pdf/1301.0167v1.pdf | author:H. R. Mamatha, S. Karthik, Murthy K. Srikanta category:cs.CV published:2013-01-02 summary:Optical Character Recognition (OCR) is one of the important fields in imageprocessing and pattern recognition domain. Handwritten character recognitionhas always been a challenging task. Only a little work can be traced towardsthe recognition of handwritten characters for the south Indian languages.Kannada is one such south Indian language which is also one of the officiallanguage of India. Accurate recognition of Kannada characters is a challengingtask because of the high degree of similarity between the characters. Hence,good quality features are to be extracted and better classifiers are needed toimprove the accuracy of the OCR for Kannada characters. This paper explores theeffectiveness of feature extraction method like run length count (RLC) anddirectional chain code (DCC) for the recognition of handwritten Kannadanumerals. In this paper, a classifier fusion method is implemented to improvethe recognition rate. For the classifier fusion, we have considered K-nearestneighbour (KNN) and Linear classifier (LC). The novelty of this method is toachieve better accuracy with few features using classifier fusion approach.Proposed method achieves an average recognition rate of 96%.
arxiv-2700-275 | Policy Evaluation with Variance Related Risk Criteria in Markov Decision Processes | http://arxiv.org/pdf/1301.0104v1.pdf | author:Aviv Tamar, Dotan Di Castro, Shie Mannor category:cs.LG stat.ML published:2013-01-01 summary:In this paper we extend temporal difference policy evaluation algorithms toperformance criteria that include the variance of the cumulative reward. Suchcriteria are useful for risk management, and are important in domains such asfinance and process control. We propose both TD(0) and LSTD(lambda) variantswith linear function approximation, prove their convergence, and demonstratetheir utility in a 4-dimensional continuous state space problem.
arxiv-2700-276 | CloudSVM : Training an SVM Classifier in Cloud Computing Systems | http://arxiv.org/pdf/1301.0082v1.pdf | author:F. Ozgur Catak, M. Erdal Balaban category:cs.LG cs.DC published:2013-01-01 summary:In conventional method, distributed support vector machines (SVM) algorithmsare trained over pre-configured intranet/internet environments to find out anoptimal classifier. These methods are very complicated and costly for largedatasets. Hence, we propose a method that is referred as the Cloud SVM trainingmechanism (CloudSVM) in a cloud computing environment with MapReduce techniquefor distributed machine learning applications. Accordingly, (i) SVM algorithmis trained in distributed cloud storage servers that work concurrently; (ii)merge all support vectors in every trained cloud node; and (iii) iterate thesetwo steps until the SVM converges to the optimal classifier function. Largescale data sets are not possible to train using SVM algorithm on a singlecomputer. The results of this study are important for training of large scaledata sets for machine learning applications. We provided that iterativetraining of splitted data set in cloud computing environment using SVM willconverge to a global optimal classifier in finite iteration size.
arxiv-2700-277 | Generating High-Order Threshold Functions with Multiple Thresholds | http://arxiv.org/pdf/1301.0048v1.pdf | author:Yukihiro Kamada, Kiyonori Miyasaki category:cs.NE published:2013-01-01 summary:In this paper, we consider situations in which a given logical function isrealized by a multithreshold threshold function. In such situations, constantfunctions can be easily obtained from multithreshold threshold functions, andtherefore, we can show that it becomes possible to optimize a class ofhigh-order neural networks. We begin by proposing a generating method forthreshold functions in which we use a vector that determines the boundarybetween the linearly separable function and the high-order threshold function.By applying this method to high-order threshold functions, we show thatfunctions with the same weight as, but a different threshold than, a thresholdfunction generated by the generation process can be easily obtained. We alsoshow that the order of the entire network can be extended while maintaining thestructure of given functions.
arxiv-2700-278 | On Distributed Online Classification in the Midst of Concept Drifts | http://arxiv.org/pdf/1301.0047v1.pdf | author:Zaid J. Towfic, Jianshu Chen, Ali H. Sayed category:math.OC cs.DC cs.LG cs.SI physics.soc-ph published:2013-01-01 summary:In this work, we analyze the generalization ability of distributed onlinelearning algorithms under stationary and non-stationary environments. We derivebounds for the excess-risk attained by each node in a connected network oflearners and study the performance advantage that diffusion strategies haveover individual non-cooperative processing. We conduct extensive simulations toillustrate the results.
arxiv-2700-279 | Semi-Supervised Domain Adaptation with Non-Parametric Copulas | http://arxiv.org/pdf/1301.0142v1.pdf | author:David Lopez-Paz, José Miguel Hernández-Lobato, Bernhard Schölkopf category:stat.ML cs.LG published:2013-01-01 summary:A new framework based on the theory of copulas is proposed to address semi-supervised domain adaptation problems. The presented method factorizes anymultivariate density into a product of marginal distributions and bivariatecop- ula functions. Therefore, changes in each of these factors can be detectedand corrected to adapt a density model accross different learning domains.Impor- tantly, we introduce a novel vine copula model, which allows for thisfactorization in a non-parametric manner. Experimental results on regressionproblems with real-world data illustrate the efficacy of the proposed approachwhen compared to state-of-the-art techniques.
arxiv-2700-280 | A Semi-automated Statistical Algorithm for Object Separation | http://arxiv.org/pdf/1301.0127v3.pdf | author:Madhur Srivastava, Satish K. Singh, Prasanta K. Panigrahi category:cs.CV published:2013-01-01 summary:We explicate a semi-automated statistical algorithm for object identificationand segregation in both gray scale and color images. The algorithm makesoptimal use of the observation that definite objects in an image are typicallyrepresented by pixel values having narrow Gaussian distributions aboutcharacteristic mean values. Furthermore, for visually distinct objects, thecorresponding Gaussian distributions have negligible overlap with each otherand hence the Mahalanobis distance between these distributions are large. Thesestatistical facts enable one to sub-divide images into multiple thresholds ofvariable sizes, each segregating similar objects. The procedure incorporatesthe sensitivity of human eye to the gray pixel values into the variablethreshold size, while mapping the Gaussian distributions into localized\delta-functions, for object separation. The effectiveness of this recursivestatistical algorithm is demonstrated using a wide variety of images.
arxiv-2700-281 | Autonomously Learning to Visually Detect Where Manipulation Will Succeed | http://arxiv.org/pdf/1212.6837v1.pdf | author:Hai Nguyen, Charles C. Kemp category:cs.RO cs.AI cs.CV published:2012-12-31 summary:Visual features can help predict if a manipulation behavior will succeed at agiven location. For example, the success of a behavior that flips lightswitches depends on the location of the switch. Within this paper, we presentmethods that enable a mobile manipulator to autonomously learn a function thattakes an RGB image and a registered 3D point cloud as input and returns a 3Dlocation at which a manipulation behavior is likely to succeed. Given a pair ofmanipulation behaviors that can change the state of the world between two sets(e.g., light switch up and light switch down), classifiers that detect wheneach behavior has been successful, and an initial hint as to where one of thebehaviors will be successful, the robot autonomously trains a pair of supportvector machine (SVM) classifiers by trying out the behaviors at locations inthe world and observing the results. When an image feature vector associatedwith a 3D location is provided as input to one of the SVMs, the SVM predicts ifthe associated manipulation behavior will be successful at the 3D location. Toevaluate our approach, we performed experiments with a PR2 robot from WillowGarage in a simulated home using behaviors that flip a light switch, push arocker-type light switch, and operate a drawer. By using active learning, therobot efficiently learned SVMs that enabled it to consistently succeed at thesetasks. After training, the robot also continued to learn in order to adapt inthe event of failure.
arxiv-2700-282 | Maximizing a Nonnegative, Monotone, Submodular Function Constrained to Matchings | http://arxiv.org/pdf/1212.6846v2.pdf | author:Sagar Kale category:cs.DS cs.AI cs.CC cs.LG stat.ML published:2012-12-31 summary:Submodular functions have many applications. Matchings have manyapplications. The bitext word alignment problem can be modeled as the problemof maximizing a nonnegative, monotone, submodular function constrained tomatchings in a complete bipartite graph where each vertex corresponds to a wordin the two input sentences and each edge represents a potential word-to-wordtranslation. We propose a more general problem of maximizing a nonnegative,monotone, submodular function defined on the edge set of a complete graphconstrained to matchings; we call this problem the CSM-Matching problem.CSM-Matching also generalizes the maximum-weight matching problem, which has apolynomial-time algorithm; however, we show that it is NP-hard to approximateCSM-Matching within a factor of e/(e-1) by reducing the max k-cover problem toit. Our main result is a simple, greedy, 3-approximation algorithm forCSM-Matching. Then we reduce CSM-Matching to maximizing a nonnegative,monotone, submodular function over two matroids, i.e., CSM-2-Matroids.CSM-2-Matroids has a (2+epsilon)-approximation algorithm - called LSV2. We showthat we can find a (4+epsilon)-approximate solution to CSM-Matching using LSV2.We extend this approach to similar problems.
arxiv-2700-283 | On Automation and Medical Image Interpretation, With Applications for Laryngeal Imaging | http://arxiv.org/pdf/1212.6933v3.pdf | author:H. J. Moukalled category:cs.CV published:2012-12-31 summary:Indeed, these are exciting times. We are in the heart of a digitalrenaissance. Automation and computer technology allow engineers and scientiststo fabricate processes that amalgamate quality of life. We anticipate muchgrowth in medical image interpretation and understanding, due to the influx ofcomputer technologies. This work should serve as a guide to introduce thereader to core themes in theoretical computer science, as well as imagingapplications for understanding vocal-fold vibrations. In this work, we motivatethe use of automation, review some mathematical models of computation. Wepresent a proof of a classical problem in image analysis that cannot beautomated by means of algorithms. Furthermore, discuss some applications forprocessing medical images of the vocal folds, and discuss some of theexhilarating directions the art of automation will take vocal-fold imageinterpretation and quite possibly other areas of biomedical image analysis.
arxiv-2700-284 | Bethe Bounds and Approximating the Global Optimum | http://arxiv.org/pdf/1301.0015v1.pdf | author:Adrian Weller, Tony Jebara category:cs.LG stat.ML published:2012-12-31 summary:Inference in general Markov random fields (MRFs) is NP-hard, thoughidentifying the maximum a posteriori (MAP) configuration of pairwise MRFs withsubmodular cost functions is efficiently solvable using graph cuts. Marginalinference, however, even for this restricted class, is in #P. We prove newformulations of derivatives of the Bethe free energy, provide bounds on thederivatives and bracket the locations of stationary points, introducing a newtechnique called Bethe bound propagation. Several results apply to pairwisemodels whether associative or not. Applying these to discretizedpseudo-marginals in the associative case we present a polynomial timeapproximation scheme for global optimization provided the maximum degree is$O(\log n)$, and discuss several extensions.
arxiv-2700-285 | Fast Solutions to Projective Monotone Linear Complementarity Problems | http://arxiv.org/pdf/1212.6958v1.pdf | author:Geoffrey J. Gordon category:cs.LG math.OC published:2012-12-31 summary:We present a new interior-point potential-reduction algorithm for solvingmonotone linear complementarity problems (LCPs) that have a particular specialstructure: their matrix $M\in{\mathbb R}^{n\times n}$ can be decomposed as$M=\Phi U + \Pi_0$, where the rank of $\Phi$ is $k<n$, and $\Pi_0$ denotesEuclidean projection onto the nullspace of $\Phi^\top$. We call such LCPsprojective. Our algorithm solves a monotone projective LCP to relative accuracy$\epsilon$ in $O(\sqrt n \ln(1/\epsilon))$ iterations, with each iterationrequiring $O(nk^2)$ flops. This complexity compares favorably withinterior-point algorithms for general monotone LCPs: these algorithms alsorequire $O(\sqrt n \ln(1/\epsilon))$ iterations, but each iteration needs tosolve an $n\times n$ system of linear equations, a much higher cost than ouralgorithm when $k\ll n$. Our algorithm works even though the solution to aprojective LCP is not restricted to lie in any low-rank subspace.
arxiv-2700-286 | Blind Analysis of EGM Signals: Sparsity-Aware Formulation | http://arxiv.org/pdf/1212.6936v1.pdf | author:David Luengo, Javier Via, Sandra Monzon, Tom Trigano, Antonio Artes-Rodriguez category:stat.ML published:2012-12-31 summary:This technical note considers the problems of blind sparse learning andinference of electrogram (EGM) signals under atrial fibrillation (AF)conditions. First of all we introduce a mathematical model for the observedsignals that takes into account the multiple foci typically appearing insidethe heart during AF. Then we propose a reconstruction model based on a fixeddictionary and discuss several alternatives for choosing the dictionary. Inorder to obtain a sparse solution that takes into account the biologicalrestrictions of the problem, a first alternative is using LASSO regularizationfollowed by a post-processing stage that removes low amplitude coefficientsviolating the refractory period characteristic of cardiac cells. As analternative we propose a novel regularization term, called cross products LASSO(CP-LASSO), that is able to incorporate the biological constraints directlyinto the optimization problem. Unfortunately, the resulting problem isnon-convex, but we show how it can be solved efficiently in an approximated waymaking use of successive convex approximations (SCA). Finally, spectralanalysis is performed on the clean activation sequence obtained from the sparselearning stage in order to estimate the number of latent foci and theirfrequencies. Simulations on synthetic and real data are provided to validatethe proposed approach.
arxiv-2700-287 | Training a Functional Link Neural Network Using an Artificial Bee Colony for Solving a Classification Problems | http://arxiv.org/pdf/1212.6922v1.pdf | author:Yana Mazwin Mohmad Hassim, Rozaida Ghazali category:cs.NE cs.LG published:2012-12-31 summary:Artificial Neural Networks have emerged as an important tool forclassification and have been widely used to classify a non-linear separablepattern. The most popular artificial neural networks model is a MultilayerPerceptron (MLP) as it is able to perform classification task with significantsuccess. However due to the complexity of MLP structure and also problems suchas local minima trapping, over fitting and weight interference have made neuralnetwork training difficult. Thus, the easy way to avoid these problems is toremove the hidden layers. This paper presents the ability of Functional LinkNeural Network (FLNN) to overcome the complexity structure of MLP by usingsingle layer architecture and propose an Artificial Bee Colony (ABC)optimization for training the FLNN. The proposed technique is expected toprovide better learning scheme for a classifier in order to get more accurateclassification result
arxiv-2700-288 | Local and global asymptotic inference in smoothing spline models | http://arxiv.org/pdf/1212.6788v3.pdf | author:Zuofeng Shang, Guang Cheng category:math.ST stat.ML stat.TH published:2012-12-30 summary:This article studies local and global inference for smoothing splineestimation in a unified asymptotic framework. We first introduce a newtechnical tool called functional Bahadur representation, which significantlygeneralizes the traditional Bahadur representation in parametric models, thatis, Bahadur [Ann. Inst. Statist. Math. 37 (1966) 577-580]. Equipped with thistool, we develop four interconnected procedures for inference: (i) pointwiseconfidence interval; (ii) local likelihood ratio testing; (iii) simultaneousconfidence band; (iv) global likelihood ratio testing. In particular, ourconfidence intervals are proved to be asymptotically valid at any point in thesupport, and they are shorter on average than the Bayesian confidence intervalsproposed by Wahba [J. R. Stat. Soc. Ser. B Stat. Methodol. 45 (1983) 133-150]and Nychka [J. Amer. Statist. Assoc. 83 (1988) 1134-1143]. We also discuss aversion of the Wilks phenomenon arising from local/global likelihood ratiotesting. It is also worth noting that our simultaneous confidence bands are thefirst ones applicable to general quasi-likelihood models. Furthermore, issuesrelating to optimality and efficiency are carefully addressed. As a by-product,we discover a surprising relationship between periodic and nonperiodicsmoothing splines in terms of inference.
arxiv-2700-289 | Focus of Attention for Linear Predictors | http://arxiv.org/pdf/1212.6659v1.pdf | author:Raphael Pelossof, Zhiliang Ying category:stat.ML cs.AI cs.LG published:2012-12-29 summary:We present a method to stop the evaluation of a prediction process when theresult of the full evaluation is obvious. This trait is highly desirable inprediction tasks where a predictor evaluates all its features for every examplein large datasets. We observe that some examples are easier to classify thanothers, a phenomenon which is characterized by the event when most of thefeatures agree on the class of an example. By stopping the feature evaluationwhen encountering an easy- to-classify example, the predictor can achievesubstantial gains in computation. Our method provides a natural attentionmechanism for linear predictors where the predictor concentrates most of itscomputation on hard-to-classify examples and quickly discards easy-to-classifyones. By modifying a linear prediction algorithm such as an SVM or AdaBoost toinclude our attentive method we prove that the average number of featurescomputed is O(sqrt(n log 1/sqrt(delta))) where n is the original number offeatures, and delta is the error rate incurred due to early stopping. Wedemonstrate the effectiveness of Attentive Prediction on MNIST, Real-sim,Gisette, and synthetic datasets.
arxiv-2700-290 | Discovering Basic Emotion Sets via Semantic Clustering on a Twitter Corpus | http://arxiv.org/pdf/1212.6527v1.pdf | author:Eugene Yuta Bann category:cs.AI cs.CL published:2012-12-28 summary:A plethora of words are used to describe the spectrum of human emotions, buthow many emotions are there really, and how do they interact? Over the past fewdecades, several theories of emotion have been proposed, each based around theexistence of a set of 'basic emotions', and each supported by an extensivevariety of research including studies in facial expression, ethology, neurologyand physiology. Here we present research based on a theory that people transmittheir understanding of emotions through the language they use surroundingemotion keywords. Using a labelled corpus of over 21,000 tweets, six of thebasic emotion sets proposed in existing literature were analysed using LatentSemantic Clustering (LSC), evaluating the distinctiveness of the semanticmeaning attached to the emotional label. We hypothesise that the more distinctthe language is used to express a certain emotion, then the more distinct theperception (including proprioception) of that emotion is, and thus more'basic'. This allows us to select the dimensions best representing the entirespectrum of emotion. We find that Ekman's set, arguably the most frequentlyused for classifying emotions, is in fact the most semantically distinctoverall. Next, taking all analysed (that is, previously proposed) emotion termsinto account, we determine the optimal semantically irreducible basic emotionset using an iterative LSC algorithm. Our newly-derived set (Accepting,Ashamed, Contempt, Interested, Joyful, Pleased, Sleepy, Stressed) generates a6.1% increase in distinctiveness over Ekman's set (Angry, Disgusted, Joyful,Sad, Scared). We also demonstrate how using LSC data can help visualiseemotions. We introduce the concept of an Emotion Profile and briefly analysecompound emotions both visually and mathematically.
arxiv-2700-291 | Localized Algorithm of Community Detection on Large-Scale Decentralized Social Networks | http://arxiv.org/pdf/1212.6323v1.pdf | author:Pili Hu, Wing Cheong Lau category:cs.SI physics.soc-ph stat.ML published:2012-12-27 summary:Despite the overwhelming success of the existing Social Networking Services(SNS), their centralized ownership and control have led to serious concerns inuser privacy, censorship vulnerability and operational robustness of theseservices. To overcome these limitations, Distributed Social Networks (DSN) haverecently been proposed and implemented. Under these new DSN architectures, nosingle party possesses the full knowledge of the entire social network. Whilethis approach solves the above problems, the lack of global knowledge for theDSN nodes makes it much more challenging to support some common but criticalSNS services like friends discovery and community detection. In this paper, wetackle the problem of community detection for a given user under the constraintof limited local topology information as imposed by common DSN architectures.By considering the Personalized Page Rank (PPR) approach as an ink spillingprocess, we justify its applicability for decentralized community detectionusing limited local topology information.Our proposed PPR-based solution has awide range of applications such as friends recommendation, targetedadvertisement, automated social relationship labeling and sybil defense. Usingdata collected from a large-scale SNS in practice, we demonstrate our adaptedversion of PPR can significantly outperform the basic PR as well as two othercommonly used heuristics. The inclusion of a few manually labeled friends inthe Escape Vector (EV) can boost the performance considerably (64.97% relativeimprovement in terms of Area Under the ROC Curve (AUC)).
arxiv-2700-292 | On-line relational SOM for dissimilarity data | http://arxiv.org/pdf/1212.6316v1.pdf | author:Madalina Olteanu, Nathalie Villa-Vialaneix, Marie Cottrell category:stat.ML cs.LG published:2012-12-27 summary:In some applications and in order to address real world situations better,data may be more complex than simple vectors. In some examples, they can beknown through their pairwise dissimilarities only. Several variants of the SelfOrganizing Map algorithm were introduced to generalize the original algorithmto this framework. Whereas median SOM is based on a rough representation of theprototypes, relational SOM allows representing these prototypes by a virtualcombination of all elements in the data set. However, this latter approachsuffers from two main drawbacks. First, its complexity can be large. Second,only a batch version of this algorithm has been studied so far and it oftenprovides results having a bad topographic organization. In this article, anon-line version of relational SOM is described and justified. The algorithm istested on several datasets, including categorical data and graphs, and comparedwith the batch version and with other SOM algorithms for non vector data.
arxiv-2700-293 | A brief experience on journey through hardware developments for image processing and its applications on Cryptography | http://arxiv.org/pdf/1212.6303v1.pdf | author:Sangeet Saha, Chandrajit pal, Rourab paul, Satyabrata Maity, Suman Sau category:cs.AR cs.CR cs.CV published:2012-12-27 summary:The importance of embedded applications on image and videoprocessing,communication and cryptography domain has been taking a larger spacein current research era. Improvement of pictorial information for betterment ofhuman perception like deblurring, de-noising in several fields such assatellite imaging, medical imaging etc are renewed research thrust.Specifically we would like to elaborate our experience on the significance ofcomputer vision as one of the domains where hardware implemented algorithmsperform far better than those implemented through software. So far embeddeddesign engineers have successfully implemented their designs by means ofApplication Specific Integrated Circuits (ASICs) and/or Digital SignalProcessors (DSP), however with the advancement of VLSI technology a verypowerful hardware device namely the Field Programmable Gate Array (FPGA)combining the key advantages of ASICs and DSPs was developed which have thepossibility of reprogramming making them a very attractive device for rapidprototyping.Communication of image and video data in multiple FPGA is no longerfar away from the thrust of secured transmission among them, and then therelevance of cryptography is indeed unavoidable. This paper shows how theXilinx hardware development platform as well Mathworks Matlab can be used todevelop hardware based computer vision algorithms and its corresponding cryptotransmission channel between multiple FPGA platform from a system levelapproach, making it favourable for developing a hardware-software co-designenvironment.
arxiv-2700-294 | Group theory, group actions, evolutionary algorithms, and global optimization | http://arxiv.org/pdf/1301.0254v3.pdf | author:Andrew Clark category:cs.NE math.DS math.OC math.RA published:2012-12-27 summary:In this paper we use group, action and orbit to understand how evolutionarysolve nonconvex optimization problems.
arxiv-2700-295 | Gaussian Process Regression with Heteroscedastic or Non-Gaussian Residuals | http://arxiv.org/pdf/1212.6246v1.pdf | author:Chunyi Wang, Radford M. Neal category:stat.ML cs.LG published:2012-12-26 summary:Gaussian Process (GP) regression models typically assume that residuals areGaussian and have the same variance for all observations. However, applicationswith input-dependent noise (heteroscedastic residuals) frequently arise inpractice, as do applications in which the residuals do not have a Gaussiandistribution. In this paper, we propose a GP Regression model with a latentvariable that serves as an additional unobserved covariate for the regression.This model (which we call GPLC) allows for heteroscedasticity since it allowsthe function to have a changing partial derivative with respect to thisunobserved covariate. With a suitable covariance function, our GPLC model canhandle (a) Gaussian residuals with input-dependent variance, or (b)non-Gaussian residuals with input-dependent variance, or (c) Gaussian residualswith constant variance. We compare our model, using synthetic datasets, with amodel proposed by Goldberg, Williams and Bishop (1998), which we refer to asGPLV, which only deals with case (a), as well as a standard GP model which canhandle only case (c). Markov Chain Monte Carlo methods are developed for bothmodelsl. Experiments show that when the data is heteroscedastic, both GPLC andGPLV give better results (smaller mean squared error and negativelog-probability density) than standard GP regression. In addition, when theresidual are Gaussian, our GPLC model is generally nearly as good as GPLV,while when the residuals are non-Gaussian, our GPLC model is better than GPLV.
arxiv-2700-296 | Efficient Multiple Object Tracking Using Mutually Repulsive Active Membranes | http://arxiv.org/pdf/1212.6209v1.pdf | author:Yi Deng, Philip Coen, Mingzhai Sun, Joshua W. Shaevitz category:q-bio.QM cs.CV physics.bio-ph published:2012-12-26 summary:Studies of social and group behavior in interacting organisms requirehigh-throughput analysis of the motion of a large number of individualsubjects. Computer vision techniques offer solutions to specific trackingproblems, and allow automated and efficient tracking with minimal humanintervention. In this work, we adopt the open active contour model to track thetrajectories of moving objects at high density. We add repulsive interactionsbetween open contours to the original model, treat the trajectories as anextrusion in the temporal dimension, and show applications to two trackingproblems. The walking behavior of Drosophila is studied at different populationdensity and gender composition. We demonstrate that individual male flies havedistinct walking signatures, and that the social interaction between flies in amixed gender arena is gender specific. We also apply our model to studies oftrajectories of gliding Myxococcus xanthus bacteria at high density. We examinethe individual gliding behavioral statistics in terms of the gliding speeddistribution. Using these two examples at very distinctive spatial scales, weillustrate the use of our algorithm on tracking both short rigid bodies(Drosophila) and long flexible objects (Myxococcus xanthus). Our repulsiveactive membrane model reaches error rates better than $5\times 10^{-6}$ per flyper second for Drosophila tracking and comparable results for Myxococcusxanthus.
arxiv-2700-297 | Transfer Learning Using Logistic Regression in Credit Scoring | http://arxiv.org/pdf/1212.6167v1.pdf | author:Farid Beninel, Waad Bouaguel, Ghazi Belmufti category:cs.LG cs.CE published:2012-12-26 summary:The credit scoring risk management is a fast growing field due to consumer'scredit requests. Credit requests, of new and existing customers, are oftenevaluated by classical discrimination rules based on customers information.However, these kinds of strategies have serious limits and don't take intoaccount the characteristics difference between current customers and the futureones. The aim of this paper is to measure credit worthiness for non customersborrowers and to model potential risk given a heterogeneous population formedby borrowers customers of the bank and others who are not. We hold on previousworks done in generalized gaussian discrimination and transpose them into thelogistic model to bring out efficient discrimination rules for non customers'subpopulation. Therefore we obtain several simple models of connection between parameters ofboth logistic models associated respectively to the two subpopulations. TheGerman credit data set is selected to experiment and to compare these models.Experimental results show that the use of links between the two subpopulationsimprove the classification accuracy for the new loan applicants.
arxiv-2700-298 | Hyperplane Arrangements and Locality-Sensitive Hashing with Lift | http://arxiv.org/pdf/1212.6110v1.pdf | author:Makiko Konoshima, Yui Noma category:cs.LG cs.IR stat.ML H.3.3; H.3.m published:2012-12-26 summary:Locality-sensitive hashing converts high-dimensional feature vectors, such asimage and speech, into bit arrays and allows high-speed similarity calculationwith the Hamming distance. There is a hashing scheme that maps feature vectorsto bit arrays depending on the signs of the inner products between featurevectors and the normal vectors of hyperplanes placed in the feature space. Thishashing can be seen as a discretization of the feature space by hyperplanes. Iflabels for data are given, one can determine the hyperplanes by using learningalgorithms. However, many proposed learning methods do not consider thehyperplanes' offsets. Not doing so decreases the number of partitioned regions,and the correlation between Hamming distances and Euclidean distances becomessmall. In this paper, we propose a lift map that converts learning algorithmswithout the offsets to the ones that take into account the offsets. With thismethod, the learning methods without the offsets give the discretizations ofspaces as if it takes into account the offsets. For the proposed method, weinput several high-dimensional feature data sets and studied the relationshipbetween the statistical characteristics of data, the number of hyperplanes, andthe effect of the proposed method.
arxiv-2700-299 | Echo State Queueing Network: a new reservoir computing learning tool | http://arxiv.org/pdf/1212.6276v1.pdf | author:Sebastián Basterrech, Gerardo Rubino category:cs.NE cs.AI cs.LG published:2012-12-26 summary:In the last decade, a new computational paradigm was introduced in the fieldof Machine Learning, under the name of Reservoir Computing (RC). RC models areneural networks which a recurrent part (the reservoir) that does notparticipate in the learning process, and the rest of the system where norecurrence (no neural circuit) occurs. This approach has grown rapidly due toits success in solving learning tasks and other computational applications.Some success was also observed with another recently proposed neural networkdesigned using Queueing Theory, the Random Neural Network (RandNN). Bothapproaches have good properties and identified drawbacks. In this paper, wepropose a new RC model called Echo State Queueing Network (ESQN), where we useideas coming from RandNNs for the design of the reservoir. ESQNs consist inESNs where the reservoir has a new dynamics inspired by recurrent RandNNs. Thepaper positions ESQNs in the global Machine Learning area, and providesexamples of their use and performances. We show on largely used benchmarks thatESQNs are very accurate tools, and we illustrate how they compare with standardESNs.
arxiv-2700-300 | High Quality Image Interpolation via Local Autoregressive and Nonlocal 3-D Sparse Regularization | http://arxiv.org/pdf/1212.6058v1.pdf | author:Xinwei Gao, Jian Zhang, Feng Jiang, Xiaopeng Fan, Siwei Ma, Debin Zhao category:cs.MM cs.CV published:2012-12-25 summary:In this paper, we propose a novel image interpolation algorithm, which isformulated via combining both the local autoregressive (AR) model and thenonlocal adaptive 3-D sparse model as regularized constraints under theregularization framework. Estimating the high-resolution image by the local ARregularization is different from these conventional AR models, which weightedcalculates the interpolation coefficients without considering the roughstructural similarity between the low-resolution (LR) and high-resolution (HR)images. Then the nonlocal adaptive 3-D sparse model is formulated to regularizethe interpolated HR image, which provides a way to modify these pixels with theproblem of numerical stability caused by AR model. In addition, a newSplit-Bregman based iterative algorithm is developed to solve the aboveoptimization problem iteratively. Experiment results demonstrate that theproposed algorithm achieves significant performance improvements over thetraditional algorithms in terms of both objective quality and visual perception
