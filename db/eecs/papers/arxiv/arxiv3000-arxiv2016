arxiv-3000-1 | Declarative Modeling and Bayesian Inference of Dark Matter Halos | http://arxiv.org/pdf/1306.0202v1.pdf | author:Gabriel Kronberger category:stat.ML astro-ph.IM published:2013-06-02 summary:Probabilistic programming allows specification of probabilistic models in adeclarative manner. Recently, several new software systems and languages forprobabilistic programming have been developed on the basis of newly developedand improved methods for approximate inference in probabilistic models. In thiscontribution a probabilistic model for an idealized dark matter localizationproblem is described. We first derive the probabilistic model for the inferenceof dark matter locations and masses, and then show how this model can beimplemented using BUGS and Infer.NET, two software systems for probabilisticprogramming. Finally, the different capabilities of both systems are discussed.The presented dark matter model includes mainly non-conjugate factors, thus, itis difficult to implement this model with Infer.NET.
arxiv-3000-2 | Dynamic Ad Allocation: Bandits with Budgets | http://arxiv.org/pdf/1306.0155v1.pdf | author:Aleksandrs Slivkins category:cs.LG cs.DS published:2013-06-01 summary:We consider an application of multi-armed bandits to internet advertising(specifically, to dynamic ad allocation in the pay-per-click model, withuncertainty on the click probabilities). We focus on an important practicalissue that advertisers are constrained in how much money they can spend ontheir ad campaigns. This issue has not been considered in the prior work onbandit-based approaches for ad allocation, to the best of our knowledge. We define a simple, stylized model where an algorithm picks one ad to displayin each round, and each ad has a \emph{budget}: the maximal amount of moneythat can be spent on this ad. This model admits a natural variant of UCB1, awell-known algorithm for multi-armed bandits with stochastic rewards. We derivestrong provable guarantees for this algorithm.
arxiv-3000-3 | An Analysis of the Connections Between Layers of Deep Neural Networks | http://arxiv.org/pdf/1306.0152v1.pdf | author:Eugenio Culurciello, Jonghoon Jin, Aysegul Dundar, Jordan Bates category:cs.CV published:2013-06-01 summary:We present an analysis of different techniques for selecting the connectionbe- tween layers of deep neural networks. Traditional deep neural networks useran- dom connection tables between layers to keep the number of connectionssmall and tune to different image features. This kind of connection performsadequately in supervised deep networks because their values are refined duringthe training. On the other hand, in unsupervised learning, one cannot rely onback-propagation techniques to learn the connections between layers. In thiswork, we tested four different techniques for connecting the first layer of thenetwork to the second layer on the CIFAR and SVHN datasets and showed that theaccuracy can be im- proved up to 3% depending on the technique used. We alsoshowed that learning the connections based on the co-occurrences of thefeatures does not confer an advantage over a random connection table in smallnetworks. This work is helpful to improve the efficiency of connections betweenthe layers of unsupervised deep neural networks.
arxiv-3000-4 | Image Inpainting by Kriging Interpolation Technique | http://arxiv.org/pdf/1306.0139v1.pdf | author:Firas A. Jassim category:cs.CV published:2013-06-01 summary:Image inpainting is the art of predicting damaged regions of an image. Themanual way of image inpainting is a time consuming. Therefore, there must be anautomatic digital method for image inpainting that recovers the image from thedamaged regions. In this paper, a novel statistical image inpainting algorithmbased on Kriging interpolation technique was proposed. Kriging techniqueautomatically fills the damaged region in an image using the informationavailable from its surrounding regions in such away that it uses the spatialcorrelation structure of points inside the k-by-k block. Kriging has theability to face the challenge of keeping the structure and texture informationas the size of damaged region heighten. Experimental results showed that,Kriging has a high PSNR value when recovering a variety of test images fromscratches and text as damaged regions.
arxiv-3000-5 | Understanding ACT-R - an Outsider's Perspective | http://arxiv.org/pdf/1306.0125v1.pdf | author:Jacob Whitehill category:cs.LG published:2013-06-01 summary:The ACT-R theory of cognition developed by John Anderson and colleaguesendeavors to explain how humans recall chunks of information and how they solveproblems. ACT-R also serves as a theoretical basis for "cognitive tutors",i.e., automatic tutoring systems that help students learn mathematics, computerprogramming, and other subjects. The official ACT-R definition is distributedacross a large body of literature spanning many articles and monographs, andhence it is difficult for an "outsider" to learn the most important aspects ofthe theory. This paper aims to provide a tutorial to the core components of theACT-R theory.
arxiv-3000-6 | One-Class Support Measure Machines for Group Anomaly Detection | http://arxiv.org/pdf/1303.0309v2.pdf | author:Krikamol Muandet, Bernhard Schölkopf category:stat.ML cs.LG published:2013-03-01 summary:We propose one-class support measure machines (OCSMMs) for group anomalydetection which aims at recognizing anomalous aggregate behaviors of datapoints. The OCSMMs generalize well-known one-class support vector machines(OCSVMs) to a space of probability measures. By formulating the problem asquantile estimation on distributions, we can establish an interestingconnection to the OCSVMs and variable kernel density estimators (VKDEs) overthe input space on which the distributions are defined, bridging the gapbetween large-margin methods and kernel density estimators. In particular, weshow that various types of VKDEs can be considered as solutions to a class ofregularization problems studied in this paper. Experiments on Sloan Digital SkySurvey dataset and High Energy Particle Physics dataset demonstrate thebenefits of the proposed framework in real-world applications.
arxiv-3000-7 | Harmony search algorithm for the container storage problem | http://arxiv.org/pdf/1306.0090v1.pdf | author:I. Ayachi, R. Kammarti, M. Ksouri, P. Borne, Lagis Ecole Centrale de Lille, Lacs Ecole Nationale category:cs.NE published:2013-06-01 summary:Recently a new metaheuristic called harmony search was developed. It mimicsthe behaviors of musicians improvising to find the better state harmony. Inthis paper, this algorithm is described and applied to solve the containerstorage problem in the harbor. The objective of this problem is to determine avalid containers arrangement, which meets customers delivery deadlines, reducesthe number of container rehandlings and minimizes the ship idle time. In thispaper, an adaptation of the harmony search algorithm to the container storageproblem is detailed and some experimental results are presented and discussed.The proposed approach was compared to a genetic algorithm previously applied tothe same problem and recorded a good results.
arxiv-3000-8 | Online Learning with Switching Costs and Other Adaptive Adversaries | http://arxiv.org/pdf/1302.4387v2.pdf | author:Nicolo Cesa-Bianchi, Ofer Dekel, Ohad Shamir category:cs.LG stat.ML published:2013-02-18 summary:We study the power of different types of adaptive (nonoblivious) adversariesin the setting of prediction with expert advice, under both full-informationand bandit feedback. We measure the player's performance using a new notion ofregret, also known as policy regret, which better captures the adversary'sadaptiveness to the player's behavior. In a setting where losses are allowed todrift, we characterize ---in a nearly complete manner--- the power of adaptiveadversaries with bounded memories and switching costs. In particular, we showthat with switching costs, the attainable rate with bandit feedback is$\widetilde{\Theta}(T^{2/3})$. Interestingly, this rate is significantly worsethan the $\Theta(\sqrt{T})$ rate attainable with switching costs in thefull-information case. Via a novel reduction from experts to bandits, we alsoshow that a bounded memory adversary can force $\widetilde{\Theta}(T^{2/3})$regret even in the full information case, proving that switching costs areeasier to control than bounded memory adversaries. Our lower bounds rely on anew stochastic adversary strategy that generates loss processes with strongdependencies.
arxiv-3000-9 | Expectation-maximization for logistic regression | http://arxiv.org/pdf/1306.0040v1.pdf | author:James G. Scott, Liang Sun category:stat.CO math.ST stat.ML stat.TH published:2013-05-31 summary:We present a family of expectation-maximization (EM) algorithms for binaryand negative-binomial logistic regression, drawing a sharp connection with thevariational-Bayes algorithm of Jaakkola and Jordan (2000). Indeed, our resultsallow a version of this variational-Bayes approach to be re-interpreted as atrue EM algorithm. We study several interesting features of the algorithm, andof this previously unrecognized connection with variational Bayes. We alsogeneralize the approach to sparsity-promoting priors, and to an online methodwhose convergence properties are easily established. This latter methodcompares favorably with stochastic-gradient descent in situations with markedcollinearity.
arxiv-3000-10 | Graph-based Generalization Bounds for Learning Binary Relations | http://arxiv.org/pdf/1302.5348v3.pdf | author:Ben London, Bert Huang, Lise Getoor category:cs.LG published:2013-02-21 summary:We investigate the generalizability of learned binary relations: functionsthat map pairs of instances to a logical indicator. This problem hasapplication in numerous areas of machine learning, such as ranking, entityresolution and link prediction. Our learning framework incorporates an examplelabeler that, given a sequence $X$ of $n$ instances and a desired training size$m$, subsamples $m$ pairs from $X \times X$ without replacement. The challengein analyzing this learning scenario is that pairwise combinations of randomvariables are inherently dependent, which prevents us from using traditionallearning-theoretic arguments. We present a unified, graph-based analysis, whichallows us to analyze this dependence using well-known graph identities. We arethen able to bound the generalization error of learned binary relations usingRademacher complexity and algorithmic stability. The rate of uniformconvergence is partially determined by the labeler's subsampling process. Wethus examine how various assumptions about subsampling affect generalization;under a natural random subsampling process, our bounds guarantee$\tilde{O}(1/\sqrt{n})$ uniform convergence.
arxiv-3000-11 | Multi-relational Learning Using Weighted Tensor Decomposition with Modular Loss | http://arxiv.org/pdf/1303.1733v2.pdf | author:Ben London, Theodoros Rekatsinas, Bert Huang, Lise Getoor category:cs.LG published:2013-03-07 summary:We propose a modular framework for multi-relational learning via tensordecomposition. In our learning setting, the training data contains multipletypes of relationships among a set of objects, which we represent by a sparsethree-mode tensor. The goal is to predict the values of the missing entries. Todo so, we model each relationship as a function of a linear combination oflatent factors. We learn this latent representation by computing a low-ranktensor decomposition, using quasi-Newton optimization of a weighted objectivefunction. Sparsity in the observed data is captured by the weighted objective,leading to improved accuracy when training data is limited. Exploiting sparsityalso improves efficiency, potentially up to an order of magnitude overunweighted approaches. In addition, our framework accommodates arbitrarycombinations of smooth, task-specific loss functions, making it better suitedfor learning different types of relations. For the typical cases of real-valuedfunctions and binary relations, we propose several loss functions and derivethe associated parameter gradients. We evaluate our method on synthetic andreal data, showing significant improvements in both accuracy and scalabilityover related factorization techniques.
arxiv-3000-12 | Adaptive low rank and sparse decomposition of video using compressive sensing | http://arxiv.org/pdf/1302.1610v2.pdf | author:Fei Yang, Hong Jiang, Zuowei Shen, Wei Deng, Dimitris Metaxas category:cs.IT cs.CV math.IT published:2013-02-06 summary:We address the problem of reconstructing and analyzing surveillance videosusing compressive sensing. We develop a new method that performs videoreconstruction by low rank and sparse decomposition adaptively. Backgroundsubtraction becomes part of the reconstruction. In our method, a backgroundmodel is used in which the background is learned adaptively as the compressivemeasurements are processed. The adaptive method has low latency, and is morerobust than previous methods. We will present experimental results todemonstrate the advantages of the proposed method.
arxiv-3000-13 | Neural Networks Built from Unreliable Components | http://arxiv.org/pdf/1301.6265v4.pdf | author:Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi, Lav Varshney category:cs.NE cs.IT math.IT published:2013-01-26 summary:Recent advances in associative memory design through strutured pattern setsand graph-based inference algorithms have allowed the reliable learning andretrieval of an exponential number of patterns. Both these and classicalassociative memories, however, have assumed internally noiseless computationalnodes. This paper considers the setting when internal computations are alsonoisy. Even if all components are noisy, the final error probability in recallcan often be made exceedingly small, as we characterize. There is a thresholdphenomenon. We also show how to optimize inference algorithm parameters whenknowing statistical properties of internal noise.
arxiv-3000-14 | A new Bayesian ensemble of trees classifier for identifying multi-class labels in satellite images | http://arxiv.org/pdf/1304.4077v2.pdf | author:Reshu Agarwal, Pritam Ranjan, Hugh Chipman category:stat.ME cs.CV cs.LG published:2013-04-15 summary:Classification of satellite images is a key component of many remote sensingapplications. One of the most important products of a raw satellite image isthe classified map which labels the image pixels into meaningful classes.Though several parametric and non-parametric classifiers have been developedthus far, accurate labeling of the pixels still remains a challenge. In thispaper, we propose a new reliable multiclass-classifier for identifying classlabels of a satellite image in remote sensing applications. The proposedmulticlass-classifier is a generalization of a binary classifier based on theflexible ensemble of regression trees model called Bayesian Additive RegressionTrees (BART). We used three small areas from the LANDSAT 5 TM image, acquiredon August 15, 2009 (path/row: 08/29, L1T product, UTM map projection) overKings County, Nova Scotia, Canada to classify the land-use. Several predictionaccuracy and uncertainty measures have been used to compare the reliability ofthe proposed classifier with the state-of-the-art classifiers in remotesensing.
arxiv-3000-15 | A Risk Comparison of Ordinary Least Squares vs Ridge Regression | http://arxiv.org/pdf/1105.0875v2.pdf | author:Paramveer S. Dhillon, Dean P. Foster, Sham M. Kakade, Lyle H. Ungar category:stat.ML published:2011-05-04 summary:We compare the risk of ridge regression to a simple variant of ordinary leastsquares, in which one simply projects the data onto a finite dimensionalsubspace (as specified by a Principal Component Analysis) and then performs anordinary (un-regularized) least squares regression in this subspace. This noteshows that the risk of this ordinary least squares method is within a constantfactor (namely 4) of the risk of ridge regression.
arxiv-3000-16 | Theoretical formulation and analysis of the deterministic dendritic cell algorithm | http://arxiv.org/pdf/1305.7476v1.pdf | author:Feng Gu, Julie Greensmith, Uwe Aickelin category:cs.NE cs.DS published:2013-05-31 summary:As one of the emerging algorithms in the field of Artificial Immune Systems(AIS), the Dendritic Cell Algorithm (DCA) has been successfully applied to anumber of challenging real-world problems. However, one criticism is the lackof a formal definition, which could result in ambiguity for understanding thealgorithm. Moreover, previous investigations have mainly focused on itsempirical aspects. Therefore, it is necessary to provide a formal definition ofthe algorithm, as well as to perform runtime analyses to revealits theoreticalaspects. In this paper, we define the deterministic version of the DCA, namedthe dDCA, using set theory and mathematical functions. Runtime analyses of thestandard algorithm and the one with additional segmentation are performed. Ouranalysis suggests that the standard dDCA has a runtime complexity of O(n2) forthe worst-case scenario, where n is the number of input data instances. Theintroduction of segmentation changes the algorithm's worst case runtimecomplexity to O(max(nN; nz)), for DC population size N with size of eachsegment z. Finally, two runtime variables of the algorithm are formulated basedon the input data, to understand its runtime behaviour as guidelines forfurther development.
arxiv-3000-17 | Wavelet feature extraction and genetic algorithm for biomarker detection in colorectal cancer data | http://arxiv.org/pdf/1305.7465v1.pdf | author:Yihui Liu, Uwe Aickelin, Jan Feyereisl, Lindy G. Durrant category:cs.NE cs.CE published:2013-05-31 summary:Biomarkers which predict patient's survival can play an important role inmedical diagnosis and treatment. How to select the significant biomarkers fromhundreds of protein markers is a key step in survival analysis. In this paper anovel method is proposed to detect the prognostic biomarkers of survival incolorectal cancer patients using wavelet analysis, genetic algorithm, and Bayesclassifier. One dimensional discrete wavelet transform (DWT) is normally usedto reduce the dimensionality of biomedical data. In this study one dimensionalcontinuous wavelet transform (CWT) was proposed to extract the features ofcolorectal cancer data. One dimensional CWT has no ability to reducedimensionality of data, but captures the missing features of DWT, and iscomplementary part of DWT. Genetic algorithm was performed on extracted waveletcoefficients to select the optimized features, using Bayes classifier to buildits fitness function. The corresponding protein markers were located based onthe position of optimized features. Kaplan-Meier curve and Cox regression modelwere used to evaluate the performance of selected biomarkers. Experiments wereconducted on colorectal cancer dataset and several significant biomarkers weredetected. A new protein biomarker CD46 was found to significantly associatewith survival time.
arxiv-3000-18 | Privileged Information for Data Clustering | http://arxiv.org/pdf/1305.7454v1.pdf | author:Jan Feyereisl, Uwe Aickelin category:cs.LG stat.ML published:2013-05-31 summary:Many machine learning algorithms assume that all input samples areindependently and identically distributed from some common distribution oneither the input space X, in the case of unsupervised learning, or the inputand output space X x Y in the case of supervised and semi-supervised learning.In the last number of years the relaxation of this assumption has been exploredand the importance of incorporation of additional information within machinelearning algorithms became more apparent. Traditionally such fusion ofinformation was the domain of semi-supervised learning. More recently theinclusion of knowledge from separate hypothetical spaces has been proposed byVapnik as part of the supervised setting. In this work we are interested inexploring Vapnik's idea of master-class learning and the associated learningusing privileged information, however within the unsupervised setting. Adoptionof the advanced supervised learning paradigm for the unsupervised settinginstigates investigation into the difference between privileged and technicaldata. By means of our proposed aRi-MAX method stability of the KMeans algorithmis improved and identification of the best clustering solution is achieved onan artificial dataset. Subsequently an information theoretic dot product basedalgorithm called P-Dot is proposed. This method has the ability to utilize awide variety of clustering techniques, individually or in combination, whilefusing privileged and technical data for improved clustering. Application ofthe P-Dot method to the task of digit recognition confirms our findings in areal-world scenario.
arxiv-3000-19 | Robustness Analysis of Hottopixx, a Linear Programming Model for Factoring Nonnegative Matrices | http://arxiv.org/pdf/1211.6687v4.pdf | author:Nicolas Gillis category:stat.ML cs.LG cs.NA math.OC published:2012-11-28 summary:Although nonnegative matrix factorization (NMF) is NP-hard in general, it hasbeen shown very recently that it is tractable under the assumption that theinput nonnegative data matrix is close to being separable (separabilityrequires that all columns of the input matrix belongs to the cone spanned by asmall subset of these columns). Since then, several algorithms have beendesigned to handle this subclass of NMF problems. In particular, Bittorf,Recht, R\'e and Tropp (`Factoring nonnegative matrices with linear programs',NIPS 2012) proposed a linear programming model, referred to as Hottopixx. Inthis paper, we provide a new and more general robustness analysis of theirmethod. In particular, we design a provably more robust variant using apost-processing strategy which allows us to deal with duplicates and nearduplicates in the dataset.
arxiv-3000-20 | Motif Detection Inspired by Immune Memory (JORS) | http://arxiv.org/pdf/1305.7434v1.pdf | author:William Wilson, Phil Birkin, Uwe Aickelin category:cs.NE published:2013-05-31 summary:The search for patterns or motifs in data represents an area of key interestto many researchers. In this paper we present the Motif Tracking Algorithm, anovel immune inspired pattern identification tool that is able to identifyvariable length unknown motifs which repeat within time series data. Thealgorithm searches from a neutral perspective that is independent of the databeing analysed and the underlying motifs. In this paper we test the flexibilityof the motif tracking algorithm by applying it to the search for patterns intwo industrial data sets. The algorithm is able to identify a population ofmeaningful motifs in both cases, and the value of these motifs is discussed.
arxiv-3000-21 | Real-world Transfer of Evolved Artificial Immune System Behaviours between Small and Large Scale Robotic Platforms | http://arxiv.org/pdf/1305.7432v1.pdf | author:Amanda Whitbrook, Uwe Aickelin, Jonathan M. Garibaldi category:cs.NE cs.RO published:2013-05-31 summary:In mobile robotics, a solid test for adaptation is the ability of a controlsystem to function not only in a diverse number of physical environments, butalso on a number of different robotic platforms. This paper demonstrates that aset of behaviours evolved in simulation on a miniature robot (epuck) can betransferred to a much larger-scale platform (Pioneer), both in simulation andin the real world. The chosen architecture uses artificial evolution of epuckbehaviours to obtain a genetic sequence, which is then employed to seed anidiotypic, artificial immune system (AIS) on the Pioneers. Despite numeroushardware and software differences between the platforms, navigation andtarget-finding experiments show that the evolved behaviours transfer very wellto the larger robot when the idiotypic AIS technique is used. In contrast,transferability is poor when reinforcement learning alone is used, whichvalidates the adaptability of the chosen architecture.
arxiv-3000-22 | The Dendritic Cell Algorithm for Intrusion Detection | http://arxiv.org/pdf/1305.7416v1.pdf | author:Feng Gu, Julie Greensmith, Uwe Aickelin category:cs.CR cs.NE published:2013-05-31 summary:As one of the solutions to intrusion detection problems, Artificial ImmuneSystems (AIS) have shown their advantages. Unlike genetic algorithms, there isno one archetypal AIS, instead there are four major paradigms. Among them, theDendritic Cell Algorithm (DCA) has produced promising results in variousapplications. The aim of this chapter is to demonstrate the potential for theDCA as a suitable candidate for intrusion detection problems. We review some ofthe commonly used AIS paradigms for intrusion detection problems anddemonstrate the advantages of one particular algorithm, the DCA. In order toclearly describe the algorithm, the background to its development and a formaldefinition are given. In addition, improvements to the original DCA arepresented and their implications are discussed, including previous work done onan online analysis component with segmentation and ongoing work on automateddata preprocessing. Based on preliminary results, both improvements appear tobe promising for online anomaly-based intrusion detection.
arxiv-3000-23 | Joint Modeling and Registration of Cell Populations in Cohorts of High-Dimensional Flow Cytometric Data | http://arxiv.org/pdf/1305.7344v1.pdf | author:Saumyadipta Pyne, Kui Wang, Jonathan Irish, Pablo Tamayo, Marc-Danie Nazaire, Tarn Duong, Sharon Lee, Shu-Kay Ng, David Hafler, Ronald Levy, Garry Nolan, Jill Mesirov, Geoffrey J. McLachlan category:stat.ML published:2013-05-31 summary:In systems biomedicine, an experimenter encounters different potentialsources of variation in data such as individual samples, multiple experimentalconditions, and multi-variable network-level responses. In multiparametriccytometry, which is often used for analyzing patient samples, such issues arecritical. While computational methods can identify cell populations inindividual samples, without the ability to automatically match them acrosssamples, it is difficult to compare and characterize the populations in typicalexperiments, such as those responding to various stimulations or distinctive ofparticular patients or time-points, especially when there are many samples.Joint Clustering and Matching (JCM) is a multi-level framework for simultaneousmodeling and registration of populations across a cohort. JCM models everypopulation with a robust multivariate probability distribution. Simultaneously,JCM fits a random-effects model to construct an overall batch template -- usedfor registering populations across samples, and classifying new samples. Bytackling systems-level variation, JCM supports practical biomedicalapplications involving large cohorts.
arxiv-3000-24 | Stochastic dynamics of lexicon learning in an uncertain and nonuniform world | http://arxiv.org/pdf/1302.5526v2.pdf | author:Rainer Reisenauer, Kenny Smith, Richard A. Blythe category:physics.soc-ph cs.CL q-bio.NC published:2013-02-22 summary:We study the time taken by a language learner to correctly identify themeaning of all words in a lexicon under conditions where many plausiblemeanings can be inferred whenever a word is uttered. We show that the mostbasic form of cross-situational learning - whereby information from multipleepisodes is combined to eliminate incorrect meanings - can perform badly whenwords are learned independently and meanings are drawn from a nonuniformdistribution. If learners further assume that no two words share a commonmeaning, we find a phase transition between a maximally-efficient learningregime, where the learning time is reduced to the shortest it can possibly be,and a partially-efficient regime where incorrect candidate meanings for wordspersist at late times. We obtain exact results for the word-learning processthrough an equivalence to a statistical mechanical problem of enumerating loopsin the space of word-meaning mappings.
arxiv-3000-25 | Robust Hyperspectral Unmixing with Correntropy based Metric | http://arxiv.org/pdf/1305.7311v1.pdf | author:Ying Wang, Chunhong Pan, Shiming Xiang, Feiyun Zhu category:cs.CV published:2013-05-31 summary:Hyperspectral unmixing is one of the crucial steps for many hyperspectralapplications. The problem of hyperspectral unmixing has proven to be adifficult task in unsupervised work settings where the endmembers andabundances are both unknown. What is more, this task becomes more challengingin the case that the spectral bands are degraded with noise. This paperpresents a robust model for unsupervised hyperspectral unmixing. Specifically,our model is developed with the correntropy based metric where the non-negativeconstraints on both endmembers and abundances are imposed to keep physicalsignificance. In addition, a sparsity prior is explicitly formulated toconstrain the distribution of the abundances of each endmember. To solve ourmodel, a half-quadratic optimization technique is developed to convert theoriginal complex optimization problem into an iteratively re-weighted NMF withsparsity constraints. As a result, the optimization of our model can adaptivelyassign small weights to noisy bands and give more emphasis on noise-free bands.In addition, with sparsity constraints, our model can naturally generate sparseabundances. Experiments on synthetic and real data demonstrate theeffectiveness of our model in comparison to the related state-of-the-artunmixing models.
arxiv-3000-26 | Non-linear dimensionality reduction: Riemannian metric estimation and the problem of geometric discovery | http://arxiv.org/pdf/1305.7255v1.pdf | author:Dominique Perraul-Joncas, Marina Meila category:stat.ML published:2013-05-30 summary:In recent years, manifold learning has become increasingly popular as a toolfor performing non-linear dimensionality reduction. This has led to thedevelopment of numerous algorithms of varying degrees of complexity that aim torecover man ifold geometry using either local or global features of the data. Building on the Laplacian Eigenmap and Diffusionmaps framework, we propose anew paradigm that offers a guarantee, under reasonable assumptions, that anymanifo ld learning algorithm will preserve the geometry of a data set. Ourapproach is based on augmenting the output of embedding algorithms withgeometric informatio n embodied in the Riemannian metric of the manifold. Weprovide an algorithm for estimating the Riemannian metric from data anddemonstrate possible application s of our approach in a variety of examples.
arxiv-3000-27 | Image restoration using sparse approximations of spatially varying blur operators in the wavelet domain | http://arxiv.org/pdf/1302.6105v2.pdf | author:Paul Escande, Pierre Weiss, Francois Malgouyres category:math.OC cs.CV math.NA published:2013-02-25 summary:Restoration of images degraded by spatially varying blurs is an issue ofincreasing importance in the context of photography, satellite or microscopyimaging. One of the main difficulty to solve this problem comes from the hugedimensions of the blur matrix. It prevents the use of naive approaches forperforming matrix-vector multiplications. In this paper, we propose toapproximate the blur operator by a matrix sparse in the wavelet domain. Wejustify this approach from a mathematical point of view and investigate theapproximation quality numerically. We finish by showing that the sparsitypattern of the matrix can be pre-defined, which is central in tasks such asblind deconvolution.
arxiv-3000-28 | Model Selection for Degree-corrected Block Models | http://arxiv.org/pdf/1207.3994v2.pdf | author:Xiaoran Yan, Cosma Rohilla Shalizi, Jacob E. Jensen, Florent Krzakala, Cristopher Moore, Lenka Zdeborova, Pan Zhang, Yaojia Zhu category:cs.SI math.ST physics.soc-ph stat.ML stat.TH published:2012-07-17 summary:The proliferation of models for networks raises challenging problems of modelselection: the data are sparse and globally dependent, and models are typicallyhigh-dimensional and have large numbers of latent variables. Together, theseissues mean that the usual model-selection criteria do not work properly fornetworks. We illustrate these challenges, and show one way to resolve them, byconsidering the key network-analysis problem of dividing a graph intocommunities or blocks of nodes with homogeneous patterns of links to the restof the network. The standard tool for doing this is the stochastic block model,under which the probability of a link between two nodes is a function solely ofthe blocks to which they belong. This imposes a homogeneous degree distributionwithin each block; this can be unrealistic, so degree-corrected block modelsadd a parameter for each node, modulating its over-all degree. The choicebetween ordinary and degree-corrected block models matters because they makevery different inferences about communities. We present the first principledand tractable approach to model selection between standard and degree-correctedblock models, based on new large-graph asymptotics for the distribution oflog-likelihood ratios under the stochastic block model, finding substantialdepartures from classical results for sparse graphs. We also developlinear-time approximations for log-likelihoods under both the stochastic blockmodel and the degree-corrected model, using belief propagation. Applications tosimulated and real networks show excellent agreement with our approximations.Our results thus both solve the practical problem of deciding on degreecorrection, and point to a general approach to model selection in networkanalysis.
arxiv-3000-29 | Lensless Imaging by Compressive Sensing | http://arxiv.org/pdf/1305.7181v1.pdf | author:Gang Huang, Hong Jiang, Kim Matthews, Paul Wilford category:cs.CV published:2013-05-30 summary:In this paper, we propose a lensless compressive imaging architecture. Thearchitecture consists of two components, an aperture assembly and a sensor. Nolens is used. The aperture assembly consists of a two dimensional array ofaperture elements. The transmittance of each aperture element is independentlycontrollable. The sensor is a single detection element. A compressive sensingmatrix is implemented by adjusting the transmittance of the individual apertureelements according to the values of the sensing matrix. The proposedarchitecture is simple and reliable because no lens is used. The architecturecan be used for capturing images of visible and other spectra such as infrared,or millimeter waves, in surveillance applications for detecting anomalies orextracting features such as speed of moving objects. Multiple sensors may beused with a single aperture assembly to capture multi-view imagessimultaneously. A prototype was built by using a LCD panel and a photoelectricsensor for capturing images of visible spectrum.
arxiv-3000-30 | Structural and Functional Discovery in Dynamic Networks with Non-negative Matrix Factorization | http://arxiv.org/pdf/1305.7169v1.pdf | author:Shawn Mankad, George Michailidis category:cs.SI physics.soc-ph stat.ML published:2013-05-30 summary:Time series of graphs are increasingly prevalent in modern data and poseunique challenges to visual exploration and pattern extraction. This paperdescribes the development and application of matrix factorizations forexploration and time-varying community detection in time-evolving graphsequences. The matrix factorization model allows the user to home in on anddisplay interesting, underlying structure and its evolution over time. Themethods are scalable to weighted networks with a large number of time points ornodes, and can accommodate sudden changes to graph topology. Our techniques aredemonstrated with several dynamic graph series from both synthetic and realworld data, including citation and trade networks. These examples illustratehow users can steer the techniques and combine them with existing methods todiscover and display meaningful patterns in sizable graphs over many timepoints.
arxiv-3000-31 | Immune System Approaches to Intrusion Detection - A Review (ICARIS) | http://arxiv.org/pdf/1305.7144v1.pdf | author:Uwe Aickelin, Julie Greensmith, Jamie Twycross category:cs.CR cs.NE published:2013-05-30 summary:The use of artificial immune systems in intrusion detection is an appealingconcept for two reasons. Firstly, the human immune system provides the humanbody with a high level of protection from invading pathogens, in a robust,self-organised and distributed manner. Secondly, current techniques used incomputer security are not able to cope with the dynamic and increasinglycomplex nature of computer systems and their security. It is hoped thatbiologically inspired approaches in this area, including the use ofimmune-based systems will be able to meet this challenge. Here we collate thealgorithms used, the development of the systems and the outcome of theirimplementation. It provides an introduction and review of the key developmentswithin this field, in addition to making suggestions for future research.
arxiv-3000-32 | Memory Implementations - Current Alternatives | http://arxiv.org/pdf/1305.7130v1.pdf | author:William Wilson, Uwe Aickelin category:cs.AI cs.NE published:2013-05-30 summary:Memory can be defined as the ability to retain and recall information in adiverse range of forms. It is a vital component of the way in which we as humanbeings operate on a day to day basis. Given a particular situation, decisionsare made and actions undertaken in response to that situation based on ourmemory of related prior events and experiences. By utilising our memory we cananticipate the outcome of our chosen actions to avoid unexpected or unwantedevents. In addition, as we subtly alter our actions and recognise alteredoutcomes we learn and create new memories, enabling us to improve theefficiency of our actions over time. However, as this process occurs sonaturally in the subconscious its importance is often overlooked.
arxiv-3000-33 | Test cost and misclassification cost trade-off using reframing | http://arxiv.org/pdf/1305.7111v1.pdf | author:Celestine Periale Ma, José Hernández-Orallo category:cs.LG published:2013-05-30 summary:Many solutions to cost-sensitive classification (and regression) rely on someor all of the following assumptions: we have complete knowledge about the costcontext at training time, we can easily re-train whenever the cost contextchanges, and we have technique-specific methods (such as cost-sensitivedecision trees) that can take advantage of that information. In this paper weaddress the problem of selecting models and minimising joint cost (integratingboth misclassification cost and test costs) without any of the aboveassumptions. We introduce methods and plots (such as the so-called JROC plots)that can work with any off-the-shelf predictive technique, including ensembles,such that we reframe the model to use the appropriate subset of attributes (thefeature configuration) during deployment time. In other words, models aretrained with the available attributes (once and for all) and then deployed bysetting missing values on the attributes that are deemed ineffective forreducing the joint cost. As the number of feature configuration combinationsgrows exponentially with the number of features we introduce quadratic methodsthat are able to approximate the optimal configuration and model choices, asshown by the experimental results.
arxiv-3000-34 | Predicting the Severity of Breast Masses with Data Mining Methods | http://arxiv.org/pdf/1305.7057v1.pdf | author:Sahar A. Mokhtar, Alaa. M. Elsayad category:cs.LG stat.ML published:2013-05-30 summary:Mammography is the most effective and available tool for breast cancerscreening. However, the low positive predictive value of breast biopsyresulting from mammogram interpretation leads to approximately 70% unnecessarybiopsies with benign outcomes. Data mining algorithms could be used to helpphysicians in their decisions to perform a breast biopsy on a suspicious lesionseen in a mammogram image or to perform a short term follow-up examinationinstead. In this research paper data mining classification algorithms; DecisionTree (DT), Artificial Neural Network (ANN), and Support Vector Machine (SVM)are analyzed on mammographic masses data set. The purpose of this study is toincrease the ability of physicians to determine the severity (benign ormalignant) of a mammographic mass lesion from BI-RADS attributes and thepatient,s age. The whole data set is divided for training the models and testthem by the ratio of 70:30% respectively and the performances of classificationalgorithms are compared through three statistical measures; sensitivity,specificity, and classification accuracy. Accuracy of DT, ANN and SVM are78.12%, 80.56% and 81.25% of test samples respectively. Our analysis shows thatout of these three classification models SVM predicts severity of breast cancerwith least error rate and highest accuracy.
arxiv-3000-35 | Dienstplanerstellung in Krankenhaeusern mittels genetischer Algorithmen | http://arxiv.org/pdf/1305.7056v1.pdf | author:Uwe Aickelin category:cs.NE published:2013-05-30 summary:This thesis investigates the use of problem-specific knowledge to enhance agenetic algorithm approach to multiple-choice optimisation problems. It showsthat such information can significantly enhance performance, but that thechoice of information and the way it is included are important factors forsuccess.
arxiv-3000-36 | A Local Active Contour Model for Image Segmentation with Intensity Inhomogeneity | http://arxiv.org/pdf/1305.7053v1.pdf | author:Kaihua Zhang, Lei Zhang, Kin-Man Lam, David Zhang category:cs.CV published:2013-05-30 summary:A novel locally statistical active contour model (ACM) for image segmentationin the presence of intensity inhomogeneity is presented in this paper. Theinhomogeneous objects are modeled as Gaussian distributions of different meansand variances, and a moving window is used to map the original image intoanother domain, where the intensity distributions of inhomogeneous objects arestill Gaussian but are better separated. The means of the Gaussiandistributions in the transformed domain can be adaptively estimated bymultiplying a bias field with the original signal within the window. Astatistical energy functional is then defined for each local region, whichcombines the bias field, the level set function, and the constant approximatingthe true signal of the corresponding object. Experiments on both synthetic andreal images demonstrate the superiority of our proposed algorithm tostate-of-the-art and representative methods.
arxiv-3000-37 | Tweets Miner for Stock Market Analysis | http://arxiv.org/pdf/1305.7014v1.pdf | author:Bohdan Pavlyshenko category:cs.IR cs.CL cs.SI published:2013-05-30 summary:In this paper, we present a software package for the data mining of Twittermicroblogs for the purpose of using them for the stock market analysis. Thepackage is written in R langauge using apropriate R packages. The model oftweets has been considered. We have also compared stock market charts withfrequent sets of keywords in Twitter microblogs messages.
arxiv-3000-38 | The Expressive Power of Word Embeddings | http://arxiv.org/pdf/1301.3226v4.pdf | author:Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, Steven Skiena category:cs.LG cs.CL stat.ML published:2013-01-15 summary:We seek to better understand the difference in quality of the severalpublicly released embeddings. We propose several tasks that help to distinguishthe characteristics of different embeddings. Our evaluation of sentimentpolarity and synonym/antonym relations shows that embeddings are able tocapture surprisingly nuanced semantics even in the absence of sentencestructure. Moreover, benchmarking the embeddings shows great variance inquality and characteristics of the semantics captured by the tested embeddings.Finally, we show the impact of varying the number of dimensions and theresolution of each dimension on the effective useful features captured by theembedding space. Our contributions highlight the importance of embeddings forNLP tasks and the effect of their quality on the final results.
arxiv-3000-39 | Video Human Segmentation using Fuzzy Object Models and its Application to Body Pose Estimation of Toddlers for Behavior Studies | http://arxiv.org/pdf/1305.6918v1.pdf | author:Thiago V. Spina, Mariano Tepper, Amy Esler, Vassilios Morellas, Nikolaos Papanikolopoulos, Alexandre X. Falcão, Guillermo Sapiro category:cs.CV published:2013-05-29 summary:Video object segmentation is a challenging problem due to the presence ofdeformable, connected, and articulated objects, intra- and inter-objectocclusions, object motion, and poor lighting. Some of these challenges call forobject models that can locate a desired object and separate it from itssurrounding background, even when both share similar colors and textures. Inthis work, we extend a fuzzy object model, named cloud system model (CSM), tohandle video segmentation, and evaluate it for body pose estimation of toddlersat risk of autism. CSM has been successfully used to model the parts of thebrain (cerebrum, left and right brain hemispheres, and cerebellum) in order toautomatically locate and separate them from each other, the connected brainstem, and the background in 3D MR-images. In our case, the objects arearticulated parts (2D projections) of the human body, which can deform, causeself-occlusions, and move along the video. The proposed CSM extension handlesarticulation by connecting the individual clouds, body parts, of the systemusing a 2D stickman model. The stickman representation naturally allows us toextract 2D body pose measures of arm asymmetry patterns during unsupported gaitof toddlers, a possible behavioral marker of autism. The results show that ourmethod can provide insightful knowledge to assist the specialist's observationsduring real in-clinic assessments.
arxiv-3000-40 | An estimation of distribution algorithm with adaptive Gibbs sampling for unconstrained global optimization | http://arxiv.org/pdf/1107.2104v2.pdf | author:Jonás Velasco, Mario A. Saucedo-Espinosa, Hugo Jair Escalante, Karlo Mendoza, César Emilio Villarreal-Rodríguez, Óscar L. Chacón-Mondragón, Adrián Rodríguez, Arturo Berrones category:cs.NE math.OC stat.ML published:2011-07-11 summary:In this paper is proposed a new heuristic approach belonging to the field ofevolutionary Estimation of Distribution Algorithms (EDAs). EDAs builds aprobability model and a set of solutions is sampled from the model whichcharacterizes the distribution of such solutions. The main framework of theproposed method is an estimation of distribution algorithm, in which anadaptive Gibbs sampling is used to generate new promising solutions and, incombination with a local search strategy, it improves the individual solutionsproduced in each iteration. The Estimation of Distribution Algorithm withAdaptive Gibbs Sampling we are proposing in this paper is called AGEDA. Weexperimentally evaluate and compare this algorithm against two deterministicprocedures and several stochastic methods in three well known test problems forunconstrained global optimization. It is empirically shown that our heuristicis robust in problems that involve three central aspects that mainly determinethe difficulty of global optimization problems, namely high-dimensionality,multi-modality and non-smoothness.
arxiv-3000-41 | Rotation invariants of two dimensional curves based on iterated integrals | http://arxiv.org/pdf/1305.6883v1.pdf | author:Joscha Diehl category:cs.CV stat.ML published:2013-05-29 summary:We introduce a novel class of rotation invariants of two dimensional curvesbased on iterated integrals. The invariants we present are in some sensecomplete and we describe an algorithm to calculate them, giving explicitcomputations up to order six. We present an application to online(stroke-trajectory based) character recognition. This seems to be the firsttime in the literature that the use of iterated integrals of a curve isproposed for (invariant) feature extraction in machine learning applications.
arxiv-3000-42 | Multi-Label Classifier Chains for Bird Sound | http://arxiv.org/pdf/1304.5862v2.pdf | author:Forrest Briggs, Xiaoli Z. Fern, Jed Irvine category:cs.LG cs.SD stat.ML published:2013-04-22 summary:Bird sound data collected with unattended microphones for automatic surveys,or mobile devices for citizen science, typically contain multiplesimultaneously vocalizing birds of different species. However, few works haveconsidered the multi-label structure in birdsong. We propose to use an ensembleof classifier chains combined with a histogram-of-segments representation formulti-label classification of birdsong. The proposed method is compared withbinary relevance and three multi-instance multi-label learning (MIML)algorithms from prior work (which focus more on structure in the sound, andless on structure in the label sets). Experiments are conducted on tworeal-world birdsong datasets, and show that the proposed method usuallyoutperforms binary relevance (using the same features and base-classifier), andis better in some cases and worse in others compared to the MIML algorithms.
arxiv-3000-43 | Unsupervised ensemble of experts (EoE) framework for automatic binarization of document images | http://arxiv.org/pdf/1305.2949v2.pdf | author:Reza Farrahi Moghaddam, Fereydoun Farrahi Moghaddam, Mohamed Cheriet category:cs.CV published:2013-05-13 summary:In recent years, a large number of binarization methods have been developed,with varying performance generalization and strength against differentbenchmarks. In this work, to leverage on these methods, an ensemble of experts(EoE) framework is introduced, to efficiently combine the outputs of variousmethods. The proposed framework offers a new selection process of thebinarization methods, which are actually the experts in the ensemble, byintroducing three concepts: confidentness, endorsement and schools of experts.The framework, which is highly objective, is built based on two generalprinciples: (i) consolidation of saturated opinions and (ii) identification ofschools of experts. After building the endorsement graph of the ensemble for aninput document image based on the confidentness of the experts, the saturatedopinions are consolidated, and then the schools of experts are identified bythresholding the consolidated endorsement graph. A variation of the framework,in which no selection is made, is also introduced that combines the outputs ofall experts using endorsement-dependent weights. The EoE framework is evaluatedon the set of participating methods in the H-DIBCO'12 contest and also on anensemble generated from various instances of grid-based Sauvola method withpromising performance.
arxiv-3000-44 | Active Sensing as Bayes-Optimal Sequential Decision Making | http://arxiv.org/pdf/1305.6650v1.pdf | author:Sheeraz Ahmad, Angela J. Yu category:cs.AI cs.CV published:2013-05-28 summary:Sensory inference under conditions of uncertainty is a major problem in bothmachine learning and computational neuroscience. An important but poorlyunderstood aspect of sensory processing is the role of active sensing. Here, wepresent a Bayes-optimal inference and control framework for active sensing,C-DAC (Context-Dependent Active Controller). Unlike previously proposedalgorithms that optimize abstract statistical objectives such as informationmaximization (Infomax) [Butko & Movellan, 2010] or one-step look-ahead accuracy[Najemnik & Geisler, 2005], our active sensing model directly minimizes acombination of behavioral costs, such as temporal delay, response error, andeffort. We simulate these algorithms on a simple visual search task toillustrate scenarios in which context-sensitivity is particularly beneficialand optimization with respect to generic statistical objectives particularlyinadequate. Motivated by the geometric properties of the C-DAC policy, wepresent both parametric and non-parametric approximations, which retaincontext-sensitivity while significantly reducing computational complexity.These approximations enable us to investigate the more complex probleminvolving peripheral vision, and we notice that the difference between C-DACand statistical policies becomes even more evident in this scenario.
arxiv-3000-45 | Normalized Online Learning | http://arxiv.org/pdf/1305.6646v1.pdf | author:Stephane Ross, Paul Mineiro, John Langford category:cs.LG stat.ML published:2013-05-28 summary:We introduce online learning algorithms which are independent of featurescales, proving regret bounds dependent on the ratio of scales existent in thedata rather than the absolute scale. This has several useful effects: there isno need to pre-normalize data, the test-time and test-space complexity arereduced, and the algorithms are more robust.
arxiv-3000-46 | Reinforcement Learning for the Soccer Dribbling Task | http://arxiv.org/pdf/1305.6568v1.pdf | author:Arthur Carvalho, Renato Oliveira category:cs.LG cs.RO stat.ML published:2013-05-28 summary:We propose a reinforcement learning solution to the \emph{soccer dribblingtask}, a scenario in which a soccer agent has to go from the beginning to theend of a region keeping possession of the ball, as an adversary attempts togain possession. While the adversary uses a stationary policy, the dribblerlearns the best action to take at each decision point. After definingmeaningful variables to represent the state space, and high-level macro-actionsto incorporate domain knowledge, we describe our application of thereinforcement learning algorithm \emph{Sarsa} with CMAC for functionapproximation. Our experiments show that, after the training period, thedribbler is able to accomplish its task against a strong adversary around 58%of the time.
arxiv-3000-47 | A Cooperative Coevolutionary Genetic Algorithm for Learning Bayesian Network Structures | http://arxiv.org/pdf/1305.6537v1.pdf | author:Arthur Carvalho category:cs.NE cs.AI published:2013-05-28 summary:We propose a cooperative coevolutionary genetic algorithm for learningBayesian network structures from fully observable data sets. Since this problemcan be decomposed into two dependent subproblems, that is to find an orderingof the nodes and an optimal connectivity matrix, our algorithm uses twosubpopulations, each one representing a subtask. We describe the empiricalresults obtained with simulations of the Alarm and Insurance networks. We showthat our algorithm outperforms the deterministic algorithm K2.
arxiv-3000-48 | Matrices of forests, analysis of networks, and ranking problems | http://arxiv.org/pdf/1305.6441v1.pdf | author:Pavel Chebotarev, Rafig Agaev category:math.CO cs.CV cs.DM cs.NI published:2013-05-28 summary:The matrices of spanning rooted forests are studied as a tool for analysingthe structure of networks and measuring their properties. The problems ofrevealing the basic bicomponents, measuring vertex proximity, and ranking frompreference relations / sports competitions are considered. It is shown that thevertex accessibility measure based on spanning forests has a number ofdesirable properties. An interpretation for the stochastic matrix ofout-forests in terms of information dissemination is given.
arxiv-3000-49 | Joint and individual variation explained (JIVE) for integrated analysis of multiple data types | http://arxiv.org/pdf/1102.4110v2.pdf | author:Eric F. Lock, Katherine A. Hoadley, J. S. Marron, Andrew B. Nobel category:stat.ML stat.AP stat.ME published:2011-02-20 summary:Research in several fields now requires the analysis of data sets in whichmultiple high-dimensional types of data are available for a common set ofobjects. In particular, The Cancer Genome Atlas (TCGA) includes data fromseveral diverse genomic technologies on the same cancerous tumor samples. Inthis paper we introduce Joint and Individual Variation Explained (JIVE), ageneral decomposition of variation for the integrated analysis of such datasets. The decomposition consists of three terms: a low-rank approximationcapturing joint variation across data types, low-rank approximations forstructured variation individual to each data type, and residual noise. JIVEquantifies the amount of joint variation between data types, reduces thedimensionality of the data and provides new directions for the visualexploration of joint and individual structures. The proposed method representsan extension of Principal Component Analysis and has clear advantages overpopular two-block methods such as Canonical Correlation Analysis and PartialLeast Squares. A JIVE analysis of gene expression and miRNA data onGlioblastoma Multiforme tumor samples reveals gene-miRNA associations andprovides better characterization of tumor types. Data and software areavailable at https://genome.unc.edu/jive/
arxiv-3000-50 | Proceedings of the 37th Annual Workshop of the Austrian Association for Pattern Recognition (ÖAGM/AAPR), 2013 | http://arxiv.org/pdf/1304.1876v3.pdf | author:Justus Piater, Antonio Rodríguez-Sánchez category:cs.CV published:2013-04-06 summary:This volume represents the proceedings of the 37th Annual Workshop of theAustrian Association for Pattern Recognition (\"OAGM/AAPR), held May 23-24,2013, in Innsbruck, Austria.
arxiv-3000-51 | Optimal rates of convergence for persistence diagrams in Topological Data Analysis | http://arxiv.org/pdf/1305.6239v1.pdf | author:Frédéric Chazal, Marc Glisse, Catherine Labruère, Bertrand Michel category:math.ST cs.CG cs.LG math.GT stat.TH published:2013-05-27 summary:Computational topology has recently known an important development towarddata analysis, giving birth to the field of topological data analysis.Topological persistence, or persistent homology, appears as a fundamental toolin this field. In this paper, we study topological persistence in generalmetric spaces, with a statistical approach. We show that the use of persistenthomology can be naturally considered in general statistical frameworks andpersistence diagrams can be used as statistics with interesting convergenceproperties. Some numerical experiments are performed in various contexts toillustrate our results.
arxiv-3000-52 | Extended Lambek calculi and first-order linear logic | http://arxiv.org/pdf/1305.6238v1.pdf | author:Richard Moot category:cs.CL cs.LO published:2013-05-27 summary:First-order multiplicative intuitionistic linear logic (MILL1) can be seen asan extension of the Lambek calculus. In addition to the fragment of MILL1 whichcorresponds to the Lambek calculus (of Moot & Piazza 2001), I will showfragments of MILL1 which generate the multiple context-free languages and whichcorrespond to the Displacement calculus of Morrilll e.a.
arxiv-3000-53 | Integrating Prior Knowledge Into Prognostic Biomarker Discovery based on Network Structure | http://arxiv.org/pdf/1212.3214v2.pdf | author:Yupeng Cun, Holger Fröhlich category:q-bio.GN stat.ML published:2012-12-13 summary:Background: Predictive, stable and interpretable gene signatures aregenerally seen as an important step towards a better personalized medicine.During the last decade various methods have been proposed for that purpose.However, one important obstacle for making gene signatures a standard tool inclinics is the typical low reproducibility of these signatures combined withthe difficulty to achieve a clear biological interpretation. For that purposein the last years there has been a growing interest in approaches that try tointegrate information from molecular interaction networks. Results: We proposea novel algorithm, called FrSVM, which integrates protein-protein interactionnetwork information into gene selection for prognostic biomarker discovery. Ourmethod is a simple filter based approach, which focuses on central genes withlarge differences in their expression. Compared to several other competingmethods our algorithm reveals a significantly better prediction performance andhigher signature stability. More- over, obtained gene lists are highly enrichedwith known disease genes and drug targets. We extendd our approach further byintegrating information on candidate disease genes and targets of diseaseassociated Transcript Factors (TFs).
arxiv-3000-54 | On some interrelations of generalized $q$-entropies and a generalized Fisher information, including a Cramér-Rao inequality | http://arxiv.org/pdf/1305.6215v1.pdf | author:Jean-François Bercher category:cs.IT cond-mat.other math.IT stat.ML published:2013-05-27 summary:In this communication, we describe some interrelations between generalized$q$-entropies and a generalized version of Fisher information. In informationtheory, the de Bruijn identity links the Fisher information and the derivativeof the entropy. We show that this identity can be extended to generalizedversions of entropy and Fisher information. More precisely, a generalizedFisher information naturally pops up in the expression of the derivative of theTsallis entropy. This generalized Fisher information also appears as a specialcase of a generalized Fisher information for estimation problems. Indeed, wederive here a new Cram\'er-Rao inequality for the estimation of a parameter,which involves a generalized form of Fisher information. This generalizedFisher information reduces to the standard Fisher information as a particularcase. In the case of a translation parameter, the general Cram\'er-Raoinequality leads to an inequality for distributions which is saturated bygeneralized $q$-Gaussian distributions. These generalized $q$-Gaussians areimportant in several areas of physics and mathematics. They are known tomaximize the $q$-entropies subject to a moment constraint. The Cram\'er-Raoinequality shows that the generalized $q$-Gaussians also minimize thegeneralized Fisher information among distributions with a fixed moment.Similarly, the generalized $q$-Gaussians also minimize the generalized Fisherinformation among distributions with a given $q$-entropy.
arxiv-3000-55 | Some results on a $χ$-divergence, an~extended~Fisher information and~generalized~Cramér-Rao inequalities | http://arxiv.org/pdf/1305.6213v1.pdf | author:Jean-François Bercher category:cs.IT math.IT stat.ML published:2013-05-27 summary:We propose a modified $\chi^{\beta}$-divergence, give some of its properties,and show that this leads to the definition of a generalized Fisher information.We give generalized Cram\'er-Rao inequalities, involving this Fisherinformation, an extension of the Fisher information matrix, and arbitrary normsand power of the estimation error. In the case of a location parameter, weobtain new characterizations of the generalized $q$-Gaussians, for instance asthe distribution with a given moment that minimizes the generalized Fisherinformation. Finally we indicate how the generalized Fisher information canlead to new uncertainty relations.
arxiv-3000-56 | Information-Theoretic Approach to Efficient Adaptive Path Planning for Mobile Robotic Environmental Sensing | http://arxiv.org/pdf/1305.6129v1.pdf | author:Kian Hsiang Low, John M. Dolan, Pradeep Khosla category:cs.LG cs.AI cs.MA cs.RO published:2013-05-27 summary:Recent research in robot exploration and mapping has focused on samplingenvironmental hotspot fields. This exploration task is formalized by Low,Dolan, and Khosla (2008) in a sequential decision-theoretic planning underuncertainty framework called MASP. The time complexity of solving MASPapproximately depends on the map resolution, which limits its use inlarge-scale, high-resolution exploration and mapping. To alleviate thiscomputational difficulty, this paper presents an information-theoretic approachto MASP (iMASP) for efficient adaptive path planning; by reformulating thecost-minimizing iMASP as a reward-maximizing problem, its time complexitybecomes independent of map resolution and is less sensitive to increasing robotteam size as demonstrated both theoretically and empirically. Using thereward-maximizing dual, we derive a novel adaptive variant of maximum entropysampling, thus improving the induced exploration policy performance. It alsoallows us to establish theoretical bounds quantifying the performance advantageof optimal adaptive over non-adaptive policies and the performance quality ofapproximately optimal vs. optimal adaptive policies. We show analytically andempirically the superior performance of iMASP-based policies for sampling thelog-Gaussian process to that of policies for the widely-used Gaussian processin mapping the hotspot field. Lastly, we provide sufficient conditions that,when met, guarantee adaptivity has no benefit under an assumed environmentmodel.
arxiv-3000-57 | Regression trees for longitudinal and multiresponse data | http://arxiv.org/pdf/1209.4690v2.pdf | author:Wei-Yin Loh, Wei Zheng category:stat.ML stat.AP stat.ME published:2012-09-21 summary:Previous algorithms for constructing regression tree models for longitudinaland multiresponse data have mostly followed the CART approach. Consequently,they inherit the same selection biases and computational difficulties as CART.We propose an alternative, based on the GUIDE approach, that treats eachlongitudinal data series as a curve and uses chi-squared tests of the residualcurve patterns to select a variable to split each node of the tree. Besidesbeing unbiased, the method is applicable to data with fixed and random timepoints and with missing values in the response or predictor variables.Simulation results comparing its mean squared prediction error with that ofMVPART are given, as well as examples comparing it with standard linear mixedeffects and generalized estimating equation models. Conditions for asymptoticconsistency of regression tree function estimates are also given.
arxiv-3000-58 | Supervised Feature Selection for Diagnosis of Coronary Artery Disease Based on Genetic Algorithm | http://arxiv.org/pdf/1305.6046v1.pdf | author:Sidahmed Mokeddem, Baghdad Atmani, Mostefa Mokaddem category:cs.LG cs.CE published:2013-05-26 summary:Feature Selection (FS) has become the focus of much research on decisionsupport systems areas for which data sets with tremendous number of variablesare analyzed. In this paper we present a new method for the diagnosis ofCoronary Artery Diseases (CAD) founded on Genetic Algorithm (GA) wrapped BayesNaive (BN) based FS. Basically, CAD dataset contains two classes defined with13 features. In GA BN algorithm, GA generates in each iteration a subset ofattributes that will be evaluated using the BN in the second step of theselection procedure. The final set of attribute contains the most relevantfeature model that increases the accuracy. The algorithm in this case produces85.50% classification accuracy in the diagnosis of CAD. Thus, the asset of theAlgorithm is then compared with the use of Support Vector Machine (SVM),MultiLayer Perceptron (MLP) and C4.5 decision tree Algorithm. The result ofclassification accuracy for those algorithms are respectively 83.5%, 83.16% and80.85%. Consequently, the GA wrapped BN Algorithm is correspondingly comparedwith other FS algorithms. The Obtained results have shown very promisingoutcomes for the diagnosis of CAD.
arxiv-3000-59 | Efficient Active Learning of Halfspaces: an Aggressive Approach | http://arxiv.org/pdf/1208.3561v3.pdf | author:Alon Gonen, Sivan Sabato, Shai Shalev-Shwartz category:cs.LG published:2012-08-17 summary:We study pool-based active learning of half-spaces. We revisit the aggressiveapproach for active learning in the realizable case, and show that it can bemade efficient and practical, while also having theoretical guarantees underreasonable assumptions. We further show, both theoretically and experimentally,that it can be preferable to mellow approaches. Our efficient aggressive activelearner of half-spaces has formal approximation guarantees that hold when thepool is separable with a margin. While our analysis is focused on therealizable setting, we show that a simple heuristic allows using the samealgorithm successfully for pools with low error as well. We further compare theaggressive approach to the mellow approach, and prove that there are cases inwhich the aggressive approach results in significantly better label complexitycompared to the mellow approach. We demonstrate experimentally that substantialimprovements in label complexity can be achieved using the aggressive approach,for both realizable and low-error settings.
arxiv-3000-60 | Reduce Meaningless Words for Joint Chinese Word Segmentation and Part-of-speech Tagging | http://arxiv.org/pdf/1305.5918v1.pdf | author:Kaixu Zhang, Maosong Sun category:cs.CL published:2013-05-25 summary:Conventional statistics-based methods for joint Chinese word segmentation andpart-of-speech tagging (S&T) have generalization ability to recognize new wordsthat do not appear in the training data. An undesirable side effect is that anumber of meaningless words will be incorrectly created. We propose aneffective and efficient framework for S&T that introduces features tosignificantly reduce meaningless words generation. A general lexicon, Wikepediaand a large-scale raw corpus of 200 billion characters are used to generateword-based features for the wordhood. The word-lattice based framework consistsof a character-based model and a word-based model in order to employ ourword-based features. Experiments on Penn Chinese treebank 5 show that thismethod has a 62.9% reduction of meaningless word generation in comparison withthe baseline. As a result, the F1 measure for segmentation is increased to0.984.
arxiv-3000-61 | ÖAGM/AAPR 2013 - The 37th Annual Workshop of the Austrian Association for Pattern Recognition | http://arxiv.org/pdf/1305.5905v1.pdf | author:Justus Piater, Antonio J. Rodríguez Sánchez category:cs.CV published:2013-05-25 summary:In this editorial, the organizers summarize facts and background about theevent.
arxiv-3000-62 | A Symmetric Rank-one Quasi Newton Method for Non-negative Matrix Factorization | http://arxiv.org/pdf/1305.5829v1.pdf | author:Shu-Zhen Lai, Hou-Biao Li, Zu-Tao Zhang category:math.NA cs.LG cs.NA 15A18 published:2013-05-24 summary:As we all known, the nonnegative matrix factorization (NMF) is a dimensionreduction method that has been widely used in image processing, textcompressing and signal processing etc. In this paper, an algorithm fornonnegative matrix approximation is proposed. This method mainly bases on theactive set and the quasi-Newton type algorithm, by using the symmetric rank-oneand negative curvature direction technologies to approximate the Hessianmatrix. Our method improves the recent results of those methods in [PatternRecognition, 45(2012)3557-3565; SIAM J. Sci. Comput., 33(6)(2011)3261-3281;Neural Computation, 19(10)(2007)2756-2779, etc.]. Moreover, the object functiondecreases faster than many other NMF methods. In addition, some numericalexperiments are presented in the synthetic data, imaging processing and textclustering. By comparing with the other six nonnegative matrix approximationmethods, our experiments confirm to our analysis.
arxiv-3000-63 | Parallel Gaussian Process Regression with Low-Rank Covariance Matrix Approximations | http://arxiv.org/pdf/1305.5826v1.pdf | author:Jie Chen, Nannan Cao, Kian Hsiang Low, Ruofei Ouyang, Colin Keng-Yan Tan, Patrick Jaillet category:stat.ML cs.DC cs.LG published:2013-05-24 summary:Gaussian processes (GP) are Bayesian non-parametric models that are widelyused for probabilistic regression. Unfortunately, it cannot scale well withlarge data nor perform real-time predictions due to its cubic time cost in thedata size. This paper presents two parallel GP regression methods that exploitlow-rank covariance matrix approximations for distributing the computationalload among parallel machines to achieve time efficiency and scalability. Wetheoretically guarantee the predictive performances of our proposed parallelGPs to be equivalent to that of some centralized approximate GP regressionmethods: The computation of their centralized counterparts can be distributedamong parallel machines, hence achieving greater time efficiency andscalability. We analytically compare the properties of our parallel GPs such astime, space, and communication complexity. Empirical evaluation on tworeal-world datasets in a cluster of 20 computing nodes shows that our parallelGPs are significantly more time-efficient and scalable than their centralizedcounterparts and exact/full GP while achieving predictive performancescomparable to full GP.
arxiv-3000-64 | Learning Topic Models and Latent Bayesian Networks Under Expansion Constraints | http://arxiv.org/pdf/1209.5350v3.pdf | author:Animashree Anandkumar, Daniel Hsu, Adel Javanmard, Sham M. Kakade category:stat.ML cs.LG stat.AP published:2012-09-24 summary:Unsupervised estimation of latent variable models is a fundamental problemcentral to numerous applications of machine learning and statistics. This workpresents a principled approach for estimating broad classes of such models,including probabilistic topic models and latent linear Bayesian networks, usingonly second-order observed moments. The sufficient conditions foridentifiability of these models are primarily based on weak expansionconstraints on the topic-word matrix, for topic models, and on the directedacyclic graph, for Bayesian networks. Because no assumptions are made on thedistribution among the latent variables, the approach can handle arbitrarycorrelations among the topics or latent factors. In addition, a tractablelearning method via $\ell_1$ optimization is proposed and studied in numericalexperiments.
arxiv-3000-65 | An Inventory of Preposition Relations | http://arxiv.org/pdf/1305.5785v1.pdf | author:Vivek Srikumar, Dan Roth category:cs.CL published:2013-05-24 summary:We describe an inventory of semantic relations that are expressed byprepositions. We define these relations by building on the word sensedisambiguation task for prepositions and propose a mapping from prepositionsenses to the relation labels by collapsing semantically related senses acrossprepositions.
arxiv-3000-66 | Adapting the Stochastic Block Model to Edge-Weighted Networks | http://arxiv.org/pdf/1305.5782v1.pdf | author:Christopher Aicher, Abigail Z. Jacobs, Aaron Clauset category:stat.ML cs.LG cs.SI published:2013-05-24 summary:We generalize the stochastic block model to the important case in which edgesare annotated with weights drawn from an exponential family distribution. Thisgeneralization introduces several technical difficulties for model estimation,which we solve using a Bayesian approach. We introduce a variational algorithmthat efficiently approximates the model's posterior distribution for densegraphs. In specific numerical experiments on edge-weighted networks, thisweighted stochastic block model outperforms the common approach of firstapplying a single threshold to all weights and then applying the classicstochastic block model, which can obscure latent block structure in networks.This model will enable the recovery of latent structure in a broader range ofnetwork data than was previously possible.
arxiv-3000-67 | Flooding edge or node weighted graphs | http://arxiv.org/pdf/1305.5756v1.pdf | author:Fernand Meyer category:cs.CV published:2013-05-24 summary:Reconstruction closings have all properties of a physical flooding of atopographic surface. They are precious for simplifying gradient images or,filling unwanted catchment basins, on which a subsequent watershed transformextracts the targeted objects. Flooding a topographic surface may be modeled asflooding a node weighted graph (TG), with unweighted edges, the node weightsrepresenting the ground level. The progression of a flooding may also bemodeled on the region adjacency graph (RAG) of a topographic surface. On a RAGeach node represents a catchment basin and edges connect neighboring nodes. Theedges are weighted by the altitude of the pass point between both adjacentregions. The graph is flooded from sources placed at the marker positions andeach node is assigned to the source by which it has been flooded. The level ofthe flood is represented on the nodes on each type of graphs. The same floodingmay thus be modeled on a TG or on a RAG. We characterize all valid floodings onboth types of graphs, as they should verify the laws of hydrostatics. We thenshow that each flooding of a node weighted graph also is a flooding of an edgeweighted graph with appropriate edge weights. The highest flooding under aceiling function may be interpreted as the shortest distance to the root forthe ultrametric flooding distance in an augmented graph. The ultrametricdistance between two nodes is the minimal altitude of a flooding for which bothnodes are flooded. This remark permits to flood edge or node weighted graphs byusing shortest path algorithms. It appears that the collection of all lakes ofa RAG has the structure of a dendrogram, on which the highest flooding under aceiling function may be rapidly found.
arxiv-3000-68 | Characterizing A Database of Sequential Behaviors with Latent Dirichlet Hidden Markov Models | http://arxiv.org/pdf/1305.5734v1.pdf | author:Yin Song, Longbing Cao, Xuhui Fan, Wei Cao, Jian Zhang category:stat.ML cs.LG H.2.8; F.1.2 published:2013-05-24 summary:This paper proposes a generative model, the latent Dirichlet hidden Markovmodels (LDHMM), for characterizing a database of sequential behaviors(sequences). LDHMMs posit that each sequence is generated by an underlyingMarkov chain process, which are controlled by the corresponding parameters(i.e., the initial state vector, transition matrix and the emission matrix).These sequence-level latent parameters for each sequence are modeled as latentDirichlet random variables and parameterized by a set of deterministicdatabase-level hyper-parameters. Through this way, we expect to model thesequence in two levels: the database level by deterministic hyper-parametersand the sequence-level by latent parameters. To learn the deterministichyper-parameters and approximate posteriors of parameters in LDHMMs, we proposean iterative algorithm under the variational EM framework, which consists of Eand M steps. We examine two different schemes, the fully-factorized andpartially-factorized forms, for the framework, based on different assumptions.We present empirical results of behavior modeling and sequence classificationon three real-world data sets, and compare them to other related models. Theexperimental results prove that the proposed LDHMMs produce bettergeneralization performance in terms of log-likelihood and deliver competitiveresults on the sequence classification problem.
arxiv-3000-69 | Edge Detection in Radar Images Using Weibull Distribution | http://arxiv.org/pdf/1305.5728v1.pdf | author:Ali El-Zaart, Wafaa Kamel Al-Jibory category:cs.CV published:2013-05-24 summary:Radar images can reveal information about the shape of the surface terrain aswell as its physical and biophysical properties. Radar images have long beenused in geological studies to map structural features that are revealed by theshape of the landscape. Radar imagery also has applications in vegetation andcrop type mapping, landscape ecology, hydrology, and volcanology. Imageprocessing is using for detecting for objects in radar images. Edge detection;which is a method of determining the discontinuities in gray level images; is avery important initial step in Image processing. Many classical edge detectorshave been developed over time. Some of the well-known edge detection operatorsbased on the first derivative of the image are Roberts, Prewitt, Sobel which istraditionally implemented by convolving the image with masks. Also Gaussiandistribution has been used to build masks for the first and second derivative.However, this distribution has limit to only symmetric shape. This paper willuse to construct the masks, the Weibull distribution which was more generalthan Gaussian because it has symmetric and asymmetric shape. The constructedmasks are applied to images and we obtained good results.
arxiv-3000-70 | Density-sensitive semisupervised inference | http://arxiv.org/pdf/1204.1685v2.pdf | author:Martin Azizyan, Aarti Singh, Larry Wasserman category:math.ST cs.LG stat.ML stat.TH published:2012-04-07 summary:Semisupervised methods are techniques for using labeled data$(X_1,Y_1),\ldots,(X_n,Y_n)$ together with unlabeled data $X_{n+1},\ldots,X_N$to make predictions. These methods invoke some assumptions that link themarginal distribution $P_X$ of X to the regression function f(x). For example,it is common to assume that f is very smooth over high density regions of$P_X$. Many of the methods are ad-hoc and have been shown to work in specificexamples but are lacking a theoretical foundation. We provide a minimaxframework for analyzing semisupervised methods. In particular, we study methodsbased on metrics that are sensitive to the distribution $P_X$. Our modelincludes a parameter $\alpha$ that controls the strength of the semisupervisedassumption. We then use the data to adapt to $\alpha$.
arxiv-3000-71 | The multi-armed bandit problem with covariates | http://arxiv.org/pdf/1110.6084v3.pdf | author:Vianney Perchet, Philippe Rigollet category:math.ST cs.LG stat.ML stat.TH published:2011-10-27 summary:We consider a multi-armed bandit problem in a setting where each arm producesa noisy reward realization which depends on an observable random covariate. Asopposed to the traditional static multi-armed bandit problem, this settingallows for dynamically changing rewards that better describe applications whereside information is available. We adopt a nonparametric model where theexpected rewards are smooth functions of the covariate and where the hardnessof the problem is captured by a margin parameter. To maximize the expectedcumulative reward, we introduce a policy called Adaptively Binned SuccessiveElimination (abse) that adaptively decomposes the global problem into suitably"localized" static bandit problems. This policy constructs an adaptivepartition using a variant of the Successive Elimination (se) policy. Ourresults include sharper regret bounds for the se policy in a static banditproblem and minimax optimal regret bounds for the abse policy in the dynamicproblem.
arxiv-3000-72 | Applications of Clifford's Geometric Algebra | http://arxiv.org/pdf/1305.5663v1.pdf | author:Eckhard Hitzer, Tohru Nitta, Yasuaki Kuroe category:math.RA cs.CV published:2013-05-24 summary:We survey the development of Clifford's geometric algebra and some of itsengineering applications during the last 15 years. Several recently developedapplications and their merits are discussed in some detail. We thus hope toclearly demonstrate the benefit of developing problem solutions in a unifiedframework for algebra and geometry with the widest possible scope: from quantumcomputing and electromagnetism to satellite navigation, from neural computingto camera geometry, image processing, robotics and beyond.
arxiv-3000-73 | ML4PG in Computer Algebra verification | http://arxiv.org/pdf/1302.6421v3.pdf | author:Jónathan Heras, Ekaterina Komendantskaya category:cs.LO cs.LG published:2013-02-26 summary:ML4PG is a machine-learning extension that provides statistical proof hintsduring the process of Coq/SSReflect proof development. In this paper, we useML4PG to find proof patterns in the CoqEAL library -- a library that wasdevised to verify the correctness of Computer Algebra algorithms. Inparticular, we use ML4PG to help us in the formalisation of an efficientalgorithm to compute the inverse of triangular matrices.
arxiv-3000-74 | Verifying a platform for digital imaging: a multi-tool strategy | http://arxiv.org/pdf/1303.1420v2.pdf | author:Jónathan Heras, Gadea Mata, Ana Romero, Julio Rubio, Rubén Sáenz category:cs.SE cs.CV published:2013-03-05 summary:Fiji is a Java platform widely used by biologists and other experimentalscientists to process digital images. In particular, in our research - madetogether with a biologists team; we use Fiji in some pre-processing stepsbefore undertaking a homological digital processing of images. In a previouswork, we have formalised the correctness of the programs which use homologicaltechniques to analyse digital images. However, the verification of Fiji'spre-processing step was missed. In this paper, we present a multi-tool approachfilling this gap, based on the combination of Why/Krakatoa, Coq and ACL2.
arxiv-3000-75 | Learning from Complex Systems: On the Roles of Entropy and Fisher Information in Pairwise Isotropic Gaussian Markov Random Fields | http://arxiv.org/pdf/1108.4973v12.pdf | author:Alexandre L. M. Levada category:cs.IT cs.AI cs.CV math.IT stat.CO published:2011-08-25 summary:Markov Random Field models are powerful tools for the study of complexsystems. However, little is known about how the interactions between theelements of such systems are encoded, especially from an information-theoreticperspective. In this paper, our goal is to enlight the connection betweenFisher information, Shannon entropy, information geometry and the behavior ofcomplex systems modeled by isotropic pairwise Gaussian Markov random fields. Wepropose analytical expressions to compute local and global versions of thesemeasures using Besag's pseudo-likelihood function, characterizing the system'sbehavior through its \emph{Fisher curve}, a parametric trajectory accross theinformation space that provides a geometric representation for the study ofcomplex systems. Computational experiments show how the proposed tools can beuseful in extrating relevant information from complex patterns. The obtainedresults quantify and support our main conclusion, which is: in terms ofinformation, moving towards higher entropy states (A --> B) is different frommoving towards lower entropy states (B --> A), since the \emph{Fisher curves}are not the same given a natural orientation (the direction of time).
arxiv-3000-76 | Cluster Forests | http://arxiv.org/pdf/1104.2930v3.pdf | author:Donghui Yan, Aiyou Chen, Michael I. Jordan category:stat.ME cs.LG stat.ML published:2011-04-14 summary:With inspiration from Random Forests (RF) in the context of classification, anew clustering ensemble method---Cluster Forests (CF) is proposed.Geometrically, CF randomly probes a high-dimensional data cloud to obtain "goodlocal clusterings" and then aggregates via spectral clustering to obtaincluster assignments for the whole dataset. The search for good localclusterings is guided by a cluster quality measure kappa. CF progressivelyimproves each local clustering in a fashion that resembles the tree growth inRF. Empirical studies on several real-world datasets under two differentperformance metrics show that CF compares favorably to its competitors.Theoretical analysis reveals that the kappa measure makes it possible to growthe local clustering in a desirable way---it is "noise-resistant". Aclosed-form expression is obtained for the mis-clustering rate of spectralclustering under a perturbation model, which yields new insights into someaspects of spectral clustering.
arxiv-3000-77 | A Primal Condition for Approachability with Partial Monitoring | http://arxiv.org/pdf/1305.5399v1.pdf | author:Shie Mannor, Vianney Perchet, Gilles Stoltz category:math.OC cs.GT cs.LG stat.ML published:2013-05-23 summary:In approachability with full monitoring there are two types of conditionsthat are known to be equivalent for convex sets: a primal and a dual condition.The primal one is of the form: a set C is approachable if and only allcontaining half-spaces are approachable in the one-shot game; while the dualone is of the form: a convex set C is approachable if and only if it intersectsall payoff sets of a certain form. We consider approachability in games withpartial monitoring. In previous works (Perchet 2011; Mannor et al. 2011) weprovided a dual characterization of approachable convex sets; we also exhibitedefficient strategies in the case where C is a polytope. In this paper weprovide primal conditions on a convex set to be approachable with partialmonitoring. They depend on a modified reward function and lead toapproachability strategies, based on modified payoff functions, that proceed byprojections similarly to Blackwell's (1956) strategy; this is in contrast withpreviously studied strategies in this context that relied mostly on thesignaling structure and aimed at estimating well the distributions of thesignals received. Our results generalize classical results by Kohlberg 1975(see also Mertens et al. 1994) and apply to games with arbitrary signalingstructure as well as to arbitrary convex sets.
arxiv-3000-78 | Constant conditional entropy and related hypotheses | http://arxiv.org/pdf/1304.7359v2.pdf | author:Ramon Ferrer-i-Cancho, Łukasz Dębowski, Fermín Moscoso del Prado Martín category:cs.CL cs.IT math.IT published:2013-04-27 summary:Constant entropy rate (conditional entropies must remain constant as thesequence length increases) and uniform information density (conditionalprobabilities must remain constant as the sequence length increases) are twoinformation theoretic principles that are argued to underlie a wide range oflinguistic phenomena. Here we revise the predictions of these principles to thelight of Hilberg's law on the scaling of conditional entropy in language andrelated laws. We show that constant entropy rate (CER) and two interpretationsfor uniform information density (UID), full UID and strong UID, areinconsistent with these laws. Strong UID implies CER but the reverse is nottrue. Full UID, a particular case of UID, leads to costly uncorrelatedsequences that are totally unrealistic. We conclude that CER and its particularcases are incomplete hypotheses about the scaling of conditional entropies.
arxiv-3000-79 | A Supervised Neural Autoregressive Topic Model for Simultaneous Image Classification and Annotation | http://arxiv.org/pdf/1305.5306v1.pdf | author:Yin Zheng, Yu-Jin Zhang, Hugo Larochelle category:cs.CV cs.LG stat.ML published:2013-05-23 summary:Topic modeling based on latent Dirichlet allocation (LDA) has been aframework of choice to perform scene recognition and annotation. Recently, anew type of topic model called the Document Neural Autoregressive DistributionEstimator (DocNADE) was proposed and demonstrated state-of-the-art performancefor document modeling. In this work, we show how to successfully apply andextend this model to the context of visual scene modeling. Specifically, wepropose SupDocNADE, a supervised extension of DocNADE, that increases thediscriminative power of the hidden topic features by incorporating labelinformation into the training objective of the model. We also describe how toleverage information about the spatial position of the visual words and how toembed additional image annotations, so as to simultaneously perform imageclassification and annotation. We test our model on the Scene15, LabelMe andUIUC-Sports datasets and show that it compares favorably to other topic modelssuch as the supervised variant of LDA.
arxiv-3000-80 | A novel automatic thresholding segmentation method with local adaptive thresholds | http://arxiv.org/pdf/1305.5160v1.pdf | author:Bo Xiao, Yuefeng Jing, Yonghong Guan category:cs.CV published:2013-05-22 summary:A novel method for segmenting bright objects from dark background forgrayscale image is proposed. The concept of this method can be stated simplyas: to pick out the local-thinnest bands on the grayscale grade-map. It turnsout to be a threshold-based method with local adaptive thresholds, where eachlocal threshold is determined by requiring the average normal-directiongradient on the object boundary to be local minimal. The method is highlyautomatic and the segmentation mimics a man's natural expectation even theobject boundaries are fuzzy.
arxiv-3000-81 | Sharp analysis of low-rank kernel matrix approximations | http://arxiv.org/pdf/1208.2015v3.pdf | author:Francis Bach category:cs.LG math.ST stat.TH published:2012-08-09 summary:We consider supervised learning problems within the positive-definite kernelframework, such as kernel ridge regression, kernel logistic regression or thesupport vector machine. With kernels leading to infinite-dimensional featurespaces, a common practical limiting difficulty is the necessity of computingthe kernel matrix, which most frequently leads to algorithms with running timeat least quadratic in the number of observations n, i.e., O(n^2). Low-rankapproximations of the kernel matrix are often considered as they allow thereduction of running time complexities to O(p^2 n), where p is the rank of theapproximation. The practicality of such methods thus depends on the requiredrank p. In this paper, we show that in the context of kernel ridge regression,for approximations based on a random subset of columns of the original kernelmatrix, the rank p may be chosen to be linear in the degrees of freedomassociated with the problem, a quantity which is classically used in thestatistical analysis of such methods, and is often seen as the implicit numberof parameters of non-parametric estimators. This result enables simplealgorithms that have sub-quadratic running time complexity, but provablyexhibit the same predictive performance than existing algorithms, for any givenproblem instance, and not only for worst-case situations.
arxiv-3000-82 | A Comparison of Random Forests and Ferns on Recognition of Instruments in Jazz Recordings | http://arxiv.org/pdf/1305.5078v1.pdf | author:Alicja A. Wieczorkowska, Miron B. Kursa category:cs.LG cs.IR cs.SD published:2013-05-22 summary:In this paper, we first apply random ferns for classification of real musicrecordings of a jazz band. No initial segmentation of audio data is assumed,i.e., no onset, offset, nor pitch data are needed. The notion of random fernsis described in the paper, to familiarize the reader with this classificationalgorithm, which was introduced quite recently and applied so far in imagerecognition tasks. The performance of random ferns is compared with randomforests for the same data. The results of experiments are presented in thepaper, and conclusions are drawn.
arxiv-3000-83 | Evolution of Covariance Functions for Gaussian Process Regression using Genetic Programming | http://arxiv.org/pdf/1305.3794v2.pdf | author:Gabriel Kronberger, Michael Kommenda category:cs.NE cs.LG stat.ML published:2013-05-16 summary:In this contribution we describe an approach to evolve composite covariancefunctions for Gaussian processes using genetic programming. A critical aspectof Gaussian processes and similar kernel-based models such as SVM is, that thecovariance function should be adapted to the modeled data. Frequently, thesquared exponential covariance function is used as a default. However, this canlead to a misspecified model, which does not fit the data well. In the proposedapproach we use a grammar for the composition of covariance functions andgenetic programming to search over the space of sentences that can be derivedfrom the grammar. We tested the proposed approach on synthetic data fromtwo-dimensional test functions, and on the Mauna Loa CO2 time series. Theresults show, that our approach is feasible, finding covariance functions thatperform much better than a default covariance function. For the CO2 data set acomposite covariance function is found, that matches the performance of ahand-tuned covariance function.
arxiv-3000-84 | PAWL-Forced Simulated Tempering | http://arxiv.org/pdf/1305.5017v1.pdf | author:Luke Bornn category:stat.CO stat.ML published:2013-05-22 summary:In this short note, we show how the parallel adaptive Wang-Landau (PAWL)algorithm of Bornn et al. (2013) can be used to automate and improve simulatedtempering algorithms. While Wang-Landau and other stochastic approximationmethods have frequently been applied within the simulated tempering framework,this note demonstrates through a simple example the additional improvementsbrought about by parallelization, adaptive proposals and automated binsplitting.
arxiv-3000-85 | Improving NSGA-II with an Adaptive Mutation Operator | http://arxiv.org/pdf/1305.4947v1.pdf | author:Arthur Carvalho, Aluizio F. R. Araujo category:cs.NE published:2013-05-21 summary:The performance of a Multiobjective Evolutionary Algorithm (MOEA) iscrucially dependent on the parameter setting of the operators. The most desiredcontrol of such parameters presents the characteristic of adaptiveness, i.e.,the capacity of changing the value of the parameter, in distinct stages of theevolutionary process, using feedbacks from the search for determining thedirection and/or magnitude of changing. Given the great popularity of thealgorithm NSGA-II, the objective of this research is to create adaptivecontrols for each parameter existing in this MOEA. With these controls, weexpect to improve even more the performance of the algorithm. In this work, we propose an adaptive mutation operator that has an adaptivecontrol which uses information about the diversity of candidate solutions forcontrolling the magnitude of the mutation. A number of experiments consideringdifferent problems suggest that this mutation operator improves the ability ofthe NSGA-II for reaching the Pareto optimal Front and for getting a betterdiversity among the final solutions.
arxiv-3000-86 | Out-of-sample Extension for Latent Position Graphs | http://arxiv.org/pdf/1305.4893v1.pdf | author:Minh Tang, Youngser Park, Carey E. Priebe category:stat.ML published:2013-05-21 summary:We consider the problem of vertex classification for graphs constructed fromthe latent position model. It was shown previously that the approach ofembedding the graphs into some Euclidean space followed by classification inthat space can yields a universally consistent vertex classifier. However, amajor technical difficulty of the approach arises when classifying unlabeledout-of-sample vertices without including them in the embedding stage. In thispaper, we studied the out-of-sample extension for the graph embedding step andits impact on the subsequent inference tasks. We show that, under the latentposition graph model and for sufficiently large $n$, the mapping of theout-of-sample vertices is close to its true latent position. We thendemonstrate that successful inference for the out-of-sample vertices ispossible.
arxiv-3000-87 | Optical Flow on Evolving Surfaces with an Application to the Analysis of 4D Microscopy Data | http://arxiv.org/pdf/1301.1576v2.pdf | author:Clemens Kirisits, Lukas F. Lang, Otmar Scherzer category:math.OC cs.CV published:2013-01-08 summary:We extend the concept of optical flow to a dynamic non-Euclidean setting.Optical flow is traditionally computed from a sequence of flat images. It isthe purpose of this paper to introduce variational motion estimation for imagesthat are defined on an evolving surface. Volumetric microscopy images depictinga live zebrafish embryo serve as both biological motivation and test data.
arxiv-3000-88 | Power to the Points: Validating Data Memberships in Clusterings | http://arxiv.org/pdf/1305.4757v1.pdf | author:Parasaran Raman, Suresh Venkatasubramanian category:cs.LG cs.CG published:2013-05-21 summary:A clustering is an implicit assignment of labels of points, based onproximity to other points. It is these labels that are then used for downstreamanalysis (either focusing on individual clusters, or identifyingrepresentatives of clusters and so on). Thus, in order to trust a clustering asa first step in exploratory data analysis, we must trust the labels assigned toindividual data. Without supervision, how can we validate this assignment? Inthis paper, we present a method to attach affinity scores to the implicitlabels of individual points in a clustering. The affinity scores capture theconfidence level of the cluster that claims to "own" the point. This method isvery general: it can be used with clusterings derived from Euclidean data,kernelized data, or even data derived from information spaces. It smoothlyincorporates importance functions on clusters, allowing us to eight differentclusters differently. It is also efficient: assigning an affinity score to apoint depends only polynomially on the number of clusters and is independent ofthe number of points in the data. The dimensionality of the underlying spaceonly appears in preprocessing. We demonstrate the value of our approach with anexperimental study that illustrates the use of these scores in different dataanalysis tasks, as well as the efficiency and flexibility of the method. Wealso demonstrate useful visualizations of these scores; these might proveuseful within an interactive analytics framework.
arxiv-3000-89 | On the Complexity Analysis of Randomized Block-Coordinate Descent Methods | http://arxiv.org/pdf/1305.4723v1.pdf | author:Zhaosong Lu, Lin Xiao category:math.OC cs.LG cs.NA math.NA stat.ML published:2013-05-21 summary:In this paper we analyze the randomized block-coordinate descent (RBCD)methods proposed in [8,11] for minimizing the sum of a smooth convex functionand a block-separable convex function. In particular, we extend Nesterov'stechnique developed in [8] for analyzing the RBCD method for minimizing asmooth convex function over a block-separable closed convex set to theaforementioned more general problem and obtain a sharper expected-value type ofconvergence rate than the one implied in [11]. Also, we obtain a betterhigh-probability type of iteration complexity, which improves upon the one in[11] by at least the amount $O(n/\epsilon)$, where $\epsilon$ is the targetsolution accuracy and $n$ is the number of problem blocks. In addition, forunconstrained smooth convex minimization, we develop a new technique called{\it randomized estimate sequence} to analyze the accelerated RBCD methodproposed by Nesterov [11] and establish a sharper expected-value type ofconvergence rate than the one given in [11].
arxiv-3000-90 | Nonparametric Unsupervised Classification | http://arxiv.org/pdf/1210.0645v5.pdf | author:Yingzhen Yang, Thomas S. Huang category:cs.LG stat.ML published:2012-10-02 summary:Unsupervised classification methods learn a discriminative classifier fromunlabeled data, which has been proven to be an effective way of simultaneouslyclustering the data and training a classifier from the data. Variousunsupervised classification methods obtain appealing results by the classifierslearned in an unsupervised manner. However, existing methods do not considerthe misclassification error of the unsupervised classifiers except unsupervisedSVM, so the performance of the unsupervised classifiers is not fully evaluated.In this work, we study the misclassification error of two popular classifiers,i.e. the nearest neighbor classifier (NN) and the plug-in classifier, in thesetting of unsupervised classification.
arxiv-3000-91 | Adaptive Crowdsourcing Algorithms for the Bandit Survey Problem | http://arxiv.org/pdf/1302.3268v2.pdf | author:Ittai Abraham, Omar Alonso, Vasilis Kandylas, Aleksandrs Slivkins category:cs.LG published:2013-02-13 summary:Very recently crowdsourcing has become the de facto platform for distributingand collecting human computation for a wide range of tasks and applicationssuch as information retrieval, natural language processing and machinelearning. Current crowdsourcing platforms have some limitations in the area ofquality control. Most of the effort to ensure good quality has to be done bythe experimenter who has to manage the number of workers needed to reach goodresults. We propose a simple model for adaptive quality control in crowdsourcedmultiple-choice tasks which we call the \emph{bandit survey problem}. Thismodel is related to, but technically different from the well-known multi-armedbandit problem. We present several algorithms for this problem, and supportthem with analysis and simulations. Our approach is based in our experienceconducting relevance evaluation for a large commercial search engine.
arxiv-3000-92 | Efficient Image Retargeting for High Dynamic Range Scenes | http://arxiv.org/pdf/1305.4544v1.pdf | author:Govind Salvi, Puneet Sharma, Shanmuganathan Raman category:cs.CV published:2013-05-20 summary:Most of the real world scenes have a very high dynamic range (HDR). Themobile phone cameras and the digital cameras available in markets are limitedin their capability in both the range and spatial resolution. Same argument canbe posed about the limited dynamic range display devices which also differ inthe spatial resolution and aspect ratios. In this paper, we address the problem of displaying the high contrast lowdynamic range (LDR) image of a HDR scene in a display device which hasdifferent spatial resolution compared to that of the capturing digital camera.The optimal solution proposed in this work can be employed with any camerawhich has the ability to shoot multiple differently exposed images of a scene.Further, the proposed solutions provide the flexibility in the depiction ofentire contrast of the HDR scene as a LDR image with an user specified spatialresolution. This task is achieved through an optimized content awareretargeting framework which preserves salient features along with the algorithmto combine multi-exposure images. We show the proposed approach performsexceedingly well in the generation of high contrast LDR image of varyingspatial resolution compared to an alternate approach.
arxiv-3000-93 | Infinite Shift-invariant Grouped Multi-task Learning for Gaussian Processes | http://arxiv.org/pdf/1203.0970v2.pdf | author:Yuyang Wang, Roni Khardon, Pavlos Protopapas category:cs.LG astro-ph.IM stat.ML published:2012-03-05 summary:Multi-task learning leverages shared information among data sets to improvethe learning performance of individual tasks. The paper applies this frameworkfor data where each task is a phase-shifted periodic time series. Inparticular, we develop a novel Bayesian nonparametric model capturing a mixtureof Gaussian processes where each task is a sum of a group-specific function anda component capturing individual variation, in addition to each task beingphase shifted. We develop an efficient \textsc{em} algorithm to learn theparameters of the model. As a special case we obtain the Gaussian mixture modeland \textsc{em} algorithm for phased-shifted periodic time series. Furthermore,we extend the proposed model by using a Dirichlet Process prior and therebyleading to an infinite mixture model that is capable of doing automatic modelselection. A Variational Bayesian approach is developed for inference in thismodel. Experiments in regression, classification and class discoverydemonstrate the performance of the proposed models using both synthetic dataand real-world time series data from astrophysics. Our methods are particularlyuseful when the time series are sparsely and non-synchronously sampled.
arxiv-3000-94 | Meta Path-Based Collective Classification in Heterogeneous Information Networks | http://arxiv.org/pdf/1305.4433v1.pdf | author:Xiangnan Kong, Bokai Cao, Philip S. Yu, Ying Ding, David J. Wild category:cs.LG stat.ML published:2013-05-20 summary:Collective classification has been intensively studied due to its impact inmany important applications, such as web mining, bioinformatics and citationanalysis. Collective classification approaches exploit the dependencies of agroup of linked objects whose class labels are correlated and need to bepredicted simultaneously. In this paper, we focus on studying the collectiveclassification problem in heterogeneous networks, which involves multiple typesof data objects interconnected by multiple types of links. Intuitively, twoobjects are correlated if they are linked by many paths in the network.However, most existing approaches measure the dependencies among objectsthrough directly links or indirect links without considering the differentsemantic meanings behind different paths. In this paper, we study thecollective classification problem taht is defined among the same type ofobjects in heterogenous networks. Moreover, by considering different linkagepaths in the network, one can capture the subtlety of different types ofdependencies among objects. We introduce the concept of meta-path baseddependencies among objects, where a meta path is a path consisting a certainsequence of linke types. We show that the quality of collective classificationresults strongly depends upon the meta paths used. To accommodate the largenetwork size, a novel solution, called HCC (meta-path based HeterogenousCollective Classification), is developed to effectively assign labels to agroup of instances that are interconnected through different meta-paths. Theproposed HCC model can capture different types of dependencies among objectswith respect to different meta paths. Empirical studies on real-world networksdemonstrate that effectiveness of the proposed meta path-based collectiveclassification approach.
arxiv-3000-95 | Quantum and Concept Combination, Entangled Measurements and Prototype Theory | http://arxiv.org/pdf/1303.2430v2.pdf | author:Diederik Aerts category:cs.AI cs.CL quant-ph published:2013-03-11 summary:We analyze the meaning of the violation of the marginal probability law forsituations of correlation measurements where entanglement is identified. Weshow that for quantum theory applied to the cognitive realm such a violationdoes not lead to the type of problems commonly believed to occur in situationsof quantum theory applied to the physical realm. We briefly situate our quantumapproach for modeling concepts and their combinations with respect to thenotions of 'extension' and 'intension' in theories of meaning, and in existingconcept theories.
arxiv-3000-96 | Ensembles of Classifiers based on Dimensionality Reduction | http://arxiv.org/pdf/1305.4345v1.pdf | author:Alon Schclar, Lior Rokach, Amir Amit category:cs.LG published:2013-05-19 summary:We present a novel approach for the construction of ensemble classifiersbased on dimensionality reduction. Dimensionality reduction methods representdatasets using a small number of attributes while preserving the informationconveyed by the original dataset. The ensemble members are trained based ondimension-reduced versions of the training set. These versions are obtained byapplying dimensionality reduction to the original training set using differentvalues of the input parameters. This construction meets both the diversity andaccuracy criteria which are required to construct an ensemble classifier wherethe former criterion is obtained by the various input parameter values and thelatter is achieved due to the decorrelation and noise reduction properties ofdimensionality reduction. In order to classify a test sample, it is firstembedded into the dimension reduced space of each individual classifier byusing an out-of-sample extension algorithm. Each classifier is then applied tothe embedded sample and the classification is obtained via a voting scheme. Wepresent three variations of the proposed approach based on the RandomProjections, the Diffusion Maps and the Random Subspaces dimensionalityreduction algorithms. We also present a multi-strategy ensemble which combinesAdaBoost and Diffusion Maps. A comparison is made with the Bagging, AdaBoost,Rotation Forest ensemble classifiers and also with the base classifier whichdoes not incorporate dimensionality reduction. Our experiments used seventeenbenchmark datasets from the UCI repository. The results obtained by theproposed algorithms were superior in many cases to other algorithms.
arxiv-3000-97 | Generalized Centroid Estimators in Bioinformatics | http://arxiv.org/pdf/1305.4339v1.pdf | author:Michiaki Hamada, Hisanori Kiryu, Wataru Iwasaki, Kiyoshi Asai category:q-bio.QM cs.LG published:2013-05-19 summary:In a number of estimation problems in bioinformatics, accuracy measures ofthe target problem are usually given, and it is important to design estimatorsthat are suitable to those accuracy measures. However, there is often adiscrepancy between an employed estimator and a given accuracy measure of theproblem. In this study, we introduce a general class of efficient estimatorsfor estimation problems on high-dimensional binary spaces, which representmanyfundamental problems in bioinformatics. Theoretical analysis reveals that theproposed estimators generally fit with commonly-used accuracy measures (e.g.sensitivity, PPV, MCC and F-score) as well as it can be computed efficiently inmany cases, and cover a wide range of problems in bioinformatics from theviewpoint of the principle of maximum expected accuracy (MEA). It is also shownthat some important algorithms in bioinformatics can be interpreted in aunified manner. Not only the concept presented in this paper gives a usefulframework to design MEA-based estimators but also it is highly extendable andsheds new light on many problems in bioinformatics.
arxiv-3000-98 | Quantum Annealing for Dirichlet Process Mixture Models with Applications to Network Clustering | http://arxiv.org/pdf/1305.4325v1.pdf | author:Issei Sato, Shu Tanaka, Kenichi Kurihara, Seiji Miyashita, Hiroshi Nakagawa category:quant-ph stat.ML published:2013-05-19 summary:We developed a new quantum annealing (QA) algorithm for Dirichlet processmixture (DPM) models based on the Chinese restaurant process (CRP). QA is aparallelized extension of simulated annealing (SA), i.e., it is a parallelstochastic optimization technique. Existing approaches [Kurihara et al.UAI2009, Sato et al. UAI2009] and cannot be applied to the CRP because their QAframework is formulated using a fixed number of mixture components. Theproposed QA algorithm can handle an unfixed number of classes in mixturemodels. We applied QA to a DPM model for clustering vertices in a network wherea CRP seating arrangement indicates a network partition. A multi core processorwas used for running QA in experiments, the results of which show that QA isbetter than SA, Markov chain Monte Carlo inference, and beam search at findinga maximum a posteriori estimation of a seating arrangement in the CRP. Sinceour QA algorithm is as easy as to implement the SA algorithm, it is suitablefor a wide range of applications.
arxiv-3000-99 | Horizon-Independent Optimal Prediction with Log-Loss in Exponential Families | http://arxiv.org/pdf/1305.4324v1.pdf | author:Peter Bartlett, Peter Grunwald, Peter Harremoes, Fares Hedayati, Wojciech Kotlowski category:cs.LG stat.ML published:2013-05-19 summary:We study online learning under logarithmic loss with regular parametricmodels. Hedayati and Bartlett (2012b) showed that a Bayesian predictionstrategy with Jeffreys prior and sequential normalized maximum likelihood(SNML) coincide and are optimal if and only if the latter is exchangeable, andif and only if the optimal strategy can be calculated without knowing the timehorizon in advance. They put forward the question what families haveexchangeable SNML strategies. This paper fully answers this open problem forone-dimensional exponential families. The exchangeability can happen only forthree classes of natural exponential family distributions, namely the Gaussian,Gamma, and the Tweedie exponential family of order 3/2. Keywords: SNMLExchangeability, Exponential Family, Online Learning, Logarithmic Loss,Bayesian Strategy, Jeffreys Prior, Fisher Information1
arxiv-3000-100 | Passive Learning with Target Risk | http://arxiv.org/pdf/1302.2157v2.pdf | author:Mehrdad Mahdavi, Rong Jin category:cs.LG published:2013-02-08 summary:In this paper we consider learning in passive setting but with a slightmodification. We assume that the target expected loss, also referred to astarget risk, is provided in advance for learner as prior knowledge. Unlike moststudies in the learning theory that only incorporate the prior knowledge intothe generalization bounds, we are able to explicitly utilize the target risk inthe learning process. Our analysis reveals a surprising result on the samplecomplexity of learning: by exploiting the target risk in the learningalgorithm, we show that when the loss function is both strongly convex andsmooth, the sample complexity reduces to $\O(\log (\frac{1}{\epsilon}))$, anexponential improvement compared to the sample complexity$\O(\frac{1}{\epsilon})$ for learning with strongly convex loss functions.Furthermore, our proof is constructive and is based on a computationallyefficient stochastic optimization algorithm for such settings which demonstratethat the proposed algorithm is practically useful.
arxiv-3000-101 | Blockwise SURE Shrinkage for Non-Local Means | http://arxiv.org/pdf/1305.4298v1.pdf | author:Yue Wu, Brian Tracey, Premkumar Natarajan, Joseph P. Noonan category:cs.CV published:2013-05-18 summary:In this letter, we investigate the shrinkage problem for the non-local means(NLM) image denoising. In particular, we derive the closed-form of the optimalblockwise shrinkage for NLM that minimizes the Stein's unbiased risk estimator(SURE). We also propose a constant complexity algorithm allowing fast blockwiseshrinkage. Simulation results show that the proposed blockwise shrinkage methodimproves NLM performance in attaining higher peak signal noise ratio (PSNR) andstructural similarity index (SSIM), and makes NLM more robust against parameterchanges. Similar ideas can be applicable to other patchwise image denoisingtechniques.
arxiv-3000-102 | Embedding Riemannian Manifolds by the Heat Kernel of the Connection Laplacian | http://arxiv.org/pdf/1305.4232v1.pdf | author:Hau-tieng Wu category:math.DG math.SP math.ST stat.ML stat.TH published:2013-05-18 summary:Given a class of closed Riemannian manifolds with prescribed geometricconditions, we introduce an embedding of the manifolds into $\ell^2$ based onthe heat kernel of the Connection Laplacian associated with the Levi-Civitaconnection on the tangent bundle. As a result, we can construct a distance inthis class which leads to a pre-compactness theorem on the class underconsideration.
arxiv-3000-103 | Machine learning on images using a string-distance | http://arxiv.org/pdf/1305.4204v1.pdf | author:Uzi Chester, Joel Ratsaby category:cs.LG cs.CV published:2013-05-17 summary:We present a new method for image feature-extraction which is based onrepresenting an image by a finite-dimensional vector of distances that measurehow different the image is from a set of image prototypes. We use the recentlyintroduced Universal Image Distance (UID) \cite{RatsabyChesterIEEE2012} tocompare the similarity between an image and a prototype image. The advantage inusing the UID is the fact that no domain knowledge nor any image analysis needto be done. Each image is represented by a finite dimensional feature vectorwhose components are the UID values between the image and a finite set of imageprototypes from each of the feature categories. The method is automatic sinceonce the user selects the prototype images, the feature vectors areautomatically calculated without the need to do any image analysis. Theprototype images can be of different size, in particular, different than theimage size. Based on a collection of such cases any supervised or unsupervisedlearning algorithm can be used to train and produce an image classifier orimage cluster analysis. In this paper we present the image feature-extractionmethod and use it on several supervised and unsupervised learning experimentsfor satellite image data.
arxiv-3000-104 | A Random Walk Based Model Incorporating Social Information for Recommendations | http://arxiv.org/pdf/1208.0787v2.pdf | author:Shang Shang, Sanjeev R. Kulkarni, Paul W. Cuff, Pan Hui category:cs.IR cs.LG published:2012-08-03 summary:Collaborative filtering (CF) is one of the most popular approaches to build arecommendation system. In this paper, we propose a hybrid collaborativefiltering model based on a Makovian random walk to address the data sparsityand cold start problems in recommendation systems. More precisely, we constructa directed graph whose nodes consist of items and users, together with itemcontent, user profile and social network information. We incorporate user'sratings into edge settings in the graph model. The model provides personalizedrecommendations and predictions to individuals and groups. The proposedalgorithms are evaluated on MovieLens and Epinions datasets. Experimentalresults show that the proposed methods perform well compared with othergraph-based methods, especially in the cold start case.
arxiv-3000-105 | Wisdom of the Crowd: Incorporating Social Influence in Recommendation Models | http://arxiv.org/pdf/1208.0782v2.pdf | author:Shang Shang, Pan Hui, Sanjeev R. Kulkarni, Paul W. Cuff category:cs.IR cs.LG cs.SI physics.soc-ph published:2012-08-03 summary:Recommendation systems have received considerable attention recently.However, most research has been focused on improving the performance ofcollaborative filtering (CF) techniques. Social networks, indispensably,provide us extra information on people's preferences, and should be consideredand deployed to improve the quality of recommendations. In this paper, wepropose two recommendation models, for individuals and for groups respectively,based on social contagion and social influence network theory. In therecommendation model for individuals, we improve the result of collaborativefiltering prediction with social contagion outcome, which simulates the resultof information cascade in the decision-making process. In the recommendationmodel for groups, we apply social influence network theory to takeinterpersonal influence into account to form a settled pattern of disagreement,and then aggregate opinions of group members. By introducing the concept ofsusceptibility and interpersonal influence, the settled rating results areflexible, and inclined to members whose ratings are "essential".
arxiv-3000-106 | Flying Triangulation - towards the 3D movie camera | http://arxiv.org/pdf/1305.4168v1.pdf | author:Florian Willomitzer, Svenja Ettl, Christian Faber, Gerd Häusler category:cs.CV physics.optics published:2013-05-17 summary:Flying Triangulation sensors enable a free-hand and motion-robust 3D dataacquisition of complex shaped objects. The measurement principle is based on amulti-line light-sectioning approach and uses sophisticated algorithms forreal-time registration (S. Ettl et al., Appl. Opt. 51 (2012) 281-289). As"single-shot principle", light sectioning enables the option to get surfacedata from one single camera exposure. But there is a drawback: A pixel-densemeasurement is not possible because of fundamental information-theoreticalreasons. By "pixel-dense" we understand that each pixel displays individuallymeasured distance information, neither interpolated from its neighbour pixelsnor using lateral context information. Hence, for monomodal single-shotprinciples, the 3D data generated from one 2D raw image display a significantlylower space-bandwidth than the camera permits. This is the price one must payfor motion robustness. Currently, our sensors project about 10 lines (each with1000 pixels), reaching an considerable lower data efficiency than theoreticallypossible for a single-shot sensor. Our aim is to push Flying Triangulation toits information-theoretical limits. Therefore, the line density as well as themeasurement depth needs to be significantly increased. This causes seriousindexing ambiguities. On the road to a single-shot 3D movie camera, we areworking on solutions to overcome the problem of false line indexing byutilizing yet unexploited information. We will present several approaches andwill discuss profound information-theoretical questions about the informationefficiency of 3D sensors.
arxiv-3000-107 | Factored expectation propagation for input-output FHMM models in systems biology | http://arxiv.org/pdf/1305.4153v1.pdf | author:Botond Cseke, Guido Sanguinetti category:stat.ML published:2013-05-17 summary:We consider the problem of joint modelling of metabolic signals and geneexpression in systems biology applications. We propose an approach based oninput-output factorial hidden Markov models and propose a structuredvariational inference approach to infer the structure and states of the model.We start from the classical free form structured variational mean fieldapproach and use a expectation propagation to approximate the expectationsneeded in the variational loop. We show that this corresponds to a factoredexpectation constrained approximate inference. We validate our model throughextensive simulations and demonstrate its applicability on a real worldbacterial data set.
arxiv-3000-108 | Rule-Based Semantic Tagging. An Application Undergoing Dictionary Glosses | http://arxiv.org/pdf/1305.3882v2.pdf | author:Daniel Christen category:cs.CL published:2013-05-16 summary:The project presented in this article aims to formalize criteria andprocedures in order to extract semantic information from parsed dictionaryglosses. The actual purpose of the project is the generation of a semanticnetwork (nearly an ontology) issued from a monolingual Italian dictionary,through unsupervised procedures. Since the project involves rule-based Parsing,Semantic Tagging and Word Sense Disambiguation techniques, its outcomes mayfind an interest also beyond this immediate intent. The cooperation of bothsyntactic and semantic features in meaning construction are investigated, andprocedures which allows a translation of syntactic dependencies in semanticrelations are discussed. The procedures that rise from this project can beapplied also to other text types than dictionary glosses, as they convert theoutput of a parsing process into a semantic representation. In addition somemechanism are sketched that may lead to a kind of procedural semantics, throughwhich multiple paraphrases of an given expression can be generated. Which meansthat these techniques may find an application also in 'query expansion'strategies, interesting Information Retrieval, Search Engines and QuestionAnswering Systems.
arxiv-3000-109 | Sémantique des déterminants dans un cadre richement typé | http://arxiv.org/pdf/1302.1422v2.pdf | author:Christian Retoré category:cs.CL published:2013-02-06 summary:The variation of word meaning according to the context leads us to enrich thetype system of our syntactical and semantic analyser of French based oncategorial grammars and Montague semantics (or lambda-DRT). The main advantageof a deep semantic analyse is too represent meaning by logical formulae thatcan be easily used e.g. for inferences. Determiners and quantifiers play afundamental role in the construction of those formulae. But in our rich typesystem the usual semantic terms do not work. We propose a solution ins- piredby the tau and epsilon operators of Hilbert, kinds of generic elements andchoice functions. This approach unifies the treatment of the different determi-ners and quantifiers as well as the dynamic binding of pronouns. Above all,this fully computational view fits in well within the wide coverage parserGrail, both from a theoretical and a practical viewpoint.
arxiv-3000-110 | Conditions for Convergence in Regularized Machine Learning Objectives | http://arxiv.org/pdf/1305.4081v1.pdf | author:Patrick Hop, Xinghao Pan category:cs.LG cs.NA math.OC published:2013-05-17 summary:Analysis of the convergence rates of modern convex optimization algorithmscan be achived through binary means: analysis of emperical convergence, oranalysis of theoretical convergence. These two pathways of capturinginformation diverge in efficacy when moving to the world of distributedcomputing, due to the introduction of non-intuitive, non-linear slowdownsassociated with broadcasting, and in some cases, gathering operations. Despitethese nuances in the rates of convergence, we can still show the existence ofconvergence, and lower bounds for the rates. This paper will serve as a helpfulcheat-sheet for machine learning practitioners encountering this problem classin the field.
arxiv-3000-111 | Font Acknowledgment and Character Extraction of Digital and Scanned Images | http://arxiv.org/pdf/1305.4064v1.pdf | author:Syed Muhammad Arsalan Bashir category:cs.CV published:2013-05-17 summary:The font recognition and character extraction is of immense importance asthese are many scenarios where data are in such a form, which cannot beprocessed like in image form or as a hard copy. So the procedure developed inthis paper is basically related to identifying the font (Times New Roman, Arialand Comic Sans MS) and afterwards recovering the text using simple correlationbased method where the binary templates are correlated to the input image textcharacters. All of this extraction is done in the presence of a little noise asimages may have noisy patterns due to photocopying. The significance of thismethod exists in extraction of data from various monitoring (Surveillance)camera footages or even more. The method is developed on Matlab\c{opyright}which takes input image and recovers text and font information from it in atext file.
arxiv-3000-112 | Binary Tree based Chinese Word Segmentation | http://arxiv.org/pdf/1305.3981v1.pdf | author:Kaixu Zhang, Can Wang, Maosong Sun category:cs.CL published:2013-05-17 summary:Chinese word segmentation is a fundamental task for Chinese languageprocessing. The granularity mismatch problem is the main cause of the errors.This paper showed that the binary tree representation can store outputs withdifferent granularity. A binary tree based framework is also designed toovercome the granularity mismatch problem. There are two steps in thisframework, namely tree building and tree pruning. The tree pruning step isspecially designed to focus on the granularity problem. Previous work forChinese word segmentation such as the sequence tagging can be easily employedin this framework. This framework can also provide quantitative error analysismethods. The experiments showed that after using a more sophisticated treepruning function for a state-of-the-art conditional random field basedbaseline, the error reduction can be up to 20%.
arxiv-3000-113 | Sparse Norm Filtering | http://arxiv.org/pdf/1305.3971v1.pdf | author:Chengxi Ye, Dacheng Tao, Mingli Song, David W. Jacobs, Min Wu category:cs.GR cs.CV cs.MM published:2013-05-17 summary:Optimization-based filtering smoothes an image by minimizing a fidelityfunction and simultaneously preserves edges by exploiting a sparse norm penaltyover gradients. It has obtained promising performance in practical problems,such as detail manipulation, HDR compression and deblurring, and thus hasreceived increasing attentions in fields of graphics, computer vision and imageprocessing. This paper derives a new type of image filter called sparse normfilter (SNF) from optimization-based filtering. SNF has a very simple form,introduces a general class of filtering techniques, and explains severalclassic filters as special implementations of SNF, e.g. the averaging filterand the median filter. It has advantages of being halo free, easy to implement,and low time and memory costs (comparable to those of the bilateral filter).Thus, it is more generic than a smoothing operator and can better adapt todifferent tasks. We validate the proposed SNF by a wide variety of applicationsincluding edge-preserving smoothing, outlier tolerant filtering, detailmanipulation, HDR compression, non-blind deconvolution, image segmentation, andcolorization.
arxiv-3000-114 | Analysis Of Interest Points Of Curvelet Coefficients Contributions Of Microscopic Images And Improvement Of Edges | http://arxiv.org/pdf/1305.3939v1.pdf | author:A. Djimeli, D. Tchiotsop, R. Tchinda category:cs.CV published:2013-05-16 summary:This paper focuses on improved edge model based on Curvelet coefficientsanalysis. Curvelet transform is a powerful tool for multiresolutionrepresentation of object with anisotropic edge. Curvelet coefficientscontributions have been analyzed using Scale Invariant Feature Transform(SIFT), commonly used to study local structure in images. The permutation ofCurvelet coefficients from original image and edges image obtained fromgradient operator is used to improve original edges. Experimental results showthat this method brings out details on edges when the decomposition scaleincreases.
arxiv-3000-115 | Geometric primitive feature extraction - concepts, algorithms, and applications | http://arxiv.org/pdf/1305.3885v1.pdf | author:Dilip K. Prasad category:cs.CV cs.CG published:2013-05-16 summary:This thesis presents important insights and concepts related to the topic ofthe extraction of geometric primitives from the edge contours of digitalimages. Three specific problems related to this topic have been studied, viz.,polygonal approximation of digital curves, tangent estimation of digitalcurves, and ellipse fitting anddetection from digital curves. For the problemof polygonal approximation, two fundamental problems have been addressed.First, the nature of the performance evaluation metrics in relation to thelocal and global fitting characteristics has been studied. Second, an expliciterror bound of the error introduced by digitizing a continuous line segment hasbeen derived and used to propose a generic non-heuristic parameter independentframework which can be used in several dominant point detection methods. Forthe problem of tangent estimation for digital curves, a simple method oftangent estimation has been proposed. It is shown that the method has adefinite upper bound of the error for conic digital curves. It has been shownthat the method performs better than almost all (seventy two) existing tangentestimation methods for conic as well as several non-conic digital curves. Forthe problem of fitting ellipses on digital curves, a geometric distanceminimization model has been considered. An unconstrained, linear,non-iterative, and numerically stable ellipse fitting method has been proposedand it has been shown that the proposed method has better selectivity forelliptic digital curves (high true positive and low false positive) as comparedto several other ellipse fitting methods. For the problem of detecting ellipsesin a set of digital curves, several innovative and fast pre-processing,grouping, and hypotheses evaluation concepts applicable for digital curves havebeen proposed and combined to form an ellipse detection method.
arxiv-3000-116 | One-Pass AUC Optimization | http://arxiv.org/pdf/1305.1363v2.pdf | author:Wei Gao, Rong Jin, Shenghuo Zhu, Zhi-Hua Zhou category:cs.LG published:2013-05-07 summary:AUC is an important performance measure and many algorithms have been devotedto AUC optimization, mostly by minimizing a surrogate convex loss on a trainingdata set. In this work, we focus on one-pass AUC optimization that requiresonly going through the training data once without storing the entire trainingdataset, where conventional online learning algorithms cannot be applieddirectly because AUC is measured by a sum of losses defined over pairs ofinstances from different classes. We develop a regression-based algorithm whichonly needs to maintain the first and second order statistics of training datain memory, resulting a storage requirement independent from the size oftraining data. To efficiently handle high dimensional data, we develop arandomized algorithm that approximates the covariance matrices by low rankmatrices. We verify, both theoretically and empirically, the effectiveness ofthe proposed algorithm.
arxiv-3000-117 | Hierarchically-coupled hidden Markov models for learning kinetic rates from single-molecule data | http://arxiv.org/pdf/1305.3640v1.pdf | author:Jan-Willem van de Meent, Jonathan E. Bronson, Frank Wood, Ruben L. Gonzalez Jr., Chris H. Wiggins category:stat.ML physics.bio-ph q-bio.QM published:2013-05-15 summary:We address the problem of analyzing sets of noisy time-varying signals thatall report on the same process but confound straightforward analyses due tocomplex inter-signal heterogeneities and measurement artifacts. In particularwe consider single-molecule experiments which indirectly measure the distinctsteps in a biomolecular process via observations of noisy time-dependentsignals such as a fluorescence intensity or bead position. Straightforwardhidden Markov model (HMM) analyses attempt to characterize such processes interms of a set of conformational states, the transitions that can occur betweenthese states, and the associated rates at which those transitions occur; butrequire ad-hoc post-processing steps to combine multiple signals. Here wedevelop a hierarchically coupled HMM that allows experimentalists to deal withinter-signal variability in a principled and automatic way. Our approach is ageneralized expectation maximization hyperparameter point estimation procedurewith variational Bayes at the level of individual time series that learns ansingle interpretable representation of the overall data generating process.
arxiv-3000-118 | Modeling Information Propagation with Survival Theory | http://arxiv.org/pdf/1305.3616v1.pdf | author:Manuel Gomez Rodriguez, Jure Leskovec, Bernhard Schoelkopf category:cs.SI cs.DS physics.soc-ph stat.ML published:2013-05-15 summary:Networks provide a skeleton for the spread of contagions, like, information,ideas, behaviors and diseases. Many times networks over which contagionsdiffuse are unobserved and need to be inferred. Here we apply survival theoryto develop general additive and multiplicative risk models under which thenetwork inference problems can be solved efficiently by exploiting theirconvexity. Our additive risk model generalizes several existing networkinference models. We show all these models are particular cases of our moregeneral model. Our multiplicative model allows for modeling scenarios in whicha node can either increase or decrease the risk of activation of another node,in contrast with previous approaches, which consider only positive riskincrements. We evaluate the performance of our network inference algorithms onlarge synthetic and real cascade datasets, and show that our models are able topredict the length and duration of cascades in real data.
arxiv-3000-119 | Hubiness, length, crossings and their relationships in dependency trees | http://arxiv.org/pdf/1304.4086v5.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL cs.DM cs.SI physics.soc-ph published:2013-04-15 summary:Here tree dependency structures are studied from three differentperspectives: their degree variance (hubiness), the mean dependency length andthe number of dependency crossings. Bounds that reveal pairwise dependenciesamong these three metrics are derived. Hubiness (the variance of degrees) playsa central role: the mean dependency length is bounded below by hubiness whilethe number of crossings is bounded above by hubiness. Our findings suggest thatthe online memory cost of a sentence might be determined not just by theordering of words but also by the hubiness of the underlying structure. The 2ndmoment of degree plays a crucial role that is reminiscent of its role in largecomplex networks.
arxiv-3000-120 | Transfer Learning for Content-Based Recommender Systems using Tree Matching | http://arxiv.org/pdf/1305.3384v1.pdf | author:Naseem Biadsy, Lior Rokach, Armin Shmilovici category:cs.LG cs.IR published:2013-05-15 summary:In this paper we present a new approach to content-based transfer learningfor solving the data sparsity problem in cases when the users' preferences inthe target domain are either scarce or unavailable, but the necessaryinformation on the preferences exists in another domain. We show that traininga system to use such information across domains can produce better performance.Specifically, we represent users' behavior patterns based on topological graphstructures. Each behavior pattern represents the behavior of a set of users,when the users' behavior is defined as the items they rated and the items'rating values. In the next step we find a correlation between behavior patternsin the source domain and behavior patterns in the target domain. This mappingis considered a bridge between the two domains. Based on the correlation andcontent-attributes of the items, we train a machine learning model to predictusers' ratings in the target domain. When we compare our approach to thepopularity approach and KNN-cross-domain on a real world dataset, the resultsshow that on an average of 83$%$ of the cases our approach outperforms bothmethods.
arxiv-3000-121 | Color image denoising by chromatic edges based vector valued diffusion | http://arxiv.org/pdf/1304.5587v2.pdf | author:V. B. Surya Prasath, Juan C. Moreno, K. Palaniappan category:cs.CV 68U10 I.4.3 published:2013-04-20 summary:In this letter we propose to denoise digital color images via an improvedgeometric diffusion scheme. By introducing edges detected from all three colorchannels into the diffusion the proposed scheme avoids color smearingartifacts. Vector valued diffusion is used to control the smoothing and thegeometry of color images are taken into consideration. Color edge strengthfunction computed from different planes is introduced and it stops thediffusion spread across chromatic edges. Experimental results indicate that thescheme achieves good denoising with edge preservation when compared to otherrelated schemes.
arxiv-3000-122 | Online Learning in a Contract Selection Problem | http://arxiv.org/pdf/1305.3334v1.pdf | author:Cem Tekin, Mingyan Liu category:cs.LG cs.GT math.OC stat.ML published:2013-05-15 summary:In an online contract selection problem there is a seller which offers a setof contracts to sequentially arriving buyers whose types are drawn from anunknown distribution. If there exists a profitable contract for the buyer inthe offered set, i.e., a contract with payoff higher than the payoff of notaccepting any contracts, the buyer chooses the contract that maximizes itspayoff. In this paper we consider the online contract selection problem tomaximize the sellers profit. Assuming that a structural property called orderedpreferences holds for the buyer's payoff function, we propose online learningalgorithms that have sub-linear regret with respect to the best set ofcontracts given the distribution over the buyer's type. This problem has manyapplications including spectrum contracts, wireless service provider data plansand recommendation systems.
arxiv-3000-123 | Feature Multi-Selection among Subjective Features | http://arxiv.org/pdf/1302.4297v3.pdf | author:Sivan Sabato, Adam Kalai category:cs.LG stat.ML published:2013-02-18 summary:When dealing with subjective, noisy, or otherwise nebulous features, the"wisdom of crowds" suggests that one may benefit from multiple judgments of thesame feature on the same object. We give theoretically-motivated `featuremulti-selection' algorithms that choose, among a large set of candidatefeatures, not only which features to judge but how many times to judge eachone. We demonstrate the effectiveness of this approach for linear regression ona crowdsourced learning task of predicting people's height and weight fromphotos, using features such as 'gender' and 'estimated weight' as well asculturally fraught ones such as 'attractive'.
arxiv-3000-124 | Efficient Density Estimation via Piecewise Polynomial Approximation | http://arxiv.org/pdf/1305.3207v1.pdf | author:Siu-On Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun category:cs.LG cs.DS stat.ML published:2013-05-14 summary:We give a highly efficient "semi-agnostic" algorithm for learning univariateprobability distributions that are well approximated by piecewise polynomialdensity functions. Let $p$ be an arbitrary distribution over an interval $I$which is $\tau$-close (in total variation distance) to an unknown probabilitydistribution $q$ that is defined by an unknown partition of $I$ into $t$intervals and $t$ unknown degree-$d$ polynomials specifying $q$ over each ofthe intervals. We give an algorithm that draws $\tilde{O}(t\new{(d+1)}/\eps^2)$samples from $p$, runs in time $\poly(t,d,1/\eps)$, and with high probabilityoutputs a piecewise polynomial hypothesis distribution $h$ that is$(O(\tau)+\eps)$-close (in total variation distance) to $p$. This samplecomplexity is essentially optimal; we show that even for $\tau=0$, anyalgorithm that learns an unknown $t$-piecewise degree-$d$ probabilitydistribution over $I$ to accuracy $\eps$ must use $\Omega({\frac {t(d+1)}{\poly(1 + \log(d+1))}} \cdot {\frac 1 {\eps^2}})$ samples from thedistribution, regardless of its running time. Our algorithm combines tools fromapproximation theory, uniform convergence, linear programming, and dynamicprogramming. We apply this general algorithm to obtain a wide range of results for manynatural problems in density estimation over both continuous and discretedomains. These include state-of-the-art results for learning mixtures oflog-concave distributions; mixtures of $t$-modal distributions; mixtures ofMonotone Hazard Rate distributions; mixtures of Poisson Binomial Distributions;mixtures of Gaussians; and mixtures of $k$-monotone densities. Our generaltechnique yields computationally efficient algorithms for all these problems,in many cases with provably optimal sample complexities (up to logarithmicfactors) in all parameters.
arxiv-3000-125 | A Bag of Words Approach for Semantic Segmentation of Monitored Scenes | http://arxiv.org/pdf/1305.3189v1.pdf | author:Wassim Bouachir, Atousa Torabi, Guillaume-Alexandre Bilodeau, Pascal Blais category:cs.CV published:2013-05-14 summary:This paper proposes a semantic segmentation method for outdoor scenescaptured by a surveillance camera. Our algorithm classifies each perceptuallyhomogenous region as one of the predefined classes learned from a collection ofmanually labelled images. The proposed approach combines two different types ofinformation. First, color segmentation is performed to divide the scene intoperceptually similar regions. Then, the second step is based on SIFT keypointsand uses the bag of words representation of the regions for the classification.The prediction is done using a Na\"ive Bayesian Network as a generativeclassifier. Compared to existing techniques, our method provides more compactrepresentations of scene contents and the segmentation result is moreconsistent with human perception due to the combination of the colorinformation with the image keypoints. The experiments conducted on a publiclyavailable data set demonstrate the validity of the proposed method.
arxiv-3000-126 | Autonomous Reinforcement of Behavioral Sequences in Neural Dynamics | http://arxiv.org/pdf/1210.3569v2.pdf | author:Sohrob Kazerounian, Matthew Luciw, Mathis Richter, Yulia Sandamirskaya category:cs.NE published:2012-10-12 summary:We introduce a dynamic neural algorithm called Dynamic Neural (DN)SARSA(\lambda) for learning a behavioral sequence from delayed reward.DN-SARSA(\lambda) combines Dynamic Field Theory models of behavioral sequencerepresentation, classical reinforcement learning, and a computationalneuroscience model of working memory, called Item and Order working memory,which serves as an eligibility trace. DN-SARSA(\lambda) is implemented on botha simulated and real robot that must learn a specific rewarding sequence ofelementary behaviors from exploration. Results show DN-SARSA(\lambda) performson the level of the discrete SARSA(\lambda), validating the feasibility ofgeneral reinforcement learning without compromising neural dynamics.
arxiv-3000-127 | Qualitative detection of oil adulteration with machine learning approaches | http://arxiv.org/pdf/1305.3149v1.pdf | author:Xiao-Bo Jin, Qiang Lu, Feng Wang, Quan-gong Huo category:cs.CE cs.LG published:2013-05-14 summary:The study focused on the machine learning analysis approaches to identify theadulteration of 9 kinds of edible oil qualitatively and answered the followingthree questions: Is the oil sample adulterant? How does it constitute? What isthe main ingredient of the adulteration oil? After extracting thehigh-performance liquid chromatography (HPLC) data on triglyceride from 370 oilsamples, we applied the adaptive boosting with multi-class Hamming loss(AdaBoost.MH) to distinguish the oil adulteration in contrast with the supportvector machine (SVM). Further, we regarded the adulterant oil and the pure oilsamples as ones with multiple labels and with only one label, respectively.Then multi-label AdaBoost.MH and multi-label learning vector quantization(ML-LVQ) model were built to determine the ingredients and their relative ratioin the adulteration oil. The experimental results on six measures show thatML-LVQ achieves better performance than multi-label AdaBoost.MH.
arxiv-3000-128 | Optimization with First-Order Surrogate Functions | http://arxiv.org/pdf/1305.3120v1.pdf | author:Julien Mairal category:stat.ML cs.LG math.OC published:2013-05-14 summary:In this paper, we study optimization methods consisting of iterativelyminimizing surrogates of an objective function. By proposing severalalgorithmic variants and simple convergence analyses, we make two maincontributions. First, we provide a unified viewpoint for several first-orderoptimization techniques such as accelerated proximal gradient, block coordinatedescent, or Frank-Wolfe algorithms. Second, we introduce a new incrementalscheme that experimentally matches or outperforms state-of-the-art solvers forlarge-scale optimization problems typically arising in machine learning.
arxiv-3000-129 | I Wish I Didn't Say That! Analyzing and Predicting Deleted Messages in Twitter | http://arxiv.org/pdf/1305.3107v1.pdf | author:Sasa Petrovic, Miles Osborne, Victor Lavrenko category:cs.SI cs.CL published:2013-05-14 summary:Twitter has become a major source of data for social media researchers. Oneimportant aspect of Twitter not previously considered are {\em deletions} --removal of tweets from the stream. Deletions can be due to a multitude ofreasons such as privacy concerns, rashness or attempts to undo publicstatements. We show how deletions can be automatically predicted ahead of timeand analyse which tweets are likely to be deleted and how.
arxiv-3000-130 | Multiple Change Point Estimation in Stationary Ergodic Time Series | http://arxiv.org/pdf/1203.1515v10.pdf | author:Azadeh Khaleghi, Daniil Ryabko category:stat.ML cs.IT math.IT math.ST stat.TH published:2012-03-07 summary:Given a heterogeneous time-series sample, the objective is to find points intime (called change points) where the probability distribution generating thedata has changed. The data are assumed to have been generated by arbitraryunknown stationary ergodic distributions. No modelling, independence or mixingassumptions are made. A novel, computationally efficient, nonparametric methodis proposed, and is shown to be asymptotically consistent in this generalframework. The theoretical results are complemented with experimentalevaluations.
arxiv-3000-131 | Early Detection of Alzheimer's - A Crucial Requirement | http://arxiv.org/pdf/1305.2713v2.pdf | author:Ijaz Bukhari category:cs.CV physics.med-ph published:2013-05-13 summary:Alzheimer's, an old age disease of people over 65 years causes problems withmemory, thinking and behavior. This disease progresses very slow and itsidentification in early stages is very difficult. The symptoms of Alzheimer'sappear slowly and gradually will have worse effects. In its early stages, notonly the patients themselves but their loved ones are generally unable toaccept that the patient is suffering from disease. In this paper, we haveproposed a new algorithm to detect patients of Alzheimer's at early stages bycomparing the Magnetic Resonance Images (MRI) of the patients with normalpersons of their age. The progress of the disease can also be monitored byperiodic comparison of the previous and current MRI.
arxiv-3000-132 | Scalable Audience Reach Estimation in Real-time Online Advertising | http://arxiv.org/pdf/1305.3014v1.pdf | author:Ali Jalali, Santanu Kolay, Peter Foldes, Ali Dasdan category:cs.LG cs.DB published:2013-05-14 summary:Online advertising has been introduced as one of the most efficient methodsof advertising throughout the recent years. Yet, advertisers are concernedabout the efficiency of their online advertising campaigns and consequently,would like to restrict their ad impressions to certain websites and/or certaingroups of audience. These restrictions, known as targeting criteria, limit thereachability for better performance. This trade-off between reachability andperformance illustrates a need for a forecasting system that can quicklypredict/estimate (with good accuracy) this trade-off. Designing such a systemis challenging due to (a) the huge amount of data to process, and, (b) the needfor fast and accurate estimates. In this paper, we propose a distributed faulttolerant system that can generate such estimates fast with good accuracy. Themain idea is to keep a small representative sample in memory across multiplemachines and formulate the forecasting problem as queries against the sample.The key challenge is to find the best strata across the past data, performmultivariate stratified sampling while ensuring fuzzy fall-back to cover thesmall minorities. Our results show a significant improvement over the uniformand simple stratified sampling strategies which are currently widely used inthe industry.
arxiv-3000-133 | Novel variational model for inpainting in the wavelet domain | http://arxiv.org/pdf/1305.3013v1.pdf | author:Dai-Qiang Chen, Li-Zhi Cheng category:cs.CV published:2013-05-14 summary:Wavelet domain inpainting refers to the process of recovering the missingcoefficients during the image compression or transmission stage. Recently, anefficient algorithm framework which is called Bregmanized operator splitting(BOS) was proposed for solving the classical variational model of waveletinpainting. However, it is still time-consuming to some extent due to the inneriteration. In this paper, a novel variational model is established to formulatethis reconstruction problem from the view of image decomposition. Then anefficient iterative algorithm based on the split-Bregman method is adopted tocalculate an optimal solution, and it is also proved to be convergent. Comparedwith the BOS algorithm the proposed algorithm avoids the inner iteration andhence is more simple. Numerical experiments demonstrate that the proposedmethod is very efficient and outperforms the current state-of-the-art methods,especially in the computational time.
arxiv-3000-134 | Real Time Bid Optimization with Smooth Budget Delivery in Online Advertising | http://arxiv.org/pdf/1305.3011v1.pdf | author:Kuang-Chih Lee, Ali Jalali, Ali Dasdan category:cs.GT cs.LG published:2013-05-14 summary:Today, billions of display ad impressions are purchased on a daily basisthrough a public auction hosted by real time bidding (RTB) exchanges. Adecision has to be made for advertisers to submit a bid for each selected RTBad request in milliseconds. Restricted by the budget, the goal is to buy a setof ad impressions to reach as many targeted users as possible. A desired action(conversion), advertiser specific, includes purchasing a product, filling out aform, signing up for emails, etc. In addition, advertisers typically prefer tospend their budget smoothly over the time in order to reach a wider range ofaudience accessible throughout a day and have a sustainable impact. However,since the conversions occur rarely and the occurrence feedback is normallydelayed, it is very challenging to achieve both budget and performance goals atthe same time. In this paper, we present an online approach to the smoothbudget delivery while optimizing for the conversion performance. Our algorithmtries to select high quality impressions and adjust the bid price based on theprior performance distribution in an adaptive manner by distributing the budgetoptimally across time. Our experimental results from real advertising campaignsdemonstrate the effectiveness of our proposed approach.
arxiv-3000-135 | Fast Linearized Alternating Direction Minimization Algorithm with Adaptive Parameter Selection for Multiplicative Noise Removal | http://arxiv.org/pdf/1305.3006v1.pdf | author:Dai-Qiang Chen, Li-Zhi Cheng category:cs.CV math.NA 68U10 published:2013-05-14 summary:Owing to the edge preserving ability and low computational cost of the totalvariation (TV), variational models with the TV regularization have been widelyinvestigated in the field of multiplicative noise removal. The key points ofthe successful application of these models lie in: the optimal selection of theregularization parameter which balances the data-fidelity term with the TVregularizer; the efficient algorithm to compute the solution. In this paper, wepropose two fast algorithms based on the linearized technique, which are ableto estimate the regularization parameter and recover the image simultaneously.In the iteration step of the proposed algorithms, the regularization parameteris adjusted by a special discrepancy function defined for multiplicative noise.The convergence properties of the proposed algorithms are proved under certainconditions, and numerical experiments demonstrate that the proposed algorithmsoverall outperform some state-of-the-art methods in the PSNR values andcomputational time.
arxiv-3000-136 | Estimating or Propagating Gradients Through Stochastic Neurons | http://arxiv.org/pdf/1305.2982v1.pdf | author:Yoshua Bengio category:cs.LG published:2013-05-14 summary:Stochastic neurons can be useful for a number of reasons in deep learningmodels, but in many cases they pose a challenging problem: how to estimate thegradient of a loss function with respect to the input of such stochasticneurons, i.e., can we "back-propagate" through these stochastic neurons? Weexamine this question, existing approaches, and present two novel families ofsolutions, applicable in different settings. In particular, it is demonstratedthat a simple biologically plausible formula gives rise to an an unbiased (butnoisy) estimator of the gradient with respect to a binary stochastic neuronfiring probability. Unlike other estimators which view the noise as a smallperturbation in order to estimate gradients by finite differences, thisestimator is unbiased even without assuming that the stochastic perturbation issmall. This estimator is also interesting because it can be applied in verygeneral settings which do not allow gradient back-propagation, including theestimation of the gradient with respect to future rewards, as required inreinforcement learning setups. We also propose an approach to approximatingthis unbiased but high-variance estimator by learning to predict it using abiased estimator. The second approach we propose assumes that an estimator ofthe gradient can be back-propagated and it provides an unbiased estimator ofthe gradient, but can only work with non-linearities unlike the hard threshold,but like the rectifier, that are not flat for all of their range. This issimilar to traditional sigmoidal units but has the advantage that for manyinputs, a hard decision (e.g., a 0 output) can be produced, which would beconvenient for conditional computation and achieving sparse representations andsparse gradients.
arxiv-3000-137 | HRF estimation improves sensitivity of fMRI encoding and decoding models | http://arxiv.org/pdf/1305.2788v1.pdf | author:Fabian Pedregosa, Michael Eickenberg, Bertrand Thirion, Alexandre Gramfort category:cs.LG stat.AP published:2013-05-13 summary:Extracting activation patterns from functional Magnetic Resonance Images(fMRI) datasets remains challenging in rapid-event designs due to the inherentdelay of blood oxygen level-dependent (BOLD) signal. The general linear model(GLM) allows to estimate the activation from a design matrix and a fixedhemodynamic response function (HRF). However, the HRF is known to varysubstantially between subjects and brain regions. In this paper, we propose amodel for jointly estimating the hemodynamic response function (HRF) and theactivation patterns via a low-rank representation of task effects.This model isbased on the linearity assumption behind the GLM and can be computed usingstandard gradient-based solvers. We use the activation patterns computed by ourmodel as input data for encoding and decoding studies and report performanceimprovement in both settings.
arxiv-3000-138 | Structure Discovery in Nonparametric Regression through Compositional Kernel Search | http://arxiv.org/pdf/1302.4922v4.pdf | author:David Duvenaud, James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum, Zoubin Ghahramani category:stat.ML cs.LG stat.ME G.3; I.2.6 published:2013-02-20 summary:Despite its importance, choosing the structural form of the kernel innonparametric regression remains a black art. We define a space of kernelstructures which are built compositionally by adding and multiplying a smallnumber of base kernels. We present a method for searching over this space ofstructures which mirrors the scientific discovery process. The learnedstructures can often decompose functions into interpretable components andenable long-range extrapolation on time-series datasets. Our structure searchmethod outperforms many widely used kernels and kernel combination methods on avariety of prediction tasks.
arxiv-3000-139 | An efficient algorithm for learning with semi-bandit feedback | http://arxiv.org/pdf/1305.2732v1.pdf | author:Gergely Neu, Gábor Bartók category:cs.LG published:2013-05-13 summary:We consider the problem of online combinatorial optimization undersemi-bandit feedback. The goal of the learner is to sequentially select itsactions from a combinatorial decision set so as to minimize its cumulativeloss. We propose a learning algorithm for this problem based on combining theFollow-the-Perturbed-Leader (FPL) prediction method with a novel lossestimation procedure called Geometric Resampling (GR). Contrary to previoussolutions, the resulting algorithm can be efficiently implemented for anydecision set where efficient offline combinatorial optimization is possible atall. Assuming that the elements of the decision set can be described withd-dimensional binary vectors with at most m non-zero entries, we show that theexpected regret of our algorithm after T rounds is O(m sqrt(dT log d)). As aside result, we also improve the best known regret bounds for FPL in the fullinformation setting to O(m^(3/2) sqrt(T log d)), gaining a factor of sqrt(d/m)over previous bounds for this algorithm.
arxiv-3000-140 | Automatic Parameter Adaptation for Multi-object Tracking | http://arxiv.org/pdf/1305.2687v1.pdf | author:Duc Phu Chau, Monique Thonnat, François Bremond category:cs.CV published:2013-05-13 summary:Object tracking quality usually depends on video context (e.g. objectocclusion level, object density). In order to decrease this dependency, thispaper presents a learning approach to adapt the tracker parameters to thecontext variations. In an offline phase, satisfactory tracking parameters arelearned for video context clusters. In the online control phase, once a contextchange is detected, the tracking parameters are tuned using the learned values.The experimental results show that the proposed approach outperforms the recenttrackers in state of the art. This paper brings two contributions: (1) aclassification method of video sequences to learn offline tracking parameters,(2) a new method to tune online tracking parameters using tracking context.
arxiv-3000-141 | A study for the effect of the Emphaticness and language and dialect for Voice Onset Time (VOT) in Modern Standard Arabic (MSA) | http://arxiv.org/pdf/1305.2680v1.pdf | author:Sulaiman S. AlDahri category:cs.CL cs.SD published:2013-05-13 summary:The signal sound contains many different features, including Voice Onset Time(VOT), which is a very important feature of stop sounds in many languages. Theonly application of VOT values is stopping phoneme subsets. This subset ofconsonant sounds is stop phonemes exist in the Arabic language, and in fact,all languages. The pronunciation of these sounds is hard and unique especiallyfor less-educated Arabs and non-native Arabic speakers. VOT can be utilized bythe human auditory system to distinguish between voiced and unvoiced stops suchas /p/ and /b/ in English.This search focuses on computing and analyzing VOT ofModern Standard Arabic (MSA), within the Arabic language, for all pairs ofnon-emphatic (namely, /d/ and /t/) and emphatic pairs (namely, /d?/ and /t?/)depending on carrier words. This research uses a database built by ourselves,and uses the carrier words syllable structure: CV-CV-CV. One of the mainoutcomes always found is the emphatic sounds (/d?/, /t?/) are less than 50% ofnon-emphatic (counter-part) sounds ( /d/, /t/).Also, VOT can be used toclassify or detect for a dialect ina language.
arxiv-3000-142 | Mean field variational Bayesian inference for support vector machine classification | http://arxiv.org/pdf/1305.2667v1.pdf | author:Jan Luts, John T. Ormerod category:stat.ME stat.ML published:2013-05-13 summary:A mean field variational Bayes approach to support vector machines (SVMs)using the latent variable representation on Polson & Scott (2012) is presented.This representation allows circumvention of many of the shortcomings associatedwith classical SVMs including automatic penalty parameter selection, theability to handle dependent samples, missing data and variable selection. Wedemonstrate on simulated and real datasets that our approach is easilyextendable to non-standard situations and outperforms the classical SVMapproach whilst remaining computationally efficient.
arxiv-3000-143 | Boosting with the Logistic Loss is Consistent | http://arxiv.org/pdf/1305.2648v1.pdf | author:Matus Telgarsky category:cs.LG stat.ML published:2013-05-13 summary:This manuscript provides optimization guarantees, generalization bounds, andstatistical consistency results for AdaBoost variants which replace theexponential loss with the logistic and similar losses (specifically, twicedifferentiable convex losses which are Lipschitz and tend to zero on one side). The heart of the analysis is to show that, in lieu of explicit regularizationand constraints, the structure of the problem is fairly rigidly controlled bythe source distribution itself. The first control of this type is in theseparable case, where a distribution-dependent relaxed weak learning rateinduces speedy convergence with high probability over any sample. Otherwise, inthe nonseparable case, the convex surrogate risk itself exhibitsdistribution-dependent levels of curvature, and consequently the algorithm'soutput has small norm with high probability.
arxiv-3000-144 | Identifying Pairs in Simulated Bio-Medical Time-Series | http://arxiv.org/pdf/1306.0541v1.pdf | author:Uri Kartoun category:cs.LG cs.CE published:2013-05-12 summary:The paper presents a time-series-based classification approach to identifysimilarities in pairs of simulated human-generated patterns. An example for apattern is a time-series representing a heart rate during a specifictime-range, wherein the time-series is a sequence of data points that representthe changes in the heart rate values. A bio-medical simulator system wasdeveloped to acquire a collection of 7,871 price patterns of financialinstruments. The financial instruments traded in real-time on three Americanstock exchanges, NASDAQ, NYSE, and AMEX, simulate bio-medical measurements. Thesystem simulates a human in which each price pattern represents one bio-medicalsensor. Data provided during trading hours from the stock exchanges allowedreal-time classification. Classification is based on new machine learningtechniques: self-labeling, which allows the application of supervised learningmethods on unlabeled time-series and similarity ranking, which applied on adecision tree learning algorithm to classify time-series regardless of type andquantity.
arxiv-3000-145 | Accelerated Mini-Batch Stochastic Dual Coordinate Ascent | http://arxiv.org/pdf/1305.2581v1.pdf | author:Shai Shalev-Shwartz, Tong Zhang category:stat.ML cs.LG published:2013-05-12 summary:Stochastic dual coordinate ascent (SDCA) is an effective technique forsolving regularized loss minimization problems in machine learning. This paperconsiders an extension of SDCA under the mini-batch setting that is often usedin practice. Our main contribution is to introduce an accelerated mini-batchversion of SDCA and prove a fast convergence rate for this method. We discussan implementation of our method over a parallel computing system, and comparethe results to both the vanilla stochastic dual coordinate ascent and to theaccelerated deterministic gradient descent method of\cite{nesterov2007gradient}.
arxiv-3000-146 | Learning Policies for Contextual Submodular Prediction | http://arxiv.org/pdf/1305.2532v1.pdf | author:Stephane Ross, Jiaji Zhou, Yisong Yue, Debadeepta Dey, J. Andrew Bagnell category:cs.LG stat.ML published:2013-05-11 summary:Many prediction domains, such as ad placement, recommendation, trajectoryprediction, and document summarization, require predicting a set or list ofoptions. Such lists are often evaluated using submodular reward functions thatmeasure both quality and diversity. We propose a simple, efficient, andprovably near-optimal approach to optimizing such prediction problems based onno-regret learning. Our method leverages a surprising result from onlinesubmodular optimization: a single no-regret online learner can compete with anoptimal sequence of predictions. Compared to previous work, which either learna sequence of classifiers or rely on stronger assumptions such asrealizability, we ensure both data-efficiency as well as performance guaranteesin the fully agnostic setting. Experiments validate the efficiency andapplicability of the approach on a wide range of problems including manipulatortrajectory optimization, news recommendation and document summarization.
arxiv-3000-147 | On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions | http://arxiv.org/pdf/1305.2505v1.pdf | author:Purushottam Kar, Bharath K Sriperumbudur, Prateek Jain, Harish C Karnick category:cs.LG stat.ML published:2013-05-11 summary:In this paper, we study the generalization properties of online learningbased stochastic methods for supervised learning problems where the lossfunction is dependent on more than one training sample (e.g., metric learning,ranking). We present a generic decoupling technique that enables us to provideRademacher complexity-based generalization error bounds. Our bounds are ingeneral tighter than those obtained by Wang et al (COLT 2012) for the sameproblem. Using our decoupling technique, we are further able to obtain fastconvergence rates for strongly convex pairwise loss functions. We are also ableto analyze a class of memory efficient online learning algorithms for pairwiselearning problems that use only a bounded subset of past training samples toupdate the hypothesis at each step. Finally, in order to complement ourgeneralization bounds, we propose a novel memory efficient online learningalgorithm for higher order learning problems with bounded regret guarantees.
arxiv-3000-148 | Geiringer Theorems: From Population Genetics to Computational Intelligence, Memory Evolutive Systems and Hebbian Learning | http://arxiv.org/pdf/1305.2504v1.pdf | author:Boris Mitavskiy, Elio Tuci, Chris Cannings, Chris Cannings, Jonathan Rowe, Jun He category:cs.NE published:2013-05-11 summary:The classical Geiringer theorem addresses the limiting frequency ofoccurrence of various alleles after repeated application of crossover. It hasbeen adopted to the setting of evolutionary algorithms and, a lot morerecently, reinforcement learning and Monte-Carlo tree search methodology tocope with a rather challenging question of action evaluation at the chancenodes. The theorem motivates novel dynamic parallel algorithms that areexplicitly described in the current paper for the first time. The algorithmsinvolve independent agents traversing a dynamically constructed directed graphthat possibly has loops. A rather elegant and profound category-theoretic modelof cognition in biological neural networks developed by a well-known Frenchmathematician, professor Andree Ehresmann jointly with a neurosurgeon, Jan PaulVanbremeersch over the last thirty years provides a hint at the connectionbetween such algorithms and Hebbian learning.
arxiv-3000-149 | Two SVDs produce more focal deep learning representations | http://arxiv.org/pdf/1301.3627v2.pdf | author:Hinrich Schuetze, Christian Scheible category:cs.CL cs.LG published:2013-01-16 summary:A key characteristic of work on deep learning and neural networks in generalis that it relies on representations of the input that support generalization,robust inference, domain adaptation and other desirable functionalities. Muchrecent progress in the field has focused on efficient and effective methods forcomputing representations. In this paper, we propose an alternative method thatis more efficient than prior work and produces representations that have aproperty we call focality -- a property we hypothesize to be important forneural network representations. The method consists of a simple application oftwo consecutive SVDs and is inspired by Anandkumar (2012).
arxiv-3000-150 | Affine Invariant Divergences associated with Composite Scores and its Applications | http://arxiv.org/pdf/1305.2473v1.pdf | author:Takafumi Kanamori, Hironori Fujisawa category:math.ST stat.ML stat.TH published:2013-05-11 summary:In statistical analysis, measuring a score of predictive performance is animportant task. In many scientific fields, appropriate scores were tailored totackle the problems at hand. A proper score is a popular tool to obtainstatistically consistent forecasts. Furthermore, a mathematicalcharacterization of the proper score was studied. As a result, it was revealedthat the proper score corresponds to a Bregman divergence, which is anextension of the squared distance over the set of probability distributions. Inthe present paper, we introduce composite scores as an extension of the typicalscores in order to obtain a wider class of probabilistic forecasting. Then, wepropose a class of composite scores, named Holder scores, that induceequivariant estimators. The equivariant estimators have a favorable property,implying that the estimator is transformed in a consistent way, when the datais transformed. In particular, we deal with the affine transformation of thedata. By using the equivariant estimators under the affine transformation, onecan obtain estimators that do no essentially depend on the choice of the systemof units in the measurement. Conversely, we prove that the Holder score ischaracterized by the invariance property under the affine transformations.Furthermore, we investigate statistical properties of the estimators usingHolder scores for the statistical problems including estimation of regressionfunctions and robust parameter estimation, and illustrate the usefulness of thenewly introduced scores for statistical forecasting.
arxiv-3000-151 | Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation | http://arxiv.org/pdf/1305.2452v1.pdf | author:James Foulds, Levi Boyles, Christopher Dubois, Padhraic Smyth, Max Welling category:cs.LG published:2013-05-10 summary:In the internet era there has been an explosion in the amount of digital textinformation available, leading to difficulties of scale for traditionalinference algorithms for topic models. Recent advances in stochasticvariational inference algorithms for latent Dirichlet allocation (LDA) havemade it feasible to learn topic models on large-scale corpora, but thesemethods do not currently take full advantage of the collapsed representation ofthe model. We propose a stochastic algorithm for collapsed variational Bayesianinference for LDA, which is simpler and more efficient than the state of theart method. We show connections between collapsed variational Bayesianinference and MAP estimation for LDA, and leverage these connections to proveconvergence properties of the proposed algorithm. In experiments on large-scaletext corpora, the algorithm was found to converge faster and often to a bettersolution than the previous method. Human-subject experiments also demonstratedthat the method can learn coherent topics in seconds on small corpora,facilitating the use of topic models in interactive document analysis software.
arxiv-3000-152 | MARFCAT: Transitioning to Binary and Larger Data Sets of SATE IV | http://arxiv.org/pdf/1207.3718v2.pdf | author:Serguei A. Mokhov, Joey Paquet, Mourad Debbabi, Yankui Sun category:cs.CR cs.PL cs.SE stat.ML K.6.5; D.3 published:2012-07-16 summary:We present a second iteration of a machine learning approach to static codeanalysis and fingerprinting for weaknesses related to security, softwareengineering, and others using the open-source MARF framework and the MARFCATapplication based on it for the NIST's SATE IV static analysis tool expositionworkshop's data sets that include additional test cases, including new largesynthetic cases. To aid detection of weak or vulnerable code, including sourceor binary on different platforms the machine learning approach proved to befast and accurate to for such tasks where other tools are either much slower orhave much smaller recall of known vulnerabilities. We use signal and NLPprocessing techniques in our approach to accomplish the identification andclassification tasks. MARFCAT's design from the beginning in 2010 made isindependent of the language being analyzed, source code, bytecode, or binary.In this follow up work with explore some preliminary results in this area. Weevaluated also additional algorithms that were used to process the data.
arxiv-3000-153 | Shape Reconstruction and Recognition with Isolated Non-directional Cues | http://arxiv.org/pdf/1305.2395v1.pdf | author:Toshiro Kubota, Jessica Ranck, Briley Acker, Herman De Haan category:cs.CV published:2013-05-10 summary:The paper investigates a hypothesis that our visual system groups visual cuesbased on how they form a surface, or more specifically triangulation derivedfrom the visual cues. To test our hypothesis, we compare shape recognition withthree different representations of visual cues: a set of isolated dotsdelineating the outline of the shape, a set of triangles obtained from Delaunaytriangulation of the set of dots, and a subset of Delaunay triangles excludingthose outside of the shape. Each participant was assigned to one particularrepresentation type and increased the number of dots (and consequentiallytriangles) until the underlying shape could be identified. We compare theaverage number of dots needed for identification among three types ofrepresentations. Our hypothesis predicts that the results from the threerepresentations will be similar. However, they show statistically significantdifferences. The paper also presents triangulation based algorithms forreconstruction and recognition of a shape from a set of isolated dots.Experiments showed that the algorithms were more effective and perceptuallyagreeable than similar contour based ones. From these experiments, we concludethat triangulation does affect our shape recognition. However, the surfacebased approach presents a number of computational advantages over the contourbased one and should be studied further.
arxiv-3000-154 | Revisiting Bayesian Blind Deconvolution | http://arxiv.org/pdf/1305.2362v1.pdf | author:David Wipf, Haichao Zhang category:cs.CV cs.LG stat.ML published:2013-05-10 summary:Blind deconvolution involves the estimation of a sharp signal or image givenonly a blurry observation. Because this problem is fundamentally ill-posed,strong priors on both the sharp image and blur kernel are required toregularize the solution space. While this naturally leads to a standard MAPestimation framework, performance is compromised by unknown trade-off parametersettings, optimization heuristics, and convergence issues stemming fromnon-convexity and/or poor prior selections. To mitigate some of these problems,a number of authors have recently proposed substituting a variational Bayesian(VB) strategy that marginalizes over the high-dimensional image space leadingto better estimates of the blur kernel. However, the underlying cost functionnow involves both integrals with no closed-form solution and complex,function-valued arguments, thus losing the transparency of MAP. Beyond standardBayesian-inspired intuitions, it thus remains unclear by exactly what mechanismthese methods are able to operate, rendering understanding, improvements andextensions more difficult. To elucidate these issues, we demonstrate that theVB methodology can be recast as an unconventional MAP problem with a veryparticular penalty/prior that couples the image, blur kernel, and noise levelin a principled way. This unique penalty has a number of useful characteristicspertaining to relative concavity, local minima avoidance, and scale-invariancethat allow us to rigorously explain the success of VB including its existingimplementational heuristics and approximations. It also provides strictcriteria for choosing the optimal image prior that, perhapscounter-intuitively, need not reflect the statistics of natural scenes. In sodoing we challenge the prevailing notion of why VB is successful for blinddeconvolution while providing a transparent platform for introducingenhancements.
arxiv-3000-155 | Novel Analysis of Population Scalability in Evolutionary Algorithms | http://arxiv.org/pdf/1108.4531v4.pdf | author:Jun He, Tianshi Chen, Boris Mitavskiy category:cs.NE published:2011-08-23 summary:Population-based evolutionary algorithms (EAs) have been widely applied tosolve various optimization problems. The question of how the performance of apopulation-based EA depends on the population size arises naturally. Theperformance of an EA may be evaluated by different measures, such as theaverage convergence rate to the optimal set per generation or the expectednumber of generations to encounter an optimal solution for the first time.Population scalability is the performance ratio between a benchmark EA andanother EA using identical genetic operators but a larger population size.Although intuitively the performance of an EA may improve if its populationsize increases, currently there exist only a few case studies for simplefitness functions. This paper aims at providing a general study for discreteoptimisation. A novel approach is introduced to analyse population scalabilityusing the fundamental matrix. The following two contributions summarize themajor results of the current article. (1) We demonstrate rigorously that forelitist EAs with identical global mutation, using a lager population sizealways increases the average rate of convergence to the optimal set; and yet,sometimes, the expected number of generations needed to find an optimalsolution (measured by either the maximal value or the average value) mayincrease, rather than decrease. (2) We establish sufficient and/or necessaryconditions for the superlinear scalability, that is, when the averageconvergence rate of a $(\mu+\mu)$ EA (where $\mu\ge2$) is bigger than $\mu$times that of a $(1+1)$ EA.
arxiv-3000-156 | Human Mood Detection For Human Computer Interaction | http://arxiv.org/pdf/1305.2827v1.pdf | author:Preeti Badar, Urmila Shrawankar category:cs.CV published:2013-05-10 summary:In this paper we propose an easiest approach for facial expressionrecognition. Here we are using concept of SVM for Expression Classification.Main problem is sub divided in three main modules. First one is Face detectionin which we are using skin filter and Face segmentation. We are given morestress on feature Extraction. This method is effective enough for applicationwhere fast execution is required. Second, Facial Feature Extraction which isessential part for expression recognition. In this module we used EdgeProjection Analysis. Finally extracted features vector is passed towards SVMclassifier for Expression Recognition. We are considering six basic Expressions(Anger, Fear, Disgust, Joy, Sadness, and Surprise)
arxiv-3000-157 | Image Optimization and Prediction | http://arxiv.org/pdf/1305.2828v1.pdf | author:Shweta Jain, Urmila Shrawankar category:cs.CV published:2013-05-10 summary:Image Processing, Optimization and Prediction of an Image play a key role inComputer Science. Image processing provides a way to analyze and identify animage .Many areas like medical image processing, Satellite images, naturalimages and artificial images requires lots of analysis and research onoptimization. In Image Optimization and Prediction we are combining thefeatures of Query Optimization, Image Processing and Prediction . Imageoptimization is used in Pattern analysis, object recognition, in medical Imageprocessing to predict the type of diseases, in satellite images for predictingweather forecast, availability of water or mineral etc. Image Processing,Optimization and analysis is a wide open area for research .Lots of researchhas been conducted in the area of Image analysis and many techniques areavailable for image analysis but, a single technique is not yet identified forimage analysis and prediction .our research is focused on identifying a globaltechnique for image analysis and Prediction.
arxiv-3000-158 | Performance Enhancement of Distributed Quasi Steady-State Genetic Algorithm | http://arxiv.org/pdf/1305.2830v1.pdf | author:Rahila Patel, Urmila Shrawankar, MM. Raghuwanshi, Anil N. Jaiswal category:cs.NE published:2013-05-10 summary:This paper proposes a new scheme for performance enhancement of distributedgenetic algorithm (DGA). Initial population is divided in two classes i.e.female and male. Simple distance based clustering is used for cluster formationaround females. For reclustering self-adaptive K-means is used, which produceswell distributed and well separated clusters. The self-adaptive K-means usedfor reclustering automatically locates initial position of centroids and numberof clusters. Four plans of co-evolution are applied on these clustersindependently. Clusters evolve separately. Merging of clusters takes placedepending on their performance. For experimentation unimodal and multimodaltest functions have been used. Test result show that the new scheme ofdistribution of population has given better performance.
arxiv-3000-159 | Beyond Physical Connections: Tree Models in Human Pose Estimation | http://arxiv.org/pdf/1305.2269v1.pdf | author:Fang Wang, Yi Li category:cs.CV published:2013-05-10 summary:Simple tree models for articulated objects prevails in the last decade.However, it is also believed that these simple tree models are not capable ofcapturing large variations in many scenarios, such as human pose estimation.This paper attempts to address three questions: 1) are simple tree modelssufficient? more specifically, 2) how to use tree models effectively in humanpose estimation? and 3) how shall we use combined parts together with singleparts efficiently? Assuming we have a set of single parts and combined parts, and the goal is toestimate a joint distribution of their locations. We surprisingly find that nolatent variables are introduced in the Leeds Sport Dataset (LSP) duringlearning latent trees for deformable model, which aims at approximating thejoint distributions of body part locations using minimal tree structure. Thissuggests one can straightforwardly use a mixed representation of single andcombined parts to approximate their joint distribution in a simple tree model.As such, one only needs to build Visual Categories of the combined parts, andthen perform inference on the learned latent tree. Our method outperformed thestate of the art on the LSP, both in the scenarios when the training images arefrom the same dataset and from the PARSE dataset. Experiments on animal imagesfrom the VOC challenge further support our findings.
arxiv-3000-160 | Efficient Stochastic Gradient Descent for Strongly Convex Optimization | http://arxiv.org/pdf/1304.5504v5.pdf | author:Tianbao Yang, Lijun Zhang category:cs.LG stat.ML published:2013-04-19 summary:We motivate this study from a recent work on a stochastic gradient descent(SGD) method with only one projection \citep{DBLP:conf/nips/MahdaviYJZY12},which aims at alleviating the computational bottleneck of the standard SGDmethod in performing the projection at each iteration, and enjoys an $O(\logT/T)$ convergence rate for strongly convex optimization. In this paper, we makefurther contributions along the line. First, we develop an epoch-projection SGDmethod that only makes a constant number of projections less than $\log_2T$ butachieves an optimal convergence rate $O(1/T)$ for {\it strongly convexoptimization}. Second, we present a proximal extension to utilize the structureof the objective function that could further speed-up the computation andconvergence for sparse regularized loss minimization problems. Finally, weconsider an application of the proposed techniques to solving the highdimensional large margin nearest neighbor classification problem, yielding aspeed-up of orders of magnitude.
arxiv-3000-161 | Multi-q Pattern Classification of Polarization Curves | http://arxiv.org/pdf/1305.2876v1.pdf | author:Ricardo Fabbri, Ivan N. Bastos, Francisco D. Moura Neto, Francisco J. P. Lopes, Wesley N. Goncalves, Odemir M. Bruno category:cs.CE cs.CV published:2013-05-10 summary:Several experimental measurements are expressed in the form ofone-dimensional profiles, for which there is a scarcity of methodologies ableto classify the pertinence of a given result to a specific group. Thepolarization curves that evaluate the corrosion kinetics of electrodes incorrosive media are an application where the behavior is chiefly analyzed fromprofiles. Polarization curves are indeed a classic method to determine theglobal kinetics of metallic electrodes, but the strong nonlinearity fromdifferent metals and alloys can overlap and the discrimination becomes achallenging problem. Moreover, even finding a typical curve from replicatedtests requires subjective judgement. In this paper we used the so-calledmulti-q approach based on the Tsallis statistics in a classification engine toseparate multiple polarization curve profiles of two stainless steels. Wecollected 48 experimental polarization curves in aqueous chloride medium of twostainless steel types, with different resistance against localized corrosion.Multi-q pattern analysis was then carried out on a wide potential range, fromcathodic up to anodic regions. An excellent classification rate was obtained,at a success rate of 90%, 80%, and 83% for low (cathodic), high (anodic), andboth potential ranges, respectively, using only 2% of the original profiledata. These results show the potential of the proposed approach towardsefficient, robust, systematic and automatic classification of highly non-linearprofile curves.
arxiv-3000-162 | Multivariate Regression with Calibration | http://arxiv.org/pdf/1305.2238v1.pdf | author:Han Liu, Lie Wang, Tuo Zhao category:stat.ML published:2013-05-10 summary:We propose a new method named calibrated multivariate regression (CMR) forfitting high dimensional multivariate regression models. Compared to existingmethods, CMR calibrates the regularization for each regression task withrespect to its noise level so that it is simultaneously tuning insensitive andachieves an improved finite sample performance. Computationally, we develop anefficient smoothed proximal gradient algorithm with a worst-case numerical rateof convergence $O(1/\epsilon)$, where $\epsilon$ is a pre-specified accuracy.Theoretically, we prove that CMR achieves the optimal rate of convergence inparameter estimation. We illustrate the usefulness of CMR by thorough numericalsimulations and show that CMR consistently outperforms existing multivariateregression methods. We also apply CMR on a brain activity prediction problemand find that CMR even outperforms the handcrafted models created by humanexperts.
arxiv-3000-163 | Joint Topic Modeling and Factor Analysis of Textual Information and Graded Response Data | http://arxiv.org/pdf/1305.1956v2.pdf | author:Andrew S. Lan, Christoph Studer, Andrew E. Waters, Richard G. Baraniuk category:stat.ML cs.LG published:2013-05-08 summary:Modern machine learning methods are critical to the development oflarge-scale personalized learning systems that cater directly to the needs ofindividual learners. The recently developed SPARse Factor Analysis (SPARFA)framework provides a new statistical model and algorithms for machinelearning-based learning analytics, which estimate a learner's knowledge of thelatent concepts underlying a domain, and content analytics, which estimate therelationships among a collection of questions and the latent concepts. SPARFAestimates these quantities given only the binary-valued graded responses to acollection of questions. In order to better interpret the estimated latentconcepts, SPARFA relies on a post-processing step that utilizes user-definedtags (e.g., topics or keywords) available for each question. In this paper, werelax the need for user-defined tags by extending SPARFA to jointly processboth graded learner responses and the text of each question and its associatedanswer(s) or other feedback. Our purely data-driven approach (i) enhances theinterpretability of the estimated latent concepts without the need ofexplicitly generating a set of tags or performing a post-processing step, (ii)improves the prediction performance of SPARFA, and (iii) scales to largetest/assessments where human annotation would prove burdensome. We demonstratethe efficacy of the proposed approach on two real educational datasets.
arxiv-3000-164 | Repairing and Inpainting Damaged Images using Diffusion Tensor | http://arxiv.org/pdf/1305.2221v1.pdf | author:Faouzi Benzarti, Hamid Amiri category:cs.CV published:2013-05-09 summary:Removing or repairing the imperfections of a digital images or videos is avery active and attractive field of research belonging to the image inpaintingtechnique. This later has a wide range of applications, such as removingscratches in old photographic image, removing text and logos or creatingcartoon and artistic effects. In this paper, we propose an efficient method torepair a damaged image based on a non linear diffusion tensor. The idea is totrack perfectly the local geometry of the damaged image and allowing diffusiononly in the isophotes curves direction. To illustrate the effective performanceof our method, we present some experimental results on test and realphotographic color images
arxiv-3000-165 | Stochastic gradient descent algorithms for strongly convex functions at O(1/T) convergence rates | http://arxiv.org/pdf/1305.2218v1.pdf | author:Shenghuo Zhu category:cs.LG cs.AI published:2013-05-09 summary:With a weighting scheme proportional to t, a traditional stochastic gradientdescent (SGD) algorithm achieves a high probability convergence rate ofO({\kappa}/T) for strongly convex functions, instead of O({\kappa} ln(T)/T). Wealso prove that an accelerated SGD algorithm also achieves a rate ofO({\kappa}/T).
arxiv-3000-166 | Generalized Bregman Divergence and Gradient of Mutual Information for Vector Poisson Channels | http://arxiv.org/pdf/1301.6648v3.pdf | author:Liming Wang, Miguel Rodrigues, Lawrence Carin category:cs.IT math.IT stat.ML published:2013-01-28 summary:We investigate connections between information-theoretic andestimation-theoretic quantities in vector Poisson channel models. Inparticular, we generalize the gradient of mutual information with respect tokey system parameters from the scalar to the vector Poisson channel model. Wealso propose, as another contribution, a generalization of the classicalBregman divergence that offers a means to encapsulate under a unifyingframework the gradient of mutual information results for scalar and vectorPoisson and Gaussian channel models. The so-called generalized Bregmandivergence is also shown to exhibit various properties akin to the propertiesof the classical version. The vector Poisson channel model is drawingconsiderable attention in view of its application in various domains: as anexample, the availability of the gradient of mutual information can be used inconjunction with gradient descent methods to effect compressive-sensingprojection designs in emerging X-ray and document classification applications.
arxiv-3000-167 | A Rank Minrelation - Majrelation Coefficient | http://arxiv.org/pdf/1305.2038v1.pdf | author:Patrick E. Meyer category:stat.ML cs.AI published:2013-05-09 summary:Improving the detection of relevant variables using a new bivariate measurecould importantly impact variable selection and large network inferencemethods. In this paper, we propose a new statistical coefficient that we callthe rank minrelation coefficient. We define a minrelation of X to Y (orequivalently a majrelation of Y to X) as a measure that estimate p(Y > X) whenX and Y are continuous random variables. The approach is similar to Lin'sconcordance coefficient that rather focuses on estimating p(X = Y). In otherwords, if a variable X exhibits a minrelation to Y then, as X increases, Y islikely to increases too. However, on the contrary to concordance orcorrelation, the minrelation is not symmetric. More explicitly, if X decreases,little can be said on Y values (except that the uncertainty on Y actuallyincreases). In this paper, we formally define this new kind of bivariatedependencies and propose a new statistical coefficient in order to detect thosedependencies. We show through several key examples that this new coefficienthas many interesting properties in order to select relevant variables, inparticular when compared to correlation.
arxiv-3000-168 | Automatic Speech Recognition Using Template Model for Man-Machine Interface | http://arxiv.org/pdf/1305.2959v1.pdf | author:Neema Mishra, Urmila Shrawankar, V M Thakare category:cs.SD cs.CL published:2013-05-09 summary:Speech is a natural form of communication for human beings, and computerswith the ability to understand speech and speak with a human voice are expectedto contribute to the development of more natural man-machine interfaces.Computers with this kind of ability are gradually becoming a reality, throughthe evolution of speech recognition technologies. Speech is being an importantmode of interaction with computers. In this paper Feature extraction isimplemented using well-known Mel-Frequency Cepstral Coefficients (MFCC).Patternmatching is done using Dynamic time warping (DTW) algorithm.
arxiv-3000-169 | An Overview of Hindi Speech Recognition | http://arxiv.org/pdf/1305.2847v1.pdf | author:Neema Mishra, Urmila Shrawankar, V M Thakare category:cs.CL cs.SD published:2013-05-09 summary:In this age of information technology, information access in a convenientmanner has gained importance. Since speech is a primary mode of communicationamong human beings, it is natural for people to expect to be able to carry outspoken dialogue with computer. Speech recognition system permits ordinarypeople to speak to the computer to retrieve information. It is desirable tohave a human computer dialogue in local language. Hindi being the most widelyspoken Language in India is the natural primary human language candidate forhuman machine interaction. There are five pairs of vowels in Hindi languages;one member is longer than the other one. This paper describes an overview ofspeech recognition system that includes how speech is produced and theproperties and characteristics of Hindi Phoneme.
arxiv-3000-170 | Opportunities & Challenges In Automatic Speech Recognition | http://arxiv.org/pdf/1305.2846v1.pdf | author:Rashmi Makhijani, Urmila Shrawankar, V M Thakare category:cs.CL cs.SD published:2013-05-09 summary:Automatic speech recognition enables a wide range of current and emergingapplications such as automatic transcription, multimedia content analysis, andnatural human-computer interfaces. This paper provides a glimpse of theopportunities and challenges that parallelism provides for automatic speechrecognition and related application research from the point of view of speechresearchers. The increasing parallelism in computing platforms opens threemajor possibilities for speech recognition systems: improving recognitionaccuracy in non-ideal, everyday noisy environments; increasing recognitionthroughput in batch processing of speech data; and reducing recognition latencyin realtime usage scenarios. This paper describes technical challenges,approaches taken, and possible directions for future research to guide thedesign of efficient parallel software and hardware infrastructures.
arxiv-3000-171 | Speech Enhancement Using Pitch Detection Approach For Noisy Environment | http://arxiv.org/pdf/1305.2352v1.pdf | author:Rashmi Makhijani, Urmila Shrawankar, V M Thakare category:cs.SD cs.CL published:2013-05-09 summary:Acoustical mismatch among training and testing phases degrades outstandinglyspeech recognition results. This problem has limited the development ofreal-world nonspecific applications, as testing conditions are highly variantor even unpredictable during the training process. Therefore the backgroundnoise has to be removed from the noisy speech signal to increase the signalintelligibility and to reduce the listener fatigue. Enhancement techniquesapplied, as pre-processing stages; to the systems remarkably improverecognition results. In this paper, a novel approach is used to enhance theperceived quality of the speech signal when the additive noise cannot bedirectly controlled. Instead of controlling the background noise, we propose toreinforce the speech signal so that it can be heard more clearly in noisyenvironments. The subjective evaluation shows that the proposed method improvesperceptual quality of speech in various noisy environments. As in some casesspeaking may be more convenient than typing, even for rapid typists: manymathematical symbols are missing from the keyboard but can be easily spoken andrecognized. Therefore, the proposed system can be used in an applicationdesigned for mathematical symbol recognition (especially symbols not availableon the keyboard) in schools.
arxiv-3000-172 | Inferring Team Strengths Using a Discrete Markov Random Field | http://arxiv.org/pdf/1305.1998v1.pdf | author:John Zech, Frank Wood category:stat.ML published:2013-05-09 summary:We propose an original model for inferring team strengths using a MarkovRandom Field, which can be used to generate historical estimates of theoffensive and defensive strengths of a team over time. This model was designedto be applied to sports such as soccer or hockey, in which contest outcomestake value in a limited discrete space. We perform inference using acombination of Expectation Maximization and Loopy Belief Propagation. Thechallenges of working with a non-convex optimization problem and ahigh-dimensional parameter space are discussed. The performance of the model isdemonstrated on professional soccer data from the English Premier League.
arxiv-3000-173 | Kernelized Bayesian Matrix Factorization | http://arxiv.org/pdf/1211.1275v3.pdf | author:Mehmet Gönen, Suleiman A. Khan, Samuel Kaski category:stat.ML published:2012-11-06 summary:We extend kernelized matrix factorization with a fully Bayesian treatment andwith an ability to work with multiple side information sources expressed asdifferent kernels. Kernel functions have been introduced to matrixfactorization to integrate side information about the rows and columns (e.g.,objects and users in recommender systems), which is necessary for makingout-of-matrix (i.e., cold start) predictions. We discuss specifically bipartitegraph inference, where the output matrix is binary, but extensions to moregeneral matrices are straightforward. We extend the state of the art in two keyaspects: (i) A fully conjugate probabilistic formulation of the kernelizedmatrix factorization problem enables an efficient variational approximation,whereas fully Bayesian treatments are not computationally feasible in theearlier approaches. (ii) Multiple side information sources are included,treated as different kernels in multiple kernel learning that additionallyreveals which side information sources are informative. Our method outperformsalternatives in predicting drug-protein interactions on two data sets. We thenshow that our framework can also be used for solving multilabel learningproblems by considering samples and labels as the two domains where matrixfactorization operates on. Our algorithm obtains the lowest Hamming loss valueson 10 out of 14 multilabel classification data sets compared to fivestate-of-the-art multilabel learning algorithms.
arxiv-3000-174 | The Dynamically Extended Mind -- A Minimal Modeling Case Study | http://arxiv.org/pdf/1305.1958v1.pdf | author:Tom Froese, Carlos Gershenson, David A. Rosenblueth category:cs.AI cs.NE nlin.CD I.2.0 published:2013-05-08 summary:The extended mind hypothesis has stimulated much interest in cognitivescience. However, its core claim, i.e. that the process of cognition can extendbeyond the brain via the body and into the environment, has been heavilycriticized. A prominent critique of this claim holds that when some part of theworld is coupled to a cognitive system this does not necessarily entail thatthe part is also constitutive of that cognitive system. This critique is knownas the "coupling-constitution fallacy". In this paper we respond to thisreductionist challenge by using an evolutionary robotics approach to create aminimal model of two acoustically coupled agents. We demonstrate how theinteraction process as a whole has properties that cannot be reduced to thecontributions of the isolated agents. We also show that the neural dynamics ofthe coupled agents has formal properties that are inherently impossible forthose neural networks in isolation. By keeping the complexity of the model toan absolute minimum, we are able to illustrate how the coupling-constitutionfallacy is in fact based on an inadequate understanding of the constitutiverole of nonlinear interactions in dynamical systems theory.
arxiv-3000-175 | Consistency of Online Random Forests | http://arxiv.org/pdf/1302.4853v2.pdf | author:Misha Denil, David Matheson, Nando de Freitas category:stat.ML published:2013-02-20 summary:As a testament to their success, the theory of random forests has long beenoutpaced by their application in practice. In this paper, we take a steptowards narrowing this gap by providing a consistency result for online randomforests.
arxiv-3000-176 | Gaussian process regression as a predictive model for Quality-of-Service in Web service systems | http://arxiv.org/pdf/1207.6910v2.pdf | author:Jakub M. Tomczak, Jerzy Swiatek, Krzysztof Latawiec category:cs.NI cs.LG published:2012-07-30 summary:In this paper, we present the Gaussian process regression as the predictivemodel for Quality-of-Service (QoS) attributes in Web service systems. The goalis to predict performance of the execution system expressed as QoS attributesgiven existing execution system, service repository, and inputs, e.g., streamsof requests. In order to evaluate the performance of Gaussian processregression the simulation environment was developed. Two quality indexes wereused, namely, Mean Absolute Error and Mean Squared Error. The results obtainedwithin the experiment show that the Gaussian process performed the best withlinear kernel and statistically significantly better comparing toClassification and Regression Trees (CART) method.
arxiv-3000-177 | Standard Fingerprint Databases: Manual Minutiae Labeling and Matcher Performance Analyses | http://arxiv.org/pdf/1305.1443v2.pdf | author:Mehmet Kayaoglu, Berkay Topcu, Umut Uludag category:cs.CV published:2013-05-07 summary:Fingerprint verification and identification algorithms based on minutiaefeatures are used in many biometric systems today (e.g., governmental e-IDprograms, border control, AFIS, personal authentication for portable devices).Researchers in industry/academia are now able to utilize many publiclyavailable fingerprint databases (e.g., Fingerprint Verification Competition(FVC) & NIST databases) to compare/evaluate their feature extraction and/ormatching algorithm performances against those of others. The results from theseevaluations are typically utilized by decision makers responsible forimplementing the cited biometric systems, in selecting/tuning specific sensors,feature extractors and matchers. In this study, for a subset of the citedpublic fingerprint databases, we report fingerprint minutiae matching results,which are based on (i) minutiae extracted automatically from fingerprintimages, and (ii) minutiae extracted manually by human subjects. By doing so, weare able to (i) quantitatively judge the performance differences between thesetwo cases, (ii) elaborate on performance upper bounds of minutiae matching,utilizing what can be termed as "ground truth" minutiae features, (iii) analyzeminutiae matching performance, without coupling it with the minutiae extractionperformance beforehand. Further, as we will freely distribute the minutiaetemplates, originating from this manual labeling study, in a standard minutiaetemplate exchange format (ISO 19794-2), we believe that other researchers inthe biometrics community will be able to utilize the associated results &templates to create their own evaluations pertaining to their fingerprintminutiae extractors/matchers.
arxiv-3000-178 | Speech: A Challenge to Digital Signal Processing Technology for Human-to-Computer Interaction | http://arxiv.org/pdf/1305.1925v1.pdf | author:Urmila Shrawankar, Anjali Mahajan category:cs.HC cs.CL published:2013-05-08 summary:This software project based paper is for a vision of the near future in whichcomputer interaction is characterized by natural face-to-face conversationswith lifelike characters that speak, emote, and gesture. The first step isspeech. The dream of a true virtual reality, a complete human-computerinteraction system will not come true unless we try to give some perception tomachine and make it perceive the outside world as humans communicate with eachother. This software project is under development for listening and replyingmachine (Computer) through speech. The Speech interface is developed to convertspeech input into some parametric form (Speech-to-Text) for further processingand the results, text output to speech synthesis (Text-to-Speech)
arxiv-3000-179 | Class Imbalance Problem in Data Mining Review | http://arxiv.org/pdf/1305.1707v1.pdf | author:Rushi Longadge, Snehalata Dongre category:cs.LG published:2013-05-08 summary:In last few years there are major changes and evolution has been done onclassification of data. As the application area of technology is increases thesize of data also increases. Classification of data becomes difficult becauseof unbounded size and imbalance nature of data. Class imbalance problem becomegreatest issue in data mining. Imbalance problem occur where one of the twoclasses having more sample than other classes. The most of algorithm are morefocusing on classification of major sample while ignoring or misclassifyingminority sample. The minority samples are those that rarely occur but veryimportant. There are different methods available for classification ofimbalance data set which is divided into three main categories, the algorithmicapproach, data-preprocessing approach and feature selection approach. Each ofthis technique has their own advantages and disadvantages. In this papersystematic study of each approach is define which gives the right direction forresearch in class imbalance problem.
arxiv-3000-180 | The Extended Parameter Filter | http://arxiv.org/pdf/1305.1704v1.pdf | author:Yusuf Erol, Lei Li, Bharath Ramsundar, Stuart J. Russell category:stat.ML cs.AI published:2013-05-08 summary:The parameters of temporal models, such as dynamic Bayesian networks, may bemodelled in a Bayesian context as static or atemporal variables that influencetransition probabilities at every time step. Particle filters fail for modelsthat include such variables, while methods that use Gibbs sampling of parametervariables may incur a per-sample cost that grows linearly with the length ofthe observation sequence. Storvik devised a method for incremental computationof exact sufficient statistics that, for some cases, reduces the per-samplecost to a constant. In this paper, we demonstrate a connection betweenStorvik's filter and a Kalman filter in parameter space and establish moregeneral conditions under which Storvik's filter works. Drawing on an analogy tothe extended Kalman filter, we develop and analyze, both theoretically andexperimentally, a Taylor approximation to the parameter posterior that allowsStorvik's method to be applied to a broader class of models. Our experiments onboth synthetic examples and real applications show improvement over existingmethods.
arxiv-3000-181 | Automatic Segmentation of Fluorescence Lifetime Microscopy Images of Cells Using Multi-Resolution Community Detection | http://arxiv.org/pdf/1208.4662v2.pdf | author:Dandan Hu, Pinaki Sarder, Peter Ronhovde, Sandra Orthaus, Samuel Achilefu, Zohar Nussinov category:physics.med-ph cs.CV published:2012-08-23 summary:We have developed an automatic method for segmenting fluorescence lifetime(FLT) imaging microscopy (FLIM) images of cells inspired by a multi-resolutioncommunity detection (MCD) based network segmentation method. The imageprocessing problem is framed as identifying segments with respective averageFLTs against a background in FLIM images. The proposed method segments a FLIMimage for a given resolution of the network composed using image pixels as thenodes and similarity between the pixels as the edges. In the resultingsegmentation, low network resolution leads to larger segments and high networkresolution leads to smaller segments. Further, the mean-square error (MSE) inestimating the FLT segments in a FLIM image using the proposed method was foundto be consistently decreasing with increasing resolution of the correspondingnetwork. The proposed MCD method outperformed a popular spectral clusteringbased method in performing FLIM image segmentation. The spectral segmentationmethod introduced noisy segments in its output at high resolution. It wasunable to offer a consistent decrease in MSE with increasing resolution.
arxiv-3000-182 | Minimizing inter-subject variability in fNIRS based Brain Computer Interfaces via multiple-kernel support vector learning | http://arxiv.org/pdf/1209.5467v4.pdf | author:Berdakh Abibullaev, Jinung An, Seung-Hyun Lee, Sang-Hyeon Jin, Jeon-Il Moon category:stat.ML cs.LG published:2012-09-25 summary:Brain signal variability in the measurements obtained from different subjectsduring different sessions significantly deteriorates the accuracy of mostbrain-computer interface (BCI) systems. Moreover these variabilities, alsoknown as inter-subject or inter-session variabilities, require lengthycalibration sessions before the BCI system can be used. Furthermore, thecalibration session has to be repeated for each subject independently andbefore use of the BCI due to the inter-session variability. In this study, wepresent an algorithm in order to minimize the above-mentioned variabilities andto overcome the time-consuming and usually error-prone calibration time. Ouralgorithm is based on linear programming support-vector machines and theirextensions to a multiple kernel learning framework. We tackle the inter-subjector -session variability in the feature spaces of the classifiers. This is doneby incorporating each subject- or session-specific feature spaces into muchricher feature spaces with a set of optimal decision boundaries. Each decisionboundary represents the subject- or a session specific spatio-temporalvariabilities of neural signals. Consequently, a single classifier withmultiple feature spaces will generalize well to new unseen test patterns evenwithout the calibration steps. We demonstrate that classifiers maintain goodperformances even under the presence of a large degree of BCI variability. Thepresent study analyzes BCI variability related to oxy-hemoglobin neural signalsmeasured using a functional near-infrared spectroscopy.
arxiv-3000-183 | High Level Pattern Classification via Tourist Walks in Networks | http://arxiv.org/pdf/1305.1679v1.pdf | author:Thiago Christiano Silva, Liang Zhao category:cs.AI cs.LG published:2013-05-07 summary:Complex networks refer to large-scale graphs with nontrivial connectionpatterns. The salient and interesting features that the complex network studyoffer in comparison to graph theory are the emphasis on the dynamicalproperties of the networks and the ability of inherently uncovering patternformation of the vertices. In this paper, we present a hybrid dataclassification technique combining a low level and a high level classifier. Thelow level term can be equipped with any traditional classification techniques,which realize the classification task considering only physical features (e.g.,geometrical or statistical features) of the input data. On the other hand, thehigh level term has the ability of detecting data patterns with semanticmeanings. In this way, the classification is realized by means of theextraction of the underlying network's features constructed from the inputdata. As a result, the high level classification process measures thecompliance of the test instances with the pattern formation of the trainingdata. Out of various high level perspectives that can be utilized to capturesemantic meaning, we utilize the dynamical features that are generated from atourist walker in a networked environment. Specifically, a weighted combinationof transient and cycle lengths generated by the tourist walk is employed forthat end. Interestingly, our study shows that the proposed technique is able tofurther improve the already optimized performance of traditional classificationtechniques.
arxiv-3000-184 | Image Similarity Using Sparse Representation and Compression Distance | http://arxiv.org/pdf/1206.2627v2.pdf | author:Tanaya Guha, Rabab K. Ward category:cs.CV published:2012-06-12 summary:A new line of research uses compression methods to measure the similaritybetween signals. Two signals are considered similar if one can be compressedsignificantly when the information of the other is known. The existingcompression-based similarity methods, although successful in the discrete onedimensional domain, do not work well in the context of images. This paperproposes a sparse representation-based approach to encode the informationcontent of an image using information from the other image, and uses thecompactness (sparsity) of the representation as a measure of itscompressibility (how much can the image be compressed) with respect to theother image. The more sparse the representation of an image, the better it canbe compressed and the more it is similar to the other image. The efficacy ofthe proposed measure is demonstrated through the high accuracies achieved inimage clustering, retrieval and classification.
arxiv-3000-185 | A Method for Visuo-Spatial Classification of Freehand Shapes Freely Sketched | http://arxiv.org/pdf/1305.1520v1.pdf | author:Ney Renau-Ferrer, Céline Remi category:cs.CV published:2013-05-07 summary:We present the principle and the main steps of a new method for thevisuo-spatial analysis of geometrical sketches recorded online. Visuo-spatialanalysis is a necessary step for multi-level analysis. Multi-level analysissimultaneously allows classification, comparison or clustering of theconstituent parts of a pattern according to their visuo-spatial properties,their procedural strategies, their structural or temporal parameters, or anycombination of two or more of those parameters. The first results provided bythis method concern the comparison of sketches to some perfect patterns ofsimple geometrical figures and the measure of dissimilarity between realsketches. The mean rates of good decision higher than 95% obtained arepromising in both cases.
arxiv-3000-186 | EURETILE 2010-2012 summary: first three years of activity of the European Reference Tiled Experiment | http://arxiv.org/pdf/1305.1459v1.pdf | author:Pier Stanislao Paolucci, Iuliana Bacivarov, Gert Goossens, Rainer Leupers, Frédéric Rousseau, Christoph Schumacher, Lothar Thiele, Piero Vicini category:cs.DC cs.AR cs.NE cs.OS cs.PL published:2013-05-07 summary:This is the summary of first three years of activity of the EURETILE FP7project 247846. EURETILE investigates and implements brain-inspired andfault-tolerant foundational innovations to the system architecture of massivelyparallel tiled computer architectures and the corresponding programmingparadigm. The execution targets are a many-tile HW platform, and a many-tilesimulator. A set of SW process - HW tile mapping candidates is generated by theholistic SW tool-chain using a combination of analytic and bio-inspiredmethods. The Hardware dependent Software is then generated, providing OSservices with maximum efficiency/minimal overhead. The many-tile simulatorcollects profiling data, closing the loop of the SW tool chain. Fine-grainparallelism inside processes is exploited by optimized intra-tile compilationtechniques, but the project focus is above the level of the elementary tile.The elementary HW tile is a multi-processor, which includes a fault tolerantDistributed Network Processor (for inter-tile communication) and ASIPaccelerators. Furthermore, EURETILE investigates and implements the innovationsfor equipping the elementary HW tile with high-bandwidth, low-latencybrain-like inter-tile communication emulating 3 levels of connection hierarchy,namely neural columns, cortical areas and cortex, and develops a dedicatedcortical simulation benchmark: DPSNN-STDP (Distributed Polychronous SpikingNeural Net with synaptic Spiking Time Dependent Plasticity). EURETILE leverageson the multi-tile HW paradigm and SW tool-chain developed by the FET-ACA SHAPESIntegrated Project (2006-2009).
arxiv-3000-187 | Speech Enhancement Modeling Towards Robust Speech Recognition System | http://arxiv.org/pdf/1305.1426v1.pdf | author:Urmila Shrawankar, V. M. Thakare category:cs.SD cs.CL published:2013-05-07 summary:Form about four decades human beings have been dreaming of an intelligentmachine which can master the natural speech. In its simplest form, this machineshould consist of two subsystems, namely automatic speech recognition (ASR) andspeech understanding (SU). The goal of ASR is to transcribe natural speechwhile SU is to understand the meaning of the transcription. Recognizing andunderstanding a spoken sentence is obviously a knowledge-intensive process,which must take into account all variable information about the speechcommunication process, from acoustics to semantics and pragmatics. Whiledeveloping an Automatic Speech Recognition System, it is observed that someadverse conditions degrade the performance of the Speech Recognition System. Inthis contribution, speech enhancement system is introduced for enhancing speechsignals corrupted by additive noise and improving the performance of AutomaticSpeech Recognizers in noisy conditions. Automatic speech recognitionexperiments show that replacing noisy speech signals by the correspondingenhanced speech signals leads to an improvement in the recognition accuracies.The amount of improvement varies with the type of the corrupting noise.
arxiv-3000-188 | A Differential Equations Approach to Optimizing Regret Trade-offs | http://arxiv.org/pdf/1305.1359v1.pdf | author:Alexandr Andoni, Rina Panigrahy category:cs.LG published:2013-05-07 summary:We consider the classical question of predicting binary sequences and studythe {\em optimal} algorithms for obtaining the best possible regret and payofffunctions for this problem. The question turns out to be also equivalent to theproblem of optimal trade-offs between the regrets of two experts in an "expertsproblem", studied before by \cite{kearns-regret}. While, say, a regret of$\Theta(\sqrt{T})$ is known, we argue that it important to ask what is theprovably optimal algorithm for this problem --- both because it leads tonatural algorithms, as well as because regret is in fact often comparable inmagnitude to the final payoffs and hence is a non-negligible term. In the basic setting, the result essentially follows from a classical resultof Cover from '65. Here instead, we focus on another standard setting, oftime-discounted payoffs, where the final "stopping time" is not specified. Weexhibit an explicit characterization of the optimal regret for this setting. To obtain our main result, we show that the optimal payoff functions have tosatisfy the Hermite differential equation, and hence are given by the solutionsto this equation. It turns out that characterization of the payoff function isqualitatively different from the classical (non-discounted) setting, and,namely, there's essentially a unique optimal solution.
arxiv-3000-189 | Speckle Noise Reduction in Medical Ultrasound Images | http://arxiv.org/pdf/1305.1344v1.pdf | author:Faouzi Benzarti, Hamid Amiri category:cs.CV published:2013-05-06 summary:Ultrasound imaging is an incontestable vital tool for diagnosis, it providesin non-invasive manner the internal structure of the body to detect eventuallydiseases or abnormalities tissues. Unfortunately, the presence of speckle noisein these images affects edges and fine details which limit the contrastresolution and make diagnostic more difficult. In this paper, we propose adenoising approach which combines logarithmic transformation and a non lineardiffusion tensor. Since speckle noise is multiplicative and nonwhite process,the logarithmic transformation is a reasonable choice to convertsignaldependent or pure multiplicative noise to an additive one. The key ideafrom using diffusion tensor is to adapt the flow diffusion towards the localorientation by applying anisotropic diffusion along the coherent structuredirection of interesting features in the image. To illustrate the effectiveperformance of our algorithm, we present some experimental results onsynthetically and real echographic images.
arxiv-3000-190 | Towards an Author-Topic-Term-Model Visualization of 100 Years of German Sociological Society Proceedings | http://arxiv.org/pdf/1305.1343v1.pdf | author:Arnim Bleier, Andreas Strotmann category:cs.DL cs.CL cs.IR published:2013-05-06 summary:Author co-citation studies employ factor analysis to reduce high-dimensionalco-citation matrices to low-dimensional and possibly interpretable factors, butthese studies do not use any information from the text bodies of publications.We hypothesise that term frequencies may yield useful information forscientometric analysis. In our work we ask if word features in combination withBayesian analysis allow well-founded science mapping studies. This work goesback to the roots of Mosteller and Wallace's (1964) statistical text analysisusing word frequency features and a Bayesian inference approach, tough withdifferent goals. To answer our research question we (i) introduce a new dataset on which the experiments are carried out, (ii) describe the Bayesian modelemployed for inference and (iii) present first results of the analysis.
arxiv-3000-191 | New Alignment Methods for Discriminative Book Summarization | http://arxiv.org/pdf/1305.1319v1.pdf | author:David Bamman, Noah A. Smith category:cs.CL published:2013-05-06 summary:We consider the unsupervised alignment of the full text of a book with ahuman-written summary. This presents challenges not seen in other textalignment problems, including a disparity in length and, consequent to this, aviolation of the expectation that individual words and phrases should align,since large passages and chapters can be distilled into a single summaryphrase. We present two new methods, based on hidden Markov models, specificallytargeted to this problem, and demonstrate gains on an extractive booksummarization task. While there is still much room for improvement,unsupervised alignment holds intrinsic value in offering insight into whatfeatures of a book are deemed worthy of summarization.
arxiv-3000-192 | A Convex Functional for Image Denoising based on Patches with Constrained Overlaps and its vectorial application to Low Dose Differential Phase Tomography | http://arxiv.org/pdf/1305.1256v1.pdf | author:Alessandro Mirone, Emmanuel Brun, Paola Coan category:math.NA cs.CV published:2013-05-06 summary:We solve the image denoising problem with a dictionary learning technique bywriting a convex functional of a new form. This functional contains beside theusual sparsity inducing term and fidelity term, a new term which inducessimilarity between overlapping patches in the overlap regions. The functionaldepends on two free regularization parameters: a coefficient multiplying thesparsity-inducing $L_{1}$ norm of the patch basis functions coefficients, and acoefficient multiplying the $L_{2}$ norm of the differences between patches inthe overlapping regions. The solution is found by applying the iterativeproximal gradient descent method with FISTA acceleration. In the case oftomography reconstruction we calculate the gradient by applying projection ofthe solution and its error backprojection at each iterative step. We study thequality of the solution, as a function of the regularization parameters andnoise, on synthetic datas for which the solution is a-priori known. We applythe method on experimental data in the case of Differential Phase Tomography.For this case we use an original approach which consists in using vectorialpatches, each patch having two components: one per each gradient component. Theresulting algorithm, implemented in the ESRF tomography reconstruction codePyHST, results to be robust, efficient, and well adapted to strongly reduce therequired dose and the number of projections in medical tomography.
arxiv-3000-193 | A Contrario Selection of Optimal Partitions for Image Segmentation | http://arxiv.org/pdf/1305.1206v1.pdf | author:Juan Cardelino, Vicent Caselles, Marcelo Bertalmio, Gregory Randall category:cs.CV published:2013-05-06 summary:We present a novel segmentation algorithm based on a hierarchicalrepresentation of images. The main contribution of this work is to explore thecapabilities of the A Contrario reasoning when applied to the segmentationproblem, and to overcome the limitations of current algorithms within thatframework. This exploratory approach has three main goals. Our first goal is to extend the search space of greedy merging algorithms tothe set of all partitions spanned by a certain hierarchy, and to cast thesegmentation as a selection problem within this space. In this way we increasethe number of tested partitions and thus we potentially improve thesegmentation results. In addition, this space is considerably smaller than thespace of all possible partitions, thus we still keep the complexity controlled. Our second goal aims to improve the locality of region merging algorithms,which usually merge pairs of neighboring regions. In this work, we overcomethis limitation by introducing a validation procedure for complete partitions,rather than for pairs of regions. The third goal is to perform an exhaustive experimental evaluationmethodology in order to provide reproducible results. Finally, we embed the selection process on a statistical A Contrarioframework which allows us to have only one free parameter related to thedesired scale.
arxiv-3000-194 | Gromov-Hausdorff Approximation of Metric Spaces with Linear Structure | http://arxiv.org/pdf/1305.1172v1.pdf | author:Frédéric Chazal, Jian Sun category:cs.CG cs.LG math.MG published:2013-05-06 summary:In many real-world applications data come as discrete metric spaces sampledaround 1-dimensional filamentary structures that can be seen as metric graphs.In this paper we address the metric reconstruction problem of such filamentarystructures from data sampled around them. We prove that they can beapproximated, with respect to the Gromov-Hausdorff distance by well-chosen Reebgraphs (and some of their variants) and we provide an efficient and easy toimplement algorithm to compute such approximations in almost linear time. Weillustrate the performances of our algorithm on a few synthetic and real datasets.
arxiv-3000-195 | A Computer Vision System for Attention Mapping in SLAM based 3D Models | http://arxiv.org/pdf/1305.1163v1.pdf | author:Lucas Paletta, Katrin Santner, Gerald Fritz, Albert Hofmann, Gerald Lodron, Georg Thallinger, Heinz Mayer category:cs.CV published:2013-05-06 summary:The study of human factors in the frame of interaction studies has beenrelevant for usability engi-neering and ergonomics for decades. Today, with theadvent of wearable eye-tracking and Google glasses, monitoring of human factorswill soon become ubiquitous. This work describes a computer vision system thatenables pervasive mapping and monitoring of human attention. The keycontribu-tion is that our methodology enables full 3D recovery of the gazepointer, human view frustum and associated human centred measurements directlyinto an automatically computed 3D model in real-time. We apply RGB-D SLAM anddescriptor matching methodologies for the 3D modelling, locali-zation and fullyautomated annotation of ROIs (regions of interest) within the acquired 3Dmodel. This innovative methodology will open new avenues for attention studiesin real world environments, bringing new potential into automated processingfor human factors technologies.
arxiv-3000-196 | Techniques for Feature Extraction In Speech Recognition System : A Comparative Study | http://arxiv.org/pdf/1305.1145v1.pdf | author:Urmila Shrawankar, V M Thakare category:cs.SD cs.CL published:2013-05-06 summary:The time domain waveform of a speech signal carries all of the auditoryinformation. From the phonological point of view, it little can be said on thebasis of the waveform itself. However, past research in mathematics, acoustics,and speech technology have provided many methods for converting data that canbe considered as information if interpreted correctly. In order to find somestatistically relevant information from incoming data, it is important to havemechanisms for reducing the information of each segment in the audio signalinto a relatively small number of parameters, or features. These featuresshould describe each segment in such a characteristic way that other similarsegments can be grouped together by comparing their features. There areenormous interesting and exceptional ways to describe the speech signal interms of parameters. Though, they all have their strengths and weaknesses, wehave presented some of the most used methods with their importance.
arxiv-3000-197 | Learning Human Activities and Object Affordances from RGB-D Videos | http://arxiv.org/pdf/1210.1207v2.pdf | author:Hema Swetha Koppula, Rudhir Gupta, Ashutosh Saxena category:cs.RO cs.AI cs.CV published:2012-10-04 summary:Understanding human activities and object affordances are two very importantskills, especially for personal robots which operate in human environments. Inthis work, we consider the problem of extracting a descriptive labeling of thesequence of sub-activities being performed by a human, and more importantly, oftheir interactions with the objects in the form of associated affordances.Given a RGB-D video, we jointly model the human activities and objectaffordances as a Markov random field where the nodes represent objects andsub-activities, and the edges represent the relationships between objectaffordances, their relations with sub-activities, and their evolution overtime. We formulate the learning problem using a structural support vectormachine (SSVM) approach, where labelings over various alternate temporalsegmentations are considered as latent variables. We tested our method on achallenging dataset comprising 120 activity videos collected from 4 subjects,and obtained an accuracy of 79.4% for affordance, 63.4% for sub-activity and75.0% for high-level activity labeling. We then demonstrate the use of suchdescriptive labeling in performing assistive tasks by a PR2 robot.
arxiv-3000-198 | Diffusion Adaptation over Networks | http://arxiv.org/pdf/1205.4220v2.pdf | author:Ali H. Sayed category:cs.MA cs.LG published:2012-05-18 summary:Adaptive networks are well-suited to perform decentralized informationprocessing and optimization tasks and to model various types of self-organizedand complex behavior encountered in nature. Adaptive networks consist of acollection of agents with processing and learning abilities. The agents arelinked together through a connection topology, and they cooperate with eachother through local interactions to solve distributed optimization, estimation,and inference problems in real-time. The continuous diffusion of informationacross the network enables agents to adapt their performance in relation tostreaming data and network conditions; it also results in improved adaptationand learning performance relative to non-cooperative agents. This articleprovides an overview of diffusion strategies for adaptation and learning overnetworks. The article is divided into several sections: 1. Motivation; 2.Mean-Square-Error Estimation; 3. Distributed Optimization via DiffusionStrategies; 4. Adaptive Diffusion Strategies; 5. Performance ofSteepest-Descent Diffusion Strategies; 6. Performance of Adaptive DiffusionStrategies; 7. Comparing the Performance of Cooperative Strategies; 8.Selecting the Combination Weights; 9. Diffusion with Noisy InformationExchanges; 10. Extensions and Further Considerations; Appendix A: Properties ofKronecker Products; Appendix B: Graph Laplacian and Network Connectivity;Appendix C: Stochastic Matrices; Appendix D: Block Maximum Norm; Appendix E:Comparison with Consensus Strategies; References.
arxiv-3000-199 | Hybridization of Otsu Method and Median Filter for Color Image Segmentation | http://arxiv.org/pdf/1305.1052v1.pdf | author:Firas Ajil Jassim, Fawzi H. Altaani category:cs.CV published:2013-05-05 summary:In this article a novel algorithm for color image segmentation has beendeveloped. The proposed algorithm based on combining two existing methods insuch a novel way to obtain a significant method to partition the color imageinto significant regions. On the first phase, the traditional Otsu method forgray channel image segmentation were applied for each of the R,G, and Bchannels separately to determine the suitable automatic threshold for eachchannel. After that, the new modified channels are integrated again toformulate a new color image. The resulted image suffers from some kind ofdistortion. To get rid of this distortion, the second phase is arise which isthe median filter to smooth the image and increase the segmented regions. Thisprocess looks very significant by the ocular eye. Experimental results werepresented on a variety of test images to support the proposed algorithm.
arxiv-3000-200 | On the Convergence and Consistency of the Blurring Mean-Shift Process | http://arxiv.org/pdf/1305.1040v1.pdf | author:Ting-Li Chen category:stat.ML cs.LG published:2013-05-05 summary:The mean-shift algorithm is a popular algorithm in computer vision and imageprocessing. It can also be cast as a minimum gamma-divergence estimation. Inthis paper we focus on the "blurring" mean shift algorithm, which is oneversion of the mean-shift process that successively blurs the dataset. Theanalysis of the blurring mean-shift is relatively more complicated compared tothe nonblurring version, yet the algorithm convergence and the estimationconsistency have not been well studied in the literature. In this paper weprove both the convergence and the consistency of the blurring mean-shift. Wealso perform simulation studies to compare the efficiency of the blurring andthe nonblurring versions of the mean-shift algorithms. Our results show thatthe blurring mean-shift has more efficiency.
arxiv-3000-201 | Inverting and Visualizing Features for Object Detection | http://arxiv.org/pdf/1212.2278v2.pdf | author:Carl Vondrick, Aditya Khosla, Tomasz Malisiewicz, Antonio Torralba category:cs.CV published:2012-12-11 summary:We introduce algorithms to visualize feature spaces used by object detectors.The tools in this paper allow a human to put on `HOG goggles' and perceive thevisual world as a HOG based object detector sees it. We found that thesevisualizations allow us to analyze object detection systems in new ways andgain new insight into the detector's failures. For example, when we visualizethe features for high scoring false alarms, we discovered that, although theyare clearly wrong in image space, they do look deceptively similar to truepositives in feature space. This result suggests that many of these falsealarms are caused by our choice of feature space, and indicates that creating abetter learning algorithm or building bigger datasets is unlikely to correctthese errors. By visualizing feature spaces, we can gain a more intuitiveunderstanding of our detection systems.
arxiv-3000-202 | Efficient Estimation of the number of neighbours in Probabilistic K Nearest Neighbour Classification | http://arxiv.org/pdf/1305.1002v1.pdf | author:Ji Won Yoon, Nial Friel category:cs.LG stat.ML published:2013-05-05 summary:Probabilistic k-nearest neighbour (PKNN) classification has been introducedto improve the performance of original k-nearest neighbour (KNN) classificationalgorithm by explicitly modelling uncertainty in the classification of eachfeature vector. However, an issue common to both KNN and PKNN is to select theoptimal number of neighbours, $k$. The contribution of this paper is toincorporate the uncertainty in $k$ into the decision making, and in so doinguse Bayesian model averaging to provide improved classification. Indeed theproblem of assessing the uncertainty in $k$ can be viewed as one of statisticalmodel selection which is one of the most important technical issues in thestatistics and machine learning domain. In this paper, a new functionalapproximation algorithm is proposed to reconstruct the density of the model(order) without relying on time consuming Monte Carlo simulations. In addition,this algorithm avoids cross validation by adopting Bayesian framework. Theperformance of this algorithm yielded very good performance on several realexperimental datasets.
arxiv-3000-203 | Forecastable Component Analysis (ForeCA) | http://arxiv.org/pdf/1205.4591v3.pdf | author:Georg M. Goerg category:stat.ME stat.ML published:2012-05-21 summary:I introduce Forecastable Component Analysis (ForeCA), a novel dimensionreduction technique for temporally dependent signals. Based on a newforecastability measure, ForeCA finds an optimal transformation to separate amultivariate time series into a forecastable and an orthogonal white noisespace. I present a converging algorithm with a fast eigenvector solution.Applications to financial and macro-economic time series show that ForeCA cansuccessfully discover informative structure, which can be used for forecastingas well as classification. The R package ForeCA(http://cran.r-project.org/web/packages/ForeCA/index.html) accompanies thiswork and is publicly available on CRAN.
arxiv-3000-204 | A Feature Selection Method for Multivariate Performance Measures | http://arxiv.org/pdf/1103.1013v2.pdf | author:Qi Mao, Ivor W. Tsang category:cs.LG published:2011-03-05 summary:Feature selection with specific multivariate performance measures is the keyto the success of many applications, such as image retrieval and textclassification. The existing feature selection methods are usually designed forclassification error. In this paper, we propose a generalized sparseregularizer. Based on the proposed regularizer, we present a unified featureselection framework for general loss functions. In particular, we study thenovel feature selection paradigm by optimizing multivariate performancemeasures. The resultant formulation is a challenging problem forhigh-dimensional data. Hence, a two-layer cutting plane algorithm is proposedto solve this problem, and the convergence is presented. In addition, we adaptthe proposed method to optimize multivariate measures for multiple instancelearning problems. The analyses by comparing with the state-of-the-art featureselection methods show that the proposed method is superior to others.Extensive experiments on large-scale and high-dimensional real world datasetsshow that the proposed method outperforms $l_1$-SVM and SVM-RFE when choosing asmall subset of features, and achieves significantly improved performances overSVM$^{perf}$ in terms of $F_1$-score.
arxiv-3000-205 | On Comparison between Evolutionary Programming Network-based Learning and Novel Evolution Strategy Algorithm-based Learning | http://arxiv.org/pdf/1305.0922v1.pdf | author:M. A. Khayer Azad, Md. Shafiqul Islam, M. M. A. Hashem category:cs.NE cs.LG published:2013-05-04 summary:This paper presents two different evolutionary systems - EvolutionaryProgramming Network (EPNet) and Novel Evolutions Strategy (NES) Algorithm.EPNet does both training and architecture evolution simultaneously, whereas NESdoes a fixed network and only trains the network. Five mutation operatorsproposed in EPNet to reflect the emphasis on evolving ANNs behaviors. Closebehavioral links between parents and their offspring are maintained by variousmutations, such as partial training and node splitting. On the other hand, NESuses two new genetic operators - subpopulation-based max-mean arithmeticalcrossover and time-variant mutation. The above-mentioned two algorithms havebeen tested on a number of benchmark problems, such as the medical diagnosisproblems (breast cancer, diabetes, and heart disease). The results and thecomparison between them are also presented in this paper.
arxiv-3000-206 | Efficient Multi-Template Learning for Structured Prediction | http://arxiv.org/pdf/1103.0890v2.pdf | author:Qi Mao, Ivor W. Tsang category:cs.LG cs.CL published:2011-03-04 summary:Conditional random field (CRF) and Structural Support Vector Machine(Structural SVM) are two state-of-the-art methods for structured predictionwhich captures the interdependencies among output variables. The success ofthese methods is attributed to the fact that their discriminative models areable to account for overlapping features on the whole input observations. Thesefeatures are usually generated by applying a given set of templates on labeleddata, but improper templates may lead to degraded performance. To alleviatethis issue, in this paper, we propose a novel multiple template learningparadigm to learn structured prediction and the importance of each templatesimultaneously, so that hundreds of arbitrary templates could be added into thelearning model without caution. This paradigm can be formulated as a specialmultiple kernel learning problem with exponential number of constraints. Thenwe introduce an efficient cutting plane algorithm to solve this problem in theprimal, and its convergence is presented. We also evaluate the proposedlearning paradigm on two widely-studied structured prediction tasks,\emph{i.e.} sequence labeling and dependency parsing. Extensive experimentalresults show that the proposed method outperforms CRFs and Structural SVMs dueto exploiting the importance of each template. Our complexity analysis andempirical results also show that our proposed method is more efficient thanOnlineMKL on very sparse and high-dimensional data. We further extend thisparadigm for structured prediction using generalized $p$-block normregularization with $p>1$, and experiments show competitive performances when$p \in [1,2)$.
arxiv-3000-207 | APPLE: Approximate Path for Penalized Likelihood Estimators | http://arxiv.org/pdf/1211.0889v3.pdf | author:Yi Yu, Yang Feng category:stat.ML cs.LG published:2012-11-02 summary:In high-dimensional data analysis, penalized likelihood estimators are shownto provide superior results in both variable selection and parameterestimation. A new algorithm, APPLE, is proposed for calculating the ApproximatePath for Penalized Likelihood Estimators. Both the convex penalty (such asLASSO) and the nonconvex penalty (such as SCAD and MCP) cases are considered.The APPLE efficiently computes the solution path for the penalized likelihoodestimator using a hybrid of the modified predictor-corrector method and thecoordinate-descent algorithm. APPLE is compared with several well-knownpackages via simulation and analysis of two gene expression data sets.
arxiv-3000-208 | Dictionary learning based image enhancement for rarity detection | http://arxiv.org/pdf/1305.0871v1.pdf | author:Weifeng Liu, Xiaomeng Wang, Yanjiang Wang category:cs.CV published:2013-05-04 summary:Image enhancement is an important image processing technique that processesimages suitably for a specific application e.g. image editing. The conventionalsolutions of image enhancement are grouped into two categories which arespatial domain processing method and transform domain processing method such ascontrast manipulation, histogram equalization, homomorphic filtering. Thisletter proposes a new image enhance method based on dictionary learning.Particularly, the proposed method adjusts the image by manipulating the rarityof dictionary atoms. Firstly, learn the dictionary through sparse codingalgorithms on divided sub-image blocks. Secondly, compute the rarity ofdictionary atoms on statistics of the corresponding sparse coefficients.Thirdly, adjust the rarity according to specific application and form a newdictionary. Finally, reconstruct the image using the updated dictionary andsparse coefficients. Compared with the traditional techniques, the proposedmethod enhances image based on the image content not on distribution of pixelgrey value or frequency. The advantages of the proposed method lie in that itis in better correspondence with the response of the human visual system andmore suitable for salient objects extraction. The experimental resultsdemonstrate the effectiveness of the proposed image enhance method.
arxiv-3000-209 | Geometrical complexity of data approximators | http://arxiv.org/pdf/1302.2645v2.pdf | author:E. M. Mirkes, A. Zinovyev, A. N. Gorban category:stat.ML cs.LG published:2013-02-11 summary:There are many methods developed to approximate a cloud of vectors embeddedin high-dimensional space by simpler objects: starting from principal pointsand linear manifolds to self-organizing maps, neural gas, elastic maps, varioustypes of principal curves and principal trees, and so on. For each type ofapproximators the measure of the approximator complexity was developed too.These measures are necessary to find the balance between accuracy andcomplexity and to define the optimal approximations of a given type. We proposea measure of complexity (geometrical complexity) which is applicable toapproximators of several types and which allows comparing data approximationsof different types.
arxiv-3000-210 | Inference in Kingman's Coalescent with Particle Markov Chain Monte Carlo Method | http://arxiv.org/pdf/1305.0855v1.pdf | author:Yifei Chen, Xiaohui Xie category:stat.ML q-bio.PE published:2013-05-03 summary:We propose a new algorithm to do posterior sampling of Kingman's coalescent,based upon the Particle Markov Chain Monte Carlo methodology. Specifically, thealgorithm is an instantiation of the Particle Gibbs Sampling method, whichalternately samples coalescent times conditioned on coalescent tree structures,and tree structures conditioned on coalescent times via the conditionalSequential Monte Carlo procedure. We implement our algorithm as a C++ package,and demonstrate its utility via a parameter estimation task in populationgenetics on both single- and multiple-locus data. The experiment results showthat the proposed algorithm performs comparable to or better than severalwell-developed methods.
arxiv-3000-211 | Group theory, group actions, evolutionary algorithms, and global optimization | http://arxiv.org/pdf/1301.0254v3.pdf | author:Andrew Clark category:cs.NE math.DS math.OC math.RA published:2012-12-27 summary:In this paper we use group, action and orbit to understand how evolutionarysolve nonconvex optimization problems.
arxiv-3000-212 | Learning from Imprecise and Fuzzy Observations: Data Disambiguation through Generalized Loss Minimization | http://arxiv.org/pdf/1305.0698v1.pdf | author:Eyke Hüllermeier category:cs.LG published:2013-05-03 summary:Methods for analyzing or learning from "fuzzy data" have attracted increasingattention in recent years. In many cases, however, existing methods (forprecise, non-fuzzy data) are extended to the fuzzy case in an ad-hoc manner,and without carefully considering the interpretation of a fuzzy set when beingused for modeling data. Distinguishing between an ontic and an epistemicinterpretation of fuzzy set-valued data, and focusing on the latter, we arguethat a "fuzzification" of learning algorithms based on an application of thegeneric extension principle is not appropriate. In fact, the extensionprinciple fails to properly exploit the inductive bias underlying statisticaland machine learning methods, although this bias, at least in principle, offersa means for "disambiguating" the fuzzy data. Alternatively, we thereforepropose a method which is based on the generalization of loss functions inempirical risk minimization, and which performs model identification and datadisambiguation simultaneously. Elaborating on the fuzzification of specifictypes of losses, we establish connections to well-known loss functions inregression and classification. We compare our approach with related methods andillustrate its use in logistic regression for binary classification.
arxiv-3000-213 | Feature Selection Based on Term Frequency and T-Test for Text Categorization | http://arxiv.org/pdf/1305.0638v1.pdf | author:Deqing Wang, Hui Zhang, Rui Liu, Weifeng Lv category:cs.LG cs.IR stat.ML published:2013-05-03 summary:Much work has been done on feature selection. Existing methods are based ondocument frequency, such as Chi-Square Statistic, Information Gain etc.However, these methods have two shortcomings: one is that they are not reliablefor low-frequency terms, and the other is that they only count whether one termoccurs in a document and ignore the term frequency. Actually, high-frequencyterms within a specific category are often regards as discriminators. This paper focuses on how to construct the feature selection function basedon term frequency, and proposes a new approach based on $t$-test, which is usedto measure the diversity of the distributions of a term between the specificcategory and the entire corpus. Extensive comparative experiments on two textcorpora using three classifiers show that our new approach is comparable to oror slightly better than the state-of-the-art feature selection methods (i.e.,$\chi^2$, and IG) in terms of macro-$F_1$ and micro-$F_1$.
arxiv-3000-214 | Anisotropic oracle inequalities in noisy quantization | http://arxiv.org/pdf/1305.0630v1.pdf | author:Sébastien Loustau category:math.ST stat.ML stat.TH published:2013-05-03 summary:The effect of errors in variables in quantization is investigated. We provegeneral exact and non-exact oracle inequalities with fast rates for anempirical minimization based on a noisy sample$Z_i=X_i+\epsilon_i,i=1,\ldots,n$, where $X_i$ are i.i.d. with density $f$ and$\epsilon_i$ are i.i.d. with density $\eta$. These rates depend on the geometryof the density $f$ and the asymptotic behaviour of the characteristic functionof $\eta$. This general study can be applied to the problem of $k$-means clustering withnoisy data. For this purpose, we introduce a deconvolution $k$-means stochasticminimization which reaches fast rates of convergence under standard Pollard'sregularity assumptions.
arxiv-3000-215 | An Improved EM algorithm | http://arxiv.org/pdf/1305.0626v1.pdf | author:Fuqiang Chen category:cs.LG cs.AI stat.ML published:2013-05-03 summary:In this paper, we firstly give a brief introduction of expectationmaximization (EM) algorithm, and then discuss the initial value sensitivity ofexpectation maximization algorithm. Subsequently, we give a short proof of EM'sconvergence. Then, we implement experiments with the expectation maximizationalgorithm (We implement all the experiments on Gaussion mixture model (GMM)).Our experiment with expectation maximization is performed in the followingthree cases: initialize randomly; initialize with result of K-means; initializewith result of K-medoids. The experiment result shows that expectationmaximization algorithm depend on its initial state or parameters. And we foundthat EM initialized with K-medoids performed better than both the oneinitialized with K-means and the one initialized randomly.
arxiv-3000-216 | CONATION: English Command Input/Output System for Computers | http://arxiv.org/pdf/1305.0625v1.pdf | author:Kamlesh Sharma, Dr. T. V. Prasad category:cs.HC cs.CL published:2013-05-03 summary:In this information technology age, a convenient and user friendly interfaceis required to operate the computer system on very fast rate. In the humanbeing, speech being a natural mode of communication has potential to being afast and convenient mode of interaction with computer. Speech recognition willplay an important role in taking technology to them. It is the need of this erato access the information within seconds. This paper describes the design anddevelopment of speaker independent and English command interpreted system forcomputers. HMM model is used to represent the phoneme like speech commands.Experiments have been done on real world data and system has been trained innormal condition for real world subject.
arxiv-3000-217 | Mixed LICORS: A Nonparametric Algorithm for Predictive State Reconstruction | http://arxiv.org/pdf/1211.3760v2.pdf | author:Georg M. Goerg, Cosma Rohilla Shalizi category:stat.ME stat.ML published:2012-11-15 summary:We introduce 'mixed LICORS', an algorithm for learning nonlinear,high-dimensional dynamics from spatio-temporal data, suitable for bothprediction and simulation. Mixed LICORS extends the recent LICORS algorithm(Goerg and Shalizi, 2012) from hard clustering of predictive distributions to anon-parametric, EM-like soft clustering. This retains the asymptotic predictiveoptimality of LICORS, but, as we show in simulations, greatly improvesout-of-sample forecasts with limited data. The new method is implemented in thepublicly-available R package "LICORS"(http://cran.r-project.org/web/packages/LICORS/).
arxiv-3000-218 | Testing Hypotheses by Regularized Maximum Mean Discrepancy | http://arxiv.org/pdf/1305.0423v1.pdf | author:Somayeh Danafar, Paola M. V. Rancoita, Tobias Glasmachers, Kevin Whittingstall, Juergen Schmidhuber category:cs.LG cs.AI stat.ML published:2013-05-02 summary:Do two data samples come from different distributions? Recent studies of thisfundamental problem focused on embedding probability distributions intosufficiently rich characteristic Reproducing Kernel Hilbert Spaces (RKHSs), tocompare distributions by the distance between their embeddings. We show thatRegularized Maximum Mean Discrepancy (RMMD), our novel measure for kernel-basedhypothesis testing, yields substantial improvements even when sample sizes aresmall, and excels at hypothesis tests involving multiple comparisons with powercontrol. We derive asymptotic distributions under the null and alternativehypotheses, and assess power control. Outstanding results are obtained on:challenging EEG data, MNIST, the Berkley Covertype, and the Flare-Solardataset.
arxiv-3000-219 | Tensor Decompositions: A New Concept in Brain Data Analysis? | http://arxiv.org/pdf/1305.0395v1.pdf | author:Andrzej Cichocki category:cs.NA cs.LG q-bio.NC stat.ML published:2013-05-02 summary:Matrix factorizations and their extensions to tensor factorizations anddecompositions have become prominent techniques for linear and multilinearblind source separation (BSS), especially multiway Independent ComponentAnalysis (ICA), NonnegativeMatrix and Tensor Factorization (NMF/NTF), SmoothComponent Analysis (SmoCA) and Sparse Component Analysis (SCA). Moreover,tensor decompositions have many other potential applications beyond multilinearBSS, especially feature extraction, classification, dimensionality reductionand multiway clustering. In this paper, we briefly overview new and emergingmodels and approaches for tensor decompositions in applications to group andlinked multiway BSS/ICA, feature extraction, classification andMultiway PartialLeast Squares (MPLS) regression problems. Keywords: Multilinear BSS, linkedmultiway BSS/ICA, tensor factorizations and decompositions, constrained Tuckerand CP models, Penalized Tensor Decompositions (PTD), feature extraction,classification, multiway PLS and CCA.
arxiv-3000-220 | Sparse/Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory | http://arxiv.org/pdf/1301.4566v2.pdf | author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:stat.ML math.OC math.ST stat.TH 62F35, 65K10 published:2013-01-19 summary:We introduce a class of quadratic support (QS) functions, many of which playa crucial role in a variety of applications, including machine learning, robuststatistical inference, sparsity promotion, and Kalman smoothing. Well knownexamples include the l2, Huber, l1 and Vapnik losses. We build on a dualrepresentation for QS functions using convex analysis, revealing the structurenecessary for a QS function to be interpreted as the negative log of aprobability density, and providing the foundation for statisticalinterpretation and analysis of QS loss functions. For a subclass of QSfunctions called piecewise linear quadratic (PLQ) penalties, we also developefficient numerical estimation schemes. These components form a flexiblestatistical modeling framework for a variety of learning applications, togetherwith a toolbox of efficient numerical methods for inference. In particular, forPLQ densities, interior point (IP) methods can be used. IP methods solvenonsmooth optimization problems by working directly with smooth systems ofequations characterizing their optimality. The efficiency of the IP approachdepends on the structure of particular applications. We consider the class ofdynamic inverse problems using Kalman smoothing, where the aim is toreconstruct the state of a dynamical system with known process and measurementmodels starting from noisy output samples. In the classical case, Gaussianerrors are assumed in the process and measurement models. The extendedframework allows arbitrary PLQ densities to be used, and the proposed IPapproach solves the generalized Kalman smoothing problem while maintaining thelinear complexity in the size of the time series, just as in the Gaussian case.This extends the computational efficiency of classic algorithms to a muchbroader nonsmooth setting, and includes many recently proposed robust andsparse smoothers as special cases.
arxiv-3000-221 | Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition | http://arxiv.org/pdf/1305.0355v1.pdf | author:Adel Javanmard, Andrea Montanari category:math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH published:2013-05-02 summary:In the high-dimensional regression model a response variable is linearlyrelated to $p$ covariates, but the sample size $n$ is smaller than $p$. Weassume that only a small subset of covariates is `active' (i.e., thecorresponding coefficients are non-zero), and consider the model-selectionproblem of identifying the active covariates. A popular approach is to estimatethe regression coefficients through the Lasso ($\ell_1$-regularized leastsquares). This is known to correctly identify the active set only if theirrelevant covariates are roughly orthogonal to the relevant ones, asquantified through the so called `irrepresentability' condition. In this paperwe study the `Gauss-Lasso' selector, a simple two-stage method that firstsolves the Lasso, and then performs ordinary least squares restricted to theLasso active set. We formulate `generalized irrepresentability condition'(GIC), an assumption that is substantially weaker than irrepresentability. Weprove that, under GIC, the Gauss-Lasso correctly recovers the active set.
arxiv-3000-222 | An Adaptive Descriptor Design for Object Recognition in the Wild | http://arxiv.org/pdf/1305.0311v1.pdf | author:Zhenyu Guo, Z. Jane Wang category:cs.CV published:2013-05-01 summary:Digital images nowadays have various styles of appearance, in the aspects ofcolor tones, contrast, vignetting, and etc. These 'picture styles' are directlyrelated to the scene radiance, image pipeline of the camera, and postprocessing functions. Due to the complexity and nonlinearity of these causes,popular gradient-based image descriptors won't be invariant to differentpicture styles, which will decline the performance of object recognition. Giventhat images shared online or created by individual users are taken with a widerange of devices and may be processed by various post processing functions, tofind a robust object recognition system is useful and challenging. In thispaper, we present the first study on the influence of picture styles for objectrecognition, and propose an adaptive approach based on the kernel view ofgradient descriptors and multiple kernel learning, without estimating orspecifying the styles of images used in training and testing. We conductexperiments on Domain Adaptation data set and Oxford Flower data set. Theexperiments also include several variants of the flower data set by processingthe images with popular photo effects. The results demonstrate that ourproposed method improve from standard descriptors in all cases.
arxiv-3000-223 | The Sample Complexity of Search over Multiple Populations | http://arxiv.org/pdf/1209.1380v2.pdf | author:Matthew L. Malloy, Gongguo Tang, Robert D. Nowak category:cs.IT math.IT stat.ML published:2012-09-06 summary:This paper studies the sample complexity of searching over multiplepopulations. We consider a large number of populations, each corresponding toeither distribution P0 or P1. The goal of the search problem studied here is tofind one population corresponding to distribution P1 with as few samples aspossible. The main contribution is to quantify the number of samples needed tocorrectly find one such population. We consider two general approaches:non-adaptive sampling methods, which sample each population a predeterminednumber of times until a population following P1 is found, and adaptive samplingmethods, which employ sequential sampling schemes for each population. We firstderive a lower bound on the number of samples required by any sampling scheme.We then consider an adaptive procedure consisting of a series of sequentialprobability ratio tests, and show it comes within a constant factor of thelower bound. We give explicit expressions for this constant when samples of thepopulations follow Gaussian and Bernoulli distributions. An alternativeadaptive scheme is discussed which does not require full knowledge of P1, andcomes within a constant factor of the optimal scheme. For comparison, a lowerbound on the sampling requirements of any non-adaptive scheme is presented.
arxiv-3000-224 | Video Segmentation via Diffusion Bases | http://arxiv.org/pdf/1305.0218v1.pdf | author:Dina Dushnik, Alon Schclar, Amir Averbuch category:cs.CV cs.MM published:2013-05-01 summary:Identifying moving objects in a video sequence, which is produced by a staticcamera, is a fundamental and critical task in many computer-visionapplications. A common approach performs background subtraction, whichidentifies moving objects as the portion of a video frame that differssignificantly from a background model. A good background subtraction algorithmhas to be robust to changes in the illumination and it should avoid detectingnon-stationary background objects such as moving leaves, rain, snow, andshadows. In addition, the internal background model should quickly respond tochanges in background such as objects that start to move or stop. We present anew algorithm for video segmentation that processes the input video sequence asa 3D matrix where the third axis is the time domain. Our approach identifiesthe background by reducing the input dimension using the \emph{diffusion bases}methodology. Furthermore, we describe an iterative method for extracting anddeleting the background. The algorithm has two versions and thus covers thecomplete range of backgrounds: one for scenes with static backgrounds and theother for scenes with dynamic (moving) backgrounds.
arxiv-3000-225 | MATAWS: A Multimodal Approach for Automatic WS Semantic Annotation | http://arxiv.org/pdf/1305.0194v1.pdf | author:Cihan Aksoy, Vincent Labatut, Chantal Cherifi, Jean-François Santucci category:cs.SE cs.CL cs.IR published:2013-05-01 summary:Many recent works aim at developing methods and tools for the processing ofsemantic Web services. In order to be properly tested, these tools must beapplied to an appropriate benchmark, taking the form of a collection ofsemantic WS descriptions. However, all of the existing publicly availablecollections are limited by their size or their realism (use of randomlygenerated or resampled descriptions). Larger and realistic syntactic (WSDL)collections exist, but their semantic annotation requires a certain level ofautomation, due to the number of operations to be processed. In this article,we propose a fully automatic method to semantically annotate such large WScollections. Our approach is multimodal, in the sense it takes advantage of thelatent semantics present not only in the parameter names, but also in the typenames and structures. Concept-to-word association is performed by using Sigma,a mapping of WordNet to the SUMO ontology. After having described in detailsour annotation method, we apply it to the larger collection of real-worldsyntactic WS descriptions we could find, and assess its efficiency.
arxiv-3000-226 | Semi-Supervised Information-Maximization Clustering | http://arxiv.org/pdf/1304.8020v2.pdf | author:Daniele Calandriello, Gang Niu, Masashi Sugiyama category:cs.LG stat.ML published:2013-04-30 summary:Semi-supervised clustering aims to introduce prior knowledge in the decisionprocess of a clustering algorithm. In this paper, we propose a novelsemi-supervised clustering algorithm based on the information-maximizationprinciple. The proposed method is an extension of a previous unsupervisedinformation-maximization clustering algorithm based on squared-loss mutualinformation to effectively incorporate must-links and cannot-links. Theproposed method is computationally efficient because the clustering solutioncan be obtained analytically via eigendecomposition. Furthermore, the proposedmethod allows systematic optimization of tuning parameters such as the kernelwidth, given the degree of belief in the must-links and cannot-links. Theusefulness of the proposed method is demonstrated through experiments.
arxiv-3000-227 | The Annealing Sparse Bayesian Learning Algorithm | http://arxiv.org/pdf/1209.1033v4.pdf | author:Benyuan Liu, Hongqi Fan, Zaiqi Lu, Qiang Fu category:cs.IT cs.LG math.IT published:2012-09-05 summary:In this paper we propose a two-level hierarchical Bayesian model and anannealing schedule to re-enable the noise variance learning capability of thefast marginalized Sparse Bayesian Learning Algorithms. The performance such asNMSE and F-measure can be greatly improved due to the annealing technique. Thisalgorithm tends to produce the most sparse solution under moderate SNRscenarios and can outperform most concurrent SBL algorithms while pertainssmall computational load.
arxiv-3000-228 | Training Neural Networks with Stochastic Hessian-Free Optimization | http://arxiv.org/pdf/1301.3641v3.pdf | author:Ryan Kiros category:cs.LG cs.NE stat.ML published:2013-01-16 summary:Hessian-free (HF) optimization has been successfully used for training deepautoencoders and recurrent networks. HF uses the conjugate gradient algorithmto construct update directions through curvature-vector products that can becomputed on the same order of time as gradients. In this paper we exploit thisproperty and study stochastic HF with gradient and curvature mini-batchesindependent of the dataset size. We modify Martens' HF for these settings andintegrate dropout, a method for preventing co-adaptation of feature detectors,to guard against overfitting. Stochastic Hessian-free optimization gives anintermediary between SGD and HF that achieves competitive performance on bothclassification and deep autoencoder experiments.
arxiv-3000-229 | Clustering Unclustered Data: Unsupervised Binary Labeling of Two Datasets Having Different Class Balances | http://arxiv.org/pdf/1305.0103v1.pdf | author:Marthinus Christoffel du Plessis, Masashi Sugiyama category:cs.LG published:2013-05-01 summary:We consider the unsupervised learning problem of assigning labels tounlabeled data. A naive approach is to use clustering methods, but this workswell only when data is properly clustered and each cluster corresponds to anunderlying class. In this paper, we first show that this unsupervised labelingproblem in balanced binary cases can be solved if two unlabeled datasets havingdifferent class balances are available. More specifically, estimation of thesign of the difference between probability densities of two unlabeled datasetsgives the solution. We then introduce a new method to directly estimate thesign of the density difference without density estimation. Finally, wedemonstrate the usefulness of the proposed method against several clusteringmethods on various toy problems and real-world datasets.
arxiv-3000-230 | Joint Training Deep Boltzmann Machines for Classification | http://arxiv.org/pdf/1301.3568v3.pdf | author:Ian J. Goodfellow, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG published:2013-01-16 summary:We introduce a new method for training deep Boltzmann machines jointly. Priormethods of training DBMs require an initial learning pass that trains the modelgreedily, one layer at a time, or do not perform well on classification tasks.In our approach, we train all layers of the DBM simultaneously, using a noveltraining procedure called multi-prediction training. The resulting model caneither be interpreted as a single generative model trained to maximize avariational approximation to the generalized pseudolikelihood, or as a familyof recurrent networks that share parameters and may be approximately averagedtogether using a novel technique we call the multi-inference trick. We showthat our approach performs competitively for classification and outperformsprevious methods in terms of accuracy of approximate inference andclassification with missing inputs.
arxiv-3000-231 | Spectral Compressed Sensing via Structured Matrix Completion | http://arxiv.org/pdf/1304.4610v2.pdf | author:Yuxin Chen, Yuejie Chi category:cs.IT cs.LG math.IT math.NA stat.ML published:2013-04-16 summary:The paper studies the problem of recovering a spectrally sparse object from asmall number of time domain samples. Specifically, the object of interest withambient dimension $n$ is assumed to be a mixture of $r$ complexmulti-dimensional sinusoids, while the underlying frequencies can assume anyvalue in the unit disk. Conventional compressed sensing paradigms suffer fromthe {\em basis mismatch} issue when imposing a discrete dictionary on theFourier representation. To address this problem, we develop a novelnonparametric algorithm, called enhanced matrix completion (EMaC), based onstructured matrix completion. The algorithm starts by arranging the data into alow-rank enhanced form with multi-fold Hankel structure, then attempts recoveryvia nuclear norm minimization. Under mild incoherence conditions, EMaC allowsperfect recovery as soon as the number of samples exceeds the order of$\mathcal{O}(r\log^{2} n)$. We also show that, in many instances, accuratecompletion of a low-rank multi-fold Hankel matrix is possible when the numberof observed entries is proportional to the information theoretical limits(except for a logarithmic gap). The robustness of EMaC against bounded noiseand its applicability to super resolution are further demonstrated by numericalexperiments.
arxiv-3000-232 | Revealing social networks of spammers through spectral clustering | http://arxiv.org/pdf/1305.0051v1.pdf | author:Kevin S. Xu, Mark Kliger, Yilun Chen, Peter J. Woolf, Alfred O. Hero III category:cs.SI cs.LG physics.soc-ph stat.ML published:2013-04-30 summary:To date, most studies on spam have focused only on the spamming phase of thespam cycle and have ignored the harvesting phase, which consists of the massacquisition of email addresses. It has been observed that spammers concealtheir identity to a lesser degree in the harvesting phase, so it may bepossible to gain new insights into spammers' behavior by studying the behaviorof harvesters, which are individuals or bots that collect email addresses. Inthis paper, we reveal social networks of spammers by identifying communities ofharvesters with high behavioral similarity using spectral clustering. The dataanalyzed was collected through Project Honey Pot, a distributed system formonitoring harvesting and spamming. Our main findings are (1) that mostspammers either send only phishing emails or no phishing emails at all, (2)that most communities of spammers also send only phishing emails or no phishingemails at all, and (3) that several groups of spammers within communitiesexhibit coherent temporal behavior and have similar IP addresses. Our findingsreveal some previously unknown behavior of spammers and suggest that there isindeed social structure between spammers to be discovered.
arxiv-3000-233 | Image Compression By Embedding Five Modulus Method Into JPEG | http://arxiv.org/pdf/1305.0020v1.pdf | author:Firas A. Jassim category:cs.CV cs.MM published:2013-04-30 summary:The standard JPEG format is almost the optimum format in image compression.The compression ratio in JPEG sometimes reaches 30:1. The compression ratio ofJPEG could be increased by embedding the Five Modulus Method (FMM) into theJPEG algorithm. The novel algorithm gives twice the time as the standard JPEGalgorithm or more. The novel algorithm was called FJPEG (Five-JPEG). Thequality of the reconstructed image after compression is approximatelyapproaches the JPEG. Standard test images have been used to support andimplement the suggested idea in this paper and the error metrics have beencomputed and compared with JPEG.
arxiv-3000-234 | Inferring ground truth from multi-annotator ordinal data: a probabilistic approach | http://arxiv.org/pdf/1305.0015v1.pdf | author:Balaji Lakshminarayanan, Yee Whye Teh category:stat.ML cs.LG published:2013-04-30 summary:A popular approach for large scale data annotation tasks is crowdsourcing,wherein each data point is labeled by multiple noisy annotators. We considerthe problem of inferring ground truth from noisy ordinal labels obtained frommultiple annotators of varying and unknown expertise levels. Annotation modelsfor ordinal data have been proposed mostly as extensions of theirbinary/categorical counterparts and have received little attention in thecrowdsourcing literature. We propose a new model for crowdsourced ordinal datathat accounts for instance difficulty as well as annotator expertise, andderive a variational Bayesian inference algorithm for parameter estimation. Weanalyze the ordinal extensions of several state-of-the-art annotator models forbinary/categorical labels and evaluate the performance of all the models on tworeal world datasets containing ordinal query-URL relevance scores, collectedthrough Amazon's Mechanical Turk. Our results indicate that the proposed modelperforms better or as well as existing state-of-the-art methods and is moreresistant to `spammy' annotators (i.e., annotators who assign labels randomlywithout actually looking at the instance) than popular baselines such as mean,median, and majority vote which do not account for annotator expertise.
arxiv-3000-235 | Fractal-Based Detection of Microcalcification Clusters in Digital Mammograms | http://arxiv.org/pdf/1304.8092v1.pdf | author:P. Shanmugavadivu, V. Sivakumar category:cs.CV published:2013-04-30 summary:In this paper, a novel method for edge detection of microcalcificationclusters in mammogram images is presented using the concept of FractalDimension and Hurst co-efficient that enables to locate the microcalcificationsin the mammograms. This technique detects the edges accurately than the onesobtained by the conventional Sobel method. Generally, Sobel method detects theedges of the regions/objects in an image using the Fudge factor that assumesits value as 0.5, by default. In this proposed technique, the Fudge factor issuitably replaced with Hurst Co-efficient, which is computed as the differenceof Fractal dimension and the topological dimension of a given input image.These two dimensions are image-dependent, and hence the respective Hurstco-efficient too varies with respect to images. Hence, the image-dependentHurst co-efficient based Sobel method is proved to produce better results thanthe Fudge factor based Sobel method. The results of the proposed methodsubstantiate the merit of the proposed technique.
arxiv-3000-236 | Uniqueness of Tensor Decompositions with Applications to Polynomial Identifiability | http://arxiv.org/pdf/1304.8087v1.pdf | author:Aditya Bhaskara, Moses Charikar, Aravindan Vijayaraghavan category:cs.DS cs.LG math.ST stat.TH published:2013-04-30 summary:We give a robust version of the celebrated result of Kruskal on theuniqueness of tensor decompositions: we prove that given a tensor whosedecomposition satisfies a robust form of Kruskal's rank condition, it ispossible to approximately recover the decomposition if the tensor is known upto a sufficiently small (inverse polynomial) error. Kruskal's theorem has found many applications in proving the identifiabilityof parameters for various latent variable models and mixture models such asHidden Markov models, topic models etc. Our robust version immediately impliesidentifiability using only polynomially many samples in many of these settings.This polynomial identifiability is an essential first step towards efficientlearning algorithms for these models. Recently, algorithms based on tensor decompositions have been used toestimate the parameters of various hidden variable models efficiently inspecial cases as long as they satisfy certain "non-degeneracy" properties. Ourmethods give a way to go beyond this non-degeneracy barrier, and establishpolynomial identifiability of the parameters under much milder conditions.Given the importance of Kruskal's theorem in the tensor literature, we expectthat this robust version will have several applications beyond the settings weexplore in this work.
arxiv-3000-237 | Digenes: genetic algorithms to discover conjectures about directed and undirected graphs | http://arxiv.org/pdf/1304.7993v1.pdf | author:Romain Absil, Hadrien Mélot category:cs.DM cs.NE published:2013-04-30 summary:We present Digenes, a new discovery system that aims to help researchers ingraph theory. While its main task is to find extremal graphs for a given(function of) invariants, it also provides some basic support in proofconception. This has already been proved to be very useful to find newconjectures since the AutoGraphiX system of Caporossi and Hansen (DiscreteMath. 212-2000). However, unlike existing systems, Digenes can be used bothwith directed or undirected graphs. In this paper, we present the principlesand functionality of Digenes, describe the genetic algorithms that have beendesigned to achieve them, and give some computational results and openquestions. This do arise some interesting questions regarding geneticalgorithms design particular to this field, such as crossover definition.
arxiv-3000-238 | ManTIME: Temporal expression identification and normalization in the TempEval-3 challenge | http://arxiv.org/pdf/1304.7942v1.pdf | author:Michele Filannino, Gavin Brown, Goran Nenadic category:cs.CL published:2013-04-30 summary:This paper describes a temporal expression identification and normalizationsystem, ManTIME, developed for the TempEval-3 challenge. The identificationphase combines the use of conditional random fields along with apost-processing identification pipeline, whereas the normalization phase iscarried out using NorMA, an open-source rule-based temporal normalizer. Weinvestigate the performance variation with respect to different feature types.Specifically, we show that the use of WordNet-based features in theidentification task negatively affects the overall performance, and that thereis no statistically significant difference in using gazetteers, shallow parsingand propositional noun phrases labels on top of the morphological features. Onthe test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in theidentification phase. Normalization accuracies are 0.84 (type attribute) and0.77 (value attribute). Surprisingly, the use of the silver data (alone or inaddition to the gold annotated ones) does not improve the performance.
arxiv-3000-239 | Clustering processes | http://arxiv.org/pdf/1005.0826v2.pdf | author:Daniil Ryabko category:cs.LG cs.IT math.IT stat.ML published:2010-05-05 summary:The problem of clustering is considered, for the case when each data point isa sample generated by a stationary ergodic process. We propose a very naturalasymptotic notion of consistency, and show that simple consistent algorithmsexist, under most general non-parametric assumptions. The notion of consistencyis as follows: two samples should be put into the same cluster if and only ifthey were generated by the same distribution. With this notion of consistency,clustering generalizes such classical statistical problems as homogeneitytesting and process classification. We show that, for the case of a knownnumber of clusters, consistency can be achieved under the only assumption thatthe joint distribution of the data is stationary ergodic (no parametric orMarkovian assumptions, no assumptions of independence, neither between norwithin the samples). If the number of clusters is unknown, consistency can beachieved under appropriate assumptions on the mixing rates of the processes.(again, no parametric or independence assumptions). In both cases we giveexamples of simple (at most quadratic in each argument) algorithms which areconsistent.
arxiv-3000-240 | Learning Densities Conditional on Many Interacting Features | http://arxiv.org/pdf/1304.7230v2.pdf | author:David C. Kessler, Jack Taylor, David B. Dunson category:stat.ML cs.LG published:2013-04-26 summary:Learning a distribution conditional on a set of discrete-valued features is acommonly encountered task. This becomes more challenging with ahigh-dimensional feature set when there is the possibility of interactionbetween the features. In addition, many frequently applied techniques consideronly prediction of the mean, but the complete conditional density is needed toanswer more complex questions. We demonstrate a novel nonparametric Bayesmethod based upon a tensor factorization of feature-dependent weights forGaussian kernels. The method makes use of multistage feature selection fordimension reduction. The resulting conditional density morphs flexibly with theselected features.
arxiv-3000-241 | Proximal Newton-type methods for minimizing composite functions | http://arxiv.org/pdf/1206.1623v13.pdf | author:Jason D. Lee, Yuekai Sun, Michael A. Saunders category:stat.ML cs.DS cs.LG cs.NA math.OC published:2012-06-07 summary:We generalize Newton-type methods for minimizing smooth functions to handle asum of two convex functions: a smooth function and a nonsmooth function with asimple proximal mapping. We show that the resulting proximal Newton-typemethods inherit the desirable convergence behavior of Newton-type methods forminimizing smooth functions, even when search directions are computedinexactly. Many popular methods tailored to problems arising in bioinformatics,signal processing, and statistical learning are special cases of proximalNewton-type methods, and our analysis yields new convergence results for someof these methods.
arxiv-3000-242 | On the Complexity of Bandit and Derivative-Free Stochastic Convex Optimization | http://arxiv.org/pdf/1209.2388v3.pdf | author:Ohad Shamir category:cs.LG math.OC stat.ML published:2012-09-11 summary:The problem of stochastic convex optimization with bandit feedback (in thelearning community) or without knowledge of gradients (in the optimizationcommunity) has received much attention in recent years, in the form ofalgorithms and performance upper bounds. However, much less is known about theinherent complexity of these problems, and there are few lower bounds in theliterature, especially for nonlinear functions. In this paper, we investigatethe attainable error/regret in the bandit and derivative-free settings, as afunction of the dimension d and the available number of queries T. We provide aprecise characterization of the attainable performance for strongly-convex andsmooth functions, which also imply a non-trivial lower bound for more generalproblems. Moreover, we prove that in both the bandit and derivative-freesetting, the required number of queries must scale at least quadratically withthe dimension. Finally, we show that on the natural class of quadraticfunctions, it is possible to obtain a "fast" O(1/T) error rate in terms of T,under mild assumptions, even without having access to gradients. To the best ofour knowledge, this is the first such rate in a derivative-free stochasticsetting, and holds despite previous results which seem to imply the contrary.
arxiv-3000-243 | Machine Translation Systems in India | http://arxiv.org/pdf/1304.7728v1.pdf | author:Sugata Sanyal, Rajdeep Borgohain category:cs.CL cs.CY published:2013-04-29 summary:Machine Translation is the translation of one natural language into anotherusing automated and computerized means. For a multilingual country like India,with the huge amount of information exchanged between various regions and indifferent languages in digitized format, it has become necessary to find anautomated process from one language to another. In this paper, we take a lookat the various Machine Translation System in India which is specifically builtfor the purpose of translation between the Indian languages. We discuss thevarious approaches taken for building the machine translation system and thendiscuss some of the Machine Translation Systems in India along with theirfeatures.
arxiv-3000-244 | Markovian models for one dimensional structure estimation on heavily noisy imagery | http://arxiv.org/pdf/1304.7713v1.pdf | author:Ana Georgina Flesia, Javier Gimenez, Elena Rufeil Fiori category:cs.CV stat.AP published:2013-04-29 summary:Radar (SAR) images often exhibit profound appearance variations due to avariety of factors including clutter noise produced by the coherent nature ofthe illumination. Ultrasound images and infrared images have similar clutteredappearance, that make 1 dimensional structures, as edges and object boundariesdifficult to locate. Structure information is usually extracted in two steps:first, building and edge strength mask classifying pixels as edge points byhypothesis testing, and secondly estimating from that mask, pixel wideconnected edges. With constant false alarm rate (CFAR) edge strength detectorsfor speckle clutter, the image needs to be scanned by a sliding window composedof several differently oriented splitting sub-windows. The accuracy of edgelocation for these ratio detectors depends strongly on the orientation of thesub-windows. In this work we propose to transform the edge strength detectionproblem into a binary segmentation problem in the undecimated wavelet domain,solvable using parallel 1d Hidden Markov Models. For general dependency models,exact estimation of the state map becomes computationally complex, but in ourmodel, exact MAP is feasible. The effectiveness of our approach is demonstratedon simulated noisy real-life natural images with available ground truth, whilethe strength of our output edge map is measured with Pratt's, Baddeley an Kappaproficiency measures. Finally, analysis and experiments on three differenttypes of SAR images, with different polarizations, resolutions and textures,illustrate that the proposed method can detect structure on SAR imageseffectively, providing a very good start point for active contour methods.
arxiv-3000-245 | Learning Geo-Temporal Non-Stationary Failure and Recovery of Power Distribution | http://arxiv.org/pdf/1304.7710v1.pdf | author:Yun Wei, Chuanyi Ji, Floyd Galvan, Stephen Couvillon, George Orellana, James Momoh category:cs.SY cs.LG physics.soc-ph published:2013-04-29 summary:Smart energy grid is an emerging area for new applications of machinelearning in a non-stationary environment. Such a non-stationary environmentemerges when large-scale failures occur at power distribution networks due toexternal disturbances such as hurricanes and severe storms. Power distributionnetworks lie at the edge of the grid, and are especially vulnerable to externaldisruptions. Quantifiable approaches are lacking and needed to learnnon-stationary behaviors of large-scale failure and recovery of powerdistribution. This work studies such non-stationary behaviors in three aspects.First, a novel formulation is derived for an entire life cycle of large-scalefailure and recovery of power distribution. Second, spatial-temporal models offailure and recovery of power distribution are developed as geo-location basedmultivariate non-stationary GI(t)/G(t)/Infinity queues. Third, thenon-stationary spatial-temporal models identify a small number of parameters tobe learned. Learning is applied to two real-life examples of large-scaledisruptions. One is from Hurricane Ike, where data from an operational networkis exact on failures and recoveries. The other is from Hurricane Sandy, whereaggregated data is used for inferring failure and recovery processes at one ofthe impacted areas. Model parameters are learned using real data. Two findingsemerge as results of learning: (a) Failure rates behave similarly at the twodifferent provider networks for two different hurricanes but differently at thegeographical regions. (b) Both rapid- and slow-recovery are present forHurricane Ike but only slow recovery is shown for a regional distributionnetwork from Hurricane Sandy.
arxiv-3000-246 | On the convergence of the IRLS algorithm in Non-Local Patch Regression | http://arxiv.org/pdf/1303.0417v2.pdf | author:Kunal N. Chaudhury category:cs.CV stat.ML published:2013-03-02 summary:Recently, it was demonstrated in [CS2012,CS2013] that the robustness of theclassical Non-Local Means (NLM) algorithm [BCM2005] can be improved byincorporating $\ell^p (0 < p \leq 2)$ regression into the NLM framework. Thisgeneral optimization framework, called Non-Local Patch Regression (NLPR),contains NLM as a special case. Denoising results on synthetic and naturalimages show that NLPR consistently performs better than NLM beyond a moderatenoise level, and significantly so when $p$ is close to zero. An iterativelyreweighted least-squares (IRLS) algorithm was proposed for solving theregression problem in NLPR, where the NLM output was used to initialize theiterations. Based on exhaustive numerical experiments, we observe that the IRLSalgorithm is globally convergent (for arbitrary initialization) in the convexregime $1 \leq p \leq 2$, and locally convergent (fails very rarely using NLMinitialization) in the non-convex regime $0 < p < 1$. In this letter, we adaptthe "majorize-minimize" framework introduced in [Voss1980] to explain theseobservations. [CS2012] Chaudhury et al. (2012), "Non-local Euclidean medians," IEEE SignalProcessing Letters. [CS2013] Chaudhury et al. (2013), "Non-local patch regression: Robust imagedenoising in patch space," IEEE ICASSP. [BCM2005] Buades et al. (2005), "A review of image denoising algorithms, witha new one," Multiscale Modeling and Simulation. [Voss1980] Voss et al. (1980), "Linear convergence of generalized Weiszfeld'smethod," Computing.
arxiv-3000-247 | A Discrete State Transition Algorithm for Generalized Traveling Salesman Problem | http://arxiv.org/pdf/1304.7607v1.pdf | author:Xiaolin Tang, Chunhua Yang, Xiaojun Zhou, Weihua Gui category:math.OC cs.AI cs.NE published:2013-04-29 summary:Generalized traveling salesman problem (GTSP) is an extension of classicaltraveling salesman problem (TSP), which is a combinatorial optimization problemand an NP-hard problem. In this paper, an efficient discrete state transitionalgorithm (DSTA) for GTSP is proposed, where a new local search operator named\textit{K-circle}, directed by neighborhood information in space, has beenintroduced to DSTA to shrink search space and strengthen search ability. Anovel robust update mechanism, restore in probability and risk in probability(Double R-Probability), is used in our work to escape from local minima. Theproposed algorithm is tested on a set of GTSP instances. Compared with otherheuristics, experimental results have demonstrated the effectiveness and strongadaptability of DSTA and also show that DSTA has better search ability than itscompetitors.
arxiv-3000-248 | Optimal amortized regret in every interval | http://arxiv.org/pdf/1304.7577v1.pdf | author:Rina Panigrahy, Preyas Popat category:cs.LG cs.DS stat.ML published:2013-04-29 summary:Consider the classical problem of predicting the next bit in a sequence ofbits. A standard performance measure is {\em regret} (loss in payoff) withrespect to a set of experts. For example if we measure performance with respectto two constant experts one that always predicts 0's and another that alwayspredicts 1's it is well known that one can get regret $O(\sqrt T)$ with respectto the best expert by using, say, the weighted majority algorithm. But thisalgorithm does not provide performance guarantee in any interval. There areother algorithms that ensure regret $O(\sqrt {x \log T})$ in any interval oflength $x$. In this paper we show a randomized algorithm that in an amortizedsense gets a regret of $O(\sqrt x)$ for any interval when the sequence ispartitioned into intervals arbitrarily. We empirically estimated the constantin the $O()$ for $T$ upto 2000 and found it to be small -- around 2.1. We alsoexperimentally evaluate the efficacy of this algorithm in predicting highfrequency stock data.
arxiv-3000-249 | Fractal structures in Adversarial Prediction | http://arxiv.org/pdf/1304.7576v1.pdf | author:Rina Panigrahy, Preyas Popat category:cs.LG published:2013-04-29 summary:Fractals are self-similar recursive structures that have been used inmodeling several real world processes. In this work we study how "fractal-like"processes arise in a prediction game where an adversary is generating asequence of bits and an algorithm is trying to predict them. We will see thatunder a certain formalization of the predictive payoff for the algorithm it ismost optimal for the adversary to produce a fractal-like sequence to minimizethe algorithm's ability to predict. Indeed it has been suggested before thatfinancial markets exhibit a fractal-like behavior. We prove that a fractal-likedistribution arises naturally out of an optimization from the adversary'sperspective. In addition, we give optimal trade-offs between predictability and expecteddeviation (i.e. sum of bits) for our formalization of predictive payoff. Thisresult is motivated by the observation that several time series data exhibithigher deviations than expected for a completely random walk.
arxiv-3000-250 | Semi-supervised Eigenvectors for Large-scale Locally-biased Learning | http://arxiv.org/pdf/1304.7528v1.pdf | author:Toke J. Hansen, Michael W. Mahoney category:cs.LG math.SP stat.ML published:2013-04-28 summary:In many applications, one has side information, e.g., labels that areprovided in a semi-supervised manner, about a specific target region of a largedata set, and one wants to perform machine learning and data analysis tasks"nearby" that prespecified target region. For example, one might be interestedin the clustering structure of a data graph near a prespecified "seed set" ofnodes, or one might be interested in finding partitions in an image that arenear a prespecified "ground truth" set of pixels. Locally-biased problems ofthis sort are particularly challenging for popular eigenvector-based machinelearning and data analysis tools. At root, the reason is that eigenvectors areinherently global quantities, thus limiting the applicability ofeigenvector-based methods in situations where one is interested in very localproperties of the data. In this paper, we address this issue by providing a methodology to constructsemi-supervised eigenvectors of a graph Laplacian, and we illustrate how theselocally-biased eigenvectors can be used to perform locally-biased machinelearning. These semi-supervised eigenvectors capturesuccessively-orthogonalized directions of maximum variance, conditioned onbeing well-correlated with an input seed set of nodes that is assumed to beprovided in a semi-supervised manner. We show that these semi-supervisedeigenvectors can be computed quickly as the solution to a system of linearequations; and we also describe several variants of our basic method that haveimproved scaling properties. We provide several empirical examplesdemonstrating how these semi-supervised eigenvectors can be used to performlocally-biased learning; and we discuss the relationship between our resultsand recent machine learning algorithms that use global eigenvectors of thegraph Laplacian.
arxiv-3000-251 | Towards a Formal Distributional Semantics: Simulating Logical Calculi with Tensors | http://arxiv.org/pdf/1304.5823v2.pdf | author:Edward Grefenstette category:math.LO cs.CL cs.LO 68T50, 03B10 F.4.1; I.2.7 published:2013-04-22 summary:The development of compositional distributional models of semanticsreconciling the empirical aspects of distributional semantics with thecompositional aspects of formal semantics is a popular topic in thecontemporary literature. This paper seeks to bring this reconciliation one stepfurther by showing how the mathematical constructs commonly used incompositional distributional models, such as tensors and matrices, can be usedto simulate different aspects of predicate logic. This paper discusses how the canonical isomorphism between tensors andmultilinear maps can be exploited to simulate a full-blown quantifier-freepredicate calculus using tensors. It provides tensor interpretations of the setof logical connectives required to model propositional calculi. It suggests avariant of these tensor calculi capable of modelling quantifiers, using fewnon-linear operations. It finally discusses the relation between thesevariants, and how this relation should constitute the subject of future work.
arxiv-3000-252 | Measuring Cultural Relativity of Emotional Valence and Arousal using Semantic Clustering and Twitter | http://arxiv.org/pdf/1304.7507v1.pdf | author:Eugene Yuta Bann, Joanna J. Bryson category:cs.CL cs.AI published:2013-04-28 summary:Researchers since at least Darwin have debated whether and to what extentemotions are universal or culture-dependent. However, previous studies haveprimarily focused on facial expressions and on a limited set of emotions. Giventhat emotions have a substantial impact on human lives, evidence for culturalemotional relativity might be derived by applying distributional semanticstechniques to a text corpus of self-reported behaviour. Here, we explore thisidea by measuring the valence and arousal of the twelve most popular emotionkeywords expressed on the micro-blogging site Twitter. We do this in threegeographical regions: Europe, Asia and North America. We demonstrate that inour sample, the valence and arousal levels of the same emotion keywords differsignificantly with respect to these geographical regions --- Europeans are, orat least present themselves as more positive and aroused, North Americans aremore negative and Asians appear to be more positive but less aroused whencompared to global valence and arousal levels of the same emotion keywords. Ourwork is the first in kind to programatically map large text corpora to adimensional model of affect.
arxiv-3000-253 | Deterministic Initialization of the K-Means Algorithm Using Hierarchical Clustering | http://arxiv.org/pdf/1304.7465v1.pdf | author:M. Emre Celebi, Hassan A. Kingravi category:cs.LG cs.CV I.5.3; H.2.8 published:2013-04-28 summary:K-means is undoubtedly the most widely used partitional clustering algorithm.Unfortunately, due to its gradient descent nature, this algorithm is highlysensitive to the initial placement of the cluster centers. Numerousinitialization methods have been proposed to address this problem. Many ofthese methods, however, have superlinear complexity in the number of datapoints, making them impractical for large data sets. On the other hand, linearmethods are often random and/or order-sensitive, which renders their resultsunrepeatable. Recently, Su and Dy proposed two highly successful hierarchicalinitialization methods named Var-Part and PCA-Part that are not only linear,but also deterministic (non-random) and order-invariant. In this paper, wepropose a discriminant analysis based approach that addresses a commondeficiency of these two methods. Experiments on a large and diverse collectionof data sets from the UCI Machine Learning Repository demonstrate that Var-Partand PCA-Part are highly competitive with one of the best random initializationmethods to date, i.e., k-means++, and that the proposed approach significantlyimproves the performance of both hierarchical methods.
arxiv-3000-254 | Storing cycles in Hopfield-type networks with pseudoinverse learning rule: admissibility and network topology | http://arxiv.org/pdf/1211.4520v2.pdf | author:Chuan Zhang, Gerhard Dangelmayr, Iuliana Oprea category:cs.NE 15A04 published:2012-11-19 summary:Cyclic patterns of neuronal activity are ubiquitous in animal nervoussystems, and partially responsible for generating and controlling rhythmicmovements such as locomotion, respiration, swallowing and so on. Clarifying therole of the network connectivities for generating cyclic patterns isfundamental for understanding the generation of rhythmic movements. In thispaper, the storage of binary cycles in neural networks is investigated. We calla cycle $\Sigma$ admissible if a connectivity matrix satisfying the cycle'stransition conditions exists, and construct it using the pseudoinverse learningrule. Our main focus is on the structural features of admissible cycles andcorresponding network topology. We show that $\Sigma$ is admissible if and onlyif its discrete Fourier transform contains exactly $r={rank}(\Sigma)$ nonzerocolumns. Based on the decomposition of the rows of $\Sigma$ into loops, where aloop is the set of all cyclic permutations of a row, cycles are classified assimple cycles, separable or inseparable composite cycles. Simple cycles containrows from one loop only, and the network topology is a feedforward chain withfeedback to one neuron if the loop-vectors in $\Sigma$ are cyclic permutationsof each other. Composite cycles contain rows from at least two disjoint loops,and the neurons corresponding to the rows in $\Sigma$ from the same loop areidentified with a cluster. Networks constructed from separable composite cyclesdecompose into completely isolated clusters. For inseparable composite cyclesat least two clusters are connected, and the cluster-connectivity is related tothe intersections of the spaces spanned by the loop-vectors of the clusters.Simulations showing successfully retrieved cycles in continuous-timeHopfield-type networks and in networks of spiking neurons are presented.
arxiv-3000-255 | On Integrating Fuzzy Knowledge Using a Novel Evolutionary Algorithm | http://arxiv.org/pdf/1304.7423v1.pdf | author:Nafisa Afrin Chowdhury, Murshida Khatun, M. M. A. Hashem category:cs.NE cs.AI published:2013-04-28 summary:Fuzzy systems may be considered as knowledge-based systems that incorporateshuman knowledge into their knowledge base through fuzzy rules and fuzzymembership functions. The intent of this study is to present a fuzzy knowledgeintegration framework using a Novel Evolutionary Strategy (NES), which cansimultaneously integrate multiple fuzzy rule sets and their membership functionsets. The proposed approach consists of two phases: fuzzy knowledge encodingand fuzzy knowledge integration. Four application domains, the hepatitisdiagnosis, the sugarcane breeding prediction, Iris plants classification, andTic-tac-toe endgame were used to show the performance ofthe proposed knowledgeapproach. Results show that the fuzzy knowledge base derived using our approachperforms better than Genetic Algorithm based approach.
arxiv-3000-256 | Bingham Procrustean Alignment for Object Detection in Clutter | http://arxiv.org/pdf/1304.7399v1.pdf | author:Jared Glover, Sanja Popovic category:cs.CV cs.RO stat.AP published:2013-04-27 summary:A new system for object detection in cluttered RGB-D images is presented. Ourmain contribution is a new method called Bingham Procrustean Alignment (BPA) toalign models with the scene. BPA uses point correspondences between orientedfeatures to derive a probability distribution over possible model poses. Theorientation component of this distribution, conditioned on the position, isshown to be a Bingham distribution. This result also applies to the classicproblem of least-squares alignment of point sets, when point features areorientation-less, and gives a principled, probabilistic way to measure poseuncertainty in the rigid alignment problem. Our detection system leverages BPAto achieve more reliable object detections in clutter.
arxiv-3000-257 | TimeML-strict: clarifying temporal annotation | http://arxiv.org/pdf/1304.7289v1.pdf | author:Leon Derczynski, Hector Llorens, Naushad UzZaman category:cs.CL I.2.7 published:2013-04-26 summary:TimeML is an XML-based schema for annotating temporal information overdiscourse. The standard has been used to annotate a variety of resources and isfollowed by a number of tools, the creation of which constitute hundreds ofthousands of man-hours of research work. However, the current state ofresources is such that many are not valid, or do not produce valid output, orcontain ambiguous or custom additions and removals. Difficulties arising fromthese variances were highlighted in the TempEval-3 exercise, which included itsown extra stipulations over conventional TimeML as a response. To unify the state of current resources, and to make progress toward easyadoption of its current incarnation ISO-TimeML, this paper introducesTimeML-strict: a valid, unambiguous, and easy-to-process subset of TimeML. Wealso introduce three resources -- a schema for TimeML-strict; a validator toolfor TimeML-strict, so that one may ensure documents are in the correct form;and a repair tool that corrects common invalidating errors and addsdisambiguating markup in order to convert documents from the laxer TimeMLstandard to TimeML-strict.
arxiv-3000-258 | Active and passive learning of linear separators under log-concave distributions | http://arxiv.org/pdf/1211.1082v3.pdf | author:Maria Florina Balcan, Philip M. Long category:cs.LG math.ST stat.ML stat.TH published:2012-11-06 summary:We provide new results concerning label efficient, polynomial time, passiveand active learning of linear separators. We prove that active learningprovides an exponential improvement over PAC (passive) learning of homogeneouslinear separators under nearly log-concave distributions. Building on this, weprovide a computationally efficient PAC algorithm with optimal (up to aconstant factor) sample complexity for such problems. This resolves an openquestion concerning the sample complexity of efficient PAC algorithms under theuniform distribution in the unit ball. Moreover, it provides the first boundfor a polynomial-time PAC algorithm that is tight for an interesting infiniteclass of hypothesis functions under a general and natural class ofdata-distributions, providing significant progress towards a longstanding openquestion. We also provide new bounds for active and passive learning in the case thatthe data might not be linearly separable, both in the agnostic case and andunder the Tsybakov low-noise condition. To derive our results, we provide newstructural results for (nearly) log-concave distributions, which might be ofindependent interest as well.
arxiv-3000-259 | In the sight of my wearable camera: Classifying my visual experience | http://arxiv.org/pdf/1304.7236v1.pdf | author:Alessandro Perina, Nebojsa Jojic category:cs.CV published:2013-04-26 summary:We introduce and we analyze a new dataset which resembles the input tobiological vision systems much more than most previously published ones. Ouranalysis leaded to several important conclusions. First, it is possible todisambiguate over dozens of visual scenes (locations) encountered over thecourse of several weeks of a human life with accuracy of over 80%, and thisopens up possibility for numerous novel vision applications, from earlydetection of dementia to everyday use of wearable camera streams for automaticreminders, and visual stream exchange. Second, our experimental resultsindicate that, generative models such as Latent Dirichlet Allocation orCounting Grids, are more suitable to such types of data, as they are morerobust to overtraining and comfortable with images at low resolution, blurredand characterized by relatively random clutter and a mix of objects.
arxiv-3000-260 | Runtime Optimizations for Prediction with Tree-Based Models | http://arxiv.org/pdf/1212.2287v2.pdf | author:Nima Asadi, Jimmy Lin, Arjen P. de Vries category:cs.DB cs.IR cs.LG published:2012-12-11 summary:Tree-based models have proven to be an effective solution for web ranking aswell as other problems in diverse domains. This paper focuses on optimizing theruntime performance of applying such models to make predictions, given analready-trained model. Although exceedingly simple conceptually, mostimplementations of tree-based models do not efficiently utilize modernsuperscalar processor architectures. By laying out data structures in memory ina more cache-conscious fashion, removing branches from the execution flow usinga technique called predication, and micro-batching predictions using atechnique called vectorization, we are able to better exploit modern processorarchitectures and significantly improve the speed of tree-based models overhard-coded if-else blocks. Our work contributes to the exploration ofarchitecture-conscious runtime implementations of machine learning algorithms.
arxiv-3000-261 | Algorithmic Optimisations for Iterative Deconvolution Methods | http://arxiv.org/pdf/1304.7211v1.pdf | author:Martin Welk, Martin Erler category:cs.CV I.4.4; F.2.1 published:2013-04-26 summary:We investigate possibilities to speed up iterative algorithms for non-blindimage deconvolution. We focus on algorithms in which convolution with thepoint-spread function to be deconvolved is used in each iteration, and aim ataccelerating these convolution operations as they are typically the mostexpensive part of the computation. We follow two approaches: First, for somepractically important specific point-spread functions, algorithmicallyefficient sliding window or list processing techniques can be used. In someconstellations this allows faster computation than via the Fourier domain.Second, as iterations progress, computation of convolutions can be restrictedto subsets of pixels. For moderate thinning rates this can be done with almostno impact on the reconstruction quality. Both approaches are demonstrated inthe context of Richardson-Lucy deconvolution but are not restricted to thismethod.
arxiv-3000-262 | Computational Lower Bounds for Sparse PCA | http://arxiv.org/pdf/1304.0828v2.pdf | author:Quentin Berthet, Philippe Rigollet category:math.ST cs.CC stat.ML stat.TH 62C20 published:2013-04-03 summary:In the context of sparse principal component detection, we bring evidencetowards the existence of a statistical price to pay for computationalefficiency. We measure the performance of a test by the smallest signalstrength that it can detect and we propose a computationally efficient methodbased on semidefinite programming. We also prove that the statisticalperformance of this test cannot be strictly improved by any computationallyefficient method. Our results can be viewed as complexity theoretic lowerbounds conditionally on the assumptions that some instances of the plantedclique problem cannot be solved in randomized polynomial time.
arxiv-3000-263 | Reading Ancient Coin Legends: Object Recognition vs. OCR | http://arxiv.org/pdf/1304.7184v1.pdf | author:Albert Kavelar, Sebastian Zambanini, Martin Kampel category:cs.CV published:2013-04-26 summary:Standard OCR is a well-researched topic of computer vision and can beconsidered solved for machine-printed text. However, when applied tounconstrained images, the recognition rates drop drastically. Therefore, theemployment of object recognition-based techniques has become state of the artin scene text recognition applications. This paper presents a scene textrecognition method tailored to ancient coin legends and compares the resultsachieved in character and word recognition experiments to a standard OCRengine. The conducted experiments show that the proposed method outperforms thestandard OCR engine on a set of 180 cropped coin legend words.
arxiv-3000-264 | Irreflexive and Hierarchical Relations as Translations | http://arxiv.org/pdf/1304.7158v1.pdf | author:Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko category:cs.LG published:2013-04-26 summary:We consider the problem of embedding entities and relations of knowledgebases in low-dimensional vector spaces. Unlike most existing approaches, whichare primarily efficient for modeling equivalence relations, our approach isdesigned to explicitly model irreflexive relations, such as hierarchies, byinterpreting them as translations operating on the low-dimensional embeddingsof the entities. Preliminary experiments show that, despite its simplicity anda smaller number of parameters than previous approaches, our approach achievesstate-of-the-art performance according to standard evaluation protocols on datafrom WordNet and Freebase.
arxiv-3000-265 | Question Answering Against Very-Large Text Collections | http://arxiv.org/pdf/1304.7157v1.pdf | author:Leon Derczynski, Richard Shaw, Ben Solway, Jun Wang category:cs.CL cs.IR published:2013-04-26 summary:Question answering involves developing methods to extract useful informationfrom large collections of documents. This is done with specialised searchengines such as Answer Finder. The aim of Answer Finder is to provide an answerto a question rather than a page listing related documents that may contain thecorrect answer. So, a question such as "How tall is the Eiffel Tower" wouldsimply return "325m" or "1,063ft". Our task was to build on the current versionof Answer Finder by improving information retrieval, and also improving thepre-processing involved in question series analysis.
arxiv-3000-266 | A Convex Approach for Image Hallucination | http://arxiv.org/pdf/1304.7153v1.pdf | author:Peter Innerhofer, Thomas Pock category:cs.CV published:2013-04-26 summary:In this paper we propose a global convex approach for image hallucination.Altering the idea of classical multi image super resolution (SU) systems tosingle image SU, we incorporate aligned images to hallucinate the output. Ourwork is based on the paper of Tappen et al. where they use a non-convex modelfor image hallucination. In comparison we formulate a convex primaloptimization problem and derive a fast converging primal-dual algorithm with aglobal optimal solution. We use a database with face images to incorporatehigh-frequency details to the high-resolution output. We show that we canachieve state-of-the-art results by using a convex approach.
arxiv-3000-267 | Cutting Recursive Autoencoder Trees | http://arxiv.org/pdf/1301.2811v3.pdf | author:Christian Scheible, Hinrich Schuetze category:cs.CL cs.AI published:2013-01-13 summary:Deep Learning models enjoy considerable success in Natural LanguageProcessing. While deep architectures produce useful representations that leadto improvements in various tasks, they are often difficult to interpret. Thismakes the analysis of learned structures particularly difficult. In this paper,we rely on empirical tests to see whether a particular structure makes sense.We present an analysis of the Semi-Supervised Recursive Autoencoder, awell-known model that produces structural representations of text. We show thatfor certain tasks, the structure of the autoencoder can be significantlyreduced without loss of classification accuracy and we evaluate the producedstructures using human judgment.
arxiv-3000-268 | Pulmonary Vascular Tree Segmentation from Contrast-Enhanced CT Images | http://arxiv.org/pdf/1304.7140v1.pdf | author:M. Helmberger, M. Urschler, M. Pienn, Z. Balint, A. Olschewski, H. Bischof category:cs.CV physics.med-ph published:2013-04-26 summary:We present a pulmonary vessel segmentation algorithm, which is fast, fullyautomatic and robust. It uses a coarse segmentation of the airway tree and aleft and right lung labeled volume to restrict a vessel enhancement filter,based on an offset medialness function, to the lungs. We show the applicationof our algorithm on contrast-enhanced CT images, where we derive a clinicalparameter to detect pulmonary hypertension (PH) in patients. Results on adataset of 24 patients show that quantitative indices derived from thesegmentation are applicable to distinguish patients with and without PH.Further work-in-progress results are shown on the VESSEL12 challenge dataset,which is composed of non-contrast-enhanced scans, where we range in themidfield of participating contestants.
arxiv-3000-269 | Combinaison d'information visuelle, conceptuelle, et contextuelle pour la construction automatique de hierarchies semantiques adaptees a l'annotation d'images | http://arxiv.org/pdf/1304.5063v2.pdf | author:Hichem Bannour, Céline Hudelot category:cs.CV cs.LG cs.MM 68T45 I.4.10 published:2013-04-18 summary:This paper proposes a new methodology to automatically build semantichierarchies suitable for image annotation and classification. The building ofthe hierarchy is based on a new measure of semantic similarity. The proposedmeasure incorporates several sources of information: visual, conceptual andcontextual as we defined in this paper. The aim is to provide a measure thatbest represents image semantics. We then propose rules based on this measure,for the building of the final hierarchy, and which explicitly encodehierarchical relationships between different concepts. Therefore, the builthierarchy is used in a semantic hierarchical classification framework for imageannotation. Our experiments and results show that the hierarchy built improvesclassification results. Ce papier propose une nouvelle methode pour la construction automatique dehierarchies semantiques adaptees a la classification et a l'annotationd'images. La construction de la hierarchie est basee sur une nouvelle mesure desimilarite semantique qui integre plusieurs sources d'informations: visuelle,conceptuelle et contextuelle que nous definissons dans ce papier. L'objectifest de fournir une mesure qui est plus proche de la semantique des images. Nousproposons ensuite des regles, basees sur cette mesure, pour la construction dela hierarchie finale qui encode explicitement les relations hierarchiques entreles differents concepts. La hierarchie construite est ensuite utilisee dans uncadre de classification semantique hierarchique d'images en concepts visuels.Nos experiences et resultats montrent que la hierarchie construite permetd'ameliorer les resultats de la classification.
arxiv-3000-270 | Filament and Flare Detection in Hα image sequences | http://arxiv.org/pdf/1304.7132v1.pdf | author:Gernot Riegler, Thomas Pock, Werner Pötzi, Astrid Veronig category:cs.CV astro-ph.IM published:2013-04-26 summary:Solar storms can have a major impact on the infrastructure of the earth. Someof the causing events are observable from ground in the H{\alpha} spectralline. In this paper we propose a new method for the simultaneous detection offlares and filaments in H{\alpha} image sequences. Therefore we perform severalpreprocessing steps to enhance and normalize the images. Based on the intensityvalues we segment the image by a variational approach. In a finalpostprecessing step we derive essential properties to classify the events andfurther demonstrate the performance by comparing our obtained results to thedata annotated by an expert. The information produced by our method can be usedfor near real-time alerts and the statistical analysis of existing data bysolar physicists.
arxiv-3000-271 | Synthesis of neural networks for spatio-temporal spike pattern recognition and processing | http://arxiv.org/pdf/1304.7118v1.pdf | author:J. Tapson, G. Cohen, S. Afshar, K. Stiefel, Y. Buskila, R. Wang, T. J. Hamilton, A. van Schaik category:cs.NE q-bio.NC published:2013-04-26 summary:The advent of large scale neural computational platforms has highlighted thelack of algorithms for synthesis of neural structures to perform predefinedcognitive tasks. The Neural Engineering Framework offers one such synthesis,but it is most effective for a spike rate representation of neural information,and it requires a large number of neurons to implement simple functions. Wedescribe a neural network synthesis method that generates synaptic connectivityfor neurons which process time-encoded neural signals, and which makes verysparse use of neurons. The method allows the user to specify, arbitrarily,neuronal characteristics such as axonal and dendritic delays, and synaptictransfer functions, and then solves for the optimal input-output relationshipusing computed dendritic weights. The method may be used for batch or onlinelearning and has an extremely fast optimization process. We demonstrate its usein generating a network to recognize speech which is sparsely encoded as spiketimes.
arxiv-3000-272 | Digit Recognition in Handwritten Weather Records | http://arxiv.org/pdf/1304.6933v2.pdf | author:Manuel Keglevic, Robert Sablatnig category:cs.CV published:2013-04-25 summary:This paper addresses the automatic recognition of handwritten temperaturevalues in weather records. The localization of table cells is based on linedetection using projection profiles. Further, a stroke-preserving line removalmethod which is based on gradient images is proposed. The presented digitrecognition utilizes features which are extracted using a set of filters and aSupport Vector Machine classifier. It was evaluated on the MNIST and the USPSdataset and our own database with about 17,000 RGB digit images. An accuracy of99.36% per digit is achieved for the entire system using a set of 84 weatherrecords.
arxiv-3000-273 | An Impossibility Result for High Dimensional Supervised Learning | http://arxiv.org/pdf/1301.6915v2.pdf | author:Mohammad Hossein Rohban, Prakash Ishwar, Birant Orten, William C. Karl, Venkatesh Saligrama category:stat.ML published:2013-01-29 summary:We study high-dimensional asymptotic performance limits of binary supervisedclassification problems where the class conditional densities are Gaussian withunknown means and covariances and the number of signal dimensions scales fasterthan the number of labeled training samples. We show that the Bayes error,namely the minimum attainable error probability with complete distributionalknowledge and equally likely classes, can be arbitrarily close to zero and yetthe limiting minimax error probability of every supervised learning algorithmis no better than a random coin toss. In contrast to related studies where theclassification difficulty (Bayes error) is made to vanish, we hold it constantwhen taking high-dimensional limits. In contrast to VC-dimension based minimaxlower bounds that consider the worst case error probability over alldistributions that have a fixed Bayes error, our worst case is over the familyof Gaussian distributions with constant Bayes error. We also show that anontrivial asymptotic minimax error probability can only be attained forparametric subsets of zero measure (in a suitable measure space). These resultsexpose the fundamental importance of prior knowledge and suggest that unless weimpose strong structural constraints, such as sparsity, on the parametricspace, supervised learning may be ineffective in high dimensional small samplesettings.
arxiv-3000-274 | Euclidean Upgrade from a Minimal Number of Segments | http://arxiv.org/pdf/1304.6990v1.pdf | author:Tanja Schilling, Tomas Pajdla category:cs.CV published:2013-04-25 summary:In this paper, we propose an algebraic approach to upgrade a projectivereconstruction to a Euclidean one, and aim at computing the rectifyinghomography from a minimal number of 9 segments of known length. Constraints arederived from these segments which yield a set of polynomial equations that wesolve by means of Gr\"obner bases. We explain how a solver for such a system ofequations can be constructed from simplified template data. Moreover, wepresent experiments that demonstrate that the given problem can be solved inthis way.
arxiv-3000-275 | On latent position inference from doubly stochastic messaging activities | http://arxiv.org/pdf/1205.5920v5.pdf | author:Nam H. Lee, Jordan Yoder, Minh Tang, Carey E Priebe category:stat.ML math.ST stat.ME stat.TH published:2012-05-26 summary:We model messaging activities as a hierarchical doubly stochastic pointprocess with three main levels, and develop an iterative algorithm forinferring actors' relative latent positions from a stream of messaging activitydata. Each of the message-exchanging actors is modeled as a process in a latentspace. The actors' latent positions are assumed to be influenced by thedistribution of a much larger population over the latent space. Each actor'smovement in the latent space is modeled as being governed by two parametersthat we call confidence and visibility, in addition to dependence on thepopulation distribution. The messaging frequency between a pair of actors isassumed to be inversely proportional to the distance between their latentpositions. Our inference algorithm is based on a projection approach to anonline filtering problem. The algorithm associates each actor with aprobability density-valued process, and each probability density is assumed tobe a mixture of basis functions. For efficient numerical experiments, wefurther develop our algorithm for the case where the basis functions areobtained by translating and scaling a standard Gaussian density.
arxiv-3000-276 | Unsupervised Feature Learning for low-level Local Image Descriptors | http://arxiv.org/pdf/1301.2840v4.pdf | author:Christian Osendorfer, Justin Bayer, Sebastian Urban, Patrick van der Smagt category:cs.CV cs.LG stat.ML published:2013-01-14 summary:Unsupervised feature learning has shown impressive results for a wide rangeof input modalities, in particular for object classification tasks in computervision. Using a large amount of unlabeled data, unsupervised feature learningmethods are utilized to construct high-level representations that arediscriminative enough for subsequently trained supervised classificationalgorithms. However, it has never been \emph{quantitatively} investigated yethow well unsupervised learning methods can find \emph{low-levelrepresentations} for image patches without any additional supervision. In thispaper we examine the performance of pure unsupervised methods on a low-levelcorrespondence task, a problem that is central to many Computer Visionapplications. We find that a special type of Restricted Boltzmann Machines(RBMs) performs comparably to hand-crafted descriptors. Additionally, a simplebinarization scheme produces compact representations that perform better thanseveral state-of-the-art descriptors.
arxiv-3000-277 | An implementation of the relational k-means algorithm | http://arxiv.org/pdf/1304.6899v1.pdf | author:Balázs Szalkai category:cs.LG cs.CV cs.MS published:2013-04-25 summary:A C# implementation of a generalized k-means variant called relationalk-means is described here. Relational k-means is a generalization of thewell-known k-means clustering method which works for non-Euclidean scenarios aswell. The input is an arbitrary distance matrix, as opposed to the traditionalk-means method, where the clustered objects need to be identified with vectors.
arxiv-3000-278 | Inverse Density as an Inverse Problem: The Fredholm Equation Approach | http://arxiv.org/pdf/1304.5575v2.pdf | author:Qichao Que, Mikhail Belkin category:cs.LG stat.ML published:2013-04-20 summary:In this paper we address the problem of estimating the ratio $\frac{q}{p}$where $p$ is a density function and $q$ is another density, or, more generallyan arbitrary function. Knowing or approximating this ratio is needed in variousproblems of inference and integration, in particular, when one needs to averagea function with respect to one probability distribution, given a sample fromanother. It is often referred as {\it importance sampling} in statisticalinference and is also closely related to the problem of {\it covariate shift}in transfer learning as well as to various MCMC methods. It may also be usefulfor separating the underlying geometry of a space, say a manifold, from thedensity function defined on it. Our approach is based on reformulating the problem of estimating$\frac{q}{p}$ as an inverse problem in terms of an integral operatorcorresponding to a kernel, and thus reducing it to an integral equation, knownas the Fredholm problem of the first kind. This formulation, combined with thetechniques of regularization and kernel methods, leads to a principledkernel-based framework for constructing algorithms and for analyzing themtheoretically. The resulting family of algorithms (FIRE, for Fredholm Inverse RegularizedEstimator) is flexible, simple and easy to implement. We provide detailed theoretical analysis including concentration bounds andconvergence rates for the Gaussian kernel in the case of densities defined on$\R^d$, compact domains in $\R^d$ and smooth $d$-dimensional sub-manifolds ofthe Euclidean space. We also show experimental results including applications to classificationand semi-supervised learning within the covariate shift framework anddemonstrate some encouraging experimental comparisons. We also show how theparameters of our algorithms can be chosen in a completely unsupervised manner.
arxiv-3000-279 | An Improved Approach for Word Ambiguity Removal | http://arxiv.org/pdf/1304.7282v1.pdf | author:Priti Saktel, Urmila Shrawankar category:cs.CL published:2013-04-25 summary:Word ambiguity removal is a task of removing ambiguity from a word, i.e.correct sense of word is identified from ambiguous sentences. This paperdescribes a model that uses Part of Speech tagger and three categories for wordsense disambiguation (WSD). Human Computer Interaction is very needful toimprove interactions between users and computers. For this, the Supervised andUnsupervised methods are combined. The WSD algorithm is used to find theefficient and accurate sense of a word based on domain information. Theaccuracy of this work is evaluated with the aim of finding best suitable domainof word.
arxiv-3000-280 | Low-rank optimization for distance matrix completion | http://arxiv.org/pdf/1304.6663v2.pdf | author:B. Mishra, G. Meyer, R. Sepulchre category:math.OC cs.LG stat.ML published:2013-04-24 summary:This paper addresses the problem of low-rank distance matrix completion. Thisproblem amounts to recover the missing entries of a distance matrix when thedimension of the data embedding space is possibly unknown but small compared tothe number of considered data points. The focus is on high-dimensionalproblems. We recast the considered problem into an optimization problem overthe set of low-rank positive semidefinite matrices and propose two efficientalgorithms for low-rank distance matrix completion. In addition, we propose astrategy to determine the dimension of the embedding space. The resultingalgorithms scale to high-dimensional problems and monotonically converge to aglobal solution of the problem. Finally, numerical experiments illustrate thegood performance of the proposed algorithms on benchmarks.
arxiv-3000-281 | Inference and learning in probabilistic logic programs using weighted Boolean formulas | http://arxiv.org/pdf/1304.6810v1.pdf | author:Daan Fierens, Guy Van den Broeck, Joris Renkens, Dimitar Shterionov, Bernd Gutmann, Ingo Thon, Gerda Janssens, Luc De Raedt category:cs.AI cs.LG cs.LO published:2013-04-25 summary:Probabilistic logic programs are logic programs in which some of the factsare annotated with probabilities. This paper investigates how classicalinference and learning tasks known from the graphical model community can betackled for probabilistic logic programs. Several such tasks such as computingthe marginals given evidence and learning from (partial) interpretations havenot really been addressed for probabilistic logic programs before. The first contribution of this paper is a suite of efficient algorithms forvarious inference tasks. It is based on a conversion of the program and thequeries and evidence to a weighted Boolean formula. This allows us to reducethe inference tasks to well-studied tasks such as weighted model counting,which can be solved using state-of-the-art methods known from the graphicalmodel and knowledge compilation literature. The second contribution is analgorithm for parameter estimation in the learning from interpretationssetting. The algorithm employs Expectation Maximization, and is built on top ofthe developed inference algorithms. The proposed approach is experimentally evaluated. The results show that theinference algorithms improve upon the state-of-the-art in probabilistic logicprogramming and that it is indeed possible to learn the parameters of aprobabilistic logic program from interpretations.
arxiv-3000-282 | k-Modulus Method for Image Transformation | http://arxiv.org/pdf/1304.6759v1.pdf | author:Firas A. Jassim category:cs.CV published:2013-04-24 summary:In this paper, we propose a new algorithm to make a novel spatial imagetransformation. The proposed approach aims to reduce the bit depth used forimage storage. The basic technique for the proposed transformation is based ofthe modulus operator. The goal is to transform the whole image into multiplesof predefined integer. The division of the whole image by that integer willguarantee that the new image surely less in size from the original image. Thek-Modulus Method could not be used as a stand alone transform for imagecompression because of its high compression ratio. It could be used as a schemeembedded in other image processing fields especially compression. According toits high PSNR value, it could be amalgamated with other methods to facilitatethe redundancy criterion.
arxiv-3000-283 | Comparison of several reweighted l1-algorithms for solving cardinality minimization problems | http://arxiv.org/pdf/1304.6655v1.pdf | author:Mohammad Javad Abdi category:math.OC stat.ML published:2013-04-24 summary:Reweighted l1-algorithms have attracted a lot of attention in the field ofapplied mathematics. A unified framework of such algorithms has been recentlyproposed by Zhao and Li. In this paper we construct a few new examples ofreweighted l1-methods. These functions are certain concave approximations ofthe l0-norm function. We focus on the numerical comparison between some new andexisting reweighted l1-algorithms. We show how the change of parameters inreweighted algorithms may affect the performance of the algorithms for findingthe solution of the cardinality minimization problem. In our experiments, theproblem data were generated according to different statistical distributions,and we test the algorithms on different sparsity level of the solution of theproblem. Our numerical results demonstrate that the reweighted l1-method is oneof the efficient methods for locating the solution of the cardinalityminimization problem.
arxiv-3000-284 | Fixed-rank matrix factorizations and Riemannian low-rank optimization | http://arxiv.org/pdf/1209.0430v2.pdf | author:B. Mishra, G. Meyer, S. Bonnabel, R. Sepulchre category:cs.LG math.OC published:2012-09-03 summary:Motivated by the problem of learning a linear regression model whoseparameter is a large fixed-rank non-symmetric matrix, we consider theoptimization of a smooth cost function defined on the set of fixed-rankmatrices. We adopt the geometric framework of optimization on Riemannianquotient manifolds. We study the underlying geometries of several well-knownfixed-rank matrix factorizations and then exploit the Riemannian quotientgeometry of the search space in the design of a class of gradient descent andtrust-region algorithms. The proposed algorithms generalize our previousresults on fixed-rank symmetric positive semidefinite matrices, apply to abroad range of applications, scale to high-dimensional problems and confer ageometric basis to recent contributions on the learning of fixed-ranknon-symmetric matrices. We make connections with existing algorithms in thecontext of low-rank matrix completion and discuss relative usefulness of theproposed framework. Numerical experiments suggest that the proposed algorithmscompete with the state-of-the-art and that manifold optimization offers aneffective and versatile framework for the design of machine learning algorithmsthat learn a fixed-rank matrix.
arxiv-3000-285 | Locally linear representation for subspace learning and clustering | http://arxiv.org/pdf/1304.6487v1.pdf | author:Liangli Zhen, Zhang Yi, Xi Peng, Dezhong Peng category:cs.LG stat.ML published:2013-04-24 summary:It is a key to construct a similarity graph in graph-oriented subspacelearning and clustering. In a similarity graph, each vertex denotes a datapoint and the edge weight represents the similarity between two points. Thereare two popular schemes to construct a similarity graph, i.e., pairwisedistance based scheme and linear representation based scheme. Most existingworks have only involved one of the above schemes and suffered from somelimitations. Specifically, pairwise distance based methods are sensitive to thenoises and outliers compared with linear representation based methods. On theother hand, there is the possibility that linear representation basedalgorithms wrongly select inter-subspaces points to represent a point, whichwill degrade the performance. In this paper, we propose an algorithm, calledLocally Linear Representation (LLR), which integrates pairwise distance withlinear representation together to address the problems. The proposed algorithmcan automatically encode each data point over a set of points that not onlycould denote the objective point with less residual error, but also are closeto the point in Euclidean space. The experimental results show that ourapproach is promising in subspace learning and subspace clustering.
arxiv-3000-286 | A Theoretical Analysis of NDCG Type Ranking Measures | http://arxiv.org/pdf/1304.6480v1.pdf | author:Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Tie-Yan Liu, Wei Chen category:cs.LG cs.IR stat.ML published:2013-04-24 summary:A central problem in ranking is to design a ranking measure for evaluation ofranking functions. In this paper we study, from a theoretical perspective, thewidely used Normalized Discounted Cumulative Gain (NDCG)-type ranking measures.Although there are extensive empirical studies of NDCG, little is known aboutits theoretical properties. We first show that, whatever the ranking functionis, the standard NDCG which adopts a logarithmic discount, converges to 1 asthe number of items to rank goes to infinity. On the first sight, this resultis very surprising. It seems to imply that NDCG cannot differentiate good andbad ranking functions, contradicting to the empirical success of NDCG in manyapplications. In order to have a deeper understanding of ranking measures ingeneral, we propose a notion referred to as consistent distinguishability. Thisnotion captures the intuition that a ranking measure should have such aproperty: For every pair of substantially different ranking functions, theranking measure can decide which one is better in a consistent manner on almostall datasets. We show that NDCG with logarithmic discount has consistentdistinguishability although it converges to the same limit for all rankingfunctions. We next characterize the set of all feasible discount functions forNDCG according to the concept of consistent distinguishability. Specifically weshow that whether NDCG has consistent distinguishability depends on how fastthe discount decays, and 1/r is a critical point. We then turn to the cut-offversion of NDCG, i.e., NDCG@k. We analyze the distinguishability of NDCG@k forvarious choices of k and the discount functions. Experimental results on realWeb search datasets agree well with the theory.
arxiv-3000-287 | The K-modes algorithm for clustering | http://arxiv.org/pdf/1304.6478v1.pdf | author:Miguel Á. Carreira-Perpiñán, Weiran Wang category:cs.LG stat.ME stat.ML published:2013-04-24 summary:Many clustering algorithms exist that estimate a cluster centroid, such asK-means, K-medoids or mean-shift, but no algorithm seems to exist that clustersdata by returning exactly K meaningful modes. We propose a natural definitionof a K-modes objective function by combining the notions of density and clusterassignment. The algorithm becomes K-means and K-medoids in the limit of verylarge and very small scales. Computationally, it is slightly slower thanK-means but much faster than mean-shift or K-medoids. Unlike K-means, it isable to find centroids that are valid patterns, truly representative of acluster, even with nonconvex clusters, and appears robust to outliers andmisspecification of the scale and number of clusters.
arxiv-3000-288 | Solving Support Vector Machines in Reproducing Kernel Banach Spaces with Positive Definite Functions | http://arxiv.org/pdf/1209.1171v3.pdf | author:Gregory E. Fasshauer, Fred J. Hickernell, Qi Ye category:stat.ML math.NA math.OC published:2012-09-06 summary:In this paper we solve support vector machines in reproducing kernel Banachspaces with reproducing kernels defined on nonsymmetric domains instead of thetraditional methods in reproducing kernel Hilbert spaces. Using theorthogonality of semi-inner-products, we can obtain the explicitrepresentations of the dual (normalized-duality-mapping) elements of supportvector machine solutions. In addition, we can introduce the reproductionproperty in a generalized native space by Fourier transform techniques suchthat it becomes a reproducing kernel Banach space, which can be even embeddedinto Sobolev spaces, and its reproducing kernel is set up by the relatedpositive definite function. The representations of the optimal solutions ofsupport vector machines (regularized empirical risks) in these reproducingkernel Banach spaces are formulated explicitly in terms of positive definitefunctions, and their finite numbers of coefficients can be computed by fixedpoint iteration. We also give some typical examples of reproducing kernelBanach spaces induced by Mat\'ern functions (Sobolev splines) so that theirsupport vector machine solutions are well computable as the classicalalgorithms. Moreover, each of their reproducing bases includes information frommultiple training data points. The concept of reproducing kernel Banach spacesoffers us a new numerical tool for solving support vector machines.
arxiv-3000-289 | On Semantic Word Cloud Representation | http://arxiv.org/pdf/1304.8016v1.pdf | author:Lukas Barth, Stephen Kobourov, Sergey Pupyrev, Torsten Ueckerdt category:cs.DS cs.CL published:2013-04-23 summary:We study the problem of computing semantic-preserving word clouds in whichsemantically related words are close to each other. While several heuristicapproaches have been described in the literature, we formalize the underlyinggeometric algorithm problem: Word Rectangle Adjacency Contact (WRAC). In thismodel each word is associated with rectangle with fixed dimensions, and thegoal is to represent semantically related words by ensuring that the twocorresponding rectangles touch. We design and analyze efficient polynomial-timealgorithms for some variants of the WRAC problem, show that several generalvariants are NP-hard, and describe a number of approximation algorithms.Finally, we experimentally demonstrate that our theoretically-sound algorithmsoutperform the early heuristics.
arxiv-3000-290 | Semi-Optimal Edge Detector based on Simple Standard Deviation with Adjusted Thresholding | http://arxiv.org/pdf/1304.6379v1.pdf | author:Firas A. Jassim category:cs.CV published:2013-04-23 summary:This paper proposes a novel method which combines both median filter andsimple standard deviation to accomplish an excellent edge detector for imageprocessing. First of all, a denoising process must be applied on the grey scaleimage using median filter to identify pixels which are likely to becontaminated by noise. The benefit of this step is to smooth the image and getrid of the noisy pixels. After that, the simple statistical standard deviationcould be computed for each 2X2 window size. If the value of the standarddeviation inside the 2X2 window size is greater than a predefined threshold,then the upper left pixel in the 2?2 window represents an edge. The visualdifferences between the proposed edge detector and the standard known edgedetectors have been shown to support the contribution in this paper.
arxiv-3000-291 | Similarity of Polygonal Curves in the Presence of Outliers | http://arxiv.org/pdf/1212.1617v2.pdf | author:Jean-Lou De Carufel, Amin Gheibi, Anil Maheshwari, Jörg-Rüdiger Sack, Christian Scheffer category:cs.CG cs.CV cs.GR published:2012-12-07 summary:The Fr\'{e}chet distance is a well studied and commonly used measure tocapture the similarity of polygonal curves. Unfortunately, it exhibits a highsensitivity to the presence of outliers. Since the presence of outliers is afrequently occurring phenomenon in practice, a robust variant of Fr\'{e}chetdistance is required which absorbs outliers. We study such a variant here. Inthis modified variant, our objective is to minimize the length of subcurves oftwo polygonal curves that need to be ignored (MinEx problem), or alternately,maximize the length of subcurves that are preserved (MaxIn problem), to achievea given Fr\'{e}chet distance. An exact solution to one problem would imply anexact solution to the other problem. However, we show that these problems arenot solvable by radicals over $\mathbb{Q}$ and that the degree of thepolynomial equations involved is unbounded in general. This motivates thesearch for approximate solutions. We present an algorithm, which approximates,for a given input parameter $\delta$, optimal solutions for the \MinEx\ and\MaxIn\ problems up to an additive approximation error $\delta$ times thelength of the input curves. The resulting running time is upper bounded by$\mathcal{O} \left(\frac{n^3}{\delta} \log \left(\frac{n}{\delta}\right)\right)$, where $n$ is the complexity of the input polygonal curves.
arxiv-3000-292 | Learning Visual Symbols for Parsing Human Poses in Images | http://arxiv.org/pdf/1304.6291v1.pdf | author:Fang Wang, Yi Li category:cs.CV published:2013-04-23 summary:Parsing human poses in images is fundamental in extracting critical visualinformation for artificial intelligent agents. Our goal is to learnself-contained body part representations from images, which we call visualsymbols, and their symbol-wise geometric contexts in this parsing process. Eachsymbol is individually learned by categorizing visual features leveraged bygeometric information. In the categorization, we use Latent Support VectorMachine followed by an efficient cross validation procedure to learn visualsymbols. Then, these symbols naturally define geometric contexts of body partsin a fine granularity. When the structure of the compositional parts is a tree,we derive an efficient approach to estimating human poses in images.Experiments on two large datasets suggest our approach outperforms state of theart methods.
arxiv-3000-293 | On Identifying Significant Edges in Graphical Models of Molecular Networks | http://arxiv.org/pdf/1104.0896v5.pdf | author:Marco Scutari, Radhakrishnan Nagarajan category:stat.ML stat.ME published:2011-04-05 summary:Objective: Modelling the associations from high-throughput experimentalmolecular data has provided unprecedented insights into biological pathways andsignalling mechanisms. Graphical models and networks have especially proven tobe useful abstractions in this regard. Ad-hoc thresholds are often used inconjunction with structure learning algorithms to determine significantassociations. The present study overcomes this limitation by proposing astatistically-motivated approach for identifying significant associations in anetwork. Methods and Materials: A new method that identifies significant associationsin graphical models by estimating the threshold minimising the $L_{\mathrm{1}}$norm between the cumulative distribution function (CDF) of the observed edgeconfidences and those of its asymptotic counterpart is proposed. Theeffectiveness of the proposed method is demonstrated on popular synthetic datasets as well as publicly available experimental molecular data corresponding togene and protein expression profiles. Results: The improved performance of the proposed approach is demonstratedacross the synthetic data sets using sensitivity, specificity and accuracy asperformance metrics. The results are also demonstrated across varying samplesizes and three different structure learning algorithms with widely varyingassumptions. In all cases, the proposed approach has specificity and accuracyclose to 1, while sensitivity increases linearly in the logarithm of the samplesize. The estimated threshold systematically outperforms common ad-hoc ones interms of sensitivity while maintaining comparable levels of specificity andaccuracy. Networks from experimental data sets are reconstructed accuratelywith respect to the results from the original papers.
arxiv-3000-294 | Counting people from above: Airborne video based crowd analysis | http://arxiv.org/pdf/1304.6213v1.pdf | author:Roland Perko, Thomas Schnabel, Gerald Fritz, Alexander Almer, Lucas Paletta category:cs.CV published:2013-04-23 summary:Crowd monitoring and analysis in mass events are highly importanttechnologies to support the security of attending persons. Proposed methodsbased on terrestrial or airborne image/video data often fail in achievingsufficiently accurate results to guarantee a robust service. We present a novelframework for estimating human count, density and motion from video data basedon custom tailored object detection techniques, a regression based densityestimate and a total variation based optical flow extraction. From the gatheredfeatures we present a detailed accuracy analysis versus ground truthmeasurements. In addition, all information is projected into world coordinatesto enable a direct integration with existing geo-information systems. Theresulting human counts demonstrate a mean error of 4% to 9% and thus representa most efficient measure that can be robustly applied in security criticalservices.
arxiv-3000-295 | Dew Point modelling using GEP based multi objective optimization | http://arxiv.org/pdf/1304.5594v2.pdf | author:Siddharth Shroff, Vipul Dabhi category:cs.NE published:2013-04-20 summary:Different techniques are used to model the relationship between temperatures,dew point and relative humidity. Gene expression programming is capable ofmodelling complex realities with great accuracy, allowing at the same time, theextraction of knowledge from the evolved models compared to other learningalgorithms. We aim to use Gene Expression Programming for modelling of dewpoint. Generally, accuracy of the model is the only objective used by selectionmechanism of GEP. This will evolve large size models with low training error.To avoid this situation, use of multiple objectives, like accuracy and size ofthe model are preferred by Genetic Programming practitioners. Solution to amulti-objective problem is a set of solutions which satisfies the objectivesgiven by decision maker. Multi objective based GEP will be used to evolvesimple models. Various algorithms widely used for multi objective optimization,like NSGA II and SPEA 2, are tested on different test problems. The resultsobtained thereafter gives idea that SPEA 2 is better than NSGA II based on thefeatures like execution time, number of solutions obtained and convergencerate. We selected SPEA 2 for dew point prediction. The multi-objective base GEPproduces accurate and simpler (smaller) solutions compared to solutionsproduced by plain GEP for dew point predictions. Thus multi objective base GEPproduces better solutions by considering the dual objectives of fitness andsize of the solution. These simple models can be used to predict future valuesof dew point.
arxiv-3000-296 | Bayesian crack detection in ultra high resolution multimodal images of paintings | http://arxiv.org/pdf/1304.5894v2.pdf | author:Bruno Cornelis, Yun Yang, Joshua T. Vogelstein, Ann Dooms, Ingrid Daubechies, David Dunson category:cs.CV cs.LG published:2013-04-22 summary:The preservation of our cultural heritage is of paramount importance. Thanksto recent developments in digital acquisition techniques, powerful imageanalysis algorithms are developed which can be useful non-invasive tools toassist in the restoration and preservation of art. In this paper we propose asemi-supervised crack detection method that can be used for high-dimensionalacquisitions of paintings coming from different modalities. Our datasetconsists of a recently acquired collection of images of the Ghent Altarpiece(1432), one of Northern Europe's most important art masterpieces. Our goal isto build a classifier that is able to discern crack pixels from the backgroundconsisting of non-crack pixels, making optimal use of the information that isprovided by each modality. To accomplish this we employ a recently developednon-parametric Bayesian classifier, that uses tensor factorizations tocharacterize any conditional probability. A prior is placed on the parametersof the factorization such that every possible interaction between predictors isallowed while still identifying a sparse subset among these predictors. Theproposed Bayesian classifier, which we will refer to as conditional Bayesiantensor factorization or CBTF, is assessed by visually comparing classificationresults with the Random Forest (RF) algorithm.
arxiv-3000-297 | Factorized Topic Models | http://arxiv.org/pdf/1301.3461v7.pdf | author:Cheng Zhang, Carl Henrik Ek, Andreas Damianou, Hedvig Kjellstrom category:cs.LG cs.CV cs.IR published:2013-01-15 summary:In this paper we present a modification to a latent topic model, which makesthe model exploit supervision to produce a factorized representation of theobserved data. The structured parameterization separately encodes variance thatis shared between classes from variance that is private to each class by theintroduction of a new prior over the topic space. The approach allows for amore eff{}icient inference and provides an intuitive interpretation of the datain terms of an informative signal together with structured noise. Thefactorized representation is shown to enhance inference performance for image,text, and video classification.
arxiv-3000-298 | A Bag of Visual Words Approach for Symbols-Based Coarse-Grained Ancient Coin Classification | http://arxiv.org/pdf/1304.6192v1.pdf | author:Hafeez Anwar, Sebastian Zambanini, Martin Kampel category:cs.CV published:2013-04-23 summary:The field of Numismatics provides the names and descriptions of the symbolsminted on the ancient coins. Classification of the ancient coins aims atassigning a given coin to its issuer. Various issuers used various symbols fortheir coins. We propose to use these symbols for a framework that will coarselyclassify the ancient coins. Bag of visual words (BoVWs) is a well establishedvisual recognition technique applied to various problems in computer visionlike object and scene recognition. Improvements have been made by incorporatingthe spatial information to this technique. We apply the BoVWs technique to ourproblem and use three symbols for coarse-grained classification. We userectangular tiling, log-polar tiling and circular tiling to incorporate spatialinformation to BoVWs. Experimental results show that the circular tiling provessuperior to the rest of the methods for our problem.
arxiv-3000-299 | The varifold representation of non-oriented shapes for diffeomorphic registration | http://arxiv.org/pdf/1304.6108v1.pdf | author:Nicolas Charon, Alain Trouvé category:cs.CG cs.CV math.DG published:2013-04-22 summary:In this paper, we address the problem of orientation that naturally ariseswhen representing shapes like curves or surfaces as currents. In the field ofcomputational anatomy, the framework of currents has indeed proved veryefficient to model a wide variety of shapes. However, in such approaches,orientation of shapes is a fundamental issue that can lead to several drawbacksin treating certain kind of datasets. More specifically, problems occur withstructures like acute pikes because of canceling effects of currents or withdata that consists in many disconnected pieces like fiber bundles for whichcurrents require a consistent orientation of all pieces. As a promisingalternative to currents, varifolds, introduced in the context of geometricmeasure theory by F. Almgren, allow the representation of any non-orientedmanifold (more generally any non-oriented rectifiable set). In particular, weexplain how varifolds can encode numerically non-oriented objects both from thediscrete and continuous point of view. We show various ways to build a Hilbertspace structure on the set of varifolds based on the theory of reproducingkernels. We show that, unlike the currents' setting, these metrics areconsistent with shape volume (theorem 4.1) and we derive a formula for thevariation of metric with respect to the shape (theorem 4.2). Finally, wepropose a generalization to non-oriented shapes of registration algorithms inthe context of Large Deformations Metric Mapping (LDDMM), which we detail witha few examples in the last part of the paper.
arxiv-3000-300 | Stochastic Variational Inference | http://arxiv.org/pdf/1206.7051v3.pdf | author:Matt Hoffman, David M. Blei, Chong Wang, John Paisley category:stat.ML cs.AI stat.CO stat.ME published:2012-06-29 summary:We develop stochastic variational inference, a scalable algorithm forapproximating posterior distributions. We develop this technique for a largeclass of probabilistic models and we demonstrate it with two probabilistictopic models, latent Dirichlet allocation and the hierarchical Dirichletprocess topic model. Using stochastic variational inference, we analyze severallarge collections of documents: 300K articles from Nature, 1.8M articles fromThe New York Times, and 3.8M articles from Wikipedia. Stochastic inference caneasily handle data sets of this size and outperforms traditional variationalinference, which can only handle a smaller subset. (We also show that theBayesian nonparametric topic model outperforms its parametric counterpart.)Stochastic variational inference lets us apply complex Bayesian models tomassive data sets.
