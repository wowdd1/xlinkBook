arxiv-1605-09593 | Controlling Exploration Improves Training for Deep Neural Networks | http://arxiv.org/abs/1605.09593 | author:Yasutoshi Ida, Yasuhiro Fujiwara, Sotetsu Iwamura category:cs.LG cs.AI stat.ML  published:2016-05-31 summary:Stochastic optimization methods are widely used for training of deep neuralnetworks. However, it is still a challenging research problem to achieveeffective training by using stochastic optimization methods. This is due to thedifficulties in finding good parameters on a loss function that have manysaddle points. In this paper, we propose a stochastic optimization methodcalled STDProp for effective training of deep neural networks. Its key idea isto effectively explore parameters on a complex surface of a loss function. Weadditionally develop momentum version of STDProp. While our approaches are easyto implement with high memory efficiency, it is more effective than otherpractical stochastic optimization methods for deep neural networks. version:1
arxiv-1605-09584 | A Sparse Representation of Complete Local Binary Pattern Histogram for Human Face Recognition | http://arxiv.org/abs/1605.09584 | author:Mawloud Guermoui, Mohamed L. Mekhalfi category:cs.CV  published:2016-05-31 summary:Human face recognition has been a long standing problem in computer visionand pattern recognition. Facial analysis can be viewed as a two-fold problem,namely (i) facial representation, and (ii) classification. So far, many facerepresentations have been proposed, a well-known method is the Local BinaryPattern (LBP), which has witnessed a growing interest. In this respect, wetreat in this paper the issues of face representation as well as classificationin a novel manner. On the one hand, we use a variant to LBP, so-called CompleteLocal Binary Pattern (CLBP), which differs from the basic LBP by coding a givenlocal region using a given central pixel and Sing_ Magnitude difference.Subsequently, most of LBPbased descriptors use a fixed grid to code a givenfacial image, which technique is, in most cases, not robust to pose variationand misalignment. To cope with such issue, a representative Multi-ResolutionHistogram (MH) decomposition is adopted in our work. On the other hand, havingthe histograms of the considered images extracted, we exploit their sparsity toconstruct a so-called Sparse Representation Classifier (SRC) for further faceclassification. Experimental results have been conducted on ORL face database,and pointed out the superiority of our scheme over other popularstate-of-the-art techniques. version:1
arxiv-1605-09582 | Model-driven Simulations for Deep Convolutional Neural Networks | http://arxiv.org/abs/1605.09582 | author:V S R Veeravasarapu, Constantin Rothkopf, Visvanathan Ramesh category:cs.CV  published:2016-05-31 summary:The use of simulated virtual environments to train deep convolutional neuralnetworks (CNN) is a currently active practice to reduce the(real)data-hungriness of the deep CNN models, especially in application domainsin which large scale real data and/or groundtruth acquisition is difficult orlaborious. Recent approaches have attempted to harness the capabilities ofexisting video games, animated movies to provide training data with highprecision groundtruth. However, a stumbling block is in how one can certifygeneralization of the learned models and their usefulness in real world datasets. This opens up fundamental questions such as: What is the role ofphotorealism of graphics simulations in training CNN models? Are the trainedmodels valid in reality? What are possible ways to reduce the performance bias?In this work, we begin to address theses issues systematically in the contextof urban semantic understanding with CNNs. Towards this end, we (a) propose asimple probabilistic urban scene model, (b) develop a parametric rendering toolto synthesize the data with groundtruth, followed by (c) a systematicexploration of the impact of level-of-realism on the generality of the trainedCNN model to real world; and domain adaptation concepts to minimize theperformance bias. version:1
arxiv-1605-09564 | Determining the Characteristic Vocabulary for a Specialized Dictionary using Word2vec and a Directed Crawler | http://arxiv.org/abs/1605.09564 | author:Gregory Grefenstette, Lawrence Muchemi category:cs.CL cs.AI cs.IR  published:2016-05-31 summary:Specialized dictionaries are used to understand concepts in specific domains,especially where those concepts are not part of the general vocabulary, orhaving meanings that differ from ordinary languages. The first step in creatinga specialized dictionary involves detecting the characteristic vocabulary ofthe domain in question. Classical methods for detecting this vocabulary involvegathering a domain corpus, calculating statistics on the terms found there, andthen comparing these statistics to a background or general language corpus.Terms which are found significantly more often in the specialized corpus thanin the background corpus are candidates for the characteristic vocabulary ofthe domain. Here we present two tools, a directed crawler, and a distributionalsemantics package, that can be used together, circumventing the need of abackground corpus. Both tools are available on the web. version:1
arxiv-1605-09559 | Modeling Photographic Composition via Triangles | http://arxiv.org/abs/1605.09559 | author:Zihan Zhou, Siqiong He, Jia Li, James Z. Wang category:cs.CV  published:2016-05-31 summary:The capacity of automatically modeling photographic composition is valuablefor many real-world machine vision applications such as digital photography,image retrieval, image understanding, and image aesthetics assessment. Thetriangle technique is among those indispensable composition methods on whichprofessional photographers often rely. This paper proposes a system that canidentify prominent triangle arrangements in two major categories ofphotographs: natural or urban scenes, and portraits. For the natural or urbanscene pictures, the focus is on the effect of linear perspective. Forportraits, we carefully examine the positioning of human subjects in a photo.We show that line analysis is highly advantageous for modeling composition inboth categories. Based on the detected triangles, new mathematical descriptorsfor composition are formulated and used to retrieve similar images. Leveragingthe rich source of high aesthetics photos online, similar approaches canpotentially be incorporated in future smart cameras to enhance a person's photocomposition skills. version:1
arxiv-1605-09553 | Attention Correctness in Neural Image Captioning | http://arxiv.org/abs/1605.09553 | author:Chenxi Liu, Junhua Mao, Fei Sha, Alan Yuille category:cs.CV cs.CL cs.LG  published:2016-05-31 summary:Attention mechanisms have recently been introduced in deep learning forvarious tasks in natural language processing and computer vision. But despitetheir popularity, the "correctness" of the implicitly-learned attention mapshas only been assessed qualitatively by visualization of several examples. Inthis paper we focus on evaluating and improving the correctness of attention inneural image captioning models. Specifically, we propose a quantitativeevaluation metric for how well the attention maps align with human judgment,using recently released datasets with alignment between regions in images andentities in captions. We then propose novel models with different levels ofexplicit supervision for learning attention maps during training. Thesupervision can be strong when alignment between regions and caption entitiesare available, or weak when only object segments and categories are provided.We show on the popular Flickr30k and COCO datasets that introducing supervisionof attention maps during training solidly improves both attention correctnessand caption quality. version:1
arxiv-1605-09546 | Semantic-Aware Depth Super-Resolution in Outdoor Scenes | http://arxiv.org/abs/1605.09546 | author:Miaomiao Liu, Mathieu Salzmann, Xuming He category:cs.CV  published:2016-05-31 summary:While depth sensors are becoming increasingly popular, their spatialresolution often remains limited. Depth super-resolution therefore emerged as asolution to this problem. Despite much progress, state-of-the-art techniquessuffer from two drawbacks: (i) they rely on the assumption that intensity edgescoincide with depth discontinuities, which, unfortunately, is only true incontrolled environments; and (ii) they typically exploit the availability ofhigh-resolution training depth maps, which can often not be acquired inpractice due to the sensors' limitations. By contrast, here, we introduce anapproach to performing depth super-resolution in more challenging conditions,such as in outdoor scenes. To this end, we first propose to exploit semanticinformation to better constrain the super-resolution process. In particular, wedesign a co-sparse analysis model that learns filters from joint intensity,depth and semantic information. Furthermore, we show how low-resolutiontraining depth maps can be employed in our learning strategy. We demonstratethe benefits of our approach over state-of-the-art depth super-resolutionmethods on two outdoor scene datasets. version:1
arxiv-1605-09533 | Robust Deep-Learning-Based Road-Prediction for Augmented Reality Navigation Systems | http://arxiv.org/abs/1605.09533 | author:Matthias Limmer, Julian Forster, Dennis Baudach, Florian Schüle, Roland Schweiger, Hendrik P. A. Lensch category:cs.CV cs.LG cs.RO  published:2016-05-31 summary:This paper proposes an approach that predicts the road course from camerasensors leveraging deep learning techniques. Road pixels are identified bytraining a multi-scale convolutional neural network on a large number offull-scene-labeled night-time road images including adverse weather conditions.A framework is presented that applies the proposed approach to longer distanceroad course estimation, which is the basis for an augmented reality navigationapplication. In this framework long range sensor data (radar) and data from amap database are fused with short range sensor data (camera) to produce aprecise longitudinal and lateral localization and road course estimation. Theproposed approach reliably detects roads with and without lane markings andthus increases the robustness and availability of road course estimations andaugmented reality navigation. Evaluations on an extensive set of high precisionground truth data taken from a differential GPS and an inertial measurementunit show that the proposed approach reaches state-of-the-art performancewithout the limitation of requiring existing lane markings. version:1
arxiv-1605-09527 | Biconvex Relaxation for Semidefinite Programming in Computer Vision | http://arxiv.org/abs/1605.09527 | author:Sohil Shah, Abhay Kumar, David Jacobs, Christoph Studer, Tom Goldstein category:cs.CV cs.NA math.NA math.OC  published:2016-05-31 summary:Semidefinite programming is an indispensable tool in computer vision, butgeneral-purpose solvers for semidefinite programs are often too slow and memoryintensive for large-scale problems. We propose a general framework toapproximately solve large-scale semidefinite problems (SDPs) at low complexity.Our approach, referred to as biconvex relaxation (BCR), transforms a generalSDP into a specific biconvex optimization problem, which can then be solved inthe original, low-dimensional variable space at low complexity. The resultingbiconvex problem is solved using an efficient alternating minimization (AM)procedure. Since AM has the potential to get stuck in local minima, we proposea general initialization scheme that enables BCR to start close to a globaloptimum - this is key for our algorithm to quickly converge to optimal ornear-optimal solutions. We showcase the efficacy of our approach on threeapplications in computer vision, namely segmentation, co-segmentation, andmanifold metric learning. BCR achieves solution quality comparable tostate-of-the-art SDP methods with speedups between 4X and 35X. At the sametime, BCR handles a more general set of SDPs than previous approaches, whichare more specialized. version:1
arxiv-1605-09526 | Intention from Motion | http://arxiv.org/abs/1605.09526 | author:Andrea Zunino, Jacopo Cavazza, Atesh Koul, Andrea Cavallo, Cristina Becchio, Vittorio Murino category:cs.CV  published:2016-05-31 summary:In this paper, we propose Intention from Motion, a new paradigm for actionprediction where, without using any contextual information, we can predicthuman intentions all originating from the same motor act, non specific of thefollowing performed action. To this purpose, we have designed a new multimodaldataset consisting of a set of motion capture marker 3D data and 2D videosequences where, by only analysing very similar movements in both training andtest phases, we are able to predict the underlying intention, i.e., the future,never observed, action. We also present an extensive three-fold experimentalevaluation as a baseline, customizing state-of-the-art techniques for 3D and 2Ddata analysis, and proposing fusion methods to combine the two types ofavailable data. This work constitutes a proof of concept for this new paradigmas we empirically prove the affordability of predicting intentions by analysingmotion patterns and without considering any additional contextual cues. version:1
arxiv-1605-09522 | Kernel Mean Embedding of Distributions: A Review and Beyonds | http://arxiv.org/abs/1605.09522 | author:Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Schölkopf category:stat.ML cs.LG  published:2016-05-31 summary:A Hilbert space embedding of distributions---in short, kernel meanembedding---has recently emerged as a powerful machinery for probabilisticmodeling, statistical inference, machine learning, and causal discovery. Thebasic idea behind this framework is to map distributions into a reproducingkernel Hilbert space (RKHS) in which the whole arsenal of kernel methods can beextended to probability measures. It gave rise to a great deal of research andnovel applications of positive definite kernels. The goal of this survey is togive a comprehensive review of existing works and recent advances in thisresearch area, and to discuss some of the most challenging issues and openproblems that could potentially lead to new research directions. The surveybegins with a brief introduction to the RKHS and positive definite kernelswhich forms the backbone of this survey, followed by a thorough discussion ofthe Hilbert space embedding of marginal distributions, theoretical guarantees,and review of its applications. The embedding of distributions enables us toapply RKHS methods to probability measures which prompts a wide range ofapplications such as kernel two-sample testing, independent testing, groupanomaly detection, and learning on distributional data. Next, we discuss theHilbert space embedding for conditional distributions, give theoreticalinsights, and review some applications. The conditional mean embedding enablesus to perform sum, product, and Bayes' rules---which are ubiquitous ingraphical model, probabilistic inference, and reinforcement learning---in anon-parametric way using the new representation of distributions in RKHS. Wethen discuss relationships between this framework and other related areas.Lastly, we give some suggestions on future research directions. version:1
arxiv-1605-09507 | Deep convolutional neural networks for predominant instrument recognition in polyphonic music | http://arxiv.org/abs/1605.09507 | author:Yoonchang Han, Jaehun Kim, Kyogu Lee category:cs.SD cs.CV cs.LG cs.NE  published:2016-05-31 summary:Identifying musical instruments in polyphonic music recordings is achallenging but important problem in the field of music information retrieval.It enables music search by instrument, helps recognize musical genres, or canmake music transcription easier and more accurate. In this paper, we present aconvolutional neural network framework for predominant instrument recognitionin real-world polyphonic music. We train our network from fixed-length musicexcerpts with a single-labeled predominant instrument and estimate an arbitrarynumber of predominant instruments from an audio signal with a variable length.To obtain the audio-excerpt-wise result, we aggregate multiple outputs fromsliding windows over the test audio. In doing so, we investigated two differentaggregation methods: one takes the average for each instrument and the othertakes the instrument-wise sum followed by normalization. In addition, weconducted extensive experiments on several important factors that affect theperformance, including analysis window size, identification threshold, andactivation functions for neural networks to find the optimal set of parameters.Using a dataset of 10k audio excerpts from 11 instruments for evaluation, wefound that convolutional neural networks are more robust than conventionalmethods that exploit spectral features and source separation with supportvector machines. Experimental results showed that the proposed convolutionalnetwork architecture obtained an F1 measure of 0.602 for micro and 0.503 formacro, respectively, achieving 19.6% and 16.4% in performance improvementcompared with other state-of-the-art algorithms. version:1
arxiv-1605-09499 | Extreme Stochastic Variational Inference: Distributed and Asynchronous | http://arxiv.org/abs/1605.09499 | author:Parameswaran Raman, Jiong Zhang, Hsiang-Fu Yu, Shihao Ji, S. V. N. Vishwanathan category:stat.ML  published:2016-05-31 summary:We propose extreme stochastic variational inference (ESVI), which allowsmultiple processors to simultaneously and asynchronously perform variationalinference updates. Moreover, by using a classic owner computes paradigm, ouralgorithm can be made lock-free. ESVI exhibits data and model parallelism, thatis, each processor only needs access to a subset of the data and a subset ofthe parameters. In our experiments we show that our new algorithm outperforms astraightforward strategy for parallelizing variational inference, whichrequires bulk synchronization after every iteration. version:1
arxiv-1605-09477 | A Neural Autoregressive Approach to Collaborative Filtering | http://arxiv.org/abs/1605.09477 | author:Yin Zheng, Bangsheng Tang, Wenkui Ding, Hanning Zhou category:cs.IR cs.LG stat.ML  published:2016-05-31 summary:This paper proposes CF-NADE, a neural autoregressive architecture forcollaborative filtering (CF) tasks, which is inspired by the RestrictedBoltzmann Machine (RBM) based CF model and the Neural AutoregressiveDistribution Estimator (NADE). We first describe the basic CF-NADE model for CFtasks. Then we propose to improve the model by sharing parameters betweendifferent ratings. A factored version of CF-NADE is also proposed for betterscalability. Furthermore, we take the ordinal nature of the preferences intoconsideration and propose an ordinal cost to optimize CF-NADE, which showssuperior performance. Finally, CF-NADE can be extended to a deep model, withonly moderately increased computational complexity. Experimental results showthat CF-NADE with a single hidden layer beats all previous state-of-the-artmethods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding morehidden layers can further improve the performance. version:1
arxiv-1605-09466 | Bayesian optimization under mixed constraints with a slack-variable augmented Lagrangian | http://arxiv.org/abs/1605.09466 | author:Victor Picheny, Robert B. Gramacy, Stefan M. Wild, Sebastien Le Digabel category:stat.CO stat.ML  published:2016-05-31 summary:An augmented Lagrangian (AL) can convert a constrained optimization probleminto a sequence of simpler (e.g., unconstrained) problems, which are thenusually solved with local solvers. Recently, surrogate-based Bayesianoptimization (BO) sub-solvers have been successfully deployed in the ALframework for a more global search in the presence of inequality constraints;however, a drawback was that expected improvement (EI) evaluations relied onMonte Carlo. Here we introduce an alternative slack variable AL, and show thatin this formulation the EI may be evaluated with library routines. The slackvariables furthermore facilitate equality as well as inequality constraints,and mixtures thereof. We show how our new slack "ALBO" compares favorably tothe original. Its superiority over conventional alternatives is reinforced onseveral mixed constraint examples. version:1
arxiv-1605-09459 | Scalable and Optimal Generalized Canonical Correlation Analysis via Alternating Optimization | http://arxiv.org/abs/1605.09459 | author:Xiao Fu, Kejun Huang, Mingyi Hong, Nicholas D. Sidiropoulos, Anthony Man-Cho So category:stat.ML  published:2016-05-31 summary:This paper considers generalized (multiview) canonical correlation analysis(GCCA) for large-scale datasets. A memory-efficient and computationallylightweight algorithm is proposed for the classic MAX-VAR GCCA formulation,which is gaining renewed interest in various applications, such as speechrecognition and natural language processing. The MAX-VAR GCCA problem can besolved optimally via eigen-decomposition of a matrix that compounds the(whitened) correlation matrices of the views. However, this route can easilylead to memory explosion and a heavy computational burden when the size of theviews becomes large. Instead, we propose an alternating optimization (AO)-basedalgorithm, which avoids instantiating the correlation matrices of the views andthus can achieve substantial saving in memory. The algorithm also maintainsdata sparsity, which can be exploited to alleviate the computational burden.Consequently, the proposed algorithm is highly scalable. Despite thenon-convexity of the MAX-VAR GCCA problem, the proposed iterative algorithm isshown to converge to a globally optimal solution under certain mild conditions.The proposed framework ensures global convergence even when the subproblems areinexactly solved, which can further reduce the complexity in practice.Simulations and large-scale word embedding tasks are employed to showcase theeffectiveness of the proposed algorithm. version:1
arxiv-1605-09458 | Training Auto-encoders Effectively via Eliminating Task-irrelevant Input Variables | http://arxiv.org/abs/1605.09458 | author:Hui Shen, Dehua Li, Hong Wu, Zhaoxiang Zang category:cs.LG  published:2016-05-31 summary:Auto-encoders are often used as building blocks of deep network classifier tolearn feature extractors, but task-irrelevant information in the input data maylead to bad extractors and result in poor generalization performance of thenetwork. In this paper,via dropping the task-irrelevant input variables theperformance of auto-encoders can be obviously improved .Specifically, animportance-based variable selection method is proposed to aim at finding thetask-irrelevant input variables and dropping them.It firstly estimatesimportance of each variable,and then drops the variables with importance valuelower than a threshold. In order to obtain better performance, the method canbe employed for each layer of stacked auto-encoders. Experimental results showthat when combined with our method the stacked denoising auto-encoders achievessignificantly improved performance on three challenging datasets. version:1
arxiv-1605-09452 | Latent Bi-constraint SVM for Video-based Object Recognition | http://arxiv.org/abs/1605.09452 | author:Yang Liu, Minh Hoai, Mang Shao, Tae-Kyun Kim category:cs.CV  published:2016-05-31 summary:We address the task of recognizing objects from video input. This importantproblem is relatively unexplored, compared with image-based object recognition.To this end, we make the following contributions. First, we introduce twocomprehensive datasets for video-based object recognition. Second, we proposeLatent Bi-constraint SVM (LBSVM), a maximum-margin framework for video-basedobject recognition. LBSVM is based on Structured-Output SVM, but extends it tohandle noisy video data and ensure consistency of the output decisionthroughout time. We apply LBSVM to recognize office objects and museumsculptures, and we demonstrate its benefits over image-based, set-based, andother video-based object recognition. version:1
arxiv-1605-09451 | Quantitative Analysis of Saliency Models | http://arxiv.org/abs/1605.09451 | author:Flora Ponjou Tasse, Jiří Kosinka, Neil Anthony Dodgson category:cs.GR cs.CV  published:2016-05-31 summary:Previous saliency detection research required the reader to evaluateperformance qualitatively, based on renderings of saliency maps on a fewshapes. This qualitative approach meant it was unclear which saliency modelswere better, or how well they compared to human perception. This paper providesa quantitative evaluation framework that addresses this issue. In the firstquantitative analysis of 3D computational saliency models, we evaluate fourcomputational saliency models and two baseline models against ground-truthsaliency collected in previous work. version:1
arxiv-1605-09444 | A Novel Fault Classification Scheme Based on Least Square SVM | http://arxiv.org/abs/1605.09444 | author:Harishchandra Dubey, A. K. Tiwari, Nandita, P. K. Ray, S. R. Mohanty, Nand Kishor category:cs.SY cs.LG  published:2016-05-30 summary:This paper presents a novel approach for fault classification and sectionidentification in a series compensated transmission line based on least squaresupport vector machine. The current signal corresponding to one-fourth of thepost fault cycle is used as input to proposed modular LS-SVM classifier. Theproposed scheme uses four binary classifier; three for selection of threephases and fourth for ground detection. The proposed classification scheme isfound to be accurate and reliable in presence of noise as well. The simulationresults validate the efficacy of proposed scheme for accurate classification offault in a series compensated transmission line. version:1
arxiv-1605-09441 | Blind Modulation Classification based on MLP and PNN | http://arxiv.org/abs/1605.09441 | author:Harishchandra Dubey, Nandita, Ashutosh Kumar Tiwari category:cs.CV cs.IT math.IT  published:2016-05-30 summary:In this work, a pattern recognition system is investigated for blindautomatic classification of digitally modulated communication signals. Theproposed technique is able to discriminate the type of modulation scheme whichis eventually used for demodulation followed by information extraction. Theproposed system is composed of two subsystems namely feature extractionsub-system (FESS) and classifier sub-system (CSS). The FESS consists ofcontinuous wavelet transform (CWT) for feature generation and principalcomponent analysis (PCA) for selection of the feature subset which is rich indiscriminatory information. The CSS uses the selected features to accuratelyclassify the modulation class of the received signal. The proposed techniqueuses probabilistic neural network (PNN) and multilayer perceptron forwardneural network (MLPFN) for comparative study of their recognition ability. PNNhave been found to perform better in terms of classification accuracy as wellas testing and training time than MLPFN. The proposed approach is robust topresence of phase offset and additive Gaussian noise. version:1
arxiv-1605-09432 | Evaluating Crowdsourcing Participants in the Absence of Ground-Truth | http://arxiv.org/abs/1605.09432 | author:Ramanathan Subramanian, Romer Rosales, Glenn Fung, Jennifer Dy category:cs.HC cs.LG  published:2016-05-30 summary:Given a supervised/semi-supervised learning scenario where multipleannotators are available, we consider the problem of identification ofadversarial or unreliable annotators. version:1
arxiv-1605-09410 | End-to-End Instance Segmentation and Counting with Recurrent Attention | http://arxiv.org/abs/1605.09410 | author:Mengye Ren, Richard S. Zemel category:cs.LG cs.CV  published:2016-05-30 summary:While convolutional neural networks have gained impressive success recentlyin solving structured prediction problems such as semantic segmentation, itremains a challenge to differentiate individual object instances in the scene.Instance segmentation is very important in a variety of applications, such asautonomous driving, image captioning, and visual question answering. Techniquesthat combine large graphical models with low-level vision have been proposed toaddress this problem; however, we propose an end-to-end recurrent neuralnetwork (RNN) architecture with an attention mechanism to model a human-likecounting process, and produce detailed instance segmentations. The network isjointly trained to sequentially produce regions of interest as well as adominant object segmentation within each region. The proposed model achievesstate-of-the-art results on the CVPPP leaf segmentation dataset and KITTIvehicle segmentation dataset. version:1
arxiv-1605-09370 | Unsupervised Discovery of El Nino Using Causal Feature Learning on Microlevel Climate Data | http://arxiv.org/abs/1605.09370 | author:Krzysztof Chalupka, Tobias Bischoff, Pietro Perona, Frederick Eberhardt category:stat.ML cs.AI cs.LG physics.ao-ph  published:2016-05-30 summary:We show that the climate phenomena of El Nino and La Nina arise naturally asstates of macro-variables when our recent causal feature learning framework(Chalupka 2015, Chalupka 2016) is applied to micro-level measures of zonal wind(ZW) and sea surface temperatures (SST) taken over the equatorial band of thePacific Ocean. The method identifies these unusual climate states on the basisof the relation between ZW and SST patterns without any input about pastoccurrences of El Nino or La Nina. The simpler alternatives of (i) clusteringthe SST fields while disregarding their relationship with ZW patterns, or (ii)clustering the joint ZW-SST patterns, do not discover El Nino. We discuss thedegree to which our method supports a causal interpretation and use alow-dimensional toy example to explain its success over other clusteringapproaches. Finally, we propose a new robust and scalable alternative to ouroriginal algorithm (Chalupka 2016), which circumvents the need forhigh-dimensional density learning. version:1
arxiv-1605-09351 | Review of Fall Detection Techniques: A Data Availability Perspective | http://arxiv.org/abs/1605.09351 | author:Shehroz S. Khan, Jesse Hoey category:cs.LG  published:2016-05-30 summary:A fall is an abnormal activity that occurs rarely; however, missing toidentify falls can have serious health and safety implications on anindividual. Due to the rarity of occurrence of falls, there may be insufficientor no training data available for them. Therefore, standard supervised machinelearning methods may not be directly applied to handle this problem. In thispaper, we present a taxonomy for the study of fall detection from theperspective of availability of fall data. The proposed taxonomy is independentof the type of sensors used and specific feature extraction/selection methods.The taxonomy identifies different categories of classification methods for thestudy of fall detection based on the availability of their data during trainingthe classifiers. Then, we present a comprehensive literature review withinthose categories and identify the approach of treating a fall as an abnormalactivity to be a plausible research direction. We conclude our paper bydiscussing several open research problems in the field and pointers for futureresearch. version:1
arxiv-1605-09346 | Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs | http://arxiv.org/abs/1605.09346 | author:Anton Osokin, Jean-Baptiste Alayrac, Isabella Lukasewitz, Puneet K. Dokania, Simon Lacoste-Julien category:cs.LG math.OC stat.ML G.1.6; I.2.6  published:2016-05-30 summary:In this paper, we propose several improvements on the block-coordinateFrank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used tooptimize the structured support vector machine (SSVM) objective in the contextof structured prediction, though it has wider applications. The key intuitionbehind our improvements is that the estimates of block gaps maintained by BCFWreveal the block suboptimality that can be used as an adaptive criterion.First, we sample objects at each iteration of BCFW in an adaptive non-uniformway via gapbased sampling. Second, we incorporate pairwise and away-stepvariants of Frank-Wolfe into the block-coordinate setting. Third, we cacheoracle calls with a cache-hit criterion based on the block gaps. Fourth, weprovide the first method to compute an approximate regularization path forSSVM. Finally, we provide an exhaustive empirical evaluation of all our methodson four structured prediction datasets. version:1
arxiv-1605-09336 | Learning the image processing pipeline | http://arxiv.org/abs/1605.09336 | author:Haomiao Jiang, Qiyuan Tian, Joyce Farrell, Brian Wandell category:cs.CV  published:2016-05-30 summary:Many creative ideas are being proposed for image sensor designs, and thesemay be useful in applications ranging from consumer photography to computervision. To understand and evaluate each new design, we must create acorresponding image processing pipeline that transforms the sensor data into aform that is appropriate for the application. The need to design and optimizethese pipelines is time-consuming and costly. We explain a method that combinesmachine learning and image systems simulation that automates the pipelinedesign. The approach is based on a new way of thinking of the image processingpipeline as a large collection of local linear filters. We illustrate how themethod has been used to design pipelines for novel sensor architectures inconsumer photography applications. version:1
arxiv-1605-09304 | Synthesizing the preferred inputs for neurons in neural networks via deep generator networks | http://arxiv.org/abs/1605.09304 | author:Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, Jeff Clune category:cs.NE cs.AI cs.CV cs.LG  published:2016-05-30 summary:Deep neural networks (DNNs) have demonstrated state-of-the-art results onmany pattern recognition tasks, especially vision classification problems.Understanding the inner workings of such computational brains is bothfascinating basic science that is interesting in its own right - similar to whywe study the human brain - and will enable researchers to further improve DNNs.One path to understanding how a neural network functions internally is to studywhat each of its neurons has learned to detect. One such method is calledactivation maximization (AM), which synthesizes an input (e.g. an image) thathighly activates a neuron. Here we dramatically improve the qualitative stateof the art of activation maximization by harnessing a powerful, learned prior:a deep generator network (DGN). The algorithm (1) generates qualitativelystate-of-the-art synthetic images that look almost real, (2) reveals thefeatures learned by each neuron in an interpretable way, (3) generalizes wellto new datasets and somewhat well to different network architectures withoutrequiring the prior to be relearned, and (4) can be considered as ahigh-quality generative method (in this case, by generating novel, creative,interesting, recognizable images). version:1
arxiv-1605-09299 | k2-means for fast and accurate large scale clustering | http://arxiv.org/abs/1605.09299 | author:Eirikur Agustsson, Radu Timofte, Luc Van Gool category:cs.LG cs.CV  published:2016-05-30 summary:We propose k^2-means, a new clustering method which efficiently copes withlarge numbers of clusters and achieves low energy solutions. k^2-means buildsupon the standard k-means (Lloyd's algorithm) and combines a new strategy toaccelerate the convergence with a new low time complexity divisiveinitialization. The accelerated convergence is achieved through only looking atk_n nearest clusters and using triangle inequality bounds in the assignmentstep while the divisive initialization employs an optimal 2-clustering along adirection. The worst-case time complexity per iteration of our k^2-means isO(nk_nd+k^2d), where d is the dimension of the n data points and k is thenumber of clusters and usually n << k << k_n. Compared to k-means' O(nkd)complexity, our k^2-means complexity is significantly lower, at the expense ofslightly increasing the memory complexity by O(nk_n+k^2). In our extensiveexperiments k^2-means is order(s) of magnitude faster than standard methods incomputing accurate clusterings on several standard datasets and settings withhundreds of clusters and high dimensional data. Moreover, the proposed divisiveinitialization generally leads to clustering energies comparable to thoseachieved with the standard k-means++ initialization, while being significantlyfaster. version:1
arxiv-1605-09293 | Internal Guidance for Satallax | http://arxiv.org/abs/1605.09293 | author:Michael Färber, Chad Brown category:cs.LO cs.AI cs.LG  published:2016-05-30 summary:We propose a new internal guidance method for automated theorem provers basedon the given-clause algorithm. Our method influences the choice of unprocessedclauses using positive and negative examples from previous proofs. To this end,we present an efficient scheme for Naive Bayesian classification bygeneralising label occurrences to types with monoid structure. This makes itpossible to extend existing fast classifiers, which consider only positiveexamples, with negative ones. We implement the method in the higher-order logicprover Satallax, where we modify the delay with which propositions areprocessed. We evaluated our method on a simply-typed higher-order logic versionof the Flyspeck project, where it solves 26% more problems than Satallaxwithout internal guidance. version:1
arxiv-1605-07678 | An Analysis of Deep Neural Network Models for Practical Applications | http://arxiv.org/abs/1605.07678 | author:Alfredo Canziani, Adam Paszke, Eugenio Culurciello category:cs.CV  published:2016-05-24 summary:Since the emergence of Deep Neural Networks (DNNs) as a prominent techniquein the field of computer vision, the ImageNet classification challenge hasplayed a major role in advancing the state-of-the-art. While accuracy figureshave steadily increased, the resource utilisation of winning models has notbeen properly taken into account. In this work, we present a comprehensiveanalysis of important metrics in practical applications: accuracy, memoryfootprint, parameters, operations count, inference time and power consumption.Key findings are: (1) fully connected layers are largely inefficient forsmaller batches of images; (2) accuracy and inference time are in a hyperbolicrelationship; (3) energy constraint are an upper bound on the maximumachievable accuracy and model complexity; (4) the number of operations is areliable estimate of the inference time. We believe our analysis provides acompelling set of information that helps design and engineer efficient DNNs. version:2
arxiv-1605-09232 | Tradeoffs between Convergence Speed and Reconstruction Accuracy in Inverse Problems | http://arxiv.org/abs/1605.09232 | author:Raja Giryes, Yonina C. Eldar, Alex M. Bronstein, Guillermo Sapiro category:cs.NA cs.LG cs.NE math.OC stat.ML 65B99  90C59  published:2016-05-30 summary:Solving inverse problems with iterative algorithms such as stochasticgradient descent is a popular technique, especially for large data. Inapplications, due to time constraints, the number of iterations one may applyis usually limited, consequently limiting the accuracy achievable by certainmethods. Given a reconstruction error one is willing to tolerate, an importantquestion is whether it is possible to modify the original iterations to obtaina faster convergence to a minimizer with the allowed error. Relying on recentrecovery techniques developed for settings in which the desired signal belongsto some low-dimensional set, we show that using a coarse estimate of this setleads to faster convergence to an error related to the accuracy of the setapproximation. Our theory ties to recent advances in sparse recovery,compressed sensing and deep learning. In particular, it provides an explanationfor the successful approximation of the ISTA solution by neural networks withlayers representing iterations. version:1
arxiv-1605-09227 | Learning Combinatorial Functions from Pairwise Comparisons | http://arxiv.org/abs/1605.09227 | author:Maria-Florina Balcan, Ellen Vitercik, Colin White category:cs.LG cs.DS  published:2016-05-30 summary:A large body of work in machine learning has focused on the problem oflearning a close approximation to an underlying combinatorial function, given asmall set of labeled examples. However, for real-valued functions, cardinallabels might not be accessible, or it may be difficult for an expert toconsistently assign real-valued labels over the entire set of examples. Forinstance, it is notoriously hard for consumers to reliably assign values tobundles of merchandise. Instead, it might be much easier for a consumer toreport which of two bundles she likes better. With this motivation in mind, weconsider an alternative learning model, wherein the algorithm must learn theunderlying function up to pairwise comparisons, from pairwise comparisons. Inthis model, we present a series of novel algorithms that learn over a widevariety of combinatorial function classes. These range from graph functions tobroad classes of valuation functions that are fundamentally important inmicroeconomic theory, the analysis of social networks, and machine learning,such as coverage, submodular, XOS, and subadditive functions, as well asfunctions with sparse Fourier support. version:1
arxiv-1605-09221 | Deep Reinforcement Learning Radio Control and Signal Detection with KeRLym, a Gym RL Agent | http://arxiv.org/abs/1605.09221 | author:Timothy J. O'Shea, T. Charles Clancy category:cs.LG  published:2016-05-30 summary:This paper presents research in progress investigating the viability andadaptation of reinforcement learning using deep neural network based functionapproximation for the task of radio control and signal detection in thewireless domain. We demonstrate a successful initial method for radio controlwhich allows naive learning of search without the need for expert features,heuristics, or search strategies. We also introduce Kerlym, an open Keras basedreinforcement learning agent collection for OpenAI's Gym. version:1
arxiv-1605-09211 | Going Deeper for Multilingual Visual Sentiment Detection | http://arxiv.org/abs/1605.09211 | author:Brendan Jou, Shih-Fu Chang category:cs.MM cs.CL cs.CV  published:2016-05-30 summary:This technical report details several improvements to the visual conceptdetector banks built on images from the Multilingual Visual Sentiment Ontology(MVSO). The detector banks are trained to detect a total of 9,918sentiment-biased visual concepts from six major languages: English, Spanish,Italian, French, German and Chinese. In the original MVSO release,adjective-noun pair (ANP) detectors were trained for the six languages using anAlexNet-styled architecture by fine-tuning from DeepSentiBank. Here, through amore extensive set of experiments, parameter tuning, and training runs, wedetail and release higher accuracy models for detecting ANPs across sixlanguages from the same image pool and setting as in the original release usinga more modern architecture, GoogLeNet, providing comparable or betterperformance with reduced network parameter cost. In addition, since the image pool in MVSO can be corrupted by user noise fromsocial interactions, we partitioned out a sub-corpus of MVSO images based ontag-restricted queries for higher fidelity labels. We show that as a result ofthese higher fidelity labels, higher performing AlexNet-styled ANP detectorscan be trained using the tag-restricted image subset as compared to the modelsin full corpus. We release all these newly trained models for public researchuse along with the list of tag-restricted images from the MVSO dataset. version:1
arxiv-1605-09186 | Does Multimodality Help Human and Machine for Translation and Image Captioning? | http://arxiv.org/abs/1605.09186 | author:Ozan Caglayan, Walid Aransa, Yaxing Wang, Marc Masana, Mercedes García-Martínez, Fethi Bougares, Loïc Barrault, Joost van de Weijer category:cs.CL cs.LG cs.NE  published:2016-05-30 summary:This paper presents the systems developed by LIUM and CVC for the WMT16Multimodal Machine Translation challenge. We explored various comparativemethods, namely phrase-based systems and attentional recurrent neural networksmodels trained using monomodal or multimodal data. We also performed a humanevaluation in order to estimate the usefulness of multimodal data for humanmachine translation and image description generation. Our systems obtained thebest results for both tasks according to the automatic evaluation metrics BLEUand METEOR. version:1
arxiv-1605-09136 | Hyperspectral Image Classification with Support Vector Machines on Kernel Distribution Embeddings | http://arxiv.org/abs/1605.09136 | author:Gianni Franchi, Jesus Angulo, Dino Sejdinovic category:cs.CV cs.LG stat.ML  published:2016-05-30 summary:We propose a novel approach for pixel classification in hyperspectral images,leveraging on both the spatial and spectral information in the data. Theintroduced method relies on a recently proposed framework for learning ondistributions -- by representing them with mean elements in reproducing kernelHilbert spaces (RKHS) and formulating a classification algorithm therein. Inparticular, we associate each pixel to an empirical distribution of itsneighbouring pixels, a judicious representation of which in an RKHS, inconjunction with the spectral information contained in the pixel itself, give anew explicit set of features that can be fed into a suite of standardclassification techniques -- we opt for a well-established framework of supportvector machines (SVM). Furthermore, the computational complexity is reduced viarandom Fourier features formalism. We study the consistency and the convergencerates of the proposed method and the experiments demonstrate strong performanceon hyperspectral data with gains in comparison to the state-of-the-art results. version:1
arxiv-1605-09131 | Classification under Streaming Emerging New Classes: A Solution using Completely Random Trees | http://arxiv.org/abs/1605.09131 | author:Xin Mu, Kai Ming Ting, Zhi-Hua Zhou category:cs.LG  published:2016-05-30 summary:This paper investigates an important problem in stream mining, i.e.,classification under streaming emerging new classes or SENC. The commonapproach is to treat it as a classification problem and solve it using either asupervised learner or a semi-supervised learner. We propose an alternativeapproach by using unsupervised learning as the basis to solve this problem. TheSENC problem can be decomposed into three sub problems: detecting emerging newclasses, classifying for known classes, and updating models to enableclassification of instances of the new class and detection of more emerging newclasses. The proposed method employs completely random trees which have beenshown to work well in unsupervised learning and supervised learningindependently in the literature. This is the first time, as far as we know,that completely random trees are used as a single common core to solve allthree sub problems: unsupervised learning, supervised learning and model updatein data streams. We show that the proposed unsupervised-learning-focused methodoften achieves significantly better outcomes than existingclassification-focused methods. version:1
arxiv-1605-06950 | A Sub-Quadratic Exact Medoid Algorithm | http://arxiv.org/abs/1605.06950 | author:James Newling, François Fleuret category:stat.ML cs.DM stat.CO  published:2016-05-23 summary:We present a new algorithm, trimed, for obtaining the medoid of a set, thatis the element of the set which minimises the mean distance to all otherelements. The algorithm is shown to have, under weak assumptions, complexityO(N^(3/2)) in R^d where N is the set size, making it the first sub-quadraticexact medoid algorithm for d>1. Experiments show that it performs very well onspatial network data, frequently requiring two orders of magnitude fewerdistances than state-of-the-art approximate algorithms. We show how trimed canbe used as a component in an accelerated K-medoids algorithm, and how it can berelaxed to obtain further computational gains with an only minor loss inquality. version:2
arxiv-1605-09128 | Control of Memory, Active Perception, and Action in Minecraft | http://arxiv.org/abs/1605.09128 | author:Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, Honglak Lee category:cs.AI cs.CV cs.LG  published:2016-05-30 summary:In this paper, we introduce a new set of reinforcement learning (RL) tasks inMinecraft (a flexible 3D world). We then use these tasks to systematicallycompare and contrast existing deep reinforcement learning (DRL) architectureswith our new memory-based DRL architectures. These tasks are designed toemphasize, in a controllable manner, issues that pose challenges for RL methodsincluding partial observability (due to first-person visual observations),delayed rewards, high-dimensional visual observations, and the need to useactive perception in a correct manner so as to perform well in the tasks. Whilethese tasks are conceptually simple to describe, by virtue of having all ofthese challenges simultaneously they are difficult for current DRLarchitectures. Additionally, we evaluate the generalization performance of thearchitectures on environments not used during training. The experimentalresults show that our new architectures generalize to unseen environmentsbetter than existing DRL architectures. version:1
arxiv-1605-09116 | Image segmentation based on the hybrid total variation model and the K-means clustering strategy | http://arxiv.org/abs/1605.09116 | author:Baoli Shi, Zhi-Feng Pang, Jing Xu category:math.OC cs.CV  published:2016-05-30 summary:The performance of image segmentation highly relies on the original inputtingimage. When the image is contaminated by some noises or blurs, we can notobtain the efficient segmentation result by using direct segmentation methods.In order to efficiently segment the contaminated image, this paper proposes atwo step method based on the hybrid total variation model with a box constraintand the K-means clustering method. In the first step, the hybrid model is basedon the weighted convex combination between the total variation functional andthe high-order total variation as the regularization term to obtain theoriginal clustering data. In order to deal with non-smooth regularization term,we solve this model by employing the alternating split Bregman method. Then, inthe second step, the segmentation can be obtained by thresholding thisclustering data into different phases, where the thresholds can be given byusing the K-means clustering method. Numerical comparisons show that ourproposed model can provide more efficient segmentation results dealing with thenoise image and blurring image. version:1
arxiv-1605-09114 | ParMAC: distributed optimisation of nested functions, with application to learning binary autoencoders | http://arxiv.org/abs/1605.09114 | author:Miguel Á. Carreira-Perpiñán, Mehdi Alizadeh category:cs.LG cs.DC cs.NE math.OC stat.ML  published:2016-05-30 summary:Many powerful machine learning models are based on the composition ofmultiple processing layers, such as deep nets, which gives rise to nonconvexobjective functions. A general, recent approach to optimise such "nested"functions is the method of auxiliary coordinates (MAC). MAC introduces anauxiliary coordinate for each data point in order to decouple the nested modelinto independent submodels. This decomposes the optimisation into steps thatalternate between training single layers and updating the coordinates. It hasthe advantage that it reuses existing single-layer algorithms, introducesparallelism, and does not need to use chain-rule gradients, so it works withnondifferentiable layers. With large-scale problems, or when distributing thecomputation is necessary for faster training, the dataset may not fit in asingle machine. It is then essential to limit the amount of communicationbetween machines so it does not obliterate the benefit of parallelism. Wedescribe a general way to achieve this, ParMAC. ParMAC works on a cluster ofprocessing machines with a circular topology and alternates two steps untilconvergence: one step trains the submodels in parallel using stochasticupdates, and the other trains the coordinates in parallel. Only submodelparameters, no data or coordinates, are ever communicated between machines.ParMAC exhibits high parallelism, low communication overhead, and facilitatesdata shuffling, load balancing, fault tolerance and streaming data processing.We study the convergence of ParMAC and propose a theoretical model of itsruntime and parallel speedup. We develop ParMAC to learn binary autoencodersfor fast, approximate image retrieval. We implement it in MPI in a distributedsystem and demonstrate nearly perfect speedups in a 128-processor cluster witha training set of 100 million high-dimensional points. version:1
arxiv-1605-08361 | No bad local minima: Data independent training error guarantees for multilayer neural networks | http://arxiv.org/abs/1605.08361 | author:Daniel Soudry, Yair Carmon category:stat.ML cs.LG cs.NE  published:2016-05-26 summary:We use smoothed analysis techniques to provide guarantees on the trainingloss of Multilayer Neural Networks (MNNs) at differentiable local minima.Specifically, we examine MNNs with piecewise linear activation functions,quadratic loss and a single output, under mild over-parametrization. We provethat for a MNN with one hidden layer, the training error is zero at everydifferentiable local minimum, for almost every dataset and dropout-like noiserealization. We then extend these results to the case of more than one hiddenlayer. Our theoretical guarantees assume essentially nothing on the trainingdata, and are verified numerically. These results suggest why the highlynon-convex loss of such MNNs can be easily optimized using local updates (e.g.,stochastic gradient descent), as observed empirically. version:2
arxiv-1605-09096 | Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change | http://arxiv.org/abs/1605.09096 | author:William L. Hamilton, Jure Leskovec, Dan Jurafsky category:cs.CL  published:2016-05-30 summary:Understanding how words change their meanings over time is key to models oflanguage and cultural evolution, but historical data on meaning is scarce,making theories hard to develop and test. Word embeddings show promise as adiachronic tool, but have not been carefully evaluated. We develop a robustmethodology for quantifying semantic change by evaluating word embeddings(PPMI, SVD, word2vec) against known historical changes. We then use thismethodology to reveal statistical laws of semantic evolution. Using sixhistorical corpora spanning four languages and two centuries, we propose twoquantitative laws of semantic change: (i) the law of conformity---the rate ofsemantic change scales with an inverse power-law of word frequency; (ii) thelaw of innovation---independent of frequency, words that are more polysemoushave higher rates of semantic change. version:1
arxiv-1605-09090 | Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention | http://arxiv.org/abs/1605.09090 | author:Yang Liu, Chengjie Sun, Lei Lin, Xiaolong Wang category:cs.CL  published:2016-05-30 summary:In this paper, we proposed a sentence encoding-based model for recognizingtext entailment. In our approach, the encoding of sentence is a two-stageprocess. Firstly, average pooling was used over word-level bidirectional LSTM(biLSTM) to generate a first-stage sentence representation. Secondly, attentionmechanism was employed to replace average pooling on the same sentence forbetter representations. Instead of using target sentence to attend words insource sentence, we utilized the sentence's first-stage representation toattend words appeared in itself, which is called "Inner-Attention" in our paper. Experiments conducted on Stanford Natural Language Inference (SNLI) Corpushas proved the effectiveness of "Inner-Attention" mechanism. With less numberof parameters, our model outperformed the existing best sentence encoding-basedapproach by a large margin. version:1
arxiv-1605-09088 | The Bayesian Linear Information Filtering Problem | http://arxiv.org/abs/1605.09088 | author:Bangrui Chen, Peter Frazier category:cs.LG  published:2016-05-30 summary:We present a Bayesian sequential decision-making formulation of theinformation filtering problem, in which an algorithm presents items (newsarticles, scientific papers, tweets) arriving in a stream, and learns relevancefrom user feedback on presented items. We model user preferences using aBayesian linear model, similar in spirit to a Bayesian linear bandit. Wecompute a computational upper bound on the value of the optimal policy, whichallows computing an optimality gap for implementable policies. We then use thisanalysis as motivation in introducing a pair of new Decompose-Then-Decide (DTD)heuristic policies, DTD-Dynamic-Programming (DTD-DP) andDTD-Upper-Confidence-Bound (DTD-UCB). We compare DTD-DP and DTD-UCB againstseveral benchmarks on real and simulated data, demonstrating significantimprovement, and show that the achieved performance is close to the upperbound. version:1
arxiv-1605-09085 | Stochastic Function Norm Regularization of Deep Networks | http://arxiv.org/abs/1605.09085 | author:Amal Rannen Triki, Matthew B. Blaschko category:cs.LG cs.CV stat.ML  published:2016-05-30 summary:Deep neural networks have had an enormous impact on image analysis.State-of-the-art training methods, based on weight decay and DropOut, result inimpressive performance when a very large training set is available. However,they tend to have large problems overfitting to small data sets. Indeed, theavailable regularization methods deal with the complexity of the networkfunction only indirectly. In this paper, we study the feasibility of directlyusing the $L_2$ function norm for regularization. Two methods to integrate thisnew regularization in the stochastic backpropagation are proposed. Moreover,the convergence of these new algorithms is studied. We finally show that theyoutperform the state-of-the-art methods in the low sample regime on benchmarkdatasets (MNIST and CIFAR10). The obtained results demonstrate very clearimprovement, especially in the context of small sample regimes with data layingin a low dimensional manifold. Source code of the method can be found at\url{https://github.com/AmalRT/DNN_Reg}. version:1
arxiv-1605-09082 | One-Pass Learning with Incremental and Decremental Features | http://arxiv.org/abs/1605.09082 | author:Chenping Hou, Zhi-Hua Zhou category:cs.LG  published:2016-05-30 summary:In many real tasks the features are evolving, with some features beingvanished and some other features augmented. For example, in environmentmonitoring some sensors expired whereas some new ones deployed; in mobile gamerecommendation some games dropped whereas some new ones added. Learning withsuch incremental and decremental features is crucial but rarely studied,particularly when the data coming like a stream and thus it is infeasible tokeep the whole data for optimization. In this paper, we study this challengingproblem and present the OPID approach. Our approach attempts to compressimportant information of vanished features into functions of survived features,and then expand to include the augmented features. It is the one-pass learningapproach, which only needs to scan each instance once and does not need tostore the whole data, and thus satisfy the evolving streaming data nature. Theeffectiveness of our approach is validated theoretically and empirically. version:1
arxiv-1605-09081 | Understanding Convolutional Neural Networks | http://arxiv.org/abs/1605.09081 | author:Jayanth Koushik category:stat.ML cs.AI cs.CV cs.LG cs.NE  published:2016-05-30 summary:Convoulutional Neural Networks (CNNs) exhibit extraordinary performance on avariety of machine learning tasks. However, their mathematical properties andbehavior are quite poorly understood. There is some work, in the form of aframework, for analyzing the operations that they perform. The goal of thisproject is to present key results from this theory, and provide intuition forwhy CNNs work. version:1
arxiv-1605-09068 | A budget-constrained inverse classification framework for smooth classifiers | http://arxiv.org/abs/1605.09068 | author:Michael T. Lash, Qihang Lin, W. Nick Street, Jennifer G. Robinson category:cs.LG stat.ML H.2.8  published:2016-05-29 summary:Inverse classification is the process of manipulating an instance such thatit is more likely to conform to a specific class. Past methods that addresssuch a problem have shortcomings. Greedy methods make changes that are overlyradical, often relying on data that is strictly discrete. Other methods rely oncertain data points, the presence of which cannot be guaranteed. In this paperwe propose a general framework and method that overcomes these and otherlimitations. The formulation of our method uses any differentiableclassification function. We demonstrate the method by using Gaussian kernelSVMs. We constrain the inverse classification to occur on features that canactually be changed, each of which incurs an individual cost. We furthersubject such changes to fall within a certain level of cumulative change(budget). Our framework can also accommodate the estimation of features whosevalues change as a consequence of actions taken (indirectly changeablefeatures). Furthermore, we propose two methods for specifying feature-valueranges that result in different algorithmic behavior. We apply our method, anda proposed sensitivity analysis-based benchmark method, to two freely availabledatasets: Student Performance, from the UCI Machine Learning Repository and areal-world cardiovascular disease dataset. The results obtained demonstrate thevalidity and benefits of our framework and method. version:1
arxiv-1605-09066 | Distributed Asynchronous Stochastic Dual Coordinate Ascent without Duality | http://arxiv.org/abs/1605.09066 | author:Zhouyuan Huo, Heng Huang category:cs.LG  published:2016-05-29 summary:In this paper, we propose new Distributed Asynchronous Dual-Free CoordinateAscent method (Asy-df SDCA), and provide the proof of convergence rate for twocases: the individual loss is convex and the individual loss is non-convex butits expected loss is convex. Stochastic Dual Coordinate Ascent (SDCA) model isa popular method and often has better performances than stochastic gradientdescent methods in solving regularized convex loss minimization problems.Dual-Free Stochastic Dual Coordinate Ascent method is a variation of SDCA, andcan be applied to non-convex problem when its dual problem is meaningless. Weextend Dual-Free Stochastic Dual Coordinate Ascent method to the distributedmode with considering the star network in this paper. version:1
arxiv-1605-09062 | Predicting Personal Traits from Facial Images using Convolutional Neural Networks Augmented with Facial Landmark Information | http://arxiv.org/abs/1605.09062 | author:Yoad Lewenberg, Yoram Bachrach, Sukrit Shankar, Antonio Criminisi category:cs.CV  published:2016-05-29 summary:We consider the task of predicting various traits of a person given an imageof their face. We estimate both objective traits, such as gender, ethnicity andhair-color; as well as subjective traits, such as the emotion a personexpresses or whether he is humorous or attractive. For sizeableexperimentation, we contribute a new Face Attributes Dataset (FAD), havingroughly 200,000 attribute labels for the above traits, for over 10,000 facialimages. Due to the recent surge of research on Deep Convolutional NeuralNetworks (CNNs), we begin by using a CNN architecture for estimating facialattributes and show that they indeed provide an impressive baselineperformance. To further improve performance, we propose a novel approach thatincorporates facial landmark information for input images as an additionalchannel, helping the CNN learn better attribute-specific features so that thelandmarks across various training images hold correspondence. We empiricallyanalyse the performance of our method, showing consistent improvement over thebaseline across traits. version:1
arxiv-1605-09049 | Recycling Randomness with Structure for Sublinear time Kernel Expansions | http://arxiv.org/abs/1605.09049 | author:Krzysztof Choromanski, Vikas Sindhwani category:cs.LG cs.NA stat.ML  published:2016-05-29 summary:We propose a scheme for recycling Gaussian random vectors into structuredmatrices to approximate various kernel functions in sublinear time via randomembeddings. Our framework includes the Fastfood construction as a special case,but also extends to Circulant, Toeplitz and Hankel matrices, and the broaderfamily of structured matrices that are characterized by the concept oflow-displacement rank. We introduce notions of coherence and graph-theoreticstructural constants that control the approximation quality, and proveunbiasedness and low-variance properties of random feature maps that arisewithin our framework. For the case of low-displacement matrices, we show howthe degree of structure and randomness can be controlled to reduce statisticalvariance at the cost of increased computation and storage requirements.Empirical results strongly support our theory and justify the use of a broaderfamily of structured matrices for scaling up kernel methods using randomfeatures. version:1
arxiv-1605-09046 | TripleSpin - a generic compact paradigm for fast machine learning computations | http://arxiv.org/abs/1605.09046 | author:Krzysztof Choromanski, Francois Fagan, Cedric Gouy-Pailler, Anne Morvan, Tamas Sarlos, Jamal Atif category:cs.LG stat.ML  published:2016-05-29 summary:We present a generic compact computational framework relying on structuredrandom matrices that can be applied to speed up several machine learningalgorithms with almost no loss of accuracy. The applications include new fastLSH-based algorithms, efficient kernel computations via random feature maps,convex optimization algorithms, quantization techniques and many more. Certainmodels of the presented paradigm are even more compressible since they applyonly bit matrices. This makes them suitable for deploying on mobile devices.All our findings come with strong theoretical guarantees. In particular, as abyproduct of the presented techniques and by using relatively newBerry-Esseen-type CLT for random vectors, we give the first theoreticalguarantees for one of the most efficient existing LSH algorithms based on the$\textbf{HD}_{3}\textbf{HD}_{2}\textbf{HD}_{1}$ structured matrix ("Practicaland Optimal LSH for Angular Distance"). These guarantees as well as theoreticalresults for other aforementioned applications follow from the same generaltheoretical principle that we present in the paper. Our structured familycontains as special cases all previously considered structured schemes,including the recently introduced $P$-model. Experimental evaluation confirmsthe accuracy and efficiency of TripleSpin matrices. version:1
arxiv-1605-09042 | MCMC assisted by Belief Propagaion | http://arxiv.org/abs/1605.09042 | author:Sungsoo Ahn, Michael Chertkov, Jinwoo Shin category:stat.ML cs.AI cs.DS  published:2016-05-29 summary:Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the mostpopular algorithms for computational inference in Graphical Models (GM). Inprinciple, MCMC is an exact probabilistic method which, however, often suffersfrom exponentially slow mixing. In contrast, BP is a deterministic method,which is typically fast, empirically very successful, however in generallacking control of accuracy over loopy graphs. In this paper, we introduce MCMCalgorithms correcting the approximation error of BP, i.e., we provide a way tocompensate for BP errors via a consecutive BP-aware MCMC. Our framework isbased on the Loop Calculus (LC) approach which allows to express the BP erroras a sum of weighted generalized loops. Although the full series iscomputationally intractable, it is known that a truncated series, summing upall 2-regular loops, is computable in polynomial-time for planar pair-wisebinary GMs and it also provides a highly accurate approximation empirically.Motivated by this, we first propose a polynomial-time approximation MCMC schemefor the truncated series of general (non-planar) pair-wise binary models. Ourmain idea here is to use the Worm algorithm, known to provide fast mixing inother (related) problems, and then design an appropriate rejection scheme tosample 2-regular loops. Furthermore, we also design an efficient rejection-freeMCMC scheme for approximating the full series. The main novelty underlying ourdesign is in utilizing the concept of cycle basis, which provides an efficientdecomposition of the generalized loops. In essence, the proposed MCMC schemesrun on transformed GM built upon the non-trivial BP solution, and ourexperiments show that this synthesis of BP and MCMC outperforms both directMCMC and bare BP schemes. version:1
arxiv-1605-09026 | Singular ridge regression with homoscedastic residuals: generalization error with estimated parameters | http://arxiv.org/abs/1605.09026 | author:Lyudmila Grigoryeva, Juan-Pablo Ortega category:stat.ML stat.ME  published:2016-05-29 summary:This paper characterizes the conditional distribution properties of thefinite sample ridge regression estimator and uses that result to evaluate totalregression and generalization errors that incorporate the inaccuraciescommitted at the time of parameter estimation. The paper provides explicitformulas for those errors. Unlike other classical references in this setup, ourresults take place in a fully singular setup that does not assume the existenceof a solution for the non-regularized regression problem. In exchange, weinvoke a conditional homoscedasticity hypothesis on the regularized regressionresiduals that is crucial in our developments. version:1
arxiv-1605-09004 | Tight (Lower) Bounds for the Fixed Budget Best Arm Identification Bandit Problem | http://arxiv.org/abs/1605.09004 | author:Alexandra Carpentier, Andrea Locatelli category:stat.ML cs.LG  published:2016-05-29 summary:We consider the problem of \textit{best arm identification} with a\textit{fixed budget $T$}, in the $K$-armed stochastic bandit setting, witharms distribution defined on $[0,1]$. We prove that any bandit strategy, for atleast one bandit problem characterized by a complexity $H$, will misidentifythe best arm with probability lower bounded by$$\exp\Big(-\frac{T}{\log(K)H}\Big),$$ where $H$ is the sum for all sub-optimalarms of the inverse of the squared gaps. Our result disproves formally thegeneral belief - coming from results in the fixed confidence setting - thatthere must exist an algorithm for this problem whose probability of error isupper bounded by $\exp(-T/H)$. This also proves that some existing strategiesbased on the Successive Rejection of the arms are optimal - closing thereforethe current gap between upper and lower bounds for the fixed budget best armidentification problem. version:1
arxiv-1605-08988 | On Explore-Then-Commit Strategies | http://arxiv.org/abs/1605.08988 | author:Aurélien Garivier, Emilie Kaufmann, Tor Lattimore category:math.ST cs.LG stat.TH  published:2016-05-29 summary:We study the problem of minimising regret in two-armed bandit problems withGaussian rewards. Our objective is to use this simple setting to illustratethat strategies based on an exploration phase (up to a stopping time) followedby exploitation are necessarily suboptimal. The results hold regardless ofwhether or not the difference in means between the two arms is known. Besidesthe main message, we also refine existing deviation inequalities, which allowus to design fully sequential strategies with finite-time regret guaranteesthat are (a) asymptotically optimal as the horizon grows and (b) order-optimalin the minimax sense. Furthermore we provide empirical evidence that the theoryalso holds in practice and discuss extensions to non-gaussian andmultiple-armed case. version:1
arxiv-1605-08961 | A simple and provable algorithm for sparse diagonal CCA | http://arxiv.org/abs/1605.08961 | author:Megasthenis Asteris, Anastasios Kyrillidis, Oluwasanmi Koyejo, Russell Poldrack category:stat.ML cs.DS cs.IT math.IT math.OC stat.ME  published:2016-05-29 summary:Given two sets of variables, derived from a common set of samples, sparseCanonical Correlation Analysis (CCA) seeks linear combinations of a smallnumber of variables in each set, such that the induced canonical variables aremaximally correlated. Sparse CCA is NP-hard. We propose a novel combinatorial algorithm for sparse diagonal CCA, i.e.,sparse CCA under the additional assumption that variables within each set arestandardized and uncorrelated. Our algorithm operates on a low rankapproximation of the input data and its computational complexity scaleslinearly with the number of input variables. It is simple to implement, andparallelizable. In contrast to most existing approaches, our algorithmadministers precise control on the sparsity of the extracted canonical vectors,and comes with theoretical data-dependent global approximation guarantees, thathinge on the spectrum of the input data. Finally, it can be straightforwardlyadapted to other constrained variants of CCA enforcing structure beyondsparsity. We empirically evaluate the proposed scheme and apply it on a realneuroimaging dataset to investigate associations between brain activity andbehavior measurements. version:1
arxiv-1605-08933 | Interaction Pursuit with Feature Screening and Selection | http://arxiv.org/abs/1605.08933 | author:Yingying Fan, Yinfei Kong, Daoji Li, Jinchi Lv category:stat.ME stat.ML  published:2016-05-28 summary:Understanding how features interact with each other is of paramountimportance in many scientific discoveries and contemporary applications. Yetinteraction identification becomes challenging even for a moderate number ofcovariates. In this paper, we suggest an efficient and flexible procedure,called the interaction pursuit (IP), for interaction identification inultra-high dimensions. The suggested method first reduces the number ofinteractions and main effects to a moderate scale by a new feature screeningapproach, and then selects important interactions and main effects in thereduced feature space using regularization methods. Compared to existingapproaches, our method screens interactions separately from main effects andthus can be more effective in interaction screening. Under a fairly generalframework, we establish that for both interactions and main effects, the methodenjoys the sure screening property in screening and oracle inequalities inselection. Our method and theoretical results are supported by severalsimulation and real data examples. version:1
arxiv-1605-08912 | A Riemannian Framework for Statistical Analysis of Topological Persistence Diagrams | http://arxiv.org/abs/1605.08912 | author:Rushil Anirudh, Vinay Venkataraman, Karthikeyan Natesan Ramamurthy, Pavan Turaga category:math.AT cs.CG cs.CV math.DG math.ST stat.TH  published:2016-05-28 summary:Topological data analysis is becoming a popular way to study high dimensionalfeature spaces without any contextual clues or assumptions. This paper concernsitself with one popular topological feature, which is the number of$d-$dimensional holes in the dataset, also known as the Betti$-d$ number. Thepersistence of the Betti numbers over various scales is encoded into apersistence diagram (PD), which indicates the birth and death times of theseholes as scale varies. A common way to compare PDs is by a point-to-pointmatching, which is given by the $n$-Wasserstein metric. However, a big drawbackof this approach is the need to solve correspondence between points beforecomputing the distance; for $n$ points, the complexity grows according to$\mathcal{O}($n$^3)$. Instead, we propose to use an entirely new frameworkbuilt on Riemannian geometry, that models PDs as 2D probability densityfunctions that are represented in the square-root framework on a HilbertSphere. The resulting space is much more intuitive with closed form expressionsfor common operations. The distance metric is 1) correspondence-free and also2) independent of the number of points in the dataset. The complexity ofcomputing distance between PDs now grows according to $\mathcal{O}(K^2)$, for a$K \times K$ discretization of $[0,1]^2$. This also enables the use of existingmachinery in differential geometry towards statistical analysis of PDs such ascomputing the mean, geodesics, classification etc. We report competitiveresults with the Wasserstein metric, at a much lower computational load,indicating the favorable properties of the proposed approach. version:1
arxiv-1605-08900 | Aspect Level Sentiment Classification with Deep Memory Network | http://arxiv.org/abs/1605.08900 | author:Duyu Tang, Bing Qin, Ting Liu category:cs.CL  published:2016-05-28 summary:We introduce a deep memory network for aspect level sentiment classification.Unlike feature-based SVM and sequential neural models such as LSTM, thisapproach explicitly captures the importance of each context word when inferringthe sentiment polarity of an aspect. Such importance degree and textrepresentation are calculated with multiple computational layers, each of whichis a neural attention model over an external memory. Experiments on laptop andrestaurant datasets demonstrate that our approach performs comparable tostate-of-art feature based SVM system, and substantially better than LSTM andattention-based LSTM architectures. On both datasets we show that multiplecomputational layers could improve the performance. Moreover, our approach isalso fast. The deep memory network with 9 layers is 15 times faster than LSTMwith a CPU implementation. version:1
arxiv-1605-08889 | Beyond Majority Voting: Generating Evaluation Scales using Item Response Theory | http://arxiv.org/abs/1605.08889 | author:John Lalor, Hao Wu, Hong Yu category:cs.CL  published:2016-05-28 summary:We introduce Item Response Theory (IRT) from psychometrics as an alternativeto majority voting to create an IRT gold standard ($GS_{IRT}$). IRT describescharacteristics of individual items in $GS_{IRT}$ - their difficulty anddiscriminating power - and is able to account for these characteristics in itsestimation of human intelligence or ability for an NLP task. In this paper, weevaluated IRT's model-fitting of a majority vote gold standard designed forRecognizing Textual Entailment (RTE), denoted as $GS_{RTE}$. By collectinghuman responses and fitting our IRT model, we found that up to 31% of$GS_{RTE}$ were not useful in building $GS_{IRT}$ for RTE. In addition, wefound low inter-annotator agreement for some items in $GS_{RTE}$ suggestingthat more work is needed for creating intelligent gold-standards. version:1
arxiv-1605-08882 | Optimal Learning for Multi-pass Stochastic Gradient Methods | http://arxiv.org/abs/1605.08882 | author:Junhong Lin, Lorenzo Rosasco category:cs.LG math.OC stat.ML  published:2016-05-28 summary:We analyze the learning properties of the stochastic gradient method whenmultiple passes over the data and mini-batches are allowed. In particular, weconsider the square loss and show that for a universal step-size choice, thenumber of passes acts as a regularization parameter, and optimal finite samplebounds can be achieved by early-stopping. Moreover, we show that largerstep-sizes are allowed when considering mini-batches. Our analysis is based ona unifying approach, encompassing both batch and stochastic gradient methods asspecial cases. version:1
arxiv-1605-08881 | Sparse Coding and Counting for Robust Visual Tracking | http://arxiv.org/abs/1605.08881 | author:Risheng Liu, Jing Wang, Yiyang Wang, Zhixun Su, Yu Cai category:cs.CV  published:2016-05-28 summary:In this paper, we propose a novel sparse coding and counting method underBayesian framwork for visual tracking. In contrast to existing methods, theproposed method employs the combination of L0 and L1 norm to regularize thelinear coefficients of incrementally updated linear basis. The sparsityconstraint enables the tracker to effectively handle difficult challenges, suchas occlusion or image corruption. To achieve realtime processing, we propose afast and efficient numerical algorithm for solving the proposed model. Althoughit is an NP-hard problem, the proposed accelerated proximal gradient (APG)approach is guaranteed to converge to a solution quickly. Besides, we provide aclosed solution of combining L0 and L1 regularized representation to obtainbetter sparsity. Experimental results on challenging video sequencesdemonstrate that the proposed method achieves state-of-the-art results both inaccuracy and speed. version:1
arxiv-1605-08872 | Online Bayesian Collaborative Topic Regression | http://arxiv.org/abs/1605.08872 | author:Chenghao Liu, Tao Jin, Steven C. H. Hoi, Peilin Zhao, Jianling Sun category:cs.LG cs.IR  published:2016-05-28 summary:Collaborative Topic Regression (CTR) combines ideas of probabilistic matrixfactorization (PMF) and topic modeling (e.g., LDA) for recommender systems,which has gained increasing successes in many applications. Despite enjoyingmany advantages, the existing CTR algorithms have some critical limitations.First of all, they are often designed to work in a batch learning manner,making them unsuitable to deal with streaming data or big data in real-worldrecommender systems. Second, the document-specific topic proportions of LDA arefed to the downstream PMF, but not reverse, which is sub-optimal as the ratinginformation is not exploited in discovering the low-dimensional representationof documents and thus can result in a sub-optimal representation forprediction. In this paper, we propose a novel scheme of Online BayesianCollaborative Topic Regression (OBCTR) which is efficient and scalable forlearning from data streams. Particularly, we {\it jointly} optimize thecombined objective function of both PMF and LDA in an online learning fashion,in which both PMF and LDA tasks can be reinforced each other during the onlinelearning process. Our encouraging experimental results on real-world datavalidate the effectiveness of the proposed method. version:1
arxiv-1605-08857 | Video Key Frame Extraction using Entropy value as Global and Local Feature | http://arxiv.org/abs/1605.08857 | author:Siddu P Algur, Vivek R category:cs.CV  published:2016-05-28 summary:Key frames play an important role in video annotation. It is one of thewidely used methods for video abstraction as this will help us for processing alarge set of video data with sufficient content representation in faster way.In this paper a novel approach for key-frame extraction using entropy value isproposed. The proposed approach classifies frames based on entropy values asglobal feature and selects frame from each class as representative key-frame.It also eliminates redundant frames from selected key-frames using entropyvalue as local feature. Evaluation of the approach on several video clips hasbeen presented. Results show that the algorithm is successful in helpingannotators automatically identify video key-frames. version:1
arxiv-1605-08856 | A Channelized Binning Method for Extraction of Dominant Color Pixel Value | http://arxiv.org/abs/1605.08856 | author:Siddu P Algur, N H Ayachit, Vivek R category:cs.CV  published:2016-05-28 summary:The Color is one of the most important and easily identifiable features fordescribing the visual content. The MPEG standard has developed a number ofdescriptors that covers different aspects of the visual content. The Dominantcolor descriptor is one of them. This paper proposes a channelized binningapproach a novel method for extraction of the dominant color pixel value whichis a variant of the dominant color descriptor. The Channelized binning methodtreats the problem as a statistical problem and tries to avoid colorquantization and interpolation guessing of number and centroid of dominantcolors. Channelized binning is an iterative approach which automaticallyestimates the number of dominant pixel values and their centroids. It operateson 24 bit full RGB color space, by considering one color channel at a time andhence avoiding the color quantization. Results show that the proposed methodcan successfully extract dominant color pixel values. version:1
arxiv-1605-08838 | Dueling Bandits with Dependent Arms | http://arxiv.org/abs/1605.08838 | author:Bangrui Chen, Peter Frazier category:cs.LG  published:2016-05-28 summary:We consider online content recommendation with implicit feedback throughpairwise comparisons. We study a new formulation of the dueling bandit problemsin which arms are dependent and regret occurs when neither pulled arm isoptimal. We propose a new algorithm, Comparing The Best (CTB), withcomputational requirements appropriate for problems with few arms, and avariation of this algorithm whose computation scales to problems with manyarms. We show both algorithms have constant expected cumulative regret. Wedemonstrate through numerical experiments on simulated and real dataset thatthese algorithms improve significantly over existing algorithms in the settingwe study. version:1
arxiv-1605-08833 | Muffled Semi-Supervised Learning | http://arxiv.org/abs/1605.08833 | author:Akshay Balsubramani, Yoav Freund category:cs.LG stat.ML  published:2016-05-28 summary:We explore a novel approach to semi-supervised learning. This approach iscontrary to the common approach in that the unlabeled examples serve to"muffle," rather than enhance, the guidance provided by the labeled examples.We provide several variants of the basic algorithm and show experimentally thatthey can achieve significantly higher AUC than boosted trees, random forestsand logistic regression when unlabeled examples are available. version:1
arxiv-1605-08831 | Weighted Residuals for Very Deep Networks | http://arxiv.org/abs/1605.08831 | author:Falong Shen, Gang Zeng category:cs.CV  published:2016-05-28 summary:Deep residual networks have recently shown appealing performance on manychallenging computer vision tasks. However, the original residual structurestill has some defects making it difficult to converge on very deep networks.In this paper, we introduce a weighted residual network to address theincompatibility between \texttt{ReLU} and element-wise addition and the deepnetwork initialization problem. The weighted residual network is able to learnto combine residuals from different layers effectively and efficiently. Theproposed models enjoy a consistent improvement over accuracy and convergencewith increasing depths from 100+ layers to 1000+ layers. Besides, the weightedresidual networks have little more computation and GPU memory burden than theoriginal residual networks. The networks are optimized by projected stochasticgradient descent. Experiments on CIFAR-10 have shown that our algorithm has a\emph{faster convergence speed} than the original residual networks and reachesa \emph{high accuracy} at 95.3\% with a 1192-layer model. version:1
arxiv-1605-08803 | Density estimation using Real NVP | http://arxiv.org/abs/1605.08803 | author:Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio category:cs.LG  published:2016-05-27 summary:Unsupervised learning of probabilistic models is a central yet challengingproblem in machine learning. Specifically, designing models with tractablelearning, sampling, inference and evaluation is crucial in solving this task.We extend the space of such models using real-valued non-volume preserving(real NVP) transformations, a set of powerful invertible and learnabletransformations, resulting in an unsupervised learning algorithm with exactlog-likelihood computation, exact sampling, exact inference of latentvariables, and an interpretable latent space. We demonstrate its ability tomodel natural images on four datasets through sampling, log-likelihoodevaluation and latent variable manipulations. version:1
arxiv-1605-08798 | Asymptotic Analysis of Objectives based on Fisher Information in Active Learning | http://arxiv.org/abs/1605.08798 | author:Jamshid Sourati, Murat Akcakaya, Todd K. Leen, Deniz Erdogmus, Jennifer G. Dy category:stat.ML cs.LG  published:2016-05-27 summary:Obtaining labels can be costly and time-consuming. Active learning allows alearning algorithm to intelligently query samples to be labeled for efficientlearning. Fisher information ratio (FIR) has been used as an objective forselecting queries in active learning. However, little is known about the theorybehind the use of FIR for active learning. There is a gap between theunderlying theory and the motivation of its usage in practice. In this paper,we attempt to fill this gap and provide a rigorous framework for analyzing theexisting FIR-based active learning methods. In particular, we show that FIR canbe asymptotically viewed as an upper bound of the expected variance of thelog-likelihood ratio. Additionally, our analysis suggests a unifying frameworkthat not only enables us to make theoretical comparisons among the existingquerying methods based on FIR, but also gives more insight into the developmentof new active learning approaches based on this objective. version:1
arxiv-1605-08764 | Stacking With Auxiliary Features | http://arxiv.org/abs/1605.08764 | author:Nazneen Fatema Rajani, Raymond J. Mooney category:cs.CL cs.CV cs.LG  published:2016-05-27 summary:Ensembling methods are well known for improving prediction accuracy. However,they are limited in the sense that they cannot discriminate among componentmodels effectively. In this paper, we propose stacking with auxiliary featuresthat learns to fuse relevant information from multiple systems to improveperformance. Auxiliary features enable the stacker to rely on systems that notjust agree on an output but also the provenance of the output. We demonstrateour approach on three very different and difficult problems -- the Cold StartSlot Filling, the Tri-lingual Entity Discovery and Linking and the ImageNetobject detection tasks. We obtain new state-of-the-art results on the first twotasks and substantial improvements on the detection task, thus verifying thepower and generality of our approach. version:1
arxiv-1605-07144 | Actively Learning Hemimetrics with Applications to Eliciting User Preferences | http://arxiv.org/abs/1605.07144 | author:Adish Singla, Sebastian Tschiatschek, Andreas Krause category:stat.ML cs.LG  published:2016-05-23 summary:Motivated by an application of eliciting users' preferences, we investigatethe problem of learning hemimetrics, i.e., pairwise distances among a set of$n$ items that satisfy triangle inequalities and non-negativity constraints. Inour application, the (asymmetric) distances quantify private costs a userincurs when substituting one item by another. We aim to learn these distances(costs) by asking the users whether they are willing to switch from one item toanother for a given incentive offer. Without exploiting structural constraintsof the hemimetric polytope, learning the distances between each pair of itemsrequires $\Theta(n^2)$ queries. We propose an active learning algorithm thatsubstantially reduces this sample complexity by exploiting the structuralconstraints on the version space of hemimetrics. Our proposed algorithmachieves provably-optimal sample complexity for various instances of the task.For example, when the items are embedded into $K$ tight clusters, the samplecomplexity of our algorithm reduces to $O(n K)$. Extensive experiments on arestaurant recommendation data set support the conclusions of our theoreticalanalysis. version:2
arxiv-1605-08722 | An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits | http://arxiv.org/abs/1605.08722 | author:Peter Auer, Chao-Kai Chiang category:cs.LG  published:2016-05-27 summary:We present an algorithm that achieves almost optimal pseudo-regret boundsagainst adversarial and stochastic bandits. Against adversarial bandits thepseudo-regret is $O(K\sqrt{n \log n})$ and against stochastic bandits thepseudo-regret is $O(\sum_i (\log n)/\Delta_i)$. We also show that no algorithmwith $O(\log n)$ pseudo-regret against stochastic bandits can achieve$\tilde{O}(\sqrt{n})$ expected regret against adaptive adversarial bandits.This complements previous results of Bubeck and Slivkins (2012) that show$\tilde{O}(\sqrt{n})$ expected adversarial regret with $O((\log n)^2)$stochastic pseudo-regret. version:1
arxiv-1605-08680 | Achieving stable subspace clustering by post-processing generic clustering results | http://arxiv.org/abs/1605.08680 | author:Duc-Son Pham, Ognjen Arandjelovic, Svetha Venkatesh category:cs.CV  published:2016-05-27 summary:We propose an effective subspace selection scheme as a post-processing stepto improve results obtained by sparse subspace clustering (SSC). Our methodstarts by the computation of stable subspaces using a novel random samplingscheme. Thus constructed preliminary subspaces are used to identify theinitially incorrectly clustered data points and then to reassign them to moresuitable clusters based on their goodness-of-fit to the preliminary model. Toimprove the robustness of the algorithm, we use a dominant nearest subspaceclassification scheme that controls the level of sensitivity againstreassignment. We demonstrate that our algorithm is convergent and superior tothe direct application of a generic alternative such as principal componentanalysis. On several popular datasets for motion segmentation and faceclustering pervasively used in the sparse subspace clustering literature theproposed method is shown to reduce greatly the incidence of clustering errorswhile introducing negligible disturbance to the data points already correctlyclustered. version:1
arxiv-1605-08675 | Boosting Question Answering by Deep Entity Recognition | http://arxiv.org/abs/1605.08675 | author:Piotr Przybyła category:cs.CL  published:2016-05-27 summary:In this paper an open-domain factoid question answering system for Polish,RAFAEL, is presented. The system goes beyond finding an answering sentence; italso extracts a single string, corresponding to the required entity. Herein thefocus is placed on different approaches to entity recognition, essential forretrieving information matching question constraints. Apart from traditionalapproach, including named entity recognition (NER) solutions, a noveltechnique, called Deep Entity Recognition (DeepER), is introduced andimplemented. It allows a comprehensive search of all forms of entity referencesmatching a given WordNet synset (e.g. an impressionist), based on a previouslyassembled entity library. It has been created by analysing the first sentencesof encyclopaedia entries and disambiguation and redirect pages. DeepER alsoprovides automatic evaluation, which makes possible numerous experiments,including over a thousand questions from a quiz TV show answered on the groundsof Polish Wikipedia. The final results of a manual evaluation on a separatequestion set show that the strength of DeepER approach lies in its ability toanswer questions that demand answers beyond the traditional categories of namedentities. version:1
arxiv-1605-08671 | An optimal algorithm for the Thresholding Bandit Problem | http://arxiv.org/abs/1605.08671 | author:Andrea Locatelli, Maurilio Gutzeit, Alexandra Carpentier category:stat.ML cs.LG  published:2016-05-27 summary:We study a specific \textit{combinatorial pure exploration stochastic banditproblem} where the learner aims at finding the set of arms whose means areabove a given threshold, up to a given precision, and \textit{for a fixed timehorizon}. We propose a parameter-free algorithm based on an original heuristic,and prove that it is optimal for this problem by deriving matching upper andlower bounds. To the best of our knowledge, this is the first non-trivial pureexploration setting with \textit{fixed budget} for which optimal strategies areconstructed. version:1
arxiv-1605-08636 | PAC-Bayesian Theory Meets Bayesian Inference | http://arxiv.org/abs/1605.08636 | author:Pascal Germain, Francis Bach, Alexandre Lacoste, Simon Lacoste-Julien category:stat.ML cs.LG  published:2016-05-27 summary:We exhibit a strong link between frequentist PAC-Bayesian bounds and theBayesian marginal likelihood. That is, for the negative log-likelihood lossfunction, we show that the minimization of PAC-Bayesian generalization boundsmaximizes the Bayesian marginal likelihood. This provides an alternativeexplanation to the Bayesian Occam's razor criteria, under the assumption thatthe data is generated by a i.i.d. distribution. Moreover, as the negativelog-likelihood is an unbounded loss function, we motivate and propose aPAC-Bayesian theorem tailored for the sub-Gamma loss family, and we show thatour approach is sound on classical Bayesian linear regression tasks. version:1
arxiv-1605-08233 | Stochastic Variance Reduced Riemannian Eigensolver | http://arxiv.org/abs/1605.08233 | author:Zhiqiang Xu, Yiping Ke category:cs.LG stat.ML  published:2016-05-26 summary:We study the stochastic Riemannian gradient algorithm for matrixeigen-decomposition. The state-of-the-art stochastic Riemannian algorithmrequires the learning rate to decay to zero and thus suffers from slowconvergence and sub-optimal solutions. In this paper, we address this issue bydeploying the variance reduction (VR) technique of stochastic gradient descent(SGD). The technique was originally developed to solve convex problems in theEuclidean space. We generalize it to Riemannian manifolds and realize it tosolve the non-convex eigen-decomposition problem. We are the first to proposeand analyze the generalization of SVRG to Riemannian manifolds. Specifically,we propose the general variance reduction form, SVRRG, in the framework of thestochastic Riemannian gradient optimization. It's then specialized to theproblem with eigensolvers and induces the SVRRG-EIGS algorithm. We provide anovel and elegant theoretical analysis on this algorithm. The theory shows thata fixed learning rate can be used in the Riemannian setting with an exponentialglobal convergence rate guaranteed. The theoretical results make a significantimprovement over existing studies, with the effectiveness empirically verified. version:2
arxiv-1605-08618 | Variational Bayesian Inference for Hidden Markov Models With Multivariate Gaussian Output Distributions | http://arxiv.org/abs/1605.08618 | author:Christian Gruhl, Bernhard Sick category:cs.LG stat.ML  published:2016-05-27 summary:Hidden Markov Models (HMM) have been used for several years in many timeseries analysis or pattern recognitions tasks. HMM are often trained by meansof the Baum-Welch algorithm which can be seen as a special variant of anexpectation maximization (EM) algorithm. Second-order training techniques suchas Variational Bayesian Inference (VI) for probabilistic models regard theparameters of the probabilistic models as random variables and definedistributions over these distribution parameters, hence the name of thistechnique. VI can also bee regarded as a special case of an EM algorithm. Inthis article, we bring both together and train HMM with multivariate Gaussianoutput distributions with VI. The article defines the new training techniquefor HMM. An evaluation based on some case studies and a comparison to relatedapproaches is part of our ongoing work. version:1
arxiv-1605-08576 | Merging MCMC Subposteriors through Gaussian-Process Approximations | http://arxiv.org/abs/1605.08576 | author:Christopher Nemeth, Chris Sherlock category:stat.CO stat.ML  published:2016-05-27 summary:Markov chain Monte Carlo (MCMC) algorithms have become powerful tools forBayesian inference. However, they do not scale well to large-data problems.Divide-and-conquer strategies, which split the data into batches and, for eachbatch, run independent MCMC algorithms targeting the correspondingsubposterior, can spread the computational burden across a number of separateworkers. The challenge with such strategies is in recombining the subposteriorsto approximate the full posterior. By creating a Gaussian-process approximationfor each log-subposterior density we create a tractable approximation for thefull posterior. This approximation is exploited through three methodologies:firstly a Hamiltonian Monte Carlo algorithm targeting the expectation of theposterior density provides a sample from an approximation to the posterior;secondly, evaluating the true posterior at the sampled points leads to animportance sampler that, asymptotically, targets the true posteriorexpectations; finally, an alternative importance sampler uses the fullGaussian-process distribution of the approximation to the log-posterior densityto re-weight any initial sample and provide both an estimate of the posteriorexpectation and a measure of the uncertainty in it. version:1
arxiv-1605-08543 | Lazy Evaluation of Convolutional Filters | http://arxiv.org/abs/1605.08543 | author:Sam Leroux, Steven Bohez, Cedric De Boom, Elias De Coninck, Tim Verbelen, Bert Vankeirsbilck, Pieter Simoens, Bart Dhoedt category:cs.CV cs.NE  published:2016-05-27 summary:In this paper we propose a technique which avoids the evaluation of certainconvolutional filters in a deep neural network. This allows to trade-off theaccuracy of a deep neural network with the computational and memoryrequirements. This is especially important on a constrained device unable tohold all the weights of the network in memory. version:1
arxiv-1605-08535 | Deep API Learning | http://arxiv.org/abs/1605.08535 | author:Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, Sunghun Kim category:cs.SE cs.CL cs.LG cs.NE D.2.13  published:2016-05-27 summary:Developers often wonder how to implement a certain functionality (e.g., howto parse XML files) using APIs. Obtaining an API usage sequence based on anAPI-related natural language query is very helpful in this regard. Given aquery, existing approaches utilize information retrieval models to search formatching API sequences. These approaches treat queries and APIs as bag-of-words(i.e., keyword matching or word-to-word alignment) and lack a deepunderstanding of the semantics of the query. We propose DeepAPI, a deep learning based approach to generate API usagesequences for a given natural language query. Instead of a bags-of-wordsassumption, it learns the sequence of words in a query and the sequence ofassociated APIs. DeepAPI adapts a neural language model named RNNEncoder-Decoder. It encodes a word sequence (user query) into a fixed-lengthcontext vector, and generates an API sequence based on the context vector. Wealso augment the RNN Encoder-Decoder by considering the importance ofindividual APIs. We empirically evaluate our approach with more than 7 millionannotated code snippets collected from GitHub. The results show that ourapproach generates largely accurate API sequences and outperforms the relatedapproaches. version:1
arxiv-1605-08527 | Stochastic Optimization for Large-scale Optimal Transport | http://arxiv.org/abs/1605.08527 | author:Genevay Aude, Marco Cuturi, Gabriel Peyré, Francis Bach category:math.OC cs.LG math.NA  published:2016-05-27 summary:Optimal transport (OT) defines a powerful framework to compare probabilitydistributions in a geometrically faithful way. However, the practical impact ofOT is still limited because of its computational burden. We propose a new classof stochastic optimization algorithms to cope with large-scale problemsroutinely encountered in machine learning applications. These methods are ableto manipulate arbitrary distributions (either discrete or continuous) by simplyrequiring to be able to draw samples from them, which is the typical setup inhigh-dimensional learning problems. This alleviates the need to discretizethese densities, while giving access to provably convergent methods that outputthe correct distance without discretization error. These algorithms rely on twomain ideas: (a) the dual OT problem can be re-cast as the maximization of anexpectation ; (b) entropic regularization of the primal OT problem results in asmooth dual optimization optimization which can be addressed with algorithmsthat have a provably faster convergence. We instantiate these ideas in threedifferent setups: (i) when comparing a discrete distribution to another, weshow that incremental stochastic optimization schemes can beat Sinkhorn'salgorithm, the current state-of-the-art finite dimensional OT solver; (ii) whencomparing a discrete distribution to a continuous density, a semi-discretereformulation of the dual program is amenable to averaged stochastic gradientdescent, leading to better performance than approximately solving the problemby discretization ; (iii) when dealing with two continuous densities, wepropose a stochastic gradient descent over a reproducing kernel Hilbert space(RKHS). This is currently the only known method to solve this problem, apartfrom computing OT on finite samples. We backup these claims on a set ofdiscrete, semi-discrete and continuous benchmark problems. version:1
arxiv-1605-08512 | SNN: Stacked Neural Networks | http://arxiv.org/abs/1605.08512 | author:Milad Mohammadi, Subhasis Das category:cs.LG cs.CV cs.NE  published:2016-05-27 summary:It has been proven that transfer learning provides an easy way to achievestate-of-the-art accuracies on several vision tasks by training a simpleclassifier on top of features obtained from pre-trained neural networks. Thegoal of this work is to generate better features for transfer learning frommultiple publicly available pre- trained neural networks. To this end, wepropose a novel architecture called Stacked Neural Networks which lever- agesthe fast training time of transfer learning while simul- taneously being muchmore accurate. We show that us- ing a stacked NN architecture can result in upto 8% im- provements in accuracy over state-of-the-art techniques us- ing onlyone pre-trained network for transfer learning. A second aim of this work is tomake network fine- tuning retain the generalizability of the base network toun- seen tasks. To this end, we propose a new technique called "jointfine-tuning" that is able to give accuracies compa- rable to finetuning thesame network individually over two datasets. We also show that a jointlyfinetuned network gen- eralizes better to unseen tasks when compared to anetwork finetuned over a single task. version:1
arxiv-1605-08501 | Local Region Sparse Learning for Image-on-Scalar Regression | http://arxiv.org/abs/1605.08501 | author:Yao Chen, Xiao Wang, Linglong Kong, Hongtu Zhu category:stat.ML cs.LG  published:2016-05-27 summary:Identification of regions of interest (ROI) associated with certain diseasehas a great impact on public health. Imposing sparsity of pixel values andextracting active regions simultaneously greatly complicate the image analysis.We address these challenges by introducing a novel region-selection penalty inthe framework of image-on-scalar regression. Our penalty combines the SmoothlyClipped Absolute Deviation (SCAD) regularization, enforcing sparsity, and theSCAD of total variation (TV) regularization, enforcing spatial contiguity, intoone group, which segments contiguous spatial regions against zero-valuedbackground. Efficient algorithm is based on the alternative direction method ofmultipliers (ADMM) which decomposes the non-convex problem into two iterativeoptimization problems with explicit solutions. Another virtue of the proposedmethod is that a divide and conquer learning algorithm is developed, therebyallowing scaling to large images. Several examples are presented and theexperimental results are compared with other state-of-the-art approaches. version:1
arxiv-1605-08497 | Universum Learning for SVM Regression | http://arxiv.org/abs/1605.08497 | author:Sauptik Dhar, Vladimir Cherkassky category:cs.LG  published:2016-05-27 summary:This paper extends the idea of Universum learning [18, 19] to regressionproblems. We propose new Universum-SVM formulation for regression problems thatincorporates a priori knowledge in the form of additional data samples. Theseadditional data samples or Universum belong to the same application domain asthe training samples, but they follow a different distribution. Severalempirical comparisons are presented to illustrate the utility of the proposedapproach. version:1
arxiv-1605-08491 | Provable Algorithms for Inference in Topic Models | http://arxiv.org/abs/1605.08491 | author:Sanjeev Arora, Rong Ge, Frederic Koehler, Tengyu Ma, Ankur Moitra category:cs.LG stat.ML  published:2016-05-27 summary:Recently, there has been considerable progress on designing algorithms withprovable guarantees -- typically using linear algebraic methods -- forparameter learning in latent variable models. But designing provable algorithmsfor inference has proven to be more challenging. Here we take a first steptowards provable inference in topic models. We leverage a property of topicmodels that enables us to construct simple linear estimators for the unknowntopic proportions that have small variance, and consequently can work withshort documents. Our estimators also correspond to finding an estimate aroundwhich the posterior is well-concentrated. We show lower bounds that for shorterdocuments it can be information theoretically impossible to find the hiddentopics. Finally, we give empirical results that demonstrate that our algorithmworks on realistic topic models. It yields good solutions on synthetic data andruns in time comparable to a {\em single} iteration of Gibbs sampling. version:1
arxiv-1605-07221 | Global Optimality of Local Search for Low Rank Matrix Recovery | http://arxiv.org/abs/1605.07221 | author:Srinadh Bhojanapalli, Behnam Neyshabur, Nathan Srebro category:stat.ML cs.LG math.OC  published:2016-05-23 summary:We show that there are no spurious local minima in the non-convex factorizedparametrization of low-rank matrix recovery from incoherent linearmeasurements. With noisy measurements we show all local minima are very closeto a global optimum. Together with a curvature bound at saddle points, thisyields a polynomial time global convergence guarantee for stochastic gradientdescent {\em from random initialization}. version:2
arxiv-1605-08481 | Open Problem: Best Arm Identification: Almost Instance-Wise Optimality and the Gap Entropy Conjecture | http://arxiv.org/abs/1605.08481 | author:Lijie Chen, Jian Li category:cs.LG  published:2016-05-27 summary:The best arm identification problem (BEST-1-ARM) is the most basic pureexploration problem in stochastic multi-armed bandits. The problem has a longhistory and attracted significant attention for the last decade. However, we donot yet have a complete understanding of the optimal sample complexity of theproblem: The state-of-the-art algorithms achieve a sample complexity of$O(\sum_{i=2}^{n} \Delta_{i}^{-2}(\ln\delta^{-1} + \ln\ln\Delta_i^{-1}))$($\Delta_{i}$ is the difference between the largest mean and the $i^{th}$mean), while the best known lower bound is $\Omega(\sum_{i=2}^{n}\Delta_{i}^{-2}\ln\delta^{-1})$ for general instances and $\Omega(\Delta^{-2}\ln\ln \Delta^{-1})$ for the two-arm instances. We propose to study theinstance-wise optimality for the BEST-1-ARM problem. Previous work has provedthat it is impossible to have an instance optimal algorithm for the 2-armproblem. However, we conjecture that modulo the additive term$\Omega(\Delta_2^{-2} \ln\ln \Delta_2^{-1})$ (which is an upper bound and worstcase lower bound for the 2-arm problem), there is an instance optimal algorithmfor BEST-1-ARM. Moreover, we introduce a new quantity, called the gap entropyfor a best-arm problem instance, and conjecture that it is the instance-wiselower bound. Hence, resolving this conjecture would provide a final answer tothe old and basic problem. version:1
arxiv-1605-08478 | Model-Free Imitation Learning with Policy Optimization | http://arxiv.org/abs/1605.08478 | author:Jonathan Ho, Jayesh K. Gupta, Stefano Ermon category:cs.LG cs.AI  published:2016-05-26 summary:In imitation learning, an agent learns how to behave in an environment withan unknown cost function by mimicking expert demonstrations. Existing imitationlearning algorithms typically involve solving a sequence of planning orreinforcement learning problems. Such algorithms are therefore not directlyapplicable to large, high-dimensional environments, and their performance cansignificantly degrade if the planning problems are not solved to optimality.Under the apprenticeship learning formalism, we develop alternative model-freealgorithms for finding a parameterized stochastic policy that performs at leastas well as an expert policy on an unknown cost function, based on sampletrajectories from the expert. Our approach, based on policy gradients, scalesto large continuous environments with guaranteed convergence to local minima. version:1
arxiv-1605-08470 | A Feature based Approach for Video Compression | http://arxiv.org/abs/1605.08470 | author:Rajer Sindhu category:cs.CV cs.MM  published:2016-05-26 summary:It is a high cost problem for panoramic image stitching via image matchingalgorithm and not practical for real-time performance. In this paper, we takefull advantage ofHarris corner invariant characterization method lightintensity parallel meaning, translation and rotation, and made a realtimepanoramic image stitching algorithm. According to the basic characteristics andperformance FPGA classical algorithm, several modules such as the feature pointextraction, and matching description is to optimize the feature-based logic.Real-time optimization system to achieve high precision match. The newalgorithm process the image from pixel domain and obtained from CCD cameraXilinx Spartan-6 hardware platform. After the image stitching algorithm, willeventually form a portable interface to output high-definition content on thedisplay. The results showed that, the proposed algorithm has higher precisionwith good real-time performance and robustness. version:1
arxiv-1605-08464 | Low-Cost Scene Modeling using a Density Function Improves Segmentation Performance | http://arxiv.org/abs/1605.08464 | author:Vivek Sharma, Sule Yildirim-Yayilgan, Luc Van Gool category:cs.CV cs.AI cs.HC cs.RO  published:2016-05-26 summary:We propose a low cost and effective way to combine a free simulation softwareand free CAD models for modeling human-object interaction in order to improvehuman & object segmentation. It is intended for research scenarios related tosafe human-robot collaboration (SHRC) and interaction (SHRI) in the industrialdomain. The task of human and object modeling has been used for detectingactivity, and for inferring and predicting actions, different from those works,we do human and object modeling in order to learn interactions in RGB-D datafor improving segmentation. For this purpose, we define a novel densityfunction to model a three dimensional (3D) scene in a virtual environment(VREP). This density function takes into account various possibleconfigurations of human-object and object-object relationships and interactionsgoverned by their affordances. Using this function, we synthesize a large,realistic and highly varied synthetic RGB-D dataset that we use for training.We train a random forest classifier, and the pixelwise predictions obtained isintegrated as a unary term in a pairwise conditional random fields (CRF). Ourevaluation shows that modeling these interactions improves segmentationperformance by ~7\% in mean average precision and recall over state-of-the-artmethods that ignore these interactions in real-world data. Our approach iscomputationally efficient, robust and can run real-time on consumer hardware. version:1
arxiv-1605-08455 | Suppressing Background Radiation Using Poisson Principal Component Analysis | http://arxiv.org/abs/1605.08455 | author:P. Tandon, P. Huggins, A. Dubrawski, S. Labov, K. Nelson category:cs.LG physics.data-an stat.ML  published:2016-05-26 summary:Performance of nuclear threat detection systems based on gamma-rayspectrometry often strongly depends on the ability to identify the part ofmeasured signal that can be attributed to background radiation. We havesuccessfully applied a method based on Principal Component Analysis (PCA) toobtain a compact null-space model of background spectra using PCA projectionresiduals to derive a source detection score. We have shown the method'sutility in a threat detection system using mobile spectrometers in urban scenes(Tandon et al 2012). While it is commonly assumed that measured photon countsfollow a Poisson process, standard PCA makes a Gaussian assumption about thedata distribution, which may be a poor approximation when photon counts arelow. This paper studies whether and in what conditions PCA with a Poisson-basedloss function (Poisson PCA) can outperform standard Gaussian PCA in modelingbackground radiation to enable more sensitive and specific nuclear threatdetection. version:1
arxiv-1605-08454 | Linear dynamical neural population models through nonlinear embeddings | http://arxiv.org/abs/1605.08454 | author:Yuanjun Gao, Evan Archer, Liam Paninski, John P. Cunningham category:q-bio.NC stat.ML  published:2016-05-26 summary:A body of recent work in modeling neural activity focuses on recoveringlow-dimensional latent features that capture the statistical structure oflarge-scale neural populations. Most such approaches have focused on lineargenerative models, where inference is computationally tractable. Here, wepropose fLDS, a general class of nonlinear generative models that permits thefiring rate of each neuron to vary as an arbitrary smooth function of a latent,linear dynamical state. This extra flexibility allows the model to capture aricher set of neural variability than a purely linear model, but retains aneasily visualizable low-dimensional latent space. To fit this class ofnon-conjugate models we propose a variational inference scheme, along with anovel approximate posterior capable of capturing rich temporal correlationsacross time. We show that our techniques permit inference in a wide class ofgenerative models.We also show in application to two neural datasets that,compared to state-of-the-art neural population models, fLDS captures a muchlarger proportion of neural variability with a small number of latentdimensions, providing superior predictive performance and interpretability. version:1
arxiv-1605-08412 | CITlab ARGUS for historical handwritten documents | http://arxiv.org/abs/1605.08412 | author:Gundram Leifert, Tobias Strauß, Tobias Grüning, Roger Labahn category:cs.CV cs.AI cs.NE 68T10  68T05  published:2016-05-26 summary:We describe CITlab's recognition system for the HTRtS competition attached tothe 13. International Conference on Document Analysis and Recognition, ICDAR2015. The task comprises the recognition of historical handwritten documents.The core algorithms of our system are based on multi-dimensional recurrentneural networks (MDRNN) and connectionist temporal classification (CTC). Thesoftware modules behind that as well as the basic utility technologies areessentially powered by PLANET's ARGUS framework for intelligent textrecognition and image processing. version:1
arxiv-1605-08401 | Dense Volume-to-Volume Vascular Boundary Detection | http://arxiv.org/abs/1605.08401 | author:Jameson Merkow, David Kriegman, Alison Marsden, Zhuowen Tu category:cs.CV  published:2016-05-26 summary:In this work, we present a novel 3D-Convolutional Neural Network (CNN)architecture called I2I-3D that predicts boundary location in volumetric data.Our fine-to-fine, deeply supervised framework addresses three critical issuesto 3D boundary detection: (1) efficient, holistic, end-to-end volumetric labeltraining and prediction (2) precise voxel-level prediction to capture finescale structures prevalent in medical data and (3) directed multi-scale,multi-level feature learning. We evaluate our approach on a dataset consistingof 93 medical image volumes with a wide variety of anatomical regions andvascular structures. In the process, we also introduce HED-3D, a 3D extensionof the state-of-the-art 2D edge detector (HED). We show that our deep learningapproach out-performs, the current state-of-the-art in 3D vascular boundarydetection (structured forests 3D), by a large margin, as well as HED applied toslices, and HED-3D while successfully localizing fine structures. With ourapproach, boundary detection takes about one minute on a typical 512x512x512volume. version:1
arxiv-1605-08400 | Total Variation Classes Beyond 1d: Minimax Rates, and the Limitations of Linear Smoothers | http://arxiv.org/abs/1605.08400 | author:Veeranjaneyulu Sadhanala, Yu-Xiang Wang, Ryan Tibshirani category:math.ST stat.ML stat.TH  published:2016-05-26 summary:We consider the problem of estimating a function defined over $n$ locationson a $d$-dimensional grid (having all side lengths equal to $n^{1/d}$). Whenthe function is constrained to have discrete total variation bounded by $C_n$,we derive the minimax optimal (squared) $\ell_2$ estimation error rate,parametrized by $n$ and $C_n$. Total variation denoising, also known as thefused lasso, is seen to be rate optimal. Several simpler estimators exist, suchas Laplacian smoothing and Laplacian eigenmaps. A natural question is: canthese simpler estimators perform just as well? We prove that these estimators,and more broadly all estimators given by linear transformations of the inputdata, are suboptimal over the class of functions with bounded variation. Thisextends fundamental findings of Donoho and Johnstone [1998] on 1-dimensionaltotal variation spaces to higher dimensions. The implication is that thecomputationally simpler methods cannot be used for such sophisticated denoisingtasks, without sacrificing statistical accuracy. We also derive minimax ratesfor discrete Sobolev spaces over $d$-dimensional grids, which are, in somesense, smaller than the total variation function spaces. Indeed, these aresmall enough spaces that linear estimators can be optimal---and a fewwell-known ones are, such as Laplacian smoothing and Laplacian eigenmaps, as weshow. Lastly, we investigate the problem of adaptivity of the total variationdenoiser to these smaller Sobolev function spaces. version:1
arxiv-1605-08397 | Domain Transfer Multi-Instance Dictionary Learning | http://arxiv.org/abs/1605.08397 | author:Ke Wang, Jiayong Liu, Daniel González category:cs.CV  published:2016-05-26 summary:In this paper, we invest the domain transfer learning problem withmulti-instance data. We assume we already have a well-trained multi-instancedictionary and its corresponding classifier from the source domain, which canbe used to represent and classify the bags. But it cannot be directly used tothe target domain. Thus we propose to adapt them to the target domain by addingan adaptive term to the source domain classifier. The adaptive function is alinear function based a domain transfer multi-instance dictionary. Given atarget domain bag, we first map it to a bag-level feature space using thedomain transfer dictionary, and then apply a the linear adaptive function toits bag-level feature vector. To learn the domain-transfer dictionary and theadaptive function parameter, we simultaneously minimize the averageclassification error of the target domain classifier over the target domaintraining set, and the complexities of both the adaptive function parameter andthe domain transfer dictionary. The minimization problem is solved by aniterative algorithm which update the dictionary and the function parameteralternately. Experiments over several benchmark data sets show the advantage ofthe proposed method over existing state-of-the-art domain transfermulti-instance learning methods. version:1
arxiv-1605-08396 | Robust Downbeat Tracking Using an Ensemble of Convolutional Networks | http://arxiv.org/abs/1605.08396 | author:S. Durand, J. P. Bello, B. David, G. Richard category:cs.SD cs.NE  published:2016-05-26 summary:In this paper, we present a novel state of the art system for automaticdownbeat tracking from music signals. The audio signal is first segmented inframes which are synchronized at the tatum level of the music. We then extractdifferent kind of features based on harmony, melody, rhythm and bass content tofeed convolutional neural networks that are adapted to take advantage of eachfeature characteristics. This ensemble of neural networks is combined to obtainone downbeat likelihood per tatum. The downbeat sequence is finally decodedwith a flexible and efficient temporal model which takes advantage of themetrical continuity of a song. We then perform an evaluation of our system on alarge base of 9 datasets, compare its performance to 4 other publishedalgorithms and obtain a significant increase of 16.8 percent points compared tothe second best system, for altogether a moderate cost in test and training.The influence of each step of the method is studied to show its strengths andshortcomings. version:1
arxiv-1605-08375 | Generalization Properties and Implicit Regularization for Multiple Passes SGM | http://arxiv.org/abs/1605.08375 | author:Junhong Lin, Raffaello Camoriano, Lorenzo Rosasco category:cs.LG stat.ML  published:2016-05-26 summary:We study the generalization properties of stochastic gradient methods forlearning with convex loss functions and linearly parameterized functions. Weshow that, in the absence of penalizations or constraints, the stability andapproximation properties of the algorithm can be controlled by tuning eitherthe step-size or the number of passes over the data. In this view, theseparameters can be seen to control a form of implicit regularization. Numericalresults complement the theoretical findings. version:1
arxiv-1605-08374 | Kronecker Determinantal Point Processes | http://arxiv.org/abs/1605.08374 | author:Zelda Mariet, Suvrit Sra category:cs.LG cs.AI stat.ML  published:2016-05-26 summary:Determinantal Point Processes (DPPs) are probabilistic models over allsubsets a ground set of $N$ items. They have recently gained prominence inseveral applications that rely on "diverse" subsets. However, theirapplicability to large problems is still limited due to the $\mathcal O(N^3)$complexity of core tasks such as sampling and learning. We enable efficientsampling and learning for DPPs by introducing KronDPP, a DPP model whose kernelmatrix decomposes as a tensor product of multiple smaller kernel matrices. Thisdecomposition immediately enables fast exact sampling. But contrary to what onemay expect, leveraging the Kronecker product structure for speeding up DPPlearning turns out to be more difficult. We overcome this challenge, and derivebatch and stochastic optimization algorithms for efficiently learning theparameters of a KronDPP. version:1
arxiv-1605-08370 | Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent | http://arxiv.org/abs/1605.08370 | author:Chi Jin, Sham M. Kakade, Praneeth Netrapalli category:cs.LG math.OC stat.ML  published:2016-05-26 summary:Matrix completion, where we wish to recover a low rank matrix by observing afew entries from it, is a widely studied problem in both theory and practicewith wide applications. Most of the provable algorithms so far on this problemhave been restricted to the offline setting where they provide an estimate ofthe unknown matrix using all observations simultaneously. However, in manyapplications, the online version, where we observe one entry at a time anddynamically update our estimate, is more appealing. While existing algorithmsare efficient for the offline setting, they could be highly inefficient for theonline setting. In this paper, we propose the first provable, efficient online algorithm formatrix completion. Our algorithm starts from an initial estimate of the matrixand then performs non-convex stochastic gradient descent (SGD). After everyobservation, it performs a fast update involving only one row of two tallmatrices, giving near linear total runtime. Our algorithm can be naturally usedin the offline setting as well, where it gives competitive sample complexityand runtime to state of the art algorithms. Our proofs introduce a generalframework to show that SGD updates tend to stay away from saddle surfaces andcould be of broader interests for other non-convex problems to prove tightrates. version:1
arxiv-1605-08359 | Pairwise Decomposition of Image Sequences for Active Multi-View Recognition | http://arxiv.org/abs/1605.08359 | author:Edward Johns, Stefan Leutenegger, Andrew J. Davison category:cs.CV cs.RO  published:2016-05-26 summary:A multi-view image sequence provides a much richer capacity for objectrecognition than from a single image. However, most existing solutions tomulti-view recognition typically adopt hand-crafted, model-based geometricmethods, which do not readily embrace recent trends in deep learning. Wepropose to bring Convolutional Neural Networks to generic multi-viewrecognition, by decomposing an image sequence into a set of image pairs,classifying each pair independently, and then learning an object classifier byweighting the contribution of each pair. This allows for recognition overarbitrary camera trajectories, without requiring explicit training over thepotentially infinite number of camera paths and lengths. Building thesepairwise relationships then naturally extends to the next-best-view problem inan active recognition framework. To achieve this, we train a secondConvolutional Neural Network to map directly from an observed image to nextviewpoint. Finally, we incorporate this into a trajectory optimisation task,whereby the best recognition confidence is sought for a given trajectorylength. We present state-of-the-art results in both guided and unguidedmulti-view recognition on the ModelNet dataset, and show how our method can beused with depth images, greyscale images, or both. version:1
arxiv-1605-07596 | Local Minimax Complexity of Stochastic Convex Optimization | http://arxiv.org/abs/1605.07596 | author:Yuancheng Zhu, Sabyasachi Chatterjee, John Duchi, John Lafferty category:stat.ML  published:2016-05-24 summary:We extend the traditional worst-case, minimax analysis of stochastic convexoptimization by introducing a localized form of minimax complexity forindividual functions. Our main result gives function-specific lower and upperbounds on the number of stochastic subgradient evaluations needed to optimizeeither the function or its "hardest local alternative" to a given numericalprecision. The bounds are expressed in terms of a localized and computationalanalogue of the modulus of continuity that is central to statistical minimaxanalysis. We show how the computational modulus of continuity can be explicitlycalculated in concrete cases, and relates to the curvature of the function atthe optimum. We also prove a superefficiency result that demonstrates it is ameaningful benchmark, acting as a computational analogue of the Fisherinformation in statistical estimation. The nature and practical implications ofthe results are demonstrated in simulations. version:3
arxiv-1605-08350 | Benign-Malignant Lung Nodule Classification with Geometric and Appearance Histogram Features | http://arxiv.org/abs/1605.08350 | author:Tizita Nesibu Shewaye, Alhayat Ali Mekonnen category:cs.CV  published:2016-05-26 summary:Lung cancer accounts for the highest number of cancer deaths globally. Earlydiagnosis of lung nodules is very important to reduce the mortality rate ofpatients by improving the diagnosis and treatment of lung cancer. This workproposes an automated system to classify lung nodules as malignant and benignin CT images. It presents extensive experimental results using a combination ofgeometric and histogram lung nodule image features and different linear andnon-linear discriminant classifiers. The proposed approach is experimentallyvalidated on the LIDC-IDRI public lung cancer screening thoracic computedtomography (CT) dataset containing nodule level diagnostic data. The obtainedresults are very encouraging correctly classifying 82% of malignant and 93% ofbenign nodules on unseen test data at best. version:1
arxiv-1605-08346 | Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks | http://arxiv.org/abs/1605.08346 | author:Adam Charles, Dong Yin, Christopher Rozell category:cs.IT math.IT stat.ML  published:2016-05-26 summary:Recurrent neural networks (RNNs) have drawn interest from machine learningresearchers because of their effectiveness at preserving past inputs fortime-varying data processing tasks. To understand the success and limitationsof RNNs, it is critical that we advance our analysis of their fundamentalmemory properties. We focus on echo state networks (ESNs), which are RNNs withsimple memoryless nodes and random connectivity. In most existing analyses, theshort-term memory (STM) capacity results conclude that the ESN network sizemust scale linearly with the input size for unstructured inputs. The maincontribution of this paper is to provide general results characterizing the STMcapacity for linear ESNs with multidimensional input streams when the inputshave common low-dimensional structure: sparsity in a basis or significantstatistical dependence between inputs. In both cases, we show that the numberof nodes in the network must scale linearly with the information rate andpoly-logarithmically with the ambient input dimension. The analysis relies onadvanced applications of random matrix theory and results in explicitnon-asymptotic bounds on the recovery error. Taken together, this analysisprovides a significant step forward in our understanding of the STM propertiesin RNNs. version:1
arxiv-1605-07588 | A Consistent Regularization Approach for Structured Prediction | http://arxiv.org/abs/1605.07588 | author:Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco category:cs.LG stat.ML  published:2016-05-24 summary:We propose and analyze a regularization approach for structured predictionproblems. We characterize a large class of loss functions that allows tonaturally embed structured outputs in a linear space. We exploit this fact todesign learning algorithms using a surrogate loss approach and regularizationtechniques. We prove universal consistency and finite sample boundscharacterizing the generalization properties of the proposed methods.Experimental results are provided to demonstrate the practical usefulness ofthe proposed approach. version:2
arxiv-1605-07969 | Adaptive Neural Compilation | http://arxiv.org/abs/1605.07969 | author:Rudy Bunel, Alban Desmaison, Pushmeet Kohli, Philip H. S. Torr, M. Pawan Kumar category:cs.AI cs.LG  published:2016-05-25 summary:This paper proposes an adaptive neural-compilation framework to address theproblem of efficient program learning. Traditional code optimisation strategiesused in compilers are based on applying pre-specified set of transformationsthat make the code faster to execute without changing its semantics. Incontrast, our work involves adapting programs to make them more efficient whileconsidering correctness only on a target input distribution. Our approach isinspired by the recent works on differentiable representations of programs. Weshow that it is possible to compile programs written in a low-level language toa differentiable representation. We also show how programs in thisrepresentation can be optimised to make them efficient on a target distributionof inputs. Experimental results demonstrate that our approach enables learningspecifically-tuned algorithms for given data distributions with a high successrate. version:2
arxiv-1605-08325 | Theano-MPI: a Theano-based Distributed Training Framework | http://arxiv.org/abs/1605.08325 | author:He Ma, Fei Mao, Graham W. Taylor category:cs.LG cs.DC  published:2016-05-26 summary:We develop a scalable and extendable training framework that can utilize GPUsacross nodes in a cluster and accelerate the training of deep learning modelsbased on data parallelism. Both synchronous and asynchronous training areimplemented in our framework, where parameter exchange among GPUs is based onCUDA-aware MPI. In this report, we analyze the convergence and capability ofthe framework to reduce training time when scaling the synchronous training ofAlexNet and GoogLeNet from 2 GPUs to 8 GPUs. In addition, we explore novel waysto reduce the communication overhead caused by exchanging parameters. Finally,we release the framework as open-source for further research on distributeddeep learning version:1
arxiv-1605-08323 | Aerial image geolocalization from recognition and matching of roads and intersections | http://arxiv.org/abs/1605.08323 | author:Dragos Costea, Marius Leordeanu category:cs.CV  published:2016-05-26 summary:Aerial image analysis at a semantic level is important in many applicationswith strong potential impact in industry and consumer use, such as automatedmapping, urban planning, real estate and environment monitoring, or disasterrelief. The problem is enjoying a great interest in computer vision and remotesensing, due to increased computer power and improvement in automated imageunderstanding algorithms. In this paper we address the task of automaticgeolocalization of aerial images from recognition and matching of roads andintersections. Our proposed method is a novel contribution in the literaturethat could enable many applications of aerial image analysis when GPS data isnot available. We offer a complete pipeline for geolocalization, from thedetection of roads and intersections, to the identification of the enclosinggeographic region by matching detected intersections to previously learnedmanually labeled ones, followed by accurate geometric alignment between thedetected roads and the manually labeled maps. We test on a novel dataset withaerial images of two European cities and use the publicly availableOpenStreetMap project for collecting ground truth roads annotations. We show inextensive experiments that our approach produces highly accurate localizationsin the challenging case when we train on images from one city and test on theother and the quality of the aerial images is relatively poor. We also showthat the the alignment between detected roads and pre-stored manual annotationscan be effectively used for improving the quality of the road detectionresults. version:1
arxiv-1605-08313 | A Light-powered, Always-On, Smart Camera with Compressed Domain Gesture Detection | http://arxiv.org/abs/1605.08313 | author:Anvesha A, Shaojie Xu, Ningyuan Cao, Justin Romberg, Arijit Raychowdhury category:cs.CV  published:2016-05-26 summary:In this paper we propose an energy-e?cient camera-based gesture recognitionsystem powered by light energy for \al- ways on" applications. Low energyconsumption is achieved by directly extracting gesture features from thecompressed measurements, which are the block averages and the linearcombinations of the image sensor's pixel values. The ges- tures are recognizedusing a nearest-neighbour (NN) classi- ?er followed by Dynamic Time Warping(DTW). The sys- tem has been implemented on an Analog Devices Black Fin ULPvision processor and powered by PV cells whose output is regulated by TI'sDC-DC buck converter with Maximum Power Point Tracking (MPPT). Measured datareveals that with only 400 compressed measurements (768? compression ratio) perframe, the system is able to recognize key wake- up gestures with greater than80% accuracy and only 95mJ of energy per frame. Owing to its fully self-poweredop- eration, the proposed system can ?nd wide applications in \always-on"vision systems such as in surveillance, robotics and consumer electronics withtouch-less operation. version:1
arxiv-1605-08301 | Predictive Coarse-Graining | http://arxiv.org/abs/1605.08301 | author:Markus Schöberl, Phaedon-Stelios Koutsourelakis, Nicholas Zabaras category:stat.ML  published:2016-05-26 summary:We propose a data-driven, coarse-graining formulation in the context ofequilibrium statistical mechanics. In contrast to existing techniques which arebased on a fine-to-coarse map, we adopt the opposite strategy by prescribing a{\em probabilistic coarse-to-fine} map. This corresponds to a directedprobabilistic model where the coarse variables play the role of latentgenerators of the fine scale (all-atom) data. From an information-theoreticperspective, the framework proposed provides an improvement upon the relativeentropy method that quantifies the uncertainty due to the information loss thatunavoidably takes place during the CG process. Furthermore, it can be readilyextended to a fully Bayesian model where various sources of uncertainties arereflected in the parameters' posterior. The latter can be used to produce notonly point estimates of fine-scale reconstructions or macroscopic observables,but more importantly, predictive posterior distributions on these quantities.These quantify the confidence of the model as a function of the amount of dataand the level of coarse-graining. The issues of model complexity and modelselection are seamlessly addressed by employing a hierarchical prior thatfavors the discovery of sparse solutions, revealing the most prominent featuresin the coarse-grained model. A flexible and parallelizable, Monte Carlo -Expectation-Maximization (MC-EM) scheme is proposed for carrying out inferenceand learning tasks. A comparative assessment of the proposed methodology ispresented for a lattice spin system and the SPC/E water model. version:1
arxiv-1605-08299 | High-Dimensional Trimmed Estimators: A General Framework for Robust Structured Estimation | http://arxiv.org/abs/1605.08299 | author:Eunho Yang, Aurelie Lozano, Aleksandr Aravkin category:stat.ML  published:2016-05-26 summary:We consider the problem of robustifying high-dimensional structuredestimation. Robust techniques are key in real-world applications, as theseoften involve outliers and data corruption. We focus on trimmed versions ofstructurally regularized M-estimators in the high-dimensional setting,including the popular Least Trimmed Squared estimator, as well as analogousestimators for generalized linear models, graphical models, using possiblynon-convex loss functions. We present a general analysis of their statisticalconvergence rates and consistency, and show how to extend any algorithms forM-estimators to fit trimmed variants. We then take a closer look at the$\ell_1$-regularized Least Trimmed Squared estimator as a special case. Ourresults show that this estimator can tolerate a larger fraction of corruptedobservations than state-of-the-art alternatives. The competitive performance ofhigh-dimensional trimmed estimators is illustrated numerically using bothsimulated and real-world genomics data. version:1
arxiv-1605-08285 | Solving Random Systems of Quadratic Equations via Truncated Generalized Gradient Flow | http://arxiv.org/abs/1605.08285 | author:Gang Wang, Georgios B. Giannakis category:stat.ML cs.IT math.IT math.OC  published:2016-05-26 summary:This paper puts forth a novel algorithm, termed \emph{truncated generalizedgradient flow} (TGGF), to solve for $\bm{x}\in\mathbb{R}^n/\mathbb{C}^n$ asystem of $m$ quadratic equations $y_i=\langle\bm{a}_i,\bm{x}\rangle^2$,$i=1,2,\ldots,m$, which even for$\left\{\bm{a}_i\in\mathbb{R}^n/\mathbb{C}^n\right\}_{i=1}^m$ random is knownto be \emph{NP-hard} in general. We prove that as soon as the number ofequations $m$ is on the order of the number of unknowns $n$, TGGF recovers thesolution exactly (up to a global unimodular constant) with high probability andcomplexity growing linearly with the time required to read the data$\left\{\left(\bm{a}_i;\,y_i\right)\right\}_{i=1}^m$. Specifically, TGGFproceeds in two stages: s1) A novel \emph{orthogonality-promoting}initialization that is obtained with simple power iterations; and, s2) arefinement of the initial estimate by successive updates of scalable\emph{truncated generalized gradient iterations}. The former is in sharpcontrast to the existing spectral initializations, while the latter handles therather challenging nonconvex and nonsmooth \emph{amplitude-based} costfunction. Numerical tests demonstrate that: i) The novelorthogonality-promoting initialization method returns more accurate and robustestimates relative to its spectral counterparts; and ii) even with the sameinitialization, our refinement/truncation outperforms Wirtinger-basedalternatives, all corroborating the superior performance of TGGF overstate-of-the-art algorithms. version:1
arxiv-1605-08283 | Discrete Deep Feature Extraction: A Theory and New Architectures | http://arxiv.org/abs/1605.08283 | author:Thomas Wiatowski, Michael Tschannen, Aleksandar Stanić, Philipp Grohs, Helmut Bölcskei category:cs.LG cs.CV cs.IT cs.NE math.IT stat.ML  published:2016-05-26 summary:First steps towards a mathematical theory of deep convolutional neuralnetworks for feature extraction were made---for the continuous-time case---inMallat, 2012, and Wiatowski and B\"olcskei, 2015. This paper considers thediscrete case, introduces new convolutional neural network architectures, andproposes a mathematical framework for their analysis. Specifically, weestablish deformation and translation sensitivity results of local and globalnature, and we investigate how certain structural properties of the inputsignal are reflected in the corresponding feature vectors. Our theory appliesto general filters and general Lipschitz-continuous non-linearities and poolingoperators. Experiments on handwritten digit classification and facial landmarkdetection---including feature importance evaluation---complement thetheoretical findings. version:1
arxiv-1605-07367 | Riemannian stochastic variance reduced gradient on Grassmann manifold | http://arxiv.org/abs/1605.07367 | author:Hiroyuki Kasai, Hiroyuki Sato, Bamdev Mishra category:cs.LG cs.NA math.OC stat.ML  published:2016-05-24 summary:Stochastic variance reduction algorithms have recently become popular forminimizing the average of a large, but finite, number of loss functions. Inthis paper, we propose a novel Riemannian extension of the Euclidean stochasticvariance reduced gradient algorithm (R-SVRG) to a compact manifold searchspace. To this end, we show the developments on the Grassmann manifold. The keychallenges of averaging, addition, and subtraction of multiple gradients areaddressed with notions like logarithm mapping and parallel translation ofvectors on the Grassmann manifold. We present a global convergence analysis ofthe proposed algorithm with decay step-sizes and a local convergence rateanalysis under fixed step-size with some natural assumptions. The proposedalgorithm is applied on a number of problems on the Grassmann manifold likeprincipal components analysis, low-rank matrix completion, and the Karcher meancomputation. In all these cases, the proposed algorithm outperforms thestandard Riemannian stochastic gradient descent algorithm. version:2
arxiv-1605-08257 | Low-rank tensor completion: a Riemannian manifold preconditioning approach | http://arxiv.org/abs/1605.08257 | author:Hiroyuki Kasai, Bamdev Mishra category:cs.LG cs.NA math.OC stat.ML  published:2016-05-26 summary:We propose a novel Riemannian manifold preconditioning approach for thetensor completion problem with rank constraint. A novel Riemannian metric orinner product is proposed that exploits the least-squares structure of the costfunction and takes into account the structured symmetry that exists in Tuckerdecomposition. The specific metric allows to use the versatile framework ofRiemannian optimization on quotient manifolds to develop preconditionednonlinear conjugate gradient and stochastic gradient descent algorithms forbatch and online setups, respectively. Concrete matrix representations ofvarious optimization-related ingredients are listed. Numerical comparisonssuggest that our proposed algorithms robustly outperform state-of-the-artalgorithms across different synthetic and real-world datasets. version:1
arxiv-1605-08254 | Margin Preservation of Deep Neural Networks | http://arxiv.org/abs/1605.08254 | author:Jure Sokolic, Raja Giryes, Guillermo Sapiro, Miguel R. D. Rodrigues category:stat.ML cs.LG cs.NE  published:2016-05-26 summary:The generalization error of deep neural networks via their classificationmargin is studied in this work, providing novel generalization error boundsthat are independent of the network depth, thereby avoiding the commonexponential depth-dependency which is unrealistic for current networks withhundreds of layers. We show that a large margin linear classifier operating atthe output of a deep neural network induces a large classification margin atthe input of the network, provided that the network preserves distances indirections normal to the decision boundary. The distance preservation ischaracterized by the average behaviour of the network's Jacobian matrix in theneighbourhood of the training samples. The introduced theory also leads to amargin preservation regularization scheme that outperforms weight decay boththeoretically and empirically. version:1
arxiv-1605-08247 | cvpaper.challenge in 2015 - A review of CVPR2015 and DeepSurvey | http://arxiv.org/abs/1605.08247 | author:Hirokatsu Kataoka, Yudai Miyashita, Tomoaki Yamabe, Soma Shirakabe, Shin'ichi Sato, Hironori Hoshino, Ryo Kato, Kaori Abe, Takaaki Imanari, Naomichi Kobayashi, Shinichiro Morita, Akio Nakamura category:cs.CV cs.LG cs.MM cs.RO  published:2016-05-26 summary:The "cvpaper.challenge" is a group composed of members from AIST, Tokyo DenkiUniv. (TDU), and Univ. of Tsukuba that aims to systematically summarize paperson computer vision, pattern recognition, and related fields. For thisparticular review, we focused on reading the ALL 602 conference paperspresented at the CVPR2015, the premier annual computer vision event held inJune 2015, in order to grasp the trends in the field. Further, we are proposing"DeepSurvey" as a mechanism embodying the entire process from the readingthrough all the papers, the generation of ideas, and to the writing of paper. version:1
arxiv-1605-08242 | Neighborhood Sensitive Mapping for Zero-Shot Classification using Independently Learned Semantic Embeddings | http://arxiv.org/abs/1605.08242 | author:Gaurav Singh, Fabrizio Silvestri, John Shawe-Taylor category:cs.LG  published:2016-05-26 summary:In a traditional setting, classifiers are trained to approximate a targetfunction $f:X \rightarrow Y$ where at least a sample for each $y \in Y$ ispresented to the training algorithm. In a zero-shot setting we have a subset ofthe labels $\hat{Y} \subset Y$ for which we do not observe any correspondingtraining instance. Still, the function $f$ that we train must be able tocorrectly assign labels also on $\hat{Y}$. In practice, zero-shot problems arevery important especially when the label set is large and the cost ofeditorially label samples for all possible values in the label set might beprohibitively high. Most recent approaches to zero-shot learning are based onfinding and exploiting relationships between labels using semantic embeddings.We show in this paper that semantic embeddings, despite being very good atcapturing relationships between labels, are not very good at capturing therelationships among labels in a data-dependent manner. For this reason, wepropose a novel two-step process for learning a zero-shot classifier. In thefirst step, we learn what we call a \emph{property embedding space} capturingthe "\emph{learnable}" features of the label set. Then, we exploit the learnedproperties in order to reduce the generalization error for a linear nearestneighbor-based classifier. version:1
arxiv-1605-08228 | Predict or classify: The deceptive role of time-locking in brain signal classification | http://arxiv.org/abs/1605.08228 | author:Marco Rusconi, Angelo Valleriani category:q-bio.NC physics.bio-ph stat.ML  published:2016-05-26 summary:Several experimental studies claim to be able to predict the outcome ofsimple decisions from brain signals measured before subjects are aware of theirdecision. Often, these studies use multivariate pattern recognition methodswith the underlying assumption that the ability to classify the brain signal isequivalent to predict the decision itself. Here we show instead that it ispossible to correctly classify a signal even if it does not contain anypredictive information about the decision. We first define a simple stochasticmodel that mimics the random decision process between two equivalentalternatives, and generate a large number of independent trials that contain nochoice-predictive information. The trials are first time-locked to the timepoint of the final event and then classified using standard machine-learningtechniques. The resulting classification accuracy is above chance level longbefore the time point of time-locking. We then analyze the same trials usinginformation theory. We demonstrate that the high classification accuracy is aconsequence of time-locking and that its time behavior is simply related to thelarge relaxation time of the process. We conclude that when time-locking is acrucial step in the analysis of neuronal activity patterns, both the emergenceand the timing of the classification accuracy are affected by structuralproperties of the network that generates the signal. version:1
arxiv-1605-08201 | Towards optimal nonlinearities for sparse recovery using higher-order statistics | http://arxiv.org/abs/1605.08201 | author:Steffen Limmer, Sławomir Stańczak category:cs.IT math.IT stat.ML  published:2016-05-26 summary:We consider machine learning techniques to develop low-latency approximatesolutions to a class of inverse problems. More precisely, we use aprobabilistic approach for the problem of recovering sparse stochastic signalsthat are members of the $\ell_p$-balls. In this context, we analyze theBayesian mean-square-error (MSE) for two types of estimators: (i) a linearestimator and (ii) a structured estimator composed of a linear operatorfollowed by a Cartesian product of univariate nonlinear mappings. Byconstruction, the complexity of the proposed nonlinear estimator is comparableto that of its linear counterpart since the nonlinear mapping can beimplemented efficiently in hardware by means of look-up tables (LUTs). Theproposed structure lends itself to neural networks and iterativeshrinkage/thresholding-type algorithms restricted to a single iterate (e.g. dueto imposed hardware or latency constraints). By resorting to an alternatingminimization technique, we obtain a sequence of optimized linear operators andnonlinear mappings that convergence in the objective. The resulting solution isattractive for real-time applications where general iterative and convexoptimization methods are infeasible. version:1
arxiv-1605-08188 | Learning Multivariate Log-concave Distributions | http://arxiv.org/abs/1605.08188 | author:Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart category:cs.LG cs.IT math.IT math.ST stat.TH  published:2016-05-26 summary:We study the problem of estimating multivariate log-concave probabilitydensity functions. We prove the first sample complexity upper bound forlearning log-concave densities on $\mathbb{R}^d$, for all $d \geq 1$. Prior toour work, no upper bound on the sample complexity of this learning problem wasknown for the case of $d>3$. In more detail, we give an estimator that, for any $d \ge 1$ and$\epsilon>0$, draws $\tilde{O}_d \left( (1/\epsilon)^{(d+5)/2} \right)$ samplesfrom an unknown target log-concave density on $\mathbb{R}^d$, and outputs ahypothesis that (with high probability) is $\epsilon$-close to the target, intotal variation distance. Our upper bound on the sample complexity comes closeto the known lower bound of $\Omega_d \left( (1/\epsilon)^{(d+1)/2} \right)$for this problem. version:1
arxiv-1605-08179 | Discovering Causal Signals in Images | http://arxiv.org/abs/1605.08179 | author:David Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Schölkopf, Léon Bottou category:stat.ML cs.CV  published:2016-05-26 summary:The purpose of this paper is to point out and assay observable causal signalswithin collections of static images. We achieve this goal in two steps. First,we take a learning approach to observational causal inference, and build aclassifier that achieves state-of-the-art performance on finding the causaldirection between pairs of random variables, when given samples from theirjoint distribution. Second, we use our causal direction finder to effectivelydistinguish between features of objects and features of their contexts incollections of static images. Our experiments demonstrate the existence of (1)a relation between the direction of causality and the difference betweenobjects and their contexts, and (2) observable causal signals in collections ofstatic images. version:1
arxiv-1605-08174 | Adiabatic Persistent Contrastive Divergence Learning | http://arxiv.org/abs/1605.08174 | author:Hyeryung Jang, Hyungwon Choi, Yung Yi, Jinwoo Shin category:cs.LG stat.ML  published:2016-05-26 summary:This paper studies the problem of parameter learning in probabilisticgraphical models having latent variables, where the standard approach is theexpectation maximization algorithm alternating expectation (E) and maximization(M) steps. However, both E and M steps are computationally intractable for highdimensional data, while the substitution of one step to a faster surrogate forcombating against intractability can often cause failure in convergence. Wepropose a new learning algorithm which is computationally efficient andprovably ensures convergence to a correct optimum. Its key idea is to run onlya few cycles of Markov Chains (MC) in both E and M steps. Such an idea ofrunning incomplete MC has been well studied only for M step in the literature,called Contrastive Divergence (CD) learning. While such known CD-based schemesfind approximated gradients of the log-likelihood via the mean-field approachin E step, our proposed algorithm does exact ones via MC algorithms in bothsteps due to the multi-time-scale stochastic approximation theory. Despite itstheoretical guarantee in convergence, the proposed scheme might suffer from theslow mixing of MC in E step. To tackle it, we also propose a hybrid approachapplying both mean-field and MC approximation in E step, where the hybridapproach outperforms the bare mean-field CD scheme in our experiments onreal-world datasets. version:1
arxiv-1605-08165 | Highly-Smooth Zero-th Order Online Optimization Vianney Perchet | http://arxiv.org/abs/1605.08165 | author:Francis Bach, Vianney Perchet category:cs.LG math.OC  published:2016-05-26 summary:The minimization of convex functions which are only available through partialand noisy information is a key methodological problem in many disciplines. Inthis paper we consider convex optimization with noisy zero-th orderinformation, that is noisy function evaluations at any desired point. We focuson problems with high degrees of smoothness, such as logistic regression. Weshow that as opposed to gradient-based algorithms, high-order smoothness may beused to improve estimation rates, with a precise dependence of our upper-boundson the degree of smoothness. In particular, we show that for infinitelydifferentiable functions, we recover the same dependence on sample size asgradient-based algorithms, with an extra dimension-dependent factor. This isdone for both convex and strongly-convex functions, with finite horizon andanytime algorithms. Finally, we also recover similar results in the onlineoptimization setting. version:1
arxiv-1605-08163 | Multiple target tracking based on sets of trajectories | http://arxiv.org/abs/1605.08163 | author:Ángel F. García-Fernández, Lennart Svensson, Mark R. Morelande category:cs.CV cs.SY  published:2016-05-26 summary:This paper proposes the set of target trajectories as the state variable formultiple target tracking. The main objective of multiple target tracking is toestimate an unknown number of target trajectories given a sequence ofmeasurements. This quantity of interest is perfectly represented as a set oftrajectories without the need of arbitrary parameters such as labels orordering. We use finite-set statistics to pose this problem in the Bayesianframework and formulate a state space model where the random state is a randomfinite set that contains trajectories. All information of interest is thuscontained in the multitrajectory filtering probability density function (PDF),which represents the multitrajectory PDF of the set of trajectories given themeasurements. For the standard measurement and dynamic models, we describe afamily of PDFs that is conjugate in the sense that the multitrajectoryfiltering PDF remains within that family during both the prediction and updatesteps. version:1
arxiv-1605-08154 | A single scale retinex based method for palm vein extraction | http://arxiv.org/abs/1605.08154 | author:Chongyang Wang, Ming Peng, Lingfeng Xu, Tong Chen category:cs.CV  published:2016-05-26 summary:Palm vein recognition is a novel biometric identification technology. But howto gain a better vein extraction result from the raw palm image is still achallenging problem, especially when the raw data collection has the problem ofasymmetric illumination. This paper proposes a method based on single scaleRetinex algorithm to extract palm vein image when strong shadow presents due toasymmetric illumination and uneven geometry of the palm. We test our method ona multispectral palm image. The experimental result shows that the proposedmethod is robust to the influence of illumination angle and shadow. Compared tothe traditional extraction methods, the proposed method can obtain palm veinlines with better visualization performance (the contrast ratio increases by18.4%, entropy increases by 1.07%, and definition increases by 18.8%). version:1
arxiv-1605-08153 | DeepMovie: Using Optical Flow and Deep Neural Networks to Stylize Movies | http://arxiv.org/abs/1605.08153 | author:Alexander G. Anderson, Cory P. Berg, Daniel P. Mossing, Bruno A. Olshausen category:cs.CV cs.NE  published:2016-05-26 summary:A recent paper by Gatys et al. describes a method for rendering an image inthe style of another image. First, they use convolutional neural networkfeatures to build a statistical model for the style of an image. Then theycreate a new image with the content of one image but the style statistics ofanother image. Here, we extend this method to render a movie in a givenartistic style. The naive solution that independently renders each frameproduces poor results because the features of the style move substantially fromone frame to the next. The other naive method that initializes the optimizationfor the next frame using the rendered version of the previous frame alsoproduces poor results because the features of the texture stay fixed relativeto the frame of the movie instead of moving with objects in the scene. The maincontribution of this paper is to use optical flow to initialize the styletransfer optimization so that the texture features move with the objects in thevideo. Finally, we suggest a method to incorporate optical flow explicitly intothe cost function. version:1
arxiv-1605-08151 | Predicting Visual Exemplars of Unseen Classes for Zero-Shot Learning | http://arxiv.org/abs/1605.08151 | author:Soravit Changpinyo, Wei-Lun Chao, Fei Sha category:cs.CV  published:2016-05-26 summary:Leveraging class semantic descriptions and examples of known objects,zero-shot learning makes it possible to train a recognition model for an objectclass whose examples are not available. In this paper, we propose a novelzero-shot learning model that takes advantage of clustering structures in thesemantic embedding space. The key idea is to impose the structural constraintthat semantic representations must be predictive of the locations of itscorresponding visual exemplars. To this end, this reduces to training multiplekernel-based regressors from semantic representation-exemplar pairs fromlabeled data of the seen object categories. Despite its simplicity, ourapproach significantly outperforms existing zero-shot learning methods in threeout of four benchmark datasets, including the ImageNet dataset with more than20,000 unseen categories. version:1
arxiv-1605-08140 | Temporal attention filters for human activity recognition in videos | http://arxiv.org/abs/1605.08140 | author:AJ Piergiovanni, Chenyou Fan, Michael S. Ryoo category:cs.CV  published:2016-05-26 summary:In this paper, we newly introduce the concept of temporal attention filters,and describe how they can be used for human activity recognition from videos.Many high-level activities are often composed of multiple temporal parts (e.g.,sub-events) with different duration/speed, and our objective is to make themodel explicitly consider such temporal structure using multiple temporalfilters. Our attention filters are designed to be fully differentiable,allowing end-of-end training of the temporal filters together with theunderlying frame-based or segment-based convolutional neural networkarchitectures. The paper not only presents an approach of learning optimalstatic temporal attention filters to be shared across different videos, butalso describes an approach of dynamically adjusting attention filters pertesting video using recurrent long short-term memory networks (LSTMs). Weexperimentally confirm that the proposed concept of temporal attention filtersbenefits the activity recognition tasks by capturing the temporal structure invideos. version:1
arxiv-1605-08754 | Faster Eigenvector Computation via Shift-and-Invert Preconditioning | http://arxiv.org/abs/1605.08754 | author:Dan Garber, Elad Hazan, Chi Jin, Sham M. Kakade, Cameron Musco, Praneeth Netrapalli, Aaron Sidford category:cs.DS cs.LG math.NA math.OC  published:2016-05-26 summary:We give faster algorithms and improved sample complexities for estimating thetop eigenvector of a matrix $\Sigma$ -- i.e. computing a unit vector $x$ suchthat $x^T \Sigma x \ge (1-\epsilon)\lambda_1(\Sigma)$: Offline Eigenvector Estimation: Given an explicit $A \in \mathbb{R}^{n \timesd}$ with $\Sigma = A^TA$, we show how to compute an $\epsilon$ approximate topeigenvector in time $\tilde O([nnz(A) + \frac{d*sr(A)}{gap^2} ]* \log1/\epsilon )$ and $\tilde O([\frac{nnz(A)^{3/4} (d*sr(A))^{1/4}}{\sqrt{gap}} ]* \log 1/\epsilon )$. Here $nnz(A)$ is the number of nonzeros in $A$, $sr(A)$is the stable rank, $gap$ is the relative eigengap. By separating the $gap$dependence from the $nnz(A)$ term, our first runtime improves upon theclassical power and Lanczos methods. It also improves prior work using fastsubspace embeddings [AC09, CW13] and stochastic optimization [Sha15c], givingsignificantly better dependencies on $sr(A)$ and $\epsilon$. Our second runningtime improves these further when $nnz(A) \le \frac{d*sr(A)}{gap^2}$. Online Eigenvector Estimation: Given a distribution $D$ with covariancematrix $\Sigma$ and a vector $x_0$ which is an $O(gap)$ approximate topeigenvector for $\Sigma$, we show how to refine to an $\epsilon$ approximationusing $ O(\frac{var(D)}{gap*\epsilon})$ samples from $D$. Here $var(D)$ is anatural notion of variance. Combining our algorithm with previous work toinitialize $x_0$, we obtain improved sample complexity and runtime resultsunder a variety of assumptions on $D$. We achieve our results using a general framework that we believe is ofindependent interest. We give a robust analysis of the classic method ofshift-and-invert preconditioning to reduce eigenvector computation toapproximately solving a sequence of linear systems. We then apply faststochastic variance reduced gradient (SVRG) based system solvers to achieve ourclaims. version:1
arxiv-1605-08125 | Automatic Action Annotation in Weakly Labeled Videos | http://arxiv.org/abs/1605.08125 | author:Waqas Sultani, Mubarak Shah category:cs.CV  published:2016-05-26 summary:Manual spatio-temporal annotation of human action in videos is laborious,requires several annotators and contains human biases. In this paper, wepresent a weakly supervised approach to automatically obtain spatio-temporalannotations of an actor in action videos. We first obtain a large number ofaction proposals in each video. To capture a few most representative actionproposals in each video and evade processing thousands of them, we rank themusing optical flow and saliency in a 3D-MRF based framework and select a fewproposals using MAP based proposal subset selection method. We demonstrate thatthis ranking preserves the high quality action proposals. Several suchproposals are generated for each video of the same action. Our next challengeis to iteratively select one proposal from each video so that all proposals areglobally consistent. We formulate this as Generalized Maximum Clique Graphproblem using shape, global and fine grained similarity of proposals across thevideos. The output of our method is the most action representative proposalsfrom each video. Our method can also annotate multiple instances of the sameaction in a video. We have validated our approach on three challenging actiondatasets: UCF Sport, sub-JHMDB and THUMOS'13 and have obtained promisingresults compared to several baseline methods. Moreover, on UCF Sports, wedemonstrate that action classifiers trained on these automatically obtainedspatio-temporal annotations have comparable performance to the classifierstrained on ground truth annotation. version:1
arxiv-1605-07912 | Encode, Review, and Decode: Reviewer Module for Caption Generation | http://arxiv.org/abs/1605.07912 | author:Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, William W. Cohen category:cs.LG cs.CL cs.CV  published:2016-05-25 summary:We propose a novel module, the reviewer module, to improve theencoder-decoder learning framework. The reviewer module is generic, and can beplugged into an existing encoder-decoder model. The reviewer module performs anumber of review steps with attention mechanism on the encoder hidden states,and outputs a fact vector after each review step; the fact vectors are used asthe input of the attention mechanism in the decoder. We show that theconventional encoder-decoders are a special case of our framework. Empirically,we show that our framework can improve over state-of-the-art encoder-decodersystems on the tasks of image captioning and source code captioning. version:2
arxiv-1605-08110 | Video Summarization with Long Short-term Memory | http://arxiv.org/abs/1605.08110 | author:Ke Zhang, Wei-Lun Chao, Fei Sha, Kristen Grauman category:cs.CV cs.LG  published:2016-05-26 summary:We propose a novel supervised learning technique for summarizing videos byautomatically selecting keyframes or key subshots. Casting the problem as astructured prediction problem on sequential data, our main idea is to use LongShort-Term Memory (LSTM), a special type of recurrent neural networks to modelthe variable-range dependencies entailed in the task of video summarization.Our learning models attain the state-of-the-art results on two benchmark videodatasets. Detailed analysis justifies the design of the models. In particular,we show that it is crucial to take into consideration the sequential structuresin videos and model them. Besides advances in modeling techniques, we introducetechniques to address the need of a large number of annotated data for trainingcomplex learning models. There, our main idea is to exploit the existence ofauxiliary annotated video datasets, albeit heterogeneous in visual styles andcontents. Specifically, we show domain adaptation techniques can improvesummarization by reducing the discrepancies in statistical properties acrossthose datasets. version:1
arxiv-1605-07696 | Exact Exponent in Optimal Rates for Crowdsourcing | http://arxiv.org/abs/1605.07696 | author:Chao Gao, Yu Lu, Dengyong Zhou category:stat.ML math.ST stat.TH  published:2016-05-25 summary:In many machine learning applications, crowdsourcing has become the primarymeans for label collection. In this paper, we study the optimal error rate foraggregating labels provided by a set of non-expert workers. Under the classicDawid-Skene model, we establish matching upper and lower bounds with an exactexponent $mI(\pi)$ in which $m$ is the number of workers and $I(\pi)$ theaverage Chernoff information that characterizes the workers' collectiveability. Such an exact characterization of the error exponent allows us tostate a precise sample size requirement$m>\frac{1}{I(\pi)}\log\frac{1}{\epsilon}$ in order to achieve an $\epsilon$misclassification error. In addition, our results imply the optimality ofvarious EM algorithms for crowdsourcing initialized by consistent estimators. version:2
arxiv-1605-08108 | FLAG: Fast Linearly-Coupled Adaptive Gradient Method | http://arxiv.org/abs/1605.08108 | author:Xiang Cheng, Farbod Roosta-Khorasani, Peter L. Bartlett, Michael W. Mahoney category:math.OC cs.LG stat.ML  published:2016-05-26 summary:The celebrated Nesterov's accelerated gradient method offers great speed-upscompared to the classical gradient descend method as it attains the optimalfirst-order oracle complexity for smooth convex optimization. On the otherhand, the popular AdaGrad algorithm competes with mirror descent under the bestregularizer by adaptively scaling the gradient. Recently, it has been shownthat the accelerated gradient descent can be viewed as a linear combination ofgradient descent and mirror descent steps. Here, we draw upon these ideas andpresent a fast linearly-coupled adaptive gradient method (FLAG) as anaccelerated version of AdaGrad, and show that our algorithm can indeed offerthe best of both worlds. Like Nesterov's accelerated algorithm and its proximalvariant, FISTA, our method has a convergence rate of $1/T^2$ after $T$iterations. Like AdaGrad our method adaptively chooses a regularizer, in a waythat performs almost as well as the best choice of regularizer in hindsight. version:1
arxiv-1605-08104 | Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning | http://arxiv.org/abs/1605.08104 | author:William Lotter, Gabriel Kreiman, David Cox category:cs.LG cs.AI cs.CV cs.NE q-bio.NC  published:2016-05-25 summary:While great strides have been made in using deep learning algorithms to solvesupervised learning tasks, the problem of unsupervised learning - leveragingunlabeled examples to learn about the structure of a domain - remains adifficult unsolved challenge. Here, we explore prediction of future frames in avideo sequence as an unsupervised learning rule for learning about thestructure of the visual world. We describe a predictive neural network("PredNet") architecture that is inspired by the concept of "predictive coding"from the neuroscience literature. These networks learn to predict future framesin a video sequence, with each layer in the network making local predictionsand only forwarding deviations from those predictions to subsequent networklayers. We show that these networks are able to robustly learn to predict themovement of synthetic (rendered) objects, and that in doing so, the networkslearn internal representations that are useful for decoding latent objectparameters (e.g. pose) that support object recognition with fewer trainingviews. We also show that these networks can scale to complex natural imagestreams (car-mounted camera videos), capturing key aspects of both egocentricmovement and the movement of objects in the visual scene, and generalizingacross video datasets. These results suggest that prediction represents apowerful framework for unsupervised learning, allowing for implicit learning ofobject and scene structure. version:1
arxiv-1605-08068 | Real-Time Human Motion Capture with Multiple Depth Cameras | http://arxiv.org/abs/1605.08068 | author:Alireza Shafaei, James J. Little category:cs.CV  published:2016-05-25 summary:Commonly used human motion capture systems require intrusive attachment ofmarkers that are visually tracked with multiple cameras. In this work wepresent an efficient and inexpensive solution to markerless motion captureusing only a few Kinect sensors. Unlike the previous work on 3d pose estimationusing a single depth camera, we relax constraints on the camera location and donot assume a co-operative user. We apply recent image segmentation techniquesto depth images and use curriculum learning to train our system on purelysynthetic data. Our method accurately localizes body parts without requiring anexplicit shape model. The body joint locations are then recovered by combiningevidence from multiple views in real-time. We also introduce a dataset of ~6million synthetic depth frames for pose estimation from multiple cameras andexceed state-of-the-art results on the Berkeley MHAD dataset. version:1
arxiv-1605-08062 | A PAC RL Algorithm for Episodic POMDPs | http://arxiv.org/abs/1605.08062 | author:Zhaohan Daniel Guo, Shayan Doroudi, Emma Brunskill category:cs.LG cs.AI stat.ML  published:2016-05-25 summary:Many interesting real world domains involve reinforcement learning (RL) inpartially observable environments. Efficient learning in such domains isimportant, but existing sample complexity bounds for partially observable RLare at least exponential in the episode length. We give, to our knowledge, thefirst partially observable RL algorithm with a polynomial bound on the numberof episodes on which the algorithm may not achieve near-optimal performance.Our algorithm is suitable for an important class of episodic POMDPs. Ourapproach builds on recent advances in method of moments for latent variablemodel estimation. version:1
arxiv-1605-07999 | Toward a general, scaleable framework for Bayesian teaching with applications to topic models | http://arxiv.org/abs/1605.07999 | author:Baxter S. Eaves Jr, Patrick Shafto category:cs.AI cs.LG  published:2016-05-25 summary:Machines, not humans, are the world's dominant knowledge accumulators buthumans remain the dominant decision makers. Interpreting and disseminating theknowledge accumulated by machines requires expertise, time, and is prone tofailure. The problem of how best to convey accumulated knowledge from computersto humans is a critical bottleneck in the broader application of machinelearning. We propose an approach based on human teaching where the problem isformalized as selecting a small subset of the data that will, with highprobability, lead the human user to the correct inference. This approach,though successful for modeling human learning in simple laboratory experiments,has failed to achieve broader relevance due to challenges in formulatinggeneral and scalable algorithms. We propose general-purpose teaching viapseudo-marginal sampling and demonstrate the algorithm by teaching topicmodels. Simulation results show our sampling-based approach: effectivelyapproximates the probability where ground-truth is possible via enumeration,results in data that are markedly different from those expected by randomsampling, and speeds learning especially for small amounts of data. Applicationto movie synopsis data illustrates differences between teaching and randomsampling for teaching distributions and specific topics, and demonstrates gainsin scalability and applicability to real-world problems. version:1
arxiv-1605-07991 | Efficient Distributed Learning with Sparsity | http://arxiv.org/abs/1605.07991 | author:Jialei Wang, Mladen Kolar, Nathan Srebro, Tong Zhang category:stat.ML cs.LG  published:2016-05-25 summary:We propose a novel, efficient approach for distributed sparse learning inhigh-dimensions, where observations are randomly partitioned across machines.Computationally, at each round our method only requires the master machine tosolve a shifted ell_1 regularized M-estimation problem, and other workers tocompute the gradient. In respect of communication, the proposed approachprovably matches the estimation error bound of centralized methods withinconstant rounds of communications (ignoring logarithmic factors). We conductextensive experiments on both simulated and real world datasets, anddemonstrate encouraging performances on high-dimensional regression andclassification tasks. version:1
arxiv-1605-07960 | Multi-Object Tracking and Identification over Sets | http://arxiv.org/abs/1605.07960 | author:Aijun Bai category:cs.CV cs.RO  published:2016-05-25 summary:The ability for an autonomous agent or robot to track and identifypotentially multiple objects in a dynamic environment is essential for manyapplications, such as automated surveillance, traffic monitoring, human-robotinteraction, etc. The main challenge is due to the noisy and incompleteperception including inevitable false negative and false positive errors from alow-level detector. In this paper, we propose a novel multi-object tracking andidentification over sets approach to address this challenge. We define jointstates and observations both as finite sets, and develop motion and observationfunctions accordingly. The object identification problem is then formulated andsolved by using expectation-maximization methods. The set formulation enablesus to avoid directly performing observation-to-object association. Weempirically confirm that the overall algorithm outperforms the state-of-the-artin a popular PETS dataset. version:1
arxiv-1605-07950 | A First Order Free Lunch for SQRT-Lasso | http://arxiv.org/abs/1605.07950 | author:Xingguo Li, Jarvis Haupt, Raman Arora, Han Liu, Mingyi Hong, Tuo Zhao category:cs.LG math.OC stat.ML  published:2016-05-25 summary:Many statistical machine learning techniques sacrifice convenientcomputational structures to gain estimation robustness and modelingflexibility. In this paper, we study this fundamental tradeoff through aSQRT-Lasso problem for sparse linear regression and sparse precision matrixestimation in high dimensions. We explain how novel optimization techniqueshelp address these computational challenges. Particularly, we propose apathwise iterative smoothing shrinkage thresholding algorithm for solving theSQRT-Lasso optimization problem. We further provide a novel model-basedperspective for analyzing the smoothing optimization framework, which allows usto establish a nearly linear convergence (R-linear convergence) guarantee forour proposed algorithm. This implies that solving the SQRT-Lasso optimizationis almost as easy as solving the Lasso optimization. Moreover, we show that ourproposed algorithm can also be applied to sparse precision matrix estimation,and enjoys good computational properties. Numerical experiments are provided tosupport our theory. version:1
arxiv-1605-07162 | Pure Exploration of Multi-armed Bandit Under Matroid Constraints | http://arxiv.org/abs/1605.07162 | author:Lijie Chen, Anupam Gupta, Jian Li category:cs.LG cs.DS  published:2016-05-23 summary:We study the pure exploration problem subject to a matroid constraint(Best-Basis) in a stochastic multi-armed bandit game. In a Best-Basis instance,we are given $n$ stochastic arms with unknown reward distributions, as well asa matroid $\mathcal{M}$ over the arms. Let the weight of an arm be the mean ofits reward distribution. Our goal is to identify a basis of $\mathcal{M}$ withthe maximum total weight, using as few samples as possible. The problem is a significant generalization of the best arm identificationproblem and the top-$k$ arm identification problem, which have attractedsignificant attentions in recent years. We study both the exact and PACversions of Best-Basis, and provide algorithms with nearly-optimal samplecomplexities for these versions. Our results generalize and/or improve onseveral previous results for the top-$k$ arm identification problem and thecombinatorial pure exploration problem when the combinatorial constraint is amatroid. version:3
arxiv-1605-07918 | Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks with Feedback Negative Sampling | http://arxiv.org/abs/1605.07918 | author:Byungsoo Kim, Hwanjo Yu, Gary Geunbae Lee category:cs.CL cs.AI cs.NE  published:2016-05-25 summary:Previous studies in Open Information Extraction (Open IE) are mainly based onextraction patterns. They manually define patterns or automatically learn themfrom a large corpus. However, these approaches are limited when grasping thecontext of a sentence, and they fail to capture implicit relations. In thispaper, we address this problem with the following methods. First, we exploitlong short-term memory (LSTM) networks to extract higher-level features alongthe shortest dependency paths, connecting headwords of relations and arguments.The path-level features from LSTM networks provide useful clues regardingcontextual information and the validity of arguments. Second, we constructedsamples to train LSTM networks without the need for manual labeling. Inparticular, feedback negative sampling picks highly negative samples amongnon-positive samples through a model trained with positive samples. Theexperimental results show that our approach produces more precise and abundantextractions than state-of-the-art open IE systems. To the best of ourknowledge, this is the first work to apply deep learning to Open IE. version:1
arxiv-1605-07906 | How priors of initial hyperparameters affect Gaussian process regression models | http://arxiv.org/abs/1605.07906 | author:Zexun Chen, Bo Wang category:stat.ML  published:2016-05-25 summary:Gaussian Process Regression (GPR) is a kernel-based nonparametric method andhas been proved to be effective and powerful. Its performance, however, relieson appropriate selection of kernel and the involving hyperparameters. Thehyperparameters for a specified kernel are often estimated from the data viathe maximum marginal likelihood. Unfortunately, the marginal likelihoodfunctions are not usually convex with respect to the hyperparameters, thereforethe optimization may not converge to global maxima. A common approach to tacklethis issue is to use multiple starting points randomly selected from a specificprior distribution. Therefore, the choice of prior distribution may play avital rule in the usefulness of this approach. In this paper, we study thesensitivity of prior distributions to the hyperparameter estimation and theperformance of GPR. We consider different types of priors, including vague anddata-dominated, for the initial values of hyperparameters for some commonlyused kernels and investigate the influence of the priors on the performance ofGPR models. The results show that the sensitivity of the hyperparameterestimation depends on the choice of kernels, but the priors have littleinfluence on the performance of the GPR models in terms of predictability. version:1
arxiv-1605-07895 | Automatic Extraction of Causal Relations from Natural Language Texts: A Comprehensive Survey | http://arxiv.org/abs/1605.07895 | author:Nabiha Asghar category:cs.AI cs.CL cs.IR  published:2016-05-25 summary:Automatic extraction of cause-effect relationships from natural languagetexts is a challenging open problem in Artificial Intelligence. Most of theearly attempts at its solution used manually constructed linguistic andsyntactic rules on small and domain-specific data sets. However, with theadvent of big data, the availability of affordable computing power and therecent popularization of machine learning, the paradigm to tackle this problemhas slowly shifted. Machines are now expected to learn generic causalextraction rules from labelled data with minimal supervision, in a domainindependent-manner. In this paper, we provide a comprehensive survey of causalrelation extraction techniques from both paradigms, and analyse their relativestrengths and weaknesses, with recommendations for future work. version:1
arxiv-1605-07891 | Query Expansion with Locally-Trained Word Embeddings | http://arxiv.org/abs/1605.07891 | author:Fernando Diaz, Bhaskar Mitra, Nick Craswell category:cs.IR cs.CL  published:2016-05-25 summary:Continuous space word embeddings have received a great deal of attention inthe natural language processing and machine learning communities for theirability to model term similarity and other relationships. We study the use ofterm relatedness in the context of query expansion for information retrieval.We demonstrate that word embeddings such as word2vec and GloVe, when trainedglobally, underperform corpus and query specific embeddings for retrievaltasks. These results suggest that other tasks benefiting from global embeddingsmay also benefit from local embeddings. version:1
arxiv-1605-07874 | BattRAE: Bidimensional Attention-Based Recursive Autoencoders for Learning Bilingual Phrase Embeddings | http://arxiv.org/abs/1605.07874 | author:Biao Zhang, Deyi Xiong, Jinsong Su category:cs.CL  published:2016-05-25 summary:In this paper, we propose a bidimensional attention based recursiveautoencoder (BattRAE) to integrate cues and source-target interactions atmultiple levels of granularity into bilingual phrase representations. We employrecursive autoencoders to generate tree structures of phrase with embeddings atdifferent levels of granularity (e.g., words, sub-phrases, phrases). Over theseembeddings on the source and target side, we introduce a bidimensionalattention network to learn their interactions encoded in a bidimensionalattention matrix, from which we extract two soft attention weight distributionssimultaneously. The weight distributions enable BattRAE to generate compositivephrase representations via convolution. Based on the learned phraserepresentations, we further use a bilinear neural model, trained via amax-margin method, to measure bilingual semantic similarity. In order toevaluate the effectiveness of BattRAE, we incorporate this semantic similarityas an additional feature into a state-of-the-art SMT system. Extensiveexperiments on NIST Chinese-English test sets show that our model achieves asubstantial improvement of up to 1.82 BLEU points over the baseline. version:1
arxiv-1605-07870 | Simultaneous Sparse Dictionary Learning and Pruning | http://arxiv.org/abs/1605.07870 | author:Simeng Qu, Xiao Wang category:stat.ML  published:2016-05-25 summary:Dictionary learning is a cutting-edge area in imaging processing, that hasrecently led to state-of-the-art results in many signal processing tasks. Theidea is to conduct a linear decomposition of a signal using a few atoms of alearned and usually over-completed dictionary instead of a pre-defined basis.Determining a proper size of the to-be-learned dictionary is crucial for bothprecision and efficiency of the process, while most of the existing dictionarylearning algorithms choose the size quite arbitrarily. In this paper, a novelregularization method called the Grouped Smoothly Clipped Absolute Deviation(GSCAD) is employed for learning the dictionary. The proposed method cansimultaneously learn a sparse dictionary and select the appropriate dictionarysize. Efficient algorithm is designed based on the alternative direction methodof multipliers (ADMM) which decomposes the joint non-convex problem with thenon-convex penalty into two convex optimization problems. Several examples arepresented for image denoising and the experimental results are compared withother state-of-the-art approaches. version:1
arxiv-1605-07869 | Variational Neural Machine Translation | http://arxiv.org/abs/1605.07869 | author:Biao Zhang, Deyi Xiong, Jinsong Su category:cs.CL  published:2016-05-25 summary:Models of neural machine translation are often from a discriminative familyof encoder-decoders that learn a conditional distribution of a target sentencegiven a source sentence. In this paper, we propose a variational model to learnthis conditional distribution for neural machine translation: a variationalencoder-decoder model that can be trained end-to-end. Different from thevanilla encoder-decoder model that generates target translations from hiddenrepresentations of source sentences alone, the variational model introduces acontinuous latent variable to explicitly model underlying semantics of sourcesentences and to guide the generation of target translations. In order toperform an efficient posterior inference, we build a neural posteriorapproximator that is conditioned only on the source side. Additionally, weemploy a reparameterization technique to estimate the variational lower boundso as to enable standard stochastic gradient optimization and large-scaletraining for the variational model. Experiments on NIST Chinese-Englishtranslation tasks show that the proposed variational neural machine translationachieves significant improvements over both state-of-the-art statistical andneural machine translation baselines. version:1
arxiv-1605-07866 | DeepCut: Object Segmentation from Bounding Box Annotations using Convolutional Neural Networks | http://arxiv.org/abs/1605.07866 | author:Martin Rajchl, Matthew C. H. Lee, Ozan Oktay, Konstantinos Kamnitsas, Jonathan Passerat-Palmbach, Wenjia Bai, Bernhard Kainz, Daniel Rueckert category:cs.CV  published:2016-05-25 summary:In this paper, we propose DeepCut, a method to obtain pixelwise objectsegmentations given an image dataset labelled with bounding box annotations. Itextends the approach of the well-known GrabCut method to include machinelearning by training a neural network classifier from bounding box annotations.We formulate the problem as an energy minimisation problem over adensely-connected conditional random field and iteratively update the trainingtargets to obtain pixelwise object segmentations. Additionally, we proposevariants of the DeepCut method and compare those to a naive approach to CNNtraining under weak supervision. We test its applicability to solve brain andlung segmentation problems on a challenging fetal magnetic resonance datasetand obtain encouraging results in terms of accuracy. version:1
arxiv-1605-07852 | Unsupervised Formation Matching in Highly Inflected Languages | http://arxiv.org/abs/1605.07852 | author:Javid Dadashkarimi, Hossein Nasr Esfahani, Heshaam Faili, Azadeh Shakery category:cs.IR cs.CL  published:2016-05-25 summary:There have been multiple attempts to resolve the various formation matchingproblem in information retrieval. Stemming is a common strategy to this end.Among many approaches for stemming, statistical stemming has shown to beeffective in a few number of languages, particularly those highly inflectedones. Common statistical approaches heavily relies on string similarity interms of prefix and suffix matching. In this paper we propose a method that isable to find any popular affix in any position of a word; specifically findinginfixes is a required task in Persian, Arabic, and Malay particularly inirregular formations and is a future challenge in informal texts. The proposedmethod aims at finding a number of transformation rules for expanding a wordwith its inflectional/derivation formations. Our experimental results on CLEF2008 and CLEF 2009 English-Persian CLIR tasks indicate that the proposed methodsignificantly outperforms all the baselines in terms of MAP. version:1
arxiv-1605-07844 | Dimension Projection among Languages based on Pseudo-relevant Documents for Query Translation | http://arxiv.org/abs/1605.07844 | author:Javid Dadashkarimi, Mahsa S. Shahshahani, Amirhossein Tebbifakhr, Heshaam Faili, Azadeh Shakery category:cs.IR cs.AI cs.CL  published:2016-05-25 summary:Taking advantage of top-ranked documents in response to a query for improvingquality of query translation has been shown to be an effective approach forcross-language information retrieval. In this paper, we propose a new methodfor query translation based on dimension projection of embedded vectors fromthe pseudo-relevant documents in the source language to their equivalents inthe target language. To this end, first we learn low-dimensionalrepresentations of the words in the pseudo-relevant collections separately andthen aim at finding a query-dependent transformation matrix between the vectorsof translation pairs. At the next step, representation of each query term isprojected to the target language and then, after using a softmax function, aquery-dependent translation model is built. Finally, the model is used forquery translation. Our experiments on four CLEF collections in French, Spanish,German, and Persian demonstrate that the proposed method outperforms allcompetitive baselines in language modelling, particularly when it is combinedwith a collection-dependent translation model. version:1
arxiv-1605-07843 | Unsupervised Word and Dependency Path Embeddings for Aspect Term Extraction | http://arxiv.org/abs/1605.07843 | author:Yichun Yin, Furu Wei, Li Dong, Kaimeng Xu, Ming Zhang, Ming Zhou category:cs.CL  published:2016-05-25 summary:In this paper, we develop a novel approach to aspect term extraction based onunsupervised learning of distributed representations of words and dependencypaths. The basic idea is to connect two words (w1 and w2) with the dependencypath (r) between them in the embedding space. Specifically, our methodoptimizes the objective w1 + r = w2 in the low-dimensional space, where themulti-hop dependency paths are treated as a sequence of grammatical relationsand modeled by a recurrent neural network. Then, we design the embeddingfeatures that consider linear context and dependency context information, forthe conditional random field (CRF) based aspect term extraction. Experimentalresults on the SemEval datasets show that, (1) with only embedding features, wecan achieve state-of-the-art results; (2) our embedding method whichincorporates the syntactic information among words yields better performancethan other representative ones in aspect term extraction. version:1
arxiv-1605-07833 | Effective Blind Source Separation Based on the Adam Algorithm | http://arxiv.org/abs/1605.07833 | author:Michele Scarpiniti, Simone Scardapane, Danilo Comminiello, Raffaele Parisi, Aurelio Uncini category:cs.LG  published:2016-05-25 summary:In this paper, we derive a modified InfoMax algorithm for the solution ofBlind Signal Separation (BSS) problems by using advanced stochastic methods.The proposed approach is based on a novel stochastic optimization approachknown as the Adaptive Moment Estimation (Adam) algorithm. The proposed BSSsolution can benefit from the excellent properties of the Adam approach. Inorder to derive the new learning rule, the Adam algorithm is introduced in thederivation of the cost function maximization in the standard InfoMax algorithm.The natural gradient adaptation is also considered. Finally, some experimentalresults show the effectiveness of the proposed approach. version:1
arxiv-1605-07826 | Asymptotically exact conditional inference in deep generative models and differentiable simulators | http://arxiv.org/abs/1605.07826 | author:Matthew M. Graham, Amos Storkey category:stat.CO stat.ML  published:2016-05-25 summary:Many generative models can be expressed as a deterministic differentiablefunction $\mathrm{\mathbf{g}}(\boldsymbol{u})$ of random variables$\boldsymbol{u}$ drawn from some simple base density. This framework includesboth deep generative architectures such as Variational Autoencoders andGenerative Adversial Nets, and a large class of dynamical system simulators. Wepresent a method for performing efficient MCMC inference in such models underconditioning constraints on the model output. For some models this offers anasymptotically exact inference method where Approximate Bayesian Computationmight otherwise be employed. We use the intuition that conditional inferencecorresponds to integrating the base density across a manifold corresponding tothe set of $\boldsymbol{u}$ consistent with the conditioning. This motivatesthe use of a constrained variant of Hamiltonian Monte Carlo which leverages thesmooth geometry of the manifold to coherently move between states satisfyingthe constraint. We validate the method by performing inference tasks in adiverse set of models: parameter inference in a dynamical predator-preysimulation, joint 3D pose and camera model inference from a 2D projection andimage in-painting with a generative model of MNIST digit images. version:1
arxiv-1605-07824 | Action Classification via Concepts and Attributes | http://arxiv.org/abs/1605.07824 | author:Amir Rosenfeld, Shimon Ullman category:cs.CV cs.LG  published:2016-05-25 summary:Classes in natural images tend to follow long tail distributions. This isproblematic when there are insufficient training examples for rare classes.This effect is emphasized in compound classes, involving the conjunction ofseveral concepts, such as those appearing in action-recognition datasets. Inthis paper, we propose to address this issue by learning how to utilize commonvisual concepts which are readily available. We detect the presence ofprominent concepts in images and use them to infer the target labels instead ofusing visual features directly, combining tools from vision andnatural-language processing. We validate our method on the recently introducedHICO dataset reaching a mAP of 31.54% and on the Stanford-40 Actions dataset,where the proposed method outperforms current state-of-the art and, combinedwith direct visual features, obtains an accuracy 83.12%. Moreover, the methodprovides for each class a semantically meaningful list of keywords and relevantimage regions relating it to its constituent concepts. version:1
arxiv-1605-07805 | Learning Moore Machines from Input-Output Traces | http://arxiv.org/abs/1605.07805 | author:Georgios Giantamidis, Stavros Tripakis category:cs.FL cs.LG  published:2016-05-25 summary:The problem of learning automata from example traces (but no equivalence ormembership queries) is fundamental in automata learning theory and practice. Inthis paper we study this problem for finite state machines with inputs andoutputs, and in particular for Moore machines. We introduce three algorithmsfor solving this problem: (1) the PTAP algorithm, which transforms a set ofinput-output traces into an incomplete Moore machine and then completes thatmachine with self-loops; (2) the PRPNI algorithm, which uses the well-knownRPNI algorithm for automata learning to learn a product of automata encoding aMoore machine; and (3) the MooreMI algorithm, which directly learns a Mooremachine using PTAP extended with state merging. We prove that MooreMI alwayslearns the right machine when the training set is a characteristic sample,which is generally not true for the other two algorithms. We also compare thealgorithms experimentally in terms of the size of the learned machine andseveral notions of accuracy, introduced in this paper. Finally, we compare withOSTIA, an algorithm that learns a more general class of transducers, and findthat OSTIA generally does not learn a Moore machine, even when fed with acharacteristic sample. version:1
arxiv-1605-07785 | Geometry-aware stationary subspace analysis | http://arxiv.org/abs/1605.07785 | author:Inbal Horev, Florian Yger, Masashi Sugiyama category:cs.LG  published:2016-05-25 summary:In many real-world applications data exhibits non-stationarity, i.e., itsdistribution changes over time. One approach to handling non-stationarity is toremove or minimize it before attempting to analyze the data. In the context ofbrain computer interface (BCI) data analysis this may be done by means ofstationary subspace analysis (SSA). The classic SSA method finds a matrix thatprojects the data onto a stationary subspace by optimizing a cost functionbased on a matrix divergence. In this work we present an alternative method forSSA based on a symmetrized version of this matrix divergence. We show that thisframes the problem in terms of distances between symmetric positive definite(SPD) matrices, suggesting a geometric interpretation of the problem. Stemmingfrom this geometric viewpoint, we introduce and analyze a method which utilizesthe geometry of the SPD matrix manifold and the invariance properties of itsmetrics. Most notably we show that these invariances alleviate the need towhiten the input matrices, a common step in many SSA methods which oftenintroduces errors. We demonstrate the usefulness of our technique inexperiments on both synthesized and real-world data. version:1
arxiv-1605-07784 | Fast Algorithms for Robust PCA via Gradient Descent | http://arxiv.org/abs/1605.07784 | author:Xinyang Yi, Dohyung Park, Yudong Chen, Constantine Caramanis category:cs.IT cs.LG math.IT math.ST stat.ML stat.TH  published:2016-05-25 summary:We consider the problem of Robust PCA in the the fully and partially observedsettings. Without corruptions, this is the well-known matrix completionproblem. From a statistical standpoint this problem has been recentlywell-studied, and conditions on when recovery is possible (how manyobservations do we need, how many corruptions can we tolerate) viapolynomial-time algorithms is by now understood. This paper presents andanalyzes a non-convex optimization approach that greatly reduces thecomputational complexity of the above problems, compared to the best availablealgorithms. In particular, in the fully observed case, with $r$ denoting rankand $d$ dimension, we reduce the complexity from$\mathcal{O}(r^2d^2\log(1/\varepsilon))$ to$\mathcal{O}(rd^2\log(1/\varepsilon))$ -- a big savings when the rank is big.For the partially observed case, we show the complexity of our algorithm is nomore than $\mathcal{O}(r^4d \log d \log(1/\varepsilon))$. Not only is this thebest-known run-time for a provable algorithm under partial observation, but inthe setting where $r$ is small compared to $d$, it also allows fornear-linear-in-$d$ run-time that can be exploited in the fully-observed case aswell, by simply running our algorithm on a subset of the observations. version:1
arxiv-1605-07779 | Neural Universal Discrete Denoiser | http://arxiv.org/abs/1605.07779 | author:Taesup Moon, Seon Woo Min category:cs.LG  published:2016-05-25 summary:We present a new framework of applying deep neural networks (DNN) to devise auniversal discrete denoiser. Unlike other approaches that utilize supervisedlearning for denoising, we do not require any additional training data. In suchsetting, while the ground-truth label, i.e., the clean data, is not available,we devise "pseudo-labels" and a novel objective function such that DNN can betrained in a same way as supervised learning to become a discrete denoiser. Weexperimentally show that our resulting algorithm, dubbed as Neural DUDE,significantly outperforms the previous state-of-the-art in several applicationswith a systematic rule of choosing the hyperparameter, which is an attractivefeature in practice. version:1
arxiv-1605-07774 | Generalized Mirror Descents in Congestion Games | http://arxiv.org/abs/1605.07774 | author:Po-An Chen, Chi-Jen Lu category:cs.GT cs.LG  published:2016-05-25 summary:Different types of dynamics have been studied in repeated game play, and oneof them which has received much attention recently consists of those based on"no-regret" algorithms from the area of machine learning. It is known thatdynamics based on generic no-regret algorithms may not converge to Nashequilibria in general, but to a larger set of outcomes, namely coarsecorrelated equilibria. Moreover, convergence results based on generic no-regretalgorithms typically use a weaker notion of convergence: the convergence of theaverage plays instead of the actual plays. Some work has been done showing thatwhen using a specific no-regret algorithm, the well-known multiplicativeupdates algorithm, convergence of actual plays to equilibria can be shown andbetter quality of outcomes in terms of the price of anarchy can be reached foratomic congestion games and load balancing games. Are there more cases ofnatural no-regret dynamics that perform well in suitable classes of games interms of convergence and quality of outcomes that the dynamics converge to? We answer this question positively in the bulletin-board model by showingthat when employing the mirror-descent algorithm, a well-known genericno-regret algorithm, the actual plays converge quickly to equilibria innonatomic congestion games. Furthermore, the bandit model considers a probablymore realistic and prevalent setting with only partial information, in which ateach time step each player only knows the cost of her own currently playedstrategy, but not any costs of unplayed strategies. For the class of atomiccongestion games, we propose a family of bandit algorithms based on themirror-descent algorithms previously presented, and show that when each playerindividually adopts such a bandit algorithm, their joint (mixed) strategyprofile quickly converges with implications. version:1
arxiv-1605-07766 | Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction | http://arxiv.org/abs/1605.07766 | author:Kim Anh Nguyen, Sabine Schulte im Walde, Ngoc Thang Vu category:cs.CL  published:2016-05-25 summary:We propose a novel vector representation that integrates lexical contrastinto distributional vectors and strengthens the most salient features fordetermining degrees of word similarity. The improved vectors significantlyoutperform standard models and distinguish antonyms from synonyms with anaverage precision of 0.66-0.76 across word classes (adjectives, nouns, verbs).Moreover, we integrate the lexical contrast vectors into the objective functionof a skip-gram model. The novel embedding outperforms state-of-the-art modelson predicting word similarities in SimLex-999, and on distinguishing antonymsfrom synonyms. version:1
arxiv-1605-07747 | NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and Stochastic Optimization | http://arxiv.org/abs/1605.07747 | author:Davood Hajinezhad, Mingyi Hong, Tuo Zhao, Zhaoran Wang category:math.OC stat.ML  published:2016-05-25 summary:We study a stochastic and distributed algorithm for nonconvex problems whoseobjective consists of a sum of $N$ nonconvex $L_i/N$-smooth functions, plus anonsmooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT)algorithm splits the problem into $N$ subproblems, and utilizes an augmentedLagrangian based primal-dual scheme to solve it in a distributed and stochasticmanner. With a special non-uniform sampling, a version of NESTT achieves$\epsilon$-stationary solution using$\mathcal{O}((\sum_{i=1}^N\sqrt{L_i/N})^2/\epsilon)$ gradient evaluations,which can be up to $\mathcal{O}(N)$ times better than the (proximal) gradientdescent methods. It also achieves Q-linear convergence rate for nonconvex$\ell_1$ penalized quadratic problems with polyhedral constraints. Further, wereveal a fundamental connection between {\it primal-dual} based methods and afew {\it primal only} methods such as IAG/SAG/SAGA. version:1
arxiv-1605-07740 | Improving energy efficiency and classification accuracy of neuromorphic chips by learning binary synaptic crossbars | http://arxiv.org/abs/1605.07740 | author:Antonio Jimeno Yepes, Jianbin Tang category:cs.NE  published:2016-05-25 summary:Deep Neural Networks (DNN) have achieved human level performance in manyimage analytics tasks but DNNs are mostly deployed to GPU platforms thatconsume a considerable amount of power. Brain-inspired spiking neuromorphicchips consume low power and can be highly parallelized. However, for deployingDNNs to energy efficient neuromorphic chips the incompatibility betweencontinuous neurons and synaptic weights of traditional DNNs, discrete spikingneurons and synapses of neuromorphic chips has to be overcome. Previous workhas achieved this by training a network to learn continuous probabilities anddeployment to a neuromorphic architecture by random sampling theseprobabilities. An ensemble of sampled networks is needed to approximate theperformance of the trained network. In the work presented in this paper, we have extended previous research bydirectly learning binary synaptic crossbars. Results on MNIST show that betterperformance can be achieved with a small network in one time step (92.7%maximum observed accuracy vs 95.98% accuracy in our work). Top results on alarger network are similar to previously published results (99.42% maximumobserved accuracy vs 99.45% accuracy in our work). More importantly, in ourwork a smaller ensemble is needed to achieve similar or better accuracy thanprevious work, which translates into significantly decreased energy consumptionfor both networks. Results of our work are stable since they do not requirerandom sampling. version:1
arxiv-1605-07736 | Learning Multiagent Communication with Backpropagation | http://arxiv.org/abs/1605.07736 | author:Sainbayar Sukhbaatar, Arthur Szlam, Rob Fergus category:cs.LG cs.AI  published:2016-05-25 summary:Many tasks in AI require the collaboration of multiple agents. Typically, thecommunication protocol between agents is manually specified and not alteredduring training. In this paper we explore a simple neural model, called CommNN,that uses continuous communication for fully cooperative tasks. The modelconsists of multiple agents and the communication between them is learnedalongside their policy. We apply this model to a diverse set of tasks,demonstrating the ability of the agents to learn to communicate amongstthemselves, yielding improved performance over non-communicative agents andbaselines. In some cases, it is possible to interpret the language devised bythe agents, revealing simple but effective strategies for solving the task athand. version:1
arxiv-1605-07735 | Design and development a children's speech database | http://arxiv.org/abs/1605.07735 | author:Radoslava Kraleva category:cs.CL cs.HC cs.SD  published:2016-05-25 summary:The report presents the process of planning, designing and the development ofa database of spoken children's speech whose native language is Bulgarian. Theproposed model is designed for children between the age of 4 and 6 withoutspeech disorders, and reflects their specific capabilities. At this age mostchildren cannot read, there is no sustained concentration, they are emotional,etc. The aim is to unite all the media information accompanying the recordingand processing of spoken speech, thereby to facilitate the work of researchersin the field of speech recognition. This database will be used for thedevelopment of systems for children's speech recognition, children's speechsynthesis systems, games which allow voice control, etc. As a result of theproposed model a prototype system for speech recognition is presented. version:1
arxiv-1605-07733 | On model architecture for a children's speech recognition interactive dialog system | http://arxiv.org/abs/1605.07733 | author:Radoslava Kraleva, Velin Kralev category:cs.HC cs.CL cs.SD  published:2016-05-25 summary:This report presents a general model of the architecture of informationsystems for the speech recognition of children. It presents a model of thespeech data stream and how it works. The result of these studies and presentedveins architectural model shows that research needs to be focused onacoustic-phonetic modeling in order to improve the quality of children's speechrecognition and the sustainability of the systems to noise and changes intransmission environment. Another important aspect is the development of moreaccurate algorithms for modeling of spontaneous child speech. version:1
arxiv-1605-07725 | Virtual Adversarial Training for Semi-Supervised Text Classification | http://arxiv.org/abs/1605.07725 | author:Takeru Miyato, Andrew M. Dai, Ian Goodfellow category:stat.ML cs.LG  published:2016-05-25 summary:Adversarial training provides a means of regularizing supervised learningalgorithms while virtual adversarial training is able to extend supervisedlearning algorithms to the semi-supervised setting. However, both methodsrequire making small perturbations to numerous entries of the input vector,which is inappropriate for sparse high-dimensional inputs such as one-hot wordrepresentations. We extend adversarial and virtual adversarial training to thetext domain by applying perturbations to the word embeddings in a recurrentneural network rather than to the original input itself. The proposed methodachieves state of the art results on multiple benchmark semi-supervised andpurely supervised tasks. We provide visualizations and analysis showing thatthe learned word embeddings have improved in quality and that while training,the model is less prone to overfitting. version:1
arxiv-1605-07723 | Data Programming: Creating Large Training Sets, Quickly | http://arxiv.org/abs/1605.07723 | author:Alexander Ratner, Christopher De Sa, Sen Wu, Daniel Selsam, Christopher Ré category:stat.ML cs.AI cs.LG  published:2016-05-25 summary:Large labeled training sets are the critical building blocks of supervisedlearning methods and are key enablers of deep learning techniques. For someapplications, creating labeled training sets is the most time-consuming andexpensive part of applying machine learning. We therefore propose a paradigmfor the programmatic creation of training sets called data programming in whichusers provide a set of labeling functions, which are programs thatheuristically label large subsets of data points, albeit noisily. By viewingthese labeling functions as implicitly describing a generative model for thisnoise, we show that we can recover the parameters of this model to "denoise"the training set. Then, we show how to modify a discriminative loss function tomake it noise-aware. We demonstrate our method over a range of discriminativemodels including logistic regression and LSTMs. We establish theoretically thatwe can recover the parameters of these generative models in a handful ofsettings. Experimentally, on the 2014 TAC-KBP relation extraction challenge, weshow that data programming would have obtained a winning score, and also showthat applying data programming to an LSTM model leads to a TAC-KBP score almost6 F1 points over a supervised LSTM baseline (and into second place in thecompetition). Additionally, in initial user studies we observed that dataprogramming may be an easier way to create machine learning models fornon-experts. version:1
arxiv-1605-07722 | Yum-me: Personalized Healthy Meal Recommender System | http://arxiv.org/abs/1605.07722 | author:Longqi Yang, Cheng-Kang Hsieh, Hongjian Yang, Nicola Dell, Serge Belongie, Deborah Estrin category:cs.HC cs.AI cs.CV cs.IR H.5.m  published:2016-05-25 summary:Many ubiquitous computing projects have addressed health and wellnessbehaviors such as healthy eating. Healthy meal recommendations have thepotential to help individuals prevent or manage conditions such as diabetes andobesity. However, learning people's food preferences and making healthyrecommendations that appeal to their palate is challenging. Existing approacheseither only learn high-level preferences or require a prolonged learningperiod. We propose Yum-me, a personalized healthy meal recommender systemdesigned to meet individuals' health goals, dietary restrictions, andfine-grained food preferences. Marrying ideas from user preference learning andhealthy eating promotion, Yum-me enables a simple and accurate food preferenceprofiling procedure via an image-based online learning framework, and projectsthe learned profile into the domain of healthy food options to find ones thatwill appeal to the user. We present the design and implementation of Yum-me,and further discuss the most critical component of it: FoodDist, astate-of-the-art food image analysis model. We demonstrate FoodDist's superiorperformance through careful benchmarking, and discuss its applicability acrossa wide array of dietary applications. We validate the feasibility andeffectiveness of Yum-me through a 60-person user study, in which Yum-meimproves the recommendation acceptance rate by 42.63% over the traditional foodpreference survey. version:1
arxiv-1605-07719 | Reshaped Wirtinger Flow for Solving Quadratic Systems of Equations | http://arxiv.org/abs/1605.07719 | author:Huishuai Zhang, Yingbin Liang category:stat.ML cs.LG  published:2016-05-25 summary:We study the problem of recovering a vector $\boldsymbol{x}\in \mathbb{R}^n$from its magnitude measurements $y_i=\langle \boldsymbol{a}_i,\boldsymbol{x}\rangle, i=1,..., m$. Our work is along the line of theWirtinger flow (WF) approach, which solves the problem by minimizing anonconvex loss function via a gradient algorithm and can be shown to convergeto a global optimal point under good initialization. In contrast to the smoothloss function used in WF, we adopt a nonsmooth but lower-order loss function,and design a gradient-like algorithm (referred to as reshaped-WF). We show thatfor random Gaussian measurements, reshaped-WF enjoys geometric convergence to aglobal optimal point as long as the number $m$ of measurements is at the orderof $\mathcal{O}(n)$, where $n$ is the dimension of the unknown$\boldsymbol{x}$. This improves the sample complexity of WF, and achieves thesame sample complexity as truncated-WF but without truncation at gradient step.Furthermore, reshaped-WF costs less computationally than WF, and runs fasternumerically than both WF and truncated-WF. Bypassing higher-order variables inthe loss function and truncations in the gradient loop, analysis of reshaped-WFis substantially simplified. version:1
arxiv-1605-07717 | Deep Structured Energy Based Models for Anomaly Detection | http://arxiv.org/abs/1605.07717 | author:Shuangfei Zhai, Yu Cheng, Weining Lu, Zhongfei Zhang category:cs.LG stat.ML  published:2016-05-25 summary:In this paper, we attack the anomaly detection problem by directly modelingthe data distribution with deep architectures. We propose deep structuredenergy based models (DSEBMs), where the energy function is the output of adeterministic deep neural network with structure. We develop novel modelarchitectures to integrate EBMs with different types of data such as staticdata, sequential data, and spatial data, and apply appropriate modelarchitectures to adapt to the data structure. Our training algorithm is builtupon the recent development of score matching \cite{sm}, which connects an EBMwith a regularized autoencoder, eliminating the need for complicated samplingmethod. Statistically sound decision criterion can be derived for anomalydetection purpose from the perspective of the energy landscape of the datadistribution. We investigate two decision criteria for performing anomalydetection: the energy score and the reconstruction error. Extensive empiricalstudies on benchmark tasks demonstrate that our proposed model consistentlymatches or outperforms all the competing methods. version:1
arxiv-1605-07716 | Deeply-Fused Nets | http://arxiv.org/abs/1605.07716 | author:Jingdong Wang, Zhen Wei, Ting Zhang, Wenjun Zeng category:cs.CV  published:2016-05-25 summary:In this paper, we present a novel deep learning approach, deeply-fused nets.The central idea of our approach is deep fusion, i.e., combine the intermediaterepresentations of base networks, where the fused output serves as the input ofthe remaining part of each base network, and perform such combinations deeplyover several intermediate representations. The resulting deeply fused netenjoys several benefits. First, it is able to learn multi-scale representationsas it enjoys the benefits of more base networks, which could form the samefused network, other than the initial group of base networks. Second, in oursuggested fused net formed by one deep and one shallow base networks, the flowsof the information from the earlier intermediate layer of the deep base networkto the output and from the input to the later intermediate layer of the deepbase network are both improved. Last, the deep and shallow base networks arejointly learnt and can benefit from each other. More interestingly, theessential depth of a fused net composed from a deep base network and a shallowbase network is reduced because the fused net could be composed from a lessdeep base network, and thus training the fused net is less difficult thantraining the initial deep base network. Empirical results demonstrate that ourapproach achieves superior performance over two closely-related methods, ResNetand Highway, and competitive performance compared to the state-of-the-arts. version:1
arxiv-1605-07708 | 2D Visual Place Recognition for Domestic Service Robots at Night | http://arxiv.org/abs/1605.07708 | author:James Mount, Michael Milford category:cs.RO cs.CV  published:2016-05-25 summary:Domestic service robots such as lawn mowing and vacuum cleaning robots arethe most numerous consumer robots in existence today. While early versionsemployed random exploration, recent systems fielded by most of the majormanufacturers have utilized range-based and visual sensors and user-placedbeacons to enable robots to map and localize. However, active range and visualsensing solutions have the disadvantages of being intrusive, expensive, or onlyproviding a 1D scan of the environment, while the requirement for beaconplacement imposes other practical limitations. In this paper we present apassive and potentially cheap vision-based solution to 2D localization at nightthat combines easily obtainable day-time maps with low resolutioncontrast-normalized image matching algorithms, image sequence-based matching intwo-dimensions, place match interpolation and recent advances in conventionallow light camera technology. In a range of experiments over a domestic lawn andin a lounge room, we demonstrate that the proposed approach enables 2Dlocalization at night, and analyse the effect on performance of varyingodometry noise levels, place match interpolation and sequence matching length.Finally we benchmark the new low light camera technology and show how it canenable robust place recognition even in an environment lit only by a moonlesssky, raising the tantalizing possibility of being able to apply allconventional vision algorithms, even in the darkest of nights. version:1
arxiv-1605-07700 | Learning Purposeful Behaviour in the Absence of Rewards | http://arxiv.org/abs/1605.07700 | author:Marlos C. Machado, Michael Bowling category:cs.LG cs.AI  published:2016-05-25 summary:Artificial intelligence is commonly defined as the ability to achieve goalsin the world. In the reinforcement learning framework, goals are encoded asreward functions that guide agent behaviour, and the sum of observed rewardsprovide a notion of progress. However, some domains have no such reward signal,or have a reward signal so sparse as to appear absent. Without reward feedback,agent behaviour is typically random, often dithering aimlessly and lackingintentionality. In this paper we present an algorithm capable of learningpurposeful behaviour in the absence of rewards. The algorithm proceeds byconstructing temporally extended actions (options), through the identificationof purposes that are "just out of reach" of the agent's current behaviour.These purposes establish intrinsic goals for the agent to learn, ultimatelyresulting in a suite of behaviours that encourage the agent to visit differentparts of the state space. Moreover, the approach is particularly suited forsettings where rewards are very sparse, and such behaviours can help in theexploration of the environment until reward is observed. version:1
arxiv-1605-07699 | Describing Human Aesthetic Perception by Deeply-learned Attributes from Flickr | http://arxiv.org/abs/1605.07699 | author:L. Zhang category:cs.CV  published:2016-05-25 summary:Many aesthetic models in computer vision suffer from two shortcomings: 1) thelow descriptiveness and interpretability of those hand-crafted aestheticcriteria (i.e., nonindicative of region-level aesthetics), and 2) thedifficulty of engineering aesthetic features adaptively and automaticallytoward different image sets. To remedy these problems, we develop a deeparchitecture to learn aesthetically-relevant visual attributes from Flickr1,which are localized by multiple textual attributes in a weakly-supervisedsetting. More specifically, using a bag-ofwords (BoW) representation of thefrequent Flickr image tags, a sparsity-constrained subspace algorithm discoversa compact set of textual attributes (e.g., landscape and sunset) for eachimage. Then, a weakly-supervised learning algorithm projects the textualattributes at image-level to the highly-responsive image patches atpixel-level. These patches indicate where humans look at appealing regions withrespect to each textual attribute, which are employed to learn the visualattributes. Psychological and anatomical studies have shown that humansperceive visual concepts hierarchically. Hence, we normalize these patches andfeed them into a five-layer convolutional neural network (CNN) to mimick thehierarchy of human perceiving the visual attributes. We apply the learned deepfeatures on image retargeting, aesthetics ranking, and retrieval. Bothsubjective and objective experimental results thoroughly demonstrate thecompetitiveness of our approach. version:1
arxiv-1605-07689 | Communication-efficient distributed statistical learning | http://arxiv.org/abs/1605.07689 | author:Michael I. Jordan, Jason D. Lee, Yun Yang category:stat.ML cs.IT cs.LG math.IT math.OC stat.ME  published:2016-05-25 summary:We present the Communication-efficient Surrogate Likelihood (CSL) frameworkfor solving distributed statistical learning problems. CSL provides acommunication-efficient surrogate to the global likelihood that can be used forlow-dimensional estimation, high-dimensional regularized estimation andBayesian inference. For low-dimensional estimation, CSL provably improves uponthe averaging schemes and facilitates the construction of confidence intervals.For high-dimensional regularized estimation, CSL leads to a minimax optimalestimator with minimal communication cost. For Bayesian inference, CSL can beused to form a communication-efficient quasi-posterior distribution thatconverges to the true posterior. This quasi-posterior procedure significantlyimproves the computational efficiency of MCMC algorithms even in anon-distributed setting. The methods are illustrated through empirical studies. version:1
arxiv-1605-07686 | Local Perturb-and-MAP for Structured Prediction | http://arxiv.org/abs/1605.07686 | author:Gedas Bertasius, Qiang Liu, Lorenzo Torresani, Jianbo Shi category:cs.CV  published:2016-05-24 summary:Two fundamental problems in the context of probabilistic graphical models arelearning and inference. Many traditional probabilistic methods resort toapproximations in either learning, inference, or even both steps due to theirlarge complexity cost. This leads to algorithms where the learning andinference steps are disjoint, which often degrades algorithm performance. Inthis work, we present a Local Perturb-and-MAP (locPMAP) method, a novelframework for structured prediction based on local optimization over randomlyperturbed potential functions. Unlike most prior methods, our proposed schemedoes not rely on approximations and also unifies the learning and inferencesteps. This allows our approach to outperform other methods on several visiontasks, where structured models are commonly used. Additionally, we demonstratehow to apply our proposed scheme for an end-to-end training of a deepstructured network. Finally, our framework gives a novel interpretation forpseudolikelihood, enabling various powerful extensions, such as the use ofpseudolikelihood under the Bayesian framework. version:1
arxiv-1605-07683 | Learning End-to-End Goal-Oriented Dialog | http://arxiv.org/abs/1605.07683 | author:Antoine Bordes, Jason Weston category:cs.CL  published:2016-05-24 summary:End-to-end dialog systems, in which all components are learnt simultaneously,have recently obtained encouraging successes. However these were mostly onconversations related to chit-chat with no clear objective and for whichevaluation is difficult. This paper proposes a set of tasks to test thecapabilities of such systems on goal-oriented dialogs, where goal completionensures a well-defined measure of performance. Built in the context ofrestaurant reservation, our tasks require to manipulate sentences and symbols,in order to properly conduct conversations, issue API calls and use the outputsof such calls. We show that an end-to-end dialog system based on MemoryNetworks can reach promising, yet imperfect, performance and learn to performnon-trivial operations. We confirm those results by comparing our system to ahand-crafted slot-filling baseline on data from the second Dialog StateTracking Challenge (Henderson et al., 2014a). version:1
arxiv-1605-07681 | Convolutional Random Walk Networks for Semantic Image Segmentation | http://arxiv.org/abs/1605.07681 | author:Gedas Bertasius, Lorenzo Torresani, Stella X. Yu, Jianbo Shi category:cs.CV  published:2016-05-24 summary:Most current semantic segmentation methods rely on fully convolutionalnetworks (FCNs). However, the use of large receptive fields and many poolinglayers, cause blurring and low spatial resolution inside the deep layers, whichoften lead to spatially fragmented FCN predictions. In this work, we addressthis problem by introducing Convolutional Random Walk Networks (RWNs) thatcombine the strengths of FCNs and random walk based methods. Our proposed RWNjointly optimizes pixelwise affinity and semantic segmentation learningobjectives, and combines these two sources of information via a novel randomwalk layer that enforces consistent spatial grouping in the deep layers of thenetwork. We show that such a grouping mechanism improves the semanticsegmentation accuracy when applied in the deep low spatial resolution FCNlayers. Our proposed RWN fully integrates pixelwise affinity learning and therandom walk process. This makes it possible to train the whole network in anend-to-end fashion with the standard back-propagation algorithm. Additionally,our RWN needs just 131 additional parameters compared to the state-of-the-artDeepLab network, and yet it produces an improvement of 1.5% according to themean IOU evaluation metric on Pascal SBD dataset. version:1
arxiv-1605-07669 | On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems | http://arxiv.org/abs/1605.07669 | author:Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, Steve Young category:cs.CL cs.LG  published:2016-05-24 summary:The ability to compute an accurate reward function is essential foroptimising a dialogue policy via reinforcement learning. In real-worldapplications, using explicit user feedback as the reward signal is oftenunreliable and costly to collect. This problem can be mitigated if the user'sintent is known in advance or data is available to pre-train a task successpredictor off-line. In practice neither of these apply for most real worldapplications. Here we propose an on-line learning framework whereby thedialogue policy is jointly trained alongside the reward model via activelearning with a Gaussian process model. This Gaussian process operates on acontinuous space dialogue representation generated in an unsupervised fashionusing a recurrent neural network encoder-decoder. The experimental resultsdemonstrate that the proposed framework is able to significantly reduce dataannotation costs and mitigate noisy user feedback in dialogue policy learning. version:1
arxiv-1605-07659 | Adaptive Newton Method for Empirical Risk Minimization to Statistical Accuracy | http://arxiv.org/abs/1605.07659 | author:Aryan Mokhtari, Alejandro Ribeiro category:cs.LG math.OC  published:2016-05-24 summary:We consider empirical risk minimization for large-scale datasets. Weintroduce Ada Newton as an adaptive algorithm that uses Newton's method withadaptive sample sizes. The main idea of Ada Newton is to increase the size ofthe training set by a factor larger than one in a way that the minimizationvariable for the current training set is in the local neighborhood of theoptimal argument of the next training set. This allows to exploit the quadraticconvergence property of Newton's method and reach the statistical accuracy ofeach training set with only one iteration of Newton's method. We showtheoretically and empirically that Ada Newton can double the size of thetraining set in each iteration to achieve the statistical accuracy of the fulltraining set with about two passes over the dataset. version:1
arxiv-1605-07651 | Self Paced Deep Learning for Weakly Supervised Object Detection | http://arxiv.org/abs/1605.07651 | author:Enver Sangineto, Moin Nabi, Dubravko Culibrk, Nicu Sebe category:cs.CV  published:2016-05-24 summary:In a weakly-supervised scenario, object detectors need to be trained usingimage-level annotation only. Since bounding-box-level ground truth is notavailable, most of the solutions proposed so far are based on an iterativeapproach in which the classifier, obtained in the previous iteration, is usedto predict the objects' positions which are used for training in the currentiteration. However, the errors in these predictions can make the process drift.In this paper we propose a self-paced learning protocol to alleviate thisproblem. The main idea is to iteratively select a subset of samples that aremost likely correct, which are used for training. While similar strategies havebeen recently adopted for SVMs and other classifiers, as far as we know, we arethe first showing that a self-paced approach can be used with deep-net-basedclassifiers. We show results on Pascal VOC and ImageNet, outperforming theprevious state of the art on both datasets and specifically obtaining more than100% relative improvement on ImageNet. version:1
arxiv-1605-07650 | Blind Analysis of CT Image Noise Using Residual Denoised Images | http://arxiv.org/abs/1605.07650 | author:Sohini Roychowdhury, Nathan Hollraft, Adam Alessio category:cs.CV  published:2016-05-24 summary:CT protocol design and quality control would benefit from automated tools toestimate the quality of generated CT images. These tools could be used toidentify erroneous CT acquisitions or refine protocols to achieve certainsignal to noise characteristics. This paper investigates blind estimationmethods to determine global signal strength and noise levels in chest CTimages. Methods: We propose novel performance metrics corresponding to theaccuracy of noise and signal estimation. We implement and evaluate the noiseestimation performance of six spatial- and frequency- based methods, derivedfrom conventional image filtering algorithms. Algorithms were tested on patientdata sets from whole-body repeat CT acquisitions performed with a higher andlower dose technique over the same scan region. Results: The proposedperformance metrics can evaluate the relative tradeoff of filter parameters andnoise estimation performance. The proposed automated methods tend tounderestimate CT image noise at low-flux levels. Initial application ofmethodology suggests that anisotropic diffusion and Wavelet-transform basedfilters provide optimal estimates of noise. Furthermore, methodology does notprovide accurate estimates of absolute noise levels, but can provide estimatesof relative change and/or trends in noise levels. version:1
arxiv-1605-07648 | FractalNet: Ultra-Deep Neural Networks without Residuals | http://arxiv.org/abs/1605.07648 | author:Gustav Larsson, Michael Maire, Gregory Shakhnarovich category:cs.CV  published:2016-05-24 summary:We introduce a design strategy for neural network macro-architecture based onself-similarity. Repeated application of a single expansion rule generates anextremely deep network whose structural layout is precisely a truncatedfractal. Such a network contains interacting subpaths of different lengths, butdoes not include any pass-through connections: every internal signal istransformed by a filter and nonlinearity before being seen by subsequentlayers. This property stands in stark contrast to the current approach ofexplicitly structuring very deep networks so that training is a residuallearning problem. Our experiments demonstrate that residual representation isnot fundamental to the success of extremely deep convolutional neural networks.A fractal design achieves an error rate of 22.85% on CIFAR-100, matching thestate-of-the-art held by residual networks. Fractal networks exhibit intriguing properties beyond their high performance.They can be regarded as a computationally efficient implicit union ofsubnetworks of every depth. We explore consequences for training, touching uponconnection with student-teacher behavior, and, most importantly, demonstratingthe ability to extract high-performance fixed-depth subnetworks. To facilitatethis latter task, we develop drop-path, a natural extension of dropout, toregularize co-adaptation of subpaths in fractal architectures. With suchregularization, fractal networks exhibit an anytime property: shallowsubnetworks provide a quick answer, while deeper subnetworks, with higherlatency, provide a more accurate answer. version:1
arxiv-1605-07604 | Posterior Dispersion Indices | http://arxiv.org/abs/1605.07604 | author:Alp Kucukelbir, David M. Blei category:stat.ML cs.AI stat.CO  published:2016-05-24 summary:Probabilistic modeling is cyclical: we specify a model, infer its posterior,and evaluate its performance. Evaluation drives the cycle, as we revise ourmodel based on how it performs. This requires a metric. Traditionally,predictive accuracy prevails. Yet, predictive accuracy does not tell the wholestory. We propose to evaluate a model through posterior dispersion. The idea isto analyze how each datapoint fares in relation to posterior uncertainty aroundthe hidden structure. We propose a family of posterior dispersion indices (PDI)that capture this idea. A PDI identifies rich patterns of model mismatch inthree real data examples: voting preferences, supermarket shopping, andpopulation genetics. version:1
arxiv-1605-07157 | Unsupervised Learning for Physical Interaction through Video Prediction | http://arxiv.org/abs/1605.07157 | author:Chelsea Finn, Ian Goodfellow, Sergey Levine category:cs.LG cs.AI cs.CV cs.RO  published:2016-05-23 summary:A core challenge for an agent learning to interact with the world is topredict how its actions affect objects in its environment. Many existingmethods for learning the dynamics of physical interactions require labeledobject information. However, to scale real-world interaction learning to avariety of scenes and objects, acquiring labeled data becomes increasinglyimpractical. To learn about physical object motion without labels, we developan action-conditioned video prediction model that explicitly models pixelmotion, by predicting a distribution over pixel motion from previous frames.Because our model explicitly predicts motion, it is partially invariant toobject appearance, enabling it to generalize to previously unseen objects. Toexplore video prediction for real-world interactive agents, we also introduce adataset of 50,000 robot interactions involving pushing motions, including atest set with novel objects. In this dataset, accurate prediction of videosconditioned on the robot's future actions amounts to learning a "visualimagination" of different futures based on different courses of action. Ourexperiments show that our proposed method not only produces more accurate videopredictions, but also more accurately predicts object motion, when compared toprior methods. version:2
arxiv-1605-07586 | Natural Scene Image Segmentation Based on Multi-Layer Feature Extraction | http://arxiv.org/abs/1605.07586 | author:Fariba Zohrizadeh, Mohsen Kheirandishfard, Farhad Kamangar category:cs.CV  published:2016-05-24 summary:This paper addresses the problem of natural image segmentation by extractinginformation from a multi-layer array which is constructed based on color,gradient, and statistical properties of the local neighborhoods in an image. AGaussian Mixture Model (GMM) is used to improve the effectiveness of localspectral histogram features. Grouping these features leads to forming a roughinitial over-segmented layer which contains coherent regions of pixels. Theregions are merged by using two proposed functions for calculating the distancebetween two neighboring regions and making decisions about their merging.Extensive experiments are performed on the Berkeley Segmentation Dataset toevaluate the performance of our proposed method and compare the results withthe recent state-of-the-art methods. The experimental results indicate that ourmethod achieves higher level of accuracy for natural images compared to recentmethods. version:1
arxiv-1605-07571 | Sequential Neural Models with Stochastic Layers | http://arxiv.org/abs/1605.07571 | author:Marco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet, Ole Winther category:stat.ML cs.LG  published:2016-05-24 summary:How can we efficiently propagate uncertainty in a latent state representationwith recurrent neural networks? This paper introduces stochastic recurrentneural networks which glue a deterministic recurrent neural network and a statespace model together to form a stochastic and sequential neural generativemodel. The clear separation of deterministic and stochastic layers allows astructured variational inference network to track the factorization of themodel's posterior distribution. By retaining both the nonlinear recursivestructure of a recurrent neural network and averaging over the uncertainty in alatent path, like a state space model, we improve the state of the art resultson the Blizzard and TIMIT speech modeling data sets by a large margin, whileachieving comparable performances to competing methods on polyphonic musicmodeling. version:1
arxiv-1605-06676 | Learning to Communicate with Deep Multi-Agent Reinforcement Learning | http://arxiv.org/abs/1605.06676 | author:Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, Shimon Whiteson category:cs.AI cs.LG cs.MA  published:2016-05-21 summary:We consider the problem of multiple agents sensing and acting in environmentswith the goal of maximising their shared utility. In these environments, agentsmust learn communication protocols in order to share information that is neededto solve the tasks. By embracing deep neural networks, we are able todemonstrate end-to-end learning of protocols in complex environments inspiredby communication riddles and multi-agent computer vision problems with partialobservability. We propose two approaches for learning in these domains:Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning(DIAL). The former uses deep Q-learning, while the latter exploits the factthat, during learning, agents can backpropagate error derivatives through(noisy) communication channels. Hence, this approach uses centralised learningbut decentralised execution. Our experiments introduce new environments forstudying the learning of communication protocols and present a set ofengineering innovations that are essential for success in these domains. version:2
arxiv-1605-07541 | Inductive quantum learning: Why you are doing it almost right | http://arxiv.org/abs/1605.07541 | author:Alex Monràs, Gael Sentís, Peter Wittek category:cs.LG quant-ph stat.ML  published:2016-05-24 summary:In supervised learning, an inductive learning algorithm extracts generalrules from observed training instances, then the rules are applied to testinstances. We show that this splitting of training and application arisesnaturally, in the classical setting, from a simple independence requirementwith a physical interpretation of being non-signalling. Thus, two seeminglydifferent definitions of inductive learning happen to coincide. This followsfrom very specific properties of classical information, which break down in thequantum setup. We prove a quantum de Finetti theorem for quantum channels,which shows that in the quantum case, the equivalence holds in the asymptoticsetting (for large number of test instances). This reveals a natural analogybetween classical learning protocols and their quantum counterparts, thusallowing to naturally enquire about standard elements in computational learningtheory, such as structural risk minimization, model and sample complexity. version:1
arxiv-1605-07515 | Neural Semantic Role Labeling with Dependency Path Embeddings | http://arxiv.org/abs/1605.07515 | author:Michael Roth, Mirella Lapata category:cs.CL  published:2016-05-24 summary:This paper introduces a novel model for semantic role labeling that makes useof neural sequence modeling techniques. Our approach is motivated by theobservation that complex syntactic structures and related phenomena, such asnested subordinations and nominal predicates, are not handled well by existingmodels. Our model treats such instances as sub-sequences of lexicalizeddependency paths and learns suitable embedding representations. Weexperimentally demonstrate that such embeddings can improve results overprevious state-of-the-art semantic role labelers, and showcase qualitativeimprovements obtained by our method. version:1
arxiv-1605-07511 | A note on privacy preserving iteratively reweighted least squares | http://arxiv.org/abs/1605.07511 | author:Mijung Park, Max Welling category:cs.CR cs.AI stat.AP stat.ML  published:2016-05-24 summary:Iteratively reweighted least squares (IRLS) is a widely-used method inmachine learning to estimate the parameters in the generalised linear models.In particular, IRLS for L1 minimisation under the linear model provides aclosed-form solution in each step, which is a simple multiplication between theinverse of the weighted second moment matrix and the weighted first momentvector. When dealing with privacy sensitive data, however, developing a privacypreserving IRLS algorithm faces two challenges. First, due to the inversion ofthe second moment matrix, the usual sensitivity analysis in differentialprivacy incorporating a single datapoint perturbation gets complicated andoften requires unrealistic assumptions. Second, due to its iterative nature, asignificant cumulative privacy loss occurs. However, adding a high level ofnoise to compensate for the privacy loss hinders from getting accurateestimates. Here, we develop a practical algorithm that overcomes thesechallenges and outputs privatised and accurate IRLS solutions. In our method,we analyse the sensitivity of each moments separately and treat the matrixinversion and multiplication as a post-processing step, which simplifies thesensitivity analysis. Furthermore, we apply the {\it{concentrated differentialprivacy}} formalism, a more relaxed version of differential privacy, whichrequires adding a significantly less amount of noise for the same level ofprivacy guarantee, compared to the conventional and advanced compositions ofdifferentially private mechanisms. version:1
arxiv-1605-07498 | Leveraging Over Priors for Boosting Control of Prosthetic Hands | http://arxiv.org/abs/1605.07498 | author:Valentina Gregori category:cs.LG  published:2016-05-24 summary:The Electromyography (EMG) signal is the electrical activity produced bycells of skeletal muscles in order to provide a movement. The non-invasiveprosthetic hand works with several electrodes, placed on the stump of anamputee, that record this signal. In order to favour the control of prosthesis,the EMG signal is analyzed with algorithms based on machine learning theory todecide the movement that the subject is going to do. In order to obtain asignificant control of the prosthesis and avoid mismatch between desired andperformed movements, a long training period is needed when we use thetraditional algorithm of machine learning (i.e. Support Vector Machines). Anactual challenge in this field concerns the reduction of the time necessary foran amputee to learn how to use the prosthesis. Recently, several algorithmsthat exploit a form of prior knowledge have been proposed. In general, we referto prior knowledge as a past experience available in the form of models. In ourcase an amputee, that attempts to perform some movements with the prosthesis,could use experience from different subjects that are already able to performthose movements. The aim of this work is to verify, with a computationalinvestigation, if for an amputee this kind of previous experience is useful inorder to reduce the training time and boost the prosthetic control.Furthermore, we want to understand if and how the final results change when theprevious knowledge of intact or amputated subjects is used for a new amputee.Our experiments indicate that: (1) the use of experience, from other subjectsalready trained to perform a task, makes us able to reduce the training time ofabout an order of magnitude; (2) it seems that an amputee that tries to learnto use the prosthesis doesn't reach different results when he/she exploitsprevious experience of amputees or intact. version:1
arxiv-1605-07496 | Alternating Optimisation and Quadrature for Robust Reinforcement Learning | http://arxiv.org/abs/1605.07496 | author:Supratik Paul, Kamil Ciosek, Michael A. Osborne, Shimon Whiteson category:cs.AI cs.LG  published:2016-05-24 summary:Bayesian optimisation has been successfully applied to a variety ofreinforcement learning problems. However, the traditional approach for learningoptimal policies in simulators does not utilise the opportunity to improvelearning by adjusting certain environment variables - state features that arerandomly determined by the environment in a physical setting but arecontrollable in a simulator. This paper considers the problem of finding anoptimal policy while taking into account the impact of environment variables.We present the alternating optimisation and quadrature algorithm which usesBayesian optimisation and Bayesian quadrature to address such settings and isrobust to the presence of significant rare events, which may not be observableunder random sampling but have a considerable impact on determining the optimalpolicy. Our experimental results show that our approach learns better andfaster than existing methods. version:1
arxiv-1605-07427 | Hierarchical Memory Networks | http://arxiv.org/abs/1605.07427 | author:Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro, Yoshua Bengio category:stat.ML cs.CL cs.LG cs.NE  published:2016-05-24 summary:Memory networks are neural networks with an explicit memory component thatcan be both read and written to by the network. The memory is often addressedin a soft way using a softmax function, making end-to-end training withbackpropagation possible. However, this is not computationally scalable forapplications which require the network to read from extremely large memories.On the other hand, it is well known that hard attention mechanisms based onreinforcement learning are challenging to train successfully. In this paper, weexplore a form of hierarchical memory network, which can be considered as ahybrid between hard and soft attention memory networks. The memory is organizedin a hierarchical structure such that reading from it is done with lesscomputation than soft attention over a flat memory, while also being easier totrain than hard attention over a flat memory. Specifically, we propose toincorporate Maximum Inner Product Search (MIPS) in the training and inferenceprocedures for our hierarchical memory network. We explore the use of variousstate-of-the art approximate MIPS techniques and report results onSimpleQuestions, a challenging large scale factoid question answering task. version:1
arxiv-1605-07422 | Web-scale Topic Models in Spark: An Asynchronous Parameter Server | http://arxiv.org/abs/1605.07422 | author:Rolf Jagerman, Carsten Eickhoff category:cs.DC cs.IR cs.LG stat.ML  published:2016-05-24 summary:In this paper, we train a Latent Dirichlet Allocation (LDA) topic model onthe ClueWeb12 data set, a 27-terabyte Web crawl. We extend Spark, a popularframework for performing large-scale data analysis, with an asynchronousparameter server. Such a parameter server provides a distributed andconcurrently accessed parameter space for the model. A Metropolis-Hastingsbased collapsed Gibbs sampler is implemented using this parameter serverachieving an amortized O(1) sampling complexity. We compare our implementationto the default Spark implementations and show that it is significantly fasterand more scalable without sacrificing model quality. A topic model with 1,000topics is trained on the full ClueWeb12 data set, uncovering some of theprevalent themes that appear on the Web. version:1
arxiv-1605-07416 | Refined Lower Bounds for Adversarial Bandits | http://arxiv.org/abs/1605.07416 | author:Sébastien Gerchinovitz, Tor Lattimore category:math.ST cs.LG stat.ML stat.TH  published:2016-05-24 summary:We provide new lower bounds on the regret that must be suffered byadversarial bandit algorithms. The new results show that recent upper boundsthat either (a) hold with high-probability or (b) depend on the total lossofthe best arm or (c) depend on the quadratic variation of the losses, are closeto tight. Besides this we prove two impossibility results. First, the existenceof a single arm that is optimal in every round cannot improve the regret in theworst case. Second, the regret cannot scale with the effective range of thelosses. In contrast, both results are possible in the full-information setting. version:1
arxiv-1605-07371 | Semiparametric energy-based probabilistic models | http://arxiv.org/abs/1605.07371 | author:Jan Humplik, Gašper Tkačik category:q-bio.NC cond-mat.stat-mech stat.ML  published:2016-05-24 summary:Probabilistic models can be defined by an energy function, where theprobability of each state is proportional to the exponential of the state'snegative energy. This paper considers a generalization of energy-based modelsin which the probability of a state is proportional to an arbitrary positive,strictly decreasing, and twice differentiable function of the state's energy.The precise shape of the nonlinear map from energies to unnormalizedprobabilities has to be learned from data together with the parameters of theenergy function. As a case study we show that the above generalization of afully visible Boltzmann machine yields an accurate model of neural activity ofretinal ganglion cells. We attribute this success to the model's ability toeasily capture distributions whose probabilities span a large dynamic range, apossible consequence of latent variables that globally couple the system.Similar features have recently been observed in many datasets, suggesting thatour new method has wide applicability. version:1
arxiv-1605-07369 | Quickest Moving Object Detection | http://arxiv.org/abs/1605.07369 | author:Dong Lao, Ganesh Sundaramoorthi category:cs.CV  published:2016-05-24 summary:We present a general framework and method for simultaneous detection andsegmentation of an object in a video that moves (or comes into view of thecamera) at some unknown time in the video. The method is an online approachbased on motion segmentation, and it operates under dynamic backgrounds causedby a moving camera or moving nuisances. The goal of the method is to detect andsegment the object as soon as it moves. Due to stochastic variability in thevideo and unreliability of the motion signal, several frames are needed toreliably detect the object. The method is designed to detect and segment withminimum delay subject to a constraint on the false alarm rate. The method isderived as a problem of Quickest Change Detection. Experiments on a datasetshow the effectiveness of our method in minimizing detection delay subject tofalse alarm constraints. version:1
arxiv-1605-07366 | Experiments in Linear Template Combination using Genetic Algorithms | http://arxiv.org/abs/1605.07366 | author:Nikhilesh Bhatnagar, Radhika Mamidi category:cs.CL 68T50  published:2016-05-24 summary:Natural Language Generation systems typically have two parts - strategic('what to say') and tactical ('how to say'). We present our experiments inbuilding an unsupervised corpus-driven template based tactical NLG system. Weconsider templates as a sequence of words containing gaps. Our idea is based onthe observation that templates are grammatical locally (within their textualspan). We posit the construction of a sentence as a highly restricted sequenceof such templates. This work is an attempt to explore the resulting searchspace using Genetic Algorithms to arrive at acceptable solutions. We present abaseline implementation of this approach which outputs gapped text. version:1
arxiv-1605-07363 | Spatio-Temporal Image Boundary Extrapolation | http://arxiv.org/abs/1605.07363 | author:Apratim Bhattacharyya, Mateusz Malinowski, Mario Fritz category:cs.CV  published:2016-05-24 summary:Boundary prediction in images as well as video has been a very active topicof research and organizing visual information into boundaries and segments isbelieved to be a corner stone of visual perception. While prior work hasfocused on predicting boundaries for observed frames, our work aims atpredicting boundaries of future unobserved frames. This requires our model tolearn about the fate of boundaries and extrapolate motion patterns. Weexperiment on established real-world video segmentation dataset, which providesa testbed for this new task. We show for the first time spatio-temporalboundary extrapolation in this challenging scenario. Furthermore, we showlong-term prediction of boundaries in situations where the motion is governedby the laws of physics. We successfully predict boundaries in a billiardscenario without any assumptions of a strong parametric model or any objectnotion. We argue that our model has with minimalistic model assumptions deriveda notion of 'intuitive physics' that can be applied to novel scenes. version:1
arxiv-1605-07358 | Consistency Analysis for the Doubly Stochastic Dirichlet Process | http://arxiv.org/abs/1605.07358 | author:Xing Sun, Nelson H. C. Yung, Edmund Y. Lam, Hayden K. -H. So category:cs.IT math.IT stat.ML  published:2016-05-24 summary:This technical report proves components consistency for the Doubly StochasticDirichlet Process with exponential convergence of posterior probability. Wealso present the fundamental properties for DSDP as well as inferencealgorithms. Simulation toy experiment and real-world experiment results forsingle and multi-cluster also support the consistency proof. This report isalso a support document for the paper "Computationally Efficient HyperspectralData Learning Based on the Doubly Stochastic Dirichlet Process". version:1
arxiv-1605-07346 | Multi-Level Analysis and Annotation of Arabic Corpora for Text-to-Sign Language MT | http://arxiv.org/abs/1605.07346 | author:Abdelaziz Lakhfif, Mohammed T. Laskri, Eric Atwell category:cs.CL  published:2016-05-24 summary:In this paper, we present an ongoing effort in lexical semantic analysis andannotation of Modern Standard Arabic (MSA) text, a semi automatic annotationtool concerned with the morphologic, syntactic, and semantic levels ofdescription. version:1
arxiv-1605-07334 | Near-optimal Bayesian Active Learning with Correlated and Noisy Tests | http://arxiv.org/abs/1605.07334 | author:Yuxin Chen, S. Hamed Hassani, Andreas Krause category:cs.LG cs.AI  published:2016-05-24 summary:We consider the Bayesian active learning and experimental design problem,where the goal is to learn the value of some unknown target variable through asequence of informative, noisy tests. In contrast to prior work, we focus onthe challenging, yet practically relevant setting where test outcomes can beconditionally dependent given the hidden target variable. Under suchassumptions, common heuristics, such as greedily performing tests that maximizethe reduction in uncertainty of the target, often perform poorly. In thispaper, we propose ECED, a novel, computationally efficient active learningalgorithm, and prove strong theoretical guarantees that hold with correlated,noisy tests. Rather than directly optimizing the prediction error, at eachstep, ECED picks the test that maximizes the gain in a surrogate objective,which takes into account the dependencies between tests. Our analysis relies onan information-theoretic auxiliary function to track the progress of ECED, andutilizes adaptive submodularity to attain the near-optimal bound. Wedemonstrate strong empirical performance of ECED on two problem instances,including a Bayesian experimental design task intended to distinguish amongeconomic theories of how people make risky decisions, and an active preferencelearning task via pairwise comparisons. version:1
arxiv-1605-07333 | Combining Recurrent and Convolutional Neural Networks for Relation Classification | http://arxiv.org/abs/1605.07333 | author:Ngoc Thang Vu, Heike Adel, Pankaj Gupta, Hinrich Schütze category:cs.CL  published:2016-05-24 summary:This paper investigates two different neural architectures for the task ofrelation classification: convolutional neural networks and recurrent neuralnetworks. For both models, we demonstrate the effect of different architecturalchoices. We present a new context representation for convolutional neuralnetworks for relation classification (extended middle context). Furthermore, wepropose connectionist bi-directional recurrent neural networks and introduceranking loss for their optimization. Finally, we show that combiningconvolutional and recurrent neural networks using a simple voting scheme isaccurate enough to improve results. Our neural models achieve state-of-the-artresults on the SemEval 2010 relation classification task. version:1
arxiv-1605-07332 | Relevant sparse codes with variational information bottleneck | http://arxiv.org/abs/1605.07332 | author:Matthew Chalk, Olivier Marre, Gasper Tkacik category:stat.ML  published:2016-05-24 summary:In many applications, it is desirable to extract only the relevant aspects ofdata. A principled way to do this is the information bottleneck (IB) method,where one seeks a code that maximizes information about a 'relevance' variable,Y, while constraining the information encoded about the original data, X.Unfortunately however, the IB method is computationally demanding when data arehigh-dimensional and/or non-gaussian. Here we propose an approximatevariational scheme for maximizing a lower bound on the IB objective, analogousto variational EM. Using this method, we derive an IB algorithm to recoverfeatures that are both relevant and sparse. Finally, we demonstrate howkernelized versions of the algorithm can be used to address a broad range ofproblems with non-linear relation between X and Y. version:1
arxiv-1605-07314 | DeepText: A Unified Framework for Text Proposal Generation and Text Detection in Natural Images | http://arxiv.org/abs/1605.07314 | author:Zhuoyao Zhong, Lianwen Jin, Shuye Zhang, Ziyong Feng category:cs.CV  published:2016-05-24 summary:In this paper, we develop a novel unified framework called DeepText for textregion proposal generation and text detection in natural images via a fullyconvolutional neural network (CNN). First, we propose the inception regionproposal network (Inception-RPN) and design a set of text characteristic priorbounding boxes to achieve high word recall with only hundred level candidateproposals. Next, we present a powerful textdetection network that embedsambiguous text category (ATC) information and multilevel region-of-interestpooling (MLRP) for text and non-text classification and accurate localization.Finally, we apply an iterative bounding box voting scheme to pursue high recallin a complementary manner and introduce a filtering algorithm to retain themost suitable bounding box, while removing redundant inner and outer boxes foreach text instance. Our approach achieves an F-measure of 0.83 and 0.85 on theICDAR 2011 and 2013 robust text detection benchmarks, outperforming previousstate-of-the-art results. version:1
arxiv-1605-07289 | EventNet Version 1.1 Technical Report | http://arxiv.org/abs/1605.07289 | author:Dongang Wang, Zheng Shou, Hongyi Liu, Shih-Fu Chang category:cs.CV  published:2016-05-24 summary:EventNet is a large-scale video corpus and event ontology consisting of 500events associated with event-specific concepts. In order to improve the qualityof the current EventNet, we conduct the following steps and introduce EventNetversion 1.1: (1) manually verify the correctness of event labels for allvideos; (2) remove the YouTube user bias by limiting the maximum number ofvideos in each event from the same YouTube user as 3; (3) remove the videoswhich are currently not accessible online; (4) remove the video belonging tomultiple event categories. After the above procedure, some events may containonly a small number of videos, and therefore we crawl more videos for thoseevents to ensure every event will contain more than 50 videos. Finally,EventNet version 1.1 contains 67,641 videos, 500 events, and 5,028event-specific concepts. In addition, we train a Convolutional Neural Network(CNN) model for event classification via fine-tuning AlexNet using EventNetversion 1.1. Then we use the trained CNN model to extract FC7 layer feature andtrain binary classifiers using linear SVM for each event-specific concept. Webelieve this new version of EventNet will significantly significantlyfacilitate research in computer vision and multimedia, and will put it onlinefor public downloading in the future. version:1
arxiv-1605-07277 | Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples | http://arxiv.org/abs/1605.07277 | author:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow category:cs.CR cs.LG  published:2016-05-24 summary:Many machine learning models are vulnerable to adversarial examples: inputsthat are specially crafted to cause a machine learning model to produce anincorrect output. Adversarial examples that affect one model often affectanother model, even if the two models have different architectures or weretrained on different training sets, so long as both models were trained toperform the same task. An attacker may therefore train their own substitutemodel, craft adversarial examples against the substitute, and transfer them toa victim model, with very little information about the victim. Recent work hasfurther developed a technique that uses the victim model as an oracle to labela synthetic training set for the substitute, so the attacker need not evencollect a training set to mount the attack. We extend these recent techniquesusing reservoir sampling to greatly enhance the efficiency of the trainingprocedure for the substitute model. We introduce new transferability attacksbetween previously unexplored (substitute, victim) pairs of machine learningmodel classes, most notably SVMs and decision trees. We demonstrate our attackson two commercial machine learning classification systems from Amazon (96.19%misclassification rate) and Google (88.94%) using only 800 queries of thevictim model, thereby showing that existing machine learning approaches are ingeneral vulnerable to systematic black-box attacks regardless of theirstructure. version:1
arxiv-1605-07272 | Matrix Completion has No Spurious Local Minimum | http://arxiv.org/abs/1605.07272 | author:Rong Ge, Jason D. Lee, Tengyu Ma category:cs.LG cs.DS stat.ML  published:2016-05-24 summary:Matrix completion is a basic machine learning problem that has wideapplications, especially in collaborative filtering and recommender systems.Simple non-convex optimization algorithms are popular and effective inpractice. Despite recent progress in proving various non-convex algorithmsconverge from a good initial point, it remains unclear why random or arbitraryinitialization suffices in practice. We prove that the commonly used non-convexobjective function for matrix completion has no spurious local minima -- alllocal minima must also be global. Therefore, many popular optimizationalgorithms such as (stochastic) gradient descent can provably solve matrixcompletion with \textit{arbitrary} initialization in polynomial time. version:1
arxiv-1605-07270 | Learning a Metric Embedding for Face Recognition using the Multibatch Method | http://arxiv.org/abs/1605.07270 | author:Oren Tadmor, Yonatan Wexler, Tal Rosenwein, Shai Shalev-Shwartz, Amnon Shashua category:cs.CV  published:2016-05-24 summary:This work is motivated by the engineering task of achieving a nearstate-of-the-art face recognition on a minimal computing budget running on anembedded system. Our main technical contribution centers around a noveltraining method, called Multibatch, for similarity learning, i.e., for the taskof generating an invariant "face signature" through training pairs of "same"and "not-same" face images. The Multibatch method first generates signaturesfor a mini-batch of $k$ face images and then constructs an unbiased estimate ofthe full gradient by relying on all $k^2-k$ pairs from the mini-batch. We provethat the variance of the Multibatch estimator is bounded by $O(1/k^2)$, undersome mild conditions. In contrast, the standard gradient estimator that relieson random $k/2$ pairs has a variance of order $1/k$. The smaller variance ofthe Multibatch estimator significantly speeds up the convergence rate ofstochastic gradient descent. Using the Multibatch method we train a deepconvolutional neural network that achieves an accuracy of $98.2\%$ on the LFWbenchmark, while its prediction runtime takes only $30$msec on a single ARMCortex A9 core. Furthermore, the entire training process took only 12 hours ona single Titan X GPU. version:1
arxiv-1605-07268 | Classifying discourse in a CSCL platform to evaluate correlations with Teacher Participation and Progress | http://arxiv.org/abs/1605.07268 | author:Eliana Scheihing, Matthieu Vernier, Javiera Born, Julio Guerra, Luis Carcamo category:cs.CY cs.CL  published:2016-05-24 summary:In Computer-Supported learning, monitoring and engaging a group of learnersis a complex task for teachers, especially when learners are workingcollaboratively: Are my students motivated? What kind of progress are theymaking? Should I intervene? Is my communication and the didactic design adaptedto my students? Our hypothesis is that the analysis of natural languageinteractions between students, and between students and teachers, provide veryvaluable information and could be used to produce qualitative indicators tohelp teachers' decisions. We develop an automatic approach in three steps (1)to explore the discursive functions of messages in a CSCL platform, (2) toclassify the messages automatically and (3) to evaluate correlations betweendiscursive attitudes and other variables linked to the learning activity.Results tend to show that some types of discourse are correlated with a notionof Progress on the learning activities and the importance of emotiveparticipation from the Teacher. version:1
arxiv-1605-07264 | Trajectory probability hypothesis density filter | http://arxiv.org/abs/1605.07264 | author:Ángel F. García-Fernández, Lennart Svensson category:stat.AP cs.CV  published:2016-05-24 summary:This paper presents the probability hypothesis density (PHD) filter for setsof trajectories. The resulting filter, which is referred to as trajectoryprobability density filter (TPHD), is capable of estimating trajectories in aprincipled way without requiring to evaluate all measurement-to-targetassociation hypotheses. As the PHD filter, the TPHD filter is based onrecursively obtaining the best Poisson approximation to the multitrajectoryfiltering density in the sense of minimising the Kullback-Leibler divergence.We also propose a Gaussian mixture implementation of the TPHD recursion, theGaussian mixture TPHD (GMTPHD), and a computationally efficient implementation,the L-scan GMTPHD, which only updates the PDF of the trajectory states of thelast L time steps. version:1
arxiv-1605-07262 | Measuring Neural Net Robustness with Constraints | http://arxiv.org/abs/1605.07262 | author:Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, Antonio Criminisi category:cs.LG cs.CV cs.NE  published:2016-05-24 summary:Despite having high accuracy, neural nets have been shown to be susceptibleto adversarial examples, where a small perturbation to an input can cause it tobecome mislabeled. We propose metrics for measuring the robustness of a neuralnet and devise a novel algorithm for approximating these metrics based on anencoding of robustness as a linear program. We show how our metrics can be usedto evaluate the robustness of deep neural nets with experiments on the MNISTand CIFAR-10 datasets. Our algorithm generates more informative estimates ofrobustness metrics compared to estimates based on existing algorithms.Furthermore, we show how existing approaches to improving robustness "overfit"to adversarial examples generated using a specific algorithm. Finally, we showthat our techniques can be used to additionally improve neural net robustnessboth according to the metrics that we propose, but also according to previouslyproposed metrics. version:1
arxiv-1605-06892 | Accelerated Stochastic Mirror Descent Algorithms For Composite Non-strongly Convex Optimization | http://arxiv.org/abs/1605.06892 | author:Le Thi Khanh Hien, Canyi Lu, Huan Xu, Jiashi Feng category:math.OC stat.ML  published:2016-05-23 summary:We consider the problem of minimizing the sum of the average functionconsisting of a large number of smooth convex component functions and a generalconvex function that can be non-differentiable. Although many methods have beenproposed to solve the problem with the assumption that the sum is stronglyconvex, few methods support the non-strongly convex cases. Adding a smallquadratic regularization is the common trick used to tackle non-strongly convexproblems; however, it may worsen certain qualities of solutions or weaken theperformance of the algorithms. Avoiding this trick, we extend the deterministicaccelerated proximal gradient methods of Paul Tseng to randomized versions forsolving the problem without the strongly convex assumption. Our algorithmsachieve the optimal convergence rate $O(\nicefrac{1}{k^2})$. Tuning involvedparameters helps our algorithms get better complexity compared with thedeterministic accelerated proximal gradient methods. We also propose a schemefor non-smooth problem. version:2
arxiv-1605-07254 | Convergence guarantees for kernel-based quadrature rules in misspecified settings | http://arxiv.org/abs/1605.07254 | author:Motonobu Kanagawa, Bharath K. Sriperumbudur, Kenji Fukumizu category:stat.ML  published:2016-05-24 summary:Kernel-based quadrature rules are powerful tools for numerical integrationwhich yield convergence rates much faster than usual Mote Carlo methods. Theserules are constructed based on the assumption that the integrand has a certaindegree of smoothness, and this assumption is expressed as that the integrandbelongs to a certain reproducing kernel Hilbert space (RKHS). However, inpractice such an assumption can be violated, and no general theory has beenestablished for the convergence in such misspecified cases. In this paper, weprove that kernel quadrature rules can be consistent even when an integranddoes not belong to an assumed RKHS, i.e., when the integrand is less smooththan assumed. We derive convergence rates that depend on the (unknown)smoothness of the integrand, where the degree of smoothness is expressed viapowers of RKHSs or via Sobolev spaces. version:1
arxiv-1605-07252 | Interaction Screening: Efficient and Sample-Optimal Learning of Ising Models | http://arxiv.org/abs/1605.07252 | author:Marc Vuffray, Sidhant Misra, Andrey Y. Lokhov, Michael Chertkov category:cs.LG  published:2016-05-24 summary:We consider the problem of learning the underlying graph of an unknown Isingmodel on p spins from a collection of i.i.d. samples generated from the model.We suggest a new estimator that is computationally efficient and requires anumber of samples that is near-optimal with respect to previously establishedinformation-theoretic lower-bound. Our statistical estimator has a physicalinterpretation in terms of "interaction screening". The estimator is consistentand is efficiently implemented using convex optimization. We prove that withappropriate regularization, the estimator recovers the underlying graph using anumber of samples that is logarithmic in the system size p and exponential inthe maximum coupling-intensity and maximum node-degree. version:1
arxiv-1605-07251 | Dense CNN Learning with Equivalent Mappings | http://arxiv.org/abs/1605.07251 | author:Jianxin Wu, Chen-Wei Xie, Jian-Hao Luo category:cs.CV  published:2016-05-24 summary:Large receptive field and dense prediction are both important for achievinghigh accuracy in pixel labeling tasks such as semantic segmentation. These twoproperties, however, contradict with each other. A pooling layer (with stride2) quadruples the receptive field size but reduces the number of predictions to25\%. Some existing methods lead to dense predictions using computations thatare not equivalent to the original model. In this paper, we propose theequivalent convolution (eConv) and equivalent pooling (ePool) layers, leadingto predictions that are both dense and equivalent to the baseline CNN model.Dense prediction models learned using eConv and ePool can transfer the baselineCNN's parameters as a starting point, and can inverse transfer the learnedparameters in a dense model back to the original one, which has both fasttesting speed and high accuracy. The proposed eConv and ePool layers haveachieved higher accuracy than baseline CNN in various tasks, including semanticsegmentation, object localization, image categorization and apparent ageestimation, not only in those tasks requiring dense pixel labeling. version:1
arxiv-1605-07246 | Adaptive ADMM with Spectral Penalty Parameter Selection | http://arxiv.org/abs/1605.07246 | author:Zheng Xu, Mario A. T. Figueiredo, Thomas Goldstein category:cs.LG cs.AI cs.NA  published:2016-05-24 summary:The alternating direction method of multipliers (ADMM) is a versatile toolfor solving a wide range of constrained optimization problems, withdifferentiable or non-differentiable objective functions. Unfortunately, itsperformance is highly sensitive to a penalty parameter, which makes ADMM oftenunreliable and hard to automate for a non-expert user. We tackle this weaknessof ADMM by proposing a method to adaptively tune the penalty parameters toachieve fast convergence. The resulting adaptive ADMM (AADMM) algorithm,inspired by the successful Barzilai-Borwein spectral method for gradientdescent, yields fast convergence and relative insensitivity to the initialstepsize and problem scaling. version:1
arxiv-1605-07230 | Deep Portfolio Theory | http://arxiv.org/abs/1605.07230 | author:J. B. Heaton, N. G. Polson, J. H. Witte category:q-fin.PM cs.LG  published:2016-05-23 summary:We construct a deep portfolio theory. By building on Markowitz's classicrisk-return trade-off, we develop a self-contained four-step routine of encode,calibrate, validate and verify to formulate an automated and general portfolioselection process. At the heart of our algorithm are deep hierarchicalcompositions of portfolios constructed in the encoding step. The calibrationstep then provides multivariate payouts in the form of deep hierarchicalportfolios that are designed to target a variety of objective functions. Thevalidate step trades-off the amount of regularization used in the encode andcalibrate steps. The verification step uses a cross validation approach totrace out an ex post deep portfolio efficient frontier. We demonstrate all foursteps of our portfolio theory numerically. version:1
arxiv-1605-07174 | Kernel-based Reconstruction of Graph Signals | http://arxiv.org/abs/1605.07174 | author:Daniel Romero, Meng Ma, Georgios B. Giannakis category:stat.ML cs.LG  published:2016-05-23 summary:A number of applications in engineering, social sciences, physics, andbiology involve inference over networks. In this context, graph signals arewidely encountered as descriptors of vertex attributes or features ingraph-structured data. Estimating such signals in all vertices given noisyobservations of their values on a subset of vertices has been extensivelyanalyzed in the literature of signal processing on graphs (SPoG). This paperadvocates kernel regression as a framework generalizing popular SPoG modelingand reconstruction and expanding their capabilities. Formulating signalreconstruction as a regression task on reproducing kernel Hilbert spaces ofgraph signals permeates benefits from statistical learning, offers freshinsights, and allows for estimators to leverage richer forms of priorinformation than existing alternatives. A number of SPoG notions such asbandlimitedness, graph filters, and the graph Fourier transform are naturallyaccommodated in the kernel framework. Additionally, this paper capitalizes onthe so-called representer theorem to devise simpler versions of existingThikhonov regularized estimators, and offers a novel probabilisticinterpretation of kernel methods on graphs based on graphical models. Motivatedby the challenges of selecting the bandwidth parameter in SPoG estimators orthe kernel map in kernel-based methods, the present paper further proposes twomulti-kernel approaches with complementary strengths. Whereas the first enablesestimation of the unknown bandwidth of bandlimited signals, the second allowsfor efficient graph filter selection. Numerical tests with synthetic as well asreal data demonstrate the merits of the proposed methods relative tostate-of-the-art alternatives. version:1
arxiv-1605-07156 | Genetic Architect: Discovering Genomic Structure with Learned Neural Architectures | http://arxiv.org/abs/1605.07156 | author:Laura Deming, Sasha Targ, Nate Sauder, Diogo Almeida, Chun Jimmie Ye category:cs.LG cs.AI cs.NE stat.ML  published:2016-05-23 summary:Each human genome is a 3 billion base pair set of encoding instructions.Decoding the genome using deep learning fundamentally differs from most tasks,as we do not know the full structure of the data and therefore cannot designarchitectures to suit it. As such, architectures that fit the structure ofgenomics should be learned not prescribed. Here, we develop a novel searchalgorithm, applicable across domains, that discovers an optimal architecturewhich simultaneously learns general genomic patterns and identifies the mostimportant sequence motifs in predicting functional genomic outcomes. Thearchitectures we find using this algorithm succeed at using only RNA expressiondata to predict gene regulatory structure, learn human-interpretablevisualizations of key sequence motifs, and surpass state-of-the-art results onbenchmark genomics challenges. version:1
arxiv-1605-07154 | Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations | http://arxiv.org/abs/1605.07154 | author:Behnam Neyshabur, Yuhuai Wu, Ruslan Salakhutdinov, Nathan Srebro category:cs.LG cs.NE  published:2016-05-23 summary:We investigate the parameter-space geometry of recurrent neural networks(RNNs), and develop an adaptation of path-SGD optimization method, attuned tothis geometry, that can learn plain RNNs with ReLU activations. On severaldatasets that require capturing long-term dependency structure, we show thatpath-SGD can significantly improve trainability of ReLU RNNs compared to RNNstrained with SGD, even with various recently suggested initialization schemes. version:1
arxiv-1605-07148 | Backprop KF: Learning Discriminative Deterministic State Estimators | http://arxiv.org/abs/1605.07148 | author:Tuomas Haarnoja, Anurag Ajay, Sergey Levine, Pieter Abbeel category:cs.LG cs.AI  published:2016-05-23 summary:Generative state estimators based on probabilistic filters and smoothers areone of the most popular classes of state estimators for robots and autonomousvehicles. However, generative models have limited capacity to handle richsensory observations, such as camera images, since they must model the entiredistribution over sensor readings. Discriminative models do not suffer fromthis limitation, but are typically more complex to train as latent variablemodels for state estimation. We present an alternative approach where theparameters of the latent state distribution are directly optimized as adeterministic computation graph, resulting in a simple and effective gradientdescent algorithm for training discriminative state estimators. We show thatthis procedure can be used to train state estimators that use complex input,such as raw camera images, which must be processed using expressive nonlinearfunction approximators such as convolutional neural networks. Our model can beviewed as a type of recurrent neural network, and the connection toprobabilistic filtering allows us to design a network architecture that isparticularly well suited for state estimation. We evaluate our approach ontracking task with raw image inputs. The results show significant improvementover both standard generative approaches and regular recurrent neural networks. version:1
arxiv-1605-07147 | Fast stochastic optimization on Riemannian manifolds | http://arxiv.org/abs/1605.07147 | author:Hongyi Zhang, Sashank J. Reddi, Suvrit Sra category:math.OC cs.LG  published:2016-05-23 summary:We study optimization of finite sums of \emph{geodesically} smooth functionson Riemannian manifolds. Although variance reduction techniques for optimizingfinite-sum problems have witnessed a huge surge of interest in recent years,all existing work is limited to vector space problems. We introduce\emph{Riemannian SVRG}, a new variance reduced Riemannian optimization method.We analyze this method for both geodesically smooth \emph{convex} and\emph{nonconvex} functions. Our analysis reveals that Riemannian SVRG comeswith advantages of the usual SVRG method, but with factors depending onmanifold curvature that influence its convergence. To the best of ourknowledge, ours is the first \emph{fast} stochastic Riemannian method.Moreover, our work offers the first non-asymptotic complexity analysis fornonconvex Riemannian optimization (even for the batch setting). Our resultshave several implications; for instance, they offer a Riemannian perspective onvariance reduced PCA, which promises a short, transparent convergence analysis. version:1
arxiv-1605-07146 | Wide Residual Networks | http://arxiv.org/abs/1605.07146 | author:Sergey Zagoruyko, Nikos Komodakis category:cs.CV cs.LG cs.NE  published:2016-05-23 summary:Deep residual networks were shown to be able to scale up to thousands oflayers and still have improving performance. However, each fraction of apercent of improved accuracy costs nearly doubling the number of layers, and sotraining very deep residual networks has a problem of diminishing featurereuse, which makes these networks very slow to train. To tackle these problems,in this paper we conduct a detailed experimental study on the architecture ofResNet blocks, based on which we propose a novel architecture where we decreasedepth and increase width of residual networks. We call the resulting networkstructures wide residual networks (WRNs) and show that these are far superiorover their commonly used thin and very deep counterparts. For example, wedemonstrate that even a simple 16-layer-deep wide residual network outperformsin accuracy and efficiency all previous deep residual networks, includingthousand-layer-deep networks, achieving new state-of-the-art results onCIFAR-10, CIFAR-100 and SVHN. version:1
arxiv-1605-07145 | Towards Optimality Conditions for Non-Linear Networks | http://arxiv.org/abs/1605.07145 | author:Devansh Arpit, Hung Q. Ngo, Yingbo Zhou, Nils Napp, Venu Govindaraju category:stat.ML cs.LG cs.NE  published:2016-05-23 summary:Training non-linear neural networks is a challenging task, but over theyears, various approaches coming from different perspectives have been proposedto improve performance. However, insights into what fundamentally constitutes\textit{optimal} network parameters remains obscure. Similarly, given whatproperties of data can we hope for a non-linear network to learn is also notwell studied. In order to address these challenges, we take a novel approach byanalysing neural network from a data generating perspective, where we assumehidden layers generate the observed data. This perspective allows us to connectseemingly disparate approaches explored independently in the machine learningcommunity such as batch normalization, Independent Component Analysis,orthogonal weight initialization, etc, as parts of a bigger picture and provideinsights into non-linear networks in terms of properties of parameter and datathat lead to better performance. version:1
arxiv-1605-07139 | Fairness in Learning: Classic and Contextual Bandits | http://arxiv.org/abs/1605.07139 | author:Matthew Joseph, Michael Kearns, Jamie Morgenstern, Aaron Roth category:cs.LG stat.ML  published:2016-05-23 summary:We introduce the study of fairness in multi-armed bandit problems. Ourfairness definition can be interpreted as demanding that given a pool ofapplicants (say, for college admission or mortgages), a worse applicant isnever favored over a better one, despite a learning algorithm's uncertaintyover the true payoffs. We prove results of two types. First, in the important special case of the classic stochastic banditsproblem (i.e., in which there are no contexts), we provide a provably fairalgorithm based on "chained" confidence intervals, and provide a cumulativeregret bound with a cubic dependence on the number of arms. We further showthat any fair algorithm must have such a dependence. When combined with regretbounds for standard non-fair algorithms such as UCB, this proves a strongseparation between fair and unfair learning, which extends to the generalcontextual case. In the general contextual case, we prove a tight connection between fairnessand the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a classof functions can be transformed into a provably fair contextual banditalgorithm, and conversely any fair contextual bandit algorithm can betransformed into a KWIK learning algorithm. This tight connection allows us toprovide a provably fair algorithm for the linear contextual bandit problem witha polynomial dependence on the dimension, and to show (for a different class offunctions) a worst-case exponential gap in regret between fair and non-fairlearning algorithms version:1
arxiv-1605-07133 | Towards Multi-Agent Communication-Based Language Learning | http://arxiv.org/abs/1605.07133 | author:Angeliki Lazaridou, Nghia The Pham, Marco Baroni category:cs.CL cs.CV cs.LG  published:2016-05-23 summary:We propose an interactive multimodal framework for language learning. Insteadof being passively exposed to large amounts of natural text, our learners(implemented as feed-forward neural networks) engage in cooperative referentialgames starting from a tabula rasa setup, and thus develop their own languagefrom the need to communicate in order to succeed at the game. Preliminaryexperiments provide promising results, but also suggest that it is important toensure that agents trained in this way do not develop an adhoc communicationcode only effective for the game they are playing version:1
arxiv-1605-07129 | Sub-Gaussian estimators of the mean of a random matrix with heavy-tailed entries | http://arxiv.org/abs/1605.07129 | author:Stanislav Minsker category:math.ST stat.ML stat.TH  published:2016-05-23 summary:Estimation of the covariance matrix has attracted a lot of attention of thestatistical research community over the years, partially due to importantapplications such as Principal Component Analysis. However, frequently usedempirical covariance estimator (and its modifications) is very sensitive tooutliers in the data. As P. J. Huber wrote in 1964, "...This raises a questionwhich could have been asked already by Gauss, but which was, as far as I know,only raised a few years ago (notably by Tukey): what happens if the truedistribution deviates slightly from the assumed normal one? As is now wellknown, the sample mean then may have a catastrophically bad performance..."Motivated by this question, we develop a new estimator of the (element-wise)mean of a random matrix, which includes covariance estimation problem as aspecial case. Assuming that the entries of a matrix possess only finite secondmoment, this new estimator admits sub-Gaussian or sub-exponential concentrationaround the unknown mean in the operator norm. We will explain the key ideasbehind our construction, as well as applications to covariance estimation andmatrix completion problems. version:1
arxiv-1605-07127 | Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks | http://arxiv.org/abs/1605.07127 | author:Stefan Depeweg, José Miguel Hernández-Lobato, Finale Doshi-Velez, Steffen Udluft category:stat.ML cs.LG  published:2016-05-23 summary:We present an algorithm for model-based reinforcement learning that combinesBayesian neural networks (BNNs) with random roll-outs and stochasticoptimization for policy learning. The BNNs are trained by minimizing$\alpha$-divergences, allowing us to capture complicated statistical patternsin the transition dynamics, e.g. multi-modality and heteroskedasticity, whichare usually missed by other common modeling approaches. We illustrate theperformance of our method by solving a challenging benchmark where model-basedapproaches usually fail and by obtaining promising results in a real-worldscenario for controlling a gas turbine. version:1
arxiv-1605-07116 | A Formal Evaluation of PSNR as Quality Measurement Parameter for Image Segmentation Algorithms | http://arxiv.org/abs/1605.07116 | author:Fernando A. Fardo, Victor H. Conforto, Francisco C. de Oliveira, Paulo S. Rodrigues category:cs.CV  published:2016-05-23 summary:Quality evaluation of image segmentation algorithms are still subject ofdebate and research. Currently, there is no generic metric that could beapplied to any algorithm reliably. This article contains an evaluation for thePSRN (Peak Signal-To-Noise Ratio) as a metric which has been used to evaluatethreshold level selection as well as the number of thresholds in the case ofmulti-level segmentation. The results obtained in this study suggest that thePSNR is not an adequate quality measurement for segmentation algorithms. version:1
arxiv-1605-07110 | Deep Learning without Poor Local Minima | http://arxiv.org/abs/1605.07110 | author:Kenji Kawaguchi category:stat.ML cs.LG math.OC  published:2016-05-23 summary:In this paper, we prove a conjecture published in 1989 and also partiallyaddress an open problem announced at the Conference on Learning Theory (COLT)2015. For an expected loss function of a deep nonlinear neural network, weprove the following statements under the independence assumption adopted fromrecent work: 1) the function is non-convex and non-concave, 2) every localminimum is a global minimum, 3) every critical point that is not a globalminimum is a saddle point, and 4) the property of saddle points differs forshallow networks (with three layers) and deeper networks (with more than threelayers). Moreover, we prove that the same four statements hold for deep linearneural networks with any depth, any widths and no unrealistic assumptions. As aresult, we present an instance, for which we can answer to the followingquestion: how difficult to directly train a deep model in theory? It is moredifficult than the classical machine learning models (because of thenon-convexity), but not too difficult (because of the nonexistence of poorlocal minima and the property of the saddle points). We note that even thoughwe have advanced the theoretical foundations of deep learning, there is still agap between theory and practice. version:1
arxiv-1605-07104 | Generic Instance Search and Re-identification from One Example via Attributes and Categories | http://arxiv.org/abs/1605.07104 | author:Ran Tao, Arnold W. M. Smeulders, Shih-Fu Chang category:cs.CV  published:2016-05-23 summary:This paper aims for generic instance search from one example where theinstance can be an arbitrary object like shoes, not just near-planar andone-sided instances like buildings and logos. First, we evaluatestate-of-the-art instance search methods on this problem. We observe that whatworks for buildings loses its generality on shoes. Second, we propose to useautomatically learned category-specific attributes to address the largeappearance variations present in generic instance search. Searching amonginstances from the same category as the query, the category-specific attributesoutperform existing approaches by a large margin on shoes and cars and performon par with the state-of-the-art on buildings. Third, we treat personre-identification as a special case of generic instance search. On the popularVIPeR dataset, we reach state-of-the-art performance with the same method.Fourth, we extend our method to search objects without restriction to thespecifically known category. We show that the combination of category-levelinformation and the category-specific attributes is superior to the alternativemethod combining category-level information with low-level features such asFisher vector. version:1
arxiv-1605-07094 | Optimal Coding in Biological and Artificial Neural Networks | http://arxiv.org/abs/1605.07094 | author:Sebastian Weichwald, Tatiana Fomina, Bernhard Schölkopf, Moritz Grosse-Wentrup category:q-bio.NC cs.IT cs.LG math.IT stat.ML  published:2016-05-23 summary:Feature representations in both, biological neural networks in the primateventral stream and artificial convolutional neural networks trained on objectrecognition, incresase in complexity and receptive field size with layer depth.Somewhat strikingly, empirical evidence indicates that this analogy extends tothe specific representations learned in each layer. This suggests thatbiological and artificial neural networks share a fundamental organisingprinciple. We shed light on this principle in the framework of optimal coding.Specifically, we first investigate which properties of a code render it robustto transmission over noisy channels and formally prove that for equientropicchannels an upper bound on the expected minimum decoding error is attained forcodes with maximum marginal entropy. We then show that the pairwise correlationof units in a deep layer of a neural network, that has been trained on anobject recognition task, increases when perturbing the distribution of inputimages, i. e., that the network exhibits properties of an optimally codingsystem. By analogy, this suggests that the layer-wise similarity of featurerepresentations in biological and artificial neural networks is a result ofoptimal coding that enables robust transmission of object information overnoisy channels. Because we find that in equientropic channels the upper boundon the expected minimum decoding error is independent of the class-conditionalentropy, our work further provides a plausible explanation why optimal codescan be learned in unsupervised settings. version:1
arxiv-1605-07081 | Depth from a Single Image by Harmonizing Overcomplete Local Network Predictions | http://arxiv.org/abs/1605.07081 | author:Ayan Chakrabarti, Jingyu Shao, Gregory Shakhnarovich category:cs.CV  published:2016-05-23 summary:A single color image can contain many cues informative towards differentaspects of local geometric structure. We approach the problem of monoculardepth estimation by using a neural network to produce a mid-levelrepresentation that summarizes these cues. This network is trained tocharacterize local scene geometry by predicting, at every image location, depthderivatives of different orders, orientations and scales. However, instead of asingle estimate for each derivative, the network outputs probabilitydistributions that allow it to express confidence about some coefficients, andambiguity about others. Scene depth is then estimated by harmonizing thisovercomplete set of network predictions, using a globalization procedure thatfinds a single consistent depth map that best matches all the local derivativedistributions. We demonstrate the efficacy of this approach through evaluationon the NYU v2 depth data set. version:1
arxiv-1605-07079 | Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets | http://arxiv.org/abs/1605.07079 | author:Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, Frank Hutter category:cs.AI cs.LG stat.ML  published:2016-05-23 summary:Bayesian optimization has become a successful tool for hyperparameteroptimization of machine learning algorithms, such as support vector machines ordeep neural networks. But it is still costly if each evaluation of theobjective requires training and validating the algorithm being optimized,which, for large datasets, often takes hours, days, or even weeks. Toaccelerate hyperparameter optimization, we propose a generative model for thevalidation error as a function of training set size, which is learned duringthe optimization process and allows exploration of preliminary configurationson small subsets, by extrapolating to the full dataset. We construct a Bayesianoptimization procedure, dubbed FABOLAS, which models loss and training time asa function of dataset size and automatically trades off high information gainabout the global optimum against computational cost. Experiments optimizingsupport vector machines and deep neural networks show that FABOLAS often findshigh-quality solutions 10 to 100 times faster than other state-of-the-artBayesian optimization methods. version:1
arxiv-1605-07078 | Learning Sensor Multiplexing Design through Back-propagation | http://arxiv.org/abs/1605.07078 | author:Ayan Chakrabarti category:cs.LG stat.ML  published:2016-05-23 summary:Recent progress on many imaging and vision tasks has been driven by the useof deep feed-forward neural networks, which are trained by propagatinggradients of a loss defined on the final output, back through the network up tothe first layer that operates directly on the image. We proposeback-propagating one step further---to learn camera sensor designs jointly withnetworks that carry out inference on the images they capture. In this paper, wespecifically consider the design and inference problems in a typical colorcamera---where the sensor is able to measure only one color channel at eachpixel location, and computational inference is required to reconstruct a fullcolor image. We learn the camera sensor's color multiplexing pattern byencoding it as layer whose learnable weights determine which color channel,from among a fixed set, will be measured at each location. These weights arejointly trained with those of a reconstruction network that operates on thecorresponding sensor measurements to produce a full color image. Our networkachieves significant improvements in accuracy over the traditional Bayerpattern used in most color cameras. It automatically learns to employ a sparsecolor measurement approach similar to that of a recent design, and moreover,improves upon that design by learning an optimal layout for these measurements. version:1
arxiv-1605-07066 | A Unifying Framework for Sparse Gaussian Process Approximation using Power Expectation Propagation | http://arxiv.org/abs/1605.07066 | author:Thang D. Bui, Josiah Yan, Richard E. Turner category:stat.ML cs.LG  published:2016-05-23 summary:Pseudo-point based sparse approximation methods sidestep the computationaland analytical intractability in Gaussian process models at the cost ofaccuracy, reducing the complexity from cubic to linear in the number of datapoints. We propose a new sparse posterior approximation framework using PowerExpectation Propagation (Power EP), unifying various existing methods into onesingle computational and algorithmic viewpoint. This allows us to bring a newposterior approximation perspective to methods that are currently understood toperform prior approximation. Crucially, we demonstrate that the proposedapproach outperforms current sparse methods on regression, classification andstate space modelling by leveraging the interpolation between Power EP'sspecial cases, variational inference and EP. version:1
arxiv-1605-07061 | On Restricted Nonnegative Matrix Factorization | http://arxiv.org/abs/1605.07061 | author:Dmitry Chistikov, Stefan Kiefer, Ines Marušić, Mahsa Shirmohammadi, James Worrell category:cs.FL cs.CC cs.LG  published:2016-05-23 summary:Nonnegative matrix factorization (NMF) is the problem of decomposing a givennonnegative $n \times m$ matrix $M$ into a product of a nonnegative $n \timesd$ matrix $W$ and a nonnegative $d \times m$ matrix $H$. Restricted NMFrequires in addition that the column spaces of $M$ and $W$ coincide. Findingthe minimal inner dimension $d$ is known to be NP-hard, both for NMF andrestricted NMF. We show that restricted NMF is closely related to a questionabout the nature of minimal probabilistic automata, posed by Paz in his seminal1971 textbook. We use this connection to answer Paz's question negatively, thusfalsifying a positive answer claimed in 1974. Furthermore, we investigatewhether a rational matrix $M$ always has a restricted NMF of minimal innerdimension whose factors $W$ and $H$ are also rational. We show that this holdsfor matrices $M$ of rank at most $3$ and we exhibit a rank-$4$ matrix for which$W$ and $H$ require irrational entries. version:1
arxiv-1605-07057 | Bayesian Model Selection of Stochastic Block Models | http://arxiv.org/abs/1605.07057 | author:Xiaoran Yan category:stat.ML cs.LG cs.SI  published:2016-05-23 summary:A central problem in analyzing networks is partitioning them into modules orcommunities. One of the best tools for this is the stochastic block model,which clusters vertices into blocks with statistically homogeneous pattern oflinks. Despite its flexibility and popularity, there has been a lack ofprincipled statistical model selection criteria for the stochastic block model.Here we propose a Bayesian framework for choosing the number of blocks as wellas comparing it to the more elaborate degree- corrected block models,ultimately leading to a universal model selection framework capable ofcomparing multiple modeling combinations. We will also investigate itsconnection to the minimum description length principle. version:1
arxiv-1605-07051 | Convergence Analysis for Rectangular Matrix Completion Using Burer-Monteiro Factorization and Gradient Descent | http://arxiv.org/abs/1605.07051 | author:Qinqing Zheng, John Lafferty category:stat.ML cs.LG  published:2016-05-23 summary:We address the rectangular matrix completion problem by lifting the unknownmatrix to a positive semidefinite matrix in higher dimension, and optimizing anonconvex objective over the semidefinite factor using a simple gradientdescent scheme. With $O( \mu r^2 \kappa^2 n \max(\mu, \log n))$ randomobservations of a $n_1 \times n_2$ $\mu$-incoherent matrix of rank $r$ andcondition number $\kappa$, where $n = \max(n_1, n_2)$, the algorithm linearlyconverges to the global optimum with high probability. version:1
arxiv-1605-07026 | Spontaneous vs. Posed smiles - can we tell the difference? | http://arxiv.org/abs/1605.07026 | author:Bappaditya Mandal, Nizar Ouarti category:cs.CV cs.AI  published:2016-05-23 summary:Smile is an irrefutable expression that shows the physical state of the mindin both true and deceptive ways. Generally, it shows happy state of the mind,however, `smiles' can be deceptive, for example people can give a smile whenthey feel happy and sometimes they might also give a smile (in a different way)when they feel pity for others. This work aims to distinguish spontaneous(felt) smile expressions from posed (deliberate) smiles by extracting andanalyzing both global (macro) motion of the face and subtle (micro) changes inthe facial expression features through both tracking a series of facialfiducial markers as well as using dense optical flow. Specifically the eyes andlips features are captured and used for analysis. It aims to automaticallyclassify all smiles into either `spontaneous' or `posed' categories, by usingsupport vector machines (SVM). Experimental results on large database showpromising results as compared to other relevant methods. version:1
arxiv-1605-07025 | Tucker Gaussian Process for Regression and Collaborative Filtering | http://arxiv.org/abs/1605.07025 | author:Hyunjik Kim, Xiaoyu Lu, Seth Flaxman, Yee Whye Teh category:stat.ML cs.IR cs.LG  published:2016-05-23 summary:We introduce the Tucker Gaussian Process (TGP), an approach to scalable GPlearning based on low-rank tensor decompositions. We show that our model isapplicable to general regression problems, and is particularly well-suited togrid-structured data and problems where the dependence on covariates is closeto being separable. Furthermore, when applied to collaborative filtering, ourmodel provides an effective GP based method that has a low-rank matrixfactorisation at its core, and gives a natural and elegant method forincorporating side information. version:1
arxiv-1605-07018 | Online Learning with Feedback Graphs Without the Graphs | http://arxiv.org/abs/1605.07018 | author:Alon Cohen, Tamir Hazan, Tomer Koren category:cs.LG stat.ML  published:2016-05-23 summary:We study an online learning framework introduced by Mannor and Shamir (2011)in which the feedback is specified by a graph, in a setting where the graph mayvary from round to round and is \emph{never fully revealed} to the learner. Weshow a large gap between the adversarial and the stochastic cases. In theadversarial case, we prove that even for dense feedback graphs, the learnercannot improve upon a trivial regret bound obtained by ignoring any additionalfeedback besides her own loss. In contrast, in the stochastic case we give analgorithm that achieves $\widetilde \Theta(\sqrt{\alpha T})$ regret over $T$rounds, provided that the independence numbers of the hidden feedback graphsare at most $\alpha$. We also extend our results to a more general feedbackmodel, in which the learner does not necessarily observe her own loss, and showthat, even in simple cases, concealing the feedback graphs might render alearnable problem unlearnable. version:1
arxiv-1605-07003 | Image Restoration with Locally Selected Class-Adapted Models | http://arxiv.org/abs/1605.07003 | author:Afonso M. Teodoro, José M. Bioucas-Dias, Mário A. T. Figueiredo category:cs.CV 94A08  68U10  47N10 I.4.5; I.4.4  published:2016-05-23 summary:In this paper, we propose an automatic way to select class-adapted Gaussianmixture priors, in image denoising or deblurring tasks. We follow the Bayesianperspective and use a maximum a posteriori criterion to determine the modelthat best explains each observed patch. In situations where it is not possibleto learn a model from the observed image, e.g. blurred images, this approach,in general, leads to better results in comparison with using only a genericmodel. In particular, when the input image belongs to a class for which we havea pre-computed model, the improvement is more pronounced. version:1
arxiv-1605-06995 | Practical Privacy For Expectation Maximization | http://arxiv.org/abs/1605.06995 | author:Mijung Park, Jimmy Foulds, Kamalika Chaudhuri, Max Welling category:cs.LG cs.AI cs.CR stat.ME stat.ML  published:2016-05-23 summary:Expectation maximization (EM) is an iterative algorithm that computes maximumlikelihood and maximum a posteriori estimates for models with unobservedvariables. While widely used, the iterative nature of EM presents challengesfor privacy-preserving estimation. Multiple iterations are required to obtainaccurate parameter estimates, yet each iteration increases the amount of noisethat must be added to achieve a reasonable degree of privacy. We propose apractical algorithm that overcomes this challenge and outputs EM parameterestimates that are both accurate and private. Our algorithm focuses on thefrequent use case of models whose joint distribution over observed andunobserved variables remains in the exponential family. For these models, theEM parameters are functions of moments of the data. Our algorithm leveragesthis to preserve privacy by perturbing the moments, for which the amount ofadditive noise scales naturally with the data. In addition, our algorithm usesa relaxed notion of the differential privacy (DP) gold standard, calledconcentrated differential privacy (CDP). Rather than focusing on single-queryloss, CDP provides high probability bounds for cumulative privacy loss, whichis well suited for iterative algorithms. For mixture models, we show that ourmethod requires a significantly smaller privacy budget for the same estimationaccuracy compared to both DP and its (epsilon, delta)-DP relaxation. Ourgeneral approach of moment perturbation equipped with CDP can be readilyextended to many iterative machine learning algorithms, which opens up variousexciting future directions. version:1
arxiv-1605-06968 | A Riemannian gossip approach to decentralized matrix completion | http://arxiv.org/abs/1605.06968 | author:Bamdev Mishra, Hiroyuki Kasai, Atul Saroop category:cs.NA cs.LG math.OC  published:2016-05-23 summary:In this paper, we propose novel gossip algorithms for the low-rankdecentralized matrix completion problem. The proposed approach is on theRiemannian Grassmann manifold that allows local matrix completion by differentagents while achieving asymptotic consensus on the global low-rank factors. Theresulting approach is scalable and parallelizable. Our numerical experimentsshow the good performance of the proposed algorithms on various benchmarks. version:1
arxiv-1605-06955 | Beyond the Low-density Separation Principle: A Novel Approach to Semi-supervised Learning | http://arxiv.org/abs/1605.06955 | author:Tomoya Sakai, Marthinus Christoffel du Plessis, Gang Niu, Masashi Sugiyama category:cs.LG  published:2016-05-23 summary:Semi-supervised learning based on the low-density separation principle suchas the cluster and manifold assumptions has been extensively studied in thelast decades. However, such semi-supervised learning methods do not alwaysperform well due to violation of the cluster and manifold assumptions. In thispaper, we propose a novel approach to semi-supervised learning that does notrequire such restrictive assumptions. Our key idea is to combine learning frompositive and negative data (standard supervised learning) and learning frompositive and unlabeled data (PU learning), the latter is guaranteed to be ableto utilize unlabeled data without the cluster and manifold assumptions. Wetheoretically and experimentally show the usefulness of our approach. version:1
arxiv-1605-06931 | Learning the Structure of Nonlinear Dynamical Networks: An Information-Theoretic Perspective | http://arxiv.org/abs/1605.06931 | author:Oliver M. Cliff, Mikhail Prokopenko, Robert Fitch category:cs.LG cs.IT math.IT stat.ML  published:2016-05-23 summary:The behaviour of many real systems can be modelled by nonlinear dynamicalsystems whereby a latent system state is observed through a filter. We areinterested in interacting subsystems of this form, which we model as asynchronous graph dynamical systems. Specifically, we study the structurelearning problem for dynamical systems coupled via a directed acyclic graph.Unlike established structure learning procedures that find locally maximumposterior probabilities of a network structure containing latent variables, ourwork shows that we can exploit the properties of certain dynamical systems tocompute globally optimal approximations of these distributions. We arrive atthis result by use of time delay embedding theorems. Taking aninformation-theoretic perspective, we show that the log-likelihood has anintuitive interpretation in terms of information transfer. version:1
arxiv-1605-06921 | Generative Choreography using Deep Learning | http://arxiv.org/abs/1605.06921 | author:Luka Crnkovic-Friis, Louise Crnkovic-Friis category:cs.AI cs.LG cs.MM cs.NE  published:2016-05-23 summary:Recent advances in deep learning have enabled the extraction of high-levelfeatures from raw sensor data which has opened up new possibilities in manydifferent fields, including computer generated choreography. In this paper wepresent a system chor-rnn for generating novel choreographic material in thenuanced choreographic language and style of an individual choreographer. Italso shows promising results in producing a higher level compositionalcohesion, rather than just generating sequences of movement. At the core ofchor-rnn is a deep recurrent neural network trained on raw motion capture dataand that can generate new dance sequences for a solo dancer. Chor-rnn can beused for collaborative human-machine choreography or as a creative catalyst,serving as inspiration for a choreographer. version:1
arxiv-1605-06914 | Embedding based on function approximation for large scale image search | http://arxiv.org/abs/1605.06914 | author:Thanh-Toan Do, Ngai-Man Cheung category:cs.CV  published:2016-05-23 summary:The objective of this paper is to design an embedding method that maps localfeatures describing an image (e.g. SIFT) to a higher dimensional representationuseful for the image retrieval problem. First, motivated by the relationshipbetween the linear approximation of a nonlinear function in high dimensionalspace and the state of-the-art feature representation used in image retrieval,i.e., VLAD, we propose a new approach for the approximation. The embeddedvectors resulted by the function approximation process are then aggregated toform a single representation for image retrieval. Second, in order to make theproposed embedding method applicable to large scale problem, we further deriveits fast version in which the embedded vectors can be efficiently computed,i.e., in the closed-form. We compare the proposed embedding methods with thestate of the art in the context of image search under various settings: whenthe images are represented by medium length vectors, short vectors, or binaryvectors. The experimental results show that the proposed embedding methodsoutperform existing the state of the art on the standard public image retrievalbenchmarks. version:1
arxiv-1605-06900 | Fast Stochastic Methods for Nonsmooth Nonconvex Optimization | http://arxiv.org/abs/1605.06900 | author:Sashank J. Reddi, Suvrit Sra, Barnabas Poczos, Alex Smola category:math.OC cs.LG stat.ML  published:2016-05-23 summary:We analyze stochastic algorithms for optimizing nonconvex, nonsmoothfinite-sum problems, where the nonconvex part is smooth and the nonsmooth partis convex. Surprisingly, unlike the smooth case, our knowledge of thisfundamental problem is very limited. For example, it is not known whether theproximal stochastic gradient method with constant minibatch converges to astationary point. To tackle this issue, we develop fast stochastic algorithmsthat provably converge to a stationary point for constant minibatches.Furthermore, using a variant of these algorithms, we show provably fasterconvergence than batch proximal gradient descent. Finally, we prove globallinear convergence rate for an interesting subclass of nonsmooth nonconvexfunctions, that subsumes several recent works. This paper builds upon ourrecent series of papers on fast stochastic methods for smooth nonconvexoptimization [22, 23], with a novel analysis for nonconvex and nonsmoothfunctions. version:1
arxiv-1605-06894 | DLAU: A Scalable Deep Learning Accelerator Unit on FPGA | http://arxiv.org/abs/1605.06894 | author:Chao Wang, Qi Yu, Lei Gong, Xi Li, Yuan Xie, Xuehai Zhou category:cs.LG cs.DC cs.NE  published:2016-05-23 summary:As the emerging field of machine learning, deep learning shows excellentability in solving complex learning problems. However, the size of the networksbecomes increasingly large scale due to the demands of the practicalapplications, which poses significant challenge to construct a high performanceimplementations of deep learning neural networks. In order to improve theperformance as well to maintain the low power cost, in this paper we designDLAU, which is a scalable accelerator architecture for large-scale deeplearning networks using FPGA as the hardware prototype. The DLAU acceleratoremploys three pipelined processing units to improve the throughput and utilizestile techniques to explore locality for deep learning applications.Experimental results on the state-of-the-art Xilinx FPGA board demonstrate thatthe DLAU accelerator is able to achieve up to 36.1x speedup comparing to theIntel Core2 processors, with the power consumption at 234mW. version:1
arxiv-1605-06886 | Stochastic Patching Process | http://arxiv.org/abs/1605.06886 | author:Xuhui Fan, Bin Li, Yi Wang, Yang Wang, Fang Chen category:cs.AI stat.ML  published:2016-05-23 summary:Stochastic partition models tailor a product space into a number ofrectangular regions such that the data within each region exhibit certain typesof homogeneity. Due to constraints of partition strategy, existing models maycause unnecessary dissections in sparse regions when fitting data in denseregions. To alleviate this limitation, we propose a parsimonious partitionmodel, named Stochastic Patching Process (SPP), to deal with multi-dimensionalarrays. SPP adopts an "enclosing" strategy to attach rectangular patches todense regions. SPP is self-consistent such that it can be extended to infinitearrays. We apply SPP to relational modeling and the experimental resultsvalidate its merit compared to the state-of-the-arts. version:1
arxiv-1605-06885 | Bridging Category-level and Instance-level Semantic Image Segmentation | http://arxiv.org/abs/1605.06885 | author:Zifeng Wu, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2016-05-23 summary:We propose an approach to instance-level image segmentation that is built ontop of category-level segmentation. Specifically, for each pixel in a semanticcategory mask, its corresponding instance bounding box is predicted using adeep fully convolutional regression network. Thus it follows a differentpipeline to the popular detect-then-segment approaches that first predictinstances' bounding boxes, which are the current state-of-the-art in instancesegmentation. We show that, by leveraging the strength of our state-of-the-artsemantic segmentation models, the proposed method can achieve comparable oreven better results to detect-then-segment approaches. We make the followingcontributions. (i) First, we propose a simple yet effective approach tosemantic instance segmentation. (ii) Second, we propose an online bootstrappingmethod during training, which is critically important for achieving goodperformance for both semantic category segmentation and instance-levelsegmentation. (iii) As the performance of semantic category segmentation has asignificant impact on the instance-level segmentation, which is the second stepof our approach, we train fully convolutional residual networks to achieve thebest semantic category segmentation accuracy. On the PASCAL VOC 2012 dataset,we obtain the currently best mean intersection-over-union score of 79.1%. (iv)We also achieve state-of-the-art results for instance-level segmentation. version:1
arxiv-1605-06878 | Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Image Recognition | http://arxiv.org/abs/1605.06878 | author:Xiu-Shen Wei, Chen-Wei Xie, Jianxin Wu category:cs.CV  published:2016-05-23 summary:Fine-grained image recognition is a challenging computer vision problem, dueto the small inter-class variations caused by highly similar subordinatecategories, and the large intra-class variations in poses, scales androtations. In this paper, we propose a novel end-to-end Mask-CNN model withoutthe fully connected layers for fine-grained recognition. Based on the partannotations of fine-grained images, the proposed model consists of a fullyconvolutional network to both locate the discriminative parts (e.g., head andtorso), and more importantly generate object/part masks for selecting usefuland meaningful convolutional descriptors. After that, a four-stream Mask-CNNmodel is built for aggregating the selected object- and part-level descriptorssimultaneously. The proposed Mask-CNN model has the smallest number ofparameters, lowest feature dimensionality and highest recognition accuracy whencompared with state-of-the-arts fine-grained approaches. version:1
arxiv-1605-06863 | Self-expressive Dictionary Learning for Dynamic 3D Reconstruction | http://arxiv.org/abs/1605.06863 | author:Enliang Zheng, Dinghuang Ji, Enrique Dunn, Jan-Michael Frahm category:cs.CV  published:2016-05-22 summary:We target the problem of sparse 3D reconstruction of dynamic objects observedby multiple unsynchronized video cameras with unknown temporal overlap. To thisend, we develop a framework to recover the unknown structure without sequencinginformation across video sequences. Our proposed compressed sensing frameworkposes the estimation of 3D structure as the problem of dictionary learning,where the dictionary is defined as an aggregation of the temporally varying 3Dstructures. Given the smooth motion of dynamic objects, we observe any elementin the dictionary can be well approximated by a sparse linear combination ofother elements in the same dictionary (i. e. self-expression). Moreover, thesparse coefficients describing a locally linear 3D structural interpolationreveal the local sequencing information. Our formulation optimizes a biconvexcost function that leverages a compressed sensing formulation and enforces bothstructural dependency coherence across video streams, as well as motionsmoothness across estimates from common video sources. We further analyze thereconstructability of our approach under different capture scenarios, and itscomparison and relation to existing methods. Experimental results on largeamounts of synthetic data as well as real imagery demonstrate the effectivenessof our approach. version:1
arxiv-1605-06855 | Smart broadcasting: Do you want to be seen? | http://arxiv.org/abs/1605.06855 | author:Mohammad Reza Karimi, Erfan Tavakoli, Mehrdad Farajtabar, Le Song, Manuel Gomez-Rodriguez category:cs.SI cs.LG stat.ML  published:2016-05-22 summary:Many users in online social networks are constantly trying to gain attentionfrom their followers by broadcasting posts to them. These broadcasters arelikely to gain greater attention if their posts can remain visible for a longerperiod of time among their followers' most recent feeds. Then when to post? Inthis paper, we study the problem of smart broadcasting using the framework oftemporal point processes, where we model users feeds and posts as discreteevents occurring in continuous time. Based on such continuous-time model, thenchoosing a broadcasting strategy for a user becomes a problem of designing theconditional intensity of her posting events. We derive a novel formula whichlinks this conditional intensity with the visibility of the user in herfollowers' feeds. Furthermore, by exploiting this formula, we develop anefficient convex optimization framework for the when-to-post problem. Ourmethod can find broadcasting strategies that reach a desired visibility levelwith provable guarantees. We experimented with data gathered from Twitter, andshow that our framework can consistently make broadcasters' post more visiblethan alternatives. version:1
arxiv-1605-06848 | Nonnegative Matrix Factorization Requires Irrationality | http://arxiv.org/abs/1605.06848 | author:Dmitry Chistikov, Stefan Kiefer, Ines Marušić, Mahsa Shirmohammadi, James Worrell category:cs.CC cs.LG math.NA  published:2016-05-22 summary:Nonnegative matrix factorization (NMF) is the problem of decomposing a givennonnegative $n \times m$ matrix $M$ into a product of a nonnegative $n \timesd$ matrix $W$ and a nonnegative $d \times m$ matrix $H$. A longstanding openquestion, posed by Cohen and Rothblum in 1993, is whether a rational matrix $M$always has an NMF of minimal inner dimension $d$ whose factors $W$ and $H$ arealso rational. We answer this question negatively, by exhibiting a matrix forwhich $W$ and $H$ require irrational entries. version:1
arxiv-1605-06838 | Causality on Longitudinal Data: Stable Specification Search in Constrained Structural Equation Modeling | http://arxiv.org/abs/1605.06838 | author:Ridho Rahmadi, Perry Groot, Marieke MHC van Rijn, Jan AJG van den Brand, Marianne Heins, Hans Knoop, Tom Heskes, the Alzheimer's Dise, the MASTERPLAN Study Group, the OPTIMISTIC Consortium category:stat.ML cs.AI  published:2016-05-22 summary:Developing causal models from observational longitudinal studies is animportant, ubiquitous problem in many disciplines. In the medical domain,especially in the case of rare diseases, revealing causal relationships from agiven data set may lead to improvement of clinical practice, e.g., developmentof treatment and medication. Many causal discovery methods have been introducedin the past decades. A disadvantage of these causal discovery algorithms,however, is the inherent instability in structure estimation. With finite datasamples small changes in the data can lead to completely different optimalstructures. The present work presents a new causal discovery algorithm forlongitudinal data that is robust for finite data samples. The method works asfollows. We model causal models using structural equation models. Models arescored along two objectives: the model fit and the model complexity. Since bothobjectives are often conflicting we use a multi-objective evolutionaryalgorithm to search for Pareto optimal models. To handle the instability ofsmall finite data samples, we repeatedly subsample the data and select thosesubstructures (from optimal models) that are both stable and parsimonious whichare then used to infer a causal model. In order to validate, we compare ourmethod with the state-of-the-art PC algorithm on a simulated data set with theknown ground truth model. Furthermore, we present the results of our discoveryalgorithm on three real-world longitudinal data sets about chronic fatiguesyndrome, Alzheimer disease and chronic kidney disease that have beencorroborated by medical experts and literature. version:1
arxiv-1605-06820 | Automated Resolution Selection for Image Segmentation | http://arxiv.org/abs/1605.06820 | author:Fares Al-Qunaieer, Hamid R. Tizhoosh, Shahryar Rahnamayan category:cs.CV  published:2016-05-22 summary:It is well-known in image processing that computational cost increasesrapidly with the number and dimensions of the images to be processed. Severalfields, such as medical imaging, routinely use numerous very large images,which might also be 3D and/or captured at several frequency bands, all addingto the computational expense. Multiresolution analysis is a method ofincreasing the efficiency of the segmentation process. One multiresolutionapproach is the coarse-to-fine segmentation strategy, whereby the segmentationstarts at a coarse resolution and is then fine-tuned during subsequent steps.The starting resolution for segmentation is generally selected arbitrarily withno clear selection criteria. The research reported in this paper showed thatstarting from different resolutions for image segmentation results in differentaccuracies and computational times, even for images of the same category(depicting similar scenes or objects). An automated method for resolutionselection for an input image would thus be beneficial. This paper introduces aframework for the automated selection of the best resolution for imagesegmentation. We propose a measure for defining the best resolution based onuser/system criteria, offering a trade-off between accuracy and computationtime. A learning approach is then introduced for the selection of theresolution, whereby extracted image features are mapped to the previouslydetermined best resolution. In the learning process, class (i.e., resolution)distribution is generally imbalanced, making effective learning from the datadifficult. Experiments conducted with three datasets using two differentsegmentation algorithms show that the resolutions selected through learningenable much faster segmentation than the original ones, while retaining atleast the original accuracy. version:1
arxiv-1605-06799 | Textual Paralanguage and Its Implications for Marketing Communications | http://arxiv.org/abs/1605.06799 | author:Andrea Webb Luangrath, Joann Peck, Victor A. Barger category:cs.CL cs.SI  published:2016-05-22 summary:Both face-to-face communication and communication in online environmentsconvey information beyond the actual verbal message. In a traditionalface-to-face conversation, paralanguage, or the ancillary meaning- andemotion-laden aspects of speech that are not actual verbal prose, givescontextual information that allows interactors to more appropriately understandthe message being conveyed. In this paper, we conceptualize textualparalanguage (TPL), which we define as written manifestations of nonverbalaudible, tactile, and visual elements that supplement or replace writtenlanguage and that can be expressed through words, symbols, images, punctuation,demarcations, or any combination of these elements. We develop a typology oftextual paralanguage using data from Twitter, Facebook, and Instagram. Wepresent a conceptual framework of antecedents and consequences of brands' useof textual paralanguage. Implications for theory and practice are discussed. version:1
arxiv-1605-06796 | Interpretable Distribution Features with Maximum Testing Power | http://arxiv.org/abs/1605.06796 | author:Wittawat Jitkrittum, Zoltan Szabo, Kacper Chwialkowski, Arthur Gretton category:stat.ML cs.LG 46E22  62G10 G.3; I.2.6  published:2016-05-22 summary:Two semimetrics on probability distributions are proposed, given as the sumof differences of expectations of analytic functions evaluated at spatial orfrequency locations (i.e, features). The features are chosen so as to maximizethe distinguishability of the distributions, by optimizing a lower bound ontest power for a statistical test using these features. The result is aparsimonious and interpretable indication of how and where two distributionsdiffer locally. An empirical estimate of the test power criterion convergeswith increasing sample size, ensuring the quality of the returned features. Inreal-world benchmarks on high-dimensional text and image data, linear-timetests using the proposed semimetrics achieve comparable performance to thestate-of-the-art quadratic-time maximum mean discrepancy test, while returninghuman-interpretable features that explain the test results. version:1
arxiv-1605-06792 | Active Nearest-Neighbor Learning in Metric Spaces | http://arxiv.org/abs/1605.06792 | author:Aryeh Kontorovich, Sivan Sabato, Ruth Urner category:cs.LG math.ST stat.TH  published:2016-05-22 summary:We propose a pool-based non-parametric active learning algorithm for generalmetric spaces, called called MArgin Regularized Metric Active Nearest Neighbor(MARMANN), which outputs a nearest-neighbor classifier. We give predictionerror guarantees that depend on the noisy-margin properties of the inputsample, and are competitive with those obtained by previously proposed passivelearners. We prove that the label complexity of MARMANN is significantly lowerthan that of any passive learner with similar error guarantees. Our algorithmis based on a generalized sample compression scheme and a new label-efficientactive model-selection procedure. version:1
arxiv-1605-06778 | openXBOW - Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit | http://arxiv.org/abs/1605.06778 | author:Maximilian Schmitt, Björn W. Schuller category:cs.CV cs.CL cs.IR  published:2016-05-22 summary:We introduce openXBOW, an open-source toolkit for the generation ofbag-of-words (BoW) representations from multimodal input. In the BoW principle,word histograms were first used as features in document classification, but theidea was and can easily be adapted to, e.g., acoustic or visual low-leveldescriptors, introducing a prior step of vector quantisation. The openXBOWtoolkit supports arbitrary numeric input features and text input andconcatenates computed subbags to a final bag. It provides a variety ofextensions and options. To our knowledge, openXBOW is the first publiclyavailable toolkit for the generation of crossmodal bags-of-words. Thecapabilities of the tool are exemplified in two sample scenarios:time-continuous speech-based emotion recognition and sentiment analysis intweets where improved results over other feature representation forms wereobserved. version:1
arxiv-1605-06776 | Sparse Signal Reconstruction with Multiple Side Information using Adaptive Weights for Multiview Sources | http://arxiv.org/abs/1605.06776 | author:Huynh Van Luong, Jürgen Seiler, André Kaup, Søren Forchhammer category:cs.CV math.OC  published:2016-05-22 summary:This work considers reconstructing a target signal in a context ofdistributed sparse sources. We propose an efficient reconstruction algorithmwith the aid of other given sources as multiple side information (SI). Theproposed algorithm takes advantage of compressive sensing (CS) with SI andadaptive weights by solving a proposed weighted $n$-$\ell_{1}$ minimization.The proposed algorithm computes the adaptive weights in two levels, first eachindividual intra-SI and then inter-SI weights are iteratively updated at everyreconstructed iteration. This two-level optimization leads the proposedreconstruction algorithm with multiple SI using adaptive weights (RAMSIA) torobustly exploit the multiple SIs with different qualities. We experimentallyperform our algorithm on generated sparse signals and also correlated featurehistograms as multiview sparse sources from a multiview image database. Theresults show that RAMSIA significantly outperforms both classical CS and CSwith single SI, and RAMSIA with higher number of SIs gained more than the onewith smaller number of SIs. version:1
arxiv-1605-06770 | Automatic Construction of Discourse Corpora for Dialogue Translation | http://arxiv.org/abs/1605.06770 | author:Longyue Wang, Xiaojun Zhang, Zhaopeng Tu, Andy Way, Qun Liu category:cs.CL  published:2016-05-22 summary:In this paper, a novel approach is proposed to automatically constructparallel discourse corpus for dialogue machine translation. Firstly, theparallel subtitle data and its corresponding monolingual movie script data arecrawled and collected from Internet. Then tags such as speaker and discourseboundary from the script data are projected to its subtitle data via aninformation retrieval approach in order to map monolingual discourse tobilingual texts. We not only evaluate the mapping results, but also integratespeaker information into the translation. Experiments show our proposed methodcan achieve 81.79% and 98.64% accuracy on speaker and dialogue boundaryannotation, and speaker-based language model adaptation can obtain around 0.5BLEU points improvement in translation qualities. Finally, we publicly releasearound 100K parallel discourse data with manual speaker and dialogue boundaryannotation. version:1
arxiv-1605-06764 | 3D Face Tracking and Texture Fusion in the Wild | http://arxiv.org/abs/1605.06764 | author:Patrik Huber, Philipp Kopp, Matthias Rätsch, William Christmas, Josef Kittler category:cs.CV 68T45  published:2016-05-22 summary:We present a fully automatic approach to real-time 3D face reconstructionfrom monocular in-the-wild videos. With the use of a cascaded-regressor basedface tracking and a 3D Morphable Face Model shape fitting, we obtain asemi-dense 3D face shape. We further use the texture information from multipleframes to build a holistic 3D face representation from the video frames. Oursystem is able to capture facial expressions and does not require anyperson-specific training. We demonstrate the robustness of our approach on thechallenging 300 Videos in the Wild (300-VW) dataset. Our real-time fittingframework is available as an open source library at http://4dface.org. version:1
arxiv-1605-06743 | Inductive Bias of Deep Convolutional Networks through Pooling Geometry | http://arxiv.org/abs/1605.06743 | author:Nadav Cohen, Amnon Shashua category:cs.NE cs.LG  published:2016-05-22 summary:Our formal understanding of the inductive bias that drives the success ofconvolutional networks on computer vision tasks is limited. In particular, itis unclear what makes hypotheses spaces born from convolution and poolingoperations so suitable for natural images. In this paper we study the abilityof convolutional arithmetic circuits to model correlations among regions oftheir input. Correlations are formalized through the notion of separation rank,which for a given input partition, measures how far a function is from beingseparable. We show that a polynomially sized deep network supportsexponentially high separation ranks for certain input partitions, while beinglimited to polynomial separation ranks for others. The network's poolinggeometry effectively determines which input partitions are favored, thus servesas a means for controlling the inductive bias. Contiguous pooling windows ascommonly employed in practice favor interleaved partitions over coarse ones,orienting the inductive bias towards the statistics of natural images. Inaddition to analyzing deep networks, we show that shallow ones support onlylinear separation ranks, and by this gain insight into the benefit of functionsbrought forth by depth - they are able to efficiently model strong correlationunder favored partitions of the input. version:1
arxiv-1605-06742 | A Rapid Pattern-Recognition Method for Driving Types Using Clustering-Based Support Vector Machines | http://arxiv.org/abs/1605.06742 | author:Wenshuo Wang, Junqiang Xi category:stat.ML cs.CV cs.LG  published:2016-05-22 summary:A rapid pattern-recognition approach to characterize driver'scurve-negotiating behavior is proposed. To shorten the recognition time andimprove the recognition of driving styles, a k-means clustering-based supportvector machine ( kMC-SVM) method is developed and used for classifying driversinto two types: aggressive and moderate. First, vehicle speed and throttleopening are treated as the feature parameters to reflect the driving styles.Second, to discriminate driver curve-negotiating behaviors and reduce thenumber of support vectors, the k-means clustering method is used to extract andgather the two types of driving data and shorten the recognition time. Then,based on the clustering results, a support vector machine approach is utilizedto generate the hyperplane for judging and predicting to which types the humandriver are subject. Lastly, to verify the validity of the kMC-SVM method, across-validation experiment is designed and conducted. The research resultsshow that the $ k $MC-SVM is an effective method to classify driving styleswith a short time, compared with SVM method. version:1
arxiv-1605-06722 | Hybrid evolutionary algorithm with extreme machine learning fitness function evaluation for two-stage capacitated facility location problem | http://arxiv.org/abs/1605.06722 | author:Peng Guo, Wenming Cheng, Yi Wang category:math.OC cs.NE  published:2016-05-22 summary:This paper considers the two-stage capacitated facility location problem(TSCFLP) in which products manufactured in plants are delivered to customersvia storage depots. Customer demands are satisfied subject to limited plantproduction and limited depot storage capacity. The objective is to determinethe locations of plants and depots in order to minimize the total costincluding the fixed cost and transportation cost. A hybrid evolutionaryalgorithm (HEA) with genetic operations and local search is proposed. To avoidthe expensive calculation of fitness of population in terms of computationaltime, the HEA uses extreme machine learning to approximate the fitness of mostof the individuals. Moreover, two heuristics based on the characteristic of theproblem is incorporated to generate a good initial population. Computational experiments are performed on two sets of test instances fromthe recent literature. The performance of the proposed algorithm is evaluatedand analyzed. Compared with the state-of-the-art genetic algorithm, theproposed algorithm can find the optimal or near-optimal solutions in areasonable computational time. version:1
arxiv-1605-06718 | The De-Biased Whittle Likelihood for Second-Order Stationary Stochastic Processes | http://arxiv.org/abs/1605.06718 | author:Adam M. Sykulski, Sofia C. Olhede, Jonathan M. Lilly category:stat.ME math.ST stat.CO stat.ML stat.TH  published:2016-05-22 summary:The Whittle likelihood is a computationally efficient pseudo-maximumlikelihood inference procedure which is known to produce biased parameterestimates for large classes of time series models. We propose a method forde-biasing Whittle likelihood parameter estimates for second-order stationarystochastic processes. We demonstrate how to compute the de-biased Whittlelikelihood in the same $\mathcal{O}(n\log n)$ computational efficiency asstandard Whittle likelihood. We prove that the method is consistent, anddemonstrate its superior performance in simulation studies. We also demonstratehow the method can be easily combined with standard methods of bias reduction,such as tapering and differencing, to further reduce bias in parameterestimates. version:1
arxiv-1605-06715 | Factored Temporal Sigmoid Belief Networks for Sequence Learning | http://arxiv.org/abs/1605.06715 | author:Jiaming Song, Zhe Gan, Lawrence Carin category:stat.ML cs.LG  published:2016-05-22 summary:Deep conditional generative models are developed to simultaneously learn thetemporal dependencies of multiple sequences. The model is designed byintroducing a three-way weight tensor to capture the multiplicativeinteractions between side information and sequences. The proposed model buildson the Temporal Sigmoid Belief Network (TSBN), a sequential stack of SigmoidBelief Networks (SBNs). The transition matrices are further factored to reducethe number of parameters and improve generalization. When side information isnot available, a general framework for semi-supervised learning based on theproposed model is constituted, allowing robust sequence classification.Experimental results show that the proposed approach achieves state-of-the-artpredictive and classification performance on sequential data, and has thecapacity to synthesize sequences, with controlled style transitioning andblending. version:1
arxiv-1605-06714 | Evolutionary Demographic Algorithms | http://arxiv.org/abs/1605.06714 | author:Marco AR Erra, Pedro MM Mitra, Agostinho C Rosa category:cs.NE  published:2016-05-22 summary:Most of the problems in genetic algorithms are very complex and demand alarge amount of resources that current technology can not offer. Our purposewas to develop a Java-JINI distributed library that implements GeneticAlgorithms with sub-populations (coarse grain) and a graphical interface inorder to configure and follow the evolution of the search. The sub-populationsare simulated/evaluated in personal computers connected trough a network,keeping in mind different models of sub-populations, migration policies andnetwork topologies. We show that this model delays the convergence of thepopulation keeping a higher level of genetic diversity and allows a muchgreater number of evaluations since they are distributed among severalcomputers compared with the traditional Genetic Algorithms. version:1
arxiv-1605-06711 | Learning From Hidden Traits: Joint Factor Analysis and Latent Clustering | http://arxiv.org/abs/1605.06711 | author:Bo Yang, Xiao Fu, Nicholas D. Sidiropoulos category:cs.LG  published:2016-05-21 summary:Dimensionality reduction techniques play an essential role in data analytics,signal processing and machine learning. Dimensionality reduction is usuallyperformed in a preprocessing stage that is separate from subsequent dataanalysis, such as clustering or classification. Finding reduced-dimensionrepresentations that are well-suited for the intended task is more appealing.This paper proposes a joint factor analysis and latent clustering framework,which aims at learning cluster-aware low-dimensional representations of matrixand tensor data. The proposed approach leverages matrix and tensorfactorization models that produce essentially unique latent representations ofthe data to unravel latent cluster structure -- which is otherwise obscuredbecause of the freedom to apply an oblique transformation in latent space. Atthe same time, latent cluster structure is used as prior information to enhancethe performance of factorization. Specific contributions include severalcustom-built problem formulations, corresponding algorithms, and discussion ofassociated convergence properties. Besides extensive simulations, real-worlddatasets such as Reuters document data and MNIST image data are also employedto showcase the effectiveness of the proposed approaches. version:1
arxiv-1605-06710 | Chess Player by Co-Evolutionary Algorithm | http://arxiv.org/abs/1605.06710 | author:Nuno Ramos, Sergio Salgado, Agostinho C Rosa category:cs.NE  published:2016-05-21 summary:A co-evolutionary algorithm (CA) based chess player is presented.Implementation details of the algorithms, namely coding, population, variationoperators are described. The alpha-beta or mini-max like behaviour of theplayer is achieved through two competitive or cooperative populations. Specialattention is given to the fitness function evaluation (the heart of thesolution). Test results on algorithms vs. algorithms or human player isprovided. version:1
arxiv-1605-06708 | Automatic Detection of Epileptiform Discharges in the EEG | http://arxiv.org/abs/1605.06708 | author:Andre Rosado, Agostinho C Rosa category:cs.CV  published:2016-05-21 summary:The diagnosis of epilepsy generally includes a visual inspection of EEGrecorded data by the Neurologist, with the purpose of checking the occurrenceof transient waveforms called interictal epileptiform discharges. Thesewaveforms have short duration (less than 100 ms), so the inspection process isusually time-consuming, particularly for ambulatory long term EEG records.Therefore, an automatic detection system of epileptiform discharges can be avaluable tool for a Neurology service. The proposed approach is the developmentof a multi stage detection algorithm, which processes the complete EEG signalsand applies decision criteria to selected waveforms. It employs EEG analysistechniques such as Wavelet Transform and Mimetic Analysis, complemented with aclassification based on Fuzzy Logic. In order to evaluate the algorithm'sperformance, data were collected from several epileptic patients, withepileptiform activity marked by a Neurologist. The average values obtained forboth Sensitivity and Specificity were respectively higher than 80 and 70percent. version:1
arxiv-1605-06695 | Fine-to-coarse Knowledge Transfer For Low-Res Image Classification | http://arxiv.org/abs/1605.06695 | author:Xingchao Peng, Judy Hoffman, Stella X. Yu, Kate Saenko category:cs.CV  published:2016-05-21 summary:We address the difficult problem of distinguishing fine-grained objectcategories in low resolution images. Wepropose a simple an effective deeplearning approach that transfers fine-grained knowledge gained from highresolution training data to the coarse low-resolution test scenario. Suchfine-to-coarse knowledge transfer has many real world applications, such asidentifying objects in surveillance photos or satellite images where the imageresolution at the test time is very low but plenty of high resolution photos ofsimilar objects are available. Our extensive experiments on two standardbenchmark datasets containing fine-grained car models and bird speciesdemonstrate that our approach can effectively transfer fine-detail knowledge tocoarse-detail imagery. version:1
arxiv-1605-06693 | Efficient Document Indexing Using Pivot Tree | http://arxiv.org/abs/1605.06693 | author:Gaurav Singh, Benjamin Piwowarski category:cs.IR cs.LG  published:2016-05-21 summary:We present a novel method for efficiently searching top-k neighbors fordocuments represented in high dimensional space of terms based on the cosinesimilarity. Mostly, documents are stored as bag-of-words tf-idf representation.One of the most used ways of computing similarity between a pair of documentsis cosine similarity between the vector representations, but cosine similarityis not a metric distance measure as it doesn't follow triangle inequality,therefore most metric searching methods can not be applied directly. We proposean efficient method for indexing documents using a pivot tree that leads toefficient retrieval. We also study the relation between precision andefficiency for the proposed method and compare it with a state of the art inthe area of document searching based on inner product. version:1
arxiv-1605-06673 | Cross Domain Adaptation by Learning Partially Shared Classifiers and Weighting Source Data Points in the Shared Subspaces | http://arxiv.org/abs/1605.06673 | author:Hongqi Wang, Anfeng Xu, Shanshan Wang, Sunny Chughtai category:cs.LG  published:2016-05-21 summary:Transfer learning is a problem defined over two domains. These two domainsshare the same feature space and class label space, but have significantlydifferent distributions. One domain has sufficient labels, named as sourcedomain, and the other domain has few labels, named as target do- main. Theproblem is to learn a effective classifier for the target domain. In thispaper, we propose a novel transfer learning method for this problem by learninga partially shared classifier for the target domain, and weighting the sourcedomain data points. We learn some shared subspaces for both the data points ofthe two domains, and a shared classifier in the shared subspaces. We hope thatin the shared subspaces, the distributions of two domain can match each otherwell, and to match the distributions, we weight the source domain data pointswith different weighting factors. Moreover, we adapt the shared classifier toeach domain by learning different adaptation functions. To learn the subspacetransformation matrices, the classifier parameters, and the adaptationparameters, we build a objective function with weighted clas- sificationerrors, parameter regularization, local reconstruction regularization, anddistribution matching. This objective function is minimized by an itera- tivealgorithm. Experiments show its effectiveness over benchmark data sets,including travel destination review data set, face expression data set, spamemail data set, etc. version:1
arxiv-1605-06652 | Bending the Curve: Improving the ROC Curve Through Error Redistribution | http://arxiv.org/abs/1605.06652 | author:Oran Richman, Shie Mannor category:cs.LG  published:2016-05-21 summary:Classification performance is often not uniform over the data. Some areas inthe input space are easier to classify than others. Features that holdinformation about the "difficulty" of the data may be non-discriminative andare therefore disregarded in the classification process. We propose ameta-learning approach where performance may be improved by post-processing.This improvement is done by establishing a dynamic threshold on thebase-classifier results. Since the base-classifier is treated as a "black box"the method presented can be used on any state of the art classifier in order totry an improve its performance. We focus our attention on how to better controlthe true-positive/false-positive trade-off known as the ROC curve. We proposean algorithm for the derivation of optimal thresholds by redistributing theerror depending on features that hold information about difficulty. Wedemonstrate the resulting benefit on both synthetic and real-life data. version:1
arxiv-1605-06651 | Gambler's Ruin Bandit Problem | http://arxiv.org/abs/1605.06651 | author:Nima Akbarzadeh, Cem Tekin category:cs.LG  published:2016-05-21 summary:In this paper, we propose a new sequential decision making problem called{\em gambler's ruin bandit problem} (GRBP). In each round of the GRBP thelearner faces a gambler's ruin problem with two possible actions: a {\emcontinuation action} that moves the learner randomly over the state spacearound the current state; and a {\em terminal action} that moves the learnerdirectly into one of the two terminal states (goal and dead-end state). Thecurrent round ends when a terminal state is reached. We first formulate GRBP asan optimization problem, and prove that the optimal policy is characterized bya simple threshold rule. The problem is solved for infinite time budget. Then,we consider the case when the state transition probabilities are unknown andprovide logarithmic problem specific regret bounds. We also identify acondition under which the learner only incurs finite regret. Numerousapplications including optimal medical treatment assignment can be formulatedas a GRBP, in which the continuation action corresponds to the conservativetreatment and the terminal action corresponds to the surgery. version:1
arxiv-1605-06650 | Latent Tree Models for Hierarchical Topic Detection | http://arxiv.org/abs/1605.06650 | author:Peixian Chen, Nevin L. Zhang, Tengfei Liu, Leonard K. M. Poon, Zhourong Chen category:cs.CL cs.IR cs.LG  published:2016-05-21 summary:We propose a novel method for hierarchical topic detection where topics areobtained by clustering documents in multiple ways. Specifically, we modeldocument collections using a class of graphical models called hierarchicallatent tree models (HLTMs). The variables at the bottom level of an HLTM areobserved binary variables that represent the presence/absence of words in adocument. The variables at other levels are discrete latent variables, withthose at the second level representing word co-occurrence patterns and those athigher levels representing co-occurrence of patterns at the level below. Eachlatent variable gives a soft partition of the documents, and document clustersin the partitions are interpreted as topics. Latent variables at high levels ofthe hierarchy capture long-range word co-occurrence patterns and hence givethematically more general topics, while those at low levels of the hierarchycapture short-range word co-occurrence patterns and give thematically morespecific topics. Compared with LDA-based topic models, a key advantage of HLTMsis that they, as graphical models, explicitly model the dependence andindependence structure among topics and words, which is conducive to thediscovery of meaningful topics and topic hierarchies. version:1
arxiv-1605-06640 | Programming with a Differentiable Forth Interpreter | http://arxiv.org/abs/1605.06640 | author:Sebastian Riedel, Matko Bošnjak, Tim Rocktäschel category:cs.NE cs.AI cs.LG  published:2016-05-21 summary:There are families of neural networks that can learn to compute any function,provided sufficient training data. However, given that in practice trainingdata is scarce for all but a small set of problems, a core question is how toincorporate prior knowledge into a model. Here we consider the case of priorprocedural knowledge such as knowing the overall recursive structure of asequence transduction program or the fact that a program will likely usearithmetic operations on real numbers to solve a task. To this end we present adifferentiable interpreter for the programming language Forth. Through a neuralimplementation of the dual stack machine that underlies Forth, programmers canwrite program sketches with slots that can be filled with learnable behaviour.As the program interpreter is end-to-end differentiable, we can optimize thisbehaviour directly through gradient descent techniques on user specifiedobjectives, and also integrate the program into any larger neural computationgraph. We show empirically that our interpreter is able to effectively leveragedifferent levels of prior program structure and learn complex transductiontasks such as sequence sorting or addition with substantially less data andbetter generalisation over problem sizes. In addition, we introduce neuralprogram optimisations based on symbolic computation and parallel branching thatlead to significant speed improvements. version:1
arxiv-1605-06636 | Deep Transfer Learning with Joint Adaptation Networks | http://arxiv.org/abs/1605.06636 | author:Mingsheng Long, Jianmin Wang, Michael I. Jordan category:cs.LG stat.ML  published:2016-05-21 summary:Deep networks rely on massive amounts of labeled data to learn powerfulmodels. For a target task short of labeled data, transfer learning enablesmodel adaptation from a different source domain. This paper addresses deeptransfer learning under a more general scenario that the joint distributions offeatures and labels may change substantially across domains. Based on thetheory of Hilbert space embedding of distributions, a novel joint distributiondiscrepancy is proposed to directly compare joint distributions across domains,eliminating the need of marginal-conditional factorization. Transfer learningis enabled in deep convolutional networks, where the dataset shifts may lingerin multiple task-specific feature layers and the classifier layer. A set ofjoint adaptation networks are crafted to match the joint distributions of theselayers across domains by minimizing the joint distribution discrepancy, whichcan be trained efficiently using back-propagation. Experiments show that thenew approach yields state of the art results on standard domain adaptationdatasets. version:1
arxiv-1605-06619 | Make Workers Work Harder: Decoupled Asynchronous Proximal Stochastic Gradient Descent | http://arxiv.org/abs/1605.06619 | author:Yitan Li, Linli Xu, Xiaowei Zhong, Qing Ling category:math.OC cs.DC cs.LG stat.ML  published:2016-05-21 summary:Asynchronous parallel optimization algorithms for solving large-scale machinelearning problems have drawn significant attention from academia to industryrecently. This paper proposes a novel algorithm, decoupled asynchronousproximal stochastic gradient descent (DAP-SGD), to minimize an objectivefunction that is the composite of the average of multiple empirical losses anda regularization term. Unlike the traditional asynchronous proximal stochasticgradient descent (TAP-SGD) in which the master carries much of the computationload, the proposed algorithm off-loads the majority of computation tasks fromthe master to workers, and leaves the master to conduct simple additionoperations. This strategy yields an easy-to-parallelize algorithm, whoseperformance is justified by theoretical convergence analyses. To be specific,DAP-SGD achieves an $O(\log T/T)$ rate when the step-size is diminishing and anergodic $O(1/\sqrt{T})$ rate when the step-size is constant, where $T$ is thenumber of total iterations. version:1
arxiv-1605-06597 | Adaptive Algorithm and Platform Selection for Visual Detection and Tracking | http://arxiv.org/abs/1605.06597 | author:Shu Zhang, Qi Zhu, Amit Roy-Chowdhury category:cs.CV  published:2016-05-21 summary:Computer vision algorithms are known to be extremely sensitive to theenvironmental conditions in which the data is captured, e.g., lightingconditions and target density. Tuning of parameters or choosing a completelynew algorithm is often needed to achieve a certain performance level,especially when there is a limitation of the computation source. In this paper,we focus on this problem and propose a framework to adaptively select the"best" algorithm-parameter combination and the computation platform underperformance and cost constraints at design time, and adapt the algorithms atruntime based on real-time inputs. This necessitates developing a mechanism toswitch between different algorithms as the nature of the input video changes.Our proposed algorithm calculates a similarity function between a test videoscenario and each training scenario, where the similarity calculation is basedon learning a manifold of image features that is shared by both the trainingand test datasets. Similarity between training and test dataset indicates thesame algorithm can be applied to both of them and achieve similar performance.We design a cost function with this similarity measure to find the most similartraining scenario to the test data. The "best" algorithm under a given platformis obtained by selecting the algorithm with a specific parameter combinationthat performs the best on the corresponding training data. The proposedframework can be used first offline to choose the platform based on performanceand cost constraints, and then online whereby the "best" algorithm is selectedfor each new incoming video segment for a given platform. In the experiments,we apply our algorithm to the problems of pedestrian detection and tracking. Weshow how to adaptively select platforms and algorithm-parameter combinations.Our results provide optimal performance on 3 publicly available datasets. version:1
arxiv-1605-06595 | WAHRSIS: A Low-cost, High-resolution Whole Sky Imager With Near-Infrared Capabilities | http://arxiv.org/abs/1605.06595 | author:Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler category:astro-ph.IM cs.CV  published:2016-05-21 summary:Cloud imaging using ground-based whole sky imagers is essential for afine-grained understanding of the effects of cloud formations, which can beuseful in many applications. Some such imagers are available commercially, buttheir cost is relatively high, and their flexibility is limited. Therefore, webuilt a new daytime Whole Sky Imager (WSI) called Wide Angle High-ResolutionSky Imaging System. The strengths of our new design are its simplicity, lowmanufacturing cost and high resolution. Our imager captures the entirehemisphere in a single high-resolution picture via a digital camera using afish-eye lens. The camera was modified to capture light across the visible aswell as the near-infrared spectral ranges. This paper describes the design ofthe device as well as the geometric and radiometric calibration of the imagingsystem. version:1
arxiv-1605-06593 | Influence Maximization with Semi-Bandit Feedback | http://arxiv.org/abs/1605.06593 | author:Zheng Wen, Branislav Kveton, Michal Valko category:cs.LG cs.SI stat.ML  published:2016-05-21 summary:We study a stochastic online problem of learning to influence in a socialnetwork with semi-bandit feedback, individual observations of how influencedusers influence others. Our problem combines challenges of partial monitoring,because the learning agent only observes the influenced portion of the network,and combinatorial bandits, because the cardinality of the feasible set isexponential in the maximum number of influencers. We propose a computationallyefficient UCB-like algorithm for solving our problem, IMLinUCB, and analyze iton forests. Our regret bounds are polynomial in all quantities of interest;reflect the structure of the network; and do not depend on inherently largequantities, such as the reciprocal of the minimum probability of beinginfluenced and the cardinality of the action set. To the best of our knowledge,these are the first such results. IMLinUCB permits linear generalization andtherefore is suitable for large-scale problems. We evaluate IMLinUCB on severalsynthetic problems and observe that the regret of IMLinUCB scales as suggestedby our upper bounds. A special form of our problem can be viewed as a linearbandit and we match the regret bounds of LinUCB in this case. version:1
arxiv-1605-06561 | DynaNewton - Accelerating Newton's Method for Machine Learning | http://arxiv.org/abs/1605.06561 | author:Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann category:cs.LG  published:2016-05-20 summary:Newton's method is a fundamental technique in optimization with quadraticconvergence within a neighborhood around the optimum. However reaching thisneighborhood is often slow and dominates the computational costs. We exploittwo properties specific to empirical risk minimization problems to accelerateNewton's method, namely, subsampling training data and increasing strongconvexity through regularization. We propose a novel continuation method, wherewe define a family of objectives over increasing sample sizes and withdecreasing regularization strength. Solutions on this path are tracked suchthat the minimizer of the previous objective is guaranteed to be within thequadratic convergence region of the next objective to be optimized. Therebyevery Newton iteration is guaranteed to achieve super-linear contractions withregard to the chosen objective, which becomes a moving target. We provide atheoretical analysis that motivates our algorithm, called DynaNewton, andcharacterizes its speed of convergence. Experiments on a wide range of datasets and problems consistently confirm the predicted computational savings. version:1
arxiv-1605-06560 | Functional Hashing for Compressing Neural Networks | http://arxiv.org/abs/1605.06560 | author:Lei Shi, Shikun Feng, ZhifanZhu category:cs.LG cs.NE  published:2016-05-20 summary:As the complexity of deep neural networks (DNNs) trend to grow to absorb theincreasing sizes of data, memory and energy consumption has been receiving moreand more attentions for industrial applications, especially on mobile devices.This paper presents a novel structure based on functional hashing to compressDNNs, namely FunHashNN. For each entry in a deep net, FunHashNN uses multiplelow-cost hash functions to fetch values in the compression space, and thenemploys a small reconstruction network to recover that entry. Thereconstruction network is plugged into the whole network and trained jointly.FunHashNN includes the recently proposed HashedNets as a degenerated case, andbenefits from larger value capacity and less reconstruction loss. We furtherdiscuss extensions with dual space hashing and multi-hops. On several benchmarkdatasets, FunHashNN demonstrates high compression ratios with little loss onprediction accuracy. version:1
arxiv-1605-06523 | TensorLog: A Differentiable Deductive Database | http://arxiv.org/abs/1605.06523 | author:William W. Cohen category:cs.AI cs.DB cs.LG  published:2016-05-20 summary:Large knowledge bases (KBs) are useful in many tasks, but it is unclear howto integrate this sort of knowledge into "deep" gradient-based learningsystems. To address this problem, we describe a probabilistic deductivedatabase, called TensorLog, in which reasoning uses a differentiable process.In TensorLog, each clause in a logical theory is first converted into certaintype of factor graph. Then, for each type of query to the factor graph, themessage-passing steps required to perform belief propagation (BP) are"unrolled" into a function, which is differentiable. We show that thesefunctions can be composed recursively to perform inference in non-triviallogical theories containing multiple interrelated clauses and predicates. Bothcompilation and inference in TensorLog are efficient: compilation is linear intheory size and proof depth, and inference is linear in database size and thenumber of message-passing steps used in BP. We also present experimentalresults with TensorLog and discuss its relationship to other first-orderprobabilistic logics. version:1
