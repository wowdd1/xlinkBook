arxiv-8700-1 | Every LWF and AMP chain graph originates from a set of causal models | http://arxiv.org/pdf/1312.2967v3.pdf | author:Jose M. Peña category:stat.ML published:2013-12-10 summary:This paper aims at justifying LWF and AMP chain graphs by showing that theydo not represent arbitrary independence models. Specifically, we show thatevery chain graph is inclusion optimal wrt the intersection of the independencemodels represented by a set of directed and acyclic graphs under conditioning.This implies that the independence model represented by the chain graph can beaccounted for by a set of causal models that are subject to selection bias,which in turn can be accounted for by a system that switches between differentregimes or configurations.
arxiv-8700-2 | A* Sampling | http://arxiv.org/pdf/1411.0030v2.pdf | author:Chris J. Maddison, Daniel Tarlow, Tom Minka category:stat.CO stat.ML published:2014-10-31 summary:The problem of drawing samples from a discrete distribution can be convertedinto a discrete optimization problem. In this work, we show how sampling from acontinuous distribution can be converted into an optimization problem overcontinuous space. Central to the method is a stochastic process recentlydescribed in mathematical statistics that we call the Gumbel process. Wepresent a new construction of the Gumbel process and A* sampling, a practicalgeneric sampling algorithm that searches for the maximum of a Gumbel processusing A* search. We analyze the correctness and convergence time of A* samplingand demonstrate empirically that it makes more efficient use of bound andlikelihood evaluations than the most closely related adaptive rejectionsampling-based algorithms.
arxiv-8700-3 | Parallel Graph Partitioning for Complex Networks | http://arxiv.org/pdf/1404.4797v3.pdf | author:Henning Meyerhenke, Peter Sanders, Christian Schulz category:cs.DC cs.DS cs.NE cs.SI physics.soc-ph published:2014-04-18 summary:Processing large complex networks like social networks or web graphs hasrecently attracted considerable interest. In order to do this in parallel, weneed to partition them into pieces of about equal size. Unfortunately, previousparallel graph partitioners originally developed for more regular mesh-likenetworks do not work well for these networks. This paper addresses this problemby parallelizing and adapting the label propagation technique originallydeveloped for graph clustering. By introducing size constraints, labelpropagation becomes applicable for both the coarsening and the refinement phaseof multilevel graph partitioning. We obtain very high quality by applying ahighly parallel evolutionary algorithm to the coarsened graph. The resultingsystem is both more scalable and achieves higher quality than state-of-the-artsystems like ParMetis or PT-Scotch. For large complex networks the performancedifferences are very big. For example, our algorithm can partition a web graphwith 3.3 billion edges in less than sixteen seconds using 512 cores of a highperformance cluster while producing a high quality partition -- none of thecompeting systems can handle this graph on our system.
arxiv-8700-4 | On a Family of Decomposable Kernels on Sequences | http://arxiv.org/pdf/1501.06284v1.pdf | author:Andrea Baisero, Florian T. Pokorny, Carl Henrik Ek category:cs.LG published:2015-01-26 summary:In many applications data is naturally presented in terms of orderings ofsome basic elements or symbols. Reasoning about such data requires a notion ofsimilarity capable of handling sequences of different lengths. In this paper wedescribe a family of Mercer kernel functions for such sequentially structureddata. The family is characterized by a decomposable structure in terms ofsymbol-level and structure-level similarities, representing a specificcombination of kernels which allows for efficient computation. We provide anexperimental evaluation on sequential classification tasks comparing kernelsfrom our family of kernels to a state of the art sequence kernel called theGlobal Alignment kernel which has been shown to outperform Dynamic Time Warping
arxiv-8700-5 | Deep Transductive Semi-supervised Maximum Margin Clustering | http://arxiv.org/pdf/1501.06237v1.pdf | author:Gang Chen category:cs.LG 68T10 I.2.6 published:2015-01-26 summary:Semi-supervised clustering is an very important topic in machine learning andcomputer vision. The key challenge of this problem is how to learn a metric,such that the instances sharing the same label are more likely close to eachother on the embedded space. However, little attention has been paid to learnbetter representations when the data lie on non-linear manifold. Fortunately,deep learning has led to great success on feature learning recently. Inspiredby the advances of deep learning, we propose a deep transductivesemi-supervised maximum margin clustering approach. More specifically, givenpairwise constraints, we exploit both labeled and unlabeled data to learn anon-linear mapping under maximum margin framework for clustering analysis.Thus, our model unifies transductive learning, feature learning and maximummargin techniques in the semi-supervised clustering framework. We pretrain thedeep network structure with restricted Boltzmann machines (RBMs) layer by layergreedily, and optimize our objective function with gradient descent. Bychecking the most violated constraints, our approach updates the modelparameters through error backpropagation, in which deep features are learnedautomatically. The experimental results shows that our model is significantlybetter than the state of the art on semi-supervised clustering.
arxiv-8700-6 | Online Optimization : Competing with Dynamic Comparators | http://arxiv.org/pdf/1501.06225v1.pdf | author:Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, Karthik Sridharan category:cs.LG math.OC stat.ML published:2015-01-26 summary:Recent literature on online learning has focused on developing adaptivealgorithms that take advantage of a regularity of the sequence of observations,yet retain worst-case performance guarantees. A complementary direction is todevelop prediction methods that perform well against complex benchmarks. Inthis paper, we address these two directions together. We present a fullyadaptive method that competes with dynamic benchmarks in which regret guaranteescales with regularity of the sequence of cost functions and comparators.Notably, the regret bound adapts to the smaller complexity measure in theproblem environment. Finally, we apply our results to drifting zero-sum,two-player games where both players achieve no regret guarantees against bestsequences of actions in hindsight.
arxiv-8700-7 | A Topic Modeling Approach to Ranking | http://arxiv.org/pdf/1412.3705v3.pdf | author:Weicong Ding, Prakash Ishwar, Venkatesh Saligrama category:cs.LG stat.ML published:2014-12-11 summary:We propose a topic modeling approach to the prediction of preferences inpairwise comparisons. We develop a new generative model for pairwisecomparisons that accounts for multiple shared latent rankings that areprevalent in a population of users. This new model also captures inconsistentuser behavior in a natural way. We show how the estimation of latent rankingsin the new generative model can be formally reduced to the estimation of topicsin a statistically equivalent topic modeling problem. We leverage recentadvances in the topic modeling literature to develop an algorithm that canlearn shared latent rankings with provable consistency as well as sample andcomputational complexity guarantees. We demonstrate that the new approach isempirically competitive with the current state-of-the-art approaches inpredicting preferences on some semi-synthetic and real world datasets.
arxiv-8700-8 | Randomized sketches for kernels: Fast and optimal non-parametric regression | http://arxiv.org/pdf/1501.06195v1.pdf | author:Yun Yang, Mert Pilanci, Martin J. Wainwright category:stat.ML cs.DS cs.LG stat.CO published:2015-01-25 summary:Kernel ridge regression (KRR) is a standard method for performingnon-parametric regression over reproducing kernel Hilbert spaces. Given $n$samples, the time and space complexity of computing the KRR estimate scale as$\mathcal{O}(n^3)$ and $\mathcal{O}(n^2)$ respectively, and so is prohibitivein many cases. We propose approximations of KRR based on $m$-dimensionalrandomized sketches of the kernel matrix, and study how small the projectiondimension $m$ can be chosen while still preserving minimax optimality of theapproximate KRR estimate. For various classes of randomized sketches, includingthose based on Gaussian and randomized Hadamard matrices, we prove that itsuffices to choose the sketch dimension $m$ proportional to the statisticaldimension (modulo logarithmic factors). Thus, we obtain fast and minimaxoptimal approximations to the KRR estimate for non-parametric regression.
arxiv-8700-9 | Exploring Human Vision Driven Features for Pedestrian Detection | http://arxiv.org/pdf/1501.06180v1.pdf | author:Shanshan Zhang, Christian Bauckhage, Dominik A. Klein, Armin B. Cremers category:cs.CV published:2015-01-25 summary:Motivated by the center-surround mechanism in the human visual attentionsystem, we propose to use average contrast maps for the challenge of pedestriandetection in street scenes due to the observation that pedestrians indeedexhibit discriminative contrast texture. Our main contributions are first todesign a local, statistical multi-channel descriptorin order to incorporateboth color and gradient information. Second, we introduce a multi-direction andmulti-scale contrast scheme based on grid-cells in order to integrateexpressive local variations. Contributing to the issue of selecting mostdiscriminative features for assessing and classification, we perform extensivecomparisons w.r.t. statistical descriptors, contrast measurements, and scalestructures. This way, we obtain reasonable results under variousconfigurations. Empirical findings from applying our optimized detector on theINRIA and Caltech pedestrian datasets show that our features yieldstate-of-the-art performance in pedestrian detection.
arxiv-8700-10 | Discrete and fuzzy dynamical genetic programming in the XCSF learning classifier system | http://arxiv.org/pdf/1201.5604v2.pdf | author:Richard J. Preen, Larry Bull category:cs.AI cs.LG cs.NE cs.SY math.OC published:2012-01-26 summary:A number of representation schemes have been presented for use withinlearning classifier systems, ranging from binary encodings to neural networks.This paper presents results from an investigation into using discrete and fuzzydynamical system representations within the XCSF learning classifier system. Inparticular, asynchronous random Boolean networks are used to represent thetraditional condition-action production system rules in the discrete case andasynchronous fuzzy logic networks in the continuous-valued case. It is shownpossible to use self-adaptive, open-ended evolution to design an ensemble ofsuch dynamical systems within XCSF to solve a number of well-known testproblems.
arxiv-8700-11 | An Occlusion Reasoning Scheme for Monocular Pedestrian Tracking in Dynamic Scenes | http://arxiv.org/pdf/1501.06129v1.pdf | author:Sourav Garg, Swagat Kumar, Rajesh Ratnakaram, Prithwijit Guha category:cs.CV published:2015-01-25 summary:This paper looks into the problem of pedestrian tracking using a monocular,potentially moving, uncalibrated camera. The pedestrians are located in eachframe using a standard human detector, which are then tracked in subsequentframes. This is a challenging problem as one has to deal with complexsituations like changing background, partial or full occlusion and cameramotion. In order to carry out successful tracking, it is necessary to resolveassociations between the detected windows in the current frame with thoseobtained from the previous frame. Compared to methods that use temporal windowsincorporating past as well as future information, we attempt to make decisionon a frame-by-frame basis. An occlusion reasoning scheme is proposed to resolvethe association problem between a pair of consecutive frames by using anaffinity matrix that defines the closeness between a pair of windows and then,uses a binary integer programming to obtain unique association between them. Asecond stage of verification based on SURF matching is used to deal with thosecases where the above optimization scheme might yield wrong associations. Theefficacy of the approach is demonstrated through experiments on severalstandard pedestrian datasets.
arxiv-8700-12 | Prediction Error Reduction Function as a Variable Importance Score | http://arxiv.org/pdf/1501.06116v1.pdf | author:Ernest Fokoué category:stat.ML 62H25, 62H30 published:2015-01-25 summary:This paper introduces and develops a novel variable importance score functionin the context of ensemble learning and demonstrates its appeal boththeoretically and empirically. Our proposed score function is simple and morestraightforward than its counterpart proposed in the context of random forest,and by avoiding permutations, it is by design computationally more efficientthan the random forest variable importance function. Just like the randomforest variable importance function, our score handles both regression andclassification seamlessly. One of the distinct advantage of our proposed scoreis the fact that it offers a natural cut off at zero, with all the positivescores indicating importance and significance, while the negative scores aredeemed indications of insignificance. An extra advantage of our proposed scorelies in the fact it works very well beyond ensemble of trees and can seamlesslybe used with any base learners in the random subspace learning context. Ourexamples, both simulated and real, demonstrate that our proposed score doescompete mostly favorably with the random forest score.
arxiv-8700-13 | Semi-blind Source Separation via Sparse Representations and Online Dictionary Learning | http://arxiv.org/pdf/1212.0451v2.pdf | author:Sirisha Rambhatla, Jarvis D. Haupt category:cs.SD stat.AP stat.ML published:2012-12-03 summary:This work examines a semi-blind single-channel source separation problem. Ourspecific aim is to separate one source whose local structure is approximatelyknown, from another a priori unspecified background source, given only a singlelinear combination of the two sources. We propose a separation technique basedon local sparse approximations along the lines of recent efforts in sparserepresentations and dictionary learning. A key feature of our procedure is theonline learning of dictionaries (using only the data itself) to sparsely modelthe background source, which facilitates its separation from thepartially-known source. Our approach is applicable to source separationproblems in various application domains; here, we demonstrate the performanceof our proposed approach via simulation on a stylized audio source separationtask.
arxiv-8700-14 | A simpler condition for consistency of a kernel independence test | http://arxiv.org/pdf/1501.06103v1.pdf | author:Arthur Gretton category:stat.ML published:2015-01-25 summary:A statistical test of independence may be constructed using theHilbert-Schmidt Independence Criterion (HSIC) as a test statistic. The HSIC isdefined as the distance between the embedding of the joint distribution, andthe embedding of the product of the marginals, in a Reproducing Kernel HilbertSpace (RKHS). It has previously been shown that when the kernel used indefining the joint embedding is characteristic (that is, the embedding of thejoint distribution to the feature space is injective), then the HSIC-based testis consistent. In particular, it is sufficient for the product of kernels onthe individual domains to be characteristic on the joint domain. In this note,it is established via a result of Lyons (2013) that HSIC-based independencetests are consistent when kernels on the marginals are characteristic on theirrespective domains, even when the product of kernels is not characteristic onthe joint domain.
arxiv-8700-15 | Between Pure and Approximate Differential Privacy | http://arxiv.org/pdf/1501.06095v1.pdf | author:Thomas Steinke, Jonathan Ullman category:cs.DS cs.CR cs.LG published:2015-01-24 summary:We show a new lower bound on the sample complexity of $(\varepsilon,\delta)$-differentially private algorithms that accurately answer statisticalqueries on high-dimensional databases. The novelty of our bound is that itdepends optimally on the parameter $\delta$, which loosely corresponds to theprobability that the algorithm fails to be private, and is the first tosmoothly interpolate between approximate differential privacy ($\delta > 0$)and pure differential privacy ($\delta = 0$). Specifically, we consider a database $D \in \{\pm1\}^{n \times d}$ and its\emph{one-way marginals}, which are the $d$ queries of the form "What fractionof individual records have the $i$-th bit set to $+1$?" We show that in orderto answer all of these queries to within error $\pm \alpha$ (on average) whilesatisfying $(\varepsilon, \delta)$-differential privacy, it is necessary that$$ n \geq \Omega\left( \frac{\sqrt{d \log(1/\delta)}}{\alpha \varepsilon}\right), $$ which is optimal up to constant factors. To prove our lower bound,we build on the connection between \emph{fingerprinting codes} and lower boundsin differential privacy (Bun, Ullman, and Vadhan, STOC'14). In addition to our lower bound, we give new purely and approximatelydifferentially private algorithms for answering arbitrary statistical queriesthat improve on the sample complexity of the standard Laplace and Gaussianmechanisms for achieving worst-case accuracy guarantees by a logarithmicfactor.
arxiv-8700-16 | Fisher Vectors Derived from Hybrid Gaussian-Laplacian Mixture Models for Image Annotation | http://arxiv.org/pdf/1411.7399v2.pdf | author:Benjamin Klein, Guy Lev, Gil Sadeh, Lior Wolf category:cs.CV published:2014-11-26 summary:In the traditional object recognition pipeline, descriptors are denselysampled over an image, pooled into a high dimensional non-linear representationand then passed to a classifier. In recent years, Fisher Vectors have provenempirically to be the leading representation for a large variety ofapplications. The Fisher Vector is typically taken as the gradients of thelog-likelihood of descriptors, with respect to the parameters of a GaussianMixture Model (GMM). Motivated by the assumption that different distributionsshould be applied for different datasets, we present two other Mixture Modelsand derive their Expectation-Maximization and Fisher Vector expressions. Thefirst is a Laplacian Mixture Model (LMM), which is based on the Laplaciandistribution. The second Mixture Model presented is a Hybrid Gaussian-LaplacianMixture Model (HGLMM) which is based on a weighted geometric mean of theGaussian and Laplacian distribution. An interesting property of theExpectation-Maximization algorithm for the latter is that in the maximizationstep, each dimension in each component is chosen to be either a Gaussian or aLaplacian. Finally, by using the new Fisher Vectors derived from HGLMMs, weachieve state-of-the-art results for both the image annotation and the imagesearch by a sentence tasks.
arxiv-8700-17 | Sparse Distance Weighted Discrimination | http://arxiv.org/pdf/1501.06066v1.pdf | author:Boxiang Wang, Hui Zou category:stat.ML stat.CO published:2015-01-24 summary:Distance weighted discrimination (DWD) was originally proposed to handle thedata piling issue in the support vector machine. In this paper, we consider thesparse penalized DWD for high-dimensional classification. The state-of-the-artalgorithm for solving the standard DWD is based on second-order coneprogramming, however such an algorithm does not work well for the sparsepenalized DWD with high-dimensional data. In order to overcome the challengingcomputation difficulty, we develop a very efficient algorithm to compute thesolution path of the sparse DWD at a given fine grid of regularizationparameters. We implement the algorithm in a publicly available R package sdwd.We conduct extensive numerical experiments to demonstrate the computationalefficiency and classification performance of our method.
arxiv-8700-18 | Consistency Analysis of Nearest Subspace Classifier | http://arxiv.org/pdf/1501.06060v1.pdf | author:Yi Wang category:stat.ML cs.LG published:2015-01-24 summary:The Nearest subspace classifier (NSS) finds an estimation of the underlyingsubspace within each class and assigns data points to the class thatcorresponds to its nearest subspace. This paper mainly studies how well NSS canbe generalized to new samples. It is proved that NSS is strongly consistentunder certain assumptions. For completeness, NSS is evaluated throughexperiments on various simulated and real data sets, in comparison with someother linear model based classifiers. It is also shown that NSS can obtaineffective classification results and is very efficient, especially for largescale data sets.
arxiv-8700-19 | Learning From Non-iid Data: Fast Rates for the One-vs-All Multiclass Plug-in Classifiers | http://arxiv.org/pdf/1408.2714v2.pdf | author:Vu Dinh, Lam Si Tung Ho, Nguyen Viet Cuong, Duy Nguyen, Binh T. Nguyen category:stat.ML published:2014-08-12 summary:We prove new fast learning rates for the one-vs-all multiclass plug-inclassifiers trained either from exponentially strongly mixing data or from datagenerated by a converging drifting distribution. These are two typicalscenarios where training data are not iid. The learning rates are obtainedunder a multiclass version of Tsybakov's margin assumption, a type of low-noiseassumption, and do not depend on the number of classes. Our results are generaland include a previous result for binary-class plug-in classifiers with iiddata as a special case. In contrast to previous works for least squares SVMsunder the binary-class setting, our results retain the optimal learning rate inthe iid case.
arxiv-8700-20 | Quantifying literature quality using complexity criteria | http://arxiv.org/pdf/1401.7077v3.pdf | author:Gerardo Febres, Klaus Jaffe category:cs.CL published:2014-01-28 summary:We measured entropy and symbolic diversity for English and Spanish textsincluding literature Nobel laureates and other famous authors. Entropy, symboldiversity and symbol frequency profiles were compared for these four groups. Wealso built a scale sensitive to the quality of writing and evaluated itsrelationship with the Flesch's readability index for English and theSzigriszt's perspicuity index for Spanish. Results suggest a correlationbetween entropy and word diversity with quality of writing. Text genre alsoinfluences the resulting entropy and diversity of the text. Results suggest theplausibility of automated quality assessment of texts.
arxiv-8700-21 | Automatic Objects Removal for Scene Completion | http://arxiv.org/pdf/1501.05970v1.pdf | author:Jianjun Yang, Yin Wang, Honggang Wang, Kun Hua, Wei Wang, Ju Shen category:cs.CV published:2015-01-23 summary:With the explosive growth of web-based cameras and mobile devices, billionsof photographs are uploaded to the internet. We can trivially collect a hugenumber of photo streams for various goals, such as 3D scene reconstruction andother big data applications. However, this is not an easy task due to the factthe retrieved photos are neither aligned nor calibrated. Furthermore, with theocclusion of unexpected foreground objects like people, vehicles, it is evenmore challenging to find feature correspondences and reconstruct realisticscenes. In this paper, we propose a structure based image completion algorithmfor object removal that produces visually plausible content with consistentstructure and scene texture. We use an edge matching technique to infer thepotential structure of the unknown region. Driven by the estimated structure,texture synthesis is performed automatically along the estimated curves. Weevaluate the proposed method on different types of images: from highlystructured indoor environment to the natural scenes. Our experimental resultsdemonstrate satisfactory performance that can be potentially used forsubsequent big data processing: 3D scene reconstruction and locationrecognition.
arxiv-8700-22 | Advances in Human Action Recognition: A Survey | http://arxiv.org/pdf/1501.05964v1.pdf | author:Guangchun Cheng, Yiwen Wan, Abdullah N. Saudagar, Kamesh Namuduri, Bill P. Buckles category:cs.CV published:2015-01-23 summary:Human action recognition has been an important topic in computer vision dueto its many applications such as video surveillance, human machine interactionand video retrieval. One core problem behind these applications isautomatically recognizing low-level actions and high-level activities ofinterest. The former is usually the basis for the latter. This survey gives anoverview of the most recent advances in human action recognition during thepast several years, following a well-formed taxonomy proposed by a previoussurvey. From this state-of-the-art survey, researchers can view a panorama ofprogress in this area for future research.
arxiv-8700-23 | Auto-encoders: reconstruction versus compression | http://arxiv.org/pdf/1403.7752v2.pdf | author:Yann Ollivier category:cs.NE cs.IT cs.LG math.IT published:2014-03-30 summary:We discuss the similarities and differences between training an auto-encoderto minimize the reconstruction error, and training the same auto-encoder tocompress the data via a generative model. Minimizing a codelength for the datausing an auto-encoder is equivalent to minimizing the reconstruction error plussome correcting terms which have an interpretation as either a denoising orcontractive property of the decoding function. These terms are related but notidentical to those used in denoising or contractive auto-encoders [Vincent etal. 2010, Rifai et al. 2011]. In particular, the codelength viewpoint fullydetermines an optimal noise level for the denoising criterion.
arxiv-8700-24 | Proximal Quasi-Newton for Computationally Intensive L1-regularized M-estimators | http://arxiv.org/pdf/1406.7321v2.pdf | author:Kai Zhong, Ian E. H. Yen, Inderjit S. Dhillon, Pradeep Ravikumar category:stat.ML published:2014-06-27 summary:We consider the class of optimization problems arising from computationallyintensive L1-regularized M-estimators, where the function or gradient valuesare very expensive to compute. A particular instance of interest is theL1-regularized MLE for learning Conditional Random Fields (CRFs), which are apopular class of statistical models for varied structured prediction problemssuch as sequence labeling, alignment, and classification with label taxonomy.L1-regularized MLEs for CRFs are particularly expensive to optimize sincecomputing the gradient values requires an expensive inference step. In thiswork, we propose the use of a carefully constructed proximal quasi-Newtonalgorithm for such computationally intensive M-estimation problems, where weemploy an aggressive active set selection technique. In a key contribution ofthe paper, we show that the proximal quasi-Newton method is provablysuper-linearly convergent, even in the absence of strong convexity, byleveraging a restricted variant of strong convexity. In our experiments, theproposed algorithm converges considerably faster than current state-of-the-arton the problems of sequence labeling and hierarchical classification.
arxiv-8700-25 | Complexity Measures and Concept Learning | http://arxiv.org/pdf/1406.7424v3.pdf | author:Andreas D. Pape, Kenneth J. Kurtz, Hiroki Sayama category:cs.IT cs.LG math.IT published:2014-06-28 summary:The nature of concept learning is a core question in cognitive science.Theories must account for the relative difficulty of acquiring differentconcepts by supervised learners. For a canonical set of six category types, twodistinct orderings of classification difficulty have been found. One ordering,which we call paradigm-specific, occurs when adult human learners classifyobjects with easily distinguishable characteristics such as size, shape, andshading. The general order occurs in all other known cases: when adult humansclassify objects with characteristics that are not readily distinguished (e.g.,brightness, saturation, hue); for children and monkeys; and when categorizationdifficulty is extrapolated from errors in identification learning. Theparadigm-specific order was found to be predictable mathematically by measuringthe logical complexity of tasks, i.e., how concisely the solution can berepresented by logical rules. However, logical complexity explains only the paradigm-specific order but notthe general order. Here we propose a new difficulty measurement, informationcomplexity, that calculates the amount of uncertainty remaining when a subsetof the dimensions are specified. This measurement is based on Shannon entropy.We show that, when the metric extracts minimal uncertainties, this newmeasurement predicts the paradigm-specific order for the canonical six categorytypes, and when the metric extracts average uncertainties, this new measurementpredicts the general order. Moreover, for learning category types beyond thecanonical six, we find that the minimal-uncertainty formulation correctlypredicts the paradigm-specific order as well or better than existing metrics(Boolean complexity and GIST) in most cases.
arxiv-8700-26 | Efficient Feature Group Sequencing for Anytime Linear Prediction | http://arxiv.org/pdf/1409.5495v2.pdf | author:Hanzhang Hu, Alexander Grubb, J. Andrew Bagnell, Martial Hebert category:cs.LG published:2014-09-19 summary:We propose a regularized linear learning algorithm to sequence groups offeatures, where each group incurs test-time cost or computation. Specifically,we develop a simple extension to Orthogonal Matching Pursuit (OMP) thatrespects the structure of groups of features with variable costs, and we provethat it achieves near-optimal anytime linear prediction at each budgetthreshold where a new group is selected. Our algorithm and analysis extends togeneralized linear models with multi-dimensional responses. We demonstrate thescalability of the resulting approach on large real-world data-sets with manyfeature groups associated with test-time computational costs. Our methodimproves over Group Lasso and Group OMP in the anytime performance of linearpredictions, measured in timeliness, an anytime prediction performance metric,while providing rigorous performance guarantees.
arxiv-8700-27 | Unsupervised Segmentation of Multispectral Images with Cellular Automata | http://arxiv.org/pdf/1501.05854v1.pdf | author:Wuilian Torres, Antonio Rueda-Toicen category:cs.CV published:2015-01-23 summary:Multispectral images acquired by satellites are used to study phenomena onthe Earth's surface. Unsupervised classification techniques analyzemultispectral image content without considering prior knowledge of the observedterrain; this is done using techniques which group pixels that have similarstatistics of digital level distribution in the various image channels. In thispaper, we propose a methodology for unsupervised classification based on adeterministic cellular automaton. The automaton is initialized in anunsupervised manner by setting seed cells, selected according to two criteria:to be representative of the spatial distribution of the dominant elements inthe image, and to take into account the diversity of spectral signatures in theimage. The automaton's evolution is based on an attack rule that is appliedsimultaneously to all its cells. Among the noteworthy advantages ofdeterministic cellular automata for multispectral processing of satelliteimagery is the consideration of topological information in the image via seedpositioning, and the ability to modify the scale of the study.
arxiv-8700-28 | A Graph Theoretic Approach for Object Shape Representation in Compositional Hierarchies Using a Hybrid Generative-Descriptive Model | http://arxiv.org/pdf/1501.05192v2.pdf | author:Umit Rusen Aktas, Mete Ozay, Ales Leonardis, Jeremy L. Wyatt category:cs.CV published:2015-01-21 summary:A graph theoretic approach is proposed for object shape representation in ahierarchical compositional architecture called Compositional Hierarchy of Parts(CHOP). In the proposed approach, vocabulary learning is performed using ahybrid generative-descriptive model. First, statistical relationships betweenparts are learned using a Minimum Conditional Entropy Clustering algorithm.Then, selection of descriptive parts is defined as a frequent subgraphdiscovery problem, and solved using a Minimum Description Length (MDL)principle. Finally, part compositions are constructed by compressing theinternal data representation with discovered substructures. Shaperepresentation and computational complexity properties of the proposed approachand algorithms are examined using six benchmark two-dimensional shape imagedatasets. Experiments show that CHOP can employ part shareability and indexingmechanisms for fast inference of part compositions using learned shapevocabularies. Additionally, CHOP provides better shape retrieval performancethan the state-of-the-art shape retrieval methods.
arxiv-8700-29 | Taking a Deeper Look at Pedestrians | http://arxiv.org/pdf/1501.05790v1.pdf | author:Jan Hosang, Mohamed Omran, Rodrigo Benenson, Bernt Schiele category:cs.CV published:2015-01-23 summary:In this paper we study the use of convolutional neural networks (convnets)for the task of pedestrian detection. Despite their recent diverse successes,convnets historically underperform compared to other pedestrian detectors. Wedeliberately omit explicitly modelling the problem into the network (e.g. partsor occlusion modelling) and show that we can reach competitive performancewithout bells and whistles. In a wide range of experiments we analyse small andbig convnets, their architectural choices, parameters, and the influence ofdifferent training data, including pre-training on surrogate tasks. We present the best convnet detectors on the Caltech and KITTI dataset. OnCaltech our convnets reach top performance both for the Caltech1x andCaltech10x training setup. Using additional data at training time our strongestconvnet model is competitive even to detectors that use additional data(optical flow) at test time.
arxiv-8700-30 | Filtered Channel Features for Pedestrian Detection | http://arxiv.org/pdf/1501.05759v1.pdf | author:Shanshan Zhang, Rodrigo Benenson, Bernt Schiele category:cs.CV published:2015-01-23 summary:This paper starts from the observation that multiple top performingpedestrian detectors can be modelled by using an intermediate layer filteringlow-level features in combination with a boosted decision forest. Based on thisobservation we propose a unifying framework and experimentally exploredifferent filter families. We report extensive results enabling a systematicanalysis. Using filtered channel features we obtain top performance on the challengingCaltech and KITTI datasets, while using only HOG+LUV as low-level features.When adding optical flow features we further improve detection quality andreport the best known results on the Caltech dataset, reaching 93% recall at 1FPPI.
arxiv-8700-31 | Application of the SP theory of intelligence to the understanding of natural vision and the development of computer vision | http://arxiv.org/pdf/1303.2071v2.pdf | author:J. Gerard Wolff category:cs.CV cs.AI published:2013-03-08 summary:The SP theory of intelligence aims to simplify and integrate concepts incomputing and cognition, with information compression as a unifying theme. Thisarticle discusses how it may be applied to the understanding of natural visionand the development of computer vision. The theory, which is described quitefully elsewhere, is described here in outline but with enough detail to ensurethat the rest of the article makes sense. Low level perceptual features such as edges or corners may be identified bythe extraction of redundancy in uniform areas in a manner that is comparablewith the run-length encoding technique for information compression. The concept of multiple alignment in the SP theory may be applied to therecognition of objects, and to scene analysis, with a hierarchy of parts andsub-parts, and at multiple levels of abstraction. The theory has potential for the unsupervised learning of visual objects andclasses of objects, and suggests how coherent concepts may be derived fromfragments. As in natural vision, both recognition and learning in the SP system isrobust in the face of errors of omission, commission and substitution. The theory suggests how, via vision, we may piece together a knowledge of thethree-dimensional structure of objects and of our environment, it provides anaccount of how we may see things that are not objectively present in an image,and how we recognise something despite variations in the size of its retinalimage. And it has things to say about the phenomena of lightness constancy andcolour constancy, the role of context in recognition, and ambiguities in visualperception. A strength of the SP theory is that it provides for the integration of visionwith other sensory modalities and with other aspects of intelligence.
arxiv-8700-32 | Bayesian Learning for Low-Rank matrix reconstruction | http://arxiv.org/pdf/1501.05740v1.pdf | author:Martin Sundin, Cristian R. Rojas, Magnus Jansson, Saikat Chatterjee category:stat.ML cs.LG cs.NA published:2015-01-23 summary:We develop latent variable models for Bayesian learning based low-rank matrixcompletion and reconstruction from linear measurements. For under-determinedsystems, the developed methods are shown to reconstruct low-rank matrices whenneither the rank nor the noise power is known a-priori. We derive relationsbetween the latent variable models and several low-rank promoting penaltyfunctions. The relations justify the use of Kronecker structured covariancematrices in a Gaussian based prior. In the methods, we use evidenceapproximation and expectation-maximization to learn the model parameters. Theperformance of the methods is evaluated through extensive numericalsimulations.
arxiv-8700-33 | On the Expressive Efficiency of Sum Product Networks | http://arxiv.org/pdf/1411.7717v3.pdf | author:James Martens, Venkatesh Medabalimi category:cs.LG stat.ML published:2014-11-27 summary:Sum Product Networks (SPNs) are a recently developed class of deep generativemodels which compute their associated unnormalized density functions using aspecial type of arithmetic circuit. When certain sufficient conditions, calledthe decomposability and completeness conditions (or "D&C" conditions), areimposed on the structure of these circuits, marginal densities and other usefulquantities, which are typically intractable for other deep generative models,can be computed by what amounts to a single evaluation of the network (which isa property known as "validity"). However, the effect that the D&C conditionshave on the capabilities of D&C SPNs is not well understood. In this work we analyze the D&C conditions, expose the various connectionsthat D&C SPNs have with multilinear arithmetic circuits, and consider thequestion of how well they can capture various distributions as a function oftheir size and depth. Among our various contributions is a result whichestablishes the existence of a relatively simple distribution with fullytractable marginal densities which cannot be efficiently captured by D&C SPNsof any depth, but which can be efficiently captured by various other deepgenerative models. We also show that with each additional layer of depthpermitted, the set of distributions which can be efficiently captured by D&CSPNs grows in size. This kind of "depth hierarchy" property has been widelyconjectured to hold for various deep models, but has never been proven for anyof them. Some of our other contributions include a new characterization of theD&C conditions as sufficient and necessary ones for a slightly strengthenednotion of validity, and various state-machine characterizations of the types ofcomputations that can be performed efficiently by D&C SPNs.
arxiv-8700-34 | Bi-Objective Nonnegative Matrix Factorization: Linear Versus Kernel-Based Models | http://arxiv.org/pdf/1501.05684v1.pdf | author:Paul Honeine, Fei Zhu category:stat.ML cs.CV cs.LG math.OC published:2015-01-22 summary:Nonnegative matrix factorization (NMF) is a powerful class of featureextraction techniques that has been successfully applied in many fields, namelyin signal and image processing. Current NMF techniques have been limited to asingle-objective problem in either its linear or nonlinear kernel-basedformulation. In this paper, we propose to revisit the NMF as a multi-objectiveproblem, in particular a bi-objective one, where the objective functionsdefined in both input and feature spaces are taken into account. By taking theadvantage of the sum-weighted method from the literature of multi-objectiveoptimization, the proposed bi-objective NMF determines a set of nondominated,Pareto optimal, solutions instead of a single optimal decomposition. Moreover,the corresponding Pareto front is studied and approximated. Experimentalresults on unmixing real hyperspectral images confirm the efficiency of theproposed bi-objective NMF compared with the state-of-the-art methods.
arxiv-8700-35 | Active Mean Fields for Probabilistic Image Segmentation: Connections with Chan-Vese and Rudin-Osher-Fatemi Models | http://arxiv.org/pdf/1501.05680v1.pdf | author:Marc Niethammer, Kilian M. Pohl, Firdaus Janoos, William M. Wells III category:cs.CV published:2015-01-22 summary:Image segmentation is a fundamental task for extracting semanticallymeaningful regions from an image. The goal is to assign object labels to eachimage location. Due to image-noise, shortcomings of algorithms and otherambiguities in the images, there is uncertainty in the assigned labels. Inmultiple application domains, estimates of this uncertainty are important. Forexample, object segmentation and uncertainty quantification is essential formany medical application, including tumor segmentation for radiation treatmentplanning. While a Bayesian characterization of the label posterior providesestimates of segmentation uncertainty, Bayesian approaches can becomputationally prohibitive for practical applications. On the other hand,typical optimization based algorithms are computationally very efficient, butonly provide maximum a-posteriori solutions and hence no estimates of labeluncertainty. In this paper, we propose Active Mean Fields (AMF), a Bayesian technique thatuses a mean-field approximation to derive an efficient segmentation anduncertainty quantification algorithm. This model, which allows combining anylabel-likelihood measure with a boundary length prior, yields a variationalformulation that is convex. A specific implementation of that model is theChan--Vese segmentation model (CV), which formulates the binary segmentationproblem through Gaussian likelihoods combined with a boundary-lengthregularizer. Furthermore, the Euler--Lagrange equations derived from the AMFmodel are equivalent to those of the popular Rudin-Osher-Fatemi (ROF) model forimage de-noising. Solutions to the AMF model can thus be implemented bydirectly utilizing highly-efficient ROF solvers on log-likelihood ratio fields.We demonstrate the approach using synthetic data, as well as real medicalimages (for heart and prostate segmentations), and on standard computer visiontest images.
arxiv-8700-36 | A New Efficient Method for Calculating Similarity Between Web Services | http://arxiv.org/pdf/1501.05940v1.pdf | author:T. Rachad, J. Boutahar, S. El ghazi category:cs.AI cs.CL cs.IR cs.SE published:2015-01-22 summary:Web services allow communication between heterogeneous systems in adistributed environment. Their enormous success and their increased use led tothe fact that thousands of Web services are present on the Internet. Thissignificant number of Web services which not cease to increase has led toproblems of the difficulty in locating and classifying web services, theseproblems are encountered mainly during the operations of web services discoveryand substitution. Traditional ways of search based on keywords are notsuccessful in this context, their results do not support the structure of Webservices and they consider in their search only the identifiers of the webservice description language (WSDL) interface elements. The methods based onsemantics (WSDLS, OWLS, SAWSDL...) which increase the WSDL description of a Webservice with a semantic description allow raising partially this problem, buttheir complexity and difficulty delays their adoption in real cases. Measuringthe similarity between the web services interfaces is the most suitablesolution for this kind of problems, it will classify available web services soas to know those that best match the searched profile and those that do notmatch. Thus, the main goal of this work is to study the degree of similaritybetween any two web services by offering a new method that is more effectivethan existing works.
arxiv-8700-37 | An Offline Technique for Localization of License Plates for Indian Commercial Vehicles | http://arxiv.org/pdf/1003.1072v2.pdf | author:Satadal Saha, Subhadip Basu, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2010-03-04 summary:Automatic License Plate Recognition (ALPR) is a challenging area of researchdue to its importance to variety of commercial applications. The overallproblem may be subdivided into two key modules, firstly, localization oflicense plates from vehicle images, and secondly, optical character recognitionof extracted license plates. In the current work, we have concentrated on thefirst part of the problem, i.e., localization of license plate regions fromIndian commercial vehicles as a significant step towards development of acomplete ALPR system for Indian vehicles. The technique is based on color basedsegmentation of vehicle images and identification of potential license plateregions. True license plates are finally localized based on four spatial andhorizontal contrast features. The technique successfully localizes the actuallicense plates in 73.4% images.
arxiv-8700-38 | Development of an automated Red Light Violation Detection System (RLVDS) for Indian vehicles | http://arxiv.org/pdf/1003.6052v2.pdf | author:Satadal Saha, Subhadip Basu, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2010-03-31 summary:Integrated Traffic Management Systems (ITMS) are now implemented in differentcities in India to primarily address the concerns of road-safety and security.An automated Red Light Violation Detection System (RLVDS) is an integral partof the ITMS. In our present work we have designed and developed a completesystem for generating the list of all stop-line violating vehicle imagesautomatically from video snapshots of road-side surveillance cameras. Thesystem first generates adaptive background images for each camera view,subtracts captured images from the corresponding background images and analysespotential occlusions over the stop-line in a traffic signal. Consideringround-the-clock operations in a real-life test environment, the developedsystem could successfully track 92% images of vehicles with violations on thestop-line in a "Red" traffic signal.
arxiv-8700-39 | A novel scheme for binarization of vehicle images using hierarchical histogram equalization technique | http://arxiv.org/pdf/1003.6059v2.pdf | author:Satadal Saha, Subhadip Basu, Mita Nasipuri, Dipak Kumar Basu category:cs.CV published:2010-03-31 summary:Automatic License Plate Recognition system is a challenging area of researchnow-a-days and binarization is an integral and most important part of it. Incase of a real life scenario, most of existing methods fail to properlybinarize the image of a vehicle in a congested road, captured through a CCDcamera. In the current work we have applied histogram equalization techniqueover the complete image and also over different hierarchy of imagepartitioning. A novel scheme is formulated for giving the membership value toeach pixel for each hierarchy of histogram equalization. Then the image isbinarized depending on the net membership value of each pixel. The technique isexhaustively evaluated on the vehicle image dataset as well as the licenseplate dataset, giving satisfactory performances.
arxiv-8700-40 | A Collaborative Kalman Filter for Time-Evolving Dyadic Processes | http://arxiv.org/pdf/1501.05624v1.pdf | author:San Gultekin, John Paisley category:stat.ML cs.LG published:2015-01-22 summary:We present the collaborative Kalman filter (CKF), a dynamic model forcollaborative filtering and related factorization models. Using the matrixfactorization approach to collaborative filtering, the CKF accounts for timeevolution by modeling each low-dimensional latent embedding as amultidimensional Brownian motion. Each observation is a random variable whosedistribution is parameterized by the dot product of the relevant Brownianmotions at that moment in time. This is naturally interpreted as a Kalmanfilter with multiple interacting state space vectors. We also present a methodfor learning a dynamically evolving drift parameter for each location bymodeling it as a geometric Brownian motion. We handle posterior intractabilityvia a mean-field variational approximation, which also preserves tractabilityfor downstream calculations in a manner similar to the Kalman filter. Weevaluate the model on several large datasets, providing quantitative evaluationon the 10 million Movielens and 100 million Netflix datasets and qualitativeevaluation on a set of 39 million stock returns divided across roughly 6,500companies from the years 1962-2014.
arxiv-8700-41 | Unsupervised image segmentation by Global and local Criteria Optimization Based on Bayesian Networks | http://arxiv.org/pdf/1501.05617v1.pdf | author:Mohamed Ali Mahjoub, Mohamed Mhiri category:cs.CV published:2015-01-22 summary:Today Bayesian networks are more used in many areas of decision support andimage processing. In this way, our proposed approach uses Bayesian Network tomodelize the segmented image quality. This quality is calculated on a set ofattributes that represent local evaluation measures. The idea is to have theselocal levels chosen in a way to be intersected into them to keep the overallappearance of segmentation. The approach operates in two phases: the firstphase is to make an over-segmentation which gives superpixels card. In thesecond phase, we model the superpixels by a Bayesian Network. To find thesegmented image with the best overall quality we used two approximate inferencemethods, the first using ICM algorithm which is widely used in Markov Modelsand a second is a recursive method called algorithm of model decompositionbased on max-product algorithm which is very popular in the recent works ofimage segmentation. For our model, we have shown that the composition of thesetwo algorithms leads to good segmentation performance.
arxiv-8700-42 | Sketch and Validate for Big Data Clustering | http://arxiv.org/pdf/1501.05590v1.pdf | author:Panagiotis A. Traganitis, Konstantinos Slavakis, Georgios B. Giannakis category:stat.ML cs.LG published:2015-01-22 summary:In response to the need for learning tools tuned to big data analytics, thepresent paper introduces a framework for efficient clustering of huge sets of(possibly high-dimensional) data. Building on random sampling and consensus(RANSAC) ideas pursued earlier in a different (computer vision) context forrobust regression, a suite of novel dimensionality and set-reduction algorithmsis developed. The advocated sketch-and-validate (SkeVa) family includes twoalgorithms that rely on K-means clustering per iteration on reduced number ofdimensions and/or feature vectors: The first operates in a batch fashion, whilethe second sequential one offers computational efficiency and suitability withstreaming modes of operation. For clustering even nonlinearly separablevectors, the SkeVa family offers also a member based on user-selected kernelfunctions. Further trading off performance for reduced complexity, a fourthmember of the SkeVa family is based on a divergence criterion for selectingproper minimal subsets of feature variables and vectors, thus bypassing theneed for K-means clustering per iteration. Extensive numerical tests onsynthetic and real data sets highlight the potential of the proposedalgorithms, and demonstrate their competitive performance relative tostate-of-the-art random projection alternatives.
arxiv-8700-43 | Estimating the Intrinsic Dimension of Hyperspectral Images Using an Eigen-Gap Approach | http://arxiv.org/pdf/1501.05552v1.pdf | author:A. Halimi, P. Honeine, M. Kharouf, C. Richard, J. -Y. Tourneret category:stat.AP cs.CV published:2015-01-22 summary:Linear mixture models are commonly used to represent hyperspectral datacubeas a linear combinations of endmember spectra. However, determining of thenumber of endmembers for images embedded in noise is a crucial task. This paperproposes a fully automatic approach for estimating the number of endmembers inhyperspectral images. The estimation is based on recent results of randommatrix theory related to the so-called spiked population model. More precisely,we study the gap between successive eigenvalues of the sample covariance matrixconstructed from high dimensional noisy samples. The resulting estimationstrategy is unsupervised and robust to correlated noise. This strategy isvalidated on both synthetic and real images. The experimental results are verypromising and show the accuracy of this algorithm with respect tostate-of-the-art algorithms.
arxiv-8700-44 | Bi-l0-l2-Norm Regularization for Blind Motion Deblurring | http://arxiv.org/pdf/1408.4712v3.pdf | author:Wen-Ze Shao, Hai-Bo Li, Michael Elad category:cs.CV published:2014-08-20 summary:In blind motion deblurring, leading methods today tend towards highlynon-convex approximations of the l0-norm, especially in the imageregularization term. In this paper, we propose a simple, effective and fastapproach for the estimation of the motion blur-kernel, through a bi-l0-l2-normregularization imposed on both the intermediate sharp image and theblur-kernel. Compared with existing methods, the proposed regularization isshown to be more effective and robust, leading to a more accurate motionblur-kernel and a better final restored image. A fast numerical scheme isdeployed for alternatingly computing the sharp image and the blur-kernel, bycoupling the operator splitting and augmented Lagrangian methods. Experimentalresults on both a benchmark image dataset and real-world motion blurred imagesshow that the proposed approach is highly competitive with state-of-the- artmethods in both deblurring effectiveness and computational efficiency.
arxiv-8700-45 | An Improved Feature Descriptor for Recognition of Handwritten Bangla Alphabet | http://arxiv.org/pdf/1501.05497v1.pdf | author:Nibaran Das, Subhadip Basu, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak kumar Basu category:cs.CV published:2015-01-22 summary:Appropriate feature set for representation of pattern classes is one of themost important aspects of handwritten character recognition. The effectivenessof features depends on the discriminating power of the features chosen torepresent patterns of different classes. However, discriminatory features arenot easily measurable. Investigative experimentation is necessary foridentifying discriminatory features. In the present work we have identified anew variation of feature set which significantly outperforms on handwrittenBangla alphabet from the previously used feature set. 132 number of features inall viz. modified shadow features, octant and centroid features, distance basedfeatures, quad tree based longest run features are used here. Using thisfeature set the recognition performance increases sharply from the 75.05%observed in our previous work [7], to 85.40% on 50 character classes with MLPbased classifier on the same dataset.
arxiv-8700-46 | A GA Based approach for selection of local features for recognition of handwritten Bangla numerals | http://arxiv.org/pdf/1501.05495v1.pdf | author:Nibaran Das, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri category:cs.CV published:2015-01-22 summary:Soft computing approaches are mainly designed to address the real worldill-defined, imprecisely formulated problems, combining different kind of novelmodels of computation, such as neural networks, genetic algorithms (GAs.Handwritten digit recognition is a typical example of one such problem. In thecurrent work we have developed a two-pass approach where the first passclassifier performs a coarse classification, based on some global features ofthe input pattern by restricting the possibility of classification decisionswithin a group of classes, smaller than the number of classes consideredinitially. In the second pass, the group specific classifiers concentrate onthe features extracted from the selected local regions, and refine the earlierdecision by combining the local and the global features for selecting the trueclass of the input pattern from the group of candidate classes selected in thefirst pass. To optimize the selection of local regions a GA based approach hasbeen developed here. The maximum recognition performance on Bangla digitsamples as achieved on the test set, during the first pass of the two passapproach is 93.35%. After combining the results of the two stage classifiers,an overall success rate of 95.25% is achieved.
arxiv-8700-47 | Design of a novel convex hull based feature set for recognition of isolated handwritten Roman numerals | http://arxiv.org/pdf/1501.05494v1.pdf | author:Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu category:cs.CV published:2015-01-22 summary:In this paper, convex hull based features are used for recognition ofisolated Roman numerals using a Multi Layer Perceptron (MLP) based classifier.Experiments of convex hull based features for handwritten character recognitionare few in numbers. Convex hull of a pattern and the centroid of the convexhull both are affine invariant attributes. In this work, 25 features areextracted based on different bays attributes of the convex hull of the digitpatterns. Then these patterns are divided into four sub-images with respect tothe centroid of the convex hull boundary. From each such sub-image 25 baysfeatures are also calculated. In all 125 convex hull based features areextracted for each numeric digit patterns under the current experiment. Theperformance of the designed feature set is tested on the standard MNIST dataset, consisting of 60000 training and 10000 test images of handwritten Romanusing an MLP based classifier a maximum success rate of 97.44% is achieved onthe test data.
arxiv-8700-48 | An Algebra to Merge Heterogeneous Classifiers | http://arxiv.org/pdf/1501.05141v2.pdf | author:Philippe J. Giabbanelli, Joseph G. Peters category:cs.DM cs.LG 97R50, 08Axx published:2015-01-21 summary:In distributed classification, each learner observes its environment anddeduces a classifier. As a learner has only a local view of its environment,classifiers can be exchanged among the learners and integrated, or merged, toimprove accuracy. However, the operation of merging is not defined for mostclassifiers. Furthermore, the classifiers that have to be merged may be ofdifferent types in settings such as ad-hoc networks in which severalgenerations of sensors may be creating classifiers. We introduce decisionspaces as a framework for merging possibly different classifiers. We formallystudy the merging operation as an algebra, and prove that it satisfies adesirable set of properties. The impact of time is discussed for the two maindata mining settings. Firstly, decision spaces can naturally be used withnon-stationary distributions, such as the data collected by sensor networks, asthe impact of a model decays over time. Secondly, we introduce an approach forstationary distributions, such as homogeneous databases partitioned overdifferent learners, which ensures that all models have the same impact. We alsopresent a method that uses storage flexibly to achieve different types of decayfor non-stationary distributions. Finally, we show that the algebraic approachdeveloped for merging can also be used to analyze the behaviour of otheroperators.
arxiv-8700-49 | Handwritten Devanagari Script Segmentation: A non-linear Fuzzy Approach | http://arxiv.org/pdf/1501.05472v1.pdf | author:Ram Sarkar, Bibhash Sen, Nibaran Das, Subhadip Basu category:cs.CV published:2015-01-22 summary:The paper concentrates on improvement of segmentation accuracy by addressingsome of the key challenges of handwritten Devanagari word image segmentationtechnique. In the present work, we have developed a new feature based approachfor identification of Matra pixels from a word image, design of a non-linearfuzzy membership functions for headline estimation and finally design of anon-linear fuzzy functions for identifying segmentation points on the Matra.The segmentation accuracy achieved by the current technique is 94.8%. Thisshows an improvement of performance by 1.8% over the previous technique [1] ona 300-word dataset, used for the current experiment.
arxiv-8700-50 | Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets | http://arxiv.org/pdf/1402.0480v5.pdf | author:Diederik P. Kingma, Max Welling category:cs.LG stat.ML published:2014-02-03 summary:Hierarchical Bayesian networks and neural networks with stochastic hiddenunits are commonly perceived as two separate types of models. We show thateither of these types of models can often be transformed into an instance ofthe other, by switching between centered and differentiable non-centeredparameterizations of the latent variables. The choice of parameterizationgreatly influences the efficiency of gradient-based posterior inference; weshow that they are often complementary to eachother, we clarify when eachparameterization is preferred and show how inference can be made robust. In thenon-centered form, a simple Monte Carlo estimator of the marginal likelihoodcan be used for learning the parameters. Theoretical results are supported byexperiments.
arxiv-8700-51 | Point Context: An Effective Shape Descriptor for RST-invariant Trajectory Recognition | http://arxiv.org/pdf/1501.05432v1.pdf | author:Xingyu Wu, Xia Mao, Lijiang Chen, Yuli Xue, Angelo Compare category:cs.CV 51A05 I.2.10; I.5.4 published:2015-01-22 summary:Motion trajectory recognition is important for characterizing the movingproperty of an object. The speed and accuracy of trajectory recognition rely ona compact and discriminative feature representation, and the situations ofvarying rotation, scaling and translation has to be specially considered. Inthis paper we propose a novel feature extraction method for trajectories.Firstly a trajectory is represented by a proposed point context, which is arotation-scale-translation (RST) invariant shape descriptor with a flexibletradeoff between computational complexity and discrimination, yet we prove thatit is a complete shape descriptor. Secondly, the shape context is nonlinearlymapped to a subspace by kernel nonparametric discriminant analysis (KNDA) toget a compact feature representation, and thus a trajectory is projected to asingle point in a low-dimensional feature space. Experimental results showthat, the proposed trajectory feature shows encouraging improvement thanstate-of-art methods.
arxiv-8700-52 | Noisy Sparse Subspace Clustering | http://arxiv.org/pdf/1309.1233v2.pdf | author:Yu-Xiang Wang, Huan Xu category:stat.ML published:2013-09-05 summary:This paper considers the problem of subspace clustering under noise.Specifically, we study the behavior of Sparse Subspace Clustering (SSC) wheneither adversarial or random noise is added to the unlabelled input datapoints, which are assumed to be in a union of low-dimensional subspaces. Weshow that a modified version of SSC is \emph{provably effective} in correctlyidentifying the underlying subspaces, even with noisy data. This extendstheoretical guarantee of this algorithm to more practical settings and providesjustification to the success of SSC in a class of real applications.
arxiv-8700-53 | Deep Multimodal Learning for Audio-Visual Speech Recognition | http://arxiv.org/pdf/1501.05396v1.pdf | author:Youssef Mroueh, Etienne Marcheret, Vaibhava Goel category:cs.CL cs.LG published:2015-01-22 summary:In this paper, we present methods in deep multimodal learning for fusingspeech and visual modalities for Audio-Visual Automatic Speech Recognition(AV-ASR). First, we study an approach where uni-modal deep networks are trainedseparately and their final hidden layers fused to obtain a joint feature spacein which another deep network is built. While the audio network alone achievesa phone error rate (PER) of $41\%$ under clean condition on the IBM largevocabulary audio-visual studio dataset, this fusion model achieves a PER of$35.83\%$ demonstrating the tremendous value of the visual channel in phoneclassification even in audio with high signal to noise ratio. Second, wepresent a new deep network architecture that uses a bilinear softmax layer toaccount for class specific correlations between modalities. We show thatcombining the posteriors from the bilinear networks with those from the fusedmodel mentioned above results in a further significant phone error ratereduction, yielding a final PER of $34.03\%$.
arxiv-8700-54 | Exploiting Big Data in Logistics Risk Assessment via Bayesian Nonparametrics | http://arxiv.org/pdf/1501.05349v1.pdf | author:Yan Shang, David B. Dunson, Jing-Sheng Song category:stat.AP stat.ML 62-07 published:2015-01-21 summary:In cargo logistics, a key performance measure is transport risk, defined asthe deviation of the actual arrival time from the planned arrival time. Neitherearliness nor tardiness is desirable for customer and freight forwarders. Inthis paper, we investigate ways to assess and forecast transport risks using ahalf-year of air cargo data, provided by a leading forwarder on 1336 routesserved by 20 airlines. Interestingly, our preliminary data analysis shows astrong multimodal feature in the transport risks, driven by unobserved events,such as cargo missing flights. To accommodate this feature, we introduce aBayesian nonparametric model -- the probit stick-breaking process (PSBP)mixture model -- for flexible estimation of the conditional (i.e.,state-dependent) density function of transport risk. We demonstrate that usingsimpler methods, such as OLS linear regression, can lead to misleadinginferences. Our model provides a tool for the forwarder to offer customizedprice and service quotes. It can also generate baseline airline performance toenable fair supplier evaluation. Furthermore, the method allows us to separaterecurrent risks from disruption risks. This is important, because hedgingstrategies for these two kinds of risks are often drastically different.
arxiv-8700-55 | The Loss Surfaces of Multilayer Networks | http://arxiv.org/pdf/1412.0233v3.pdf | author:Anna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, Yann LeCun category:cs.LG published:2014-11-30 summary:We study the connection between the highly non-convex loss function of asimple model of the fully-connected feed-forward neural network and theHamiltonian of the spherical spin-glass model under the assumptions of: i)variable independence, ii) redundancy in network parametrization, and iii)uniformity. These assumptions enable us to explain the complexity of the fullydecoupled neural network through the prism of the results from random matrixtheory. We show that for large-size decoupled networks the lowest criticalvalues of the random loss function form a layered structure and they arelocated in a well-defined band lower-bounded by the global minimum. The numberof local minima outside that band diminishes exponentially with the size of thenetwork. We empirically verify that the mathematical model exhibits similarbehavior as the computer simulations, despite the presence of high dependenciesin real networks. We conjecture that both simulated annealing and SGD convergeto the band of low critical points, and that all critical points found thereare local minima of high quality measured by the test error. This emphasizes amajor difference between large- and small-size networks where for the latterpoor quality local minima have non-zero probability of being recovered.Finally, we prove that recovering the global minimum becomes harder as thenetwork size increases and that it is in practice irrelevant as global minimumoften leads to overfitting.
arxiv-8700-56 | Machine Learning Etudes in Astrophysics: Selection Functions for Mock Cluster Catalogs | http://arxiv.org/pdf/1409.1576v2.pdf | author:Amir Hajian, Marcelo Alvarez, J. Richard Bond category:astro-ph.CO astro-ph.IM cs.LG stat.ML published:2014-09-04 summary:Making mock simulated catalogs is an important component of astrophysicaldata analysis. Selection criteria for observed astronomical objects are oftentoo complicated to be derived from first principles. However the existence ofan observed group of objects is a well-suited problem for machine learningclassification. In this paper we use one-class classifiers to learn theproperties of an observed catalog of clusters of galaxies from ROSAT and topick clusters from mock simulations that resemble the observed ROSAT catalog.We show how this method can be used to study the cross-correlations of thermalSunya'ev-Zeldovich signals with number density maps of X-ray selected clustercatalogs. The method reduces the bias due to hand-tuning the selection functionand is readily scalable to large catalogs with a high-dimensional space ofastrophysical features.
arxiv-8700-57 | On the impact of topological properties of smart grids in power losses optimization problems | http://arxiv.org/pdf/1501.04659v2.pdf | author:Francesca Possemato, Maurizio Paschero, Lorenzo Livi, Antonello Rizzi, Alireza Sadeghian category:cs.CE cs.NE published:2015-01-19 summary:Power losses reduction is one of the main targets for any electrical energydistribution company. In this paper, we face the problem of joint optimizationof both topology and network parameters in a real smart grid. We consider aportion of the Italian electric distribution network managed by the ACEADistribuzione S.p.A. located in Rome. We perform both the power factorcorrection (PFC) for tuning the generators and the distributed feederreconfiguration (DFR) to set the state of the breakers. This joint optimizationproblem is faced considering a suitable objective function and by adoptinggenetic algorithms as global optimization strategy. We analyze admissiblenetwork configurations, showing that some of these violate constraints oncurrent and voltage at branches and nodes. Such violations depend only on puretopological properties of the configurations. We perform tests by feeding thesimulation environment with real data concerning hourly samples of dissipatedand generated active and reactive power values of the ACEA smart grid. Resultsshow that removing the configurations violating the electrical constraints fromthe solution space leads to interesting improvements in terms of power lossreduction. To conclude, we provide also an electrical interpretation of thephenomenon using graph-based pattern analysis techniques.
arxiv-8700-58 | Extreme Entropy Machines: Robust information theoretic classification | http://arxiv.org/pdf/1501.05279v1.pdf | author:Wojciech Marian Czarnecki, Jacek Tabor category:cs.LG published:2015-01-21 summary:Most of the existing classification methods are aimed at minimization ofempirical risk (through some simple point-based error measured with lossfunction) with added regularization. We propose to approach this problem in amore information theoretic way by investigating applicability of entropymeasures as a classification model objective function. We focus on quadraticRenyi's entropy and connected Cauchy-Schwarz Divergence which leads to theconstruction of Extreme Entropy Machines (EEM). The main contribution of this paper is proposing a model based on theinformation theoretic concepts which on the one hand shows new, entropicperspective on known linear classifiers and on the other leads to aconstruction of very robust method competetitive with the state of the artnon-information theoretic ones (including Support Vector Machines and ExtremeLearning Machines). Evaluation on numerous problems spanning from small, simple ones from UCIrepository to the large (hundreads of thousands of samples) extremelyunbalanced (up to 100:1 classes' ratios) datasets shows wide applicability ofthe EEM in real life problems and that it scales well.
arxiv-8700-59 | Plug-and-play dual-tree algorithm runtime analysis | http://arxiv.org/pdf/1501.05222v1.pdf | author:Ryan R. Curtin, Dongryeol Lee, William B. March, Parikshit Ram category:cs.DS cs.LG published:2015-01-21 summary:Numerous machine learning algorithms contain pairwise statistical problems attheir core---that is, tasks that require computations over all pairs of inputpoints if implemented naively. Often, tree structures are used to solve theseproblems efficiently. Dual-tree algorithms can efficiently solve or approximatemany of these problems. Using cover trees, rigorous worst-case runtimeguarantees have been proven for some of these algorithms. In this paper, wepresent a problem-independent runtime guarantee for any dual-tree algorithmusing the cover tree, separating out the problem-dependent and theproblem-independent elements. This allows us to just plug in bounds for theproblem-dependent elements to get runtime guarantees for dual-tree algorithmsfor any pairwise statistical problem without re-deriving the entire proof. Wedemonstrate this plug-and-play procedure for nearest-neighbor search andapproximate kernel density estimation to get improved runtime guarantees. Undermild assumptions, we also present the first linear runtime guarantee fordual-tree based range search.
arxiv-8700-60 | Minimax Optimal Sparse Signal Recovery with Poisson Statistics | http://arxiv.org/pdf/1501.05200v1.pdf | author:Mohammad H. Rohban, Delaram Motamedvaziri, Venkatesh Saligrama category:stat.ML published:2015-01-21 summary:We are motivated by problems that arise in a number of applications such asOnline Marketing and Explosives detection, where the observations are usuallymodeled using Poisson statistics. We model each observation as a Poisson randomvariable whose mean is a sparse linear superposition of known patterns. Unlikemany conventional problems observations here are not identically distributedsince they are associated with different sensing modalities. We analyze theperformance of a Maximum Likelihood (ML) decoder, which for our Poisson settinginvolves a non-linear optimization but yet is computationally tractable. Wederive fundamental sample complexity bounds for sparse recovery when themeasurements are contaminated with Poisson noise. In contrast to theleast-squares linear regression setting with Gaussian noise, we observe that inaddition to sparsity, the scale of the parameters also fundamentally impacts$\ell_2$ error in the Poisson setting. We show tightness of our upper boundsboth theoretically and experimentally. In particular, we derive a minimaxmatching lower bound on the mean-squared error and show that our constrained MLdecoder is minimax optimal for this regime.
arxiv-8700-61 | Mirror, mirror on the wall, tell me, is the error small? | http://arxiv.org/pdf/1501.05152v1.pdf | author:Heng Yang, Ioannis Patras category:cs.CV published:2015-01-21 summary:Do object part localization methods produce bilaterally symmetric results onmirror images? Surprisingly not, even though state of the art methods augmentthe training set with mirrored images. In this paper we take a closer look intothis issue. We first introduce the concept of mirrorability as the ability of amodel to produce symmetric results in mirrored images and introduce acorresponding measure, namely the \textit{mirror error} that is defined as thedifference between the detection result on an image and the mirror of thedetection result on its mirror image. We evaluate the mirrorability of severalstate of the art algorithms in two of the most intensively studied problems,namely human pose estimation and face alignment. Our experiments lead toseveral interesting findings: 1) Surprisingly, most of state of the art methodsstruggle to preserve the mirror symmetry, despite the fact that they do havevery similar overall performance on the original and mirror images; 2) the lowmirrorability is not caused by training or testing sample bias - all algorithmsare trained on both the original images and their mirrored versions; 3) themirror error is strongly correlated to the localization/alignment error (withcorrelation coefficients around 0.7). Since the mirror error is calculatedwithout knowledge of the ground truth, we show two interesting applications -in the first it is used to guide the selection of difficult samples and in thesecond to give feedback in a popular Cascaded Pose Regression method for facealignment.
arxiv-8700-62 | Lazier ABC | http://arxiv.org/pdf/1501.05144v1.pdf | author:Dennis Prangle category:stat.CO stat.ML published:2015-01-21 summary:ABC algorithms involve a large number of simulations from the model ofinterest, which can be very computationally costly. This paper summarises thelazy ABC algorithm of Prangle (2015), which reduces the computational demand byabandoning many unpromising simulations before completion. By using a randomstopping decision and reweighting the output sample appropriately, the targetdistribution is the same as for standard ABC. Lazy ABC is also extended here tothe case of non-uniform ABC kernels, which is shown to simplify the process oftuning the algorithm effectively.
arxiv-8700-63 | A General Theory of Hypothesis Tests and Confidence Regions for Sparse High Dimensional Models | http://arxiv.org/pdf/1412.8765v2.pdf | author:Yang Ning, Han Liu category:stat.ML published:2014-12-30 summary:We consider the problem of uncertainty assessment for low dimensionalcomponents in high dimensional models. Specifically, we propose a decorrelatedscore function to handle the impact of high dimensional nuisance parameters. Weconsider both hypothesis tests and confidence regions for generic penalizedM-estimators. Unlike most existing inferential methods which are tailored forindividual models, our approach provides a general framework for highdimensional inference and is applicable to a wide range of applications. Fromthe testing perspective, we develop general theorems to characterize thelimiting distributions of the decorrelated score test statistic under both nullhypothesis and local alternatives. These results provide asymptotic guaranteeson the type I errors and local powers of the proposed test. Furthermore, weshow that the decorrelated score function can be used to construct point andconfidence region estimators that are semiparametrically efficient. We alsogeneralize this framework to broaden its applications. First, we extend it tohandle high dimensional null hypothesis, where the number of parameters ofinterest can increase exponentially fast with the sample size. Second, weestablish the theory for model misspecification. Third, we go beyond thelikelihood framework, by introducing the generalized score test based ongeneral loss functions. Thorough numerical studies are conducted to back up thedeveloped theoretical results.
arxiv-8700-64 | Tracking an Object with Unknown Accelerations using a Shadowing Filter | http://arxiv.org/pdf/1502.07743v1.pdf | author:Kevin Judd category:cs.SY cs.CV math.OC published:2015-01-21 summary:A commonly encountered problem is the tracking of a physical object, like amaneuvering ship, aircraft, land vehicle, spacecraft or animate creaturecarrying a wireless device. The sensor data is often limited and inaccurateobservations of range or bearing. This problem is more difficult than trackinga ballistic trajectory, because an operative affects unknown and arbitrarilychanging accelerations. Although stochastic methods of filtering or stateestimation (Kalman filters and particle filters) are widely used, out of voguevariational methods are more appropriate in this tracking context, because theobjects do not typically display any significant random motions at the lengthand time scales of interest. This leads us to propose a rather elegant approachbased on a \emph{shadowing filter}. The resulting filter is efficient (reducesto the solution of linear equations) and robust (uneffected by missing data andsingular correlations that would cause catastrophic failure of Bayesianfilters.) The tracking is so robust, that in some common situations it actuallyperforms better by ignoring error correlations that are so vital to Kalmanfilters.
arxiv-8700-65 | Convergent Bayesian formulations of blind source separation and electromagnetic source estimation | http://arxiv.org/pdf/1501.05069v1.pdf | author:Kevin H. Knuth, Herbert G. Vaughan Jr category:physics.med-ph stat.ML published:2015-01-21 summary:We consider two areas of research that have been developing in parallel overthe last decade: blind source separation (BSS) and electromagnetic sourceestimation (ESE). BSS deals with the recovery of source signals when onlymixtures of signals can be obtained from an array of detectors and the onlyprior knowledge consists of some information about the nature of the sourcesignals. On the other hand, ESE utilizes knowledge of the electromagneticforward problem to assign source signals to their respective generators, whileinformation about the signals themselves is typically ignored. We demonstratethat these two techniques can be derived from the same starting point using theBayesian formalism. This suggests a means by which new algorithms can bedeveloped that utilize as much relevant information as possible. We alsobriefly mention some preliminary work that supports the value of integratinginformation used by these two techniques and review the kinds of informationthat may be useful in addressing the ESE problem.
arxiv-8700-66 | Difficulties applying recent blind source separation techniques to EEG and MEG | http://arxiv.org/pdf/1501.05068v1.pdf | author:Kevin H. Knuth category:physics.med-ph stat.ML published:2015-01-21 summary:High temporal resolution measurements of human brain activity can beperformed by recording the electric potentials on the scalp surface(electroencephalography, EEG), or by recording the magnetic fields near thesurface of the head (magnetoencephalography, MEG). The analysis of the data isproblematic due to the fact that multiple neural generators may besimultaneously active and the potentials and magnetic fields from these sourcesare superimposed on the detectors. It is highly desirable to un-mix the datainto signals representing the behaviors of the original individual generators.This general problem is called blind source separation and several recenttechniques utilizing maximum entropy, minimum mutual information, and maximumlikelihood estimation have been applied. These techniques have had much successin separating signals such as natural sounds or speech, but appear to beineffective when applied to EEG or MEG signals. Many of these techniquesimplicitly assume that the source distributions have a large kurtosis, whereasan analysis of EEG/MEG signals reveals that the distributions are multimodal.This suggests that more effective separation techniques could be designed forEEG and MEG signals.
arxiv-8700-67 | Abrupt Motion Tracking via Nearest Neighbor Field Driven Stochastic Sampling | http://arxiv.org/pdf/1410.7484v2.pdf | author:Tianfei Zhou, Yao Lu, Feng Lv, Huijun Di, Qingjie Zhao, Jian Zhang category:cs.CV published:2014-10-28 summary:Stochastic sampling based trackers have shown good performance for abruptmotion tracking so that they have gained popularity in recent years. However,conventional methods tend to use a two-stage sampling paradigm, in which thesearch space needs to be uniformly explored with an inefficient preliminarysampling phase. In this paper, we propose a novel sampling-based method in theBayesian filtering framework to address the problem. Within the framework,nearest neighbor field estimation is utilized to compute the importanceproposal probabilities, which guide the Markov chain search towards promisingregions and thus enhance the sampling efficiency; given the motion priors, asmoothing stochastic sampling Monte Carlo algorithm is proposed to approximatethe posterior distribution through a smoothing weight-updating scheme.Moreover, to track the abrupt and the smooth motions simultaneously, we developan abrupt-motion detection scheme which can discover the presence of abruptmotions during online tracking. Extensive experiments on challenging imagesequences demonstrate the effectiveness and the robustness of our algorithm inhandling the abrupt motions.
arxiv-8700-68 | Regroupement sémantique de définitions en espagnol | http://arxiv.org/pdf/1501.04920v1.pdf | author:Gerardo Sierra, Juan-Manuel Torres-Moreno, Alejandro Molina category:cs.IR cs.CL published:2015-01-20 summary:This article focuses on the description and evaluation of a new unsupervisedlearning method of clustering of definitions in Spanish according to theirsemantic. Textual Energy was used as a clustering measure, and we study anadaptation of the Precision and Recall to evaluate our method.
arxiv-8700-69 | Scalable Multi-Output Label Prediction: From Classifier Chains to Classifier Trellises | http://arxiv.org/pdf/1501.04870v1.pdf | author:J. Read, L. Martino, P. Olmos, D. Luengo category:stat.ML cs.CV cs.DS cs.LG stat.CO published:2015-01-20 summary:Multi-output inference tasks, such as multi-label classification, have becomeincreasingly important in recent years. A popular method for multi-labelclassification is classifier chains, in which the predictions of individualclassifiers are cascaded along a chain, thus taking into account inter-labeldependencies and improving the overall performance. Several varieties ofclassifier chain methods have been introduced, and many of them perform verycompetitively across a wide range of benchmark datasets. However, scalabilitylimitations become apparent on larger datasets when modeling a fully-cascadedchain. In particular, the methods' strategies for discovering and modeling agood chain structure constitutes a mayor computational bottleneck. In thispaper, we present the classifier trellis (CT) method for scalable multi-labelclassification. We compare CT with several recently proposed classifier chainmethods to show that it occupies an important niche: it is highly competitiveon standard multi-label problems, yet it can also scale up to thousands or eventens of thousands of labels.
arxiv-8700-70 | Separation of undersampled composite signals using the Dantzig selector with overcomplete dictionaries | http://arxiv.org/pdf/1501.04819v1.pdf | author:Ashley Prater, Lixin Shen category:math.NA stat.ML published:2015-01-20 summary:In many applications one may acquire a composition of several signals thatmay be corrupted by noise, and it is a challenging problem to reliably separatethe components from one another without sacrificing significant details. Addingto the challenge, in a compressive sensing framework, one is given only anundersampled set of linear projections of the composite signal. In this paper,we propose using the Dantzig selector model incorporating an overcompletedictionary to separate a noisy undersampled collection of composite signals,and present an algorithm to efficiently solve the model. The Dantzig selector is a statistical approach to finding a solution to anoisy linear regression problem by minimizing the $\ell_1$ norm of candidatecoefficient vectors while constraining the scope of the residuals. If theunderlying coefficient vector is sparse, then the Dantzig selector performswell in the recovery and separation of the unknown composite signal. In thefollowing, we propose a proximity operator based algorithm to recover andseparate unknown noisy undersampled composite signals through the Dantzigselector. We present numerical simulations comparing the proposed algorithmwith the competing Alternating Direction Method, and the proposed algorithm isfound to be faster, while producing similar quality results. Additionally, wedemonstrate the utility of the proposed algorithm in several experiments byapplying it in various domain applications including the recovery ofcomplex-valued coefficient vectors, the removal of impulse noise from smoothsignals, and the separation and classification of a composition of handwrittendigits.
arxiv-8700-71 | Visualizing the Effects of a Changing Distance on Data Using Continuous Embeddings | http://arxiv.org/pdf/1311.1911v2.pdf | author:Gina Gruenhage, Manfred Opper, Simon Barthelme category:stat.ML published:2013-11-08 summary:Most ML methods, from clustering to classification, rely on a distancefunction to describe relationships between datapoints. For complex datasets itis hard to avoid making some arbitrary choices when defining a distancefunction. To compare images, one must choose a spatial scale, for signals, atemporal scale. The right scale is hard to pin down and it is preferable whenresults do not depend too tightly on the exact value one picked. Topologicaldata analysis seeks to address this issue by focusing on the notion ofneighbourhood instead of distance. Here, we show that in some cases a simplersolution is available. One can check how strongly distance relationships dependon a hyperparameter using dimensionality reduction. We formulate a variant ofdynamical multi-dimensional scaling (MDS), which embeds datapoints as curves.The resulting algorithm is based on the Concave-Convex Procedure (CCCP) andprovides a simple and efficient way of visualizing changes and invariances indistance patterns as a hyperparameter is varied. We also present a variant toanalyze the dependence on multiple hyperparameters. We provide a cMDS algorithmthat is straightforward to implement, use and extend. To illustrate thepossibilities of cMDS, we apply cMDS to several real-world data sets.
arxiv-8700-72 | Fast Localization of Facial Landmark Points | http://arxiv.org/pdf/1403.6888v2.pdf | author:Nenad Markuš, Miroslav Frljak, Igor S. Pandžić, Jörgen Ahlberg, Robert Forchheimer category:cs.CV published:2014-03-26 summary:Localization of salient facial landmark points, such as eye corners or thetip of the nose, is still considered a challenging computer vision problemdespite recent efforts. This is especially evident in unconstrainedenvironments, i.e., in the presence of background clutter and large head posevariations. Most methods that achieve state-of-the-art accuracy are slow, and,thus, have limited applications. We describe a method that can accuratelyestimate the positions of relevant facial landmarks in real-time even onhardware with limited processing power, such as mobile devices. This isachieved with a sequence of estimators based on ensembles of regression trees.The trees use simple pixel intensity comparisons in their internal nodes andthis makes them able to process image regions very fast. We test the developedsystem on several publicly available datasets and analyse its processing speedon various devices. Experimental results show that our method has practicalvalue.
arxiv-8700-73 | Distributed Data Association in Smart Camera Networks via Dual Decomposition | http://arxiv.org/pdf/1501.04754v1.pdf | author:Jiuqing Wan, Yuting Nie, Li Liu category:cs.CV published:2015-01-20 summary:One of the fundamental requirements for visual surveillance using smartcamera networks is the correct association of each persons observationsgenerated on different cameras. Recently, distributed data association thatinvolves only local information processing on each camera node and mutualinformation exchanging between neighboring cameras has attracted many researchinterests due to its superiority in large scale applications. In this paper, weformulate the problem of data association in smart camera networks as anInteger Programming problem by introducing a set of linking variables, andpropose two distributed algorithms, namely L-DD and Q-DD, to solve the IntegerProgramming problem using dual decomposition technique. In our algorithms, theoriginal IP problem is decomposed into several sub-problems, which can besolved locally and efficiently on each smart camera, and then differentsub-problems reach consensus on their solutions in a rigorous way by adjustingtheir parameters based on projected sub-gradient optimization. The proposedmethods are simple and flexible, in that (i) we can incorporate any featureextraction and matching technique into our framework to measure the similaritybetween two observations, which is used to define the cost of each link, and(ii) we can decompose the original problem in any way as long as the resultingsub-problem can be solved independently on individual camera. We show thecompetitiveness of our methods in both accuracy and speed by theoreticalanalysis and experimental comparison with state of the art algorithms on tworeal data sets collected by camera networks in our campus garden and officebuilding.
arxiv-8700-74 | Building DNN Acoustic Models for Large Vocabulary Speech Recognition | http://arxiv.org/pdf/1406.7806v2.pdf | author:Andrew L. Maas, Peng Qi, Ziang Xie, Awni Y. Hannun, Christopher T. Lengerich, Daniel Jurafsky, Andrew Y. Ng category:cs.CL cs.LG cs.NE stat.ML published:2014-06-30 summary:Deep neural networks (DNNs) are now a central component of nearly allstate-of-the-art speech recognition systems. Building neural network acousticmodels requires several design decisions including network architecture, size,and training loss function. This paper offers an empirical investigation onwhich aspects of DNN acoustic model design are most important for speechrecognition system performance. We report DNN classifier performance and finalspeech recognizer word error rates, and compare DNNs using several metrics toquantify factors influencing differences in task performance. Our first set ofexperiments use the standard Switchboard benchmark corpus, which containsapproximately 300 hours of conversational telephone speech. We compare standardDNNs to convolutional networks, and present the first experiments usinglocally-connected, untied neural networks for acoustic modeling. Weadditionally build systems on a corpus of 2,100 hours of training data bycombining the Switchboard and Fisher corpora. This larger corpus allows us tomore thoroughly examine performance of large DNN models -- with up to ten timesmore parameters than those typically used in speech recognition systems. Ourresults suggest that a relatively simple DNN architecture and optimizationtechnique produces strong results. These findings, along with previous work,help establish a set of best practices for building DNN hybrid speechrecognition systems with maximum likelihood training. Our experiments in DNNoptimization additionally serve as a case study for training DNNs withdiscriminative loss functions for speech tasks, as well as DNN classifiers moregenerally.
arxiv-8700-75 | Learning Invariants using Decision Trees | http://arxiv.org/pdf/1501.04725v1.pdf | author:Siddharth Krishna, Christian Puhrsch, Thomas Wies category:cs.PL cs.LG published:2015-01-20 summary:The problem of inferring an inductive invariant for verifying program safetycan be formulated in terms of binary classification. This is a standard problemin machine learning: given a sample of good and bad points, one is asked tofind a classifier that generalizes from the sample and separates the two sets.Here, the good points are the reachable states of the program, and the badpoints are those that reach a safety property violation. Thus, a learnedclassifier is a candidate invariant. In this paper, we propose a new algorithmthat uses decision trees to learn candidate invariants in the form of arbitraryBoolean combinations of numerical inequalities. We have used our algorithm toverify C programs taken from the literature. The algorithm is able to infersafe invariants for a range of challenging benchmarks and compares favorably toother ML-based invariant inference techniques. In particular, it scales well tolarge sample sets.
arxiv-8700-76 | Robust Face Recognition by Constrained Part-based Alignment | http://arxiv.org/pdf/1501.04717v1.pdf | author:Yuting Zhang, Kui Jia, Yueming Wang, Gang Pan, Tsung-Han Chan, Yi Ma category:cs.CV cs.LG published:2015-01-20 summary:Developing a reliable and practical face recognition system is along-standing goal in computer vision research. Existing literature suggeststhat pixel-wise face alignment is the key to achieve high-accuracy facerecognition. By assuming a human face as piece-wise planar surfaces, where eachsurface corresponds to a facial part, we develop in this paper a ConstrainedPart-based Alignment (CPA) algorithm for face recognition across pose and/orexpression. Our proposed algorithm is based on a trainable CPA model, whichlearns appearance evidence of individual parts and a tree-structured shapeconfiguration among different parts. Given a probe face, CPA simultaneouslyaligns all its parts by fitting them to the appearance evidence withconsideration of the constraint from the tree-structured shape configuration.This objective is formulated as a norm minimization problem regularized bygraph likelihoods. CPA can be easily integrated with many existing classifiersto perform part-based face recognition. Extensive experiments on benchmark facedatasets show that CPA outperforms or is on par with existing methods forrobust face recognition across pose, expression, and/or illumination changes.
arxiv-8700-77 | DeepHash: Getting Regularization, Depth and Fine-Tuning Right | http://arxiv.org/pdf/1501.04711v1.pdf | author:Jie Lin, Olivier Morere, Vijay Chandrasekhar, Antoine Veillard, Hanlin Goh category:cs.CV cs.IR published:2015-01-20 summary:This work focuses on representing very high-dimensional global imagedescriptors using very compact 64-1024 bit binary hashes for instanceretrieval. We propose DeepHash: a hashing scheme based on deep networks. Key tomaking DeepHash work at extremely low bitrates are three importantconsiderations -- regularization, depth and fine-tuning -- each requiringsolutions specific to the hashing problem. In-depth evaluation shows that ourscheme consistently outperforms state-of-the-art methods across all data setsfor both Fisher Vectors and Deep Convolutional Neural Network features, by upto 20 percent over other schemes. The retrieval performance with 256-bit hashesis close to that of the uncompressed floating point features -- a remarkable512 times compression.
arxiv-8700-78 | Comment on "Clustering by fast search and find of density peaks" | http://arxiv.org/pdf/1501.04267v2.pdf | author:Shuliang Wang, Dakui Wang, Caoyuan Li, Yan Li category:cs.LG published:2015-01-18 summary:In [1], a clustering algorithm was given to find the centers of clustersquickly. However, the accuracy of this algorithm heavily depend on thethreshold value of d-c. Furthermore, [1] has not provided any efficient way toselect the threshold value of d-c, that is, one can have to estimate the valueof d_c depend on one's subjective experience. In this paper, based on the datafield [2], we propose a new way to automatically extract the threshold value ofd_c from the original data set by using the potential entropy of data field.For any data set to be clustered, the most reasonable value of d_c can beobjectively calculated from the data set by using our proposed method. The sameexperiments in [1] are redone with our proposed method on the same experimentaldata set used in [1], the results of which shows that the problem to calculatethe threshold value of d_c in [1] has been solved by using our method.
arxiv-8700-79 | Learning Mixtures of Bernoulli Templates by Two-Round EM with Performance Guarantee | http://arxiv.org/pdf/1305.0319v6.pdf | author:Adrian Barbu, Tianfu Wu, Ying Nian Wu category:stat.ML published:2013-05-02 summary:Dasgupta and Shulman showed that a two-round variant of the EM algorithm canlearn mixture of Gaussian distributions with near optimal precision with highprobability if the Gaussian distributions are well separated and if thedimension is sufficiently high. In this paper, we generalize their theory tolearning mixture of high-dimensional Bernoulli templates. Each template is abinary vector, and a template generates examples by randomly switching itsbinary components independently with a certain probability. In computer visionapplications, a binary vector is a feature map of an image, where each binarycomponent indicates whether a local feature or structure is present or absentwithin a certain cell of the image domain. A Bernoulli template can beconsidered as a statistical model for images of objects (or parts of objects)from the same category. We show that the two-round EM algorithm can learnmixture of Bernoulli templates with near optimal precision with highprobability, if the Bernoulli templates are sufficiently different and if thenumber of features is sufficiently high. We illustrate the theoretical resultsby synthetic and real examples.
arxiv-8700-80 | Naive-Deep Face Recognition: Touching the Limit of LFW Benchmark or Not? | http://arxiv.org/pdf/1501.04690v1.pdf | author:Erjin Zhou, Zhimin Cao, Qi Yin category:cs.CV published:2015-01-20 summary:Face recognition performance improves rapidly with the recent deep learningtechnique developing and underlying large training dataset accumulating. Inthis paper, we report our observations on how big data impacts the recognitionperformance. According to these observations, we build our Megvii FaceRecognition System, which achieves 99.50% accuracy on the LFW benchmark,outperforming the previous state-of-the-art. Furthermore, we report theperformance in a real-world security certification scenario. There still existsa clear gap between machine recognition and human performance. We summarize ourexperiments and present three challenges lying ahead in recent facerecognition. And we indicate several possible solutions towards thesechallenges. We hope our work will stimulate the community's discussion of thedifference between research benchmark and real-world applications.
arxiv-8700-81 | Deep Convolutional Neural Networks for Action Recognition Using Depth Map Sequences | http://arxiv.org/pdf/1501.04686v1.pdf | author:Pichao Wang, Wanqing Li, Zhimin Gao, Jing Zhang, Chang Tang, Philip Ogunbona category:cs.CV published:2015-01-20 summary:Recently, deep learning approach has achieved promising results in variousfields of computer vision. In this paper, a new framework called HierarchicalDepth Motion Maps (HDMM) + 3 Channel Deep Convolutional Neural Networks(3ConvNets) is proposed for human action recognition using depth map sequences.Firstly, we rotate the original depth data in 3D pointclouds to mimic therotation of cameras, so that our algorithms can handle view variant cases.Secondly, in order to effectively extract the body shape and motioninformation, we generate weighted depth motion maps (DMM) at several temporalscales, referred to as Hierarchical Depth Motion Maps (HDMM). Then, threechannels of ConvNets are trained on the HDMMs from three projected orthogonalplanes separately. The proposed algorithms are evaluated on MSRAction3D,MSRAction3DExt, UTKinect-Action and MSRDailyActivity3D datasets respectively.We also combine the last three datasets into a larger one (called CombinedDataset) and test the proposed method on it. The results show that our approachcan achieve state-of-the-art results on the individual datasets and withoutdramatical performance degradation on the Combined Dataset.
arxiv-8700-82 | Sparse Bayesian Learning for EEG Source Localization | http://arxiv.org/pdf/1501.04621v1.pdf | author:Sajib Saha, Frank de Hoog, Ya. I. Nesterets, Rajib Rana, M. Tahtali, T. E. Gureyev category:q-bio.QM cs.LG q-bio.NC published:2015-01-19 summary:Purpose: Localizing the sources of electrical activity fromelectroencephalographic (EEG) data has gained considerable attention over thelast few years. In this paper, we propose an innovative source localizationmethod for EEG, based on Sparse Bayesian Learning (SBL). Methods: To betterspecify the sparsity profile and to ensure efficient source localization, theproposed approach considers grouping of the electrical current dipoles insidehuman brain. SBL is used to solve the localization problem in addition withimposed constraint that the electric current dipoles associated with the brainactivity are isotropic. Results: Numerical experiments are conducted on arealistic head model that is obtained by segmentation of MRI images of the headand includes four major components, namely the scalp, the skull, thecerebrospinal fluid (CSF) and the brain, with appropriate relative conductivityvalues. The results demonstrate that the isotropy constraint significantlyimproves the performance of SBL. In a noiseless environment, the proposedmethod was 1 found to accurately (with accuracy of >75%) locate up to 6simultaneously active sources, whereas for SBL without the isotropy constraint,the accuracy of finding just 3 simultaneously active sources was <75%.Conclusions: Compared to the state-of-the-art algorithms, the proposed methodis potentially more consistent in specifying the sparsity profile of humanbrain activity and is able to produce better source localization for EEG.
arxiv-8700-83 | Implementable confidence sets in high dimensional regression | http://arxiv.org/pdf/1501.04467v1.pdf | author:Alexandra Carpentier category:stat.ML published:2015-01-19 summary:We consider the setting of linear regression in high dimension. We focus onthe problem of constructing adaptive and honest confidence sets for the sparseparameter \theta, i.e. we want to construct a confidence set for theta thatcontains theta with high probability, and that is as small as possible. The l_2diameter of a such confidence set should depend on the sparsity S of \theta -the larger S, the wider the confidence set. However, in practice, S is unknown.This paper focuses on constructing a confidence set for \theta which contains\theta with high probability, whose diameter is adaptive to the unknownsparsity S, and which is implementable in practice.
arxiv-8700-84 | An Evolutionary Optimization Approach to Risk Parity Portfolio Selection | http://arxiv.org/pdf/1411.7494v2.pdf | author:Ronald Hochreiter category:q-fin.PM cs.NE published:2014-11-27 summary:In this paper we present an evolutionary optimization approach to solve therisk parity portfolio selection problem. While there exist convex optimizationapproaches to solve this problem when long-only portfolios are considered, theoptimization problem becomes non-trivial in the long-short case. To solve thisproblem, we propose a genetic algorithm as well as a local search heuristic.This algorithmic framework is able to compute solutions successfully. Numericalresults using real-world data substantiate the practicability of the approachpresented in this paper.
arxiv-8700-85 | Statistical-mechanical analysis of pre-training and fine tuning in deep learning | http://arxiv.org/pdf/1501.04413v1.pdf | author:Masayuki Ohzeki category:stat.ML cs.AI cs.LG published:2015-01-19 summary:In this paper, we present a statistical-mechanical analysis of deep learning.We elucidate some of the essential components of deep learning---pre-trainingby unsupervised learning and fine tuning by supervised learning. We formulatethe extraction of features from the training data as a margin criterion in ahigh-dimensional feature-vector space. The self-organized classifier is thensupplied with small amounts of labelled data, as in deep learning. Although weemploy a simple single-layer perceptron model, rather than directly analyzing amulti-layer neural network, we find a nontrivial phase transition that isdependent on the number of unlabelled data in the generalization error of theresultant classifier. In this sense, we evaluate the efficacy of theunsupervised learning component of deep learning. The analysis is performed bythe replica method, which is a sophisticated tool in statistical mechanics. Wevalidate our result in the manner of deep learning, using a simple iterativealgorithm to learn the weight vector on the basis of belief propagation.
arxiv-8700-86 | Structure Learning in Bayesian Networks of Moderate Size by Efficient Sampling | http://arxiv.org/pdf/1501.04370v1.pdf | author:Ru He, Jin Tian, Huaiqing Wu category:cs.AI cs.LG stat.ML published:2015-01-19 summary:We study the Bayesian model averaging approach to learning Bayesian networkstructures (DAGs) from data. We develop new algorithms including the firstalgorithm that is able to efficiently sample DAGs according to the exactstructure posterior. The DAG samples can then be used to construct estimatorsfor the posterior of any feature. We theoretically prove good properties of ourestimators and empirically show that our estimators considerably outperform theestimators from the previous state-of-the-art methods.
arxiv-8700-87 | Reconstruction-free action inference from compressive imagers | http://arxiv.org/pdf/1501.04367v1.pdf | author:Kuldeep Kulkarni, Pavan Turaga category:cs.CV published:2015-01-18 summary:Persistent surveillance from camera networks, such as at parking lots, UAVs,etc., often results in large amounts of video data, resulting in significantchallenges for inference in terms of storage, communication and computation.Compressive cameras have emerged as a potential solution to deal with the datadeluge issues in such applications. However, inference tasks such as actionrecognition require high quality features which implies reconstructing theoriginal video data. Much work in compressive sensing (CS) theory is gearedtowards solving the reconstruction problem, where state-of-the-art methods arecomputationally intensive and provide low-quality results at high compressionrates. Thus, reconstruction-free methods for inference are much desired. Inthis paper, we propose reconstruction-free methods for action recognition fromcompressive cameras at high compression ratios of 100 and above. Recognizingactions directly from CS measurements requires features which are mostlynonlinear and thus not easily applicable. This leads us to search for suchproperties that are preserved in compressive measurements. To this end, wepropose the use of spatio-temporal smashed filters, which are compressivedomain versions of pixel-domain matched filters. We conduct experiments onpublicly available databases and show that one can obtain recognition ratesthat are comparable to the oracle method in uncompressed setup, even for highcompression ratios.
arxiv-8700-88 | Mathematical Language Processing: Automatic Grading and Feedback for Open Response Mathematical Questions | http://arxiv.org/pdf/1501.04346v1.pdf | author:Andrew S. Lan, Divyanshu Vats, Andrew E. Waters, Richard G. Baraniuk category:stat.ML cs.AI cs.CL cs.LG published:2015-01-18 summary:While computer and communication technologies have provided effective meansto scale up many aspects of education, the submission and grading ofassessments such as homework assignments and tests remains a weak link. In thispaper, we study the problem of automatically grading the kinds of open responsemathematical questions that figure prominently in STEM (science, technology,engineering, and mathematics) courses. Our data-driven framework formathematical language processing (MLP) leverages solution data from a largenumber of learners to evaluate the correctness of their solutions, assignpartial-credit scores, and provide feedback to each learner on the likelylocations of any errors. MLP takes inspiration from the success of naturallanguage processing for text data and comprises three main steps. First, weconvert each solution to an open response mathematical question into a seriesof numerical features. Second, we cluster the features from several solutionsto uncover the structures of correct, partially correct, and incorrectsolutions. We develop two different clustering approaches, one that leveragesgeneric clustering algorithms and one based on Bayesian nonparametrics. Third,we automatically grade the remaining (potentially large number of) solutionsbased on their assigned cluster and one instructor-provided grade per cluster.As a bonus, we can track the cluster assignment of each step of a multistepsolution and determine when it departs from a cluster of correct solutions,which enables us to indicate the likely locations of errors to learners. Wetest and validate MLP on real-world MOOC data to demonstrate how it cansubstantially reduce the human effort required in large-scale educationalplatforms.
arxiv-8700-89 | Deep Belief Nets for Topic Modeling | http://arxiv.org/pdf/1501.04325v1.pdf | author:Lars Maaloe, Morten Arngren, Ole Winther category:cs.CL cs.LG stat.ML published:2015-01-18 summary:Applying traditional collaborative filtering to digital publishing ischallenging because user data is very sparse due to the high volume ofdocuments relative to the number of users. Content based approaches, on theother hand, is attractive because textual content is often very informative. Inthis paper we describe large-scale content based collaborative filtering fordigital publishing. To solve the digital publishing recommender problem wecompare two approaches: latent Dirichlet allocation (LDA) and deep belief nets(DBN) that both find low-dimensional latent representations for documents.Efficient retrieval can be carried out in the latent representation. We workboth on public benchmarks and digital media content provided by Issuu, anonline publishing platform. This article also comes with a newly developed deepbelief nets toolbox for topic modeling tailored towards performance evaluationof the DBN model and comparisons to the LDA model.
arxiv-8700-90 | Phrase Based Language Model For Statistical Machine Translation | http://arxiv.org/pdf/1501.04324v1.pdf | author:Jia Xu, Geliang Chen category:cs.CL published:2015-01-18 summary:We consider phrase based Language Models (LM), which generalize the commonlyused word level models. Similar concept on phrase based LMs appears in speechrecognition, which is rather specialized and thus less suitable for machinetranslation (MT). In contrast to the dependency LM, we first introduce theexhaustive phrase-based LMs tailored for MT use. Preliminary experimentalresults show that our approach outperform word based LMs with the respect toperplexity and translation quality.
arxiv-8700-91 | A Generalized Affinity Propagation Clustering Algorithm for Nonspherical Cluster Discovery | http://arxiv.org/pdf/1501.04318v1.pdf | author:Teng Qiu, Yongjie Li category:cs.LG cs.CV stat.ML published:2015-01-18 summary:Clustering analysis aims to discover the underlying clusters in the datapoints according to their similarities. It has wide applications ranging frombioinformatics to astronomy. Here, we proposed a Generalized AffinityPropagation (G-AP) clustering algorithm. Data points are first organized in asparsely connected in-tree (IT) structure by a physically inspired strategy.Then, additional edges are added to the IT structure for those reachable nodes.This expanded structure is subsequently trimmed by affinity propagation method.Consequently, the underlying cluster structure, with separate clusters,emerges. In contrast to other IT-based methods, G-AP is fully automatic andtakes as input the pairs of similarities between data points only. Unlikeaffinity propagation, G-AP is capable of discovering nonspherical clusters.
arxiv-8700-92 | Information Theory and its Relation to Machine Learning | http://arxiv.org/pdf/1501.04309v1.pdf | author:Bao-Gang Hu category:cs.IT cs.LG math.IT published:2015-01-18 summary:In this position paper, I first describe a new perspective on machinelearning (ML) by four basic problems (or levels), namely, "What to learn?","How to learn?", "What to evaluate?", and "What to adjust?". The paper stressesmore on the first level of "What to learn?", or "Learning Target Selection".Towards this primary problem within the four levels, I briefly review theexisting studies about the connection between information theoretical learning(ITL [1]) and machine learning. A theorem is given on the relation between theempirically-defined similarity measure and information measures. Finally, aconjecture is proposed for pursuing a unified mathematical interpretation tolearning target selection.
arxiv-8700-93 | Image classification by visual bag-of-words refinement and reduction | http://arxiv.org/pdf/1501.04292v1.pdf | author:Zhiwu Lu, Liwei Wang, Ji-Rong Wen category:cs.CV published:2015-01-18 summary:This paper presents a new framework for visual bag-of-words (BOW) refinementand reduction to overcome the drawbacks associated with the visual BOW modelwhich has been widely used for image classification. Although very influentialin the literature, the traditional visual BOW model has two distinct drawbacks.Firstly, for efficiency purposes, the visual vocabulary is commonly constructedby directly clustering the low-level visual feature vectors extracted fromlocal keypoints, without considering the high-level semantics of images. Thatis, the visual BOW model still suffers from the semantic gap, and thus may leadto significant performance degradation in more challenging tasks (e.g. socialimage classification). Secondly, typically thousands of visual words aregenerated to obtain better performance on a relatively large image dataset. Dueto such large vocabulary size, the subsequent image classification may takesheer amount of time. To overcome the first drawback, we develop a graph-basedmethod for visual BOW refinement by exploiting the tags (easy to accessalthough noisy) of social images. More notably, for efficient imageclassification, we further reduce the refined visual BOW model to a muchsmaller size through semantic spectral clustering. Extensive experimentalresults show the promising performance of the proposed framework for visual BOWrefinement and reduction.
arxiv-8700-94 | Pairwise Constraint Propagation on Multi-View Data | http://arxiv.org/pdf/1501.04284v1.pdf | author:Zhiwu Lu, Liwei Wang category:cs.CV cs.LG published:2015-01-18 summary:This paper presents a graph-based learning approach to pairwise constraintpropagation on multi-view data. Although pairwise constraint propagation hasbeen studied extensively, pairwise constraints are usually defined over pairsof data points from a single view, i.e., only intra-view constraint propagationis considered for multi-view tasks. In fact, very little attention has beenpaid to inter-view constraint propagation, which is more challenging sincepairwise constraints are now defined over pairs of data points from differentviews. In this paper, we propose to decompose the challenging inter-viewconstraint propagation problem into semi-supervised learning subproblems sothat they can be efficiently solved based on graph-based label propagation. Tothe best of our knowledge, this is the first attempt to give an efficientsolution to inter-view constraint propagation from a semi-supervised learningviewpoint. Moreover, since graph-based label propagation has been adopted forbasic optimization, we develop two constrained graph construction methods forinterview constraint propagation, which only differ in how the intra-viewpairwise constraints are exploited. The experimental results in cross-viewretrieval have shown the promising performance of our inter-view constraintpropagation.
arxiv-8700-95 | Regularized maximum correntropy machine | http://arxiv.org/pdf/1501.04282v1.pdf | author:Jim Jing-Yan Wang, Yunji Wang, Bing-Yi Jing, Xin Gao category:cs.LG published:2015-01-18 summary:In this paper we investigate the usage of regularized correntropy frameworkfor learning of classifiers from noisy labels. The class label predictorslearned by minimizing transitional loss functions are sensitive to the noisyand outlying labels of training samples, because the transitional lossfunctions are equally applied to all the samples. To solve this problem, wepropose to learn the class label predictors by maximizing the correntropybetween the predicted labels and the true labels of the training samples, underthe regularized Maximum Correntropy Criteria (MCC) framework. Moreover, weregularize the predictor parameter to control the complexity of the predictor.The learning problem is formulated by an objective function considering theparameter regularization and MCC simultaneously. By optimizing the objectivefunction alternately, we develop a novel predictor learning algorithm. Theexperiments on two chal- lenging pattern classification tasks show that itsignificantly outperforms the machines with transitional loss functions.
arxiv-8700-96 | Correntropy Induced L2 Graph for Robust Subspace Clustering | http://arxiv.org/pdf/1501.04277v1.pdf | author:Canyi Lu, Jinhui Tang, Min Lin, Liang Lin, Shuicheng Yan, Zhouchen Lin category:cs.CV published:2015-01-18 summary:In this paper, we study the robust subspace clustering problem, which aims tocluster the given possibly noisy data points into their underlying subspaces. Alarge pool of previous subspace clustering methods focus on the graphconstruction by different regularization of the representation coefficient. Weinstead focus on the robustness of the model to non-Gaussian noises. We proposea new robust clustering method by using the correntropy induced metric, whichis robust for handling the non-Gaussian and impulsive noises. Also we furtherextend the method for handling the data with outlier rows/features. Themultiplicative form of half-quadratic optimization is used to optimize thenon-convex correntropy objective function of the proposed models. Extensiveexperiments on face datasets well demonstrate that the proposed methods aremore robust to corruptions and occlusions.
arxiv-8700-97 | Correlation Adaptive Subspace Segmentation by Trace Lasso | http://arxiv.org/pdf/1501.04276v1.pdf | author:Canyi Lu, Jiashi Feng, Zhouchen Lin, Shuicheng Yan category:cs.CV published:2015-01-18 summary:This paper studies the subspace segmentation problem. Given a set of datapoints drawn from a union of subspaces, the goal is to partition them intotheir underlying subspaces they were drawn from. The spectral clustering methodis used as the framework. It requires to find an affinity matrix which is closeto block diagonal, with nonzero entries corresponding to the data point pairsfrom the same subspace. In this work, we argue that both sparsity and thegrouping effect are important for subspace segmentation. A sparse affinitymatrix tends to be block diagonal, with less connections between data pointsfrom different subspaces. The grouping effect ensures that the highly correcteddata which are usually from the same subspace can be grouped together. SparseSubspace Clustering (SSC), by using $\ell^1$-minimization, encourages sparsityfor data selection, but it lacks of the grouping effect. On the contrary,Low-Rank Representation (LRR), by rank minimization, and Least SquaresRegression (LSR), by $\ell^2$-regularization, exhibit strong grouping effect,but they are short in subset selection. Thus the obtained affinity matrix isusually very sparse by SSC, yet very dense by LRR and LSR. In this work, we propose the Correlation Adaptive Subspace Segmentation(CASS) method by using trace Lasso. CASS is a data correlation dependent methodwhich simultaneously performs automatic data selection and groups correlateddata together. It can be regarded as a method which adaptively balances SSC andLSR. Both theoretical and experimental results show the effectiveness of CASS.
arxiv-8700-98 | Implementation of Auto Monitoring and Short-Message-Service System via GSM Modem | http://arxiv.org/pdf/1501.01548v2.pdf | author:Akilan Thangarajah, Buddhapala Wongkaew, Mongkol Ekpanyapong category:cs.CV published:2015-01-07 summary:Auto-Monitoring and Short-Messaging-Service System is a real-time monitoringsystem for any critical operational environments. It detects an undesired eventoccurring in the environment, generates an alert with detailed message andsends it to the user to prevent hazards. This system employs a Friendly ARM asmain controller while, sensors and terminals to interact with the real world. AGSM network is utilized to bridge the communication between monitoring systemand user. This paper presents details of prototyping the system.
arxiv-8700-99 | Generalised Random Forest Space Overview | http://arxiv.org/pdf/1501.04244v1.pdf | author:Miron B. Kursa category:cs.LG published:2015-01-17 summary:Assuming a view of the Random Forest as a special case of a nested ensembleof interchangeable modules, we construct a generalisation space allowing one toeasily develop novel methods based on this algorithm. We discuss the role andrequired properties of modules at each level, especially in context of somealready proposed RF generalisations.
arxiv-8700-100 | Constructing the L2-Graph for Robust Subspace Learning and Subspace Clustering | http://arxiv.org/pdf/1209.0841v7.pdf | author:Xi Peng, Zhiding Yu, Huajin Tang, Zhang Yi category:cs.CV cs.MM published:2012-09-05 summary:Under the framework of graph-based learning, the key to robust subspaceclustering and subspace learning is to obtain a good similarity graph thateliminates the effects of errors and retains only connections between the datapoints from the same subspace (i.e., intra-subspace data points). Recent worksachieve good performance by modeling errors into their objective functions toremove the errors from the inputs. However, these approaches face thelimitations that the structure of errors should be known prior and a complexconvex problem must be solved. In this paper, we present a novel method toeliminate the effects of the errors from the projection space (representation)rather than from the input space. We first prove that $\ell_1$-, $\ell_2$-,$\ell_{\infty}$-, and nuclear-norm based linear projection spaces share theproperty of Intra-subspace Projection Dominance (IPD), i.e., the coefficientsover intra-subspace data points are larger than those over inter-subspace datapoints. Based on this property, we introduce a method to construct a sparsesimilarity graph, called L2-Graph. The subspace clustering and subspacelearning algorithms are developed upon L2-Graph. Experiments show that L2-Graphalgorithms outperform the state-of-the-art methods for feature extraction,image clustering, and motion segmentation in terms of accuracy, robustness, andtime efficiency.
arxiv-8700-101 | Meaningful Objects Segmentation from SAR Images via A Multi-Scale Non-Local Active Contour Model | http://arxiv.org/pdf/1501.04163v1.pdf | author:Gui-Song Xia, Gang Liu, Wen Yang category:cs.CV published:2015-01-17 summary:The segmentation of synthetic aperture radar (SAR) images is a longstandingyet challenging task, not only because of the presence of speckle, but also dueto the variations of surface backscattering properties in the images.Tremendous investigations have been made to eliminate the speckle effects forthe segmentation of SAR images, while few work devotes to dealing with thevariations of backscattering coefficients in the images. In order to overcomeboth the two difficulties, this paper presents a novel SAR image segmentationmethod by exploiting a multi-scale active contour model based on the non-localprocessing principle. More precisely, we first formulize the SAR segmentationproblem with an active contour model by integrating the non-local interactionsbetween pairs of patches inside and outside the segmented regions. Secondly, amulti-scale strategy is proposed to speed up the non-local active contoursegmentation procedure and to avoid falling into local minimum for achievingmore accurate segmentation results. Experimental results on simulated and realSAR images demonstrate the efficiency and feasibility of the proposed method:it can not only achieve precise segmentations for images with heavy specklesand non-local intensity variations, but also can be used for SAR images fromdifferent types of sensors.
arxiv-8700-102 | A Fast Fractal Image Compression Algorithm Using Predefined Values for Contrast Scaling | http://arxiv.org/pdf/1501.04140v1.pdf | author:H. Miar Naimi, M. Salarian category:cs.CV published:2015-01-17 summary:In this paper a new fractal image compression algorithm is proposed in whichthe time of encoding process is considerably reduced. The algorithm exploits adomain pool reduction approach, along with using innovative predefined valuesfor contrast scaling factor, S, instead of scanning the parameter space [0,1].Within this approach only domain blocks with entropies greater than a thresholdare considered. As a novel point, it is assumed that in each step of theencoding process, the domain block with small enough distance shall be foundonly for the range blocks with low activity (equivalently low entropy). Thisnovel point is used to find reasonable estimations of S, and use them in theencoding process as predefined values, mentioned above. The algorithm has beenexamined for some well-known images. This result shows that our proposedalgorithm considerably reduces the encoding time producing images that areapproximately the same in quality.
arxiv-8700-103 | Multidimensional Digital Filters for Point-Target Detection in Cluttered Infrared Scenes | http://arxiv.org/pdf/1408.2590v3.pdf | author:Hugh L. Kennedy category:cs.CV published:2014-08-12 summary:A 3-D spatiotemporal prediction-error filter (PEF), is used to enhanceforeground/background contrast in (real and simulated) sensor image sequences.Relative velocity is utilized to extract point-targets that would otherwise beindistinguishable on spatial frequency alone. An optical-flow field isgenerated using local estimates of the 3-D autocorrelation function via theapplication of the fast Fourier transform (FFT) and inverse FFT. Velocityestimates are then used to tune in a background-whitening PEF that is matchedto the motion and texture of the local background. Finite-impulse-response(FIR) filters are designed and implemented in the frequency domain. Ananalytical expression for the frequency response of velocity-tuned FIR filters,of odd or even dimension, with an arbitrary delay in each dimension, isderived.
arxiv-8700-104 | Semi-Supervised Sparse Coding | http://arxiv.org/pdf/1311.6834v2.pdf | author:Jim Jing-Yan Wang, Xin Gao category:stat.ML cs.LG published:2013-11-26 summary:Sparse coding approximates the data sample as a sparse linear combination ofsome basic codewords and uses the sparse codes as new presentations. In thispaper, we investigate learning discriminative sparse codes by sparse coding ina semi-supervised manner, where only a few training samples are labeled. Byusing the manifold structure spanned by the data set of both labeled andunlabeled samples and the constraints provided by the labels of the labeledsamples, we learn the variable class labels for all the samples. Furthermore,to improve the discriminative ability of the learned sparse codes, we assumethat the class labels could be predicted from the sparse codes directly using alinear classifier. By solving the codebook, sparse codes, class labels andclassifier parameters simultaneously in a unified objective function, wedevelop a semi-supervised sparse coding algorithm. Experiments on tworeal-world pattern recognition problems demonstrate the advantage of theproposed methods over supervised sparse coding methods on partially labeleddata sets.
arxiv-8700-105 | Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm | http://arxiv.org/pdf/1310.5715v5.pdf | author:Deanna Needell, Nathan Srebro, Rachel Ward category:math.NA cs.CV cs.LG math.OC stat.ML published:2013-10-21 summary:We obtain an improved finite-sample guarantee on the linear convergence ofstochastic gradient descent for smooth and strongly convex objectives,improving from a quadratic dependence on the conditioning $(L/\mu)^2$ (where$L$ is a bound on the smoothness and $\mu$ on the strong convexity) to a lineardependence on $L/\mu$. Furthermore, we show how reweighting the samplingdistribution (i.e. importance sampling) is necessary in order to furtherimprove convergence, and obtain a linear dependence in the average smoothness,dominating previous results. We also discuss importance sampling for SGD morebroadly and show how it can improve convergence also in other scenarios. Ourresults are based on a connection we make between SGD and the randomizedKaczmarz algorithm, which allows us to transfer ideas between the separatebodies of literature studying each of the two methods. In particular, we recastthe randomized Kaczmarz algorithm as an instance of SGD, and apply our resultsto prove its exponential convergence, but to the solution of a weighted leastsquares problem rather than the original least squares problem. We then presenta modified Kaczmarz algorithm with partially biased sampling which doesconverge to the original least squares solution with the same exponentialconvergence rate.
arxiv-8700-106 | Coevolutionary intransitivity in games: A landscape analysis | http://arxiv.org/pdf/1501.04010v1.pdf | author:Hendrik Richter category:cs.NE q-bio.PE published:2015-01-16 summary:Intransitivity is supposed to be a main reason for deficits in coevolutionaryprogress and inheritable superiority. Besides, coevolutionary dynamics ischaracterized by interactions yielding subjective fitness, but aiming atsolutions that are superior with respect to an objective measurement. Such anapproximation of objective fitness may be, for instance, generalizationperformance. In the paper a link between rating-- and ranking--based measuresof intransitivity and fitness landscapes that can address the dichotomy betweensubjective and objective fitness is explored. The approach is illustrated bynumerical experiments involving a simple random game with continuously tunabledegree of randomness.
arxiv-8700-107 | Stochastic Gradient Based Extreme Learning Machines For Online Learning of Advanced Combustion Engines | http://arxiv.org/pdf/1501.03975v1.pdf | author:Vijay Manikandan Janakiraman, XuanLong Nguyen, Dennis Assanis category:cs.NE cs.LG cs.SY published:2015-01-16 summary:In this article, a stochastic gradient based online learning algorithm forExtreme Learning Machines (ELM) is developed (SG-ELM). A stability criterionbased on Lyapunov approach is used to prove both asymptotic stability ofestimation error and stability in the estimated parameters suitable foridentification of nonlinear dynamic systems. The developed algorithm not onlyguarantees stability, but also reduces the computational demand compared to theOS-ELM approach based on recursive least squares. In order to demonstrate theeffectiveness of the algorithm on a real-world scenario, an advanced combustionengine identification problem is considered. The algorithm is applied to twocase studies: An online regression learning for system identification of aHomogeneous Charge Compression Ignition (HCCI) Engine and an onlineclassification learning (with class imbalance) for identifying the dynamicoperating envelope of the HCCI Engine. The results indicate that the accuracyof the proposed SG-ELM is comparable to that of the state-of-the-art but addsstability and a reduction in computational effort.
arxiv-8700-108 | Nonlinear Model Predictive Control of A Gasoline HCCI Engine Using Extreme Learning Machines | http://arxiv.org/pdf/1501.03969v1.pdf | author:Vijay Manikandan Janakiraman, XuanLong Nguyen, Dennis Assanis category:cs.SY cs.NE published:2015-01-16 summary:Homogeneous charge compression ignition (HCCI) is a futuristic combustiontechnology that operates with a high fuel efficiency and reduced emissions.HCCI combustion is characterized by complex nonlinear dynamics whichnecessitates a model based control approach for automotive application. HCCIengine control is a nonlinear, multi-input multi-output problem with state andactuator constraints which makes controller design a challenging task. TypicalHCCI controllers make use of a first principles based model which involves along development time and cost associated with expert labor and calibration. Inthis paper, an alternative approach based on machine learning is presentedusing extreme learning machines (ELM) and nonlinear model predictive control(MPC). A recurrent ELM is used to learn the nonlinear dynamics of HCCI engineusing experimental data and is shown to accurately predict the engine behaviorseveral steps ahead in time, suitable for predictive control. Using the ELMengine models, an MPC based control algorithm with a simplified quadraticprogram update is derived for real time implementation. The working andeffectiveness of the MPC approach has been analyzed on a nonlinear HCCI enginemodel for tracking multiple reference quantities along with constraints definedby HCCI states, actuators and operational limits.
arxiv-8700-109 | Value Iteration with Options and State Aggregation | http://arxiv.org/pdf/1501.03959v1.pdf | author:Kamil Ciosek, David Silver category:cs.AI cs.LG stat.ML published:2015-01-16 summary:This paper presents a way of solving Markov Decision Processes that combinesstate abstraction and temporal abstraction. Specifically, we combine stateaggregation with the options framework and demonstrate that they work welltogether and indeed it is only after one combines the two that the full benefitof each is realized. We introduce a hierarchical value iteration algorithmwhere we first coarsely solve subgoals and then use these approximate solutionsto exactly solve the MDP. This algorithm solved several problems faster thanvanilla value iteration.
arxiv-8700-110 | Mind the Gap: Subspace based Hierarchical Domain Adaptation | http://arxiv.org/pdf/1501.03952v1.pdf | author:Anant Raj, Vinay P. Namboodiri, Tinne Tuytelaars category:cs.CV published:2015-01-16 summary:Domain adaptation techniques aim at adapting a classifier learnt on a sourcedomain to work on the target domain. Exploiting the subspaces spanned byfeatures of the source and target domains respectively is one approach that hasbeen investigated towards solving this problem. These techniques normallyassume the existence of a single subspace for the entire source / targetdomain. In this work, we consider the hierarchical organization of the data andconsider multiple subspaces for the source and target domain based on thehierarchy. We evaluate different subspace based domain adaptation techniquesunder this setting and observe that using different subspaces based on thehierarchy yields consistent improvement over a non-hierarchical baseline
arxiv-8700-111 | Multi-view learning for multivariate performance measures optimization | http://arxiv.org/pdf/1501.03786v2.pdf | author:Jim Jing-Yan Wang category:cs.LG published:2015-01-15 summary:In this paper, we propose the problem of optimizing multivariate performancemeasures from multi-view data, and an effective method to solve it. Thisproblem has two features: the data points are presented by multiple views, andthe target of learning is to optimize complex multivariate performancemeasures. We propose to learn a linear discriminant functions for each view,and combine them to construct a overall multivariate mapping function formult-view data. To learn the parameters of the linear dis- criminant functionsof different views to optimize multivariate performance measures, we formulatea optimization problem. In this problem, we propose to minimize the complexityof the linear discriminant functions of each view, encourage the consistencesof the responses of different views over the same data points, and minimize theupper boundary of a given multivariate performance measure. To optimize thisproblem, we employ the cutting-plane method in an iterative algorithm. In eachiteration, we update a set of constrains, and optimize the mapping functionparameter of each view one by one.
arxiv-8700-112 | Feature Selection based on Machine Learning in MRIs for Hippocampal Segmentation | http://arxiv.org/pdf/1501.03915v1.pdf | author:Sabina Tangaro, Nicola Amoroso, Massimo Brescia, Stefano Cavuoti, Andrea Chincarini, Rosangela Errico, Paolo Inglese, Giuseppe Longo, Rosalia Maglietta, Andrea Tateo, Giuseppe Riccio, Roberto Bellotti category:physics.med-ph cs.CV cs.LG published:2015-01-16 summary:Neurodegenerative diseases are frequently associated with structural changesin the brain. Magnetic Resonance Imaging (MRI) scans can show these variationsand therefore be used as a supportive feature for a number of neurodegenerativediseases. The hippocampus has been known to be a biomarker for Alzheimerdisease and other neurological and psychiatric diseases. However, it requiresaccurate, robust and reproducible delineation of hippocampal structures. Fullyautomatic methods are usually the voxel based approach, for each voxel a numberof local features were calculated. In this paper we compared four differenttechniques for feature selection from a set of 315 features extracted for eachvoxel: (i) filter method based on the Kolmogorov-Smirnov test; two wrappermethods, respectively, (ii) Sequential Forward Selection and (iii) SequentialBackward Elimination; and (iv) embedded method based on the Random ForestClassifier on a set of 10 T1-weighted brain MRIs and tested on an independentset of 25 subjects. The resulting segmentations were compared with manualreference labelling. By using only 23 features for each voxel (sequentialbackward elimination) we obtained comparable state of-the-art performances withrespect to the standard tool FreeSurfer.
arxiv-8700-113 | Prediction and Modularity in Dynamical Systems | http://arxiv.org/pdf/1106.3703v2.pdf | author:Artemy Kolchinsky, Luis M. Rocha category:nlin.AO cs.AI cs.IT cs.LG cs.SY math.IT q-bio.QM stat.ME G.3 published:2011-06-19 summary:Identifying and understanding modular organizations is centrally important inthe study of complex systems. Several approaches to this problem have beenadvanced, many framed in information-theoretic terms. Our treatment starts fromthe complementary point of view of statistical modeling and prediction ofdynamical systems. It is known that for finite amounts of training data,simpler models can have greater predictive power than more complex ones. We usethe trade-off between model simplicity and predictive accuracy to generateoptimal multiscale decompositions of dynamical networks into weakly-coupled,simple modules. State-dependent and causal versions of our method are alsoproposed.
arxiv-8700-114 | A new ADMM algorithm for the Euclidean median and its application to robust patch regression | http://arxiv.org/pdf/1501.03879v1.pdf | author:Kunal N. Chaudhury, K. R. Ramakrishnan category:cs.CV published:2015-01-16 summary:The Euclidean Median (EM) of a set of points $\Omega$ in an Euclidean spaceis the point x minimizing the (weighted) sum of the Euclidean distances of x tothe points in $\Omega$. While there exits no closed-form expression for the EM,it can nevertheless be computed using iterative methods such as the Wieszfeldalgorithm. The EM has classically been used as a robust estimator of centralityfor multivariate data. It was recently demonstrated that the EM can be used toperform robust patch-based denoising of images by generalizing the popularNon-Local Means algorithm. In this paper, we propose a novel algorithm forcomputing the EM (and its box-constrained counterpart) using variable splittingand the method of augmented Lagrangian. The attractive feature of this approachis that the subproblems involved in the ADMM-based optimization of theaugmented Lagrangian can be resolved using simple closed-form projections. Theproposed ADMM solver is used for robust patch-based image denoising and isshown to exhibit faster convergence compared to an existing solver.
arxiv-8700-115 | Bayesian Nonparametrics in Topic Modeling: A Brief Tutorial | http://arxiv.org/pdf/1501.03861v1.pdf | author:Alexander Spangher category:stat.ML published:2015-01-16 summary:Using nonparametric methods has been increasingly explored in Bayesianhierarchical modeling as a way to increase model flexibility. Although thefield shows a lot of promise, inference in many models, including HierachicalDirichlet Processes (HDP), remain prohibitively slow. One promising pathforward is to exploit the submodularity inherent in Indian Buffet Process (IBP)to derive near-optimal solutions in polynomial time. In this work, I willpresent a brief tutorial on Bayesian nonparametric methods, especially as theyare applied to topic modeling. I will show a comparison between differentnon-parametric models and the current state-of-the-art parametric model, LatentDirichlet Allocation (LDA).
arxiv-8700-116 | Stable Camera Motion Estimation Using Convex Programming | http://arxiv.org/pdf/1312.5047v3.pdf | author:Onur Ozyesil, Amit Singer, Ronen Basri category:cs.CV published:2013-12-18 summary:We study the inverse problem of estimating n locations $t_1, ..., t_n$ (up toglobal scale, translation and negation) in $R^d$ from noisy measurements of asubset of the (unsigned) pairwise lines that connect them, that is, from noisymeasurements of $\pm (t_i - t_j)/\t_i - t_j\$ for some pairs (i,j) (where thesigns are unknown). This problem is at the core of the structure from motion(SfM) problem in computer vision, where the $t_i$'s represent camera locationsin $R^3$. The noiseless version of the problem, with exact line measurements,has been considered previously under the general title of parallel rigiditytheory, mainly in order to characterize the conditions for unique realizationof locations. For noisy pairwise line measurements, current methods tend toproduce spurious solutions that are clustered around a few locations. Thissensitivity of the location estimates is a well-known problem in SfM,especially for large, irregular collections of images. In this paper we introduce a semidefinite programming (SDP) formulation,specially tailored to overcome the clustering phenomenon. We further identifythe implications of parallel rigidity theory for the location estimationproblem to be well-posed, and prove exact (in the noiseless case) and stablelocation recovery results. We also formulate an alternating direction method tosolve the resulting semidefinite program, and provide a distributed version ofour formulation for large numbers of locations. Specifically for the cameralocation estimation problem, we formulate a pairwise line estimation methodbased on robust camera orientation and subspace estimation. Lastly, wedemonstrate the utility of our algorithm through experiments on real images.
arxiv-8700-117 | PAC-Bayes with Minimax for Confidence-Rated Transduction | http://arxiv.org/pdf/1501.03838v1.pdf | author:Akshay Balsubramani, Yoav Freund category:cs.LG stat.ML published:2015-01-15 summary:We consider using an ensemble of binary classifiers for transductiveprediction, when unlabeled test data are known in advance. We derive minimaxoptimal rules for confidence-rated prediction in this setting. By usingPAC-Bayes analysis on these rules, we obtain data-dependent performanceguarantees without distributional assumptions on the data. Our analysistechniques are readily extended to a setting in which the predictor is allowedto abstain.
arxiv-8700-118 | Perfect Clustering for Stochastic Blockmodel Graphs via Adjacency Spectral Embedding | http://arxiv.org/pdf/1310.0532v4.pdf | author:Vince Lyzinski, Daniel Sussman, Minh Tang, Avanti Athreya, Carey Priebe category:stat.ML published:2013-10-02 summary:Vertex clustering in a stochastic blockmodel graph has wide applicability andhas been the subject of extensive research. In thispaper, we provide a shortproof that the adjacency spectral embedding can be used to obtain perfectclustering for the stochastic blockmodel and the degree-corrected stochasticblockmodel. We also show an analogous result for the more general random dotproduct graph model.
arxiv-8700-119 | The Fast Convergence of Incremental PCA | http://arxiv.org/pdf/1501.03796v1.pdf | author:Akshay Balsubramani, Sanjoy Dasgupta, Yoav Freund category:cs.LG stat.ML published:2015-01-15 summary:We consider a situation in which we see samples in $\mathbb{R}^d$ drawni.i.d. from some distribution with mean zero and unknown covariance A. We wishto compute the top eigenvector of A in an incremental fashion - with analgorithm that maintains an estimate of the top eigenvector in O(d) space, andincrementally adjusts the estimate with each new data point that arrives. Twoclassical such schemes are due to Krasulina (1969) and Oja (1983). We givefinite-sample convergence rates for both.
arxiv-8700-120 | Computer-assisted polyp matching between optical colonoscopy and CT colonography: a phantom study | http://arxiv.org/pdf/1501.03779v1.pdf | author:Holger R. Roth, Thomas E. Hampshire, Emma Helbren, Mingxing Hu, Roser Vega, Steve Halligan, David J. Hawkes category:cs.CV published:2015-01-15 summary:Potentially precancerous polyps detected with CT colonography (CTC) need tobe removed subsequently, using an optical colonoscope (OC). Due to largecolonic deformations induced by the colonoscope, even very experiencedcolonoscopists find it difficult to pinpoint the exact location of thecolonoscope tip in relation to polyps reported on CTC. This can cause undulyprolonged OC examinations that are stressful for the patient, colonoscopist andsupporting staff. We developed a method, based on monocular 3D reconstruction from OC images,that automatically matches polyps observed in OC with polyps reported on priorCTC. A matching cost is computed, using rigid point-based registration betweensurface point clouds extracted from both modalities. A 3D printed and paintedphantom of a 25 cm long transverse colon segment was used to validate themethod on two medium sized polyps. Results indicate that the matching cost issmaller at the correct corresponding polyp between OC and CTC: the value is 3.9times higher at the incorrect polyp, comparing the correct match between polypsto the incorrect match. Furthermore, we evaluate the matching of thereconstructed polyp from OC with other colonic endoluminal surface structuressuch as haustral folds and show that there is a minimum at the correct polypfrom CTC. Automated matching between polyps observed at OC and prior CTC wouldfacilitate the biopsy or removal of true-positive pathology or exclusion offalse-positive CTC findings, and would reduce colonoscopy false-negative(missed) polyps. Ultimately, such a method might reduce healthcare costs,patient inconvenience and discomfort.
arxiv-8700-121 | Submodular relaxation for inference in Markov random fields | http://arxiv.org/pdf/1501.03771v1.pdf | author:Anton Osokin, Dmitry Vetrov category:cs.CV math.OC stat.ML published:2015-01-15 summary:In this paper we address the problem of finding the most probable state of adiscrete Markov random field (MRF), also known as the MRF energy minimizationproblem. The task is known to be NP-hard in general and its practicalimportance motivates numerous approximate algorithms. We propose a submodularrelaxation approach (SMR) based on a Lagrangian relaxation of the initialproblem. Unlike the dual decomposition approach of Komodakis et al., 2011 SMRdoes not decompose the graph structure of the initial problem but constructs asubmodular energy that is minimized within the Lagrangian relaxation. Ourapproach is applicable to both pairwise and high-order MRFs and allows to takeinto account global potentials of certain types. We study theoreticalproperties of the proposed approach and evaluate it experimentally.
arxiv-8700-122 | Visual Analytics of Image-Centric Cohort Studies in Epidemiology | http://arxiv.org/pdf/1501.04009v1.pdf | author:Bernhard Preim, Paul Klemm, Helwig Hauser, Katrin Hegenscheid, Steffen Oeltze, Klaus Toennies, Henry Völzke category:cs.CV cs.CY published:2015-01-15 summary:Epidemiology characterizes the influence of causes to disease and healthconditions of defined populations. Cohort studies are population-based studiesinvolving usually large numbers of randomly selected individuals and comprisingnumerous attributes, ranging from self-reported interview data to results fromvarious medical examinations, e.g., blood and urine samples. Since recently,medical imaging has been used as an additional instrument to assess riskfactors and potential prognostic information. In this chapter, we discuss suchstudies and how the evaluation may benefit from visual analytics. Clusteranalysis to define groups, reliable image analysis of organs in medical imagingdata and shape space exploration to characterize anatomical shapes are amongthe visual analytics tools that may enable epidemiologists to fully exploit thepotential of their huge and complex data. To gain acceptance, visual analyticstools need to complement more classical epidemiologic tools, primarilyhypothesis-driven statistical analysis.
arxiv-8700-123 | Toward the Coevolution of Novel Vertical-Axis Wind Turbines | http://arxiv.org/pdf/1308.3136v2.pdf | author:Richard J. Preen, Larry Bull category:cs.NE cs.AI cs.CE published:2013-08-13 summary:The production of renewable and sustainable energy is one of the mostimportant challenges currently facing mankind. Wind has made an increasingcontribution to the world's energy supply mix, but still remains a long wayfrom reaching its full potential. In this paper, we investigate the use ofartificial evolution to design vertical-axis wind turbine prototypes that arephysically instantiated and evaluated under fan generated wind conditions.Initially a conventional evolutionary algorithm is used to explore the designspace of a single wind turbine and later a cooperative coevolutionary algorithmis used to explore the design space of an array of wind turbines. Artificialneural networks are used throughout as surrogate models to assist learning andfound to reduce the number of fabrications required to reach a higheraerodynamic efficiency. Unlike in other approaches, such as computational fluiddynamics simulations, no mathematical formulations are used and no modelassumptions are made.
arxiv-8700-124 | LATCH: Learned Arrangements of Three Patch Codes | http://arxiv.org/pdf/1501.03719v1.pdf | author:Gil Levi, Tal Hassner category:cs.CV published:2015-01-15 summary:We present a novel means of describing local image appearances using binarystrings. Binary descriptors have drawn increasing interest in recent years dueto their speed and low memory footprint. A known shortcoming of theserepresentations is their inferior performance compared to larger, histogrambased descriptors such as the SIFT. Our goal is to close this performance gapwhile maintaining the benefits attributed to binary representations. To thisend we propose the Learned Arrangements of Three Patch Codes descriptors, orLATCH. Our key observation is that existing binary descriptors are at anincreased risk from noise and local appearance variations. This, as theycompare the values of pixel pairs; changes to either of the pixels can easilylead to changes in descriptor values, hence damaging its performance. In orderto provide more robustness, we instead propose a novel means of comparing pixelpatches. This ostensibly small change, requires a substantial redesign of thedescriptors themselves and how they are produced. Our resulting LATCHrepresentation is rigorously compared to state-of-the-art binary descriptorsand shown to provide far better performance for similar computation and spacerequirements.
arxiv-8700-125 | Design Mining Interacting Wind Turbines | http://arxiv.org/pdf/1410.0547v2.pdf | author:Richard J. Preen, Larry Bull category:cs.NE cs.AI cs.CE published:2014-10-02 summary:An initial study of surrogate-assisted evolutionary algorithms used to designvertical-axis wind turbines wherein candidate prototypes are evaluated underfan generated wind conditions after being physically instantiated by a 3Dprinter has recently been presented. Unlike other approaches, such ascomputational fluid dynamics simulations, no mathematical formulations wereused and no model assumptions were made. This paper extends that work byexploring alternative surrogate modelling and evolutionary techniques. Theaccuracy of various modelling algorithms used to estimate the fitness ofevaluated individuals from the initial experiments is compared. The effect oftemporally windowing surrogate model training samples is explored. Asurrogate-assisted approach based on an enhanced local search is introduced;and alternative coevolution collaboration schemes are examined.
arxiv-8700-126 | Hard to Cheat: A Turing Test based on Answering Questions about Images | http://arxiv.org/pdf/1501.03302v2.pdf | author:Mateusz Malinowski, Mario Fritz category:cs.AI cs.CL cs.CV cs.LG published:2015-01-14 summary:Progress in language and image understanding by machines has sparkled theinterest of the research community in more open-ended, holistic tasks, andrefueled an old AI dream of building intelligent machines. We discuss a fewprominent challenges that characterize such holistic tasks and argue for"question answering about images" as a particular appealing instance of such aholistic task. In particular, we point out that it is a version of a TuringTest that is likely to be more robust to over-interpretations and contrast itwith tasks like grounding and generation of descriptions. Finally, we discusstools to measure progress in this field.
arxiv-8700-127 | A Modified No Search Algorithm for Fractal Image Compression | http://arxiv.org/pdf/1501.02894v2.pdf | author:Mehdi. Salarian, Babak. Mohamadinia, Jalil Rasekhi category:cs.CV cs.MM published:2015-01-13 summary:Fractal image compression has some desirable properties like high quality athigh compression ratio, fast decoding, and resolution independence. Thereforeit can be used for many applications such as texture mapping and patternrecognition and image watermarking. But it suffers from long encoding time dueto its need to find the best match between sub blocks. This time is related tothe approach that is used. In this paper we present a fast encoding Algorithmbased on no search method. Our goal is that more blocks are covered in initialstep of quad tree algorithm. Experimental result has been compared with othernew fast fractal coding methods, showing it is better in term of bit rate insame condition while the other parameters are fixed.
arxiv-8700-128 | Classifying sequences by the optimized dissimilarity space embedding approach: a case study on the solubility analysis of the E. coli proteome | http://arxiv.org/pdf/1408.3873v2.pdf | author:Lorenzo Livi, Antonello Rizzi, Alireza Sadeghian category:cs.CV cs.AI physics.bio-ph q-bio.BM I.5 published:2014-08-17 summary:We evaluate a version of the recently-proposed classification system namedOptimized Dissimilarity Space Embedding (ODSE) that operates in the input spaceof sequences of generic objects. The ODSE system has been originally presentedas a classification system for patterns represented as labeled graphs. However,since ODSE is founded on the dissimilarity space representation of the inputdata, the classifier can be easily adapted to any input domain where it ispossible to define a meaningful dissimilarity measure. Here we demonstrate theeffectiveness of the ODSE classifier for sequences by considering anapplication dealing with the recognition of the solubility degree of theEscherichia coli proteome. Solubility, or analogously aggregation propensity,is an important property of protein molecules, which is intimately related tothe mechanisms underlying the chemico-physical process of folding. Each proteinof our dataset is initially associated with a solubility degree and it isrepresented as a sequence of symbols, denoting the 20 amino acid residues. Theherein obtained computational results, which we stress that have been achievedwith no context-dependent tuning of the ODSE system, confirm the validity andgenerality of the ODSE-based approach for structured data classification.
arxiv-8700-129 | Dirichlet Process Parsimonious Mixtures for clustering | http://arxiv.org/pdf/1501.03347v1.pdf | author:Faicel Chamroukhi, Marius Bartcus, Hervé Glotin category:stat.ML cs.LG stat.ME published:2015-01-14 summary:The parsimonious Gaussian mixture models, which exploit an eigenvaluedecomposition of the group covariance matrices of the Gaussian mixture, haveshown their success in particular in cluster analysis. Their estimation is ingeneral performed by maximum likelihood estimation and has also been consideredfrom a parametric Bayesian prospective. We propose new Dirichlet ProcessParsimonious mixtures (DPPM) which represent a Bayesian nonparametricformulation of these parsimonious Gaussian mixture models. The proposed DPPMmodels are Bayesian nonparametric parsimonious mixture models that allow tosimultaneously infer the model parameters, the optimal number of mixturecomponents and the optimal parsimonious mixture structure from the data. Wedevelop a Gibbs sampling technique for maximum a posteriori (MAP) estimation ofthe developed DPMM models and provide a Bayesian model selection framework byusing Bayes factors. We apply them to cluster simulated data and real datasets, and compare them to the standard parsimonious mixture models. Theobtained results highlight the effectiveness of the proposed nonparametricparsimonious mixture models as a good nonparametric alternative for theparametric parsimonious models.
arxiv-8700-130 | Image enhancement in intensity projected multichannel MRI using spatially adaptive directional anisotropic diffusion | http://arxiv.org/pdf/1501.03320v1.pdf | author:P. K. Akshara, J. S. Paul category:cs.CV published:2015-01-14 summary:Anisotropic Diffusion is widely used for noise reduction with simultaneouspreservation of vascular structures in maximum intensity projected (MIP)angiograms. However, extension to minimum intensity projected (mIP) venogramsin Susceptibility Weighted Imaging (SWI) poses difficulties due to spatiallyvarying baseline. Here, we introduce a modified version of the directionalanisotropic diffusion which allows us to simultaneously reduce the noise andenhance vascular structures reconstructed using both M/mIP angiograms. Thismethod is based on spatial adaptation of the diffusion function, separately inthe directions of the gradient, and along those of the minimum and maximumcurvatures. The existing approach of directional anisotropic diffusion usesbinary switched diffusion function to ensure diffusion along the direction ofmaximum curvature stopped near the vessel borders. Here, the choice of athreshold for detecting the upper limit of diffusion becomes difficult in thepresence of spatially varying baseline. Also, the approach of using vesselnessmeasure to steer the diffusion process results in structural discontinuitiesdue to junction suppression in mIP. The merits of the proposed method includeelimination of the need for an apriori choice of a threshold to detect thevessel, and problems due to junction suppression. The proposed method is alsoextended to multi-channel phase contrast angiogram.
arxiv-8700-131 | Classification with Low Rank and Missing Data | http://arxiv.org/pdf/1501.03273v1.pdf | author:Elad Hazan, Roi Livni, Yishay Mansour category:cs.LG published:2015-01-14 summary:We consider classification and regression tasks where we have missing dataand assume that the (clean) data resides in a low rank subspace. Finding ahidden subspace is known to be computationally hard. Nevertheless, using anon-proper formulation we give an efficient agnostic algorithm that classifiesas good as the best linear classifier coupled with the best low-dimensionalsubspace in which the data resides. A direct implication is that our algorithmcan linearly (and non-linearly through kernels) classify provably as well asthe best classifier that has access to the full data.
arxiv-8700-132 | Higher dimensional homodyne filtering for suppression of incidental phase artifacts in multichannel MRI | http://arxiv.org/pdf/1501.03271v1.pdf | author:Joseph Suresh Paul, Uma Krishna Swamy Pillai category:cs.CV physics.med-ph published:2015-01-14 summary:The aim of this paper is to introduce procedural steps for extension of the1D homodyne phase correction for k-space truncation in all gradient encodingdirections. Compared to the existing method applied to 2D partial k-space,signal losses introduced by the phase correction filter is observed to beminimal for the extended approach. In addition, the modified form of phasecorrection mitigates Incidental Phase Artifacts (IPA) due to truncation. Forparallel imaging with undersampling along phase encode direction, the extendedhomodyne filtering is shown to be effective for minimizing these artifacts wheneach of the channel k-spaces are truncated along both phase and frequencyencode directions. This is illustrated with 2D partial k-space for flowcompensated multichannel Susceptibility Weighted Imaging (SWI). Extension ofour method to 3D partial k-space shows improved reconstruction of flowinformation in phase contrast angiography.
arxiv-8700-133 | Quantifying Prosodic Variability in Middle English Alliterative Poetry | http://arxiv.org/pdf/1501.03214v1.pdf | author:Roger Bilisoly category:stat.AP cs.CL published:2015-01-14 summary:Interest in the mathematical structure of poetry dates back to at least the19th century: after retiring from his mathematics position, J. J. Sylvesterwrote a book on prosody called $\textit{The Laws of Verse}$. Today there isinterest in the computer analysis of poems, and this paper discusses how astatistical approach can be applied to this task. Starting with the definitionof what Middle English alliteration is, $\textit{Sir Gawain and the GreenKnight}$ and William Langland's $\textit{Piers Plowman}$ are used to illustratethe methodology. Theory first developed for analyzing data from a Riemannianmanifold turns out to be applicable to strings allowing one to compute ageneralized mean and variance for textual data, which is applied to the poemsabove. The ratio of these two variances produces the analogue of the F test,and resampling allows p-values to be estimated. Consequently, this methodologyprovides a way to compare prosodic variability between two texts.
arxiv-8700-134 | Towards Deep Semantic Analysis Of Hashtags | http://arxiv.org/pdf/1501.03210v1.pdf | author:Piyush Bansal, Romil Bansal, Vasudeva Varma category:cs.IR cs.CL published:2015-01-13 summary:Hashtags are semantico-syntactic constructs used across various socialnetworking and microblogging platforms to enable users to start a topicspecific discussion or classify a post into a desired category. Segmenting andlinking the entities present within the hashtags could therefore help in betterunderstanding and extraction of information shared across the social media.However, due to lack of space delimiters in the hashtags (e.g #nsavssnowden),the segmentation of hashtags into constituent entities ("NSA" and "EdwardSnowden" in this case) is not a trivial task. Most of the currentstate-of-the-art social media analytics systems like Sentiment Analysis andEntity Linking tend to either ignore hashtags, or treat them as a single word.In this paper, we present a context aware approach to segment and link entitiesin the hashtags to a knowledge base (KB) entry, based on the context within thetweet. Our approach segments and links the entities in hashtags such that thecoherence between hashtag semantics and the tweet is maximized. To the best ofour knowledge, no existing study addresses the issue of linking entities inhashtags for extracting semantic information. We evaluate our method on twodifferent datasets, and demonstrate the effectiveness of our technique inimproving the overall entity linking in tweets via additional semanticinformation provided by segmenting and linking entities in a hashtag.
arxiv-8700-135 | Annotating Cognates and Etymological Origin in Turkic Languages | http://arxiv.org/pdf/1501.03191v1.pdf | author:Benjamin S. Mericli, Michael Bloodgood category:cs.CL I.2.7 published:2015-01-13 summary:Turkic languages exhibit extensive and diverse etymological relationshipsamong lexical items. These relationships make the Turkic languages promisingfor exploring automated translation lexicon induction by leveraging cognate andother etymological information. However, due to the extent and diversity of thetypes of relationships between words, it is not clear how to annotate suchinformation. In this paper, we present a methodology for annotating cognatesand etymological origin in Turkic languages. Our method strives to balance theamount of research effort the annotator expends with the utility of theannotations for supporting research on improving automated translation lexiconinduction.
arxiv-8700-136 | Robust and Real Time Detection of Curvy Lanes (Curves) with Desired Slopes for Driving Assistance and Autonomous Vehicles | http://arxiv.org/pdf/1501.03124v1.pdf | author:Amartansh Dubey, K. M. Bhurchandi category:cs.CV published:2015-01-13 summary:One of the biggest reasons for road accidents is curvy lanes and blind turns.Even one of the biggest hurdles for new autonomous vehicles is to detect curvylanes, multiple lanes and lanes with a lot of discontinuity and noise. Thispaper presents very efficient and advanced algorithm for detecting curveshaving desired slopes (especially for detecting curvy lanes in real time) anddetection of curves (lanes) with a lot of noise, discontinuity anddisturbances. Overall aim is to develop robust method for this task which isapplicable even in adverse conditions. Even in some of most famous and usefullibraries like OpenCV and Matlab, there is no function available for detectingcurves having desired slopes , shapes, discontinuities. Only few predefinedshapes like circle, ellipse, etc, can be detected using presently availablefunctions. Proposed algorithm can not only detect curves with discontinuity,noise, desired slope but also it can perform shadow and illumination correctionand detect/ differentiate between different curves.
arxiv-8700-137 | Deep Learning with Nonparametric Clustering | http://arxiv.org/pdf/1501.03084v1.pdf | author:Gang Chen category:cs.LG 68T10 I.2.6 published:2015-01-13 summary:Clustering is an essential problem in machine learning and data mining. Onevital factor that impacts clustering performance is how to learn or design thedata representation (or features). Fortunately, recent advances in deeplearning can learn unsupervised features effectively, and have yielded state ofthe art performance in many classification problems, such as characterrecognition, object recognition and document categorization. However, littleattention has been paid to the potential of deep learning for unsupervisedclustering problems. In this paper, we propose a deep belief network withnonparametric clustering. As an unsupervised method, our model first leveragesthe advantages of deep learning for feature representation and dimensionreduction. Then, it performs nonparametric clustering under a maximum marginframework -- a discriminative clustering model and can be trained onlineefficiently in the code space. Lastly model parameters are refined in the deepbelief network. Thus, this model can learn features for clustering and infermodel complexity in an unified framework. The experimental results show theadvantage of our approach over competitive baselines.
arxiv-8700-138 | Feature Selection Based on Confidence Machine | http://arxiv.org/pdf/1410.5473v2.pdf | author:Chang Liu, Yi Xu category:cs.LG published:2014-10-20 summary:In machine learning and pattern recognition, feature selection has been a hottopic in the literature. Unsupervised feature selection is challenging due tothe loss of labels which would supply the related information.How to define anappropriate metric is the key for feature selection. We propose a filter methodfor unsupervised feature selection which is based on the Confidence Machine.Confidence Machine offers an estimation of confidence on a feature'reliability.In this paper, we provide the math model of Confidence Machine in the contextof feature selection, which maximizes the relevance and minimizes theredundancy of the selected feature. We compare our method against classicfeature selection methods Laplacian Score, Pearson Correlation and PrincipalComponent Analysis on benchmark data sets. The experimental results demonstratethe efficiency and effectiveness of our method.
arxiv-8700-139 | An Adaptive Neuro-Fuzzy Inference System Modeling for Grid-Adaptive Interpolation over Depth Images | http://arxiv.org/pdf/1501.03058v1.pdf | author:Arbaaz Singh Sidhu category:cs.CV published:2015-01-13 summary:A suitable interpolation method is essential to keep the noise level minimumalong with the time-delay. In recent years, many different interpolationfilters have been developed for instance H.264-6 tap filter, and AVS- 4 tapfilter. The present work uses Adaptive Neuro-Fuzzy Inference System (ANFIS)technique to model and investigate the effects of a four-tap low-pass tapfilter (Grid-adaptive filter) on a hole-filled depth image. The workdemonstrates the general form of uniform interpolations for both integer andsub-pixel locations in terms of the sampling interval and filter length ofdepth-images via diverse finite impulse response filtering schemes. Thedemonstrated model combined modelling function of fuzzy inference with thelearning ability of artificial neural network.
arxiv-8700-140 | Multi-modal Image Registration for Correlative Microscopy | http://arxiv.org/pdf/1411.3229v2.pdf | author:Tian Cao, Christopher Zach, Shannon Modla, Debbie Powell, Kirk Czymmek, Marc Niethammer category:cs.CV published:2014-11-12 summary:Correlative microscopy is a methodology combining the functionality of lightmicroscopy with the high resolution of electron microscopy and other microscopytechnologies. Image registration for correlative microscopy is quitechallenging because it is a multi-modal, multi-scale and multi-dimensionalregistration problem. In this report, I introduce two methods of imageregistration for correlative microscopy. The first method is based on fiducials(beads). I generate landmarks from the fiducials and compute the similaritytransformation matrix based on three pairs of nearest corresponding landmarks.A least-squares matching process is applied afterwards to further refine theregistration. The second method is inspired by the image analogies approach. Iintroduce the sparse representation model into image analogies. I first trainrepresentative image patches (dictionaries) for pre-registered datasets fromtwo different modalities, and then I use the sparse coding technique totransfer a given image to a predicted image from one modality to another basedon the learned dictionaries. The final image registration is between thepredicted image and the original image corresponding to the given image in thedifferent modality. The method transforms a multi-modal registration problem toa mono-modal one. I test my approaches on Transmission Electron Microscopy(TEM) and confocal microscopy images. Experimental results of the methods arealso shown in this report.
arxiv-8700-141 | Exploring the efficacy of molecular fragments of different complexity in computational SAR modeling | http://arxiv.org/pdf/1501.03015v1.pdf | author:Albrecht Zimmermann, Björn Bringmann, Luc De Raedt category:cs.CE cs.LG published:2015-01-13 summary:An important first step in computational SAR modeling is to transform thecompounds into a representation that can be processed by predictive modelingtechniques. This is typically a feature vector where each feature indicates thepresence or absence of a molecular fragment. While the traditional approach toSAR modeling employed size restricted fingerprints derived from path fragments,much research in recent years focussed on mining more complex graph basedfragments. Today, there seems to be a growing consensus in the data miningcommunity that these more expressive fragments should be more useful. Wequestion this consensus and show experimentally that fragments of lowcomplexity, i.e. sequences, perform better than equally large sets of morecomplex ones, an effect we explain by pairwise correlation among fragments andthe ability of a fragment set to encode compounds from different classesdistinctly. The size restriction on these sets is based on ordering thefragments by class-correlation scores. In addition, we also evaluate theeffects of using a significance value instead of a length restriction for pathfragments and find a significant reduction in the number of features withlittle loss in performance.
arxiv-8700-142 | An Improvement to the Domain Adaptation Bound in a PAC-Bayesian context | http://arxiv.org/pdf/1501.03002v1.pdf | author:Pascal Germain, Amaury Habrard, Francois Laviolette, Emilie Morvant category:stat.ML cs.LG published:2015-01-13 summary:This paper provides a theoretical analysis of domain adaptation based on thePAC-Bayesian theory. We propose an improvement of the previous domainadaptation bound obtained by Germain et al. in two ways. We first give anothergeneralization bound tighter and easier to interpret. Moreover, we provide anew analysis of the constant term appearing in the bound that can be of highinterest for developing new algorithmic solutions.
arxiv-8700-143 | On Generalizing the C-Bound to the Multiclass and Multi-label Settings | http://arxiv.org/pdf/1501.03001v1.pdf | author:Francois Laviolette, Emilie Morvant, Liva Ralaivola, Jean-Francis Roy category:stat.ML cs.LG published:2015-01-13 summary:The C-bound, introduced in Lacasse et al., gives a tight upper bound on therisk of a binary majority vote classifier. In this work, we present a firststep towards extending this work to more complex outputs, by providinggeneralizations of the C-bound to the multiclass and multi-label settings.
arxiv-8700-144 | Improved 8-point Approximate DCT for Image and Video Compression Requiring Only 14 Additions | http://arxiv.org/pdf/1501.02995v1.pdf | author:U. S. Potluri, A. Madanayake, R. J. Cintra, F. M. Bayer, S. Kulasekera, A. Edirisuriya category:cs.MM cs.CV cs.NA stat.ME published:2015-01-13 summary:Video processing systems such as HEVC requiring low energy consumption neededfor the multimedia market has lead to extensive development in fast algorithmsfor the efficient approximation of 2-D DCT transforms. The DCT is employed in amultitude of compression standards due to its remarkable energy compactionproperties. Multiplier-free approximate DCT transforms have been proposed thatoffer superior compression performance at very low circuit complexity. Suchapproximations can be realized in digital VLSI hardware using additions andsubtractions only, leading to significant reductions in chip area and powerconsumption compared to conventional DCTs and integer transforms. In thispaper, we introduce a novel 8-point DCT approximation that requires only 14addition operations and no multiplications. The proposed transform possesseslow computational complexity and is compared to state-of-the-art DCTapproximations in terms of both algorithm complexity and peak signal-to-noiseratio. The proposed DCT approximation is a candidate for reconfigurable videostandards such as HEVC. The proposed transform and several other DCTapproximations are mapped to systolic-array digital architectures andphysically realized as digital prototype circuits using FPGA technology andmapped to 45 nm CMOS technology.
arxiv-8700-145 | Random Bits Regression: a Strong General Predictor for Big Data | http://arxiv.org/pdf/1501.02990v1.pdf | author:Yi Wang, Yi Li, Momiao Xiong, Li Jin category:stat.ML cs.LG published:2015-01-13 summary:To improve accuracy and speed of regressions and classifications, we presenta data-based prediction method, Random Bits Regression (RBR). This method firstgenerates a large number of random binary intermediate/derived features basedon the original input matrix, and then performs regularized linear/logisticregression on those intermediate/derived features to predict the outcome.Benchmark analyses on a simulated dataset, UCI machine learning repositorydatasets and a GWAS dataset showed that RBR outperforms other popular methodsin accuracy and robustness. RBR (available onhttps://sourceforge.net/projects/rbr/) is very fast and requires reasonablememories, therefore, provides a strong, robust and fast predictor in the bigdata era.
arxiv-8700-146 | Geometric Tight Frame based Stylometry for Art Authentication of van Gogh Paintings | http://arxiv.org/pdf/1407.0439v3.pdf | author:Haixia Liu, Raymond H. Chan, Yuan Yao category:cs.LG cs.CV published:2014-07-02 summary:This paper is about authenticating genuine van Gogh paintings from forgeries.The authentication process depends on two key steps: feature extraction andoutlier detection. In this paper, a geometric tight frame and some simplestatistics of the tight frame coefficients are used to extract features fromthe paintings. Then a forward stage-wise rank boosting is used to select asmall set of features for more accurate classification so that van Goghpaintings are highly concentrated towards some center point while forgeries arespread out as outliers. Numerical results show that our method can achieve86.08% classification accuracy under the leave-one-out cross-validationprocedure. Our method also identifies five features that are much morepredominant than other features. Using just these five features forclassification, our method can give 88.61% classification accuracy which is thehighest so far reported in literature. Evaluation of the five features is alsoperformed on two hundred datasets generated by bootstrap sampling withreplacement. The median and the mean are 88.61% and 87.77% respectively. Ourresults show that a small set of statistics of the tight frame coefficientsalong certain orientations can serve as discriminative features for van Goghpaintings. It is more important to look at the tail distributions of suchdirectional coefficients than mean values and standard deviations. It reflectsa highly consistent style in van Gogh's brushstroke movements, where manyforgeries demonstrate a more diverse spread in these features.
arxiv-8700-147 | $\ell_0$ Sparsifying Transform Learning with Efficient Optimal Updates and Convergence Guarantees | http://arxiv.org/pdf/1501.02859v1.pdf | author:Saiprasad Ravishankar, Yoram Bresler category:stat.ML cs.LG published:2015-01-13 summary:Many applications in signal processing benefit from the sparsity of signalsin a certain transform domain or dictionary. Synthesis sparsifying dictionariesthat are directly adapted to data have been popular in applications such asimage denoising, inpainting, and medical image reconstruction. In this work, wefocus instead on the sparsifying transform model, and study the learning ofwell-conditioned square sparsifying transforms. The proposed algorithmsalternate between a $\ell_0$ "norm"-based sparse coding step, and a non-convextransform update step. We derive the exact analytical solution for each ofthese steps. The proposed solution for the transform update step achieves theglobal minimum in that step, and also provides speedups over iterativesolutions involving conjugate gradients. We establish that our alternatingalgorithms are globally convergent to the set of local minimizers of thenon-convex transform learning problems. In practice, the algorithms areinsensitive to initialization. We present results illustrating the promisingperformance and significant speed-ups of transform learning over synthesisK-SVD in image denoising.
arxiv-8700-148 | SPRITE: A Response Model For Multiple Choice Testing | http://arxiv.org/pdf/1501.02844v1.pdf | author:Ryan Ning, Andrew E. Waters, Christoph Studer, Richard G. Baraniuk category:stat.ML published:2015-01-12 summary:Item response theory (IRT) models for categorical response data are widelyused in the analysis of educational data, computerized adaptive testing, andpsychological surveys. However, most IRT models rely on both the assumptionthat categories are strictly ordered and the assumption that this ordering isknown a priori. These assumptions are impractical in many real-world scenarios,such as multiple-choice exams where the levels of incorrectness for thedistractor categories are often unknown. While a number of results exist on IRTmodels for unordered categorical data, they tend to have restrictive modelingassumptions that lead to poor data fitting performance in practice.Furthermore, existing unordered categorical models have parameters that aredifficult to interpret. In this work, we propose a novel methodology forunordered categorical IRT that we call SPRITE (short for stochastic polytomousresponse item model) that: (i) analyzes both ordered and unordered categories,(ii) offers interpretable outputs, and (iii) provides improved data fittingcompared to existing models. We compare SPRITE to existing item response modelsand demonstrate its efficacy on both synthetic and real-world educationaldatasets.
arxiv-8700-149 | Risk-consistency of cross-validation with lasso-type procedures | http://arxiv.org/pdf/1308.0810v2.pdf | author:Darren Homrighausen, Daniel J. McDonald category:math.ST stat.ML stat.TH published:2013-08-04 summary:The lasso and related sparsity inducing algorithms have been the target ofsubstantial theoretical and applied research. Correspondingly, many results areknown about their behavior for a fixed or optimally chosen tuning parameterspecified up to unknown constants. In practice, however, this oracle tuningparameter is inaccessible, so one must instead use the data to choose a tuningparameter. Common statistical practice is to use one of a few variants ofcross-validation for this task. However, very little is known about thetheoretical properties of the resulting predictions using data-dependentmethods. We consider the high-dimensional setting with random design whereinthe number of predictors $p$ grows with the number of observations $n$. We showthat the lasso remains risk consistent relative to its linear oracle even whenthe tuning parameter is chosen via cross-validation and the true model is notnecessarily linear. We generalize these results to the group lasso and$\sqrt{\mbox{lasso}}$ and compare the performance of cross-validation to othertuning parameter selection methods via simulations.
arxiv-8700-150 | A Survey on Recent Advances of Computer Vision Algorithms for Egocentric Video | http://arxiv.org/pdf/1501.02825v1.pdf | author:Sven Bambach category:cs.CV published:2015-01-12 summary:Recent technological advances have made lightweight, head mounted camerasboth practical and affordable and products like Google Glass show firstapproaches to introduce the idea of egocentric (first-person) video to themainstream. Interestingly, the computer vision community has only recentlystarted to explore this new domain of egocentric vision, where research canroughly be categorized into three areas: Object recognition, activitydetection/recognition, video summarization. In this paper, we try to give abroad overview about the different problems that have been addressed andcollect and compare evaluation results. Moreover, along with the emergence ofthis new domain came the introduction of numerous new and versatile benchmarkdatasets, which we summarize and compare as well.
arxiv-8700-151 | Max-Cost Discrete Function Evaluation Problem under a Budget | http://arxiv.org/pdf/1501.02702v1.pdf | author:Feng Nan, Joseph Wang, Venkatesh Saligrama category:cs.LG published:2015-01-12 summary:We propose novel methods for max-cost Discrete Function Evaluation Problem(DFEP) under budget constraints. We are motivated by applications such asclinical diagnosis where a patient is subjected to a sequence of (possiblyexpensive) tests before a decision is made. Our goal is to develop strategiesfor minimizing max-costs. The problem is known to be NP hard and greedy methodsbased on specialized impurity functions have been proposed. We develop a broadclass of \emph{admissible} impurity functions that admit monomials, classes ofpolynomials, and hinge-loss functions that allow for flexible impurity designwith provably optimal approximation bounds. This flexibility is important fordatasets when max-cost can be overly sensitive to "outliers." Outliers biasmax-cost to a few examples that require a large number of tests forclassification. We design admissible functions that allow for accuracy-costtrade-off and result in $O(\log n)$ guarantees of the optimal cost among treeswith corresponding classification accuracy levels.
arxiv-8700-152 | A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse Learning | http://arxiv.org/pdf/1404.2644v3.pdf | author:Aurélien Bellet, Yingyu Liang, Alireza Bagheri Garakani, Maria-Florina Balcan, Fei Sha category:cs.DC cs.LG stat.ML published:2014-04-09 summary:Learning sparse combinations is a frequent theme in machine learning. In thispaper, we study its associated optimization problem in the distributed settingwhere the elements to be combined are not centrally located but spread over anetwork. We address the key challenges of balancing communication costs andoptimization errors. To this end, we propose a distributed Frank-Wolfe (dFW)algorithm. We obtain theoretical guarantees on the optimization error$\epsilon$ and communication cost that do not depend on the total number ofcombining elements. We further show that the communication cost of dFW isoptimal by deriving a lower-bound on the communication cost required toconstruct an $\epsilon$-approximate solution. We validate our theoreticalanalysis with empirical studies on synthetic and real-world data, whichdemonstrate that dFW outperforms both baselines and competing methods. We alsostudy the performance of dFW when the conditions of our analysis are relaxed,and show that dFW is fairly robust.
arxiv-8700-153 | Navigating the Semantic Horizon using Relative Neighborhood Graphs | http://arxiv.org/pdf/1501.02670v1.pdf | author:Amaru Cuba Gyllensten, Magnus Sahlgren category:cs.CL published:2015-01-12 summary:This paper is concerned with nearest neighbor search in distributionalsemantic models. A normal nearest neighbor search only returns a ranked list ofneighbors, with no information about the structure or topology of the localneighborhood. This is a potentially serious shortcoming of the mode of queryinga distributional semantic model, since a ranked list of neighbors may conflateseveral different senses. We argue that the topology of neighborhoods insemantic space provides important information about the different senses ofterms, and that such topological structures can be used for word-senseinduction. We also argue that the topology of the neighborhoods in semanticspace can be used to determine the semantic horizon of a point, which we defineas the set of neighbors that have a direct connection to the point. Weintroduce relative neighborhood graphs as method to uncover the topologicalproperties of neighborhoods in semantic models. We also provide examples ofrelative neighborhood graphs for three well-known semantic models; the PMImodel, the GloVe model, and the skipgram model.
arxiv-8700-154 | Efficient Online Relative Comparison Kernel Learning | http://arxiv.org/pdf/1501.01242v2.pdf | author:Eric Heim, Matthew Berger, Lee M. Seversky, Milos Hauskrecht category:cs.LG published:2015-01-06 summary:Learning a kernel matrix from relative comparison human feedback is animportant problem with applications in collaborative filtering, objectretrieval, and search. For learning a kernel over a large number of objects,existing methods face significant scalability issues inhibiting the applicationof these methods to settings where a kernel is learned in an online and timelyfashion. In this paper we propose a novel framework called Efficient onlineRelative comparison Kernel LEarning (ERKLE), for efficiently learning thesimilarity of a large set of objects in an online manner. We learn a kernelfrom relative comparisons via stochastic gradient descent, one query responseat a time, by taking advantage of the sparse and low-rank properties of thegradient to efficiently restrict the kernel to lie in the space of positivesemidefinite matrices. In addition, we derive a passive-aggressive onlineupdate for minimally satisfying new relative comparisons as to not disrupt theinfluence of previously obtained comparisons. Experimentally, we demonstrate aconsiderable improvement in speed while obtaining improved or comparableaccuracy compared to current methods in the online learning setting.
arxiv-8700-155 | Photonic Delay Systems as Machine Learning Implementations | http://arxiv.org/pdf/1501.02592v1.pdf | author:Michiel Hermans, Miguel Soriano, Joni Dambre, Peter Bienstman, Ingo Fischer category:cs.NE cs.LG published:2015-01-12 summary:Nonlinear photonic delay systems present interesting implementation platformsfor machine learning models. They can be extremely fast, offer great degrees ofparallelism and potentially consume far less power than digital processors. Sofar they have been successfully employed for signal processing using theReservoir Computing paradigm. In this paper we show that their range ofapplicability can be greatly extended if we use gradient descent withbackpropagation through time on a model of the system to optimize the inputencoding of such systems. We perform physical experiments that demonstrate thatthe obtained input encodings work well in reality, and we show that optimizedsystems perform significantly better than the common Reservoir Computingapproach. The results presented here demonstrate that common gradient descenttechniques from machine learning may well be applicable on physicalneuro-inspired analog computers.
arxiv-8700-156 | Combined modeling of sparse and dense noise for improvement of Relevance Vector Machine | http://arxiv.org/pdf/1501.02579v1.pdf | author:Martin Sundin, Saikat Chatterjee, Magnus Jansson category:stat.ML published:2015-01-12 summary:Using a Bayesian approach, we consider the problem of recovering sparsesignals under additive sparse and dense noise. Typically, sparse noise modelsoutliers, impulse bursts or data loss. To handle sparse noise, existing methodssimultaneously estimate the sparse signal of interest and the sparse noise ofno interest. For estimating the sparse signal, without the need of estimatingthe sparse noise, we construct a robust Relevance Vector Machine (RVM). In theRVM, sparse noise and ever present dense noise are treated through a combinednoise model. The precision of combined noise is modeled by a diagonal matrix.We show that the new RVM update equations correspond to a non-symmetricsparsity inducing cost function. Further, the combined modeling is found to becomputationally more efficient. We also extend the method to block-sparsesignals and noise with known and unknown block structures. Through simulations,we show the performance and computation efficiency of the new RVM in severalapplications: recovery of sparse and block sparse signals, housing priceprediction and image denoising.
arxiv-8700-157 | Learning Deep Temporal Representations for Brain Decoding | http://arxiv.org/pdf/1412.7522v4.pdf | author:Orhan Firat, Emre Aksan, Ilke Oztekin, Fatos T. Yarman Vural category:cs.LG cs.NE published:2014-12-23 summary:Functional magnetic resonance imaging produces high dimensional data, with aless then ideal number of labelled samples for brain decoding tasks (predictingbrain states). In this study, we propose a new deep temporal convolutionalneural network architecture with spatial pooling for brain decoding which aimsto reduce dimensionality of feature space along with improved classificationperformance. Temporal representations (filters) for each layer of theconvolutional model are learned by leveraging unlabelled fMRI data in anunsupervised fashion with regularized autoencoders. Learned temporalrepresentations in multiple levels capture the regularities in the temporaldomain and are observed to be a rich bank of activation patterns which alsoexhibit similarities to the actual hemodynamic responses. Further, spatialpooling layers in the convolutional architecture reduce the dimensionalitywithout losing excessive information. By employing the proposed temporalconvolutional architecture with spatial pooling, raw input fMRI data is mappedto a non-linear, highly-expressive and low-dimensional feature space where thefinal classification is conducted. In addition, we propose a simple heuristicapproach for hyper-parameter tuning when no validation data is available.Proposed method is tested on a ten class recognition memory experiment withnine subjects. The results support the efficiency and potential of the proposedmodel, compared to the baseline multi-voxel pattern analysis techniques.
arxiv-8700-158 | A Dataset for Movie Description | http://arxiv.org/pdf/1501.02530v1.pdf | author:Anna Rohrbach, Marcus Rohrbach, Niket Tandon, Bernt Schiele category:cs.CV cs.CL cs.IR published:2015-01-12 summary:Descriptive video service (DVS) provides linguistic descriptions of moviesand allows visually impaired people to follow a movie along with their peers.Such descriptions are by design mainly visual and thus naturally form aninteresting data source for computer vision and computational linguistics. Inthis work we propose a novel dataset which contains transcribed DVS, which istemporally aligned to full length HD movies. In addition we also collected thealigned movie scripts which have been used in prior work and compare the twodifferent sources of descriptions. In total the Movie Description datasetcontains a parallel corpus of over 54,000 sentences and video snippets from 72HD movies. We characterize the dataset by benchmarking different approaches forgenerating video descriptions. Comparing DVS to scripts, we find that DVS isfar more visual and describes precisely what is shown rather than what shouldhappen according to the scripts created prior to movie production.
arxiv-8700-159 | Autodetection and Classification of Hidden Cultural City Districts from Yelp Reviews | http://arxiv.org/pdf/1501.02527v1.pdf | author:Harini Suresh, Nicholas Locascio category:cs.CL cs.AI cs.IR published:2015-01-12 summary:Topic models are a way to discover underlying themes in an otherwiseunstructured collection of documents. In this study, we specifically used theLatent Dirichlet Allocation (LDA) topic model on a dataset of Yelp reviews toclassify restaurants based off of their reviews. Furthermore, we hypothesizethat within a city, restaurants can be grouped into similar "clusters" based onboth location and similarity. We used several different clustering methods,including K-means Clustering and a Probabilistic Mixture Model, in order touncover and classify districts, both well-known and hidden (i.e. cultural areaslike Chinatown or hearsay like "the best street for Italian restaurants")within a city. We use these models to display and label different clusters on amap. We also introduce a topic similarity heatmap that displays the similaritydistribution in a city to a new restaurant.
arxiv-8700-160 | Learning the Conditional Independence Structure of Stationary Time Series: A Multitask Learning Approach | http://arxiv.org/pdf/1404.1361v3.pdf | author:Alexander Jung category:stat.ML published:2014-04-04 summary:We propose a method for inferring the conditional independence graph (CIG) ofa high-dimensional Gaussian vector time series (discrete-time process) from afinite-length observation. By contrast to existing approaches, we do not relyon a parametric process model (such as, e.g., an autoregressive model) for theobserved random process. Instead, we only require certain smoothness properties(in the Fourier domain) of the process. The proposed inference scheme workseven for sample sizes much smaller than the number of scalar process componentsif the true underlying CIG is sufficiently sparse. A theoretical performanceanalysis provides conditions which guarantee that the probability of theproposed inference method to deliver a wrong CIG is below a prescribed value.These conditions imply lower bounds on the sample size such that the new methodis consistent asymptotically. Some numerical experiments validate ourtheoretical performance analysis and demonstrate superior performance of ourscheme compared to an existing (parametric) approach in case of model mismatch.
arxiv-8700-161 | Identifiability and optimal rates of convergence for parameters of multiple types in finite mixtures | http://arxiv.org/pdf/1501.02497v1.pdf | author:Nhat Ho, XuanLong Nguyen category:math.ST stat.ML stat.TH published:2015-01-11 summary:This paper studies identifiability and convergence behaviors for parametersof multiple types in finite mixtures, and the effects of model fitting withextra mixing components. First, we present a general theory for strongidentifiability, which extends from the previous work of Nguyen [2013] and Chen[1995] to address a broad range of mixture models and to handle matrix-variateparameters. These models are shown to share the same Wasserstein distance basedoptimal rates of convergence for the space of mixing distributions ---$n^{-1/2}$ under $W_1$ for the exact-fitted and $n^{-1/4}$ under $W_2$ for theover-fitted setting, where $n$ is the sample size. This theory, however, is notapplicable to several important model classes, including location-scalemultivariate Gaussian mixtures, shape-scale Gamma mixtures andlocation-scale-shape skew-normal mixtures. The second part of this work isdevoted to demonstrating that for these "weakly identifiable" classes,algebraic structures of the density family play a fundamental role indetermining convergence rates of the model parameters, which display a veryrich spectrum of behaviors. For instance, the optimal rate of parameterestimation in an over-fitted location-covariance Gaussian mixture is preciselydetermined by the order of a solvable system of polynomial equations --- theserates deteriorate rapidly as more extra components are added to the model. Theestablished rates for a variety of settings are illustrated by a simulationstudy.
arxiv-8700-162 | Crowd-ML: A Privacy-Preserving Learning Framework for a Crowd of Smart Devices | http://arxiv.org/pdf/1501.02484v1.pdf | author:Jihun Hamm, Adam Champion, Guoxing Chen, Mikhail Belkin, Dong Xuan category:cs.LG cs.CR cs.DC cs.NI published:2015-01-11 summary:Smart devices with built-in sensors, computational capabilities, and networkconnectivity have become increasingly pervasive. The crowds of smart devicesoffer opportunities to collectively sense and perform computing tasks in anunprecedented scale. This paper presents Crowd-ML, a privacy-preserving machinelearning framework for a crowd of smart devices, which can solve a wide rangeof learning problems for crowdsensing data with differential privacyguarantees. Crowd-ML endows a crowdsensing system with an ability to learnclassifiers or predictors online from crowdsensing data privately with minimalcomputational overheads on devices and servers, suitable for a practical andlarge-scale employment of the framework. We analyze the performance and thescalability of Crowd-ML, and implement the system with off-the-shelfsmartphones as a proof of concept. We demonstrate the advantages of Crowd-MLwith real and simulated experiments under various conditions.
arxiv-8700-163 | Semi-supervised Feature Analysis by Mining Correlations among Multiple Tasks | http://arxiv.org/pdf/1411.6232v2.pdf | author:Xiaojun Chang, Yi Yang category:cs.LG published:2014-11-23 summary:In this paper, we propose a novel semi-supervised feature selection frameworkby mining correlations among multiple tasks and apply it to differentmultimedia applications. Instead of independently computing the importance offeatures for each task, our algorithm leverages shared knowledge from multiplerelated tasks, thus, improving the performance of feature selection. Note thatwe build our algorithm on assumption that different tasks share commonstructures. The proposed algorithm selects features in a batch mode, by whichthe correlations between different features are taken into consideration.Besides, considering the fact that labeling a large amount of training data inreal world is both time-consuming and tedious, we adopt manifold learning whichexploits both labeled and unlabeled training data for feature space analysis.Since the objective function is non-smooth and difficult to solve, we proposean iterative algorithm with fast convergence. Extensive experiments ondifferent applications demonstrate that our algorithm outperforms otherstate-of-the-art feature selection algorithms.
arxiv-8700-164 | Online Handwritten Devanagari Stroke Recognition Using Extended Directional Features | http://arxiv.org/pdf/1501.02887v1.pdf | author:Lajish VL, Sunil Kumar Kopparapu category:cs.CV published:2015-01-11 summary:This paper describes a new feature set, called the extended directionalfeatures (EDF) for use in the recognition of online handwritten strokes. We useEDF specifically to recognize strokes that form a basis for producingDevanagari script, which is the most widely used Indian language script. Itshould be noted that stroke recognition in handwritten script is equivalent tophoneme recognition in speech signals and is generally very poor and of theorder of 20% for singing voice. Experiments are conducted for the automaticrecognition of isolated handwritten strokes. Initially we describe the proposedfeature set, namely EDF and then show how this feature can be effectivelyutilized for writer independent script recognition through stroke recognition.Experimental results show that the extended directional feature set performswell with about 65+% stroke level recognition accuracy for writer independentdata set.
arxiv-8700-165 | Entropic one-class classifiers | http://arxiv.org/pdf/1407.7556v3.pdf | author:Lorenzo Livi, Alireza Sadeghian, Witold Pedrycz category:cs.CV cs.LG stat.ML I.2.6; K.2.3 published:2014-07-28 summary:The one-class classification problem is a well-known research endeavor inpattern recognition. The problem is also known under different names, such asoutlier and novelty/anomaly detection. The core of the problem consists inmodeling and recognizing patterns belonging only to a so-called target class.All other patterns are termed non-target, and therefore they should berecognized as such. In this paper, we propose a novel one-class classificationsystem that is based on an interplay of different techniques. Primarily, wefollow a dissimilarity representation based approach; we embed the input datainto the dissimilarity space by means of an appropriate parametricdissimilarity measure. This step allows us to process virtually any type ofdata. The dissimilarity vectors are then represented through a weightedEuclidean graphs, which we use to (i) determine the entropy of the datadistribution in the dissimilarity space, and at the same time (ii) deriveeffective decision regions that are modeled as clusters of vertices. Since thedissimilarity measure for the input data is parametric, we optimize itsparameters by means of a global optimization scheme, which considers bothmesoscopic and structural characteristics of the data represented through thegraphs. The proposed one-class classifier is designed to provide both hard(Boolean) and soft decisions about the recognition of test patterns, allowingan accurate description of the classification process. We evaluate theperformance of the system on different benchmarking datasets, containing eitherfeature-based or structured patterns. Experimental results demonstrate theeffectiveness of the proposed technique.
arxiv-8700-166 | Fast and optimal nonparametric sequential design for astronomical observations | http://arxiv.org/pdf/1501.02467v1.pdf | author:Justin J. Yang, Xufei Wang, Pavlos Protopapas, Luke Bornn category:stat.ME stat.AP stat.ML published:2015-01-11 summary:The spectral energy distribution (SED) is a relatively easy way forastronomers to distinguish between different astronomical objects such asgalaxies, black holes, and stellar objects. By comparing the observations froma source at different frequencies with template models, astronomers are able toinfer the type of this observed object. In this paper, we take a Bayesian modelaveraging perspective to learn astronomical objects, employing a Bayesiannonparametric approach to accommodate the deviation from convex combinations ofknown log-SEDs. To effectively use telescope time for observations, we thenstudy Bayesian nonparametric sequential experimental design without conjugacy,in which we use sequential Monte Carlo as an efficient tool to maximize thevolume of information stored in the posterior distribution of the parameters ofinterest. A new technique for performing inferences in log-Gaussian Coxprocesses called the Poisson log-normal approximation is also proposed.Simulations show the speed, accuracy, and usefulness of our method. While thestrategy we propose in this paper is brand new in the astronomy literature, theinferential techniques developed apply to more general nonparametric sequentialexperimental design problems.
arxiv-8700-167 | Learning a Fuzzy Hyperplane Fat Margin Classifier with Minimum VC dimension | http://arxiv.org/pdf/1501.02432v1.pdf | author:Jayadeva, Sanjit Singh Batra, Siddarth Sabharwal category:cs.LG I.5.1; I.5.2 published:2015-01-11 summary:The Vapnik-Chervonenkis (VC) dimension measures the complexity of a learningmachine, and a low VC dimension leads to good generalization. The recentlyproposed Minimal Complexity Machine (MCM) learns a hyperplane classifier byminimizing an exact bound on the VC dimension. This paper extends the MCMclassifier to the fuzzy domain. The use of a fuzzy membership is known toreduce the effect of outliers, and to reduce the effect of noise on learning.Experimental results show, that on a number of benchmark datasets, the thefuzzy MCM classifier outperforms SVMs and the conventional MCM in terms ofgeneralization, and that the fuzzy MCM uses fewer support vectors. On severalbenchmark datasets, the fuzzy MCM classifier yields excellent test setaccuracies while using one-tenth the number of support vectors used by SVMs.
arxiv-8700-168 | A Gaussian Particle Filter Approach for Sensors to Track Multiple Moving Targets | http://arxiv.org/pdf/1501.02411v1.pdf | author:Haojun Li category:cs.LG published:2015-01-11 summary:In a variety of problems, the number and state of multiple moving targets areunknown and are subject to be inferred from their measurements obtained by asensor with limited sensing ability. This type of problems is raised in avariety of applications, including monitoring of endangered species, cleaning,and surveillance. Particle filters are widely used to estimate target statefrom its prior information and its measurements that recently become available,especially for the cases when the measurement model and the prior distributionof state of interest are non-Gaussian. However, the problem of estimatingnumber of total targets and their state becomes intractable when the number oftotal targets and the measurement-target association are unknown. This paperpresents a novel Gaussian particle filter technique that combines Kalman filterand particle filter for estimating the number and state of total targets basedon the measurement obtained online. The estimation is represented by a set ofweighted particles, different from classical particle filter, where eachparticle is a Gaussian distribution instead of a point mass.
arxiv-8700-169 | Techniques for clustering interaction data as a collection of graphs | http://arxiv.org/pdf/1406.6319v3.pdf | author:Nam H. Lee, Carey Priebe, Youngser Park, I-Jeng Wang, Michael Rosen category:stat.ML published:2014-06-24 summary:A natural approach to analyze interaction data of form"what-connects-to-what-when" is to create a time-series (or rather a sequence)of graphs through temporal discretization (bandwidth selection) and spatialdiscretization (vertex contraction). Such discretization together withnon-negative factorization techniques can be useful for obtaining clustering ofgraphs. Motivating application of performing clustering of graphs (as opposedto vertex clustering) can be found in neuroscience and in social networkanalysis, and it can also be used to enhance community detection (i.e., vertexclustering) by way of conditioning on the cluster labels. In this paper, weformulate a problem of clustering of graphs as a model selection problem. Ourapproach involves information criteria, non-negative matrix factorization andsingular value thresholding, and we illustrate our techniques using real andsimulated data.
arxiv-8700-170 | Riemannian Metric Learning for Symmetric Positive Definite Matrices | http://arxiv.org/pdf/1501.02393v1.pdf | author:Raviteja Vemulapalli, David W. Jacobs category:cs.CV cs.LG published:2015-01-10 summary:Over the past few years, symmetric positive definite (SPD) matrices have beenreceiving considerable attention from computer vision community. Though variousdistance measures have been proposed in the past for comparing SPD matrices,the two most widely-used measures are affine-invariant distance andlog-Euclidean distance. This is because these two measures are true geodesicdistances induced by Riemannian geometry. In this work, we focus on thelog-Euclidean Riemannian geometry and propose a data-driven approach forlearning Riemannian metrics/geodesic distances for SPD matrices. We show thatthe geodesic distance learned using the proposed approach performs better thanvarious existing distance measures when evaluated on face matching andclustering tasks.
arxiv-8700-171 | Autonomous Farm Vehicles: Prototype of Power Reaper | http://arxiv.org/pdf/1501.02379v1.pdf | author:Abdul Qadeer Khan, Ayyaz Akhtar, Muhammad Zubair Ahmad category:cs.RO cs.CV cs.CY published:2015-01-10 summary:Chapter 2 will begin with introduction of Agricultural Robotics. There willbe a literature review of the mechanical structure, vision and controlalgorithms. In chapter 3 we will discuss the methodology in detail using blockdiagrams and flowcharts. The results of the tested and the proposed algorithmswill also be displayed. In chapter 4 we will discuss the results in detail andhow they are of significance in our work. In chapter 5 we will conclude ourwork and discuss some future perspectives. In appendices we will provide somebackground information necessary regarding this project.
arxiv-8700-172 | Low Cost Semi-Autonomous Agricultural Robots In Pakistan-Vision Based Navigation Scalable methodology for wheat harvesting | http://arxiv.org/pdf/1501.02378v1.pdf | author:Muhammad Zubair Ahmad, Ayyaz Akhtar, Abdul Qadeer Khan, Amir Ali Khan, Muhammad Murtaza Khan category:cs.RO cs.CV cs.CY published:2015-01-10 summary:Robots have revolutionized our way of life in recent years.One of the domainsthat has not yet completely benefited from the robotic automation is theagricultural sector. Agricultural Robotics should complement humans in thearduous tasks during different sub-domains of this sector. Extensive researchin Agricultural Robotics has been carried out in Japan, USA, Australia andGermany focusing mainly on the heavy agricultural machinery. Pakistan is anagricultural rich country and its economy and food security are closely tiedwith agriculture in general and wheat in particular. However, agriculturalresearch in Pakistan is still carried out using the conventional methodologies.This paper is an attempt to trigger the research in this modern domain so thatwe can benefit from cost effective and resource efficient autonomousagricultural methodologies. This paper focuses on a scalable low costsemi-autonomous technique for wheat harvest which primarily focuses on thefarmers with small land holdings. The main focus will be on the vision part ofthe navigation system deployed by the proposed robot.
arxiv-8700-173 | Simplified vision based automatic navigation for wheat harvesting in low income economies | http://arxiv.org/pdf/1501.02376v1.pdf | author:Muhammad Zubair Ahmad, Ayyaz Akhtar, Abdul Qadeer Khan, Amir A. Khan category:cs.RO cs.CV cs.CY published:2015-01-10 summary:Recent developments in the domain of agricultural robotics have resulted indevelopment of complex and efficient systems. Most of the land owners in theSouth Asian region are low income farmers. The agricultural experience for themis still a completely manual process. However, the extreme weather conditions,heat and flooding, often combine to put a lot of stress on these small landowners and the associated labor. In this paper, we propose a prototype for anautomated power reaper for the wheat crop. This automated vehicle is navigatedusing a simple vision based approach employing the low-cost camera and assistedGPS. The mechanical platform is driven by three motors controlled through aninterface between the proposed vision algorithm and the electrical drive. Theproposed methodology is applied on some real field scenarios to demonstrate theefficiency of the vision based algorithm.
arxiv-8700-174 | On the Distribution of Salient Objects in Web Images and its Influence on Salient Object Detection | http://arxiv.org/pdf/1501.03383v1.pdf | author:Boris Schauerte, Rainer Stiefelhagen category:cs.CV published:2015-01-10 summary:It has become apparent that a Gaussian center bias can serve as an importantprior for visual saliency detection, which has been demonstrated for predictinghuman eye fixations and salient object detection. Tseng et al. have shown thatthe photographer's tendency to place interesting objects in the center is alikely cause for the center bias of eye fixations. We investigate the influenceof the photographer's center bias on salient object detection, extending ourprevious work. We show that the centroid locations of salient objects inphotographs of Achanta and Liu's data set in fact correlate strongly with aGaussian model. This is an important insight, because it provides an empiricalmotivation and justification for the integration of such a center bias insalient object detection algorithms and helps to understand why Gaussian modelsare so effective. To assess the influence of the center bias on salient objectdetection, we integrate an explicit Gaussian center bias model into twostate-of-the-art salient object detection algorithms. This way, first, wequantify the influence of the Gaussian center bias on pixel- and segment-basedsalient object detection. Second, we improve the performance in terms of F1score, Fb score, area under the recall-precision curve, area under the receiveroperating characteristic curve, and hit-rate on the well-known data set byAchanta and Liu. Third, by debiasing Cheng et al.'s region contrast model, weexemplarily demonstrate that implicit center biases are partially responsiblefor the outstanding performance of state-of-the-art algorithms. Last but notleast, as a result of debiasing Cheng et al.'s algorithm, we introduce anon-biased salient object detection method, which is of interest forapplications in which the image data is not likely to have a photographer'scenter bias (e.g., image data of surveillance cameras or autonomous robots).
arxiv-8700-175 | Graph Matching: Relax at Your Own Risk | http://arxiv.org/pdf/1405.3133v3.pdf | author:Vince Lyzinski, Donniell Fishkind, Marcelo Fiori, Joshua T. Vogelstein, Carey E. Priebe, Guillermo Sapiro category:stat.ML math.OC published:2014-05-13 summary:Graph matching---aligning a pair of graphs to minimize their edgedisagreements---has received wide-spread attention from both theoretical andapplied communities over the past several decades, including combinatorics,computer vision, and connectomics. Its attention can be partially attributed toits computational difficulty. Although many heuristics have previously beenproposed in the literature to approximately solve graph matching, very few haveany theoretical support for their performance. A common technique is to relaxthe discrete problem to a continuous problem, therefore enabling practitionersto bring gradient-descent-type algorithms to bear. We prove that an indefiniterelaxation (when solved exactly) almost always discovers the optimalpermutation, while a common convex relaxation almost always fails to discoverthe optimal permutation. These theoretical results suggest that initializingthe indefinite algorithm with the convex optimum might yield improved practicalperformance. Indeed, experimental results illuminate and corroborate thesetheoretical findings, demonstrating that excellent results are achieved in bothbenchmark and real data problems by amalgamating the two approaches.
arxiv-8700-176 | Exploring Sparsity in Multi-class Linear Discriminant Analysis | http://arxiv.org/pdf/1412.7983v2.pdf | author:Dong Xia category:stat.ML I.5.2 published:2014-12-26 summary:Recent studies in the literature have paid much attention to the sparsity inlinear classification tasks. One motivation of imposing sparsity assumption onthe linear discriminant direction is to rule out the noninformative features,making hardly contribution to the classification problem. Most of those workwere focused on the scenarios of binary classification. In the presence ofmulti-class data, preceding researches recommended individually pairwise sparselinear discriminant analysis(LDA). However, further sparsity should beexplored. In this paper, an estimator of grouped LASSO type is proposed to takeadvantage of sparsity for multi-class data. It enjoys appealing non-asymptoticproperties which allows insignificant correlations among features. Thisestimator exhibits superior capability on both simulated and real data.
arxiv-8700-177 | Survey schemes for stochastic gradient descent with applications to M-estimation | http://arxiv.org/pdf/1501.02218v1.pdf | author:Stéphan Clémençon, Patrice Bertail, Emilie Chautru, Guillaume Papa category:stat.ML published:2015-01-09 summary:In certain situations that shall be undoubtedly more and more common in theBig Data era, the datasets available are so massive that computing statisticsover the full sample is hardly feasible, if not unfeasible. A natural approachin this context consists in using survey schemes and substituting the "fulldata" statistics with their counterparts based on the resulting random samples,of manageable size. It is the main purpose of this paper to investigate theimpact of survey sampling with unequal inclusion probabilities on stochasticgradient descent-based M-estimation methods in large-scale statistical andmachine-learning problems. Precisely, we prove that, in presence of some apriori information, one may significantly increase asymptotic accuracy whenchoosing appropriate first order inclusion probabilities, without affectingcomplexity. These striking results are described here by limit theorems and arealso illustrated by numerical experiments.
arxiv-8700-178 | Investigation of a chaotic spiking neuron model | http://arxiv.org/pdf/1501.02192v1.pdf | author:M. Alhawarat, T. Olde Scheper, N. T. Crook category:cs.NE cs.AI published:2015-01-09 summary:Chaos provides many interesting properties that can be used to achievecomputational tasks. Such properties are sensitivity to initial conditions,space filling, control and synchronization. Chaotic neural models have beendevised to exploit such properties. In this paper, a chaotic spiking neuronmodel is investigated experimentally. This investigation is performed tounderstand the dynamic behaviours of the model. The aim of this research is to investigate the dynamics of the nonlineardynamic state neuron (NDS) experimentally. The experimental approach hasrevealed some quantitative and qualitative properties of the NDS model such asthe control mechanism, the reset mechanism, and the way the model may exhibitdynamic behaviours in phase space. It is shown experimentally in this paperthat both the reset mechanism and the self-feed back control mechanism areimportant for the NDS model to work and to stabilise to one of the large numberof available unstable periodic orbits (UPOs) that are embedded in itsattractor. The experimental investigation suggests that the internal dynamicsof the NDS neuron provide a rich set of dynamic behaviours that can becontrolled and stabilised. These wide range of dynamic behaviours may beexploited to carry out information processing tasks.
arxiv-8700-179 | Introduction and Ranking Results of the ICSI 2014 Competition on Single Objective Optimization | http://arxiv.org/pdf/1501.02128v1.pdf | author:Ying Tan, Junzhi Li, Zhongyang Zheng category:cs.NE published:2015-01-09 summary:This technical report includes the introduction and ranking results of theICSI 2014 Competition on Single Objective Optimization.
arxiv-8700-180 | Filter Design and Performance Evaluation for Fingerprint Image Segmentation | http://arxiv.org/pdf/1501.02113v1.pdf | author:Duy Hoang Thai, Stephan Huckemann, Carsten Gottschlich category:cs.CV published:2015-01-09 summary:Fingerprint recognition plays an important role in many commercialapplications and is used by millions of people every day, e.g. for unlockingmobile phones. Fingerprint image segmentation is typically the first processingstep of most fingerprint algorithms and it divides an image into foreground,the region of interest, and background. Two types of error can occur duringthis step which both have a negative impact on the recognition performance:'true' foreground can be labeled as background and features like minutiae canbe lost, or conversely 'true' background can be misclassified as foreground andspurious features can be introduced. The contribution of this paper isthreefold: firstly, we propose a novel factorized directional bandpass (FDB)segmentation method for texture extraction based on the directional Hilberttransform of a Butterworth bandpass (DHBB) filter interwoven withsoft-thresholding. Secondly, we provide a manually marked ground truthsegmentation for 10560 images as an evaluation benchmark. Thirdly, we conduct asystematic performance comparison between the FDB method and four of the mostoften cited fingerprint segmentation algorithms showing that the FDBsegmentation method clearly outperforms these four widely used methods. Thebenchmark and the implementation of the FDB method are made publicly available.
arxiv-8700-181 | Margins of discrete Bayesian networks | http://arxiv.org/pdf/1501.02103v1.pdf | author:Robin J. Evans category:math.ST stat.ML stat.TH published:2015-01-09 summary:In this paper we provide a complete algebraic characterization of the modelimplied by a Bayesian network with latent variables when the observed variablesare discrete. We show that it is algebraically equivalent to the so-callednested Markov model, meaning that the two are the same up to inequalityconstraints on the joint probabilities. The nested Markov model is thereforethe best possible approximation to the latent variable model whilst avoidinginequalities, which are extremely complicated in general. Latent variablemodels also suffer from difficulties of unidentifiable parameters andnon-regular asymptotics; in contrast the nested Markov model is fullyidentifiable, represents a curved exponential family of known dimension, andcan easily be fitted using an explicit parameterization.
arxiv-8700-182 | HOG based Fast Human Detection | http://arxiv.org/pdf/1501.02058v1.pdf | author:M. Kachouane, S. Sahki, M. Lakrouf, N. Ouadah category:cs.RO cs.CV cs.LG published:2015-01-09 summary:Objects recognition in image is one of the most difficult problems incomputer vision. It is also an important step for the implementation of severalexisting applications that require high-level image interpretation. Therefore,there is a growing interest in this research area during the last years. Inthis paper, we present an algorithm for human detection and recognition inreal-time, from images taken by a CCD camera mounted on a car-like mobilerobot. The proposed technique is based on Histograms of Oriented Gradient (HOG)and SVM classifier. The implementation of our detector has provided goodresults, and can be used in robotics tasks.
arxiv-8700-183 | The Vapnik-Chervonenkis Dimension of Norms on $\mathbb{R}^d$ | http://arxiv.org/pdf/1412.6612v2.pdf | author:Christian J. J. Despres category:math.CO math.MG stat.ML published:2014-12-20 summary:The Vapnik-Chervonenkis dimension of a collection of subsets of a set is animportant combinatorial parameter in machine learning. In this paper we showthat the VC dimension of the family of d-dimensional cubes in $\mathbb{R}^d$(that is, the closed balls according to the $\ell^\infty$ norm) is $\lfloor(3d+1)/2 \rfloor$. We also prove that the VC dimension of certain families ofconvex sets in $\mathbb{R}^2$ (including the balls of all norms) is at most 3,and that there is a norm in $\mathbb{R}^3$ the collection of whose balls hasinfinite VC dimension.
arxiv-8700-184 | Co-clustering for directed graphs: the Stochastic co-Blockmodel and spectral algorithm Di-Sim | http://arxiv.org/pdf/1204.2296v2.pdf | author:Karl Rohe, Tai Qin, Bin Yu category:stat.ML math.ST stat.TH published:2012-04-10 summary:Directed graphs have asymmetric connections, yet the current graph clusteringmethodologies cannot identify the potentially global structure of theseasymmetries. We give a spectral algorithm called di-sim that builds on a dualmeasure of similarity that correspond to how a node (i) sends and (ii) receivesedges. Using di-sim, we analyze the global asymmetries in the networks of Enronemails, political blogs, and the c elegans neural connectome. In each example,a small subset of nodes have persistent asymmetries; these nodes send edgeswith one cluster, but receive edges with another cluster. Previous approacheswould have assigned these asymmetric nodes to only one cluster, failing toidentify their sending/receiving asymmetries. Regularization and "projection"are two steps of di-sim that are essential for spectral clustering algorithmsto work in practice. The theoretical results show that these steps make thealgorithm weakly consistent under the degree corrected Stochasticco-Blockmodel, a model that generalizes the Stochastic Blockmodel to allow forboth (i) degree heterogeneity and (ii) the global asymmetries that we intend todetect. The theoretical results make no assumptions on the smallest degreenodes. Instead, the theorem requires that the average degree grows sufficientlyfast and that the weak consistency only applies to the subset of the nodes withsufficiently large leverage scores. The results results also apply to bipartitegraphs.
arxiv-8700-185 | A Conditional Dependence Measure with Applications to Undirected Graphical Models | http://arxiv.org/pdf/1501.01617v2.pdf | author:Jianqing Fan, Yang Feng, Lucy Xia category:stat.ME math.ST stat.AP stat.ML stat.TH published:2015-01-07 summary:Measuring conditional dependence is an important topic in statistics withbroad applications including graphical models. Under a factor model setting, anew conditional dependence measure is proposed. The measure is derived by usingdistance covariance after adjusting the common observable factors orcovariates. The corresponding conditional independence test is given with theasymptotic null distribution unveiled. The latter gives a somewhat surprisingresult: the estimating errors in factor loading matrices, while of root$-n$order, do not have material impact on the asymptotic null distribution of thetest statistic, which is also in the root$-n$ domain. It is also shown that thenew test has strict control over the asymptotic significance level and can becalculated efficiently. A generic method for building dependency graphs usingthe new test is elaborated. Numerical results and real data analysis show thesuperiority of the new method.
arxiv-8700-186 | Less is More: Building Selective Anomaly Ensembles | http://arxiv.org/pdf/1501.01924v1.pdf | author:Shebuti Rayana, Leman Akoglu category:cs.DB cs.LG published:2015-01-08 summary:Ensemble techniques for classification and clustering have long proveneffective, yet anomaly ensembles have been barely studied. In this work, we tapinto this gap and propose a new ensemble approach for anomaly mining, withapplication to event detection in temporal graphs. Our method aims to combineresults from heterogeneous detectors with varying outputs, and leverage theevidence from multiple sources to yield better performance. However, trustingall the results may deteriorate the overall ensemble accuracy, as somedetectors may fall short and provide inaccurate results depending on the natureof the data in hand. This suggests that being selective in which results tocombine is vital in building effective ensembles---hence "less is more". In this paper we propose SELECT; an ensemble approach for anomaly mining thatemploys novel techniques to automatically and systematically select the resultsto assemble in a fully unsupervised fashion. We apply our method to eventdetection in temporal graphs, where SELECT successfully utilizes five basedetectors and seven consensus methods under a unified ensemble framework. Weprovide extensive quantitative evaluation of our approach on five real-worlddatasets (four with ground truth), including Enron email communications, NewYork Times news corpus, and World Cup 2014 Twitter news feed. Thanks to itsselection mechanism, SELECT yields superior performance compared to individualdetectors alone, the full ensemble (naively combining all results), and anexisting diversity-based ensemble.
arxiv-8700-187 | Quantifying Scripts: Defining metrics of characters for quantitative and descriptive analysis | http://arxiv.org/pdf/1501.01894v1.pdf | author:Vinodh Rajan category:cs.CL published:2015-01-08 summary:Analysis of scripts plays an important role in paleography and inquantitative linguistics. Especially in the field of digital paleographyquantitative features are much needed to differentiate glyphs. We describe anelaborate set of metrics that quantify qualitative information contained incharacters and hence indirectly also quantify the scribal features. We broadlydivide the metrics into several categories and describe each individual metricwith its underlying qualitative significance. The metrics are largely derivedfrom the related area of gesture design and recognition. We also proposeseveral novel metrics. The proposed metrics are soundly grounded on theprinciples of handwriting production and handwriting analysis. These computedmetrics could serve as descriptors for scripts and also be used for comparingand analyzing scripts. We illustrate some quantitative analysis based on theproposed metrics by applying it to the paleographic evolution of the medievalTamil script from Brahmi. We also outline future work.
arxiv-8700-188 | The Hebrew Bible as Data: Laboratory - Sharing - Experiences | http://arxiv.org/pdf/1501.01866v1.pdf | author:Dirk Roorda category:cs.CL cs.DL published:2015-01-08 summary:The systematic study of ancient texts including their production,transmission and interpretation is greatly aided by the digital methods thatstarted taking off in the 1970s. But how is that research in turn transmittedto new generations of researchers? We tell a story of Bible and computer acrossthe decades and then point out the current challenges: (1) finding a stabledata representation for changing methods of computation; (2) sharing results ininter- and intra-disciplinary ways, for reproducibility andcross-fertilization. We report recent developments in meeting these challenges.The scene is the text database of the Hebrew Bible, constructed by the EepTalstra Centre for Bible and Computer (ETCBC), which is still growing in detailand sophistication. We show how a subtle mix of computational ingredientsenable scholars to research the transmission and interpretation of the HebrewBible in new ways: (1) a standard data format, Linguistic Annotation Framework(LAF); (2) the methods of scientific computing, made accessible by(interactive) Python and its associated ecosystem. Additionally, we show howthese efforts have culminated in the construction of a new, publicly accessiblesearch engine SHEBANQ, where the text of the Hebrew Bible and its underlyingdata can be queried in a simple, yet powerful query language MQL, and wherethose queries can be saved and shared.
arxiv-8700-189 | No-Regret Learnability for Piecewise Linear Losses | http://arxiv.org/pdf/1411.5649v3.pdf | author:Arthur Flajolet, Patrick Jaillet category:cs.LG published:2014-11-20 summary:In the convex optimization approach to online regret minimization, manymethods have been developed to guarantee a $O(\sqrt{T})$ regret bound forsubdifferentiable convex loss functions with bounded subgradients by means of areduction to bounded linear loss functions. This suggests that the latter tendto be the hardest loss functions to learn against. We investigate this questionin a systematic fashion as a function of the decision set and the environment'sset of moves. On the one hand, we exhibit a localization property for linearlosses leading to $o(\sqrt{T})$ learning rates and provide examples where thisproperty holds. On the other hand, we establish $\Omega(\sqrt{T})$ lower boundson the minimum achievable regret for a class of piecewise linear loss functionsthat subsumes the class of bounded linear loss functions and for polyhedraldecision sets. These results hold in a completely adversarial setting. Incontrast, we show that the minimum achievable regret can be significantlysmaller when the opponent is greedy.
arxiv-8700-190 | Optimal Radiometric Calibration for Camera-Display Communication | http://arxiv.org/pdf/1501.01744v1.pdf | author:Wenjia Yuan, Eric Wengrowski, Kristin J. Dana, Ashwin Ashok, Marco Gruteser, Narayan Mandayam category:cs.CV published:2015-01-08 summary:We present a novel method for communicating between a camera and display byembedding and recovering hidden and dynamic information within a displayedimage. A handheld camera pointed at the display can receive not only thedisplay image, but also the underlying message. These active scenes arefundamentally different from traditional passive scenes like QR codes becauseimage formation is based on display emittance, not surface reflectance.Detecting and decoding the message requires careful photometric modeling forcomputational message recovery. Unlike standard watermarking and steganographymethods that lie outside the domain of computer vision, our message recoveryalgorithm uses illumination to optically communicate hidden messages in realworld scenes. The key innovation of our approach is an algorithm that performssimultaneous radiometric calibration and message recovery in one convexoptimization problem. By modeling the photometry of the system using acamera-display transfer function (CDTF), we derive a physics-based kernelfunction for support vector machine classification. We demonstrate that ourmethod of optimal online radiometric calibration (OORC) leads to an efficientand robust algorithm for computational messaging between nine commercialcameras and displays.
arxiv-8700-191 | An Effective Image Feature Classiffication using an improved SOM | http://arxiv.org/pdf/1501.01723v1.pdf | author:M. Abdelsamea, Marghny H. Mohamed, Mohamed Bamatraf category:cs.CV published:2015-01-08 summary:Image feature classification is a challenging problem in many computer visionapplications, specifically, in the fields of remote sensing, image analysis andpattern recognition. In this paper, a novel Self Organizing Map, termedimproved SOM (iSOM), is proposed with the aim of effectively classifyingMammographic images based on their texture feature representation. The maincontribution of the iSOM is to introduce a new node structure for the maprepresentation and adopting a learning technique based on Kohonen SOMaccordingly. The main idea is to control, in an unsupervised fashion, theweight updating procedure depending on the class reliability of the node,during the weight update time. Experiments held on a real Mammographic images.Results showed high accuracy compared to classical SOM and other state-of-artclassifiers.
arxiv-8700-192 | Sparse Solutions to Nonnegative Linear Systems and Applications | http://arxiv.org/pdf/1501.01689v1.pdf | author:Aditya Bhaskara, Ananda Theertha Suresh, Morteza Zadimoghaddam category:cs.DS cs.IT cs.LG math.IT published:2015-01-07 summary:We give an efficient algorithm for finding sparse approximate solutions tolinear systems of equations with nonnegative coefficients. Unlike most knownresults for sparse recovery, we do not require {\em any} assumption on thematrix other than non-negativity. Our algorithm is combinatorial in nature,inspired by techniques for the set cover problem, as well as the multiplicativeweight update method. We then present a natural application to learning mixture models in the PACframework. For learning a mixture of $k$ axis-aligned Gaussians in $d$dimensions, we give an algorithm that outputs a mixture of $O(k/\epsilon^3)$Gaussians that is $\epsilon$-close in statistical distance to the truedistribution, without any separation assumptions. The time and samplecomplexity is roughly $O(kd/\epsilon^3)^{d}$. This is polynomial when $d$ isconstant -- precisely the regime in which known methods fail to identify thecomponents efficiently. Given that non-negativity is a natural assumption, we believe that our resultmay find use in other settings in which we wish to approximately explain datausing a small number of a (large) candidate set of components.
arxiv-8700-193 | An Introduction to Matrix Concentration Inequalities | http://arxiv.org/pdf/1501.01571v1.pdf | author:Joel A. Tropp category:math.PR cs.DS cs.IT cs.NA math.IT stat.ML published:2015-01-07 summary:In recent years, random matrices have come to play a major role incomputational mathematics, but most of the classical areas of random matrixtheory remain the province of experts. Over the last decade, with the advent ofmatrix concentration inequalities, research has advanced to the point where wecan conquer many (formerly) challenging problems with a page or two ofarithmetic. The aim of this monograph is to describe the most successfulmethods from this area along with some interesting examples that thesetechniques can illuminate.
arxiv-8700-194 | Scaling laws in human speech, decreasing emergence of new words and a generalized model | http://arxiv.org/pdf/1412.4846v2.pdf | author:Ruokuang Lin, Qianli D. Y. Ma, Chunhua Bian category:cs.CL published:2014-12-16 summary:Human language, as a typical complex system, its organization and evolutionis an attractive topic for both physical and cultural researchers. In thispaper, we present the first exhaustive analysis of the text organization ofhuman speech. Two important results are that: (i) the construction andorganization of spoken language can be characterized as Zipf's law and Heaps'law, as observed in written texts; (ii) word frequency vs. rank distributionand the growth of distinct words with the increase of text length showssignificant differences between book and speech. In speech word frequencydistribution are more concentrated on higher frequency words, and the emergenceof new words decreases much rapidly when the content length grows. Based onthese observations, a new generalized model is proposed to explain thesecomplex dynamical behaviors and the differences between speech and book.
arxiv-8700-195 | Comparison of Selection Methods in On-line Distributed Evolutionary Robotics | http://arxiv.org/pdf/1501.01457v1.pdf | author:Iñaki Fernández Pérez, Amine Boumaza, François Charpillet category:cs.AI cs.MA cs.NE cs.RO published:2015-01-07 summary:In this paper, we study the impact of selection methods in the context ofon-line on-board distributed evolutionary algorithms. We propose a variant ofthe mEDEA algorithm in which we add a selection operator, and we apply it in ataskdriven scenario. We evaluate four selection methods that induce differentintensity of selection pressure in a multi-robot navigation with obstacleavoidance task and a collective foraging task. Experiments show that a smallintensity of selection pressure is sufficient to rapidly obtain goodperformances on the tasks at hand. We introduce different measures to comparethe selection methods, and show that the higher the selection pressure, thebetter the performances obtained, especially for the more challenging foodforaging task.
arxiv-8700-196 | Roman Urdu Opinion Mining System (RUOMiS) | http://arxiv.org/pdf/1501.01386v1.pdf | author:Misbah Daud, Rafiullah Khan, Mohibullah, Aitazaz Daud category:cs.CL cs.IR published:2015-01-07 summary:Convincing a customer is always considered as a challenging task in everybusiness. But when it comes to online business, this task becomes even moredifficult. Online retailers try everything possible to gain the trust of thecustomer. One of the solutions is to provide an area for existing users toleave their comments. This service can effectively develop the trust of thecustomer however normally the customer comments about the product in theirnative language using Roman script. If there are hundreds of comments thismakes difficulty even for the native customers to make a buying decision. Thisresearch proposes a system which extracts the comments posted in Roman Urdu,translate them, find their polarity and then gives us the rating of theproduct. This rating will help the native and non-native customers to makebuying decision efficiently from the comments posted in Roman Urdu.
arxiv-8700-197 | Leader Follower Formation Control of Ground Vehicles Using Camshift Based Guidance | http://arxiv.org/pdf/1501.01364v1.pdf | author:S. M. Vaitheeswaran, Bharath M. K., Gokul M category:cs.CV published:2015-01-07 summary:Autonomous ground vehicles have been designed for the purpose of that relieson ranging and bearing information received from forward looking camera on theFormation control . A visual guidance control algorithm is designed where realtime image processing is used to provide feedback signals. The vision subsystemand control subsystem work in parallel to accomplish formation control. Aproportional navigation and line of sight guidance laws are used to estimatethe range and bearing information from the leader vehicle using the visionsubsystem. The algorithms for vision detection and localization used here aresimilar to approaches for many computer vision tasks such as face tracking anddetection that are based color-and texture based features, and non-parametricContinuously Adaptive Mean-shift algorithms to keep track of the leader. Thisis being proposed for the first time in the leader follower framework. Thealgorithms are simple but effective for real time and provide an alternateapproach to traditional based approaches like the Viola Jones algorithm.Further to stabilize the follower to the leader trajectory, the sliding modecontroller is used to dynamically track the leader. The performance of theresults is demonstrated in simulation and in practical experiments.
arxiv-8700-198 | Deep Autoencoders for Dimensionality Reduction of High-Content Screening Data | http://arxiv.org/pdf/1501.01348v1.pdf | author:Lee Zamparo, Zhaolei Zhang category:cs.LG published:2015-01-07 summary:High-content screening uses large collections of unlabeled cell image data toreason about genetics or cell biology. Two important tasks are to identifythose cells which bear interesting phenotypes, and to identify sub-populationsenriched for these phenotypes. This exploratory data analysis usually involvesdimensionality reduction followed by clustering, in the hope that clustersrepresent a phenotype. We propose the use of stacked de-noising auto-encodersto perform dimensionality reduction for high-content screening. We demonstratethe superior performance of our approach over PCA, Local Linear Embedding,Kernel PCA and Isomap.
arxiv-8700-199 | Reconstructing subclonal composition and evolution from whole genome sequencing of tumors | http://arxiv.org/pdf/1406.7250v3.pdf | author:Amit G. Deshwar, Shankar Vembu, Christina K. Yung, Gun Ho Jang, Lincoln Stein, Quaid Morris category:q-bio.PE cs.LG stat.ML published:2014-06-27 summary:Tumors often contain multiple subpopulations of cancerous cells defined bydistinct somatic mutations. We describe a new method, PhyloWGS, that can beapplied to WGS data from one or more tumor samples to reconstruct completegenotypes of these subpopulations based on variant allele frequencies (VAFs) ofpoint mutations and population frequencies of structural variations. Weintroduce a principled phylogenic correction for VAFs in loci affected by copynumber alterations and we show that this correction greatly improves subclonalreconstruction compared to existing methods.
arxiv-8700-200 | ITCM: A Real Time Internet Traffic Classifier Monitor | http://arxiv.org/pdf/1501.01321v1.pdf | author:Silas Santiago Lopes Pereira, José Everardo Bessa Maia, Jorge Luiz de Castro e Silva category:cs.NI cs.LG published:2015-01-06 summary:The continual growth of high speed networks is a challenge for real-timenetwork analysis systems. The real time traffic classification is an issue forcorporations and ISPs (Internet Service Providers). This work presents thedesign and implementation of a real time flow-based network trafficclassification system. The classifier monitor acts as a pipeline consisting ofthree modules: packet capture and pre-processing, flow reassembly, andclassification with Machine Learning (ML). The modules are built as concurrentprocesses with well defined data interfaces between them so that any module canbe improved and updated independently. In this pipeline, the flow reassemblyfunction becomes the bottleneck of the performance. In this implementation, wasused a efficient method of reassembly which results in a average delivery delayof 0.49 seconds, approximately. For the classification module, the performancesof the K-Nearest Neighbor (KNN), C4.5 Decision Tree, Naive Bayes (NB), FlexibleNaive Bayes (FNB) and AdaBoost Ensemble Learning Algorithm are compared inorder to validate our approach.
arxiv-8700-201 | Arabic Text Categorization Algorithm using Vector Evaluation Method | http://arxiv.org/pdf/1501.01318v1.pdf | author:Ashraf Odeh, Aymen Abu-Errub, Qusai Shambour, Nidal Turab category:cs.IR cs.CL published:2015-01-06 summary:Text categorization is the process of grouping documents into categoriesbased on their contents. This process is important to make informationretrieval easier, and it became more important due to the huge textualinformation available online. The main problem in text categorization is how toimprove the classification accuracy. Although Arabic text categorization is anew promising field, there are a few researches in this field. This paperproposes a new method for Arabic text categorization using vector evaluation.The proposed method uses a categorized Arabic documents corpus, and then theweights of the tested document's words are calculated to determine the documentkeywords which will be compared with the keywords of the corpus categorizes todetermine the tested document's best category.
arxiv-8700-202 | Unknown Words Analysis in POS tagging of Sinhala Language | http://arxiv.org/pdf/1501.01254v1.pdf | author:A. J. P. M. P. Jayaweera, N. G. J. Dias category:cs.CL I.2.7 published:2015-01-06 summary:Part of Speech (POS) is a very vital topic in Natural Language Processing(NLP) task in any language, which involves analysing the construction of thelanguage, behaviours and the dynamics of the language, the knowledge that couldbe utilized in computational linguistics analysis and automation applications.In this context, dealing with unknown words (words do not appear in the lexiconreferred as unknown words) is also an important task, since growing NLP systemsare used in more and more new applications. One aid of predicting lexicalcategories of unknown words is the use of syntactical knowledge of thelanguage. The distinction between open class words and closed class wordstogether with syntactical features of the language used in this research topredict lexical categories of unknown words in the tagging process. Anexperiment is performed to investigate the ability of the approach to parseunknown words using syntactical knowledge without human intervention. Thisexperiment shows that the performance of the tagging process is enhanced whenword class distinction is used together with syntactic rules to parse sentencescontaining unknown words in Sinhala language.
arxiv-8700-203 | Optimisation using Natural Language Processing: Personalized Tour Recommendation for Museums | http://arxiv.org/pdf/1501.01252v1.pdf | author:Mayeul Mathias, Assema Moussa, Fen Zhou, Juan-Manuel Torres-Moreno, Marie-Sylvie Poli, Didier Josselin, Marc El-Bèze, Andréa Carneiro Linhares, Francoise Rigat category:cs.AI cs.CL published:2015-01-06 summary:This paper proposes a new method to provide personalized tour recommendationfor museum visits. It combines an optimization of preference criteria ofvisitors with an automatic extraction of artwork importance from museuminformation based on Natural Language Processing using textual energy. Thisproject includes researchers from computer and social sciences. Some resultsare obtained with numerical experiments. They show that our model clearlyimproves the satisfaction of the visitor who follows the proposed tour. Thiswork foreshadows some interesting outcomes and applications about on-demandpersonalized visit of museums in a very near future.
arxiv-8700-204 | Un résumeur à base de graphes, indépéndant de la langue | http://arxiv.org/pdf/1501.01243v1.pdf | author:Juan-Manuel Torres-Moreno, Javier Ramirez, Iria da Cunha category:cs.CL published:2015-01-06 summary:In this paper we present REG, a graph-based approach for study a fundamentalproblem of Natural Language Processing (NLP): the automatic text summarization.The algorithm maps a document as a graph, then it computes the weight of theirsentences. We have applied this approach to summarize documents in threelanguages.
arxiv-8700-205 | Analog Signal Processing Approach for Coarse and Fine Depth Estimation | http://arxiv.org/pdf/1603.09712v1.pdf | author:Nihar Athreyas, Zhiguo Lai, Jai Gupta, Dev Gupta category:cs.CV published:2015-01-06 summary:Imaging and Image sensors is a field that is continuously evolving. There arenew products coming into the market every day. Some of these have very severeSize, Weight and Power constraints whereas other devices have to handle veryhigh computational loads. Some require both these conditions to be metsimultaneously. Current imaging architectures and digital image processingsolutions will not be able to meet these ever increasing demands. There is aneed to develop novel imaging architectures and image processing solutions toaddress these requirements. In this work we propose analog signal processing asa solution to this problem. The analog processor is not suggested as areplacement to a digital processor but it will be used as an augmentationdevice which works in parallel with the digital processor, making the systemfaster and more efficient. In order to show the merits of analog processing twostereo correspondence algorithms are implemented. We propose novelmodifications to the algorithms and new imaging architectures which,significantly reduces the computation time.
arxiv-8700-206 | Compressed sensing for longitudinal MRI: An adaptive-weighted approach | http://arxiv.org/pdf/1407.2602v3.pdf | author:Lior Weizman, Yonina C. Eldar, Dafna Ben Bashat category:physics.med-ph cs.CV published:2014-07-10 summary:Purpose: Repeated brain MRI scans are performed in many clinical scenarios,such as follow up of patients with tumors and therapy response assessment. Inthis paper, the authors show an approach to utilize former scans of the patientfor the acceleration of repeated MRI scans. Methods: The proposed approach utilizes the possible similarity of therepeated scans in longitudinal MRI studies. Since similarity is not guaranteed,sampling and reconstruction are adjusted during acquisition to match the actualsimilarity between the scans. The baseline MR scan is utilized both in thesampling stage, via adaptive sampling, and in the reconstruction stage, withweighted reconstruction. In adaptive sampling, k-space sampling locations areoptimized during acquisition. Weighted reconstruction uses the locations of thenonzero coefficients in the sparse domains as a prior in the recovery process.The approach was tested on 2D and 3D MRI scans of patients with brain tumors. Results: The longitudinal adaptive CS MRI (LACS-MRI) scheme providesreconstruction quality which outperforms other CS-based approaches for rapidMRI. Examples are shown on patients with brain tumors and demonstrate improvedspatial resolution. Compared with data sampled at Nyquist rate, LACS-MRIexhibits Signal-to-Error Ratio (SER) of 24.8dB with undersampling factor of16.6 in 3D MRI. Conclusions: The authors have presented a novel method for imagereconstruction utilizing similarity of scans in longitudinal MRI studies, wherepossible. The proposed approach can play a major part and significantly reducescanning time in many applications that consist of disease follow-up andmonitoring of longitudinal changes in brain MRI.
arxiv-8700-207 | An Effective Semi-supervised Divisive Clustering Algorithm | http://arxiv.org/pdf/1412.7625v2.pdf | author:Teng Qiu, Yongjie Li category:cs.LG cs.CV stat.ML published:2014-12-24 summary:Nowadays, data are generated massively and rapidly from scientific fields asbioinformatics, neuroscience and astronomy to business and engineering fields.Cluster analysis, as one of the major data analysis tools, is therefore moresignificant than ever. We propose in this work an effective Semi-supervisedDivisive Clustering algorithm (SDC). Data points are first organized by aminimal spanning tree. Next, this tree structure is transitioned to the in-treestructure, and then divided into sub-trees under the supervision of the labeleddata, and in the end, all points in the sub-trees are directly associated withspecific cluster centers. SDC is fully automatic, non-iterative, involving nofree parameter, insensitive to noise, able to detect irregularly shaped clusterstructures, applicable to the data sets of high dimensionality and differentattributes. The power of SDC is demonstrated on several datasets.
arxiv-8700-208 | A Study on Clustering for Clustering Based Image De-Noising | http://arxiv.org/pdf/1501.01106v1.pdf | author:Hossein Bakhshi Golestani, Mohsen Joneidi, Mostafa Sadeghi category:cs.CV published:2015-01-06 summary:In this paper, the problem of de-noising of an image contaminated withAdditive White Gaussian Noise (AWGN) is studied. This subject is an openproblem in signal processing for more than 50 years. Local methods suggested inrecent years, have obtained better results than global methods. However by moreintelligent training in such a way that first, important data is more effectivefor training, second, clustering in such way that training blocks lie inlow-rank subspaces, we can design a dictionary applicable for image de-noisingand obtain results near the state of the art local methods. In the presentpaper, we suggest a method based on global clustering of image constructingblocks. As the type of clustering plays an important role in clustering-basedde-noising methods, we address two questions about the clustering. The first,which parts of the data should be considered for clustering? and the second,what data clustering method is suitable for de-noising.? Then clustering isexploited to learn an over complete dictionary. By obtaining sparsedecomposition of the noisy image blocks in terms of the dictionary atoms, thede-noised version is achieved. In addition to our framework, 7 populardictionary learning methods are simulated and compared. The results arecompared based on two major factors: (1) de-noising performance and (2)execution time. Experimental results show that our dictionary learningframework outperforms its competitors in terms of both factors.
arxiv-8700-209 | A Novel Technique for Grading of Dates using Shape and Texture Features | http://arxiv.org/pdf/1501.01090v1.pdf | author:S. H. Mohana, C. J. Prabhakar category:cs.CV published:2015-01-06 summary:This paper presents a novel method to grade the date fruits based on thecombination of shape and texture features. The method begins with reducing thespecular reflection and small noise using a bilateral filter. Threshold basedsegmentation is performed for background removal and fruit part selection fromthe given image. Shape features is extracted using the contour of the datefruit and texture features are extracted using Curvelet transform and LocalBinary Pattern (LBP) from the selected date fruit region. Finally, combinationsof shape and texture features are fused to grade the dates into six grades.k-Nearest Neighbour(k-NN) classifier yields the best grading rate compared toother two classifiers such as Support Vector Machine (SVM) and LinearDiscriminant(LDA) classifiers. The experiment result shows that our techniqueachieves highest accuracy.
arxiv-8700-210 | Stem-Calyx Recognition of an Apple using Shape Descriptors | http://arxiv.org/pdf/1501.01083v1.pdf | author:S. H. Mohana, C. J. Prabhakar category:cs.CV published:2015-01-06 summary:This paper presents a novel method to recognize stem - calyx of an appleusing shape descriptors. The main drawback of existing apple grading techniquesis that stem - calyx part of an apple is treated as defects, this leads to poorgrading of apples. In order to overcome this drawback, we proposed an approachto recognize stem-calyx and differentiated from true defects based on shapefeatures. Our method comprises of steps such as segmentation of apple usinggrow-cut method, candidate objects such as stem-calyx and small defects aredetected using multi-threshold segmentation. The shape features are extractedfrom detected objects using Multifractal, Fourier and Radon descriptor andfinally stem-calyx regions are recognized and differentiated from true defectsusing SVM classifier. The proposed algorithm is evaluated using experimentsconducted on apple image dataset and results exhibit considerable improvementin recognition of stem-calyx region compared to other techniques.
arxiv-8700-211 | The Effect of Wedge Tip Angles on Stress Intensity Factors in the Contact Problem between Tilted Wedge and a Half Plane with an Edge Crack Using Digital Image Correlation | http://arxiv.org/pdf/1501.02246v1.pdf | author:Seyedmeysam Khaleghian, Anahita Emami, Mohammad Yadegari, Nasser Soltani category:cs.CV physics.optics published:2015-01-06 summary:The first and second mode stress intensity factors (SIFs) of a contactproblem between a half-plane with an edge crack and an asymmetric tilted wedgewere obtained using experimental method of Digital Image Correlation (DIC). Inthis technique, displacement and strain fields can be measured using twodigital images of the same sample at different stages of loading. However,several images were taken consequently in each stage of this experiment toavoid the noise effect. A pair of images of each stage was compared to eachother. Then, the correlation coefficients between them were studied using acomputer code. The pairs with the correlation coefficient higher than 0.8 wereselected as the acceptable match for displacement measurements near the cracktip. Subsequently, the SIFs of specimens were calculated using displacementfields obtained from DIC method. The effect of wedge tips angle on their SIFswas also studied. Moreover, the results of DIC method were compared with theresults of photoelasticity method and a close agreement between them wasobserved.
arxiv-8700-212 | Skincure: An Innovative Smart Phone-Based Application To Assist In Melanoma Early Detection And Prevention | http://arxiv.org/pdf/1501.01075v1.pdf | author:Omar Abuzaghleh, Miad Faezipour, Buket D. Barkana category:cs.CV cs.CY published:2015-01-06 summary:Melanoma spreads through metastasis, and therefore it has been proven to bevery fatal. Statistical evidence has revealed that the majority of deathsresulting from skin cancer are as a result of melanoma. Further investigationshave shown that the survival rates in patients depend on the stage of theinfection; early detection and intervention of melanoma implicates higherchances of cure. Clinical diagnosis and prognosis of melanoma is challengingsince the processes are prone to misdiagnosis and inaccuracies due to doctorssubjectivity. This paper proposes an innovative and fully functionalsmart-phone based application to assist in melanoma early detection andprevention. The application has two major components; the first component is areal-time alert to help users prevent skin burn caused by sunlight; a novelequation to compute the time for skin to burn is thereby introduced. The secondcomponent is an automated image analysis module which contains imageacquisition, hair detection and exclusion, lesion segmentation, featureextraction, and classification. The proposed system exploits PH2 Dermoscopyimage database from Pedro Hispano Hospital for development and testingpurposes. The image database contains a total of 200 dermoscopy images oflesions, including normal, atypical, and melanoma cases. The experimentalresults show that the proposed system is efficient, achieving classification ofthe normal, atypical and melanoma images with accuracy of 96.3%, 95.7% and97.5%, respectively.
arxiv-8700-213 | Improved Graph Clustering | http://arxiv.org/pdf/1210.3335v3.pdf | author:Yudong Chen, Sujay Sanghavi, Huan Xu category:stat.ML published:2012-10-11 summary:Graph clustering involves the task of dividing nodes into clusters, so thatthe edge density is higher within clusters as opposed to across clusters. Anatural, classic and popular statistical setting for evaluating solutions tothis problem is the stochastic block model, also referred to as the plantedpartition model. In this paper we present a new algorithm--a convexified version of MaximumLikelihood--for graph clustering. We show that, in the classic stochastic blockmodel setting, it outperforms existing methods by polynomial factors when thecluster size is allowed to have general scalings. In fact, it is withinlogarithmic factors of known lower bounds for spectral methods, and there isevidence suggesting that no polynomial time algorithm would do significantlybetter. We then show that this guarantee carries over to a more general extension ofthe stochastic block model. Our method can handle the settings of semi-randomgraphs, heterogeneous degree distributions, unequal cluster sizes, unaffiliatednodes, partially observed graphs and planted clique/coloring etc. Inparticular, our results provide the best exact recovery guarantees to date forthe planted partition, planted k-disjoint-cliques and planted noisy coloringmodels with general cluster sizes; in other settings, we match the bestexisting results up to logarithmic factors.
arxiv-8700-214 | Salient Object Detection: A Benchmark | http://arxiv.org/pdf/1501.02741v1.pdf | author:Ali Borji, Ming-Ming Cheng, Huaizu Jiang, Jia Li category:cs.CV published:2015-01-05 summary:We extensively compare, qualitatively and quantitatively, 40 state-of-the-artmodels (28 salient object detection, 10 fixation prediction, 1 objectness, and1 baseline) over 6 challenging datasets for the purpose of benchmarking salientobject detection and segmentation methods. From the results obtained so far,our evaluation shows a consistent rapid progress over the last few years interms of both accuracy and running time. The top contenders in this benchmarksignificantly outperform the models identified as the best in the previousbenchmark conducted just two years ago. We find that the models designedspecifically for salient object detection generally work better than models inclosely related areas, which in turn provides a precise definition and suggestsan appropriate treatment of this problem that distinguishes it from otherproblems. In particular, we analyze the influences of center bias and scenecomplexity in model performance, which, along with the hard cases forstate-of-the-art models, provide useful hints towards constructing morechallenging large scale datasets and better saliency models. Finally, wepropose probable solutions for tackling several open problems such asevaluation scores and dataset bias, which also suggest future researchdirections in the rapidly-growing field of salient object detection.
arxiv-8700-215 | Adaptive Objectness for Object Tracking | http://arxiv.org/pdf/1501.00909v1.pdf | author:Pengpeng Liang, Chunyuan Liao, Xue Mei, Haibin Ling category:cs.CV published:2015-01-05 summary:Object tracking is a long standing problem in vision. While great effortshave been spent to improve tracking performance, a simple yet reliable priorknowledge is left unexploited: the target object in tracking must be an objectother than non-object. The recently proposed and popularized objectness measureprovides a natural way to model such prior in visual tracking. Thus motivated,in this paper we propose to adapt objectness for visual object tracking.Instead of directly applying an existing objectness measure that is generic andhandles various objects and environments, we adapt it to be compatible to thespecific tracking sequence and object. More specifically, we use the newlyproposed BING objectness as the base, and then train an object-adaptiveobjectness for each tracking task. The training is implemented by using anadaptive support vector machine that integrates information from the specifictracking target into the BING measure. We emphasize that the benefit of theproposed adaptive objectness, named ADOBING, is generic. To show this, wecombine ADOBING with seven top performed trackers in recent evaluations. We runthe ADOBING-enhanced trackers with their base trackers on two popularbenchmarks, the CVPR2013 benchmark (50 sequences) and the Princeton TrackingBenchmark (100 sequences). On both benchmarks, our methods not onlyconsistently improve the base trackers, but also achieve the best knownperformances. Noting that the way we integrate objectness in visual tracking isgeneric and straightforward, we expect even more improvement by usingtracker-specific objectness.
arxiv-8700-216 | Likelihood Estimation with Incomplete Array Variate Observations | http://arxiv.org/pdf/1209.2669v9.pdf | author:Deniz Akdemir category:stat.ME math.ST stat.ML stat.TH published:2012-09-12 summary:Missing data is an important challenge when dealing with high dimensionaldata arranged in the form of an array. In this paper, we propose methods forestimation of the parameters of array variate normal probability model frompartially observed multiway data. The methods developed here are useful formissing data imputation, estimation of mean and covariance parameters formultiway data. A multiway semi-parametric mixed effects model that allowsseparation of multiway covariance effects is also defined and an efficientalgorithm for estimation is recommended. We provide simulation results alongwith real life data from genetics to demonstrate these methods.
arxiv-8700-217 | Fast forward feature selection for the nonlinear classification of hyperspectral images | http://arxiv.org/pdf/1501.00857v1.pdf | author:Mathieu Fauvel, Clement Dechesne, Anthony Zullo, Frédéric Ferraty category:cs.CV published:2015-01-05 summary:A fast forward feature selection algorithm is presented in this paper. It isbased on a Gaussian mixture model (GMM) classifier. GMM are used forclassifying hyperspectral images. The algorithm selects iteratively spectralfeatures that maximizes an estimation of the classification rate. Theestimation is done using the k-fold cross validation. In order to perform fastin terms of computing time, an efficient implementation is proposed. First, theGMM can be updated when the estimation of the classification rate is computed,rather than re-estimate the full model. Secondly, using marginalization of theGMM, sub models can be directly obtained from the full model learned with allthe spectral features. Experimental results for two real hyperspectral datasets show that the method performs very well in terms of classificationaccuracy and processing time. Furthermore, the extracted model contains veryfew spectral channels.
arxiv-8700-218 | Neural Network Regularization via Robust Weight Factorization | http://arxiv.org/pdf/1412.6630v2.pdf | author:Jan Rudy, Weiguang Ding, Daniel Jiwoong Im, Graham W. Taylor category:cs.LG cs.NE stat.ML published:2014-12-20 summary:Regularization is essential when training large neural networks. As deepneural networks can be mathematically interpreted as universal functionapproximators, they are effective at memorizing sampling noise in the trainingdata. This results in poor generalization to unseen data. Therefore, it is nosurprise that a new regularization technique, Dropout, was partiallyresponsible for the now-ubiquitous winning entry to ImageNet 2012 by theUniversity of Toronto. Currently, Dropout (and related methods such asDropConnect) are the most effective means of regularizing large neuralnetworks. These amount to efficiently visiting a large number of related modelsat training time, while aggregating them to a single predictor at test time.The proposed FaMe model aims to apply a similar strategy, yet learns afactorization of each weight matrix such that the factors are robust to noise.
arxiv-8700-219 | Chasing the Ghosts of Ibsen: A computational stylistic analysis of drama in translation | http://arxiv.org/pdf/1501.00841v1.pdf | author:Gerard Lynch, Carl Vogel category:cs.CL published:2015-01-05 summary:Research into the stylistic properties of translations is an issue which hasreceived some attention in computational stylistics. Previous work by Rybicki(2006) on the distinguishing of character idiolects in the work of Polishauthor Henryk Sienkiewicz and two corresponding English translations usingBurrow's Delta method concluded that idiolectal differences could be observedin the source texts and this variation was preserved to a large degree in bothtranslations. This study also found that the two translations were also highlydistinguishable from one another. Burrows (2002) examined English translationsof Juvenal also using the Delta method, results of this work suggest that sometranslators are more adept at concealing their own style when translating theworks of another author whereas other authors tend to imprint their own styleto a greater extent on the work they translate. Our work examines the writingof a single author, Norwegian playwright Henrik Ibsen, and these writingstranslated into both German and English from Norwegian, in an attempt toinvestigate the preservation of characterization, defined here as thedistinctiveness of textual contributions of characters.
arxiv-8700-220 | Inverse Renormalization Group Transformation in Bayesian Image Segmentations | http://arxiv.org/pdf/1501.00834v1.pdf | author:Kazuyuki Tanaka, Shun Kataoka, Muneki Yasuda, Masayuki Ohzeki category:cs.CV stat.ML published:2015-01-05 summary:A new Bayesian image segmentation algorithm is proposed by combining a loopybelief propagation with an inverse real space renormalization grouptransformation to reduce the computational time. In results of our experiment,we observe that the proposed method can reduce the computational time to lessthan one-tenth of that taken by conventional Bayesian approaches.
arxiv-8700-221 | Group $K$-Means | http://arxiv.org/pdf/1501.00825v1.pdf | author:Jianfeng Wang, Shuicheng Yan, Yi Yang, Mohan S Kankanhalli, Shipeng Li, Jingdong Wang category:cs.CV published:2015-01-05 summary:We study how to learn multiple dictionaries from a dataset, and approximateany data point by the sum of the codewords each chosen from the correspondingdictionary. Although theoretically low approximation errors can be achieved bythe global solution, an effective solution has not been well studied inpractice. To solve the problem, we propose a simple yet effective algorithm\textit{Group $K$-Means}. Specifically, we take each dictionary, or any twoselected dictionaries, as a group of $K$-means cluster centers, and then dealwith the approximation issue by minimizing the approximation errors. Besides,we propose a hierarchical initialization for such a non-convex problem.Experimental results well validate the effectiveness of the approach.
arxiv-8700-222 | Sparse Deep Stacking Network for Image Classification | http://arxiv.org/pdf/1501.00777v1.pdf | author:Jun Li, Heyou Chang, Jian Yang category:cs.CV cs.LG cs.NE published:2015-01-05 summary:Sparse coding can learn good robust representation to noise and model morehigher-order representation for image classification. However, the inferencealgorithm is computationally expensive even though the supervised signals areused to learn compact and discriminative dictionaries in sparse codingtechniques. Luckily, a simplified neural network module (SNNM) has beenproposed to directly learn the discriminative dictionaries for avoiding theexpensive inference. But the SNNM module ignores the sparse representations.Therefore, we propose a sparse SNNM module by adding the mixed-normregularization (l1/l2 norm). The sparse SNNM modules are further stacked tobuild a sparse deep stacking network (S-DSN). In the experiments, we evaluateS-DSN with four databases, including Extended YaleB, AR, 15 scene andCaltech101. Experimental results show that our model outperforms relatedclassification methods with only a linear classifier. It is worth noting thatwe reach 98.8% recognition accuracy on 15 scene.
arxiv-8700-223 | Hashing with binary autoencoders | http://arxiv.org/pdf/1501.00756v1.pdf | author:Miguel Á. Carreira-Perpiñán, Ramin Raziperchikolaei category:cs.LG cs.CV math.OC stat.ML published:2015-01-05 summary:An attractive approach for fast search in image databases is binary hashing,where each high-dimensional, real-valued image is mapped onto alow-dimensional, binary vector and the search is done in this binary space.Finding the optimal hash function is difficult because it involves binaryconstraints, and most approaches approximate the optimization by relaxing theconstraints and then binarizing the result. Here, we focus on the binaryautoencoder model, which seeks to reconstruct an image from the binary codeproduced by the hash function. We show that the optimization can be simplifiedwith the method of auxiliary coordinates. This reformulates the optimization asalternating two easier steps: one that learns the encoder and decoderseparately, and one that optimizes the code for each image. Image retrievalexperiments, using precision/recall and a measure of code utilization, show theresulting hash function outperforms or is competitive with state-of-the-artmethods for binary hashing.
arxiv-8700-224 | Concave Penalized Estimation of Sparse Gaussian Bayesian Networks | http://arxiv.org/pdf/1401.0852v2.pdf | author:Bryon Aragam, Qing Zhou category:stat.ME cs.LG stat.ML published:2014-01-04 summary:We develop a penalized likelihood estimation framework to estimate thestructure of Gaussian Bayesian networks from observational data. In contrast torecent methods which accelerate the learning problem by restricting the searchspace, our main contribution is a fast algorithm for score-based structurelearning which does not restrict the search space in any way and works onhigh-dimensional datasets with thousands of variables. Our use of concaveregularization, as opposed to the more popular $\ell_0$ (e.g. BIC) penalty, isnew. Moreover, we provide theoretical guarantees which generalize existingasymptotic results when the underlying distribution is Gaussian. Most notably,our framework does not require the existence of a so-called faithful DAGrepresentation, and as a result the theory must handle the inherentnonidentifiability of the estimation problem in a novel way. Finally, as amatter of independent interest, we provide a comprehensive comparison of ourapproach to several standard structure learning methods using open-sourcepackages developed for the R language. Based on these experiments, we show thatour algorithm is significantly faster than other competing methods whileobtaining higher sensitivity with comparable false discovery rates forhigh-dimensional data. In particular, the total runtime for our method togenerate a solution path of 20 estimates for DAGs with 8000 nodes is around onehour.
arxiv-8700-225 | Differential Search Algorithm-based Parametric Optimization of Fuzzy Generalized Eigenvalue Proximal Support Vector Machine | http://arxiv.org/pdf/1501.00728v1.pdf | author:M. H. Marghny, Rasha M. Abd ElAziz, Ahmed I. Taloba category:cs.LG published:2015-01-04 summary:Support Vector Machine (SVM) is an effective model for many classificationproblems. However, SVM needs the solution of a quadratic program which requirespecialized code. In addition, SVM has many parameters, which affects theperformance of SVM classifier. Recently, the Generalized Eigenvalue ProximalSVM (GEPSVM) has been presented to solve the SVM complexity. In real worldapplications data may affected by error or noise, working with this data is achallenging problem. In this paper, an approach has been proposed to overcomethis problem. This method is called DSA-GEPSVM. The main improvements arecarried out based on the following: 1) a novel fuzzy values in the linear case.2) A new Kernel function in the nonlinear case. 3) Differential SearchAlgorithm (DSA) is reformulated to find near optimal values of the GEPSVMparameters and its kernel parameters. The experimental results show that theproposed approach is able to find the suitable parameter values, and has higherclassification accuracy compared with some other algorithms.
arxiv-8700-226 | A generalization error bound for sparse and low-rank multivariate Hawkes processes | http://arxiv.org/pdf/1501.00725v1.pdf | author:Emmanuel Bacry, Stéphane Gaïffas, Jean-François Muzy category:stat.ML published:2015-01-04 summary:We consider the problem of unveiling the implicit network structure of userinteractions in a social network, based only on high-frequency timestamps. Ourinference is based on the minimization of the least-squares loss associatedwith a multivariate Hawkes model, penalized by $\ell_1$ and trace norms. Weprovide a first theoretical analysis of the generalization error for thisproblem, that includes sparsity and low-rank inducing priors. This resultinvolves a new data-driven concentration inequality for matrix martingales incontinuous time with observable variance, which is a result of independentinterest. A consequence of our analysis is the construction of sharply tuned$\ell_1$ and trace-norm penalizations, that leads to a data-driven scaling ofthe variability of information available for each users. Numerical experimentsillustrate the strong improvements achieved by the use of such data-drivenpenalizations.
arxiv-8700-227 | Protein Secondary Structure Prediction with Long Short Term Memory Networks | http://arxiv.org/pdf/1412.7828v2.pdf | author:Søren Kaae Sønderby, Ole Winther category:q-bio.QM cs.LG cs.NE published:2014-12-25 summary:Prediction of protein secondary structure from the amino acid sequence is aclassical bioinformatics problem. Common methods use feed forward neuralnetworks or SVMs combined with a sliding window, as these models does notnaturally handle sequential data. Recurrent neural networks are angeneralization of the feed forward neural network that naturally handlesequential data. We use a bidirectional recurrent neural network with longshort term memory cells for prediction of secondary structure and evaluateusing the CB513 dataset. On the secondary structure 8-class problem we reportbetter performance (0.674) than state of the art (0.664). Our model includesfeed forward networks between the long short term memory cells, a path that canbe further explored.
arxiv-8700-228 | On Enhancing The Performance Of Nearest Neighbour Classifiers Using Hassanat Distance Metric | http://arxiv.org/pdf/1501.00687v1.pdf | author:Mouhammd Alkasassbeh, Ghada A. Altarawneh, Ahmad B. A. Hassanat category:cs.LG published:2015-01-04 summary:We showed in this work how the Hassanat distance metric enhances theperformance of the nearest neighbour classifiers. The results demonstrate thesuperiority of this distance metric over the traditional and most-useddistances, such as Manhattan distance and Euclidian distance. Moreover, weproved that the Hassanat distance metric is invariant to data scale, noise andoutliers. Throughout this work, it is clearly notable that both ENN and IINCperformed very well with the distance investigated, as their accuracy increasedsignificantly by 3.3% and 3.1% respectively, with no significant advantage ofthe ENN over the IINC in terms of accuracy. Correspondingly, it can be notedfrom our results that there is no optimal algorithm that can solve allreal-life problems perfectly; this is supported by the no-free-lunch theorem
arxiv-8700-229 | A New Method for Signal and Image Analysis: The Square Wave Method | http://arxiv.org/pdf/1501.00680v1.pdf | author:Osvaldo Skliar, Ricardo E. Monge, Sherry Gapper category:cs.NA cs.CV math.NA 94A12, 65F99 published:2015-01-04 summary:A brief review is provided of the use of the Square Wave Method (SWM) in thefield of signal and image analysis and it is specified how results thusobtained are expressed using the Square Wave Transform (SWT), in the frequencydomain. To illustrate the new approach introduced in this field, the results oftwo cases are analyzed: a) a sequence of samples (that is, measured values) ofan electromyographic recording; and b) the classic image of Lenna.
arxiv-8700-230 | Hostile Intent Identification by Movement Pattern Analysis: Using Artificial Neural Networks | http://arxiv.org/pdf/1501.00653v1.pdf | author:Souham Biswas, Manisha J. Nene category:cs.AI cs.NE published:2015-01-04 summary:In the recent years, the problem of identifying suspicious behavior hasgained importance and identifying this behavior using computational systems andautonomous algorithms is highly desirable in a tactical scenario. So far, thesolutions have been primarily manual which elicit human observation of entitiesto discern the hostility of the situation. To cater to this problem statement,a number of fully automated and partially automated solutions exist. But, thesesolutions lack the capability of learning from experiences and work inconjunction with human supervision which is extremely prone to error. In thispaper, a generalized methodology to predict the hostility of a given objectbased on its movement patterns is proposed which has the ability to learn andis based upon the mechanism of humans of learning from experiences. Themethodology so proposed has been implemented in a computer simulation. Theresults show that the posited methodology has the potential to be applied inreal world tactical scenarios.
arxiv-8700-231 | Microbial community pattern detection in human body habitats via ensemble clustering framework | http://arxiv.org/pdf/1412.7384v3.pdf | author:Peng Yang, Xiaoquan Su, Le Ou-Yang, Hon-Nian Chua, Xiao-Li Li, Kang Ning category:q-bio.QM cs.CE cs.LG q-bio.GN published:2014-12-21 summary:The human habitat is a host where microbial species evolve, function, andcontinue to evolve. Elucidating how microbial communities respond to humanhabitats is a fundamental and critical task, as establishing baselines of humanmicrobiome is essential in understanding its role in human disease and health.However, current studies usually overlook a complex and interconnectedlandscape of human microbiome and limit the ability in particular body habitatswith learning models of specific criterion. Therefore, these methods could notcapture the real-world underlying microbial patterns effectively. To obtain acomprehensive view, we propose a novel ensemble clustering framework to minethe structure of microbial community pattern on large-scale metagenomic data.Particularly, we first build a microbial similarity network via integrating1920 metagenomic samples from three body habitats of healthy adults. Then anovel symmetric Nonnegative Matrix Factorization (NMF) based ensemble model isproposed and applied onto the network to detect clustering pattern. Extensiveexperiments are conducted to evaluate the effectiveness of our model onderiving microbial community with respect to body habitat and host gender. Fromclustering results, we observed that body habitat exhibits a strong bound butnon-unique microbial structural patterns. Meanwhile, human microbiome revealsdifferent degree of structural variations over body habitat and host gender. Insummary, our ensemble clustering framework could efficiently explore integratedclustering results to accurately identify microbial communities, and provide acomprehensive view for a set of microbial communities. Such trends depict anintegrated biography of microbial communities, which offer a new insighttowards uncovering pathogenic model of human microbiome.
arxiv-8700-232 | Understanding Trajectory Behavior: A Motion Pattern Approach | http://arxiv.org/pdf/1501.00614v1.pdf | author:Mahdi M. Kalayeh, Stephen Mussmann, Alla Petrakova, Niels da Vitoria Lobo, Mubarak Shah category:cs.CV published:2015-01-04 summary:Mining the underlying patterns in gigantic and complex data is of greatimportance to data analysts. In this paper, we propose a motion patternapproach to mine frequent behaviors in trajectory data. Motion patterns,defined by a set of highly similar flow vector groups in a spatial locality,have been shown to be very effective in extracting dominant motion behaviors invideo sequences. Inspired by applications and properties of motion patterns, wehave designed a framework that successfully solves the general task oftrajectory clustering. Our proposed algorithm consists of four phases: flowvector computation, motion component extraction, motion component'sreachability set creation, and motion pattern formation. For the first phase,we break down trajectories into flow vectors that indicate instantaneousmovements. In the second phase, via a Kmeans clustering approach, we createmotion components by clustering the flow vectors with respect to their locationand velocity. Next, we create motion components' reachability set in terms ofspatial proximity and motion similarity. Finally, for the fourth phase, wecluster motion components using agglomerative clustering with the weightedJaccard distance between the motion components' signatures, a set created usingpath reachability. We have evaluated the effectiveness of our proposed methodin an extensive set of experiments on diverse datasets. Further, we have shownhow our proposed method handles difficulties in the general task of trajectoryclustering that challenge the existing state-of-the-art methods.
arxiv-8700-233 | Evaluation of Predictive Data Mining Algorithms in Erythemato-Squamous Disease Diagnosis | http://arxiv.org/pdf/1501.00607v1.pdf | author:Kwetishe Danjuma, Adenike O. Osofisan category:cs.LG cs.CE published:2015-01-03 summary:A lot of time is spent searching for the most performing data miningalgorithms applied in clinical diagnosis. The study set out to identify themost performing predictive data mining algorithms applied in the diagnosis ofErythemato-squamous diseases. The study used Naive Bayes, Multilayer Perceptronand J48 decision tree induction to build predictive data mining models on 366instances of Erythemato-squamous diseases datasets. Also, 10-foldcross-validation and sets of performance metrics were used to evaluate thebaseline predictive performance of the classifiers. The comparative analysisshows that the Naive Bayes performed best with accuracy of 97.4%, MultilayerPerceptron came out second with accuracy of 96.6%, and J48 came out the worstwith accuracy of 93.5%. The evaluation of these classifiers on clinicaldatasets, gave an insight into the predictive ability of different data miningalgorithms applicable in clinical diagnosis especially in the diagnosis ofErythemato-squamous diseases.
arxiv-8700-234 | A Taxonomy of Big Data for Optimal Predictive Machine Learning and Data Mining | http://arxiv.org/pdf/1501.00604v1.pdf | author:Ernest Fokoue category:stat.ML 60K35 published:2015-01-03 summary:Big data comes in various ways, types, shapes, forms and sizes. Indeed,almost all areas of science, technology, medicine, public health, economics,business, linguistics and social science are bombarded by ever increasing flowsof data begging to analyzed efficiently and effectively. In this paper, wepropose a rough idea of a possible taxonomy of big data, along with some of themost commonly used tools for handling each particular category of bigness. Thedimensionality p of the input space and the sample size n are usually the mainingredients in the characterization of data bigness. The specific statisticalmachine learning technique used to handle a particular big data set will dependon which category it falls in within the bigness taxonomy. Large p small n datasets for instance require a different set of tools from the large n small pvariety. Among other tools, we discuss Preprocessing, Standardization,Imputation, Projection, Regularization, Penalization, Compression, Reduction,Selection, Kernelization, Hybridization, Parallelization, Aggregation,Randomization, Replication, Sequentialization. Indeed, it is important toemphasize right away that the so-called no free lunch theorem applies here, inthe sense that there is no universally superior method that outperforms allother methods on all categories of bigness. It is also important to stress thefact that simplicity in the sense of Ockham's razor non plurality principle ofparsimony tends to reign supreme when it comes to massive data. We concludewith a comparison of the predictive performance of some of the most commonlyused methods on a few data sets.
arxiv-8700-235 | The Learnability of Unknown Quantum Measurements | http://arxiv.org/pdf/1501.00559v1.pdf | author:Hao-Chung Cheng, Min-Hsiu Hsieh, Ping-Cheng Yeh category:quant-ph cs.LG stat.ML published:2015-01-03 summary:Quantum machine learning has received significant attention in recent years,and promising progress has been made in the development of quantum algorithmsto speed up traditional machine learning tasks. In this work, however, we focuson investigating the information-theoretic upper bounds of sample complexity -how many training samples are sufficient to predict the future behaviour of anunknown target function. This kind of problem is, arguably, one of the mostfundamental problems in statistical learning theory and the bounds forpractical settings can be completely characterised by a simple measure ofcomplexity. Our main result in the paper is that, for learning an unknown quantummeasurement, the upper bound, given by the fat-shattering dimension, islinearly proportional to the dimension of the underlying Hilbert space.Learning an unknown quantum state becomes a dual problem to ours, and as abyproduct, we can recover Aaronson's famous result [Proc. R. Soc. A463:3089-3144 (2007)] solely using a classical machine learning technique. Inaddition, other famous complexity measures like covering numbers and Rademachercomplexities are derived explicitly. We are able to connect measures of samplecomplexity with various areas in quantum information science, e.g. quantumstate/measurement tomography, quantum state discrimination and quantum randomaccess codes, which may be of independent interest. Lastly, with the assistanceof general Bloch-sphere representation, we show that learning quantummeasurements/states can be mathematically formulated as a neural network.Consequently, classical ML algorithms can be applied to efficiently accomplishthe two quantum learning tasks.
arxiv-8700-236 | An Empirical Study of the L2-Boost technique with Echo State Networks | http://arxiv.org/pdf/1501.00503v1.pdf | author:Sebastián Basterrech category:cs.LG cs.NE published:2015-01-02 summary:A particular case of Recurrent Neural Network (RNN) was introduced at thebeginning of the 2000s under the name of Echo State Networks (ESNs). The ESNmodel overcomes the limitations during the training of the RNNs whileintroducing no significant disadvantages. Although the model presents somewell-identified drawbacks when the parameters are not well initialised. Theperformance of an ESN is highly dependent on its internal parameters andpattern of connectivity of the hidden-hidden weights Often, the tuning of thenetwork parameters can be hard and can impact in the accuracy of the models. In this work, we investigate the performance of a specific boosting technique(called L2-Boost) with ESNs as single predictors. The L2-Boost technique hasbeen shown to be an effective tool to combine "weak" predictors in regressionproblems. In this study, we use an ensemble of random initialized ESNs (withoutcontrol their parameters) as "weak" predictors of the boosting procedure. Weevaluate our approach on five well-know time-series benchmark problems.Additionally, we compare this technique with a baseline approach that consistsof averaging the prediction of an ensemble of ESNs.
arxiv-8700-237 | Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent | http://arxiv.org/pdf/1407.1537v4.pdf | author:Zeyuan Allen-Zhu, Lorenzo Orecchia category:cs.DS cs.LG cs.NA math.OC stat.ML published:2014-07-06 summary:First-order methods play a central role in large-scale convex optimization.Even though many variations exist, each suited to a particular problem form,almost all such methods fundamentally rely on two types of algorithmic stepsand two corresponding types of analysis: gradient-descent steps, which yieldprimal progress, and mirror-descent steps, which yield dual progress. In thispaper, we observe that the performances of these two types of step arecomplementary, so that faster algorithms can be designed by linearly couplingthe two steps. In particular, we obtain a simple accelerated gradient method for the classof smooth convex optimization problems. The first such method was proposed byNesterov back to 1983, but to the best of our knowledge, the proof of the fastconvergence of accelerated gradient methods has not found a clearinterpretation and is still regarded by many as crucially relying on "algebraictricks". We apply our novel insights to construct a new accelerated gradientmethod as a natural linear coupling of gradient descent and mirror descent andto write its proof of convergence as a simple combination of the convergenceanalyses of the two underlying descent steps. We believe that the complementary view and the linear coupling technique inthis paper will prove very useful in the design of first-order methods as itallows us to design fast algorithms in a conceptually easier way. For instance,our technique greatly facilitates the recent breakthroughs in solving packingand covering linear programs [AO14, AO15].
arxiv-8700-238 | Feature Augmentation via Nonparametrics and Selection (FANS) in High Dimensional Classification | http://arxiv.org/pdf/1401.0211v2.pdf | author:Jianqing Fan, Yang Feng, Jiancheng Jiang, Xin Tong category:stat.ME math.ST stat.AP stat.ML stat.TH published:2013-12-31 summary:We propose a high dimensional classification method that involvesnonparametric feature augmentation. Knowing that marginal density ratios arethe most powerful univariate classifiers, we use the ratio estimates totransform the original feature measurements. Subsequently, penalized logisticregression is invoked, taking as input the newly transformed or augmentedfeatures. This procedure trains models equipped with local complexity andglobal simplicity, thereby avoiding the curse of dimensionality while creatinga flexible nonlinear decision boundary. The resulting method is called FeatureAugmentation via Nonparametrics and Selection (FANS). We motivate FANS bygeneralizing the Naive Bayes model, writing the log ratio of joint densities asa linear combination of those of marginal densities. It is related togeneralized additive models, but has better interpretability and computability.Risk bounds are developed for FANS. In numerical analysis, FANS is comparedwith competing methods, so as to provide a guideline on its best applicationdomain. Real data analysis demonstrates that FANS performs very competitivelyon benchmark email spam and gene expression data sets. Moreover, FANS isimplemented by an extremely fast algorithm through parallel computing.
arxiv-8700-239 | Computational Feasibility of Clustering under Clusterability Assumptions | http://arxiv.org/pdf/1501.00437v1.pdf | author:Shai Ben-David category:cs.CC cs.LG 68Q25, 68Q32 published:2015-01-02 summary:It is well known that most of the common clustering objectives are NP-hard tooptimize. In practice, however, clustering is being routinely carried out. Oneapproach for providing theoretical understanding of this seeming discrepancy isto come up with notions of clusterability that distinguish realisticallyinteresting input data from worst-case data sets. The hope is that there willbe clustering algorithms that are provably efficient on such 'clusterable'instances. In other words, hope that "Clustering is difficult only when it doesnot matter" (CDNM thesis, for short). We believe that to some extent this may indeed be the case. This paperprovides a survey of recent papers along this line of research and a criticalevaluation their results. Our bottom line conclusion is that that CDNM thesisis still far from being formally substantiated. We start by discussing whichrequirements should be met in order to provide formal support the validity ofthe CDNM thesis. In particular, we list some implied requirements for notionsof clusterability. We then examine existing results in view of thoserequirements and outline some research challenges and open questions.
arxiv-8700-240 | An Experimental Analysis of the Echo State Network Initialization Using the Particle Swarm Optimization | http://arxiv.org/pdf/1501.00436v1.pdf | author:Sebastián Basterrech, Enrique Alba, Václav Snášel category:cs.NE published:2015-01-02 summary:This article introduces a robust hybrid method for solving supervisedlearning tasks, which uses the Echo State Network (ESN) model and the ParticleSwarm Optimization (PSO) algorithm. An ESN is a Recurrent Neural Network withthe hidden-hidden weights fixed in the learning process. The recurrent part ofthe network stores the input information in internal states of the network.Another structure forms a free-memory method used as supervised learning tool.The setting procedure for initializing the recurrent structure of the ESN modelcan impact on the model performance. On the other hand, the PSO has been shownto be a successful technique for finding optimal points in complex spaces.Here, we present an approach to use the PSO for finding some initialhidden-hidden weights of the ESN model. We present empirical results thatcompare the canonical ESN model with this hybrid method on a wide range ofbenchmark problems.
arxiv-8700-241 | Efficiently Discovering Frequent Motifs in Large-scale Sensor Data | http://arxiv.org/pdf/1501.00405v1.pdf | author:Puneet Agarwal, Gautam Shroff, Sarmimala Saikia, Zaigham Khan category:cs.DB cs.LG published:2015-01-02 summary:While analyzing vehicular sensor data, we found that frequently occurringwaveforms could serve as features for further analysis, such as rule mining,classification, and anomaly detection. The discovery of waveform patterns, alsoknown as time-series motifs, has been studied extensively; however, availabletechniques for discovering frequently occurring time-series motifs were foundlacking in either efficiency or quality: Standard subsequence clusteringresults in poor quality, to the extent that it has even been termed'meaningless'. Variants of hierarchical clustering using techniques forefficient discovery of 'exact pair motifs' find high-quality frequent motifs,but at the cost of high computational complexity, making such techniquesunusable for our voluminous vehicular sensor data. We show that good qualityfrequent motifs can be discovered using bounded spherical clustering oftime-series subsequences, which we refer to as COIN clustering, with nearlinear complexity in time-series size. COIN clustering addresses many of thechallenges that previously led to subsequence clustering being viewed asmeaningless. We describe an end-to-end motif-discovery procedure using asequence of pre and post-processing techniques that remove trivial-matches andshifted-motifs, which also plagued previous subsequence-clustering approaches.We demonstrate that our technique efficiently discovers frequent motifs involuminous vehicular sensor data as well as in publicly available data sets.
arxiv-8700-242 | Passing Expectation Propagation Messages with Kernel Methods | http://arxiv.org/pdf/1501.00375v1.pdf | author:Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess category:stat.ML cs.LG published:2015-01-02 summary:We propose to learn a kernel-based message operator which takes as input allexpectation propagation (EP) incoming messages to a factor node and produces anoutgoing message. In ordinary EP, computing an outgoing message involvesestimating a multivariate integral which may not have an analytic expression.Learning such an operator allows one to bypass the expensive computation of theintegral during inference by directly mapping all incoming messages into anoutgoing message. The operator can be learned from training data (examples ofinput and output messages) which allows automated inference to be made on anykind of factor that can be sampled.
arxiv-8700-243 | Stochastic Optimization with Importance Sampling | http://arxiv.org/pdf/1401.2753v2.pdf | author:Peilin Zhao, Tong Zhang category:stat.ML cs.LG published:2014-01-13 summary:Uniform sampling of training data has been commonly used in traditionalstochastic optimization algorithms such as Proximal Stochastic Gradient Descent(prox-SGD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Althoughuniform sampling can guarantee that the sampled stochastic quantity is anunbiased estimate of the corresponding true quantity, the resulting estimatormay have a rather high variance, which negatively affects the convergence ofthe underlying optimization procedure. In this paper we study stochasticoptimization with importance sampling, which improves the convergence rate byreducing the stochastic variance. Specifically, we study prox-SGD (actually,stochastic mirror descent) with importance sampling and prox-SDCA withimportance sampling. For prox-SGD, instead of adopting uniform samplingthroughout the training process, the proposed algorithm employs importancesampling to minimize the variance of the stochastic gradient. For prox-SDCA,the proposed importance sampling scheme aims to achieve higher expected dualvalue at each dual coordinate ascent step. We provide extensive theoreticalanalysis to show that the convergence rates with the proposed importancesampling methods can be significantly improved under suitable conditions bothfor prox-SGD and for prox-SDCA. Experiments are provided to verify thetheoretical analysis.
arxiv-8700-244 | Comprehend DeepWalk as Matrix Factorization | http://arxiv.org/pdf/1501.00358v1.pdf | author:Cheng Yang, Zhiyuan Liu category:cs.LG published:2015-01-02 summary:Word2vec, as an efficient tool for learning vector representation of wordshas shown its effectiveness in many natural language processing tasks. Mikolovet al. issued Skip-Gram and Negative Sampling model for developing thistoolbox. Perozzi et al. introduced the Skip-Gram model into the study of socialnetwork for the first time, and designed an algorithm named DeepWalk forlearning node embedding on a graph. We prove that the DeepWalk algorithm isactually factoring a matrix M where each entry M_{ij} is logarithm of theaverage probability that node i randomly walks to node j in fix steps.
arxiv-8700-245 | Multi-Access Communications with Energy Harvesting: A Multi-Armed Bandit Model and the Optimality of the Myopic Policy | http://arxiv.org/pdf/1501.00329v1.pdf | author:Pol Blasco, Deniz Gunduz category:cs.IT cs.LG math.IT published:2015-01-01 summary:A multi-access wireless network with N transmitting nodes, each equipped withan energy harvesting (EH) device and a rechargeable battery of finite capacity,is studied. At each time slot (TS) a node is operative with a certainprobability, which may depend on the availability of data, or the state of itschannel. The energy arrival process at each node is modelled as an independenttwo-state Markov process, such that, at each TS, a node either harvests oneunit of energy, or none. At each TS a subset of the nodes is scheduled by theaccess point (AP). The scheduling policy that maximises the total throughput isstudied assuming that the AP does not know the states of either the EHprocesses or the batteries. The problem is identified as a restless multiarmedbandit (RMAB) problem, and an upper bound on the optimal scheduling policy isfound. Under certain assumptions regarding the EH processes and the batterysizes, the optimality of the myopic policy (MP) is proven. For the generalcase, the performance of MP is compared numerically to the upper bound.
arxiv-8700-246 | SIRF: Simultaneous Image Registration and Fusion in A Unified Framework | http://arxiv.org/pdf/1411.5065v2.pdf | author:Chen Chen, Yeqing Li, Wei Liu, Junzhou Huang category:cs.CV published:2014-11-18 summary:In this paper, we propose a novel method for image fusion with ahigh-resolution panchromatic image and a low-resolution multispectral image atthe same geographical location. The fusion is formulated as a convexoptimization problem which minimizes a linear combination of a least-squaresfitting term and a dynamic gradient sparsity regularizer. The former is topreserve accurate spectral information of the multispectral image, while thelatter is to keep sharp edges of the high-resolution panchromatic image. Wefurther propose to simultaneously register the two images during the fusingprocess, which is naturally achieved by virtue of the dynamic gradient sparsityproperty. An efficient algorithm is then devised to solve the optimizationproblem, accomplishing a linear computational complexity in the size of theoutput image in each iteration. We compare our method against sevenstate-of-the-art image fusion methods on multispectral image datasets from foursatellites. Extensive experimental results demonstrate that the proposed methodsubstantially outperforms the others in terms of both spatial and spectralqualities. We also show that our method can provide high-quality products fromcoarsely registered real-world datasets. Finally, a MATLAB implementation isprovided to facilitate future research.
arxiv-8700-247 | A robust sub-linear time R-FFAST algorithm for computing a sparse DFT | http://arxiv.org/pdf/1501.00320v1.pdf | author:Sameer Pawar, Kannan Ramchandran category:cs.IT cs.LG math.IT published:2015-01-01 summary:The Fast Fourier Transform (FFT) is the most efficiently known way to computethe Discrete Fourier Transform (DFT) of an arbitrary n-length signal, and has acomputational complexity of O(n log n). If the DFT X of the signal x has only knon-zero coefficients (where k < n), can we do better? In [1], we addressedthis question and presented a novel FFAST (Fast Fourier Aliasing-based SparseTransform) algorithm that cleverly induces sparse graph alias codes in the DFTdomain, via a Chinese-Remainder-Theorem (CRT)-guided sub-sampling operation ofthe time-domain samples. The resulting sparse graph alias codes are thenexploited to devise a fast and iterative onion-peeling style decoder thatcomputes an n length DFT of a signal using only O(k) time-domain samples andO(klog k) computations. The FFAST algorithm is applicable whenever k issub-linear in n (i.e. k = o(n)), but is obviously most attractive when k ismuch smaller than n. In this paper, we adapt the FFAST framework of [1] to the case where thetime-domain samples are corrupted by a white Gaussian noise. In particular, weshow that the extended noise robust algorithm R-FFAST computes an n-lengthk-sparse DFT X using O(klog ^3 n) noise-corrupted time-domain samples, inO(klog^4n) computations, i.e., sub-linear time complexity. While ourtheoretical results are for signals with a uniformly random support of thenon-zero DFT coefficients and additive white Gaussian noise, we providesimulation results which demonstrates that the R-FFAST algorithm performs welleven for signals like MR images, that have an approximately sparse Fourierspectrum with a non-uniform support for the dominant DFT coefficients.
arxiv-8700-248 | Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima | http://arxiv.org/pdf/1305.2436v2.pdf | author:Po-Ling Loh, Martin J. Wainwright category:math.ST cs.IT math.IT stat.ML stat.TH 62F12 published:2013-05-10 summary:We provide novel theoretical results regarding local optima of regularized$M$-estimators, allowing for nonconvexity in both loss and penalty functions.Under restricted strong convexity on the loss and suitable regularityconditions on the penalty, we prove that \emph{any stationary point} of thecomposite objective function will lie within statistical precision of theunderlying parameter vector. Our theory covers many nonconvex objectivefunctions of interest, including the corrected Lasso for errors-in-variableslinear models; regression for generalized linear models with nonconvexpenalties such as SCAD, MCP, and capped-$\ell_1$; and high-dimensionalgraphical model estimation. We quantify statistical accuracy by providingbounds on the $\ell_1$-, $\ell_2$-, and prediction error between stationarypoints and the population-level optimum. We also propose a simple modificationof composite gradient descent that may be used to obtain a near-global optimumwithin statistical precision $\epsilon$ in $\log(1/\epsilon)$ steps, which isthe fastest possible rate of any first-order method. We provide simulationstudies illustrating the sharpness of our theoretical results.
arxiv-8700-249 | Statistical consistency and asymptotic normality for high-dimensional robust M-estimators | http://arxiv.org/pdf/1501.00312v1.pdf | author:Po-Ling Loh category:math.ST cs.IT math.IT stat.ML stat.TH 62F12 published:2015-01-01 summary:We study theoretical properties of regularized robust M-estimators,applicable when data are drawn from a sparse high-dimensional linear model andcontaminated by heavy-tailed distributions and/or outliers in the additiveerrors and covariates. We first establish a form of local statisticalconsistency for the penalized regression estimators under fairly mildconditions on the error distribution: When the derivative of the loss functionis bounded and satisfies a local restricted curvature condition, all stationarypoints within a constant radius of the true regression vector converge at theminimax rate enjoyed by the Lasso with sub-Gaussian errors. When an appropriatenonconvex regularizer is used in place of an l_1-penalty, we show that suchstationary points are in fact unique and equal to the local oracle solutionwith the correct support---hence, results on asymptotic normality in thelow-dimensional case carry over immediately to the high-dimensional setting.This has important implications for the efficiency of regularized nonconvexM-estimators when the errors are heavy-tailed. Our analysis of the localcurvature of the loss function also has useful consequences for optimizationwhen the robust regression function and/or regularizer is nonconvex and theobjective function possesses stationary points outside the local region. Weshow that as long as a composite gradient descent algorithm is initializedwithin a constant radius of the true regression vector, successive iterateswill converge at a linear rate to a stationary point within the local region.Furthermore, the global optimum of a convex regularized robust regressionfunction may be used to obtain a suitable initialization. The result is a noveltwo-step procedure that uses a convex M-estimator to achieve consistency and anonconvex M-estimator to increase efficiency.
arxiv-8700-250 | QANUS: An Open-source Question-Answering Platform | http://arxiv.org/pdf/1501.00311v1.pdf | author:Jun-Ping Ng, Min-Yen Kan category:cs.IR cs.CL published:2015-01-01 summary:In this paper, we motivate the need for a publicly available, genericsoftware framework for question-answering (QA) systems. We present anopen-source QA framework QANUS which researchers can leverage on to build newQA systems easily and rapidly. The framework implements much of the code thatwill otherwise have been repeated across different QA systems. To demonstratethe utility and practicality of the framework, we further present a fullyfunctioning factoid QA system QA-SYS built on top of QANUS.
arxiv-8700-251 | Sequence Modeling using Gated Recurrent Neural Networks | http://arxiv.org/pdf/1501.00299v1.pdf | author:Mohammad Pezeshki category:cs.NE cs.LG published:2015-01-01 summary:In this paper, we have used Recurrent Neural Networks to capture and modelhuman motion data and generate motions by prediction of the next immediate datapoint at each time-step. Our RNN is armed with recently proposed GatedRecurrent Units which has shown promising results in some sequence modelingproblems such as Machine Translation and Speech Synthesis. We demonstrate thatthis model is able to capture long-term dependencies in data and generaterealistic motions.
arxiv-8700-252 | Consistent Classification Algorithms for Multi-class Non-Decomposable Performance Metrics | http://arxiv.org/pdf/1501.00287v1.pdf | author:Harish G. Ramaswamy, Harikrishna Narasimhan, Shivani Agarwal category:cs.LG stat.ML published:2015-01-01 summary:We study consistency of learning algorithms for a multi-class performancemetric that is a non-decomposable function of the confusion matrix of aclassifier and cannot be expressed as a sum of losses on individual datapoints; examples of such performance metrics include the macro F-measurepopular in information retrieval and the G-mean metric used in class-imbalancedproblems. While there has been much work in recent years in understanding theconsistency properties of learning algorithms for `binary' non-decomposablemetrics, little is known either about the form of the optimal classifier for ageneral multi-class non-decomposable metric, or about how these learningalgorithms generalize to the multi-class case. In this paper, we provide aunified framework for analysing a multi-class non-decomposable performancemetric, where the problem of finding the optimal classifier for the performancemetric is viewed as an optimization problem over the space of all confusionmatrices achievable under the given distribution. Using this framework, we showthat (under a continuous distribution) the optimal classifier for a multi-classperformance metric can be obtained as the solution of a cost-sensitiveclassification problem, thus generalizing several previous results on specificbinary non-decomposable metrics. We then design a consistent learning algorithmfor concave multi-class performance metrics that proceeds via a sequence ofcost-sensitive classification problems, and can be seen as applying theconditional gradient (CG) optimization method over the space of feasibleconfusion matrices. To our knowledge, this is the first efficient learningalgorithm (whose running time is polynomial in the number of classes) that isconsistent for a large family of multi-class non-decomposable metrics. Ourconsistency proof uses a novel technique based on the convergence analysis ofthe CG method.
arxiv-8700-253 | Communication-Efficient Distributed Optimization of Self-Concordant Empirical Loss | http://arxiv.org/pdf/1501.00263v1.pdf | author:Yuchen Zhang, Lin Xiao category:math.OC cs.LG stat.ML published:2015-01-01 summary:We consider distributed convex optimization problems originated from sampleaverage approximation of stochastic optimization, or empirical riskminimization in machine learning. We assume that each machine in thedistributed computing system has access to a local empirical loss function,constructed with i.i.d. data sampled from a common distribution. We propose acommunication-efficient distributed algorithm to minimize the overall empiricalloss, which is the average of the local empirical losses. The algorithm isbased on an inexact damped Newton method, where the inexact Newton steps arecomputed by a distributed preconditioned conjugate gradient method. We analyzeits iteration complexity and communication efficiency for minimizingself-concordant empirical loss functions, and discuss the results fordistributed ridge regression, logistic regression and binary classificationwith a smoothed hinge loss. In a standard setting for supervised learning, therequired number of communication rounds of the algorithm does not increase withthe sample size, and only grows slowly with the number of machines.
arxiv-8700-254 | Beta-Negative Binomial Process and Exchangeable Random Partitions for Mixed-Membership Modeling | http://arxiv.org/pdf/1410.7812v2.pdf | author:Mingyuan Zhou category:stat.ME stat.ML published:2014-10-28 summary:The beta-negative binomial process (BNBP), an integer-valued stochasticprocess, is employed to partition a count vector into a latent random countmatrix. As the marginal probability distribution of the BNBP that governs theexchangeable random partitions of grouped data has not yet been developed,current inference for the BNBP has to truncate the number of atoms of the betaprocess. This paper introduces an exchangeable partition probability functionto explicitly describe how the BNBP clusters the data points of each group intoa random number of exchangeable partitions, which are shared across all thegroups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to anovel nonparametric Bayesian topic model that is distinct from existing ones,with simple implementation, fast convergence, good mixing, and state-of-the-artpredictive performance.
arxiv-8700-255 | The continuum-of-urns scheme, generalized beta and Indian buffet processes, and hierarchies thereof | http://arxiv.org/pdf/1501.00208v1.pdf | author:Daniel M. Roy category:math.PR math.ST stat.ML stat.TH published:2014-12-31 summary:We describe the combinatorial stochastic process underlying a sequence ofconditionally independent Bernoulli processes with a shared beta process hazardmeasure. As shown by Thibaux and Jordan [TJ07], in the special case when theunderlying beta process has a constant concentration function and a finite andnonatomic mean, the combinatorial structure is that of the Indian buffetprocess (IBP) introduced by Griffiths and Ghahramani [GG05]. By reinterpretingthe beta process introduced by Hjort [Hjo90] as a measurable family ofDirichlet processes, we obtain a simple predictive rule for the general case,which can be thought of as a continuum of Blackwell-MacQueen urn schemes (orequivalently, one-parameter Hoppe urn schemes). The corresponding measurablefamily of Perman-Pitman-Yor processes leads to a continuum of two-parameterHoppe urn schemes, whose ordinary component is the three-parameter IBPintroduced by Teh and G\"or\"ur [TG09], which exhibits power-law behavior, asfurther studied by Broderick, Jordan, and Pitman [BJP12]. The idea extends toarbitrary measurable families of exchangeable partition probability functionsand gives rise to generalizations of the beta process with matching buffetprocesses. Finally, in the same way that hierarchies of Dirichlet processeswere given Chinese restaurant franchise representations by Teh, Jordan, Beal,and Blei [Teh+06], one can construct representations of sequences of Bernoulliprocesses directed by hierarchies of beta processes (and their generalizations)using the stochastic process we uncover.
arxiv-8700-256 | ACCAMS: Additive Co-Clustering to Approximate Matrices Succinctly | http://arxiv.org/pdf/1501.00199v1.pdf | author:Alex Beutel, Amr Ahmed, Alexander J. Smola category:cs.LG stat.ML published:2014-12-31 summary:Matrix completion and approximation are popular tools to capture a user'spreferences for recommendation and to approximate missing data. Instead ofusing low-rank factorization we take a drastically different approach, based onthe simple insight that an additive model of co-clusterings allows one toapproximate matrices efficiently. This allows us to build a concise model that,per bit of model learned, significantly beats all factorization approaches tomatrix approximation. Even more surprisingly, we find that summing over smallco-clusterings is more effective in modeling matrices than classicco-clustering, which uses just one large partitioning of the matrix. Following Occam's razor principle suggests that the simple structure inducedby our model better captures the latent preferences and decision makingprocesses present in the real world than classic co-clustering or matrixfactorization. We provide an iterative minimization algorithm, a collapsedGibbs sampler, theoretical guarantees for matrix approximation, and excellentempirical evidence for the efficacy of our approach. We achievestate-of-the-art results on the Netflix problem with a fraction of the modelcomplexity.
arxiv-8700-257 | Maximum Margin Clustering for State Decomposition of Metastable Systems | http://arxiv.org/pdf/1501.00125v1.pdf | author:Hao Wu category:cs.LG cs.NA cs.SY math.NA published:2014-12-31 summary:When studying a metastable dynamical system, a prime concern is how todecompose the phase space into a set of metastable states. Unfortunately, themetastable state decomposition based on simulation or experimental data isstill a challenge. The most popular and simplest approach is geometricclustering which is developed based on the classical clustering technique.However, the prerequisites of this approach are: (1) data are obtained fromsimulations or experiments which are in global equilibrium and (2) thecoordinate system is appropriately selected. Recently, the kinetic clusteringapproach based on phase space discretization and transition probabilityestimation has drawn much attention due to its applicability to more generalcases, but the choice of discretization policy is a difficult task. In thispaper, a new decomposition method designated as maximum margin metastableclustering is proposed, which converts the problem of metastable statedecomposition to a semi-supervised learning problem so that the large margintechnique can be utilized to search for the optimal decomposition without phasespace discretization. Moreover, several simulation examples are given toillustrate the effectiveness of the proposed method.
arxiv-8700-258 | HSI based colour image equalization using iterative nth root and nth power | http://arxiv.org/pdf/1501.00108v1.pdf | author:Gholamreza Anbarjafari category:cs.CV cs.GR published:2014-12-31 summary:In this paper an equalization technique for colour images is introduced. Themethod is based on nth root and nth power equalization approach but withoptimization of the mean of the image in different colour channels such as RGBand HSI. The performance of the proposed method has been measured by the meansof peak signal to noise ratio. The proposed algorithm has been compared withconventional histogram equalization and the visual and quantitativeexperimental results are showing that the proposed method over perform thehistogram equalization.
arxiv-8700-259 | Face recognition using color local binary pattern from mutually independent color channels | http://arxiv.org/pdf/1501.00105v1.pdf | author:Gholamreza Anbarjafari category:cs.CV published:2014-12-31 summary:In this paper, a high performance face recognition system based on localbinary pattern (LBP) using the probability distribution functions (PDF) ofpixels in different mutually independent color channels which are robust tofrontal homogenous illumination and planer rotation is proposed. Theillumination of faces is enhanced by using the state-of-the-art technique whichis using discrete wavelet transform (DWT) and singular value decomposition(SVD). After equalization, face images are segmented by use of local SuccessiveMean Quantization Transform (SMQT) followed by skin color based face detectionsystem. Kullback-Leibler Distance (KLD) between the concatenated PDFs of agiven face obtained by LBP and the concatenated PDFs of each face in thedatabase is used as a metric in the recognition process. Various decisionfusion techniques have been used in order to improve the recognition rate. Theproposed system has been tested on the FERET, HP, and Bosphorus face databases.The proposed system is compared with conventional and thestate-of-the-arttechniques. The recognition rates obtained using FVF approach for FERETdatabase is 99.78% compared with 79.60% and 68.80% for conventional gray scaleLBP and Principle Component Analysis (PCA) based face recognition techniquesrespectively.
arxiv-8700-260 | Detailed Derivations of Small-Variance Asymptotics for some Hierarchical Bayesian Nonparametric Models | http://arxiv.org/pdf/1501.00052v1.pdf | author:Jonathan H. Huggins, Ardavan Saeedi, Matthew J. Johnson category:stat.ML cs.LG published:2014-12-31 summary:In this note we provide detailed derivations of two versions ofsmall-variance asymptotics for hierarchical Dirichlet process (HDP) mixturemodels and the HDP hidden Markov model (HDP-HMM, a.k.a. the infinite HMM). Weinclude derivations for the probabilities of certain CRP and CRF partitions,which are of more general interest.
arxiv-8700-261 | Discriminative Clustering with Relative Constraints | http://arxiv.org/pdf/1501.00037v1.pdf | author:Yuanli Pei, Xiaoli Z. Fern, Rómer Rosales, Teresa Vania Tjahja category:cs.LG published:2014-12-30 summary:We study the problem of clustering with relative constraints, where eachconstraint specifies relative similarities among instances. In particular, eachconstraint $(x_i, x_j, x_k)$ is acquired by posing a query: is instance $x_i$more similar to $x_j$ than to $x_k$? We consider the scenario where answers tosuch queries are based on an underlying (but unknown) class concept, which weaim to discover via clustering. Different from most existing methods that onlyconsider constraints derived from yes and no answers, we also incorporate don'tknow responses. We introduce a Discriminative Clustering method with RelativeConstraints (DCRC) which assumes a natural probabilistic relationship betweeninstances, their underlying cluster memberships, and the observed constraints.The objective is to maximize the model likelihood given the constraints, and inthe meantime enforce cluster separation and cluster balance by also making useof the unlabeled instances. We evaluated the proposed method using constraintsgenerated from ground-truth class labels, and from (noisy) human judgments froma user study. Experimental results demonstrate: 1) the usefulness of relativeconstraints, in particular when don't know answers are considered; 2) theimproved performance of the proposed method over state-of-the-art methods thatutilize either relative or pairwise constraints; and 3) the robustness of ourmethod in the presence of noisy constraints, such as those provided by humanjudgement.
arxiv-8700-262 | Breaking the Curse of Dimensionality with Convex Neural Networks | http://arxiv.org/pdf/1412.8690v1.pdf | author:Francis Bach category:cs.LG math.OC math.ST stat.TH published:2014-12-30 summary:We consider neural networks with a single hidden layer and non-decreasinghomogeneous activa-tion functions like the rectified linear units. By lettingthe number of hidden units grow unbounded and using classical non-Euclideanregularization tools on the output weights, we provide a detailed theoreticalanalysis of their generalization performance, with a study of both theapproximation and the estimation errors. We show in particular that they areadaptive to unknown underlying linear structures, such as the dependence on theprojection of the input variables onto a low-dimensional subspace. Moreover,when using sparsity-inducing norms on the input weights, we show thathigh-dimensional non-linear variable selection may be achieved, without anystrong assumption regarding the data and with a total number of variablespotentially exponential in the number of ob-servations. In addition, we providea simple geometric interpretation to the non-convex problem of addition of anew unit, which is the core potentially hard computational element in theframework of learning from continuously many basis functions. We provide simpleconditions for convex relaxations to achieve the same generalization errorbounds, even when constant-factor approxi-mations cannot be found (e.g.,because it is NP-hard such as for the zero-homogeneous activation function). Wewere not able to find strong enough convex relaxations and leave open theexistence or non-existence of polynomial-time algorithms.
arxiv-8700-263 | Large scale canonical correlation analysis with iterative least squares | http://arxiv.org/pdf/1407.4508v2.pdf | author:Yichao Lu, Dean P. Foster category:stat.ML published:2014-07-16 summary:Canonical Correlation Analysis (CCA) is a widely used statistical tool withboth well established theory and favorable performance for a wide range ofmachine learning problems. However, computing CCA for huge datasets can be veryslow since it involves implementing QR decomposition or singular valuedecomposition of huge matrices. In this paper we introduce L-CCA, a iterativealgorithm which can compute CCA fast on huge sparse datasets. Theory on boththe asymptotic convergence and finite time accuracy of L-CCA are established.The experiments also show that L-CCA outperform other fast CCA approximationschemes on two real datasets.
arxiv-8700-264 | A multistep segmentation algorithm for vessel extraction in medical imaging | http://arxiv.org/pdf/1412.8656v1.pdf | author:Nasser Aghazadeh, Ladan Sharafyan Cigaroudy category:cs.CV math.NA published:2014-12-30 summary:The main contribution of this paper is to propose an iterative procedure fortubular structure segmentation of 2D images, which combines tight frame ofCurvelet transforms with a SURE technique thresholding which is based onprinciple obtained by minimizing Stein Unbiased Risk Estimate for denoising.This proposed algorithm is mainly based on the TFA proposal presented in [1,9], which we use eigenvectors of Hessian matrix of image for improving thisiterative part in segmenting unclear and narrow vessels and filling the gapbetween separate pieces of detected vessels. The experimental results arepresented to demonstrate the effectiveness of the proposed model.
arxiv-8700-265 | Modular proximal optimization for multidimensional total-variation regularization | http://arxiv.org/pdf/1411.0589v2.pdf | author:Álvaro Barbero, Suvrit Sra category:stat.ML math.OC published:2014-11-03 summary:One of the most frequently used notions of "structured sparsity" is that ofsparse (discrete) gradients, a structure typically elicited through\emph{Total-Variation (TV)} regularizers. This paper focuses on anisotropicTV-regularizers, in particular on $\ell_p$-norm \emph{weighted TV regularizers}for which it develops efficient algorithms to compute the correspondingproximity operators. Our algorithms enable one to scalably incorporate TVregularization of vector, matrix, or tensor data into a proximal convexoptimization solvers. For the special case of vectors, we derive and implementa highly efficient weighted 1D-TV solver. This solver provides a backbone forsubsequently handling the more complex task of higher-dimensional (two or more)TV by means of a modular proximal optimization approach. We present numericalexperiments that demonstrate how our 1D-TV solver matches or exceeds the bestknown 1D-TV solvers. Thereafter, we illustrate the benefits of our modulardesign through extensive experiments on: (i) image denoising; (ii) imagedeconvolution; and (iii) four variants of fused-lasso. Our results show theflexibility and speed our TV solvers offer over competing approaches. Tounderscore our claims, we provide our TV solvers in an easy to usemulti-threaded C++ library (which also aids reproducibility of our results).
arxiv-8700-266 | Holistic random encoding for imaging through multimode fibers | http://arxiv.org/pdf/1501.03997v1.pdf | author:Hwanchol Jang, Changhyeong Yoon, Euiheon Chung, Wonshik Choi, Heung-No Lee category:physics.optics cs.CV published:2014-12-30 summary:The input numerical aperture (NA) of multimode fiber (MMF) can be effectivelyincreased by placing turbid media at the input end of the MMF. This providesthe potential for high-resolution imaging through the MMF. While the input NAis increased, the number of propagation modes in the MMF and hence the outputNA remains the same. This makes the image reconstruction processunderdetermined and may limit the quality of the image reconstruction. In thispaper, we aim to improve the signal to noise ratio (SNR) of the imagereconstruction in imaging through MMF. We notice that turbid media placed inthe input of the MMF transforms the incoming waves into a better format forinformation transmission and information extraction. We call thistransformation as holistic random (HR) encoding of turbid media. By exploitingthe HR encoding, we make a considerable improvement on the SNR of the imagereconstruction. For efficient utilization of the HR encoding, we employ sparserepresentation (SR), a relatively new signal reconstruction framework when itis provided with a HR encoded signal. This study shows for the first time toour knowledge the benefit of utilizing the HR encoding of turbid media forrecovery in the optically underdetermined systems where the output NA of it issmaller than the input NA for imaging through MMF.
arxiv-8700-267 | Consistency of spectral clustering in stochastic block models | http://arxiv.org/pdf/1312.2050v3.pdf | author:Jing Lei, Alessandro Rinaldo category:math.ST stat.ML stat.TH published:2013-12-07 summary:We analyze the performance of spectral clustering for community extraction instochastic block models. We show that, under mild conditions, spectralclustering applied to the adjacency matrix of the network can consistentlyrecover hidden communities even when the order of the maximum expected degreeis as small as $\log n$, with $n$ the number of nodes. This result applies tosome popular polynomial time spectral clustering algorithms and is furtherextended to degree corrected stochastic block models using a spherical$k$-median spectral clustering method. A key component of our analysis is acombinatorial bound on the spectrum of binary random matrices, which is sharperthan the conventional matrix Bernstein inequality and may be of independentinterest.
arxiv-8700-268 | A Bayesian encourages dropout | http://arxiv.org/pdf/1412.7003v3.pdf | author:Shin-ichi Maeda category:cs.LG cs.NE stat.ML published:2014-12-22 summary:Dropout is one of the key techniques to prevent the learning fromoverfitting. It is explained that dropout works as a kind of modified L2regularization. Here, we shed light on the dropout from Bayesian standpoint.Bayesian interpretation enables us to optimize the dropout rate, which isbeneficial for learning of weight parameters and prediction after learning. Theexperiment result also encourages the optimization of the dropout.
arxiv-8700-269 | Accurate and Conservative Estimates of MRF Log-likelihood using Reverse Annealing | http://arxiv.org/pdf/1412.8566v1.pdf | author:Yuri Burda, Roger B. Grosse, Ruslan Salakhutdinov category:cs.LG stat.ML published:2014-12-30 summary:Markov random fields (MRFs) are difficult to evaluate as generative modelsbecause computing the test log-probabilities requires the intractable partitionfunction. Annealed importance sampling (AIS) is widely used to estimate MRFpartition functions, and often yields quite accurate results. However, AIS isprone to overestimate the log-likelihood with little indication that anythingis wrong. We present the Reverse AIS Estimator (RAISE), a stochastic lowerbound on the log-likelihood of an approximation to the original MRF model.RAISE requires only the same MCMC transition operators as standard AIS.Experimental results indicate that RAISE agrees closely with AISlog-probability estimates for RBMs, DBMs, and DBNs, but typically errs on theside of underestimating, rather than overestimating, the log-likelihood.
arxiv-8700-270 | A Study of Entanglement in a Categorical Framework of Natural Language | http://arxiv.org/pdf/1405.2874v2.pdf | author:Dimitri Kartsaklis, Mehrnoosh Sadrzadeh category:cs.CL cs.AI math.CT quant-ph published:2014-05-12 summary:In both quantum mechanics and corpus linguistics based on vector spaces, thenotion of entanglement provides a means for the various subsystems tocommunicate with each other. In this paper we examine a number ofimplementations of the categorical framework of Coecke, Sadrzadeh and Clark(2010) for natural language, from an entanglement perspective. Specifically,our goal is to better understand in what way the level of entanglement of therelational tensors (or the lack of it) affects the compositional structures inpractical situations. Our findings reveal that a number of proposals for verbconstruction lead to almost separable tensors, a fact that considerablysimplifies the interactions between the words. We examine the ramifications ofthis fact, and we show that the use of Frobenius algebras mitigates thepotential problems to a great extent. Finally, we briefly examine a machinelearning method that creates verb tensors exhibiting a sufficient level ofentanglement.
arxiv-8700-271 | Complexity of Grammar Induction for Quantum Types | http://arxiv.org/pdf/1404.3925v2.pdf | author:Antonin Delpeuch category:cs.CL math.CT published:2014-04-13 summary:Most categorical models of meaning use a functor from the syntactic categoryto the semantic category. When semantic information is available, the problemof grammar induction can therefore be defined as finding preimages of thesemantic types under this forgetful functor, lifting the information flow fromthe semantic level to a valid reduction at the syntactic level. We study thecomplexity of grammar induction, and show that for a variety of type systems,including pivotal and compact closed categories, the grammar induction problemis NP-complete. Our approach could be extended to linguistic type systems suchas autonomous or bi-closed categories.
arxiv-8700-272 | Disjunctive Normal Networks | http://arxiv.org/pdf/1412.8534v1.pdf | author:Mehdi Sajjadi, Mojtaba Seyedhosseini, Tolga Tasdizen category:cs.LG cs.NE published:2014-12-30 summary:Artificial neural networks are powerful pattern classifiers; however, theyhave been surpassed in accuracy by methods such as support vector machines andrandom forests that are also easier to use and faster to train.Backpropagation, which is used to train artificial neural networks, suffersfrom the herd effect problem which leads to long training times and limitclassification accuracy. We use the disjunctive normal form and approximate theboolean conjunction operations with products to construct a novel networkarchitecture. The proposed model can be trained by minimizing an error functionand it allows an effective and intuitive initialization which solves theherd-effect problem associated with backpropagation. This leads to state-of-theart classification accuracy and fast training times. In addition, our model canbe jointly optimized with convolutional features in an unified structureleading to state-of-the-art results on computer vision problems with fastconvergence rates. A GPU implementation of LDNN with optional convolutionalfeatures is also available
arxiv-8700-273 | From Logical to Distributional Models | http://arxiv.org/pdf/1412.8527v1.pdf | author:Anne Preller category:cs.LO cs.CL quant-ph F.4.0; F.4.1 published:2014-12-30 summary:The paper relates two variants of semantic models for natural language,logical functional models and compositional distributional vector space models,by transferring the logic and reasoning from the logical to the distributionalmodels. The geometrical operations of quantum logic are reformulated as algebraicoperations on vectors. A map from functional models to vector space modelsmakes it possible to compare the meaning of sentences word by word.
arxiv-8700-274 | Probing the topological properties of complex networks modeling short written texts | http://arxiv.org/pdf/1412.8504v1.pdf | author:Diego R. Amancio category:cs.CL physics.soc-ph published:2014-12-29 summary:In recent years, graph theory has been widely employed to probe severallanguage properties. More specifically, the so-called word adjacency model hasbeen proven useful for tackling several practical problems, especially thoserelying on textual stylistic analysis. The most common approach to treat textsas networks has simply considered either large pieces of texts or entire books.This approach has certainly worked well -- many informative discoveries havebeen made this way -- but it raises an uncomfortable question: could there beimportant topological patterns in small pieces of texts? To address thisproblem, the topological properties of subtexts sampled from entire books wasprobed. Statistical analyzes performed on a dataset comprising 50 novelsrevealed that most of the traditional topological measurements are stable forshort subtexts. When the performance of the authorship recognition task wasanalyzed, it was found that a proper sampling yields a discriminability similarto the one found with full texts. Surprisingly, the support vector machineclassification based on the characterization of short texts outperformed theone performed with entire books. These findings suggest that a localtopological analysis of large documents might improve its globalcharacterization. Most importantly, it was verified, as a proof of principle,that short texts can be analyzed with the methods and concepts of complexnetworks. As a consequence, the techniques described here can be extended in astraightforward fashion to analyze texts as time-varying complex networks.
arxiv-8700-275 | Accurate Localization in Dense Urban Area Using Google Street View Image | http://arxiv.org/pdf/1412.8496v1.pdf | author:Mahdi Salarian category:cs.CV published:2014-12-29 summary:Accurate information about the location and orientation of a camera in mobiledevices is central to the utilization of location-based services (LBS). Most ofsuch mobile devices rely on GPS data but this data is subject to inaccuracy dueto imperfections in the quality of the signal provided by satellites. Thisshortcoming has spurred the research into improving the accuracy oflocalization. Since mobile devices have camera, a major thrust of this researchhas been seeks to acquire the local scene and apply image retrieval techniquesby querying a GPS-tagged image database to find the best match for the acquiredscene.. The techniques are however computationally demanding and unsuitable forreal-time applications such as assistive technology for navigation by the blindand visually impaired which motivated out work. To overcome the high complexityof those techniques, we investigated the use of inertial sensors as an aid inimage-retrieval-based approach. Armed with information of media other thanimages, such as data from the GPS module along with orientation sensors such asaccelerometer and gyro, we sought to limit the size of the image set to csearch for the best match. Specifically, data from the orientation sensorsalong with Dilution of precision (DOP) from GPS are used to find the angle ofview and estimation of position. We present analysis of the reduction in theimage set size for the search as well as simulations to demonstrate theeffectiveness in a fast implementation with 98% Estimated Position Error.
arxiv-8700-276 | An ADMM algorithm for solving a proximal bound-constrained quadratic program | http://arxiv.org/pdf/1412.8493v1.pdf | author:Miguel Á. Carreira-Perpiñán category:math.OC cs.LG stat.ML published:2014-12-29 summary:We consider a proximal operator given by a quadratic function subject tobound constraints and give an optimization algorithm using the alternatingdirection method of multipliers (ADMM). The algorithm is particularly efficientto solve a collection of proximal operators that share the same quadratic form,or if the quadratic program is the relaxation of a binary quadratic problem.
arxiv-8700-277 | Spectral classification using convolutional neural networks | http://arxiv.org/pdf/1412.8341v1.pdf | author:Pavel Hála category:cs.CV astro-ph.IM cs.NE published:2014-12-29 summary:There is a great need for accurate and autonomous spectral classificationmethods in astrophysics. This thesis is about training a convolutional neuralnetwork (ConvNet) to recognize an object class (quasar, star or galaxy) fromone-dimension spectra only. Author developed several scripts and C programs fordatasets preparation, preprocessing and postprocessing of the data. EBLearnlibrary (developed by Pierre Sermanet and Yann LeCun) was used to createConvNets. Application on dataset of more than 60000 spectra yielded successrate of nearly 95%. This thesis conclusively proved great potential ofconvolutional neural networks and deep learning methods in astrophysics.
arxiv-8700-278 | Globally Optimal Joint Image Segmentation and Shape Matching Based on Wasserstein Modes | http://arxiv.org/pdf/1407.3956v2.pdf | author:Bernhard Schmitzer, Christoph Schnörr category:cs.CV 49Q10, 62H35 published:2014-07-15 summary:A functional for joint variational object segmentation and shape matching isdeveloped. The formulation is based on optimal transport w.r.t. geometricdistance and local feature similarity. Geometric invariance and modelling ofobject-typical statistical variations is achieved by introducing degrees offreedom that describe transformations and deformations of the shape template.The shape model is mathematically equivalent to contour-based approaches butinference can be performed without conversion between the contour and regionrepresentations, allowing combination with other convex segmentation approachesand simplifying optimization. While the overall functional is non-convex,non-convexity is confined to a low-dimensional variable. We propose a locallyoptimal alternating optimization scheme and a globally optimal branch and boundscheme, based on adaptive convex relaxation. Combining both methods allows toeliminate the delicate initialization problem inherent to many contour basedapproaches while remaining computationally practical. The properties of thefunctional, its ability to adapt to a wide range of input data structures andthe different optimization schemes are illustrated and compared by numericalexperiments.
arxiv-8700-279 | Improving approximate RPCA with a k-sparsity prior | http://arxiv.org/pdf/1412.8291v1.pdf | author:Maximilian Karl, Christian Osendorfer category:cs.NE cs.LG published:2014-12-29 summary:A process centric view of robust PCA (RPCA) allows its fast approximateimplementation based on a special form o a deep neural network with weightsshared across all layers. However, empirically this fast approximation to RPCAfails to find representations that are parsemonious. We resolve these bad localminima by relaxing the elementwise L1 and L2 priors and instead utilize astructure inducing k-sparsity prior. In a discriminative classification taskthe newly learned representations outperform these from the originalapproximate RPCA formulation significantly.
arxiv-8700-280 | Rigid and Non-rigid Shape Evolutions for Shape Alignment and Recovery in Images | http://arxiv.org/pdf/1412.8287v1.pdf | author:Junyan Wang, Kap-Luk Chan category:cs.CV published:2014-12-29 summary:The same type of objects in different images may vary in their shapes becauseof rigid and non-rigid shape deformations, occluding foreground as well ascluttered background. The problem concerned in this work is the shapeextraction in such challenging situations. We approach the shape extractionthrough shape alignment and recovery. This paper presents a novel and generalmethod for shape alignment and recovery by using one example shapes based ondeterministic energy minimization. Our idea is to use general model of shapedeformation in minimizing active contour energies. Given \emph{a priori} formof the shape deformation, we show how the curve evolution equationcorresponding to the shape deformation can be derived. The curve evolution iscalled the prior variation shape evolution (PVSE). We also derive theenergy-minimizing PVSE for minimizing active contour energies. For shaperecovery, we propose to use the PVSE that deforms the shape while preservingits shape characteristics. For choosing such shape-preserving PVSE, a theory ofshape preservability of the PVSE is established. Experimental results validatethe theory and the formulations, and they demonstrate the effectiveness of ourmethod.
arxiv-8700-281 | Improving Persian Document Classification Using Semantic Relations between Words | http://arxiv.org/pdf/1412.8147v1.pdf | author:Saeed Parseh, Ahmad Baraani category:cs.IR cs.LG published:2014-12-28 summary:With the increase of information, document classification as one of themethods of text mining, plays vital role in many management and organizinginformation. Document classification is the process of assigning a document toone or more predefined category labels. Document classification includesdifferent parts such as text processing, term selection, term weighting andfinal classification. The accuracy of document classification is veryimportant. Thus improvement in each part of classification should lead tobetter results and higher precision. Term weighting has a great impact on theaccuracy of the classification. Most of the existing weighting methods exploitthe statistical information of terms in documents and do not consider semanticrelations between words. In this paper, an automated document classificationsystem is presented that uses a novel term weighting method based on semanticrelations between terms. To evaluate the proposed method, three standardPersian corpuses are used. Experiment results show 2 to 4 percent improvementin classification accuracy compared with the best previous designed system forPersian documents.
arxiv-8700-282 | Proceedings of the 11th workshop on Quantum Physics and Logic | http://arxiv.org/pdf/1412.8102v1.pdf | author:Bob Coecke, Ichiro Hasuo, Prakash Panangaden category:cs.LO cs.CL cs.PL quant-ph published:2014-12-28 summary:This volume contains the proceedings of the 11th International Workshop onQuantum Physics and Logic (QPL 2014), which was held from the 4th to the 6th ofJune, 2014, at Kyoto University, Japan. The goal of the QPL workshop series is to bring together researchers workingon mathematical foundations of quantum physics, quantum computing andspatio-temporal causal structures, and in particular those that use logicaltools, ordered algebraic and category-theoretic structures, formal languages,semantic methods and other computer science methods for the study of physicalbehavior in general. Over the past few years, there has been growing activityin these foundational approaches, together with a renewed interest in thefoundations of quantum theory, which complement the more mainstream research inquantum computation. Earlier workshops in this series, with the same acronymunder the name "Quantum Programming Languages", were held in Ottawa (2003),Turku (2004), Chicago (2005), and Oxford (2006). The first QPL under the newname Quantum Physics and Logic was held in Reykjavik (2008), followed by Oxford(2009 and 2010), Nijmegen (2011), Brussels (2012) and Barcelona (2013).
arxiv-8700-283 | Persian Sentiment Analyzer: A Framework based on a Novel Feature Selection Method | http://arxiv.org/pdf/1412.8079v1.pdf | author:Ayoub Bagheri, Mohamad Saraee category:cs.CL cs.IR published:2014-12-27 summary:In the recent decade, with the enormous growth of digital content in internetand databases, sentiment analysis has received more and more attention betweeninformation retrieval and natural language processing researchers. Sentimentanalysis aims to use automated tools to detect subjective information fromreviews. One of the main challenges in sentiment analysis is feature selection.Feature selection is widely used as the first stage of analysis andclassification tasks to reduce the dimension of problem, and improve speed bythe elimination of irrelevant and redundant features. Up to now as there arefew researches conducted on feature selection in sentiment analysis, there arevery rare works for Persian sentiment analysis. This paper considers theproblem of sentiment classification using different feature selection methodsfor online customer reviews in Persian language. Three of the challenges ofPersian text are using of a wide variety of declensional suffixes, differentword spacing and many informal or colloquial words. In this paper we studythese challenges by proposing a model for sentiment classification of Persianreview documents. The proposed model is based on lemmatization and featureselection and is employed Naive Bayes algorithm for classification. We evaluatethe performance of the model on a manually gathered collection of cellphonereviews, where the results show the effectiveness of the proposed approaches.
arxiv-8700-284 | Functional correspondence by matrix completion | http://arxiv.org/pdf/1412.8070v1.pdf | author:Artiom Kovnatsky, Michael M. Bronstein, Xavier Bresson, Pierre Vandergheynst category:cs.CV published:2014-12-27 summary:In this paper, we consider the problem of finding dense intrinsiccorrespondence between manifolds using the recently introduced functionalframework. We pose the functional correspondence problem as matrix completionwith manifold geometric structure and inducing functional localization with the$L_1$ norm. We discuss efficient numerical procedures for the solution of ourproblem. Our method compares favorably to the accuracy of state-of-the-artcorrespondence algorithms on non-rigid shape matching benchmarks, and isespecially advantageous in settings when only scarce data is available.
arxiv-8700-285 | Scalable detection of statistically significant communities and hierarchies, using message-passing for modularity | http://arxiv.org/pdf/1403.5787v3.pdf | author:Pan Zhang, Cristopher Moore category:physics.soc-ph cs.SI stat.ML published:2014-03-23 summary:Modularity is a popular measure of community structure. However, maximizingthe modularity can lead to many competing partitions, with almost the samemodularity, that are poorly correlated with each other. It can also produceillusory "communities" in random graphs where none exist. We address thisproblem by using the modularity as a Hamiltonian at finite temperature, andusing an efficient Belief Propagation algorithm to obtain the consensus of manypartitions with high modularity, rather than looking for a single partitionthat maximizes it. We show analytically and numerically that the proposedalgorithm works all the way down to the detectability transition in networksgenerated by the stochastic block model. It also performs well on real-worldnetworks, revealing large communities in some networks where previous work hasclaimed no communities exist. Finally we show that by applying our algorithmrecursively, subdividing communities until no statistically-significantsubcommunities can be found, we can detect hierarchical structure in real-worldnetworks more efficiently than previous methods.
arxiv-8700-286 | Learning of Proto-object Representations via Fixations on Low Resolution | http://arxiv.org/pdf/1412.7242v2.pdf | author:Chengyao Shen, Xun Huang, Qi Zhao category:cs.CV published:2014-12-23 summary:While previous researches in eye fixation prediction typically rely onintegrating low-level features (e.g. color, edge) to form a saliency map,recently it has been found that the structural organization of these featuresinto a proto-object representation can play a more significant role. In thiswork, we present a computational framework based on deep network to demonstratethat proto-object representations can be learned from low-resolution imagepatches from fixation regions. We advocate the use of low-resolution inputs inthis work due to the following reasons: (1) Proto-objects are computed inparallel over an entire visual field (2) People can perceive or recognizeobjects well even it is in low resolution. (3) Fixations from lower resolutionimages can predict fixations on higher resolution images. In the proposedcomputational model, we extract multi-scale image patches on fixation regionsfrom eye fixation datasets, resize them to low resolution and feed them into ahierarchical. With layer-wise unsupervised feature learning, we find that manyproto-objects like features responsive to different shapes of object blobs arelearned out. Visualizations also show that these features are selective topotential objects in the scene and the responses of these features work well inpredicting eye fixations on the images when combined with learned weights.
arxiv-8700-287 | Construction of Vietnamese SentiWordNet by using Vietnamese Dictionary | http://arxiv.org/pdf/1412.8010v1.pdf | author:Xuan-Son Vu, Seong-Bae Park category:cs.CL published:2014-12-27 summary:SentiWordNet is an important lexical resource supporting sentiment analysisin opinion mining applications. In this paper, we propose a novel approach toconstruct a Vietnamese SentiWordNet (VSWN). SentiWordNet is typically generatedfrom WordNet in which each synset has numerical scores to indicate its opinionpolarities. Many previous studies obtained these scores by applying a machinelearning method to WordNet. However, Vietnamese WordNet is not availableunfortunately by the time of this paper. Therefore, we propose a method toconstruct VSWN from a Vietnamese dictionary, not from WordNet. We show theeffectiveness of the proposed method by generating a VSWN with 39,561 synsetsautomatically. The method is experimentally tested with 266 synsets with aspectof positivity and negativity. It attains a competitive result compared withEnglish SentiWordNet that is 0.066 and 0.052 differences for positivity andnegativity sets respectively.
arxiv-8700-288 | On the Relation between Realizable and Nonrealizable Cases of the Sequence Prediction Problem | http://arxiv.org/pdf/1005.5603v3.pdf | author:Daniil Ryabko category:cs.LG cs.IT math.IT math.ST stat.TH published:2010-05-31 summary:A sequence $x_1,\dots,x_n,\dots$ of discrete-valued observations is generatedaccording to some unknown probabilistic law (measure) $\mu$. After observingeach outcome, one is required to give conditional probabilities of the nextobservation. The realizable case is when the measure $\mu$ belongs to anarbitrary but known class $\mathcal C$ of process measures. The non-realizablecase is when $\mu$ is completely arbitrary, but the prediction performance ismeasured with respect to a given set $\mathcal C$ of process measures. We areinterested in the relations between these problems and between their solutions,as well as in characterizing the cases when a solution exists and finding thesesolutions. We show that if the quality of prediction is measured using thetotal variation distance, then these problems coincide, while if it is measuredusing the expected average KL divergence, then they are different. For some ofthe formalizations we also show that when a solution exists, it can be obtainedas a Bayes mixture over a countable subset of $\mathcal C$. We also obtainseveral characterization of those sets $\mathcal C$ for which solutions to theconsidered problems exist. As an illustration to the general results obtained,we show that a solution to the non-realizable case of the sequence predictionproblem exists for the set of all finite-memory processes, but does not existfor the set of all stationary processes. It should be emphasized that the framework is completely general: theprocesses measures considered are not required to be i.i.d., mixing,stationary, or to belong to any parametric family.
arxiv-8700-289 | Predicting User Engagement in Twitter with Collaborative Ranking | http://arxiv.org/pdf/1412.7990v1.pdf | author:Ernesto Diaz-Aviles, Hoang Thanh Lam, Fabio Pinelli, Stefano Braghin, Yiannis Gkoufas, Michele Berlingerio, Francesco Calabrese category:cs.IR cs.CY cs.LG H.3.3; I.2.6 published:2014-12-26 summary:Collaborative Filtering (CF) is a core component of popular web-basedservices such as Amazon, YouTube, Netflix, and Twitter. Most applications useCF to recommend a small set of items to the user. For instance, YouTubepresents to a user a list of top-n videos she would likely watch next based onher rating and viewing history. Current methods of CF evaluation have beenfocused on assessing the quality of a predicted rating or the rankingperformance for top-n recommended items. However, restricting the recommendersystem evaluation to these two aspects is rather limiting and neglects otherdimensions that could better characterize a well-perceived recommendation. Inthis paper, instead of optimizing rating or top-n recommendation, we focus onthe task of predicting which items generate the highest user engagement. Inparticular, we use Twitter as our testbed and cast the problem as aCollaborative Ranking task where the rich features extracted from the metadataof the tweets help to complement the transaction information limited to userids, item ids, ratings and timestamps. We learn a scoring function thatdirectly optimizes the user engagement in terms of nDCG@10 on the predictedranking. Experiments conducted on an extended version of the MovieTweetingsdataset, released as part of the RecSys Challenge 2014, show the effectivenessof our approach.
arxiv-8700-290 | Texture analysis by multi-resolution fractal descriptors | http://arxiv.org/pdf/1412.7963v1.pdf | author:João B. Florindo, Odemir M. Bruno category:cs.CV published:2014-12-26 summary:This work proposes a texture descriptor based on fractal theory. The methodis based on the Bouligand-Minkowski descriptors. We decompose the originalimage recursively into 4 equal parts. In each recursion step, we estimate theaverage and the deviation of the Bouligand-Minkowski descriptors computed overeach part. Thus, we extract entropy features from both average and deviation.The proposed descriptors are provided by the concatenation of such measures.The method is tested in a classification experiment under well known datasets,that is, Brodatz and Vistex. The results demonstrate that the proposedtechnique achieves better results than classical and state-of-the-art texturedescriptors, such as Gabor-wavelets and co-occurrence matrix.
arxiv-8700-291 | Detect2Rank : Combining Object Detectors Using Learning to Rank | http://arxiv.org/pdf/1412.7957v1.pdf | author:Sezer Karaoglu, Yang Liu, Theo Gevers category:cs.CV published:2014-12-26 summary:Object detection is an important research area in the field of computervision. Many detection algorithms have been proposed. However, each objectdetector relies on specific assumptions of the object appearance and imagingconditions. As a consequence, no algorithm can be considered as universal. Withthe large variety of object detectors, the subsequent question is how to selectand combine them. In this paper, we propose a framework to learn how to combine objectdetectors. The proposed method uses (single) detectors like DPM, CN and EES,and exploits their correlation by high level contextual features to yield acombined detection list. Experiments on the PASCAL VOC07 and VOC10 datasets show that the proposedmethod significantly outperforms single object detectors, DPM (8.4%), CN (6.8%)and EES (17.0%) on VOC07 and DPM (6.5%), CN (5.5%) and EES (16.2%) on VOC10.
arxiv-8700-292 | Unsupervised Learning through Prediction in a Model of Cortex | http://arxiv.org/pdf/1412.7955v1.pdf | author:Christos H. Papadimitriou, Santosh S. Vempala category:cs.NE cs.DS q-bio.NC stat.ML published:2014-12-26 summary:We propose a primitive called PJOIN, for "predictive join," which combinesand extends the operations JOIN and LINK, which Valiant proposed as the basisof a computational theory of cortex. We show that PJOIN can be implemented inValiant's model. We also show that, using PJOIN, certain reasonably complexlearning and pattern matching tasks can be performed, in a way that involvesphenomena which have been observed in cognition and the brain, namelymemory-based prediction and downward traffic in the cortical hierarchy.
arxiv-8700-293 | A Novel Feature Selection and Extraction Technique for Classification | http://arxiv.org/pdf/1412.7934v1.pdf | author:Kratarth Goel, Raunaq Vohra, Ainesh Bakshi category:cs.LG cs.CV published:2014-12-26 summary:This paper presents a versatile technique for the purpose of featureselection and extraction - Class Dependent Features (CDFs). We use CDFs toimprove the accuracy of classification and at the same time controlcomputational expense by tackling the curse of dimensionality. In order todemonstrate the generality of this technique, it is applied to handwrittendigit recognition and text categorization.
arxiv-8700-294 | Polyphonic Music Generation by Modeling Temporal Dependencies Using a RNN-DBN | http://arxiv.org/pdf/1412.7927v1.pdf | author:Kratarth Goel, Raunaq Vohra, J. K. Sahoo category:cs.LG cs.AI cs.NE published:2014-12-26 summary:In this paper, we propose a generic technique to model temporal dependenciesand sequences using a combination of a recurrent neural network and a DeepBelief Network. Our technique, RNN-DBN, is an amalgamation of the memory stateof the RNN that allows it to provide temporal information and a multi-layer DBNthat helps in high level representation of the data. This makes RNN-DBNs idealfor sequence generation. Further, the use of a DBN in conjunction with the RNNmakes this model capable of significantly more complex data representation thanan RBM. We apply this technique to the task of polyphonic music generation.
arxiv-8700-295 | Visualizing and Comparing Convolutional Neural Networks | http://arxiv.org/pdf/1412.6631v2.pdf | author:Wei Yu, Kuiyuan Yang, Yalong Bai, Hongxun Yao, Yong Rui category:cs.CV published:2014-12-20 summary:Convolutional Neural Networks (CNNs) have achieved comparable error rates towell-trained human on ILSVRC2014 image classification task. To achieve betterperformance, the complexity of CNNs is continually increasing with deeper andbigger architectures. Though CNNs achieved promising external classificationbehavior, understanding of their internal work mechanism is still limited. Inthis work, we attempt to understand the internal work mechanism of CNNs byprobing the internal representations in two comprehensive aspects, i.e.,visualizing patches in the representation spaces constructed by differentlayers, and visualizing visual information kept in each layer. We furthercompare CNNs with different depths and show the advantages brought by deeperarchitecture.
arxiv-8700-296 | Structure Tensor Based Image Interpolation Method | http://arxiv.org/pdf/1402.5564v3.pdf | author:Ahmadreza Baghaie, Zeyun Yu category:cs.CV published:2014-02-22 summary:Feature preserving image interpolation is an active area in image processingfield. In this paper a new direct edge directed image super-resolutionalgorithm based on structure tensors is proposed. Using an isotropic Gaussianfilter, the structure tensor at each pixel of the input image is computed andthe pixels are classified to three distinct classes; uniform region, cornersand edges, according to the eigenvalues of the structure tensor. Due toapplication of the isotropic Gaussian filter, the classification is robust tonoise presented in image. Based on the tangent eigenvector of the structuretensor, the edge direction is determined and used for interpolation along theedges. In comparison to some previous edge directed image interpolationmethods, the proposed method achieves higher quality in both subjective andobjective aspects. Also the proposed method outperforms previous methods incase of noisy and JPEG compressed images. Furthermore, without the need foroptimization in the process, the algorithm can achieve higher speed.
arxiv-8700-297 | Improved texture image classification through the use of a corrosion-inspired cellular automaton | http://arxiv.org/pdf/1412.7889v1.pdf | author:Núbia Rosa da Silva, Pieter Van der Weeën, Bernard De Baets, Odemir Martinez Bruno category:cs.CV published:2014-12-26 summary:In this paper, the problem of classifying synthetic and natural textureimages is addressed. To tackle this problem, an innovative method is proposedthat combines concepts from corrosion modeling and cellular automata togenerate a texture descriptor. The core processes of metal (pitting) corrosionare identified and applied to texture images by incorporating the basicmechanisms of corrosion in the transition function of the cellular automaton.The surface morphology of the image is analyzed before and during theapplication of the transition function of the cellular automaton. In eachiteration the cumulative mass of corroded product is obtained to construct eachof the attributes of the texture descriptor. In a final step, this texturedescriptor is used for image classification by applying Linear DiscriminantAnalysis. The method was tested on the well-known Brodatz and Vistex databases.In addition, in order to verify the robustness of the method, its invariance tonoise and rotation were tested. To that end, different variants of the originaltwo databases were obtained through addition of noise to and rotation of theimages. The results showed that the method is effective for textureclassification according to the high success rates obtained in all cases. Thisindicates the potential of employing methods inspired on natural phenomena inother fields.
arxiv-8700-298 | Sparkle Vision: Seeing the World through Random Specular Microfacets | http://arxiv.org/pdf/1412.7884v1.pdf | author:Zhengdong Zhang, Phillip Isola, Edward H. Adelson category:cs.CV published:2014-12-26 summary:In this paper, we study the problem of reproducing the world lighting from asingle image of an object covered with random specular microfacets on thesurface. We show that such reflectors can be interpreted as a randomizedmapping from the lighting to the image. Such specular objects have verydifferent optical properties from both diffuse surfaces and smooth specularobjects like metals, so we design special imaging system to robustly andeffectively photograph them. We present simple yet reliable algorithms tocalibrate the proposed system and do the inference. We conduct experiments toverify the correctness of our model assumptions and prove the effectiveness ofour pipeline.
arxiv-8700-299 | Enhancing fractal descriptors on images by combining boundary and interior of Minkowski dilation | http://arxiv.org/pdf/1412.7880v1.pdf | author:Marcos W. S. Oliveira, Dalcimar Casanova, João B. Florindo, Odemir Martinez Bruno category:cs.CV published:2014-12-26 summary:This work proposes to obtain novel fractal descriptors from gray-leveltexture images by combining information from interior and boundary measures ofthe Minkowski dilation applied to the texture surface. At first, the image isconverted into a surface where the height of each point is the gray intensityof the respective pixel in that position in the image. Thus, this surface ismorphologically dilated by spheres. The radius of such spheres is ranged withinan interval and the volume and the external area of the dilated structure arecomputed for each radius. The final descriptors are given by such measuresconcatenated and subject to a canonical transform to reduce the dimensionality.The proposal is an enhancement to the classical Bouligand-Minkowski fractaldescriptors, where only the volume (interior) information is considered. Asdifferent structures may have the same volume, but not the same area, theproposal yields to more rich descriptors as confirmed by results on theclassification of benchmark databases.
arxiv-8700-300 | Gaussian Process Pseudo-Likelihood Models for Sequence Labeling | http://arxiv.org/pdf/1412.7868v1.pdf | author:P. K. Srijith, P. Balamurugan, Shirish Shevade category:cs.LG stat.ML published:2014-12-25 summary:Several machine learning problems arising in natural language processing canbe modeled as a sequence labeling problem. We provide Gaussian process modelsbased on pseudo-likelihood approximation to perform sequence labeling. Gaussianprocesses (GPs) provide a Bayesian approach to learning in a kernel basedframework. The pseudo-likelihood model enables one to capture long rangedependencies among the output components of the sequence without becomingcomputationally intractable. We use an efficient variational Gaussianapproximation method to perform inference in the proposed model. We alsoprovide an iterative algorithm which can effectively make use of theinformation from the neighboring labels to perform prediction. The ability tocapture long range dependencies makes the proposed approach useful for a widerange of sequence labeling problems. Numerical experiments on some sequencelabeling data sets demonstrate the usefulness of the proposed approach.
