arxiv-1605-00894 | Recurrent Convolutional Neural Network Regression for Continuous Pain Intensity Estimation in Video | http://arxiv.org/abs/1605.00894 | id:1605.00894 author:Jing Zhou, Xiaopeng Hong, Fei Su, Guoying Zhao category:cs.CV  published:2016-05-03 summary:Automatic pain intensity estimation possesses a significant position in healthcare and medical field. Traditional static methods prefer to extract features from frames separately in a video, which would result in unstable changes and peaks among adjacent frames. To overcome this problem, we propose a real-time regression framework based on the recurrent convolutional neural network for automatic frame-level pain intensity estimation. Given vector sequences of AAM-warped facial images, we used a sliding-window strategy to obtain fixed-length input samples for the recurrent network. We then carefully design the architecture of the recurrent network to output continuous-valued pain intensity. The proposed end-to-end pain intensity regression framework can predict the pain intensity of each frame by considering a sufficiently large historical frames while limiting the scale of the parameters within the model. Our method achieves promising results regarding both accuracy and running speed on the published UNBC-McMaster Shoulder Pain Expression Archive Database. version:1
arxiv-1605-00855 | Improving Image Captioning by Concept-based Sentence Reranking | http://arxiv.org/abs/1605.00855 | id:1605.00855 author:Xirong Li, Qin Jin category:cs.CV cs.CL  published:2016-05-03 summary:This paper describes our winning entry in the ImageCLEF 2015 image sentence generation task. We improve Google's CNN-LSTM model by introducing concept-based sentence reranking, a data-driven approach which exploits the large amounts of concept-level annotations on Flickr. Different from previous usage of concept detection that is tailored to specific image captioning models, the propose approach reranks predicted sentences in terms of their matches with detected concepts, essentially treating the underlying model as a black box. This property makes the approach applicable to a number of existing solutions. We also experiment with fine tuning on the deep language model, which improves the performance further. Scoring METEOR of 0.1875 on the ImageCLEF 2015 test set, our system outperforms the runner-up (METEOR of 0.1687) with a clear margin. version:1
arxiv-1508-07964 | Learning to Aggregate Information for Sequential Inferences | http://arxiv.org/abs/1508.07964 | id:1508.07964 author:Diyan Teng, Emre Ertin category:stat.ML cs.LG  published:2015-08-31 summary:We consider the problem of training a binary sequential classifier under an error rate constraint. It is well known that for known densities, accumulating the likelihood ratio statistics is time optimal under a fixed error rate constraint. For the case of unknown densities, we formulate the learning for sequential detection problem as a constrained density ratio estimation problem. Specifically, we show that the problem can be posed as a convex optimization problem using a Reproducing Kernel Hilbert Space representation for the log-density ratio function. The proposed binary sequential classifier is tested on synthetic data set and UC Irvine human activity recognition data set, together with previous approaches for density ratio estimation. Our empirical results show that the classifier trained through the proposed technique achieves smaller average sampling cost than previous classifiers proposed in the literature for the same error rate. version:2
arxiv-1605-00779 | Temporal Clustering of Time Series via Threshold Autoregressive Models: Application to Commodity Prices | http://arxiv.org/abs/1605.00779 | id:1605.00779 author:Sipan Aslan, Ceylan Yozgatligil, Cem Iyigun category:stat.ML stat.AP stat.ME  published:2016-05-03 summary:This study aimed to find temporal clusters for several commodity prices using the threshold non-linear autoregressive model. It is expected that the process of determining the commodity groups that are time-dependent will advance the current knowledge about the dynamics of co-moving and coherent prices, and can serve as a basis for multivariate time series analyses. The clustering of commodity prices was examined using the proposed clustering approach based on time series models to incorporate the time varying properties of price series into the clustering scheme. Accordingly, the primary aim in this study was grouping time series according to the similarity between their Data Generating Mechanisms (DGMs) rather than comparing pattern similarities in the time series traces. The approximation to the DGM of each series was accomplished using threshold autoregressive models, which are recognized for their ability to represent nonlinear features in time series, such as abrupt changes, time-irreversibility and regime-shifting behavior. Through the use of the proposed approach, one can determine and monitor the set of co-moving time series variables across the time dimension. Furthermore, generating a time varying commodity price index and sub-indexes can become possible. Consequently, we conducted a simulation study to assess the effectiveness of the proposed clustering approach and the results are presented for both the simulated and real data sets. version:1
arxiv-1605-00775 | Spatially Aware Dictionary Learning and Coding for Fossil Pollen Identification | http://arxiv.org/abs/1605.00775 | id:1605.00775 author:Shu Kong, Surangi Punyasena, Charless Fowlkes category:cs.CV q-bio.PE q-bio.QM  published:2016-05-03 summary:We propose a robust approach for performing automatic species-level recognition of fossil pollen grains in microscopy images that exploits both global shape and local texture characteristics in a patch-based matching methodology. We introduce a novel criteria for selecting meaningful and discriminative exemplar patches. We optimize this function during training using a greedy submodular function optimization framework that gives a near-optimal solution with bounded approximation error. We use these selected exemplars as a dictionary basis and propose a spatially-aware sparse coding method to match testing images for identification while maintaining global shape correspondence. To accelerate the coding process for fast matching, we introduce a relaxed form that uses spatially-aware soft-thresholding during coding. Finally, we carry out an experimental study that demonstrates the effectiveness and efficiency of our exemplar selection and classification mechanisms, achieving $86.13\%$ accuracy on a difficult fine-grained species classification task distinguishing three types of fossil spruce pollen. version:1
arxiv-1311-6238 | Exact post-selection inference, with application to the lasso | http://arxiv.org/abs/1311.6238 | id:1311.6238 author:Jason D. Lee, Dennis L. Sun, Yuekai Sun, Jonathan E. Taylor category:math.ST stat.ME stat.ML stat.TH  published:2013-11-25 summary:We develop a general approach to valid inference after model selection. At the core of our framework is a result that characterizes the distribution of a post-selection estimator conditioned on the selection event. We specialize the approach to model selection by the lasso to form valid confidence intervals for the selected coefficients and test whether all relevant variables have been included in the model. version:8
arxiv-1605-00763 | Automatic Identification of Retinal Arteries and Veins in Fundus Images using Local Binary Patterns | http://arxiv.org/abs/1605.00763 | id:1605.00763 author:Nima Hatami, Michael Goldbaum category:cs.CV  published:2016-05-03 summary:Artery and vein (AV) classification of retinal images is a key to necessary tasks, such as automated measurement of arteriolar-to-venular diameter ratio (AVR). This paper comprehensively reviews the state-of-the art in AV classification methods. To improve on previous methods, a new Local Bi- nary Pattern-based method (LBP) is proposed. Beside its simplicity, LBP is robust against low contrast and low quality fundus images; and it helps the process by including additional AV texture and shape information. Experimental results compare the performance of the new method with the state-of-the art; and also methods with different feature extraction and classification schemas. version:1
arxiv-1605-00758 | Efficient Distributed Estimation of Inverse Covariance Matrices | http://arxiv.org/abs/1605.00758 | id:1605.00758 author:Jesús Arroyo, Elizabeth Hou category:stat.ME stat.ML  published:2016-05-03 summary:In distributed systems, communication is a major concern due to issues such as its vulnerability or efficiency. In this paper, we are interested in estimating sparse inverse covariance matrices when samples are distributed into different machines. We address communication efficiency by proposing a method where, in a single round of communication, each machine transfers a small subset of the entries of the inverse covariance matrix. We show that, with this efficient distributed method, the error rates can be comparable with estimation in a non-distributed setting, and correct model selection is still possible. Practical performance is shown through simulations. version:1
arxiv-1602-01528 | EIE: Efficient Inference Engine on Compressed Deep Neural Network | http://arxiv.org/abs/1602.01528 | id:1602.01528 author:Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, William J. Dally category:cs.CV cs.AR  published:2016-02-04 summary:State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power. Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by pruning the redundant connections and having multiple connections share the same weight. We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing. Going from DRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x; Weight sharing gives 8x; Skipping zero activations from ReLU saves another 3x. Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to CPU and GPU implementations of the same DNN without compression. EIE has a processing power of 102GOPS/s working directly on a compressed network, corresponding to 3TOPS/s on an uncompressed network, and processes FC layers of AlexNet at 1.88x10^4 frames/sec with a power dissipation of only 600mW. It is 24,000x and 3,400x more energy efficient than a CPU and GPU respectively. Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy efficiency and area efficiency. version:2
arxiv-1603-01942 | A Two-Stage Shape Retrieval (TSR) Method with Global and Local Features | http://arxiv.org/abs/1603.01942 | id:1603.01942 author:Xiaqing Pan, Sachin Chachada, C. -C. Jay Kuo category:cs.CV  published:2016-03-07 summary:A robust two-stage shape retrieval (TSR) method is proposed to address the 2D shape retrieval problem. Most state-of-the-art shape retrieval methods are based on local features matching and ranking. Their retrieval performance is not robust since they may retrieve globally dissimilar shapes in high ranks. To overcome this challenge, we decompose the decision process into two stages. In the first irrelevant cluster filtering (ICF) stage, we consider both global and local features and use them to predict the relevance of gallery shapes with respect to the query. Irrelevant shapes are removed from the candidate shape set. After that, a local-features-based matching and ranking (LMR) method follows in the second stage. We apply the proposed TSR system to MPEG-7, Kimia99 and Tari1000 three datasets and show that it outperforms all other existing methods. The robust retrieval performance of the TSR system is demonstrated. version:3
arxiv-1605-00743 | Learning Attributes Equals Multi-Source Domain Generalization | http://arxiv.org/abs/1605.00743 | id:1605.00743 author:Chuang Gan, Tianbao Yang, Boqing Gong category:cs.CV  published:2016-05-03 summary:Attributes possess appealing properties and benefit many computer vision problems, such as object recognition, learning with humans in the loop, and image retrieval. Whereas the existing work mainly pursues utilizing attributes for various computer vision problems, we contend that the most basic problem---how to accurately and robustly detect attributes from images---has been left under explored. Especially, the existing work rarely explicitly tackles the need that attribute detectors should generalize well across different categories, including those previously unseen. Noting that this is analogous to the objective of multi-source domain generalization, if we treat each category as a domain, we provide a novel perspective to attribute detection and propose to gear the techniques in multi-source domain generalization for the purpose of learning cross-category generalizable attribute detectors. We validate our understanding and approach with extensive experiments on four challenging datasets and three different problems. version:1
arxiv-1605-00740 | VLSI Extreme Learning Machine: A Design Space Exploration | http://arxiv.org/abs/1605.00740 | id:1605.00740 author:Enyi Yao, Arindam Basu category:cs.LG cs.ET  published:2016-05-03 summary:In this paper, we describe a compact low-power, high performance hardware implementation of the extreme learning machine (ELM) for machine learning applications. Mismatch in current mirrors are used to perform the vector-matrix multiplication that forms the first stage of this classifier and is the most computationally intensive. Both regression and classification (on UCI data sets) are demonstrated and a design space trade-off between speed, power and accuracy is explored. Our results indicate that for a wide set of problems, $\sigma V_T$ in the range of $15-25$mV gives optimal results. An input weight matrix rotation method to extend the input dimension and hidden layer size beyond the physical limits imposed by the chip is also described. This allows us to overcome a major limit imposed on most hardware machine learners. The chip is implemented in a $0.35 \mu$m CMOS process and occupies a die area of around 5 mm $\times$ 5 mm. Operating from a $1$ V power supply, it achieves an energy efficiency of $0.47$ pJ/MAC at a classification rate of $31.6$ kHz. version:1
arxiv-1605-00732 | A propagation matting method based on the Local Sampling and KNN Classification with adaptive feature space | http://arxiv.org/abs/1605.00732 | id:1605.00732 author:Xiao Chen, Fazhi He category:cs.CV  published:2016-05-03 summary:Closed Form is a propagation based matting algorithm, functioning well on images with good propagation . The deficiency of the Closed Form method is that for complex areas with poor image propagation , such as hole areas or areas of long and narrow structures. The right results are usually hard to get. On these areas, if certain flags are provided, it can improve the effects of matting. In this paper, we design a matting algorithm by local sampling and the KNN classifier propagation based matting algorithm. First of all, build the corresponding features space according to the different components of image colors to reduce the influence of overlapping between the foreground and background, and to improve the classification accuracy of KNN classifier. Second, adaptively use local sampling or using local KNN classifier for processing based on the pros and cons of the sample performance of unknown image areas. Finally, based on different treatment methods for the unknown areas, we will use different weight for augmenting constraints to make the treatment more effective. In this paper, by combining qualitative observation and quantitative analysis, we will make evaluation of the experimental results through online standard set of evaluation tests. It shows that on images with good propagation , this method is as effective as the Closed Form method, while on images in complex regions, it can perform even better than Closed Form. version:1
arxiv-1511-06606 | Data Representation and Compression Using Linear-Programming Approximations | http://arxiv.org/abs/1511.06606 | id:1511.06606 author:Hristo S. Paskov, John C. Mitchell, Trevor J. Hastie category:cs.LG  published:2015-11-20 summary:We propose `Dracula', a new framework for unsupervised feature selection from sequential data such as text. Dracula learns a dictionary of $n$-grams that efficiently compresses a given corpus and recursively compresses its own dictionary; in effect, Dracula is a `deep' extension of Compressive Feature Learning. It requires solving a binary linear program that may be relaxed to a linear program. Both problems exhibit considerable structure, their solution paths are well behaved, and we identify parameters which control the depth and diversity of the dictionary. We also discuss how to derive features from the compressed documents and show that while certain unregularized linear models are invariant to the structure of the compressed dictionary, this structure may be used to regularize learning. Experiments are presented that demonstrate the efficacy of Dracula's features. version:5
arxiv-1605-00716 | Radio Transformer Networks: Attention Models for Learning to Synchronize in Wireless Systems | http://arxiv.org/abs/1605.00716 | id:1605.00716 author:Timothy J O'Shea, Latha Pemula, Dhruv Batra, T. Charles Clancy category:cs.LG cs.NI cs.SY  published:2016-05-03 summary:We introduce learned attention models into the radio machine learning domain for the task of modulation recognition by leveraging spatial transformer networks and introducing new radio domain appropriate transformations. This attention model allows the network to learn a localization network capable of synchronizing and normalizing a radio signal blindly with zero knowledge of the signals structure based on optimization of the network for classification accuracy, sparse representation, and regularization. Using this architecture we are able to outperform our prior results in accuracy vs signal to noise ratio against an identical system without attention, however we believe such an attention model has implication far beyond the task of modulation recognition. version:1
arxiv-1502-01425 | Provable Sparse Tensor Decomposition | http://arxiv.org/abs/1502.01425 | id:1502.01425 author:Will Wei Sun, Junwei Lu, Han Liu, Guang Cheng category:stat.ML  published:2015-02-05 summary:We propose a novel sparse tensor decomposition method, namely Tensor Truncated Power (TTP) method, that incorporates variable selection into the estimation of decomposition components. The sparsity is achieved via an efficient truncation step embedded in the tensor power iteration. Our method applies to a broad family of high dimensional latent variable models, including high dimensional Gaussian mixture and mixtures of sparse regressions. A thorough theoretical investigation is further conducted. In particular, we show that the final decomposition estimator is guaranteed to achieve a local statistical rate, and further strengthen it to the global statistical rate by introducing a proper initialization procedure. In high dimensional regimes, the obtained statistical rate significantly improves those shown in the existing non-sparse decomposition methods. The empirical advantages of TTP are confirmed in extensive simulated results and two real applications of click-through rate prediction and high-dimensional gene clustering. version:3
arxiv-1605-00707 | Discovering Useful Parts for Pose Estimation in Sparsely Annotated Datasets | http://arxiv.org/abs/1605.00707 | id:1605.00707 author:Mikhail Breslav, Tyson L. Hedrick, Stan Sclaroff, Margrit Betke category:cs.CV  published:2016-05-02 summary:Our work introduces a novel way to increase pose estimation accuracy by discovering parts from unannotated regions of training images. Discovered parts are used to generate more accurate appearance likelihoods for traditional part-based models like Pictorial Structures [13] and its derivatives. Our experiments on images of a hawkmoth in flight show that our proposed approach significantly improves over existing work [27] for this application, while also being more generally applicable. Our proposed approach localizes landmarks at least twice as accurately as a baseline based on a Mixture of Pictorial Structures (MPS) model. Our unique High-Resolution Moth Flight (HRMF) dataset is made publicly available with annotations. version:1
arxiv-1507-03955 | Robust Estimation of Self-Exciting Point Process Models with Application to Neuronal Modeling | http://arxiv.org/abs/1507.03955 | id:1507.03955 author:Abbas Kazemipour, Min Wu, Behtash Babadi category:cs.NE cs.IT cs.SY math.IT math.OC stat.AP  published:2015-07-14 summary:We consider the problem of estimating discrete self-exciting point process models from limited binary observations, where the history of the process serves as the covariate. We analyze the performance of two classes of estimators, namely the $\ell_1$-regularized maximum likelihood and greedy estimators, for a canonical self-exciting point process and characterize the sampling tradeoffs required for stable recovery in the non-asymptotic regime. Our results extend those of compressed sensing for linear and generalized linear models with i.i.d. covariates to point processes with highly inter-dependent covariates. We further provide simulation studies as well as application to real spiking data from mouse's lateral geniculate nucleus and ferret's retinal ganglion cells which agree with our theoretical predictions. version:2
arxiv-1605-00659 | Predicting online extremism, content adopters, and interaction reciprocity | http://arxiv.org/abs/1605.00659 | id:1605.00659 author:Emilio Ferrara, Wen-Qiang Wang, Onur Varol, Alessandro Flammini, Aram Galstyan category:cs.SI cs.LG physics.soc-ph  published:2016-05-02 summary:We present a machine learning framework that leverages a mixture of metadata, network, and temporal features to detect extremist users, and predict content adopters and interaction reciprocity in social media. We exploit a unique dataset containing millions of tweets generated by more than 25 thousand users who have been manually identified, reported, and suspended by Twitter due to their involvement with extremist campaigns. We also leverage millions of tweets generated by a random sample of 25 thousand regular users who were exposed to, or consumed, extremist content. We carry out three forecasting tasks, (i) to detect extremist users, (ii) to estimate whether regular users will adopt extremist content, and finally (iii) to predict whether users will reciprocate contacts initiated by extremists. All forecasting tasks are set up in two scenarios: a post hoc (time independent) prediction task on aggregated data, and a simulated real-time prediction task. The performance of our framework is extremely promising, yielding in the different forecasting scenarios up to 93% AUC for extremist user detection, up to 80% AUC for content adoption prediction, and finally up to 72% AUC for interaction reciprocity forecasting. We conclude by providing a thorough feature analysis that helps determine which are the emerging signals that provide predictive power in different scenarios. version:1
arxiv-1604-02917 | Gaussian Process Domain Experts for Model Adaptation in Facial Behavior Analysis | http://arxiv.org/abs/1604.02917 | id:1604.02917 author:Stefanos Eleftheriadis, Ognjen Rudovic, Marc P. Deisenroth, Maja Pantic category:stat.ML cs.CV cs.LG  published:2016-04-11 summary:We present a novel approach for supervised domain adaptation that is based upon the probabilistic framework of Gaussian processes (GPs). Specifically, we introduce domain-specific GPs as local experts for facial expression classification from face images. The adaptation of the classifier is facilitated in probabilistic fashion by conditioning the target expert on multiple source experts. Furthermore, in contrast to existing adaptation approaches, we also learn a target expert from available target data solely. Then, a single and confident classifier is obtained by combining the predictions from multiple experts based on their confidence. Learning of the model is efficient and requires no retraining/reweighting of the source classifiers. We evaluate the proposed approach on two publicly available datasets for multi-class (MultiPIE) and multi-label (DISFA) facial expression classification. To this end, we perform adaptation of two contextual factors: 'where' (view) and 'who' (subject). We show in our experiments that the proposed approach consistently outperforms both source and target classifiers, while using as few as 30 target examples. It also outperforms the state-of-the-art approaches for supervised domain adaptation. version:2
arxiv-1605-00609 | Algorithms for Learning Sparse Additive Models with Interactions in High Dimensions | http://arxiv.org/abs/1605.00609 | id:1605.00609 author:Hemant Tyagi, Anastasios Kyrillidis, Bernd Gärtner, Andreas Krause category:cs.LG cs.IT math.IT math.NA stat.ML  published:2016-05-02 summary:A function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is a Sparse Additive Model (SPAM), if it is of the form $f(\mathbf{x}) = \sum_{l \in \mathcal{S}}\phi_{l}(x_l)$ where $\mathcal{S} \subset [d]$, $ \mathcal{S} \ll d$. Assuming $\phi$'s, $\mathcal{S}$ to be unknown, there exists extensive work for estimating $f$ from its samples. In this work, we consider a generalized version of SPAMs, that also allows for the presence of a sparse number of second order interaction terms. For some $\mathcal{S}_1 \subset [d], \mathcal{S}_2 \subset {[d] \choose 2}$, with $ \mathcal{S}_1 \ll d, \mathcal{S}_2 \ll d^2$, the function $f$ is now assumed to be of the form: $\sum_{p \in \mathcal{S}_1}\phi_{p} (x_p) + \sum_{(l,l^{\prime}) \in \mathcal{S}_2}\phi_{(l,l^{\prime})} (x_l,x_{l^{\prime}})$. Assuming we have the freedom to query $f$ anywhere in its domain, we derive efficient algorithms that provably recover $\mathcal{S}_1,\mathcal{S}_2$ with finite sample bounds. Our analysis covers the noiseless setting where exact samples of $f$ are obtained, and also extends to the noisy setting where the queries are corrupted with noise. For the noisy setting in particular, we consider two noise models namely: i.i.d Gaussian noise and arbitrary but bounded noise. Our main methods for identification of $\mathcal{S}_2$ essentially rely on estimation of sparse Hessian matrices, for which we provide two novel compressed sensing based schemes. Once $\mathcal{S}_1, \mathcal{S}_2$ are known, we show how the individual components $\phi_p$, $\phi_{(l,l^{\prime})}$ can be estimated via additional queries of $f$, with uniform error bounds. Lastly, we provide simulation results on synthetic data that validate our theoretical findings. version:1
arxiv-1506-07924 | Decentralized Q-Learning for Stochastic Teams and Games | http://arxiv.org/abs/1506.07924 | id:1506.07924 author:Gürdal Arslan, Serdar Yüksel category:math.OC cs.GT cs.LG  published:2015-06-25 summary:There are only a few learning algorithms applicable to stochastic dynamic teams and games which generalize Markov decision processes to decentralized stochastic control problems involving possibly self-interested decision makers. Learning in games is generally difficult because of the non-stationary environment in which each decision maker aims to learn its optimal decisions with minimal information in the presence of the other decision makers who are also learning. In stochastic dynamic games, learning is more challenging because, while learning, the decision makers alter the state of the system and hence the future cost. In this paper, we present decentralized Q-learning algorithms for stochastic games, and study their convergence for the weakly acyclic case which includes team problems as an important special case. The algorithm is decentralized in that each decision maker has access to only its local information, the state information, and the local cost realizations; furthermore, it is completely oblivious to the presence of other decision makers. We show that these algorithms converge to equilibrium policies almost surely in large classes of stochastic games. version:2
arxiv-1605-00596 | Graph Clustering Bandits for Recommendation | http://arxiv.org/abs/1605.00596 | id:1605.00596 author:Shuai Li, Claudio Gentile, Alexandros Karatzoglou category:stat.ML cs.AI cs.IR cs.LG  published:2016-05-02 summary:We investigate an efficient context-dependent clustering technique for recommender systems based on exploration-exploitation strategies through multi-armed bandits over multiple users. Our algorithm dynamically groups users based on their observed behavioral similarity during a sequence of logged activities. In doing so, the algorithm reacts to the currently served user by shaping clusters around him/her but, at the same time, it explores the generation of clusters over users which are not currently engaged. We motivate the effectiveness of this clustering policy, and provide an extensive empirical analysis on real-world datasets, showing scalability and improved prediction performance over state-of-the-art methods for sequential clustering of users in multi-armed bandit scenarios. version:1
arxiv-1605-00591 | The geometry of learning | http://arxiv.org/abs/1605.00591 | id:1605.00591 author:Gianluca Calcagni category:q-bio.QM cs.NE  published:2016-05-02 summary:We establish a correspondence between classical conditioning processes and fractals. The association strength at a given training trial corresponds to a point in a disconnected set at a given iteration level. In this way, one can represent a training process as a hopping on a fractal set, instead of the traditional learning curve as a function of the trial. The main advantage of this novel perspective is to provide an elegant classification of associative theories in terms of the geometric features of fractal sets. In particular, the dimension of fractals is a parameter that can both measure the efficiency of a given conditioning model (in terms of the characteristics of the stimuli) and compare the efficiency of different models. We illustrate the correspondence with the examples of the Hull, Rescorla-Wagner, and Mackintosh models and show that they are equivalent to a Cantor set. In doing so, we approximate the Mackintosh model with a new formulation in terms of a nonlinear recursive equation for the strength of association. version:1
arxiv-1512-00297 | Sequential visibility-graph motifs | http://arxiv.org/abs/1512.00297 | id:1512.00297 author:Jacopo Iacovacci, Lucas Lacasa category:physics.data-an cs.LG nlin.CD  published:2015-12-01 summary:Visibility algorithms transform time series into graphs and encode dynamical information in their topology, paving the way for graph-theoretical time series analysis as well as building a bridge between nonlinear dynamics and network science. In this work we introduce and study the concept of sequential visibility graph motifs, smaller substructures of n consecutive nodes that appear with characteristic frequencies. We develop a theory to compute in an exact way the motif profiles associated to general classes of deterministic and stochastic dynamics. We find that this simple property is indeed a highly informative and computationally efficient feature capable to distinguish among different dynamics and robust against noise contamination. We finally confirm that it can be used in practice to perform unsupervised learning, by extracting motif profiles from experimental heart-rate series and being able, accordingly, to disentangle meditative from other relaxation states. Applications of this general theory include the automatic classification and description of physical, biological, and financial time series. version:2
arxiv-1605-00572 | Comparison of Optimization Methods in Optical Flow Estimation | http://arxiv.org/abs/1605.00572 | id:1605.00572 author:Noranart Vesdapunt, Utkarsh Sinha category:cs.CV  published:2016-05-02 summary:Optical flow estimation is a widely known problem in computer vision introduced by Gibson, J.J(1950) to describe the visual perception of human by stimulus objects. Estimation of optical flow model can be achieved by solving for the motion vectors from region of interest in the the different timeline. In this paper, we assumed slightly uniform change of velocity between two nearby frames, and solve the optical flow problem by traditional method, Lucas-Kanade(1981). This method performs minimization of errors between template and target frame warped back onto the template. Solving minimization steps requires optimization methods which have diverse convergence rate and error. We explored first and second order optimization methods, and compare their results with Gauss-Newton method in Lucas-Kanade. We generated 105 videos with 10,500 frames by synthetic objects, and 10 videos with 1,000 frames from real world footage. Our experimental results could be used as tuning parameters for Lucas-Kanade method. version:1
arxiv-1605-00561 | Parallel Wavelet Schemes for Images | http://arxiv.org/abs/1605.00561 | id:1605.00561 author:David Barina, Michal Kula, Pavel Zemcik category:cs.CV  published:2016-05-02 summary:In this paper, we introduce several new schemes for calculation of discrete wavelet transforms of images. These schemes reduce the number of steps and, as a consequence, allow to reduce the number of synchronizations on parallel architectures. As an additional useful property, the proposed schemes can reduce also the number of arithmetic operations. The schemes are primarily demonstrated on CDF 5/3 and CDF 9/7 wavelets employed in JPEG 2000 image compression standard. However, the presented method is general and it can be applied on any wavelet transform. As a result, our scheme requires only two memory barriers for 2-D CDF 5/3 transform compared to four barriers in the original separable form or three barriers in the non-separable scheme recently published. Our reasoning is supported by exhaustive experiments on high-end graphics cards. version:1
arxiv-1412-2813 | Joint Segmentation and Deconvolution of Ultrasound Images Using a Hierarchical Bayesian Model based on Generalized Gaussian Priors | http://arxiv.org/abs/1412.2813 | id:1412.2813 author:Ningning Zhao, Adrian Basarab, Denis Kouame, Jean-Yves Tourneret category:cs.CV  published:2014-12-08 summary:This paper proposes a joint segmentation and deconvolution Bayesian method for medical ultrasound (US) images. Contrary to piecewise homogeneous images, US images exhibit heavy characteristic speckle patterns correlated with the tissue structures. The generalized Gaussian distribution (GGD) has been shown to be one of the most relevant distributions for characterizing the speckle in US images. Thus, we propose a GGD-Potts model defined by a label map coupling US image segmentation and deconvolution. The Bayesian estimators of the unknown model parameters, including the US image, the label map and all the hyperparameters are difficult to be expressed in closed form. Thus, we investigate a Gibbs sampler to generate samples distributed according to the posterior of interest. These generated samples are finally used to compute the Bayesian estimators of the unknown parameters. The performance of the proposed Bayesian model is compared with existing approaches via several experiments conducted on realistic synthetic data and in vivo US images. version:4
arxiv-1511-07247 | NetVLAD: CNN architecture for weakly supervised place recognition | http://arxiv.org/abs/1511.07247 | id:1511.07247 author:Relja Arandjelović, Petr Gronat, Akihiko Torii, Tomas Pajdla, Josef Sivic category:cs.CV cs.LG  published:2015-11-23 summary:We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the "Vector of Locally Aggregated Descriptors" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks. version:3
arxiv-1605-00529 | Tradeoffs for Space, Time, Data and Risk in Unsupervised Learning | http://arxiv.org/abs/1605.00529 | id:1605.00529 author:Mario Lucic, Mesrob I. Ohannessian, Amin Karbasi, Andreas Krause category:stat.ML cs.LG  published:2016-05-02 summary:Faced with massive data, is it possible to trade off (statistical) risk, and (computational) space and time? This challenge lies at the heart of large-scale machine learning. Using k-means clustering as a prototypical unsupervised learning problem, we show how we can strategically summarize the data (control space) in order to trade off risk and time when data is generated by a probabilistic model. Our summarization is based on coreset constructions from computational geometry. We also develop an algorithm, TRAM, to navigate the space/time/data/risk tradeoff in practice. In particular, we show that for a fixed risk (or data size), as the data size increases (resp. risk increases) the running time of TRAM decreases. Our extensive experiments on real data sets demonstrate the existence and practical utility of such tradeoffs, not only for k-means but also for Gaussian Mixture Models. version:1
arxiv-1603-08042 | On the Compression of Recurrent Neural Networks with an Application to LVCSR acoustic modeling for Embedded Speech Recognition | http://arxiv.org/abs/1603.08042 | id:1603.08042 author:Rohit Prabhavalkar, Ouais Alsharif, Antoine Bruguier, Ian McGraw category:cs.CL cs.LG cs.NE  published:2016-03-25 summary:We study the problem of compressing recurrent neural networks (RNNs). In particular, we focus on the compression of RNN acoustic models, which are motivated by the goal of building compact and accurate speech recognition systems which can be run efficiently on mobile devices. In this work, we present a technique for general recurrent model compression that jointly compresses both recurrent and non-recurrent inter-layer weight matrices. We find that the proposed technique allows us to reduce the size of our Long Short-Term Memory (LSTM) acoustic model to a third of its original size with negligible loss in accuracy. version:2
arxiv-1605-00519 | Linear-time Outlier Detection via Sensitivity | http://arxiv.org/abs/1605.00519 | id:1605.00519 author:Mario Lucic, Olivier Bachem, Andreas Krause category:stat.ML cs.LG  published:2016-05-02 summary:Outliers are ubiquitous in modern data sets. Distance-based techniques are a popular non-parametric approach to outlier detection as they require no prior assumptions on the data generating distribution and are simple to implement. Scaling these techniques to massive data sets without sacrificing accuracy is a challenging task. We propose a novel algorithm based on the intuition that outliers have a significant influence on the quality of divergence-based clustering solutions. We propose sensitivity - the worst-case impact of a data point on the clustering objective - as a measure of outlierness. We then prove that influence, a (non-trivial) upper-bound on the sensitivity, can be computed by a simple linear time algorithm. To scale beyond a single machine, we propose a communication efficient distributed algorithm. In an extensive experimental evaluation, we demonstrate the effectiveness and establish the statistical significance of the proposed approach. In particular, it outperforms the most popular distance-based approaches while being several orders of magnitude faster. version:1
arxiv-1508-05243 | Strong Coresets for Hard and Soft Bregman Clustering with Applications to Exponential Family Mixtures | http://arxiv.org/abs/1508.05243 | id:1508.05243 author:Mario Lucic, Olivier Bachem, Andreas Krause category:stat.ML cs.LG  published:2015-08-21 summary:Coresets are efficient representations of data sets such that models trained on the coreset are provably competitive with models trained on the original data set. As such, they have been successfully used to scale up clustering models such as K-Means and Gaussian mixture models to massive data sets. However, until now, the algorithms and the corresponding theory were usually specific to each clustering problem. We propose a single, practical algorithm to construct strong coresets for a large class of hard and soft clustering problems based on Bregman divergences. This class includes hard clustering with popular distortion measures such as the Squared Euclidean distance, the Mahalanobis distance, KL-divergence and Itakura-Saito distance. The corresponding soft clustering problems are directly related to popular mixture models due to a dual relationship between Bregman divergences and Exponential family distributions. Our theoretical results further imply a randomized polynomial-time approximation scheme for hard clustering. We demonstrate the practicality of the proposed algorithm in an empirical evaluation. version:2
arxiv-1605-00513 | Fuzzy clustering of distribution-valued data using adaptive L2 Wasserstein distances | http://arxiv.org/abs/1605.00513 | id:1605.00513 author:Antonio Irpino, Francisco De Carvalho, Rosanna Verde category:stat.ML 62A86  62H30  62G30 G.3; I.5.1; H.3.3  published:2016-05-02 summary:Distributional (or distribution-valued) data are a new type of data arising from several sources and are considered as realizations of distributional variables. A new set of fuzzy c-means algorithms for data described by distributional variables is proposed. The algorithms use the $L2$ Wasserstein distance between distributions as dissimilarity measures. Beside the extension of the fuzzy c-means algorithm for distributional data, and considering a decomposition of the squared $L2$ Wasserstein distance, we propose a set of algorithms using different automatic way to compute the weights associated with the variables as well as with their components, globally or cluster-wise. The relevance weights are computed in the clustering process introducing product-to-one constraints. The relevance weights induce adaptive distances expressing the importance of each variable or of each component in the clustering process, acting also as a variable selection method in clustering. We have tested the proposed algorithms on artificial and real-world data. Results confirm that the proposed methods are able to better take into account the cluster structure of the data with respect to the standard fuzzy c-means, with non-adaptive distances. version:1
arxiv-1402-5830 | A hybrid swarm-based algorithm for single-objective optimization problems involving high-cost analyses | http://arxiv.org/abs/1402.5830 | id:1402.5830 author:Enrico Ampellio, Luca Vassio category:math.OC cs.AI cs.DC cs.NE  published:2014-02-24 summary:In many technical fields, single-objective optimization procedures in continuous domains involve expensive numerical simulations. In this context, an improvement of the Artificial Bee Colony (ABC) algorithm, called the Artificial super-Bee enhanced Colony (AsBeC), is presented. AsBeC is designed to provide fast convergence speed, high solution accuracy and robust performance over a wide range of problems. It implements enhancements of the ABC structure and hybridizations with interpolation strategies. The latter are inspired by the quadratic trust region approach for local investigation and by an efficient global optimizer for separable problems. Each modification and their combined effects are studied with appropriate metrics on a numerical benchmark, which is also used for comparing AsBeC with some effective ABC variants and other derivative-free algorithms. In addition, the presented algorithm is validated on two recent benchmarks adopted for competitions in international conferences. Results show remarkable competitiveness and robustness for AsBeC. version:2
arxiv-1605-00507 | Methods for Sparse and Low-Rank Recovery under Simplex Constraints | http://arxiv.org/abs/1605.00507 | id:1605.00507 author:Ping Li, Syama Sundar Rangapuram, Martin Slawski category:stat.ME cs.LG  published:2016-05-02 summary:The de-facto standard approach of promoting sparsity by means of $\ell_1$-regularization becomes ineffective in the presence of simplex constraints, i.e.,~the target is known to have non-negative entries summing up to a given constant. The situation is analogous for the use of nuclear norm regularization for low-rank recovery of Hermitian positive semidefinite matrices with given trace. In the present paper, we discuss several strategies to deal with this situation, from simple to more complex. As a starting point, we consider empirical risk minimization (ERM). It follows from existing theory that ERM enjoys better theoretical properties w.r.t.~prediction and $\ell_2$-estimation error than $\ell_1$-regularization. In light of this, we argue that ERM combined with a subsequent sparsification step like thresholding is superior to the heuristic of using $\ell_1$-regularization after dropping the sum constraint and subsequent normalization. At the next level, we show that any sparsity-promoting regularizer under simplex constraints cannot be convex. A novel sparsity-promoting regularization scheme based on the inverse or negative of the squared $\ell_2$-norm is proposed, which avoids shortcomings of various alternative methods from the literature. Our approach naturally extends to Hermitian positive semidefinite matrices with given trace. Numerical studies concerning compressed sensing, sparse mixture density estimation, portfolio optimization and quantum state tomography are used to illustrate the key points of the paper. version:1
arxiv-1602-02070 | Compressive PCA for Low-Rank Matrices on Graphs | http://arxiv.org/abs/1602.02070 | id:1602.02070 author:Nauman Shahid, Nathanael Perraudin, Gilles Puy, Pierre Vandergheynst category:cs.LG  published:2016-02-05 summary:We introduce a novel framework for an approximate recovery of data matrices which are low-rank on graphs, from sampled measurements. The rows and columns of such matrices belong to the span of the first few eigenvectors of the graphs constructed between their rows and columns. We leverage this property to recover the non-linear low-rank structures efficiently from sampled data measurements, with a low cost (linear in $n$). First, a Resrtricted Isometry Property (RIP) condition is introduced for efficient uniform sampling of the rows and columns of such matrices based on the cumulative coherence of graph eigenvectors. Secondly, a state-of-the-art fast low-rank recovery method is suggested for the sampled data. Finally, several efficient, parallel and parameter-free decoders are presented along with their theoretical analysis for decoding the low-rank and cluster indicators for the full data matrix. Thus, we overcome the computational limitations of the standard \textit{linear} low-rank recovery methods for big datasets. Our method can also be seen as a major step towards efficient recovery of non-linear low-rank structures. On a single core machine, our method gains a speed up of $p^2/k$ over Robust PCA, where $k \ll p$ is the subspace dimension. Numerically, we can recover a low-rank matrix of size $10304 \times 1000$ in 15 secs, which is 100 times faster than Robust PCA. version:3
arxiv-1510-00143 | Fast Single Image Super-Resolution | http://arxiv.org/abs/1510.00143 | id:1510.00143 author:Ningning Zhao, Qi Wei, Adrian Basarab, Nicolas Dobigeon, Denis Kouame, Jean-Yves Tourneret category:cs.CV  published:2015-10-01 summary:This paper addresses the problem of single image super-resolution (SR), which consists of recovering a high resolution image from its blurred, decimated and noisy version. The existing algorithms for single image SR use different strategies to handle the decimation and blurring operators. In addition to the traditional first-order gradient methods, recent techniques investigate splitting-based methods dividing the SR problem into up-sampling and deconvolution steps that can be easily solved. Instead of following this splitting strategy, we propose to deal with the decimation and blurring operators simultaneously by taking advantage of their particular properties in the frequency domain, leading to a new fast SR approach. Specifically, an analytical solution can be obtained and implemented efficiently for the Gaussian prior or any other regularization that can be formulated into an $\ell_2$-regularized quadratic model, i.e., an $\ell_2$-$\ell_2$ optimization problem. Furthermore, the flexibility of the proposed SR scheme is shown through the use of various priors/regularizations, ranging from generic image priors to learning-based approaches. In the case of non-Gaussian priors, we show how the analytical solution derived from the Gaussian case can be embedded intotraditional splitting frameworks, allowing the computation cost of existing algorithms to be decreased significantly. Simulation results conducted on several images with different priors illustrate the effectiveness of our fast SR approach compared with the existing techniques. version:3
arxiv-1605-00475 | Rolling Shutter Camera Relative Pose: Generalized Epipolar Geometry | http://arxiv.org/abs/1605.00475 | id:1605.00475 author:Yuchao Dai, Hongdong Li, Laurent Kneip category:cs.CV  published:2016-05-02 summary:The vast majority of modern consumer-grade cameras employ a rolling shutter mechanism. In dynamic geometric computer vision applications such as visual SLAM, the so-called rolling shutter effect therefore needs to be properly taken into account. A dedicated relative pose solver appears to be the first problem to solve, as it is of eminent importance to bootstrap any derivation of multi-view geometry. However, despite its significance, it has received inadequate attention to date. This paper presents a detailed investigation of the geometry of the rolling shutter relative pose problem. We introduce the rolling shutter essential matrix, and establish its link to existing models such as the push-broom cameras, summarized in a clean hierarchy of multi-perspective cameras. The generalization of well-established concepts from epipolar geometry is completed by a definition of the Sampson distance in the rolling shutter case. The work is concluded with a careful investigation of the introduced epipolar geometry for rolling shutter cameras on several dedicated benchmarks. version:1
arxiv-1605-00459 | Multi30K: Multilingual English-German Image Descriptions | http://arxiv.org/abs/1605.00459 | id:1605.00459 author:Desmond Elliott, Stella Frank, Khalil Sima'an, Lucia Specia category:cs.CL cs.CV  published:2016-05-02 summary:We introduce the Multi30K dataset to stimulate multilingual multimodal research. Recent advances in image description have been demonstrated on English-language datasets almost exclusively, but image description should not be limited to English. This dataset extends the Flickr30K dataset with i) German translations created by professional translators over a subset of the English descriptions, and ii) descriptions crowdsourced independently of the original English descriptions. We outline how the data can be used for multilingual image description and multimodal machine translation, but we anticipate the data will be useful for a broader range of tasks. version:1
arxiv-1605-00452 | Fourier Analysis and q-Gaussian Functions: Analytical and Numerical Results | http://arxiv.org/abs/1605.00452 | id:1605.00452 author:Paulo Sérgio Silva Rodrigues, Gilson Antonio Giraldi category:cs.CV  published:2016-05-02 summary:It is a consensus in signal processing that the Gaussian kernel and its partial derivatives enable the development of robust algorithms for feature detection. Fourier analysis and convolution theory have central role in such development. In this paper we collect theoretical elements to follow this avenue but using the q-Gaussian kernel that is a nonextensive generalization of the Gaussian one. Firstly, we review some theoretical elements behind the one-dimensional q-Gaussian and its Fourier transform. Then, we consider the two-dimensional q-Gaussian and we highlight the issues behind its analytical Fourier transform computation. We analyze the q-Gaussian kernel in the space and Fourier domains using the concepts of space window, cut-off frequency, and the Heisenberg inequality. version:1
arxiv-1605-00420 | An Enhanced Harmony Search Method for Bangla Handwritten Character Recognition Using Region Sampling | http://arxiv.org/abs/1605.00420 | id:1605.00420 author:Ritesh Sarkhel, Amit K Saha, Nibaran Das category:cs.CV  published:2016-05-02 summary:Identification of minimum number of local regions of a handwritten character image, containing well-defined discriminating features which are sufficient for a minimal but complete description of the character is a challenging task. A new region selection technique based on the idea of an enhanced Harmony Search methodology has been proposed here. The powerful framework of Harmony Search has been utilized to search the region space and detect only the most informative regions for correctly recognizing the handwritten character. The proposed method has been tested on handwritten samples of Bangla Basic, Compound and mixed (Basic and Compound characters) characters separately with SVM based classifier using a longest run based feature-set obtained from the image subregions formed by a CG based quad-tree partitioning approach. Applying this methodology on the above mentioned three types of datasets, respectively 43.75%, 12.5% and 37.5% gains have been achieved in terms of region reduction and 2.3%, 0.6% and 1.2% gains have been achieved in terms of recognition accuracy. The results show a sizeable reduction in the minimal number of descriptive regions as well a significant increase in recognition accuracy for all the datasets using the proposed technique. Thus the time and cost related to feature extraction is decreased without dampening the corresponding recognition accuracy. version:1
arxiv-1605-00404 | Simple2Complex: Global Optimization by Gradient Descent | http://arxiv.org/abs/1605.00404 | id:1605.00404 author:Ming Li category:cs.LG cs.NE  published:2016-05-02 summary:A method named simple2complex for modeling and training deep neural networks is proposed. Simple2complex train deep neural networks by smoothly adding more and more layers to the shallow networks, as the learning procedure going on, the network is just like growing. Compared with learning by end2end, simple2complex is with less possibility trapping into local minimal, namely, owning ability for global optimization. Cifar10 is used for verifying the superiority of simple2complex. version:1
arxiv-1602-05980 | Revise Saturated Activation Functions | http://arxiv.org/abs/1602.05980 | id:1602.05980 author:Bing Xu, Ruitong Huang, Mu Li category:cs.LG  published:2016-02-18 summary:In this paper, we revise two commonly used saturated functions, the logistic sigmoid and the hyperbolic tangent (tanh). We point out that, besides the well-known non-zero centered property, slope of the activation function near the origin is another possible reason making training deep networks with the logistic function difficult to train. We demonstrate that, with proper rescaling, the logistic sigmoid achieves comparable results with tanh. Then following the same argument, we improve tahn by penalizing in the negative part. We show that "penalized tanh" is comparable and even outperforms the state-of-the-art non-saturated functions including ReLU and leaky ReLU on deep convolution neural networks. Our results contradict to the conclusion of previous works that the saturation property causes the slow convergence. It suggests further investigation is necessary to better understand activation functions in deep architectures. version:2
arxiv-1605-00392 | Revisiting Human Action Recognition: Personalization vs. Generalization | http://arxiv.org/abs/1605.00392 | id:1605.00392 author:Andrea Zunino, Jacopo Cavazza, Vittorio Murino category:cs.CV  published:2016-05-02 summary:By thoroughly revisiting the classic human action recognition paradigm, this paper aims at proposing a new approach for the design of effective action classification systems. Taking as testbed publicly available three-dimensional (MoCap) action/activity datasets, we analyzed and validated different training/testing strategies. In particular, considering that each human action in the datasets is performed several times by different subjects, we were able to precisely quantify the effect of inter- and intra-subject variability, so as to figure out the impact of several learning approaches in terms of classification performance. The net result is that standard testing strategies consisting in cross-validating the algorithm using typical splits of the data (holdout, k-fold, or one-subject-out) is always outperformed by a "personalization" strategy which learns how a subject is performing an action. In other words, it is advantageous to customize (i.e., personalize) the method to learn the actions carried out by each subject, rather than trying to generalize the actions executions across subjects. Consequently, we finally propose an action recognition framework consisting of a two-stage classification approach where, given a test action, the subject is first identified before the actual recognition of the action takes place. Despite the basic, off-the-shelf descriptors and standard classifiers adopted, we noted a relevant increase in performance with respect to standard state-of-the-art algorithms, so motivating the usage of personalized approaches for designing effective action recognition systems. version:1
arxiv-1605-00391 | Recovery of non-linear cause-effect relationships from linearly mixed neuroimaging data | http://arxiv.org/abs/1605.00391 | id:1605.00391 author:Sebastian Weichwald, Arthur Gretton, Bernhard Schölkopf, Moritz Grosse-Wentrup category:stat.ME cs.LG stat.AP stat.ML  published:2016-05-02 summary:Causal inference concerns the identification of cause-effect relationships between variables. However, often only linear combinations of variables constitute meaningful causal variables. For example, recovering the signal of a cortical source from electroencephalography requires a well-tuned combination of signals recorded at multiple electrodes. We recently introduced the MERLiN (Mixture Effect Recovery in Linear Networks) algorithm that is able to recover, from an observed linear mixture, a causal variable that is a linear effect of another given variable. Here we relax the assumption of this cause-effect relationship being linear and present an extended algorithm that can pick up non-linear cause-effect relationships. Thus, the main contribution is an algorithm (and ready to use code) that has broader applicability and allows for a richer model class. Furthermore, a comparative analysis indicates that the assumption of linear cause-effect relationships is not restrictive in analysing electroencephalographic data. version:1
arxiv-1605-00366 | Compression Artifacts Removal Using Convolutional Neural Networks | http://arxiv.org/abs/1605.00366 | id:1605.00366 author:Pavel Svoboda, Michal Hradis, David Barina, Pavel Zemcik category:cs.CV  published:2016-05-02 summary:This paper shows that it is possible to train large and deep convolutional neural networks (CNN) for JPEG compression artifacts reduction, and that such networks can provide significantly better reconstruction quality compared to previously used smaller networks as well as to any other state-of-the-art methods. We were able to train networks with 8 layers in a single step and in relatively short time by combining residual learning, skip architecture, and symmetric weight initialization. We provide further insights into convolution networks for JPEG artifact reduction by evaluating three different objectives, generalization with respect to training dataset size, and generalization with respect to JPEG quality level. version:1
arxiv-1605-00355 | Contrastive Structured Anomaly Detection for Gaussian Graphical Models | http://arxiv.org/abs/1605.00355 | id:1605.00355 author:Abhinav Maurya, Mark Cheung category:stat.ML  published:2016-05-02 summary:Gaussian graphical models (GGMs) are probabilistic tools of choice for analyzing conditional dependencies between variables in complex systems. Finding changepoints in the structural evolution of a GGM is therefore essential to detecting anomalies in the underlying system modeled by the GGM. In order to detect structural anomalies in a GGM, we consider the problem of estimating changes in the precision matrix of the corresponding Gaussian distribution. We take a two-step approach to solving this problem:- (i) estimating a background precision matrix using system observations from the past without any anomalies, and (ii) estimating a foreground precision matrix using a sliding temporal window during anomaly monitoring. Our primary contribution is in estimating the foreground precision using a novel contrastive inverse covariance estimation procedure. In order to accurately learn only the structural changes to the GGM, we maximize a penalized log-likelihood where the penalty is the $l_1$ norm of difference between the foreground precision being estimated and the already learned background precision. We modify the alternating direction method of multipliers (ADMM) algorithm for sparse inverse covariance estimation to perform contrastive estimation of the foreground precision matrix. Our results on simulated GGM data show significant improvement in precision and recall for detecting structural changes to the GGM, compared to a non-contrastive sliding window baseline. version:1
arxiv-1511-05622 | Predicting distributions with Linearizing Belief Networks | http://arxiv.org/abs/1511.05622 | id:1511.05622 author:Yann N. Dauphin, David Grangier category:cs.LG cs.CV  published:2015-11-17 summary:Conditional belief networks introduce stochastic binary variables in neural networks. Contrary to a classical neural network, a belief network can predict more than the expected value of the output $Y$ given the input $X$. It can predict a distribution of outputs $Y$ which is useful when an input can admit multiple outputs whose average is not necessarily a valid answer. Such networks are particularly relevant to inverse problems such as image prediction for denoising, or text to speech. However, traditional sigmoid belief networks are hard to train and are not suited to continuous problems. This work introduces a new family of networks called linearizing belief nets or LBNs. A LBN decomposes into a deep linear network where each linear unit can be turned on or off by non-deterministic binary latent units. It is a universal approximator of real-valued conditional distributions and can be trained using gradient descent. Moreover, the linear pathways efficiently propagate continuous information and they act as multiplicative skip-connections that help optimization by removing gradient diffusion. This yields a model which trains efficiently and improves the state-of-the-art on image denoising and facial expression generation with the Toronto faces dataset. version:4
arxiv-1206-2068 | Revolvable Indoor Panoramas Using a Rectified Azimuthal Projection | http://arxiv.org/abs/1206.2068 | id:1206.2068 author:Chamberlain Fong category:cs.CV math.DG  published:2012-06-10 summary:We present an algorithm for converting an indoor spherical panorama into a photograph with a simulated overhead view. The resulting image will have an extremely wide field of view covering up to 4{\pi} steradians of the spherical panorama. We argue that our method complements the stereographic projection commonly used in the "little planet" effect. The stereographic projection works well in creating little planets of outdoor scenes; whereas our method is a well-suited counterpart for indoor scenes. The main innovation of our method is the introduction of a novel azimuthal map projection that can smoothly blend between the stereographic projection and the Lambert azimuthal equal-area projection. Our projection has an adjustable parameter that allows one to control and compromise between distortions in shape and distortions in size within the projected panorama. This extra control parameter gives our projection the ability to produce superior results over the stereographic projection. version:4
arxiv-1605-00329 | Some Insights into the Geometry and Training of Neural Networks | http://arxiv.org/abs/1605.00329 | id:1605.00329 author:Ewout van den Berg category:cs.LG  published:2016-05-02 summary:Neural networks have been successfully used for classification tasks in a rapidly growing number of practical applications. Despite their popularity and widespread use, there are still many aspects of training and classification that are not well understood. In this paper we aim to provide some new insights into training and classification by analyzing neural networks from a feature-space perspective. We review and explain the formation of decision regions and study some of their combinatorial aspects. We place a particular emphasis on the connections between the neural network weight and bias terms and properties of decision boundaries and other regions that exhibit varying levels of classification confidence. We show how the error backpropagates in these regions and emphasize the important role they have in the formation of gradients. These findings expose the connections between scaling of the weight parameters and the density of the training samples. This sheds more light on the vanishing gradient problem, explains the need for regularization, and suggests an approach for subsampling training data to improve performance. version:1
arxiv-1411-7564 | Large-scale Binary Quadratic Optimization Using Semidefinite Relaxation and Applications | http://arxiv.org/abs/1411.7564 | id:1411.7564 author:Peng Wang, Chunhua Shen, Anton van den Hengel, Philip H. S. Torr category:cs.CV  published:2014-11-27 summary:In computer vision, many problems such as image segmentation, pixel labelling, and scene parsing can be formulated as binary quadratic programs (BQPs). For submodular problems, cuts based methods can be employed to efficiently solve large-scale problems. However, general nonsubmodular problems are significantly more challenging to solve. Finding a solution when the problem is of large size to be of practical interest, however, typically requires relaxation. Two standard relaxation methods are widely used for solving general BQPs--spectral methods and semidefinite programming (SDP), each with their own advantages and disadvantages. Spectral relaxation is simple and easy to implement, but its bound is loose. Semidefinite relaxation has a tighter bound, but its computational complexity is high, especially for large scale problems. In this work, we present a new SDP formulation for BQPs, with two desirable properties. First, it has a similar relaxation bound to conventional SDP formulations. Second, compared with conventional SDP methods, the new SDP formulation leads to a significantly more efficient and scalable dual optimization approach, which has the same degree of complexity as spectral methods. We then propose two solvers, namely, quasi-Newton and smoothing Newton methods, for the dual problem. Both of them are significantly more efficiently than standard interior-point methods. In practice, the smoothing Newton solver is faster than the quasi-Newton solver for dense or medium-sized problems, while the quasi-Newton solver is preferable for large sparse/structured problems. Our experiments on a few computer vision applications including clustering, image segmentation, co-segmentation and registration show the potential of our SDP formulation for solving large-scale BQPs. version:4
arxiv-1605-00324 | Dominant Codewords Selection with Topic Model for Action Recognition | http://arxiv.org/abs/1605.00324 | id:1605.00324 author:Hirokatsu Kataoka, Masaki Hayashi, Kenji Iwata, Yutaka Satoh, Yoshimitsu Aoki, Slobodan Ilic category:cs.CV  published:2016-05-01 summary:In this paper, we propose a framework for recognizing human activities that uses only in-topic dominant codewords and a mixture of intertopic vectors. Latent Dirichlet allocation (LDA) is used to develop approximations of human motion primitives; these are mid-level representations, and they adaptively integrate dominant vectors when classifying human activities. In LDA topic modeling, action videos (documents) are represented by a bag-of-words (input from a dictionary), and these are based on improved dense trajectories. The output topics correspond to human motion primitives, such as finger moving or subtle leg motion. We eliminate the impurities, such as missed tracking or changing light conditions, in each motion primitive. The assembled vector of motion primitives is an improved representation of the action. We demonstrate our method on four different datasets. version:1
arxiv-1605-00316 | Directional Statistics in Machine Learning: a Brief Review | http://arxiv.org/abs/1605.00316 | id:1605.00316 author:Suvrit Sra category:stat.ML  published:2016-05-01 summary:The modern data analyst must cope with data encoded in various forms, vectors, matrices, strings, graphs, or more. Consequently, statistical and machine learning models tailored to different data encodings are important. We focus on data encoded as normalized vectors, so that their "direction" is more important than their magnitude. Specifically, we consider high-dimensional vectors that lie either on the surface of the unit hypersphere or on the real projective plane. For such data, we briefly review common mathematical models prevalent in machine learning, while also outlining some technical aspects, software, applications, and open mathematical challenges. version:1
arxiv-1605-00286 | Multidimensional Scaling on Multiple Input Distance Matrices | http://arxiv.org/abs/1605.00286 | id:1605.00286 author:Song Bai, Xiang Bai, Longin Jan Latecki, Qi Tian category:cs.CV  published:2016-05-01 summary:Multidimensional Scaling (MDS) is a classic technique that seeks vectorial representations for data points, given the pairwise distances between them. However, in recent years, data are usually collected from diverse sources or have multiple heterogeneous representations. How to do multidimensional scaling on multiple input distance matrices is still unsolved to our best knowledge. In this paper, we first define this new task formally. Then, we propose a new algorithm called Multi-View Multidimensional Scaling (MVMDS) by considering each input distance matrix as one view. Our algorithm is able to learn the weights of views (i.e., distance matrices) automatically by exploring the consensus information and complementary nature of views. Experimental results on synthetic as well as real datasets demonstrate the effectiveness of MVMDS. We hope that our work encourages a wider consideration in many domains where MDS is needed. version:1
arxiv-1511-04690 | Robust Elastic Net Regression | http://arxiv.org/abs/1511.04690 | id:1511.04690 author:Weiyang Liu, Rongmei Lin, Meng Yang category:cs.LG stat.ML  published:2015-11-15 summary:We propose a robust elastic net (REN) model for high-dimensional sparse regression and give its performance guarantees (both the statistical error bound and the optimization bound). A simple idea of trimming the inner product is applied to the elastic net model. Specifically, we robustify the covariance matrix by trimming the inner product based on the intuition that the trimmed inner product can not be significant affected by a bounded number of arbitrarily corrupted points (outliers). The REN model can also derive two interesting special cases: robust Lasso and robust soft thresholding. Comprehensive experimental results show that the robustness of the proposed model consistently outperforms the original elastic net and matches the performance guarantees nicely. version:2
arxiv-1605-00278 | Particle Smoothing for Hidden Diffusion Processes: Adaptive Path Integral Smoother | http://arxiv.org/abs/1605.00278 | id:1605.00278 author:H. -Ch. Ruiz, H. J. Kappen category:cs.LG stat.CO  published:2016-05-01 summary:Particle smoothing methods are used for inference of stochastic processes based on noisy observations. Typically, the estimation of the marginal posterior distribution given all observations is cumbersome and computational intensive. In this paper, we propose a simple algorithm based on path integral control theory to estimate the smoothing distribution of continuous-time diffusion processes with partial observations. In particular, we use an adaptive importance sampling method to improve the effective sampling size of the posterior over processes given the observations and the reliability of the estimation of the marginals. This is achieved by estimating a feedback controller to sample efficiently from the joint smoothing distributions. We compare the results with estimations obtained from the standard Forward Filter/Backward Simulator for two diffusion processes of different complexity. We show that the proposed method gives more reliable estimations than the standard FFBSi when the smoothing distribution is poorly represented by the filter distribution. version:1
arxiv-1503-02619 | MODS: Fast and Robust Method for Two-View Matching | http://arxiv.org/abs/1503.02619 | id:1503.02619 author:Dmytro Mishkin, Jiri Matas, Michal Perdoch category:cs.CV  published:2015-03-09 summary:A novel algorithm for wide-baseline matching called MODS - Matching On Demand with view Synthesis - is presented. The MODS algorithm is experimentally shown to solve a broader range of wide-baseline problems than the state of the art while being nearly as fast as standard matchers on simple problems. The apparent robustness vs. speed trade-off is finessed by the use of progressively more time-consuming feature detectors and by on-demand generation of synthesized images that is performed until a reliable estimate of geometry is obtained. We introduce an improved method for tentative correspondence selection, applicable both with and without view synthesis. A modification of the standard first to second nearest distance rule increases the number of correct matches by 5-20% at no additional computational cost. Performance of the MODS algorithm is evaluated on several standard publicly available datasets, and on a new set of geometrically challenging wide baseline problems that is made public together with the ground truth. Experiments show that the MODS outperforms the state-of-the-art in robustness and speed. Moreover, MODS performs well on other classes of difficult two-view problems like matching of images from different modalities, with wide temporal baseline or with significant lighting changes. version:2
arxiv-1605-00252 | Fast Rates with Unbounded Losses | http://arxiv.org/abs/1605.00252 | id:1605.00252 author:Peter D. Grünwald, Nishant A. Mehta category:cs.LG stat.ML  published:2016-05-01 summary:We present new excess risk bounds for randomized and deterministic estimators for general unbounded loss functions including log loss and squared loss. Our bounds are expressed in terms of the information complexity and hold under the recently introduced $v$-central condition, allowing for high-probability bounds, and its weakening, the $v$-pseudoprobability convexity condition, allowing for bounds in expectation even under heavy-tailed distributions. The parameter $v$ determines the achievable rate and is akin to the exponent in the Tsybakov margin condition and the Bernstein condition for bounded losses, which the $v$-conditions generalize; favorable $v$ in combination with small information complexity leads to $\tilde{O}(1/n)$ rates. While these fast rate conditions control the lower tail of the excess loss, the upper tail is controlled by a new type of witness-of-badness condition which allows us to connect the excess risk to a generalized R\'enyi divergence, generalizing previous results connecting Hellinger distance to KL divergence. version:1
arxiv-1605-00251 | A vector-contraction inequality for Rademacher complexities | http://arxiv.org/abs/1605.00251 | id:1605.00251 author:Andreas Maurer category:cs.LG stat.ML  published:2016-05-01 summary:The contraction inequality for Rademacher averages is extended to Lipschitz functions with vector-valued domains, and it is also shown that in the bounding expression the Rademacher variables can be replaced by arbitrary iid symmetric and sub-gaussian variables. Example applications are given for multi-category learning, K-means clustering and learning-to-learn. version:1
arxiv-1311-3959 | Clustering Markov Decision Processes For Continual Transfer | http://arxiv.org/abs/1311.3959 | id:1311.3959 author:M. M. Hassan Mahmud, Majd Hawasly, Benjamin Rosman, Subramanian Ramamoorthy category:cs.AI cs.LG 68T05 I.2.6  published:2013-11-15 summary:We present algorithms to effectively represent a set of Markov decision processes (MDPs), whose optimal policies have already been learned, by a smaller source subset for lifelong, policy-reuse-based transfer learning in reinforcement learning. This is necessary when the number of previous tasks is large and the cost of measuring similarity counteracts the benefit of transfer. The source subset forms an `$\epsilon$-net' over the original set of MDPs, in the sense that for each previous MDP $M_p$, there is a source $M^s$ whose optimal policy has $<\epsilon$ regret in $M_p$. Our contributions are as follows. We present EXP-3-Transfer, a principled policy-reuse algorithm that optimally reuses a given source policy set when learning for a new MDP. We present a framework to cluster the previous MDPs to extract a source subset. The framework consists of (i) a distance $d_V$ over MDPs to measure policy-based similarity between MDPs; (ii) a cost function $g(\cdot)$ that uses $d_V$ to measure how good a particular clustering is for generating useful source tasks for EXP-3-Transfer and (iii) a provably convergent algorithm, MHAV, for finding the optimal clustering. We validate our algorithms through experiments in a surveillance domain. version:4
arxiv-1605-00241 | Common-Description Learning: A Framework for Learning Algorithms and Generating Subproblems from Few Examples | http://arxiv.org/abs/1605.00241 | id:1605.00241 author:Basem G. El-Barashy category:cs.AI cs.LG  published:2016-05-01 summary:Current learning algorithms face many difficulties in learning simple patterns and using them to learn more complex ones. They also require more examples than humans do to learn the same pattern, assuming no prior knowledge. In this paper, a new learning framework is introduced that is called common-description learning (CDL). This framework has been tested on 32 small multi-task datasets, and the results show that it was able to learn complex algorithms from a few number of examples. The final model is perfectly interpretable and its depth depends on the question. What is meant by depth here is that whenever needed, the model learns to break down the problem into simpler subproblems and solves them using previously learned models. Finally, we explain the capabilities of our framework in discovering complex relations in data and how it can help in improving language understanding in machines. version:1
arxiv-1605-00223 | Text-mining the NeuroSynth corpus using Deep Boltzmann Machines | http://arxiv.org/abs/1605.00223 | id:1605.00223 author:Ricardo Pio Monti, Romy Lorenz, Robert Leech, Christoforos Anagnostopoulos, Giovanni Montana category:cs.LG cs.CL q-bio.NC stat.ML  published:2016-05-01 summary:Large-scale automated meta-analysis of neuroimaging data has recently established itself as an important tool in advancing our understanding of human brain function. This research has been pioneered by NeuroSynth, a database collecting both brain activation coordinates and associated text across a large cohort of neuroimaging research papers. One of the fundamental aspects of such meta-analysis is text-mining. To date, word counts and more sophisticated methods such as Latent Dirichlet Allocation have been proposed. In this work we present an unsupervised study of the NeuroSynth text corpus using Deep Boltzmann Machines (DBMs). The use of DBMs yields several advantages over the aforementioned methods, principal among which is the fact that it yields both word and document embeddings in a high-dimensional vector space. Such embeddings serve to facilitate the use of traditional machine learning techniques on the text corpus. The proposed DBM model is shown to learn embeddings with a clear semantic structure. version:1
arxiv-1512-06658 | Deep Learning for Surface Material Classification Using Haptic And Visual Information | http://arxiv.org/abs/1512.06658 | id:1512.06658 author:Haitian Zheng, Lu Fang, Mengqi Ji, Matti Strese, Yigitcan Ozer, Eckehard Steinbach category:cs.RO cs.CV cs.LG  published:2015-12-21 summary:When a user scratches a hand-held rigid tool across an object surface, an acceleration signal can be captured, which carries relevant information about the surface. More importantly, such a haptic signal is complementary to the visual appearance of the surface, which suggests the combination of both modalities for the recognition of the surface material. In this paper, we present a novel deep learning method dealing with the surface material classification problem based on a Fully Convolutional Network (FCN), which takes as input the aforementioned acceleration signal and a corresponding image of the surface texture. Compared to previous surface material classification solutions, which rely on a careful design of hand-crafted domain-specific features, our method automatically extracts discriminative features utilizing the advanced deep learning methodologies. Experiments performed on the TUM surface material database demonstrate that our method achieves state-of-the-art classification accuracy robustly and efficiently. version:2
arxiv-1605-00201 | Further properties of the forward-backward envelope with applications to difference-of-convex programming | http://arxiv.org/abs/1605.00201 | id:1605.00201 author:Tianxiang Liu, Ting Kei Pong category:math.OC stat.ML  published:2016-05-01 summary:In this paper, we further study the forward-backward envelope first introduced in [27] and [29] for problems whose objective is the sum of a proper closed convex function and a smooth possibly nonconvex function with Lipschitz continuous gradient. We derive sufficient conditions on the original problem for the corresponding forward-backward envelope to be a level-bounded and Kurdyka-{\L}ojasiewicz function with an exponent of $\frac12$; these results are important for the efficient minimization of the forward-backward envelope by classical optimization algorithms. In addition, we demonstrate how to minimize some difference-of-convex regularized least squares problems by minimizing a suitably constructed forward-backward envelope. Our preliminary numerical results on randomly generated instances of large-scale $\ell_{1-2}$ regularized least squares problems [36] illustrate that an implementation of this approach with a limited-memory BFGS scheme outperforms some standard first-order methods such as the nonmonotone proximal gradient method in [34]. version:1
arxiv-1503-06388 | Adaptive Concentration of Regression Trees, with Application to Random Forests | http://arxiv.org/abs/1503.06388 | id:1503.06388 author:Stefan Wager, Guenther Walther category:math.ST stat.ML stat.TH  published:2015-03-22 summary:We study the convergence of the predictive surface of regression trees and forests. To support our analysis we introduce a notion of adaptive concentration for regression trees. This approach breaks tree training into a model selection phase in which we pick the tree splits, followed by a model fitting phase where we find the best regression model consistent with these splits. We then show that the fitted regression tree concentrates around the optimal predictor with the same splits: as d and n get large, the discrepancy is with high probability bounded on the order of sqrt(log(d) log(n)/k) uniformly over the whole regression surface, where d is the dimension of the feature space, n is the number of training examples, and k is the minimum leaf size for each tree. We also provide rate-matching lower bounds for this adaptive concentration statement. From a practical perspective, our result enables us to prove consistency results for adaptively grown forests in high dimensions, and to carry out valid post-selection inference in the sense of Berk et al. [2013] for subgroups defined by tree leaves. version:3
arxiv-1605-00170 | Enforcing Template Representability and Temporal Consistency for Adaptive Sparse Tracking | http://arxiv.org/abs/1605.00170 | id:1605.00170 author:Xue Yang, Fei Han, Hua Wang, Hao Zhang category:cs.CV  published:2016-04-30 summary:Sparse representation has been widely studied in visual tracking, which has shown promising tracking performance. Despite a lot of progress, the visual tracking problem is still a challenging task due to appearance variations over time. In this paper, we propose a novel sparse tracking algorithm that well addresses temporal appearance changes, by enforcing template representability and temporal consistency (TRAC). By modeling temporal consistency, our algorithm addresses the issue of drifting away from a tracking target. By exploring the templates' long-term-short-term representability, the proposed method adaptively updates the dictionary using the most descriptive templates, which significantly improves the robustness to target appearance changes. We compare our TRAC algorithm against the state-of-the-art approaches on 12 challenging benchmark image sequences. Both qualitative and quantitative results demonstrate that our algorithm significantly outperforms previous state-of-the-art trackers. version:1
arxiv-1511-06390 | Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks | http://arxiv.org/abs/1511.06390 | id:1511.06390 author:Jost Tobias Springenberg category:stat.ML cs.LG  published:2015-11-19 summary:In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method - which we dub categorical generative adversarial networks (or CatGAN) - on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM). version:2
arxiv-1605-00164 | Look-ahead before you leap: end-to-end active recognition by forecasting the effect of motion | http://arxiv.org/abs/1605.00164 | id:1605.00164 author:Dinesh Jayaraman, Kristen Grauman category:cs.CV cs.AI cs.LG cs.RO  published:2016-04-30 summary:Visual recognition systems mounted on autonomous moving agents face the challenge of unconstrained data, but simultaneously have the opportunity to improve their performance by moving to acquire new views of test data. In this work, we first show how a recurrent neural network-based system may be trained to perform end-to-end learning of motion policies suited for the "active recognition" setting. Further, we hypothesize that active vision requires an agent to have the capacity to reason about the effects of its motions on its view of the world. To verify this hypothesis, we attempt to induce this capacity in our active recognition pipeline, by simultaneously learning to forecast the effects of the agent's motions on its internal representation of its cumulative knowledge obtained from all past views. Results across two challenging datasets confirm both that our end-to-end system successfully learns meaningful policies for active recognition, and that "learning to look ahead" further boosts recognition performance. version:1
arxiv-1605-00155 | Kernel Balancing: A flexible non-parametric weighting procedure for estimating causal effects | http://arxiv.org/abs/1605.00155 | id:1605.00155 author:Chad Hazlett category:stat.ME math.ST stat.AP stat.ML stat.TH  published:2016-04-30 summary:In the absence of unobserved confounders, matching and weighting methods are widely used to estimate causal quantities including the Average Treatment Effect on the Treated (ATT). Unfortunately, these methods do not necessarily achieve their goal of making the multivariate distribution of covariates for the control group identical to that of the treated, leaving some (potentially multivariate) functions of the covariates with different means between the two groups. When these "imbalanced" functions influence the non-treatment potential outcome, the conditioning on observed covariates fails, and ATT estimates may be biased. Kernel balancing, introduced here, targets a weaker requirement for unbiased ATT estimation, specifically, that the expected non-treatment potential outcome for the treatment and control groups are equal. The conditional expectation of the non-treatment potential outcome is assumed to fall in the space of functions associated with a choice of kernel, implying a set of basis functions in which this regression surface is linear. Weights are then chosen on the control units such that the treated and control group have equal means on these basis functions. As a result, the expectation of the non-treatment potential outcome must also be equal for the treated and control groups after weighting, allowing unbiased ATT estimation by subsequent difference in means or an outcome model using these weights. Moreover, the weights produced are (1) precisely those that equalize a particular kernel-based approximation of the multivariate distribution of covariates for the treated and control, and (2) equivalent to a form of stabilized inverse propensity score weighting, though it does not require assuming any model of the treatment assignment mechanism. An R package, KBAL, is provided to implement this approach. version:1
arxiv-1511-07122 | Multi-Scale Context Aggregation by Dilated Convolutions | http://arxiv.org/abs/1511.07122 | id:1511.07122 author:Fisher Yu, Vladlen Koltun category:cs.CV  published:2015-11-23 summary:State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy. version:3
arxiv-1605-00129 | 3D Keypoint Detection Based on Deep Neural Network with Sparse Autoencoder | http://arxiv.org/abs/1605.00129 | id:1605.00129 author:Xinyu Lin, Ce Zhu, Qian Zhang, Yipeng Liu category:cs.CV  published:2016-04-30 summary:Researchers have proposed various methods to extract 3D keypoints from the surface of 3D mesh models over the last decades, but most of them are based on geometric methods, which lack enough flexibility to meet the requirements for various applications. In this paper, we propose a new method on the basis of deep learning by formulating the 3D keypoint detection as a regression problem using deep neural network (DNN) with sparse autoencoder (SAE) as our regression model. Both local information and global information of a 3D mesh model in multi-scale space are fully utilized to detect whether a vertex is a keypoint or not. SAE can effectively extract the internal structure of these two kinds of information and formulate high-level features for them, which is beneficial to the regression model. Three SAEs are used to formulate the hidden layers of the DNN and then a logistic regression layer is trained to process the high-level features produced in the third SAE. Numerical experiments show that the proposed DNN based 3D keypoint detection algorithm outperforms current five state-of-the-art methods for various 3D mesh models. version:1
arxiv-1504-06779 | Computational Cost Reduction in Learned Transform Classifications | http://arxiv.org/abs/1504.06779 | id:1504.06779 author:Emerson Lopes Machado, Cristiano Jacques Miosso, Ricardo von Borries, Murilo Coutinho, Pedro de Azevedo Berger, Thiago Marques, Ricardo Pezzuol Jacobi category:cs.CV stat.ML  published:2015-04-26 summary:We present a theoretical analysis and empirical evaluations of a novel set of techniques for computational cost reduction of classifiers that are based on learned transform and soft-threshold. By modifying optimization procedures for dictionary and classifier training, as well as the resulting dictionary entries, our techniques allow to reduce the bit precision and to replace each floating-point multiplication by a single integer bit shift. We also show how the optimization algorithms in some dictionary training methods can be modified to penalize higher-energy dictionaries. We applied our techniques with the classifier Learning Algorithm for Soft-Thresholding, testing on the datasets used in its original paper. Our results indicate it is feasible to use solely sums and bit shifts of integers to classify at test time with a limited reduction of the classification accuracy. These low power operations are a valuable trade off in FPGA implementations as they increase the classification throughput while decrease both energy consumption and manufacturing cost. version:2
arxiv-1604-05170 | A Repeated Signal Difference for Recognising Patterns | http://arxiv.org/abs/1604.05170 | id:1604.05170 author:Kieran Greer category:cs.NE cs.AI q-bio.NC  published:2016-04-18 summary:This paper describes a new mechanism that might help with defining pattern sequences, by the fact that it can produce an upper bound on the ensemble value that can persistently oscillate with the actual values produced from each pattern. With every firing event, a node also receives an on/off feedback switch. If the node fires, then it sends a feedback result depending on the input signal strength. If the input signal is positive or larger, it can store 'on' switch feedback for the next iteration. If the signal is negative or smaller, it can store an 'off' switch feedback for the next iteration. If the node does not fire, then it does not affect the current feedback situation and receives the switch command produced by the last active pattern event for the same neuron. The upper bound therefore also represents the largest or most enclosing pattern set and the lower value is for the actual set of firing patterns. If the pattern sequence repeats, it will oscillate between the two values, allowing them to be recognised and measured more easily, over time. Tests show that changing the sequence ordering can also be measured. version:2
arxiv-1603-00285 | Kernel-based Tests for Joint Independence | http://arxiv.org/abs/1603.00285 | id:1603.00285 author:Niklas Pfister, Peter Bühlmann, Bernhard Schölkopf, Jonas Peters category:math.ST stat.ML stat.TH  published:2016-03-01 summary:We investigate the problem of testing whether $d$ random variables, which may or may not be continuous, are jointly (or mutually) independent. Our method builds on ideas of the two variable Hilbert-Schmidt independence criterion (HSIC) but allows for an arbitrary number of variables. We embed the $d$-dimensional joint distribution and the product of the marginals into a reproducing kernel Hilbert space and define the $d$-variable Hilbert-Schmidt independence criterion (dHSIC) as the squared distance between the embeddings. In the population case, the value of dHSIC is zero if and only if the $d$ variables are jointly independent, as long as the kernel is characteristic. Based on an empirical estimate of dHSIC, we define three different non-parametric hypothesis tests: a permutation test, a bootstrap test and a test based on a Gamma approximation. We prove that the permutation test achieves the significance level and that the bootstrap test achieves pointwise asymptotic significance level as well as pointwise asymptotic consistency (i.e., it is able to detect any type of fixed dependence in the large sample limit). The Gamma approximation does not come with these guarantees; however, it is computationally very fast and for small $d$, it performs well in practice. Finally, we apply the test to a problem in causal discovery. version:2
arxiv-1605-00097 | Application of artificial neural networks and genetic algorithms for crude fractional distillation process modeling | http://arxiv.org/abs/1605.00097 | id:1605.00097 author:Lukasz Pater category:cs.NE  published:2016-04-30 summary:This work presents the application of the artificial neural networks, trained and structurally optimized by genetic algorithms, for modeling of crude distillation process at PKN ORLEN S.A. refinery. Models for the main fractionator distillation column products were developed using historical data. Quality of the fractions were predicted based on several chosen process variables. The performance of the model was validated using test data. Neural networks used in companion with genetic algorithms proved that they can accurately predict fractions quality shifts, reproducing the results of the standard laboratory analysis. Simple knowledge extraction method from neural network model built was also performed. Genetic algorithms can be successfully utilized in efficient training of large neural networks and finding their optimal structures. version:1
arxiv-1605-00079 | Constructive neural network learning | http://arxiv.org/abs/1605.00079 | id:1605.00079 author:Shaobo Lin, Jinshan Zeng, Xiaoqin Zhang category:cs.LG  published:2016-04-30 summary:In this paper, we aim at developing scalable neural network-type learning systems. Motivated by the idea of "constructive neural networks" in approximation theory, we focus on "constructing" rather than "training" feed-forward neural networks (FNNs) for learning, and propose a novel FNNs learning system called the constructive feed-forward neural network (CFN). Theoretically, we prove that the proposed method not only overcomes the classical saturation problem for FNN approximation, but also reaches the optimal learning rate when the regression function is smooth, while the state-of-the-art learning rates established for traditional FNNs are only near optimal (up to a logarithmic factor). A series of numerical simulations are provided to show the efficiency and feasibility of CFN via comparing with the well-known regularized least squares (RLS) with Gaussian kernel and extreme learning machine (ELM). version:1
arxiv-1605-00075 | Deep Colorization | http://arxiv.org/abs/1605.00075 | id:1605.00075 author:Zezhou Cheng, Qingxiong Yang, Bin Sheng category:cs.CV  published:2016-04-30 summary:This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a high-quality fully-automatic colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However, patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. We further develop an adaptive image clustering technique to incorporate the global image information. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed. version:1
arxiv-1308-1269 | On b-bit min-wise hashing for large-scale regression and classification with sparse data | http://arxiv.org/abs/1308.1269 | id:1308.1269 author:Rajen D. Shah, Nicolai Meinshausen category:math.ST stat.ML stat.TH  published:2013-08-06 summary:Large-scale regression problems where both the number of variables, $p$, and the number of observations, $n$, may be large and in the order of millions or more, are becoming increasingly more common. Typically the data are sparse: only a fraction of a percent of the entries in the design matrix are non-zero. Nevertheless, often the only computationally feasible approach is to perform dimension reduction to obtain a new design matrix with far fewer columns, and then work with this compressed data. $b$-bit min-wise hashing (Li and Konig, 2011) is a promising dimension reduction scheme for sparse matrices. In this work we study the prediction error of procedures which perform regression in the new lower-dimensional space after applying the method. For both linear and logistic models we show that the average prediction error vanishes asymptotically as long as $q \ \beta^*\ _2^2 /n \rightarrow 0$, where $q$ is the average number of non-zero entries in each row of the design matrix and $\beta^*$ is the coefficient of the linear predictor. We also show that ordinary least squares or ridge regression applied to the reduced data in a sense amounts to a non-parametric regression and can in fact allow us fit more flexible models. We obtain non-asymptotic prediction error bounds for interaction models and for models where an unknown row normalisation must be applied before the signal is linear in the predictors. version:3
arxiv-1605-00064 | Higher Order Recurrent Neural Networks | http://arxiv.org/abs/1605.00064 | id:1605.00064 author:Rohollah Soltani, Hui Jiang category:cs.NE cs.AI  published:2016-04-30 summary:In this paper, we study novel neural network structures to better model long term dependency in sequential data. We propose to use more memory units to keep track of more preceding states in recurrent neural networks (RNNs), which are all recurrently fed to the hidden layers as feedback through different weighted paths. By extending the popular recurrent structure in RNNs, we provide the models with better short-term memory mechanism to learn long term dependency in sequences. Analogous to digital filters in signal processing, we call these structures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also be learned using the back-propagation through time method. HORNNs are generally applicable to a variety of sequence modelling tasks. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and English text8 data sets. Experimental results have shown that the proposed HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs. version:1
arxiv-1512-08512 | Visually Indicated Sounds | http://arxiv.org/abs/1512.08512 | id:1512.08512 author:Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward H. Adelson, William T. Freeman category:cs.CV cs.LG cs.SD  published:2015-12-28 summary:Objects make distinctive sounds when they are hit or scratched. These sounds reveal aspects of an object's material properties, as well as the actions that produced them. In this paper, we propose the task of predicting what sound an object makes when struck as a way of studying physical interactions within a visual scene. We present an algorithm that synthesizes sound from silent videos of people hitting and scratching objects with a drumstick. This algorithm uses a recurrent neural network to predict sound features from videos and then produces a waveform from these features with an example-based synthesis procedure. We show that the sounds predicted by our model are realistic enough to fool participants in a "real or fake" psychophysical experiment, and that they convey significant information about material properties and physical interactions. version:2
arxiv-1605-00057 | Distributed Cell Association for Energy Harvesting IoT Devices in Dense Small Cell Networks: A Mean-Field Multi-Armed Bandit Approach | http://arxiv.org/abs/1605.00057 | id:1605.00057 author:Setareh Maghsudi, Ekram Hossain category:cs.NI cs.LG cs.MA  published:2016-04-30 summary:The emerging Internet of Things (IoT)-driven ultra-dense small cell networks (UD-SCNs) will need to combat a variety of challenges. On one hand, massive number of devices sharing the limited wireless resources will render centralized control mechanisms infeasible due to the excessive cost of information acquisition and computations. On the other hand, to reduce energy consumption from fixed power grid and/or battery, many IoT devices may need to depend on the energy harvested from the ambient environment (e.g., from RF transmissions, environmental sources). However, due to the opportunistic nature of energy harvesting, this will introduce uncertainty in the network operation. In this article, we study the distributed cell association problem for energy harvesting IoT devices in UD-SCNs. After reviewing the state-of-the-art research on the cell association problem in small cell networks, we outline the major challenges for distributed cell association in IoT-driven UD-SCNs where the IoT devices will need to perform cell association in a distributed manner in presence of uncertainty (e.g., limited knowledge on channel/network) and limited computational capabilities. To this end, we propose an approach based on mean-field multi-armed bandit games to solve the uplink cell association problem for energy harvesting IoT devices in a UD-SCN. This approach is particularly suitable to analyze large multi-agent systems under uncertainty and lack of information. We provide some theoretical results as well as preliminary performance evaluation results for the proposed approach. version:1
arxiv-1605-00055 | DisturbLabel: Regularizing CNN on the Loss Layer | http://arxiv.org/abs/1605.00055 | id:1605.00055 author:Lingxi Xie, Jingdong Wang, Zhen Wei, Meng Wang, Qi Tian category:cs.CV  published:2016-04-30 summary:During a long period of time we are combating over-fitting in the CNN training process with model regularization, including weight decay, model averaging, data augmentation, etc. In this paper, we present DisturbLabel, an extremely simple algorithm which randomly replaces a part of labels as incorrect values in each iteration. Although it seems weird to intentionally generate incorrect training labels, we show that DisturbLabel prevents the network training from over-fitting by implicitly averaging over exponentially many networks which are trained with different label sets. To the best of our knowledge, DisturbLabel serves as the first work which adds noises on the loss layer. Meanwhile, DisturbLabel cooperates well with Dropout to provide complementary regularization functions. Experiments demonstrate competitive recognition results on several popular image recognition datasets. version:1
arxiv-1605-00052 | InterActive: Inter-Layer Activeness Propagation | http://arxiv.org/abs/1605.00052 | id:1605.00052 author:Lingxi Xie, Liang Zheng, Jingdong Wang, Alan Yuille, Qi Tian category:cs.CV  published:2016-04-30 summary:An increasing number of computer vision tasks can be tackled with deep features, which are the intermediate outputs of a pre-trained Convolutional Neural Network. Despite the astonishing performance, deep features extracted from low-level neurons are still below satisfaction, arguably because they cannot access the spatial context contained in the higher layers. In this paper, we present InterActive, a novel algorithm which computes the activeness of neurons and network connections. Activeness is propagated through a neural network in a top-down manner, carrying high-level context and improving the descriptive power of low-level and mid-level neurons. Visualization indicates that neuron activeness can be interpreted as spatial-weighted neuron responses. We achieve state-of-the-art classification performance on a wide range of image datasets. version:1
arxiv-1605-00042 | Improved Sparse Low-Rank Matrix Estimation | http://arxiv.org/abs/1605.00042 | id:1605.00042 author:Ankit Parekh, Ivan W. Selesnick category:math.OC cs.LG stat.ML  published:2016-04-29 summary:This paper addresses the problem of estimating a sparse low-rank matrix from its noisy observation. We propose a convex objective function consisting of a data-fidelity term and two parameterized non-convex penalty functions. The non-convex penalty functions induce sparsity of the singular values and the entries of the matrix to be estimated. We show how to set the parameters of the non-convex penalty functions, in order to ensure that the objective function is strictly convex. The proposed objective function better estimates sparse low-rank matrices than the convex method which utilizes the sum of the nuclear norm and the $\ell_1$ norm. We derive an algorithm (as an instance of ADMM) to solve the proposed problem, and guarantee its convergence provided the scalar augmented Lagrangian parameter is set appropriately. version:1
arxiv-1605-00031 | Deep Convolutional Neural Networks on Cartoon Functions | http://arxiv.org/abs/1605.00031 | id:1605.00031 author:Philipp Grohs, Thomas Wiatowski, Helmut Bölcskei category:cs.LG cs.CV math.NA stat.ML  published:2016-04-29 summary:Wiatowski and B\"olcskei, 2015, proved that deformation stability and vertical translation invariance of deep convolutional neural network-based feature extractors are guaranteed by the network structure per se rather than the specific convolution kernels and non-linearities. While the translation invariance result applies to square-integrable functions, the deformation stability bound holds for band-limited functions only. Many signals of practical relevance (such as natural images) exhibit, however, sharp and curved discontinuities and are hence not band-limited. The main contribution of this paper is a deformation stability result that takes these structural properties into account. Specifically, we establish deformation stability bounds for the class of cartoon functions introduced by Donoho, 2001. version:1
arxiv-1605-00029 | Multi-Atlas Segmentation using Partially Annotated Data: Methods and Annotation Strategies | http://arxiv.org/abs/1605.00029 | id:1605.00029 author:Lisa M. Koch, Martin Rajchl, Wenjia Bai, Christian F. Baumgartner, Tong Tong, Jonathan Passerat-Palmbach, Paul Aljabar, Daniel Rueckert category:cs.CV  published:2016-04-29 summary:Multi-atlas segmentation is a widely used tool in medical image analysis, providing robust and accurate results by learning from annotated atlas datasets. However, the availability of fully annotated atlas images for training is limited due to the time required for the labelling task. Segmentation methods requiring only a proportion of each atlas image to be labelled could therefore reduce the workload on expert raters tasked with annotating atlas images. To address this issue, we first re-examine the labelling problem common in many existing approaches and formulate its solution in terms of a Markov Random Field energy minimisation problem on a graph connecting atlases and the target image. This provides a unifying framework for multi-atlas segmentation. We then show how modifications in the graph configuration of the proposed framework enable the use of partially annotated atlas images and investigate different partial annotation strategies. The proposed method was evaluated on two Magnetic Resonance Imaging (MRI) datasets for hippocampal and cardiac segmentation. Experiments were performed aimed at (1) recreating existing segmentation techniques with the proposed framework and (2) demonstrating the potential of employing sparsely annotated atlas data for multi-atlas segmentation. version:1
arxiv-1605-00017 | deepMiRGene: Deep Neural Network based Precursor microRNA Prediction | http://arxiv.org/abs/1605.00017 | id:1605.00017 author:Seunghyun Park, Seonwoo Min, Hyunsoo Choi, Sungroh Yoon category:cs.LG q-bio.QM  published:2016-04-29 summary:Since microRNAs (miRNAs) play a crucial role in post-transcriptional gene regulation, miRNA identification is one of the most essential problems in computational biology. miRNAs are usually short in length ranging between 20 and 23 base pairs. It is thus often difficult to distinguish miRNA-encoding sequences from other non-coding RNAs and pseudo miRNAs that have a similar length, and most previous studies have recommended using precursor miRNAs instead of mature miRNAs for robust detection. A great number of conventional machine-learning-based classification methods have been proposed, but they often have the serious disadvantage of requiring manual feature engineering, and their performance is limited as well. In this paper, we propose a novel miRNA precursor prediction algorithm, deepMiRGene, based on recurrent neural networks, specifically long short-term memory networks. deepMiRGene automatically learns suitable features from the data themselves without manual feature engineering and constructs a model that can successfully reflect structural characteristics of precursor miRNAs. For the performance evaluation of our approach, we have employed several widely used evaluation metrics on three recent benchmark datasets and verified that deepMiRGene delivered comparable performance among the current state-of-the-art tools. version:1
arxiv-1604-08934 | An efficient and expressive similarity measure for relational clustering using neighbourhood trees | http://arxiv.org/abs/1604.08934 | id:1604.08934 author:Sebastijan Dumancic, Hendrik Blockeel category:stat.ML cs.AI cs.LG  published:2016-04-29 summary:Clustering is an underspecified task: there are no universal criteria for what makes a good clustering. This is especially true for relational data, where similarity can be based on the features of individuals, the relationships between them, or a mix of both. Existing methods for relational clustering have strong and often implicit biases in this respect. In this paper, we introduce a novel similarity measure for relational data. It is the first measure to incorporate a wide variety of types of similarity, including similarity of attributes, similarity of relational context, and proximity in a hypergraph. We experimentally evaluate how using this similarity affects the quality of clustering on very different types of datasets. The experiments demonstrate that (a) using this similarity in standard clustering methods consistently gives good results, whereas other measures work well only on datasets that match their bias; and (b) on most datasets, the novel similarity outperforms even the best among the existing ones. version:1
arxiv-1605-00003 | Predicting the direction of stock market prices using random forest | http://arxiv.org/abs/1605.00003 | id:1605.00003 author:Luckyson Khaidem, Snehanshu Saha, Sudeepa Roy Dey category:cs.LG cs.CE  published:2016-04-29 summary:Predicting trends in stock market prices has been an area of interest for researchers for many years due to its complex and dynamic nature. Intrinsic volatility in stock market across the globe makes the task of prediction challenging. Forecasting and diffusion modeling, although effective can't be the panacea to the diverse range of problems encountered in prediction, short-term or otherwise. Market risk, strongly correlated with forecasting errors, needs to be minimized to ensure minimal risk in investment. The authors propose to minimize forecasting error by treating the forecasting problem as a classification problem, a popular suite of algorithms in Machine learning. In this paper, we propose a novel way to minimize the risk of investment in stock market by predicting the returns of a stock using a class of powerful machine learning algorithms known as ensemble learning. Some of the technical indicators such as Relative Strength Index (RSI), stochastic oscillator etc are used as inputs to train our model. The learning model used is an ensemble of multiple decision trees. The algorithm is shown to outperform existing algo- rithms found in the literature. Out of Bag (OOB) error estimates have been found to be encouraging. Key Words: Random Forest Classifier, stock price forecasting, Exponential smoothing, feature extraction, OOB error and convergence. version:1
arxiv-1603-09302 | Confidence driven TGV fusion | http://arxiv.org/abs/1603.09302 | id:1603.09302 author:Valsamis Ntouskos, Fiora Pirri category:cs.CV  published:2016-03-30 summary:We introduce a novel model for spatially varying variational data fusion, driven by point-wise confidence values. The proposed model allows for the joint estimation of the data and the confidence values based on the spatial coherence of the data. We discuss the main properties of the introduced model as well as suitable algorithms for estimating the solution of the corresponding biconvex minimization problem and their convergence. The performance of the proposed model is evaluated considering the problem of depth image fusion by using both synthetic and real data from publicly available datasets. version:2
arxiv-1604-07866 | Learning by tracking: Siamese CNN for robust target association | http://arxiv.org/abs/1604.07866 | id:1604.07866 author:Laura Leal-Taixé, Cristian Canton-Ferrer, Konrad Schindler category:cs.LG cs.CV  published:2016-04-26 summary:This paper introduces a novel approach to the task of data association within the context of pedestrian tracking, by introducing a two-stage learning scheme to match pairs of detections. First, a Siamese convolutional neural network (CNN) is trained to learn descriptors encoding local spatio-temporal structures between the two input image patches, aggregating pixel values and optical flow information. Second, a set of contextual features derived from the position and size of the compared input patches are combined with the CNN output by means of a gradient boosting classifier to generate the final matching probability. This learning approach is validated by using a linear programming based multi-person tracker showing that even a simple and efficient tracker may outperform much more complex models when fed with our learned matching probabilities. Results on publicly available sequences show that our method meets state-of-the-art standards in multiple people tracking. version:2
arxiv-1604-08893 | Faster R-CNN Features for Instance Search | http://arxiv.org/abs/1604.08893 | id:1604.08893 author:Amaia Salvador, Xavier Giro-i-Nieto, Ferran Marques, Shin'ichi Satoh category:cs.CV  published:2016-04-29 summary:Image representations derived from pre-trained Convolutional Neural Networks (CNNs) have become the new state of the art in computer vision tasks such as instance retrieval. This work explores the suitability for instance retrieval of image- and region-wise representations pooled from an object detection CNN such as Faster R-CNN. We take advantage of the object proposals learned by a Region Proposal Network (RPN) and their associated CNN features to build an instance search pipeline composed of a first filtering stage followed by a spatial reranking. We further investigate the suitability of Faster R-CNN features when the network is fine-tuned for the same objects one wants to retrieve. We assess the performance of our proposed system with the Oxford Buildings 5k, Paris Buildings 6k and a subset of TRECVid Instance Search 2013, achieving competitive results. version:1
arxiv-1604-08880 | Deep, Convolutional, and Recurrent Models for Human Activity Recognition using Wearables | http://arxiv.org/abs/1604.08880 | id:1604.08880 author:Nils Y. Hammerla, Shane Halloran, Thomas Ploetz category:cs.LG cs.AI cs.HC stat.ML  published:2016-04-29 summary:Human activity recognition (HAR) in ubiquitous computing is beginning to adopt deep learning to substitute for well-established analysis techniques that rely on hand-crafted feature extraction and classification techniques. From these isolated applications of custom deep architectures it is, however, difficult to gain an overview of their suitability for problems ranging from the recognition of manipulative gestures to the segmentation and identification of physical activities like running or ascending stairs. In this paper we rigorously explore deep, convolutional, and recurrent approaches across three representative datasets that contain movement data captured with wearable sensors. We describe how to train recurrent approaches in this setting, introduce a novel regularisation approach, and illustrate how they outperform the state-of-the-art on a large benchmark dataset. Across thousands of recognition experiments with randomly sampled model configurations we investigate the suitability of each model for different tasks in HAR, explore the impact of hyperparameters using the fANOVA framework, and provide guidelines for the practitioner who wants to apply deep learning in their problem setting. version:1
arxiv-1604-08865 | Convolutional Neural Networks for Facial Attribute-based Active Authentication on Mobile Devices | http://arxiv.org/abs/1604.08865 | id:1604.08865 author:Pouya Samangouei, Rama Chellappa category:cs.CV  published:2016-04-29 summary:We present Deep Convolutional Neural Network (DCNN) architectures for the task of continuous authentication on mobile devices by learning intermediate features to reduce the complexity of the networks. The intermediate features for face images are attributes like gender, and hair color. We present a multi-task, part-based DCNN architecture for attributes detection are better than or comparable to state-of-the-art methods in terms of accuracy. As a byproduct of the proposed architecture, we explore the embedding space of the attributes extracted from different facial parts, such as mouth and eyes. We show that it is possible to discover new attributes by performing subspace clustering of the embedded features. Furthermore, through extensive experimentation, we show that the attribute features extracted by our method performs better than previously attribute-based authentication method and the baseline LBP method. Lastly, we deploy our architecture on a mobile device and demonstrate the effectiveness of the proposed method. version:1
arxiv-1604-08852 | Joint Sound Source Separation and Speaker Recognition | http://arxiv.org/abs/1604.08852 | id:1604.08852 author:Jeroen Zegers, Hugo Van hamme category:cs.SD cs.LG  published:2016-04-29 summary:Non-negative Matrix Factorization (NMF) has already been applied to learn speaker characterizations from single or non-simultaneous speech for speaker recognition applications. It is also known for its good performance in (blind) source separation for simultaneous speech. This paper explains how NMF can be used to jointly solve the two problems in a multichannel speaker recognizer for simultaneous speech. It is shown how state-of-the-art multichannel NMF for blind source separation can be easily extended to incorporate speaker recognition. Experiments on the CHiME corpus show that this method outperforms the sequential approach of first applying source separation, followed by speaker recognition that uses state-of-the-art i-vector techniques. version:1
arxiv-1604-08826 | Improved Dense Trajectory with Cross Streams | http://arxiv.org/abs/1604.08826 | id:1604.08826 author:Katsunori Ohnishi, Masatoshi Hidaka, Tatsuya Harada category:cs.CV  published:2016-04-29 summary:Improved dense trajectories (iDT) have shown great performance in action recognition, and their combination with the two-stream approach has achieved state-of-the-art performance. It is, however, difficult for iDT to completely remove background trajectories from video with camera shaking. Trajectories in less discriminative regions should be given modest weights in order to create more discriminative local descriptors for action recognition. In addition, the two-stream approach, which learns appearance and motion information separately, cannot focus on motion in important regions when extracting features from spatial convolutional layers of the appearance network, and vice versa. In order to address the above mentioned problems, we propose a new local descriptor that pools a new convolutional layer obtained from crossing two networks along iDT. This new descriptor is calculated by applying discriminative weights learned from one network to a convolutional layer of the other network. Our method has achieved state-of-the-art performance on ordinal action recognition datasets, 92.3% on UCF101, and 66.2% on HMDB51. version:1
arxiv-1604-08789 | Effective Backscatter Approximation for Photometry in Murky Water | http://arxiv.org/abs/1604.08789 | id:1604.08789 author:Chourmouzios Tsiotsios, Maria E. Angelopoulou, Andrew J. Davison, Tae-Kyun Kim category:cs.CV  published:2016-04-29 summary:Shading-based approaches like Photometric Stereo assume that the image formation model can be effectively optimized for the scene normals. However, in murky water this is a very challenging problem. The light from artificial sources is not only reflected by the scene but it is also scattered by the medium particles, yielding the backscatter component. Backscatter corresponds to a complex term with several unknown variables, and makes the problem of normal estimation hard. In this work, we show that instead of trying to optimize the complex backscatter model or use previous unrealistic simplifications, we can approximate the per-pixel backscatter signal directly from the captured images. Our method is based on the observation that backscatter is saturated beyond a certain distance, i.e. it becomes scene-depth independent, and finally corresponds to a smoothly varying signal which depends strongly on the light position with respect to each pixel. Our backscatter approximation method facilitates imaging and scene reconstruction in murky water when the illumination is artificial as in Photometric Stereo. Specifically, we show that it allows accurate scene normal estimation and offers potentials like single image restoration. We evaluate our approach using numerical simulations and real experiments within both the controlled environment of a big water-tank and real murky port-waters. version:1
arxiv-1604-00187 | PHOCNet: A Deep Convolutional Neural Network for Word Spotting in Handwritten Documents | http://arxiv.org/abs/1604.00187 | id:1604.00187 author:Sebastian Sudholt, Gernot A. Fink category:cs.CV  published:2016-04-01 summary:In recent years, deep convolutional neural networks have achieved state of the art performance in various computer vision task such as classification, detection or segmentation. Due to their outstanding performance, CNNs are more and more used in the field of document image analysis as well. In this work, we present a CNN architecture that is trained with the recently proposed PHOC representation. We show empirically that our CNN architecture is able to outperform state of the art results for various word spotting benchmarks while exhibiting short training and test times. version:2
arxiv-1604-08781 | Teaching natural language to computers | http://arxiv.org/abs/1604.08781 | id:1604.08781 author:Joseph Corneli, Miriam Corneli category:cs.CL cs.AI H.5.2; D.1.2; J.5  published:2016-04-29 summary:"Natural Language," whether spoken and attended to by humans, or processed and generated by computers, requires a series of structures and networks that reflect creative processes in semantic, syntactic, phonetic, linguistic, social, emotional, and cultural modules. Being able to produce novel and useful behavior following repeated practice gets to the root of both artificial intelligence and human language. This paper investigates current modalities involved in language-like applications that computers -- and programmers -- are engaged with, and seeks ways of fine tuning the questions we ask, to better account for context, self-awareness, and embodiment. version:1
arxiv-1604-08772 | Towards Conceptual Compression | http://arxiv.org/abs/1604.08772 | id:1604.08772 author:Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, Daan Wierstra category:stat.ML cs.CV cs.LG  published:2016-04-29 summary:We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'. version:1
arxiv-1604-08740 | MetaGrad: Faster Convergence Without Curvature in Online Convex Optimization | http://arxiv.org/abs/1604.08740 | id:1604.08740 author:Wouter M. Koolen, Tim van Erven category:cs.LG  published:2016-04-29 summary:In online convex optimization it is well known that objective functions with curvature are much easier than arbitrary convex functions. Here we show that the regret can be significantly reduced even without curvature, in cases where there is a stable optimum to converge to. More precisely, the regret of existing methods is determined by the norms of the encountered gradients, and matching worst-case performance lower bounds tell us that this cannot be improved uniformly. Yet we argue that this is a rather pessimistic assessment of the complexity of the problem. We introduce a new parameter-free algorithm, called MetaGrad, for which the gradient norms in the regret are scaled down by the distance to the (unknown) optimum. So when the optimum is reasonably stable over time, making the algorithm converge, this new scaling leads to orders of magnitude smaller regret even when the gradients themselves do not vanish. MetaGrad does not require any manual tuning, but instead tunes a learning rate parameter automatically for the data. Unlike all previous methods with provable guarantees, its learning rates are not monotonically decreasing over time, but instead are based on a novel aggregation technique. We provide two versions of MetaGrad. The first maintains a full covariance matrix to guarantee the sharpest bounds for problems where we can afford update time quadratic in the dimension. The second version maintains only the diagonal. Its linear cost in the dimension makes it suitable for large-scale problems. version:1
arxiv-1603-03369 | Summary Transfer: Exemplar-based Subset Selection for Video Summarization | http://arxiv.org/abs/1603.03369 | id:1603.03369 author:Ke Zhang, Wei-Lun Chao, Fei Sha, Kristen Grauman category:cs.CV  published:2016-03-10 summary:Video summarization has unprecedented importance to help us digest, browse, and search today's ever-growing video collections. We propose a novel subset selection technique that leverages supervision in the form of human-created summaries to perform automatic keyframe-based video summarization. The main idea is to nonparametrically transfer summary structures from annotated videos to unseen test videos. We show how to extend our method to exploit semantic side information about the video's category/genre to guide the transfer process by those training videos semantically consistent with the test input. We also show how to generalize our method to subshot-based summarization, which not only reduces computational costs but also provides more flexible ways of defining visual similarity across subshots spanning several frames. We conduct extensive evaluation on several benchmarks and demonstrate promising results, outperforming existing methods in several settings. version:3
arxiv-1604-08723 | Music transcription modelling and composition using deep learning | http://arxiv.org/abs/1604.08723 | id:1604.08723 author:Bob L. Sturm, João Felipe Santos, Oded Ben-Tal, Iryna Korshunova category:cs.SD cs.LG  published:2016-04-29 summary:We apply deep learning methods, specifically long short-term memory (LSTM) networks, to music transcription modelling and composition. We build and train LSTM networks using approximately 23,000 music transcriptions expressed with a high-level vocabulary (ABC notation), and use them to generate new transcriptions. Our practical aim is to create music transcription models useful in particular contexts of music composition. We present results from three perspectives: 1) at the population level, comparing descriptive statistics of the set of training transcriptions and generated transcriptions; 2) at the individual level, examining how a generated transcription reflects the conventions of a music practice in the training transcriptions (Celtic folk); 3) at the application level, using the system for idea generation in music composition. We make our datasets, software and sound examples open and available: \url{https://github.com/IraKorshunova/folk-rnn}. version:1
arxiv-1604-08716 | Learning Compact Structural Representations for Audio Events Using Regressor Banks | http://arxiv.org/abs/1604.08716 | id:1604.08716 author:Huy Phan, Marco Maass, Lars Hertel, Radoslaw Mazur, Ian McLoughlin, Alfred Mertins category:cs.SD cs.LG stat.ML  published:2016-04-29 summary:We introduce a new learned descriptor for audio signals which is efficient for event representation. The entries of the descriptor are produced by evaluating a set of regressors on the input signal. The regressors are class-specific and trained using the random regression forests framework. Given an input signal, each regressor estimates the onset and offset positions of the target event. The estimation confidence scores output by a regressor are then used to quantify how the target event aligns with the temporal structure of the corresponding category. Our proposed descriptor has two advantages. First, it is compact, i.e. the dimensionality of the descriptor is equal to the number of event classes. Second, we show that even simple linear classification models, trained on our descriptor, yield better accuracies on audio event classification task than not only the nonlinear baselines but also the state-of-the-art results. version:1
arxiv-1604-03265 | Volumetric and Multi-View CNNs for Object Classification on 3D Data | http://arxiv.org/abs/1604.03265 | id:1604.03265 author:Charles R. Qi, Hao Su, Matthias Niessner, Angela Dai, Mengyuan Yan, Leonidas J. Guibas category:cs.CV cs.AI  published:2016-04-12 summary:3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-the-art methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multi-resolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data. version:2
arxiv-1604-08697 | Sparse Generalized Eigenvalue Problem: Optimal Statistical Rates via Truncated Rayleigh Flow | http://arxiv.org/abs/1604.08697 | id:1604.08697 author:Kean Ming Tan, Zhaoran Wang, Han Liu, Tong Zhang category:stat.ML  published:2016-04-29 summary:Sparse generalized eigenvalue problem plays a pivotal role in a large family of high-dimensional learning tasks, including sparse Fisher's discriminant analysis, canonical correlation analysis, and sufficient dimension reduction. However, the theory of sparse generalized eigenvalue problem remains largely unexplored. In this paper, we exploit a non-convex optimization perspective to study this problem. In particular, we propose the truncated Rayleigh flow method (Rifle) to estimate the leading generalized eigenvector and show that it converges linearly to a solution with the optimal statistical rate of convergence. Our theory involves two key ingredients: (i) a new analysis of the gradient descent method on non-convex objective functions, as well as (ii) a fine-grained characterization of the evolution of sparsity patterns along the solution path. Thorough numerical studies are provided to back up our theory. Finally, we apply our proposed method in the context of sparse sufficient dimension reduction to two gene expression data sets. version:1
arxiv-1604-08685 | Single Image 3D Interpreter Network | http://arxiv.org/abs/1604.08685 | id:1604.08685 author:Jiajun Wu, Tianfan Xue, Joseph J. Lim, Yuandong Tian, Joshua B. Tenenbaum, Antonio Torralba, William T. Freeman category:cs.CV  published:2016-04-29 summary:Understanding 3D object structure from a single image is an important but difficult task in computer vision, mostly due to the lack of 3D object annotations in real images. Previous work tackles this problem by either solving an optimization task given 2D keypoint positions, or training on synthetic data with ground truth 3D information. In this work, we propose 3D INterpreter Network (3D-INN), an end-to-end framework which sequentially estimates 2D keypoint heatmaps and 3D object structure, trained on both real 2D-annotated images and synthetic 3D data. This is made possible mainly by two technical innovations. First, we propose a Projection Layer, which projects estimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3D structural parameters supervised by 2D annotations on real images. Second, heatmaps of keypoints serve as an intermediate representation connecting real and synthetic data, enabling 3D-INN to benefit from the variation and abundance of synthetic 3D objects, without suffering from the difference between the statistics of real and synthesized images due to imperfect rendering. The network achieves state-of-the-art performance on both 2D keypoint estimation and 3D structure recovery. We also show that the recovered 3D information can be used in other vision applications, such as 3D rendering and image retrieval. version:1
arxiv-1604-08683 | Top-push Video-based Person Re-identification | http://arxiv.org/abs/1604.08683 | id:1604.08683 author:Jinjie You, Ancong Wu, Xiang Li, Wei-Shi Zheng category:cs.CV  published:2016-04-29 summary:Most existing person re-identification (re-id) models focus on matching still person images across disjoint camera views. Since only limited information can be exploited from still images, it is hard (if not impossible) to overcome the occlusion, pose and camera-view change, and lighting variation problems. In comparison, video-based re-id methods can utilize extra space-time information, which contains much more rich cues for matching to overcome the mentioned problems. However, we find that when using video-based representation, some inter-class difference can be much more obscure than the one when using still-image based representation, because different people could not only have similar appearance but also have similar motions and actions which are hard to align. To solve this problem, we propose a top-push distance learning model (TDL), in which we integrate a top-push constrain for matching video features of persons. The top-push constraint enforces the optimization on top-rank matching in re-id, so as to make the matching model more effective towards selecting more discriminative features to distinguish different persons. Our experiments show that the proposed video-based re-id framework outperforms the state-of-the-art video-based re-id methods. version:1
arxiv-1511-06856 | Data-dependent Initializations of Convolutional Neural Networks | http://arxiv.org/abs/1511.06856 | id:1511.06856 author:Philipp Krähenbühl, Carl Doersch, Jeff Donahue, Trevor Darrell category:cs.CV cs.LG  published:2015-11-21 summary:Convolutional Neural Networks spread through computer vision like a wildfire, impacting almost all visual tasks imaginable. Despite this, few researchers dare to train their models from scratch. Most work builds on one of a handful of ImageNet pre-trained models, and fine-tunes or adapts these for specific tasks. This is in large part due to the difficulty of properly initializing these networks from scratch. A small miscalibration of the initial weights leads to vanishing or exploding gradients, as well as poor convergence properties. In this work we present a fast and simple data-dependent initialization procedure, that sets the weights of a network such that all units in the network train at roughly the same rate, avoiding vanishing or exploding gradients. Our initialization matches the current state-of-the-art unsupervised or self-supervised pre-training methods on standard computer vision tasks, such as image classification and object detection, while being roughly three orders of magnitude faster. When combined with pre-training methods, our initialization significantly outperforms prior work, narrowing the gap between supervised and unsupervised pre-training. version:2
arxiv-1604-08672 | Attention-Based Deep Distance Metric Learning for Aspect-Phrase Grouping | http://arxiv.org/abs/1604.08672 | id:1604.08672 author:Shufeng Xiong, Donghong Ji category:cs.CL  published:2016-04-29 summary:Aspect phrase grouping is an important task for aspect finding in aspect-level sentiment analysis and it is a challenging problem due to polysemy and its context dependency. In this paper we propose an Attention-based Deep Distance Metric Learning (ADDML) method, which is more beneficial for clustering by considering aspect phrase representation as well as context representation and their combination. First, we feed word embeddings of aspect phrases and its contexts into an attention-based network to learn feature representation of contexts. Then, both of aspect phrase embedding and context embedding as the input of a multilayer perceptron which is used to learn deep feature subspace, under which the distance of each intra-group pair is smaller and that of each inter-group pair is bigger, respectively. After obtaining the learned representations, we use K-means to cluster them. Experiments on four domain review datasets shows that the proposed method outperforms strong baseline methods. version:1
arxiv-1604-08671 | Deep Edge Guided Recurrent Residual Learning for Image Super-Resolution | http://arxiv.org/abs/1604.08671 | id:1604.08671 author:Wenhan Yang, Jiashi Feng, Jianchao Yang, Fang Zhao, Jiaying Liu, Zongming Guo, Shuicheng Yan category:cs.CV  published:2016-04-29 summary:In this work, we consider the image super-resolution (SR) problem. The main challenge of image SR is to recover high-frequency details of a low-resolution (LR) image that are important for human perception. To address this essentially ill-posed problem, we introduce a Deep Edge Guided REcurrent rEsidual~(DEGREE) network to progressively recover the high-frequency details. Different from most of existing methods that aim at predicting high-resolution (HR) images directly, DEGREE investigates an alternative route to recover the difference between a pair of LR and HR images by recurrent residual learning. DEGREE further augments the SR process with edge-preserving capability, namely the LR image and its edge map can jointly infer the sharp edge details of the HR image during the recurrent recovery process. To speed up its training convergence rate, by-pass connections across multiple layers of DEGREE are constructed. In addition, we offer an understanding on DEGREE from the view-point of sub-band frequency decomposition on image signal and experimentally demonstrate how DEGREE can recover different frequency bands separately. Extensive experiments on three benchmark datasets clearly demonstrate the superiority of DEGREE over well-established baselines and DEGREE also provides new state-of-the-arts on these datasets. version:1
arxiv-1604-08660 | Crowd Counting via Weighted VLAD on Dense Attribute Feature Maps | http://arxiv.org/abs/1604.08660 | id:1604.08660 author:Biyun Sheng, Chunhua Shen, Guosheng Lin, Jun Li, Wankou Yang, Changyin Sun category:cs.CV  published:2016-04-29 summary:Crowd counting is an important task in computer vision, which has many applications in video surveillance. Although the regression-based framework has achieved great improvements for crowd counting, how to improve the discriminative power of image representation is still an open problem. Conventional holistic features used in crowd counting often fail to capture semantic attributes and spatial cues of the image. In this paper, we propose integrating semantic information into learning locality-aware feature sets for accurate crowd counting. First, with the help of convolutional neural network (CNN), the original pixel space is mapped onto a dense attribute feature map, where each dimension of the pixel-wise feature indicates the probabilistic strength of a certain semantic class. Then, locality-aware features (LAF) built on the idea of spatial pyramids on neighboring patches are proposed to explore more spatial context and local information. Finally, the traditional VLAD encoding method is extended to a more generalized form in which diverse coefficient weights are taken into consideration. Experimental results validate the effectiveness of our presented method. version:1
arxiv-1603-00570 | Without-Replacement Sampling for Stochastic Gradient Methods: Convergence Results and Application to Distributed Optimization | http://arxiv.org/abs/1603.00570 | id:1603.00570 author:Ohad Shamir category:cs.LG math.OC stat.ML  published:2016-03-02 summary:Stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled with replacement. In practice, however, sampling without replacement is very common, easier to implement in many cases, and often performs better. In this paper, we provide competitive convergence guarantees for without-replacement sampling, under various scenarios, for three types of algorithms: Any algorithm with online regret guarantees, stochastic gradient descent, and SVRG. A useful application of our SVRG analysis is a nearly-optimal algorithm for regularized least squares in a distributed setting, in terms of both communication complexity and runtime complexity, when the data is randomly partitioned and the condition number can be as large as the data size (up to logarithmic factors). Our proof techniques combine ideas from stochastic optimization, adversarial online learning, and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems. version:2
arxiv-1604-08642 | On the representation and embedding of knowledge bases beyond binary relations | http://arxiv.org/abs/1604.08642 | id:1604.08642 author:Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, Richong Zhang category:cs.LG cs.AI  published:2016-04-28 summary:The models developed to date for knowledge base embedding are all based on the assumption that the relations contained in knowledge bases are binary. For the training and testing of these embedding models, multi-fold (or n-ary) relational data are converted to triples (e.g., in FB15K dataset) and interpreted as instances of binary relations. This paper presents a canonical representation of knowledge bases containing multi-fold relations. We show that the existing embedding models on the popular FB15K datasets correspond to a sub-optimal modelling framework, resulting in a loss of structural information. We advocate a novel modelling framework, which models multi-fold relations directly using this canonical representation. Using this framework, the existing TransH model is generalized to a new model, m-TransH. We demonstrate experimentally that m-TransH outperforms TransH by a large margin, thereby establishing a new state of the art. version:1
arxiv-1605-01755 | DCTNet and PCANet for acoustic signal feature extraction | http://arxiv.org/abs/1605.01755 | id:1605.01755 author:Yin Xian, Andrew Thompson, Xiaobai Sun, Douglas Nowacek, Loren Nolte category:cs.SD cs.LG  published:2016-04-28 summary:We introduce the use of DCTNet, an efficient approximation and alternative to PCANet, for acoustic signal classification. In PCANet, the eigenfunctions of the local sample covariance matrix (PCA) are used as filterbanks for convolution and feature extraction. When the eigenfunctions are well approximated by the Discrete Cosine Transform (DCT) functions, each layer of of PCANet and DCTNet is essentially a time-frequency representation. We relate DCTNet to spectral feature representation methods, such as the the short time Fourier transform (STFT), spectrogram and linear frequency spectral coefficients (LFSC). Experimental results on whale vocalization data show that DCTNet improves classification rate, demonstrating DCTNet's applicability to signal processing problems such as underwater acoustics. version:1
arxiv-1604-08634 | Optimal Transport vs. Fisher-Rao distance between Copulas for Clustering Multivariate Time Series | http://arxiv.org/abs/1604.08634 | id:1604.08634 author:Gautier Marti, Sébastien Andler, Frank Nielsen, Philippe Donnat category:stat.ML  published:2016-04-28 summary:We present a methodology for clustering N objects which are described by multivariate time series, i.e. several sequences of real-valued random variables. This clustering methodology leverages copulas which are distributions encoding the dependence structure between several random variables. To take fully into account the dependence information while clustering, we need a distance between copulas. In this work, we compare renowned distances between distributions: the Fisher-Rao geodesic distance, related divergences and optimal transport, and discuss their advantages and disadvantages. Applications of such methodology can be found in the clustering of financial assets. A tutorial, experiments and implementation for reproducible research can be found at www.datagrapple.com/Tech. version:1
arxiv-1604-08633 | Word Ordering Without Syntax | http://arxiv.org/abs/1604.08633 | id:1604.08633 author:Allen Schmaltz, Alexander M. Rush, Stuart M. Shieber category:cs.CL  published:2016-04-28 summary:Recent work on word ordering has argued that syntactic structure is important, or even required, for effectively recovering the order of a sentence. We find that, in fact, an n-gram language model with a simple heuristic gives strong results on this task. Furthermore, we show that a long short-term memory (LSTM) language model is comparatively effective at recovering order, with our basic model outperforming a state-of-the-art syntactic model by 11.5 BLEU points. Additional data and larger beams yield further gains, at the expense of training and search time. version:1
arxiv-1509-01546 | Minimum Spectral Connectivity Projection Pursuit for Unsupervised Classification | http://arxiv.org/abs/1509.01546 | id:1509.01546 author:David P. Hofmeyr, Nicos G. Pavlidis, Idris A. Eckley category:stat.ML cs.LG 62H30 I.5.3  published:2015-09-04 summary:We study the problem of determining the optimal low dimensional projection for maximising the separability of a binary partition of an unlabelled dataset, as measured by spectral graph theory. This is achieved by finding projections which minimise the second eigenvalue of the Laplacian matrices of the projected data, which corresponds to a non-convex, non-smooth optimisation problem. We show that the optimal univariate projection based on spectral connectivity converges to the vector normal to the maximum margin hyperplane through the data, as the scaling parameter is reduced to zero. This establishes a connection between connectivity as measured by spectral graph theory and maximal Euclidean separation. It also allows us to apply our methodology to the problem of finding large margin linear separators. The computational cost associated with each eigen-problem is quadratic in the number of data. To mitigate this problem, we propose an approximation method using microclusters with provable approximation error bounds. We evaluate the performance of the proposed method on a large collection of benchmark datasets and find that it compares favourably with existing methods for projection pursuit and dimension reduction for unsupervised data partitioning. version:2
arxiv-1512-05463 | Continuous online sequence learning with an unsupervised neural network model | http://arxiv.org/abs/1512.05463 | id:1512.05463 author:Yuwei Cui, Subutai Ahmad, Jeff Hawkins category:cs.NE q-bio.NC  published:2015-12-17 summary:The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory is recently proposed as a theoretical framework for sequence learning in the cortex. In this paper, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable-order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods: autoregressive integrated moving average (ARIMA), feedforward neural networks: online sequential extreme learning machine (ELM), and recurrent neural networks: long short-term memory (LSTM) and echo-state networks (ESN), on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyper- parameters tuning. Therefore the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem, but is also applicable to a wide range of real-world problems such as discrete and continuous sequence prediction, anomaly detection, and sequence classification. version:2
arxiv-1604-08610 | Artistic style transfer for videos | http://arxiv.org/abs/1604.08610 | id:1604.08610 author:Manuel Ruder, Alexey Dosovitskiy, Thomas Brox category:cs.CV  published:2016-04-28 summary:In the past, manually re-drawing an image in a certain artistic style required a professional artist and a long time. Doing this for a video sequence single-handed was beyond imagination. Nowadays computers provide new possibilities. We present an approach that transfers the style from one image (for example, a painting) to a whole video sequence. We make use of recent advances in style transfer in still images and propose new initializations and loss functions applicable to videos. This allows us to generate consistent and stable stylized video sequences, even in cases with large motion and strong occlusion. We show that the proposed method clearly outperforms simpler baselines both qualitatively and quantitatively. version:1
arxiv-1604-08561 | Comparing Fifty Natural Languages and Twelve Genetic Languages Using Word Embedding Language Divergence (WELD) as a Quantitative Measure of Language Distance | http://arxiv.org/abs/1604.08561 | id:1604.08561 author:Ehsaneddin Asgari, Mohammad R. K. Mofrad category:cs.CL  published:2016-04-28 summary:We introduce a new measure of distance between languages based on word embedding, called word embedding language divergence (WELD). WELD is defined as divergence between unified similarity distribution of words between languages. Using such a measure, we perform language comparison for fifty natural languages and twelve genetic languages. Our natural language dataset is a collection of sentence-aligned parallel corpora from bible translations for fifty languages spanning a variety of language families. Although we use parallel corpora, which guarantees having the same content in all languages, interestingly in many cases languages within the same family cluster together. In addition to natural languages, we perform language comparison for the coding regions in the genomes of 12 different organisms (4 plants, 6 animals, and two human subjects). Our result confirms a significant high-level difference in the genetic language model of humans/animals versus plants. The proposed method is a step toward defining a quantitative measure of similarity between languages, with applications in languages classification, genre identification, dialect identification, and evaluation of translations. version:1
arxiv-1604-08524 | A Probabilistic Adaptive Search System for Exploring the Face Space | http://arxiv.org/abs/1604.08524 | id:1604.08524 author:Andres G. Abad, Luis I. Reyes Castro category:stat.ML cs.CV  published:2016-04-28 summary:Face recall is a basic human cognitive process performed routinely, e.g., when meeting someone and determining if we have met that person before. Assisting a subject during face recall by suggesting candidate faces can be challenging. One of the reasons is that the search space - the face space - is quite large and lacks structure. A commercial application of face recall is facial composite systems - such as Identikit, PhotoFIT, and CD-FIT - where a witness searches for an image of a face that resembles his memory of a particular offender. The inherent uncertainty and cost in the evaluation of the objective function, the large size and lack of structure of the search space, and the unavailability of the gradient concept makes this problem inappropriate for traditional optimization methods. In this paper we propose a novel evolutionary approach for searching the face space that can be used as a facial composite system. The approach is inspired by methods of Bayesian optimization and differs from other applications in the use of the skew-normal distribution as its acquisition function. This choice of acquisition function provides greater granularity, with regularized, conservative, and realistic results. version:1
arxiv-1206-1386 | Robust subspace recovery by Tyler's M-estimator | http://arxiv.org/abs/1206.1386 | id:1206.1386 author:Teng Zhang category:stat.ML  published:2012-06-07 summary:This paper considers the problem of robust subspace recovery: given a set of $N$ points in $\mathbb{R}^D$, if many lie in a $d$-dimensional subspace, then can we recover the underlying subspace? We show that Tyler's M-estimator can be used to recover the underlying subspace, if the percentage of the inliers is larger than $d/D$ and the data points lie in general position. Empirically, Tyler's M-estimator compares favorably with other convex subspace recovery algorithms in both simulations and experiments on real data sets. version:3
arxiv-1509-04309 | Sparse Representation for 3D Shape Estimation: A Convex Relaxation Approach | http://arxiv.org/abs/1509.04309 | id:1509.04309 author:Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Kostas Daniilidis category:cs.CV  published:2015-09-14 summary:We investigate the problem of estimating the 3D shape of an object defined by a set of 3D landmarks, given their 2D correspondences in a single image. A successful approach to alleviating the reconstruction ambiguity is the 3D deformable shape model and a sparse representation is often used to capture complex shape variability. But the model inference is still a challenge due to the nonconvexity in optimization resulted from joint estimation of shape and viewpoint. In contrast to prior work that relies on a alternating scheme with solutions depending on initialization, we propose a convex approach to addressing this challenge and develop an efficient algorithm to solve the proposed convex program. Moreover, we propose a robust model to handle gross errors in the 2D correspondences. We demonstrate the exact recovery property of the proposed method, the advantage compared to the nonconvex baseline methods and the applicability to recover 3D human poses and car models from single images. version:2
arxiv-1511-09439 | Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video | http://arxiv.org/abs/1511.09439 | id:1511.09439 author:Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Kosta Derpanis, Kostas Daniilidis category:cs.CV  published:2015-11-30 summary:This paper addresses the challenge of 3D full-body human pose estimation from a monocular image sequence. Here, two cases are considered: (i) the image locations of the human joints are provided and (ii) the image locations of joints are unknown. In the former case, a novel approach is introduced that integrates a sparsity-driven 3D geometric prior and temporal smoothness. In the latter case, the former case is extended by treating the image locations of the joints as latent variables. A deep fully convolutional network is trained to predict the uncertainty maps of the 2D joint locations. The 3D pose estimates are realized via an Expectation-Maximization algorithm over the entire sequence, where it is shown that the 2D joint location uncertainties can be conveniently marginalized out during inference. Empirical evaluation on the Human3.6M dataset shows that the proposed approaches achieve greater 3D pose estimation accuracy over state-of-the-art baselines. Further, the proposed approach outperforms a publicly available 2D pose estimation baseline on the challenging PennAction dataset. version:2
arxiv-1604-08402 | Two Differentially Private Rating Collection Mechanisms for Recommender Systems | http://arxiv.org/abs/1604.08402 | id:1604.08402 author:Wenjie Zheng category:stat.ML cs.CR cs.IR  published:2016-04-28 summary:We design two mechanisms for the recommender system to collect user ratings. One is modified Laplace mechanism, and the other is randomized response mechanism. We prove that they are both differentially private and preserve the data utility. version:1
arxiv-1602-02514 | Fast k-means with accurate bounds | http://arxiv.org/abs/1602.02514 | id:1602.02514 author:James Newling, François Fleuret category:stat.ML cs.LG  published:2016-02-08 summary:We propose a novel accelerated exact k-means algorithm, which performs better than the current state-of-the-art low-dimensional algorithm in 18 of 22 experiments, running up to 3 times faster. We also propose a general improvement of existing state-of-the-art accelerated exact k-means algorithms through better estimates of the distance bounds used to reduce the number of distance calculations, and get a speedup in 36 of 44 experiments, up to 1.8 times faster. We have conducted experiments with our own implementations of existing methods to ensure homogeneous evaluation of performance, and we show that our implementations perform as well or better than existing available implementations. Finally, we propose simplified variants of standard approaches and show that they are faster than their fully-fledged counterparts in 59 of 62 experiments. version:5
arxiv-1604-08382 | Convolutional Neural Networks For Automatic State-Time Feature Extraction in Reinforcement Learning Applied to Residential Load Control | http://arxiv.org/abs/1604.08382 | id:1604.08382 author:Bert J. Claessens, Peter Vrancx, Frederik Ruelens category:cs.LG cs.SY  published:2016-04-28 summary:Direct load control of a heterogeneous cluster of residential demand flexibility sources is a high-dimensional control problem with partial observability. This work proposes a novel approach that uses a convolutional neural network to extract hidden state-time features to mitigate the curse of partial observability. More specific, a convolutional neural network is used as a function approximator to estimate the state-action value function or Q-function in the supervised learning step of fitted Q-iteration. The approach is evaluated in a qualitative simulation, comprising a cluster of thermostatically controlled loads that only share their air temperature, whilst their envelope temperature remains hidden. The simulation results show that the presented approach is able to capture the underlying hidden features and successfully reduce the electricity cost the cluster. version:1
arxiv-1604-08352 | Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition | http://arxiv.org/abs/1604.08352 | id:1604.08352 author:Théodore Bluche category:cs.CV cs.LG cs.NE  published:2016-04-28 summary:Offline handwriting recognition systems require cropped text line images for both training and recognition. On the one hand, the annotation of position and transcript at line level is costly to obtain. On the other hand, automatic line segmentation algorithms are prone to errors, compromising the subsequent recognition. In this paper, we propose a modification of the popular and efficient multi-dimensional long short-term memory recurrent neural networks (MDLSTM-RNNs) to enable end-to-end processing of handwritten paragraphs. More particularly, we replace the collapse layer transforming the two-dimensional representation into a sequence of predictions by a recurrent version which can recognize one line at a time. In the proposed model, a neural network performs a kind of implicit line segmentation by computing attention weights on the image representation. The experiments on paragraphs of Rimes and IAM database yield results that are competitive with those of networks trained at line level, and constitute a significant step towards end-to-end transcription of full documents. version:1
arxiv-1604-07807 | An Enhanced Deep Feature Representation for Person Re-identification | http://arxiv.org/abs/1604.07807 | id:1604.07807 author:Shangxuan Wu, Ying-Cong Chen, Xiang Li, An-Cong Wu, Jin-Jie You, Wei-Shi Zheng category:cs.CV  published:2016-04-26 summary:Feature representation and metric learning are two critical components in person re-identification models. In this paper, we focus on the feature representation and claim that hand-crafted histogram features can be complementary to Convolutional Neural Network (CNN) features. We propose a novel feature extraction model called Feature Fusion Net (FFN) for pedestrian image representation. In FFN, back propagation makes CNN features constrained by the handcrafted features. Utilizing color histogram features (RGB, HSV, YCbCr, Lab and YIQ) and texture features (multi-scale and multi-orientation Gabor features), we get a new deep feature representation that is more discriminative and compact. Experiments on three challenging datasets (VIPeR, CUHK01, PRID450s) validates the effectiveness of our proposal. version:2
arxiv-1510-08660 | RATM: Recurrent Attentive Tracking Model | http://arxiv.org/abs/1510.08660 | id:1510.08660 author:Samira Ebrahimi Kahou, Vincent Michalski, Roland Memisevic category:cs.LG  published:2015-10-29 summary:We present an attention-based modular neural framework for computer vision. The framework uses a soft attention mechanism allowing models to be trained with gradient descent. It consists of three modules: a recurrent attention module controlling where to look in an image or video frame, a feature-extraction module providing a representation of what is seen, and an objective module formalizing why the model learns its attentive behavior. The attention module allows the model to focus computation on task-related information in the input. We apply the framework to several object tracking tasks and explore various design choices. We experiment with three data sets, bouncing ball, moving digits and the real-world KTH data set. The proposed Recurrent Attentive Tracking Model performs well on all three tasks and can generalize to related but previously unseen sequences from a challenging tracking data set. version:4
arxiv-1604-08320 | Sequential Bayesian optimal experimental design via approximate dynamic programming | http://arxiv.org/abs/1604.08320 | id:1604.08320 author:Xun Huan, Youssef M. Marzouk category:stat.ME math.OC stat.CO stat.ML  published:2016-04-28 summary:The design of multiple experiments is commonly undertaken via suboptimal strategies, such as batch (open-loop) design that omits feedback or greedy (myopic) design that does not account for future effects. This paper introduces new strategies for the optimal design of sequential experiments. First, we rigorously formulate the general sequential optimal experimental design (sOED) problem as a dynamic program. Batch and greedy designs are shown to result from special cases of this formulation. We then focus on sOED for parameter inference, adopting a Bayesian formulation with an information theoretic design objective. To make the problem tractable, we develop new numerical approaches for nonlinear design with continuous parameter, design, and observation spaces. We approximate the optimal policy by using backward induction with regression to construct and refine value function approximations in the dynamic program. The proposed algorithm iteratively generates trajectories via exploration and exploitation to improve approximation accuracy in frequently visited regions of the state space. Numerical results are verified against analytical solutions in a linear-Gaussian setting. Advantages over batch and greedy design are then demonstrated on a nonlinear source inversion problem where we seek an optimal policy for sequential sensing. version:1
arxiv-1505-06795 | An Empirical Evaluation of Current Convolutional Architectures' Ability to Manage Nuisance Location and Scale Variability | http://arxiv.org/abs/1505.06795 | id:1505.06795 author:Nikolaos Karianakis, Jingming Dong, Stefano Soatto category:cs.CV cs.LG cs.NE  published:2015-05-26 summary:We conduct an empirical study to test the ability of Convolutional Neural Networks (CNNs) to reduce the effects of nuisance transformations of the input data, such as location, scale and aspect ratio. We isolate factors by adopting a common convolutional architecture either deployed globally on the image to compute class posterior distributions, or restricted locally to compute class conditional distributions given location, scale and aspect ratios of bounding boxes determined by proposal heuristics. In theory, averaging the latter should yield inferior performance compared to proper marginalization. Yet empirical evidence suggests the converse, leading us to conclude that - at the current level of complexity of convolutional architectures and scale of the data sets used to train them - CNNs are not very effective at marginalizing nuisance variability. We also quantify the effects of context on the overall classification task and its impact on the performance of CNNs, and propose improved sampling techniques for heuristic proposal schemes that improve end-to-end performance to state-of-the-art levels. We test our hypothesis on a classification task using the ImageNet Challenge benchmark and on a wide-baseline matching task using the Oxford and Fischer's datasets. version:2
arxiv-1506-01144 | What value do explicit high level concepts have in vision to language problems? | http://arxiv.org/abs/1506.01144 | id:1506.01144 author:Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, Anton van den Hengel category:cs.CV  published:2015-06-03 summary:Much of the recent progress in Vision-to-Language (V2L) problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. We propose here a method of incorporating high-level concepts into the very successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art performance in both image captioning and visual question answering. We also show that the same mechanism can be used to introduce external semantic information and that doing so further improves performance. In doing so we provide an analysis of the value of high level semantic information in V2L problems. version:6
arxiv-1511-06783 | Recognizing Activities of Daily Living with a Wrist-mounted Camera | http://arxiv.org/abs/1511.06783 | id:1511.06783 author:Katsunori Ohnishi, Atsushi Kanehira, Asako Kanezaki, Tatsuya Harada category:cs.CV  published:2015-11-20 summary:We present a novel dataset and a novel algorithm for recognizing activities of daily living (ADL) from a first-person wearable camera. Handled objects are crucially important for egocentric ADL recognition. For specific examination of objects related to users' actions separately from other objects in an environment, many previous works have addressed the detection of handled objects in images captured from head-mounted and chest-mounted cameras. Nevertheless, detecting handled objects is not always easy because they tend to appear small in images. They can be occluded by a user's body. As described herein, we mount a camera on a user's wrist. A wrist-mounted camera can capture handled objects at a large scale, and thus it enables us to skip object detection process. To compare a wrist-mounted camera and a head-mounted camera, we also develop a novel and publicly available dataset that includes videos and annotations of daily activities captured simultaneously by both cameras. Additionally, we propose a discriminative video representation that retains spatial and temporal information after encoding frame descriptors extracted by Convolutional Neural Networks (CNN). version:2
arxiv-1604-08291 | Streaming View Learning | http://arxiv.org/abs/1604.08291 | id:1604.08291 author:Chang Xu, Dacheng Tao, Chao Xu category:stat.ML cs.LG  published:2016-04-28 summary:An underlying assumption in conventional multi-view learning algorithms is that all views can be simultaneously accessed. However, due to various factors when collecting and pre-processing data from different views, the streaming view setting, in which views arrive in a streaming manner, is becoming more common. By assuming that the subspaces of a multi-view model trained over past views are stable, here we fine tune their combination weights such that the well-trained multi-view model is compatible with new views. This largely overcomes the burden of learning new view functions and updating past view functions. We theoretically examine convergence issues and the influence of streaming views in the proposed algorithm. Experimental results on real-world datasets suggest that studying the streaming views problem in multi-view learning is significant and that the proposed algorithm can effectively handle streaming views in different applications. version:1
arxiv-1604-08275 | Crafting Adversarial Input Sequences for Recurrent Neural Networks | http://arxiv.org/abs/1604.08275 | id:1604.08275 author:Nicolas Papernot, Patrick McDaniel, Ananthram Swami, Richard Harang category:cs.CR cs.LG cs.NE  published:2016-04-28 summary:Machine learning models are frequently used to solve complex security problems, as well as to make decisions in sensitive situations like guiding autonomous vehicles or predicting financial market behaviors. Previous efforts have shown that numerous machine learning models were vulnerable to adversarial manipulations of their inputs taking the form of adversarial samples. Such inputs are crafted by adding carefully selected perturbations to legitimate inputs so as to force the machine learning model to misbehave, for instance by outputting a wrong class if the machine learning task of interest is classification. In fact, to the best of our knowledge, all previous work on adversarial samples crafting for neural network considered models used to solve classification tasks, most frequently in computer vision applications. In this paper, we contribute to the field of adversarial machine learning by investigating adversarial input sequences for recurrent neural networks processing sequential data. We show that the classes of algorithms introduced previously to craft adversarial samples misclassified by feed-forward neural networks can be adapted to recurrent neural networks. In a experiment, we show that adversaries can craft adversarial sequences misleading both categorical and sequential recurrent neural networks. version:1
arxiv-1511-05284 | Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data | http://arxiv.org/abs/1511.05284 | id:1511.05284 author:Lisa Anne Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Trevor Darrell category:cs.CV cs.CL  published:2015-11-17 summary:While recent deep neural network models have achieved promising results on the image captioning task, they rely largely on the availability of corpora with paired image and sentence captions to describe objects in context. In this work, we propose the Deep Compositional Captioner (DCC) to address the task of generating descriptions of novel objects which are not present in paired image-sentence datasets. Our method achieves this by leveraging large object recognition datasets and external text corpora and by transferring knowledge between semantically similar concepts. Current deep caption models can only describe objects contained in paired image-sentence corpora, despite the fact that they are pre-trained with large object recognition datasets, namely ImageNet. In contrast, our model can compose sentences that describe novel objects and their interactions with other objects. We demonstrate our model's ability to describe novel concepts by empirically evaluating its performance on MSCOCO and show qualitative results on ImageNet images of objects for which no paired image-caption data exist. Further, we extend our approach to generate descriptions of objects in video clips. Our results show that DCC has distinct advantages over existing image and video captioning approaches for generating descriptions of new objects in context. version:2
arxiv-1604-08269 | Efficient Optimization for Rank-based Loss Functions | http://arxiv.org/abs/1604.08269 | id:1604.08269 author:Pritish Mohapatra, Michal Rolinek, C. V. Jawahar, Vladimir Kolmogorov, M. Pawan Kumar category:cs.CV  published:2016-04-27 summary:The accuracy of information retrieval systems is often measured using complex non-decomposable loss functions such as the average precision (AP) or the normalized discounted cumulative gain (NDCG). Given a set of positive (relevant) and negative (non-relevant) samples, the parameters of a retrieval system can be estimated using a rank SVM framework, which minimizes a regularized convex upper bound on the empirical loss. However, the high computational complexity of loss-augmented inference, which is required to learn a rank SVM, prohibits its use in large training datasets. To alleviate this de?ciency, we present a novel quicksort avored algorithm for a large class of nondecomposable loss functions. We provide a complete characterization of the loss functions that are amenable to our algorithm. Furthermore, we prove that no comparison based algorithm can improve upon the computational complexity of our approach asymptotically. We demonstrate that it is possible to reduce the constant factors of the complexity by exploiting the special structure of the AP loss. Using the PASCAL VOC action recognition and object detection datasets, we show that our approach provides signi?cantly better results than baseline methods that use a simpler decomposable loss in comparable runtime. version:1
arxiv-1604-08256 | Multiview Differential Geometry of Curves | http://arxiv.org/abs/1604.08256 | id:1604.08256 author:Ricardo Fabbri, Benjamin Kimia category:cs.CV cs.CG cs.GR math.DG 53A04  53A17  53A20 I.4.8; I.3.5  published:2016-04-27 summary:The field of multiple view geometry has seen tremendous progress in reconstruction and calibration due to methods for extracting reliable point features and key developments in projective geometry. Point features, however, are not available in certain applications and result in unstructured point cloud reconstructions. General image curves provide a complementary feature when keypoints are scarce, and result in 3D curve geometry, but face challenges not addressed by the usual projective geometry of points and algebraic curves. We address these challenges by laying the theoretical foundations of a framework based on the differential geometry of general curves, including stationary curves, occluding contours, and non-rigid curves, aiming at stereo correspondence, camera estimation (including calibration, pose, and multiview epipolar geometry), and 3D reconstruction given measured image curves. By gathering previous results into a cohesive theory, novel results were made possible, yielding three contributions. First we derive the differential geometry of an image curve (tangent, curvature, curvature derivative) from that of the underlying space curve (tangent, curvature, curvature derivative, torsion). Second, we derive the differential geometry of a space curve from that of two corresponding image curves. Third, the differential motion of an image curve is derived from camera motion and the differential geometry and motion of the space curve. The availability of such a theory enables novel curve-based multiview reconstruction and camera estimation systems to augment existing point-based approaches. This theory has been used to reconstruct a "3D curve sketch", to determine camera pose from local curve geometry, and tracking; other developments are underway. version:1
arxiv-1506-09039 | Scalable Discrete Sampling as a Multi-Armed Bandit Problem | http://arxiv.org/abs/1506.09039 | id:1506.09039 author:Yutian Chen, Zoubin Ghahramani category:stat.ML cs.LG  published:2015-06-30 summary:Drawing a sample from a discrete distribution is one of the building components for Monte Carlo methods. Like other sampling algorithms, discrete sampling suffers from the high computational burden in large-scale inference problems. We study the problem of sampling a discrete random variable with a high degree of dependency that is typical in large-scale Bayesian inference and graphical models, and propose an efficient approximate solution with a subsampling approach. We make a novel connection between the discrete sampling and Multi-Armed Bandits problems with a finite reward population and provide three algorithms with theoretical guarantees. Empirical evaluations show the robustness and efficiency of the approximate algorithms in both synthetic and real-world large-scale problems. version:3
arxiv-1604-08242 | The IBM 2016 English Conversational Telephone Speech Recognition System | http://arxiv.org/abs/1604.08242 | id:1604.08242 author:George Saon, Tom Sercu, Steven Rennie, Hong-Kwang J. Kuo category:cs.CL  published:2016-04-27 summary:We describe a collection of acoustic and language modeling techniques that lowered the word error rate of our English conversational telephone LVCSR system to a record 6.9% on the Switchboard subset of the Hub5 2000 evaluation testset. On the acoustic side, we use a score fusion of three strong models: recurrent nets with maxout activations, very deep convolutional nets with 3x3 kernels, and bidirectional long-short term memory nets which operate on bottleneck features. On the language modeling side, we use an updated model "M" and hierarchical neural network LMs. version:1
arxiv-1604-08220 | Diving deeper into mentee networks | http://arxiv.org/abs/1604.08220 | id:1604.08220 author:Ragav Venkatesan, Baoxin Li category:cs.LG cs.CV cs.NE  published:2016-04-27 summary:Modern computer vision is all about the possession of powerful image representations. Deeper and deeper convolutional neural networks have been built using larger and larger datasets and are made publicly available. A large swath of computer vision scientists use these pre-trained networks with varying degrees of successes in various tasks. Even though there is tremendous success in copying these networks, the representational space is not learnt from the target dataset in a traditional manner. One of the reasons for opting to use a pre-trained network over a network learnt from scratch is that small datasets provide less supervision and require meticulous regularization, smaller and careful tweaking of learning rates to even achieve stable learning without weight explosion. It is often the case that large deep networks are not portable, which necessitates the ability to learn mid-sized networks from scratch. In this article, we dive deeper into training these mid-sized networks on small datasets from scratch by drawing additional supervision from a large pre-trained network. Such learning also provides better generalization accuracies than networks trained with common regularization techniques such as l2, l1 and dropouts. We show that features learnt thus, are more general than those learnt independently. We studied various characteristics of such networks and found some interesting behaviors. version:1
arxiv-1604-08202 | Amodal Instance Segmentation | http://arxiv.org/abs/1604.08202 | id:1604.08202 author:Ke Li, Jitendra Malik category:cs.CV  published:2016-04-27 summary:We consider the problem of amodal instance segmentation, the objective of which is to predict the region encompassing both visible and occluded parts of each object. Thus far, the lack of publicly available amodal segmentation annotations has stymied the development of amodal segmentation methods. In this paper, we sidestep this issue by relying solely on standard modal instance segmentation annotations to train our model. The result is a new method for amodal instance segmentation, which represents the first such method to the best of our knowledge. We demonstrate the proposed method's effectiveness both qualitatively and quantitatively. version:1
arxiv-1502-06309 | Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle | http://arxiv.org/abs/1502.06309 | id:1502.06309 author:Yu-Xiang Wang, Jing Lei, Stephen E. Fienberg category:stat.ML cs.CR cs.LG  published:2015-02-23 summary:While machine learning has proven to be a powerful data-driven solution to many real-life problems, its use in sensitive domains has been limited due to privacy concerns. A popular approach known as **differential privacy** offers provable privacy guarantees, but it is often observed in practice that it could substantially hamper learning accuracy. In this paper we study the learnability (whether a problem can be learned by any algorithm) under Vapnik's general learning setting with differential privacy constraint, and reveal some intricate relationships between privacy, stability and learnability. In particular, we show that a problem is privately learnable **if an only if** there is a private algorithm that asymptotically minimizes the empirical risk (AERM). In contrast, for non-private learning AERM alone is not sufficient for learnability. This result suggests that when searching for private learning algorithms, we can restrict the search to algorithms that are AERM. In light of this, we propose a conceptual procedure that always finds a universally consistent algorithm whenever the problem is learnable under privacy constraint. We also propose a generic and practical algorithm and show that under very general conditions it privately learns a wide class of learning problems. Lastly, we extend some of the results to the more practical $(\epsilon,\delta)$-differential privacy and establish the existence of a phase-transition on the class of problems that are approximately privately learnable with respect to how small $\delta$ needs to be. version:3
arxiv-1604-08201 | Interpretable Deep Neural Networks for Single-Trial EEG Classification | http://arxiv.org/abs/1604.08201 | id:1604.08201 author:Irene Sturm, Sebastian Bach, Wojciech Samek, Klaus-Robert Müller category:cs.NE stat.ML  published:2016-04-27 summary:Background: In cognitive neuroscience the potential of Deep Neural Networks (DNNs) for solving complex classification tasks is yet to be fully exploited. The most limiting factor is that DNNs as notorious 'black boxes' do not provide insight into neurophysiological phenomena underlying a decision. Layer-wise Relevance Propagation (LRP) has been introduced as a novel method to explain individual network decisions. New Method: We propose the application of DNNs with LRP for the first time for EEG data analysis. Through LRP the single-trial DNN decisions are transformed into heatmaps indicating each data point's relevance for the outcome of the decision. Results: DNN achieves classification accuracies comparable to those of CSP-LDA. In subjects with low performance subject-to-subject transfer of trained DNNs can improve the results. The single-trial LRP heatmaps reveal neurophysiologically plausible patterns, resembling CSP-derived scalp maps. Critically, while CSP patterns represent class-wise aggregated information, LRP heatmaps pinpoint neural patterns to single time points in single trials. Comparison with Existing Method(s): We compare the classification performance of DNNs to that of linear CSP-LDA on two data sets related to motor-imaginery BCI. Conclusion: We have demonstrated that DNN is a powerful non-linear tool for EEG analysis. With LRP a new quality of high-resolution assessment of neural activity can be reached. LRP is a potential remedy for the lack of interpretability of DNNs that has limited their utility in neuroscientific applications. The extreme specificity of the LRP-derived heatmaps opens up new avenues for investigating neural activity underlying complex perception or decision-related processes. version:1
arxiv-1604-08182 | Unsupervised Classification in Hyperspectral Imagery with Nonlocal Total Variation and Primal-Dual Hybrid Gradient Algorithm | http://arxiv.org/abs/1604.08182 | id:1604.08182 author:Wei Zhu, Victoria Chayes, Alexandre Tiard, Stephanie Sanchez, Devin Dahlberg, Da Kuang, Andrea L. Bertozzi, Stanley Osher, Dominique Zosso category:cs.CV  published:2016-04-27 summary:We propose a graph-based nonlocal total variation method (NLTV) for unsupervised classification of hyperspectral images (HSI). The variation problem is solved by the primal-dual hybrid gradient (PDHG) algorithm. By squaring the labeling function and using a stable simplex clustering routine, we can implement an unsupervised clustering method with random initialization. Finally, we speed up the calculation using a $k$-d tree and approximate nearest neighbor search algorithm for calculation of the weight matrix for distances between pixel signatures. The effectiveness of this proposed algorithm is illustrated on both synthetic and real-world HSI, and numerical results show that our algorithm outperform other standard unsupervised clustering methods such as spherical K-means, nonnegative matrix factorization (NMF), and the graph-based Merriman-Bence-Osher (MBO) scheme. version:1
arxiv-1603-08016 | Classifying Syntactic Regularities for Hundreds of Languages | http://arxiv.org/abs/1603.08016 | id:1603.08016 author:Reed Coke, Ben King, Dragomir Radev category:cs.CL  published:2016-03-25 summary:This paper presents a comparison of classification methods for linguistic typology for the purpose of expanding an extensive, but sparse language resource: the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). We experimented with a variety of regression and nearest-neighbor methods for use in classification over a set of 325 languages and six syntactic rules drawn from WALS. To classify each rule, we consider the typological features of the other five rules; linguistic features extracted from a word-aligned Bible in each language; and genealogical features (genus and family) of each language. In general, we find that propagating the majority label among all languages of the same genus achieves the best accuracy in label pre- diction. Following this, a logistic regression model that combines typological and linguistic features offers the next best performance. Interestingly, this model actually outperforms the majority labels among all languages of the same family. version:2
arxiv-1503-00024 | Influence Maximization with Bandits | http://arxiv.org/abs/1503.00024 | id:1503.00024 author:Sharan Vaswani, Laks. V. S. Lakshmanan, Mark Schmidt category:cs.SI cs.LG stat.ML  published:2015-02-27 summary:We consider the problem of \emph{influence maximization}, the problem of maximizing the number of people that become aware of a product by finding the `best' set of `seed' users to expose the product to. Most prior work on this topic assumes that we know the probability of each user influencing each other user, or we have data that lets us estimate these influences. However, this information is typically not initially available or is difficult to obtain. To avoid this assumption, we adopt a combinatorial multi-armed bandit paradigm that estimates the influence probabilities as we sequentially try different seed sets. We establish bounds on the performance of this procedure under the existing edge-level feedback as well as a novel and more realistic node-level feedback. Beyond our theoretical results, we describe a practical implementation and experimentally demonstrate its efficiency and effectiveness on four real datasets. version:4
arxiv-1604-06433 | Walk and Learn: Facial Attribute Representation Learning from Egocentric Video and Contextual Data | http://arxiv.org/abs/1604.06433 | id:1604.06433 author:Jing Wang, Yu Cheng, Rogerio Schmidt Feris category:cs.CV  published:2016-04-21 summary:The way people look in terms of facial attributes (ethnicity, hair color, facial hair, etc.) and the clothes or accessories they wear (sunglasses, hat, hoodies, etc.) is highly dependent on geo-location and weather condition, respectively. This work explores, for the first time, the use of this contextual information, as people with wearable cameras walk across different neighborhoods of a city, in order to learn a rich feature representation for facial attribute classification, without the costly manual annotation required by previous methods. By tracking the faces of casual walkers on more than 40 hours of egocentric video, we are able to cover tens of thousands of different identities and automatically extract nearly 5 million pairs of images connected by or from different face tracks, along with their weather and location context, under pose and lighting variations. These image pairs are then fed into a deep network that preserves similarity of images connected by the same track, in order to capture identity-related attribute features, and optimizes for location and weather prediction to capture additional facial attribute features. Finally, the network is fine-tuned with manually annotated samples. We perform an extensive experimental analysis on wearable data and two standard benchmark datasets based on web images (LFWA and CelebA). Our method outperforms by a large margin a network trained from scratch. Moreover, even without using manually annotated identity labels for pre-training as in previous methods, our approach achieves results that are better than the state of the art. version:2
arxiv-1604-08120 | Extracting Temporal and Causal Relations between Events | http://arxiv.org/abs/1604.08120 | id:1604.08120 author:Paramita Mirza category:cs.CL  published:2016-04-27 summary:Structured information resulting from temporal information processing is crucial for a variety of natural language processing tasks, for instance to generate timeline summarization of events from news documents, or to answer temporal/causal-related questions about some events. In this thesis we present a framework for an integrated temporal and causal relation extraction system. We first develop a robust extraction component for each type of relations, i.e. temporal order and causality. We then combine the two extraction components into an integrated relation extraction system, CATENA---CAusal and Temporal relation Extraction from NAtural language texts---, by utilizing the presumption about event precedence in causality, that causing events must happened BEFORE resulting events. Several resources and techniques to improve our relation extraction systems are also discussed, including word embeddings and training data expansion. Finally, we report our adaptation efforts of temporal information processing for languages other than English, namely Italian and Indonesian. version:1
arxiv-1603-07235 | Global-Local Face Upsampling Network | http://arxiv.org/abs/1603.07235 | id:1603.07235 author:Oncel Tuzel, Yuichi Taguchi, John R. Hershey category:cs.CV cs.LG  published:2016-03-23 summary:Face hallucination, which is the task of generating a high-resolution face image from a low-resolution input image, is a well-studied problem that is useful in widespread application areas. Face hallucination is particularly challenging when the input face resolution is very low (e.g., 10 x 12 pixels) and/or the image is captured in an uncontrolled setting with large pose and illumination variations. In this paper, we revisit the algorithm introduced in [1] and present a deep interpretation of this framework that achieves state-of-the-art under such challenging scenarios. In our deep network architecture the global and local constraints that define a face can be efficiently modeled and learned end-to-end using training data. Conceptually our network design can be partitioned into two sub-networks: the first one implements the holistic face reconstruction according to global constraints, and the second one enhances face-specific details and enforces local patch statistics. We optimize the deep network using a new loss function for super-resolution that combines reconstruction error with a learned face quality measure in adversarial setting, producing improved visual results. We conduct extensive experiments in both controlled and uncontrolled setups and show that our algorithm improves the state of the art both numerically and visually. version:2
arxiv-1604-08102 | An ABC interpretation of the multiple auxiliary variable method | http://arxiv.org/abs/1604.08102 | id:1604.08102 author:Dennis Prangle, Richard G. Everitt category:stat.CO stat.ML  published:2016-04-27 summary:We show that the auxiliary variable method (M{\o}ller et al., 2006; Murray et al., 2006) for inference of Markov random fields can be viewed as an approximate Bayesian computation method for likelihood estimation. version:1
arxiv-1604-08088 | Detecting Violence in Video using Subclasses | http://arxiv.org/abs/1604.08088 | id:1604.08088 author:Xirong Li, Yujia Huo, Jieping Xu, Qin Jin category:cs.MM cs.CV  published:2016-04-27 summary:This paper attacks the challenging problem of violence detection in videos. Different from existing works focusing on combining multi-modal features, we go one step further by adding and exploiting subclasses visually related to violence. We enrich the MediaEval 2015 violence dataset by \emph{manually} labeling violence videos with respect to the subclasses. Such fine-grained annotations not only help understand what have impeded previous efforts on learning to fuse the multi-modal features, but also enhance the generalization ability of the learned fusion to novel test data. The new subclass based solution, with AP of 0.303 and P100 of 0.55 on the MediaEval 2015 test set, outperforms several state-of-the-art alternatives. Notice that our solution does not require fine-grained annotations on the test set, so it can be directly applied on novel and fully unlabeled videos. Interestingly, our study shows that motion related features, though being essential part in previous systems, are dispensable. version:1
arxiv-1604-08079 | UBL: an R package for Utility-based Learning | http://arxiv.org/abs/1604.08079 | id:1604.08079 author:Paula Branco, Rita P. Ribeiro, Luis Torgo category:cs.MS cs.LG stat.ML  published:2016-04-27 summary:This document describes the R package UBL that allows the use of several methods for handling utility-based learning problems. Classification and regression problems that assume non-uniform costs and/or benefits pose serious challenges to predictive analytics tasks. In the context of meteorology, finance, medicine, ecology, among many other, specific domain information concerning the preference bias of the users must be taken into account to enhance the models predictive performance. To deal with this problem, a large number of techniques was proposed by the research community for both classification and regression tasks. The main goal of UBL package is to facilitate the utility-based predictive analytics task by providing a set of methods to deal with this type of problems in the R environment. It is a versatile tool that provides mechanisms to handle both regression and classification (binary and multiclass) tasks. Moreover, UBL package allows the user to specify his domain preferences, but it also provides some automatic methods that try to infer those preference bias from the domain, considering some common known settings. version:1
arxiv-1604-06232 | Incremental Reconstruction of Urban Environments by Edge-Points Delaunay Triangulation | http://arxiv.org/abs/1604.06232 | id:1604.06232 author:Andrea Romanoni, Matteo Matteucci category:cs.CV cs.RO I.4.5  published:2016-04-21 summary:Urban reconstruction from a video captured by a surveying vehicle constitutes a core module of automated mapping. When computational power represents a limited resource and, a detailed map is not the primary goal, the reconstruction can be performed incrementally, from a monocular video, carving a 3D Delaunay triangulation of sparse points; this allows online incremental mapping for tasks such as traversability analysis or obstacle avoidance. To exploit the sharp edges of urban landscape, we propose to use a Delaunay triangulation of Edge-Points, which are the 3D points corresponding to image edges. These points constrain the edges of the 3D Delaunay triangulation to real-world edges. Besides the use of the Edge-Points, a second contribution of this paper is the Inverse Cone Heuristic that preemptively avoids the creation of artifacts in the reconstructed manifold surface. We force the reconstruction of a manifold surface since it makes it possible to apply computer graphics or photometric refinement algorithms to the output mesh. We evaluated our approach on four real sequences of the public available KITTI dataset by comparing the incremental reconstruction against Velodyne measurements. version:2
arxiv-1604-08010 | Deep Learning for Saliency Prediction in Natural Video | http://arxiv.org/abs/1604.08010 | id:1604.08010 author:Souad Chaabouni, Jenny Benois-Pineau, Ofer Hadar, Chokri Ben Amar category:cs.CV  published:2016-04-27 summary:The purpose of this paper is the detection of salient areas in natural video by using the new deep learning techniques. Salient patches in video frames are predicted first. Then the predicted visual fixation maps are built upon them. We design the deep architecture on the basis of CaffeNet implemented with Caffe toolkit. We show that changing the way of data selection for optimisation of network parameters, we can save computation cost up to 12 times. We extend deep learning approaches for saliency prediction in still images with RGB values to specificity of video using the sensitivity of the human visual system to residual motion. Furthermore, we complete primary colour pixel values by contrast features proposed in classical visual attention prediction models. The experiments are conducted on two publicly available datasets. The first is IRCCYN video database containing 31 videos with an overall amount of 7300 frames and eye fixations of 37 subjects. The second one is HOLLYWOOD2 provided 2517 movie clips with the eye fixations of 19 subjects. On IRCYYN dataset, the accuracy obtained is of 89.51%. On HOLLYWOOD2 dataset, results in prediction of saliency of patches show the improvement up to 2% with regard to RGB use only. The resulting accuracy of 76, 6% is obtained. The AUC metric in comparison of predicted saliency maps with visual fixation maps shows the increase up to 16% on a sample of video clips from this dataset. version:1
arxiv-1601-02919 | Using Filter Banks in Convolutional Neural Networks for Texture Classification | http://arxiv.org/abs/1601.02919 | id:1601.02919 author:Vincent Andrearczyk, Paul F. Whelan category:cs.CV cs.NE  published:2016-01-12 summary:Deep learning has established many new state of the art solutions in the last decade in areas such as object, scene and speech recognition. In particular Convolutional Neural Network (CNN) is a category of deep learning which obtains excellent results in object detection and recognition tasks. Its architecture is indeed well suited to object analysis by learning and classifying complex (deep) features that represent parts of an object or the object itself. However, some of its features are very similar to texture analysis methods. CNN layers can be thought of as filter banks of complexity increasing with the depth. Filter banks are powerful tools to extract texture features and have been widely used in texture analysis. In this paper we develop a simple network architecture named Texture CNN (T-CNN) which explores this observation. It is built on the idea that the overall shape information extracted by the fully connected layers of a classic CNN is of minor importance in texture analysis. Therefore, we pool an energy measure from the last convolution layer which we connect to a fully connected layer. We show that our approach can improve the performance of a network while greatly reducing the memory usage and computation. version:3
arxiv-1604-07057 | Multi-Fold Gabor, PCA and ICA Filter Convolution Descriptor for Face Recognition | http://arxiv.org/abs/1604.07057 | id:1604.07057 author:Cheng Yaw Low, Andrew Beng Jin Teoh, Cong Jie Ng category:cs.CV  published:2016-04-24 summary:This paper devises a new means of filter diversification, dubbed multi-fold filter convolution (M-FFC), for face recognition. On the assumption that M-FFC receives single-scale Gabor filters of varying orientations as input, these filters are self-cross convolved by M-fold to instantiate an offspring set. The M-FFC flexibility also permits the self-cross convolution amongst Gabor filters and other filter banks of profoundly dissimilar traits, e.g., principal component analysis (PCA) filters, and independent component analysis (ICA) filters, in our case. A 2-FFC instance therefore yields three offspring sets from: (1) Gabor filters solely, (2) Gabor and PCA filters, and (3) Gabor and ICA filters, to render the learning-free and the learning-based 2-FFC descriptors. To facilitate a sensible Gabor filter selection for M-FFC, the 40 multi-scale, multi-orientation Gabor filters is condensed into 8 elementary filters. In addition to that, an average pooling operator is used to leverage the 2-FFC histogram features, prior to whitening PCA compression. The empirical results substantiate that the 2-FFC descriptors prevail over, or on par with, other face descriptors on both identification and verification tasks. version:2
arxiv-1510-04850 | Change Detection in Multivariate Datastreams: Likelihood and Detectability Loss | http://arxiv.org/abs/1510.04850 | id:1510.04850 author:Cesare Alippi, Giacomo Boracchi, Diego Carrera, Manuel Roveri category:stat.ML  published:2015-10-16 summary:We address the problem of detecting changes in multivariate datastreams, and we investigate the intrinsic difficulty that change-detection methods have to face when the data dimension scales. In particular, we consider a general approach where changes are detected by comparing the distribution of the log-likelihood of the datastream over different time windows. Despite the fact that this approach constitutes the frame of several change-detection methods, its effectiveness when data dimension scales has never been investigated, which is indeed the goal of our paper. We show that the magnitude of the change can be naturally measured by the symmetric Kullback-Leibler divergence between the pre- and post-change distributions, and that the detectability of a change of a given magnitude worsens when the data dimension increases. This problem, which we refer to as \emph{detectability loss}, is due to the linear relationship between the variance of the log-likelihood and the data dimension. We analytically derive the detectability loss on Gaussian-distributed datastreams, and empirically demonstrate that this problem holds also on real-world datasets and that can be harmful even at low data-dimensions (say, 10). version:3
arxiv-1604-07953 | Simultaneous Food Localization and Recognition | http://arxiv.org/abs/1604.07953 | id:1604.07953 author:Marc Bolaños, Petia Radeva category:cs.CV  published:2016-04-27 summary:The development of automatic nutrition diaries, which would allow to keep track objectively of everything we eat, could enable a whole new world of possibilities for people concerned about their nutrition patterns. With this purpose, in this paper we propose the first method for simultaneous food localization and recognition. Our method is based on two main steps, which consist in, first, produce a food activation map on the input image (i.e. heat map of probabilities) for generating bounding boxes proposals and, second, recognize each of the food types or food-related objects present in each bounding box. We demonstrate that our proposal, compared to the most similar problem nowadays - object localization, is able to obtain high precision and reasonable recall levels with only a few bounding boxes. Furthermore, we show that it is applicable to both conventional and egocentric images. version:1
arxiv-1604-07952 | Zero-shot object prediction and context modeling using semantic scene knowledge | http://arxiv.org/abs/1604.07952 | id:1604.07952 author:Rene Grzeszick, Gernot A. Fink category:cs.CV  published:2016-04-27 summary:This work will focus on the semantic relations between scenes and objects for visual object recognition. Semantic knowledge can be a powerful source of information especially in scenarios with less or no annotated training samples. These scenarios are referred to as zero-shot recognition and often build on visual attributes. Here, instead of attributes a more direct way is pursued, relating scenes and objects. The contribution of this paper is two-fold: First, it will be shown that scene knowledge can be an important cue for predicting objects in an unsupervised manner. This is especially useful in cluttered scenes where visual recognition may be difficult. Second, it will be shown that this information can easily be integrated as a context model for object detection in a supervised setting. version:1
arxiv-1604-07948 | Graph Laplacian Regularization for Inverse Imaging: Analysis in the Continuous Domain | http://arxiv.org/abs/1604.07948 | id:1604.07948 author:Jiahao Pang, Gene Cheung category:cs.CV  published:2016-04-27 summary:Inverse imaging problems are inherently under-determined, and hence it is important to employ appropriate image priors for regularization. One recent popular prior---the graph Laplacian regularizer---assumes that the target pixel patch is smooth with respect to an appropriately chosen graph. However, the mechanisms and implications of imposing the graph Laplacian regularizer on the original inverse problem are not well understood. To address this problem, in this paper we interpret neighborhood graphs of pixel patches as discrete counterparts of Riemannian manifolds and perform analysis in the continuous domain, providing insights into several fundamental aspects of graph Laplacian regularization. Specifically, we first show the convergence of the graph Laplacian regularizer to a continuous-domain functional, integrating a norm measured in a locally adaptive metric space. Focusing on image denoising, we derive an optimal metric space assuming nonlocal self-similarity of pixel patches, leading to an optimal graph Laplacian regularizer for denoising in the discrete domain. We then interpret graph Laplacian regularization as an anisotropic diffusion scheme to explain its behavior during iterations, e.g., its tendency to promote piecewise smooth signals under certain settings. To verify our analysis, an iterative image denoising algorithm is developed. Experimental results show that our algorithm performs competitively with state-of-the-art denoising methods such as BM3D for natural images, and outperforms them significantly for piecewise smooth images. version:1
arxiv-1603-07054 | A Richly Annotated Dataset for Pedestrian Attribute Recognition | http://arxiv.org/abs/1603.07054 | id:1603.07054 author:Dangwei Li, Zhang Zhang, Xiaotang Chen, Haibin Ling, Kaiqi Huang category:cs.CV  published:2016-03-23 summary:In this paper, we aim to improve the dataset foundation for pedestrian attribute recognition in real surveillance scenarios. Recognition of human attributes, such as gender, and clothes types, has great prospects in real applications. However, the development of suitable benchmark datasets for attribute recognition remains lagged behind. Existing human attribute datasets are collected from various sources or an integration of pedestrian re-identification datasets. Such heterogeneous collection poses a big challenge on developing high quality fine-grained attribute recognition algorithms. Furthermore, human attribute recognition are generally severely affected by environmental or contextual factors, such as viewpoints, occlusions and body parts, while existing attribute datasets barely care about them. To tackle these problems, we build a Richly Annotated Pedestrian (RAP) dataset from real multi-camera surveillance scenarios with long term collection, where data samples are annotated with not only fine-grained human attributes but also environmental and contextual factors. RAP has in total 41,585 pedestrian samples, each of which is annotated with 72 attributes as well as viewpoints, occlusions, body parts information. To our knowledge, the RAP dataset is the largest pedestrian attribute dataset, which is expected to greatly promote the study of large-scale attribute recognition systems. Furthermore, we empirically analyze the effects of different environmental and contextual factors on pedestrian attribute recognition. Experimental results demonstrate that viewpoints, occlusions and body parts information could assist attribute recognition a lot in real applications. version:3
arxiv-1604-07944 | DASC: Robust Dense Descriptor for Multi-modal and Multi-spectral Correspondence Estimation | http://arxiv.org/abs/1604.07944 | id:1604.07944 author:Seungryong Kim, Dongbo Min, Bumsub Ham, Minh N. Do, Kwanghoon Sohn category:cs.CV  published:2016-04-27 summary:Establishing dense correspondences between multiple images is a fundamental task in many applications. However, finding a reliable correspondence in multi-modal or multi-spectral images still remains unsolved due to their challenging photometric and geometric variations. In this paper, we propose a novel dense descriptor, called dense adaptive self-correlation (DASC), to estimate multi-modal and multi-spectral dense correspondences. Based on an observation that self-similarity existing within images is robust to imaging modality variations, we define the descriptor with a series of an adaptive self-correlation similarity measure between patches sampled by a randomized receptive field pooling, in which a sampling pattern is obtained using a discriminative learning. The computational redundancy of dense descriptors is dramatically reduced by applying fast edge-aware filtering. Furthermore, in order to address geometric variations including scale and rotation, we propose a geometry-invariant DASC (GI-DASC) descriptor that effectively leverages the DASC through a superpixel-based representation. For a quantitative evaluation of the GI-DASC, we build a novel multi-modal benchmark as varying photometric and geometric conditions. Experimental results demonstrate the outstanding performance of the DASC and GI-DASC in many cases of multi-modal and multi-spectral dense correspondences. version:1
arxiv-1510-04747 | Tensor vs Matrix Methods: Robust Tensor Decomposition under Block Sparse Perturbations | http://arxiv.org/abs/1510.04747 | id:1510.04747 author:Animashree Anandkumar, Prateek Jain, Yang Shi, U. N. Niranjan category:cs.LG cs.IT math.IT stat.ML  published:2015-10-15 summary:Robust tensor CP decomposition involves decomposing a tensor into low rank and sparse components. We propose a novel non-convex iterative algorithm with guaranteed recovery. It alternates between low-rank CP decomposition through gradient ascent (a variant of the tensor power method), and hard thresholding of the residual. We prove convergence to the globally optimal solution under natural incoherence conditions on the low rank component, and bounded level of sparse perturbations. We compare our method with natural baselines which apply robust matrix PCA either to the {\em flattened} tensor, or to the matrix slices of the tensor. Our method can provably handle a far greater level of perturbation when the sparse tensor is block-structured. This naturally occurs in many applications such as the activity detection task in videos. Our experiments validate these findings. Thus, we establish that tensor methods can tolerate a higher level of gross corruptions compared to matrix methods. version:7
arxiv-1601-03650 | Smoothing parameter estimation framework for IBM word alignment models | http://arxiv.org/abs/1601.03650 | id:1601.03650 author:Vuong Van Bui, Cuong Anh Le category:cs.CL  published:2016-01-14 summary:IBM models are very important word alignment models in Machine Translation. Following the Maximum Likelihood Estimation principle to estimate their parameters, the models will easily overfit the training data when the data are sparse. While smoothing is a very popular solution in Language Model, there still lacks studies on smoothing for word alignment. In this paper, we propose a framework which generalizes the notable work Moore [2004] of applying additive smoothing to word alignment models. The framework allows developers to customize the smoothing amount for each pair of word. The added amount will be scaled appropriately by a common factor which reflects how much the framework trusts the adding strategy according to the performance on data. We also carefully examine various performance criteria and propose a smoothened version of the error count, which generally gives the best result. version:4
arxiv-1512-08949 | Simple, Robust and Optimal Ranking from Pairwise Comparisons | http://arxiv.org/abs/1512.08949 | id:1512.08949 author:Nihar B. Shah, Martin J. Wainwright category:cs.LG cs.AI cs.IT math.IT stat.ML  published:2015-12-30 summary:We consider data in the form of pairwise comparisons of n items, with the goal of precisely identifying the top k items for some value of k < n, or alternatively, recovering a ranking of all the items. We analyze the Copeland counting algorithm that ranks the items in order of the number of pairwise comparisons won, and show it has three attractive features: (a) its computational efficiency leads to speed-ups of several orders of magnitude in computation time as compared to prior work; (b) it is robust in that theoretical guarantees impose no conditions on the underlying matrix of pairwise-comparison probabilities, in contrast to some prior work that applies only to the BTL parametric model; and (c) it is an optimal method up to constant factors, meaning that it achieves the information-theoretic limits for recovering the top k-subset. We extend our results to obtain sharp guarantees for approximate recovery under the Hamming distortion metric, and more generally, to any arbitrary error requirement that satisfies a simple and natural monotonicity condition. version:2
arxiv-1511-01419 | Train and Test Tightness of LP Relaxations in Structured Prediction | http://arxiv.org/abs/1511.01419 | id:1511.01419 author:Ofer Meshi, Mehrdad Mahdavi, Adrian Weller, David Sontag category:stat.ML cs.AI cs.LG  published:2015-11-04 summary:Structured prediction is used in areas such as computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation to the striking observation that approximations based on linear programming (LP) relaxations are often tight on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that tightness generalizes from train to test data. version:3
arxiv-1602-05682 | Audio Recording Device Identification Based on Deep Learning | http://arxiv.org/abs/1602.05682 | id:1602.05682 author:Simeng Qi, Zheng Huang, Yan Li, Shaopei Shi category:cs.SD cs.LG  published:2016-02-18 summary:In this paper we present a research on identification of audio recording devices from background noise, thus providing a method for forensics. The audio signal is the sum of speech signal and noise signal. Usually, people pay more attention to speech signal, because it carries the information to deliver. So a great amount of researches have been dedicated to getting higher Signal-Noise-Ratio (SNR). There are many speech enhancement algorithms to improve the quality of the speech, which can be seen as reducing the noise. However, noises can be regarded as the intrinsic fingerprint traces of an audio recording device. These digital traces can be characterized and identified by new machine learning techniques. Therefore, in our research, we use the noise as the intrinsic features. As for the identification, multiple classifiers of deep learning methods are used and compared. The identification result shows that the method of getting feature vector from the noise of each device and identifying them with deep learning techniques is viable, and well-preformed. version:2
arxiv-1604-07904 | Image Colorization Using a Deep Convolutional Neural Network | http://arxiv.org/abs/1604.07904 | id:1604.07904 author:Tung Nguyen, Kazuki Mori, Ruck Thawonmas category:cs.CV cs.LG cs.NE I.2.6; I.4.9; J.5  published:2016-04-27 summary:In this paper, we present a novel approach that uses deep learning techniques for colorizing grayscale images. By utilizing a pre-trained convolutional neural network, which is originally designed for image classification, we are able to separate content and style of different images and recombine them into a single image. We then propose a method that can add colors to a grayscale image by combining its content with style of a color image having semantic similarity with the grayscale one. As an application, to our knowledge the first of its kind, we use the proposed method to colorize images of ukiyo-e a genre of Japanese painting?and obtain interesting results, showing the potential of this method in the growing field of computer assisted art. version:1
arxiv-1602-03258 | Interactive Bayesian Hierarchical Clustering | http://arxiv.org/abs/1602.03258 | id:1602.03258 author:Sharad Vikram, Sanjoy Dasgupta category:cs.LG  published:2016-02-10 summary:Clustering is a powerful tool in data analysis, but it is often difficult to find a grouping that aligns with a user's needs. To address this, several methods incorporate constraints obtained from users into clustering algorithms, but unfortunately do not apply to hierarchical clustering. We design an interactive Bayesian algorithm that incorporates user interaction into hierarchical clustering while still utilizing the geometry of the data by sampling a constrained posterior distribution over hierarchies. We also suggest several ways to intelligently query a user. The algorithm, along with the querying schemes, shows promising results on real data. version:3
arxiv-1604-04327 | Invariant feature extraction from event based stimuli | http://arxiv.org/abs/1604.04327 | id:1604.04327 author:Thusitha N. Chandrapala, Bertram E. Shi category:cs.CV  published:2016-04-15 summary:We propose a novel architecture, the event-based GASSOM for learning and extracting invariant representations from event streams originating from neuromorphic vision sensors. The framework is inspired by feed-forward cortical models for visual processing. The model, which is based on the concepts of sparsity and temporal slowness, is able to learn feature extractors that resemble neurons in the primary visual cortex. Layers of units in the proposed model can be cascaded to learn feature extractors with different levels of complexity and selectivity. We explore the applicability of the framework on real world tasks by using the learned network for object recognition. The proposed model achieve higher classification accuracy compared to other state-of-the-art event based processing methods. Our results also demonstrate the generality and robustness of the method, as the recognizers for different data sets and different tasks all used the same set of learned feature detectors, which were trained on data collected independently of the testing data. version:2
arxiv-1604-08500 | Detection of epileptic seizure in EEG signals using linear least squares preprocessing | http://arxiv.org/abs/1604.08500 | id:1604.08500 author:Z. Roshan Zamir category:cs.LG math.OC  published:2016-04-27 summary:An epileptic seizure is a transient event of abnormal excessive neuronal discharge in the brain. This unwanted event can be obstructed by detection of electrical changes in the brain that happen before the seizure takes place. The automatic detection of seizures is necessary since the visual screening of EEG recordings is a time consuming task and requires experts to improve the diagnosis. Four linear least squares-based preprocessing models are proposed to extract key features of an EEG signal in order to detect seizures. The first two models are newly developed. The original signal (EEG) is approximated by a sinusoidal curve. Its amplitude is formed by a polynomial function and compared with the pre developed spline function.Different statistical measures namely classification accuracy, true positive and negative rates, false positive and negative rates and precision are utilized to assess the performance of the proposed models. These metrics are derived from confusion matrices obtained from classifiers. Different classifiers are used over the original dataset and the set of extracted features. The proposed models significantly reduce the dimension of the classification problem and the computational time while the classification accuracy is improved in most cases. The first and third models are promising feature extraction methods. Logistic, LazyIB1, LazyIB5 and J48 are the best classifiers. Their true positive and negative rates are $1$ while false positive and negative rates are zero and the corresponding precision values are $1$. Numerical results suggest that these models are robust and efficient for detecting epileptic seizure. version:1
arxiv-1512-06735 | Instance-Level Segmentation for Autonomous Driving with Deep Densely Connected MRFs | http://arxiv.org/abs/1512.06735 | id:1512.06735 author:Ziyu Zhang, Sanja Fidler, Raquel Urtasun category:cs.CV  published:2015-12-21 summary:Our aim is to provide a pixel-wise instance-level labeling of a monocular image in the context of autonomous driving. We build on recent work [Zhang et al., ICCV15] that trained a convolutional neural net to predict instance labeling in local image patches, extracted exhaustively in a stride from an image. A simple Markov random field model using several heuristics was then proposed in [Zhang et al., ICCV15] to derive a globally consistent instance labeling of the image. In this paper, we formulate the global labeling problem with a novel densely connected Markov random field and show how to encode various intuitive potentials in a way that is amenable to efficient mean field inference [Kr\"ahenb\"uhl et al., NIPS11]. Our potentials encode the compatibility between the global labeling and the patch-level predictions, contrast-sensitive smoothness as well as the fact that separate regions form different instances. Our experiments on the challenging KITTI benchmark [Geiger et al., CVPR12] demonstrate that our method achieves a significant performance boost over the baseline [Zhang et al., ICCV15]. version:2
arxiv-1506-02753 | Inverting Visual Representations with Convolutional Networks | http://arxiv.org/abs/1506.02753 | id:1506.02753 author:Alexey Dosovitskiy, Thomas Brox category:cs.NE cs.CV cs.LG  published:2015-06-09 summary:Feature representations, both hand-designed and learned ones, are often hard to analyze and interpret, even when they are extracted from visual data. We propose a new approach to study image representations by inverting them with an up-convolutional neural network. We apply the method to shallow representations (HOG, SIFT, LBP), as well as to deep networks. For shallow representations our approach provides significantly better reconstructions than existing methods, revealing that there is surprisingly rich information contained in these features. Inverting a deep network trained on ImageNet provides several insights into the properties of the feature representation learned by the network. Most strikingly, the colors and the rough contours of an image can be reconstructed from activations in higher network layers and even from the predicted class probabilities. version:4
arxiv-1511-04776 | Mixtures of Sparse Autoregressive Networks | http://arxiv.org/abs/1511.04776 | id:1511.04776 author:Marc Goessling, Yali Amit category:stat.ML cs.LG  published:2015-11-15 summary:We consider high-dimensional distribution estimation through autoregressive networks. By combining the concepts of sparsity, mixtures and parameter sharing we obtain a simple model which is fast to train and which achieves state-of-the-art or better results on several standard benchmark datasets. Specifically, we use an L1-penalty to regularize the conditional distributions and introduce a procedure for automatic parameter sharing between mixture components. Moreover, we propose a simple distributed representation which permits exact likelihood evaluations since the latent variables are interleaved with the observable variables and can be easily integrated out. Our model achieves excellent generalization performance and scales well to extremely high dimensions. version:4
arxiv-1604-07878 | Evaluating the effect of topic consideration in identifying communities of rating-based social networks | http://arxiv.org/abs/1604.07878 | id:1604.07878 author:Ali Reihanian, Behrouz Minaei-Bidgoli, Muhammad Yousefnezhad category:cs.SI cs.LG stat.ML  published:2016-04-26 summary:Finding meaningful communities in social network has attracted the attentions of many researchers. The community structure of complex networks reveals both their organization and hidden relations among their constituents. Most of the researches in the field of community detection mainly focus on the topological structure of the network without performing any content analysis. Nowadays, real world social networks are containing a vast range of information including shared objects, comments, following information, etc. In recent years, a number of researches have proposed approaches which consider both the contents that are interchanged in the networks and the topological structures of the networks in order to find more meaningful communities. In this research, the effect of topic analysis in finding more meaningful communities in social networking sites in which the users express their feelings toward different objects (like movies) by the means of rating is demonstrated by performing extensive experiments. version:1
arxiv-1604-07872 | Are Face and Object Recognition Independent? A Neurocomputational Modeling Exploration | http://arxiv.org/abs/1604.07872 | id:1604.07872 author:Panqu Wang, Isabel Gauthier, Garrison Cottrell category:q-bio.NC cs.CV  published:2016-04-26 summary:Are face and object recognition abilities independent? Although it is commonly believed that they are, Gauthier et al.(2014) recently showed that these abilities become more correlated as experience with nonface categories increases. They argued that there is a single underlying visual ability, v, that is expressed in performance with both face and nonface categories as experience grows. Using the Cambridge Face Memory Test and the Vanderbilt Expertise Test, they showed that the shared variance between Cambridge Face Memory Test and Vanderbilt Expertise Test performance increases monotonically as experience increases. Here, we address why a shared resource across different visual domains does not lead to competition and to an inverse correlation in abilities? We explain this conundrum using our neurocomputational model of face and object processing (The Model, TM). Our results show that, as in the behavioral data, the correlation between subordinate level face and object recognition accuracy increases as experience grows. We suggest that different domains do not compete for resources because the relevant features are shared between faces and objects. The essential power of experience is to generate a "spreading transform" for faces that generalizes to objects that must be individuated. Interestingly, when the task of the network is basic level categorization, no increase in the correlation between domains is observed. Hence, our model predicts that it is the type of experience that matters and that the source of the correlation is in the fusiform face area, rather than in cortical areas that subserve basic level categorization. This result is consistent with our previous modeling elucidating why the FFA is recruited for novel domains of expertise (Tong et al., 2008). version:1
arxiv-1604-07809 | Entities as topic labels: Improving topic interpretability and evaluability combining Entity Linking and Labeled LDA | http://arxiv.org/abs/1604.07809 | id:1604.07809 author:Federico Nanni, Pablo Ruiz Fabo category:cs.CL  published:2016-04-26 summary:In order to create a corpus exploration method providing topics that are easier to interpret than standard LDA topic models, here we propose combining two techniques called Entity linking and Labeled LDA. Our method identifies in an ontology a series of descriptive labels for each document in a corpus. Then it generates a specific topic for each label. Having a direct relation between topics and labels makes interpretation easier; using an ontology as background knowledge limits label ambiguity. As our topics are described with a limited number of clear-cut labels, they promote interpretability, and this may help quantitative evaluation. We illustrate the potential of the approach by applying it in order to define the most relevant topics addressed by each party in the European Parliament's fifth mandate (1999-2004). version:1
arxiv-1604-07806 | Using Indirect Encoding of Multiple Brains to Produce Multimodal Behavior | http://arxiv.org/abs/1604.07806 | id:1604.07806 author:Jacob Schrum, Joel Lehman, Sebastian Risi category:cs.AI cs.NE  published:2016-04-26 summary:An important challenge in neuroevolution is to evolve complex neural networks with multiple modes of behavior. Indirect encodings can potentially answer this challenge. Yet in practice, indirect encodings do not yield effective multimodal controllers. Thus, this paper introduces novel multimodal extensions to HyperNEAT, a popular indirect encoding. A previous multimodal HyperNEAT approach called situational policy geometry assumes that multiple brains benefit from being embedded within an explicit geometric space. However, experiments here illustrate that this assumption unnecessarily constrains evolution, resulting in lower performance. Specifically, this paper introduces HyperNEAT extensions for evolving many brains without assuming geometric relationships between them. The resulting Multi-Brain HyperNEAT can exploit human-specified task divisions to decide when each brain controls the agent, or can automatically discover when brains should be used, by means of preference neurons. A further extension called module mutation allows evolution to discover the number of brains, enabling multimodal behavior with even less expert knowledge. Experiments in several multimodal domains highlight that multi-brain approaches are more effective than HyperNEAT without multimodal extensions, and show that brains without a geometric relation to each other outperform situational policy geometry. The conclusion is that Multi-Brain HyperNEAT provides several promising techniques for evolving complex multimodal behavior. version:1
arxiv-1604-07796 | Scale Normalization | http://arxiv.org/abs/1604.07796 | id:1604.07796 author:Henry Z. Lo, Kevin Amaral, Wei Ding category:cs.NE cs.LG stat.ML  published:2016-04-26 summary:One of the difficulties of training deep neural networks is caused by improper scaling between layers. Scaling issues introduce exploding / gradient problems, and have typically been addressed by careful scale-preserving initialization. We investigate the value of preserving scale, or isometry, beyond the initial weights. We propose two methods of maintaing isometry, one exact and one stochastic. Preliminary experiments show that for both determinant and scale-normalization effectively speeds up learning. Results suggest that isometry is important in the beginning of learning, and maintaining it leads to faster learning. version:1
arxiv-1604-07788 | A Framework for Human Pose Estimation in Videos | http://arxiv.org/abs/1604.07788 | id:1604.07788 author:Dong Zhang, Mubarak Shah category:cs.CV  published:2016-04-26 summary:In this paper, we present a method to estimate a sequence of human poses in unconstrained videos. We aim to demonstrate that by using temporal information, the human pose estimation results can be improved over image based pose estimation methods. In contrast to the commonly employed graph optimization formulation, which is NP-hard and needs approximate solutions, we formulate this problem into a unified two stage tree-based optimization problem for which an efficient and exact solution exists. Although the proposed method finds an exact solution, it does not sacrifice the ability to model the spatial and temporal constraints between body parts in the frames; in fact it models the {\em symmetric} parts better than the existing methods. The proposed method is based on two main ideas: `Abstraction' and `Association' to enforce the intra- and inter-frame body part constraints without inducing extra computational complexity to the polynomial time solution. Using the idea of `Abstraction', a new concept of `abstract body part' is introduced to conceptually combine the symmetric body parts and model them in the tree based body part structure. Using the idea of `Association', the optimal tracklets are generated for each abstract body part, in order to enforce the spatiotemporal constraints between body parts in adjacent frames. A sequence of the best poses is inferred from the abstract body part tracklets through the tree-based optimization. Finally, the poses are refined by limb alignment and refinement schemes. We evaluated the proposed method on three publicly available video based human pose estimation datasets, and obtained dramatically improved performance compared to the state-of-the-art methods. version:1
arxiv-1604-07741 | EgoSampling: Wide View Hyperlapse from Single and Multiple Egocentric Videos | http://arxiv.org/abs/1604.07741 | id:1604.07741 author:Tavi Halperin, Yair Poleg, Chetan Arora, Shmuel Peleg category:cs.CV cs.MM  published:2016-04-26 summary:The possibility of sharing one's point of view makes use of wearable cameras compelling. These videos are often long, boring and coupled with extreme shake as the camera is worn on a moving person. Fast forwarding (i.e. frame sampling) is a natural choice for faster video browsing. However, this accentuates the shake caused by natural head motion in an egocentric video, making the fast forwarded video useless. We propose EgoSampling, an adaptive frame sampling that gives more stable, fast forwarded, hyperlapse videos. Adaptive frame sampling is formulated as energy minimization, whose optimal solution can be found in polynomial time. We further turn the camera shake from a drawback into a feature, enabling the increase of the field-of-view. This is obtained when each output frame is mosaiced from several input frames. Stitching multiple frames also enables the generation of a single hyperlapse video from multiple egocentric videos, allowing even faster video consumption. version:1
arxiv-1511-06321 | Neural network-based clustering using pairwise constraints | http://arxiv.org/abs/1511.06321 | id:1511.06321 author:Yen-Chang Hsu, Zsolt Kira category:cs.LG stat.ML  published:2015-11-19 summary:This paper presents a neural network-based end-to-end clustering framework. We design a novel strategy to utilize the contrastive criteria for pushing data-forming clusters directly from raw data, in addition to learning a feature embedding suitable for such clustering. The network is trained with weak labels, specifically partial pairwise relationships between data instances. The cluster assignments and their probabilities are then obtained at the output layer by feed-forwarding the data. The framework has the interesting characteristic that no cluster centers need to be explicitly specified, thus the resulting cluster distribution is purely data-driven and no distance metrics need to be predefined. The experiments show that the proposed approach beats the conventional two-stage method (feature embedding with k-means) by a significant margin. It also compares favorably to the performance of the standard cross entropy loss for classification. Robustness analysis also shows that the method is largely insensitive to the number of clusters. Specifically, we show that the number of dominant clusters is close to the true number of clusters even when a large k is used for clustering. version:5
arxiv-1505-06770 | Sketching for Sequential Change-Point Detection | http://arxiv.org/abs/1505.06770 | id:1505.06770 author:Yang Cao, Andrew Thompson, Meng Wang, Yao Xie category:cs.LG stat.ML  published:2015-05-25 summary:We study sequential change-point detection using sketches (linear projections) of high-dimensional signal vectors, by presenting the sketching procedures that are derived based on the generalized likelihood ratio statistic. We consider both fixed and time-varying projections, and derive theoretical approximations to two fundamental performance metrics: the average run length (ARL) and the expected detection delay (EDD); these approximations are shown to be highly accurate by numerical simulations. We also characterize the performance of the procedure when the projection is a Gaussian random projection or a sparse 0-1 matrix (in particular, an expander graph). Finally, we demonstrate the good performance of the sketching performance using simulation and real-data examples on solar flare detection and failure detection in power networks. version:3
arxiv-1604-07711 | Condorcet's Jury Theorem for Consensus Clustering | http://arxiv.org/abs/1604.07711 | id:1604.07711 author:Brijnesh J. Jain category:stat.ML cs.LG  published:2016-04-26 summary:The goal of consensus clustering is to improve the quality of clustering by combining a sample of partitions of a dataset to a single consensus partition. This contribution extends Condorcet's Jury Theorem to the mean partition approach of consensus clustering. As a consequence of the proposed result, we challenge and reappraise the role of diversity in consensus clustering. version:1
arxiv-1604-07704 | Tournament selection in zeroth-level classifier systems based on average reward reinforcement learning | http://arxiv.org/abs/1604.07704 | id:1604.07704 author:Zhaoxiang Zang, Zhao Li, Junying Wang, Zhiping Dan category:cs.AI cs.NE I.2  published:2016-04-26 summary:As a genetics-based machine learning technique, zeroth-level classifier system (ZCS) is based on a discounted reward reinforcement learning algorithm, bucket-brigade algorithm, which optimizes the discounted total reward received by an agent but is not suitable for all multi-step problems, especially large-size ones. There are some undiscounted reinforcement learning methods available, such as R-learning, which optimize the average reward per time step. In this paper, R-learning is used as the reinforcement learning employed by ZCS, to replace its discounted reward reinforcement learning approach, and tournament selection is used to replace roulette wheel selection in ZCS. The modification results in classifier systems that can support long action chains, and thus is able to solve large multi-step problems. version:1
arxiv-1604-07681 | Efficient Splitting-based Method for Global Image Smoothing | http://arxiv.org/abs/1604.07681 | id:1604.07681 author:Youngjung Kim, Dongbo Min, Bumsub Ham, Kwanghoon Sohn category:cs.CV  published:2016-04-26 summary:Edge-preserving smoothing (EPS) can be formulated as minimizing an objective function that consists of data and prior terms. This global EPS approach shows better smoothing performance than a local one that typically has a form of weighted averaging, at the price of high computational cost. In this paper, we introduce a highly efficient splitting-based method for global EPS that minimizes the objective function of ${l_2}$ data and prior terms (possibly non-smooth and non-convex) in linear time. Different from previous splitting-based methods that require solving a large linear system, our approach solves an equivalent constrained optimization problem, resulting in a sequence of 1D sub-problems. This enables linear time solvers for weighted-least squares and -total variation problems. Our solver converges quickly, and its runtime is even comparable to state-of-the-art local EPS approaches. We also propose a family of fast iteratively re-weighted algorithms using a non-convex prior term. Experimental results demonstrate the effectiveness and flexibility of our approach in a range of computer vision and image processing tasks. version:1
arxiv-1604-07669 | Real-time Action Recognition with Enhanced Motion Vector CNNs | http://arxiv.org/abs/1604.07669 | id:1604.07669 author:Bowen Zhang, Limin Wang, Zhe Wang, Yu Qiao, Hanli Wang category:cs.CV  published:2016-04-26 summary:The deep two-stream architecture exhibited excellent performance on video based action recognition. The most computationally expensive step in this approach comes from the calculation of optical flow which prevents it to be real-time. This paper accelerates this architecture by replacing optical flow with motion vector which can be obtained directly from compressed videos without extra calculation. However, motion vector lacks fine structures, and contains noisy and inaccurate motion patterns, leading to the evident degradation of recognition performance. Our key insight for relieving this problem is that optical flow and motion vector are inherent correlated. Transferring the knowledge learned with optical flow CNN to motion vector CNN can significantly boost the performance of the latter. Specifically, we introduce three strategies for this, initialization transfer, supervision transfer and their combination. Experimental results show that our method achieves comparable recognition performance to the state-of-the-art, while our method can process 390.7 frames per second, which is 27 times faster than the original two-stream method. version:1
arxiv-1511-06624 | TEMPO: Feature-Endowed Teichmüller Extremal Mappings of Point Clouds | http://arxiv.org/abs/1511.06624 | id:1511.06624 author:Ting Wei Meng, Gary Pui-Tung Choi, Lok Ming Lui category:cs.CG cs.CV cs.GR math.DG  published:2015-11-20 summary:In recent decades, the use of 3D point clouds has been widespread in computer industry. The development of techniques in analyzing point clouds is increasingly important. In particular, mapping of point clouds has been a challenging problem. In this paper, we develop a discrete analogue of the Teichm\"{u}ller extremal mappings, which guarantee uniform conformality distortions, on point cloud surfaces. Based on the discrete analogue, we propose a novel method called TEMPO for computing Teichm\"{u}ller extremal mappings between feature-endowed point clouds. Using our proposed method, the Teichm\"{u}ller metric is introduced for evaluating the dissimilarity of point clouds. Consequently, our algorithm enables accurate recognition and classification of point clouds. Experimental results demonstrate the effectiveness of our proposed method. version:2
arxiv-1604-07638 | Online Influence Maximization in Non-Stationary Social Networks | http://arxiv.org/abs/1604.07638 | id:1604.07638 author:Yixin Bao, Xiaoke Wang, Zhi Wang, Chuan Wu, Francis C. M. Lau category:cs.SI cs.DS cs.LG  published:2016-04-26 summary:Social networks have been popular platforms for information propagation. An important use case is viral marketing: given a promotion budget, an advertiser can choose some influential users as the seed set and provide them free or discounted sample products; in this way, the advertiser hopes to increase the popularity of the product in the users' friend circles by the world-of-mouth effect, and thus maximizes the number of users that information of the production can reach. There has been a body of literature studying the influence maximization problem. Nevertheless, the existing studies mostly investigate the problem on a one-off basis, assuming fixed known influence probabilities among users, or the knowledge of the exact social network topology. In practice, the social network topology and the influence probabilities are typically unknown to the advertiser, which can be varying over time, i.e., in cases of newly established, strengthened or weakened social ties. In this paper, we focus on a dynamic non-stationary social network and design a randomized algorithm, RSB, based on multi-armed bandit optimization, to maximize influence propagation over time. The algorithm produces a sequence of online decisions and calibrates its explore-exploit strategy utilizing outcomes of previous decisions. It is rigorously proven to achieve an upper-bounded regret in reward and applicable to large-scale social networks. Practical effectiveness of the algorithm is evaluated using both synthetic and real-world datasets, which demonstrates that our algorithm outperforms previous stationary methods under non-stationary conditions. version:1
arxiv-1505-07909 | Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered Word Embedding | http://arxiv.org/abs/1505.07909 | id:1505.07909 author:Huazheng Wang, Fei Tian, Bin Gao, Jiang Bian, Tie-Yan Liu category:cs.CL cs.IR cs.LG  published:2015-05-29 summary:Intelligence Quotient (IQ) Test is a set of standardized questions designed to evaluate human intelligence. Verbal comprehension questions appear very frequently in IQ tests, which measure human's verbal ability including the understanding of the words with multiple senses, the synonyms and antonyms, and the analogies among words. In this work, we explore whether such tests can be solved automatically by artificial intelligence technologies, especially the deep learning technologies that are recently developed and successfully applied in a number of fields. However, we found that the task was quite challenging, and simply applying existing technologies (e.g., word embedding) could not achieve a good performance, mainly due to the multiple senses of words and the complex relations among words. To tackle these challenges, we propose a novel framework consisting of three components. First, we build a classifier to recognize the specific type of a verbal question (e.g., analogy, classification, synonym, or antonym). Second, we obtain distributed representations of words and relations by leveraging a novel word embedding method that considers the multi-sense nature of words and the relational knowledge among words (or their senses) contained in dictionaries. Third, for each type of questions, we propose a specific solver based on the obtained distributed word representations and relation representations. Experimental results have shown that the proposed framework can not only outperform existing methods for solving verbal comprehension questions but also exceed the average performance of the Amazon Mechanical Turk workers involved in the study. The results indicate that with appropriate uses of the deep learning technologies we might be a further step closer to the human intelligence. version:4
arxiv-1307-0426 | An Empirical Study into Annotator Agreement, Ground Truth Estimation, and Algorithm Evaluation | http://arxiv.org/abs/1307.0426 | id:1307.0426 author:Thomas A. Lampert, André Stumpf, Pierre Gançarski category:cs.CV cs.AI I.4.6; I.5.4  published:2013-07-01 summary:Although agreement between annotators has been studied in the past from a statistical viewpoint, little work has attempted to quantify the extent to which this phenomenon affects the evaluation of computer vision (CV) object detection algorithms. Many researchers utilise ground truth (GT) in experiments and more often than not this GT is derived from one annotator's opinion. How does the difference in opinion affect an algorithm's evaluation? Four examples of typical CV problems are chosen, and a methodology is applied to each to quantify the inter-annotator variance and to offer insight into the mechanisms behind agreement and the use of GT. It is found that when detecting linear objects annotator agreement is very low. The agreement in object position, linear or otherwise, can be partially explained through basic image properties. Automatic object detectors are compared to annotator agreement and it is found that a clear relationship exists. Several methods for calculating GTs from a number of annotations are applied and the resulting differences in the performance of the object detectors are quantified. It is found that the rank of a detector is highly dependent upon the method used to form the GT. It is also found that although the STAPLE and LSML GT estimation methods appear to represent the mean of the performance measured using the individual annotations, when there are few annotations, or there is a large variance in them, these estimates tend to degrade. Furthermore, one of the most commonly adopted annotation combination methods--consensus voting--accentuates more obvious features, which results in an overestimation of the algorithm's performance. Finally, it is concluded that in some datasets it may not be possible to state with any confidence that one algorithm outperforms another when evaluating upon one GT and a method for calculating confidence bounds is discussed. version:3
arxiv-1604-07602 | Spot On: Action Localization from Pointly-Supervised Proposals | http://arxiv.org/abs/1604.07602 | id:1604.07602 author:Pascal Mettes, Jan C. van Gemert, Cees G. M. Snoek category:cs.CV  published:2016-04-26 summary:We strive for spatio-temporal localization of actions in videos. The state-of-the-art relies on action proposals at test time and selects the best one with a classifier demanding carefully annotated box annotations at train time. Annotating action boxes in video is cumbersome, tedious, and error prone. Rather than annotating boxes, we propose to annotate actions in video with points on a sparse subset of frames only. We introduce an overlap measure between action proposals and points and incorporate them all into the objective of a non-convex Multiple Instance Learning optimization. Experimental evaluation on the UCF Sports and UCF 101 datasets shows that (i) spatio-temporal proposals can be used to train classifiers while retaining the localization performance, (ii) point annotations yield results comparable to box annotations while being significantly faster to annotate, (iii) with a minimum amount of supervision our approach is competitive to the state-of-the-art. Finally, we introduce spatio-temporal action annotations on the train and test videos of Hollywood2, resulting in Hollywood2Tubes, available at tinyurl.com/hollywood2tubes. version:1
arxiv-1604-07554 | A New Approach in Persian Handwritten Letters Recognition Using Error Correcting Output Coding | http://arxiv.org/abs/1604.07554 | id:1604.07554 author:Maziar Kazemi, Muhammad Yousefnezhad, Saber Nourian category:cs.CV cs.LG stat.ML  published:2016-04-26 summary:Classification Ensemble, which uses the weighed polling of outputs, is the art of combining a set of basic classifiers for generating high-performance, robust and more stable results. This study aims to improve the results of identifying the Persian handwritten letters using Error Correcting Output Coding (ECOC) ensemble method. Furthermore, the feature selection is used to reduce the costs of errors in our proposed method. ECOC is a method for decomposing a multi-way classification problem into many binary classification tasks; and then combining the results of the subtasks into a hypothesized solution to the original problem. Firstly, the image features are extracted by Principal Components Analysis (PCA). After that, ECOC is used for identification the Persian handwritten letters which it uses Support Vector Machine (SVM) as the base classifier. The empirical results of applying this ensemble method using 10 real-world data sets of Persian handwritten letters indicate that this method has better results in identifying the Persian handwritten letters than other ensemble methods and also single classifications. Moreover, by testing a number of different features, this paper found that we can reduce the additional cost in feature selection stage by using this method. version:1
arxiv-1510-02513 | A novel mutation operator based on the union of fitness and design spaces information for Differential Evolution | http://arxiv.org/abs/1510.02513 | id:1510.02513 author:H. Sharifi Noghabi, H. Rajabi Mashhadi, K. Shojaei category:cs.NE  published:2015-10-08 summary:Differential Evolution (DE) is one of the most successful and powerful evolutionary algorithms for global optimization problem. The most important operator in this algorithm is mutation operator which parents are selected randomly to participate in it. Recently, numerous papers are tried to make this operator more intelligent by selection of parents for mutation intelligently. The intelligent selection for mutation vectors is performed by applying design space (also known as decision space) criterion or fitness space criterion, however, in both cases, half of valuable information of the problem space is disregarded. In this article, a Universal Differential Evolution (UDE) is proposed which takes advantage of both design and fitness spaces criteria for intelligent selection of mutation vectors. The experimental analysis on UDE are performed on CEC2005 benchmarks and the results stated that UDE significantly improved the performance of differential evolution in comparison with other methods that only use one criterion for intelligent selection. version:2
arxiv-1604-07547 | Towards Miss Universe Automatic Prediction: The Evening Gown Competition | http://arxiv.org/abs/1604.07547 | id:1604.07547 author:Johanna Carvajal, Arnold Wiliem, Conrad Sanderson, Brian Lovell category:cs.CV cs.CY cs.MM 68T45  published:2016-04-26 summary:Can we predict the winner of Miss Universe after watching how they strode down the catwalk during the evening gown competition? Fashion gurus say they can! In our work, we study this question from the perspective of computer vision. In particular, we want to understand whether existing computer vision approaches can be used to automatically extract the qualities exhibited by the Miss Universe winners during their catwalk. This study could pave the way towards new vision-based applications for the fashion industry. To this end, we propose a novel video dataset, called the Miss Universe dataset, comprising 10 years of the evening gown competition selected between 1996-2010. We further propose two ranking-related problems: (1) the Miss Universe Listwise Ranking and (2) the Miss Universe Pairwise Ranking problems. In addition, we also develop an approach that simultaneously addresses the two proposed problems. To describe the videos we employ the recently proposed Stacked Fisher Vectors in conjunction with robust local spatio-temporal features. From our evaluation we found that although the addressed problems are extremely challenging, the proposed system is able to rank the winner in the top 3 best predicted scores for 5 out of 10 Miss Universe competitions. version:1
arxiv-1604-06626 | The Mean Partition Theorem of Consensus Clustering | http://arxiv.org/abs/1604.06626 | id:1604.06626 author:Brijnesh J. Jain category:cs.LG cs.CV stat.ML  published:2016-04-22 summary:To devise efficient solutions for approximating a mean partition in consensus clustering, Dimitriadou et al. [3] presented a necessary condition of optimality for a consensus function based on least square distances. We show that their result is pivotal for deriving interesting properties of consensus clustering beyond optimization. For this, we present the necessary condition of optimality in a slightly stronger form in terms of the Mean Partition Theorem and extend it to the Expected Partition Theorem. To underpin its versatility, we show three examples that apply the Mean Partition Theorem: (i) equivalence of the mean partition and optimal multiple alignment, (ii) construction of profiles and motifs, and (iii) relationship between consensus clustering and cluster stability. version:2
arxiv-1604-07528 | Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification | http://arxiv.org/abs/1604.07528 | id:1604.07528 author:Tong Xiao, Hongsheng Li, Wanli Ouyang, Xiaogang Wang category:cs.CV  published:2016-04-26 summary:Learning generic and robust feature representations with data from multiple domains for the same problem is of great value, especially for the problems that have multiple datasets but none of them are large enough to provide abundant data variations. In this work, we present a pipeline for learning deep feature representations from multiple domains with Convolutional Neural Networks (CNNs). When training a CNN with data from all the domains, some neurons learn representations shared across several domains, while some others are effective only for a specific one. Based on this important observation, we propose a Domain Guided Dropout algorithm to improve the feature learning procedure. Experiments show the effectiveness of our pipeline and the proposed algorithm. Our methods on the person re-identification problem outperform state-of-the-art methods on multiple datasets by large margins. version:1
arxiv-1511-08400 | Regularizing RNNs by Stabilizing Activations | http://arxiv.org/abs/1511.08400 | id:1511.08400 author:David Krueger, Roland Memisevic category:cs.NE cs.CL cs.LG stat.ML  published:2015-11-26 summary:We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms. This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modeling and phoneme recognition, and outperforming weight noise and dropout. We achieve competitive performance (18.6\% PER) on the TIMIT phoneme recognition task for RNNs evaluated without beam search or an RNN transducer. With this penalty term, IRNN can achieve similar performance to LSTM on language modeling, although adding the penalty term to the LSTM results in superior performance. Our penalty term also prevents the exponential growth of IRNN's activations outside of their training horizon, allowing them to generalize to much longer sequences. version:7
arxiv-1604-07513 | Semantic Change Detection with Hypermaps | http://arxiv.org/abs/1604.07513 | id:1604.07513 author:Hirokatsu Kataoka, Soma Shirakabe, Yudai Miyashita, Akio Nakamura, Kenji Iwata, Yutaka Satoh category:cs.CV cs.AI  published:2016-04-26 summary:Change detection is the study of detecting changes between two different images of a scene taken at different times. This paper proposes the concept of semantic change detection, which involves intuitively inserting semantic meaning into detected change areas. The problem to be solved consists of two parts, semantic segmentation and change detection. In order to solve this problem and obtain a high-level of performance, we propose an improvement to the hypercolumns representation, hereafter known as hypermaps, which effectively uses convolutional maps obtained from convolutional neural networks (CNNs). We also employ multi-scale feature representation captured by different image patches. We applied our method to the TSUNAMI Panoramic Change Detection dataset, and re-annotated the changed areas of the dataset via semantic classes. The results show that our multi-scale hypermaps provided outstanding performance on the re-annotated TSUNAMI dataset. version:1
arxiv-1511-06645 | DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation | http://arxiv.org/abs/1511.06645 | id:1511.06645 author:Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter Gehler, Bernt Schiele category:cs.CV  published:2015-11-20 summary:This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation: it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form configurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation. Models and code available at http://pose.mpi-inf.mpg.de. version:2
arxiv-1604-07507 | Once for All: a Two-flow Convolutional Neural Network for Visual Tracking | http://arxiv.org/abs/1604.07507 | id:1604.07507 author:Kai Chen, Wenbing Tao category:cs.CV  published:2016-04-26 summary:One of the main challenges of visual object tracking comes from the arbitrary appearance of objects. Most existing algorithms try to resolve this problem as an object-specific task, i.e., the model is trained to regenerate or classify a specific object. As a result, the model need to be initialized and retrained for different objects. In this paper, we propose a more generic approach utilizing a novel two-flow convolutional neural network (named YCNN). The YCNN takes two inputs (one is object image patch, the other is search image patch), then outputs a response map which predicts how likely the object appears in a specific location. Unlike those object-specific approach, the YCNN is trained to measure the similarity between two image patches. Thus it will not be confined to any specific object. Furthermore the network can be end-to-end trained to extract both shallow and deep convolutional features which are dedicated for visual tracking. And once properly trained, the YCNN can be applied to track all kinds of objects without further training and updating. Benefiting from the once-for-all model, our algorithm is able to run at a very high speed of 45 frames-per-second. The experiments on 51 sequences also show that our algorithm achieves an outstanding performance. version:1
arxiv-1604-07499 | Modern Physiognomy: An Investigation on Predicting Personality Traits and Intelligence from the Human Face | http://arxiv.org/abs/1604.07499 | id:1604.07499 author:Rizhen Qin, Wei Gao, Huarong Xu, Zhanyi Hu category:cs.CV  published:2016-04-26 summary:The human behavior of evaluating other individuals with respect to their personality traits and intelligence by evaluating their faces plays a crucial role in human relations. These trait judgments might influence important social outcomes in our lives such as elections and court sentences. Previous studies have reported that human can make valid inferences for at least four personality traits. In addition, some studies have demonstrated that facial trait evaluation can be learned using machine learning methods accurately. In this work, we experimentally explore whether self-reported personality traits and intelligence can be predicted reliably from a facial image. More specifically, the prediction problem is separately cast in two parts: a classification task and a regression task. A facial structural feature is constructed from the relations among facial salient points, and an appearance feature is built by five texture descriptors. In addition, a minutia-based fingerprint feature from a fingerprint image is also explored. The classification results show that the personality traits "Rule-consciousness" and "Vigilance" can be predicted reliably, and that the traits of females can be predicted more accurately than those of male. However, the regression experiments show that it is difficult to predict scores for individual personality traits and intelligence. The residual plots and the correlation results indicate no evident linear correlation between the measured scores and the predicted scores. Both the classification and the regression results reveal that "Rule-consciousness" and "Tension" can be reliably predicted from the facial features, while "Social boldness" gets the worst prediction results. The experiments results show that it is difficult to predict intelligence from either the facial features or the fingerprint feature, a finding that is in agreement with previous studies. version:1
arxiv-1604-07484 | Deep Multi-fidelity Gaussian Processes | http://arxiv.org/abs/1604.07484 | id:1604.07484 author:Maziar Raissi, George Karniadakis category:cs.LG stat.ML  published:2016-04-26 summary:We develop a novel multi-fidelity framework that goes far beyond the classical AR(1) Co-kriging scheme of Kennedy and O'Hagan (2000). Our method can handle general discontinuous cross-correlations among systems with different levels of fidelity. A combination of multi-fidelity Gaussian Processes (AR(1) Co-kriging) and deep neural networks enables us to construct a method that is immune to discontinuities. We demonstrate the effectiveness of the new technology using standard benchmark problems designed to resemble the outputs of complicated high- and low-fidelity codes. version:1
arxiv-1604-07480 | Joint Semantic Segmentation and Depth Estimation with Deep Convolutional Networks | http://arxiv.org/abs/1604.07480 | id:1604.07480 author:Arsalan Mousavian, Hamed Pirsiavash, Jana Kosecka category:cs.CV  published:2016-04-25 summary:Multi-scale deep CNNs have been used successfully for problems mapping each pixel to a label, such as depth estimation and semantic segmentation. It has also been shown that such architectures are reusable and can be used for multiple tasks. These networks are typically trained independently for each task by varying the output layer(s) and training objective. In this work we present a new model for simultaneous depth estimation and semantic segmentation from a single RGB image. Our approach demonstrates the feasibility of training parts of the model for each task and then fine tuning the full, combined model on both tasks simultaneously using a single loss function. Furthermore we couple the deep CNN with fully connected CRF, which captures the contextual relationships and interactions between the semantic and depth cues improving the accuracy of the final results. The proposed model is trained and evaluated on NYUDepth V2 dataset outperforming the state of the art methods on semantic segmentation and achieving comparable results on the task of depth estimation. version:1
arxiv-1604-07468 | Long-Term Identity-Aware Multi-Person Tracking for Surveillance Video Summarization | http://arxiv.org/abs/1604.07468 | id:1604.07468 author:Shoou-I Yu, Yi Yang, Xuanchong Li, Alexander G. Hauptmann category:cs.CV  published:2016-04-25 summary:In multi-person tracking scenarios, gaining access to the identity of each tracked individual is crucial for many applications such as long-term surveillance video analysis. Therefore, we propose a long-term multi-person tracker which utilizes face recognition information to not only enhance tracking performance, but also assign identities to tracked people. As face recognition information is not available in many frames, the proposed tracker utilizes manifold learning techniques to propagate identity information to frames without face recognition information. Our tracker is formulated as a constrained quadratic optimization problem, which is solved with nonnegative matrix optimization techniques. Tracking experiments performed on challenging data sets, including a 116.25 hour complex indoor tracking data set, showed that our method is effective in tracking each individual. We further explored the utility of long-term identity-aware multi-person tracking output by performing video summarization experiments based on our tracking output. Results showed that the computed trajectories were sufficient to generate a reasonable visual diary (i.e. a summary of what a person did) for different people, thus potentially opening the door to summarization of hundreds or even thousands of hours of surveillance video. version:1
arxiv-1311-5179 | Sparse PCA via Covariance Thresholding | http://arxiv.org/abs/1311.5179 | id:1311.5179 author:Yash Deshpande, Andrea Montanari category:math.ST stat.ML stat.TH  published:2013-11-20 summary:In sparse principal component analysis we are given noisy observations of a low-rank matrix of dimension $n\times p$ and seek to reconstruct it under additional sparsity assumptions. In particular, we assume here each of the principal components $\mathbf{v}_1,\dots,\mathbf{v}_r$ has at most $s_0$ non-zero entries. We are particularly interested in the high dimensional regime wherein $p$ is comparable to, or even much larger than $n$. In an influential paper, \cite{johnstone2004sparse} introduced a simple algorithm that estimates the support of the principal vectors $\mathbf{v}_1,\dots,\mathbf{v}_r$ by the largest entries in the diagonal of the empirical covariance. This method can be shown to identify the correct support with high probability if $s_0\le K_1\sqrt{n/\log p}$, and to fail with high probability if $s_0\ge K_2 \sqrt{n/\log p}$ for two constants $0<K_1,K_2<\infty$. Despite a considerable amount of work over the last ten years, no practical algorithm exists with provably better support recovery guarantees. Here we analyze a covariance thresholding algorithm that was recently proposed by \cite{KrauthgamerSPCA}. On the basis of numerical simulations (for the rank-one case), these authors conjectured that covariance thresholding correctly recover the support with high probability for $s_0\le K\sqrt{n}$ (assuming $n$ of the same order as $p$). We prove this conjecture, and in fact establish a more general guarantee including higher-rank as well as $n$ much smaller than $p$. Recent lower bounds \cite{berthet2013computational, ma2015sum} suggest that no polynomial time algorithm can do significantly better. The key technical component of our analysis develops new bounds on the norm of kernel random matrices, in regimes that were not considered before. version:5
arxiv-1604-07464 | Nonparametric Bayesian Negative Binomial Factor Analysis | http://arxiv.org/abs/1604.07464 | id:1604.07464 author:Mingyuan Zhou category:stat.ME stat.ML  published:2016-04-25 summary:A common approach to analyze an attribute-instance count matrix, an element of which represents how many times an attribute appears in an instance, is to factorize it under the Poisson likelihood. We show its limitation in capturing the tendency for an attribute present in an instance to both repeat itself and excite related ones. To address this limitation, we construct negative binomial factor analysis (NBFA) to factorize the matrix under the negative binomial likelihood, and relate it to a Dirichlet-multinomial distribution based mixed-membership model. To support countably infinite factors, we propose the hierarchical gamma-negative binomial process. By exploiting newly proved connections between discrete distributions, we construct two blocked and a collapsed Gibbs sampler that all adaptively truncate their number of factors, and demonstrate that the blocked Gibbs sampler developed under a compound Poisson representation converges fast and has low computational complexity. Example results show that NBFA has a distinct mechanism in adjusting its number of inferred factors according to the instance lengths, and provides clear advantages in parsimonious representation, predictive power, and computational complexity over previously proposed discrete latent variable models, which either completely ignore burstiness, or model only the burstiness of the attributes but not that of the factors. version:1
arxiv-1604-07463 | Dynamic Pricing with Demand Covariates | http://arxiv.org/abs/1604.07463 | id:1604.07463 author:Sheng Qiang, Mohsen Bayati category:stat.ML  published:2016-04-25 summary:We consider a firm that sells products over $T$ periods without knowing the demand function. The firm sequentially sets prices to earn revenue and to learn the underlying demand function simultaneously. A natural heuristic for this problem, commonly used in practice, is greedy iterative least squares (GILS). At each time period, GILS estimates the demand as a linear function of the price by applying least squares to the set of prior prices and realized demands. Then a price that maximizes the revenue, given the estimated demand function, is used for the next time period. The performance is measured by the regret, which is the expected revenue loss from the optimal (oracle) pricing policy when the demand function is known. Recently, den Boer and Zwart (2014) and Keskin and Zeevi (2014) demonstrated that GILS is sub-optimal. They introduced algorithms which integrate forced price dispersion with GILS and achieve asymptotically optimal performance. In this paper, we consider this dynamic pricing problem in a data-rich environment. In particular, we assume that the firm knows the expected demand under a particular price from historical data, and in each period, before setting the price, the firm has access to extra information (demand covariates) which may be predictive of the demand. We prove that in this setting GILS achieves asymptotically optimal regret of order $\log(T)$. We also show the following surprising result: in the original dynamic pricing problem of den Boer and Zwart (2014) and Keskin and Zeevi (2014), inclusion of any set of covariates in GILS as potential demand covariates (even though they could carry no information) would make GILS asymptotically optimal. We validate our results via extensive numerical simulations on synthetic and real data sets. version:1
arxiv-1604-07457 | Modeling the Contribution of Central Versus Peripheral Vision in Scene, Object, and Face Recognition | http://arxiv.org/abs/1604.07457 | id:1604.07457 author:Panqu Wang, Garrison Cottrell category:q-bio.NC cs.CV  published:2016-04-25 summary:It is commonly believed that the central visual field is important for recognizing objects and faces, and the peripheral region is useful for scene recognition. However, the relative importance of central versus peripheral information for object, scene, and face recognition is unclear. In a behavioral study, Larson and Loschky (2009) investigated this question by measuring the scene recognition accuracy as a function of visual angle, and demonstrated that peripheral vision was indeed more useful in recognizing scenes than central vision. In this work, we modeled and replicated the result of Larson and Loschky (2009), using deep convolutional neural networks. Having fit the data for scenes, we used the model to predict future data for large-scale scene recognition as well as for objects and faces. Our results suggest that the relative order of importance of using central visual field information is face recognition>object recognition>scene recognition, and vice-versa for peripheral information. version:1
arxiv-1604-07451 | Learning Local Dependence In Ordered Data | http://arxiv.org/abs/1604.07451 | id:1604.07451 author:Guo Yu, Jacob Bien category:math.ST stat.CO stat.ME stat.ML stat.TH  published:2016-04-25 summary:In many applications, data comes with a natural ordering. This ordering can often induce local dependence among nearby variables. However, in complex data, the width of this dependence may vary, making simple assumptions such as a constant neighborhood size unrealistic. We propose a framework for learning this local dependence based on estimating the inverse of the Cholesky factor of the covariance matrix. Penalized maximum likelihood estimation of this matrix yields a simple regression interpretation for local dependence in which variables are predicted by their neighbors. Our proposed method involves solving a convex, penalized Gaussian likelihood problem with a hierarchical group lasso penalty. The problem decomposes into independent subproblems which can be solved efficiently in parallel using first-order methods. Our method yields a sparse, symmetric, positive definite estimator of the precision matrix, encoding a Gaussian graphical model. We derive theoretical results not found in existing methods attaining this structure. In particular, our conditions for signed support recovery and estimation consistency rates in multiple norms are as mild as those in a regression problem. Empirical results show our method performing favorably compared to existing methods. We apply our method to genomic data to flexibly model linkage disequilibrium. version:1
arxiv-1511-03719 | Universum Prescription: Regularization using Unlabeled Data | http://arxiv.org/abs/1511.03719 | id:1511.03719 author:Xiang Zhang, Yann LeCun category:cs.LG  published:2015-11-11 summary:This paper shows that simply prescribing "none of the above" labels to unlabeled data has a beneficial regularization effect to supervised learning. We call it universum prescription by the fact that the prescribed labels cannot be one of the supervised labels. In spite of its simplicity, universum prescription obtained competitive results in training deep convolutional networks for CIFAR-10, CIFAR-100, STL-10 and ImageNet datasets. A qualitative justification of these approaches using Rademacher complexity is presented. The effect of a regularization parameter -- probability of sampling from unlabeled data -- is also studied empirically. version:6
arxiv-1512-07729 | G-CNN: an Iterative Grid Based Object Detector | http://arxiv.org/abs/1512.07729 | id:1512.07729 author:Mahyar Najibi, Mohammad Rastegari, Larry S. Davis category:cs.CV  published:2015-12-24 summary:We introduce G-CNN, an object detection technique based on CNNs which works without proposal algorithms. G-CNN starts with a multi-scale grid of fixed bounding boxes. We train a regressor to move and scale elements of the grid towards objects iteratively. G-CNN models the problem of object detection as finding a path from a fixed grid to boxes tightly surrounding the objects. G-CNN with around 180 boxes in a multi-scale grid performs comparably to Fast R-CNN which uses around 2K bounding boxes generated with a proposal technique. This strategy makes detection faster by removing the object proposal stage as well as reducing the number of boxes to be processed. version:2
arxiv-1604-07429 | Balancing Appearance and Context in Sketch Interpretation | http://arxiv.org/abs/1604.07429 | id:1604.07429 author:Yale Song, Randall Davis, Kaichen Ma, Dana L. Penny category:cs.AI cs.CV  published:2016-04-25 summary:We describe a sketch interpretation system that detects and classifies clock numerals created by subjects taking the Clock Drawing Test, a clinical tool widely used to screen for cognitive impairments (e.g., dementia). We describe how it balances appearance and context, and document its performance on some 2,000 drawings (about 24K clock numerals) produced by a wide spectrum of patients. We calibrate the utility of different forms of context, describing experiments with Conditional Random Fields trained and tested using a variety of features. We identify context that contributes to interpreting otherwise ambiguous or incomprehensible strokes. We describe ST-slices, a novel representation that enables "unpeeling" the layers of ink that result when people overwrite, which often produces ink impossible to analyze if only the final drawing is examined. We characterize when ST-slices work, calibrate their impact on performance, and consider their breadth of applicability. version:1
arxiv-1604-07407 | Conversational Markers of Constructive Discussions | http://arxiv.org/abs/1604.07407 | id:1604.07407 author:Vlad Niculae, Cristian Danescu-Niculescu-Mizil category:cs.CL cs.AI cs.SI physics.soc-ph stat.ML  published:2016-04-25 summary:Group discussions are essential for organizing every aspect of modern life, from faculty meetings to senate debates, from grant review panels to papal conclaves. While costly in terms of time and organization effort, group discussions are commonly seen as a way of reaching better decisions compared to solutions that do not require coordination between the individuals (e.g. voting)---through discussion, the sum becomes greater than the parts. However, this assumption is not irrefutable: anecdotal evidence of wasteful discussions abounds, and in our own experiments we find that over 30% of discussions are unproductive. We propose a framework for analyzing conversational dynamics in order to determine whether a given task-oriented discussion is worth having or not. We exploit conversational patterns reflecting the flow of ideas and the balance between the participants, as well as their linguistic choices. We apply this framework to conversations naturally occurring in an online collaborative world exploration game developed and deployed to support this research. Using this setting, we show that linguistic cues and conversational patterns extracted from the first 20 seconds of a team discussion are predictive of whether it will be a wasteful or a productive one. version:1
arxiv-1604-07379 | Context Encoders: Feature Learning by Inpainting | http://arxiv.org/abs/1604.07379 | id:1604.07379 author:Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros category:cs.CV cs.AI cs.GR cs.LG  published:2016-04-25 summary:We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods. version:1
arxiv-1502-07770 | Total variation on a tree | http://arxiv.org/abs/1502.07770 | id:1502.07770 author:Vladimir Kolmogorov, Thomas Pock, Michal Rolinek category:cs.CV  published:2015-02-26 summary:We consider the problem of minimizing the continuous valued total variation subject to different unary terms on trees and propose fast direct algorithms based on dynamic programming to solve these problems. We treat both the convex and the non-convex case and derive worst case complexities that are equal or better than existing methods. We show applications to total variation based 2D image processing and computer vision problems based on a Lagrangian decomposition approach. The resulting algorithms are very efficient, offer a high degree of parallelism and come along with memory requirements which are only in the order of the number of image pixels. version:3
arxiv-1604-07370 | Parsing Argumentation Structures in Persuasive Essays | http://arxiv.org/abs/1604.07370 | id:1604.07370 author:Christian Stab, Iryna Gurevych category:cs.CL  published:2016-04-25 summary:In this article, we present the first end-to-end approach for parsing argumentation structures in persuasive essays. We model the argumentation structure as a tree including several types of argument components connected with argumentative support and attack relations. We consider the identification of argumentation structures in several consecutive steps. First, we segment a persuasive essay in order to identify relevant argument components. Second, we jointly model the classification of argument components and the identification of argumentative relations using Integer Linear Programming. Third, we recognize the stance of each argument component in order to discriminate between argumentative support and attack relations. By evaluating the joint model using two corpora, we show that our approach not only considerably improves the identification of argument component types and argumentative relations but also significantly outperforms a challenging heuristic baseline. In addition, we introduce a novel corpus including 402 persuasive essays annotated with argumentation structures and show that our new annotation guideline successfully guides annotators to substantial agreement. version:1
arxiv-1604-07360 | Attributes for Improved Attributes: A Multi-Task Network for Attribute Classification | http://arxiv.org/abs/1604.07360 | id:1604.07360 author:Emily M. Hand, Rama Chellappa category:cs.CV  published:2016-04-25 summary:Attributes, or semantic features, have gained popularity in the past few years in domains ranging from activity recognition in video to face verification. Improving the accuracy of attribute classifiers is an important first step in any application which uses these attributes. In most works to date, attributes have been considered to be independent. However, we know this not to be the case. Many attributes are very strongly related, such as heavy makeup and wearing lipstick. We propose to take advantage of attribute relationships in three ways: by using a multi-task deep convolutional neural network (MCNN) sharing the lowest layers amongst all attributes, sharing the higher layers for related attributes, and by building an auxiliary network on top of the MCNN which utilizes the scores from all attributes to improve the final classification of each attribute. We demonstrate the effectiveness of our method by producing results on two challenging publicly available datasets. version:1
arxiv-1604-07356 | Fast nonlinear embeddings via structured matrices | http://arxiv.org/abs/1604.07356 | id:1604.07356 author:Krzysztof Choromanski, Francois Fagan category:stat.ML cs.LG G.3  published:2016-04-25 summary:We present a new paradigm for speeding up randomized computations of several frequently used functions in machine learning. In particular, our paradigm can be applied for improving computations of kernels based on random embeddings. Above that, the presented framework covers multivariate randomized functions. As a byproduct, we propose an algorithmic approach that also leads to a significant reduction of space complexity. Our method is based on careful recycling of Gaussian vectors into structured matrices that share properties of fully random matrices. The quality of the proposed structured approach follows from combinatorial properties of the graphs encoding correlations between rows of these structured matrices. Our framework covers as special cases already known structured approaches such as the Fast Johnson-Lindenstrauss Transform, but is much more general since it can be applied also to highly nonlinear embeddings. We provide strong concentration results showing the quality of the presented paradigm. version:1
arxiv-1506-07405 | Global Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation | http://arxiv.org/abs/1506.07405 | id:1506.07405 author:Dejiao Zhang, Laura Balzano category:cs.NA math.NA stat.ML 90C52  65Y20 G.1.6; F.2.1  published:2015-06-24 summary:It has been observed in a variety of contexts that gradient descent methods have great success in solving low-rank matrix factorization problems, despite the relevant problem formulation being non-convex. We tackle a particular instance of this scenario, where we seek the $d$-dimensional subspace spanned by a streaming data matrix. We apply the natural first order incremental gradient descent method, constraining the gradient method to the Grassmannian. In this paper, we propose an adaptive step size scheme that is greedy for the noiseless case, that maximizes the improvement of our metric of convergence at each data index $t$, and yields an expected improvement for the noisy case. We show that, with noise-free data, this method converges from any random initialization to the global minimum of the problem. For noisy data, we provide the expected convergence rate of the proposed algorithm per iteration. version:2
arxiv-1604-07335 | Scalable Gaussian Processes for Supervised Hashing | http://arxiv.org/abs/1604.07335 | id:1604.07335 author:Bahadir Ozdemir, Larry S. Davis category:cs.CV  published:2016-04-25 summary:We propose a flexible procedure for large-scale image search by hash functions with kernels. Our method treats binary codes and pairwise semantic similarity as latent and observed variables, respectively, in a probabilistic model based on Gaussian processes for binary classification. We present an efficient inference algorithm with the sparse pseudo-input Gaussian process (SPGP) model and parallelization. Experiments on three large-scale image dataset demonstrate the effectiveness of the proposed hashing method, Gaussian Process Hashing (GPH), for short binary codes and the datasets without predefined classes in comparison to the state-of-the-art supervised hashing methods. version:1
arxiv-1604-01444 | A Convolutional Neural Network Neutrino Event Classifier | http://arxiv.org/abs/1604.01444 | id:1604.01444 author:A. Aurisano, A. Radovic, D. Rocco, A. Himmel, M. D. Messier, E. Niner, G. Pawloski, F. Psihas, A. Sousa, P. Vahle category:hep-ex cs.CV  published:2016-04-05 summary:Convolutional neural networks (CNNs) have been widely applied in the computer vision community to solve complex problems in image recognition and analysis. We describe an application of the CNN technology to the problem of identifying particle interactions in sampling calorimeters used commonly in high energy physics and high energy neutrino physics in particular. Following a discussion of the core concepts of CNNs and recent innovations in CNN architectures related to the field of deep learning, we outline a specific application to the NOvA neutrino detector. This algorithm, CVN (Convolutional Visual Network) identifies neutrino interactions based on their topology without the need for detailed reconstruction and outperforms algorithms currently in use by the NOvA collaboration. version:2
arxiv-1604-07319 | Semi-supervised Dictionary Learning Based on Hilbert-Schmidt Independence Criterion | http://arxiv.org/abs/1604.07319 | id:1604.07319 author:Mehrdad J. Gangeh, Safaa M. A. Bedawi, Ali Ghodsi, Fakhri Karray category:cs.CV  published:2016-04-25 summary:In this paper, a novel semi-supervised dictionary learning and sparse representation (SS-DLSR) is proposed. The proposed method benefits from the supervisory information by learning the dictionary in a space where the dependency between the data and class labels is maximized. This maximization is performed using Hilbert-Schmidt independence criterion (HSIC). On the other hand, the global distribution of the underlying manifolds were learned from the unlabeled data by minimizing the distances between the unlabeled data and the corresponding nearest labeled data in the space of the dictionary learned. The proposed SS-DLSR algorithm has closed-form solutions for both the dictionary and sparse coefficients, and therefore does not have to learn the two iteratively and alternately as is common in the literature of the DLSR. This makes the solution for the proposed algorithm very fast. The experiments confirm the improvement in classification performance on benchmark datasets by including the information from both labeled and unlabeled data, particularly when there are many unlabeled data. version:1
arxiv-1604-07316 | End to End Learning for Self-Driving Cars | http://arxiv.org/abs/1604.07316 | id:1604.07316 author:Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, Karol Zieba category:cs.CV cs.LG cs.NE  published:2016-04-25 summary:We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS). version:1
arxiv-1604-07279 | Actionness Estimation Using Hybrid Fully Convolutional Networks | http://arxiv.org/abs/1604.07279 | id:1604.07279 author:Limin Wang, Yu Qiao, Xiaoou Tang, Luc Van Gool category:cs.CV  published:2016-04-25 summary:Actionness was introduced to quantify the likelihood of containing a generic action instance at a specific location. Accurate and efficient estimation of actionness is important in video analysis and may benefit other relevant tasks such as action recognition and action detection. This paper presents a new deep architecture for actionness estimation, called hybrid fully convolutional network (H-FCN), which is composed of appearance FCN (A-FCN) and motion FCN (M-FCN). These two FCNs leverage the strong capacity of deep models to estimate actionness maps from the perspectives of static appearance and dynamic motion, respectively. In addition, the fully convolutional nature of H-FCN allows it to efficiently process videos with arbitrary sizes. Experiments are conducted on the challenging datasets of Stanford40, UCF Sports, and JHMDB to verify the effectiveness of H-FCN on actionness estimation, which demonstrate that our method achieves superior performance to previous ones. Moreover, we apply the estimated actionness maps on action proposal generation and action detection. Our actionness maps advance the current state-of-the-art performance of these tasks substantially. version:1
arxiv-1604-07269 | CMA-ES for Hyperparameter Optimization of Deep Neural Networks | http://arxiv.org/abs/1604.07269 | id:1604.07269 author:Ilya Loshchilov, Frank Hutter category:cs.NE cs.LG  published:2016-04-25 summary:Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. As an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy example comparing CMA-ES and state-of-the-art Bayesian optimization algorithms for tuning the hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. version:1
arxiv-1511-06343 | Online Batch Selection for Faster Training of Neural Networks | http://arxiv.org/abs/1511.06343 | id:1511.06343 author:Ilya Loshchilov, Frank Hutter category:cs.LG cs.NE math.OC  published:2015-11-19 summary:Deep neural networks are commonly trained using stochastic non-convex optimization procedures, which are driven by gradient information estimated on fractions (batches) of the dataset. While it is commonly accepted that batch size is an important parameter for offline tuning, the benefits of online selection of batches remain poorly understood. We investigate online batch selection strategies for two state-of-the-art methods of stochastic gradient-based optimization, AdaDelta and Adam. As the loss function to be minimized for the whole dataset is an aggregation of loss functions of individual datapoints, intuitively, datapoints with the greatest loss should be considered (selected in a batch) more frequently. However, the limitations of this intuition and the proper control of the selection pressure over time are open questions. We propose a simple strategy where all datapoints are ranked w.r.t. their latest known loss value and the probability to be selected decays exponentially as a function of rank. Our experimental results on the MNIST dataset suggest that selecting batches speeds up both AdaDelta and Adam by a factor of about 5. version:4
arxiv-1604-07255 | A Deep Hierarchical Approach to Lifelong Learning in Minecraft | http://arxiv.org/abs/1604.07255 | id:1604.07255 author:Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J. Mankowitz, Shie Mannor category:cs.AI cs.LG  published:2016-04-25 summary:The ability to reuse or transfer knowledge from one task to another in lifelong learning problems, such as Minecraft, is one of the major challenges faced in AI. Reusing knowledge across tasks is crucial to solving tasks efficiently with lower sample complexity. We provide a Reinforcement Learning agent with the ability to transfer knowledge by learning reusable skills, a type of temporally extended action (also know as Options (Sutton et. al. 1999)). The agent learns reusable skills using Deep Q Networks (Mnih et. al. 2015) to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks (DSNs), are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture. The H-DRLN is a hierarchical version of Deep QNetworks and learns to efficiently solve tasks by reusing knowledge from previously learned DSNs. The H-DRLN exhibits superior performance and lower learning sample complexity (by taking advantage of temporal extension) compared to the regular Deep Q Network (Mnih et. al. 2015) in subdomains of Minecraft. We also show the potential to transfer knowledge between related Minecraft tasks without any additional learning. version:1
arxiv-1604-07243 | Expectation Maximization for Sum-Product Networks as Exponential Family Mixture Models | http://arxiv.org/abs/1604.07243 | id:1604.07243 author:Mattia Desana, Christoph Schnörr category:cs.LG  published:2016-04-25 summary:Sum-Product Networks (SPNs) are a recent class of probabilistic models which encode very large mixtures compactly by exploiting efficient reuse of computation in inference. Crucially, in SPNs the cost of inference scales linearly with the number of edges $E$ but the encoded mixture size $C$ can be exponentially larger than $E$. In this paper we obtain an efficient ($O(E)$) implementation of Expectation Maximization (EM) for SPNs which is the first to include EM updates both on mixture coefficients (corresponding to SPN weights) and mixture components (corresponding to SPN leaves). In particular, the update on mixture components translates to a weighted maximum likelihood problem on leaf distributions, and can be solved exactly when leaves are in the exponential family. This opens new application areas for SPNs, such as learning large mixtures of tree graphical models. We validate the algorithm on a synthetic but non trivial "soft-parity" distribution with $2^{n}$ modes encoded by a SPN with only $O(n)$ edges. version:1
arxiv-1507-01978 | Learning vector autoregressive models with focalised Granger-causality graphs | http://arxiv.org/abs/1507.01978 | id:1507.01978 author:Magda Gregorova, Alexandros Kalousis, Stéphane Marchand-Maillet category:cs.LG stat.ML  published:2015-07-07 summary:We consider the problem of learning models for forecasting multiple time-series systems together with discovering the leading indicators that serve as good predictors for the system. We model the systems by linear vector autoregressive models (VAR) and link the discovery of leading indicators to inferring sparse graphs of Granger-causality. We propose new problem formulations and develop two new methods to learn such models, gradually increasing the complexity of assumptions and approaches. While the first method assumes common structures across the whole system, our second method uncovers model clusters based on the Granger-causality and leading indicators together with learning the model parameters. We study the performance of our methods on a comprehensive set of experiments and confirm their efficacy and their advantages over state-of-the-art sparse VAR and graphical Granger learning methods. version:2
arxiv-1604-07236 | Towards Real-Time, Country-Level Location Classification of Worldwide Tweets | http://arxiv.org/abs/1604.07236 | id:1604.07236 author:Arkaitz Zubiaga, Alex Voss, Rob Procter, Maria Liakata, Bo Wang, Adam Tsakalidis category:cs.IR cs.CL cs.SI  published:2016-04-25 summary:With the increase of interest in using social media as a source for research, many have tackled the task of automatically geolocating tweets, motivated by the lack of explicit location information in the majority of tweets. While others have focused on state- or city-level classification of tweets restricted to a specific country, here we undertake the task in a broader context by classifying global tweets at the country level, so far unexplored in a real-time scenario. We analyse the extent to which a tweet's country of origin can be determined by making use of eight tweet-inherent features for classification using Support Vector Machines. Furthermore, we use two datasets, collected a year apart from each other, to analyse the extent to which a model trained from historical tweets can still be leveraged for classification of new tweets. With classification experiments on all 217 countries in our datasets, as well as on the top 25 countries, we offer some insights into the best use of tweet-inherent features for an accurate country-level classification of tweets. Among the features inherent in a tweet, we observe that the validity of historical tweet content fades over time, and other metadata associated with the tweet, such as the language of the tweet, the name of the user, or the time zone in which the user is located, lead to more accurate classification. While no feature set is optimal for all countries, and each country needs to be treated differently, we show that remarkably high performance values above 0.9 in terms of F1 score can be achieved for countries with unique characteristics such as those having a language that is not spoken in many other countries or a unique time zone. However, the difficulty of achieving an accurate classification increases for countries with multiple commonalities, especially for English and Spanish speaking countries. version:1
arxiv-1507-02216 | Robust Sparse Blind Source Separation | http://arxiv.org/abs/1507.02216 | id:1507.02216 author:Cecile Chenot, Jerome Bobin, Jeremy Rapin category:stat.AP cs.LG stat.ML  published:2015-07-08 summary:Blind Source Separation is a widely used technique to analyze multichannel data. In many real-world applications, its results can be significantly hampered by the presence of unknown outliers. In this paper, a novel algorithm coined rGMCA (robust Generalized Morphological Component Analysis) is introduced to retrieve sparse sources in the presence of outliers. It explicitly estimates the sources, the mixing matrix, and the outliers. It also takes advantage of the estimation of the outliers to further implement a weighting scheme, which provides a highly robust separation procedure. Numerical experiments demonstrate the efficiency of rGMCA to estimate the mixing matrix in comparison with standard BSS techniques. version:2
arxiv-1506-00122 | IDSA: Intelligent Distributed Sensor Activation Algorithm For Target Tracking With Wireless Sensor Network | http://arxiv.org/abs/1506.00122 | id:1506.00122 author:Mohammad Sabokrou, Mahmood Fathy, Mojtaba Hoseini category:cs.NI cs.NE  published:2015-05-30 summary:One important application of the Wireless Sensor Network(WSN) is target tracking, the aim of this application is converging to an event or object in an area. In this paper, we propose an energy-efficient distributed sensor activation protocol based on predicted location technique, called Intelligent Distributed Sensor Activation Algorithm (IDSA). The proposed algorithm predicts the location of target in the next time interval, by analyzing current location and movement history of the target, this prediction is done by computational intelligence. The fewest essential number of sensor nodes within the predicted location will be activated to cover the target. The results show that the proposed method outperforms the existing methods such as Na\"ive and DSA in terms of energy consumption and the number of nodes that was involved in tracking the target. version:3
arxiv-1604-07211 | Towards Reduced Reference Parametric Models for Estimating Audiovisual Quality in Multimedia Services | http://arxiv.org/abs/1604.07211 | id:1604.07211 author:Edip Demirbilek, Jean-Charles Grégoire category:cs.MM cs.LG  published:2016-04-25 summary:We have developed reduced reference parametric models for estimating perceived quality in audiovisual multimedia services. We have created 144 unique configurations for audiovisual content including various application and network parameters such as bitrates and distortions in terms of bandwidth, packet loss rate and jitter. To generate the data needed for model training and validation we have tasked 24 subjects, in a controlled environment, to rate the overall audiovisual quality on the absolute category rating (ACR) 5-level quality scale. We have developed models using Random Forest and Neural Network based machine learning methods in order to estimate Mean Opinion Scores (MOS) values. We have used information retrieved from the packet headers and side information provided as network parameters for model training. Random Forest based models have performed better in terms of Root Mean Square Error (RMSE) and Pearson correlation coefficient. The side information proved to be very effective in developing the model. We have found that, while the model performance might be improved by replacing the side information with more accurate bit stream level measurements, they are performing well in estimating perceived quality in audiovisual multimedia services. version:1
arxiv-1604-07209 | Unbiased Comparative Evaluation of Ranking Functions | http://arxiv.org/abs/1604.07209 | id:1604.07209 author:Tobias Schnabel, Adith Swaminathan, Peter Frazier, Thorsten Joachims category:cs.IR cs.LG  published:2016-04-25 summary:Eliciting relevance judgments for ranking evaluation is labor-intensive and costly, motivating careful selection of which documents to judge. Unlike traditional approaches that make this selection deterministically, probabilistic sampling has shown intriguing promise since it enables the design of estimators that are provably unbiased even when reusing data with missing judgments. In this paper, we first unify and extend these sampling approaches by viewing the evaluation problem as a Monte Carlo estimation task that applies to a large number of common IR metrics. Drawing on the theoretical clarity that this view offers, we tackle three practical evaluation scenarios: comparing two systems, comparing $k$ systems against a baseline, and ranking $k$ systems. For each scenario, we derive an estimator and a variance-optimizing sampling distribution while retaining the strengths of sampling-based evaluation, including unbiasedness, reusability despite missing data, and ease of use in practice. In addition to the theoretical contribution, we empirically evaluate our methods against previously used sampling heuristics and find that they generally cut the number of required relevance judgments at least in half. version:1
arxiv-1604-07180 | Observing and Recommending from a Social Web with Biases | http://arxiv.org/abs/1604.07180 | id:1604.07180 author:Steffen Staab, Sophie Stalla-Bourdillon, Laura Carmichael category:cs.DB cs.LG K.5.0; H.2.8  published:2016-04-25 summary:The research question this report addresses is: how, and to what extent, those directly involved with the design, development and employment of a specific black box algorithm can be certain that it is not unlawfully discriminating (directly and/or indirectly) against particular persons with protected characteristics (e.g. gender, race and ethnicity)? version:1
arxiv-1604-07178 | Weighted Spectral Cluster Ensemble | http://arxiv.org/abs/1604.07178 | id:1604.07178 author:Muhammad Yousefnezhad, Daoqiang Zhang category:cs.LG cs.AI stat.ML  published:2016-04-25 summary:Clustering explores meaningful patterns in the non-labeled data sets. Cluster Ensemble Selection (CES) is a new approach, which can combine individual clustering results for increasing the performance of the final results. Although CES can achieve better final results in comparison with individual clustering algorithms and cluster ensemble methods, its performance can be dramatically affected by its consensus diversity metric and thresholding procedure. There are two problems in CES: 1) most of the diversity metrics is based on heuristic Shannon's entropy and 2) estimating threshold values are really hard in practice. The main goal of this paper is proposing a robust approach for solving the above mentioned problems. Accordingly, this paper develops a novel framework for clustering problems, which is called Weighted Spectral Cluster Ensemble (WSCE), by exploiting some concepts from community detection arena and graph based clustering. Under this framework, a new version of spectral clustering, which is called Two Kernels Spectral Clustering, is used for generating graphs based individual clustering results. Further, by using modularity, which is a famous metric in the community detection, on the transformed graph representation of individual clustering results, our approach provides an effective diversity estimation for individual clustering results. Moreover, this paper introduces a new approach for combining the evaluated individual clustering results without the procedure of thresholding. Experimental study on varied data sets demonstrates that the prosed approach achieves superior performance to state-of-the-art methods. version:1
arxiv-1604-07176 | Protein Secondary Structure Prediction Using Cascaded Convolutional and Recurrent Neural Networks | http://arxiv.org/abs/1604.07176 | id:1604.07176 author:Zhen Li, Yizhou Yu category:q-bio.BM cs.AI cs.LG cs.NE q-bio.QM  published:2016-04-25 summary:Protein secondary structure prediction is an important problem in bioinformatics. Inspired by the recent successes of deep neural networks, in this paper, we propose an end-to-end deep network that predicts protein secondary structures from integrated local and global contextual features. Our deep architecture leverages convolutional neural networks with different kernel sizes to extract multiscale local contextual features. In addition, considering long-range dependencies existing in amino acid sequences, we set up a bidirectional neural network consisting of gated recurrent unit to capture global contextual features. Furthermore, multi-task learning is utilized to predict secondary structure labels and amino-acid solvent accessibility simultaneously. Our proposed deep network demonstrates its effectiveness by achieving state-of-the-art performance, i.e., 69.7% Q8 accuracy on the public benchmark CB513, 76.9% Q8 accuracy on CASP10 and 73.1% Q8 accuracy on CASP11. Our model and results are publicly available. version:1
arxiv-1604-05000 | RGB-D Scene Labeling with Long Short-Term Memorized Fusion Model | http://arxiv.org/abs/1604.05000 | id:1604.05000 author:Zhen Li, Yukang Gan, Xiaodan Liang, Yizhou Yu, Hui Cheng, Liang Lin category:cs.CV cs.NE  published:2016-04-18 summary:Semantic labeling of RGB-D scenes is crucial to many intelligent applications including perceptual robotics. It generates pixelwise and fine-grained label maps from simultaneously sensed photometric (RGB) and depth channels. This paper addresses this problem by i) developing a novel Long Short-Term Memorized Fusion (LSTM-F) Model that captures and fuses contextual information from multiple channels of photometric and depth data, and ii) incorporating this model into deep convolutional neural networks (CNNs) for end-to-end training. Specifically, global contexts in photometric and depth channels are, respectively, captured by stacking several convolutional layers and a long short-term memory layer; the memory layer encodes both short-range and long-range spatial dependencies in an image along the vertical direction. Another long short-term memorized fusion layer is set up to integrate the contexts along the vertical direction from different channels, and perform bi-directional propagation of the fused vertical contexts along the horizontal direction to obtain true 2D global contexts. At last, the fused contextual representation is concatenated with the convolutional features extracted from the photometric channels in order to improve the accuracy of fine-scale semantic labeling. Our proposed model has set a new state of the art, i.e., 48.1% average class accuracy over 37 categories 11.8% improvement), on the large-scale SUNRGBD dataset.1 version:2
arxiv-1507-07505 | Real-time 2D/3D Registration via CNN Regression | http://arxiv.org/abs/1507.07505 | id:1507.07505 author:Shun Miao, Z. Jane Wang, Rui Liao category:cs.CV  published:2015-07-27 summary:In this paper, we present a Convolutional Neural Network (CNN) regression approach for real-time 2-D/3-D registration. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the Digitally Reconstructed Radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. The CNN regressors are trained for local zones and applied in a hierarchical manner to break down the complex regression task into simpler sub-tasks that can be learned separately. Our experiment results demonstrate the advantage of the proposed method in computational efficiency with negligible degradation of registration accuracy compared to intensity-based methods. version:3
arxiv-1604-07143 | Neural Random Forests | http://arxiv.org/abs/1604.07143 | id:1604.07143 author:Gérard Biau, Erwan Scornet, Johannes Welbl category:stat.ML cs.LG  published:2016-04-25 summary:Given an ensemble of randomized regression trees, it is possible to restructure them as a collection of multilayered neural networks with particular connection weights. Following this principle, we reformulate the random forest method of Breiman (2001) into a neural network setting, and in turn propose two new hybrid procedures that we call neural random forests. Both predictors exploit prior knowledge of regression trees for their architecture, have less parameters to tune than standard networks, and less restrictions on the geometry of the decision boundaries. Consistency results are proved, and substantial numerical evidence is provided on both synthetic and real data sets to assess the excellent performance of our methods in a large variety of prediction problems. version:1
arxiv-1604-07108 | Modeling the Evolution of Gene-Culture Divergence | http://arxiv.org/abs/1604.07108 | id:1604.07108 author:Chris Marriott, Jobran Chebib category:cs.NE cs.MA q-bio.PE  published:2016-04-25 summary:We present a model for evolving agents using both genetic and cultural inheritance mechanisms. Within each agent our model maintains two distinct information stores we call the genome and the memome. Processes of adaptation are modeled as evolutionary processes at each level of adaptation (phylogenetic, ontogenetic, sociogenetic). We review relevant competing models and we show how our model improves on previous attempts to model genetic and cultural evolutionary processes. In particular we argue our model can achieve divergent gene-culture co-evolution. version:1
arxiv-1506-02565 | Learning to Select Pre-Trained Deep Representations with Bayesian Evidence Framework | http://arxiv.org/abs/1506.02565 | id:1506.02565 author:Yong-Deok Kim, Taewoong Jang, Bohyung Han, Seungjin Choi category:cs.CV cs.LG stat.ML  published:2015-06-08 summary:We propose a Bayesian evidence framework to facilitate transfer learning from pre-trained deep convolutional neural networks (CNNs). Our framework is formulated on top of a least squares SVM (LS-SVM) classifier, which is simple and fast in both training and testing, and achieves competitive performance in practice. The regularization parameters in LS-SVM is estimated automatically without grid search and cross-validation by maximizing evidence, which is a useful measure to select the best performing CNN out of multiple candidates for transfer learning; the evidence is optimized efficiently by employing Aitken's delta-squared process, which accelerates convergence of fixed point update. The proposed Bayesian evidence framework also provides a good solution to identify the best ensemble of heterogeneous CNNs through a greedy algorithm. Our Bayesian evidence framework for transfer learning is tested on 12 visual recognition datasets and illustrates the state-of-the-art performance consistently in terms of prediction accuracy and modeling efficiency. version:4
arxiv-1604-06518 | Approximation Vector Machines for Large-scale Online Learning | http://arxiv.org/abs/1604.06518 | id:1604.06518 author:Trung Le, Tu Dinh Nguyen, Vu Nguyen, Dinh Phung category:cs.LG  published:2016-04-22 summary:One of the most challenging problems in kernel online learning is to bound the model size and to promote the model sparsity. Sparse models not only improve computation and memory usage, but also enhance the generalization capacity, a principle that concurs with the law of parsimony. However, inappropriate sparsity modeling may also significantly degrade the performance. In this paper, we propose Approximation Vector Machine (AVM), a model that can simultaneously encourage the sparsity and safeguard its risk in compromising the performance. When an incoming instance arrives, we approximate this instance by one of its neighbors whose distance to it is less than a predefined threshold. Our key intuition is that since the newly seen instance is expressed by its nearby neighbor the optimal performance can be analytically formulated and maintained. We develop theoretical foundations to support this intuition and further establish an analysis to characterize the gap between the approximation and optimal solutions. This gap crucially depends on the frequency of approximation and the predefined threshold. We perform the convergence analysis for a wide spectrum of loss functions including Hinge, smooth Hinge, and Logistic for classification task, and $l_1$, $l_2$, and $\epsilon$-insensitive for regression task. We conducted extensive experiments for classification task in batch and online modes, and regression task in online mode over several benchmark datasets. The results show that our proposed AVM achieved a comparable predictive performance with current state-of-the-art methods while simultaneously achieving significant computational speed-up due to the ability of the proposed AVM in maintaining the model size. version:2
arxiv-1604-07102 | Makeup like a superstar: Deep Localized Makeup Transfer Network | http://arxiv.org/abs/1604.07102 | id:1604.07102 author:Si Liu, Xinyu Ou, Ruihe Qian, Wei Wang, Xiaochun Cao category:cs.CV cs.AI  published:2016-04-25 summary:In this paper, we propose a novel Deep Localized Makeup Transfer Network to automatically recommend the most suitable makeup for a female and synthesis the makeup on her face. Given a before-makeup face, her most suitable makeup is determined automatically. Then, both the beforemakeup and the reference faces are fed into the proposed Deep Transfer Network to generate the after-makeup face. Our end-to-end makeup transfer network have several nice properties including: (1) with complete functions: including foundation, lip gloss, and eye shadow transfer; (2) cosmetic specific: different cosmetics are transferred in different manners; (3) localized: different cosmetics are applied on different facial regions; (4) producing naturally looking results without obvious artifacts; (5) controllable makeup lightness: various results from light makeup to heavy makeup can be generated. Qualitative and quantitative experiments show that our network performs much better than the methods of [Guo and Sim, 2009] and two variants of NerualStyle [Gatys et al., 2015a]. version:1
arxiv-1604-07101 | Double Thompson Sampling for Dueling Bandits | http://arxiv.org/abs/1604.07101 | id:1604.07101 author:Huasen Wu, Xin Liu, R. Srikant category:cs.LG stat.ML  published:2016-04-25 summary:In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for dueling bandit problems. As indicated by its name, D-TS selects both the first and the second candidates according to Thompson Sampling. Specifically, D-TS maintains a posterior distribution for the preference matrix, and chooses the pair of arms for comparison by sampling twice from the posterior distribution. This simple algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as its special case. For general Copeland dueling bandits, we show that D-TS achieves $O(K^2 \log T)$ regret. For Condorcet dueling bandits, we further simplify the D-TS algorithm and show that the simplified D-TS algorithm achieves $O(K \log T + K^2 \log \log T)$ regret. Simulation results based on both synthetic and real-world data demonstrate the efficiency of the proposed D-TS algorithm. version:1
arxiv-1602-04259 | A Minimalistic Approach to Sum-Product Network Learning for Real Applications | http://arxiv.org/abs/1602.04259 | id:1602.04259 author:Viktoriya Krakovna, Moshe Looks category:cs.AI cs.LG stat.ML  published:2016-02-12 summary:Sum-Product Networks (SPNs) are a class of expressive yet tractable hierarchical graphical models. LearnSPN is a structure learning algorithm for SPNs that uses hierarchical co-clustering to simultaneously identifying similar entities and similar features. The original LearnSPN algorithm assumes that all the variables are discrete and there is no missing data. We introduce a practical, simplified version of LearnSPN, MiniSPN, that runs faster and can handle missing data and heterogeneous features common in real applications. We demonstrate the performance of MiniSPN on standard benchmark datasets and on two datasets from Google's Knowledge Graph exhibiting high missingness rates and a mix of discrete and continuous features. version:3
arxiv-1604-07093 | Semi-supervised Vocabulary-informed Learning | http://arxiv.org/abs/1604.07093 | id:1604.07093 author:Yanwei Fu, Leonid Sigal category:cs.CV cs.AI cs.LG stat.AP stat.ML  published:2016-04-24 summary:Despite significant progress in object categorization, in recent years, a number of important challenges remain, mainly, ability to learn from limited labeled data and ability to recognize object classes within large, potentially open, set of labels. Zero-shot learning is one way of addressing these challenges, but it has only been shown to work with limited sized class vocabularies and typically requires separation between supervised and unsupervised classes, allowing former to inform the latter but not vice versa. We propose the notion of semi-supervised vocabulary-informed learning to alleviate the above mentioned challenges and address problems of supervised, zero-shot and open set recognition using a unified framework. Specifically, we propose a maximum margin framework for semantic manifold-based recognition that incorporates distance constraints from (both supervised and unsupervised) vocabulary atoms, ensuring that labeled samples are projected closest to their correct prototypes, in the embedding space, than to others. We show that resulting model shows improvements in supervised, zero-shot, and large open set recognition, with up to 310K class vocabulary on AwA and ImageNet datasets. version:1
arxiv-1604-00119 | Semi-supervised and Unsupervised Methods for Categorizing Posts in Web Discussion Forums | http://arxiv.org/abs/1604.00119 | id:1604.00119 author:Krish Perumal category:cs.CL cs.IR cs.LG cs.SI  published:2016-04-01 summary:Web discussion forums are used by millions of people worldwide to share information belonging to a variety of domains such as automotive vehicles, pets, sports, etc. They typically contain posts that fall into different categories such as problem, solution, feedback, spam, etc. Automatic identification of these categories can aid information retrieval that is tailored for specific user requirements. Previously, a number of supervised methods have attempted to solve this problem; however, these depend on the availability of abundant training data. A few existing unsupervised and semi-supervised approaches are either focused on identifying a single category or do not report category-specific performance. In contrast, this work proposes unsupervised and semi-supervised methods that require no or minimal training data to achieve this objective without compromising on performance. A fine-grained analysis is also carried out to discuss their limitations. The proposed methods are based on sequence models (specifically, Hidden Markov Models) that can model language for each category using word and part-of-speech probability distributions, and manually specified features. Empirical evaluations across domains demonstrate that the proposed methods are better suited for this task than existing ones. version:3
arxiv-1511-07902 | Performance Limits of Online Stochastic Sub-Gradient Learning | http://arxiv.org/abs/1511.07902 | id:1511.07902 author:Bicheng Ying, Ali H. Sayed category:stat.ML cs.LG cs.MA  published:2015-11-24 summary:This work examines the performance of stochastic sub-gradient learning strategies under weaker conditions than usually considered in the literature. The conditions are shown to be automatically satisfied by several important cases of interest including the construction of Linear-SVM, LASSO, and Total-Variation denoising formulations. In comparison, these problems do not satisfy the traditional assumptions automatically and, therefore, conclusions derived based on these earlier assumptions are not directly applicable to these problems. The analysis establishes that stochastic sub-gradient strategies can attain exponential convergence rates, as opposed to sub-linear rates, to the steady-state. A realizable exponential-weighting procedure is proposed to smooth the intermediate iterates by the sub-gradient procedure and to guarantee the established performance bounds in terms of convergence rate and excessive risk performance. Both single-agent and multi-agent scenarios are studied, where the latter case assumes that a collection of agents are interconnected by a topology and can only interact locally with their neighbors. The theoretical conclusions are illustrated by several examples and simulations, including comparisons with the FISTA procedure. version:2
arxiv-1604-07078 | Unsupervised Representation Learning of Structured Radio Communication Signals | http://arxiv.org/abs/1604.07078 | id:1604.07078 author:Timothy J. O'Shea, Johnathan Corgan, T. Charles Clancy category:cs.LG  published:2016-04-24 summary:We explore unsupervised representation learning of radio communication signals in raw sampled time series representation. We demonstrate that we can learn modulation basis functions using convolutional autoencoders and visually recognize their relationship to the analytic bases used in digital communications. We also propose and evaluate quantitative met- rics for quality of encoding using domain relevant performance metrics. version:1
arxiv-1511-01844 | A note on the evaluation of generative models | http://arxiv.org/abs/1511.01844 | id:1511.01844 author:Lucas Theis, Aäron van den Oord, Matthias Bethge category:stat.ML cs.LG  published:2015-11-05 summary:Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided. version:3
arxiv-1604-07070 | Fast-and-Light Stochastic ADMM | http://arxiv.org/abs/1604.07070 | id:1604.07070 author:Shuai Zheng, James T. Kwok category:cs.LG stat.ML  published:2016-04-24 summary:The alternating direction method of multipliers (ADMM) is a powerful optimization solver in machine learning. Recently, stochastic ADMM has been integrated with variance reduction methods for stochastic gradient, leading to SAG-ADMM and SDCA-ADMM that have fast convergence rates and low iteration complexities. However, their space requirements can still be high. In this paper, we propose an integration of ADMM with the method of stochastic variance reduced gradient (SVRG). Unlike another recent integration attempt called SCAS-ADMM, the proposed algorithm retains the fast convergence benefits of SAG-ADMM and SDCA-ADMM, but is more advantageous in that its storage requirement is very low, even independent of the sample size $n$. Experimental results demonstrate that it is as fast as SAG-ADMM and SDCA-ADMM, much faster than SCAS-ADMM, and can be used on much bigger data sets. version:1
arxiv-1604-07060 | Binary Codes for Tagging X-Ray Images via Deep De-Noising Autoencoders | http://arxiv.org/abs/1604.07060 | id:1604.07060 author:Antonio Sze-To, Hamid R. Tizhoosh, Andrew K. C. Wong category:cs.CV  published:2016-04-24 summary:A Content-Based Image Retrieval (CBIR) system which identifies similar medical images based on a query image can assist clinicians for more accurate diagnosis. The recent CBIR research trend favors the construction and use of binary codes to represent images. Deep architectures could learn the non-linear relationship among image pixels adaptively, allowing the automatic learning of high-level features from raw pixels. However, most of them require class labels, which are expensive to obtain, particularly for medical images. The methods which do not need class labels utilize a deep autoencoder for binary hashing, but the code construction involves a specific training algorithm and an ad-hoc regularization technique. In this study, we explored using a deep de-noising autoencoder (DDA), with a new unsupervised training scheme using only backpropagation and dropout, to hash images into binary codes. We conducted experiments on more than 14,000 x-ray images. By using class labels only for evaluating the retrieval results, we constructed a 16-bit DDA and a 512-bit DDA independently. Comparing to other unsupervised methods, we succeeded to obtain the lowest total error by using the 512-bit codes for retrieval via exhaustive search, and speed up 9.27 times with the use of the 16-bit codes while keeping a comparable total error. We found that our new training scheme could reduce the total retrieval error significantly by 21.9%. To further boost the image retrieval performance, we developed Radon Autoencoder Barcode (RABC) which are learned from the Radon projections of images using a de-noising autoencoder. Experimental results demonstrated its superior performance in retrieval when it was combined with DDA binary codes. version:1
arxiv-1510-06423 | Optimization as Estimation with Gaussian Processes in Bandit Settings | http://arxiv.org/abs/1510.06423 | id:1510.06423 author:Zi Wang, Bolei Zhou, Stefanie Jegelka category:stat.ML cs.LG  published:2015-10-21 summary:Recently, there has been rising interest in Bayesian optimization -- the optimization of an unknown function with assumptions usually expressed by a Gaussian Process (GP) prior. We study an optimization strategy that directly uses an estimate of the argmax of the function. This strategy offers both practical and theoretical advantages: no tradeoff parameter needs to be selected, and, moreover, we establish close connections to the popular GP-UCB and GP-PI strategies. Our approach can be understood as automatically and adaptively trading off exploration and exploitation in GP-UCB and GP-PI. We illustrate the effects of this adaptive tuning via bounds on the regret as well as an extensive empirical evaluation on robotics and vision tasks, demonstrating the robustness of this strategy for a range of performance criteria. version:3
arxiv-1604-07045 | Rotation-Invariant Restricted Boltzmann Machine using shared gradient filters | http://arxiv.org/abs/1604.07045 | id:1604.07045 author:Mario Valerio Giuffrida, Sotirios A. Tsaftaris category:cs.CV  published:2016-04-24 summary:Finding suitable features has been an essential problem in computer vision. We focus on Restricted Boltzmann Machines (RBMs), which, despite their versatility, cannot accommodate transformations that may occur in the scene. As result, several approaches have been proposed that consider a set of transformations, which are used to either augment the training set or transform the actual learned filters. In this paper, we propose the Explicit Rotation-Invariant Restricted Boltzmann Machine, which exploits prior information coming from the dominant orientation of images. Our model extends the standard RBM, by adding a suitable number of weight matrices, associated to each dominant gradient. We show that our approach is able to learn rotation-invariant features, comparing it with the classic formulation of RBM on the MNIST benchmark dataset. Overall, requiring less hidden units, our method learns compact features, which are robust to rotations. version:1
arxiv-1601-07381 | Investigating echo state networks dynamics by means of recurrence analysis | http://arxiv.org/abs/1601.07381 | id:1601.07381 author:Filippo Maria Bianchi, Lorenzo Livi, Cesare Alippi category:physics.data-an cs.LG nlin.CD  published:2016-01-26 summary:In this paper, we elaborate over the well-known interpretability issue in echo state networks. The idea is to investigate the dynamics of reservoir neurons with time-series analysis techniques taken from research on complex systems. Notably, we analyze time-series of neuron activations with Recurrence Plots (RPs) and Recurrence Quantification Analysis (RQA), which permit to visualize and characterize high-dimensional dynamical systems. We show that this approach is useful in a number of ways. First, the two-dimensional representation offered by RPs provides a way for visualizing the high-dimensional dynamics of a reservoir. Our results suggest that, if the network is stable, reservoir and input denote similar line patterns in the respective RPs. Conversely, the more unstable the ESN, the more the RP of the reservoir presents instability patterns. As a second result, we show that the $\mathrm{L_{max}}$ measure is highly correlated with the well-established maximal local Lyapunov exponent. This suggests that complexity measures based on RP diagonal lines distribution provide a valuable tool to quantify the degree of network stability. Finally, our analysis shows that all RQA measures fluctuate on the proximity of the so-called edge of stability, where an ESN typically achieves maximum computational capability. We verify that the determination of the edge of stability provided by such RQA measures is more accurate than two well-known criteria based on the Jacobian matrix of the reservoir. Therefore, we claim that RPs and RQA-based analyses can be used as valuable tools to design an effective network given a specific problem. version:2
arxiv-1102-2254 | Matrix completion with column manipulation: Near-optimal sample-robustness-rank tradeoffs | http://arxiv.org/abs/1102.2254 | id:1102.2254 author:Yudong Chen, Huan Xu, Constantine Caramanis, Sujay Sanghavi category:stat.ML cs.IT math.IT  published:2011-02-10 summary:This paper considers the problem of matrix completion when some number of the columns are completely and arbitrarily corrupted, potentially by a malicious adversary. It is well-known that standard algorithms for matrix completion can return arbitrarily poor results, if even a single column is corrupted. One direct application comes from robust collaborative filtering. Here, some number of users are so-called manipulators who try to skew the predictions of the algorithm by calibrating their inputs to the system. In this paper, we develop an efficient algorithm for this problem based on a combination of a trimming procedure and a convex program that minimizes the nuclear norm and the $\ell_{1,2}$ norm. Our theoretical results show that given a vanishing fraction of observed entries, it is nevertheless possible to complete the underlying matrix even when the number of corrupted columns grows. Significantly, our results hold without any assumptions on the locations or values of the observed entries of the manipulated columns. Moreover, we show by an information-theoretic argument that our guarantees are nearly optimal in terms of the fraction of sampled entries on the authentic columns, the fraction of corrupted columns, and the rank of the underlying matrix. Our results therefore sharply characterize the tradeoffs between sample, robustness and rank in matrix completion. version:2
arxiv-1604-05519 | M$^2$S-Net: Multi-Modal Similarity Metric Learning based Deep Convolutional Network for Answer Selection | http://arxiv.org/abs/1604.05519 | id:1604.05519 author:Lingxun Meng, Yan Li category:cs.CL  published:2016-04-19 summary:Recent works using artificial neural networks based on distributed word representation greatly boost performance on various natural language processing tasks, especially the answer selection problem. Nevertheless, most of the previous works used deep learning methods (like LSTM-RNN, CNN, etc.) only to capture semantic representation of each sentence separately, without considering the interdependence between each other. In this paper, we propose a novel end-to-end learning framework which constitutes deep convolutional neural network based on multi-modal similarity metric learning (M$^2$S-Net) on pairwise tokens. The proposed model demonstrates its performance by surpassing previous state-of-the-art systems on the answer selection benchmark, i.e., TREC-QA dataset, in both MAP and MRR metrics. version:2
arxiv-1604-06979 | Cardiac Motion Analysis by Temporal Flow Graphs | http://arxiv.org/abs/1604.06979 | id:1604.06979 author:V S R Veeravasarapu, Jayanthi Sivaswamy, Vishanji Karani category:cs.CV  published:2016-04-24 summary:Cardiac motion analysis from B-mode ultrasound sequence is a key task in assessing the health of the heart. The paper proposes a new methodology for cardiac motion analysis based on the temporal behaviour of points of interest on the myocardium. We define a new signal called the Temporal Flow Graph (TFG) which depicts the movement of a point of interest over time. It is a graphical representation derived from a flow field and describes the temporal evolution of a point. We prove that TFG for an object undergoing periodic motion is also periodic. This principle can be utilized to derive both global and local information from a given sequence. We demonstrate this for detecting motion irregularities at the sequence, as well as regional levels on real and synthetic data. A coarse localisation of anatomical landmarks such as centres of left/right cavities and valve points is also demonstrated using TFGs. version:1
arxiv-1509-06729 | Algebraic Clustering of Affine Subspaces | http://arxiv.org/abs/1509.06729 | id:1509.06729 author:Manolis C. Tsakiris, Rene Vidal category:cs.CV  published:2015-09-22 summary:Subspace clustering is an important problem in machine learning with many applications in computer vision and pattern recognition. Prior work has studied this problem using algebraic, iterative, statistical, low-rank and sparse representation techniques. While these methods have been applied to both linear and affine subspaces, theoretical results have only been established in the case of linear subspaces. For example, algebraic subspace clustering (ASC) is guaranteed to provide the correct clustering when the data points are in general position and the union of subspaces is transversal. In this paper we study in a rigorous fashion the properties of ASC in the case of affine subspaces. Using notions from algebraic geometry, we prove that the homogenization trick, which embeds points in a union of affine subspaces into points in a union of linear subspaces, preserves the general position of the points and the transversality of the union of subspaces in the embedded space, thus establishing the correctness of ASC for affine subpaces. version:2
arxiv-1604-06970 | Bayesian Inference of Recursive Sequences of Group Activities from Tracks | http://arxiv.org/abs/1604.06970 | id:1604.06970 author:Ernesto Brau, Colin Dawson, Alfredo Carrillo, David Sidi, Clayton T. Morrison category:cs.AI cs.CV  published:2016-04-24 summary:We present a probabilistic generative model for inferring a description of coordinated, recursively structured group activities at multiple levels of temporal granularity based on observations of individuals' trajectories. The model accommodates: (1) hierarchically structured groups, (2) activities that are temporally and compositionally recursive, (3) component roles assigning different subactivity dynamics to subgroups of participants, and (4) a nonparametric Gaussian Process model of trajectories. We present an MCMC sampling framework for performing joint inference over recursive activity descriptions and assignment of trajectories to groups, integrating out continuous parameters. We demonstrate the model's expressive power in several simulated and complex real-world scenarios from the VIRAT and UCLA Aerial Event video data sets. version:1
arxiv-1604-06968 | Agnostic Estimation of Mean and Covariance | http://arxiv.org/abs/1604.06968 | id:1604.06968 author:Kevin A. Lai, Anup B. Rao, Santosh Vempala category:cs.DS cs.LG stat.ML  published:2016-04-24 summary:We consider the problem of estimating the mean and covariance of a distribution from iid samples in $\mathbb{R}^n$, in the presence of an $\eta$ fraction of malicious noise; this is in contrast to much recent work where the noise itself is assumed to be from a distribution of known type. The agnostic problem includes many interesting special cases, e.g., learning the parameters of a single Gaussian (or finding the best-fit Gaussian) when $\eta$ fraction of data is adversarially corrupted, agnostically learning a mixture of Gaussians, agnostic ICA, etc. We present polynomial-time algorithms to estimate the mean and covariance with error guarantees in terms of information-theoretic lower bounds. As a corollary, we also obtain an agnostic algorithm for Singular Value Decomposition. version:1
arxiv-1511-05202 | Efficient AUC Optimization for Information Ranking Applications | http://arxiv.org/abs/1511.05202 | id:1511.05202 author:Sean J. Welleck category:cs.IR stat.ML  published:2015-11-16 summary:Adequate evaluation of an information retrieval system to estimate future performance is a crucial task. Area under the ROC curve (AUC) is widely used to evaluate the generalization of a retrieval system. However, the objective function optimized in many retrieval systems is the error rate and not the AUC value. This paper provides an efficient and effective non-linear approach to optimize AUC using additive regression trees, with a special emphasis on the use of multi-class AUC (MAUC) because multiple relevance levels are widely used in many ranking applications. Compared to a conventional linear approach, the performance of the non-linear approach is comparable on binary-relevance benchmark datasets and is better on multi-relevance benchmark datasets. version:3
arxiv-1511-05641 | Net2Net: Accelerating Learning via Knowledge Transfer | http://arxiv.org/abs/1511.05641 | id:1511.05641 author:Tianqi Chen, Ian Goodfellow, Jonathon Shlens category:cs.LG  published:2015-11-18 summary:We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset. version:4
arxiv-1601-06403 | Synthesis of Gaussian Trees with Correlation Sign Ambiguity: An Information Theoretic Approach | http://arxiv.org/abs/1601.06403 | id:1601.06403 author:Ali Moharrer, Shuangqing Wei, George T. Amariucai, Jing Deng category:cs.IT cs.CV math.IT stat.ML  published:2016-01-24 summary:In latent Gaussian trees the pairwise correlation signs between the variables are intrinsically unrecoverable. Such information is vital since it completely determines the direction in which two variables are associated. %such singularity has been ignored in many studies. In this work, we resort to information theoretical approaches to achieve two fundamental goals: First, we quantify the amount of information loss due to unrecoverable sign information. Second, we show the importance of such information in determining the maximum achievable rate region, in which the observed output vector can be synthesized, given its probability density function. In particular, we model the graphical model as a communication channel and propose a new layered encoding framework to synthesize observed data using upper layer Gaussian inputs and independent Bernoulli correlation sign inputs from each layer. We find the achievable rate region for the rate tuples of multi-layer latent Gaussian messages to synthesize the desired observables. version:3
arxiv-1604-06952 | Jacques Lacan's Registers of the Psychoanalytic Field, Applied using Geometric Data Analysis to Edgar Allan Poe's "The Purloined Letter" | http://arxiv.org/abs/1604.06952 | id:1604.06952 author:Fionn Murtagh, Giuseppe Iurato category:cs.CL stat.ML 62H25  62H30  published:2016-04-23 summary:In a first investigation, a Lacan-motivated template of the Poe story is fitted to the data. A segmentation of the storyline is used in order to map out the diachrony. Based on this, it will be shown how synchronous aspects, potentially related to Lacanian registers, can be sought. This demonstrates the effectiveness of an approach based on a model template of the storyline narrative. In a second and more comprehensive investigation, we develop an approach for revealing, that is, uncovering, Lacanian register relationships. Objectives of this work include the wide and general application of our methodology. This methodology is strongly based on the "letting the data speak" Correspondence Analysis analytics platform of Jean-Paul Benz\'ecri, that is also the geometric data analysis, both qualitative and quantitative analytics, developed by Pierre Bourdieu. version:1
arxiv-1509-07983 | Probably certifiably correct k-means clustering | http://arxiv.org/abs/1509.07983 | id:1509.07983 author:Takayuki Iguchi, Dustin G. Mixon, Jesse Peterson, Soledad Villar category:cs.IT cs.DS cs.LG math.IT math.ST stat.TH  published:2015-09-26 summary:Recently, Bandeira [arXiv:1509.00824] introduced a new type of algorithm (the so-called probably certifiably correct algorithm) that combines fast solvers with the optimality certificates provided by convex relaxations. In this paper, we devise such an algorithm for the problem of k-means clustering. First, we prove that Peng and Wei's semidefinite relaxation of k-means is tight with high probability under a distribution of planted clusters called the stochastic ball model. Our proof follows from a new dual certificate for integral solutions of this semidefinite program. Next, we show how to test the optimality of a proposed k-means solution using this dual certificate in quasilinear time. Finally, we analyze a version of spectral clustering from Peng and Wei that is designed to solve k-means in the case of two clusters. In particular, we show that this quasilinear-time method typically recovers planted clusters under the stochastic ball model. version:2
arxiv-1604-06939 | An information theoretic formulation of the Dictionary Learning and Sparse Coding Problems on Statistical Manifolds | http://arxiv.org/abs/1604.06939 | id:1604.06939 author:Rudrasis Chakraborty, Monami Banerjee, Victoria Crawford, Baba C. Vemuri category:cs.CV  published:2016-04-23 summary:In this work, we propose a novel information theoretic framework for dictionary learning (DL) and sparse coding (SC) on a statistical manifold (the manifold of probability distributions). Unlike the traditional DL and SC framework, our new formulation {\it does not explicitly incorporate any sparsity inducing norm in the cost function but yet yields SCs}. Moreover, we extend this framework to the manifold of symmetric positive definite matrices, $\mathcal{P}_n$. Our algorithm approximates the data points, which are probability distributions, by the weighted Kullback-Leibeler center (KL-center) of the dictionary atoms. The KL-center is the minimizer of the maximum KL-divergence between the unknown center and members of the set whose center is being sought. Further, {\it we proved that this KL-center is a sparse combination of the dictionary atoms}. Since, the data reside on a statistical manifold, the data fidelity term can not be as simple as in the case of the vector-space data. We therefore employ the geodesic distance between the data and a sparse approximation of the data element. This cost function is minimized using an acceleterated gradient descent algorithm. An extensive set of experimental results show the effectiveness of our proposed framework. We present several experiments involving a variety of classification problems in Computer Vision applications. Further, we demonstrate the performance of our algorithm by comparing it to several state-of-the-art methods both in terms of classification accuracy and sparsity. version:1
arxiv-1604-06929 | Memory and Information Processing in Recurrent Neural Networks | http://arxiv.org/abs/1604.06929 | id:1604.06929 author:Alireza Goudarzi, Sarah Marzen, Peter Banda, Guy Feldman, Christof Teuscher, Darko Stefanovic category:cs.NE  published:2016-04-23 summary:Recurrent neural networks (RNN) are simple dynamical systems whose computational power has been attributed to their short-term memory. Short-term memory of RNNs has been previously studied analytically only for the case of orthogonal networks, and only under annealed approximation, and uncorrelated input. Here for the first time, we present an exact solution to the memory capacity and the task-solving performance as a function of the structure of a given network instance, enabling direct determination of the function--structure relation in RNNs. We calculate the memory capacity for arbitrary networks with exponentially correlated input and further related it to the performance of the system on signal processing tasks in a supervised learning setup. We compute the expected error and the worst-case error bound as a function of the spectra of the network and the correlation structure of its inputs and outputs. Our results give an explanation for learning and generalization of task solving using short-term memory, which is crucial for building alternative computer architectures using physical phenomena based on the short-term memory principle. version:1
arxiv-1512-08183 | Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews | http://arxiv.org/abs/1512.08183 | id:1512.08183 author:Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao category:cs.CL  published:2015-12-27 summary:Despite the loss of semantic information, bag-of-ngram based methods still achieve state-of-the-art results for tasks such as sentiment classification of long movie reviews. Many document embeddings methods have been proposed to capture semantics, but they still can't outperform bag-of-ngram based methods on this task. In this paper, we modify the architecture of the recently proposed Paragraph Vector, allowing it to learn document vectors by predicting not only words, but n-gram features as well. Our model is able to capture both semantics and word order in documents while keeping the expressive power of learned vectors. Experimental results on IMDB movie review dataset shows that our model outperforms previous deep learning models and bag-of-ngram based models due to the above advantages. More robust results are also obtained when our model is combined with other models. The source code of our model will be also published together with this paper. version:5
arxiv-1604-06915 | On the Sample Complexity of End-to-end Training vs. Semantic Abstraction Training | http://arxiv.org/abs/1604.06915 | id:1604.06915 author:Shai Shalev-Shwartz, Amnon Shashua category:cs.LG  published:2016-04-23 summary:We compare the end-to-end training approach to a modular approach in which a system is decomposed into semantically meaningful components. We focus on the sample complexity aspect, in the regime where an extremely high accuracy is necessary, as is the case in autonomous driving applications. We demonstrate cases in which the number of training examples required by the end-to-end approach is exponentially larger than the number of examples required by the semantic abstraction approach. version:1
arxiv-1510-02899 | TagBook: A Semantic Video Representation without Supervision for Event Detection | http://arxiv.org/abs/1510.02899 | id:1510.02899 author:Masoud Mazloom, Xirong Li, Cees G. M. Snoek category:cs.CV cs.MM  published:2015-10-10 summary:We consider the problem of event detection in video for scenarios where only few, or even zero examples are available for training. For this challenging setting, the prevailing solutions in the literature rely on a semantic video representation obtained from thousands of pre-trained concept detectors. Different from existing work, we propose a new semantic video representation that is based on freely available social tagged videos only, without the need for training any intermediate concept detectors. We introduce a simple algorithm that propagates tags from a video's nearest neighbors, similar in spirit to the ones used for image retrieval, but redesign it for video event detection by including video source set refinement and varying the video tag assignment. We call our approach TagBook and study its construction, descriptiveness and detection performance on the TRECVID 2013 and 2014 multimedia event detection datasets and the Columbia Consumer Video dataset. Despite its simple nature, the proposed TagBook video representation is remarkably effective for few-example and zero-example event detection, even outperforming very recent state-of-the-art alternatives building on supervised representations. version:2
arxiv-1604-05225 | Annotation Order Matters: Recurrent Image Annotator for Arbitrary Length Image Tagging | http://arxiv.org/abs/1604.05225 | id:1604.05225 author:Jiren Jin, Hideki Nakayama category:cs.CV  published:2016-04-18 summary:Automatic image annotation has been an important research topic in facilitating large scale image management and retrieval. Existing methods focus on learning image-tag correlation or correlation between tags to improve annotation accuracy. However, most of these methods evaluate their performance using top-k retrieval performance, where k is fixed. Although such setting gives convenience for comparing different methods, it is not the natural way that humans annotate images. The number of annotated tags should depend on image contents. Inspired by the recent progress in machine translation and image captioning, we propose a novel Recurrent Image Annotator (RIA) model that forms image annotation task as a sequence generation problem so that RIA can natively predict the proper length of tags according to image contents. We evaluate the proposed model on various image annotation datasets. In addition to comparing our model with existing methods using the conventional top-k evaluation measures, we also provide our model as a high quality baseline for the arbitrary length image tagging task. Moreover, the results of our experiments show that the order of tags in training phase has a great impact on the final annotation performance. version:2
arxiv-1601-00238 | Dimensionality-Dependent Generalization Bounds for $k$-Dimensional Coding Schemes | http://arxiv.org/abs/1601.00238 | id:1601.00238 author:Tongliang Liu, Dacheng Tao, Dong Xu category:stat.ML cs.LG  published:2016-01-03 summary:The $k$-dimensional coding schemes refer to a collection of methods that attempt to represent data using a set of representative $k$-dimensional vectors, and include non-negative matrix factorization, dictionary learning, sparse coding, $k$-means clustering and vector quantization as special cases. Previous generalization bounds for the reconstruction error of the $k$-dimensional coding schemes are mainly dimensionality independent. A major advantage of these bounds is that they can be used to analyze the generalization error when data is mapped into an infinite- or high-dimensional feature space. However, many applications use finite-dimensional data features. Can we obtain dimensionality-dependent generalization bounds for $k$-dimensional coding schemes that are tighter than dimensionality-independent bounds when data is in a finite-dimensional feature space? The answer is positive. In this paper, we address this problem and derive a dimensionality-dependent generalization bound for $k$-dimensional coding schemes by bounding the covering number of the loss function class induced by the reconstruction error. The bound is of order $\mathcal{O}\left(\left(mk\ln(mkn)/n\right)^{\lambda_n}\right)$, where $m$ is the dimension of features, $k$ is the number of the columns in the linear implementation of coding schemes, $n$ is the size of sample, $\lambda_n>0.5$ when $n$ is finite and $\lambda_n=0.5$ when $n$ is infinite. We show that our bound can be tighter than previous results, because it avoids inducing the worst-case upper bound on $k$ of the loss function and converges faster. The proposed generalization bound is also applied to some specific coding schemes to demonstrate that the dimensionality-dependent bound is an indispensable complement to these dimensionality-independent generalization bounds. version:2
arxiv-1604-06877 | Text Flow: A Unified Text Detection System in Natural Scene Images | http://arxiv.org/abs/1604.06877 | id:1604.06877 author:Shangxuan Tian, Yifeng Pan, Chang Huang, Shijian Lu, Kai Yu, Chew Lim Tan category:cs.CV  published:2016-04-23 summary:The prevalent scene text detection approach follows four sequential steps comprising character candidate detection, false character candidate removal, text line extraction, and text line verification. However, errors occur and accumulate throughout each of these sequential steps which often lead to low detection performance. To address these issues, we propose a unified scene text detection system, namely Text Flow, by utilizing the minimum cost (min-cost) flow network model. With character candidates detected by cascade boosting, the min-cost flow network model integrates the last three sequential steps into a single process which solves the error accumulation problem at both character level and text line level effectively. The proposed technique has been tested on three public datasets, i.e, ICDAR2011 dataset, ICDAR2013 dataset and a multilingual dataset and it outperforms the state-of-the-art methods on all three datasets with much higher recall and F-score. The good performance on the multilingual dataset shows that the proposed technique can be used for the detection of texts in different languages. version:1
arxiv-1604-06852 | Contextual object categorization with energy-based model | http://arxiv.org/abs/1604.06852 | id:1604.06852 author:Changyong Ri, Duho Pak, Cholryong Choe, Suhyang Kim, Yonghak Sin category:cs.CV  published:2016-04-23 summary:Object categorization is a hot issue of an image mining. Contextual information between objects is one of the important semantic knowledge of an image. However, the previous researches for an object categorization have not made full use of the contextual information, especially the spatial relations between objects. In addition, the object categorization methods, which generally use the probabilistic graphical models to implement the incorporation of contextual information with appearance of objects, are almost inevitable to evaluate the intractable partition function for normalization. In this work, we introduced fully-connected fuzzy spatial relations including directional, distance and topological relations between object regions, so the spatial relational information could be fully utilized. Then, the spatial relations were considered as well as co-occurrence and appearance of objects by using energy-based model, where the energy function was defined as the region-object association potential and the configuration potential of objects. Minimizing the energy function of whole image arrangement, we obtained the optimal label set about the image regions and addressed the evaluation of intractable partition function in conditional random fields. Experimental results show the validity and reliability of this proposed method. version:1
arxiv-1602-06023 | Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond | http://arxiv.org/abs/1602.06023 | id:1602.06023 author:Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, Bing Xiang category:cs.CL  published:2016-02-19 summary:In this work, we cast abstractive text summarization as a sequence-to-sequence problem and employ the framework of Attentional Encoder-Decoder Recurrent Neural Networks to this problem, outperforming state-of-the art model of Rush et. al. (2015) on two different corpora. We also move beyond the basic architecture, and propose several novel models to address important problems in summarization including modeling key-words, capturing the hierarchy of sentence-to-word structure and addressing the problem of words that are key to a document, but rare elsewhere. Our work shows that many of our proposed solutions contribute to further improvement in performance. In addition, we propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research. version:3
arxiv-1604-06849 | A Computational Model for Situated Task Learning with Interactive Instruction | http://arxiv.org/abs/1604.06849 | id:1604.06849 author:Shiwali Mohan, James Kirk, John Laird category:cs.AI cs.LG  published:2016-04-23 summary:Learning novel tasks is a complex cognitive activity requiring the learner to acquire diverse declarative and procedural knowledge. Prior ACT-R models of acquiring task knowledge from instruction focused on learning procedural knowledge from declarative instructions encoded in semantic memory. In this paper, we identify the requirements for designing compu- tational models that learn task knowledge from situated task- oriented interactions with an expert and then describe and evaluate a model of learning from situated interactive instruc- tion that is implemented in the Soar cognitive architecture. version:1
arxiv-1604-06838 | Word2VisualVec: Cross-Media Retrieval by Visual Feature Prediction | http://arxiv.org/abs/1604.06838 | id:1604.06838 author:Jianfeng Dong, Xirong Li, Cees G. M. Snoek category:cs.CV  published:2016-04-23 summary:This paper attacks the challenging problem of cross-media retrieval. That is, given an image find the text best describing its content, or the other way around. Different from existing works, which either rely on a joint space, or a text space, we propose to perform cross-media retrieval in a visual space only. We contribute \textit{Word2VisualVec}, a deep neural network architecture that learns to predict a deep visual encoding of textual input. We discuss its architecture for prediction of CaffeNet and GoogleNet features, as well as its loss functions for learning from text/image pairs in large-scale click-through logs and image sentences. Experiments on the Clickture-Lite and Flickr8K corpora demonstrate the robustness for both Text-to-Image and Image-to-Text retrieval, outperforming the state-of-the-art on both accounts. Interestingly, an embedding in predicted visual feature space is also highly effective when searching in text only. version:1
arxiv-1510-06096 | When Are Nonconvex Problems Not Scary? | http://arxiv.org/abs/1510.06096 | id:1510.06096 author:Ju Sun, Qing Qu, John Wright category:math.OC cs.IT math.IT stat.ML  published:2015-10-21 summary:In this note, we focus on smooth nonconvex optimization problems that obey: (1) all local minimizers are also global; and (2) around any saddle point or local maximizer, the objective has a negative directional curvature. Concrete applications such as dictionary learning, generalized phase retrieval, and orthogonal tensor decomposition are known to induce such structures. We describe a second-order trust-region algorithm that provably converges to a global minimizer efficiently, without special initializations. Finally we highlight alternatives, and open problems in this direction. version:2
arxiv-1604-03053 | Variational Latent Gaussian Process for Recovering Single-Trial Dynamics from Population Spike Trains | http://arxiv.org/abs/1604.03053 | id:1604.03053 author:Yuan Zhao, Il Memming Park category:stat.ML q-bio.NC  published:2016-04-11 summary:A small number of common factors often explain most of the interdependence among simultaneously recorded neurons, a signature of underlying low-dimensional dynamics. We posit that simple neural coding and computation manifest as low-dimensional nonlinear dynamics implemented redundantly within a large population of neurons. Recovering the latent dynamics from observations can offer a deeper understanding of neural computation. We improve upon previously-proposed methods for recovering latent dynamics, which assume either an inappropriate observation model or linear dynamics. We propose a practical and efficient inference method for a generative model with explicit point process observations and an assumption of smooth nonlinear dynamics. We validate our method on both simulated data and population recording from primary visual cortex. version:3
arxiv-1604-02376 | Finding Optimal Combination of Kernels using Genetic Programming | http://arxiv.org/abs/1604.02376 | id:1604.02376 author:Jyothi Korra category:cs.CV cs.LG cs.NE  published:2016-04-08 summary:In Computer Vision, problem of identifying or classifying the objects present in an image is called Object Categorization. It is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. Many vision features have been proposed which aid object categorization even in such adverse conditions. Past research has shown that, employing multiple features rather than any single features leads to better recognition. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of features for object categorization. Existing MKL methods use linear combination of base kernels which may not be optimal for object categorization. Real-world object categorization may need to consider complex combination of kernels(non-linear) and not only linear combination. Evolving non-linear functions of base kernels using Genetic Programming is proposed in this report. Experiment results show that non-kernel generated using genetic programming gives good accuracy as compared to linear combination of kernels. version:2
arxiv-1604-05242 | Can Boosting with SVM as Week Learners Help? | http://arxiv.org/abs/1604.05242 | id:1604.05242 author:Dinesh Govindaraj category:cs.CV cs.LG  published:2016-04-18 summary:Object recognition in images involves identifying objects with partial occlusions, viewpoint changes, varying illumination, cluttered backgrounds. Recent work in object recognition uses machine learning techniques SVM-KNN, Local Ensemble Kernel Learning, Multiple Kernel Learning. In this paper, we want to utilize SVM as week learners in AdaBoost. Experiments are done with classifiers like near- est neighbor, k-nearest neighbor, Support vector machines, Local learning(SVM- KNN) and AdaBoost. Models use Scale-Invariant descriptors and Pyramid his- togram of gradient descriptors. AdaBoost is trained with set of week classifier as SVMs, each with kernel distance function on different descriptors. Results shows AdaBoost with SVM outperform other methods for Object Categorization dataset. version:2
arxiv-1604-06832 | Refining Architectures of Deep Convolutional Neural Networks | http://arxiv.org/abs/1604.06832 | id:1604.06832 author:Sukrit Shankar, Duncan Robertson, Yani Ioannou, Antonio Criminisi, Roberto Cipolla category:cs.CV  published:2016-04-22 summary:Deep Convolutional Neural Networks (CNNs) have recently evinced immense success for various image recognition tasks. However, a question of paramount importance is somewhat unanswered in deep learning research - is the selected CNN optimal for the dataset in terms of accuracy and model size? In this paper, we intend to answer this question and introduce a novel strategy that alters the architecture of a given CNN for a specified dataset, to potentially enhance the original accuracy while possibly reducing the model size. We use two operations for architecture refinement, viz. stretching and symmetrical splitting. Our procedure starts with a pre-trained CNN for a given dataset, and optimally decides the stretch and split factors across the network to refine the architecture. We empirically demonstrate the necessity of the two operations. We evaluate our approach on two natural scenes attributes datasets, SUN Attributes and CAMIT-NSAD, with architectures of GoogleNet and VGG-11, that are quite contrasting in their construction. We justify our choice of datasets, and show that they are interestingly distinct from each other, and together pose a challenge to our architectural refinement algorithm. Our results substantiate the usefulness of the proposed method. version:1
arxiv-1504-01294 | A Probabilistic $\ell_1$ Method for Clustering High Dimensional Data | http://arxiv.org/abs/1504.01294 | id:1504.01294 author:Tsvetan Asamov, Adi Ben-Israel category:math.ST cs.LG math.OC stat.ML stat.TH  published:2015-04-06 summary:In general, the clustering problem is NP-hard, and global optimality cannot be established for non-trivial instances. For high-dimensional data, distance-based methods for clustering or classification face an additional difficulty, the unreliability of distances in very high-dimensional spaces. We propose a distance-based iterative method for clustering data in very high-dimensional space, using the $\ell_1$-metric that is less sensitive to high dimensionality than the Euclidean distance. For $K$ clusters in $\mathbb{R}^n$, the problem decomposes to $K$ problems coupled by probabilities, and an iteration reduces to finding $Kn$ weighted medians of points on a line. The complexity of the algorithm is linear in the dimension of the data space, and its performance was observed to improve significantly as the dimension increases. version:2
arxiv-1601-00062 | Practical Algorithms for Learning Near-Isometric Linear Embeddings | http://arxiv.org/abs/1601.00062 | id:1601.00062 author:Jerry Luo, Kayla Shapiro, Hao-Jun Michael Shi, Qi Yang, Kan Zhu category:stat.ML cs.LG math.OC 90C90  published:2016-01-01 summary:We propose two practical non-convex approaches for learning near-isometric, linear embeddings of finite sets of data points. Given a set of training points $\mathcal{X}$, we consider the secant set $S(\mathcal{X})$ that consists of all pairwise difference vectors of $\mathcal{X}$, normalized to lie on the unit sphere. The problem can be formulated as finding a symmetric and positive semi-definite matrix $\boldsymbol{\Psi}$ that preserves the norms of all the vectors in $S(\mathcal{X})$ up to a distortion parameter $\delta$. Motivated by non-negative matrix factorization, we reformulate our problem into a Frobenius norm minimization problem, which is solved by the Alternating Direction Method of Multipliers (ADMM) and develop an algorithm, FroMax. Another method solves for a projection matrix $\boldsymbol{\Psi}$ by minimizing the restricted isometry property (RIP) directly over the set of symmetric, postive semi-definite matrices. Applying ADMM and a Moreau decomposition on a proximal mapping, we develop another algorithm, NILE-Pro, for dimensionality reduction. FroMax is shown to converge faster for smaller $\delta$ while NILE-Pro converges faster for larger $\delta$. Both non-convex approaches are then empirically demonstrated to be more computationally efficient than prior convex approaches for a number of applications in machine learning and signal processing. version:2
arxiv-1604-06815 | Non-convex Global Minimization and False Discovery Rate Control for the TREX | http://arxiv.org/abs/1604.06815 | id:1604.06815 author:Jacob Bien, Irina Gaynanova, Johannes Lederer, Christian Müller category:stat.ML cs.OH stat.CO stat.ME  published:2016-04-22 summary:The TREX is a recently introduced method for performing sparse high-dimensional regression. Despite its statistical promise as an alternative to the lasso, square-root lasso, and scaled lasso, the TREX is computationally challenging in that it requires solving a non-convex optimization problem. This paper shows a remarkable result: despite the non-convexity of the TREX problem, there exists a polynomial-time algorithm that is guaranteed to find the global minimum. This result adds the TREX to a very short list of non-convex optimization problems that can be globally optimized (principal components analysis being a famous example). After deriving and developing this new approach, we demonstrate that (i) the ability of the TREX heuristic to reach the global minimum is strongly dependent on the difficulty of the underlying statistical problem, (ii) the polynomial-time algorithm for TREX permits a novel variable ranking and selection scheme, (iii) this scheme can be incorporated into a rule that controls the false discovery rate (FDR) of included features in the model. To achieve this last aim, we provide an extension of the results of Barber & Candes (2015) to establish that the knockoff filter framework can be applied to the TREX. This investigation thus provides both a rare case study of a heuristic for non-convex optimization and a novel way of exploiting non-convexity for statistical inference. version:1
arxiv-1604-06174 | Training Deep Nets with Sublinear Memory Cost | http://arxiv.org/abs/1604.06174 | id:1604.06174 author:Tianqi Chen, Bing Xu, Chiyuan Zhang, Carlos Guestrin category:cs.LG  published:2016-04-21 summary:We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O(sqrt(n)) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the GPU memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory - giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O(log n) with as little as O(n log n) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent additional running time cost on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences. version:2
arxiv-1307-3782 | Handwritten Digits Recognition using Deep Convolutional Neural Network: An Experimental Study using EBlearn | http://arxiv.org/abs/1307.3782 | id:1307.3782 author:Karim M. Mahmoud category:cs.NE cs.CV  published:2013-07-14 summary:In this paper, results of an experimental study of a deep convolution neural network architecture which can classify different handwritten digits using EBLearn library are reported. The purpose of this neural network is to classify input images into 10 different classes or digits (0-9) and to explore new findings. The input dataset used consists of digits images of size 32X32 in grayscale (MNIST dataset). version:3
arxiv-1604-06751 | evt_MNIST: A spike based version of traditional MNIST | http://arxiv.org/abs/1604.06751 | id:1604.06751 author:Mazdak Fatahi, Mahmood Ahmadi, Mahyar Shahsavari, Arash Ahmadi, Philippe Devienne category:cs.NE  published:2016-04-22 summary:Benchmarks and datasets have important role in evaluation of machine learning algorithms and neural network implementations. Traditional dataset for images such as MNIST is applied to evaluate efficiency of different training algorithms in neural networks. This demand is different in Spiking Neural Networks (SNN) as they require spiking inputs. It is widely believed, in the biological cortex the timing of spikes is irregular. Poisson distributions provide adequate descriptions of the irregularity in generating appropriate spikes. Here, we introduce a spike-based version of MNSIT (handwritten digits dataset),using Poisson distribution and show the Poissonian property of the generated streams. We introduce a new version of evt_MNIST which can be used for neural network evaluation. version:1
arxiv-1604-06749 | Learning a Tree-Structured Ising Model in Order to Make Predictions | http://arxiv.org/abs/1604.06749 | id:1604.06749 author:Guy Bresler, Mina Karzand category:math.ST cs.IT math.IT math.PR stat.ML stat.TH  published:2016-04-22 summary:We study the problem of learning a tree graphical model from samples such that low-order marginals are accurate. We define a distance ("small set TV" or ssTV) between distributions $P$ and $Q$ by taking the maximum, over all subsets $\mathcal{S}$ of a given size, of the total variation between the marginals of P and Q on $\mathcal{S}$. Approximating a distribution to within small ssTV allows making predictions based on partial observations. Focusing on pairwise marginals and tree-structured Ising models on $p$ nodes with maximum edge strength $\beta$, we prove that $\max\{e^{2\beta}, \eta^{-2}\} \log p$ i.i.d. samples suffices to get a distribution (from the same class) with ssTV at most $\eta$ from the one generating the samples. version:1
arxiv-1604-06743 | Latent Contextual Bandits and their Application to Personalized Recommendations for New Users | http://arxiv.org/abs/1604.06743 | id:1604.06743 author:Li Zhou, Emma Brunskill category:cs.LG cs.AI  published:2016-04-22 summary:Personalized recommendations for new users, also known as the cold-start problem, can be formulated as a contextual bandit problem. Existing contextual bandit algorithms generally rely on features alone to capture user variability. Such methods are inefficient in learning new users' interests. In this paper we propose Latent Contextual Bandits. We consider both the benefit of leveraging a set of learned latent user classes for new users, and how we can learn such latent classes from prior users. We show that our approach achieves a better regret bound than existing algorithms. We also demonstrate the benefit of our approach using a large real world dataset and a preliminary user study. version:1
arxiv-1604-06737 | Entity Embeddings of Categorical Variables | http://arxiv.org/abs/1604.06737 | id:1604.06737 author:Cheng Guo, Felix Berkhahn category:cs.LG  published:2016-04-22 summary:We map categorical variables in a function approximation problem into Euclidean spaces, which are the entity embeddings of the categorical variables. The mapping is learned by a neural network during the standard supervised training process. Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables. We applied it successfully in a recent Kaggle competition and were able to reach the third position with relative simple features. We further demonstrate in this paper that entity embedding helps the neural network to generalize better when the data is sparse and statistics is unknown. Thus it is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit. We also demonstrate that the embeddings obtained from the trained neural network boost the performance of all tested machine learning methods considerably when used as the input features instead. As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering. version:1
arxiv-1604-06730 | Developing an ICU scoring system with interaction terms using a genetic algorithm | http://arxiv.org/abs/1604.06730 | id:1604.06730 author:Chee Chun Gan, Gerard Learmonth category:cs.NE cs.LG stat.ML  published:2016-04-22 summary:ICU mortality scoring systems attempt to predict patient mortality using predictive models with various clinical predictors. Examples of such systems are APACHE, SAPS and MPM. However, most such scoring systems do not actively look for and include interaction terms, despite physicians intuitively taking such interactions into account when making a diagnosis. One barrier to including such terms in predictive models is the difficulty of using most variable selection methods in high-dimensional datasets. A genetic algorithm framework for variable selection with logistic regression models is used to search for two-way interaction terms in a clinical dataset of adult ICU patients, with separate models being built for each category of diagnosis upon admittance to the ICU. The models had good discrimination across all categories, with a weighted average AUC of 0.84 (>0.90 for several categories) and the genetic algorithm was able to find several significant interaction terms, which may be able to provide greater insight into mortality prediction for health practitioners. The GA selected models had improved performance against stepwise selection and random forest models, and provides greater flexibility in terms of variable selection by being able to optimize over any modeler-defined model performance metric instead of a specific variable importance metric. version:1
arxiv-1604-06727 | An improved chromosome formulation for genetic algorithms applied to variable selection with the inclusion of interaction terms | http://arxiv.org/abs/1604.06727 | id:1604.06727 author:Chee Chun Gan, Gerard Learmonth category:stat.ML cs.NE  published:2016-04-22 summary:Genetic algorithms are a well-known method for tackling the problem of variable selection. As they are non-parametric and can use a large variety of fitness functions, they are well-suited as a variable selection wrapper that can be applied to many different models. In almost all cases, the chromosome formulation used in these genetic algorithms consists of a binary vector of length n for n potential variables indicating the presence or absence of the corresponding variables. While the aforementioned chromosome formulation has exhibited good performance for relatively small n, there are potential problems when the size of n grows very large, especially when interaction terms are considered. We introduce a modification to the standard chromosome formulation that allows for better scalability and model sparsity when interaction terms are included in the predictor search space. Experimental results show that the indexed chromosome formulation demonstrates improved computational efficiency and sparsity on high-dimensional datasets with interaction terms compared to the standard chromosome formulation. version:1
