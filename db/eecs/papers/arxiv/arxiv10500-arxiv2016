arxiv-10500-1 | Boosting Named Entity Recognition with Neural Character Embeddings | http://arxiv.org/pdf/1505.05008v2.pdf | author:Cicero Nogueira dos Santos, Victor Guimarães category:cs.CL published:2015-05-19 summary:Most state-of-the-art named entity recognition (NER) systems rely onhandcrafted features and on the output of other NLP tasks such aspart-of-speech (POS) tagging and text chunking. In this work we propose alanguage-independent NER system that uses automatically learned features only.Our approach is based on the CharWNN deep neural network, which uses word-leveland character-level representations (embeddings) to perform sequentialclassification. We perform an extensive number of experiments using twoannotated corpora in two different languages: HAREM I corpus, which containstexts in Portuguese; and the SPA CoNLL-2002 corpus, which contains texts inSpanish. Our experimental results shade light on the contribution of neuralcharacter embeddings for NER. Moreover, we demonstrate that the same neuralnetwork which has been successfully applied to POS tagging can also achievestate-of-the-art results for language-independet NER, using the samehyperparameters, and without any handcrafted features. For the HAREM I corpus,CharWNN outperforms the state-of-the-art system by 7.9 points in the F1-scorefor the total scenario (ten NE classes), and by 7.2 points in the F1 for theselective scenario (five NE classes).
arxiv-10500-2 | Fast and Accurate Bilateral Filtering using Gauss-Polynomial Decomposition | http://arxiv.org/pdf/1505.00077v2.pdf | author:Kunal N. Chaudhury category:cs.CV published:2015-05-01 summary:The bilateral filter is a versatile non-linear filter that has found diverseapplications in image processing, computer vision, computer graphics, andcomputational photography. A widely-used form of the filter is the Gaussianbilateral filter in which both the spatial and range kernels are Gaussian. Adirect implementation of this filter requires $O(\sigma^2)$ operations perpixel, where $\sigma$ is the standard deviation of the spatial Gaussian. Inthis paper, we propose an accurate approximation algorithm that can cut downthe computational complexity to $O(1)$ per pixel for any arbitrary $\sigma$(constant-time implementation). This is based on the observation that the rangekernel operates via the translations of a fixed Gaussian over the range space,and that these translated Gaussians can be accurately approximated using theso-called Gauss-polynomials. The overall algorithm emerging from thisapproximation involves a series of spatial Gaussian filtering, which can beimplemented in constant-time using separability and recursion. We present somepreliminary results to demonstrate that the proposed algorithm comparesfavorably with some of the existing fast algorithms in terms of speed andaccuracy.
arxiv-10500-3 | Image Denoising using Optimally Weighted Bilateral Filters: A Sure and Fast Approach | http://arxiv.org/pdf/1505.00074v2.pdf | author:Kunal N. Chaudhury, Kollipara Rithwik category:cs.CV published:2015-05-01 summary:The bilateral filter is known to be quite effective in denoising imagescorrupted with small dosages of additive Gaussian noise. The denoisingperformance of the filter, however, is known to degrade quickly with theincrease in noise level. Several adaptations of the filter have been proposedin the literature to address this shortcoming, but often at a substantialcomputational overhead. In this paper, we report a simple pre-processing stepthat can substantially improve the denoising performance of the bilateralfilter, at almost no additional cost. The modified filter is designed to berobust at large noise levels, and often tends to perform poorly below a certainnoise threshold. To get the best of the original and the modified filter, wepropose to combine them in a weighted fashion, where the weights are chosen tominimize (a surrogate of) the oracle mean-squared-error (MSE). Theoptimally-weighted filter is thus guaranteed to perform better than either ofthe component filters in terms of the MSE, at all noise levels. We also providea fast algorithm for the weighted filtering. Visual and quantitative denoisingresults on standard test images are reported which demonstrate that theimprovement over the original filter is significant both visually and in termsof PSNR. Moreover, the denoising performance of the optimally-weightedbilateral filter is competitive with the computation-intensive non-local meansfilter.
arxiv-10500-4 | A Simple Yet Effective Improvement to the Bilateral Filter for Image Denoising | http://arxiv.org/pdf/1505.06578v1.pdf | author:Kollipara Rithwik, Kunal Narayan Chaudhury category:cs.CV published:2015-05-25 summary:The bilateral filter has diverse applications in image processing, computervision, and computational photography. In particular, this non-linear filter isquite effective in denoising images corrupted with additive Gaussian noise. Thefilter, however, is known to perform poorly at large noise levels. Severaladaptations of the filter have been proposed in the literature to address thisshortcoming, but often at an added computational cost. In this paper, we reporta simple yet effective modification that improves the denoising performance ofthe bilateral filter at almost no additional cost. We provide visual andquantitative results on standard test images which show that this improvementis significant both visually and in terms of PSNR and SSIM (often as large as 5dB). We also demonstrate how the proposed filtering can be implemented atreduced complexity by adapting a recent idea for fast bilateral filtering.
arxiv-10500-5 | Clustering via Content-Augmented Stochastic Blockmodels | http://arxiv.org/pdf/1505.06538v1.pdf | author:J. Massey Cashore, Xiaoting Zhao, Alexander A. Alemi, Yujia Liu, Peter I. Frazier category:stat.ML cs.LG cs.SI published:2015-05-25 summary:Much of the data being created on the web contains interactions between usersand items. Stochastic blockmodels, and other methods for community detectionand clustering of bipartite graphs, can infer latent user communities andlatent item clusters from this interaction data. These methods, however,typically ignore the items' contents and the information they provide aboutitem clusters, despite the tendency of items in the same latent cluster toshare commonalities in content. We introduce content-augmented stochasticblockmodels (CASB), which use item content together with user-item interactiondata to enhance the user communities and item clusters learned. Comparisons toseveral state-of-the-art benchmark methods, on datasets arising from scientistsinteracting with scientific articles, show that content-augmented stochasticblockmodels provide highly accurate clusters with respect to metricsrepresentative of the underlying community structure.
arxiv-10500-6 | Affine and Regional Dynamic Time Warpng | http://arxiv.org/pdf/1505.06531v1.pdf | author:Tsu-Wei Chen, Meena Abdelmaseeh, Daniel Stashuk category:cs.CV cs.CE cs.LG published:2015-05-25 summary:Pointwise matches between two time series are of great importance in timeseries analysis, and dynamic time warping (DTW) is known to provide generallyreasonable matches. There are situations where time series alignment should beinvariant to scaling and offset in amplitude or where local regions of theconsidered time series should be strongly reflected in pointwise matches. Twodifferent variants of DTW, affine DTW (ADTW) and regional DTW (RDTW), areproposed to handle scaling and offset in amplitude and provide regionalemphasis respectively. Furthermore, ADTW and RDTW can be combined in twodifferent ways to generate alignments that incorporate advantages from bothmethods, where the affine model can be applied either globally to the entiretime series or locally to each region. The proposed alignment methodsoutperform DTW on specific simulated datasets, and one-nearest-neighborclassifiers using their associated difference measures are competitive with thedifference measures associated with state-of-the-art alignment methods on realdatasets.
arxiv-10500-7 | Structural Similarity Index SSIMplified: Is there really a simpler concept at the heart of image quality measurement? | http://arxiv.org/pdf/1503.06680v2.pdf | author:Kieran Gerard Larkin category:cs.CV published:2015-01-29 summary:The Structural Similarity Index (SSIM) is generally considered to be amilestone in the recent history of Image Quality Assessment (IQA). Alas, SSIM'saccepted development from the product of three heuristic factors continues toobscure it's real underlying simplicity. Starting instead from asymmetric-antisymmetric reformulation we first show SSIM to be a contrast orvisibility function in the classic sense. Furthermore, the previously enigmaticstructural covariance is revealed to be the difference of variances. The secondstep, eliminating the intrinsic quadratic nature of SSIM, allows a near linearcorrelation with human observer scores, and without invoking the usual, butarbitrary, sigmoid model fitting. We conclude that SSIM can be re-interpretedin terms of perceptual masking: it is essentially equivalent to a normalisederror or noise visibility function (NVF), and, furthermore, the NVF aloneexplains it success in modelling perceptual image quality. We use the termDissimilarity Quotient (DQ) for the specifically anti/symmetric SSIM derivedNVF. It seems that IQA researchers may now have two choices: 1) Continue to usethe complex SSIM formula, but noting that SSIM only works coincidentally sincethe covariance term is actually the mean square error (MSE) in disguise. 2) Usethe simplest of all perceptually-masked image quality metrics, namely NVF orDQ. On this choice Occam is clear: in the absence of differences in predictiveability, the fewer assumptions that are made, the better.
arxiv-10500-8 | Constrained 1-Spectral Clustering | http://arxiv.org/pdf/1505.06485v1.pdf | author:Syama Sundar Rangapuram, Matthias Hein category:stat.ML cs.LG published:2015-05-24 summary:An important form of prior information in clustering comes in form ofcannot-link and must-link constraints. We present a generalization of thepopular spectral clustering technique which integrates such constraints.Motivated by the recently proposed $1$-spectral clustering for theunconstrained problem, our method is based on a tight relaxation of theconstrained normalized cut into a continuous optimization problem. Opposite toall other methods which have been suggested for constrained spectralclustering, we can always guarantee to satisfy all constraints. Moreover, oursoft formulation allows to optimize a trade-off between normalized cut and thenumber of violated constraints. An efficient implementation is provided whichscales to large datasets. We outperform consistently all other proposed methodsin the experiments.
arxiv-10500-9 | Tight Continuous Relaxation of the Balanced $k$-Cut Problem | http://arxiv.org/pdf/1505.06478v1.pdf | author:Syama Sundar Rangapuram, Pramod Kaushik Mudrakarta, Matthias Hein category:stat.ML cs.LG published:2015-05-24 summary:Spectral Clustering as a relaxation of the normalized/ratio cut has becomeone of the standard graph-based clustering methods. Existing methods for thecomputation of multiple clusters, corresponding to a balanced $k$-cut of thegraph, are either based on greedy techniques or heuristics which have weakconnection to the original motivation of minimizing the normalized cut. In thispaper we propose a new tight continuous relaxation for any balanced $k$-cutproblem and show that a related recently proposed relaxation is in most casesloose leading to poor performance in practice. For the optimization of ourtight continuous relaxation we propose a new algorithm for the difficultsum-of-ratios minimization problem which achieves monotonic descent. Extensivecomparisons show that our method outperforms all existing approaches for ratiocut and other balanced $k$-cut criteria.
arxiv-10500-10 | Deterministic Conditions for Subspace Identifiability from Incomplete Sampling | http://arxiv.org/pdf/1410.0633v3.pdf | author:Daniel L. Pimentel-Alarcón, Robert D. Nowak, Nigel Boston category:stat.ML cs.LG math.CO published:2014-10-02 summary:Consider a generic $r$-dimensional subspace of $\mathbb{R}^d$, $r<d$, andsuppose that we are only given projections of this subspace onto small subsetsof the canonical coordinates. The paper establishes necessary and sufficientdeterministic conditions on the subsets for subspace identifiability.
arxiv-10500-11 | A Characterization of Deterministic Sampling Patterns for Low-Rank Matrix Completion | http://arxiv.org/pdf/1503.02596v2.pdf | author:Daniel L. Pimentel-Alarcón, Nigel Boston, Robert D. Nowak category:stat.ML cs.LG math.AG published:2015-03-09 summary:Low-rank matrix completion (LRMC) problems arise in a wide variety ofapplications. Previous theory mainly provides conditions for completion undermissing-at-random samplings. An incomplete $d \times N$ matrix is$\textit{finitely completable}$ if there are at most finitely many rank-$r$matrices that agree with all its observed entries. Finite completability is thetipping point in LRMC, as a few additional samples of a finitely completablematrix guarantee its $\textit{unique}$ completability. The main contribution ofthis paper is a full characterization of finitely completable observation sets.We use this characterization to derive sufficient deterministic samplingconditions for unique completability. We also show that under uniform randomsampling schemes, these conditions are satisfied with high probability if atleast $\mathscr{O}(\max\{r,\log d \})$ entries per column are observed.
arxiv-10500-12 | Detecting bird sound in unknown acoustic background using crowdsourced training data | http://arxiv.org/pdf/1505.06443v1.pdf | author:Timos Papadopoulos, Stephen Roberts, Kathy Willis category:stat.ML cs.LG cs.SD published:2015-05-24 summary:Biodiversity monitoring using audio recordings is achievable at a trulyglobal scale via large-scale deployment of inexpensive, unattended recordingstations or by large-scale crowdsourcing using recording and speciesrecognition on mobile devices. The ability, however, to reliably identifyvocalising animal species is limited by the fact that acoustic signatures ofinterest in such recordings are typically embedded in a diverse and complexacoustic background. To avoid the problems associated with modelling suchbackgrounds, we build generative models of bird sounds and use the concept ofnovelty detection to screen recordings to detect sections of data which arelikely bird vocalisations. We present detection results against variousacoustic environments and different signal-to-noise ratios. We discuss theissues related to selecting the cost function and setting detection thresholdsin such algorithms. Our methods are designed to be scalable and automaticallyapplicable to arbitrary selections of species depending on the specificgeographic region and time period of deployment.
arxiv-10500-13 | Classifying Relations by Ranking with Convolutional Neural Networks | http://arxiv.org/pdf/1504.06580v2.pdf | author:Cicero Nogueira dos Santos, Bing Xiang, Bowen Zhou category:cs.CL cs.LG cs.NE published:2015-04-24 summary:Relation classification is an important semantic processing task for whichstate-ofthe-art systems still rely on costly handcrafted features. In this workwe tackle the relation classification task using a convolutional neural networkthat performs classification by ranking (CR-CNN). We propose a new pairwiseranking loss function that makes it easy to reduce the impact of artificialclasses. We perform experiments using the the SemEval-2010 Task 8 dataset,which is designed for the task of classifying the relationship between twonominals marked in a sentence. Using CRCNN, we outperform the state-of-the-artfor this dataset and achieve a F1 of 84.1 without using any costly handcraftedfeatures. Additionally, our experimental results show that: (1) our approach ismore effective than CNN followed by a softmax classifier; (2) omitting therepresentation of the artificial class Other improves both precision andrecall; and (3) using only word embeddings as input features is enough toachieve state-of-the-art results if we consider only the text between the twotarget nominals.
arxiv-10500-14 | Deep Speaker Vectors for Semi Text-independent Speaker Verification | http://arxiv.org/pdf/1505.06427v1.pdf | author:Lantian Li, Dong Wang, Zhiyong Zhang, Thomas Fang Zheng category:cs.CL cs.LG cs.NE published:2015-05-24 summary:Recent research shows that deep neural networks (DNNs) can be used to extractdeep speaker vectors (d-vectors) that preserve speaker characteristics and canbe used in speaker verification. This new method has been tested ontext-dependent speaker verification tasks, and improvement was reported whencombined with the conventional i-vector method. This paper extends the d-vector approach to semi text-independent speakerverification tasks, i.e., the text of the speech is in a limited set of shortphrases. We explore various settings of the DNN structure used for d-vectorextraction, and present a phone-dependent training which employs the posteriorfeatures obtained from an ASR system. The experimental results show that it ispossible to apply d-vectors on semi text-independent speaker recognition, andthe phone-dependent training improves system performance.
arxiv-10500-15 | Compute Less to Get More: Using ORC to Improve Sparse Filtering | http://arxiv.org/pdf/1409.4689v2.pdf | author:Johannes Lederer, Sergio Guadarrama category:cs.CV cs.LG published:2014-09-16 summary:Sparse Filtering is a popular feature learning algorithm for imageclassification pipelines. In this paper, we connect the performance of SparseFiltering with spectral properties of the corresponding feature matrices. Thisconnection provides new insights into Sparse Filtering; in particular, itsuggests early stopping of Sparse Filtering. We therefore introduce the OptimalRoundness Criterion (ORC), a novel stopping criterion for Sparse Filtering. Weshow that this stopping criterion is related with pre-processing proceduressuch as Statistical Whitening and demonstrate that it can make imageclassification with Sparse Filtering considerably faster and more accurate.
arxiv-10500-16 | Don't Fall for Tuning Parameters: Tuning-Free Variable Selection in High Dimensions With the TREX | http://arxiv.org/pdf/1404.0541v3.pdf | author:Johannes Lederer, Christian Müller category:stat.ME stat.ML published:2014-04-02 summary:Lasso is a seminal contribution to high-dimensional statistics, but it hingeson a tuning parameter that is difficult to calibrate in practice. A partialremedy for this problem is Square-Root Lasso, because it inherently calibratesto the noise variance. However, Square-Root Lasso still requires thecalibration of a tuning parameter to all other aspects of the model. In thisstudy, we introduce TREX, an alternative to Lasso with an inherent calibrationto all aspects of the model. This adaptation to the entire model renders TREXan estimator that does not require any calibration of tuning parameters. Weshow that TREX can outperform cross-validated Lasso in terms of variableselection and computational efficiency. We also introduce a bootstrappedversion of TREX that can further improve variable selection. We illustrate thepromising performance of TREX both on synthetic data and on a recenthigh-dimensional biological data set that considers riboflavin production in B.subtilis.
arxiv-10500-17 | Domain Adaptation Extreme Learning Machines for Drift Compensation in E-nose Systems | http://arxiv.org/pdf/1505.06405v1.pdf | author:Lei Zhang, David Zhang category:cs.LG published:2015-05-24 summary:This paper addresses an important issue, known as sensor drift that behaves anonlinear dynamic property in electronic nose (E-nose), from the viewpoint ofmachine learning. Traditional methods for drift compensation are laborious andcostly due to the frequent acquisition and labeling process for gases samplesrecalibration. Extreme learning machines (ELMs) have been confirmed to beefficient and effective learning techniques for pattern recognition andregression. However, ELMs primarily focus on the supervised, semi-supervisedand unsupervised learning problems in single domain (i.e. source domain). Toour best knowledge, ELM with cross-domain learning capability has never beenstudied. This paper proposes a unified framework, referred to as DomainAdaptation Extreme Learning Machine (DAELM), which learns a robust classifierby leveraging a limited number of labeled data from target domain for driftcompensation as well as gases recognition in E-nose systems, without loss ofthe computational efficiency and learning ability of traditional ELM. In theunified framework, two algorithms called DAELM-S and DAELM-T are proposed forthe purpose of this paper, respectively. In order to percept the differencesamong ELM, DAELM-S and DAELM-T, two remarks are provided. Experiments on thepopular sensor drift data with multiple batches collected by E-nose systemclearly demonstrate that the proposed DAELM significantly outperforms existingdrift compensation methods without cumbersome measures, and also bring newperspectives for ELM.
arxiv-10500-18 | The evolutionary origins of hierarchy | http://arxiv.org/pdf/1505.06353v1.pdf | author:Henok Mengistu, Joost Huizinga, Jean-Baptiste Mouret, Jeff Clune category:cs.NE published:2015-05-23 summary:Hierarchical organization -- the recursive composition of sub-modules -- isubiquitous in biological networks, including neural, metabolic, ecological, andgenetic regulatory networks, and in human-made systems, such as largeorganizations and the Internet. To date, most research on hierarchy in networkshas been limited to quantifying this property. However, an open, importantquestion in evolutionary biology is why hierarchical organization evolves inthe first place. It has recently been shown that modularity evolves because ofthe presence of a cost for network connections. Here we investigate whethersuch connection costs also tend to cause a hierarchical organization of suchmodules. In computational simulations, we find that networks without aconnection cost do not evolve to be hierarchical, even when the task has ahierarchical structure. However, with a connection cost, networks evolve to beboth modular and hierarchical, and these networks exhibit higher overallperformance and evolvability (i.e. faster adaptation to new environments).Additional analyses confirm that hierarchy independently improves adaptabilityafter controlling for modularity. Overall, our results suggest that the sameforce--the cost of connections--promotes the evolution of both hierarchy andmodularity, and that these properties are important drivers of networkperformance and adaptability. In addition to shedding light on the emergence ofhierarchy across the many domains in which it appears, these findings will alsoaccelerate future research into evolving more complex, intelligentcomputational brains in the fields of artificial intelligence and robotics.
arxiv-10500-19 | Image Data Compression for Covariance and Histogram Descriptors | http://arxiv.org/pdf/1412.1740v2.pdf | author:Matt J. Kusner, Nicholas I. Kolkin, Stephen Tyree, Kilian Q. Weinberger category:stat.ML cs.CV cs.LG published:2014-12-04 summary:Covariance and histogram image descriptors provide an effective way tocapture information about images. Both excel when used in combination withspecial purpose distance metrics. For covariance descriptors these metricsmeasure the distance along the non-Euclidean Riemannian manifold of symmetricpositive definite matrices. For histogram descriptors the Earth Mover'sdistance measures the optimal transport between two histograms. Although moreprecise, these distance metrics are very expensive to compute, making themimpractical in many applications, even for data sets of only a few thousandexamples. In this paper we present two methods to compress the size ofcovariance and histogram datasets with only marginal increases in test errorfor k-nearest neighbor classification. Specifically, we show that we can reducedata sets to 16% and in some cases as little as 2% of their original size,while approximately matching the test error of kNN classification on the fulltraining set. In fact, because the compressed set is learned in a supervisedfashion, it sometimes even outperforms the full data set, while requiring onlya fraction of the space and drastically reducing test-time computation.
arxiv-10500-20 | The Minimum Spanning Tree of Maximum Entropy | http://arxiv.org/pdf/1505.06319v1.pdf | author:Samuel de Sousa, Walter G. Kropatsch category:cs.CV published:2015-05-23 summary:In computer vision, we have the problem of creating graphs out ofunstructured point-sets, i.e. the data graph. A common approach for thisproblem consists of building a triangulation which might not always lead to thebest solution. Small changes in the location of the points might generategraphs with unstable configurations and the topology of the graph could changesignificantly. After building the data-graph, one could apply Graph Matchingtechniques to register the original point-sets. In this paper, we propose adata graph technique based on the Minimum Spanning Tree of Maximum Entropty(MSTME). We aim at a data graph construction which could be more stable thanthe Delaunay triangulation with respect to small variations in the neighborhoodof points. Our technique aims at creating data graphs which could help thepoint-set registration process. We propose an algorithm with a single freeparameter that weighs the importance between the total weight cost and theentropy of the current spanning tree. We compare our algorithm on a number ofdifferent databases with the Delaunay triangulation.
arxiv-10500-21 | A Frobenius Model of Information Structure in Categorical Compositional Distributional Semantics | http://arxiv.org/pdf/1505.06294v1.pdf | author:Dimitri Kartsaklis, Mehrnoosh Sadrzadeh category:cs.CL cs.AI math.CT math.RA published:2015-05-23 summary:The categorical compositional distributional model of Coecke, Sadrzadeh andClark provides a linguistically motivated procedure for computing the meaningof a sentence as a function of the distributional meaning of the words therein.The theoretical framework allows for reasoning about compositional aspects oflanguage and offers structural ways of studying the underlying relationships.While the model so far has been applied on the level of syntactic structures, asentence can bring extra information conveyed in utterances via intonationalmeans. In the current paper we extend the framework in order to accommodatethis additional information, using Frobenius algebraic structures canonicallyinduced over the basis of finite-dimensional vector spaces. We detail thetheory, provide truth-theoretic and distributional semantics for meanings ofintonationally-marked utterances, and present justifications and extensiveexamples.
arxiv-10500-22 | Low-Rank Matrix Recovery from Row-and-Column Affine Measurements | http://arxiv.org/pdf/1505.06292v1.pdf | author:Avishai Wagner, Or Zuk category:cs.LG cs.IT math.IT math.ST stat.CO stat.ML stat.TH 15A83 published:2015-05-23 summary:We propose and study a row-and-column affine measurement scheme for low-rankmatrix recovery. Each measurement is a linear combination of elements in onerow or one column of a matrix $X$. This setting arises naturally inapplications from different domains. However, current algorithms developed forstandard matrix recovery problems do not perform well in our case, hence theneed for developing new algorithms and theory for our problem. We propose asimple algorithm for the problem based on Singular Value Decomposition ($SVD$)and least-squares ($LS$), which we term \alg. We prove that (a simplifiedversion of) our algorithm can recover $X$ exactly with the minimum possiblenumber of measurements in the noiseless case. In the general noisy case, weprove performance guarantees on the reconstruction accuracy under the Frobeniusnorm. In simulations, our row-and-column design and \alg algorithm showimproved speed, and comparable and in some cases better accuracy compared tostandard measurements designs and algorithms. Our theoretical and experimentalresults suggest that the proposed row-and-column affine measurements scheme,together with our recovery algorithm, may provide a powerful framework foraffine matrix reconstruction.
arxiv-10500-23 | Exposing ambiguities in a relation-extraction gold standard with crowdsourcing | http://arxiv.org/pdf/1505.06256v1.pdf | author:Tong Shu Li, Benjamin M. Good, Andrew I. Su category:cs.CL q-bio.QM published:2015-05-23 summary:Semantic relation extraction is one of the frontiers of biomedical naturallanguage processing research. Gold standards are key tools for advancing thisresearch. It is challenging to generate these standards because of the highcost of expert time and the difficulty in establishing agreement betweenannotators. We implemented and evaluated a microtask crowdsourcing approachthat can produce a gold standard for extracting drug-disease relations. Theaggregated crowd judgment agreed with expert annotations from a pre-existingcorpus on 43 of 60 sentences tested. The levels of crowd agreement varied in asimilar manner to the levels of agreement among the original expert annotators.This work rein-forces the power of crowdsourcing in the process of assemblinggold standards for relation extraction. Further, it high-lights the importanceof exposing the levels of agreement between human annotators, expert or crowd,in gold standard corpora as these are reproducible signals indicatingambiguities in the data or in the annotation guidelines.
arxiv-10500-24 | Efficient Large Scale Video Classification | http://arxiv.org/pdf/1505.06250v1.pdf | author:Balakrishnan Varadarajan, George Toderici, Sudheendra Vijayanarasimhan, Apostol Natsev category:cs.CV cs.MM cs.NE published:2015-05-22 summary:Video classification has advanced tremendously over the recent years. A largepart of the improvements in video classification had to do with the work doneby the image classification community and the use of deep convolutionalnetworks (CNNs) which produce competitive results with hand- crafted motionfeatures. These networks were adapted to use video frames in various ways andhave yielded state of the art classification results. We present two methodsthat build on this work, and scale it up to work with millions of videos andhundreds of thousands of classes while maintaining a low computational cost. Inthe context of large scale video processing, training CNNs on video frames isextremely time consuming, due to the large number of frames involved. Wepropose to avoid this problem by training CNNs on either YouTube thumbnails orFlickr images, and then using these networks' outputs as features for otherhigher level classifiers. We discuss the challenges of achieving this andpropose two models for frame-level and video-level classification. The first isa highly efficient mixture of experts while the latter is based on long shortterm memory neural networks. We present results on the Sports-1M video dataset(1 million videos, 487 classes) and on a new dataset which has 12 millionvideos and 150,000 labels.
arxiv-10500-25 | Greedy Biomarker Discovery in the Genome with Applications to Antimicrobial Resistance | http://arxiv.org/pdf/1505.06249v1.pdf | author:Alexandre Drouin, Sébastien Giguère, Maxime Déraspe, François Laviolette, Mario Marchand, Jacques Corbeil category:q-bio.GN cs.LG stat.ML published:2015-05-22 summary:The Set Covering Machine (SCM) is a greedy learning algorithm that producessparse classifiers. We extend the SCM for datasets that contain a huge numberof features. The whole genetic material of living organisms is an example ofsuch a case, where the number of feature exceeds 10^7. Three human pathogenswere used to evaluate the performance of the SCM at predicting antimicrobialresistance. Our results show that the SCM compares favorably in terms ofsparsity and accuracy against L1 and L2 regularized Support Vector Machines andCART decision trees. Moreover, the SCM was the only algorithm that couldconsider the full feature space. For all other algorithms, the latter had to befiltered as a preprocessing step.
arxiv-10500-26 | Tunnel Surface 3D Reconstruction from Unoriented Image Sequences | http://arxiv.org/pdf/1505.06237v1.pdf | author:Arnold Bauer, Karlheinz Gutjahr, Gerhard Paar, Heiner Kontrus, Robert Glatzl category:cs.CV published:2015-05-22 summary:The 3D documentation of the tunnel surface during construction requires fastand robust measurement systems. In the solution proposed in this paper, duringtunnel advance a single camera is taking pictures of the tunnel surface fromseveral positions. The recorded images are automatically processed to gain a 3Dtunnel surface model. Image acquisition is realized by thetunneling/advance/driving personnel close to the tunnel face (= the front endof the advance). Based on the following fully automatic analysis/evaluation, adecision on the quality of the outbreak can be made within a few minutes. Thispaper describes the image recording system and conditions as well as thestereo-photogrammetry based workflow for the continuously merged dense 3Dreconstruction of the entire advance region. Geo-reference is realized by meansof signalized targets that are automatically detected in the images. We reporton the results of recent testing under real construction conditions, andconclude with prospects for further development in terms of on-siteperformance.
arxiv-10500-27 | Keyphrase Based Evaluation of Automatic Text Summarization | http://arxiv.org/pdf/1505.06228v1.pdf | author:Fatma Elghannam, Tarek El-Shishtawy category:cs.CL 94AXX published:2015-05-22 summary:The development of methods to deal with the informative contents of the textunits in the matching process is a major challenge in automatic summaryevaluation systems that use fixed n-gram matching. The limitation causesinaccurate matching between units in a peer and reference summaries. Thepresent study introduces a new Keyphrase based Summary Evaluator KpEval forevaluating automatic summaries. The KpEval relies on the keyphrases since theyconvey the most important concepts of a text. In the evaluation process, thekeyphrases are used in their lemma form as the matching text unit. The systemwas applied to evaluate different summaries of Arabic multi-document data setpresented at TAC2011. The results showed that the new evaluation techniquecorrelates well with the known evaluation systems: Rouge1, Rouge2, RougeSU4,and AutoSummENG MeMoG. KpEval has the strongest correlation with AutoSummENGMeMoG, Pearson and spearman correlation coefficient measures are 0.8840, 0.9667respectively.
arxiv-10500-28 | Learning Dynamic Feature Selection for Fast Sequential Prediction | http://arxiv.org/pdf/1505.06169v1.pdf | author:Emma Strubell, Luke Vilnis, Kate Silverstein, Andrew McCallum category:cs.CL cs.LG published:2015-05-22 summary:We present paired learning and inference algorithms for significantlyreducing computation and increasing speed of the vector dot products in theclassifiers that are at the heart of many NLP components. This is accomplishedby partitioning the features into a sequence of templates which are orderedsuch that high confidence can often be reached using only a small fraction ofall features. Parameter estimation is arranged to maximize accuracy and earlyconfidence in this sequence. Our approach is simpler and better suited to NLPthan other related cascade methods. We present experiments in left-to-rightpart-of-speech tagging, named entity recognition, and transition-baseddependency parsing. On the typical benchmarking datasets we can preserve POStagging accuracy above 97% and parsing LAS above 88.5% both with over afive-fold reduction in run-time, and NER F1 above 88 with more than 2x increasein speed.
arxiv-10500-29 | Design and Implementation of Real-time Algorithms for Eye Tracking and PERCLOS Measurement for on board Estimation of Alertness of Drivers | http://arxiv.org/pdf/1505.06162v1.pdf | author:Anjith George, Aurobinda Routray category:cs.CV published:2015-05-22 summary:The alertness level of drivers can be estimated with the use of computervision based methods. The level of fatigue can be found from the value ofPERCLOS. It is the ratio of closed eye frames to the total frames processed.The main objective of the thesis is the design and implementation of real-timealgorithms for measurement of PERCLOS. In this work we have developed areal-time system which is able to process the video onboard and to alarm thedriver in case the driver is in alert. For accurate estimation of PERCLOS theframe rate should be greater than 4 and accuracy should be greater than 90%.For eye detection we have used mainly two approaches Haar classifier basedmethod and Principal Component Analysis (PCA) based method for day time. Duringnight time active Near Infra Red (NIR) illumination is used. Local BinaryPattern (LBP) histogram based method is used for the detection of eyes at nighttime. The accuracy rate of the algorithms was found to be more than 90% atframe rates more than 5 fps which was suitable for the application.
arxiv-10500-30 | Correlation Clustering with Constrained Cluster Sizes and Extended Weights Bounds | http://arxiv.org/pdf/1411.0547v3.pdf | author:Gregory J. Puleo, Olgica Milenkovic category:cs.LG cs.DS published:2014-11-03 summary:We consider the problem of correlation clustering on graphs with constraintson both the cluster sizes and the positive and negative weights of edges. Ourcontributions are twofold: First, we introduce the problem of correlationclustering with bounded cluster sizes. Second, we extend the regime of weightvalues for which the clustering may be performed with constant approximationguarantees in polynomial time and apply the results to the bounded cluster sizeproblem.
arxiv-10500-31 | Statistical Estimation and Clustering of Group-invariant Orientation Parameters | http://arxiv.org/pdf/1503.04474v2.pdf | author:Yu-Hui Chen, Dennis Wei, Gregory Newstadt, Marc DeGraef, Jeffrey Simmons, Alfred Hero category:stat.ML published:2015-03-15 summary:We treat the problem of estimation of orientation parameters whose values areinvariant to transformations from a spherical symmetry group. Previous work hasshown that any such group-invariant distribution must satisfy a restrictedfinite mixture representation, which allows the orientation parameter to beestimated using an Expectation Maximization (EM) maximum likelihood (ML)estimation algorithm. In this paper, we introduce two parametric models forthis spherical symmetry group estimation problem: 1) the hyperbolic Von MisesFisher (VMF) mixture distribution and 2) the Watson mixture distribution. Wealso introduce a new EM-ML algorithm for clustering samples that come frommixtures of group-invariant distributions with different parameters. We applythe models to the problem of mean crystal orientation estimation under thespherically symmetric group associated with the crystal form, e.g., cubic oroctahedral or hexahedral. Simulations and experiments establish the advantagesof the extended EM-VMF and EM-Watson estimators for data acquired by ElectronBackscatter Diffraction (EBSD) microscopy of a polycrystalline Nickel alloysample.
arxiv-10500-32 | Machine Learning for Indoor Localization Using Mobile Phone-Based Sensors | http://arxiv.org/pdf/1505.06125v1.pdf | author:David Mascharka, Eric Manley category:cs.LG cs.NI published:2015-05-22 summary:In this paper we investigate the problem of localizing a mobile device basedon readings from its embedded sensors utilizing machine learning methodologies.We consider a real-world environment, collect a large dataset of 3110datapoints, and examine the performance of a substantial number of machinelearning algorithms in localizing a mobile device. We have found algorithmsthat give a mean error as accurate as 0.76 meters, outperforming other indoorlocalization systems reported in the literature. We also propose a hybridinstance-based approach that results in a speed increase by a factor of tenwith no loss of accuracy in a live deployment over standard instance-basedmethods, allowing for fast and accurate localization. Further, we determine howsmaller datasets collected with less density affect accuracy of localization,important for use in real-world environments. Finally, we demonstrate thatthese approaches are appropriate for real-world deployment by evaluating theirperformance in an online, in-motion experiment.
arxiv-10500-33 | A Mixture of Generalized Hyperbolic Factor Analyzers | http://arxiv.org/pdf/1311.6530v3.pdf | author:Cristina Tortora, Paul D. McNicholas, Ryan P. Browne category:stat.ME stat.ML published:2013-11-26 summary:Model-based clustering imposes a finite mixture modelling structure on datafor clustering. Finite mixture models assume that the population is a convexcombination of a finite number of densities, the distribution within eachpopulation is a basic assumption of each particular model. Among alldistributions that have been tried, the generalized hyperbolic distribution hasthe advantage that is a generalization of several other methods, such as theGaussian distribution, the skew t-distribution, etc. With specific parameters,it can represent either a symmetric or a skewed distribution. While itsinherent flexibility is an advantage in many ways, it means the estimation ofmore parameters than its special and limiting cases. The aim of this work is topropose a mixture of generalized hyperbolic factor analyzers to introduceparsimony and extend the method to high dimensional data. This work can be seenas an extension of the mixture of factor analyzers model to generalizedhyperbolic mixtures. The performance of our generalized hyperbolic factoranalyzers is illustrated on real data, where it performs favourably compared toits Gaussian analogue.
arxiv-10500-34 | Robust Rotation Synchronization via Low-rank and Sparse Matrix Decomposition | http://arxiv.org/pdf/1505.06079v1.pdf | author:Federica Arrigoni, Andrea Fusiello, Beatrice Rossi, Pasqualina Fragneto category:cs.CV published:2015-05-22 summary:This paper deals with the rotation synchronization problem, which arises inglobal registration of 3D point-sets and in structure from motion. The problemis formulated in an unprecedented way as a "low-rank and sparse" matrixdecomposition that handles both outliers and missing data. A minimizationstrategy, dubbed R-GoDec, is also proposed and evaluated experimentally againststate-of-the-art algorithms on simulated and real data. The results show thatR-GoDec is the fastest among the robust algorithms.
arxiv-10500-35 | GPTIPS 2: an open-source software platform for symbolic data mining | http://arxiv.org/pdf/1412.4690v2.pdf | author:Dominic P. Searson category:cs.MS cs.NE published:2014-12-15 summary:GPTIPS is a free, open source MATLAB based software platform for symbolicdata mining (SDM). It uses a multigene variant of the biologically inspiredmachine learning method of genetic programming (MGGP) as the engine that drivesthe automatic model discovery process. Symbolic data mining is the process ofextracting hidden, meaningful relationships from data in the form of symbolicequations. In contrast to other data-mining methods, the structuraltransparency of the generated predictive equations can give new insights intothe physical systems or processes that generated the data. Furthermore, thistransparency makes the models very easy to deploy outside of MATLAB. Therationale behind GPTIPS is to reduce the technical barriers to using,understanding, visualising and deploying GP based symbolic models of data,whilst at the same time remaining highly customisable and delivering robustnumerical performance for power users. In this chapter, notable new features ofthe latest version of the software are discussed with these aims in mind.Additionally, a simplified variant of the MGGP high level gene crossovermechanism is proposed. It is demonstrated that the new functionality of GPTIPS2 (a) facilitates the discovery of compact symbolic relationships from datausing multiple approaches, e.g. using novel gene-centric visualisation analysisto mitigate horizontal bloat and reduce complexity in multigene symbolicregression models (b) provides numerous methods for visualising the propertiesof symbolic models (c) emphasises the generation of graphically navigablelibraries of models that are optimal in terms of the Pareto trade off surfaceof model performance and complexity and (d) expedites real world applicationsby the simple, rapid and robust deployment of symbolic models outside thesoftware environment they were developed in.
arxiv-10500-36 | Instant Learning: Parallel Deep Neural Networks and Convolutional Bootstrapping | http://arxiv.org/pdf/1505.05972v1.pdf | author:Andrew J. R. Simpson category:cs.LG 68Txx published:2015-05-22 summary:Although deep neural networks (DNN) are able to scale with direct advances incomputational power (e.g., memory and processing speed), they are not wellsuited to exploit the recent trends for parallel architectures. In particular,gradient descent is a sequential process and the resulting serial dependenciesmean that DNN training cannot be parallelized effectively. Here, we show that aDNN may be replicated over a massive parallel architecture and used to providea cumulative sampling of local solution space which results in rapid and robustlearning. We introduce a complimentary convolutional bootstrapping approachthat enhances performance of the parallel architecture further. Ourparallelized convolutional bootstrapping DNN out-performs an identicalfully-trained traditional DNN after only a single iteration of training.
arxiv-10500-37 | Learning Program Embeddings to Propagate Feedback on Student Code | http://arxiv.org/pdf/1505.05969v1.pdf | author:Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran Sahami, Leonidas Guibas category:cs.LG cs.NE cs.SE published:2015-05-22 summary:Providing feedback, both assessing final work and giving hints to stuckstudents, is difficult for open-ended assignments in massive online classeswhich can range from thousands to millions of students. We introduce a neuralnetwork method to encode programs as a linear mapping from an embeddedprecondition space to an embedded postcondition space and propose an algorithmfor feedback at scale using these linear maps as features. We apply ouralgorithm to assessments from the Code.org Hour of Code and StanfordUniversity's CS1 course, where we propagate human comments on studentassignments to orders of magnitude more submissions.
arxiv-10500-38 | Distributed Gaussian Processes | http://arxiv.org/pdf/1502.02843v3.pdf | author:Marc Peter Deisenroth, Jun Wei Ng category:stat.ML published:2015-02-10 summary:To scale Gaussian processes (GPs) to large data sets we introduce the robustBayesian Committee Machine (rBCM), a practical and scalable product-of-expertsmodel for large-scale distributed GP regression. Unlike state-of-the-art sparseGP approximations, the rBCM is conceptually simple and does not rely oninducing or variational parameters. The key idea is to recursively distributecomputations to independent computational units and, subsequently, recombinethem to form an overall result. Efficient closed-form inference allows forstraightforward parallelisation and distributed computations with a smallmemory footprint. The rBCM is independent of the computational graph and can beused on heterogeneous computing infrastructures, ranging from laptops toclusters. With sufficient computing resources our distributed GP model canhandle arbitrarily large data sets.
arxiv-10500-39 | Joint Inference of Groups, Events and Human Roles in Aerial Videos | http://arxiv.org/pdf/1505.05957v1.pdf | author:Tianmin Shu, Dan Xie, Brandon Rothrock, Sinisa Todorovic, Song-Chun Zhu category:cs.CV published:2015-05-22 summary:With the advent of drones, aerial video analysis becomes increasinglyimportant; yet, it has received scant attention in the literature. This paperaddresses a new problem of parsing low-resolution aerial videos of largespatial areas, in terms of 1) grouping, 2) recognizing events and 3) assigningroles to people engaged in events. We propose a novel framework aimed atconducting joint inference of the above tasks, as reasoning about each inisolation typically fails in our setting. Given noisy tracklets of people anddetections of large objects and scene surfaces (e.g., building, grass), we usea spatiotemporal AND-OR graph to drive our joint inference, using Markov ChainMonte Carlo and dynamic programming. We also introduce a new formalism ofspatiotemporal templates characterizing latent sub-events. For evaluation, wehave collected and released a new aerial videos dataset using a hex-rotorflying over picnic areas rich with group events. Our results demonstrate thatwe successfully address above inference tasks under challenging conditions.
arxiv-10500-40 | Quantum Deep Learning | http://arxiv.org/pdf/1412.3489v2.pdf | author:Nathan Wiebe, Ashish Kapoor, Krysta M. Svore category:quant-ph cs.LG cs.NE published:2014-12-10 summary:In recent years, deep learning has had a profound impact on machine learningand artificial intelligence. At the same time, algorithms for quantum computershave been shown to efficiently solve some problems that are intractable onconventional, classical computers. We show that quantum computing not onlyreduces the time required to train a deep restricted Boltzmann machine, butalso provides a richer and more comprehensive framework for deep learning thanclassical computing and leads to significant improvements in the optimizationof the underlying objective function. Our quantum methods also permit efficienttraining of full Boltzmann machines and multi-layer, fully connected models anddo not have well known classical counterparts.
arxiv-10500-41 | Rendering of Eyes for Eye-Shape Registration and Gaze Estimation | http://arxiv.org/pdf/1505.05916v1.pdf | author:Erroll Wood, Tadas Baltrusaitis, Xucong Zhang, Yusuke Sugano, Peter Robinson, Andreas Bulling category:cs.CV published:2015-05-21 summary:Images of the eye are key in several computer vision problems, such as shaperegistration and gaze estimation. Recent large-scale supervised methods forthese problems require time-consuming data collection and manual annotation,which can be unreliable. We propose synthesizing perfectly labelledphoto-realistic training data in a fraction of the time. We used computergraphics techniques to build a collection of dynamic eye-region models fromhead scan geometry. These were randomly posed to synthesize close-up eye imagesfor a wide range of head poses, gaze directions, and illumination conditions.We used our model's controllability to verify the importance of realisticillumination and shape variations in eye-region training data. Finally, wedemonstrate the benefits of our synthesized training data (SynthesEyes) byout-performing state-of-the-art methods for eye-shape registration as well ascross-dataset appearance-based gaze estimation in the wild.
arxiv-10500-42 | Qualitatively characterizing neural network optimization problems | http://arxiv.org/pdf/1412.6544v6.pdf | author:Ian J. Goodfellow, Oriol Vinyals, Andrew M. Saxe category:cs.NE cs.LG stat.ML published:2014-12-19 summary:Training neural networks involves solving large-scale non-convex optimizationproblems. This task has long been believed to be extremely difficult, with fearof local minima and other obstacles motivating a variety of schemes to improveoptimization, such as unsupervised pretraining. However, modern neural networksare able to achieve negligible training error on complex tasks, using onlydirect training with stochastic gradient descent. We introduce a simpleanalysis technique to look for evidence that such networks are overcoming localoptima. We find that, in fact, on a straight path from initialization tosolution, a variety of state of the art neural networks never encounter anysignificant obstacles.
arxiv-10500-43 | The IBM 2015 English Conversational Telephone Speech Recognition System | http://arxiv.org/pdf/1505.05899v1.pdf | author:George Saon, Hong-Kwang J. Kuo, Steven Rennie, Michael Picheny category:cs.CL published:2015-05-21 summary:We describe the latest improvements to the IBM English conversationaltelephone speech recognition system. Some of the techniques that were foundbeneficial are: maxout networks with annealed dropout rates; networks with avery large number of outputs trained on 2000 hours of data; joint modeling ofpartially unfolded recurrent neural networks and convolutional nets bycombining the bottleneck and output layers and retraining the resulting model;and lastly, sophisticated language model rescoring with exponential and neuralnetwork LMs. These techniques result in an 8.0% word error rate on theSwitchboard part of the Hub5-2000 evaluation test set which is 23% relativebetter than our previous best published result.
arxiv-10500-44 | Translation Memory Retrieval Methods | http://arxiv.org/pdf/1505.05841v1.pdf | author:Michael Bloodgood, Benjamin Strauss category:cs.CL I.2.7 published:2015-05-21 summary:Translation Memory (TM) systems are one of the most widely used translationtechnologies. An important part of TM systems is the matching algorithm thatdetermines what translations get retrieved from the bank of availabletranslations to assist the human translator. Although detailed accounts of thematching algorithms used in commercial systems can't be found in theliterature, it is widely believed that edit distance algorithms are used. Thispaper investigates and evaluates the use of several matching algorithms,including the edit distance algorithm that is believed to be at the heart ofmost modern commercial TM systems. This paper presents results showing how wellvarious matching algorithms correlate with human judgments of helpfulness(collected via crowdsourcing with Amazon's Mechanical Turk). A new algorithmbased on weighted n-gram precision that can be adjusted for translator lengthpreferences consistently returns translations judged to be most helpful bytranslators for multiple domains and language pairs.
arxiv-10500-45 | Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret | http://arxiv.org/pdf/1505.05798v1.pdf | author:Haitham Bou Ammar, Rasul Tutunov, Eric Eaton category:cs.LG published:2015-05-21 summary:Lifelong reinforcement learning provides a promising framework for developingversatile agents that can accumulate knowledge over a lifetime of experienceand rapidly learn new tasks by building upon prior knowledge. However, currentlifelong learning methods exhibit non-vanishing regret as the amount ofexperience increases and include limitations that can lead to suboptimal orunsafe control policies. To address these issues, we develop a lifelong policygradient learner that operates in an adversarial set- ting to learn multipletasks online while enforcing safety constraints on the learned policies. Wedemonstrate, for the first time, sublinear regret for lifelong policy search,and validate our algorithm on several benchmark dynamical systems and anapplication to quadrotor control.
arxiv-10500-46 | On distinguishability criteria for estimating generative models | http://arxiv.org/pdf/1412.6515v4.pdf | author:Ian J. Goodfellow category:stat.ML published:2014-12-19 summary:Two recently introduced criteria for estimation of generative models are bothbased on a reduction to binary classification. Noise-contrastive estimation(NCE) is an estimation procedure in which a generative model is trained to beable to distinguish data samples from noise samples. Generative adversarialnetworks (GANs) are pairs of generator and discriminator networks, with thegenerator network learning to generate samples by attempting to fool thediscriminator network into believing its samples are real data. Both estimationprocedures use the same function to drive learning, which naturally raisesquestions about how they are related to each other, as well as whether thisfunction is related to maximum likelihood estimation (MLE). NCE corresponds totraining an internal data model belonging to the {\em discriminator} networkbut using a fixed generator network. We show that a variant of NCE, with adynamic generator network, is equivalent to maximum likelihood estimation.Since pairing a learned discriminator with an appropriate dynamically selectedgenerator recovers MLE, one might expect the reverse to hold for pairing alearned generator with a certain discriminator. However, we show thatrecovering MLE for a learned generator requires departing from thedistinguishability game. Specifically: (i) The expected gradient of the NCE discriminator can be made to match theexpected gradient of MLE, if one is allowed to use a non-stationary noise distribution for NCE, (ii) No choice of discriminator network can make the expected gradient forthe GAN generator match that of MLE, and (iii) The existing theory does not guarantee that GANs will converge in thenon-convex case. This suggests that the key next step in GAN research is to determine whetherGANs converge, and if not, to modify their training algorithm to forceconvergence.
arxiv-10500-47 | Watch and Learn: Semi-Supervised Learning of Object Detectors from Videos | http://arxiv.org/pdf/1505.05769v1.pdf | author:Ishan Misra, Abhinav Shrivastava, Martial Hebert category:cs.CV published:2015-05-21 summary:We present a semi-supervised approach that localizes multiple unknown objectinstances in long videos. We start with a handful of labeled boxes anditeratively learn and label hundreds of thousands of object instances. Wepropose criteria for reliable object detection and tracking for constrainingthe semi-supervised learning process and minimizing semantic drift. Ourapproach does not assume exhaustive labeling of each object instance in anysingle frame, or any explicit annotation of negative data. Working in such ageneric setting allow us to tackle multiple object instances in video, many ofwhich are static. In contrast, existing approaches either do not considermultiple object instances per video, or rely heavily on the motion of theobjects present. The experiments demonstrate the effectiveness of our approachby evaluating the automatically labeled data on a variety of metrics likequality, coverage (recall), diversity, and relevance to training an objectdetector.
arxiv-10500-48 | GazeDPM: Early Integration of Gaze Information in Deformable Part Models | http://arxiv.org/pdf/1505.05753v1.pdf | author:Iaroslav Shcherbatyi, Andreas Bulling, Mario Fritz category:cs.CV cs.HC published:2015-05-21 summary:An increasing number of works explore collaborative human-computer systems inwhich human gaze is used to enhance computer vision systems. For objectdetection these efforts were so far restricted to late integration approachesthat have inherent limitations, such as increased precision without increase inrecall. We propose an early integration approach in a deformable part model,which constitutes a joint formulation over gaze and visual data. We show thatour GazeDPM method improves over the state-of-the-art DPM baseline by 4% and arecent method for gaze-supported object detection by 3% on the public POETdataset. Our approach additionally provides introspection of the learnt models,can reveal salient image structures, and allows us to investigate the interplaybetween gaze attracting and repelling areas, the importance of view-specificmodels, as well as viewers' personal biases in gaze patterns. We finally studyimportant practical aspects of our approach, such as the impact of usingsaliency maps instead of real fixations, the impact of the number of fixations,as well as robustness to gaze estimation error.
arxiv-10500-49 | Weight Uncertainty in Neural Networks | http://arxiv.org/pdf/1505.05424v2.pdf | author:Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra category:stat.ML cs.LG published:2015-05-20 summary:We introduce a new, efficient, principled and backpropagation-compatiblealgorithm for learning a probability distribution on the weights of a neuralnetwork, called Bayes by Backprop. It regularises the weights by minimising acompression cost, known as the variational free energy or the expected lowerbound on the marginal likelihood. We show that this principled kind ofregularisation yields comparable performance to dropout on MNISTclassification. We then demonstrate how the learnt uncertainty in the weightscan be used to improve generalisation in non-linear regression problems, andhow this weight uncertainty can be used to drive the exploration-exploitationtrade-off in reinforcement learning.
arxiv-10500-50 | Symmetric low-rank representation for subspace clustering | http://arxiv.org/pdf/1410.8618v2.pdf | author:Jie Chen, Haixian Zhang, Hua Mao, Yongsheng Sang, Zhang Yi category:cs.CV published:2014-10-31 summary:We propose a symmetric low-rank representation (SLRR) method for subspaceclustering, which assumes that a data set is approximately drawn from the unionof multiple subspaces. The proposed technique can reveal the membership ofmultiple subspaces through the self-expressiveness property of the data. Inparticular, the SLRR method considers a collaborative representation combinedwith low-rank matrix recovery techniques as a low-rank representation to learna symmetric low-rank representation, which preserves the subspace structures ofhigh-dimensional data. In contrast to performing iterative singular valuedecomposition in some existing low-rank representation based algorithms, thesymmetric low-rank representation in the SLRR method can be calculated as aclosed form solution by solving the symmetric low-rank optimization problem. Bymaking use of the angular information of the principal directions of thesymmetric low-rank representation, an affinity graph matrix is constructed forspectral clustering. Extensive experimental results show that it outperformsstate-of-the-art subspace clustering algorithms.
arxiv-10500-51 | Graph edit distance : a new binary linear programming formulation | http://arxiv.org/pdf/1505.05740v1.pdf | author:Julien Lerouge, Zeina Abu-Aisheh, Romain Raveaux, Pierre Héroux, Sébastien Adam category:cs.DS cs.CV published:2015-05-21 summary:Graph edit distance (GED) is a powerful and flexible graph matching paradigmthat can be used to address different tasks in structural pattern recognition,machine learning, and data mining. In this paper, some new binary linearprogramming formulations for computing the exact GED between two graphs areproposed. A major strength of the formulations lies in their genericity sincethe GED can be computed between directed or undirected fully attributed graphs(i.e. with attributes on both vertices and edges). Moreover, a relaxation ofthe domain constraints in the formulations provides efficient lower boundapproximations of the GED. A complete experimental study comparing the proposedformulations with 4 state-of-the-art algorithms for exact and approximate graphedit distances is provided. By considering both the quality of the proposedsolution and the efficiency of the algorithms as performance criteria, theresults show that none of the compared methods dominates the others in thePareto sense. As a consequence, faced to a given real-world problem, atrade-off between quality and efficiency has to be chosen w.r.t. theapplication constraints. In this context, this paper provides a guide that canbe used to choose the appropriate method.
arxiv-10500-52 | On the relation between accuracy and fairness in binary classification | http://arxiv.org/pdf/1505.05723v1.pdf | author:Indre Zliobaite category:cs.LG cs.AI published:2015-05-21 summary:Our study revisits the problem of accuracy-fairness tradeoff in binaryclassification. We argue that comparison of non-discriminatory classifiersneeds to account for different rates of positive predictions, otherwiseconclusions about performance may be misleading, because accuracy anddiscrimination of naive baselines on the same dataset vary with different ratesof positive predictions. We provide methodological recommendations for soundcomparison of non-discriminatory classifiers, and present a brief theoreticaland empirical analysis of tradeoffs between accuracy and non-discrimination.
arxiv-10500-53 | A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network | http://arxiv.org/pdf/1505.05667v1.pdf | author:Chenxi Zhu, Xipeng Qiu, Xinchi Chen, Xuanjing Huang category:cs.CL cs.LG cs.NE published:2015-05-21 summary:In this work, we address the problem to model all the nodes (words orphrases) in a dependency tree with the dense representations. We propose arecursive convolutional neural network (RCNN) architecture to capture syntacticand compositional-semantic representations of phrases and words in a dependencytree. Different with the original recursive neural network, we introduce theconvolution and pooling layers, which can model a variety of compositions bythe feature maps and choose the most informative compositions by the poolinglayers. Based on RCNN, we use a discriminative model to re-rank a $k$-best listof candidate dependency parsing trees. The experiments show that RCNN is veryeffective to improve the state-of-the-art dependency parsing on both Englishand Chinese datasets.
arxiv-10500-54 | Inferring Graphs from Cascades: A Sparse Recovery Framework | http://arxiv.org/pdf/1505.05663v1.pdf | author:Jean Pouget-Abadie, Thibaut Horel category:cs.SI cs.LG stat.ML published:2015-05-21 summary:In the Network Inference problem, one seeks to recover the edges of anunknown graph from the observations of cascades propagating over this graph. Inthis paper, we approach this problem from the sparse recovery perspective. Weintroduce a general model of cascades, including the voter model and theindependent cascade model, for which we provide the first algorithm whichrecovers the graph's edges with high probability and $O(s\log m)$ measurementswhere $s$ is the maximum degree of the graph and $m$ is the number of nodes.Furthermore, we show that our algorithm also recovers the edge weights (theparameters of the diffusion process) and is robust in the context ofapproximate sparsity. Finally we prove an almost matching lower bound of$\Omega(s\log\frac{m}{s})$ and validate our approach empirically on syntheticgraphs.
arxiv-10500-55 | Object Modelling with a Handheld RGB-D Camera | http://arxiv.org/pdf/1505.05643v1.pdf | author:Aitor Aldoma, Johann Prankl, Alexander Svejda, Markus Vincze category:cs.CV published:2015-05-21 summary:This work presents a flexible system to reconstruct 3D models of objectscaptured with an RGB-D sensor. A major advantage of the method is that ourreconstruction pipeline allows the user to acquire a full 3D model of theobject. This is achieved by acquiring several partial 3D models in differentsessions that are automatically merged together to reconstruct a full model. Inaddition, the 3D models acquired by our system can be directly used bystate-of-the-art object instance recognition and object tracking modules,providing object-perception capabilities for different applications, such ashuman-object interaction analysis or robot grasping. The system does not imposeconstraints in the appearance of objects (textured, untextured) nor in themodelling setup (moving camera with static object or a turn-table setup). Theproposed reconstruction system has been used to model a large number of objectsresulting in metrically accurate and visually appealing 3D models.
arxiv-10500-56 | Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views | http://arxiv.org/pdf/1505.05641v1.pdf | author:Hao Su, Charles R. Qi, Yangyan Li, Leonidas Guibas category:cs.CV published:2015-05-21 summary:Object viewpoint estimation from 2D images is an essential task in computervision. However, two issues hinder its progress: scarcity of training data withviewpoint annotations, and a lack of powerful features. Inspired by the growingavailability of 3D models, we propose a framework to address both issues bycombining render-based image synthesis and CNNs. We believe that 3D models havethe potential in generating a large number of images of high variation, whichcan be well exploited by deep CNN with a high learning capacity. Towards thisgoal, we propose a scalable and overfit-resistant image synthesis pipeline,together with a novel CNN specifically tailored for the viewpoint estimationtask. Experimentally, we show that the viewpoint estimation from our pipelinecan significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark.
arxiv-10500-57 | Unsupervised Segmentation of Overlapping Cervical Cell Cytoplasm | http://arxiv.org/pdf/1505.05601v1.pdf | author:S L Happy, Swarnadip Chatterjee, Debdoot Sheet category:cs.CV published:2015-05-21 summary:Overlapping of cervical cells and poor contrast of cell cytoplasm are themajor issues in accurate detection and segmentation of cervical cells. Anunsupervised cell segmentation approach is presented here. Cell clumpsegmentation was carried out using the extended depth of field (EDF) imagecreated from the images of different focal planes. A modified Otsu method withprior class weights is proposed for accurate segmentation of nuclei from thecell clumps. The cell cytoplasm was further segmented from cell clump dependingupon the number of nucleus detected in that cell clump. Level set model wasused for cytoplasm segmentation.
arxiv-10500-58 | The development of an information criterion for Change-Point Analysis | http://arxiv.org/pdf/1505.05572v1.pdf | author:Paul A. Wiggins, Colin H. LaMont category:cs.LG stat.ML published:2015-05-21 summary:Change-point analysis is a flexible and computationally tractable tool forthe analysis of times series data from systems that transition between discretestates and whose observables are corrupted by noise. The change-point algorithmis used to identify the time indices (change points) at which the systemtransitions between these discrete states. We present a unifiedinformation-based approach to testing for the existence of change points. Thisnew approach reconciles two previously disparate approaches to Change-PointAnalysis (frequentist and information-based) for testing transitions betweenstates. The resulting method is statistically principled, parameter and priorfree and widely applicable to a wide range of change-point problems.
arxiv-10500-59 | Counterfactual Risk Minimization: Learning from Logged Bandit Feedback | http://arxiv.org/pdf/1502.02362v2.pdf | author:Adith Swaminathan, Thorsten Joachims category:cs.LG stat.ML published:2015-02-09 summary:We develop a learning principle and an efficient algorithm for batch learningfrom logged bandit feedback. This learning setting is ubiquitous in onlinesystems (e.g., ad placement, web search, recommendation), where an algorithmmakes a prediction (e.g., ad ranking) for a given input (e.g., query) andobserves bandit feedback (e.g., user clicks on presented ads). We first addressthe counterfactual nature of the learning problem through propensity scoring.Next, we prove generalization error bounds that account for the variance of thepropensity-weighted empirical risk estimator. These constructive bounds giverise to the Counterfactual Risk Minimization (CRM) principle. We show how CRMcan be used to derive a new learning method -- called Policy Optimizer forExponential Models (POEM) -- for learning stochastic linear rules forstructured output prediction. We present a decomposition of the POEM objectivethat enables efficient stochastic gradient optimization. POEM is evaluated onseveral multi-label classification problems showing substantially improvedrobustness and generalization performance compared to the state-of-the-art.
arxiv-10500-60 | Harmonic Exponential Families on Manifolds | http://arxiv.org/pdf/1505.04413v2.pdf | author:Taco S. Cohen, Max Welling category:stat.ML published:2015-05-17 summary:In a range of fields including the geosciences, molecular biology, roboticsand computer vision, one encounters problems that involve random variables onmanifolds. Currently, there is a lack of flexible probabilistic models onmanifolds that are fast and easy to train. We define an extremely flexibleclass of exponential family distributions on manifolds such as the torus,sphere, and rotation groups, and show that for these distributions the gradientof the log-likelihood can be computed efficiently using a non-commutativegeneralization of the Fast Fourier Transform (FFT). We discuss applications toBayesian camera motion estimation (where harmonic exponential families serve asconjugate priors), and modelling of the spatial distribution of earthquakes onthe surface of the earth. Our experimental results show that harmonic densitiesyield a significantly higher likelihood than the best competing method, whilebeing orders of magnitude faster to train.
arxiv-10500-61 | TurkerGaze: Crowdsourcing Saliency with Webcam based Eye Tracking | http://arxiv.org/pdf/1504.06755v2.pdf | author:Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R. Kulkarni, Jianxiong Xiao category:cs.CV published:2015-04-25 summary:Traditional eye tracking requires specialized hardware, which meanscollecting gaze data from many observers is expensive, tedious and slow.Therefore, existing saliency prediction datasets are order-of-magnitudessmaller than typical datasets for other vision recognition tasks. The smallsize of these datasets limits the potential for training data intensivealgorithms, and causes overfitting in benchmark evaluation. To address thisdeficiency, this paper introduces a webcam-based gaze tracking system thatsupports large-scale, crowdsourced eye tracking deployed on Amazon MechanicalTurk (AMTurk). By a combination of careful algorithm and gaming protocoldesign, our system obtains eye tracking data for saliency prediction comparableto data gathered in a traditional lab setting, with relatively lower cost andless effort on the part of the researchers. Using this tool, we build asaliency dataset for a large number of natural images. We will open-source ourtool and provide a web server where researchers can upload their images to geteye tracking results from AMTurk.
arxiv-10500-62 | Kinect Range Sensing: Structured-Light versus Time-of-Flight Kinect | http://arxiv.org/pdf/1505.05459v1.pdf | author:Hamed Sarbolandi, Damien Lefloch, Andreas Kolb category:cs.CV published:2015-05-20 summary:Recently, the new Kinect One has been issued by Microsoft, providing the nextgeneration of real-time range sensing devices based on the Time-of-Flight (ToF)principle. As the first Kinect version was using a structured light approach,one would expect various differences in the characteristics of the range datadelivered by both devices. This paper presents a detailed and in-depthcomparison between both devices. In order to conduct the comparison, we proposea framework of seven different experimental setups, which is a generic basisfor evaluating range cameras such as Kinect. The experiments have been designedwith the goal to capture individual effects of the Kinect devices as isolatedlyas possible and in a way, that they can also be adopted, in order to apply themto any other range sensing device. The overall goal of this paper is to providea solid insight into the pros and cons of either device. Thus, scientists thatare interested in using Kinect range sensing cameras in their specificapplication scenario can directly assess the expected, specific benefits andpotential problem of either device.
arxiv-10500-63 | Fuzzy Least Squares Twin Support Vector Machines | http://arxiv.org/pdf/1505.05451v1.pdf | author:Javad Salimi Sartakhti, Nasser Ghadiri, Homayun Afrabandpey category:cs.AI cs.LG published:2015-05-20 summary:Least Squares Twin Support Vector Machine (LSTSVM) is an extremely efficientand fast version of SVM algorithm for binary classification. LSTSVM combinesthe idea of Least Squares SVM and Twin SVM in which two non-parallelhyperplanes are found by solving two systems of linear equations. Although, thealgorithm is very fast and efficient in many classification tasks, it is unableto cope with two features of real-world problems. First, in many real-worldclassification problems, it is almost impossible to assign data points to asingle class. Second, data points in real-world problems may have differentimportance. In this study, we propose a novel version of LSTSVM based on fuzzyconcepts to deal with these two characteristics of real-world data. Thealgorithm is called Fuzzy LSTSVM (FLSTSVM) which provides more flexibility thanbinary classification of LSTSVM. Two models are proposed for the algorithm. Inthe first model, a fuzzy membership value is assigned to each data point andthe hyperplanes are optimized based on these fuzzy samples. In the second modelwe construct fuzzy hyperplanes to classify data. Finally, we apply our proposedFLSTSVM to an artificial as well as three real-world datasets. Resultsdemonstrate that FLSTSVM obtains better performance than SVM and LSTSVM.
arxiv-10500-64 | DRAW: A Recurrent Neural Network For Image Generation | http://arxiv.org/pdf/1502.04623v2.pdf | author:Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra category:cs.CV cs.LG cs.NE published:2015-02-16 summary:This paper introduces the Deep Recurrent Attentive Writer (DRAW) neuralnetwork architecture for image generation. DRAW networks combine a novelspatial attention mechanism that mimics the foveation of the human eye, with asequential variational auto-encoding framework that allows for the iterativeconstruction of complex images. The system substantially improves on the stateof the art for generative models on MNIST, and, when trained on the Street ViewHouse Numbers dataset, it generates images that cannot be distinguished fromreal data with the naked eye.
arxiv-10500-65 | DropSample: A New Training Method to Enhance Deep Convolutional Neural Networks for Large-Scale Unconstrained Handwritten Chinese Character Recognition | http://arxiv.org/pdf/1505.05354v1.pdf | author:Weixin Yang, Lianwen Jin, Dacheng Tao, Zecheng Xie, Ziyong Feng category:cs.CV published:2015-05-20 summary:Inspired by the theory of Leitners learning box from the field of psychology,we propose DropSample, a new method for training deep convolutional neuralnetworks (DCNNs), and apply it to large-scale online handwritten Chinesecharacter recognition (HCCR). According to the principle of DropSample, eachtraining sample is associated with a quota function that is dynamicallyadjusted on the basis of the classification confidence given by the DCNNsoftmax output. After a learning iteration, samples with low confidence willhave a higher probability of being selected as training data in the nextiteration; in contrast, well-trained and well-recognized samples with very highconfidence will have a lower probability of being involved in the next trainingiteration and can be gradually eliminated. As a result, the learning processbecomes more efficient as it progresses. Furthermore, we investigate the use ofdomain-specific knowledge to enhance the performance of DCNN by adding a domainknowledge layer before the traditional CNN. By adopting DropSample togetherwith different types of domain-specific knowledge, the accuracy of HCCR can beimproved efficiently. Experiments on the CASIA-OLHDWB 1.0, CASIA-OLHWDB 1.1,and ICDAR 2013 online HCCR competition datasets yield outstanding recognitionrates of 97.33%, 97.06%, and 97.51% respectively, all of which aresignificantly better than the previous best results reported in the literature.
arxiv-10500-66 | Algorithmic Analysis of Edge Ranking and Profiling for MTF Determination of an Imaging System | http://arxiv.org/pdf/1505.05338v1.pdf | author:Poorna Banerjee Dasgupta category:cs.CV published:2015-05-20 summary:Edge detection is one of the most principal techniques for detectingdiscontinuities in the gray levels of image pixels. The Modulation TransferFunction (MTF) is one of the main criteria for assessing imaging quality and isa parameter frequently used for measuring the sharpness of an imaging system.In order to determine the MTF, it is essential to determine the best edge fromthe target image so that an edge profile can be developed and then the linespread function and hence the MTF, can be computed accordingly. For regularimage sizes, the human visual system is adept enough to identify suitable edgesfrom the image. But considering huge image datasets, such as those obtainedfrom satellites, the image size may range in few gigabytes and in such a case,manual inspection of images for determination of the best suitable edge is notplausible and hence, edge profiling tasks have to be automated. This paperpresents a novel, yet simple, algorithm for edge ranking and detection fromimage data-sets for MTF computation, which is ideal for automation onvectorised graphical processing units.
arxiv-10500-67 | Measuring Visibility using Atmospheric Transmission and Digital Surface Model | http://arxiv.org/pdf/1505.05286v1.pdf | author:Jean-Philippe Andreu, Stefan Mayer, Karlheinz Gutjahr, Harald Ganster category:cs.CV published:2015-05-20 summary:Reliable and exact assessment of visibility is essential for safe airtraffic. In order to overcome the drawbacks of the currently subjective reportsfrom human observers, we present an approach to automatically derive visibilitymeasures by means of image processing. It first exploits image based estimationof the atmospheric transmission describing the portion of the light that is notscattered by atmospheric phenomena (e.g., haze, fog, smoke) and reaches thecamera. Once the atmospheric transmission is estimated, a 3D representation ofthe vicinity (digital surface model: DMS) is used to compute depth measurementsfor the haze-free pixels and then derive a global visibility estimation for theairport. Results on foggy images demonstrate the validity of the proposedmethod.
arxiv-10500-68 | Biometric Matching and Fusion System for Fingerprints from Non-Distal Phalanges | http://arxiv.org/pdf/1505.04028v2.pdf | author:Mehmet Kayaoglu, Berkay Topcu, Umut Uludag category:cs.CV published:2015-05-15 summary:Market research indicates that fingerprints are still the most popularbiometric modality for personal authentication. Even with the onset of newmodalities (e.g. vein matching), many applications within different domains(e-ID, banking, border control...) and geographies rely on fingerprintsobtained from the distal phalanges (a.k.a. sections, digits) of the human handstructure. Motivated by the problem of poor quality distal fingerprint imagesaffecting a non-trivial portion of the population (which decreases associatedauthentication accuracy), we designed and tested a multifinger, multiphalanxfusion scheme, that combines minutiae matching scores originating fromnon-distal (ie. middle and proximal) phalanges based on (i) simple sum fusion,(ii) NFIQ image-quality-based fusion, and (iii) phalanx-type-based fusion.Utilizing a medium-size (50 individuals, 400 unique fingers, 1600 distinctimages) database collected in our laboratory with a commercial opticalfingerprint sensor, and a commercial minutiae extractor & matcher (without anymodification), allowed us to simulate a real-world fingerprint authenticationsetting. Detailed analyses including ROC curves with statistical confidenceintervals show that the proposed system can be a viable alternative for caseswhere (i) distal phalanx images are not usable (e.g. due to missing digits, orlow quality finger surface due to manual labor), and (ii) switching to a newbiometric modality (e.g. iris) is not possible due to economical orinfrastructure limits. Further, we show that when distal phalanx images are infact usable, combining them with images from other phalanges increases accuracyas well.
arxiv-10500-69 | Kernel-Based Adaptive Online Reconstruction of Coverage Maps With Side Information | http://arxiv.org/pdf/1404.0979v3.pdf | author:Martin Kasparick, Renato L. G. Cavalcante, Stefan Valentin, Slawomir Stanczak, Masahiro Yukawa category:cs.NI cs.LG stat.ML published:2014-04-03 summary:In this paper, we address the problem of reconstructing coverage maps frompath-loss measurements in cellular networks. We propose and evaluate twokernel-based adaptive online algorithms as an alternative to typical offlinemethods. The proposed algorithms are application-tailored extensions ofpowerful iterative methods such as the adaptive projected subgradient methodand a state-of-the-art adaptive multikernel method. Assuming that the movingtrajectories of users are available, it is shown how side information can beincorporated in the algorithms to improve their convergence performance and thequality of the estimation. The complexity is significantly reduced by imposingsparsity-awareness in the sense that the algorithms exploit the compressibilityof the measurement data to reduce the amount of data which is saved andprocessed. Finally, we present extensive simulations based on realistic data toshow that our algorithms provide fast, robust estimates of coverage maps inreal-world scenarios. Envisioned applications include path-loss predictionalong trajectories of mobile users as a building block for anticipatorybuffering or traffic offloading.
arxiv-10500-70 | Live Video Synopsis for Multiple Cameras | http://arxiv.org/pdf/1505.05254v1.pdf | author:Yedid Hoshen, Shmuel Peleg category:cs.CV published:2015-05-20 summary:Video surveillance cameras generate most of recorded video, and there is farmore recorded video than operators can watch. Much progress has recently beenmade using summarization of recorded video, but such techniques do not havemuch impact on live video surveillance. We assume a camera hierarchy where a Master camera observes thedecision-critical region, and one or more Slave cameras observe regions wherepast activity is important for making the current decision. We propose thatwhen people appear in the live Master camera, the Slave cameras will displaytheir past activities, and the operator could use past information forreal-time decision making. The basic units of our method are action tubes, representing objects andtheir trajectories over time. Our object-based method has advantages over framebased methods, as it can handle multiple people, multiple activities for eachperson, and can address re-identification uncertainty.
arxiv-10500-71 | Big Data Small Data, In Domain Out-of Domain, Known Word Unknown Word: The Impact of Word Representation on Sequence Labelling Tasks | http://arxiv.org/pdf/1504.05319v2.pdf | author:Lizhen Qu, Gabriela Ferraro, Liyuan Zhou, Weiwei Hou, Nathan Schneider, Timothy Baldwin category:cs.CL published:2015-04-21 summary:Word embeddings -- distributed word representations that can be learned fromunlabelled data -- have been shown to have high utility in many naturallanguage processing applications. In this paper, we perform an extrinsicevaluation of five popular word embedding methods in the context of foursequence labelling tasks: POS-tagging, syntactic chunking, NER and MWEidentification. A particular focus of the paper is analysing the effects oftask-based updating of word representations. We show that when using wordembeddings as features, as few as several hundred training instances aresufficient to achieve competitive results, and that word embeddings lead toimprovements over OOV words and out of domain. Perhaps more surprisingly, ourresults indicate there is little difference between the different wordembedding methods, and that simple Brown clusters are often competitive withword embeddings across all tasks we consider.
arxiv-10500-72 | Learning to Search Better Than Your Teacher | http://arxiv.org/pdf/1502.02206v2.pdf | author:Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daumé III, John Langford category:cs.LG stat.ML published:2015-02-08 summary:Methods for learning to search for structured prediction typically imitate areference policy, with existing theoretical guarantees demonstrating low regretcompared to that reference. This is unsatisfactory in many applications wherethe reference policy is suboptimal and the goal of learning is to improve uponit. Can learning to search work even when the reference is poor? We provide a new learning to search algorithm, LOLS, which does well relativeto the reference policy, but additionally guarantees low regret compared todeviations from the learned policy: a local-optimality guarantee. Consequently,LOLS can improve upon the reference policy, unlike previous algorithms. Thisenables us to develop structured contextual bandits, a partial informationstructured prediction setting with many potential applications.
arxiv-10500-73 | Benchmarking KAZE and MCM for Multiclass Classification | http://arxiv.org/pdf/1505.05240v1.pdf | author:Siddharth Srivastava, Prerana Mukherjee, Brejesh Lall category:cs.CV cs.IR published:2015-05-20 summary:In this paper, we propose a novel approach for feature generation byappropriately fusing KAZE and SIFT features. We then use this feature set alongwith Minimal Complexity Machine(MCM) for object classification. We show thatKAZE and SIFT features are complementary. Experimental results indicate that anelementary integration of these techniques can outperform the state-of-the-artapproaches.
arxiv-10500-74 | Multi-scale recognition with DAG-CNNs | http://arxiv.org/pdf/1505.05232v1.pdf | author:Songfan Yang, Deva Ramanan category:cs.CV published:2015-05-20 summary:We explore multi-scale convolutional neural nets (CNNs) for imageclassification. Contemporary approaches extract features from a single outputlayer. By extracting features from multiple layers, one can simultaneouslyreason about high, mid, and low-level features during classification. Theresulting multi-scale architecture can itself be seen as a feed-forward modelthat is structured as a directed acyclic graph (DAG-CNNs). We use DAG-CNNs tolearn a set of multiscale features that can be effectively shared betweencoarse and fine-grained classification tasks. While fine-tuning such modelshelps performance, we show that even "off-the-self" multiscale features performquite well. We present extensive analysis and demonstrate state-of-the-artclassification performance on three standard scene benchmarks (SUN397, MIT67,and Scene15). In terms of the heavily benchmarked MIT67 and Scene15 datasets,our results reduce the lowest previously-reported error by 23.9% and 9.5%,respectively.
arxiv-10500-75 | Bounds on the Minimax Rate for Estimating a Prior over a VC Class from Independent Learning Tasks | http://arxiv.org/pdf/1505.05231v1.pdf | author:Liu Yang, Steve Hanneke, Jaime Carbonell category:cs.LG published:2015-05-20 summary:We study the optimal rates of convergence for estimating a prior distributionover a VC class from a sequence of independent data sets respectively labeledby independent target functions sampled from the prior. We specifically deriveupper and lower bounds on the optimal rates under a smoothness condition on thecorrect prior, with the number of samples per data set equal the VC dimension.These results have implications for the improvements achievable via transferlearning. We additionally extend this setting to real-valued function, where weestablish consistency of an estimator for the prior, and discuss an additionalapplication to a preference elicitation problem in algorithmic economics.
arxiv-10500-76 | Variable subset selection via GA and information complexity in mixtures of Poisson and negative binomial regression models | http://arxiv.org/pdf/1505.05229v1.pdf | author:T. J. Massaro, H. Bozdogan category:stat.ML stat.ME published:2015-05-20 summary:Count data, for example the number of observed cases of a disease in a city,often arise in the fields of healthcare analytics and epidemiology. In thispaper, we consider performing regression on multivariate data in which ouroutcome is a count. Specifically, we derive log-likelihood functions for finitemixtures of regression models involving counts that come from a Poissondistribution, as well as a negative binomial distribution when the counts aresignificantly overdispersed. Within our proposed modeling framework, we carryout optimal component selection using the information criteria scores AIC, BIC,CAIC, and ICOMP. We demonstrate applications of our approach on simulated data,as well as on a real data set of HIV cases in Tennessee counties from the year2010. Finally, using a genetic algorithm within our framework, we performvariable subset selection to determine the covariates that are most responsiblefor categorizing Tennessee counties. This leads to some interesting insightsinto the traits of counties that have high HIV counts.
arxiv-10500-77 | Image aesthetic evaluation using paralleled deep convolution neural network | http://arxiv.org/pdf/1505.05225v1.pdf | author:Guo Lihua, Li Fudi category:cs.CV cs.MM 68U10 I.4.7 published:2015-05-20 summary:Image aesthetic evaluation has attracted much attention in recent years.Image aesthetic evaluation methods heavily depend on the effective aestheticfeature. Traditional meth-ods always extract hand-crafted features. However,these hand-crafted features are always designed to adapt particu-lar datasets,and extraction of them needs special design. Rather than extractinghand-crafted features, an automati-cally learn of aesthetic features based ondeep convolutional neural network (DCNN) is first adopt in this paper. As weall know, when the training dataset is given, the DCNN architecture with highcomplexity may meet the over-fitting problem. On the other side, the DCNNarchitecture with low complexity would not efficiently extract effectivefeatures. For these reasons, we further propose a paralleled convolutionalneural network (PDCNN) with multi-level structures to automatically adapt tothe training dataset. Experimental results show that our proposed PDCNNarchitecture achieves better performance than other traditional methods.
arxiv-10500-78 | Convergence Analysis of Policy Iteration | http://arxiv.org/pdf/1505.05216v1.pdf | author:Ali Heydari category:cs.SY math.OC stat.ML published:2015-05-20 summary:Adaptive optimal control of nonlinear dynamic systems with deterministic andknown dynamics under a known undiscounted infinite-horizon cost function isinvestigated. Policy iteration scheme initiated using a stabilizing initialcontrol is analyzed in solving the problem. The convergence of the iterationsand the optimality of the limit functions, which follows from the establisheduniqueness of the solution to the Bellman equation, are the main results ofthis study. Furthermore, a theoretical comparison between the speed ofconvergence of policy iteration versus value iteration is presented. Finally,the convergence results are extended to the case of multi-step look-aheadpolicy iteration.
arxiv-10500-79 | Learning with a Drifting Target Concept | http://arxiv.org/pdf/1505.05215v1.pdf | author:Steve Hanneke, Varun Kanade, Liu Yang category:cs.LG published:2015-05-20 summary:We study the problem of learning in the presence of a drifting targetconcept. Specifically, we provide bounds on the error rate at a given time,given a learner with access to a history of independent samples labeledaccording to a target concept that can change on each round. One of our maincontributions is a refinement of the best previous results for polynomial-timealgorithms for the space of linear separators under a uniform distribution. Wealso provide general results for an algorithm capable of adapting to a variablerate of drift of the target concept. Some of the results also describe anactive learning variant of this setting, and provide bounds on the number ofqueries for the labels of points in the sequence sufficient to obtain thestated bounds on the error rates.
arxiv-10500-80 | Extrinsic Methods for Coding and Dictionary Learning on Grassmann Manifolds | http://arxiv.org/pdf/1401.8126v2.pdf | author:Mehrtash Harandi, Richard Hartley, Chunhua Shen, Brian Lovell, Conrad Sanderson category:cs.LG cs.CV stat.ML published:2014-01-31 summary:Sparsity-based representations have recently led to notable results invarious visual recognition tasks. In a separate line of research, Riemannianmanifolds have been shown useful for dealing with features and models that donot lie in Euclidean spaces. With the aim of building a bridge between the tworealms, we address the problem of sparse coding and dictionary learning overthe space of linear subspaces, which form Riemannian structures known asGrassmann manifolds. To this end, we propose to embed Grassmann manifolds intothe space of symmetric matrices by an isometric mapping. This in turn enablesus to extend two sparse coding schemes to Grassmann manifolds. Furthermore, wepropose closed-form solutions for learning a Grassmann dictionary, atom byatom. Lastly, to handle non-linearity in data, we extend the proposed Grassmannsparse coding and dictionary learning algorithms through embedding into Hilbertspaces. Experiments on several classification tasks (gender recognition, gestureclassification, scene analysis, face recognition, action recognition anddynamic texture classification) show that the proposed approaches achieveconsiderable improvements in discrimination accuracy, in comparison tostate-of-the-art methods such as kernelized Affine Hull Method andgraph-embedding Grassmann discriminant analysis.
arxiv-10500-81 | Barcode Annotations for Medical Image Retrieval: A Preliminary Investigation | http://arxiv.org/pdf/1505.05212v1.pdf | author:Hamid R. Tizhoosh category:cs.CV published:2015-05-19 summary:This paper proposes to generate and to use barcodes to annotate medicalimages and/or their regions of interest such as organs, tumors and tissuetypes. A multitude of efficient feature-based image retrieval methods alreadyexist that can assign a query image to a certain image class. Visualannotations may help to increase the retrieval accuracy if combined withexisting feature-based classification paradigms. Whereas with annotations weusually mean textual descriptions, in this paper barcode annotations areproposed. In particular, Radon barcodes (RBC) are introduced. As well, localbinary patterns (LBP) and local Radon binary patterns (LRBP) are implemented asbarcodes. The IRMA x-ray dataset with 12,677 training images and 1,733 testimages is used to verify how barcodes could facilitate image retrieval.
arxiv-10500-82 | oASIS: Adaptive Column Sampling for Kernel Matrix Approximation | http://arxiv.org/pdf/1505.05208v1.pdf | author:Raajen Patel, Thomas A. Goldstein, Eva L. Dyer, Azalia Mirhoseini, Richard G. Baraniuk category:stat.ML cs.LG G.1.0; G.4 published:2015-05-19 summary:Kernel matrices (e.g. Gram or similarity matrices) are essential for manystate-of-the-art approaches to classification, clustering, and dimensionalityreduction. For large datasets, the cost of forming and factoring such kernelmatrices becomes intractable. To address this challenge, we introduce a newadaptive sampling algorithm called Accelerated Sequential Incoherence Selection(oASIS) that samples columns without explicitly computing the entire kernelmatrix. We provide conditions under which oASIS is guaranteed to exactlyrecover the kernel matrix with an optimal number of columns selected. Numericalexperiments on both synthetic and real-world datasets demonstrate that oASISachieves performance comparable to state-of-the-art adaptive sampling methodsat a fraction of the computational cost. The low runtime complexity of oASISand its low memory footprint enable the solution of large problems that aresimply intractable using other adaptive methods.
arxiv-10500-83 | Image Reconstruction from Bag-of-Visual-Words | http://arxiv.org/pdf/1505.05190v1.pdf | author:Hiroharu Kato, Tatsuya Harada category:cs.CV cs.AI published:2015-05-19 summary:The objective of this work is to reconstruct an original image fromBag-of-Visual-Words (BoVW). Image reconstruction from features can be a meansof identifying the characteristics of features. Additionally, it enables us togenerate novel images via features. Although BoVW is the de facto standardfeature for image recognition and retrieval, successful image reconstructionfrom BoVW has not been reported yet. What complicates this task is that BoVWlacks the spatial information for including visual words. As described in thispaper, to estimate an original arrangement, we propose an evaluation functionthat incorporates the naturalness of local adjacency and the global position,with a method to obtain related parameters using an external image database. Toevaluate the performance of our method, we reconstruct images of objects of 101kinds. Additionally, we apply our method to analyze object classifiers and togenerate novel images via BoVW.
arxiv-10500-84 | Posterior Contraction Rates of the Phylogenetic Indian Buffet Processes | http://arxiv.org/pdf/1307.8229v2.pdf | author:Mengjie Chen, Chao Gao, Hongyu Zhao category:stat.ML math.ST q-bio.QM stat.AP stat.TH published:2013-07-31 summary:By expressing prior distributions as general stochastic processes,nonparametric Bayesian methods provide a flexible way to incorporate priorknowledge and constrain the latent structure in statistical inference. TheIndian buffet process (IBP) is such an example that can be used to define aprior distribution on infinite binary features, where the exchangeability amongsubjects is assumed. The phylogenetic Indian buffet process (pIBP), aderivative of IBP, enables the modeling of non-exchangeability among subjectsthrough a stochastic process on a rooted tree, which is similar to that used inphylogenetics, to describe relationships among the subjects. In this paper, westudy the theoretical properties of IBP and pIBP under a binary factor model.We establish the posterior contraction rates for both IBP and pIBP andsubstantiate the theoretical results through simulation studies. This is thefirst work addressing the frequentist property of the posterior behaviors ofIBP and pIBP. We also demonstrated its practical usefulness by applying pIBPprior to a real data example arising in the field of cancer genomics where theexchangeability among subjects is violated.
arxiv-10500-85 | Non-Gaussian Discriminative Factor Models via the Max-Margin Rank-Likelihood | http://arxiv.org/pdf/1504.07468v3.pdf | author:Xin Yuan, Ricardo Henao, Ephraim L. Tsalik, Raymond J. Langley, Lawrence Carin category:stat.ML published:2015-04-28 summary:We consider the problem of discriminative factor analysis for data that arein general non-Gaussian. A Bayesian model based on the ranks of the data isproposed. We first introduce a new {\em max-margin} version of therank-likelihood. A discriminative factor model is then developed, integratingthe max-margin rank-likelihood and (linear) Bayesian support vector machines,which are also built on the max-margin principle. The discriminative factormodel is further extended to the {\em nonlinear} case through mixtures of locallinear classifiers, via Dirichlet processes. Fully local conjugacy of the modelyields efficient inference with both Markov Chain Monte Carlo and variationalBayes approaches. Extensive experiments on benchmark and real data demonstratesuperior performance of the proposed model and its potential for applicationsin computational biology.
arxiv-10500-86 | Vector-Space Markov Random Fields via Exponential Families | http://arxiv.org/pdf/1505.05117v1.pdf | author:Wesley Tansey, Oscar Hernan Madrid Padilla, Arun Sai Suggala, Pradeep Ravikumar category:stat.ML published:2015-05-19 summary:We present Vector-Space Markov Random Fields (VS-MRFs), a novel class ofundirected graphical models where each variable can belong to an arbitraryvector space. VS-MRFs generalize a recent line of work on scalar-valued,uni-parameter exponential family and mixed graphical models, thereby greatlybroadening the class of exponential families available (e.g., allowingmultinomial and Dirichlet distributions). Specifically, VS-MRFs are the jointgraphical model distributions where the node-conditional distributions belongto generic exponential families with general vector space domains. We alsopresent a sparsistent $M$-estimator for learning our class of MRFs thatrecovers the correct set of edges with high probability. We validate ourapproach via a set of synthetic data experiments as well as a real-world casestudy of over four million foods from the popular diet tracking appMyFitnessPal. Our results demonstrate that our algorithm performs wellempirically and that VS-MRFs are capable of capturing and highlightinginteresting structure in complex, real-world data. All code for our algorithmis open source and publicly available.
arxiv-10500-87 | EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow | http://arxiv.org/pdf/1501.02565v2.pdf | author:Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, Cordelia Schmid category:cs.CV published:2015-01-12 summary:We propose a novel approach for optical flow estimation , targeted at largedisplacements with significant oc-clusions. It consists of two steps: i) densematching by edge-preserving interpolation from a sparse set of matches; ii)variational energy minimization initialized with the dense matches. Thesparse-to-dense interpolation relies on an appropriate choice of the distance,namely an edge-aware geodesic distance. This distance is tailored to handleocclusions and motion boundaries -- two common and difficult issues for opticalflow computation. We also propose an approximation scheme for the geodesicdistance to allow fast computation without loss of performance. Subsequent tothe dense interpolation step, standard one-level variational energyminimization is carried out on the dense matches to obtain the final flowestimation. The proposed approach, called Edge-Preserving Interpolation ofCorrespondences (EpicFlow) is fast and robust to large displacements. Itsignificantly outperforms the state of the art on MPI-Sintel and performs onpar on Kitti and Middlebury.
arxiv-10500-88 | Markov Chain Monte Carlo and Variational Inference: Bridging the Gap | http://arxiv.org/pdf/1410.6460v4.pdf | author:Tim Salimans, Diederik P. Kingma, Max Welling category:stat.CO stat.ML published:2014-10-23 summary:Recent advances in stochastic gradient variational inference have made itpossible to perform variational Bayesian inference with posteriorapproximations containing auxiliary random variables. This enables us toexplore a new synthesis of variational inference and Monte Carlo methods wherewe incorporate one or more steps of MCMC into our variational approximation. Bydoing so we obtain a rich class of inference algorithms bridging the gapbetween variational methods and MCMC, and offering the best of both worlds:fast posterior approximation through the maximization of an explicit objective,with the option of trading off additional computation for additional accuracy.We describe the theoretical foundations that make this possible and show somepromising first results.
arxiv-10500-89 | Risk and Regret of Hierarchical Bayesian Learners | http://arxiv.org/pdf/1505.04984v1.pdf | author:Jonathan H. Huggins, Joshua B. Tenenbaum category:cs.LG stat.ML published:2015-05-19 summary:Common statistical practice has shown that the full power of Bayesian methodsis not realized until hierarchical priors are used, as these allow for greater"robustness" and the ability to "share statistical strength." Yet it is anongoing challenge to provide a learning-theoretically sound formalism of suchnotions that: offers practical guidance concerning when and how best to utilizehierarchical models; provides insights into what makes for a good hierarchicalprior; and, when the form of the prior has been chosen, can guide the choice ofhyperparameter settings. We present a set of analytical tools for understandinghierarchical priors in both the online and batch learning settings. We provideregret bounds under log-loss, which show how certain hierarchical modelscompare, in retrospect, to the best single model in the model class. We alsoshow how to convert a Bayesian log-loss regret bound into a Bayesian risk boundfor any bounded loss, a result which may be of independent interest. Risk andregret bounds for Student's $t$ and hierarchical Gaussian priors allow us toformalize the concepts of "robustness" and "sharing statistical strength."Priors for feature selection are investigated as well. Our results suggest thatthe learning-theoretic benefits of using hierarchical priors can often come atlittle cost on practical problems.
arxiv-10500-90 | Multi-task additive models with shared transfer functions based on dictionary learning | http://arxiv.org/pdf/1505.04966v1.pdf | author:Alhussein Fawzi, Mathieu Sinn, Pascal Frossard category:stat.ML cs.LG published:2015-05-19 summary:Additive models form a widely popular class of regression models whichrepresent the relation between covariates and response variables as the sum oflow-dimensional transfer functions. Besides flexibility and accuracy, a keybenefit of these models is their interpretability: the transfer functionsprovide visual means for inspecting the models and identifying domain-specificrelations between inputs and outputs. However, in large-scale problemsinvolving the prediction of many related tasks, learning independently additivemodels results in a loss of model interpretability, and can cause overfittingwhen training data is scarce. We introduce a novel multi-task learning approachwhich provides a corpus of accurate and interpretable additive models for alarge number of related forecasting tasks. Our key idea is to share transferfunctions across models in order to reduce the model complexity and ease theexploration of the corpus. We establish a connection with sparse dictionarylearning and propose a new efficient fitting algorithm which alternates betweensparse coding and transfer function updates. The former step is solved via anextension of Orthogonal Matching Pursuit, whose properties are analyzed using anovel recovery condition which extends existing results in the literature. Thelatter step is addressed using a traditional dictionary update rule.Experiments on real-world data demonstrate that our approach compares favorablyto baseline methods while yielding an interpretable corpus of models, revealingstructure among the individual tasks and being more robust when training datais scarce. Our framework therefore extends the well-known benefits of additivemodels to common regression settings possibly involving thousands of tasks.
arxiv-10500-91 | General factorization framework for context-aware recommendations | http://arxiv.org/pdf/1401.4529v2.pdf | author:Balázs Hidasi, Domonkos Tikk category:cs.IR cs.LG published:2014-01-18 summary:Context-aware recommendation algorithms focus on refining recommendations byconsidering additional information, available to the system. This topic hasgained a lot of attention recently. Among others, several factorization methodswere proposed to solve the problem, although most of them assume explicitfeedback which strongly limits their real-world applicability. While thesealgorithms apply various loss functions and optimization strategies, thepreference modeling under context is less explored due to the lack of toolsallowing for easy experimentation with various models. As context dimensionsare introduced beyond users and items, the space of possible preference modelsand the importance of proper modeling largely increases. In this paper we propose a General Factorization Framework (GFF), a singleflexible algorithm that takes the preference model as an input and computeslatent feature matrices for the input dimensions. GFF allows us to easilyexperiment with various linear models on any context-aware recommendation task,be it explicit or implicit feedback based. The scaling properties makes itusable under real life circumstances as well. We demonstrate the framework's potential by exploring various preferencemodels on a 4-dimensional context-aware problem with contexts that areavailable for almost any real life datasets. We show in our experiments --performed on five real life, implicit feedback datasets -- that properpreference modelling significantly increases recommendation accuracy, andpreviously unused models outperform the traditional ones. Novel models in GFFalso outperform state-of-the-art factorization algorithms. We also extend the method to be fully compliant to the MultidimensionalDataspace Model, one of the most extensive data models of context-enricheddata. Extended GFF allows the seamless incorporation of information into thefac[truncated]
arxiv-10500-92 | High Performance Offline Handwritten Chinese Character Recognition Using GoogLeNet and Directional Feature Maps | http://arxiv.org/pdf/1505.04925v1.pdf | author:Zhuoyao Zhong, Lianwen Jin, Zecheng Xie category:cs.CV published:2015-05-19 summary:Just like its great success in solving many computer vision problems, theconvolutional neural networks (CNN) provided new end-to-end approach tohandwritten Chinese character recognition (HCCR) with very promising results inrecent years. However, previous CNNs so far proposed for HCCR were neither deepenough nor slim enough. We show in this paper that, a deeper architecture canbenefit HCCR a lot to achieve higher performance, meanwhile can be designedwith less parameters. We also show that the traditional feature extractionmethods, such as Gabor or gradient feature maps, are still useful for enhancingthe performance of CNN. We design a streamlined version of GoogLeNet [13],which was original proposed for image classification in recent years with verydeep architecture, for HCCR (denoted as HCCR-GoogLeNet). The HCCR-GoogLeNet weused is 19 layers deep but involves with only 7.26 million parameters.Experiments were conducted using the ICDAR 2013 offline HCCR competitiondataset. It has been shown that with the proper incorporation with traditionaldirectional feature maps, the proposed single and ensemble HCCR-GoogLeNetmodels achieve new state of the art recognition accuracy of 96.35% and 96.74%,respectively, outperforming previous best result with significant gap.
arxiv-10500-93 | Character-level Chinese Writer Identification using Path Signature Feature, DropStroke and Deep CNN | http://arxiv.org/pdf/1505.04922v1.pdf | author:Weixin Yang, Lianwen Jin, Manfei Liu category:cs.CV published:2015-05-19 summary:Most existing online writer-identification systems require that the textcontent is supplied in advance and rely on separately designed features andclassifiers. The identifications are based on lines of text, entire paragraphs,or entire documents; however, these materials are not always available. In thispaper, we introduce a path-signature feature to an end-to-end text-independentwriter-identification system with a deep convolutional neural network (DCNN).Because deep models require a considerable amount of data to achieve goodperformance, we propose a data-augmentation method named DropStroke to enrichpersonal handwriting. Experiments were conducted on online handwritten Chinesecharacters from the CASIA-OLHWDB1.0 dataset, which consists of 3,866 classesfrom 420 writers. For each writer, we only used 200 samples for training andthe remaining 3,666. The results reveal that the path-signature feature isuseful for writer identification, and the proposed DropStroke techniqueenhances the generalization and significantly improves performance.
arxiv-10500-94 | Have a Look at What I See | http://arxiv.org/pdf/1505.04873v1.pdf | author:Lior Talker, Yael Moses, Ilan Shimshoni category:cs.CV published:2015-05-19 summary:We propose a method for guiding a photographer to rotate her/his smartphonecamera to obtain an image that overlaps with another image of the same scene.The other image is taken by another photographer from a different viewpoint.Our method is applicable even when the images do not have overlapping fields ofview. Straightforward applications of our method include sharing attention toregions of interest for social purposes, or adding missing images to improvestructure for motion results. Our solution uses additional images of the scene,which are often available since many people use their smartphone camerasregularly. These images may be available online from other photographers whoare present at the scene. Our method avoids 3D scene reconstruction; it reliesinstead on a new representation that consists of the spatial orders of thescene points on two axes, x and y. This representation allows a sequence ofpoints to be chosen efficiently and projected onto the photographers images,using epipolar point transfer. Overlaying these epipolar lines on the livepreview of the camera produces a convenient interface to guide the user. Themethod was tested on challenging datasets of images and succeeded in guiding aphotographer from one view to a non-overlapping destination view.
arxiv-10500-95 | Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors | http://arxiv.org/pdf/1505.04868v1.pdf | author:Limin Wang, Yu Qiao, Xiaoou Tang category:cs.CV published:2015-05-19 summary:Visual features are of vital importance for human action understanding invideos. This paper presents a new video representation, calledtrajectory-pooled deep-convolutional descriptor (TDD), which shares the meritsof both hand-crafted features and deep-learned features. Specifically, weutilize deep architectures to learn discriminative convolutional feature maps,and conduct trajectory-constrained pooling to aggregate these convolutionalfeatures into effective descriptors. To enhance the robustness of TDDs, wedesign two normalization methods to transform convolutional feature maps,namely spatiotemporal normalization and channel normalization. The advantagesof our features come from (i) TDDs are automatically learned and contain highdiscriminative capacity compared with those hand-crafted features; (ii) TDDstake account of the intrinsic characteristics of temporal dimension andintroduce the strategies of trajectory-constrained sampling and pooling foraggregating deep-learned features. We conduct experiments on two challengingdatasets: HMDB51 and UCF101. Experimental results show that TDDs outperformprevious hand-crafted features and deep-learned features. Our method alsoachieves superior performance to the state of the art on these datasets (HMDB5165.9%, UCF101 91.5%).
arxiv-10500-96 | Logarithmic Time Online Multiclass prediction | http://arxiv.org/pdf/1406.1822v13.pdf | author:Anna Choromanska, John Langford category:cs.LG published:2014-06-06 summary:We study the problem of multiclass classification with an extremely largenumber of classes (k), with the goal of obtaining train and test timecomplexity logarithmic in the number of classes. We develop top-down treeconstruction approaches for constructing logarithmic depth trees. On thetheoretical front, we formulate a new objective function, which is optimized ateach node of the tree and creates dynamic partitions of the data which are bothpure (in terms of class labels) and balanced. We demonstrate that underfavorable conditions, we can construct logarithmic depth trees that have leaveswith low label entropy. However, the objective function at the nodes ischallenging to optimize computationally. We address the empirical problem witha new online decision tree construction procedure. Experiments demonstrate thatthis online algorithm quickly achieves improvement in test error compared tomore common logarithmic training time approaches, which makes it a plausiblemethod in computationally constrained large-k applications.
arxiv-10500-97 | Naive Bayes and Text Classification I - Introduction and Theory | http://arxiv.org/pdf/1410.5329v3.pdf | author:Sebastian Raschka category:cs.LG published:2014-10-16 summary:Naive Bayes classifiers, a family of classifiers that are based on thepopular Bayes' probability theorem, are known for creating simple yet wellperforming models, especially in the fields of document classification anddisease prediction. In this article, we will look at the main concepts of naiveBayes classification in the context of document categorization.
arxiv-10500-98 | Tensor Factorization via Matrix Factorization | http://arxiv.org/pdf/1501.07320v2.pdf | author:Volodymyr Kuleshov, Arun Tejasvi Chaganty, Percy Liang category:cs.LG stat.ML published:2015-01-29 summary:Tensor factorization arises in many machine learning applications, suchknowledge base modeling and parameter estimation in latent variable models.However, numerical methods for tensor factorization have not reached the levelof maturity of matrix factorization methods. In this paper, we propose a newmethod for CP tensor factorization that uses random projections to reduce theproblem to simultaneous matrix diagonalization. Our method is conceptuallysimple and also applies to non-orthogonal and asymmetric tensors of arbitraryorder. We prove that a small number random projections essentially preservesthe spectral information in the tensor, allowing us to remove the dependence onthe eigengap that plagued earlier tensor-to-matrix reductions. Experimentally,our method outperforms existing tensor factorization methods on both simulateddata and two real datasets.
arxiv-10500-99 | Towards a Learning Theory of Cause-Effect Inference | http://arxiv.org/pdf/1502.02398v2.pdf | author:David Lopez-Paz, Krikamol Muandet, Bernhard Schölkopf, Ilya Tolstikhin category:stat.ML math.PR math.ST stat.TH published:2015-02-09 summary:We pose causal inference as the problem of learning to classify probabilitydistributions. In particular, we assume access to a collection$\{(S_i,l_i)\}_{i=1}^n$, where each $S_i$ is a sample drawn from theprobability distribution of $X_i \times Y_i$, and $l_i$ is a binary labelindicating whether "$X_i \to Y_i$" or "$X_i \leftarrow Y_i$". Given these data,we build a causal inference rule in two steps. First, we featurize each $S_i$using the kernel mean embedding associated with some characteristic kernel.Second, we train a binary classifier on such embeddings to distinguish betweencausal directions. We present generalization bounds showing the statisticalconsistency and learning rates of the proposed approach, and provide a simpleimplementation that achieves state-of-the-art cause-effect inference.Furthermore, we extend our ideas to infer causal relationships between morethan two variables.
arxiv-10500-100 | An Asynchronous Mini-Batch Algorithm for Regularized Stochastic Optimization | http://arxiv.org/pdf/1505.04824v1.pdf | author:Hamid Reza Feyzmahdavian, Arda Aytekin, Mikael Johansson category:math.OC cs.SY stat.ML published:2015-05-18 summary:Mini-batch optimization has proven to be a powerful paradigm for large-scalelearning. However, the state of the art parallel mini-batch algorithms assumesynchronous operation or cyclic update orders. When worker nodes areheterogeneous (due to different computational capabilities or differentcommunication delays), synchronous and cyclic operations are inefficient sincethey will leave workers idle waiting for the slower nodes to complete theircomputations. In this paper, we propose an asynchronous mini-batch algorithmfor regularized stochastic optimization problems with smooth loss functionsthat eliminates idle waiting and allows workers to run at their maximal updaterates. We show that by suitably choosing the step-size values, the algorithmachieves a rate of the order $O(1/\sqrt{T})$ for general convex regularizationfunctions, and the rate $O(1/T)$ for strongly convex regularization functions,where $T$ is the number of iterations. In both cases, the impact of asynchronyon the convergence rate of our algorithm is asymptotically negligible, and anear-linear speedup in the number of workers can be expected. Theoreticalresults are confirmed in real implementations on a distributed computinginfrastructure.
arxiv-10500-101 | HyperSpectral classification with adaptively weighted L1-norm regularization and spatial postprocessing | http://arxiv.org/pdf/1412.2684v2.pdf | author:Victor Stefan Aldea, M. O. Ahmad, W. E. Lynch category:math.OC cs.CV published:2014-12-08 summary:Sparse regression methods have been proven effective in a wide range ofsignal processing problems such as image compression, speech coding, channelequalization, linear regression and classification. In this paper we develop anew method of hyperspectral image classification based on the sparse unmixingalgorithm SUnSAL for which a pixel adaptive L1-norm regularization term isintroduced. To further enhance class separability, the algorithm is kernelizedusing a RBF kernel and the final results are improved by a combination ofspatial pre and post-processing operations. We show that our method iscompetitive with state of the art algorithms such as SVM-CK, KLR-CK, KSOMP andKSSP.
arxiv-10500-102 | Predicting Important Objects for Egocentric Video Summarization | http://arxiv.org/pdf/1505.04803v1.pdf | author:Yong Jae Lee, Kristen Grauman category:cs.CV published:2015-05-18 summary:We present a video summarization approach for egocentric or "wearable" cameradata. Given hours of video, the proposed method produces a compact storyboardsummary of the camera wearer's day. In contrast to traditional keyframeselection techniques, the resulting summary focuses on the most importantobjects and people with which the camera wearer interacts. To accomplish this,we develop region cues indicative of high-level saliency in egocentricvideo---such as the nearness to hands, gaze, and frequency of occurrence---andlearn a regressor to predict the relative importance of any new region based onthese cues. Using these predictions and a simple form of temporal eventdetection, our method selects frames for the storyboard that reflect the keyobject-driven happenings. We adjust the compactness of the final summary giveneither an importance selection criterion or a length budget; for the latter, wedesign an efficient dynamic programming solution that accounts for importance,visual uniqueness, and temporal displacement. Critically, the approach isneither camera-wearer-specific nor object-specific; that means the learnedimportance metric need not be trained for a given user or context, and it canpredict the importance of objects and people that have never been seenpreviously. Our results on two egocentric video datasets show the method'spromise relative to existing techniques for saliency and summarization.
arxiv-10500-103 | On the tightness of an SDP relaxation of k-means | http://arxiv.org/pdf/1505.04778v1.pdf | author:Takayuki Iguchi, Dustin G. Mixon, Jesse Peterson, Soledad Villar category:cs.IT cs.DS cs.LG math.IT math.ST stat.ML stat.TH published:2015-05-18 summary:Recently, Awasthi et al. introduced an SDP relaxation of the $k$-meansproblem in $\mathbb R^m$. In this work, we consider a random model for the datapoints in which $k$ balls of unit radius are deterministically distributedthroughout $\mathbb R^m$, and then in each ball, $n$ points are drawn accordingto a common rotationally invariant probability distribution. For any fixed ballconfiguration and probability distribution, we prove that the SDP relaxation ofthe $k$-means problem exactly recovers these planted clusters with probability$1-e^{-\Omega(n)}$ provided the distance between any two of the ball centers is$>2+\epsilon$, where $\epsilon$ is an explicit function of the configuration ofthe ball centers, and can be arbitrarily small when $m$ is large.
arxiv-10500-104 | DopeLearning: A Computational Approach to Rap Lyrics Generation | http://arxiv.org/pdf/1505.04771v1.pdf | author:Eric Malmi, Pyry Takala, Hannu Toivonen, Tapani Raiko, Aristides Gionis category:cs.LG cs.AI cs.CL cs.NE I.2.7; H.3.3 published:2015-05-18 summary:Writing rap lyrics requires both creativity, to construct a meaningful and aninteresting story, and lyrical skills, to produce complex rhyme patterns, whichare the cornerstone of a good flow. We present a method for capturing both ofthese aspects. Our approach is based on two machine-learning techniques: theRankSVM algorithm, and a deep neural network model with a novel structure. Forthe problem of distinguishing the real next line from a randomly selected one,we achieve an 82 % accuracy. We employ the resulting prediction method forcreating new rap lyrics by combining lines from existing songs. In terms ofquantitative rhyme density, the produced lyrics outperform best human rappersby 21 %. The results highlight the benefit of our rhyme density metric and ourinnovative predictor of next lines.
arxiv-10500-105 | Cascading Bandits: Learning to Rank in the Cascade Model | http://arxiv.org/pdf/1502.02763v2.pdf | author:Branislav Kveton, Csaba Szepesvari, Zheng Wen, Azin Ashkan category:cs.LG stat.ML published:2015-02-10 summary:A search engine usually outputs a list of $K$ web pages. The user examinesthis list, from the first web page to the last, and chooses the firstattractive page. This model of user behavior is known as the cascade model. Inthis paper, we propose cascading bandits, a learning variant of the cascademodel where the objective is to identify $K$ most attractive items. Weformulate our problem as a stochastic combinatorial partial monitoring problem.We propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB. Wealso prove gap-dependent upper bounds on the regret of these algorithms andderive a lower bound on the regret in cascading bandits. The lower boundmatches the upper bound of CascadeKL-UCB up to a logarithmic factor. Weexperiment with our algorithms on several problems. The algorithms performsurprisingly well even when our modeling assumptions are violated.
arxiv-10500-106 | Foundational principles for large scale inference: Illustrations through correlation mining | http://arxiv.org/pdf/1505.02475v2.pdf | author:Alfred O. Hero, Bala Rajaratnam category:math.ST stat.ML stat.TH published:2015-05-11 summary:When can reliable inference be drawn in the "Big Data" context? This paperpresents a framework for answering this fundamental question in the context ofcorrelation mining, with implications for general large scale inference. Inlarge scale data applications like genomics, connectomics, and eco-informaticsthe dataset is often variable-rich but sample-starved: a regime where thenumber $n$ of acquired samples (statistical replicates) is far fewer than thenumber $p$ of observed variables (genes, neurons, voxels, or chemicalconstituents). Much of recent work has focused on understanding thecomputational complexity of proposed methods for "Big Data." Sample complexityhowever has received relatively less attention, especially in the setting whenthe sample size $n$ is fixed, and the dimension $p$ grows without bound. Toaddress this gap, we develop a unified statistical framework that explicitlyquantifies the sample complexity of various inferential tasks. Sampling regimescan be divided into several categories: 1) the classical asymptotic regimewhere the variable dimension is fixed and the sample size goes to infinity; 2)the mixed asymptotic regime where both variable dimension and sample size go toinfinity at comparable rates; 3) the purely high dimensional asymptotic regimewhere the variable dimension goes to infinity and the sample size is fixed.Each regime has its niche but only the latter regime applies to exa-scale datadimension. We illustrate this high dimensional framework for the problem ofcorrelation mining, where it is the matrix of pairwise and partial correlationsamong the variables that are of interest. We demonstrate various regimes ofcorrelation mining based on the unifying perspective of high dimensionallearning rates and sample complexity for different structured covariance modelsand different inference tasks.
arxiv-10500-107 | Extraction of Pharmacokinetic Evidence of Drug-drug Interactions from the Literature | http://arxiv.org/pdf/1412.0744v2.pdf | author:Artemy Kolchinsky, Anália Lourenço, Heng-Yi Wu, Lang Li, Luis M. Rocha category:stat.ML cs.IR q-bio.QM published:2014-12-02 summary:Drug-drug interaction (DDI) is a major cause of morbidity and mortality and asubject of intense scientific interest. Biomedical literature mining can aidDDI research by extracting evidence for large numbers of potential interactionsfrom published literature and clinical databases. Though DDI is investigated indomains ranging in scale from intracellular biochemistry to human populations,literature mining has not been used to extract specific types of experimentalevidence, which are reported differently for distinct experimental goals. Wefocus on pharmacokinetic evidence for DDI, essential for identifying causalmechanisms of putative interactions and as input for further pharmacologicaland pharmaco-epidemiology investigations. We used manually curated corpora ofPubMed abstracts and annotated sentences to evaluate the efficacy of literaturemining on two tasks: first, identifying PubMed abstracts containingpharmacokinetic evidence of DDIs; second, extracting sentences containing suchevidence from abstracts. We implemented a text mining pipeline and evaluated itusing several linear classifiers and a variety of feature transforms. The mostimportant textual features in the abstract and sentence classification taskswere analyzed. We also investigated the performance benefits of using featuresderived from PubMed metadata fields, various publicly available named entityrecognizers, and pharmacokinetic dictionaries. Several classifiers performedvery well in distinguishing relevant and irrelevant abstracts (reachingF1~=0.93, MCC~=0.74, iAUC~=0.99) and sentences (F1~=0.76, MCC~=0.65,iAUC~=0.83). We found that word bigram features were important for achievingoptimal classifier performance and that features derived from Medical SubjectHeadings (MeSH) terms significantly improved abstract classification. ...
arxiv-10500-108 | WhittleSearch: Interactive Image Search with Relative Attribute Feedback | http://arxiv.org/pdf/1505.04141v2.pdf | author:Adriana Kovashka, Devi Parikh, Kristen Grauman category:cs.CV published:2015-05-15 summary:We propose a novel mode of feedback for image search, where a user describeswhich properties of exemplar images should be adjusted in order to more closelymatch his/her mental model of the image sought. For example, perusing imageresults for a query "black shoes", the user might state, "Show me shoe imageslike these, but sportier." Offline, our approach first learns a set of rankingfunctions, each of which predicts the relative strength of a nameable attributein an image (e.g., sportiness). At query time, the system presents the userwith a set of exemplar images, and the user relates them to his/her targetimage with comparative statements. Using a series of such constraints in themulti-dimensional attribute space, our method iteratively updates its relevancefunction and re-ranks the database of images. To determine which exemplarimages receive feedback from the user, we present two variants of the approach:one where the feedback is user-initiated and another where the feedback isactively system-initiated. In either case, our approach allows a user toefficiently "whittle away" irrelevant portions of the visual feature space,using semantic language to precisely communicate her preferences to the system.We demonstrate our technique for refining image search for people, products,and scenes, and we show that it outperforms traditional binary relevancefeedback in terms of search speed and accuracy. In addition, the ordinal natureof relative attributes helps make our active approach efficient -- bothcomputationally for the machine when selecting the reference images, and forthe user by requiring less user interaction than conventional passive andactive methods.
arxiv-10500-109 | Ensemble of Example-Dependent Cost-Sensitive Decision Trees | http://arxiv.org/pdf/1505.04637v1.pdf | author:Alejandro Correa Bahnsen, Djamila Aouada, Bjorn Ottersten category:cs.LG published:2015-05-18 summary:Several real-world classification problems are example-dependentcost-sensitive in nature, where the costs due to misclassification vary betweenexamples and not only within classes. However, standard classification methodsdo not take these costs into account, and assume a constant cost ofmisclassification errors. In previous works, some methods that take intoaccount the financial costs into the training of different algorithms have beenproposed, with the example-dependent cost-sensitive decision tree algorithmbeing the one that gives the highest savings. In this paper we propose a newframework of ensembles of example-dependent cost-sensitive decision-trees. Theframework consists in creating different example-dependent cost-sensitivedecision trees on random subsamples of the training set, and then combiningthem using three different combination approaches. Moreover, we propose two newcost-sensitive combination approaches; cost-sensitive weighted voting andcost-sensitive stacking, the latter being based on the cost-sensitive logisticregression method. Finally, using five different databases, from fourreal-world applications: credit card fraud detection, churn modeling, creditscoring and direct marketing, we evaluate the proposed method againststate-of-the-art example-dependent cost-sensitive techniques, namely,cost-proportionate sampling, Bayes minimum risk and cost-sensitive decisiontrees. The results show that the proposed algorithms have better results forall databases, in the sense of higher savings.
arxiv-10500-110 | Graph Partitioning via Parallel Submodular Approximation to Accelerate Distributed Machine Learning | http://arxiv.org/pdf/1505.04636v1.pdf | author:Mu Li, Dave G. Andersen, Alexander J. Smola category:cs.DC cs.AI cs.LG published:2015-05-18 summary:Distributed computing excels at processing large scale data, but thecommunication cost for synchronizing the shared parameters may slow down theoverall performance. Fortunately, the interactions between parameter and datain many problems are sparse, which admits efficient partition in order toreduce the communication overhead. In this paper, we formulate data placement as a graph partitioning problem.We propose a distributed partitioning algorithm. We give both theoreticalguarantees and a highly efficient implementation. We also provide a highlyefficient implementation of the algorithm and demonstrate its promising resultson both text datasets and social networks. We show that the proposed algorithmleads to 1.6x speedup of a state-of-the-start distributed machine learningsystem by eliminating 90\% of the network communication.
arxiv-10500-111 | Simple regret for infinitely many armed bandits | http://arxiv.org/pdf/1505.04627v1.pdf | author:Alexandra Carpentier, Michal Valko category:cs.LG stat.ML published:2015-05-18 summary:We consider a stochastic bandit problem with infinitely many arms. In thissetting, the learner has no chance of trying all the arms even once and has todedicate its limited number of samples only to a certain number of arms. Allprevious algorithms for this setting were designed for minimizing thecumulative regret of the learner. In this paper, we propose an algorithm aimingat minimizing the simple regret. As in the cumulative regret setting ofinfinitely many armed bandits, the rate of the simple regret will depend on aparameter $\beta$ characterizing the distribution of the near-optimal arms. Weprove that depending on $\beta$, our algorithm is minimax optimal either up toa multiplicative constant or up to a $\log(n)$ factor. We also provideextensions to several important cases: when $\beta$ is unknown, in a naturalsetting where the near-optimal arms have a small variance, and in the case ofunknown time horizon.
arxiv-10500-112 | Fractally-organized Connectionist Networks: Conjectures and Preliminary Results | http://arxiv.org/pdf/1505.04618v1.pdf | author:Vincenzo De Florio category:cs.NE published:2015-05-18 summary:A strict interpretation of connectionism mandates complex networks of simplecomponents. The question here is, is this simplicity to be interpreted inabsolute terms? I conjecture that absolute simplicity might not be an essentialattribute of connectionism, and that it may be effectively exchanged with arequirement for relative simplicity, namely simplicity with respect to thecurrent organizational level. In this paper I provide some elements to theanalysis of the above question. In particular I conjecture that fractallyorganized connectionist networks may provide a convenient means to achive whatLeibniz calls an "art of complication", namely an effective way to encapsulatecomplexity and practically extend the applicability of connectionism to domainssuch as sociotechnical system modeling and design. Preliminary evidence to myclaim is brought by considering the design of the software architecturedesigned for the telemonitoring service of Flemish project "Little Sister".
arxiv-10500-113 | Joint Representation Classification for Collective Face Recognition | http://arxiv.org/pdf/1505.04617v1.pdf | author:Liping Wang, Songcan Chen category:cs.CV math.OC published:2015-05-18 summary:Sparse representation based classification (SRC) is popularly used in manyapplications such as face recognition, and implemented in two steps:representation coding and classification. For a given set of testing images,SRC codes every image over the base images as a sparse representation thenclassifies it to the class with the least representation error. This schemeutilizes an individual representation rather than the collective one toclassify such a set of images, doing so obviously ignores the correlation amongthe given images. In this paper, a joint representation classification (JRC)for collective face recognition is proposed. JRC takes the correlation ofmultiple images as well as a single representation into account. Under theassumption that the given face images are generally related to each other, JRCcodes all the testing images over the base images simultaneously to facilitaterecognition. To this end, the testing inputs are aligned into a matrix and thejoint representation coding is formulated to a generalized$l_{2,q}-l_{2,p}$-minimization problem. To uniformly solve the inducedoptimization problems for any $q\in[1,2]$ and $p\in (0,2]$, an iterativequadratic method (IQM) is developed. IQM is proved to be a strict descentalgorithm with convergence to the optimal solution. Moreover, a more practicalIQM is proposed for large-scale case. Experimental results on three publicdatabases show that the JRC with practical IQM no only saves much computationalcost but also achieves better performance in collective face recognition thanthe state-of-the-arts.
arxiv-10500-114 | Entailment Among Probabilistic Implications | http://arxiv.org/pdf/1501.04826v2.pdf | author:Albert Atserias, José L. Balcázar category:cs.LO cs.DB cs.LG published:2015-01-20 summary:We study a natural variant of the implicational fragment of propositionallogic. Its formulas are pairs of conjunctions of positive literals, relatedtogether by an implicational-like connective; the semantics of this sort ofimplication is defined in terms of a threshold on a conditional probability ofthe consequent, given the antecedent: we are dealing with what the dataanalysis community calls confidence of partial implications or associationrules. Existing studies of redundancy among these partial implications havecharacterized so far only entailment from one premise and entailment from twopremises. By exploiting a previously noted alternative view of this entailmentin terms of linear programming duality, we characterize exactly the cases ofentailment from arbitrary numbers of premises. As a result, we obtain decisionalgorithms of better complexity; additionally, for each potential case ofentailment, we identify a critical confidence threshold and show that it is,actually, intrinsic to each set of premises and antecedent of the conclusion.
arxiv-10500-115 | Learning Discriminative Stein Kernel for SPD Matrices and Its Applications | http://arxiv.org/pdf/1407.1974v3.pdf | author:Jianjia Zhang, Lei Wang, Luping Zhou, Wanqing Li category:cs.CV published:2014-07-08 summary:Stein kernel has recently shown promising performance on classifying imagesrepresented by symmetric positive definite (SPD) matrices. It evaluates thesimilarity between two SPD matrices through their eigenvalues. In this paper,we argue that directly using the original eigenvalues may be problematicbecause: i) Eigenvalue estimation becomes biased when the number of samples isinadequate, which may lead to unreliable kernel evaluation; ii) Moreimportantly, eigenvalues only reflect the property of an individual SPD matrix.They are not necessarily optimal for computing Stein kernel when the goal is todiscriminate different sets of SPD matrices. To address the two issues in oneshot, we propose a discriminative Stein kernel, in which an extra parametervector is defined to adjust the eigenvalues of the input SPD matrices. Theoptimal parameter values are sought by optimizing a proxy of classificationperformance. To show the generality of the proposed method, three differentkernel learning criteria that are commonly used in the literature are employedrespectively as a proxy. A comprehensive experimental study is conducted on avariety of image classification tasks to compare our proposed discriminativeStein kernel with the original Stein kernel and other commonly used methods forevaluating the similarity between SPD matrices. The experimental resultsdemonstrate that, the discriminative Stein kernel can attain greaterdiscrimination and better align with classification tasks by altering theeigenvalues. This makes it produce higher classification performance than theoriginal Stein kernel and other commonly used methods.
arxiv-10500-116 | U-Net: Convolutional Networks for Biomedical Image Segmentation | http://arxiv.org/pdf/1505.04597v1.pdf | author:Olaf Ronneberger, Philipp Fischer, Thomas Brox category:cs.CV published:2015-05-18 summary:There is large consent that successful training of deep networks requiresmany thousand annotated training samples. In this paper, we present a networkand training strategy that relies on the strong use of data augmentation to usethe available annotated samples more efficiently. The architecture consists ofa contracting path to capture context and a symmetric expanding path thatenables precise localization. We show that such a network can be trainedend-to-end from very few images and outperforms the prior best method (asliding-window convolutional network) on the ISBI challenge for segmentation ofneuronal structures in electron microscopic stacks. Using the same networktrained on transmitted light microscopy images (phase contrast and DIC) we wonthe ISBI cell tracking challenge 2015 in these categories by a large margin.Moreover, the network is fast. Segmentation of a 512x512 image takes less thana second on a recent GPU. The full implementation (based on Caffe) and thetrained networks are available athttp://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .
arxiv-10500-117 | Global Variational Method for Fingerprint Segmentation by Three-part Decomposition | http://arxiv.org/pdf/1505.04585v1.pdf | author:Duy Hoang Thai, Carsten Gottschlich category:cs.CV published:2015-05-18 summary:Verifying an identity claim by fingerprint recognition is a commonplaceexperience for millions of people in their daily life, e.g. for unlocking atablet computer or smartphone. The first processing step after fingerprintimage acquisition is segmentation, i.e. dividing a fingerprint image into aforeground region which contains the relevant features for the comparisonalgorithm, and a background region. We propose a novel segmentation method byglobal three-part decomposition (G3PD). Based on global variational analysis,the G3PD method decomposes a fingerprint image into cartoon, texture and noiseparts. After decomposition, the foreground region is obtained from the non-zerocoefficients in the texture image using morphological processing. Thesegmentation performance of the G3PD method is compared to fivestate-of-the-art methods on a benchmark which comprises manually marked groundtruth segmentation for 10560 images. Performance evaluations show that the G3PDmethod consistently outperforms existing methods in terms of segmentationaccuracy.
arxiv-10500-118 | BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation | http://arxiv.org/pdf/1503.01640v2.pdf | author:Jifeng Dai, Kaiming He, Jian Sun category:cs.CV published:2015-03-05 summary:Recent leading approaches to semantic segmentation rely on deep convolutionalnetworks trained with human-annotated, pixel-level segmentation masks. Suchpixel-accurate supervision demands expensive labeling effort and limits theperformance of deep networks that usually benefit from more training data. Inthis paper, we propose a method that achieves competitive accuracy but onlyrequires easily obtained bounding box annotations. The basic idea is to iteratebetween automatically generating region proposals and training convolutionalnetworks. These two steps gradually recover segmentation masks for improvingthe networks, and vise versa. Our method, called BoxSup, produces competitiveresults supervised by boxes only, on par with strong baselines fully supervisedby masks under the same setting. By leveraging a large amount of boundingboxes, BoxSup further unleashes the power of deep convolutional networks andyields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT.
arxiv-10500-119 | Place Recognition with Event-based Cameras and a Neural Implementation of SeqSLAM | http://arxiv.org/pdf/1505.04548v1.pdf | author:Michael Milford, Hanme Kim, Michael Mangan, Stefan Leutenegger, Tom Stone, Barbara Webb, Andrew Davison category:cs.RO cs.CV published:2015-05-18 summary:Event-based cameras offer much potential to the fields of robotics andcomputer vision, in part due to their large dynamic range and extremely high"frame rates". These attributes make them, at least in theory, particularlysuitable for enabling tasks like navigation and mapping on high speed roboticplatforms under challenging lighting conditions, a task which has beenparticularly challenging for traditional algorithms and camera sensors. Beforethese tasks become feasible however, progress must be made towards adapting andinnovating current RGB-camera-based algorithms to work with event-basedcameras. In this paper we present ongoing research investigating two distinctapproaches to incorporating event-based cameras for robotic navigation: theinvestigation of suitable place recognition / loop closure techniques, and thedevelopment of efficient neural implementations of place recognition techniquesthat enable the possibility of place recognition using event-based cameras atvery high frame rates using neuromorphic computing hardware.
arxiv-10500-120 | Emergence-focused design in complex system simulation | http://arxiv.org/pdf/1505.04518v1.pdf | author:Chris Marriott, Jobran Chebib category:q-bio.PE cs.AI cs.NE published:2015-05-18 summary:Emergence is a phenomenon taken for granted in science but also still notwell understood. We have developed a model of artificial genetic evolutionintended to allow for emergence on genetic, population and social levels. Wepresent the details of the current state of our environment, agent, andreproductive models. In developing our models we have relied on a principle ofusing non-linear systems to model as many systems as possible includingmutation and recombination, gene-environment interaction, agent metabolism,agent survival, resource gathering and sexual reproduction. In this paper wereview the genetic dynamics that have emerged in our system includinggenotype-phenotype divergence, genetic drift, pseudogenes, and geneduplication. We conclude that emergence-focused design in complex systemsimulation is necessary to reproduce the multilevel emergence seen in thenatural world.
arxiv-10500-121 | Reproducible Evaluation of Pan-Tilt-Zoom Tracking | http://arxiv.org/pdf/1505.04502v1.pdf | author:Gengjie Chen, Pierre-Luc St-Charles, Wassim Bouachir, Thomas Joeisseint, Guillaume-Alexandre Bilodeau, Robert Bergevin category:cs.CV published:2015-05-18 summary:Tracking with a Pan-Tilt-Zoom (PTZ) camera has been a research topic incomputer vision for many years. However, it is very difficult to assess theprogress that has been made on this topic because there is no standardevaluation methodology. The difficulty in evaluating PTZ tracking algorithmsarises from their dynamic nature. In contrast to other forms of tracking, PTZtracking involves both locating the target in the image and controlling themotors of the camera to aim it so that the target stays in its field of view.This type of tracking can only be performed online. In this paper, we propose anew evaluation framework based on a virtual PTZ camera. With this framework,tracking scenarios do not change for each experiment and we are able toreplicate online PTZ camera control and behavior including camera positioningdelays, tracker processing delays, and numerical zoom. We tested our evaluationframework with the Camshift tracker to show its viability and to establishbaseline results.
arxiv-10500-122 | HEp-2 Cell Image Classification with Deep Convolutional Neural Networks | http://arxiv.org/pdf/1504.02531v2.pdf | author:Zhimin Gao, Lei Wang, Luping Zhou, Jianjia Zhang category:cs.CV published:2015-04-10 summary:Efficient Human Epithelial-2 (HEp-2) cell image classification can facilitatethe diagnosis of many autoimmune diseases. This paper presents an automaticframework for this classification task, by utilizing the deep convolutionalneural networks (CNNs) which have recently attracted intensive attention invisual recognition. This paper elaborates the important components of thisframework, discusses multiple key factors that impact the efficiency oftraining a deep CNN, and systematically compares this framework with thewell-established image classification models in the literature. Experiments onbenchmark datasets show that i) the proposed framework can effectivelyoutperform existing models by properly applying data augmentation; ii) ourCNN-based framework demonstrates excellent adaptability across differentdatasets, which is highly desirable for classification under varying laboratorysettings. Our system is ranked high in the cell image classificationcompetition hosted by ICPR 2014.
arxiv-10500-123 | Visual Semantic Role Labeling | http://arxiv.org/pdf/1505.04474v1.pdf | author:Saurabh Gupta, Jitendra Malik category:cs.CV published:2015-05-17 summary:In this paper we introduce the problem of Visual Semantic Role Labeling:given an image we want to detect people doing actions and localize the objectsof interaction. Classical approaches to action recognition either study thetask of action classification at the image or video clip level or at bestproduce a bounding box around the person doing the action. We believe such anoutput is inadequate and a complete understanding can only come when we areable to associate objects in the scene to the different semantic roles of theaction. To enable progress towards this goal, we annotate a dataset of 16Kpeople instances in 10K images with actions they are doing and associateobjects in the scene with different semantic roles for each action. Finally, weprovide a set of baseline algorithms for this task and analyze error modesproviding directions for future work.
arxiv-10500-124 | Exploring Nearest Neighbor Approaches for Image Captioning | http://arxiv.org/pdf/1505.04467v1.pdf | author:Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick category:cs.CV published:2015-05-17 summary:We explore a variety of nearest neighbor baseline approaches for imagecaptioning. These approaches find a set of nearest neighbor images in thetraining set from which a caption may be borrowed for the query image. Weselect a caption for the query image by finding the caption that bestrepresents the "consensus" of the set of candidate captions gathered from thenearest neighbor images. When measured by automatic evaluation metrics on theMS COCO caption evaluation server, these approaches perform as well as manyrecent approaches that generate novel captions. However, human studies showthat a method that generates novel captions is still preferred over the nearestneighbor approach.
arxiv-10500-125 | The Best of Both Worlds: Combining Data-independent and Data-driven Approaches for Action Recognition | http://arxiv.org/pdf/1505.04427v1.pdf | author:Zhenzhong Lan, Dezhong Yao, Ming Lin, Shoou-I Yu, Alexander Hauptmann category:cs.CV published:2015-05-17 summary:Motivated by the success of data-driven convolutional neural networks (CNNs)in object recognition on static images, researchers are working hard towardsdeveloping CNN equivalents for learning video features. However, learning videofeatures globally has proven to be quite a challenge due to its highdimensionality, the lack of labelled data and the difficulty in processinglarge-scale video data. Therefore, we propose to leverage effective techniquesfrom both data-driven and data-independent approaches to improve actionrecognition system. Our contribution is three-fold. First, we propose a two-stream StackedConvolutional Independent Subspace Analysis (ConvISA) architecture to show thatunsupervised learning methods can significantly boost the performance oftraditional local features extracted from data-independent models. Second, wedemonstrate that by learning on video volumes detected by Improved DenseTrajectory (IDT), we can seamlessly combine our novel local descriptors withhand-crafted descriptors. Thus we can utilize available feature enhancingtechniques developed for hand-crafted descriptors. Finally, similar tomulti-class classification framework in CNNs, we propose a training-freere-ranking technique that exploits the relationship among action classes toimprove the overall performance. Our experimental results on four benchmarkaction recognition datasets show significantly improved performance.
arxiv-10500-126 | Improved Microaneurysm Detection using Deep Neural Networks | http://arxiv.org/pdf/1505.04424v1.pdf | author:Mrinal Haloi category:cs.CV 68T45 published:2015-05-17 summary:In this work, we propose a novel microaneurysm (MA) detection for earlydieabetic ratinopathy screening using color fundus images. Since MA usually thefirst lesions to appear as a indicator of diabetic retinopathy, accuratedetection of MA is necessary for treatment. Each pixel of the image isclassified as either MA or non-MA using deep neural network with dropouttraining procedure using maxout activation function. No preprocessing step ormanual feature extraction is required. Substantial improvements over standardMA detection method based on pipeline of preprocessing, feature extraction,classification followed by postprocessing is achieved. The presented method isevaluated in publicly available Retinopathy Online Challenge (ROC) andDiaretdb1v2 database and achieved state-of-the-art accuracy.
arxiv-10500-127 | CCG Parsing and Multiword Expressions | http://arxiv.org/pdf/1505.04420v1.pdf | author:Miryam de Lhoneux category:cs.CL published:2015-05-17 summary:This thesis presents a study about the integration of information aboutMultiword Expressions (MWEs) into parsing with Combinatory Categorial Grammar(CCG). We build on previous work which has shown the benefit of addinginformation about MWEs to syntactic parsing by implementing a similar pipelinewith CCG parsing. More specifically, we collapse MWEs to one token in trainingand test data in CCGbank, a corpus which contains sentences annotated with CCGderivations. Our collapsing algorithm however can only deal with MWEs when theyform a constituent in the data which is one of the limitations of our approach. We study the effect of collapsing training and test data. A parsing effectcan be obtained if collapsed data help the parser in its decisions and atraining effect can be obtained if training on the collapsed data improvesresults. We also collapse the gold standard and show that our modelsignificantly outperforms the baseline model on our gold standard, whichindicates that there is a training effect. We show that the baseline modelperforms significantly better on our gold standard when the data are collapsedbefore parsing than when the data are collapsed after parsing which indicatesthat there is a parsing effect. We show that these results can lead to improvedperformance on the non-collapsed standard benchmark although we fail to showthat it does so significantly. We conclude that despite the limited settings,there are noticeable improvements from using MWEs in parsing. We discuss waysin which the incorporation of MWEs into parsing can be improved and hypothesizethat this will lead to more substantial results. We finally show that turning the MWE recognition part of the pipeline into anexperimental part is a useful thing to do as we obtain different results withdifferent recognizers.
arxiv-10500-128 | Identifying Cover Songs Using Information-Theoretic Measures of Similarity | http://arxiv.org/pdf/1407.2433v3.pdf | author:Peter Foster, Simon Dixon, Anssi Klapuri category:cs.IR cs.LG stat.ML published:2014-07-09 summary:This paper investigates methods for quantifying similarity between audiosignals, specifically for the task of of cover song detection. We consider aninformation-theoretic approach, where we compute pairwise measures ofpredictability between time series. We compare discrete-valued approachesoperating on quantised audio features, to continuous-valued approaches. In thediscrete case, we propose a method for computing the normalised compressiondistance, where we account for correlation between time series. In thecontinuous case, we propose to compute information-based measures of similarityas statistics of the prediction error between time series. We evaluate ourmethods on two cover song identification tasks using a data set comprised of300 Jazz standards and using the Million Song Dataset. For both datasets, weobserve that continuous-valued approaches outperform discrete-valuedapproaches. We consider approaches to estimating the normalised compressiondistance (NCD) based on string compression and prediction, where we observethat our proposed normalised compression distance with alignment (NCDA)improves average performance over NCD, for sequential compression algorithms.Finally, we demonstrate that continuous-valued distances may be combined toimprove performance with respect to baseline approaches. Using a large-scalefilter-and-refine approach, we demonstrate state-of-the-art performance forcover song identification using the Million Song Dataset.
arxiv-10500-129 | Robust Visual Knowledge Transfer via EDA | http://arxiv.org/pdf/1505.04382v1.pdf | author:Lei Zhang, David Zhang category:cs.CV published:2015-05-17 summary:We address the problem of visual knowledge adaptation by leveraging labeledpatterns from the source domain and a very limited number of labeled instancesin target domain to learn a robust classifier for visual categorization. Weintroduce a new semi-supervised cross-domain network learning framework,referred to as Extreme Domain Adaptation (EDA), that allows us tosimultaneously learn a category transformation and an extreme classifier byminimizing the L(2,1)-norm of the output weights and the learning error, inwhich the network output weights can be analytically determined. The unlabeledtarget data, as useful knowledge, is also learned as a fidelity term byminimizing the matching error between the extreme classifier and a baseclassifier to guarantee the stability during cross domain learning, into whichmany existing classifiers can be readily incorporated as base classifiers.Additionally, a manifold regularization with Laplacian graph is incorporatedinto EDA, such that it is beneficial to semi-supervised learning. Under theEDA, we also propose an extensive model learned with multiple views.Experiments on three visual data sets for video event recognition and objectrecognition, respectively, demonstrate that our EDA outperforms existingcross-domain learning methods.
arxiv-10500-130 | Evolutionary Cost-sensitive Extreme Learning Machine and Subspace Extension | http://arxiv.org/pdf/1505.04373v1.pdf | author:Lei Zhang, David Zhang category:cs.CV published:2015-05-17 summary:Conventional extreme learning machines solve a Moore-Penrose generalizedinverse of hidden layer activated matrix and analytically determine the outputweights to achieve generalized performance, by assuming the same loss fromdifferent types of misclassification. The assumption may not hold incost-sensitive recognition tasks, such as face recognition based access controlsystem, where misclassifying a stranger as a family member and allowed to enterthe house may result in more serious disaster than misclassifying a familymember as a stranger and not allowed to enter. Though recent cost-sensitivelearning can reduce the total loss with a given cost matrix that quantifies howsevere one type of mistake against another type of mistake, in many realisticcases the cost matrix is unknown to users. Motivated by these concerns, thispaper proposes an evolutionary cost-sensitive extreme learning machine(ECSELM), with the following merits: 1) to our best knowledge, it is the firstproposal of cost-sensitive ELM; 2) it well addresses the open issue of how todefine the cost matrix in cost-sensitive learning tasks; 3) an evolutionarybacktracking search algorithm is induced for adaptive cost matrix optimization.Extensively, an ECSLDA method is generalized by coupling with cost-sensitivesubspace learning. Experiments in a variety of cost-sensitive tasks welldemonstrate the efficiency and effectiveness of the proposed approaches,specifically, 5%~10% improvements in classification are obtained on severaldatasets compared with ELMs; the computational efficiency is also 10 timesfaster than cost-sensitive subspace learning methods.
arxiv-10500-131 | Shrinkage degree in $L_2$-re-scale boosting for regression | http://arxiv.org/pdf/1505.04369v1.pdf | author:Lin Xu, Shaobo Lin, Yao Wang, Zongben Xu category:cs.LG published:2015-05-17 summary:Re-scale boosting (RBoosting) is a variant of boosting which can essentiallyimprove the generalization performance of boosting learning. The key feature ofRBoosting lies in introducing a shrinkage degree to re-scale the ensembleestimate in each gradient-descent step. Thus, the shrinkage degree determinesthe performance of RBoosting. The aim of this paper is to develop a concrete analysis concerning how todetermine the shrinkage degree in $L_2$-RBoosting. We propose two feasible waysto select the shrinkage degree. The first one is to parameterize the shrinkagedegree and the other one is to develope a data-driven approach of it. Afterrigorously analyzing the importance of the shrinkage degree in $L_2$-RBoostinglearning, we compare the pros and cons of the proposed methods. We find thatalthough these approaches can reach the same learning rates, the structure ofthe final estimate of the parameterized approach is better, which sometimesyields a better generalization capability when the number of sample is finite.With this, we recommend to parameterize the shrinkage degree of$L_2$-RBoosting. To this end, we present an adaptive parameter-selectionstrategy for shrinkage degree and verify its feasibility through boththeoretical analysis and numerical verification. The obtained results enhance the understanding of RBoosting and further giveguidance on how to use $L_2$-RBoosting for regression tasks.
arxiv-10500-132 | Learning Deconvolution Network for Semantic Segmentation | http://arxiv.org/pdf/1505.04366v1.pdf | author:Hyeonwoo Noh, Seunghoon Hong, Bohyung Han category:cs.CV published:2015-05-17 summary:We propose a novel semantic segmentation algorithm by learning adeconvolution network. We learn the network on top of the convolutional layersadopted from VGG 16-layer net. The deconvolution network is composed ofdeconvolution and unpooling layers, which identify pixel-wise class labels andpredict segmentation masks. We apply the trained network to each proposal in aninput image, and construct the final semantic segmentation map by combining theresults from all proposals in a simple manner. The proposed algorithm mitigatesthe limitations of the existing methods based on fully convolutional networksby integrating deep deconvolution network and proposal-wise prediction; oursegmentation method typically identifies detailed structures and handlesobjects in multiple scales naturally. Our network demonstrates outstandingperformance in PASCAL VOC 2012 dataset, and we achieve the best accuracy(72.5%) among the methods trained with no external data through ensemble withthe fully convolutional network.
arxiv-10500-133 | Salient Structure Detection by Context-Guided Visual Search | http://arxiv.org/pdf/1505.04364v1.pdf | author:Kai-Fu Yang, Hui Li, Chao-Yi Li, Yong-Jie Li category:cs.CV published:2015-05-17 summary:We define the task of salient structure (SS) detection to unify thesaliency-related tasks like fixation prediction, salient object detection, andother detection of structures of interest. In this study, we propose a unifiedframework for SS detection by modeling the two-pathway-based guided searchstrategy of biological vision. Firstly, context-based spatial prior (CBSP) isextracted based on the layout of edges in the given scene along a fast visualpathway, called non-selective pathway. This is a rough and non-selectiveestimation of the locations where the potential SSs present. Secondly, anotherflow of local feature extraction is executed in parallel along the selectivepathway. Finally, Bayesian inference is used to integrate local cues guided byCBSP, and to predict the exact locations of SSs in the input scene. Theproposed model is invariant to size and features of objects. Experimentalresults on four datasets (two fixation prediction datasets and two salientobject datasets) demonstrate that our system achieves competitive performancefor SS detection (i.e., both the tasks of fixation prediction and salientobject detection) comparing to the state-of-the-art methods.
arxiv-10500-134 | Local identifiability of $l_1$-minimization dictionary learning: a sufficient and almost necessary condition | http://arxiv.org/pdf/1505.04363v1.pdf | author:Siqi Wu, Bin Yu category:stat.ML published:2015-05-17 summary:We study the theoretical properties of learning a dictionary from a set of$N$ signals $\mathbf x_i\in \mathbb R^K$ for $i=1,...,N$ via$l_1$-minimization. We assume that the signals $\mathbf x_i$'s are generated as$i.i.d.$ random linear combinations of the $K$ atoms from a complete referencedictionary $\mathbf D_0 \in \mathbb R^{K\times K}$. For the random linearcoefficients, we consider two generative models: the $s$-sparse Gaussian modelwith $s = 1,..,K$, and the Bernoulli($p$)-Gaussian model with $p\in (0,1]$.First, for the population case and under each of the two generative models, weestablish a sufficient and almost necessary condition for the referencedictionary $\mathbf D_0$ to be locally identifiable, i.e. a local minimum ofthe expected $l_1$-norm objective function. Our condition covers both thesparse and dense cases of signal generation, and significantly improves thesufficient condition by Gribonval and Schnass (2010). It fully describes theinteraction between the collinearity of dictionary atoms $\mathbf M_0 = \mathbfD_0^T\mathbf D_0$, and the sparsity parameter $s$ or $p$ of the randomcoefficients in achieving local identifiability. We also provide sharp andeasy-to-compute lower and upper bounds for the quantities involved in ourconditions. With these bounds, we show that local identifiability is possiblewith sparsity level $s$ or $pK$ up to the order $O(\mu^{-2})$ for a complete$\mu$-coherent reference dictionary, i.e. a dictionary with maximum absolutecollinearity $\mu = \max_{i\neq j} \mathbf M_0[i,j]$. Moreover, our localidentifiability results also translate to the finite sample case with highprobability provided that the number of signals $N$ scales as $O(K\log K)$.
arxiv-10500-135 | Evolving Spiking Networks with Variable Resistive Memories | http://arxiv.org/pdf/1505.04357v1.pdf | author:Gerard David Howard, Larry Bull, Ben de Lacy Costello, Andrew Adamatzky, Ella Gale category:cs.NE published:2015-05-17 summary:Neuromorphic computing is a brainlike information processing paradigm thatrequires adaptive learning mechanisms. A spiking neuro-evolutionary system isused for this purpose; plastic resistive memories are implemented as synapsesin spiking neural networks. The evolutionary design process exploits parameterself-adaptation and allows the topology and synaptic weights to be evolved foreach network in an autonomous manner. Variable resistive memories are the focusof this research; each synapse has its own conductance profile which modifiesthe plastic behaviour of the device and may be altered during evolution. Thesevariable resistive networks are evaluated on a noisy robotic dynamic-rewardscenario against two static resistive memories and a system containing standardconnections only. Results indicate that the extra behavioural degrees offreedom available to the networks incorporating variable resistive memoriesenable them to outperform the comparative synapse types.
arxiv-10500-136 | Tight bounds for learning a mixture of two gaussians | http://arxiv.org/pdf/1404.4997v3.pdf | author:Moritz Hardt, Eric Price category:cs.LG cs.DS stat.ML published:2014-04-19 summary:We consider the problem of identifying the parameters of an unknown mixtureof two arbitrary $d$-dimensional gaussians from a sequence of independentrandom samples. Our main results are upper and lower bounds giving acomputationally efficient moment-based estimator with an optimal convergencerate, thus resolving a problem introduced by Pearson (1894). Denoting by$\sigma^2$ the variance of the unknown mixture, we prove that$\Theta(\sigma^{12})$ samples are necessary and sufficient to estimate eachparameter up to constant additive error when $d=1.$ Our upper bound extends toarbitrary dimension $d>1$ up to a (provably necessary) logarithmic loss in $d$using a novel---yet simple---dimensionality reduction technique. We furtheridentify several interesting special cases where the sample complexity isnotably smaller than our optimal worst-case bound. For instance, if the meansof the two components are separated by $\Omega(\sigma)$ the sample complexityreduces to $O(\sigma^2)$ and this is again optimal. Our results also apply to learning each component of the mixture up to smallerror in total variation distance, where our algorithm gives strongimprovements in sample complexity over previous work. We also extend our lowerbound to mixtures of $k$ Gaussians, showing that $\Omega(\sigma^{6k-2})$samples are necessary to estimate each parameter up to constant additive error.
arxiv-10500-137 | Hyper-parameter optimization of Deep Convolutional Networks for object recognition | http://arxiv.org/pdf/1501.07645v2.pdf | author:Sachin S. Talathi category:cs.CV cs.LG published:2015-01-30 summary:Recently sequential model based optimization (SMBO) has emerged as apromising hyper-parameter optimization strategy in machine learning. In thiswork, we investigate SMBO to identify architecture hyper-parameters of deepconvolution networks (DCNs) object recognition. We propose a simple SMBOstrategy that starts from a set of random initial DCN architectures to generatenew architectures, which on training perform well on a given dataset. Using theproposed SMBO strategy we are able to identify a number of DCN architecturesthat produce results that are comparable to state-of-the-art results on objectrecognition benchmarks.
arxiv-10500-138 | Provably Correct Active Sampling Algorithms for Matrix Column Subset Selection with Missing Data | http://arxiv.org/pdf/1505.04343v1.pdf | author:Yining Wang, Aarti Singh category:stat.ML cs.LG published:2015-05-17 summary:We consider the problem of matrix column subset selection, which selects asubset of columns from an input matrix such that the input can be wellapproximated by the span of the selected columns. Column subset selection hasbeen applied to numerous real-world data applications such as populationgenetics summarization, electronic circuits testing and recommendation systems.In many applications the complete data matrix is unavailable and one needs toselect representative columns by inspecting only a small portion of the inputmatrix. In this paper we propose the first provably correct column subsetselection algorithms for partially observed data matrices. Our proposedalgorithms exhibit different merits and drawbacks in terms of statisticalaccuracy, computational efficiency, sample complexity and sampling schemes,which provides a nice exploration of the tradeoff between these desiredproperties for column subset selection. The proposed methods employ the idea offeedback driven sampling and are inspired by several sampling schemespreviously introduced for low-rank matrix approximation tasks [DMM08, FKV04,DV06, KS14]. Our analysis shows that two of the proposed algorithms enjoy arelative error bound, which is preferred for column subset selection and matrixapproximation purposes. We also demonstrate through both theoretical andempirical analysis the power of feedback driven sampling compared to uniformrandom sampling on input matrices with highly correlated columns.
arxiv-10500-139 | Efficient Approximations for the Marginal Likelihood of Incomplete Data Given a Bayesian Network | http://arxiv.org/pdf/1302.3567v2.pdf | author:David Maxwell Chickering, David Heckerman category:cs.LG cs.AI stat.ML published:2013-02-13 summary:We discuss Bayesian methods for learning Bayesian networks when data sets areincomplete. In particular, we examine asymptotic approximations for themarginal likelihood of incomplete data given a Bayesian network. We considerthe Laplace approximation and the less accurate but more efficient BIC/MDLapproximation. We also consider approximations proposed by Draper (1993) andCheeseman and Stutz (1995). These approximations are as efficient as BIC/MDL,but their accuracy has not been studied in any depth. We compare the accuracyof these approximations under the assumption that the Laplace approximation isthe most accurate. In experiments using synthetic data generated from discretenaive-Bayes models having a hidden root node, we find that the CS measure isthe most accurate.
arxiv-10500-140 | Asymptotic Model Selection for Directed Networks with Hidden Variables | http://arxiv.org/pdf/1302.3580v2.pdf | author:Dan Geiger, David Heckerman, Christopher Meek category:cs.LG cs.AI stat.ML published:2013-02-13 summary:We extend the Bayesian Information Criterion (BIC), an asymptoticapproximation for the marginal likelihood, to Bayesian networks with hiddenvariables. This approximation can be used to select models given large samplesof data. The standard BIC as well as our extension punishes the complexity of amodel according to the dimension of its parameters. We argue that the dimensionof a Bayesian network with hidden variables is the rank of the Jacobian matrixof the transformation between the parameters of the network and the parametersof the observable variables. We compute the dimensions of several networksincluding the naive Bayes model with a hidden root node.
arxiv-10500-141 | Structure and Parameter Learning for Causal Independence and Causal Interaction Models | http://arxiv.org/pdf/1302.1561v2.pdf | author:Christopher Meek, David Heckerman category:cs.AI cs.LG published:2013-02-06 summary:This paper discusses causal independence models and a generalization of thesemodels called causal interaction models. Causal interaction models are modelsthat have independent mechanisms where a mechanism can have several causes. Inaddition to introducing several particular types of causal interaction models,we show how we can apply the Bayesian approach to learning causal interactionmodels obtaining approximate posterior distributions for the models and obtainMAP and ML estimates for the parameters. We illustrate the approach with asimulation study of learning model posteriors.
arxiv-10500-142 | A Bayesian Approach to Learning Bayesian Networks with Local Structure | http://arxiv.org/pdf/1302.1528v2.pdf | author:David Maxwell Chickering, David Heckerman, Christopher Meek category:cs.LG cs.AI stat.ML published:2013-02-06 summary:Recently several researchers have investigated techniques for using data tolearn Bayesian networks containing compact representations for the conditionalprobability distributions (CPDs) stored at each node. The majority of this workhas concentrated on using decision-tree representations for the CPDs. Inaddition, researchers typically apply non-Bayesian (or asymptotically Bayesian)scoring functions such as MDL to evaluate the goodness-of-fit of networks tothe data. In this paper we investigate a Bayesian approach to learning Bayesiannetworks that contain the more general decision-graph representations of theCPDs. First, we describe how to evaluate the posterior probability that is, theBayesian score of such a network, given a database of observed cases. Second,we describe various search spaces that can be used, in conjunction with ascoring function and a search procedure, to identify one or more high-scoringnetworks. Finally, we present an experimental evaluation of the search spaces,using a greedy algorithm and a Bayesian scoring function.
arxiv-10500-143 | Learning Mixtures of DAG Models | http://arxiv.org/pdf/1301.7415v2.pdf | author:Bo Thiesson, Christopher Meek, David Maxwell Chickering, David Heckerman category:cs.LG cs.AI stat.ML published:2013-01-30 summary:We describe computationally efficient methods for learning mixtures in whicheach component is a directed acyclic graphical model (mixtures of DAGs orMDAGs). We argue that simple search-and-score algorithms are infeasible for avariety of problems, and introduce a feasible approach in which parameter andstructure search is interleaved and expected data is treated as real data. Ourapproach can be viewed as a combination of (1) the Cheeseman--Stutz asymptoticapproximation for model posterior probability and (2) theExpectation--Maximization algorithm. We evaluate our procedure for selectingamong MDAGs on synthetic and real examples.
arxiv-10500-144 | An Experimental Comparison of Several Clustering and Initialization Methods | http://arxiv.org/pdf/1301.7401v2.pdf | author:Marina Meila, David Heckerman category:cs.LG stat.ML published:2013-01-30 summary:We examine methods for clustering in high dimensions. In the first part ofthe paper, we perform an experimental comparison between three batch clusteringalgorithms: the Expectation-Maximization (EM) algorithm, a winner take allversion of the EM algorithm reminiscent of the K-means algorithm, andmodel-based hierarchical agglomerative clustering. We learn naive-Bayes modelswith a hidden root node, using high-dimensional discrete-variable data sets(both real and synthetic). We find that the EM algorithm significantlyoutperforms the other methods, and proceed to investigate the effect of variousinitialization schemes on the final solution produced by the EM algorithm. Theinitializations that we consider are (1) parameters sampled from anuninformative prior, (2) random perturbations of the marginal distribution ofthe data, and (3) the output of hierarchical agglomerative clustering. Althoughthe methods are substantially different, they lead to learned models that arestrikingly similar in quality.
arxiv-10500-145 | Inferring Informational Goals from Free-Text Queries: A Bayesian Approach | http://arxiv.org/pdf/1301.7382v2.pdf | author:David Heckerman, Eric J. Horvitz category:cs.IR cs.AI cs.CL published:2013-01-30 summary:People using consumer software applications typically do not use technicaljargon when querying an online database of help topics. Rather, they attempt tocommunicate their goals with common words and phrases that describe softwarefunctionality in terms of structure and objects they understand. We describe aBayesian approach to modeling the relationship between words in a user's queryfor assistance and the informational goals of the user. After reviewing thegeneral method, we describe several extensions that center on integratingadditional distinctions and structure about language usage and user goals intothe Bayesian models.
arxiv-10500-146 | Fast Learning from Sparse Data | http://arxiv.org/pdf/1301.6685v2.pdf | author:David Maxwell Chickering, David Heckerman category:cs.LG stat.ML published:2013-01-23 summary:We describe two techniques that significantly improve the running time ofseveral standard machine-learning algorithms when data is sparse. The firsttechnique is an algorithm that effeciently extracts one-way and two-waycounts--either real or expected-- from discrete data. Extracting such counts isa fundamental step in learning algorithms for constructing a variety of modelsincluding decision trees, decision graphs, Bayesian networks, and naive-Bayesclustering models. The second technique is an algorithm that efficientlyperforms the E-step of the EM algorithm (i.e. inference) when applied to anaive-Bayes clustering model. Using real-world data sets, we demonstrate adramatic decrease in running time for algorithms that incorporate thesetechniques.
arxiv-10500-147 | Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions | http://arxiv.org/pdf/1301.6697v2.pdf | author:Dan Geiger, David Heckerman category:cs.LG stat.ML published:2013-01-23 summary:We show that the only parameter prior for complete Gaussian DAG models thatsatisfies global parameter independence, complete model equivalence, and someweak regularity assumptions, is the normal-Wishart distribution. Our analysisis based on the following new characterization of the Wishart distribution: letW be an n x n, n >= 3, positive-definite symmetric matrix of random variablesand f(W) be a pdf of W. Then, f(W) is a Wishart distribution if and only ifW_{11}-W_{12}W_{22}^{-1}W_{12}' is independent of {W_{12}, W_{22}} for everyblock partitioning W_{11}, W_{12}, W_{12}', W_{22} of W. Similarcharacterizations of the normal and normal-Wishart distributions are providedas well. We also show how to construct a prior for every DAG model over X fromthe prior of a single regression model.
arxiv-10500-148 | An MDP-based Recommender System | http://arxiv.org/pdf/1301.0600v2.pdf | author:Guy Shani, Ronen I. Brafman, David Heckerman category:cs.LG cs.AI cs.IR published:2012-12-12 summary:Typical Recommender systems adopt a static view of the recommendation processand treat it as a prediction problem. We argue that it is more appropriate toview the problem of generating recommendations as a sequential decision problemand, consequently, that Markov decision processes (MDP) provide a moreappropriate model for Recommender systems. MDPs introduce two benefits: theytake into account the long-term effects of each recommendation, and they takeinto account the expected value of each recommendation. To succeed in practice,an MDP-based Recommender system must employ a strong initial model; and thebulk of this paper is concerned with the generation of such a model. Inparticular, we suggest the use of an n-gram predictive model for generating theinitial MDP. Our n-gram model induces a Markov-chain model of user behaviorwhose predictive accuracy is greater than that of existing predictive models.We describe our predictive model in detail and evaluate its performance on realdata. In addition, we show how the model can be used in an MDP-basedRecommender system.
arxiv-10500-149 | Continuous Time Dynamic Topic Models | http://arxiv.org/pdf/1206.3298v2.pdf | author:Chong Wang, David Blei, David Heckerman category:cs.IR cs.LG stat.ML published:2012-06-13 summary:In this paper, we develop the continuous time dynamic topic model (cDTM). ThecDTM is a dynamic topic model that uses Brownian motion to model the latenttopics through a sequential collection of documents, where a "topic" is apattern of word use that we expect to evolve over the course of the collection.We derive an efficient variational approximate inference algorithm that takesadvantage of the sparsity of observations in text, a property that lets useasily handle many time points. In contrast to the cDTM, the originaldiscrete-time dynamic topic model (dDTM) requires that time be discretized.Moreover, the complexity of variational inference for the dDTM grows quickly astime granularity increases, a drawback which limits fine-graineddiscretization. We demonstrate the cDTM on two news corpora, reporting bothpredictive perplexity and the novel task of time stamp prediction.
arxiv-10500-150 | Learning Exponential Families in High-Dimensions: Strong Convexity and Sparsity | http://arxiv.org/pdf/0911.0054v2.pdf | author:Sham M. Kakade, Ohad Shamir, Karthik Sridharan, Ambuj Tewari category:cs.LG stat.ML I.2.6 published:2009-10-31 summary:The versatility of exponential families, along with their attendant convexityproperties, make them a popular and effective statistical model. A centralissue is learning these models in high-dimensions, such as when there is somesparsity pattern of the optimal parameter. This work characterizes a certainstrong convexity property of general exponential families, which allow theirgeneralization ability to be quantified. In particular, we show how thisproperty can be used to analyze generic exponential families under L_1regularization.
arxiv-10500-151 | A type-theoretical approach to Universal Grammar | http://arxiv.org/pdf/1505.04313v1.pdf | author:Erkki Luuk category:cs.CL math.LO 03 published:2015-05-16 summary:The idea of Universal Grammar (UG) as the hypothetical linguistic structureshared by all human languages harkens back at least to the 13th century. Thebest known modern elaborations of the idea are due to Chomsky. Following adevastating critique from theoretical, typological and field linguistics, theseelaborations, the idea of UG itself and the more general idea of languageuniversals stand untenable and are largely abandoned. The proposal tackles thehypothetical contents of UG using dependent and polymorphic type theory in aframework very different from the Chomskyan ones. We introduce a type logic fora precise, universal and parsimonious representation of natural languagemorphosyntax and compositional semantics. The logic handles grammaticalambiguity (with polymorphic types), selectional restrictions and diverse kindsof anaphora (with dependent types), and features a partly universal set ofmorphosyntactic types (by the Curry-Howard isomorphism).
arxiv-10500-152 | Robust Real-time Extraction of Fiducial Facial Feature Points using Haar-like Features | http://arxiv.org/pdf/1505.04286v1.pdf | author:Harry Commin category:cs.CV published:2015-05-16 summary:In this paper, we explore methods of robustly extracting fiducial facialfeature points - an important process for numerous facial image processingtasks. We consider various methods to first detect face, then facial featuresand finally salient facial feature points. Colour-based models are analysed andtheir overall unsuitability for this task is summarised. The bulk of the reportis then dedicated to proposing a learning-based method centred on theViola-Jones algorithm. The specific difficulties and considerations relating tofeature point detection are laid out in this context and a novel approach isestablished to address these issues. On a sequence of clear and unobstructedface images, our proposed system achieves average detection rates of over 90%.Then, using a more varied sample dataset, we identify some possible areas forfuture development of our system.
arxiv-10500-153 | The color of smiling: computational synaesthesia of facial expressions | http://arxiv.org/pdf/1505.04260v1.pdf | author:Vittorio Cuculo, Raffaella Lanzarotti, Giuseppe Boccignone category:cs.CV published:2015-05-16 summary:This note gives a preliminary account of the transcoding or rechannelingproblem between different stimuli as it is of interest for the naturalinteraction or affective computing fields. By the consideration of a simpleexample, namely the color response of an affective lamp to a sensed facialexpression, we frame the problem within an information- theoretic perspective.A full justification in terms of the Information Bottleneck principle promotesa latent affective space, hitherto surmised as an appealing and intuitivesolution, as a suitable mediator between the different stimuli.
arxiv-10500-154 | A New Perspective on Boosting in Linear Regression via Subgradient Optimization and Relatives | http://arxiv.org/pdf/1505.04243v1.pdf | author:Robert M. Freund, Paul Grigas, Rahul Mazumder category:math.ST cs.LG math.OC stat.ML stat.TH published:2015-05-16 summary:In this paper we analyze boosting algorithms in linear regression from a newperspective: that of modern first-order methods in convex optimization. We showthat classic boosting algorithms in linear regression, namely the incrementalforward stagewise algorithm (FS$_\varepsilon$) and least squares boosting(LS-Boost($\varepsilon$)), can be viewed as subgradient descent to minimize theloss function defined as the maximum absolute correlation between the featuresand residuals. We also propose a modification of FS$_\varepsilon$ that yieldsan algorithm for the Lasso, and that may be easily extended to an algorithmthat computes the Lasso path for different values of the regularizationparameter. Furthermore, we show that these new algorithms for the Lasso mayalso be interpreted as the same master algorithm (subgradient descent), appliedto a regularized version of the maximum absolute correlation loss function. Wederive novel, comprehensive computational guarantees for several boostingalgorithms in linear regression (including LS-Boost($\varepsilon$) andFS$_\varepsilon$) by using techniques of modern first-order methods in convexoptimization. Our computational guarantees inform us about the statisticalproperties of boosting algorithms. In particular they provide, for the firsttime, a precise theoretical description of the amount of data-fidelity andregularization imparted by running a boosting algorithm with a prespecifiedlearning rate for a fixed but arbitrary number of iterations, for any dataset.
arxiv-10500-155 | Automatic Photo Adjustment Using Deep Neural Networks | http://arxiv.org/pdf/1412.7725v2.pdf | author:Zhicheng Yan, Hao Zhang, Baoyuan Wang, Sylvain Paris, Yizhou Yu category:cs.CV cs.GR cs.LG published:2014-12-24 summary:Photo retouching enables photographers to invoke dramatic visual impressionsby artistically enhancing their photos through stylistic color and toneadjustments. However, it is also a time-consuming and challenging task thatrequires advanced skills beyond the abilities of casual photographers. Using anautomated algorithm is an appealing alternative to manual work but such analgorithm faces many hurdles. Many photographic styles rely on subtleadjustments that depend on the image content and even its semantics. Further,these adjustments are often spatially varying. Because of thesecharacteristics, existing automatic algorithms are still limited and cover onlya subset of these challenges. Recently, deep machine learning has shown uniqueabilities to address hard problems that resisted machine algorithms for long.This motivated us to explore the use of deep learning in the context of photoediting. In this paper, we explain how to formulate the automatic photoadjustment problem in a way suitable for this approach. We also introduce animage descriptor that accounts for the local semantics of an image. Ourexperiments demonstrate that our deep learning formulation applied using thesedescriptors successfully capture sophisticated photographic styles. Inparticular and unlike previous techniques, it can model local adjustments thatdepend on the image semantics. We show on several examples that this yieldsresults that are qualitatively and quantitatively better than previous work.
arxiv-10500-156 | HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition | http://arxiv.org/pdf/1410.0736v4.pdf | author:Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis DeCoste, Wei Di, Yizhou Yu category:cs.CV cs.LG cs.NE published:2014-10-03 summary:In image classification, visual separability between different objectcategories is highly uneven, and some categories are more difficult todistinguish than others. Such difficult categories demand more dedicatedclassifiers. However, existing deep convolutional neural networks (CNN) aretrained as flat N-way classifiers, and few efforts have been made to leveragethe hierarchical structure of categories. In this paper, we introducehierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a categoryhierarchy. An HD-CNN separates easy classes using a coarse category classifierwhile distinguishing difficult classes using fine category classifiers. DuringHD-CNN training, component-wise pretraining is followed by global finetuningwith a multinomial logistic loss regularized by a coarse category consistencyterm. In addition, conditional executions of fine category classifiers andlayer parameter compression make HD-CNNs scalable for large-scale visualrecognition. We achieve state-of-the-art results on both CIFAR100 andlarge-scale ImageNet 1000-class benchmark datasets. In our experiments, webuild up three different HD-CNNs and they lower the top-1 error of the standardCNNs by 2.65%, 3.1% and 1.1%, respectively.
arxiv-10500-157 | An Analysis of Active Learning With Uniform Feature Noise | http://arxiv.org/pdf/1505.04215v1.pdf | author:Aaditya Ramdas, Barnabas Poczos, Aarti Singh, Larry Wasserman category:stat.ML cs.AI cs.LG math.ST stat.TH published:2015-05-15 summary:In active learning, the user sequentially chooses values for feature $X$ andan oracle returns the corresponding label $Y$. In this paper, we consider theeffect of feature noise in active learning, which could arise either because$X$ itself is being measured, or it is corrupted in transmission to the oracle,or the oracle returns the label of a noisy version of the query point. Instatistics, feature noise is known as "errors in variables" and has beenstudied extensively in non-active settings. However, the effect of featurenoise in active learning has not been studied before. We consider thewell-known Berkson errors-in-variables model with additive uniform noise ofwidth $\sigma$. Our simple but revealing setting is that of one-dimensional binaryclassification setting where the goal is to learn a threshold (point where theprobability of a $+$ label crosses half). We deal with regression functionsthat are antisymmetric in a region of size $\sigma$ around the threshold andalso satisfy Tsybakov's margin condition around the threshold. We prove minimaxlower and upper bounds which demonstrate that when $\sigma$ is smaller than theminimiax active/passive noiseless error derived in \cite{CN07}, then noise hasno effect on the rates and one achieves the same noiseless rates. For larger$\sigma$, the \textit{unflattening} of the regression function on convolutionwith uniform noise, along with its local antisymmetry around the threshold,together yield a behaviour where noise \textit{appears} to be beneficial. Ourkey result is that active learning can buy significant improvement over apassive strategy even in the presence of feature noise.
arxiv-10500-158 | Algorithmic Connections Between Active Learning and Stochastic Convex Optimization | http://arxiv.org/pdf/1505.04214v1.pdf | author:Aaditya Ramdas, Aarti Singh category:cs.LG cs.AI math.OC stat.ML published:2015-05-15 summary:Interesting theoretical associations have been established by recent papersbetween the fields of active learning and stochastic convex optimization due tothe common role of feedback in sequential querying mechanisms. In this paper,we continue this thread in two parts by exploiting these relations for thefirst time to yield novel algorithms in both fields, further motivating thestudy of their intersection. First, inspired by a recent optimization algorithmthat was adaptive to unknown uniform convexity parameters, we present a newactive learning algorithm for one-dimensional thresholds that can yield minimaxrates by adapting to unknown noise parameters. Next, we show that one canperform $d$-dimensional stochastic minimization of smooth uniformly convexfunctions when only granted oracle access to noisy gradient signs along anycoordinate instead of real-valued gradients, by using a simple randomizedcoordinate descent procedure where each line search can be solved by$1$-dimensional active learning, provably achieving the same error convergencerate as having the entire real-valued gradient. Combining these two partsyields an algorithm that solves stochastic convex optimization of uniformlyconvex and smooth functions using only noisy gradient signs by repeatedlyperforming active learning, achieves optimal rates and is adaptive to allunknown convexity and smoothness parameters.
arxiv-10500-159 | Discontinuous Piecewise Polynomial Neural Networks | http://arxiv.org/pdf/1505.04211v1.pdf | author:John Loverich category:cs.NE published:2015-05-15 summary:An artificial neural network is presented where each link is represented by agrid of weights defining a series of piecewise polynomial functions withdiscontinuities between each polynomial. The polynomial order ranges from firstto fifth order. The unit averages the input values from each link. A backpropagation technique that works with discontinuous link functions andactivation functions is presented. The use of discontinuous links functionmeans that only a subset of the network is active for a given input and so onlya subset of the network is trained for a particular training example. Unitdropout is used for regularization and a parameter free weight update is used.Better performance is obtained by moving from piecewise linear links topiecewise quadratic and higher order links. The algorithm is tested on theMNIST data set, using multiple autoencoders, with good results.
arxiv-10500-160 | Arabic Inquiry-Answer Dialogue Acts Annotation Schema | http://arxiv.org/pdf/1505.04197v1.pdf | author:AbdelRahim A. Elmadany, Sherif M. Abdou, Mervat Gheith category:cs.CL published:2015-05-15 summary:We present an annotation schema as part of an effort to create a manuallyannotated corpus for Arabic dialogue language understanding including spokendialogue and written "chat" dialogue for inquiry-answer domain. The proposedschema handles mainly the request and response acts that occurs frequently ininquiry-answer debate conversations expressing request services, suggests, andoffers. We applied the proposed schema on 83 Arabic inquiry-answer dialogues.
arxiv-10500-161 | Stabilizing Value Iteration with and without Approximation Errors | http://arxiv.org/pdf/1412.5675v2.pdf | author:Ali Heydari category:cs.SY math.OC stat.ML published:2014-12-17 summary:Adaptive optimal control using value iteration (VI) initiated from astabilizing policy is theoretically analyzed in various aspects including thecontinuity of the result, the stability of the system operated using anysingle/constant resulting control policy, the stability of the system operatedusing the evolving/time-varying control policy, the convergence of thealgorithm, and the optimality of the limit function. Afterwards, the effect ofpresence of approximation errors in the involved function approximationprocesses is incorporated and another set of results for boundedness of theapproximate VI as well as stability of the system operated under the resultsfor both cases of applying a single policy or an evolving policy are derived. Afeature of the presented results is providing estimations of the region ofattraction so that if the initial condition is within the region, the wholetrajectory will remain inside it and hence, the function approximation resultswill be reliable.
arxiv-10500-162 | Theoretical and Numerical Analysis of Approximate Dynamic Programming with Approximation Errors | http://arxiv.org/pdf/1412.6095v3.pdf | author:Ali Heydari category:cs.SY cs.LG math.OC stat.ML published:2014-12-18 summary:This study is aimed at answering the famous question of how the approximationerrors at each iteration of Approximate Dynamic Programming (ADP) affect thequality of the final results considering the fact that errors at each iterationaffect the next iteration. To this goal, convergence of Value Iteration schemeof ADP for deterministic nonlinear optimal control problems with undiscountedcost functions is investigated while considering the errors existing inapproximating respective functions. The boundedness of the results around theoptimal solution is obtained based on quantities which are known in a generaloptimal control problem and assumptions which are verifiable. Moreover, sincethe presence of the approximation errors leads to the deviation of the resultsfrom optimality, sufficient conditions for stability of the system operated bythe result obtained after a finite number of value iterations, along with anestimation of its region of attraction, are derived in terms of a calculableupper bound of the control approximation error. Finally, the process ofimplementation of the method on an orbital maneuver problem is investigatedthrough which the assumptions made in the theoretical developments are verifiedand the sufficient conditions are applied for guaranteeing stability and nearoptimality.
arxiv-10500-163 | Reinforcement Learning applied to Single Neuron | http://arxiv.org/pdf/1505.04150v1.pdf | author:Zhipeng Wang, Mingbo Cai category:cs.AI cs.NE published:2015-05-15 summary:This paper extends the reinforcement learning ideas into the multi-agentssystem, which is far more complicated than the previously studied single-agentsystem. We studied two different multi-agents systems. One is thefully-connected neural network consists of multiple single neurons. Another oneis the simplified mechanical arm system which is controlled by multipleneurons. We suppose that each neuron is like an agent and it can do Gibbssampling of the posterior probability of stimulus features. The policy isoptimized in a way that the cumulative global rewards are maximized. Thealgorithm for the second system is based on the same idea but we incorporatethe physics model into the constraints. The simulation results show that forthe first system our algorithm converges well. For the second system it doesnot converge well in a reasonable simulation time length. In summary, we tookthe initial endeavor to study the reinforcement learning for multi-agentssystem. The computational complexity is always an issue and significant amountof works have to be done in order to better understand the problem.
arxiv-10500-164 | Dense Semantic Correspondence where Every Pixel is a Classifier | http://arxiv.org/pdf/1505.04143v1.pdf | author:Hilton Bristow, Jack Valmadre, Simon Lucey category:cs.CV published:2015-05-15 summary:Determining dense semantic correspondences across objects and scenes is adifficult problem that underpins many higher-level computer vision algorithms.Unlike canonical dense correspondence problems which consider images that arespatially or temporally adjacent, semantic correspondence is characterized byimages that share similar high-level structures whose exact appearance andgeometry may differ. Motivated by object recognition literature and recent work on rapidlyestimating linear classifiers, we treat semantic correspondence as aconstrained detection problem, where an exemplar LDA classifier is learned foreach pixel. LDA classifiers have two distinct benefits: (i) they exhibit higheraverage precision than similarity metrics typically used in correspondenceproblems, and (ii) unlike exemplar SVM, can output globally interpretableposterior probabilities without calibration, whilst also being significantlyfaster to train. We pose the correspondence problem as a graphical model, where the unarypotentials are computed via convolution with the set of exemplar classifiers,and the joint potentials enforce smoothly varying correspondence assignment.
arxiv-10500-165 | Consistent Algorithms for Multiclass Classification with a Reject Option | http://arxiv.org/pdf/1505.04137v1.pdf | author:Harish G. Ramaswamy, Ambuj Tewari, Shivani Agarwal category:cs.LG stat.ML published:2015-05-15 summary:We consider the problem of $n$-class classification ($n\geq 2$), where theclassifier can choose to abstain from making predictions at a given cost, say,a factor $\alpha$ of the cost of misclassification. Designing consistentalgorithms for such $n$-class classification problems with a `reject option' isthe main goal of this paper, thereby extending and generalizing previouslyknown results for $n=2$. We show that the Crammer-Singer surrogate and the onevs all hinge loss, albeit with a different predictor than the standard argmax,yield consistent algorithms for this problem when $\alpha=\frac{1}{2}$. Moreinterestingly, we design a new convex surrogate that is also consistent forthis problem when $\alpha=\frac{1}{2}$ and operates on a much lower dimensionalspace ($\log(n)$ as opposed to $n$). We also generalize all three surrogates tobe consistent for any $\alpha\in[0, \frac{1}{2}]$.
arxiv-10500-166 | Discrimination and characterization of Parkinsonian rest tremors by analyzing long-term correlations and multifractal signatures | http://arxiv.org/pdf/1504.02756v2.pdf | author:Lorenzo Livi, Alireza Sadeghian, Hamid Sadeghian category:physics.med-ph cs.CV published:2015-04-10 summary:In this paper, we analyze 48 signals of rest tremor velocity related to 12distinct subjects affected by Parkinson's disease. The subjects belong to twodifferent groups, formed by four and eight subjects with, respectively, high-and low-amplitude rest tremors. Each subject is tested in four settings, givenby combining the use of deep brain stimulation and L-DOPA medication. Wedevelop two main feature-based representations of such signals, which areobtained by considering (i) the long-term correlations and multifractalproperties, and (ii) the power spectra. The feature-based representations areinitially utilized for the purpose of characterizing the subjects underdifferent settings. In agreement with previous studies, we show that deep brainstimulation does not significantly characterize neither of the two groups,regardless of the adopted representation. On the other hand, the medicationeffect yields statistically significant differences in both high- andlow-amplitude tremor groups. We successively test several different instancesof the two feature-based representations of the signals in the setting ofsupervised classification and (nonlinear) feature transformation. We considerthree different classification problems, involving the recognition of (i) thepresence of medication, (ii) the use of deep brain stimulation, and (iii) themembership to the high- and low-amplitude tremor groups. Classification resultsshow that the use of medication can be discriminated with higher accuracy,considering many of the feature-based representations. Notably, we show thatthe best results are obtained with a parsimonious, two-dimensionalrepresentation encoding the long-term correlations and multifractal characterof the signals.
arxiv-10500-167 | Margins, Kernels and Non-linear Smoothed Perceptrons | http://arxiv.org/pdf/1505.04123v1.pdf | author:Aaditya Ramdas, Javier Peña category:cs.LG cs.AI cs.NA math.OC published:2015-05-15 summary:We focus on the problem of finding a non-linear classification function thatlies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point ofview (finding a perfect separator when one exists) and the dual point of view(giving a certificate of non-existence), with special focus on generalizationsof two classical schemes - the Perceptron (primal) and Von-Neumann (dual)algorithms. We cast our problem as one of maximizing the regularized normalizedhard-margin ($\rho$) in an RKHS and %use the Representer Theorem to rephrase itin terms of a Mahalanobis dot-product/semi-norm associated with the kernel's(normalized and signed) Gram matrix. We derive an accelerated smoothedalgorithm with a convergence rate of $\tfrac{\sqrt {\log n}}{\rho}$ given $n$separable points, which is strikingly similar to the classical kernelizedPerceptron algorithm whose rate is $\tfrac1{\rho^2}$. When no such classifierexists, we prove a version of Gordan's separation theorem for RKHSs, and give areinterpretation of negative margins. This allows us to give guarantees for aprimal-dual algorithm that halts in $\min\{\tfrac{\sqrt n}{\rho},\tfrac{\sqrt n}{\epsilon}\}$ iterations with a perfect separator in the RKHS ifthe primal is feasible or a dual $\epsilon$-certificate of near-infeasibility.
arxiv-10500-168 | Discovering Attribute Shades of Meaning with the Crowd | http://arxiv.org/pdf/1505.04117v1.pdf | author:Adriana Kovashka, Kristen Grauman category:cs.CV published:2015-05-15 summary:To learn semantic attributes, existing methods typically train onediscriminative model for each word in a vocabulary of nameable properties.However, this "one model per word" assumption is problematic: while a wordmight have a precise linguistic definition, it need not have a precise visualdefinition. We propose to discover shades of attribute meaning. Given anattribute name, we use crowdsourced image labels to discover the latent factorsunderlying how different annotators perceive the named concept. We show thatstructure in those latent factors helps reveal shades, that is, interpretationsfor the attribute shared by some group of annotators. Using these shades, wetrain classifiers to capture the primary (often subtle) variants of theattribute. The resulting models are both semantic and visually precise. Bycatering to users' interpretations, they improve attribute prediction accuracyon novel images. Shades also enable more successful attribute-based imagesearch, by providing robust personalized models for retrieving multi-attributequery results. They are widely applicable to tasks that involve describingvisual content, such as zero-shot category learning and organization of photocollections.
arxiv-10500-169 | MCODE: Multivariate Conditional Outlier Detection | http://arxiv.org/pdf/1505.04097v1.pdf | author:Charmgil Hong, Milos Hauskrecht category:cs.AI cs.LG stat.ML published:2015-05-15 summary:Outlier detection aims to identify unusual data instances that deviate fromexpected patterns. The outlier detection is particularly challenging whenoutliers are context dependent and when they are defined by unusualcombinations of multiple outcome variable values. In this paper, we develop andstudy a new conditional outlier detection approach for multivariate outcomespaces that works by (1) transforming the conditional detection to the outlierdetection problem in a new (unconditional) space and (2) defining outlierscores by analyzing the data in the new space. Our approach relies on theclassifier chain decomposition of the multi-dimensional classification problemthat lets us transform the output space into a probability vector, oneprobability for each dimension of the output space. Outlier scores applied tothese transformed vectors are then used to detect the outliers. Experiments onmultiple multi-dimensional classification problems with the different outlierinjection rates show that our methodology is robust and able to successfullyidentify outliers when outliers are either sparse (manifested in one or veryfew dimensions) or dense (affecting multiple dimensions).
arxiv-10500-170 | Optimal Low-Rank Tensor Recovery from Separable Measurements: Four Contractions Suffice | http://arxiv.org/pdf/1505.04085v1.pdf | author:Parikshit Shah, Nikhil Rao, Gongguo Tang category:stat.ML cs.IT cs.LG math.IT math.OC published:2015-05-15 summary:Tensors play a central role in many modern machine learning and signalprocessing applications. In such applications, the target tensor is usually oflow rank, i.e., can be expressed as a sum of a small number of rank onetensors. This motivates us to consider the problem of low rank tensor recoveryfrom a class of linear measurements called separable measurements. As specificexamples, we focus on two distinct types of separable measurement mechanisms(a) Random projections, where each measurement corresponds to an inner productof the tensor with a suitable random tensor, and (b) the completion problemwhere measurements constitute revelation of a random set of entries. We presenta computationally efficient algorithm, with rigorous and order-optimal samplecomplexity results (upto logarithmic factors) for tensor recovery. Our methodis based on reduction to matrix completion sub-problems and adaptation ofLeurgans' method for tensor decomposition. We extend the methodology and samplecomplexity results to higher order tensors, and experimentally validate ourtheoretical results.
arxiv-10500-171 | Coarse Ricci curvature with applications to manifold learning problem | http://arxiv.org/pdf/1410.3351v2.pdf | author:Antonio G. Ache, Micah W. Warren category:math.DG cs.LG math.MG stat.ML 53 published:2014-10-13 summary:Consider a sample of $n$ points taken i.i.d from a submanifold of Euclideanspace. This defines a metric measure space. We show that there is an explicitset of scales $t_{n}\rightarrow0$ such that a coarse Ricci curvature at scale$t_{n}$ on this metric measure space converges almost surely to the coarseRicci curvature of the underlying manifold.
arxiv-10500-172 | Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices | http://arxiv.org/pdf/1505.04073v1.pdf | author:Jie Wang, Jieping Ye category:cs.LG published:2015-05-15 summary:Multi-task feature learning (MTFL) is a powerful technique in boosting thepredictive performance by learning multiple relatedclassification/regression/clustering tasks simultaneously. However, solving theMTFL problem remains challenging when the feature dimension is extremely large.In this paper, we propose a novel screening rule---that is based on the dualprojection onto convex sets (DPC)---to quickly identify the inactivefeatures---that have zero coefficients in the solution vectors across alltasks. One of the appealing features of DPC is that: it is safe in the sensethat the detected inactive features are guaranteed to have zero coefficients inthe solution vectors across all tasks. Thus, by removing the inactive featuresfrom the training phase, we may have substantial savings in the computationalcost and memory usage without sacrificing accuracy. To the best of ourknowledge, it is the first screening rule that is applicable to sparse modelswith multiple data matrices. A key challenge in deriving DPC is to solve anonconvex problem. We show that we can solve for the global optimum efficientlyvia a properly chosen parametrization of the constraint set. Moreover, DPC hasvery low computational cost and can be integrated with any existing solvers. Wehave evaluated the proposed DPC rule on both synthetic and real data sets. Theexperiments indicate that DPC is very effective in identifying the inactivefeatures---especially for high dimensional data---which leads to a speedup upto several orders of magnitude.
arxiv-10500-173 | A Real Time Facial Expression Classification System Using Local Binary Patterns | http://arxiv.org/pdf/1505.04058v1.pdf | author:S. L. Happy, Anjith George, Aurobinda Routray category:cs.CV published:2015-05-15 summary:Facial expression analysis is one of the popular fields of research in humancomputer interaction (HCI). It has several applications in next generation userinterfaces, human emotion analysis, behavior and cognitive modeling. In thispaper, a facial expression classification algorithm is proposed which uses Haarclassifier for face detection purpose, Local Binary Patterns (LBP) histogram ofdifferent block sizes of a face image as feature vectors and classifies variousfacial expressions using Principal Component Analysis (PCA). The algorithm isimplemented in real time for expression classification since the computationalcomplexity of the algorithm is small. A customizable approach is proposed forfacial expression analysis, since the various expressions and intensity ofexpressions vary from person to person. The system uses grayscale frontal faceimages of a person to classify six basic emotions namely happiness, sadness,disgust, fear, surprise and anger.
arxiv-10500-174 | A Video Database of Human Faces under Near Infra-Red Illumination for Human Computer Interaction Aplications | http://arxiv.org/pdf/1505.04055v1.pdf | author:S L Happy, Anirban Dasgupta, Anjith George, Aurobinda Routray category:cs.CV published:2015-05-15 summary:Human Computer Interaction (HCI) is an evolving area of research for coherentcommunication between computers and human beings. Some of the importantapplications of HCI as reported in literature are face detection, face poseestimation, face tracking and eye gaze estimation. Development of algorithmsfor these applications is an active field of research. However, availability ofstandard database to validate such algorithms is insufficient. This paperdiscusses the creation of such a database created under Near Infra-Red (NIR)illumination. NIR illumination has gained its popularity for night modeapplications since prolonged exposure to Infra-Red (IR) lighting may lead tomany health issues. The database contains NIR videos of 60 subjects indifferent head orientations and with different facial expressions, facialocclusions and illumination variation. This new database can be a very valuableresource for development and evaluation of algorithms on face detection, eyedetection, head tracking, eye gaze tracking etc. in NIR lighting.
arxiv-10500-175 | Robust Facial Expression Classification Using Shape and Appearance Features | http://arxiv.org/pdf/1505.04030v1.pdf | author:S. L. Happy, Aurobinda Routray category:cs.CV published:2015-05-15 summary:Facial expression recognition has many potential applications which hasattracted the attention of researchers in the last decade. Feature extractionis one important step in expression analysis which contributes toward fast andaccurate expression recognition. This paper represents an approach of combiningthe shape and appearance features to form a hybrid feature vector. We haveextracted Pyramid of Histogram of Gradients (PHOG) as shape descriptors andLocal Binary Patterns (LBP) as appearance features. The proposed frameworkinvolves a novel approach of extracting hybrid features from active facialpatches. The active facial patches are located on the face regions whichundergo a major change during different expressions. After detection of faciallandmarks, the active patches are localized and hybrid features are calculatedfrom these patches. The use of small parts of face instead of the whole facefor extracting features reduces the computational cost and prevents theover-fitting of the features for classification. By using linear discriminantanalysis, the dimensionality of the feature is reduced which is furtherclassified by using the support vector machine (SVM). The experimental resultson two publicly available databases show promising accuracy in recognizing allexpression classes.
arxiv-10500-176 | Automatic Facial Expression Recognition Using Features of Salient Facial Patches | http://arxiv.org/pdf/1505.04026v1.pdf | author:S L Happy, Aurobinda Routray category:cs.CV published:2015-05-15 summary:Extraction of discriminative features from salient facial patches plays avital role in effective facial expression recognition. The accurate detectionof facial landmarks improves the localization of the salient patches on faceimages. This paper proposes a novel framework for expression recognition byusing appearance features of selected facial patches. A few prominent facialpatches, depending on the position of facial landmarks, are extracted which areactive during emotion elicitation. These active patches are further processedto obtain the salient patches which contain discriminative features forclassification of each pair of expressions, thereby selecting different facialpatches as salient for different pair of expression classes. One-against-oneclassification method is adopted using these features. In addition, anautomated learning-free facial landmark detection technique has been proposed,which achieves similar performances as that of other state-of-art landmarkdetection methods, yet requires significantly less execution time. The proposedmethod is found to perform well consistently in different resolutions, hence,providing a solution for expression recognition in low resolution images.Experiments on CK+ and JAFFE facial expression databases show the effectivenessof the proposed system.
arxiv-10500-177 | An Image is Worth More than a Thousand Favorites: Surfacing the Hidden Beauty of Flickr Pictures | http://arxiv.org/pdf/1505.03358v2.pdf | author:Rossano Schifanella, Miriam Redi, Luca Aiello category:cs.SI cs.CV cs.CY cs.MM published:2015-05-13 summary:The dynamics of attention in social media tend to obey power laws. Attentionconcentrates on a relatively small number of popular items and neglecting thevast majority of content produced by the crowd. Although popularity can be anindication of the perceived value of an item within its community, previousresearch has hinted to the fact that popularity is distinct from intrinsicquality. As a result, content with low visibility but high quality lurks in thetail of the popularity distribution. This phenomenon can be particularlyevident in the case of photo-sharing communities, where valuable photographerswho are not highly engaged in online social interactions contribute withhigh-quality pictures that remain unseen. We propose to use a computer visionmethod to surface beautiful pictures from the immense pool ofnear-zero-popularity items, and we test it on a large dataset ofcreative-commons photos on Flickr. By gathering a large crowdsourced groundtruth of aesthetics scores for Flickr images, we show that our method retrievesphotos whose median perceived beauty score is equal to the most popular ones,and whose average is lower by only 1.5%.
arxiv-10500-178 | Compound Poisson Processes, Latent Shrinkage Priors and Bayesian Nonconvex Penalization | http://arxiv.org/pdf/1308.6069v3.pdf | author:Zhihua Zhang, Jin Li category:stat.ML stat.ME published:2013-08-28 summary:In this paper we discuss Bayesian nonconvex penalization for sparse learningproblems. We explore a nonparametric formulation for latent shrinkageparameters using subordinators which are one-dimensional L\'{e}vy processes. Weparticularly study a family of continuous compound Poisson subordinators and afamily of discrete compound Poisson subordinators. We exemplify four specificsubordinators: Gamma, Poisson, negative binomial and squared Besselsubordinators. The Laplace exponents of the subordinators are Bernsteinfunctions, so they can be used as sparsity-inducing nonconvex penaltyfunctions. We exploit these subordinators in regression problems, yielding ahierarchical model with multiple regularization parameters. We devise ECME(Expectation/Conditional Maximization Either) algorithms to simultaneouslyestimate regression coefficients and regularization parameters. The empiricalevaluation of simulated data shows that our approach is feasible and effectivein high-dimensional data analysis.
arxiv-10500-179 | Automatic Subspace Learning via Principal Coefficients Embedding | http://arxiv.org/pdf/1411.4419v3.pdf | author:Xi Peng, Jiwen Lu, Rui Yan, Zhang Yi category:cs.CV published:2014-11-17 summary:In this paper, we address two problems in unsupervised subspace learning: 1)how to automatically identify the feature dimension of the learned subspace,and 2) how to learn the underlying subspace in the presence of grosscorruptions such as Gaussian noise. We show that these two problems are twosides of one coin, i.e. they can be solved by removing possible errors fromtraining data $\mathbf{D}\in \mathds{R}^{m\times n}$. To achieve this, wepropose a new method (called Principal Coefficients Embedding, PCE) that cansimultaneously learn a clean data set $\mathbf{D}_{0}\in \mathds{R}^{m\timesn}$ and a linear representation (denoted by $\mathbf{C}$) from $\mathbf{D}$. Byembedding $\mathbf{C}$ into a $k$-dimensional space, PCE obtains a projectionmatrix that preserves some desirable properties of inputs, where $k\ll m$ isexactly the rank of $\mathbf{C}$. PCE has three advantages: 1) it canautomatically determine the feature dimension even though data are sampled froma union of multiple linear subspaces; 2) it is robust to various noises andreal disguises; 3) it has a closed-form solution and can be calculated veryfast. Extensive experimental results show the superiority of PCE on a range ofdatabases with respect to classification accuracy, robustness and efficiency.
arxiv-10500-180 | Ensembling classification models based on phalanxes of variables with applications in drug discovery | http://arxiv.org/pdf/1303.4805v4.pdf | author:Jabed H. Tomal, William J. Welch, Ruben H. Zamar category:stat.ML stat.CO published:2013-03-20 summary:Statistical detection of a rare class of objects in a two-classclassification problem can pose several challenges. Because the class ofinterest is rare in the training data, there is relatively little informationin the known class response labels for model building. At the same time theavailable explanatory variables are often moderately high dimensional. In thefour assays of our drug-discovery application, compounds are active or notagainst a specific biological target, such as lung cancer tumor cells, andactive compounds are rare. Several sets of chemical descriptor variables fromcomputational chemistry are available to classify the active versus inactiveclass; each can have up to thousands of variables characterizing molecularstructure of the compounds. The statistical challenge is to make use of therichness of the explanatory variables in the presence of scant responseinformation. Our algorithm divides the explanatory variables into subsetsadaptively and passes each subset to a base classifier. The various baseclassifiers are then ensembled to produce one model to rank new objects bytheir estimated probabilities of belonging to the rare class of interest. Theessence of the algorithm is to choose the subsets such that variables in thesame group work well together; we call such groups phalanxes.
arxiv-10500-181 | Using Ensemble Models in the Histological Examination of Tissue Abnormalities | http://arxiv.org/pdf/1505.03932v1.pdf | author:Giancarlo Crocetti, Michael Coakley, Phil Dressner, Wanda Kellum, Tamba Lamin category:cs.CV cs.CE cs.LG published:2015-05-15 summary:Classification models for the automatic detection of abnormalities onhistological samples do exists, with an active debate on the cost associatedwith false negative diagnosis (underdiagnosis) and false positive diagnosis(overdiagnosis). Current models tend to underdiagnose, failing to recognize apotentially fatal disease. The objective of this study is to investigate the possibility ofautomatically identifying abnormalities in tissue samples through the use of anensemble model on data generated by histological examination and to minimizethe number of false negative cases.
arxiv-10500-182 | A Multivariate Biomarker for Parkinson's Disease | http://arxiv.org/pdf/1602.07264v1.pdf | author:Giancarlo Crocetti, Michael Coakley, Phil Dressner, Wanda Kellum, Tamba Lamin category:cs.LG published:2015-05-15 summary:In this study, we executed a genomic analysis with the objective of selectinga set of genes (possibly small) that would help in the detection andclassification of samples from patients affected by Parkinson Disease. Weperformed a complete data analysis and during the exploratory phase, weselected a list of differentially expressed genes. Despite their associationwith the diseased state, we could not use them as a biomarker tool. Therefore,our research was extended to include a multivariate analysis approach resultingin the identification and selection of a group of 20 genes that showed a clearpotential in detecting and correctly classify Parkinson Disease samples even inthe presence of other neurodegenerative disorders.
arxiv-10500-183 | General Riemannian SOM | http://arxiv.org/pdf/1505.03917v1.pdf | author:Jascha A. Schewtschenko category:cs.NE published:2015-05-14 summary:Kohonen's Self-Organizing Maps (SOMs) have proven to be a successfuldata-reduction method to identify the intrinsic lower-dimensional sub-manifoldof a data set that is scattered in the higher-dimensional feature space.Motivated by the possibly non-Euclidian nature of the feature space and of theintrinsic geometry of the data set, we extend the definition of classic SOMs toobtain the General Riemannian SOM (GRiSOM). We additionally provide animplementation as a proof-of-concept for geometries with constant curvature. Wefurthermore perform the analytic and numerical analysis of the stability limitsof certain (GRi)SOM configurations covering the different possible regulartessellation of the map space in each geometry. A deviation between thenumerical and analytic stability limit has been observed for the square andhexagonal Euclidean maps for very small neighbourhoods in the map space as wellas agreement in case of longer-ranged relations between the map nodes.
arxiv-10500-184 | Training generative neural networks via Maximum Mean Discrepancy optimization | http://arxiv.org/pdf/1505.03906v1.pdf | author:Gintare Karolina Dziugaite, Daniel M. Roy, Zoubin Ghahramani category:stat.ML cs.LG published:2015-05-14 summary:We consider training a deep neural network to generate samples from anunknown distribution given i.i.d. data. We frame learning as an optimizationminimizing a two-sample test statistic---informally speaking, a good generatornetwork produces samples that cause a two-sample test to fail to reject thenull hypothesis. As our two-sample test statistic, we use an unbiased estimateof the maximum mean discrepancy, which is the centerpiece of the nonparametrickernel two-sample test proposed by Gretton et al. (2012). We compare to theadversarial nets framework introduced by Goodfellow et al. (2014), in whichlearning is a two-player game between a generator network and an adversarialdiscriminator network, both trained to outwit the other. From this perspective,the MMD statistic plays the role of the discriminator. In addition to empiricalcomparisons, we prove bounds on the generalization error incurred by optimizingthe empirical MMD.
arxiv-10500-185 | Pinball Loss Minimization for One-bit Compressive Sensing | http://arxiv.org/pdf/1505.03898v1.pdf | author:Xiaolin Huang, Lei Shi, Ming Yan, Johan A. K. Suykens category:cs.IT math.IT math.NA math.OC stat.ML published:2015-05-14 summary:The one-bit quantization can be implemented by one single comparator, whichoperates at low power and a high rate. Hence one-bit compressive sensing(\emph{1bit-CS}) becomes very attractive in signal processing. When themeasurements are corrupted by noise during signal acquisition and transmission,1bit-CS is usually modeled as minimizing a loss function with a sparsityconstraint. The existing loss functions include the hinge loss and the linearloss. Though 1bit-CS can be regarded as a binary classification problem becausea one-bit measurement only provides the sign information, the choice of thehinge loss over the linear loss in binary classification is not true for1bit-CS. Many experiments show that the linear loss performs better than thehinge loss for 1bit-CS. Motivated by this observation, we consider the pinballloss, which provides a bridge between the hinge loss and the linear loss. Usingthis bridge, two 1bit-CS models and two corresponding algorithms are proposed.Pinball loss iterative hard thresholding improves the performance of the binaryiterative hard theresholding proposed in [6] and is suitable for the case whenthe sparsity of the true signal is given. Elastic-net pinball support vectormachine generalizes the passive model proposed in [11] and is suitable for thecase when the sparsity of the true signal is not given. A fast dual coordinateascent algorithm is proposed to solve the elastic-net pinball support vectormachine problem, and its convergence is proved. The numerical experimentsdemonstrate that the pinball loss, as a trade-off between the hinge loss andthe linear loss, improves the existing 1bit-CS models with better performances.
arxiv-10500-186 | Petuum: A New Platform for Distributed Machine Learning on Big Data | http://arxiv.org/pdf/1312.7651v2.pdf | author:Eric P. Xing, Qirong Ho, Wei Dai, Jin Kyu Kim, Jinliang Wei, Seunghak Lee, Xun Zheng, Pengtao Xie, Abhimanu Kumar, Yaoliang Yu category:stat.ML cs.LG cs.SY published:2013-12-30 summary:What is a systematic way to efficiently apply a wide spectrum of advanced MLprograms to industrial scale problems, using Big Models (up to 100s of billionsof parameters) on Big Data (up to terabytes or petabytes)? Modernparallelization strategies employ fine-grained operations and scheduling beyondthe classic bulk-synchronous processing paradigm popularized by MapReduce, oreven specialized graph-based execution that relies on graph representations ofML programs. The variety of approaches tends to pull systems and algorithmsdesign in different directions, and it remains difficult to find a universalplatform applicable to a wide range of ML programs at scale. We propose ageneral-purpose framework that systematically addresses data- andmodel-parallel challenges in large-scale ML, by observing that many ML programsare fundamentally optimization-centric and admit error-tolerant,iterative-convergent algorithmic solutions. This presents unique opportunitiesfor an integrative system design, such as bounded-error network synchronizationand dynamic scheduling based on ML program structure. We demonstrate theefficacy of these system designs versus well-known implementations of modern MLalgorithms, allowing ML programs to run in much less time and at considerablylarger model sizes, even on modestly-sized compute clusters.
arxiv-10500-187 | Task-Based Optimization of Computed Tomography Imaging Systems | http://arxiv.org/pdf/1505.03891v1.pdf | author:Adrian A. Sanchez category:physics.med-ph cs.CV published:2015-05-14 summary:The goal of this thesis is to provide a framework for the use of task-basedmetrics of image quality to aid in the design, implementation, and evaluationof CT image reconstruction algorithms and CT systems in general. We support theview that task-based metrics of image quality can be useful in guiding thealgorithm design and implementation process in order to yield images ofobjectively superior quality and higher utility for a given task. Further, webelieve that metrics such as the Hotelling observer (HO) SNR can be used assummary scalar metrics of image quality for the evaluation of images producedby novel reconstruction algorithms. In this work, we aim to construct a conciseand versatile formalism for image reconstruction algorithm design,implementation, and assessment. The bulk of the work focuses on linearanalytical algorithms, specifically the ubiquitous filtered back-projection(FBP) algorithm. However, due to the demonstrated importance ofoptimization-based algorithms in a wide variety of CT applications, we devoteone chapter to the characterization of noise properties in TV-based iterativereconstruction, as the understanding of image statistics in optimization-basedreconstruction is the limiting factor in applying HO metrics.
arxiv-10500-188 | Improving Image Classification with Location Context | http://arxiv.org/pdf/1505.03873v1.pdf | author:Kevin Tang, Manohar Paluri, Li Fei-Fei, Rob Fergus, Lubomir Bourdev category:cs.CV published:2015-05-14 summary:With the widespread availability of cellphones and cameras that have GPScapabilities, it is common for images being uploaded to the Internet today tohave GPS coordinates associated with them. In addition to research that triesto predict GPS coordinates from visual features, this also opens up the door toproblems that are conditioned on the availability of GPS coordinates. In thiswork, we tackle the problem of performing image classification with locationcontext, in which we are given the GPS coordinates for images in both the trainand test phases. We explore different ways of encoding and extracting featuresfrom the GPS coordinates, and show how to naturally incorporate these featuresinto a Convolutional Neural Network (CNN), the current state-of-the-art formost image classification and recognition problems. We also show how it ispossible to simultaneously learn the optimal pooling radii for a subset of ourfeatures within the CNN framework. To evaluate our model and to help promoteresearch in this area, we identify a set of location-sensitive concepts andannotate a subset of the Yahoo Flickr Creative Commons 100M dataset that hasGPS coordinates with these concepts, which we make publicly available. Byleveraging location context, we are able to achieve almost a 7% gain in meanaverage precision.
arxiv-10500-189 | Recursive Neural Networks Can Learn Logical Semantics | http://arxiv.org/pdf/1406.1827v4.pdf | author:Samuel R. Bowman, Christopher Potts, Christopher D. Manning category:cs.CL cs.LG cs.NE published:2014-06-06 summary:Tree-structured recursive neural networks (TreeRNNs) for sentence meaninghave been successful for many applications, but it remains an open questionwhether the fixed-length representations that they learn can support tasks asdemanding as logical deduction. We pursue this question by evaluating whethertwo such models---plain TreeRNNs and tree-structured neural tensor networks(TreeRNTNs)---can correctly learn to identify logical relationships such asentailment and contradiction using these representations. In our first set ofexperiments, we generate artificial data from a logical grammar and use it toevaluate the models' ability to learn to handle basic relational reasoning,recursive structures, and quantification. We then evaluate the models on themore natural SICK challenge data. Both models perform competitively on the SICKdata and generalize well in all three experiments on simulated data, suggestingthat they can learn suitable representations for logical inference in naturallanguage.
arxiv-10500-190 | Non-unique games over compact groups and orientation estimation in cryo-EM | http://arxiv.org/pdf/1505.03840v1.pdf | author:Afonso S. Bandeira, Yutong Chen, Amit Singer category:cs.CV cs.DS math.OC published:2015-05-14 summary:Let $\mathcal{G}$ be a compact group and let $f_{ij} \in L^2(\mathcal{G})$.We define the Non-Unique Games (NUG) problem as finding $g_1,\dots,g_n \in\mathcal{G}$ to minimize $\sum_{i,j=1}^n f_{ij} \left( g_i g_j^{-1}\right)$. Wedevise a relaxation of the NUG problem to a semidefinite program (SDP) bytaking the Fourier transform of $f_{ij}$ over $\mathcal{G}$, which can then besolved efficiently. The NUG framework can be seen as a generalization of thelittle Grothendieck problem over the orthogonal group and the Unique Gamesproblem and includes many practically relevant problems, such as the maximumlikelihood estimator} to registering bandlimited functions over the unit spherein $d$-dimensions and orientation estimation in cryo-Electron Microscopy.
arxiv-10500-191 | Parametric Regression on the Grassmannian | http://arxiv.org/pdf/1505.03832v1.pdf | author:Yi Hong, Nikhil Singh, Roland Kwitt, Nuno Vasconcelos, Marc Niethammer category:cs.CV published:2015-05-14 summary:We address the problem of fitting parametric curves on the Grassmann manifoldfor the purpose of intrinsic parametric regression. As customary in theliterature, we start from the energy minimization formulation of linearleast-squares in Euclidean spaces and generalize this concept to generalnonflat Riemannian manifolds, following an optimal-control point of view. Wethen specialize this idea to the Grassmann manifold and demonstrate that ityields a simple, extensible and easy-to-implement solution to the parametricregression problem. In fact, it allows us to extend the basic geodesic model to(1) a time-warped variant and (2) cubic splines. We demonstrate the utility ofthe proposed solution on different vision problems, such as shape regression asa function of age, traffic-speed estimation and crowd-counting fromsurveillance video clips. Most notably, these problems can be convenientlysolved within the same framework without any specifically-tailored steps alongthe processing pipeline.
arxiv-10500-192 | Unsupervised Object Discovery and Tracking in Video Collections | http://arxiv.org/pdf/1505.03825v1.pdf | author:Suha Kwak, Minsu Cho, Ivan Laptev, Jean Ponce, Cordelia Schmid category:cs.CV published:2015-05-14 summary:This paper addresses the problem of automatically localizing dominant objectsas spatio-temporal tubes in a noisy collection of videos with minimal or evenno supervision. We formulate the problem as a combination of two complementaryprocesses: discovery and tracking. The first one establishes correspondencesbetween prominent regions across videos, and the second one associatessuccessive similar object regions within the same video. Interestingly, ouralgorithm also discovers the implicit topology of frames associated withinstances of the same object class across different videos, a role normallyleft to supervisory information in the form of class labels in conventionalimage and video understanding methods. Indeed, as demonstrated by ourexperiments, our method can handle video collections featuring multiple objectclasses, and substantially outperforms the state of the art in colocalization,even though it tackles a broader problem with much less supervision.
arxiv-10500-193 | Statistical models and regularization strategies in statistical image reconstruction of low-dose X-ray CT: a survey | http://arxiv.org/pdf/1412.1732v3.pdf | author:Hao Zhang, Jing Wang, Jianhua Ma, Hongbing Lu, Zhengrong Liang category:physics.med-ph cs.CV published:2014-12-04 summary:Statistical image reconstruction (SIR) methods have shown potential tosubstantially improve the image quality of low-dose X-ray computed tomography(CT) as compared to the conventional filtered back-projection (FBP) method forvarious clinical tasks. According to the maximum a posterior (MAP) estimation,the SIR methods can be typically formulated by an objective function consistingof two terms: (1) data-fidelity (or equivalently, data-fitting ordata-mismatch) term modeling the statistics of projection measurements, and (2)regularization (or equivalently, prior or penalty) term reflecting priorknowledge or expectation on the characteristics of the image to bereconstructed. Existing SIR methods for low-dose CT can be divided into twogroups: (1) those that use calibrated transmitted photon counts (beforelog-transform) with penalized maximum likelihood (pML) criterion, and (2) thosethat use calibrated line-integrals (after log-transform) with penalizedweighted least-squares (PWLS) criterion. Accurate statistical modeling of theprojection measurements is a prerequisite for SIR, while the regularizationterm in the objective function also plays a critical role for successful imagereconstruction. This paper reviews several statistical models on CT projectionmeasurements and various regularization strategies incorporating priorknowledge or expected properties of the image to be reconstructed, whichtogether formulate the objective function of the SIR methods for low-dose X-rayCT.
arxiv-10500-194 | Fast and numerically stable circle fit | http://arxiv.org/pdf/1505.03795v1.pdf | author:Houssam Abdul-Rahman, Nikolai Chernov category:cs.CV published:2015-05-14 summary:We develop a new algorithm for fitting circles that does not have drawbackscommonly found in existing circle fits. Our fit achieves ultimate accuracy (tomachine precision), avoids divergence, and is numerically stable even whenfitting circles get arbitrary large. Lastly, our algorithm takes less than 10iterations to converge, on average.
arxiv-10500-195 | Rank diversity of languages: Generic behavior in computational linguistics | http://arxiv.org/pdf/1505.03783v1.pdf | author:Germinal Cocho, Jorge Flores, Carlos Gershenson, Carlos Pineda, Sergio Sánchez category:cs.CL published:2015-05-14 summary:Statistical studies of languages have focused on the rank-frequencydistribution of words. Instead, we introduce here a measure of how word rankschange in time and call this distribution \emph{rank diversity}. We calculatethis diversity for books published in six European languages since 1800, andfind that it follows a universal lognormal distribution. Based on the mean andstandard deviation associated with the lognormal distribution, we define threedifferent word regimes of languages: "heads" consist of words which almost donot change their rank in time, "bodies" are words of general use, while "tails"are comprised by context-specific words and vary their rank considerably intime. The heads and bodies reflect the size of language cores identified bylinguists for basic communication. We propose a Gaussian random walk modelwhich reproduces the rank variation of words in time and thus the diversity.Rank diversity of words can be understood as the result of random variations inrank, where the size of the variation depends on the rank itself. We find thatthe core size is similar for all languages studied.
arxiv-10500-196 | Modified Hausdorff Fractal Dimension (MHFD) | http://arxiv.org/pdf/1505.03493v2.pdf | author:Reza Farrahi Moghaddam, Mohamed Cheriet category:cs.CV published:2015-05-13 summary:The Hausdorff fractal dimension has been a fast-to-calculate method toestimate complexity of fractal shapes. In this work, a modified version of thisfractal dimension is presented in order to make it more robust when applied inestimating complexity of non-fractal images. The modified Hausdorff fractaldimension stands on two features that weaken the requirement of presence of ashape and also reduce the impact of the noise possibly presented in the inputimage. The new algorithm has been evaluated on a set of images of differentcharacter with promising performance.
arxiv-10500-197 | A PCA-Based Convolutional Network | http://arxiv.org/pdf/1505.03703v1.pdf | author:Yanhai Gan, Jun Liu, Junyu Dong, Guoqiang Zhong category:cs.LG cs.CV cs.NE published:2015-05-14 summary:In this paper, we propose a novel unsupervised deep learning model, calledPCA-based Convolutional Network (PCN). The architecture of PCN is composed ofseveral feature extraction stages and a nonlinear output stage. Particularly,each feature extraction stage includes two layers: a convolutional layer and afeature pooling layer. In the convolutional layer, the filter banks are simplylearned by PCA. In the nonlinear output stage, binary hashing is applied. Forthe higher convolutional layers, the filter banks are learned from the featuremaps that were obtained in the previous stage. To test PCN, we conductedextensive experiments on some challenging tasks, including handwritten digitsrecognition, face recognition and texture classification. The results show thatPCN performs competitive with or even better than state-of-the-art deeplearning models. More importantly, since there is no back propagation forsupervised finetuning, PCN is much more efficient than existing deep networks.
arxiv-10500-198 | Leveraging Image based Prior for Visual Place Recognition | http://arxiv.org/pdf/1505.03205v2.pdf | author:Tsukamoto Taisho, Tanaka Kanji category:cs.CV published:2015-05-13 summary:In this study, we propose a novel scene descriptor for visual placerecognition. Unlike popular bag-of-words scene descriptors which rely on alibrary of vector quantized visual features, our proposed descriptor is basedon a library of raw image data, such as publicly available photo collectionsfrom Google StreetView and Flickr. The library images need not to be associatedwith spatial information regarding the viewpoint and orientation of the scene.As a result, these images are cheaper than the database images; in addition,they are readily available. Our proposed descriptor directly mines the imagelibrary to discover landmarks (i.e., image patches) that suitably match aninput query/database image. The discovered landmarks are then compactlydescribed by their pose and shape (i.e., library image ID, bounding boxes) andused as a compact discriminative scene descriptor for the input image. Weevaluate the effectiveness of our scene description framework by comparing itsperformance to that of previous approaches.
arxiv-10500-199 | Looking outside of the Box: Object Detection and Localization with Multi-scale Patterns | http://arxiv.org/pdf/1505.03597v1.pdf | author:Eshed Ohn-Bar, M. M. Trivedi category:cs.CV published:2015-05-14 summary:Detection and localization of objects at multiple scales often involvessliding a single scale template in order to score windows at different scalesindependently. Nonetheless, multi-scale visual information at a given imagelocation is highly correlated. This fundamental insight allows us to generalizethe traditional multi-scale sliding window technique by jointly consideringimage features at all scales in order to detect and localize objects. Twomax-margin approaches are studied for learning the multi-scale templates andleveraging the highly structured multi-scale information which would have beenignored if a single-scale template was used. The multi-scale formulation isshown to significantly improve general detection performance (measured on thePASCAL VOC dataset). The experimental analysis shows the method to be effectivewith different visual features, both HOG and CNN. Surprisingly, for a givenwindow in a specific scale, visual information from windows at the same imagelocation but other scales (`out-of-scale' information) contains most of thediscriminative information for detection.
arxiv-10500-200 | CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research | http://arxiv.org/pdf/1505.03581v1.pdf | author:Ali Borji, Laurent Itti category:cs.CV published:2015-05-14 summary:Saliency modeling has been an active research area in computer vision forabout two decades. Existing state of the art models perform very well inpredicting where people look in natural scenes. There is, however, the riskthat these models may have been overfitting themselves to available small scalebiased datasets, thus trapping the progress in a local minimum. To gain adeeper insight regarding current issues in saliency modeling and to bettergauge progress, we recorded eye movements of 120 observers while they freelyviewed a large number of naturalistic and artificial images. Our stimuliincludes 4000 images; 200 from each of 20 categories covering different typesof scenes such as Cartoons, Art, Objects, Low resolution images, Indoor,Outdoor, Jumbled, Random, and Line drawings. We analyze some basic propertiesof this dataset and compare some successful models. We believe that our datasetopens new challenges for the next generation of saliency models and helpsconduct behavioral studies on bottom-up visual attention.
arxiv-10500-201 | Vanishing Point Attracts Eye Movements in Scene Free-viewing | http://arxiv.org/pdf/1505.03578v1.pdf | author:Ali Borji, Mengyang Feng, Huchuan Lu category:cs.CV published:2015-05-14 summary:Eye movements are crucial in understanding complex scenes. By predictingwhere humans look in natural scenes, we can understand how they percieve scenesand priotriaze information for further high-level processing. Here, we studythe effect of a particular type of scene structural information known asvanishing point and show that human gaze is attracted to vanishing pointregions. We then build a combined model of traditional saliency and vanishingpoint channel that outperforms state of the art saliency models.
arxiv-10500-202 | Bootstrapped Adaptive Threshold Selection for Statistical Model Selection and Estimation | http://arxiv.org/pdf/1505.03511v1.pdf | author:Kristofer E. Bouchard category:stat.ML published:2015-05-13 summary:A central goal of neuroscience is to understand how activity in the nervoussystem is related to features of the external world, or to features of thenervous system itself. A common approach is to model neural responses as aweighted combination of external features, or vice versa. The structure of themodel weights can provide insight into neural representations. Often, neuralinput-output relationships are sparse, with only a few inputs contributing tothe output. In part to account for such sparsity, structured regularizers areincorporated into model fitting optimization. However, by imposing priors,structured regularizers can make it difficult to interpret learned modelparameters. Here, we investigate a simple, minimally structured modelestimation method for accurate, unbiased estimation of sparse models based onBootstrapped Adaptive Threshold Selection followed by ordinary least-squaresrefitting (BoATS). Through extensive numerical investigations, we show thatthis method often performs favorably compared to L1 and L2 regularizers. Inparticular, for a variety of model distributions and noise levels, BoATS moreaccurately recovers the parameters of sparse models, leading to moreparsimonious explanations of outputs. Finally, we apply this method to the taskof decoding human speech production from ECoG recordings.
arxiv-10500-203 | On a spatial-temporal decomposition of the optical flow | http://arxiv.org/pdf/1505.03505v1.pdf | author:Aniello Raffale Patrone, Otmar Scherzer category:cs.CV published:2015-05-13 summary:In this paper we present the first variational spatial-temporal decompositionalgorithm for computation of the optical flow of a dynamic sequence. Weconsider several applications, such as the extraction of temporal motionpatterns of different scales and motion detection in dynamic sequences undervarying illumination conditions, such as they appear for instance inpsychological flickering experiments. In order to take into account variableillumination conditions we review the derivation, and modify, the optical flowequation. Concerning the numerical implementation, we propose a relaxationapproach for the adapted model such that the resulting optimality condition isan integro-differential equation, which is numerically solved by a fixed pointiteration. For comparison purposes we use the standard time dependent opticalflow algorithm from Weickert-Schn\"orr, which in contrast to our method,constitutes in solving a spatial-temporal differential equation.
arxiv-10500-204 | Prediction and Quantification of Individual Athletic Performance | http://arxiv.org/pdf/1505.01147v2.pdf | author:Duncan A. J. Blythe, Franz J. Király category:stat.AP stat.ML published:2015-05-05 summary:We provide scientific foundations for athletic performance prediction on anindividual level, exposing the phenomenology of individual athletic runningperformance in the form of a low-rank model dominated by an individual powerlaw. We present, evaluate, and compare a selection of methods for prediction ofindividual running performance, including our own, \emph{local matrixcompletion} (LMC), which we show to perform best. We also show that manydocumented phenomena in quantitative sports science, such as the form ofscoring tables, the success of existing prediction methods including Riegel'sformula, the Purdy points scheme, the power law for world records performancesand the broken power law for world record speeds may be explained on the basisof our findings in a unified way.
arxiv-10500-205 | A Review Paper: Noise Models in Digital Image Processing | http://arxiv.org/pdf/1505.03489v1.pdf | author:Ajay Kumar Boyat, Brijendra Kumar Joshi category:cs.CV published:2015-05-13 summary:Noise is always presents in digital images during image acquisition, coding,transmission, and processing steps. Noise is very difficult to remove it fromthe digital images without the prior knowledge of noise model. That is why,review of noise models are essential in the study of image denoisingtechniques. In this paper, we express a brief overview of various noise models.These noise models can be selected by analysis of their origin. In this way, wepresent a complete and quantitative analysis of noise models available indigital images.
arxiv-10500-206 | A Survey of Predictive Modelling under Imbalanced Distributions | http://arxiv.org/pdf/1505.01658v2.pdf | author:Paula Branco, Luis Torgo, Rita Ribeiro category:cs.LG I.2.6 published:2015-05-07 summary:Many real world data mining applications involve obtaining predictive modelsusing data sets with strongly imbalanced distributions of the target variable.Frequently, the least common values of this target variable are associated withevents that are highly relevant for end users (e.g. fraud detection, unusualreturns on stock markets, anticipation of catastrophes, etc.). Moreover, theevents may have different costs and benefits, which when associated with therarity of some of them on the available training data creates serious problemsto predictive modelling techniques. This paper presents a survey of existingtechniques for handling these important applications of predictive analytics.Although most of the existing work addresses classification tasks (nominaltarget variables), we also describe methods designed to handle similar problemswithin regression tasks (numeric target variables). In this survey we discussthe main challenges raised by imbalanced distributions, describe the mainapproaches to these problems, propose a taxonomy of these methods and refer tosome related problems within predictive modelling.
arxiv-10500-207 | MRF Optimization by Graph Approximation | http://arxiv.org/pdf/1505.03365v1.pdf | author:Wonsik Kim, Kyoung Mu Lee category:cs.CV published:2015-05-13 summary:Graph cuts-based algorithms have achieved great success in energyminimization for many computer vision applications. These algorithms provideapproximated solutions for multi-label energy functions via move-makingapproach. This approach fuses the current solution with a proposal to generatea lower-energy solution. Thus, generating the appropriate proposals isnecessary for the success of the move-making approach. However, not muchresearch efforts has been done on the generation of "good" proposals,especially for non-metric energy functions. In this paper, we propose anapplication-independent and energy-based approach to generate "good" proposals.With these proposals, we present a graph cuts-based move-making algorithmcalled GA-fusion (fusion with graph approximation-based proposals). Extensiveexperiments support that our proposal generation is effective across differentclasses of energy functions. The proposed algorithm outperforms others both onreal and synthetic problems.
arxiv-10500-208 | A Vision Based System for Monitoring the Loss of Attention in Automotive Drivers | http://arxiv.org/pdf/1505.03352v1.pdf | author:Anirban Dasgupta, Anjith George, S. L. Happy, Aurobinda Routray category:cs.CV published:2015-05-13 summary:On board monitoring of the alertness level of an automotive driver has been achallenging research in transportation safety and management. In this paper, wepropose a robust real time embedded platform to monitor the loss of attentionof the driver during day as well as night driving conditions. The PERcentage ofeye CLOSure (PERCLOS) has been used as the indicator of the alertness level. Inthis approach, the face is detected using Haar like features and tracked usinga Kalman Filter. The Eyes are detected using Principal Component Analysis (PCA)during day time and the block Local Binary Pattern (LBP) features during night.Finally the eye state is classified as open or closed using Support VectorMachines(SVM). In plane and off plane rotations of the drivers face have beencompensated using Affine and Perspective Transformation respectively.Compensation in illumination variation is carried out using Bi HistogramEqualization (BHE). The algorithm has been cross validated using brain signalsand finally been implemented on a Single Board Computer (SBC) having Intel Atomprocessor, 1 GB RAM, 1.66 GHz clock, x86 architecture, Windows Embedded XPoperating system. The system is found to be robust under actual drivingconditions.
arxiv-10500-209 | Analysis of PCA Algorithms in Distributed Environments | http://arxiv.org/pdf/1503.05214v2.pdf | author:Tarek Elgamal, Mohamed Hefeeda category:cs.DC cs.LG cs.NA published:2015-03-17 summary:Classical machine learning algorithms often face scalability bottlenecks whenthey are applied to large-scale data. Such algorithms were designed to workwith small data that is assumed to fit in the memory of one machine. In thisreport, we analyze different methods for computing an important machine learingalgorithm, namely Principal Component Analysis (PCA), and we comment on itslimitations in supporting large datasets. The methods are analyzed and comparedacross two important metrics: time complexity and communication complexity. Weconsider the worst-case scenarios for both metrics, and we identify thesoftware libraries that implement each method. The analysis in this reporthelps researchers and engineers in (i) understanding the main bottlenecks forscalability in different PCA algorithms, (ii) choosing the most appropriatemethod and software library for a given application and data setcharacteristics, and (iii) designing new scalable PCA algorithms.
arxiv-10500-210 | Stability and Performance Limits of Adaptive Primal-Dual Networks | http://arxiv.org/pdf/1408.3693v3.pdf | author:Zaid J. Towfic, Ali H. Sayed category:math.OC cs.DC cs.LG cs.MA published:2014-08-16 summary:This work studies distributed primal-dual strategies for adaptation andlearning over networks from streaming data. Two first-order methods areconsidered based on the Arrow-Hurwicz (AH) and augmented Lagrangian (AL)techniques. Several revealing results are discovered in relation to theperformance and stability of these strategies when employed over adaptivenetworks. The conclusions establish that the advantages that these methods havefor deterministic optimization problems do not necessarily carry over tostochastic optimization problems. It is found that they have narrower stabilityranges and worse steady-state mean-square-error performance than primal methodsof the consensus and diffusion type. It is also found that the AH technique canbecome unstable under a partial observation model, while the other techniquesare able to recover the unknown under this scenario. A method to enhance theperformance of AL strategies is proposed by tying the selection of thestep-size to their regularization parameter. It is shown that this methodallows the AL algorithm to approach the performance of consensus and diffusionstrategies but that it remains less stable than these other strategies.
arxiv-10500-211 | A Framework for Fast Face and Eye Detection | http://arxiv.org/pdf/1505.03344v1.pdf | author:Anjith George, Anirban Dasgupta, Aurobinda Routray category:cs.CV published:2015-05-13 summary:Face detection is an essential step in many computer vision applications likesurveillance, tracking, medical analysis, facial expression analysis etc.Several approaches have been made in the direction of face detection. Amongthem, Haar-like features based method is a robust method. In spite of therobustness, Haar - like features work with some limitations. However, with somesimple modifications in the algorithm, its performance can be made faster andmore robust. The present work refers to the increase in speed of operation ofthe original algorithm by down sampling the frames and its analysis withdifferent scale factors. It also discusses the detection of tilted faces usingan affine transformation of the input image.
arxiv-10500-212 | Optimal linear estimation under unknown nonlinear transform | http://arxiv.org/pdf/1505.03257v1.pdf | author:Xinyang Yi, Zhaoran Wang, Constantine Caramanis, Han Liu category:stat.ML cs.IT math.IT published:2015-05-13 summary:Linear regression studies the problem of estimating a model parameter$\beta^* \in \mathbb{R}^p$, from $n$ observations$\{(y_i,\mathbf{x}_i)\}_{i=1}^n$ from linear model $y_i = \langle\mathbf{x}_i,\beta^* \rangle + \epsilon_i$. We consider a significantgeneralization in which the relationship between $\langle \mathbf{x}_i,\beta^*\rangle$ and $y_i$ is noisy, quantized to a single bit, potentially nonlinear,noninvertible, as well as unknown. This model is known as the single-indexmodel in statistics, and, among other things, it represents a significantgeneralization of one-bit compressed sensing. We propose a novel spectral-basedestimation procedure and show that we can recover $\beta^*$ in settings (i.e.,classes of link function $f$) where previous algorithms fail. In general, ouralgorithm requires only very mild restrictions on the (unknown) functionalrelationship between $y_i$ and $\langle \mathbf{x}_i,\beta^* \rangle$. We alsoconsider the high dimensional setting where $\beta^*$ is sparse ,and introducea two-stage nonconvex framework that addresses estimation challenges in highdimensional regimes where $p \gg n$. For a broad class of link functionsbetween $\langle \mathbf{x}_i,\beta^* \rangle$ and $y_i$, we establish minimaxlower bounds that demonstrate the optimality of our estimators in both theclassical and high dimensional regimes.
arxiv-10500-213 | Feature selection using Fisher's ratio technique for automatic speech recognition | http://arxiv.org/pdf/1505.03239v1.pdf | author:Sarika Hegde, K. K. Achary, Surendra Shetty category:cs.CL published:2015-05-13 summary:Automatic Speech Recognition involves mainly two steps; feature extractionand classification . Mel Frequency Cepstral Coefficient is used as one of theprominent feature extraction techniques in ASR. Usually, the set of all 12 MFCCcoefficients is used as the feature vector in the classification step. But thequestion is whether the same or improved classification accuracy can beachieved by using a subset of 12 MFCC as feature vector. In this paper,Fisher's ratio technique is used for selecting a subset of 12 MFCC coefficientsthat contribute more in discriminating a pattern. The selected coefficients areused in classification with Hidden Markov Model algorithm. The classificationaccuracies that we get by using 12 coefficients and by using the selectedcoefficients are compared.
arxiv-10500-214 | Hybrid data clustering approach using K-Means and Flower Pollination Algorithm | http://arxiv.org/pdf/1505.03236v1.pdf | author:R. Jensi, G. Wiselin Jiji category:cs.LG cs.IR cs.NE published:2015-05-13 summary:Data clustering is a technique for clustering set of objects into knownnumber of groups. Several approaches are widely applied to data clustering sothat objects within the clusters are similar and objects in different clustersare far away from each other. K-Means, is one of the familiar center basedclustering algorithms since implementation is very easy and fast convergence.However, K-Means algorithm suffers from initialization, hence trapped in localoptima. Flower Pollination Algorithm (FPA) is the global optimizationtechnique, which avoids trapping in local optimum solution. In this paper, anovel hybrid data clustering approach using Flower Pollination Algorithm andK-Means (FPAKM) is proposed. The proposed algorithm results are compared withK-Means and FPA on eight datasets. From the experimental results, FPAKM isbetter than FPA and K-Means.
arxiv-10500-215 | APAC: Augmented PAttern Classification with Neural Networks | http://arxiv.org/pdf/1505.03229v1.pdf | author:Ikuro Sato, Hiroki Nishimura, Kensuke Yokoi category:cs.CV published:2015-05-13 summary:Deep neural networks have been exhibiting splendid accuracies in many ofvisual pattern classification problems. Many of the state-of-the-art methodsemploy a technique known as data augmentation at the training stage. This paperaddresses an issue of decision rule for classifiers trained with augmenteddata. Our method is named as APAC: the Augmented PAttern Classification, whichis a way of classification using the optimal decision rule for augmented datalearning. Discussion of methods of data augmentation is not our primary focus.We show clear evidences that APAC gives far better generalization performancethan the traditional way of class prediction in several experiments. Ourconvolutional neural network model with APAC achieved a state-of-the-artaccuracy on the MNIST dataset among non-ensemble classifiers. Even ourmultilayer perceptron model beats some of the convolutional models withrecently invented stochastic regularization techniques on the CIFAR-10 dataset.
arxiv-10500-216 | PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Edge-Preserving Coherence | http://arxiv.org/pdf/1505.03227v1.pdf | author:Keze Wang, Liang Lin, Jiangbo Lu, Chenglong Li, Keyang Shi category:cs.CV 68U10 published:2015-05-13 summary:Driven by recent vision and graphics applications such as image segmentationand object recognition, computing pixel-accurate saliency values to uniformlyhighlight foreground objects becomes increasingly important. In this paper, wepropose a unified framework called PISA, which stands for Pixelwise ImageSaliency Aggregating various bottom-up cues and priors. It generates spatiallycoherent yet detail-preserving, pixel-accurate and fine-grained saliency, andovercomes the limitations of previous methods which use homogeneoussuperpixel-based and color only treatment. PISA aggregates multiple saliencycues in a global context such as complementary color and structure contrastmeasures with their spatial priors in the image domain. The saliency confidenceis further jointly modeled with a neighborhood consistence constraint into anenergy minimization formulation, in which each pixel will be evaluated withmultiple hypothetical saliency levels. Instead of using global discreteoptimization methods, we employ the cost-volume filtering technique to solveour formulation, assigning the saliency levels smoothly while preserving theedge-aware structure details. In addition, a faster version of PISA isdeveloped using a gradient-driven image sub-sampling strategy to greatlyimprove the runtime efficiency while keeping comparable detection accuracy.Extensive experiments on a number of public datasets suggest that PISAconvincingly outperforms other state-of-the-art approaches. In addition, withthis work we also create a new dataset containing $800$ commodity images forevaluating saliency detection. The dataset and source code of PISA can bedownloaded at http://vision.sysu.edu.cn/project/PISA/
arxiv-10500-217 | Exemplar Dynamics and Sound Merger in Language | http://arxiv.org/pdf/1412.1841v2.pdf | author:P. F. Tupper category:cs.CL math.DS nlin.AO 91F20, 70F99 published:2014-12-02 summary:We develop a model of phonological contrast in natural language.Specifically, the model describes the maintenance of contrast between differentwords in a language, and the elimination of such contrast when sounds in thewords merge. An example of such a contrast is that provided by the two vowelsounds 'i' and 'e', which distinguish pairs of words such as 'pin' and 'pen' inmost dialects of English. We model language users' knowledge of thepronunciation of a word as consisting of collections of labeled exemplarsstored in memory. Each exemplar is a detailed memory of a particular utteranceof the word in question. In our model an exemplar is represented by one or twophonetic variables along with a weight indicating how strong the memory of theutterance is. Starting from an exemplar-level model we deriveintegro-differential equations for the evolution of exemplar density fields inphonetic space. Using these latter equations we investigate under whatconditions two sounds merge, thus eliminating the contrast. Our main conclusionis that for the preservation of phonological contrast, it is necessary thatanomalous utterances of a given word are discarded, and not merely stored inmemory as an exemplar of another word.
arxiv-10500-218 | Equitability, interval estimation, and statistical power | http://arxiv.org/pdf/1505.02212v2.pdf | author:Yakir A. Reshef, David N. Reshef, Pardis C. Sabeti, Michael M. Mitzenmacher category:math.ST cs.LG q-bio.QM stat.ME stat.ML stat.TH published:2015-05-09 summary:For analysis of a high-dimensional dataset, a common approach is to test anull hypothesis of statistical independence on all variable pairs using anon-parametric measure of dependence. However, because this approach attemptsto identify any non-trivial relationship no matter how weak, it oftenidentifies too many relationships to be useful. What is needed is a way ofidentifying a smaller set of relationships that merit detailed furtheranalysis. Here we formally present and characterize equitability, a property ofmeasures of dependence that aims to overcome this challenge. Notionally, anequitable statistic is a statistic that, given some measure of noise, assignssimilar scores to equally noisy relationships of different types [Reshef et al.2011]. We begin by formalizing this idea via a new object called theinterpretable interval, which functions as an interval estimate of the amountof noise in a relationship of unknown type. We define an equitable statistic asone with small interpretable intervals. We then draw on the equivalence of interval estimation and hypothesis testingto show that under moderate assumptions an equitable statistic is one thatyields well powered tests for distinguishing not only between trivial andnon-trivial relationships of all kinds but also between non-trivialrelationships of different strengths. This means that equitability allows us tospecify a threshold relationship strength $x_0$ and to search for relationshipsof all kinds with strength greater than $x_0$. Thus, equitability can bethought of as a strengthening of power against independence that enablesfruitful analysis of data sets with a small number of strong, interestingrelationships and a large number of weaker ones. We conclude with ademonstration of how our two equivalent characterizations of equitability canbe used to evaluate the equitability of a statistic in practice.
arxiv-10500-219 | Theoretical Foundations of Equitability and the Maximal Information Coefficient | http://arxiv.org/pdf/1408.4908v3.pdf | author:Yakir A. Reshef, David N. Reshef, Pardis C. Sabeti, Michael Mitzenmacher category:stat.ME cs.IT math.IT math.ST q-bio.QM stat.ML stat.TH published:2014-08-21 summary:The maximal information coefficient (MIC) is a tool for finding the strongestpairwise relationships in a data set with many variables (Reshef et al., 2011).MIC is useful because it gives similar scores to equally noisy relationships ofdifferent types. This property, called {\em equitability}, is important foranalyzing high-dimensional data sets. Here we formalize the theory behind both equitability and MIC in the languageof estimation theory. This formalization has a number of advantages. First, itallows us to show that equitability is a generalization of power againststatistical independence. Second, it allows us to compute and discuss thepopulation value of MIC, which we call MIC_*. In doing so we generalize andstrengthen the mathematical results proven in Reshef et al. (2011) and clarifythe relationship between MIC and mutual information. Introducing MIC_* alsoenables us to reason about the properties of MIC more abstractly: for instance,we show that MIC_* is continuous and that there is a sense in which it is acanonical "smoothing" of mutual information. We also prove an alternate,equivalent characterization of MIC_* that we use to state new estimators of itas well as an algorithm for explicitly computing it when the joint probabilitydensity function of a pair of random variables is known. Our hope is that thispaper provides a richer theoretical foundation for MIC and equitability goingforward. This paper will be accompanied by a forthcoming companion paper that performsextensive empirical analysis and comparison to other methods and discusses thepractical aspects of both equitability and the use of MIC and its relatedstatistics.
arxiv-10500-220 | An Empirical Study of Leading Measures of Dependence | http://arxiv.org/pdf/1505.02214v2.pdf | author:David N. Reshef, Yakir A. Reshef, Pardis C. Sabeti, Michael M. Mitzenmacher category:stat.ME cs.IT cs.LG math.IT q-bio.QM stat.ML published:2015-05-09 summary:In exploratory data analysis, we are often interested in identifyingpromising pairwise associations for further analysis while filtering outweaker, less interesting ones. This can be accomplished by computing a measureof dependence on all variable pairs and examining the highest-scoring pairs,provided the measure of dependence used assigns similar scores to equally noisyrelationships of different types. This property, called equitability, isformalized in Reshef et al. [2015b]. In addition to equitability, measures ofdependence can also be assessed by the power of their correspondingindependence tests as well as their runtime. Here we present extensive empirical evaluation of the equitability, poweragainst independence, and runtime of several leading measures of dependence.These include two statistics introduced in Reshef et al. [2015a]: MICe, whichhas equitability as its primary goal, and TICe, which has power againstindependence as its goal. Regarding equitability, our analysis finds that MICeis the most equitable method on functional relationships in most of thesettings we considered, although mutual information estimation proves the mostequitable at large sample sizes in some specific settings. Regarding poweragainst independence, we find that TICe, along with Heller and Gorfine's S^DDP,is the state of the art on the relationships we tested. Our analyses also showa trade-off between power against independence and equitability consistent withthe theory in Reshef et al. [2015b]. In terms of runtime, MICe and TICe aresignificantly faster than many other measures of dependence tested, andcomputing either one makes computing the other trivial. This suggests that afast and useful strategy for achieving a combination of power againstindependence and equitability may be to filter relationships by TICe and thento examine the MICe of only the significant ones.
arxiv-10500-221 | Measuring dependence powerfully and equitably | http://arxiv.org/pdf/1505.02213v2.pdf | author:Yakir A. Reshef, David N. Reshef, Hilary K. Finucane, Pardis C. Sabeti, Michael M. Mitzenmacher category:stat.ME cs.IT cs.LG math.IT q-bio.QM stat.ML published:2015-05-09 summary:For high-dimensional datasets, it is common to evaluate a measure ofdependence on every variable pair and retain the highest-scoring pairs forfollow-up. If the statistic used systematically assigns higher scores to somerelationship types over others, important relationships may be overlooked. Thisdifficulty is avoided if the statistic is equitable [Reshef et al. 2015a],i.e., if, for some measure of noise, it assigns similar scores to equally noisyrelationships regardless of relationship type. In this paper, we introduce and characterize a population measure ofdependence called MIC*. We show three ways that MIC* can be viewed: as thepopulation value of MIC, a highly equitable statistic from [Reshef et al.2011], as a canonical "smoothing" of mutual information, and as the supremum ofan infinite sequence defined in terms of optimal one-dimensional partitions ofthe marginals of the joint distribution. Based on this theory, we introduce anefficient algorithm for computing MIC* from the density of a pair of randomvariables, and we define a new consistent estimator MICe for MIC* that isefficiently computable. (In contrast, there is no known polynomial-timealgorithm for computing MIC.) We show through simulations that MICe has betterbias-variance properties than MIC, and that it has high equitability withrespect to R^2 on a set of functional relationships. While MICe is designed forequitability rather than independence testing, we introduce a relatedstatistic, TICe, that is a trivial side-product of the computation of MICe. Weprove the consistency of independence testing based on TICe and show insimulations that this approach achieves excellent power. This paper is accompanied by a companion paper [Reshef et al. 2015b] focusedon in-depth empirical evaluation of several leading measures of dependence thatfinds that the performance of MICe and TICe is state-of-the-art.
arxiv-10500-222 | Correlation Clustering with Noisy Partial Information | http://arxiv.org/pdf/1406.5667v2.pdf | author:Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan category:cs.DS cs.LG published:2014-06-22 summary:In this paper, we propose and study a semi-random model for the CorrelationClustering problem on arbitrary graphs G. We give two approximation algorithmsfor Correlation Clustering instances from this model. The first algorithm findsa solution of value $(1+ \delta) optcost + O_{\delta}(n\log^3 n)$ with highprobability, where $optcost$ is the value of the optimal solution (for every$\delta > 0$). The second algorithm finds the ground truth clustering with anarbitrarily small classification error $\eta$ (under some additionalassumptions on the instance).
arxiv-10500-223 | Event Retrieval Using Motion Barcodes | http://arxiv.org/pdf/1412.1455v3.pdf | author:Gil Ben-Artzi, Michael Werman, Shmuel Peleg category:cs.CV published:2014-12-03 summary:We introduce a simple and effective method for retrieval of videos showing aspecific event, even when the videos of that event were captured fromsignificantly different viewpoints. Appearance-based methods fail in suchcases, as appearances change with large changes of viewpoints. Our method is based on a pixel-based feature, "motion barcode", which recordsthe existence/non-existence of motion as a function of time. While appearance,motion magnitude, and motion direction can vary greatly between disparateviewpoints, the existence of motion is viewpoint invariant. Based on the motionbarcode, a similarity measure is developed for videos of the same event takenfrom very different viewpoints. This measure is robust to occlusions commonunder different viewpoints, and can be computed efficiently. Event retrieval is demonstrated using challenging videos from stationary andhand held cameras.
arxiv-10500-224 | Sentiment Analysis For Modern Standard Arabic And Colloquial | http://arxiv.org/pdf/1505.03105v1.pdf | author:Hossam S. Ibrahim, Sherif M. Abdou, Mervat Gheith category:cs.CL published:2015-05-12 summary:The rise of social media such as blogs and social networks has fueledinterest in sentiment analysis. With the proliferation of reviews, ratings,recommendations and other forms of online expression, online opinion has turnedinto a kind of virtual currency for businesses looking to market theirproducts, identify new opportunities and manage their reputations, thereforemany are now looking to the field of sentiment analysis. In this paper, wepresent a feature-based sentence level approach for Arabic sentiment analysis.Our approach is using Arabic idioms/saying phrases lexicon as a key importancefor improving the detection of the sentiment polarity in Arabic sentences aswell as a number of novels and rich set of linguistically motivated featurescontextual Intensifiers, contextual Shifter and negation handling), syntacticfeatures for conflicting phrases which enhance the sentiment classificationaccuracy. Furthermore, we introduce an automatic expandable wide coveragepolarity lexicon of Arabic sentiment words. The lexicon is built withgold-standard sentiment words as a seed which is manually collected andannotated and it expands and detects the sentiment orientation automatically ofnew sentiment words using synset aggregation technique and free online Arabiclexicons and thesauruses. Our data focus on modern standard Arabic (MSA) andEgyptian dialectal Arabic tweets and microblogs (hotel reservation, productreviews, etc.). The experimental results using our resources and techniqueswith SVM classifier indicate high performance levels, with accuracies of over95%.
arxiv-10500-225 | A new Level-set based Protocol for Accurate Bone Segmentation from CT Imaging | http://arxiv.org/pdf/1505.03093v1.pdf | author:Manuel Pinheiro, J. L. Alves category:physics.med-ph cs.CV published:2015-05-12 summary:In this work it is proposed a medical image segmentation pipeline foraccurate bone segmentation from CT imaging. It is a two-step methodology, witha pre-segmentation step and a segmentation refinement step. First, the userperforms a rough segmenting of the desired region of interest. Next, a fullyautomatic refinement step is applied to the pre-segmented data. The automaticsegmentation refinement is composed by several sub-stpng, namely imagedeconvolution, image cropping and interpolation. The user-definedpre-segmentation is then refined over the deconvolved, cropped, and up-sampledversion of the image. The algorithm is applied in the segmentation of CT imagesof a composite femur bone, reconstructed with different reconstructionprotocols. Segmentation outcomes are validated against a gold standard modelobtained with coordinate measuring machine Nikon Metris LK V20 with a digitalline scanner LC60-D that guarantees an accuracy of 28 $\mu m$. High sub-pixelaccuracy models were obtained for all tested Datasets. The algorithm is able toproduce high quality segmentation of the composite femur regardless of thesurface meshing strategy used.
arxiv-10500-226 | Indonesian Social Media Sentiment Analysis With Sarcasm Detection | http://arxiv.org/pdf/1505.03085v1.pdf | author:Edwin Lunando, Ayu Purwarianti category:cs.CL published:2015-05-12 summary:Sarcasm is considered one of the most difficult problem in sentimentanalysis. In our ob-servation on Indonesian social media, for cer-tain topics,people tend to criticize something using sarcasm. Here, we proposed twoadditional features to detect sarcasm after a common sentiment analysis isconducted. The features are the negativity information and the number ofinterjection words. We also employed translated SentiWordNet in the sentimentclassification. All the classifications were conducted with machine learningalgorithms. The experimental results showed that the additional features arequite effective in the sarcasm detection.
arxiv-10500-227 | A Survey of Arabic Dialogues Understanding for Spontaneous Dialogues and Instant Message | http://arxiv.org/pdf/1505.03084v1.pdf | author:AbdelRahim A. Elmadany, Sherif M. Abdou, Mervat Gheith category:cs.CL published:2015-05-12 summary:Building dialogues systems interaction has recently gained considerableattention, but most of the resources and systems built so far are tailored toEnglish and other Indo-European languages. The need for designing systems forother languages is increasing such as Arabic language. For this reasons, thereare more interest for Arabic dialogue acts classification task because it a keyplayer in Arabic language understanding to building this systems. This papersurveys different techniques for dialogue acts classification for Arabic. Wedescribe the main existing techniques for utterances segmentations andclassification, annotation schemas, and test corpora for Arabic dialoguesunderstanding that have introduced in the literature
arxiv-10500-228 | Turn Segmentation into Utterances for Arabic Spontaneous Dialogues and Instance Messages | http://arxiv.org/pdf/1505.03081v1.pdf | author:AbdelRahim A. Elmadany, Sherif M. Abdou, Mervat Gheith category:cs.CL published:2015-05-12 summary:Text segmentation task is an essential processing task for many of NaturalLanguage Processing (NLP) such as text summarization, text translation,dialogue language understanding, among others. Turns segmentation consideredthe key player in dialogue understanding task for building automaticHuman-Computer systems. In this paper, we introduce a novel approach to turnsegmentation into utterances for Egyptian spontaneous dialogues and InstanceMessages (IM) using Machine Learning (ML) approach as a part of automaticunderstanding Egyptian spontaneous dialogues and IM task. Due to the lack ofEgyptian dialect dialogue corpus the system evaluated by our corpus includes3001 turns, which are collected, segmented, and annotated manually fromEgyptian call-centers. The system achieves F1 scores of 90.74% and accuracy of95.98%.
arxiv-10500-229 | Removing systematic errors for exoplanet search via latent causes | http://arxiv.org/pdf/1505.03036v1.pdf | author:Bernhard Schölkopf, David W. Hogg, Dun Wang, Daniel Foreman-Mackey, Dominik Janzing, Carl-Johann Simon-Gabriel, Jonas Peters category:stat.ML astro-ph.EP astro-ph.IM cs.LG published:2015-05-12 summary:We describe a method for removing the effect of confounders in order toreconstruct a latent quantity of interest. The method, referred to ashalf-sibling regression, is inspired by recent work in causal inference usingadditive noise models. We provide a theoretical justification and illustratethe potential of the method in a challenging astronomy application.
arxiv-10500-230 | WxBS: Wide Baseline Stereo Generalizations | http://arxiv.org/pdf/1504.06603v2.pdf | author:Dmytro Mishkin, Jiri Matas, Michal Perdoch, Karel Lenc category:cs.CV published:2015-04-24 summary:We have presented a new problem -- the wide multiple baseline stereo (WxBS)-- which considers matching of images that simultaneously differ in more thanone image acquisition factor such as viewpoint, illumination, sensor type orwhere object appearance changes significantly, e.g. over time. A new datasetwith the ground truth for evaluation of matching algorithms has been introducedand will be made public. We have extensively tested a large set of popular and recent detectors anddescriptors and show than the combination of RootSIFT and HalfRootSIFT asdescriptors with MSER and Hessian-Affine detectors works best for manydifferent nuisance factors. We show that simple adaptive thresholding improvesHessian-Affine, DoG, MSER (and possibly other) detectors and allows to use themon infrared and low contrast images. A novel matching algorithm for addressing the WxBS problem has beenintroduced. We have shown experimentally that the WxBS-M matcher dominantes thestate-of-the-art methods both on both the new and existing datasets.
arxiv-10500-231 | Spectral Clustering via the Power Method -- Provably | http://arxiv.org/pdf/1311.2854v3.pdf | author:Christos Boutsidis, Alex Gittens, Prabhanjan Kambadur category:cs.LG cs.NA published:2013-11-12 summary:Spectral clustering is one of the most important algorithms in data miningand machine intelligence; however, its computational complexity limits itsapplication to truly large scale data analysis. The computational bottleneck inspectral clustering is computing a few of the top eigenvectors of the(normalized) Laplacian matrix corresponding to the graph representing the datato be clustered. One way to speed up the computation of these eigenvectors isto use the "power method" from the numerical linear algebra literature.Although the power method has been empirically used to speed up spectralclustering, the theory behind this approach, to the best of our knowledge,remains unexplored. This paper provides the \emph{first} such rigoroustheoretical justification, arguing that a small number of power iterationssuffices to obtain near-optimal partitionings using the approximateeigenvectors. Specifically, we prove that solving the $k$-means clusteringproblem on the approximate eigenvectors obtained via the power method gives anadditive-error approximation to solving the $k$-means problem on the optimaleigenvectors.
arxiv-10500-232 | Automatic Script Identification in the Wild | http://arxiv.org/pdf/1505.02982v1.pdf | author:Baoguang Shi, Cong Yao, Chengquan Zhang, Xiaowei Guo, Feiyue Huang, Xiang Bai category:cs.CV published:2015-05-12 summary:With the rapid increase of transnational communication and cooperation,people frequently encounter multilingual scenarios in various situations. Inthis paper, we are concerned with a relatively new problem: scriptidentification at word or line levels in natural scenes. A large-scale datasetwith a great quantity of natural images and 10 types of widely used languagesis constructed and released. In allusion to the challenges in scriptidentification in real-world scenarios, a deep learning based algorithm isproposed. The experiments on the proposed dataset demonstrate that ouralgorithm achieves superior performance, compared with conventional imageclassification methods, such as the original CNN architecture and LLC.
arxiv-10500-233 | Comparing methods for Twitter Sentiment Analysis | http://arxiv.org/pdf/1505.02973v1.pdf | author:Evangelos Psomakelis, Konstantinos Tserpes, Dimosthenis Anagnostopoulos, Theodora Varvarigou category:cs.CL cs.IR cs.SI published:2015-05-12 summary:This work extends the set of works which deal with the popular problem ofsentiment analysis in Twitter. It investigates the most popular document("tweet") representation methods which feed sentiment evaluation mechanisms. Inparticular, we study the bag-of-words, n-grams and n-gram graphs approaches andfor each of them we evaluate the performance of a lexicon-based and 7learning-based classification algorithms (namely SVM, Na\"ive BayesianNetworks, Logistic Regression, Multilayer Perceptrons, Best-First Trees,Functional Trees and C4.5) as well as their combinations, using a set of 4451manually annotated tweets. The results demonstrate the superiority oflearning-based methods and in particular of n-gram graphs approaches forpredicting the sentiment of tweets. They also show that the combinatoryapproach has impressive effects on n-grams, raising the confidence up to 83.15%on the 5-Grams, using majority vote and a balanced dataset (equal number ofpositive, negative and neutral tweets for training). In the n-gram graph casesthe improvement was small to none, reaching 94.52% on the 4-gram graphs, usingOrthodromic distance and a threshold of 0.001.
arxiv-10500-234 | Contour Detection Using Cost-Sensitive Convolutional Neural Networks | http://arxiv.org/pdf/1412.6857v5.pdf | author:Jyh-Jing Hwang, Tyng-Luh Liu category:cs.CV cs.LG cs.NE published:2014-12-22 summary:We address the problem of contour detection via per-pixel classifications ofedge point. To facilitate the process, the proposed approach leverages withDenseNet, an efficient implementation of multiscale convolutional neuralnetworks (CNNs), to extract an informative feature vector for each pixel anduses an SVM classifier to accomplish contour detection. The main challenge liesin adapting a pre-trained per-image CNN model for yielding per-pixel imagefeatures. We propose to base on the DenseNet architecture to achieve pixelwisefine-tuning and then consider a cost-sensitive strategy to further improve thelearning with a small dataset of edge and non-edge image patches. In theexperiment of contour detection, we look into the effectiveness of combiningper-pixel features from different CNN layers and obtain comparable performancesto the state-of-the-art on BSDS500.
arxiv-10500-235 | Fractional Max-Pooling | http://arxiv.org/pdf/1412.6071v4.pdf | author:Benjamin Graham category:cs.CV published:2014-12-18 summary:Convolutional networks almost always incorporate some form of spatialpooling, and very often it is alpha times alpha max-pooling with alpha=2.Max-pooling act on the hidden layers of the network, reducing their size by aninteger multiplicative factor alpha. The amazing by-product of discarding 75%of your data is that you build into the network a degree of invariance withrespect to translations and elastic distortions. However, if you simplyalternate convolutional layers with max-pooling layers, performance is limiteddue to the rapid reduction in spatial size, and the disjoint nature of thepooling regions. We have formulated a fractional version of max-pooling wherealpha is allowed to take non-integer values. Our version of max-pooling isstochastic as there are lots of different ways of constructing suitable poolingregions. We find that our form of fractional max-pooling reduces overfitting ona variety of datasets: for instance, we improve on the state-of-the art forCIFAR-100 without even using dropout.
arxiv-10500-236 | Incorporating Type II Error Probabilities from Independence Tests into Score-Based Learning of Bayesian Network Structure | http://arxiv.org/pdf/1505.02870v1.pdf | author:Eliot Brenner, David Sontag category:cs.LG stat.ML published:2015-05-12 summary:We give a new consistent scoring function for structure learning of Bayesiannetworks. In contrast to traditional approaches to score-based structurelearning, such as BDeu or MDL, the complexity penalty that we propose isdata-dependent and is given by the probability that a conditional independencetest correctly shows that an edge cannot exist. What really distinguishes thisnew scoring function from earlier work is that it has the property of becomingcomputationally easier to maximize as the amount of data increases. We prove apolynomial sample complexity result, showing that maximizing this score isguaranteed to correctly learn a structure with no false edges and adistribution close to the generating distribution, whenever there exists aBayesian network which is a perfect map for the data generating distribution.Although the new score can be used with any search algorithm, in our relatedUAI 2013 paper [BS13], we have given empirical results showing that it isparticularly effective when used together with a linear programming relaxationapproach to Bayesian network structure learning. The present paper contains alldetails of the proofs of the finite-sample complexity results in [BS13] as wellas detailed explanation of the computation of the certain error probabilitiescalled beta-values, whose precomputation and tabulation is necessary for theimplementation of the algorithm in [BS13].
arxiv-10500-237 | The Boundary Forest Algorithm for Online Supervised and Unsupervised Learning | http://arxiv.org/pdf/1505.02867v1.pdf | author:Charles Mathy, Nate Derbinsky, José Bento, Jonathan Rosenthal, Jonathan Yedidia category:cs.LG cs.DS cs.IR stat.ML published:2015-05-12 summary:We describe a new instance-based learning algorithm called the BoundaryForest (BF) algorithm, that can be used for supervised and unsupervisedlearning. The algorithm builds a forest of trees whose nodes store previouslyseen examples. It can be shown data points one at a time and updates itselfincrementally, hence it is naturally online. Few instance-based algorithms havethis property while being simultaneously fast, which the BF is. This is crucialfor applications where one needs to respond to input data in real time. Thenumber of children of each node is not set beforehand but obtained from thetraining procedure, which makes the algorithm very flexible with regards towhat data manifolds it can learn. We test its generalization performance andspeed on a range of benchmark datasets and detail in which settings itoutperforms the state of the art. Empirically we find that training time scalesas O(DNlog(N)) and testing as O(Dlog(N)), where D is the dimensionality and Nthe amount of data,
arxiv-10500-238 | An iterative step-function estimator for graphons | http://arxiv.org/pdf/1412.2129v2.pdf | author:Diana Cai, Nathanael Ackerman, Cameron Freer category:math.ST stat.CO stat.ML stat.TH published:2014-12-05 summary:Exchangeable graphs arise via a sampling procedure from measurable functionsknown as graphons. A natural estimation problem is how well we can recover agraphon given a single graph sampled from it. One general framework forestimating a graphon uses step-functions obtained by partitioning the nodes ofthe graph according to some clustering algorithm. We propose an iterativestep-function estimator (ISFE) that, given an initial partition, iterativelyclusters nodes based on their edge densities with respect to the previousiteration's partition. We analyze ISFE and demonstrate its performance incomparison with other graphon estimation techniques.
arxiv-10500-239 | On Markov chain Monte Carlo methods for tall data | http://arxiv.org/pdf/1505.02827v1.pdf | author:Rémi Bardenet, Arnaud Doucet, Chris Holmes category:stat.ME stat.CO stat.ML published:2015-05-11 summary:Markov chain Monte Carlo methods are often deemed too computationallyintensive to be of any practical use for big data applications, and inparticular for inference on datasets containing a large number $n$ ofindividual data points, also known as tall datasets. In scenarios where dataare assumed independent, various approaches to scale up the Metropolis-Hastingsalgorithm in a Bayesian inference context have been recently proposed inmachine learning and computational statistics. These approaches can be groupedinto two categories: divide-and-conquer approaches and, subsampling-basedalgorithms. The aims of this article are as follows. First, we present acomprehensive review of the existing literature, commenting on the underlyingassumptions and theoretical guarantees of each method. Second, by leveragingour understanding of these limitations, we propose an originalsubsampling-based approach which samples from a distribution provably close tothe posterior distribution of interest, yet can require less than $O(n)$ datapoint likelihood evaluations at each iteration for certain statistical modelsin favourable scenarios. Finally, we have only been able so far to proposesubsampling-based methods which display good performance in scenarios where theBernstein-von Mises approximation of the target posterior distribution isexcellent. It remains an open challenge to develop such methods in scenarioswhere the Bernstein-von Mises approximation is poor.
arxiv-10500-240 | Sample complexity of learning Mahalanobis distance metrics | http://arxiv.org/pdf/1505.02729v1.pdf | author:Nakul Verma, Kristin Branson category:cs.LG cs.AI stat.ML published:2015-05-11 summary:Metric learning seeks a transformation of the feature space that enhancesprediction quality for the given task at hand. In this work we providePAC-style sample complexity rates for supervised metric learning. We givematching lower- and upper-bounds showing that the sample complexity scales withthe representation dimension when no assumptions are made about the underlyingdata distribution. However, by leveraging the structure of the datadistribution, we show that one can achieve rates that are fine-tuned to aspecific notion of intrinsic complexity for a given dataset. Our analysisreveals that augmenting the metric learning optimization criterion with asimple norm-based regularization can help adapt to a dataset's intrinsiccomplexity, yielding better generalization. Experiments on benchmark datasetsvalidate our analysis and show that regularizing the metric can help discernthe signal even when the data contains high amounts of noise.
arxiv-10500-241 | Improving neural networks with bunches of neurons modeled by Kumaraswamy units: Preliminary study | http://arxiv.org/pdf/1505.02581v1.pdf | author:Jakub Mikolaj Tomczak category:cs.LG cs.NE published:2015-05-11 summary:Deep neural networks have recently achieved state-of-the-art results in manymachine learning problems, e.g., speech recognition or object recognition.Hitherto, work on rectified linear units (ReLU) provides empirical andtheoretical evidence on performance increase of neural networks comparing totypically used sigmoid activation function. In this paper, we investigate a newmanner of improving neural networks by introducing a bunch of copies of thesame neuron modeled by the generalized Kumaraswamy distribution. As a result,we propose novel non-linear activation function which we refer to asKumaraswamy unit which is closely related to ReLU. In the experimental studywith MNIST image corpora we evaluate the Kumaraswamy unit applied tosingle-layer (shallow) neural network and report a significant drop in testclassification error and test cross-entropy in comparison to sigmoid unit, ReLUand Noisy ReLU.
arxiv-10500-242 | A Two-Layer Local Constrained Sparse Coding Method for Fine-Grained Visual Categorization | http://arxiv.org/pdf/1505.02505v1.pdf | author:Guo Lihua, Guo Chenggan category:cs.CV 68T45 I.4.10 published:2015-05-11 summary:Fine-grained categories are more difficulty distinguished than genericcategories due to the similarity of inter-class and the diversity ofintra-class. Therefore, the fine-grained visual categorization (FGVC) isconsidered as one of challenge problems in computer vision recently. A newfeature learning framework, which is based on a two-layer local constrainedsparse coding architecture, is proposed in this paper. The two-layerarchitecture is introduced for learning intermediate-level features, and thelocal constrained term is applied to guarantee the local smooth of codingcoefficients. For extracting more discriminative information, local orientationhistograms are the input of sparse coding instead of raw pixels. Moreover, aquick dictionary updating process is derived to further improve the trainingspeed. Two experimental results show that our method achieves 85.29% accuracyon the Oxford 102 flowers dataset and 67.8% accuracy on the CUB-200-2011 birddataset, and the performance of our framework is highly competitive withexisting literatures.
arxiv-10500-243 | Training Deeper Convolutional Networks with Deep Supervision | http://arxiv.org/pdf/1505.02496v1.pdf | author:Liwei Wang, Chen-Yu Lee, Zhuowen Tu, Svetlana Lazebnik category:cs.CV published:2015-05-11 summary:One of the most promising ways of improving the performance of deepconvolutional neural networks is by increasing the number of convolutionallayers. However, adding layers makes training more difficult andcomputationally expensive. In order to train deeper networks, we propose to addauxiliary supervision branches after certain intermediate layers duringtraining. We formulate a simple rule of thumb to determine where these branchesshould be added. The resulting deeply supervised structure makes the trainingmuch easier and also produces better classification results on ImageNet and therecently released, larger MIT Places dataset
arxiv-10500-244 | An Online Learning Algorithm for Neuromorphic Hardware Implementation | http://arxiv.org/pdf/1505.02495v1.pdf | author:Chetan Singh Thakur, Runchun Wang, Saeed Afshar, Tara Julia Hamilton, Jonathan Tapson, Andre van Schaik category:cs.NE published:2015-05-11 summary:We propose a sign-based online learning (SOL) algorithm for a neuromorphichardware framework called Trainable Analogue Block (TAB). The TAB frameworkutilises the principles of neural population coding, implying that it encodesthe input stimulus using a large pool of nonlinear neurons. The SOL algorithmis a simple weight update rule that employs the sign of the hidden layeractivation and the sign of the output error, which is the difference betweenthe target output and the predicted output. The SOL algorithm is easilyimplementable in hardware, and can be used in any artificial neural networkframework that learns weights by minimising a convex cost function. We showthat the TAB framework can be trained for various regression tasks using theSOL algorithm.
arxiv-10500-245 | Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition | http://arxiv.org/pdf/1410.4281v2.pdf | author:Xiangang Li, Xihong Wu category:cs.CL cs.NE published:2014-10-16 summary:Long short-term memory (LSTM) based acoustic modeling methods have recentlybeen shown to give state-of-the-art performance on some speech recognitiontasks. To achieve a further performance improvement, in this research, deepextensions on LSTM are investigated considering that deep hierarchical modelhas turned out to be more efficient than a shallow one. Motivated by previousresearch on constructing deep recurrent neural networks (RNNs), alternativedeep LSTM architectures are proposed and empirically evaluated on a largevocabulary conversational telephone speech recognition task. Meanwhile,regarding to multi-GPU devices, the training process for LSTM networks isintroduced and discussed. Experimental results demonstrate that the deep LSTMnetworks benefit from the depth and yield the state-of-the-art performance onthis task.
arxiv-10500-246 | Contrastive Pessimistic Likelihood Estimation for Semi-Supervised Classification | http://arxiv.org/pdf/1503.00269v2.pdf | author:Marco Loog category:stat.ML cs.LG stat.ME I.2.6; I.5.1 published:2015-03-01 summary:Improvement guarantees for semi-supervised classifiers can currently only begiven under restrictive conditions on the data. We propose a general way toperform semi-supervised parameter estimation for likelihood-based classifiersfor which, on the full training set, the estimates are never worse than thesupervised solution in terms of the log-likelihood. We argue, moreover, that wemay expect these solutions to really improve upon the supervised classifier inparticular cases. In a worked-out example for LDA, we take it one step furtherand essentially prove that its semi-supervised version is strictly better thanits supervised counterpart. The two new concepts that form the core of ourestimation principle are contrast and pessimism. The former refers to the factthat our objective function takes the supervised estimates into account,enabling the semi-supervised solution to explicitly control the potentialimprovements over this estimate. The latter refers to the fact that ourestimates are conservative and therefore resilient to whatever form the truelabeling of the unlabeled data takes on. Experiments demonstrate theimprovements in terms of both the log-likelihood and the classification errorrate on independent test sets.
arxiv-10500-247 | Spike and Slab Gaussian Process Latent Variable Models | http://arxiv.org/pdf/1505.02434v1.pdf | author:Zhenwen Dai, James Hensman, Neil Lawrence category:stat.ML cs.LG published:2015-05-10 summary:The Gaussian process latent variable model (GP-LVM) is a popular approach tonon-linear probabilistic dimensionality reduction. One design choice for themodel is the number of latent variables. We present a spike and slab prior forthe GP-LVM and propose an efficient variational inference procedure that givesa lower bound of the log marginal likelihood. The new model provides a moreprincipled approach for selecting latent dimensions than the standard way ofthresholding the length-scale parameters. The effectiveness of our approach isdemonstrated through experiments on real and simulated data. Further, we extendmulti-view Gaussian processes that rely on sharing latent dimensions (known asmanifold relevance determination) with spike and slab priors. This allows amore principled approach for selecting a subset of the latent space for eachview of data. The extended model outperforms the previous state-of-the-art whenapplied to a cross-modal multimedia retrieval task.
arxiv-10500-248 | Fast Rhetorical Structure Theory Discourse Parsing | http://arxiv.org/pdf/1505.02425v1.pdf | author:Michael Heilman, Kenji Sagae category:cs.CL published:2015-05-10 summary:In recent years, There has been a variety of research on discourse parsing,particularly RST discourse parsing. Most of the recent work on RST parsing hasfocused on implementing new types of features or learning algorithms in orderto improve accuracy, with relatively little focus on efficiency, robustness, orpractical use. Also, most implementations are not widely available. Here, wedescribe an RST segmentation and parsing system that adapts models and featuresets from various previous work, as described below. Its accuracy is nearstate-of-the-art, and it was developed to be fast, robust, and practical. Forexample, it can process short documents such as news articles or essays in lessthan a second.
arxiv-10500-249 | Bounded-Distortion Metric Learning | http://arxiv.org/pdf/1505.02377v1.pdf | author:Renjie Liao, Jianping Shi, Ziyang Ma, Jun Zhu, Jiaya Jia category:cs.LG published:2015-05-10 summary:Metric learning aims to embed one metric space into another to benefit taskslike classification and clustering. Although a greatly distorted metric spacehas a high degree of freedom to fit training data, it is prone to overfittingand numerical inaccuracy. This paper presents {\it bounded-distortion metriclearning} (BDML), a new metric learning framework which amounts to finding anoptimal Mahalanobis metric space with a bounded-distortion constraint. Anefficient solver based on the multiplicative weights update method is proposed.Moreover, we generalize BDML to pseudo-metric learning and devise thesemidefinite relaxation and a randomized algorithm to approximately solve it.We further provide theoretical analysis to show that distortion is a keyingredient for stability and generalization ability of our BDML algorithm.Extensive experiments on several benchmark datasets yield promising results.
arxiv-10500-250 | Optimization via Low-rank Approximation for Community Detection in Networks | http://arxiv.org/pdf/1406.0067v2.pdf | author:Can M. Le, Elizaveta Levina, Roman Vershynin category:stat.ML cs.SI math.ST physics.soc-ph stat.TH 62E10, 62G05 published:2014-05-31 summary:Community detection is one of the fundamental problems of network analysis,for which a number of methods have been proposed. Most model-based orcriteria-based methods have to solve an optimization problem over a discreteset of labels to find communities, which is computationally infeasible. Somefast spectral algorithms have been proposed for specific methods or models, butonly on a case-by-case basis. Here we propose a general approach for maximizinga function of a network adjacency matrix over discrete labels by projecting theset of labels onto a subspace approximating the leading eigenvectors of theexpected adjacency matrix. This projection onto a low-dimensional space makesthe feasible set of labels much smaller and the optimization problem mucheasier. We prove a general result about this method and show how to apply it toseveral previously proposed community detection criteria, establishing itsconsistency for label estimation in each case and demonstrating the fundamentalconnection between spectral properties of the network and various model-basedapproaches to community detection. Simulations and applications to real-worlddata are included to demonstrate our method performs well for multiple problemsover a wide range of parameters.
arxiv-10500-251 | Bayesian Sparse Tucker Models for Dimension Reduction and Tensor Completion | http://arxiv.org/pdf/1505.02343v1.pdf | author:Qibin Zhao, Liqing Zhang, Andrzej Cichocki category:cs.LG cs.NA stat.ML published:2015-05-10 summary:Tucker decomposition is the cornerstone of modern machine learning ontensorial data analysis, which have attracted considerable attention formultiway feature extraction, compressive sensing, and tensor completion. Themost challenging problem is related to determination of model complexity (i.e.,multilinear rank), especially when noise and missing data are present. Inaddition, existing methods cannot take into account uncertainty information oflatent factors, resulting in low generalization performance. To address theseissues, we present a class of probabilistic generative Tucker models for tensordecomposition and completion with structural sparsity over multilinear latentspace. To exploit structural sparse modeling, we introduce two group sparsityinducing priors by hierarchial representation of Laplace and Student-tdistributions, which facilitates fully posterior inference. For model learning,we derived variational Bayesian inferences over all model (hyper)parameters,and developed efficient and scalable algorithms based on multilinearoperations. Our methods can automatically adapt model complexity and infer anoptimal multilinear rank by the principle of maximum lower bound of modelevidence. Experimental results and comparisons on synthetic, chemometrics andneuroimaging data demonstrate remarkable performance of our models forrecovering ground-truth of multilinear rank and missing entries.
arxiv-10500-252 | Relations Between Adjacency and Modularity Graph Partitioning | http://arxiv.org/pdf/1505.03481v1.pdf | author:Hansi Jiang, Carl Meyer category:stat.ML published:2015-05-09 summary:In this paper the exact linear relation between the leading eigenvector ofthe unnormalized modularity matrix and the eigenvectors of the adjacency matrixis developed. Based on this analysis a method to approximate the leadingeigenvector of the modularity matrix is given, and the relative error of theapproximation is derived. A complete proof of the equivalence betweennormalized modularity clustering and normalized adjacency clustering is alsogiven. A new metric is defined to describe the agreement of two clusteringmethods, and some applications and experiments are given to illustrate andcorroborate the points that are made in the theoretical development.
arxiv-10500-253 | Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes | http://arxiv.org/pdf/1410.6834v2.pdf | author:Yves-Laurent Kom Samo, Stephen Roberts category:stat.ML published:2014-10-24 summary:In this paper we propose the first non-parametric Bayesian model usingGaussian Processes to make inference on Poisson Point Processes withoutresorting to gridding the domain or to introducing latent thinning points.Unlike competing models that scale cubically and have a squared memoryrequirement in the number of data points, our model has a linear complexity andmemory requirement. We propose an MCMC sampler and show that our model isfaster, more accurate and generates less correlated samples than competingmodels on both synthetic and real-life data. Finally, we show that our modeleasily handles data sizes not considered thus far by alternate approaches.
arxiv-10500-254 | Should we really use post-hoc tests based on mean-ranks? | http://arxiv.org/pdf/1505.02288v1.pdf | author:Alessio Benavoli, Giorgio Corani, Francesca Mangili category:cs.LG math.ST q-bio.QM stat.ML stat.TH published:2015-05-09 summary:The statistical comparison of multiple algorithms over multiple data sets isfundamental in machine learning. This is typically carried out by the Friedmantest. When the Friedman test rejects the null hypothesis, multiple comparisonsare carried out to establish which are the significant differences amongalgorithms. The multiple comparisons are usually performed using the mean-rankstest. The aim of this technical note is to discuss the inconsistencies of themean-ranks post-hoc test with the goal of discouraging its use in machinelearning as well as in medicine, psychology, etc.. We show that the outcome ofthe mean-ranks test depends on the pool of algorithms originally included inthe experiment. In other words, the outcome of the comparison betweenalgorithms A and B depends also on the performance of the other algorithmsincluded in the original experiment. This can lead to paradoxical situations.For instance the difference between A and B could be declared significant ifthe pool comprises algorithms C, D, E and not significant if the pool comprisesalgorithms F, G, H. To overcome these issues, we suggest instead to perform themultiple comparison using a test whose outcome only depends on the twoalgorithms being compared, such as the sign-test or the Wilcoxon signed-ranktest.
arxiv-10500-255 | Subset Feature Learning for Fine-Grained Category Classification | http://arxiv.org/pdf/1505.02269v1.pdf | author:Zongyuan Ge, Christopher Mccool, Conrad Sanderson, Peter Corke category:cs.CV published:2015-05-09 summary:Fine-grained categorisation has been a challenging problem due to smallinter-class variation, large intra-class variation and low number of trainingimages. We propose a learning system which first clusters visually similarclasses and then learns deep convolutional neural network features specific toeach subset. Experiments on the popular fine-grained Caltech-UCSD bird datasetshow that the proposed method outperforms recent fine-grained categorisationmethods under the most difficult setting: no bounding boxes are presented attest time. It achieves a mean accuracy of 77.5%, compared to the previous bestperformance of 73.2%. We also show that progressive transfer learning allows usto first learn domain-generic features (for bird classification) which can thenbe adapted to specific set of bird classes, yielding improvements in accuracy.
arxiv-10500-256 | Probabilistic Cascading for Large Scale Hierarchical Classification | http://arxiv.org/pdf/1505.02251v1.pdf | author:Aris Kosmopoulos, Georgios Paliouras, Ion Androutsopoulos category:cs.LG cs.CL cs.IR published:2015-05-09 summary:Hierarchies are frequently used for the organization of objects. Given ahierarchy of classes, two main approaches are used, to automatically classifynew instances: flat classification and cascade classification. Flatclassification ignores the hierarchy, while cascade classification greedilytraverses the hierarchy from the root to the predicted leaf. In this paper wepropose a new approach, which extends cascade classification to predict theright leaf by estimating the probability of each root-to-leaf path. We provideexperimental results which indicate that, using the same classificationalgorithm, one can achieve better results with our approach, compared to thetraditional flat and cascade classifications.
arxiv-10500-257 | Newton Sketch: A Linear-time Optimization Algorithm with Linear-Quadratic Convergence | http://arxiv.org/pdf/1505.02250v1.pdf | author:Mert Pilanci, Martin J. Wainwright category:math.OC cs.DS cs.LG stat.ML published:2015-05-09 summary:We propose a randomized second-order method for optimization known as theNewton Sketch: it is based on performing an approximate Newton step using arandomly projected or sub-sampled Hessian. For self-concordant functions, weprove that the algorithm has super-linear convergence with exponentially highprobability, with convergence and complexity guarantees that are independent ofcondition numbers and related problem-dependent quantities. Given a suitableinitialization, similar guarantees also hold for strongly convex and smoothobjectives without self-concordance. When implemented using randomizedprojections based on a sub-sampled Hadamard basis, the algorithm typically hassubstantially lower complexity than Newton's method. We also describeextensions of our methods to programs involving convex constraints that areequipped with self-concordant barriers. We discuss and illustrate applicationsto linear programs, quadratic programs with convex constraints, logisticregression and other generalized linear models, as well as semidefiniteprograms.
arxiv-10500-258 | Performance Evaluation of Vision-Based Algorithms for MAVs | http://arxiv.org/pdf/1505.02247v1.pdf | author:T. Holzmann, R. Prettenthaler, J. Pestana, D. Muschick, G. Graber, C. Mostegel, F. Fraundorfer, H. Bischof category:cs.CV published:2015-05-09 summary:An important focus of current research in the field of Micro Aerial Vehicles(MAVs) is to increase the safety of their operation in general unstructuredenvironments. Especially indoors, where GPS cannot be used for localization,reliable algorithms for localization and mapping of the environment arenecessary in order to keep an MAV airborne safely. In this paper, we comparevision-based real-time capable methods for localization and mapping and pointout their strengths and weaknesses. Additionally, we describe algorithms forstate estimation, control and navigation, which use the localization andmapping results of our vision-based algorithms as input.
arxiv-10500-259 | Machine Learning Techniques for Intrusion Detection | http://arxiv.org/pdf/1312.2177v2.pdf | author:Mahdi Zamani, Mahnush Movahedi category:cs.CR cs.LG cs.NI C.2.0; K.6.5 published:2013-12-08 summary:An Intrusion Detection System (IDS) is a software that monitors a single or anetwork of computers for malicious activities (attacks) that are aimed atstealing or censoring information or corrupting network protocols. Mosttechniques used in today's IDS are not able to deal with the dynamic andcomplex nature of cyber attacks on computer networks. Hence, efficient adaptivemethods like various techniques of machine learning can result in higherdetection rates, lower false alarm rates and reasonable computation andcommunication costs. In this paper, we study several such schemes and comparetheir performance. We divide the schemes into methods based on classicalartificial intelligence (AI) and methods based on computational intelligence(CI). We explain how various characteristics of CI techniques can be used tobuild efficient IDS.
arxiv-10500-260 | Bilevel approaches for learning of variational imaging models | http://arxiv.org/pdf/1505.02120v1.pdf | author:Luca Calatroni, Cao Chung, Juan Carlos De Los Reyes, Carola-Bibiane Schönlieb, Tuomo Valkonen category:math.OC cs.CV published:2015-05-08 summary:We review some recent learning approaches in variational imaging, based onbilevel optimisation, and emphasize the importance of their treatment infunction space. The paper covers both analytical and numerical techniques.Analytically, we include results on the existence and structure of minimisers,as well as optimality conditions for their characterisation. Based on thisinformation, Newton type methods are studied for the solution of the problemsat hand, combining them with sampling techniques in case of large databases.The computational verification of the developed techniques is extensivelydocumented, covering instances with different type of regularisers, severalnoise models, spatially dependent weights and large image databases.
arxiv-10500-261 | Deep Learning for Medical Image Segmentation | http://arxiv.org/pdf/1505.02000v1.pdf | author:Matthew Lai category:cs.LG cs.AI cs.CV published:2015-05-08 summary:This report provides an overview of the current state of the art deeplearning architectures and optimisation techniques, and uses the ADNIhippocampus MRI dataset as an example to compare the effectiveness andefficiency of different convolutional architectures on the task of patch-based3-dimensional hippocampal segmentation, which is important in the diagnosis ofAlzheimer's Disease. We found that a slightly unconventional "stacked 2D"approach provides much better classification performance than simple 2D patcheswithout requiring significantly more computational power. We also examined thepopular "tri-planar" approach used in some recently published studies, andfound that it provides much better results than the 2D approaches, but alsowith a moderate increase in computational power requirement. Finally, weevaluated a full 3D convolutional architecture, and found that it providesmarginally better results than the tri-planar approach, but at the cost of avery significant increase in computational power requirement.
arxiv-10500-262 | Evolving Boolean Networks with RNA Editing | http://arxiv.org/pdf/1505.01980v1.pdf | author:Larry Bull category:cs.NE q-bio.MN q-bio.PE published:2015-05-08 summary:The editing of transcribed RNA by other molecules such that the form of thefinal product differs from that specified in the corresponding DNA sequence isubiquitous. This paper uses an abstract, tunable Boolean genetic regulatorynetwork model to explore aspects of RNA editing. In particular, it is shown howdynamically altering expressed sequences via a guide RNA-inspired mechanism canbe selected for by simulated evolution under various single and multicellularscenarios.
arxiv-10500-263 | The structure of optimal parameters for image restoration problems | http://arxiv.org/pdf/1505.01953v1.pdf | author:Juan Carlos De Los Reyes, Carola-Bibiane Schönlieb, Tuomo Valkonen category:math.OC cs.CV published:2015-05-08 summary:We study the qualitative properties of optimal regularisation parameters invariational models for image restoration. The parameters are solutions ofbilevel optimisation problems with the image restoration problem as constraint.A general type of regulariser is considered, which encompasses total variation(TV), total generalized variation (TGV) and infimal-convolution total variation(ICTV). We prove that under certain conditions on the given data optimalparameters derived by bilevel optimisation problems exist. A crucial point inthe existence proof turns out to be the boundedness of the optimal parametersaway from $0$ which we prove in this paper. The analysis is done on theoriginal -- in image restoration typically non-smooth variational problem -- aswell as on a smoothed approximation set in Hilbert space which is the oneconsidered in numerical computations. For the smoothed bilevel problem we alsoprove that it $\Gamma$ converges to the original problem as the smoothingvanishes. All analysis is done in function spaces rather than on thediscretised learning problem.
arxiv-10500-264 | Shadow Optimization from Structured Deep Edge Detection | http://arxiv.org/pdf/1505.01589v2.pdf | author:Li Shen, Teck Wee Chua, Karianto Leman category:cs.CV published:2015-05-07 summary:Local structures of shadow boundaries as well as complex interactions ofimage regions remain largely unexploited by previous shadow detectionapproaches. In this paper, we present a novel learning-based framework forshadow region recovery from a single image. We exploit the local structures ofshadow edges by using a structured CNN learning framework. We show that usingthe structured label information in the classification can improve the localconsistency of the results and avoid spurious labelling. We further propose andformulate a shadow/bright measure to model the complex interactions among imageregions. The shadow and bright measures of each patch are computed from theshadow edges detected in the image. Using the global interaction constraints onpatches, we formulate a least-square optimization problem for shadow recoverythat can be solved efficiently. Our shadow recovery method achievesstate-of-the-art results on the major shadow benchmark databases collectedunder various conditions.
arxiv-10500-265 | Noise in Structured-Light Stereo Depth Cameras: Modeling and its Applications | http://arxiv.org/pdf/1505.01936v1.pdf | author:Avishek Chatterjee, Venu Madhav Govindu category:cs.CV published:2015-05-08 summary:Depth maps obtained from commercially available structured-light stereo baseddepth cameras, such as the Kinect, are easy to use but are affected bysignificant amounts of noise. This paper is devoted to a study of the intrinsicnoise characteristics of such depth maps, i.e. the standard deviation of noisein estimated depth varies quadratically with the distance of the object fromthe depth camera. We validate this theoretical model against empiricalobservations and demonstrate the utility of this noise model in three popularapplications: depth map denoising, volumetric scan merging for 3D modeling, andidentification of 3D planes in depth maps.
arxiv-10500-266 | Exact and Heuristic Algorithms for Semi-Nonnegative Matrix Factorization | http://arxiv.org/pdf/1410.7220v3.pdf | author:Nicolas Gillis, Abhishek Kumar category:math.NA cs.LG cs.NA math.OC stat.ML published:2014-10-27 summary:Given a matrix $M$ (not necessarily nonnegative) and a factorization rank$r$, semi-nonnegative matrix factorization (semi-NMF) looks for a matrix $U$with $r$ columns and a nonnegative matrix $V$ with $r$ rows such that $UV$ isthe best possible approximation of $M$ according to some metric. In this paper,we study the properties of semi-NMF from which we develop exact and heuristicalgorithms. Our contribution is threefold. First, we prove that the error of asemi-NMF of rank $r$ has to be smaller than the best unconstrainedapproximation of rank $r-1$. This leads us to a new initialization procedurebased on the singular value decomposition (SVD) with a guarantee on the qualityof the approximation. Second, we propose an exact algorithm (that is, analgorithm that finds an optimal solution), also based on the SVD, for a certainclass of matrices (including nonnegative irreducible matrices) from which wederive an initialization for matrices not belonging to that class. Numericalexperiments illustrate that this second approach performs extremely well, andallows us to compute optimal semi-NMF decompositions in many situations.Finally, we analyze the computational complexity of semi-NMF proving itsNP-hardness, already in the rank-one case (that is, for $r = 1$), and we showthat semi-NMF is sometimes ill-posed (that is, an optimal solution does notexist).
arxiv-10500-267 | Learning to Segment Moving Objects in Videos | http://arxiv.org/pdf/1412.6504v2.pdf | author:Katerina Fragkiadaki, Pablo Arbelaez, Panna Felsen, Jitendra Malik category:cs.CV published:2014-12-19 summary:We segment moving objects in videos by ranking spatio-temporal segmentproposals according to "moving objectness": how likely they are to contain amoving object. In each video frame, we compute segment proposals using multiplefigure-ground segmentations on per frame motion boundaries. We rank them with aMoving Objectness Detector trained on image and motion fields to detect movingobjects and discard over/under segmentations or background parts of the scene.We extend the top ranked segments into spatio-temporal tubes using randomwalkers on motion affinities of dense point trajectories. Our final tuberanking consistently outperforms previous segmentation methods in the twolargest video segmentation benchmarks currently available, for any number ofproposals. Further, our per frame moving object proposals increase thedetection rate up to 7\% over previous state-of-the-art static proposalmethods.
arxiv-10500-268 | PRISM: Person Re-Identification via Structured Matching | http://arxiv.org/pdf/1406.4444v4.pdf | author:Ziming Zhang, Venkatesh Saligrama category:cs.CV cs.LG stat.ML published:2014-06-13 summary:Person re-identification (re-id), an emerging problem in visual surveillance,deals with maintaining entities of individuals whilst they traverse variouslocations surveilled by a camera network. From a visual perspective re-id ischallenging due to significant changes in visual appearance of individuals incameras with different pose, illumination and calibration. Globally thechallenge arises from the need to maintain structurally consistent matchesamong all the individual entities across different camera views. We proposePRISM, a structured matching method to jointly account for these challenges. Weview the global problem as a weighted graph matching problem and estimate edgeweights by learning to predict them based on the co-occurrences of visualpatterns in the training examples. These co-occurrence based scores in turnaccount for appearance changes by inferring likely and unlikely visualco-occurrences appearing in training instances. We implement PRISM on singleshot and multi-shot scenarios. PRISM uniformly outperforms state-of-the-art interms of matching rate while being computationally efficient.
arxiv-10500-269 | Exploiting Image-trained CNN Architectures for Unconstrained Video Classification | http://arxiv.org/pdf/1503.04144v3.pdf | author:Shengxin Zha, Florian Luisier, Walter Andrews, Nitish Srivastava, Ruslan Salakhutdinov category:cs.CV published:2015-03-13 summary:We conduct an in-depth exploration of different strategies for doing eventdetection in videos using convolutional neural networks (CNNs) trained forimage classification. We study different ways of performing spatial andtemporal pooling, feature normalization, choice of CNN layers as well as choiceof classifiers. Making judicious choices along these dimensions led to a verysignificant increase in performance over more naive approaches that have beenused till now. We evaluate our approach on the challenging TRECVID MED'14dataset with two popular CNN architectures pretrained on ImageNet. On thisMED'14 dataset, our methods, based entirely on image-trained CNN features, canoutperform several state-of-the-art non-CNN models. Our proposed late fusion ofCNN- and motion-based features can further increase the mean average precision(mAP) on MED'14 from 34.95% to 38.74%. The fusion approach achieves thestate-of-the-art classification performance on the challenging UCF-101 dataset.
arxiv-10500-270 | Optimal Neuron Selection: NK Echo State Networks for Reinforcement Learning | http://arxiv.org/pdf/1505.01887v1.pdf | author:Darrell Whitley, Renato Tinós, Francisco Chicano category:cs.NE I.2.8 published:2015-05-07 summary:This paper introduces the NK Echo State Network. The problem of learning inthe NK Echo State Network is reduced to the problem of optimizing a specialform of a Spin Glass Problem known as an NK Landscape. No weight adjustment isused; all learning is accomplished by spinning up (turning on) or spinning down(turning off) neurons in order to find a combination of neurons that worktogether to achieve the desired computation. For special types of NKLandscapes, an exact global solution can be obtained in polynomial time usingdynamic programming. The NK Echo State Network is applied to a reinforcementlearning problem requiring a recurrent network: balancing two poles on a cartgiven no velocity information. Empirical results shows that the NK Echo StateNetwork learns very rapidly and yields very good generalization.
arxiv-10500-271 | Learning to Search for Dependencies | http://arxiv.org/pdf/1503.05615v2.pdf | author:Kai-Wei Chang, He He, Hal Daumé III, John Langford category:cs.CL cs.LG published:2015-03-18 summary:We demonstrate that a dependency parser can be built using a creditassignment compiler which removes the burden of worrying about low-levelmachine learning details from the parser implementation. The result is a simpleparser which robustly applies to many languages that provides similarstatistical and computational performance with best-to-date transition-basedparsing approaches, while avoiding various downsides including randomization,extra feature requirements, and custom learning algorithms.
arxiv-10500-272 | DART: Dropouts meet Multiple Additive Regression Trees | http://arxiv.org/pdf/1505.01866v1.pdf | author:K. V. Rashmi, Ran Gilad-Bachrach category:cs.LG stat.ML published:2015-05-07 summary:Multiple Additive Regression Trees (MART), an ensemble model of boostedregression trees, is known to deliver high prediction accuracy for diversetasks, and it is widely used in practice. However, it suffers an issue which wecall over-specialization, wherein trees added at later iterations tend toimpact the prediction of only a few instances, and make negligible contributiontowards the remaining instances. This negatively affects the performance of themodel on unseen data, and also makes the model over-sensitive to thecontributions of the few, initially added tress. We show that the commonly usedtool to address this issue, that of shrinkage, alleviates the problem only to acertain extent and the fundamental issue of over-specialization still remains.In this work, we explore a different approach to address the problem that ofemploying dropouts, a tool that has been recently proposed in the context oflearning deep neural networks. We propose a novel way of employing dropouts inMART, resulting in the DART algorithm. We evaluate DART on ranking, regressionand classification tasks, using large scale, publicly available datasets, andshow that DART outperforms MART in each of the tasks, with a significantmargin. We also show that DART overcomes the issue of over-specialization to aconsiderable extent.
arxiv-10500-273 | Optimal Decision-Theoretic Classification Using Non-Decomposable Performance Metrics | http://arxiv.org/pdf/1505.01802v1.pdf | author:Nagarajan Natarajan, Oluwasanmi Koyejo, Pradeep Ravikumar, Inderjit S. Dhillon category:cs.LG stat.ML published:2015-05-07 summary:We provide a general theoretical analysis of expected out-of-sample utility,also referred to as decision-theoretic classification, for non-decomposablebinary classification metrics such as F-measure and Jaccard coefficient. Ourkey result is that the expected out-of-sample utility for many performancemetrics is provably optimized by a classifier which is equivalent to a signedthresholding of the conditional probability of the positive class. Our analysisbridges a gap in the literature on binary classification, revealed in light ofrecent results for non-decomposable metrics in population utility maximizationstyle classification. Our results identify checkable properties of aperformance metric which are sufficient to guarantee a probability rankingprinciple. We propose consistent estimators for optimal expected out-of-sampleclassification. As a consequence of the probability ranking principle,computational requirements can be reduced from exponential to cubic complexityin the general case, and further reduced to quadratic complexity in specialcases. We provide empirical results on simulated and benchmark datasetsevaluating the performance of the proposed algorithms for decision-theoreticclassification and comparing them to baseline and state-of-the-art methods inpopulation utility maximization for non-decomposable metrics.
arxiv-10500-274 | Contextual Analysis for Middle Eastern Languages with Hidden Markov Models | http://arxiv.org/pdf/1505.01757v1.pdf | author:Kazem Taghva category:cs.CL cs.AI published:2015-05-07 summary:Displaying a document in Middle Eastern languages requires contextualanalysis due to different presentational forms for each character of thealphabet. The words of the document will be formed by the joining of thecorrect positional glyphs representing corresponding presentational forms ofthe characters. A set of rules defines the joining of the glyphs. As usual,these rules vary from language to language and are subject to interpretation bythe software developers. In this paper, we propose a machine learning approach for contextual analysisbased on the first order Hidden Markov Model. We will design and build a modelfor the Farsi language to exhibit this technology. The Farsi model achieves 94\% accuracy with the training based on a short list of 89 Farsi vocabulariesconsisting of 2780 Farsi characters. The experiment can be easily extended to many languages including Arabic,Urdu, and Sindhi. Furthermore, the advantage of this approach is that the samesoftware can be used to perform contextual analysis without coding complexrules for each specific language. Of particular interest is that the languageswith fewer speakers can have greater representation on the web, since they aretypically ignored by software developers due to lack of financial incentives.
arxiv-10500-275 | Fast Spectral Unmixing based on Dykstra's Alternating Projection | http://arxiv.org/pdf/1505.01740v1.pdf | author:Qi Wei, Jose Bioucas-Dias, Nicolas Dobigeon, Jean-Yves Tourneret category:cs.CV published:2015-05-07 summary:This paper presents a fast spectral unmixing algorithm based on Dykstra'salternating projection. The proposed algorithm formulates the fully constrainedleast squares optimization problem associated with the spectral unmixing taskas an unconstrained regression problem followed by a projection onto theintersection of several closed convex sets. This projection is achieved byiteratively projecting onto each of the convex sets individually, followingDyktra's scheme. The sequence thus obtained is guaranteed to converge to thesought projection. Thanks to the preliminary matrix decomposition and variablesubstitution, the projection is implemented intrinsically in a subspace, whosedimension is very often much lower than the number of bands. A benefit of thisstrategy is that the order of the computational complexity for each projectionis decreased from quadratic to linear time. Numerical experiments consideringdiverse spectral unmixing scenarios provide evidence that the proposedalgorithm competes with the state-of-the-art, namely when the number ofendmembers is relatively small, a circumstance often observed in realhyperspectral applications.
arxiv-10500-276 | An inexact Newton-Krylov algorithm for constrained diffeomorphic image registration | http://arxiv.org/pdf/1408.6299v3.pdf | author:Andreas Mang, George Biros category:math.NA cs.CV cs.NA math.OC published:2014-08-27 summary:We propose numerical algorithms for solving large deformation diffeomorphicimage registration problems. We formulate the nonrigid image registrationproblem as a problem of optimal control. This leads to an infinite-dimensionalpartial differential equation (PDE) constrained optimization problem. The PDE constraint consists, in its simplest form, of a hyperbolic transportequation for the evolution of the image intensity. The control variable is thevelocity field. Tikhonov regularization on the control ensures well-posedness.We consider standard smoothness regularization based on $H^1$- or$H^2$-seminorms. We augment this regularization scheme with a constraint on thedivergence of the velocity field rendering the deformation incompressible andthus ensuring that the determinant of the deformation gradient is equal to one,up to the numerical error. We use a Fourier pseudospectral discretization in space and a Chebyshevpseudospectral discretization in time. We use a preconditioned, globalized,matrix-free, inexact Newton-Krylov method for numerical optimization. Aparameter continuation is designed to estimate an optimal regularizationparameter. Regularity is ensured by controlling the geometric properties of thedeformation field. Overall, we arrive at a black-box solver. We study spectralproperties of the Hessian, grid convergence, numerical accuracy, computationalefficiency, and deformation regularity of our scheme. We compare the designedNewton-Krylov methods with a globalized preconditioned gradient descent. Westudy the influence of a varying number of unknowns in time. The reported results demonstrate excellent numerical accuracy, guaranteedlocal deformation regularity, and computational efficiency with an optionalcontrol on local mass conservation. The Newton-Krylov methods clearlyoutperform the Picard method if high accuracy of the inversion is required.
arxiv-10500-277 | Characterizing the Google Books corpus: Strong limits to inferences of socio-cultural and linguistic evolution | http://arxiv.org/pdf/1501.00960v2.pdf | author:Eitan Adam Pechenick, Christopher M. Danforth, Peter Sheridan Dodds category:physics.soc-ph cs.CL stat.AP published:2015-01-05 summary:It is tempting to treat frequency trends from the Google Books data sets asindicators of the "true" popularity of various words and phrases. Doing soallows us to draw quantitatively strong conclusions about the evolution ofcultural perception of a given topic, such as time or gender. However, theGoogle Books corpus suffers from a number of limitations which make it anobscure mask of cultural popularity. A primary issue is that the corpus is ineffect a library, containing one of each book. A single, prolific author isthereby able to noticeably insert new phrases into the Google Books lexicon,whether the author is widely read or not. With this understood, the GoogleBooks corpus remains an important data set to be considered more lexicon-likethan text-like. Here, we show that a distinct problematic feature arises fromthe inclusion of scientific texts, which have become an increasinglysubstantive portion of the corpus throughout the 1900s. The result is a surgeof phrases typical to academic articles but less common in general, such asreferences to time in the form of citations. We highlight these dynamics byexamining and comparing major contributions to the statistical divergence ofEnglish data sets between decades in the period 1800--2000. We find that onlythe English Fiction data set from the second version of the corpus is notheavily affected by professional texts, in clear contrast to the first versionof the fiction data set and both unfiltered English data sets. Our findingsemphasize the need to fully characterize the dynamics of the Google Bookscorpus before using these data sets to draw broad conclusions about culturaland linguistic evolution.
arxiv-10500-278 | Data Fusion of Objects Using Techniques Such as Laser Scanning, Structured Light and Photogrammetry for Cultural Heritage Applications | http://arxiv.org/pdf/1505.01631v1.pdf | author:Citlalli Gamez Serna, Ruven Pillay, Alain Tremeau category:cs.CV published:2015-05-07 summary:In this paper we present a semi-automatic 2D-3D local registration pipelinecapable of coloring 3D models obtained from 3D scanners by using uncalibratedimages. The proposed pipeline exploits the Structure from Motion (SfM)technique in order to reconstruct a sparse representation of the 3D object andobtain the camera parameters from image feature matches. We then coarselyregister the reconstructed 3D model to the scanned one through the ScaleIterative Closest Point (SICP) algorithm. SICP provides the global scale,rotation and translation parameters, using minimal manual user intervention. Inthe final processing stage, a local registration refinement algorithm optimizesthe color projection of the aligned photos on the 3D object removing theblurring/ghosting artefacts introduced due to small inaccuracies during theregistration. The proposed pipeline is capable of handling real world caseswith a range of characteristics from objects with low level geometric featuresto complex ones.
arxiv-10500-279 | Bayesian Optimization for Synthetic Gene Design | http://arxiv.org/pdf/1505.01627v1.pdf | author:Javier González, Joseph Longworth, David C. James, Neil D. Lawrence category:stat.ML published:2015-05-07 summary:We address the problem of synthetic gene design using Bayesian optimization.The main issue when designing a gene is that the design space is defined interms of long strings of characters of different lengths, which renders theoptimization intractable. We propose a three-step approach to deal with thisissue. First, we use a Gaussian process model to emulate the behavior of thecell. As inputs of the model, we use a set of biologically meaningful genefeatures, which allows us to define optimal gene designs rules. Based on themodel outputs we define a multi-task acquisition function to optimizesimultaneously severals aspects of interest. Finally, we define an evaluationfunction, which allow us to rank sets of candidate gene sequences that arecoherent with the optimal design strategy. We illustrate the performance ofthis approach in a real gene design experiment with mammalian cells.
arxiv-10500-280 | Context-Aware Mobility Management in HetNets: A Reinforcement Learning Approach | http://arxiv.org/pdf/1505.01625v1.pdf | author:Meryem Simsek, Mehdi Bennis, Ismail Güvenc category:cs.NI cs.LG published:2015-05-07 summary:The use of small cell deployments in heterogeneous network (HetNet)environments is expected to be a key feature of 4G networks and beyond, andessential for providing higher user throughput and cell-edge coverage. However,due to different coverage sizes of macro and pico base stations (BSs), such aparadigm shift introduces additional requirements and challenges in densenetworks. Among these challenges is the handover performance of user equipment(UEs), which will be impacted especially when high velocity UEs traversepicocells. In this paper, we propose a coordination-based and context-awaremobility management (MM) procedure for small cell networks using tools fromreinforcement learning. Here, macro and pico BSs jointly learn their long-termtraffic loads and optimal cell range expansion, and schedule their UEs based ontheir velocities and historical rates (exchanged among tiers). The proposedapproach is shown to not only outperform the classical MM in terms of UEthroughput, but also to enable better fairness. In average, a gain of up to80\% is achieved for UE throughput, while the handover failure probability isreduced up to a factor of three by the proposed learning based MM approaches.
arxiv-10500-281 | Blind Compressive Sensing Framework for Collaborative Filtering | http://arxiv.org/pdf/1505.01621v1.pdf | author:Anupriya Gogna, Angshul Majumdar category:cs.IR cs.LG published:2015-05-07 summary:Existing works based on latent factor models have focused on representing therating matrix as a product of user and item latent factor matrices, both beingdense. Latent (factor) vectors define the degree to which a trait is possessedby an item or the affinity of user towards that trait. A dense user matrix is areasonable assumption as each user will like/dislike a trait to certain extent.However, any item will possess only a few of the attributes and never all.Hence, the item matrix should ideally have a sparse structure rather than adense one as formulated in earlier works. Therefore we propose to factor theratings matrix into a dense user matrix and a sparse item matrix which leads usto the Blind Compressed Sensing (BCS) framework. We derive an efficientalgorithm for solving the BCS problem based on Majorization Minimization (MM)technique. Our proposed approach is able to achieve significantly higheraccuracy and shorter run times as compared to existing approaches.
arxiv-10500-282 | Fast Differentially Private Matrix Factorization | http://arxiv.org/pdf/1505.01419v2.pdf | author:Ziqi Liu, Yu-Xiang Wang, Alexander J. Smola category:cs.LG cs.AI published:2015-05-06 summary:Differentially private collaborative filtering is a challenging task, both interms of accuracy and speed. We present a simple algorithm that is provablydifferentially private, while offering good performance, using a novelconnection of differential privacy to Bayesian posterior sampling viaStochastic Gradient Langevin Dynamics. Due to its simplicity the algorithmlends itself to efficient implementation. By careful systems design and byexploiting the power law behavior of the data to maximize CPU cache bandwidthwe are able to generate 1024 dimensional models at a rate of 8.5 millionrecommendations per second on a single PC.
arxiv-10500-283 | Filter characteristics in image decomposition with singular spectrum analysis | http://arxiv.org/pdf/1505.01599v1.pdf | author:Kenji Kume, Naoko Nose-Togawa category:cs.CV cs.NA published:2015-05-07 summary:Singular spectrum analysis is developed as a nonparametric spectraldecomposition of a time series. It can be easily extended to the decompositionof multidimensional lattice-like data through the filtering interpretation. Inthis viewpoint, the singular spectrum analysis can be understood as theadaptive and optimal generation of the filters and their two-steppoint-symmetric operation to the original data. In this paper, we point outthat, when applied to the multidimensional data, the adaptively generatedfilters exhibit symmetry properties resulting from the bisymmetric nature ofthe lag-covariance matrices. The eigenvectors of the lag-covariance matrix areeither symmetric or antisymmetric, and for the 2D image data, these lead to thedifferential-type filters with even- or odd-order derivatives. The dominantfilter is a smoothing filter, reflecting the dominance of low-frequencycomponents of the photo images. The others are the edge-enhancement or thenoise filters corresponding to the band-pass or the high-pass filters. Theimplication of the decomposition to the image denoising is briefly discussed.
arxiv-10500-284 | Learning and Optimization with Submodular Functions | http://arxiv.org/pdf/1505.01576v1.pdf | author:Bharath Sankaran, Marjan Ghazvininejad, Xinran He, David Kale, Liron Cohen category:cs.LG published:2015-05-07 summary:In many naturally occurring optimization problems one needs to ensure thatthe definition of the optimization problem lends itself to solutions that aretractable to compute. In cases where exact solutions cannot be computedtractably, it is beneficial to have strong guarantees on the tractableapproximate solutions. In order operate under these criterion most optimizationproblems are cast under the umbrella of convexity or submodularity. In thisreport we will study design and optimization over a common class of functionscalled submodular functions. Set functions, and specifically submodular setfunctions, characterize a wide variety of naturally occurring optimizationproblems, and the property of submodularity of set functions has deeptheoretical consequences with wide ranging applications. Informally, theproperty of submodularity of set functions concerns the intuitive "principle ofdiminishing returns. This property states that adding an element to a smallerset has more value than adding it to a larger set. Common examples ofsubmodular monotone functions are entropies, concave functions of cardinality,and matroid rank functions; non-monotone examples include graph cuts, networkflows, and mutual information. In this paper we will review the formal definition of submodularity; theoptimization of submodular functions, both maximization and minimization; andfinally discuss some applications in relation to learning and reasoning usingsubmodular functions.
arxiv-10500-285 | Adaptive Nonparametric Image Parsing | http://arxiv.org/pdf/1505.01560v1.pdf | author:Tam V. Nguyen, Canyi Lu, Jose Sepulveda, Shuicheng Yan category:cs.CV published:2015-05-07 summary:In this paper, we present an adaptive nonparametric solution to the imageparsing task, namely annotating each image pixel with its correspondingcategory label. For a given test image, first, a locality-aware retrieval setis extracted from the training data based on super-pixel matching similarities,which are augmented with feature extraction for better differentiation of localsuper-pixels. Then, the category of each super-pixel is initialized by themajority vote of the $k$-nearest-neighbor super-pixels in the retrieval set.Instead of fixing $k$ as in traditional non-parametric approaches, here wepropose a novel adaptive nonparametric approach which determines thesample-specific k for each test image. In particular, $k$ is adaptively set tobe the number of the fewest nearest super-pixels which the images in theretrieval set can use to get the best category prediction. Finally, the initialsuper-pixel labels are further refined by contextual smoothing. Extensiveexperiments on challenging datasets demonstrate the superiority of the newsolution over other state-of-the-art nonparametric solutions.
arxiv-10500-286 | Semi-Orthogonal Multilinear PCA with Relaxed Start | http://arxiv.org/pdf/1504.08142v2.pdf | author:Qiquan Shi, Haiping Lu category:stat.ML cs.CV cs.LG I.2.6 published:2015-04-30 summary:Principal component analysis (PCA) is an unsupervised method for learninglow-dimensional features with orthogonal projections. Multilinear PCA methodsextend PCA to deal with multidimensional data (tensors) directly viatensor-to-tensor projection or tensor-to-vector projection (TVP). However,under the TVP setting, it is difficult to develop an effective multilinear PCAmethod with the orthogonality constraint. This paper tackles this problem byproposing a novel Semi-Orthogonal Multilinear PCA (SO-MPCA) approach. SO-MPCAlearns low-dimensional features directly from tensors via TVP by imposing theorthogonality constraint in only one mode. This formulation results in morecaptured variance and more learned features than full orthogonality. For bettergeneralization, we further introduce a relaxed start (RS) strategy to getSO-MPCA-RS by fixing the starting projection vectors, which increases the biasand reduces the variance of the learning model. Experiments on both face (2D)and gait (3D) data demonstrate that SO-MPCA-RS outperforms other competingalgorithms on the whole, and the relaxed start strategy is also effective forother TVP-based PCA methods.
arxiv-10500-287 | Graphical Potential Games | http://arxiv.org/pdf/1505.01539v1.pdf | author:Luis E. Ortiz category:cs.GT cs.AI stat.ML published:2015-05-06 summary:Potential games, originally introduced in the early 1990's by Lloyd Shapley,the 2012 Nobel Laureate in Economics, and his colleague Dov Monderer, are avery important class of models in game theory. They have special propertiessuch as the existence of Nash equilibria in pure strategies. This noteintroduces graphical versions of potential games. Special cases of graphicalpotential games have already found applicability in many areas of science andengineering beyond economics, including artificial intelligence, computervision, and machine learning. They have been effectively applied to the studyand solution of important real-world problems such as routing and congestion innetworks, distributed resource allocation (e.g., public goods), andrelaxation-labeling for image segmentation. Implicit use of graphical potentialgames goes back at least 40 years. Several classes of games considered standardin the literature, including coordination games, local interaction games,lattice games, congestion games, and party-affiliation games, are instances ofgraphical potential games. This note provides several characterizations ofgraphical potential games by leveraging well-known results from the literatureon probabilistic graphical models. A major contribution of the work presentedhere that particularly distinguishes it from previous work is establishing thatthe convergence of certain type of game-playing rules implies that theagents/players must be embedded in some graphical potential game.
arxiv-10500-288 | Category-Specific Object Reconstruction from a Single Image | http://arxiv.org/pdf/1411.6069v2.pdf | author:Abhishek Kar, Shubham Tulsiani, João Carreira, Jitendra Malik category:cs.CV published:2014-11-22 summary:Object reconstruction from a single image -- in the wild -- is a problemwhere we can make progress and get meaningful results today. This is the mainmessage of this paper, which introduces an automated pipeline with pixels asinputs and 3D surfaces of various rigid categories as outputs in images ofrealistic scenes. At the core of our approach are deformable 3D models that canbe learned from 2D annotations available in existing object detection datasets,that can be driven by noisy automatic object segmentations and which wecomplement with a bottom-up module for recovering high-frequency shape details.We perform a comprehensive quantitative analysis and ablation study of ourapproach using the recently introduced PASCAL 3D+ dataset and show veryencouraging automatic reconstructions on PASCAL VOC.
arxiv-10500-289 | Depth-based hand pose estimation: methods, data, and challenges | http://arxiv.org/pdf/1504.06378v2.pdf | author:James Steven Supancic III, Gregory Rogez, Yi Yang, Jamie Shotton, Deva Ramanan category:cs.CV published:2015-04-24 summary:Hand pose estimation has matured rapidly in recent years. The introduction ofcommodity depth sensors and a multitude of practical applications have spurrednew advances. We provide an extensive analysis of the state-of-the-art,focusing on hand pose estimation from a single depth frame. To do so, we haveimplemented a considerable number of systems, and will release all software andevaluation code. We summarize important conclusions here: (1) Pose estimationappears roughly solved for scenes with isolated hands. However, methods stillstruggle to analyze cluttered scenes where hands may be interacting with nearbyobjects and surfaces. To spur further progress we introduce a challenging newdataset with diverse, cluttered scenes. (2) Many methods evaluate themselveswith disparate criteria, making comparisons difficult. We define a consistentevaluation criteria, rigorously motivated by human experiments. (3) Weintroduce a simple nearest-neighbor baseline that outperforms most existingsystems. This implies that most systems do not generalize beyond their trainingsets. This also reinforces the under-appreciated point that training data is asimportant as the model itself. We conclude with directions for future progress.
arxiv-10500-290 | Retaining Experience and Growing Solutions | http://arxiv.org/pdf/1505.01474v1.pdf | author:Robyn Ffrancon category:cs.NE published:2015-05-06 summary:Generally, when genetic programming (GP) is used for function synthesis anyvaluable experience gained by the system is lost from one problem to the next,even when the problems are closely related. With the aim of developing a systemwhich retains beneficial experience from problem to problem, this paperintroduces the novel Node-by-Node Growth Solver (NNGS) algorithm which featuresa component, called the controller, which can be adapted and improved for useacross a set of related problems. NNGS grows a single solution tree from rootto leaves. Using semantic backpropagation and acting locally on each node inturn, the algorithm employs the controller to assign subsequent child nodesuntil a fully formed solution is generated. The aim of this paper is to pave a path towards the use of a neural networkas the controller component and also, separately, towards the use of meta-GP asa mechanism for improving the controller component. A proof-of-conceptcontroller is discussed which demonstrates the success and potential of theNNGS algorithm. In this case, the controller constitutes a set of hand writtenrules which can be used to deterministically and greedily solve standardBoolean function synthesis benchmarks. Even before employing machine learningto improve the controller, the algorithm vastly outperforms other well knownrecent algorithms on run times, maintains comparable solution sizes, and has a100% success rate on all Boolean function synthesis benchmarks tested so far.
arxiv-10500-291 | Deviation Based Pooling Strategies For Full Reference Image Quality Assessment | http://arxiv.org/pdf/1504.06786v2.pdf | author:Hossein Ziaei Nafchi, Rachid Hedjam, Atena Shahkolaei, Mohamed Cheriet category:cs.MM cs.CV published:2015-04-26 summary:The state-of-the-art pooling strategies for perceptual image qualityassessment (IQA) are based on the mean and the weighted mean. They are robustpooling strategies which usually provide a moderate to high performance fordifferent IQAs. Recently, standard deviation (SD) pooling was also proposed.Although, this deviation pooling provides a very high performance for a fewIQAs, its performance is lower than mean poolings for many other IQAs. In thispaper, we propose to use the mean absolute deviation (MAD) and show that it isa more robust and accurate pooling strategy for a wider range of IQAs. In fact,MAD pooling has the advantages of both mean pooling and SD pooling. The jointcomputation and use of the MAD and SD pooling strategies is also considered inthis paper. Experimental results provide useful information on the choice ofthe proper deviation pooling strategy for different IQA models.
arxiv-10500-292 | Pooled Motion Features for First-Person Videos | http://arxiv.org/pdf/1412.6505v2.pdf | author:M. S. Ryoo, Brandon Rothrock, Larry Matthies category:cs.CV published:2014-12-19 summary:In this paper, we present a new feature representation for first-personvideos. In first-person video understanding (e.g., activity recognition), it isvery important to capture both entire scene dynamics (i.e., egomotion) andsalient local motion observed in videos. We describe a representation frameworkbased on time series pooling, which is designed to abstractshort-term/long-term changes in feature descriptor elements. The idea is tokeep track of how descriptor values are changing over time and summarize themto represent motion in the activity video. The framework is general, handlingany types of per-frame feature descriptors including conventional motiondescriptors like histogram of optical flows (HOF) as well as appearancedescriptors from more recent convolutional neural networks (CNN). Weexperimentally confirm that our approach clearly outperforms previous featurerepresentations including bag-of-visual-words and improved Fisher vector (IFV)when using identical underlying feature descriptors. We also confirm that ourfeature representation has superior performance to existing state-of-the-artfeatures like local spatio-temporal features and Improved Trajectory Features(originally developed for 3rd-person videos) when handling first-person videos.Multiple first-person activity datasets were tested under various settings toconfirm these findings.
arxiv-10500-293 | Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence | http://arxiv.org/pdf/1505.01462v1.pdf | author:Nihar B. Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh, Kannan Ramchandran, Martin J. Wainwright category:cs.LG cs.IT math.IT stat.ML published:2015-05-06 summary:Data in the form of pairwise comparisons arises in many domains, includingpreference elicitation, sporting competitions, and peer grading among others.We consider parametric ordinal models for such pairwise comparison datainvolving a latent vector $w^* \in \mathbb{R}^d$ that represents the"qualities" of the $d$ items being compared; this class of models includes thetwo most widely used parametric models--the Bradley-Terry-Luce (BTL) and theThurstone models. Working within a standard minimax framework, we provide tightupper and lower bounds on the optimal error in estimating the quality scorevector $w^*$ under this class of models. The bounds depend on the topology ofthe comparison graph induced by the subset of pairs being compared via itsLaplacian spectrum. Thus, in settings where the subset of pairs may be chosen,our results provide principled guidelines for making this choice. Finally, wecompare these error rates to those under cardinal measurement models and showthat the error rates in the ordinal and cardinal settings have identicalscalings apart from constant pre-factors.
arxiv-10500-294 | Efficient Image-Space Extraction and Representation of 3D Surface Topography | http://arxiv.org/pdf/1504.08308v3.pdf | author:Matthias Zeppelzauer, Markus Seidl category:cs.CV published:2015-04-30 summary:Surface topography refers to the geometric micro-structure of a surface anddefines its tactile characteristics (typically in the sub-millimeter range).High-resolution 3D scanning techniques developed recently enable the 3Dreconstruction of surfaces including their surface topography. In his paper, wepresent an efficient image-space technique for the extraction of surfacetopography from high-resolution 3D reconstructions. Additionally, we filternoise and enhance topographic attributes to obtain an improved representationfor subsequent topography classification. Comprehensive experiments show thatthe our representation captures well topographic attributes and significantlyimproves classification performance compared to alternative 2D and 3Drepresentations.
arxiv-10500-295 | Mining Scientific Papers for Bibliometrics: a (very) Brief Survey of Methods and Tools | http://arxiv.org/pdf/1505.01393v1.pdf | author:Iana Atanassova, Marc Bertin, Philipp Mayr category:cs.DL cs.CL published:2015-05-06 summary:The Open Access movement in scientific publishing and search engines likeGoogle Scholar have made scientific articles more broadly accessible. Duringthe last decade, the availability of scientific papers in full text has becomemore and more widespread thanks to the growing number of publications on onlineplatforms such as ArXiv and CiteSeer. The efforts to provide articles inmachine-readable formats and the rise of Open Access publishing have resultedin a number of standardized formats for scientific papers (such as NLM-JATS,TEI, DocBook). Our aim is to stimulate research at the intersection ofBibliometrics and Computational Linguistics in order to study the waysBibliometrics can benefit from large-scale text analytics and sense mining ofscientific papers, thus exploring the interdisciplinarity of Bibliometrics andNatural Language Processing.
arxiv-10500-296 | Re-scale boosting for regression and classification | http://arxiv.org/pdf/1505.01371v1.pdf | author:Shaobo Lin, Yao Wang, Lin Xu category:cs.LG stat.ML F.2.2 published:2015-05-06 summary:Boosting is a learning scheme that combines weak prediction rules to producea strong composite estimator, with the underlying intuition that one can obtainaccurate prediction rules by combining "rough" ones. Although boosting isproved to be consistent and overfitting-resistant, its numerical convergencerate is relatively slow. The aim of this paper is to develop a new boostingstrategy, called the re-scale boosting (RBoosting), to accelerate the numericalconvergence rate and, consequently, improve the learning performance ofboosting. Our studies show that RBoosting possesses the almost optimalnumerical convergence rate in the sense that, up to a logarithmic factor, itcan reach the minimax nonlinear approximation rate. We then use RBoosting totackle both the classification and regression problems, and deduce a tightgeneralization error estimate. The theoretical and experimental results showthat RBoosting outperforms boosting in terms of generalization.
arxiv-10500-297 | Person Re-identification by Local Maximal Occurrence Representation and Metric Learning | http://arxiv.org/pdf/1406.4216v2.pdf | author:Shengcai Liao, Yang Hu, Xiangyu Zhu, Stan Z. Li category:cs.CV published:2014-06-17 summary:Person re-identification is an important technique towards automatic searchof a person's presence in a surveillance video. Two fundamental problems arecritical for person re-identification, feature representation and metriclearning. An effective feature representation should be robust to illuminationand viewpoint changes, and a discriminant metric should be learned to matchvarious person images. In this paper, we propose an effective featurerepresentation called Local Maximal Occurrence (LOMO), and a subspace andmetric learning method called Cross-view Quadratic Discriminant Analysis(XQDA). The LOMO feature analyzes the horizontal occurrence of local features,and maximizes the occurrence to make a stable representation against viewpointchanges. Besides, to handle illumination variations, we apply the Retinextransform and a scale invariant texture operator. To learn a discriminantmetric, we propose to learn a discriminant low dimensional subspace bycross-view quadratic discriminant analysis, and simultaneously, a QDA metric islearned on the derived subspace. We also present a practical computation methodfor XQDA, as well as its regularization. Experiments on four challenging personre-identification databases, VIPeR, QMUL GRID, CUHK Campus, and CUHK03, showthat the proposed method improves the state-of-the-art rank-1 identificationrates by 2.2%, 4.88%, 28.91%, and 31.55% on the four databases, respectively.
arxiv-10500-298 | Visual Summary of Egocentric Photostreams by Representative Keyframes | http://arxiv.org/pdf/1505.01130v2.pdf | author:Marc Bolaños, Ricard Mestre, Estefanía Talavera, Xavier Giró-i-Nieto, Petia Radeva category:cs.CV cs.IR published:2015-05-05 summary:Building a visual summary from an egocentric photostream captured by alifelogging wearable camera is of high interest for different applications(e.g. memory reinforcement). In this paper, we propose a new summarizationmethod based on keyframes selection that uses visual features extracted bymeans of a convolutional neural network. Our method applies an unsupervisedclustering for dividing the photostreams into events, and finally extracts themost relevant keyframe for each event. We assess the results by applying ablind-taste test on a group of 20 people who assessed the quality of thesummaries.
arxiv-10500-299 | Classification of Occluded Objects using Fast Recurrent Processing | http://arxiv.org/pdf/1505.01350v1.pdf | author:Ozgur Yilmaz category:cs.CV published:2015-05-06 summary:Recurrent neural networks are powerful tools for handling incomplete dataproblems in computer vision, thanks to their significant generativecapabilities. However, the computational demand for these algorithms is toohigh to work in real time, without specialized hardware or software solutions.In this paper, we propose a framework for augmenting recurrent processingcapabilities into a feedforward network without sacrificing much fromcomputational efficiency. We assume a mixture model and generate samples of thelast hidden layer according to the class decisions of the output layer, modifythe hidden layer activity using the samples, and propagate to lower layers. Forvisual occlusion problem, the iterative procedure emulates feedforward-feedbackloop, filling-in the missing hidden layer activity with meaningfulrepresentations. The proposed algorithm is tested on a widely used dataset, andshown to achieve 2$\times$ improvement in classification accuracy for occludedobjects. When compared to Restricted Boltzmann Machines, our algorithm showssuperior performance for occluded object classification.
arxiv-10500-300 | Template-based Monocular 3D Shape Recovery using Laplacian Meshes | http://arxiv.org/pdf/1503.04643v2.pdf | author:Dat Tien Ngo, Jonas Ostlund, Pascal Fua category:cs.CV published:2015-03-16 summary:We show that by extending the Laplacian formalism, which was first introducedin the Graphics community to regularize 3D meshes, we can turn the monocular 3Dshape reconstruction of a deformable surface given correspondences with areference image into a much better-posed problem. This allows us to quickly andreliably eliminate outliers by simply solving a linear least squares problem.This yields an initial 3D shape estimate, which is not necessarily accurate,but whose 2D projections are. The initial shape is then refined by aconstrained optimization problem to output the final surface reconstruction. Our approach allows us to reduce the dimensionality of the surfacereconstruction problem without sacrificing accuracy, thus allowing forreal-time implementations.
