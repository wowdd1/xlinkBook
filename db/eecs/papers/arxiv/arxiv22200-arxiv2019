arxiv-1610-09274 | Toward Implicit Sample Noise Modeling: Deviation-driven Matrix Factorization | http://arxiv.org/abs/1610.09274 | id:1610.09274 author:Guang-He Lee, Shao-Wen Yang, Shou-De Lin category:cs.LG cs.IR stat.ML  published:2016-10-28 summary:The objective function of a matrix factorization model usually aims to minimize the average of a regression error contributed by each element. However, given the existence of stochastic noises, the implicit deviations of sample data from their true values are almost surely diverse, which makes each data point not equally suitable for fitting a model. In this case, simply averaging the cost among data in the objective function is not ideal. Intuitively we would like to emphasize more on the reliable instances (i.e., those contain smaller noise) while training a model. Motivated by such observation, we derive our formula from a theoretical framework for optimal weighting under heteroscedastic noise distribution. Specifically, by modeling and learning the deviation of data, we design a novel matrix factorization model. Our model has two advantages. First, it jointly learns the deviation and conducts dynamic reweighting of instances, allowing the model to converge to a better solution. Second, during learning the deviated instances are assigned lower weights, which leads to faster convergence since the model does not need to overfit the noise. The experiments are conducted in clean recommendation and noisy sensor datasets to test the effectiveness of the model in various scenarios. The results show that our model outperforms the state-of-the-art factorization and deep learning models in both accuracy and efficiency. version:1
arxiv-1610-09269 | Hierarchical Clustering via Spreading Metrics | http://arxiv.org/abs/1610.09269 | id:1610.09269 author:Aurko Roy, Sebastian Pokutta category:cs.LG  published:2016-10-28 summary:We study the cost function for hierarchical clusterings introduced by [arXiv:1510.05043] where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in [arXiv:1510.05043] that a top-down algorithm returns a hierarchical clustering of cost at most $O\left(\alpha_n \log n\right)$ times the cost of the optimal hierarchical clustering, where $\alpha_n$ is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao-Vazirani, the top down algorithm returns a hierarchical clustering of cost at most $O\left(\log^{3/2} n\right)$ times the cost of the optimal solution. We improve this by giving an $O(\log{n})$-approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of \emph{sphere growing} which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an $O(\log{n})$-approximate hierarchical clustering for a generalization of this cost function also studied in [arXiv:1510.05043]. Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We also give constant factor inapproximability results for this problem. version:1
arxiv-1610-09263 | Flexible constrained sampling with guarantees for pattern mining | http://arxiv.org/abs/1610.09263 | id:1610.09263 author:Vladimir Dzyuba, Matthijs van Leeuwen, Luc De Raedt category:cs.AI cs.DB stat.ML  published:2016-10-28 summary:Pattern sampling has been proposed as a potential solution to the infamous pattern explosion. Instead of enumerating all patterns that satisfy the constraints, individual patterns are sampled proportional to a given quality measure. Several sampling algorithms have been proposed, but each of them has its limitations when it comes to 1) flexibility in terms of quality measures and constraints that can be used, and/or 2) guarantees with respect to sampling accuracy. We therefore present Flexics, the first flexible pattern sampler that supports a broad class of quality measures and constraints, while providing strong guarantees regarding sampling accuracy. To achieve this, we leverage the perspective on pattern mining as a constraint satisfaction problem and build upon the latest advances in sampling solutions in SAT as well as existing pattern mining algorithms. Furthermore, the proposed algorithm is applicable to a variety of pattern languages, which allows us to introduce and tackle the novel task of sampling sets of patterns. We introduce and empirically evaluate two variants of Flexics: 1) a generic variant that addresses the well-known itemset sampling task and the novel pattern set sampling task as well as a wide range of expressive constraints within these tasks, and 2) a specialized variant that exploits existing frequent itemset techniques to achieve substantial speed-ups. Experiments show that Flexics is both accurate and efficient, making it a useful tool for pattern-based data exploration. version:1
arxiv-1610-09237 | Learnable Visual Markers | http://arxiv.org/abs/1610.09237 | id:1610.09237 author:Oleg Grinchuk, Vadim Lebedev, Victor Lempitsky category:cs.CV  published:2016-10-28 summary:We propose a new approach to designing visual markers (analogous to QR-codes, markers for augmented reality, and robotic fiducial tags) based on the advances in deep generative networks. In our approach, the markers are obtained as color images synthesized by a deep network from input bit strings, whereas another deep network is trained to recover the bit strings back from the photos of these markers. The two networks are trained simultaneously in a joint backpropagation process that takes characteristic photometric and geometric distortions associated with marker fabrication and marker scanning into account. Additionally, a stylization loss based on statistics of activations in a pretrained classification network can be inserted into the learning in order to shift the marker appearance towards some texture prototype. In the experiments, we demonstrate that the markers obtained using our approach are capable of retaining bit strings that are long enough to be practical. The ability to automatically adapt markers according to the usage scenario and the desired capacity as well as the ability to combine information encoding with artistic stylization are the unique properties of our approach. As a byproduct, our approach provides an insight on the structure of patterns that are most suitable for recognition by ConvNets and on their ability to distinguish composite patterns. version:1
arxiv-1610-09226 | Text Segmentation using Named Entity Recognition and Co-reference Resolution in English and Greek Texts | http://arxiv.org/abs/1610.09226 | id:1610.09226 author:Pavlina Fragkou category:cs.CL cs.IR  published:2016-10-28 summary:In this paper we examine the benefit of performing named entity recognition (NER) and co-reference resolution to an English and a Greek corpus used for text segmentation. The aim here is to examine whether the combination of text segmentation and information extraction can be beneficial for the identification of the various topics that appear in a document. NER was performed manually in the English corpus and was compared with the output produced by publicly available annotation tools while, an already existing tool was used for the Greek corpus. Produced annotations from both corpora were manually corrected and enriched to cover four types of named entities. Co-reference resolution i.e., substitution of every reference of the same instance with the same named entity identifier was subsequently performed. The evaluation, using five text segmentation algorithms for the English corpus and four for the Greek corpus leads to the conclusion that, the benefit highly depends on the segment's topic, the number of named entity instances appearing in it, as well as the segment's length. version:1
arxiv-1610-09225 | Sentiment Analysis of Twitter Data for Predicting Stock Market Movements | http://arxiv.org/abs/1610.09225 | id:1610.09225 author:Venkata Sasank Pagolu, Kamal Nayan Reddy Challa, Ganapati Panda, Babita Majhi category:cs.IR cs.CL cs.SI  published:2016-10-28 summary:Predicting stock market movements is a well-known problem of interest. Now-a-days social media is perfectly representing the public sentiment and opinion about current events. Especially, twitter has attracted a lot of attention from researchers for studying the public sentiments. Stock market prediction on the basis of public sentiments expressed on twitter has been an intriguing field of research. Previous studies have concluded that the aggregate public mood collected from twitter may well be correlated with Dow Jones Industrial Average Index (DJIA). The thesis of this work is to observe how well the changes in stock prices of a company, the rises and falls, are correlated with the public opinions being expressed in tweets about that company. Understanding author's opinion from a piece of text is the objective of sentiment analysis. The present paper have employed two different textual representations, Word2vec and N-gram, for analyzing the public sentiments in tweets. In this paper, we have applied sentiment analysis and supervised machine learning principles to the tweets extracted from twitter and analyze the correlation between stock market movements of a company and sentiments in tweets. In an elaborate way, positive news and tweets in social media about a company would definitely encourage people to invest in the stocks of that company and as a result the stock price of that company would increase. At the end of the paper, it is shown that a strong correlation exists between the rise and falls in stock prices with the public sentiments in tweets. version:1
arxiv-1610-09204 | Judging a Book By its Cover | http://arxiv.org/abs/1610.09204 | id:1610.09204 author:Brian Kenji Iwana, Seiichi Uchida category:cs.CV  published:2016-10-28 summary:Book covers communicate information to potential readers, but can the same information be learned by computers? We propose a method of using a Convolutional Neural Network (CNN) to predict the genre of a book based on the visual clues provided by its cover. The purpose is to investigate whether relationships between books and their covers can be learned. However, determining the genre of a book is a difficult task because covers can be ambiguous and genres can be overarching. Despite this, we show that a CNN can extract features and learn underlying design rules set by the designer to define a genre. Using machine learning, we can bring the large amount of resources available to the book cover design process. version:1
arxiv-1610-09369 | Discriminative Gaifman Models | http://arxiv.org/abs/1610.09369 | id:1610.09369 author:Mathias Niepert category:cs.LG  published:2016-10-28 summary:We present discriminative Gaifman models, a novel family of relational machine learning models. Gaifman models learn feature representations bottom up from representations of locally connected and bounded-size regions of knowledge bases (KBs). Considering local and bounded-size neighborhoods of knowledge bases renders logical inference and learning tractable, mitigates the problem of overfitting, and facilitates weight sharing. Gaifman models sample neighborhoods of knowledge bases so as to make the learned relational models more robust to missing objects and relations which is a common situation in open-world KBs. We present the core ideas of Gaifman models and apply them to large-scale relational learning problems. We also discuss the ways in which Gaifman models relate to some existing relational machine learning approaches. version:1
arxiv-1610-09158 | Towards a continuous modeling of natural language domains | http://arxiv.org/abs/1610.09158 | id:1610.09158 author:Sebastian Ruder, Parsa Ghaffari, John G. Breslin category:cs.CL cs.LG  published:2016-10-28 summary:Humans continuously adapt their style and language to a variety of domains. However, a reliable definition of `domain' has eluded researchers thus far. Additionally, the notion of discrete domains stands in contrast to the multiplicity of heterogeneous domains that humans navigate, many of which overlap. In order to better understand the change and variation of human language, we draw on research in domain adaptation and extend the notion of discrete domains to the continuous spectrum. We propose representation learning-based models that can adapt to continuous domains and detail how these can be used to investigate variation in language. To this end, we propose to use dialogue modeling as a test bed due to its proximity to language modeling and its social component. version:1
arxiv-1610-09157 | Towards automatic pulmonary nodule management in lung cancer screening with deep learning | http://arxiv.org/abs/1610.09157 | id:1610.09157 author:Francesco Ciompi, Kaman Chung, Sarah J. van Riel, Arnaud Arindra Adiyoso Setio, Paul K. Gerke, Colin Jacobs, Ernst Th. Scholten, Cornelia Schaefer-Prokop, Mathilde M. W. Wille, Alfonso Marchiano, Ugo Pastorino, Mathias Prokop, Bram van Ginneken category:cs.CV  published:2016-10-28 summary:The introduction of lung cancer screening programs will produce an unprecedented amount of chest CT scans in the near future, which radiologists will have to read in order to decide on a patient follow-up strategy. According to the current guidelines, the workup of screen-detected nodules strongly relies on nodule size and nodule type. In this paper, we present a deep learning system based on multi-stream multi-scale convolutional networks, which automatically classifies all nodule types relevant for nodule workup. The system processes raw CT data containing a nodule without the need for any additional information such as nodule segmentation or nodule size and learns a representation of 3D data by analyzing an arbitrary number of 2D views of a given nodule. The deep learning system was trained with data from the Italian MILD screening trial and validated on an independent set of data from the Danish DLCST screening trial. We analyze the advantage of processing nodules at multiple scales with a multi-stream convolutional network architecture, and we show that the proposed deep learning system achieves performance at classifying nodule type within the inter-observer variability among four experienced human observers. version:1
arxiv-1610-09156 | Fuzzy Bayesian Learning | http://arxiv.org/abs/1610.09156 | id:1610.09156 author:Indranil Pan, Dirk Bester category:stat.ML cs.AI  published:2016-10-28 summary:In this paper we propose a novel approach for learning from data using rule based fuzzy inference systems where the model parameters are estimated using Bayesian inference and Markov Chain Monte Carlo (MCMC) techniques. We show the applicability of the method for regression and classification tasks using synthetic data-sets and also a real world example in the financial services industry. Then we demonstrate how the method can be extended for knowledge extraction to select the individual rules in a Bayesian way which best explains the given data. Finally we discuss the advantages and pitfalls of using this method over state-of-the-art techniques and highlight the specific class of problems where this would be useful. version:1
arxiv-1610-08844 | Single- and Multi-Task Architectures for Surgical Workflow Challenge at M2CAI 2016 | http://arxiv.org/abs/1610.08844 | id:1610.08844 author:Andru P. Twinanda, Didier Mutter, Jacques Marescaux, Michel de Mathelin, Nicolas Padoy category:cs.CV  published:2016-10-27 summary:The surgical workflow challenge at M2CAI 2016 consists of identifying 8 surgical phases in cholecystectomy procedures. Here, we propose to use deep architectures that are based on our previous work where we presented several architectures to perform multiple recognition tasks on laparoscopic videos. In this technical report, we present the phase recognition results using two architectures: (1) a single-task architecture designed to perform solely the surgical phase recognition task and (2) a multi-task architecture designed to perform jointly phase recognition and tool presence detection. On top of these architectures we propose to use two different approaches to enforce the temporal constraints of the surgical workflow: (1) HMM-based and (2) LSTM-based pipelines. The results show that the LSTM-based approach is able to outperform the HMM-based approach and also to properly enforce the temporal constraints into the recognition process. version:2
arxiv-1610-09127 | A framework for adaptive regularization in streaming Lasso models | http://arxiv.org/abs/1610.09127 | id:1610.09127 author:Ricardo Pio Monti, Christoforos Anagnostopoulos, Giovanni Montana category:stat.ML cs.LG  published:2016-10-28 summary:Large scale, streaming datasets are ubiquitous in modern machine learning. Streaming algorithms must be scalable, amenable to incremental training and robust to the presence of non-stationarity. In this work consider the problem of learning $\ell_1$ regularized linear models in the context of streaming data. In particular, the focus of this work revolves around how to select the regularization parameter when data arrives sequentially and the underlying distribution is non-stationary (implying the choice of optimal regularization parameter is itself time-varying). We propose a novel framework through which to infer an adaptive regularization parameter. Our approach employs an $\ell_1$ penalty constraint where the corresponding sparsity parameter is iteratively updated via stochastic gradient descent. This serves to reformulate the choice of regularization parameter in a principled framework for online learning and allows for the derivation of convergence guarantees in a non-stochastic setting. We validate our approach using simulated and real datasets and present an application to a neuroimaging dataset. version:1
arxiv-1610-09112 | Decentralized Clustering and Linking by Networked Agents | http://arxiv.org/abs/1610.09112 | id:1610.09112 author:Sahar Khawatmi, Ali H. Sayed, Abdelhak M. Zoubir category:math.OC cs.MA stat.ML  published:2016-10-28 summary:We consider the problem of decentralized clustering and estimation over multi-task networks, where agents infer and track different models of interest. The agents do not know beforehand which model is generating their own data. They also do not know which agents in their neighborhood belong to the same cluster. We propose a decentralized clustering algorithm aimed at identifying and forming clusters of agents of similar objectives, and at guiding cooperation to enhance the inference performance. One key feature of the proposed technique is the integration of the learning and clustering tasks into a single strategy. We analyze the performance of the procedure and show that the error probabilities of types I and II decay exponentially to zero with the step-size parameter. While links between agents following different objectives are ignored in the clustering process, we nevertheless show how to exploit these links to relay critical information across the network for enhanced performance. Simulation results illustrate the performance of the proposed method in comparison to other useful techniques. version:1
arxiv-1610-09110 | $f$-Divergence Inequalities via Functional Domination | http://arxiv.org/abs/1610.09110 | id:1610.09110 author:Igal Sason, Sergio Verd√∫ category:cs.IT cs.LG math.IT math.PR math.ST stat.TH  published:2016-10-28 summary:This paper considers derivation of $f$-divergence inequalities via the approach of functional domination. Bounds on an $f$-divergence based on one or several other $f$-divergences are introduced, dealing with pairs of probability measures defined on arbitrary alphabets. In addition, a variety of bounds are shown to hold under boundedness assumptions on the relative information. The journal paper, which includes more approaches for the derivation of f-divergence inequalities and proofs, is available on the arXiv at https://arxiv.org/abs/1508.00335, and it has been published in the IEEE Trans. on Information Theory, vol. 62, no. 11, pp. 5973-6006, November 2016. version:1
arxiv-1610-09091 | Representation Learning Models for Entity Search | http://arxiv.org/abs/1610.09091 | id:1610.09091 author:Shijia E, Yang Xiang, Mohan Zhang category:cs.CL  published:2016-10-28 summary:We focus on the problem of learning distributed representations for entity search queries, named entities, and their short descriptions. With our representation learning models, the entity search query, named entity and description can be represented as low-dimensional vectors. Our goal is to develop a simple but effective model that can make the distributed representations of query related entities similar to the query in the vector space. Hence, we propose three kinds of learning strategies, and the difference between them mainly lies in how to deal with the relationship between an entity and its description. We analyze the strengths and weaknesses of each learning strategy and validate our methods on public datasets which contain four kinds of named entities, i.e., movies, TV shows, restaurants and celebrities. The experimental results indicate that our proposed methods can adapt to different types of entity search queries, and outperform the current state-of-the-art methods based on keyword matching and vanilla word2vec models. Besides, the proposed methods can be trained fast and be easily extended to other similar tasks. version:1
arxiv-1610-09087 | Recent advances in content based video copy detection | http://arxiv.org/abs/1610.09087 | id:1610.09087 author:Sanket Shinde, Girija Chiddarwar category:cs.CV  published:2016-10-28 summary:With the immense number of videos being uploaded to the video sharing sites, issue of copyright infringement arises with uploading of illicit copies or transformed versions of original video. Thus safeguarding copyright of digital media has become matter of concern. To address this concern, it is obliged to have a video copy detection system which is sufficiently robust to detect these transformed videos with ability to pinpoint location of copied segments. This paper outlines recent advancement in content based video copy detection, mainly focusing on different visual features employed by video copy detection systems. Finally we evaluate performance of existing video copy detection systems. version:1
arxiv-1610-09083 | SOL: A Library for Scalable Online Learning Algorithms | http://arxiv.org/abs/1610.09083 | id:1610.09083 author:Yue Wu, Steven C. H. Hoi, Chenghao Liu, Jing Lu, Doyen Sahoo, Nenghai Yu category:cs.LG stat.ML  published:2016-10-28 summary:SOL is an open-source library for scalable online learning algorithms, and is particularly suitable for learning with high-dimensional data. The library provides a family of regular and sparse online learning algorithms for large-scale binary and multi-class classification tasks with high efficiency, scalability, portability, and extensibility. SOL was implemented in C++, and provided with a collection of easy-to-use command-line tools, python wrappers and library calls for users and developers, as well as comprehensive documents for both beginners and advanced users. SOL is not only a practical machine learning toolbox, but also a comprehensive experimental platform for online learning research. Experiments demonstrate that SOL is highly efficient and scalable for large-scale machine learning with high-dimensional data. version:1
arxiv-1610-09075 | Missing Data Imputation for Supervised Learning | http://arxiv.org/abs/1610.09075 | id:1610.09075 author:Jason Poulos, Rafael Valle category:stat.ML cs.LG  published:2016-10-28 summary:This paper compares methods for imputing missing categorical data for supervised learning tasks. The ability of researchers to accurately fit a model and yield unbiased estimates may be compromised by missing data, which are prevalent in survey-based social science research. We experiment on two machine learning benchmark datasets with missing categorical data, comparing classifiers trained on non-imputed (i.e., one-hot encoded) or imputed data with different degrees of missing-data perturbation. The results show imputation methods can increase predictive accuracy in the presence of missing-data perturbation. Additionally, we find that for imputed models, missing-data perturbation can improve prediction accuracy by regularizing the classifier. version:1
arxiv-1610-09072 | Orthogonal Random Features | http://arxiv.org/abs/1610.09072 | id:1610.09072 author:Felix X. Yu, Ananda Theertha Suresh, Krzysztof Choromanski, Daniel Holtmann-Rice, Sanjiv Kumar category:cs.LG stat.ML  published:2016-10-28 summary:We present an intriguing discovery related to Random Fourier Features: in Gaussian kernel approximation, replacing the random Gaussian matrix by a properly scaled random orthogonal matrix significantly decreases kernel approximation error. We call this technique Orthogonal Random Features (ORF), and provide theoretical and empirical justification for this behavior. Motivated by this discovery, we further propose Structured Orthogonal Random Features (SORF), which uses a class of structured discrete orthogonal matrices to speed up the computation. The method reduces the time cost from $\mathcal{O}(d^2)$ to $\mathcal{O}(d \log d)$, where $d$ is the data dimensionality, with almost no compromise in kernel approximation quality compared to ORF. Experiments on several datasets verify the effectiveness of ORF and SORF over the existing methods. We also provide discussions on using the same type of discrete orthogonal structure for a broader range of applications. version:1
arxiv-1610-06209 | Structured adaptive and random spinners for fast machine learning computations | http://arxiv.org/abs/1610.06209 | id:1610.06209 author:Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Francois Fagan, Cedric Gouy-Pailler, Anne Morvan, Nouri Sakr, Tamas Sarlos, Jamal Atif category:cs.LG  published:2016-10-19 summary:We consider an efficient computational framework for speeding up several machine learning algorithms with almost no loss of accuracy. The proposed framework relies on projections via structured matrices that we call Structured Spinners, which are formed as products of three structured matrix-blocks that incorporate rotations. The approach is highly generic, i.e. i) structured matrices under consideration can either be fully-randomized or learned, ii) our structured family contains as special cases all previously considered structured schemes, iii) the setting extends to the non-linear case where the projections are followed by non-linear functions, and iv) the method finds numerous applications including kernel approximations via random feature maps, dimensionality reduction algorithms, new fast cross-polytope LSH techniques, deep learning, convex optimization algorithms via Newton sketches, quantization with random projection trees, and more. The proposed framework comes with theoretical guarantees characterizing the capacity of the structured model in reference to its unstructured counterpart and is based on a general theoretical principle that we describe in the paper. As a consequence of our theoretical analysis, we provide the first theoretical guarantees for one of the most efficient existing LSH algorithms based on the HD3HD2HD1 structured matrix [Andoni et al., 2015]. The exhaustive experimental evaluation confirms the accuracy and efficiency of structured spinners for a variety of different applications. version:2
arxiv-1610-07488 | Laplacian regularized low rank subspace clustering | http://arxiv.org/abs/1610.07488 | id:1610.07488 author:Yu Song, Yiquan Wu category:cs.CV  published:2016-10-24 summary:The problem of fitting a union of subspaces to a collection of data points drawn from multiple subspaces is considered in this paper. In the traditional low rank representation model, the dictionary used to represent the data points is chosen as the data points themselves and thus the dictionary is corrupted with noise. This problem is solved in the low rank subspace clustering model which decomposes the corrupted data matrix as the sum of a clean and self-expressive dictionary plus a matrix of noise and gross errors. Also, the clustering results of the low rank representation model can be enhanced by using a graph of data similarity. This model is called Laplacian regularized low rank representation model with a graph regularization term added to the objective function. Inspired from the above two ideas, in this paper a Laplacian regularized low rank subspace clustering model is proposed. This model uses a clean dictionary to represent the data points and a graph regularization term is also incorporated in the objective function. Experimental results show that, compared with the traditional low rank representation model, low rank subspace clustering model and several other state-of-the-art subspace clustering model, the model proposed in this paper can get better subspace clustering results with lower clustering error. version:2
arxiv-1610-09038 | Professor Forcing: A New Algorithm for Training Recurrent Networks | http://arxiv.org/abs/1610.09038 | id:1610.09038 author:Alex Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG  published:2016-10-27 summary:The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar. version:1
arxiv-1610-09034 | Geometric Dirichlet Means algorithm for topic inference | http://arxiv.org/abs/1610.09034 | id:1610.09034 author:Mikhail Yurochkin, XuanLong Nguyen category:stat.ML  published:2016-10-27 summary:We propose a geometric algorithm for topic learning and inference that is built on the convex geometry of topics arising from the Latent Dirichlet Allocation (LDA) model and its nonparametric extensions. To this end we study the optimization of a geometric loss function, which is a surrogate to the LDA's likelihood. Our method involves a fast optimization based weighted clustering procedure augmented with geometric corrections, which overcomes the computational and statistical inefficiencies encountered by other techniques based on Gibbs sampling and variational inference, while achieving the accuracy comparable to that of a Gibbs sampler. The topic estimates produced by our method are shown to be statistically consistent under some conditions. The algorithm is evaluated with extensive experiments on simulated and real data. version:1
arxiv-1610-09032 | Icon: An Interactive Approach to Train Deep Neural Networks for Segmentation of Neuronal Structures | http://arxiv.org/abs/1610.09032 | id:1610.09032 author:Felix Gonda, Verena Kaynig, Ray Thouis, Daniel Haehn, Jeff Lichtman, Toufiq Parag, Hanspeter Pfister category:cs.CV  published:2016-10-27 summary:We present an interactive approach to train a deep neural network pixel classifier for the segmentation of neuronal structures. An interactive training scheme reduces the extremely tedious manual annotation task that is typically required for deep networks to perform well on image segmentation problems. Our proposed method employs a feedback loop that captures sparse annotations using a graphical user interface, trains a deep neural network based on recent and past annotations, and displays the prediction output to users in almost real-time. Our implementation of the algorithm also allows multiple users to provide annotations in parallel and receive feedback from the same classifier. Quick feedback on classifier performance in an interactive setting enables users to identify and label examples that are more important than others for segmentation purposes. Our experiments show that an interactively-trained pixel classifier produces better region segmentation results on Electron Microscopy (EM) images than those generated by a network of the same architecture trained offline on exhaustive ground-truth labels. version:1
arxiv-1610-09027 | Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes | http://arxiv.org/abs/1610.09027 | id:1610.09027 author:Jack W Rae, Jonathan J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg Wayne, Alex Graves, Timothy P Lillicrap category:cs.LG  published:2016-10-27 summary:Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows --- limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs $1,\!000\times$ faster and with $3,\!000\times$ less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring $100,\!000$s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer. version:1
arxiv-1610-09013 | Compressive Holographic Video | http://arxiv.org/abs/1610.09013 | id:1610.09013 author:Zihao Wang, Leonidas Spinoulas, Kuan He, Huaijin Chen, Lei Tian, Aggelos K. Katsaggelos, Oliver Cossairt category:cs.CV  published:2016-10-27 summary:Compressed sensing has been discussed separately in spatial and temporal domains. Compressive holography has been introduced as a method that allows 3D tomographic reconstruction at different depths from a single 2D image. Coded exposure is a temporal compressed sensing method for high speed video acquisition. In this work, we combine compressive holography and coded exposure techniques and extend the discussion to 4D reconstruction in space and time from one coded captured image. In our prototype, digital in-line holography was used for imaging macroscopic, fast moving objects. The pixel-wise temporal modulation was implemented by a digital micromirror device. In this paper we demonstrate $10\times$ temporal super resolution with multiple depths recovery from a single image. Two examples are presented for the purpose of recording subtle vibrations and tracking small particles within 5 ms. version:1
arxiv-1610-09003 | Cross-Modal Scene Networks | http://arxiv.org/abs/1610.09003 | id:1610.09003 author:Yusuf Aytar, Lluis Castrejon, Carl Vondrick, Hamed Pirsiavash, Antonio Torralba category:cs.CV cs.LG cs.MM  published:2016-10-27 summary:People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality. version:1
arxiv-1610-09001 | SoundNet: Learning Sound Representations from Unlabeled Video | http://arxiv.org/abs/1610.09001 | id:1610.09001 author:Yusuf Aytar, Carl Vondrick, Antonio Torralba category:cs.CV cs.LG cs.SD  published:2016-10-27 summary:We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels. version:1
arxiv-1610-06258 | Using Fast Weights to Attend to the Recent Past | http://arxiv.org/abs/1610.06258 | id:1610.06258 author:Jimmy Ba, Geoffrey Hinton, Volodymyr Mnih, Joel Z. Leibo, Catalin Ionescu category:stat.ML cs.LG cs.NE  published:2016-10-20 summary:Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These "fast weights" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns. version:2
arxiv-1610-08936 | Learning Scalable Deep Kernels with Recurrent Structure | http://arxiv.org/abs/1610.08936 | id:1610.08936 author:Maruan Al-Shedivat, Andrew Gordon Wilson, Yunus Saatchi, Zhiting Hu, Eric P. Xing category:cs.LG cs.AI stat.ML  published:2016-10-27 summary:Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic procedure and exploit the structure of these kernels for fast and scalable training and prediction. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable. version:1
arxiv-1610-08928 | Rapid Posterior Exploration in Bayesian Non-negative Matrix Factorization | http://arxiv.org/abs/1610.08928 | id:1610.08928 author:M. Arjumand Masood, Finale Doshi-Velez category:stat.ML  published:2016-10-27 summary:Non-negative Matrix Factorization (NMF) is a popular tool for data exploration. Bayesian NMF promises to also characterize uncertainty in the factorization. Unfortunately, current inference approaches such as MCMC mix slowly and tend to get stuck on single modes. We introduce a novel approach using rapidly-exploring random trees (RRTs) to asymptotically cover regions of high posterior density. These are placed in a principled Bayesian framework via an online extension to nonparametric variational inference. On experiments on real and synthetic data, we obtain greater coverage of the posterior and higher ELBO values than standard NMF inference approaches. version:1
arxiv-1610-08927 | Voice Conversion using Convolutional Neural Networks | http://arxiv.org/abs/1610.08927 | id:1610.08927 author:Shariq Mobin, Joan Bruna category:stat.ML cs.SD  published:2016-10-27 summary:The human auditory system is able to distinguish the vocal source of thousands of speakers, yet not much is known about what features the auditory system uses to do this. Fourier Transforms are capable of capturing the pitch and harmonic structure of the speaker but this alone proves insufficient at identifying speakers uniquely. The remaining structure, often referred to as timbre, is critical to identifying speakers but we understood little about it. In this paper we use recent advances in neural networks in order to manipulate the voice of one speaker into another by transforming not only the pitch of the speaker, but the timbre. We review generative models built with neural networks as well as architectures for creating neural networks that learn analogies. Our preliminary results converting voices from one speaker to another are encouraging. version:1
arxiv-1610-08914 | Ex Machina: Personal Attacks Seen at Scale | http://arxiv.org/abs/1610.08914 | id:1610.08914 author:Ellery Wulczyn, Nithum Thain, Lucas Dixon category:cs.CL  published:2016-10-27 summary:The damage personal attacks make to online discourse motivates many platforms to try to curb the phenomenon. However, understanding the prevalence and impact of personal attacks in online platforms at scale remains surprisingly difficult. The contribution of this paper is to develop and illustrate a method that combines crowdsourcing and machine learning to analyze personal attacks at scale. We show an evaluation method for a classifier in terms of the aggregated number of crowd-workers it can approximate. We apply our methodology to English Wikipedia, generating a corpus of over 100k high quality human-labeled comments and 63M machine-labeled ones from a classifier that is as good as the aggregate of 3 crowd-workers. Using the corpus of machine-labeled scores, our methodology allows us to explore some of the open questions about the nature of online personal attacks. This reveals that the majority of personal attacks on Wikipedia are not the result of a few malicious users, nor primarily the consequence of allowing anonymous contributions. version:1
arxiv-1610-08904 | Local Similarity-Aware Deep Feature Embedding | http://arxiv.org/abs/1610.08904 | id:1610.08904 author:Chen Huang, Chen Change Loy, Xiaoou Tang category:cs.CV cs.LG  published:2016-10-27 summary:Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images, where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the Euclidean feature distance. However, the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper, we introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets. version:1
arxiv-1610-03774 | Parallelizing Stochastic Approximation Through Mini-Batching and Tail-Averaging | http://arxiv.org/abs/1610.03774 | id:1610.03774 author:Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, Aaron Sidford category:stat.ML cs.DS cs.LG  published:2016-10-12 summary:This work characterizes the benefits of averaging techniques widely used in conjunction with stochastic gradient descent (SGD). In particular, this work sharply analyzes: (1) mini-batching, a method of averaging many samples of the gradient to both reduce the variance of a stochastic gradient estimate and for parallelizing SGD and (2) tail-averaging, a method involving averaging the final few iterates of SGD in order to decrease the variance in SGD's final iterate. This work presents the first tight non-asymptotic generalization error bounds for these schemes for the stochastic approximation problem of least squares regression. Furthermore, this work establishes a precise problem-dependent extent to which mini-batching can be used to yield provable near-linear parallelization speedups over SGD with batch size one. These results are utilized in providing a highly parallelizable SGD algorithm that obtains the optimal statistical error rate with nearly the same number of serial updates as batch gradient descent, which improves significantly over existing SGD-style methods. Finally, this work sheds light on some fundamental differences in SGD's behavior when dealing with agnostic noise in the (non-realizable) least squares regression problem. In particular, the work shows that the stepsizes that ensure optimal statistical error rates for the agnostic case must be a function of the noise properties. The central analysis tools used by this paper are obtained through generalizing the operator view of averaged SGD, introduced by Defossez and Bach (2015) followed by developing a novel analysis in bounding these operators to characterize the generalization error. These techniques may be of broader interest in analyzing various computational aspects of stochastic approximation. version:2
arxiv-1610-08871 | Detecting People in Artwork with CNNs | http://arxiv.org/abs/1610.08871 | id:1610.08871 author:Nicholas Westlake, Hongping Cai, Peter Hall category:cs.CV  published:2016-10-27 summary:CNNs have massively improved performance in object detection in photographs. However research into object detection in artwork remains limited. We show state-of-the-art performance on a challenging dataset, People-Art, which contains people from photos, cartoons and 41 different artwork movements. We achieve this high performance by fine-tuning a CNN for this task, thus also demonstrating that training CNNs on photos results in overfitting for photos: only the first three or four layers transfer from photos to artwork. Although the CNN's performance is the highest yet, it remains less than 60\% AP, suggesting further work is needed for the cross-depiction problem. The final publication is available at Springer via http://dx.doi.org/10.1007/978-3-319-46604-0_57 version:1
arxiv-1610-04673 | Road Curb Extraction from Mobile LiDAR Point Clouds | http://arxiv.org/abs/1610.04673 | id:1610.04673 author:Sheng Xu, Ruisheng Wang, Han Zheng category:cs.CV  published:2016-10-15 summary:Automatic extraction of road curbs from uneven, unorganized, noisy and massive 3D point clouds is a challenging task. Existing methods often project 3D point clouds onto 2D planes to extract curbs. However, the projection causes loss of 3D information which degrades the performance of the detection. This paper presents a robust, accurate and efficient method to extract road curbs from 3D mobile LiDAR point clouds. Our method consists of two steps: 1) extracting the candidate points of curbs based on the proposed novel energy function and 2) refining the candidate points using the proposed least cost path model. We evaluated our method on a large-scale of residential area (16.7GB, 300 million points) and an urban area (1.07GB, 20 million points) mobile LiDAR point clouds. Results indicate that the proposed method is superior to the state-of-the-art methods in terms of robustness, accuracy and efficiency. The proposed curb extraction method achieved a completeness of 78.62% and a correctness of 83.29%. These experiments demonstrate that the proposed method is a promising solution to extract road curbs from mobile LiDAR point clouds. version:2
arxiv-1610-08861 | On Bochner's and Polya's Characterizations of Positive-Definite Kernels and the Respective Random Feature Maps | http://arxiv.org/abs/1610.08861 | id:1610.08861 author:Jie Chen, Dehua Cheng, Yan Liu category:stat.ML cs.LG  published:2016-10-27 summary:Positive-definite kernel functions are fundamental elements of kernel methods and Gaussian processes. A well-known construction of such functions comes from Bochner's characterization, which connects a positive-definite function with a probability distribution. Another construction, which appears to have attracted less attention, is Polya's criterion that characterizes a subset of these functions. In this paper, we study the latter characterization and derive a number of novel kernels little known previously. In the context of large-scale kernel machines, Rahimi and Recht (2007) proposed a random feature map (random Fourier) that approximates a kernel function, through independent sampling of the probability distribution in Bochner's characterization. The authors also suggested another feature map (random binning), which, although not explicitly stated, comes from Polya's characterization. We show that with the same number of random samples, the random binning map results in an Euclidean inner product closer to the kernel than does the random Fourier map. The superiority of the random binning map is confirmed empirically through regressions and classifications in the reproducing kernel Hilbert space. version:1
arxiv-1610-08854 | Tool and Phase recognition using contextual CNN features | http://arxiv.org/abs/1610.08854 | id:1610.08854 author:Manish Sahu, Anirban Mukhopadhyay, Angelika Szengel, Stefan Zachow category:cs.CV  published:2016-10-27 summary:A transfer learning method for generating features suitable for surgical tools and phase recognition from the ImageNet classification features [1] is proposed here. In addition, methods are developed for generating contextual features and combining them with time series analysis for final classification using multi-class random forest. The proposed pipeline is tested over the training and testing datasets of M2CAI16 challenges: tool and phase detection. Encouraging results are obtained by leave-one-out cross validation evaluation on the training dataset. version:1
arxiv-1610-08851 | Single- and Multi-Task Architectures for Tool Presence Detection Challenge at M2CAI 2016 | http://arxiv.org/abs/1610.08851 | id:1610.08851 author:Andru P. Twinanda, Didier Mutter, Jacques Marescaux, Michel de Mathelin, Nicolas Padoy category:cs.CV  published:2016-10-27 summary:The tool presence detection challenge at M2CAI 2016 consists of identifying the presence/absence of seven surgical tools in the images of cholecystectomy videos. Here, we propose to use deep architectures that are based on our previous work where we presented several architectures to perform multiple recognition tasks on laparoscopic videos. In this technical report, we present the tool presence detection results using two architectures: (1) a single-task architecture designed to perform solely the tool presence detection task and (2) a multi-task architecture designed to perform jointly phase recognition and tool presence detection. The results show that the multi-task network only slightly improves the tool presence detection results. In constrast, a significant improvement is obtained when there are more data available to train the networks. This significant improvement can be regarded as a call for action for other institutions to start working toward publishing more datasets into the community, so that better models could be generated to perform the task. version:1
arxiv-1610-08838 | A Category Space Approach to Supervised Dimensionality Reduction | http://arxiv.org/abs/1610.08838 | id:1610.08838 author:Anthony O. Smith, Anand Rangarajan category:stat.ML cs.LG  published:2016-10-27 summary:Supervised dimensionality reduction has emerged as an important theme in the last decade. Despite the plethora of models and formulations, there is a lack of a simple model which aims to project the set of patterns into a space defined by the classes (or categories). To this end, we set up a model in which each class is represented as a 1D subspace of the vector space formed by the features. Assuming the set of classes does not exceed the cardinality of the features, the model results in multi-class supervised learning in which the features of each class are projected into the class subspace. Class discrimination is automatically guaranteed via the imposition of orthogonality of the 1D class sub-spaces. The resulting optimization problem - formulated as the minimization of a sum of quadratic functions on a Stiefel manifold - while being non-convex (due to the constraints), nevertheless has a structure for which we can identify when we have reached a global minimum. After formulating a version with standard inner products, we extend the formulation to reproducing kernel Hilbert spaces in a straightforward manner. The optimization approach also extends in a similar fashion to the kernel version. Results and comparisons with the multi-class Fisher linear (and kernel) discriminants and principal component analysis (linear and kernel) showcase the relative merits of this approach to dimensionality reduction. version:1
arxiv-1610-08815 | A Deeper Look into Sarcastic Tweets Using Deep Convolutional Neural Networks | http://arxiv.org/abs/1610.08815 | id:1610.08815 author:Soujanya Poria, Erik Cambria, Devamanyu Hazarika, Prateek Vij category:cs.CL  published:2016-10-27 summary:Sarcasm detection is a key task for many natural language processing tasks. In sentiment analysis, for example, sarcasm can flip the polarity of an "apparently positive" sentence and, hence, negatively affect polarity detection performance. To date, most approaches to sarcasm detection have treated the task primarily as a text categorization problem. Sarcasm, however, can be expressed in very subtle ways and requires a deeper understanding of natural language that standard text categorization techniques cannot grasp. In this work, we develop models based on a pre-trained convolutional neural network for extracting sentiment, emotion and personality features for sarcasm detection. Such features, along with the network's baseline features, allow the proposed models to outperform the state of the art on benchmark datasets. We also address the often ignored generalizability issue of classifying data that have not been seen by the models at learning phase. version:1
arxiv-1610-08813 | Sparse Signal Subspace Decomposition Based on Adaptive Over-complete Dictionary | http://arxiv.org/abs/1610.08813 | id:1610.08813 author:Hong Sun, Chengwei Sang, Didier Le Ruyet category:stat.ML  published:2016-10-27 summary:This paper proposes a subspace decomposition method based on an over-complete dictionary in sparse representation, called "Sparse Signal Subspace Decomposition" (or 3SD) method. This method makes use of a novel criterion based on the occurrence frequency of atoms of the dictionary over the data set. This criterion, well adapted to subspace-decomposition over a dependent basis set, adequately re ects the intrinsic characteristic of regularity of the signal. The 3SD method combines variance, sparsity and component frequency criteria into an unified framework. It takes benefits from using an over-complete dictionary which preserves details and from subspace decomposition which rejects strong noise. The 3SD method is very simple with a linear retrieval operation. It does not require any prior knowledge on distributions or parameters. When applied to image denoising, it demonstrates high performances both at preserving fine details and suppressing strong noise. version:1
arxiv-1610-08763 | CoType: Joint Extraction of Typed Entities and Relations with Knowledge Bases | http://arxiv.org/abs/1610.08763 | id:1610.08763 author:Xiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, Tarek F. Abdelzaher, Jiawei Han category:cs.CL cs.LG  published:2016-10-27 summary:Extracting entities and relations for types of interest from text is important for understanding massive text corpora. Traditionally, systems of entity relation extraction have relied on human-annotated corpora for training and adopted an incremental pipeline. Such systems require additional human expertise to be ported to a new domain, and are vulnerable to errors cascading down the pipeline. In this paper, we investigate joint extraction of typed entities and relations with labeled data heuristically obtained from knowledge bases (i.e., distant supervision). As our algorithm for type labeling via distant supervision is context-agnostic, noisy training data poses unique challenges for the task. We propose a novel domain-independent framework, called CoType, that runs a data-driven text segmentation algorithm to extract entity mentions, and jointly embeds entity mentions, relation mentions, text features and type labels into two low-dimensional spaces (for entity and relation mentions respectively), where, in each space, objects whose types are close will also have similar representations. CoType, then using these learned embeddings, estimates the types of test (unlinkable) mentions. We formulate a joint optimization problem to learn embeddings from text corpora and knowledge bases, adopting a novel partial-label loss function for noisy labeled data and introducing an object "translation" function to capture the cross-constraints of entities and relations on each other. Experiments on three public datasets demonstrate the effectiveness of CoType across different domains (e.g., news, biomedical), with an average of 25% improvement in F1 score compared to the next best method. version:1
arxiv-1610-08749 | Differentially Private Variational Inference for Non-conjugate Models | http://arxiv.org/abs/1610.08749 | id:1610.08749 author:Joonas J√§lk√∂, Onur Dikmen, Antti Honkela category:stat.ML cs.CR cs.LG stat.ME  published:2016-10-27 summary:As collecting huge amounts of personal data from individuals has been established as a standard nowadays, it is really important to use these data in a conscientious way. For example, when performing inference using these data, one has to make sure individuals' identities or the privacy of the data are not compromised. Differential privacy is a powerful framework that introduces stochasticity into the computation to guarantee that it is difficult to breach the privacy using the output of the computation. Differentially private versions of many important machine learning methods have been proposed, but still there is a long way to pave towards an efficient unified approach applicable to handle many models. In this paper, we propose a differentially private variational inference method with a very wide applicability. The variational inference is based on stochastic gradient ascent and can handle non-conjugate models as well as conjugate ones. Differential privacy is achieved by perturbing the gradients. We explore ways to make the algorithm more efficient through privacy amplification from subsampling and through clipping the gradients to limit the amount of information they leak. We explore the effect of different parameter combinations in logistic regression problems where the method can reach an accuracy close to non-private level under reasonably strong privacy guarantees. version:1
arxiv-1610-08239 | Things Bayes can't do | http://arxiv.org/abs/1610.08239 | id:1610.08239 author:Daniil Ryabko category:cs.LG math.ST stat.ML stat.TH  published:2016-10-26 summary:The problem of forecasting conditional probabilities of the next event given the past is considered in a general probabilistic setting. Given an arbitrary (large, uncountable) set C of predictors, we would like to construct a single predictor that performs asymptotically as well as the best predictor in C, on any data. Here we show that there are sets C for which such predictors exist, but none of them is a Bayesian predictor with a prior concentrated on C. In other words, there is a predictor with sublinear regret, but every Bayesian predictor must have a linear regret. This negative finding is in sharp contrast with previous results that establish the opposite for the case when one of the predictors in $C$ achieves asymptotically vanishing error. In such a case, if there is a predictor that achieves asymptotically vanishing error for any measure in C, then there is a Bayesian predictor that also has this property, and whose prior is concentrated on (a countable subset of) C. version:2
arxiv-1610-08738 | Compressive K-means | http://arxiv.org/abs/1610.08738 | id:1610.08738 author:Nicolas Keriven, Nicolas Tremblay, Yann Traonmilin, R√©mi Gribonval category:cs.LG stat.ML  published:2016-10-27 summary:The Lloyd-Max algorithm is a classical approach to perform K-means clustering. Unfortunately, its cost becomes prohibitive as the training dataset grows large. We propose a compressive version of K-means (CKM), that estimates cluster centers from a sketch, i.e. from a drastically compressed representation of the training dataset. We demonstrate empirically that CKM performs similarly to Lloyd-Max, for a sketch size proportional to the number of cen-troids times the ambient dimension, and independent of the size of the original dataset. Given the sketch, the computational complexity of CKM is also independent of the size of the dataset. Unlike Lloyd-Max which requires several replicates, we further demonstrate that CKM is almost insensitive to initialization. For a large dataset of 10^7 data points, we show that CKM can run two orders of magnitude faster than five replicates of Lloyd-Max, with similar clustering performance on artificial data. Finally, CKM achieves lower classification errors on handwritten digits classification. version:1
arxiv-1610-08735 | Stratification of patient trajectories using covariate latent variable models | http://arxiv.org/abs/1610.08735 | id:1610.08735 author:Kieran R. Campbell, Christopher Yau category:stat.ML q-bio.GN q-bio.QM  published:2016-10-27 summary:Standard models assign disease progression to discrete categories or stages based on well-characterized clinical markers. However, such a system is potentially at odds with our understanding of the underlying biology, which in highly complex systems may support a (near-)continuous evolution of disease from inception to terminal state. To learn such a continuous disease score one could infer a latent variable from dynamic omics data such as RNA-seq that correlates with an outcome of interest such as survival time. However, such analyses may be confounded by additional data such as clinical covariates measured in electronic health records (EHRs). As a solution to this we introduce covariate latent variable models, a novel type of latent variable model that learns a low-dimensional data representation in the presence of two (asymmetric) views of the same data source. We apply our model to TCGA colorectal cancer RNA-seq data and demonstrate how incorporating microsatellite-instability (MSI) and metastatic status as external covariates allows us to identify genes that stratify patients on an immune-response trajectory. Finally, we propose an extension termed Covariate Gaussian Process Latent Variable Models for learning nonparametric, nonlinear representations. version:1
arxiv-1610-08733 | GPflow: A Gaussian process library using TensorFlow | http://arxiv.org/abs/1610.08733 | id:1610.08733 author:Alexander G. de G. Matthews, Mark van der Wilk, Tom Nickson, Keisuke Fujii, Alexis Boukouvalas, Pablo Le√≥n-Villagr√°, Zoubin Ghahramani, James Hensman category:stat.ML  published:2016-10-27 summary:GPflow is a Gaussian process library that uses TensorFlow for its core computations and Python for its front end. The distinguishing features of GPflow are that it uses variational inference as the primary approximation method, provides concise code through the use of automatic differentiation, has been engineered with a particular emphasis on software testing and is able to exploit GPU hardware. version:1
arxiv-1610-08696 | Learning Bound for Parameter Transfer Learning | http://arxiv.org/abs/1610.08696 | id:1610.08696 author:Wataru Kumagai category:stat.ML cs.LG  published:2016-10-27 summary:We consider a transfer-learning problem by using the parameter transfer approach, where a suitable parameter of feature mapping is learned through one task and applied to another objective task. Then, we introduce the notion of the local stability of parametric feature mapping and parameter transfer learnability, and thereby derive a learning bound for parameter transfer algorithms. As an application of parameter transfer learning, we discuss the performance of sparse coding in self-taught learning. Although self-taught learning algorithms with plentiful unlabeled data often show excellent empirical performance, their theoretical analysis has not been studied. In this paper, we also provide the first theoretical learning bound for self-taught learning. version:1
arxiv-1610-06269 | Embodiment of Learning in Electro-Optical Signal Processors | http://arxiv.org/abs/1610.06269 | id:1610.06269 author:Michiel Hermans, Piotr Antonik, Marc Haelterman, Serge Massar category:cs.ET cs.NE  published:2016-10-20 summary:Delay-coupled electro-optical systems have received much attention for their dynamical properties and their potential use in signal processing. In particular it has recently been demonstrated, using the artificial intelligence algorithm known as reservoir computing, that photonic implementations of such systems solve complex tasks such as speech recognition. Here we show how the backpropagation algorithm can be physically implemented on the same electro-optical delay-coupled architecture used for computation with only minor changes to the original design. We find that, compared when the backpropagation algorithm is not used, the error rate of the resulting computing device, evaluated on three benchmark tasks, decreases considerably. This demonstrates that electro-optical analog computers can embody a large part of their own training process, allowing them to be applied to new, more difficult tasks. version:2
arxiv-1610-08664 | A random version of principal component analysis in data clustering | http://arxiv.org/abs/1610.08664 | id:1610.08664 author:Luigi Leonardo Palese category:q-bio.QM cs.LG  published:2016-10-27 summary:Principal component analysis (PCA) is a widespread technique for data analysis that relies on the covariance-correlation matrix of the analyzed data. However to properly work with high-dimensional data, PCA poses severe mathematical constraints on the minimum number of different replicates or samples that must be included in the analysis. Here we show that a modified algorithm works not only on well dimensioned datasets, but also on degenerated ones. version:1
arxiv-1610-08637 | Statistical Inference for Model Parameters in Stochastic Gradient Descent | http://arxiv.org/abs/1610.08637 | id:1610.08637 author:Xi Chen, Jason D. Lee, Xin T. Tong, Yichen Zhang category:stat.ML  published:2016-10-27 summary:The stochastic gradient descent (SGD) algorithm has been widely used in statistical estimation for large-scale data due to its computational and memory efficiency. While most existing work focuses on the convergence of the objective function or the error of the obtained solution, we investigate the problem of statistical inference of the true model parameters based on SGD. To this end, we propose two consistent estimators of the asymptotic covariance of the average iterate from SGD: (1) an intuitive plug-in estimator and (2) a computationally more efficient batch-means estimator, which only uses the iterates from SGD. As the SGD process forms a time-inhomogeneous Markov chain, our batch-means estimator with carefully chosen increasing batch sizes generalizes the classical batch-means estimator designed for time-homogenous Markov chains. The proposed batch-means estimator is of independent interest, which can be potentially used for estimating the covariance of other time-inhomogeneous Markov chains. Both proposed estimators allow us to construct asymptotically exact confidence intervals and hypothesis tests. We further discuss an extension to conducting inference based on SGD for high-dimensional linear regression. Using a variant of the SGD algorithm, we construct a debiased estimator of each regression coefficient that is asymptotically normal. This gives a one-pass algorithm for computing both the sparse regression coefficient estimator and confidence intervals, which is computationally attractive and applicable to online data. version:1
arxiv-1610-08628 | Regret Bounds for Lifelong Learning | http://arxiv.org/abs/1610.08628 | id:1610.08628 author:Pierre Alquier, The Tien Mai, Massimiliano Pontil category:stat.ML cs.LG  published:2016-10-27 summary:We consider the problem of transfer learning in an online setting. Different tasks are presented sequentially and processed by a within-task algorithm. We propose a lifelong learning strategy which refines the underlying data representation used by the within-task algorithm, thereby transferring information from one task to the next. We show that when the within-task algorithm comes with some regret bound, our strategy inherits this good property. Our bounds are in expectation for a general loss function, and uniform for a convex loss. We discuss applications to dictionary learning and finite set of predictors. In the latter case, we improve previous $O(1/\sqrt{m})$ bounds to $O(1/m)$ where $m$ is the per task sample size. version:1
arxiv-1610-08627 | Estimation of Bandlimited Grayscale Images From the Single Bit Observations of Pixels Affected by Additive Gaussian Noise | http://arxiv.org/abs/1610.08627 | id:1610.08627 author:Abhinav Kumar, Animesh Kumar category:cs.CV math.ST stat.TH  published:2016-10-27 summary:The estimation of grayscale images using their single-bit zero mean Gaussian noise-affected pixels is presented in this paper. The images are assumed to be bandlimited in the Fourier Cosine transform (FCT) domain. The images are oversampled over their Nyquist rate in the FCT domain. We propose a non-recursive approach based on first order approximation of Cumulative Distribution Function (CDF) to estimate the image from single bit pixels which itself is based on Banach's contraction theorem. The decay rate for mean squared error of estimating such images is found to be independent of the precision of the quantizer and it varies as $O(1/N)$ where $N$ is the "effective" oversampling ratio with respect to the Nyquist rate in the FCT domain. version:1
arxiv-1610-08481 | Mask-off: Synthesizing Face Images in the Presence of Head-mounted Displays | http://arxiv.org/abs/1610.08481 | id:1610.08481 author:Yajie Zhao, Qingguo Xu, Xinyu Huang, Ruigang Yang category:cs.CV  published:2016-10-26 summary:A head-mounted display (HMD) could be an important component of augmented reality system. However, as the upper face region is seriously occluded by the device, the user experience could be affected in applications such as telecommunication and multi-player video games. In this paper, we first present a novel experimental setup that consists of two near-infrared (NIR) cameras to point to the eye regions and one visible-light RGB camera to capture the visible face region. The main purpose of this paper is to synthesize realistic face images without occlusions based on the images captured by these cameras. To this end, we propose a novel synthesis framework that contains four modules: 3D head reconstruction, face alignment and tracking, face synthesis, and eye synthesis. In face synthesis, we propose a novel algorithm that can robustly align and track a personalized 3D head model given a face that is severely occluded by the HMD. In eye synthesis, in order to generate accurate eye movements and dynamic wrinkle variations around eye regions, we propose another novel algorithm to colorize the NIR eye images and further remove the "red eye" effects caused by the colorization. Results show that both hardware setup and system framework are robust to synthesize realistic face images in video sequences. version:2
arxiv-1610-08624 | PCM and APCM Revisited: An Uncertainty Perspective | http://arxiv.org/abs/1610.08624 | id:1610.08624 author:Peixin Hou, Hao Deng, Jiguang Yue, Shuguang Liu category:cs.CV stat.ML  published:2016-10-27 summary:In this paper, we take a new look at the possibilistic c-means (PCM) and adaptive PCM (APCM) clustering algorithms from the perspective of uncertainty. This new perspective offers us insights into the clustering process, and also provides us greater degree of flexibility. We analyze the clustering behavior of PCM-based algorithms and introduce parameters $\sigma_v$ and $\alpha$ to characterize uncertainty of estimated bandwidth and noise level of the dataset respectively. Then uncertainty (fuzziness) of membership values caused by uncertainty of the estimated bandwidth parameter is modeled by a conditional fuzzy set, which is a new formulation of the type-2 fuzzy set. Experiments show that parameters $\sigma_v$ and $\alpha$ make the clustering process more easy to control, and main features of PCM and APCM are unified in this new clustering framework (UPCM). More specifically, UPCM reduces to PCM when we set a small $\alpha$ or a large $\sigma_v$, and UPCM reduces to APCM when clusters are confined in their physical clusters and possible cluster elimination are ensured. Finally we present further researches of this paper. version:1
arxiv-1610-08623 | Poisson intensity estimation with reproducing kernels | http://arxiv.org/abs/1610.08623 | id:1610.08623 author:Seth Flaxman, Yee Whye Teh, Dino Sejdinovic category:stat.ML  published:2016-10-27 summary:Despite the fundamental nature of the inhomogeneous Poisson process in the theory and application of stochastic processes, and its attractive generalizations (e.g.~Cox process), few tractable nonparametric modeling approaches of intensity functions exist, especially in high dimensional settings. In this paper we develop a new, computationally tractable Reproducing Kernel Hilbert Space (RKHS) formulation for the inhomogeneous Poisson process. We model the square root of the intensity as an RKHS function. The modeling challenge is that the usual representer theorem arguments no longer apply due to the form of the inhomogeneous Poisson process likelihood. However, we prove that the representer theorem does hold in an appropriately transformed RKHS, guaranteeing that the optimization of the penalized likelihood can be cast as a tractable finite-dimensional problem. The resulting approach is simple to implement, and readily scales to high dimensions and large-scale datasets. version:1
arxiv-1610-08619 | Exploiting Structure Sparsity for Covariance-based Visual Representation | http://arxiv.org/abs/1610.08619 | id:1610.08619 author:Jianjia Zhang, Lei Wang, Luping Zhou category:cs.CV  published:2016-10-27 summary:The past few years have witnessed increasing research interest on covariance-based feature representation. A variety of methods have been proposed to boost its efficacy, with some recent ones resorting to nonlinear kernel technique. Noting that the essence of this feature representation is to characterise the underlying structure of visual features, this paper argues that an equally, if not more, important approach to boosting its efficacy shall be to improve the quality of this characterisation. Following this idea, we propose to exploit the structure sparsity of visual features in skeletal human action recognition, and compute sparse inverse covariance estimate (SICE) as feature representation. We discuss the advantage of this new representation on dealing with small sample, high dimensionality, and modelling capability. Furthermore, utilising the monotonicity property of SICE, we efficiently generate a hierarchy of SICE matrices to characterise the structure of visual features at different sparsity levels, and two discriminative learning algorithms are then developed to adaptively integrate them to perform recognition. As demonstrated by extensive experiments, the proposed representation leads to significantly improved recognition performance over the state-of-the-art comparable methods. In particular, as a method fully based on linear technique, it is comparable or even better than those employing nonlinear kernel technique. This result well demonstrates the value of exploiting structure sparsity for covariance-based feature representation. version:1
arxiv-1610-08616 | Joint Detection and Tracking for Multipath Targets: A Variational Bayesian Approach | http://arxiv.org/abs/1610.08616 | id:1610.08616 author:Hua Lan, Shuai Sun, Zengfu Wang, Quan Pan, Zhishan Zhang category:cs.CV 65K10  published:2016-10-27 summary:Different from traditional point target tracking systems assuming that a target generates at most one single measurement per scan, there exists a class of multipath target tracking systems where each measurement may originate from the interested target via one of multiple propagation paths or from clutter, while the correspondence among targets, measurements, and propagation paths is unknown. The performance of multipath target tracking systems can be improved if multiple measurements from the same target are effectively utilized, but suffers from two major challenges. The first is multipath detection that detects appearing and disappearing targets automatically, while one target may produce $s$ tracks for $s$ propagation paths. The second is multipath tracking that calculates the target-to-measurement-to-path assignment matrices to estimate target states, which is computationally intractable due to the combinatorial explosion. Based on variational Bayesian framework, this paper introduces a novel probabilistic joint detection and tracking algorithm (JDT-VB) that incorporates data association, path association, state estimation and automatic track management. The posterior probabilities of these latent variables are derived in a closed-form iterative manner, which is effective for dealing with the coupling issue of multipath data association identification risk and state estimation error. Loopy belief propagation (LBP) is exploited to approximate the multipath data association, which significantly reduces the computational cost. The proposed JDT-VB algorithm can simultaneously deal with the track initiation, maintenance, and termination for multiple multipath target tracking with time-varying number of targets, and its performance is verified by a numerical simulation of over-the-horizon radar. version:1
arxiv-1610-08613 | Can Active Memory Replace Attention? | http://arxiv.org/abs/1610.08613 | id:1610.08613 author:≈Åukasz Kaiser, Samy Bengio category:cs.LG cs.CL  published:2016-10-27 summary:Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice. version:1
arxiv-1610-08611 | Causal Network Learning from Multiple Interventions of Unknown Manipulated Targets | http://arxiv.org/abs/1610.08611 | id:1610.08611 author:Yango He, Zhi Geng category:stat.ML cs.LG  published:2016-10-27 summary:In this paper, we discuss structure learning of causal networks from multiple data sets obtained by external intervention experiments where we do not know what variables are manipulated. For example, the conditions in these experiments are changed by changing temperature or using drugs, but we do not know what target variables are manipulated by the external interventions. From such data sets, the structure learning becomes more difficult. For this case, we first discuss the identifiability of causal structures. Next we present a graph-merging method for learning causal networks for the case that the sample sizes are large for these interventions. Then for the case that the sample sizes of these interventions are relatively small, we propose a data-pooling method for learning causal networks in which we pool all data sets of these interventions together for the learning. Further we propose a re-sampling approach to evaluate the edges of the causal network learned by the data-pooling method. Finally we illustrate the proposed learning methods by simulations. version:1
arxiv-1610-08606 | Fast Low-rank Shared Dictionary Learning for Image Classification | http://arxiv.org/abs/1610.08606 | id:1610.08606 author:Tiep Vu, Vishal Monga category:cs.CV  published:2016-10-27 summary:Despite the fact that different objects possess distinct class-specific features, they also usually share common patterns. This observation has been exploited partially in a recently proposed dictionary learning framework by separating the particularity and the commonality (COPAR). Inspired by this, we propose a novel method to explicitly and simultaneously learn a set of common patterns as well as class-specific features for classification with more intuitive constraints. Our dictionary learning framework is hence characterized by both a shared dictionary and particular (class-specific) dictionaries. For the shared dictionary, we enforce a low-rank constraint, i.e. claim that its spanning subspace should have low dimension and the coefficients corresponding to this dictionary should be similar. For the particular dictionaries, we impose on them the well-known constraints stated in the Fisher discrimination dictionary learning (FDDL). Further, we develop new fast and accurate algorithms to solve the subproblems in the learning step, accelerating its convergence. The said algorithms could also be applied to FDDL and its extensions. The efficiencies of these algorithms are theoretically and experimentally verified by comparing their complexities and running time with those of other well-known dictionary learning methods. Experimental results on widely used image datasets establish the advantages of our method over state-of-the-art dictionary learning methods. version:1
arxiv-1610-08597 | Word Embeddings to Enhance Twitter Gang Member Profile Identification | http://arxiv.org/abs/1610.08597 | id:1610.08597 author:Sanjaya Wijeratne, Lakshika Balasuriya, Derek Doran, Amit Sheth category:cs.SI cs.CL cs.CY cs.IR  published:2016-10-27 summary:Gang affiliates have joined the masses who use social media to share thoughts and actions publicly. Interestingly, they use this public medium to express recent illegal actions, to intimidate others, and to share outrageous images and statements. Agencies able to unearth these profiles may thus be able to anticipate, stop, or hasten the investigation of gang-related crimes. This paper investigates the use of word embeddings to help identify gang members on Twitter. Building on our previous work, we generate word embeddings that translate what Twitter users post in their profile descriptions, tweets, profile images, and linked YouTube content to a real vector format amenable for machine learning classification. Our experimental results show that pre-trained word embeddings can boost the accuracy of supervised learning algorithms trained over gang members social media posts. version:1
arxiv-1610-08589 | Iterative Inversion of Deformation Vector Fields with Feedback Control | http://arxiv.org/abs/1610.08589 | id:1610.08589 author:Abhishek Kumar Dubey, Alexandros-Stavros Iliopoulos, Xiaobai Sun, Fang-Fang Yin, Lei Ren category:cs.CV  published:2016-10-27 summary:Purpose: The inverse of a deformation vector field (DVF) is often needed in deformable registration, 4D image reconstruction, and adaptive radiation therapy. This study aims at improving both the accuracy with respect to inverse consistency and efficiency of the numerical DVF inversion by developing a fixed-point iteration method with feedback control. Method: We introduce an iterative method with active feedback control for DVF inversion. The method is built upon a previous fixed-point iteration method, which is represented as a particular passive instance in the new method. At each iteration step, we measure the inconsistency, namely the residual, between the iterative inverse estimate and the input DVF. The residual is modulated by a feedback control mechanism before being incorporated into the next iterate. The feedback control design is based on analysis of error propagation in the iteration process. The design goal is to make the convergence region as large as possible, and to make estimate errors vanish faster. The feedback control design is assessed with two data sets: an analytic DVF pair, and a DVF between two phases of a 4D XCAT phantom. Results: The single-parameter feedback control improved both the convergence region and convergence rate of the iterative algorithm, for both datasets. With the analytic data, the iteration becomes convergent over the entire image domain, whereas the the precursor method diverges as the deformation becomes larger. With the XCAT DVF data, feedback control reduced the 95th percentile of residuals from 1 mm to 1E-6 mm. Additionally, convergence rate was accelerated by at least a factor of 2 for both datasets. Conclusion: The introduced method shows the previously unexplored possibility in exercising active feedback control in DVF inversion, and the unexploited potential in improving both numerical accuracy and computational efficiency. version:1
arxiv-1610-08557 | Knowledge-Based Biomedical Word Sense Disambiguation with Neural Concept Embeddings and Distant Supervision | http://arxiv.org/abs/1610.08557 | id:1610.08557 author:A. K. M. Sabbir, Antonio Jimeno Yepes, Ramakanth Kavuluru category:cs.CL  published:2016-10-26 summary:Biomedical word sense disambiguation (WSD) is an important intermediate task in many natural language processing applications such as named entity recognition, syntactic parsing, and relation extraction. In this paper, we employ knowledge-based approaches that also exploit recent advances in neural word/concept embeddings to improve over the state-of-the-art in biomedical WSD using the MSH WSD dataset as the test set. Our methods involve distant supervision - we do not use any hand-labeled examples for WSD to build our prediction models; however, we employ an existing well known named entity recognition and concept mapping program, MetaMap, to obtain our concept vectors. Over the MSH WSD dataset, our linear time (in terms of numbers of senses and words in the test instance) method achieves an accuracy of 92.24% which is an absolute 3% improvement over the best known results obtained via unsupervised or knowledge-based means. A more expensive approach that we developed relies on a nearest neighbor framework and achieves an accuracy of 94.34%. Employing dense vector representations learned from unlabeled free text has been shown to benefit many language processing tasks recently and our efforts show that biomedical WSD is no exception to this trend. For a complex and rapidly evolving domain such as biomedicine, building labeled datasets for larger sets of ambiguous terms may be impractical. Here we demonstrate that distant supervision that leverages recent advances in representation learning can rival supervised approaches in biomedical WSD. version:1
arxiv-1610-08500 | Synthesis of Shared Control Protocols with Provable Safety and Performance Guarantees | http://arxiv.org/abs/1610.08500 | id:1610.08500 author:Nils Jansen, Murat Cubuktepe, Ufuk Topcu category:cs.RO cs.AI cs.LG  published:2016-10-26 summary:We formalize synthesis of shared control protocols with correctness guarantees for temporal logic specifications. More specifically, we introduce a modeling formalism in which both a human and an autonomy protocol can issue commands to a robot towards performing a certain task. These commands are blended into a joint input to the robot. The autonomy protocol is synthesized using an abstraction of possible human commands accounting for randomness in decisions caused by factors such as fatigue or incomprehensibility of the problem at hand. The synthesis is designed to ensure that the resulting robot behavior satisfies given safety and performance specifications, e.g., in temporal logic. Our solution is based on nonlinear programming and we address the inherent scalability issue by presenting alternative methods. We assess the feasibility and the scalability of the approach by an experimental evaluation. version:1
arxiv-1610-08762 | Volumetric Light-field Encryption at the Microscopic Scale | http://arxiv.org/abs/1610.08762 | id:1610.08762 author:Haoyu Li, Changliang Guo, Inbarasan Muniraj, Bryce C. Schroeder, John T. Sheridan, Shu Jia category:cs.CR cs.CV physics.bio-ph physics.optics  published:2016-10-26 summary:We report a light-field based method that allows the optical encryption of three-dimensional (3D) volumetric information at the microscopic scale in a single 2D light-field image. The system consists of a microlens array and an array of random phase/amplitude masks. The method utilizes a wave optics model to account for the dominant diffraction effect at this new scale, and the system point-spread function (PSF) serves as the key for encryption and decryption. We successfully developed and demonstrated a deconvolution algorithm to retrieve spatially multiplexed discrete and continuous volumetric data from 2D light-field images. Showing that the method is practical for data transmission and storage, we obtained a faithful reconstruction of the 3D volumetric information from a digital copy of the encrypted light-field image. The method represents a new level of optical encryption, paving the way for broad industrial and biomedical applications in processing and securing 3D data at the microscopic scale. version:1
arxiv-1610-08473 | Estimating the Size of a Large Network and its Communities from a Random Sample | http://arxiv.org/abs/1610.08473 | id:1610.08473 author:Lin Chen, Amin Karbasi, Forrest W. Crawford category:stat.ML cs.SI physics.soc-ph  published:2016-10-26 summary:Most real-world networks are too large to be measured or studied directly and there is substantial interest in estimating global network properties from smaller sub-samples. One of the most important global properties is the number of vertices/nodes in the network. Estimating the number of vertices in a large network is a major challenge in computer science, epidemiology, demography, and intelligence analysis. In this paper we consider a population random graph G = (V;E) from the stochastic block model (SBM) with K communities/blocks. A sample is obtained by randomly choosing a subset W and letting G(W) be the induced subgraph in G of the vertices in W. In addition to G(W), we observe the total degree of each sampled vertex and its block membership. Given this partial information, we propose an efficient PopULation Size Estimation algorithm, called PULSE, that correctly estimates the size of the whole population as well as the size of each community. To support our theoretical analysis, we perform an exhaustive set of experiments to study the effects of sample size, K, and SBM model parameters on the accuracy of the estimates. The experimental results also demonstrate that PULSE significantly outperforms a widely-used method called the network scale-up estimator in a wide variety of scenarios. We conclude with extensions and directions for future work. version:1
arxiv-1610-08466 | Recurrent switching linear dynamical systems | http://arxiv.org/abs/1610.08466 | id:1610.08466 author:Scott W. Linderman, Andrew C. Miller, Ryan P. Adams, David M. Blei, Liam Paninski, Matthew J. Johnson category:stat.ML  published:2016-10-26 summary:Many natural systems, such as neurons firing in the brain or basketball teams traversing a court, give rise to time series data with complex, nonlinear dynamics. We can gain insight into these systems by decomposing the data into segments that are each explained by simpler dynamic units. Building on switching linear dynamical systems (SLDS), we present a new model class that not only discovers these dynamical units, but also explains how their switching behavior depends on observations or continuous latent states. These "recurrent" switching linear dynamical systems provide further insight by discovering the conditions under which each unit is deployed, something that traditional SLDS models fail to do. We leverage recent algorithmic advances in approximate inference to make Bayesian inference in these models easy, fast, and scalable. version:1
arxiv-1610-08465 | Bayesian latent structure discovery from multi-neuron recordings | http://arxiv.org/abs/1610.08465 | id:1610.08465 author:Scott W. Linderman, Ryan P. Adams, Jonathan W. Pillow category:stat.ML q-bio.NC  published:2016-10-26 summary:Neural circuits contain heterogeneous groups of neurons that differ in type, location, connectivity, and basic response properties. However, traditional methods for dimensionality reduction and clustering are ill-suited to recovering the structure underlying the organization of neural circuits. In particular, they do not take advantage of the rich temporal dependencies in multi-neuron recordings and fail to account for the noise in neural spike trains. Here we describe new tools for inferring latent structure from simultaneously recorded spike train data using a hierarchical extension of a multi-neuron point process model commonly known as the generalized linear model (GLM). Our approach combines the GLM with flexible graph-theoretic priors governing the relationship between latent features and neural connectivity patterns. Fully Bayesian inference via P\'olya-gamma augmentation of the resulting model allows us to classify neurons and infer latent dimensions of circuit organization from correlated spike trains. We demonstrate the effectiveness of our method with applications to synthetic data and multi-neuron recordings in primate retina, revealing latent patterns of neural types and locations from spike trains alone. version:1
arxiv-1610-08462 | Distraction-Based Neural Networks for Document Summarization | http://arxiv.org/abs/1610.08462 | id:1610.08462 author:Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang category:cs.CL  published:2016-10-26 summary:Distributed representation learned with neural networks has recently shown to be effective in modeling natural languages at fine granularities such as words, phrases, and even sentences. Whether and how such an approach can be extended to help model larger spans of text, e.g., documents, is intriguing, and further investigation would still be desirable. This paper aims to enhance neural network models for such a purpose. A typical problem of document-level modeling is automatic summarization, which aims to model documents in order to generate summaries. In this paper, we propose neural models to train computers not just to pay attention to specific regions and content of input documents with attention models, but also distract them to traverse between different content of a document so as to better grasp the overall meaning for summarization. Without engineering any features, we train the models on two large datasets. The models achieve the state-of-the-art performance, and they significantly benefit from the distraction modeling, particularly when input documents are long. version:1
arxiv-1610-08452 | Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment | http://arxiv.org/abs/1610.08452 | id:1610.08452 author:Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna P. Gummadi category:stat.ML cs.LG  published:2016-10-26 summary:As the use of automated decision making systems becomes wide-spread, there is a growing concern about their potential unfairness towards people with certain traits. Anti-discrimination laws in various countries prohibit unfair treatment of individuals based on specific traits, also called sensitive attributes (e.g., gender, race). In many learning scenarios, the trained algorithms (classifiers) make decisions with certain inaccuracy (misclassification rate). As learning mechanisms target minimizing the error rate for all decisions, it is quite possible that the optimally trained algorithm makes decisions for users belonging to different sensitive attribute groups with different error rates (e.g., decision errors for females are higher than for males). To account for and avoid such unfairness when learning, in this paper, we introduce a new notion of unfairness, disparate mistreatment, which is defined in terms of misclassification rates. We then propose an intuitive measure of disparate mistreatment for decision boundary-based classifiers, which can be easily incorporated into their formulation as a convex-concave constraint. Experiments on synthetic as well as real world datasets show that our methodology is effective at avoiding disparate mistreatment, often at a small cost in terms of accuracy. version:1
arxiv-1610-08450 | Body movement to sound interface with vector autoregressive hierarchical hidden Markov models | http://arxiv.org/abs/1610.08450 | id:1610.08450 author:Dimitrije Markoviƒá, Borjana Valƒçiƒá, Neboj≈°a Male≈°eviƒá category:stat.AP cs.HC stat.ML  published:2016-10-26 summary:Interfacing a kinetic action of a person to an action of a machine system is an important research topic in many application areas. One of the key factors for intimate human-machine interaction is the ability of the control algorithm to detect and classify different user commands with shortest possible latency, thus making a highly correlated link between cause and effect. In our research, we focused on the task of mapping user kinematic actions into sound samples. The presented methodology relies on the wireless sensor nodes equipped with inertial measurement units and the real-time algorithm dedicated for early detection and classification of a variety of movements/gestures performed by a user. The core algorithm is based on the approximate Bayesian inference of Vector Autoregressive Hierarchical Hidden Markov Models (VAR-HHMM), where models database is derived from the set of motion gestures. The performance of the algorithm was compared with an online version of the K-nearest neighbours (KNN) algorithm, where we used offline expert based classification as the benchmark. In almost all of the evaluation metrics (e.g. confusion matrix, recall and precision scores) the VAR-HHMM algorithm outperformed KNN. Furthermore, the VAR-HHMM algorithm, in some cases, achieved faster movement onset detection compared with the offline standard. The proposed concept, although envisioned for movement-to-sound application, could be implemented in other human-machine interfaces. version:1
arxiv-1610-08445 | New Liftable Classes for First-Order Probabilistic Inference | http://arxiv.org/abs/1610.08445 | id:1610.08445 author:Seyed Mehran Kazemi, Angelika Kimmig, Guy Van den Broeck, David Poole category:cs.AI stat.ML  published:2016-10-26 summary:Statistical relational models provide compact encodings of probabilistic dependencies in relational domains, but result in highly intractable graphical models. The goal of lifted inference is to carry out probabilistic inference without needing to reason about each individual separately, by instead treating exchangeable, undistinguished objects as a whole. In this paper, we study the domain recursion inference rule, which, despite its central role in early theoretical results on domain-lifted inference, has later been believed redundant. We show that this rule is more powerful than expected, and in fact significantly extends the range of models for which lifted inference runs in time polynomial in the number of individuals in the domain. This includes an open problem called S4, the symmetric transitivity model, and a first-order logic encoding of the birthday paradox. We further identify new classes S2FO2 and S2RU of domain-liftable theories, which respectively subsume FO2 and recursively unary theories, the largest classes of domain-liftable theories known so far, and show that using domain recursion can achieve exponential speedup even in theories that cannot fully be lifted with the existing set of inference rules. version:1
arxiv-1610-08436 | Estimating the concentration of gold nanoparticles incorporated on Natural Rubber membranes using Multi-Level Starlet Optimal Segmentation | http://arxiv.org/abs/1610.08436 | id:1610.08436 author:Alexandre Fioravante de Siqueira, Fl√°vio Camargo Cabrera, Aylton Pagamisse, Aldo Eloizo Job category:cs.CV  published:2016-10-26 summary:This study consolidates Multi-Level Starlet Segmentation (MLSS) and Multi-Level Starlet Optimal Segmentation (MLSOS), techniques for photomicrograph segmentation that use starlet wavelet detail levels to separate areas of interest in an input image. Several segmentation levels can be obtained using Multi-Level Starlet Segmentation; after that, Matthews correlation coefficient (MCC) is used to choose an optimal segmentation level, giving rise to Multi-Level Starlet Optimal Segmentation. In this paper, MLSOS is employed to estimate the concentration of gold nanoparticles with diameter around 47 nm, reducted on natural rubber membranes. These samples were used on the construction of SERS/SERRS substrates and in the study of natural rubber membranes with incorporated gold nanoparticles influence on Leishmania braziliensis physiology. Precision, recall and accuracy are used to evaluate the segmentation performance, and MLSOS presents accuracy greater than 88% for this application. version:1
arxiv-1610-08431 | Broad Context Language Modeling as Reading Comprehension | http://arxiv.org/abs/1610.08431 | id:1610.08431 author:Zewei Chu, Hai Wang, Kevin Gimpel, David McAllester category:cs.CL  published:2016-10-26 summary:Progress in text understanding has been driven by the availability of large datasets that test particular capabilities, like recent datasets for assessing reading comprehension. We focus here on the LAMBADA dataset, a word prediction task requiring broader context than the immediate sentence. We view the LAMBADA task as a reading comprehension problem and apply off-the-shelf comprehension models based on neural networks. Though these models are constrained to choose a word from the context, they improve the state of the art on LAMBADA from 7.3% to 45.4%. We analyze 100 instances, finding that neural network readers perform well in cases that involve selecting a name from the context based on dialogue or discourse cues but struggle when coreference resolution or external knowledge is needed. version:1
arxiv-1610-08424 | Counterfactual Reasoning about Intent for Interactive Navigation in Dynamic Environments | http://arxiv.org/abs/1610.08424 | id:1610.08424 author:A. Bordallo, F. Previtali, N. Nardelli, S. Ramamoorthy category:cs.RO cs.LG  published:2016-10-26 summary:Many modern robotics applications require robots to function autonomously in dynamic environments including other decision making agents, such as people or other robots. This calls for fast and scalable interactive motion planning. This requires models that take into consideration the other agent's intended actions in one's own planning. We present a real-time motion planning framework that brings together a few key components including intention inference by reasoning counterfactually about potential motion of the other agents as they work towards different goals. By using a light-weight motion model, we achieve efficient iterative planning for fluid motion when avoiding pedestrians, in parallel with goal inference for longer range movement prediction. This inference framework is coupled with a novel distributed visual tracking method that provides reliable and robust models for the current belief-state of the monitored environment. This combined approach represents a computationally efficient alternative to previously studied policy learning methods that often require significant offline training or calibration and do not yet scale to densely populated environments. We validate this framework with experiments involving multi-robot and human-robot navigation. We further validate the tracker component separately on much larger scale unconstrained pedestrian data sets. version:1
arxiv-1610-08417 | Probabilistic Linear Multistep Methods | http://arxiv.org/abs/1610.08417 | id:1610.08417 author:Onur Teymur, Konstantinos Zygalakis, Ben Calderhead category:math.NA cs.NA stat.CO stat.ML  published:2016-10-26 summary:We present a derivation and theoretical investigation of the Adams-Bashforth and Adams-Moulton family of linear multistep methods for solving ordinary differential equations, starting from a Gaussian process (GP) framework. In the limit, this formulation coincides with the classical deterministic methods, which have been used as higher-order initial value problem solvers for over a century. Furthermore, the natural probabilistic framework provided by the GP formulation allows us to derive probabilistic versions of these methods, in the spirit of a number of other probabilistic ODE solvers presented in the recent literature. In contrast to higher-order Runge-Kutta methods, which require multiple intermediate function evaluations per step, Adams family methods make use of previous function evaluations, so that increased accuracy arising from a higher-order multistep approach comes at very little additional computational cost. We show that through a careful choice of covariance function for the GP, the posterior mean and standard deviation over the numerical solution can be made to exactly coincide with the value given by the deterministic method and its local truncation error respectively. We provide a rigorous proof of the convergence of these new methods, as well as an empirical investigation (up to fifth order) demonstrating their convergence rates in practice. version:1
arxiv-1610-08401 | Universal adversarial perturbations | http://arxiv.org/abs/1610.08401 | id:1610.08401 author:Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard category:cs.CV cs.AI cs.LG stat.ML  published:2016-10-26 summary:Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images. version:1
arxiv-1610-08400 | Video Analysis of "YouTube Funnies" to Aid the Study of Human Gait and Falls - Preliminary Results and Proof of Concept | http://arxiv.org/abs/1610.08400 | id:1610.08400 author:Babak Taati, Pranay Lohia, Avril Mansfield, Ahmed Ashraf category:cs.CV  published:2016-10-26 summary:Because falls are funny, YouTube and other video sharing sites contain a large repository of real-life falls. We propose extracting gait and balance information from these videos to help us better understand some of the factors that contribute to falls. Proof-of-concept is explored in a single video containing multiple (n=14) falls/non-falls in the presence of an unexpected obstacle. The analysis explores: computing spatiotemporal parameters of gait in a video captured from an arbitrary viewpoint; the relationship between parameters of gait from the last few steps before the obstacle and falling vs. not falling; and the predictive capacity of a multivariate model in predicting a fall in the presence of an unexpected obstacle. Homography transformations correct the perspective projection distortion and allow for the consistent tracking of gait parameters as an individual walks in an arbitrary direction in the scene. A synthetic top view allows for computing the average stride length and a synthetic side view allows for measuring up and down motions of the head. In leave-one-out cross-validation, we were able to correctly predict whether a person would fall or not in 11 out of the 14 cases (78.6%), just by looking at the average stride length and the range of vertical head motion during the 1-4 most recent steps prior to reaching the obstacle. version:1
arxiv-1610-06602 | Iterative Refinement for Machine Translation | http://arxiv.org/abs/1610.06602 | id:1610.06602 author:Roman Novak, Michael Auli, David Grangier category:cs.CL  published:2016-10-20 summary:Existing machine translation decoding algorithms generate translations in a strictly monotonic fashion and never revisit previous decisions. As a result, earlier mistakes cannot be corrected at a later stage. In this paper, we present a translation scheme that starts from an initial guess and then makes iterative improvements that may revisit previous decisions. We parameterize our model as a convolutional neural network that predicts discrete substitutions to an existing translation based on an attention mechanism over both the source sentence as well as the current translation output. By making less than one modification per sentence, we improve the output of a phrase-based translation system by up to 0.4 BLEU on WMT15 German-English translation. version:2
arxiv-1610-07557 | Automatic and Manual Segmentation of Hippocampus in Epileptic Patients MRI | http://arxiv.org/abs/1610.07557 | id:1610.07557 author:Mohammad-Parsa Hosseini, Mohammad-Reza Nazem-Zadeh, Dario Pompili, Kourosh Jafari-Khouzani, Kost Elisevich, Hamid Soltanian-Zadeh category:cs.CV physics.med-ph  published:2016-10-24 summary:The hippocampus is a seminal structure in the most common surgically-treated form of epilepsy. Accurate segmentation of the hippocampus aids in establishing asymmetry regarding size and signal characteristics in order to disclose the likely site of epileptogenicity. With sufficient refinement, it may ultimately aid in the avoidance of invasive monitoring with its expense and risk for the patient. To this end, a reliable and consistent method for segmentation of the hippocampus from magnetic resonance imaging (MRI) is needed. In this work, we present a systematic and statistical analysis approach for evaluation of automated segmentation methods in order to establish one that reliably approximates the results achieved by manual tracing of the hippocampus. version:2
arxiv-1610-08375 | Content Selection in Data-to-Text Systems: A Survey | http://arxiv.org/abs/1610.08375 | id:1610.08375 author:Dimitra Gkatzia category:cs.CL  published:2016-10-26 summary:Data-to-text systems are powerful in generating reports from data automatically and thus they simplify the presentation of complex data. Rather than presenting data using visualisation techniques, data-to-text systems use natural (human) language, which is the most common way for human-human communication. In addition, data-to-text systems can adapt their output content to users' preferences, background or interests and therefore they can be pleasant for users to interact with. Content selection is an important part of every data-to-text system, because it is the module that determines which from the available information should be conveyed to the user. This survey initially introduces the field of data-to-text generation, describes the general data-to-text system architecture and then it reviews the state-of-the-art content selection methods. Finally, it provides recommendations for choosing an approach and discusses opportunities for future research. version:1
arxiv-1611-00287 | Structured illumination microscopy with unknown patterns and a statistical prior | http://arxiv.org/abs/1611.00287 | id:1611.00287 author:Li-Hao Yeh, Lei Tian, Laura Waller category:cs.CV physics.optics  published:2016-10-26 summary:Structured illumination microscopy (SIM) improves resolution by down-modulating high-frequency information of an object to fit within the passband of the optical system. Generally, the reconstruction process requires prior knowledge of the illumination patterns, which implies a well-calibrated and aberration-free system. Here, we propose a new algorithmic self-calibration strategy for SIM that does not need to know the exact patterns a priori, but only their covariance. The algorithm, termed PE-SIMS, includes a Pattern-Estimation (PE) step and a SIM reconstruction procedure using a Statistical prior (SIMS). Additionally, we perform a pixel reassignment process (SIMS-PR) to enhance the reconstruction quality. We achieve 2x better resolution than a conventional widefield microscope, while remaining insensitive to aberration-induced pattern distortion and robust against parameter tuning compared to other blind SIM algorithms. version:1
arxiv-1610-06656 | Single Pass PCA of Matrix Products | http://arxiv.org/abs/1610.06656 | id:1610.06656 author:Shanshan Wu, Srinadh Bhojanapalli, Sujay Sanghavi, Alexandros G. Dimakis category:stat.ML cs.DS cs.IT cs.LG math.IT  published:2016-10-21 summary:In this paper we present a new algorithm for computing a low rank approximation of the product $A^TB$ by taking only a single pass of the two matrices $A$ and $B$. The straightforward way to do this is to (a) first sketch $A$ and $B$ individually, and then (b) find the top components using PCA on the sketch. Our algorithm in contrast retains additional summary information about $A,B$ (e.g. row and column norms etc.) and uses this additional information to obtain an improved approximation from the sketches. Our main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in addition we also provide results from an Apache Spark implementation that shows better computational and statistical performance on real-world and synthetic evaluation datasets. version:2
arxiv-1610-07796 | Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks | http://arxiv.org/abs/1610.07796 | id:1610.07796 author:Carsten Schnober, Steffen Eger, Erik-L√¢n Do Dinh, Iryna Gurevych category:cs.CL  published:2016-10-25 summary:We analyze the performance of encoder-decoder neural models and compare them with well-known established methods. The latter represent different classes of traditional approaches that are applied to the monotone sequence-to-sequence tasks OCR post-correction, spelling correction, grapheme-to-phoneme conversion, and lemmatization. Such tasks are of practical relevance for various higher-level research fields including digital humanities, automatic text correction, and speech recognition. We investigate how well generic deep-learning approaches adapt to these tasks, and how they perform in comparison with established and more specialized methods, including our own adaptation of pruned CRFs. version:2
arxiv-1610-08251 | Quantum-enhanced machine learning | http://arxiv.org/abs/1610.08251 | id:1610.08251 author:Vedran Dunjko, Jacob M. Taylor, Hans J. Briegel category:quant-ph cs.AI cs.LG  published:2016-10-26 summary:The emerging field of quantum machine learning has the potential to substantially aid in the problems and scope of artificial intelligence. This is only enhanced by recent successes in the field of classical machine learning. In this work we propose an approach for the systematic treatment of machine learning, from the perspective of quantum information. Our approach is general and covers all three main branches of machine learning: supervised, unsupervised and reinforcement learning. While quantum improvements in supervised and unsupervised learning have been reported, reinforcement learning has received much less attention. Within our approach, we tackle the problem of quantum enhancements in reinforcement learning as well, and propose a systematic scheme for providing improvements. As an example, we show that quadratic improvements in learning efficiency, and exponential improvements in performance over limited time periods, can be obtained for a broad class of learning problems. version:1
arxiv-1610-08250 | An Improved Approach for Prediction of Parkinson's Disease using Machine Learning Techniques | http://arxiv.org/abs/1610.08250 | id:1610.08250 author:Kamal Nayan Reddy Challa, Venkata Sasank Pagolu, Ganapati Panda, Babita Majhi category:cs.LG  published:2016-10-26 summary:Parkinson's disease (PD) is one of the major public health problems in the world. It is a well-known fact that around one million people suffer from Parkinson's disease in the United States whereas the number of people suffering from Parkinson's disease worldwide is around 5 million. Thus, it is important to predict Parkinson's disease in early stages so that early plan for the necessary treatment can be made. People are mostly familiar with the motor symptoms of Parkinson's disease, however, an increasing amount of research is being done to predict the Parkinson's disease from non-motor symptoms that precede the motor ones. If an early and reliable prediction is possible then a patient can get a proper treatment at the right time. Nonmotor symptoms considered are Rapid Eye Movement (REM) sleep Behaviour Disorder (RBD) and olfactory loss. Developing machine learning models that can help us in predicting the disease can play a vital role in early prediction. In this paper, we extend a work which used the non-motor features such as RBD and olfactory loss. Along with this the extended work also uses important biomarkers. In this paper, we try to model this classifier using different machine learning models that have not been used before. We developed automated diagnostic models using Multilayer Perceptron, BayesNet, Random Forest and Boosted Logistic Regression. It has been observed that Boosted Logistic Regression provides the best performance with an impressive accuracy of 97.159 % and the area under the ROC curve was 98.9%. Thus, it is concluded that these models can be used for early prediction of Parkinson's disease. version:1
arxiv-1610-08229 | Word Embeddings and Their Use In Sentence Classification Tasks | http://arxiv.org/abs/1610.08229 | id:1610.08229 author:Amit Mandelbaum, Adi Shalev category:cs.LG cs.CL  published:2016-10-26 summary:This paper have two parts. In the first part we discuss word embeddings. We discuss the need for them, some of the methods to create them, and some of their interesting properties. We also compare them to image embeddings and see how word embedding and image embedding can be combined to perform different tasks. In the second part we implement a convolutional neural network trained on top of pre-trained word vectors. The network is used for several sentence-level classification tasks, and achieves state-of-art (or comparable) results, demonstrating the great power of pre-trainted word embeddings over random ones. version:1
arxiv-1610-08189 | Tensor Decompositions for Identifying Directed Graph Topologies and Tracking Dynamic Networks | http://arxiv.org/abs/1610.08189 | id:1610.08189 author:Yanning Shen, Brian Baingana, Georgios B. Giannakis category:stat.ML  published:2016-10-26 summary:Directed networks are pervasive both in nature and engineered systems, often underlying the complex behavior observed in biological systems, microblogs and social interactions over the web, as well as global financial markets. Since their structures are often unobservable, in order to facilitate network analytics, one generally resorts to approaches capitalizing on measurable nodal processes to infer the unknown topology. Structural equation models (SEMs) are capable of incorporating exogenous inputs to resolve inherent directional ambiguities. However, conventional SEMs assume full knowledge of exogenous inputs, which may not be readily available in some practical settings. The present paper advocates a novel SEM-based topology inference approach that entails factorization of a three-way tensor, constructed from the observed nodal data, using the well-known parallel factor (PARAFAC) decomposition. It turns out that second-order piecewise stationary statistics of exogenous variables suffice to identify the hidden topology. Capitalizing on the uniqueness properties inherent to high-order tensor factorizations, it is shown that topology identification is possible under reasonably mild conditions. In addition, to facilitate real-time operation and inference of time-varying networks, an adaptive (PARAFAC) tensor decomposition scheme which tracks the topology-revealing tensor factors is developed. Extensive tests on simulated and real stock quote data demonstrate the merits of the novel tensor-based approach. version:1
arxiv-1610-07667 | Predicting Counterfactuals from Large Historical Data and Small Randomized Trials | http://arxiv.org/abs/1610.07667 | id:1610.07667 author:Nir Rosenfeld, Yishay Mansour, Elad Yom-Tov category:cs.LG  published:2016-10-24 summary:When a new treatment is considered for use, whether a pharmaceutical drug or a search engine ranking algorithm, a typical question that arises is, will its performance exceed that of the current treatment? The conventional way to answer this counterfactual question is to estimate the effect of the new treatment in comparison to that of the conventional treatment by running a controlled, randomized experiment. While this approach theoretically ensures an unbiased estimator, it suffers from several drawbacks, including the difficulty in finding representative experimental populations as well as the cost of running such trials. Moreover, such trials neglect the huge quantities of available control-condition data which are often completely ignored. In this paper we propose a discriminative framework for estimating the performance of a new treatment given a large dataset of the control condition and data from a small (and possibly unrepresentative) randomized trial comparing new and old treatments. Our objective, which requires minimal assumptions on the treatments, models the relation between the outcomes of the different conditions. This allows us to not only estimate mean effects but also to generate individual predictions for examples outside the randomized sample. We demonstrate the utility of our approach through experiments in three areas: Search engine operation, treatments to diabetes patients, and market value estimation for houses. Our results demonstrate that our approach can reduce the number and size of the currently performed randomized controlled experiments, thus saving significant time, money and effort on the part of practitioners. version:2
arxiv-1610-08035 | Parallelizable sparse inverse formulation Gaussian processes (SpInGP) | http://arxiv.org/abs/1610.08035 | id:1610.08035 author:Alexander Grigorievskiy, Neil Lawrence, Simo S√§rkk√§ category:stat.ML  published:2016-10-25 summary:We propose a parallelizable sparse inverse formulation Gaussian process (SpInGP) al- gorithm for temporal Gaussian process mod- els. It uses a sparse precision GP formulation and sparse matrix routines to speed up the computations. Due to the state-space formu- lation used in the algorithm, the time com- plexity of the basic SpInGP is linear, and be- cause all the computations are parallelizable, the parallel form of the algorithm is sublin- ear in the number of data points. We provide example algorithms to implement the sparse matrix routines and experimentally test the method using both simulated and real data. version:2
arxiv-1610-08166 | Automatic measurement of vowel duration via structured prediction | http://arxiv.org/abs/1610.08166 | id:1610.08166 author:Yossi Adi, Joseph Keshet, Emily Cibelli, Erin Gustafson, Cynthia Clopper, Matthew Goldrick category:stat.ML cs.LG cs.SD  published:2016-10-26 summary:A key barrier to making phonetic studies scalable and replicable is the need to rely on subjective, manual annotation. To help meet this challenge, a machine learning algorithm was developed for automatic measurement of a widely used phonetic measure: vowel duration. Manually-annotated data were used to train a model that takes as input an arbitrary length segment of the acoustic signal containing a single vowel that is preceded and followed by consonants and outputs the duration of the vowel. The model is based on the structured prediction framework. The input signal and a hypothesized set of a vowel's onset and offset are mapped to an abstract vector space by a set of acoustic feature functions. The learning algorithm is trained in this space to minimize the difference in expectations between predicted and manually-measured vowel durations. The trained model can then automatically estimate vowel durations without phonetic or orthographic transcription. Results comparing the model to three sets of manually annotated data suggest it out-performed the current gold standard for duration measurement, an HMM-based forced aligner (which requires orthographic or phonetic transcription as an input). version:1
arxiv-1610-08133 | Incremental Nonparametric Weighted Feature Extraction for OnlineSubspace Pattern Classification | http://arxiv.org/abs/1610.08133 | id:1610.08133 author:Hamid Abrishami Moghaddam, Elaheh Raisi category:cs.CV  published:2016-10-26 summary:In this paper, a new online method based on nonparametric weighted feature extraction (NWFE) is proposed. NWFE was introduced to enjoy optimum characteristics of linear discriminant analysis (LDA) and nonparametric discriminant analysis (NDA) while rectifying their drawbacks. It emphasizes the points near decision boundary by putting greater weights on them and deemphasizes other points. Incremental nonparametric weighted feature extraction (INWFE) is the online version of NWFE. INWFE has advantages of NWFE method such as extracting more than L-1 features in contrast to LDA. It is independent of the class distribution and performs well in complex distributed data. The effects of outliers are reduced due to the nature of its nonparametric scatter matrix. Furthermore, it is possible to add new samples asynchronously, i.e. whenever a new sample becomes available at any given time, it can be added to the algorithm. This is useful for many real world applications since all data cannot be available in advance. This method is implemented on Gaussian and non-Gaussian multidimensional data, a number of UCI datasets and Indian Pine dataset. Results are compared with NWFE in terms of classification accuracy and execution time. For nearest neighbour classifier it shows that this technique converges to NWFE at the end of learning process. In addition, the computational complexity is reduced in comparison with NWFE in terms of execution time. version:1
arxiv-1610-07258 | Representation Learning with Deconvolution for Multivariate Time Series Classification and Visualization | http://arxiv.org/abs/1610.07258 | id:1610.07258 author:Zhiguang Wang, Wei Song, Lu Liu, Fan Zhang, Junxiao Xue, Yangdong Ye, Ming Fan, Mingliang Xu category:cs.LG cs.NE  published:2016-10-24 summary:We propose a new model based on the deconvolutional networks and SAX discretization to learn the representation for multivariate time series. Deconvolutional networks fully exploit the advantage the powerful expressiveness of deep neural networks in the manner of unsupervised learning. We design a network structure specifically to capture the cross-channel correlation with deconvolution, forcing the pooling operation to perform the dimension reduction along each position in the individual channel. Discretization based on Symbolic Aggregate Approximation is applied on the feature vectors to further extract the bag of features. We show how this representation and bag of features helps on classification. A full comparison with the sequence distance based approach is provided to demonstrate the effectiveness of our approach on the standard datasets. We further build the Markov matrix from the discretized representation from the deconvolution to visualize the time series as complex networks, which show more class-specific statistical properties and clear structures with respect to different labels. version:2
arxiv-1610-08127 | Fast Bayesian Non-Negative Matrix Factorisation and Tri-Factorisation | http://arxiv.org/abs/1610.08127 | id:1610.08127 author:Thomas Brouwer, Jes Frellsen, Pietro Lio' category:cs.LG cs.AI cs.NA stat.ML  published:2016-10-26 summary:We present a fast variational Bayesian algorithm for performing non-negative matrix factorisation and tri-factorisation. We show that our approach achieves faster convergence per iteration and timestep (wall-clock) than Gibbs sampling and non-probabilistic approaches, and do not require additional samples to estimate the posterior. We show that in particular for matrix tri-factorisation convergence is difficult, but our variational Bayesian approach offers a fast solution, allowing the tri-factorisation approach to be used more effectively. version:1
arxiv-1610-08123 | Socratic Learning | http://arxiv.org/abs/1610.08123 | id:1610.08123 author:Rose Yu, Paroma Varma, Dan Iter, Christopher De Sa, Christopher R√© category:cs.LG  published:2016-10-25 summary:Modern machine learning techniques, such as deep learning, often use discriminative models that require large amounts of labeled data. An alternative approach is to use a generative model, which leverages heuristics from domain experts to train on unlabeled data. Domain experts often prefer to use generative models because they "tell a story" about their data. Unfortunately, generative models are typically less accurate than discriminative models. Several recent approaches combine both types of model to exploit their strengths. In this setting, a misspecified generative model can hurt the performance of subsequent discriminative training. To address this issue, we propose a framework called Socratic learning that automatically uses information from the discriminative model to correct generative model misspecification. Furthermore, this process provides users with interpretable feedback about how to improve their generative model. We evaluate Socratic learning on real-world relation extraction tasks and observe an immediate improvement in classification accuracy that could otherwise require several weeks of effort by domain experts. version:1
arxiv-1610-08120 | Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards | http://arxiv.org/abs/1610.08120 | id:1610.08120 author:Suchet Bargoti, James Underwood category:cs.RO cs.CV cs.LG  published:2016-10-25 summary:Ground vehicles equipped with monocular vision systems are a valuable source of high resolution image data for precision agriculture applications in orchards. This paper presents an image processing framework for fruit detection and counting using orchard image data. A general purpose image segmentation approach is used, including two feature learning algorithms; multi-scale Multi-Layered Perceptrons (MLP) and Convolutional Neural Networks (CNN). These networks were extended by including contextual information about how the image data was captured (metadata), which correlates with some of the appearance variations and/or class distributions observed in the data. The pixel-wise fruit segmentation output is processed using the Watershed Segmentation (WS) and Circular Hough Transform (CHT) algorithms to detect and count individual fruits. Experiments were conducted in a commercial apple orchard near Melbourne, Australia. The results show an improvement in fruit segmentation performance with the inclusion of metadata on the previously benchmarked MLP network. We extend this work with CNNs, bringing agrovision closer to the state-of-the-art in computer vision, where although metadata had negligible influence, the best pixel-wise F1-score of $0.791$ was achieved. The WS algorithm produced the best apple detection and counting results, with a detection F1-score of $0.858$. As a final step, image fruit counts were accumulated over multiple rows at the orchard and compared against the post-harvest fruit counts that were obtained from a grading and counting machine. The count estimates using CNN and WS resulted in the best performance for this dataset, with a squared correlation coefficient of $r^2=0.826$. version:1
arxiv-1610-08119 | Predicting First Impressions with Deep Learning | http://arxiv.org/abs/1610.08119 | id:1610.08119 author:Mel McCurrie, Fernando Beletti, Lucas Parzianello, Allen Westendorp, Samuel Anthony, Walter Scheirer category:cs.CV  published:2016-10-25 summary:Describable visual facial attributes are now commonplace in human biometrics and affective computing, with existing algorithms even reaching a sufficient point of maturity for placement into commercial products. These algorithms model objective facets of facial appearance, such as hair and eye color, expression, and aspects of the geometry of the face. A natural extension, which has not been studied to any great extent thus far, is the ability to model subjective attributes that are assigned to a face based purely on visual judgements. For instance, with just a glance, our first impression of a face may lead us to believe that a person is smart, worthy of our trust, and perhaps even our admiration - regardless of the underlying truth behind such attributes. Psychologists believe that these judgements are based on a variety of factors such as emotional states, personality traits, and other physiognomic cues. But work in this direction leads to an interesting question: how do we create models for problems where there is no ground truth, only measurable behavior? In this paper, we introduce a new convolutional neural network-based regression framework that allows us to train predictive models of crowd behavior for social attribute assignment. Over images from the AFLW face database, these models demonstrate strong correlations with human crowd ratings. version:1
arxiv-1610-02590 | Indirect Gaussian Graph Learning beyond Gaussianity | http://arxiv.org/abs/1610.02590 | id:1610.02590 author:Yiyuan She, Shao Tang, Qiaoya Zhang category:stat.ML stat.ME  published:2016-10-08 summary:This paper studies how to capture dependency graph structures from real data which may not be multivariate Gaussian. Starting from marginal loss functions not necessarily derived from probability distributions, we use an additive over-parametrization with shrinkage to incorporate variable dependencies into the criterion. An iterative Gaussian graph learning algorithm is proposed with ease in implementation. Statistical analysis shows that with the error measured in terms of a proper Bregman divergence, the estimators have fast rate of convergence. Real-life examples in different settings are given to demonstrate the efficacy of the proposed methodology. version:2
arxiv-1610-08095 | Modeling Ambiguity, Subjectivity, and Diverging Viewpoints in Opinion Question Answering Systems | http://arxiv.org/abs/1610.08095 | id:1610.08095 author:Mengting Wan, Julian McAuley category:cs.IR cs.CL  published:2016-10-25 summary:Product review websites provide an incredible lens into the wide variety of opinions and experiences of different people, and play a critical role in helping users discover products that match their personal needs and preferences. To help address questions that can't easily be answered by reading others' reviews, some review websites also allow users to pose questions to the community via a question-answering (QA) system. As one would expect, just as opinions diverge among different reviewers, answers to such questions may also be subjective, opinionated, and divergent. This means that answering such questions automatically is quite different from traditional QA tasks, where it is assumed that a single `correct' answer is available. While recent work introduced the idea of question-answering using product reviews, it did not account for two aspects that we consider in this paper: (1) Questions have multiple, often divergent, answers, and this full spectrum of answers should somehow be used to train the system; and (2) What makes a `good' answer depends on the asker and the answerer, and these factors should be incorporated in order for the system to be more personalized. Here we build a new QA dataset with 800 thousand questions---and over 3.1 million answers---and show that explicitly accounting for personalization and ambiguity leads both to quantitatively better answers, but also a more nuanced view of the range of supporting, but subjective, opinions. version:1
arxiv-1610-08078 | Dis-S2V: Discourse Informed Sen2Vec | http://arxiv.org/abs/1610.08078 | id:1610.08078 author:Tanay Kumar Saha, Shafiq Joty, Naeemul Hassan, Mohammad Al Hasan category:cs.CL cs.IR  published:2016-10-25 summary:Vector representation of sentences is important for many text processing tasks that involve clustering, classifying, or ranking sentences. Recently, distributed representation of sentences learned by neural models from unlabeled data has been shown to outperform the traditional bag-of-words representation. However, most of these learning methods consider only the content of a sentence and disregard the relations among sentences in a discourse by and large. In this paper, we propose a series of novel models for learning latent representations of sentences (Sen2Vec) that consider the content of a sentence as well as inter-sentence relations. We first represent the inter-sentence relations with a language network and then use the network to induce contextual information into the content-based Sen2Vec models. Two different approaches are introduced to exploit the information in the network. Our first approach retrofits (already trained) Sen2Vec vectors with respect to the network in two different ways: (1) using the adjacency relations of a node, and (2) using a stochastic sampling method which is more flexible in sampling neighbors of a node. The second approach uses a regularizer to encode the information in the network into the existing Sen2Vec model. Experimental results show that our proposed models outperform existing methods in three fundamental information system tasks demonstrating the effectiveness of our approach. The models leverage the computational power of multi-core CPUs to achieve fine-grained computational efficiency. We make our code publicly available upon acceptance. version:1
arxiv-1610-09201 | A Conceptual Development of Quench Prediction App build on LSTM and ELQA framework | http://arxiv.org/abs/1610.09201 | id:1610.09201 author:Matej Mertik, Maciej Wielgosz, Andrzej Skocze≈Ñ category:cs.LG  published:2016-10-25 summary:This article presents a development of web application for quench prediction in \gls{te-mpe-ee} at CERN. The authors describe an ELectrical Quality Assurance (ELQA) framework, a platform which was designed for rapid development of web integrated data analysis applications for different analysis needed during the hardware commissioning of the Large Hadron Collider (LHC). In second part the article describes a research carried out with the data collected from Quench Detection System by means of using an LSTM recurrent neural network. The article discusses and presents a conceptual work of implementing quench prediction application for \gls{te-mpe-ee} based on the ELQA and quench prediction algorithm. version:1
arxiv-1610-08077 | A statistical framework for fair predictive algorithms | http://arxiv.org/abs/1610.08077 | id:1610.08077 author:Kristian Lum, James Johndrow category:stat.ML cs.LG  published:2016-10-25 summary:Predictive modeling is increasingly being employed to assist human decision-makers. One purported advantage of replacing human judgment with computer models in high stakes settings-- such as sentencing, hiring, policing, college admissions, and parole decisions-- is the perceived "neutrality" of computers. It is argued that because computer models do not hold personal prejudice, the predictions they produce will be equally free from prejudice. There is growing recognition that employing algorithms does not remove the potential for bias, and can even amplify it, since training data were inevitably generated by a process that is itself biased. In this paper, we provide a probabilistic definition of algorithmic bias. We propose a method to remove bias from predictive models by removing all information regarding protected variables from the permitted training data. Unlike previous work in this area, our framework is general enough to accommodate arbitrary data types, e.g. binary, continuous, etc. Motivated by models currently in use in the criminal justice system that inform decisions on pre-trial release and paroling, we apply our proposed method to a dataset on the criminal histories of individuals at the time of sentencing to produce "race-neutral" predictions of re-arrest. In the process, we demonstrate that the most common approach to creating "race-neutral" models-- omitting race as a covariate-- still results in racially disparate predictions. We then demonstrate that the application of our proposed method to these data removes racial disparities from predictions with minimal impact on predictive accuracy. version:1
arxiv-1610-08074 | Gaussian Process Kernels for Popular State-Space Time Series Models | http://arxiv.org/abs/1610.08074 | id:1610.08074 author:Alexander Grigorievskiy, Juha Karhunen category:stat.ML  published:2016-10-25 summary:In this paper we investigate a link between state- space models and Gaussian Processes (GP) for time series modeling and forecasting. In particular, several widely used state- space models are transformed into continuous time form and corresponding Gaussian Process kernels are derived. Experimen- tal results demonstrate that the derived GP kernels are correct and appropriate for Gaussian Process Regression. An experiment with a real world dataset shows that the modeling is identical with state-space models and with the proposed GP kernels. The considered connection allows the researchers to look at their models from a different angle and facilitate sharing ideas between these two different modeling approaches. version:1
arxiv-1610-08000 | Statistical Machine Translation for Indian Languages: Mission Hindi 2 | http://arxiv.org/abs/1610.08000 | id:1610.08000 author:Raj Nath Patel, Prakash B. Pimpale category:cs.CL  published:2016-10-25 summary:This paper presents Centre for Development of Advanced Computing Mumbai's (CDACM) submission to NLP Tools Contest on Statistical Machine Translation in Indian Languages (ILSMT) 2015 (collocated with ICON 2015). The aim of the contest was to collectively explore the effectiveness of Statistical Machine Translation (SMT) while translating within Indian languages and between English and Indian languages. In this paper, we report our work on all five language pairs, namely Bengali-Hindi (\bnhi), Marathi-Hindi (\mrhi), Tamil-Hindi (\tahi), Telugu-Hindi (\tehi), and English-Hindi (\enhi) for Health, Tourism, and General domains. We have used suffix separation, compound splitting and preordering prior to SMT training and testing. version:1
arxiv-1610-07940 | End-to-end Learning of Deep Visual Representations for Image Retrieval | http://arxiv.org/abs/1610.07940 | id:1610.07940 author:Albert Gordo, Jon Almazan, Jerome Revaud, Diane Larlus category:cs.CV  published:2016-10-25 summary:While deep learning has become a key ingredient in the top performing methods for many computer vision tasks, it has failed so far to bring similar improvements to instance-level image retrieval. In this article, we argue that reasons for the underwhelming results of deep methods on image retrieval are threefold: i) noisy training data, ii) inappropriate deep architecture, and iii) suboptimal training procedure. We address all three issues. First, we leverage a large-scale but noisy landmark dataset and develop an automatic cleaning method that produces a suitable training set for deep retrieval. Second, we build on the recent R-MAC descriptor, show that it can be interpreted as a deep and differentiable architecture, and present improvements to enhance it. Last, we train this network with a siamese architecture that combines three streams with a triplet loss. At the end of the training process, the proposed architecture produces a global image representation in a single forward pass that is well suited for image retrieval. Extensive experiments show that our approach significantly outperforms previous retrieval approaches, including state-of-the-art methods based on costly local descriptor indexing and spatial verification. On Oxford 5k, Paris 6k and Holidays, we respectively report 94.7, 96.6, and 94.8 mean average precision. Our representations can also be heavily compressed using product quantization with little loss in accuracy. For additional material, please see www.xrce.xerox.com/Deep-Image-Retrieval. version:1
arxiv-1610-07935 | PATH: Person Authentication using Trace Histories | http://arxiv.org/abs/1610.07935 | id:1610.07935 author:Upal Mahbub, Rama Chellappa category:cs.CV  published:2016-10-25 summary:In this paper, a solution to the problem of Active Authentication using trace histories is addressed. Specifically, the task is to perform user verification on mobile devices using historical location traces of the user as a function of time. Considering the movement of a human as a Markovian motion, a modified Hidden Markov Model (HMM)-based solution is proposed. The proposed method, namely the Marginally Smoothed HMM (MSHMM), utilizes the marginal probabilities of location and timing information of the observations to smooth-out the emission probabilities while training. Hence, it can efficiently handle unforeseen observations during the test phase. The verification performance of this method is compared to a sequence matching (SM) method , a Markov Chain-based method (MC) and an HMM with basic Laplace Smoothing (HMM-lap). Experimental results using the location information of the UMD Active Authentication Dataset-02 (UMDAA02) and the GeoLife dataset are presented. The proposed MSHMM method outperforms the compared methods in terms of equal error rate (EER). Additionally, the effects of different parameters on the proposed method are discussed. version:1
arxiv-1610-07931 | Anatomically Constrained Video-CT Registration via the V-IMLOP Algorithm | http://arxiv.org/abs/1610.07931 | id:1610.07931 author:Seth D. Billings, Ayushi Sinha, Austin Reiter, Simon Leonard, Masaru Ishii, Gregory D. Hager, Russell H. Taylor category:cs.CV  published:2016-10-25 summary:Functional endoscopic sinus surgery (FESS) is a surgical procedure used to treat acute cases of sinusitis and other sinus diseases. FESS is fast becoming the preferred choice of treatment due to its minimally invasive nature. However, due to the limited field of view of the endoscope, surgeons rely on navigation systems to guide them within the nasal cavity. State of the art navigation systems report registration accuracy of over 1mm, which is large compared to the size of the nasal airways. We present an anatomically constrained video-CT registration algorithm that incorporates multiple video features. Our algorithm is robust in the presence of outliers. We also test our algorithm on simulated and in-vivo data, and test its accuracy against degrading initializations. version:1
arxiv-1610-07930 | Active User Authentication for Smartphones: A Challenge Data Set and Benchmark Results | http://arxiv.org/abs/1610.07930 | id:1610.07930 author:Upal Mahbub, Sayantan Sarkar, Vishal M. Patel, Rama Chellappa category:cs.CV cs.DB  published:2016-10-25 summary:In this paper, automated user verification techniques for smartphones are investigated. A unique non-commercial dataset, the University of Maryland Active Authentication Dataset 02 (UMDAA-02) for multi-modal user authentication research is introduced. This paper focuses on three sensors - front camera, touch sensor and location service while providing a general description for other modalities. Benchmark results for face detection, face verification, touch-based user identification and location-based next-place prediction are presented, which indicate that more robust methods fine-tuned to the mobile platform are needed to achieve satisfactory verification accuracy. The dataset will be made available to the research community for promoting additional research. version:1
arxiv-1610-07918 | Sequence Segmentation Using Joint RNN and Structured Prediction Models | http://arxiv.org/abs/1610.07918 | id:1610.07918 author:Yossi Adi, Joseph Keshet, Emily Cibelli, Matthew Goldrick category:cs.CL  published:2016-10-25 summary:We describe and analyze a simple and effective algorithm for sequence segmentation applied to speech processing tasks. We propose a neural architecture that is composed of two modules trained jointly: a recurrent neural network (RNN) module and a structured prediction model. The RNN outputs are considered as feature functions to the structured model. The overall model is trained with a structured loss function which can be designed to the given segmentation task. We demonstrate the effectiveness of our method by applying it to two simple tasks commonly used in phonetic studies: word segmentation and voice onset time segmentation. Results sug- gest the proposed model is superior to previous methods, ob- taining state-of-the-art results on the tested datasets. version:1
arxiv-1610-07883 | Generalization Bounds for Weighted Automata | http://arxiv.org/abs/1610.07883 | id:1610.07883 author:Borja Balle, Mehryar Mohri category:cs.LG cs.FL  published:2016-10-25 summary:This paper studies the problem of learning weighted automata from a finite labeled training sample. We consider several general families of weighted automata defined in terms of three different measures: the norm of an automaton's weights, the norm of the function computed by an automaton, or the norm of the corresponding Hankel matrix. We present new data-dependent generalization guarantees for learning weighted automata expressed in terms of the Rademacher complexity of these families. We further present upper bounds on these Rademacher complexities, which reveal key new data-dependent terms related to the complexity of learning weighted automata. version:1
arxiv-1610-07882 | Maxmin convolutional neural networks for image classification | http://arxiv.org/abs/1610.07882 | id:1610.07882 author:Michael Blot, Matthieu Cord, Nicolas Thome category:cs.CV  published:2016-10-25 summary:Convolutional neural networks (CNN) are widely used in computer vision, especially in image classification. However, the way in which information and invariance properties are encoded through in deep CNN architectures is still an open question. In this paper, we propose to modify the standard convo- lutional block of CNN in order to transfer more information layer after layer while keeping some invariance within the net- work. Our main idea is to exploit both positive and negative high scores obtained in the convolution maps. This behav- ior is obtained by modifying the traditional activation func- tion step before pooling. We are doubling the maps with spe- cific activations functions, called MaxMin strategy, in order to achieve our pipeline. Extensive experiments on two classical datasets, MNIST and CIFAR-10, show that our deep MaxMin convolutional net outperforms standard CNN. version:1
arxiv-1610-07844 | Improving historical spelling normalization with bi-directional LSTMs and multi-task learning | http://arxiv.org/abs/1610.07844 | id:1610.07844 author:Marcel Bollmann, Anders S√∏gaard category:cs.CL  published:2016-10-25 summary:Natural-language processing of historical documents is complicated by the abundance of variant spellings and lack of annotated data. A common approach is to normalize the spelling of historical words to modern forms. We explore the suitability of a deep neural network architecture for this task, particularly a deep bi-LSTM network applied on a character level. Our model compares well to previously established normalization algorithms when evaluated on a diverse set of texts from Early New High German. We show that multi-task learning with additional normalization data can improve our model's performance further. version:1
arxiv-1610-07393 | Record Counting in Historical Handwritten Documents with Convolutional Neural Networks | http://arxiv.org/abs/1610.07393 | id:1610.07393 author:Samuele Capobianco, Simone Marinai category:cs.CV  published:2016-10-24 summary:In this paper, we investigate the use of Convolutional Neural Networks for counting the number of records in historical handwritten documents. With this work we demonstrate that training the networks only with synthetic images allows us to perform a near perfect evaluation of the number of records printed on historical documents. The experiments have been performed on a benchmark dataset composed by marriage records and outperform previous results on this dataset. version:2
arxiv-1610-07809 | How Document Pre-processing affects Keyphrase Extraction Performance | http://arxiv.org/abs/1610.07809 | id:1610.07809 author:Florian Boudin, Hugo Mougard, Damien Cram category:cs.CL  published:2016-10-25 summary:The SemEval-2010 benchmark dataset has brought renewed attention to the task of automatic keyphrase extraction. This dataset is made up of scientific articles that were automatically converted from PDF format to plain text and thus require careful preprocessing so that irrevelant spans of text do not negatively affect keyphrase extraction performance. In previous work, a wide range of document preprocessing techniques were described but their impact on the overall performance of keyphrase extraction models is still unexplored. Here, we re-assess the performance of several keyphrase extraction models and measure their robustness against increasingly sophisticated levels of document preprocessing. version:1
arxiv-1610-07804 | mdBrief - A Fast Online Adaptable, Distorted Binary Descriptor for Real-Time Applications Using Calibrated Wide-Angle Or Fisheye Cameras | http://arxiv.org/abs/1610.07804 | id:1610.07804 author:Steffen Urban, Stefan Hinz category:cs.CV  published:2016-10-25 summary:Fast binary descriptors build the core for many vision based applications with real-time demands like object detection, Visual Odometry or SLAM. Commonly it is assumed, that the acquired images and thus the patches extracted around keypoints originate from a perspective projection ignoring image distortion or completely different types of projections such as omnidirectional or fisheye. Usually the deviations from a perfect perspective projection are corrected by undistortion. Latter, however, introduces severe artifacts if the cameras field-of-view gets larger. In this paper, we propose a distorted and masked version of the BRIEF descriptor for calibrated cameras. Instead of correcting the distortion holistically, we distort the binary tests and thus adapt the descriptor to different image regions. version:1
arxiv-1610-07797 | Frank-Wolfe Algorithms for Saddle Point Problems | http://arxiv.org/abs/1610.07797 | id:1610.07797 author:Gauthier Gidel, Tony Jebara, Simon Lacoste-Julien category:math.OC cs.LG stat.ML 90C52  90C90  68T05 G.1.6; I.2.6  published:2016-10-25 summary:We extend the Frank-Wolfe (FW) optimization algorithm to solve constrained smooth convex-concave saddle point (SP) problems. Remarkably, the method only requires access to linear minimization oracles. Leveraging recent advances in FW optimization, we provide the first proof of convergence of a FW-type saddle point solver over polytopes, thereby partially answering a 30 year-old conjecture. We also survey other convergence results and highlight gaps in the theoretical underpinnings of FW-style algorithms. Motivating applications without known efficient alternatives are explored through structured prediction with combinatorial penalties as well as games over matching polytopes involving an exponential number of constraints. version:1
arxiv-1610-07407 | C-mix: a high dimensional mixture model for censored durations, with applications to genetic data | http://arxiv.org/abs/1610.07407 | id:1610.07407 author:Simon Bussy, Agathe Guilloux, St√©phane Ga√Øffas, Anne-Sophie Jannot category:stat.ML  published:2016-10-24 summary:We introduce a mixture model for censored durations (C-mix), and develop maximum likelihood inference for the joint estimation of the time distributions and latent regression parameters of the model. We consider a high-dimensional setting, with datasets containing a large number of biomedical covariates. We therefore penalize the negative log-likelihood by the Elastic-Net, which leads to a sparse parameterization of the model. Inference is achieved using an efficient Quasi-Newton Expectation Maximization (QNEM) algorithm, for which we provide convergence properties. We then propose a score by assessing the patients risk of early adverse event. The statistical performance of the method is examined on an extensive Monte Carlo simulation study, and finally illustrated on three genetic datasets with high-dimensional covariates. We show that our approach outperforms the state-of-the-art, namely both the CURE and Cox proportional hazards models for this task, both in terms of C-index and AUC(t). version:2
arxiv-1610-07758 | Image Clustering without Ground Truth | http://arxiv.org/abs/1610.07758 | id:1610.07758 author:Abhisek Dash, Sujoy Chatterjee, Tripti Prasad, Malay Bhattacharyya category:cs.HC cs.CV 68Txx H.1.2; I.2  published:2016-10-25 summary:Cluster analysis has become one of the most exercised research areas over the past few decades in computer science. As a consequence, numerous clustering algorithms have already been developed to find appropriate partitions of a set of objects. Given multiple such clustering solutions, it is a challenging task to obtain an ensemble of these solutions. This becomes more challenging when the ground truth about the number of clusters is unavailable. In this paper, we introduce a crowd-powered model to collect solutions of image clustering from the general crowd and pose it as a clustering ensemble problem with variable number of clusters. The varying number of clusters basically reflects the crowd workers' perspective toward a particular set of objects. We allow a set of crowd workers to independently cluster the images as per their perceptions. We address the problem by finding out centroid of the clusters using an appropriate distance measure and prioritize the likelihood of similarity of the individual cluster sets. The effectiveness of the proposed method is demonstrated by applying it on multiple artificial datasets obtained from crowd. version:1
arxiv-1610-07753 | A Novel Boundary Matching Algorithm for Video Temporal Error Concealment | http://arxiv.org/abs/1610.07753 | id:1610.07753 author:Seyed Mojtaba Marvasti-Zadeh, Hossein Ghanei-Yakhdan, Shohreh Kasaei category:cs.MM cs.CV  published:2016-10-25 summary:With the fast growth of communication networks, the video data transmission from these networks is extremely vulnerable. Error concealment is a technique to estimate the damaged data by employing the correctly received data at the decoder. In this paper, an efficient boundary matching algorithm for estimating damaged motion vectors (MVs) is proposed. The proposed algorithm performs error concealment for each damaged macro block (MB) according to the list of identified priority of each frame. It then uses a classic boundary matching criterion or the proposed boundary matching criterion adaptively to identify matching distortion in each boundary of candidate MB. Finally, the candidate MV with minimum distortion is selected as an MV of damaged MB and the list of priorities is updated. Experimental results show that the proposed algorithm improves both objective and subjective qualities of reconstructed frames without any significant increase in computational cost. The PSNR for test sequences in some frames is increased about 4.7, 4.5, and 4.4 dB compared to the classic boundary matching, directional boundary matching, and directional temporal boundary matching algorithm, respectively. version:1
arxiv-1610-07752 | Big Models for Big Data using Multi objective averaged one dependence estimators | http://arxiv.org/abs/1610.07752 | id:1610.07752 author:Mrutyunjaya Panda category:cs.NE cs.LG  published:2016-10-25 summary:Even though, many researchers tried to explore the various possibilities on multi objective feature selection, still it is yet to be explored with best of its capabilities in data mining applications rather than going for developing new ones. In this paper, multi-objective evolutionary algorithm ENORA is used to select the features in a multi-class classification problem. The fusion of AnDE (averaged n-dependence estimators) with n=1, a variant of naive Bayes with efficient feature selection by ENORA is performed in order to obtain a fast hybrid classifier which can effectively learn from big data. This method aims at solving the problem of finding optimal feature subset from full data which at present still remains to be a difficult problem. The efficacy of the obtained classifier is extensively evaluated with a range of most popular 21 real world dataset, ranging from small to big. The results obtained are encouraging in terms of time, Root mean square error, zero-one loss and classification accuracy. version:1
arxiv-1610-07748 | Balancing, Regression, Difference-In-Differences and Synthetic Control Methods: A Synthesis | http://arxiv.org/abs/1610.07748 | id:1610.07748 author:Nikolay Doudchenko, Guido W. Imbens category:stat.AP stat.ML  published:2016-10-25 summary:In a seminal paper Abadie, Diamond, and Hainmueller [2010] (ADH) develop the synthetic control procedure for estimating the effect of a treatment, in the presence of a single treated unit and a number of control units, with pre-treatment outcomes observed for all units. The method constructs a set of weights such that covariates and pre-treatment outcomes of the treated unit are approximately matched by a weighted average of control units. The weights are restricted to be nonnegative and sum to one, which allows the procedure to obtain the weights even when the number of lagged outcomes is modest relative to the number of control units, a setting that is not uncommon in applications. In the current paper we propose a more general class of synthetic control estimators that allows researchers to relax some of the restrictions in the ADH method. We allow the weights to be negative, do not necessarily restrict the sum of the weights, and allow for a permanent additive difference between the treated unit and the controls, similar to difference-in-difference procedures. The weights directly minimize the distance between the lagged outcomes for the treated and the control units, using regularization methods to deal with a potentially large number of possible control units. version:1
arxiv-1610-07733 | Approximate cross-validation formula for Bayesian linear regression | http://arxiv.org/abs/1610.07733 | id:1610.07733 author:Yoshiyuki Kabashima, Tomoyuki Obuchi, Makoto Uemura category:stat.ML cs.LG  published:2016-10-25 summary:Cross-validation (CV) is a technique for evaluating the ability of statistical models/learning systems based on a given data set. Despite its wide applicability, the rather heavy computational cost can prevent its use as the system size grows. To resolve this difficulty in the case of Bayesian linear regression, we develop a formula for evaluating the leave-one-out CV error approximately without actually performing CV. The usefulness of the developed formula is tested by statistical mechanical analysis for a synthetic model. This is confirmed by application to a real-world supernova data set as well. version:1
arxiv-1610-07728 | Camera Fingerprint: A New Perspective for Identifying User's Identity | http://arxiv.org/abs/1610.07728 | id:1610.07728 author:Xiang Jiang, Shikui Wei, Ruizhen Zhao, Yao Zhao, Xindong Wu category:cs.CV  published:2016-10-25 summary:Identifying user's identity is a key problem in many data mining applications, such as product recommendation, customized content delivery and criminal identification. Given a set of accounts from the same or different social network platforms, user identification attempts to identify all accounts belonging to the same person. A commonly used solution is to build the relationship among different accounts by exploring their collective patterns, e.g., user profile, writing style, similar comments. However, this kind of method doesn't work well in many practical scenarios, since the information posted explicitly by users may be false due to various reasons. In this paper, we re-inspect the user identification problem from a novel perspective, i.e., identifying user's identity by matching his/her cameras. The underlying assumption is that multiple accounts belonging to the same person contain the same or similar camera fingerprint information. The proposed framework, called User Camera Identification (UCI), is based on camera fingerprints, which takes fully into account the problems of multiple cameras and reposting behaviors. version:1
arxiv-1610-07722 | Sparse Hierarchical Tucker Factorization and its Application to Healthcare | http://arxiv.org/abs/1610.07722 | id:1610.07722 author:Ioakeim Perros, Robert Chen, Richard Vuduc, Jimeng Sun category:cs.LG cs.NA  published:2016-10-25 summary:We propose a new tensor factorization method, called the Sparse Hierarchical-Tucker (Sparse H-Tucker), for sparse and high-order data tensors. Sparse H-Tucker is inspired by its namesake, the classical Hierarchical Tucker method, which aims to compute a tree-structured factorization of an input data set that may be readily interpreted by a domain expert. However, Sparse H-Tucker uses a nested sampling technique to overcome a key scalability problem in Hierarchical Tucker, which is the creation of an unwieldy intermediate dense core tensor; the result of our approach is a faster, more space-efficient, and more accurate method. We extensively test our method on a real healthcare dataset, which is collected from 30K patients and results in an 18th order sparse data tensor. Unlike competing methods, Sparse H-Tucker can analyze the full data set on a single multi-threaded machine. It can also do so more accurately and in less time than the state-of-the-art: on a 12th order subset of the input data, Sparse H-Tucker is 18x more accurate and 7.5x faster than a previously state-of-the-art method. Even for analyzing low order tensors (e.g., 4-order), our method requires close to an order of magnitude less time and over two orders of magnitude less memory, as compared to traditional tensor factorization methods such as CP and Tucker. Moreover, we observe that Sparse H-Tucker scales nearly linearly in the number of non-zero tensor elements. The resulting model also provides an interpretable disease hierarchy, which is confirmed by a clinical expert. version:1
arxiv-1610-07717 | Distributed and parallel time series feature extraction for industrial big data applications | http://arxiv.org/abs/1610.07717 | id:1610.07717 author:Maximilian Christ, Andreas W. Kempa-Liehr, Michael Feindt category:cs.LG 62M10 I.2.11  published:2016-10-25 summary:The all-relevant problem of feature selection is the identification of all strongly and weakly relevant attributes. This problem is especially hard to solve for time series classification and regression in industrial applications such as predictive maintenance or production line optimization, for which each label or regression target is associated with several time series and meta-information simultaneously. Here, we are proposing an efficient, scalable feature extraction algorithm, which filters the available features in an early stage of the machine learning pipeline with respect to their significance for the classification or regression task, while controlling the expected percentage of selected but irrelevant features. The proposed algorithm combines established feature extraction methods with a feature importance filter. It has a low computational complexity, allows to start on a problem with only limited domain knowledge available, can be trivially parallelized, is highly scalable and based on well studied non-parametric hypothesis tests. We benchmark our proposed algorithm on all binary classification problems of the UCR time series classification archive as well as time series from a production line optimization project and simulated stochastic processes with underlying qualitative change of dynamics. version:1
arxiv-1610-07710 | EmojiNet: Building a Machine Readable Sense Inventory for Emoji | http://arxiv.org/abs/1610.07710 | id:1610.07710 author:Sanjaya Wijeratne, Lakshika Balasuriya, Amit Sheth, Derek Doran category:cs.CL I.2.7  published:2016-10-25 summary:Emoji are a contemporary and extremely popular way to enhance electronic communication. Without rigid semantics attached to them, emoji symbols take on different meanings based on the context of a message. Thus, like the word sense disambiguation task in natural language processing, machines also need to disambiguate the meaning or sense of an emoji. In a first step toward achieving this goal, this paper presents EmojiNet, the first machine readable sense inventory for emoji. EmojiNet is a resource enabling systems to link emoji with their context-specific meaning. It is automatically constructed by integrating multiple emoji resources with BabelNet, which is the most comprehensive multilingual sense inventory available to date. The paper discusses its construction, evaluates the automatic resource creation process, and presents a use case where EmojiNet disambiguates emoji usage in tweets. EmojiNet is available online for use at http://emojinet.knoesis.org. version:1
arxiv-1610-07708 | Knowledge will Propel Machine Understanding of Content: Extrapolating from Current Examples | http://arxiv.org/abs/1610.07708 | id:1610.07708 author:Amit Sheth, Sujan Perera, Sanjaya Wijeratne category:cs.AI cs.CL I.2  published:2016-10-25 summary:Machine Learning has been a big success story during the AI resurgence. One particular stand out success relates to unsupervised learning from a massive amount of data, albeit much of it relates to one modality/type of data at a time. In spite of early assertions of the unreasonable effectiveness of data, there is increasing recognition of utilizing knowledge whenever it is available or can be created purposefully. In this paper, we focus on discussing the indispensable role of knowledge for deeper understanding of complex text and multimodal data in situations where (i) large amounts of training data (labeled/unlabeled) are not available or labor intensive to create, (ii) the objects (particularly text) to be recognized are complex (i.e., beyond simple entity-person/location/organization names), such as implicit entities and highly subjective content, and (iii) applications need to use complementary or related data in multiple modalities/media. What brings us to the cusp of rapid progress is our ability to (a) create knowledge, varying from comprehensive or cross domain to domain or application specific, and (b) carefully exploit the knowledge to further empower or extend the applications of ML/NLP techniques. Using the early results in several diverse situations - both in data types and applications - we seek to foretell unprecedented progress in our ability for deeper understanding and exploitation of multimodal data. version:1
arxiv-1610-07703 | Scalable Dynamic Topic Modeling with Clustered Latent Dirichlet Allocation (CLDA) | http://arxiv.org/abs/1610.07703 | id:1610.07703 author:Chris Gropp, Alexander Herzog, Ilya Safro, Paul W. Wilson, Amy W. Apon category:cs.IR stat.ML  published:2016-10-25 summary:Topic modeling is an increasingly important component of Big Data analytics, enabling the sense-making of highly dynamic and diverse streams of text data. Traditional methods such as Dynamic Topic Modeling (DTM), while mathematically elegant, do not lend themselves well to direct parallelization because of dependencies from one time step to another. Data decomposition approaches that partition data across time segments and then combine results in a global view of the dynamic change of topics enable execution of topic models on much larger datasets than is possibly without data decomposition. However, these methods are difficult to analyze mathematically and are relatively untested for quality of topics and performance on parallel systems. In this paper, we introduce and empirically analyze Clustered Latent Dirichlet Allocation (CLDA), a method for extracting dynamic latent topics from a collection of documents. CLDA uses a data decomposition strategy to partition data. CLDA takes advantage of parallelism, enabling fast execution for even very large datasets and a large number of topics. A large corpus is split into local segments to extract textual information from different time steps. Latent Dirichlet Allocation (LDA) is applied to infer topics at local segments. The results are merged, and clustering is used to combine topics from different segments into global topics. Results show that the perplexity is comparable and that topics generated by this algorithm are similar to those generated by DTM. In addition, CLDA is two orders of magnitude faster than existing approaches and allows for more freedom of experiment design. In this paper CLDA is applied successfully to seventeen years of NIPS conference papers, seventeen years of computer science journal abstracts, and to forty years of the PubMed corpus. version:1
arxiv-1610-07690 | Operational calculus on programming spaces and generalized tensor networks | http://arxiv.org/abs/1610.07690 | id:1610.07690 author:≈Ωiga Sajovic, Martin Vuk category:cs.FL cs.NE math.FA math.OA  published:2016-10-25 summary:In this paper, we develop the theory of analytic virtual machines, that implement analytic programming spaces and operators acting upon them. A programming space is a subspace of the function space of maps on the virtual memory. We can construct a differential operator on programming spaces as we extend the virtual memory to a tensor product of a virtual space with tensor algebra of its dual. Extended virtual memory serves by itself as an algebra of programs, giving the expansion of the original program as an infinite tensor series at program's input values. We present a theory of operators on programming spaces, that enables analysis of programs and computations on the operator level, which favors general implementation. Theory enables approximation and transformations of programs to a more appropriate function basis'. We also present several examples of how the theory can be used in computer science. We generalize neural networks by constructing general tensor networks, that naturally exist in virtual memory. Transformations of programs to these trainable networks are derived, providing a meaningful way of network initialization. Theory opens new doors in program analysis, while fully retaining algorithmic control flow. We develop a general procedure which takes a program that tests an object for a property and constructs a program that imposes that property upon any object. We use it to generalize state of the art methods for analyzing neural networks to general programs and tensor networks. Expanding upon them, we study dynamics of computation through principles they induce into the system. version:1
arxiv-1610-07686 | Co-Occuring Directions Sketching for Approximate Matrix Multiply | http://arxiv.org/abs/1610.07686 | id:1610.07686 author:Youssef Mroueh, Etienne Marcheret, Vaibhava Goel category:cs.LG  published:2016-10-25 summary:We introduce co-occurring directions sketching, a deterministic algorithm for approximate matrix product (AMM), in the streaming model. We show that co-occuring directions achieves a better error bound for AMM than other randomized and deterministic approaches for AMM. Co-occurring directions gives a $1 + \epsilon$ -approximation of the optimal low rank approximation of a matrix product. Empirically our algorithm outperforms competing methods for AMM, for a small sketch size. We validate empirically our theoretical findings and algorithms version:1
arxiv-1610-07677 | A Bayesian Ensemble for Unsupervised Anomaly Detection | http://arxiv.org/abs/1610.07677 | id:1610.07677 author:Edward Yu, Parth Parekh category:stat.ML cs.LG  published:2016-10-24 summary:Methods for unsupervised anomaly detection suffer from the fact that the data is unlabeled, making it difficult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classification and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classifier combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data. version:1
arxiv-1610-06145 | A global optimization algorithm for sparse mixed membership matrix factorization | http://arxiv.org/abs/1610.06145 | id:1610.06145 author:Fan Zhang, Chuangqi Wang, Andrew Trapp, Patrick Flaherty category:stat.ME math.OC stat.ML  published:2016-10-19 summary:Mixed membership factorization is a popular approach for analyzing data sets that have within-sample heterogeneity. In recent years, several algorithms have been developed for mixed membership matrix factorization, but they only guarantee estimates from a local optimum. Here, we derive a global optimization (GOP) algorithm that provides a guaranteed $\epsilon$-global optimum for a sparse mixed membership matrix factorization problem. We test the algorithm on simulated data and find the algorithm always bounds the global optimum across random initializations and explores multiple modes efficiently. version:2
arxiv-1610-05812 | Small-footprint Highway Deep Neural Networks for Speech Recognition | http://arxiv.org/abs/1610.05812 | id:1610.05812 author:Liang Lu, Steve Renals category:cs.CL cs.LG  published:2016-10-18 summary:State-of-the-art speech recognition systems typically employ neural network acoustic models. However, compared to Gaussian mixture models, deep neural network (DNN) based acoustic models often have many more model parameters, making it challenging for them to be deployed on resource-constrained platforms, such as mobile devices. In this paper, we study the application of the recently proposed highway deep neural network (HDNN) for training small-footprint acoustic models. HDNNs are a depth-gated feedforward neural network, which include two types of gate functions to facilitate the information flow through different layers. Our study demonstrates that HDNNs are more compact than regular DNNs for acoustic modeling, i.e., they can achieve comparable recognition accuracy with many fewer model parameters. Furthermore, HDNNs are more controllable than DNNs: the gate functions of an HDNN largely control the behavior of the whole network using a very small number of model parameters. Finally, we show that HDNNs are more adaptable than DNNs. For example, simply updating the gate functions using the adaptation data can result in considerable gains in accuracy. We demonstrate these aspects by experiments using the publicly available AMI corpus, which has around 80 hours of training data. version:2
arxiv-1610-07651 | UTD-CRSS Systems for 2016 NIST Speaker Recognition Evaluation | http://arxiv.org/abs/1610.07651 | id:1610.07651 author:Chunlei Zhang, Fahimeh Bahmaninezhad, Shivesh Ranjan, Chengzhu Yu, Navid Shokouhi, John H. L. Hansen category:cs.CL  published:2016-10-24 summary:This document briefly describes the systems submitted by the Center for Robust Speech Systems (CRSS) from The University of Texas at Dallas (UTD) to the 2016 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE). We developed several UBM and DNN i-Vector based speaker recognition systems with different data sets and feature representations. Given that the emphasis of the NIST SRE 2016 is on language mismatch between training and enrollment/test data, so-called domain mismatch, in our system development we focused on: (1) using unlabeled in-domain data for centralizing data to alleviate the domain mismatch problem, (2) finding the best data set for training LDA/PLDA, (3) using newly proposed dimension reduction technique incorporating unlabeled in-domain data before PLDA training, (4) unsupervised speaker clustering of unlabeled data and using them alone or with previous SREs for PLDA training, (5) score calibration using only unlabeled data and combination of unlabeled and development (Dev) data as separate experiments. version:1
arxiv-1610-07650 | A Theoretical Analysis of Noisy Sparse Subspace Clustering on Dimensionality-Reduced Data | http://arxiv.org/abs/1610.07650 | id:1610.07650 author:Yining Wang, Yu-Xiang Wang, Aarti Singh category:stat.ML cs.LG  published:2016-10-24 summary:Subspace clustering is the problem of partitioning unlabeled data points into a number of clusters so that data points within one cluster lie approximately on a low-dimensional linear subspace. In many practical scenarios, the dimensionality of data points to be clustered are compressed due to constraints of measurement, computation or privacy. In this paper, we study the theoretical properties of a popular subspace clustering algorithm named sparse subspace clustering (SSC) and establish formal success conditions of SSC on dimensionality-reduced data. Our analysis applies to the most general fully deterministic model where both underlying subspaces and data points within each subspace are deterministically positioned, and also a wide range of dimensionality reduction techniques (e.g., Gaussian random projection, uniform subsampling, sketching) that fall into a subspace embedding framework (Meng & Mahoney, 2013; Avron et al., 2014). Finally, we apply our analysis to a differentially private SSC algorithm and established both privacy and utility guarantees of the proposed method. version:1
arxiv-1610-07647 | Learning to Reason With Adaptive Computation | http://arxiv.org/abs/1610.07647 | id:1610.07647 author:Mark Neumann, Pontus Stenetorp, Sebastian Riedel category:cs.CL cs.NE  published:2016-10-24 summary:Multi-hop inference is necessary for machine learning systems to successfully solve tasks such as Recognising Textual Entailment and Machine Reading. In this work, we demonstrate the effectiveness of adaptive computation for learning the number of inference steps required for examples of different complexity and that learning the correct number of inference steps is difficult. We introduce the first model involving Adaptive Computation Time which provides a small performance benefit on top of a similar model without an adaptive component as well as enabling considerable insight into the reasoning process of the model. version:1
arxiv-1610-07629 | A Learned Representation For Artistic Style | http://arxiv.org/abs/1610.07629 | id:1610.07629 author:Vincent Dumoulin, Jonathon Shlens, Manjunath Kudlur category:cs.CV cs.LG  published:2016-10-24 summary:The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style. version:1
arxiv-1610-07584 | Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling | http://arxiv.org/abs/1610.07584 | id:1610.07584 author:Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Freeman, Joshua B. Tenenbaum category:cs.CV cs.LG  published:2016-10-24 summary:We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods. version:1
arxiv-1610-07570 | A data augmentation methodology for training machine/deep learning gait recognition algorithms | http://arxiv.org/abs/1610.07570 | id:1610.07570 author:Christoforos C. Charalambous, Anil A. Bharath category:cs.CV  published:2016-10-24 summary:There are several confounding factors that can reduce the accuracy of gait recognition systems. These factors can reduce the distinctiveness, or alter the features used to characterise gait, they include variations in clothing, lighting, pose and environment, such as the walking surface. Full invariance to all confounding factors is challenging in the absence of high-quality labelled training data. We introduce a simulation-based methodology and a subject-specific dataset which can be used for generating synthetic video frames and sequences for data augmentation. With this methodology, we generated a multi-modal dataset. In addition, we supply simulation files that provide the ability to simultaneously sample from several confounding variables. The basis of the data is real motion capture data of subjects walking and running on a treadmill at different speeds. Results from gait recognition experiments suggest that information about the identity of subjects is retained within synthetically generated examples. The dataset and methodology allow studies into fully-invariant identity recognition spanning a far greater number of observation conditions than would otherwise be possible. version:1
arxiv-1610-07569 | Geometry of Polysemy | http://arxiv.org/abs/1610.07569 | id:1610.07569 author:Jiaqi Mu, Suma Bhat, Pramod Viswanath category:cs.CL cs.LG stat.ML  published:2016-10-24 summary:Vector representations of words have heralded a transformational approach to classical problems in NLP; the most popular example is word2vec. However, a single vector does not suffice to model the polysemous nature of many (frequent) words, i.e., words with multiple meanings. In this paper, we propose a three-fold approach for unsupervised polysemy modeling: (a) context representations, (b) sense induction and disambiguation and (c) lexeme (as a word and sense pair) representations. A key feature of our work is the finding that a sentence containing a target word is well represented by a low rank subspace, instead of a point in a vector space. We then show that the subspaces associated with a particular sense of the target word tend to intersect over a line (one-dimensional subspace), which we use to disambiguate senses using a clustering algorithm that harnesses the Grassmannian geometry of the representations. The disambiguation algorithm, which we call $K$-Grassmeans, leads to a procedure to label the different senses of the target word in the corpus -- yielding lexeme vector representations, all in an unsupervised manner starting from a large (Wikipedia) corpus in English. Apart from several prototypical target (word,sense) examples and a host of empirical studies to intuit and justify the various geometric representations, we validate our algorithms on standard sense induction and disambiguation datasets and present new state-of-the-art results. version:1
arxiv-1610-07563 | On Multiplicative Multitask Feature Learning | http://arxiv.org/abs/1610.07563 | id:1610.07563 author:Xin Wang, Jinbo Bi, Shipeng Yu, Jiangwen Sun category:cs.LG  published:2016-10-24 summary:We investigate a general framework of multiplicative multitask feature learning which decomposes each task's model parameters into a multiplication of two components. One of the components is used across all tasks and the other component is task-specific. Several previous methods have been proposed as special cases of our framework. We study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components. We prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters, but with a more general form of regularizers. Further, an analytical formula is derived for the across-task component as related to the task-specific component for all these regularizers, leading to a better understanding of the shrinkage effect. Study of this framework motivates new multitask learning algorithms. We propose two new learning formulations by varying the parameters in the proposed framework. Empirical studies have revealed the relative advantages of the two new formulations by comparing with the state of the art, which provides instructive insights into the feature learning problem with multiple tasks. version:1
arxiv-1610-07560 | Automated OCT Segmentation for Images with DME | http://arxiv.org/abs/1610.07560 | id:1610.07560 author:Sohini Roychowdhury, Dara D. Koozekanani, Michael Reinsbach, Keshab K. Parhi category:cs.CV  published:2016-10-24 summary:This paper presents a novel automated system that segments six sub-retinal layers from optical coherence tomography (OCT) image stacks of healthy patients and patients with diabetic macular edema (DME). First, each image in the OCT stack is denoised using a Wiener deconvolution algorithm that estimates the additive speckle noise variance using a novel Fourier-domain based structural error. This denoising method enhances the image SNR by an average of 12dB. Next, the denoised images are subjected to an iterative multi-resolution high-pass filtering algorithm that detects seven sub-retinal surfaces in six iterative steps. The thicknesses of each sub-retinal layer for all scans from a particular OCT stack are then compared to the manually marked groundtruth. The proposed system uses adaptive thresholds for denoising and segmenting each image and hence it is robust to disruptions in the retinal micro-structure due to DME. The proposed denoising and segmentation system has an average error of 1.2-5.8 $\mu m$ and 3.5-26$\mu m$ for segmenting sub-retinal surfaces in normal and abnormal images with DME, respectively. For estimating the sub-retinal layer thicknesses, the proposed system has an average error of 0.2-2.5 $\mu m$ and 1.8-18 $\mu m$ in normal and abnormal images, respectively. Additionally, the average inner sub-retinal layer thickness in abnormal images is estimated as 275$\mu m (r=0.92)$ with an average error of 9.3 $\mu m$, while the average thickness of the outer layers in abnormal images is estimated as 57.4$\mu m (r=0.74)$ with an average error of 3.5 $\mu m$. The proposed system can be useful for tracking the disease progression for DME over a period of time. version:1
arxiv-1610-07524 | Fair prediction with disparate impact: A study of bias in recidivism prediction instruments | http://arxiv.org/abs/1610.07524 | id:1610.07524 author:Alexandra Chouldechova category:stat.AP cs.CY stat.ML  published:2016-10-24 summary:Recidivism prediction instruments provide decision makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. While such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This paper discusses a fairness criterion originating in the field of educational and psychological testing that has recently been applied to assess the fairness of recidivism prediction instruments. We demonstrate how adherence to the criterion may lead to considerable disparate impact when recidivism prevalence differs across groups. version:1
arxiv-1610-07520 | Nonlinear Adaptive Algorithms on Rank-One Tensor Models | http://arxiv.org/abs/1610.07520 | id:1610.07520 author:Felipe C. Pinheiro, Cassio G. Lopes category:cs.SY cs.LG  published:2016-10-24 summary:This work proposes a low complexity nonlinearity model and develops adaptive algorithms over it. The model is based on the decomposable---or rank-one, in tensor language---Volterra kernels. It may also be described as a product of FIR filters, which explains its low-complexity. The rank-one model is also interesting because it comes from a well-posed problem in approximation theory. The paper uses such model in an estimation theory context to develop an exact gradient-type algorithm, from which adaptive algorithms such as the least mean squares (LMS) filter and its data-reuse version---the TRUE-LMS---are derived. Stability and convergence issues are addressed. The algorithms are then tested in simulations, which show its good performance when compared to other nonlinear processing algorithms in the literature. version:1
arxiv-1610-07519 | A Variational Bayesian Approach for Restoring Data Corrupted with Non-Gaussian Noise | http://arxiv.org/abs/1610.07519 | id:1610.07519 author:Yosra Marnissi, Yuling Zheng, Emilie Chouzenoux, Jean-Christophe Pesquet category:math.OC cs.LG stat.ML  published:2016-10-24 summary:In this paper, a methodology is investigated for signal recovery in the presence of non-Gaussian noise. In contrast with regularized minimization approaches often adopted in the literature, in our algorithm the regularization parameter is reliably estimated from the observations. As the posterior density of the unknown parameters is analytically intractable, the estimation problem is derived in a variational Bayesian framework where the goal is to provide a good approximation to the posterior distribution in order to compute posterior mean estimates. Moreover, a majorization technique is employed to circumvent the difficulties raised by the intricate forms of the non-Gaussian likelihood and of the prior density. We demonstrate the potential of the proposed approach through comparisons with state-of-the-art techniques that are specifically tailored to signal recovery in the presence of mixed Poisson-Gaussian noise. Results show that the proposed approach is efficient and achieves performance comparable with other methods where the regularization parameter is manually tuned from an available ground truth. version:1
arxiv-1610-07487 | Parallelizing Spectral Algorithms for Kernel Learning | http://arxiv.org/abs/1610.07487 | id:1610.07487 author:Gilles Blanchard, Nicole M√ºcke category:math.ST stat.ML stat.TH  published:2016-10-24 summary:We consider a distributed learning approach in supervised learning for a large class of spectral regularization methods in an RKHS framework. The data set of size n is partitioned into $m=O(n^\alpha)$, $\alpha \leq \frac{1}{2}$, disjoint subsets. On each subset, some spectral regularization method (belonging to a large class, including in particular Kernel Ridge Regression, $L^2$-boosting and spectral cut-off) is applied. The regression function $f$ is then estimated via simple averaging, leading to a substantial reduction in computation time. We show that minimax optimal rates of convergence are preserved if m grows sufficiently slowly (corresponding to an upper bound for $\alpha$) as $n \to \infty$, depending on the smoothness assumptions on $f$ and the intrinsic dimensionality. In spirit, our approach is classical. version:1
arxiv-1610-04631 | A Harmonic Mean Linear Discriminant Analysis for Robust Image Classification | http://arxiv.org/abs/1610.04631 | id:1610.04631 author:Shuai Zheng, Feiping Nie, Chris Ding, Heng Huang category:cs.CV cs.AI  published:2016-10-14 summary:Linear Discriminant Analysis (LDA) is a widely-used supervised dimensionality reduction method in computer vision and pattern recognition. In null space based LDA (NLDA), a well-known LDA extension, between-class distance is maximized in the null space of the within-class scatter matrix. However, there are some limitations in NLDA. Firstly, for many data sets, null space of within-class scatter matrix does not exist, thus NLDA is not applicable to those datasets. Secondly, NLDA uses arithmetic mean of between-class distances and gives equal consideration to all between-class distances, which makes larger between-class distances can dominate the result and thus limits the performance of NLDA. In this paper, we propose a harmonic mean based Linear Discriminant Analysis, Multi-Class Discriminant Analysis (MCDA), for image classification, which minimizes the reciprocal of weighted harmonic mean of pairwise between-class distance. More importantly, MCDA gives higher priority to maximize small between-class distances. MCDA can be extended to multi-label dimension reduction. Results on 7 single-label data sets and 4 multi-label data sets show that MCDA has consistently better performance than 10 other single-label approaches and 4 other multi-label approaches in terms of classification accuracy, macro and micro average F1 score. version:2
arxiv-1610-07475 | Feature Sensitive Label Fusion with Random Walker for Atlas-based Image Segmentation | http://arxiv.org/abs/1610.07475 | id:1610.07475 author:Siqi Bao, Albert C. S. Chung category:cs.CV  published:2016-10-24 summary:In this paper, a novel label fusion method is proposed for brain magnetic resonance image segmentation. This label fusion method is formulated on a graph, which embraces both label priors from atlases and anatomical priors from target image. To represent a pixel in a comprehensive way, three kinds of feature vectors are generated, including intensity, gradient and structural signature. To select candidate atlas nodes for fusion, rather than exact searching, randomized k-d tree with spatial constraint is introduced as an efficient approximation for high-dimensional feature matching. Feature Sensitive Label Prior (FSLP), which takes both the consistency and variety of different features into consideration, is proposed to gather atlas priors. As FSLP is a non-convex problem, one heuristic approach is further designed to solve it efficiently. Moreover, based on the anatomical knowledge, parts of the target pixels are also employed as graph seeds to assist the label fusion process and an iterative strategy is utilized to gradually update the label map. The comprehensive experiments carried out on two publicly available databases give results to demonstrate that the proposed method can obtain better segmentation quality. version:1
arxiv-1610-07448 | A Framework for Parallel and Distributed Training of Neural Networks | http://arxiv.org/abs/1610.07448 | id:1610.07448 author:Simone Scardapane, Paolo Di Lorenzo category:stat.ML cs.LG  published:2016-10-24 summary:The aim of this paper is to develop a general framework for training neural networks (NNs) in a distributed environment, where training data is partitioned over a set of agents that communicate with each other through a sparse, possibly time-varying, connectivity pattern. In such distributed scenario, the training problem can be formulated as the (regularized) optimization of a non-convex social cost function, given by the sum of local (non-convex) costs, where each agent contributes with a single error term defined with respect to its local dataset. To devise a flexible and efficient solution, we customize a recently proposed framework for non-convex optimization over networks, which hinges on a (primal) convexification-decomposition technique to handle non-convexity, and a dynamic consensus procedure to diffuse information among the agents. Several typical choices for the training criterion (e.g., squared loss, cross entropy, etc.) and regularization (e.g., $\ell_2$ norm, sparsity inducing penalties, etc.) are included in the framework and explored along the paper. Convergence to a stationary solution of the social non-convex problem is guaranteed under mild assumptions. Additionally, we show a principled way allowing each agent to exploit a multi-core architecture (e.g., a local cloud) in order to parallelize its local optimization step, resulting in strategies that are both distributed (across the agents) and parallel (inside each agent) in nature. A comprehensive set of experimental results validate the proposed approach. version:1
arxiv-1610-07432 | Virtual Embodiment: A Scalable Long-Term Strategy for Artificial Intelligence Research | http://arxiv.org/abs/1610.07432 | id:1610.07432 author:Douwe Kiela, Luana Bulat, Anita L. Vero, Stephen Clark category:cs.AI cs.CL cs.CV 68T01 I.2.6  published:2016-10-24 summary:Meaning has been called the "holy grail" of a variety of scientific disciplines, ranging from linguistics to philosophy, psychology and the neurosciences. The field of Artifical Intelligence (AI) is very much a part of that list: the development of sophisticated natural language semantics is a sine qua non for achieving a level of intelligence comparable to humans. Embodiment theories in cognitive science hold that human semantic representation depends on sensori-motor experience; the abundant evidence that human meaning representation is grounded in the perception of physical reality leads to the conclusion that meaning must depend on a fusion of multiple (perceptual) modalities. Despite this, AI research in general, and its subdisciplines such as computational linguistics and computer vision in particular, have focused primarily on tasks that involve a single modality. Here, we propose virtual embodiment as an alternative, long-term strategy for AI research that is multi-modal in nature and that allows for the kind of scalability required to develop the field coherently and incrementally, in an ethically responsible fashion. version:1
arxiv-1610-07420 | Reordering rules for English-Hindi SMT | http://arxiv.org/abs/1610.07420 | id:1610.07420 author:Raj Nath Patel, Rohit Gupta, Prakash B. Pimpale, Sasikumar M category:cs.CL  published:2016-10-24 summary:Reordering is a preprocessing stage for Statistical Machine Translation (SMT) system where the words of the source sentence are reordered as per the syntax of the target language. We are proposing a rich set of rules for better reordering. The idea is to facilitate the training process by better alignments and parallel phrase extraction for a phrase-based SMT system. Reordering also helps the decoding process and hence improving the machine translation quality. We have observed significant improvements in the translation quality by using our approach over the baseline SMT. We have used BLEU, NIST, multi-reference word error rate, multi-reference position independent error rate for judging the improvements. We have exploited open source SMT toolkit MOSES to develop the system. version:1
arxiv-1610-07419 | Using Machine Learning to Detect Noisy Neighbors in 5G Networks | http://arxiv.org/abs/1610.07419 | id:1610.07419 author:Udi Margolin, Alberto Mozo, Bruno Ordozgoiti, Danny Raz, Elisha Rosensweig, Itai Segall category:cs.NI cs.LG  published:2016-10-24 summary:5G networks are expected to be more dynamic and chaotic in their structure than current networks. With the advent of Network Function Virtualization (NFV), Network Functions (NF) will no longer be tightly coupled with the hardware they are running on, which poses new challenges in network management. Noisy neighbor is a term commonly used to describe situations in NFV infrastructure where an application experiences degradation in performance due to the fact that some of the resources it needs are occupied by other applications in the same cloud node. These situations cannot be easily identified using straightforward approaches, which calls for the use of sophisticated methods for NFV infrastructure management. In this paper we demonstrate how Machine Learning (ML) techniques can be used to identify such events. Through experiments using data collected at real NFV infrastructure, we show that standard models for automated classification can detect the noisy neighbor phenomenon with an accuracy of more than 90% in a simple scenario. version:1
arxiv-1610-07418 | Statistical Machine Translation for Indian Languages: Mission Hindi | http://arxiv.org/abs/1610.07418 | id:1610.07418 author:Raj Nath Patel, Prakash B. Pimpale, Sasikumar M category:cs.CL  published:2016-10-24 summary:This paper discusses Centre for Development of Advanced Computing Mumbai's (CDACM) submission to the NLP Tools Contest on Statistical Machine Translation in Indian Languages (ILSMT) 2014 (collocated with ICON 2014). The objective of the contest was to explore the effectiveness of Statistical Machine Translation (SMT) for Indian language to Indian language and English-Hindi machine translation. In this paper, we have proposed that suffix separation and word splitting for SMT from agglutinative languages to Hindi significantly improves over the baseline (BL). We have also shown that the factored model with reordering outperforms the phrase-based SMT for English-Hindi (\enhi). We report our work on all five pairs of languages, namely Bengali-Hindi (\bnhi), Marathi-Hindi (\mrhi), Tamil-Hindi (\tahi), Telugu-Hindi (\tehi), and \enhi for Health, Tourism, and General domains. version:1
arxiv-1610-08015 | Savu: A Python-based, MPI Framework for Simultaneous Processing of Multiple, N-dimensional, Large Tomography Datasets | http://arxiv.org/abs/1610.08015 | id:1610.08015 author:Nicola Wadeson, Mark Basham category:cs.DC cs.CV cs.DB  published:2016-10-24 summary:Diamond Light Source (DLS), the UK synchrotron facility, attracts scientists from across the world to perform ground-breaking x-ray experiments. With over 3000 scientific users per year, vast amounts of data are collected across the experimental beamlines, with the highest volume of data collected during tomographic imaging experiments. A growing interest in tomography as an imaging technique, has led to an expansion in the range of experiments performed, in addition to a growth in the size of the data per experiment. Savu is a portable, flexible, scientific processing pipeline capable of processing multiple, n-dimensional datasets in serial on a PC, or in parallel across a cluster. Developed at DLS, and successfully deployed across the beamlines, it uses a modular plugin format to enable experiment-specific processing and utilises parallel HDF5 to remove RAM restrictions. The Savu design, described throughout this paper, focuses on easy integration of existing and new functionality, flexibility and ease of use for users and developers alike. version:1
arxiv-1610-07381 | Theoretical Analysis of Active Contours on Graphs | http://arxiv.org/abs/1610.07381 | id:1610.07381 author:Christos Sakaridis, Kimon Drakopoulos, Petros Maragos category:cs.CV I.4.6  published:2016-10-24 summary:Active contour models based on partial differential equations have proved successful in image segmentation, yet the study of their geometric formulation on arbitrary geometric graphs is still at an early stage. In this paper, we introduce geometric approximations of gradient and curvature, which are used in the geodesic active contour model. We prove convergence in probability of our gradient approximation to the true gradient value and derive an asymptotic upper bound for the error of this approximation for the class of random geometric graphs. Two different approaches for the approximation of curvature are presented and both are also proved to converge in probability in the case of random geometric graphs. We propose neighborhood-based filtering on graphs to improve the accuracy of the aforementioned approximations and define two variants of Gaussian smoothing on graphs which include normalization in order to adapt to graph non-uniformities. The performance of our active contour framework on graphs is demonstrated in the segmentation of regular images and geographical data defined on arbitrary graphs. version:1
arxiv-1610-07379 | Truncated Variance Reduction: A Unified Approach to Bayesian Optimization and Level-Set Estimation | http://arxiv.org/abs/1610.07379 | id:1610.07379 author:Ilija Bogunovic, Jonathan Scarlett, Andreas Krause, Volkan Cevher category:stat.ML cs.IT cs.LG math.IT  published:2016-10-24 summary:We present a new algorithm, truncated variance reduction (TruVaR), that treats Bayesian optimization (BO) and level-set estimation (LSE) with Gaussian processes in a unified fashion. The algorithm greedily shrinks a sum of truncated variances within a set of potential maximizers (BO) or unclassified points (LSE), which is updated based on confidence bounds. TruVaR is effective in several important settings that are typically non-trivial to incorporate into myopic algorithms, including pointwise costs and heteroscedastic noise. We provide a general theoretical guarantee for TruVaR covering these aspects, and use it to recover and strengthen existing results on BO and LSE. Moreover, we provide a new result for a setting where one can select from a number of noise levels having associated costs. We demonstrate the effectiveness of the algorithm on both synthetic and real-world data sets. version:1
arxiv-1610-07365 | Introduction: Cognitive Issues in Natural Language Processing | http://arxiv.org/abs/1610.07365 | id:1610.07365 author:Thierry Poibeau, Shravan Vasishth category:cs.CL cs.AI cs.HC  published:2016-10-24 summary:This special issue is dedicated to get a better picture of the relationships between computational linguistics and cognitive science. It specifically raises two questions: "what is the potential contribution of computational language modeling to cognitive science?" and conversely: "what is the influence of cognitive science in contemporary computational linguistics?" version:1
arxiv-1610-07363 | Learning Reporting Dynamics during Breaking News for Rumour Detection in Social Media | http://arxiv.org/abs/1610.07363 | id:1610.07363 author:Arkaitz Zubiaga, Maria Liakata, Rob Procter category:cs.CL cs.IR cs.SI  published:2016-10-24 summary:Breaking news leads to situations of fast-paced reporting in social media, producing all kinds of updates related to news stories, albeit with the caveat that some of those early updates tend to be rumours, i.e., information with an unverified status at the time of posting. Flagging information that is unverified can be helpful to avoid the spread of information that may turn out to be false. Detection of rumours can also feed a rumour tracking system that ultimately determines their veracity. In this paper we introduce a novel approach to rumour detection that learns from the sequential dynamics of reporting during breaking news in social media to detect rumours in new stories. Using Twitter datasets collected during five breaking news stories, we experiment with Conditional Random Fields as a sequential classifier that leverages context learnt during an event for rumour detection, which we compare with the state-of-the-art rumour detection system as well as other baselines. In contrast to existing work, our classifier does not need to observe tweets querying a piece of information to deem it a rumour, but instead we detect rumours from the tweet alone by exploiting context learnt during the event. Our classifier achieves competitive performance, beating the state-of-the-art classifier that relies on querying tweets with improved precision and recall, as well as outperforming our best baseline with nearly 40% improvement in terms of F1 score. The scale and diversity of our experiments reinforces the generalisability of our classifier. version:1
arxiv-1610-05654 | The infochemical core | http://arxiv.org/abs/1610.05654 | id:1610.05654 author:Antoni Hern√°ndez-Fern√°ndez, Ramon Ferrer-i-Cancho category:q-bio.NC cs.CL  published:2016-10-18 summary:Vocalizations and less often gestures have been the object of linguistic research over decades. However, the development of a general theory of communication with human language as a particular case requires a clear understanding of the organization of communication through other means. Infochemicals are chemical compounds that carry information and are employed by small organisms that cannot emit acoustic signals of optimal frequency to achieve successful communication. Here the distribution of infochemicals across species is investigated when they are ranked by their degree or the number of species with which it is associated (because they produce or they are sensitive to it). The quality of the fit of different functions to the dependency between degree and rank is evaluated with a penalty for the number of parameters of the function. Surprisingly, a double Zipf (a Zipf distribution with two regimes with a different exponent each) is the model yielding the best fit although it is the function with the largest number of parameters. This suggests that the world wide repertoire of infochemicals contains a chemical nucleus shared by many species and reminiscent of the core vocabularies found for human language in dictionaries or large corpora. version:2
arxiv-1610-07355 | STDP allows close-to-optimal spatiotemporal spike pattern detection by single coincidence detector neurons | http://arxiv.org/abs/1610.07355 | id:1610.07355 author:Timoth√©e Masquelier category:cs.NE q-bio.NC  published:2016-10-24 summary:By recording multiple cells simultaneously, electrophysiologists have found evidence for repeating spatiotemporal spike patterns. In sensory systems in particular, repeating a sensory sequence typically elicits a reproducible spike pattern, which carries information about the sensory sequence. How this information is readout by downstream neurons is unclear. In this theoretical paper, we investigate to what extent a single cell could detect a given spike pattern and what are the optimal parameters to do so, in particular the membrane time constant $\tau$. Using a leaky integrate-and-fire (LIF) neuron with instantaneous synapses and homogeneous Poisson inputs, we computed this optimum analytically. Our results indicate that a relatively small $\tau$ (at most a few tens of ms) is usually optimal, even when the pattern is longer. This is somewhat surprising as the resulting detector ignores most of the pattern, due to its fast memory decay. Next, we wondered if spike-timing-dependent plasticity (STDP) could enable a neuron to reach the theoretical optimum. We simulated a LIF neuron equipped with additive STDP, and repeatedly exposed to a given input spike pattern. As in previous studies, the LIF progressively became selective to the repeating pattern with no supervision, even when the pattern was embedded in Poisson activity. Here we show that, using certain STDP parameters, the resulting pattern detector can be optimal. Taken together, these results may explain how humans can learn repeating visual or auditory sequences. Long sequences could be recognized thanks to coincidence detectors working at a much shorter timescale. This is consistent with the fact that recognition is still possible if a sound sequence is compressed of played backward, or scrambled using 10ms bins. Coincidence detection is a simple yet powerful mechanism, which could be the main function of neurons in the brain. version:1
arxiv-1610-07336 | MultiCol-SLAM - A Modular Real-Time Multi-Camera SLAM System | http://arxiv.org/abs/1610.07336 | id:1610.07336 author:Steffen Urban, Stefan Hinz category:cs.CV  published:2016-10-24 summary:The basis for most vision based applications like robotics, self-driving cars and potentially augmented and virtual reality is a robust, continuous estimation of the position and orientation of a camera system w.r.t the observed environment (scene). In recent years many vision based systems that perform simultaneous localization and mapping (SLAM) have been presented and released as open source. In this paper, we extend and improve upon a state-of-the-art SLAM to make it applicable to arbitrary, rigidly coupled multi-camera systems (MCS) using the MultiCol model. In addition, we include a performance evaluation on accurate ground truth and compare the robustness of the proposed method to a single camera version of the SLAM system. An open source implementation of the proposed multi-fisheye camera SLAM system can be found on-line https://github.com/urbste/MultiCol-SLAM. version:1
arxiv-1610-07324 | A coarse-to-fine algorithm for registration in 3D street-view cross-source point clouds | http://arxiv.org/abs/1610.07324 | id:1610.07324 author:Xiaoshui Huang, Jian Zhang, Qiang Wu, Lixin Fan, Chun Yuan category:cs.CV  published:2016-10-24 summary:With the development of numerous 3D sensing technologies, object registration on cross-source point cloud has aroused researchers' interests. When the point clouds are captured from different kinds of sensors, there are large and different kinds of variations. In this study, we address an even more challenging case in which the differently-source point clouds are acquired from a real street view. One is produced directly by the LiDAR system and the other is generated by using VSFM software on image sequence captured from RGB cameras. When it confronts to large scale point clouds, previous methods mostly focus on point-to-point level registration, and the methods have many limitations.The reason is that the least mean error strategy shows poor ability in registering large variable cross-source point clouds. In this paper, different from previous ICP-based methods, and from a statistic view, we propose a effective coarse-to-fine algorithm to detect and register a small scale SFM point cloud in a large scale Lidar point cloud. Seen from the experimental results, the model can successfully run on LiDAR and SFM point clouds, hence it can make a contribution to many applications, such as robotics and smart city development. version:1
arxiv-1610-05392 | AutoGP: Exploring the Capabilities and Limitations of Gaussian Process Models | http://arxiv.org/abs/1610.05392 | id:1610.05392 author:Karl Krauth, Edwin V. Bonilla, Kurt Cutajar, Maurizio Filippone category:stat.ML  published:2016-10-18 summary:We investigate the capabilities and limitations of Gaussian process (GP) models by jointly exploring three complementary directions: (i) scalable and statistically efficient inference; (ii) flexible kernels; and (iii) objective functions for hyperparameter learning alternative to the marginal likelihood. Our approach outperforms all previously reported GP methods on the standard MNIST dataset; achieves state-of-the-art performance in a task particularly hard for kernel-based methods using the RECTANGLES-IMAGE dataset; and breaks the 1% error-rate barrier in GP models using the MNIST8M dataset, showing along the way the scalability of our method at unprecedented scale for GP models (8 million observations) in classification problems. Overall, our approach represents a significant breakthrough in kernel methods and GP models, bridging the gap between deep learning approaches and kernel machines. version:2
arxiv-1610-07273 | Encoding Temporal Markov Dynamics in Graph for Time Series Visualization | http://arxiv.org/abs/1610.07273 | id:1610.07273 author:Lu Liu, Zhiguang Wang category:cs.LG cs.HC  published:2016-10-24 summary:Time series is attracting more attention across statistics, machine learning and pattern recognition as it appears widely in both industry and academia, but few advances has been achieved in effective time series visualization due to its temporal dimensionality and complex dynamics. Inspired by recent effort on using network metrics to characterize time series for classification, we present an approach to visualize time series as complex networks based on first order Markov process and temporal ordering. Different to classical bar charts, line plots and other statistics based graph, our approach delivers more intuitive visualization that better preserves both the temporal dependency and frequency structures. It provides a natural inverse operation to map the graph back to time series, making it possible to use graph statistics to characterize time series for better visual exploration and statistical analysis. Our experimental results suggest the effectiveness on various tasks such as system identification, classification and anomaly detection on both synthetic and the real time series data. version:1
arxiv-1610-07272 | Bridging Neural Machine Translation and Bilingual Dictionaries | http://arxiv.org/abs/1610.07272 | id:1610.07272 author:Jiajun Zhang, Chengqing Zong category:cs.CL  published:2016-10-24 summary:Neural Machine Translation (NMT) has become the new state-of-the-art in several language pairs. However, it remains a challenging problem how to integrate NMT with a bilingual dictionary which mainly contains words rarely or never seen in the bilingual training data. In this paper, we propose two methods to bridge NMT and the bilingual dictionaries. The core idea behind is to design novel models that transform the bilingual dictionaries into adequate sentence pairs, so that NMT can distil latent bilingual mappings from the ample and repetitive phenomena. One method leverages a mixed word/character model and the other attempts at synthesizing parallel sentences guaranteeing massive occurrence of the translation lexicon. Extensive experiments demonstrate that the proposed methods can remarkably improve the translation quality, and most of the rare words in the test sentences can obtain correct translations if they are covered by the dictionary. version:1
arxiv-1610-07262 | Bayesian Nonparametric Modeling of Heterogeneous Groups of Censored Data | http://arxiv.org/abs/1610.07262 | id:1610.07262 author:Alexandre Pich√©, Russell Steele, Ian Shrier, Stephanie Long category:stat.ML  published:2016-10-24 summary:Applied statisticians often encounter large samples of time-to-event data arising from a number of different groups with only a small number of observations per group. Bayesian nonparametric modelling approaches can be used to model such datasets given their ability to flexibly share information across groups. In this paper, we will compare three popular Bayesian nonparametric methods for modelling the survival functions of heterogeneous groups. Specifically, we will first compare the modelilng accuracy of the Dirichlet process, the hierarchical Dirichlet process, and the nested Dirichlet process on simulated datasets of different sizes, where group survival curves differ in shape or in expectation. We then will compare the models on two real world injury datasets. version:1
arxiv-1610-06620 | Proposing Plausible Answers for Open-ended Visual Question Answering | http://arxiv.org/abs/1610.06620 | id:1610.06620 author:Omid Bakhshandeh, Trung Bui, Zhe Lin, Walter Chang category:cs.CL cs.AI cs.CV  published:2016-10-20 summary:Answering open-ended questions is an essential capability for any intelligent agent. One of the most interesting recent open-ended question answering challenges is Visual Question Answering (VQA) which attempts to evaluate a system's visual understanding through its answers to natural language questions about images. There exist many approaches to VQA, the majority of which do not exhibit deeper semantic understanding of the candidate answers they produce. We study the importance of generating plausible answers to a given question by introducing the novel task of `Answer Proposal': for a given open-ended question, a system should generate a ranked list of candidate answers informed by the semantics of the question. We experiment with various models including a neural generative model as well as a semantic graph matching one. We provide both intrinsic and extrinsic evaluations for the task of Answer Proposal, showing that our best model learns to propose plausible answers with a high recall and performs competitively with some other solutions to VQA. version:2
arxiv-1610-07238 | SPiKeS: Superpixel-Keypoints Structure for Robust Visual Tracking | http://arxiv.org/abs/1610.07238 | id:1610.07238 author:Fran√ßois-Xavier Derue, Guillaume-Alexandre Bilodeau, Robert Bergevin category:cs.CV  published:2016-10-23 summary:In visual tracking, part-based trackers are attractive since they are robust against occlusion and deformation. However, a part represented by a rectangular patch does not account for the shape of the target, while a superpixel does thanks to its boundary evidence. Nevertheless, tracking superpixels is difficult due to their lack of discriminative power. Therefore, to enable superpixels to be tracked discriminatively as object parts, we propose to enhance them with keypoints. By combining properties of these two features, we build a novel element designated as a Superpixel-Keypoints structure (SPiKeS). Being discriminative, these new object parts can be located efficiently by a simple nearest neighbor matching process. Then, in a tracking process, each match votes for the target's center to give its location. In addition, the interesting properties of our new feature allows the development of an efficient model update for more robust tracking. According to experimental results, our SPiKeS-based tracker proves to be robust in many challenging scenarios by performing favorably against the state-of-the-art. version:1
arxiv-1610-07231 | Template Matching Advances and Applications in Image Analysis | http://arxiv.org/abs/1610.07231 | id:1610.07231 author:Nazanin Sadat Hashemi, Roya Babaie Aghdam, Atieh Sadat Bayat Ghiasi, Parastoo Fatemi category:cs.CV cs.AI  published:2016-10-23 summary:In most computer vision and image analysis problems, it is necessary to define a similarity measure between two or more different objects or images. Template matching is a classic and fundamental method used to score similarities between objects using certain mathematical algorithms. In this paper, we reviewed the basic concept of matching, as well as advances in template matching and applications such as invariant features or novel applications in medical image analysis. Additionally, deformable models and templates originating from classic template matching were discussed. These models have broad applications in image registration, and they are a fundamental aspect of novel machine vision or deep learning algorithms, such as convolutional neural networks (CNN), which perform shift and scale invariant functions followed by classification. In general, although template matching methods have restrictions which limit their application, they are recommended for use with other object recognition methods as pre- or post-processing steps. Combining a template matching technique such as normalized cross-correlation or dice coefficient with a robust decision-making algorithm yields a significant improvement in the accuracy rate for object detection and recognition. version:1
arxiv-1610-07216 | Inertial Regularization and Selection (IRS): Sequential Regression in High-Dimension and Sparsity | http://arxiv.org/abs/1610.07216 | id:1610.07216 author:Chitta Ranjan, Samaneh Ebrahimi, Kamran Paynabar category:stat.ML  published:2016-10-23 summary:In this paper, we develop a new sequential regression modeling approach for data streams. Data streams are commonly found around us, e.g in a retail enterprise sales data is continuously collected every day. A demand forecasting model is an important outcome from the data that needs to be continuously updated with the new incoming data. The main challenge in such modeling arises when there is a) high dimensional and sparsity, b) need for an adaptive use of prior knowledge, and/or c) structural changes in the system. The proposed approach addresses these challenges by incorporating an adaptive L1-penalty and inertia terms in the loss function, and thus called Inertial Regularization and Selection (IRS). The former term performs model selection to handle the first challenge while the latter is shown to address the last two challenges. A recursive estimation algorithm is developed, and shown to outperform the commonly used state-space models, such as Kalman Filters, in experimental studies and real data. version:1
arxiv-1610-07214 | 3D Hand Pose Tracking and Estimation Using Stereo Matching | http://arxiv.org/abs/1610.07214 | id:1610.07214 author:Jiawei Zhang, Jianbo Jiao, Mingliang Chen, Liangqiong Qu, Xiaobin Xu, Qingxiong Yang category:cs.CV  published:2016-10-23 summary:3D hand pose tracking/estimation will be very important in the next generation of human-computer interaction. Most of the currently available algorithms rely on low-cost active depth sensors. However, these sensors can be easily interfered by other active sources and require relatively high power consumption. As a result, they are currently not suitable for outdoor environments and mobile devices. This paper aims at tracking/estimating hand poses using passive stereo which avoids these limitations. A benchmark with 18,000 stereo image pairs and 18,000 depth images captured from different scenarios and the ground-truth 3D positions of palm and finger joints (obtained from the manual label) is thus proposed. This paper demonstrates that the performance of the state-of-the art tracking/estimation algorithms can be maintained with most stereo matching algorithms on the proposed benchmark, as long as the hand segmentation is correct. As a result, a novel stereo-based hand segmentation algorithm specially designed for hand tracking/estimation is proposed. The quantitative evaluation demonstrates that the proposed algorithm is suitable for the state-of-the-art hand pose tracking/estimation algorithms and the tracking quality is comparable to the use of active depth sensors under different challenging scenarios. version:1
arxiv-1610-07921 | Formulas for Counting the Sizes of Markov Equivalence Classes of Directed Acyclic Graphs | http://arxiv.org/abs/1610.07921 | id:1610.07921 author:Yangbo He, Bin Yu category:stat.ML cs.DM  published:2016-10-23 summary:The sizes of Markov equivalence classes of directed acyclic graphs play important roles in measuring the uncertainty and complexity in causal learning. A Markov equivalence class can be represented by an essential graph and its undirected subgraphs determine the size of the class. In this paper, we develop a method to derive the formulas for counting the sizes of Markov equivalence classes. We first introduce a new concept of core graph. The size of a Markov equivalence class of interest is a polynomial of the number of vertices given its core graph. Then, we discuss the recursive and explicit formula of the polynomial, and provide an algorithm to derive the size formula via symbolic computation for any given core graph. The proposed size formula derivation sheds light on the relationships between the size of a Markov equivalence class and its representation graph, and makes size counting efficient, even when the essential graphs contain non-sparse undirected subgraphs. version:1
arxiv-1610-07193 | Simpler PAC-Bayesian Bounds for Hostile Data | http://arxiv.org/abs/1610.07193 | id:1610.07193 author:Pierre Alquier, Benjamin Guedj category:stat.ML math.ST stat.TH  published:2016-10-23 summary:PAC-Bayesian learning bounds are of the utmost interest to the learning community. Their role is to connect the generalization ability of an aggregation distribution $\rho$ to its empirical risk and to its Kullback-Leibler divergence with respect to some prior distribution $\pi$. Unfortunately, most of the available bounds typically rely on heavy assumptions such as boundedness and independence of the observations. This paper aims at relaxing these constraints and provides PAC-Bayesian learning bounds that hold for dependent, heavy-tailed observations (hereafter referred to as \emph{hostile data}). In these bounds the Kullack-Leibler divergence is replaced with a general version of Csisz\'ar's $f$-divergence. We prove a general PAC-Bayesian bound, and show how to use it in various hostile settings. version:1
arxiv-1610-07187 | Learning Deep Architectures for Interaction Prediction in Structure-based Virtual Screening | http://arxiv.org/abs/1610.07187 | id:1610.07187 author:Adam Gonczarek, Jakub M. Tomczak, Szymon Zarƒôba, Joanna Kaczmar, Piotr DƒÖbrowski, Micha≈Ç J. Walczak category:stat.ML cs.LG  published:2016-10-23 summary:We introduce a deep learning architecture for structure-based virtual screening that generates fixed-sized fingerprints of proteins and small molecules by applying learnable atom convolution and softmax operations to each compound separately. These fingerprints are further transformed non-linearly, their inner-product is calculated and used to predict the binding potential. Moreover, we show that widely used benchmark datasets may be insufficient for testing structure-based virtual screening methods that utilize machine learning. Therefore, we introduce a new benchmark dataset, which we constructed based on DUD-E and PDBBind databases. version:1
arxiv-1610-07183 | How to be Fair and Diverse? | http://arxiv.org/abs/1610.07183 | id:1610.07183 author:L. Elisa Celis, Amit Deshpande, Tarun Kathuria, Nisheeth K. Vishnoi category:cs.LG  published:2016-10-23 summary:Due to the recent cases of algorithmic bias in data-driven decision-making, machine learning methods are being put under the microscope in order to understand the root cause of these biases and how to correct them. Here, we consider a basic algorithmic task that is central in machine learning: subsampling from a large data set. Subsamples are used both as an end-goal in data summarization (where fairness could either be a legal, political or moral requirement) and to train algorithms (where biases in the samples are often a source of bias in the resulting model). Consequently, there is a growing effort to modify either the subsampling methods or the algorithms themselves in order to ensure fairness. However, in doing so, a question that seems to be overlooked is whether it is possible to produce fair subsamples that are also adequately representative of the feature space of the data set - an important and classic requirement in machine learning. Can diversity and fairness be simultaneously ensured? We start by noting that, in some applications, guaranteeing one does not necessarily guarantee the other, and a new approach is required. Subsequently, we present an algorithmic framework which allows us to produce both fair and diverse samples. Our experimental results on an image summarization task show marked improvements in fairness without compromising feature diversity by much, giving us the best of both the worlds. version:1
arxiv-1610-07161 | Stochastic inference with spiking neurons in the high-conductance state | http://arxiv.org/abs/1610.07161 | id:1610.07161 author:Mihai A. Petrovici, Johannes Bill, Ilja Bytschok, Johannes Schemmel, Karlheinz Meier category:q-bio.NC cond-mat.dis-nn stat.ML  published:2016-10-23 summary:The highly variable dynamics of neocortical circuits observed in vivo have been hypothesized to represent a signature of ongoing stochastic inference but stand in apparent contrast to the deterministic response of neurons measured in vitro. Based on a propagation of the membrane autocorrelation across spike bursts, we provide an analytical derivation of the neural activation function that holds for a large parameter space, including the high-conductance state. On this basis, we show how an ensemble of leaky integrate-and-fire neurons with conductance-based synapses embedded in a spiking environment can attain the correct firing statistics for sampling from a well-defined target distribution. For recurrent networks, we examine convergence toward stationarity in computer simulations and demonstrate sample-based Bayesian inference in a mixed graphical model. This points to a new computational role of high-conductance states and establishes a rigorous link between deterministic neuron models and functional stochastic dynamics on the network level. version:1
arxiv-1610-07159 | Real-time Halfway Domain Reconstruction of Motion and Geometry | http://arxiv.org/abs/1610.07159 | id:1610.07159 author:Lucas Thies, Michael Zollh√∂fer, Christian Richardt, Christian Theobalt, G√ºnther Greiner category:cs.CV I.4.8  published:2016-10-23 summary:We present a novel approach for real-time joint reconstruction of 3D scene motion and geometry from binocular stereo videos. Our approach is based on a novel variational halfway-domain scene flow formulation, which allows us to obtain highly accurate spatiotemporal reconstructions of shape and motion. We solve the underlying optimization problem at real-time frame rates using a novel data-parallel robust non-linear optimization strategy. Fast convergence and large displacement flows are achieved by employing a novel hierarchy that stores delta flows between hierarchy levels. High performance is obtained by the introduction of a coarser warp grid that decouples the number of unknowns from the input resolution of the images. We demonstrate our approach in a live setup that is based on two commodity webcams, as well as on publicly available video data. Our extensive experiments and evaluations show that our approach produces high-quality dense reconstructions of 3D geometry and scene flow at real-time frame rates, and compares favorably to the state of the art. version:1
arxiv-1610-07149 | Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems | http://arxiv.org/abs/1610.07149 | id:1610.07149 author:Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, Ming Zhang category:cs.CL  published:2016-10-23 summary:Open-domain human-computer conversation has attracted much attention in the field of NLP. Contrary to rule- or template-based domain-specific dialog systems, open-domain conversation usually requires data-driven approaches, which can be roughly divided into two categories: retrieval-based and generation-based systems. Retrieval systems search a user-issued utterance (called a query) in a large database, and return a reply that best matches the query. Generative approaches, typically based on recurrent neural networks (RNNs), can synthesize new replies, but they suffer from the problem of generating short, meaningless utterances. In this paper, we propose a novel ensemble of retrieval-based and generation-based dialog systems in the open domain. In our approach, the retrieved candidate, in addition to the original query, is fed to an RNN-based reply generator, so that the neural model is aware of more information. The generated reply is then fed back as a new candidate for post-reranking. Experimental results show that such ensemble outperforms each single part of it by a large margin. version:1
arxiv-1610-07126 | Multi-View Subspace Clustering via Relaxed $L_1$-Norm of Tensor Multi-Rank | http://arxiv.org/abs/1610.07126 | id:1610.07126 author:Yuan Xie, Dacheng Tao, Wensheng Zhang, Lei Zhang category:cs.CV  published:2016-10-23 summary:In this paper, we address the multi-view subspace clustering problem. Our method utilize the circulant algebra for tensor, which is constructed by stacking the subspace representation matrices of different views and then shifting, to explore the high order correlations underlying multi-view data. By introducing a recently proposed tensor factorization, namely tensor-Singular Value Decomposition (t-SVD) \cite{kilmer13}, we can impose a new type of low-rank tensor constraint on the shifted tensor to capture the complementary information from multiple views. Different from traditional unfolding based tensor norm, this low-rank tensor constraint has optimality properties similar to that of matrix rank derived from SVD, so that complementary information among views can be explored more efficiently and thoroughly. The established model, called t-SVD based Multi-view Subspace Clustering (t-SVD-MSC), falls into the applicable scope of augmented Lagrangian method, and its minimization problem can be efficiently solved with theoretical convergence guarantee and relatively low computational complexity. Extensive experimental testing on eight challenging image dataset shows that the proposed method has achieved highly competent objective performance compared to several state-of-the-art multi-view clustering methods. version:1
arxiv-1610-07119 | Cross Device Matching for Online Advertising with Neural Feature Ensembles : First Place Solution at CIKM Cup 2016 | http://arxiv.org/abs/1610.07119 | id:1610.07119 author:Yi Tay, Cong-Minh Phan, Tuan-Anh Nguyen Pham category:cs.LG cs.IR stat.ML  published:2016-10-23 summary:We describe the 1st place winning approach for the CIKM Cup 2016 Challenge. In this paper, we provide an approach to reasonably identify same users across multiple devices based on browsing logs. Our approach regards a candidate ranking problem as pairwise classification and utilizes an unsupervised neural feature ensemble approach to learn latent features of users. Combined with traditional hand crafted features, each user pair feature is fed into a supervised classifier in order to perform pairwise classification. Lastly, we propose supervised and unsupervised inference techniques. version:1
arxiv-1610-07116 | Online Classification with Complex Metrics | http://arxiv.org/abs/1610.07116 | id:1610.07116 author:Bowei Yan, Oluwasanmi Koyejo, Kai Zhong, Pradeep Ravikumar category:stat.ML cs.LG  published:2016-10-23 summary:We present a framework and analysis of consistent binary classification for complex and non-decomposable performance metrics such as the F-measure and the Jaccard measure. The proposed framework is general, as it applies to both batch and online learning, and to both linear and non-linear models. Our work follows recent results showing that the Bayes optimal classifier for many complex metrics is given by a thresholding of the conditional probability of the positive class. This manuscript extends this thresholding characterization -- showing that the utility is strictly locally quasi-concave with respect to the threshold for a wide range of models and performance metrics. This, in turn, motivates simple normalized gradient ascent updates for threshold estimation. We present a finite-sample regret analysis for the resulting procedure. In particular, the risk for the batch case converges to the Bayes risk at the same rate as that of the underlying conditional probability estimation, and the risk of proposed online algorithm converges at a rate that depends on the conditional probability estimation risk. For instance, in the special case where the conditional probability model is logistic regression, our procedure achieves $O(\frac{1}{\sqrt{n}})$ sample complexity, both for batch and online training. Empirical evaluation shows that the proposed algorithms out-perform alternatives in practice, with comparable or better prediction performance and reduced run time for various metrics and datasets. version:1
arxiv-1610-07108 | Fast and Reliable Parameter Estimation from Nonlinear Observations | http://arxiv.org/abs/1610.07108 | id:1610.07108 author:Samet Oymak, Mahdi Soltanolkotabi category:stat.ML cs.IT math.IT math.OC  published:2016-10-23 summary:In this paper we study the problem of recovering a structured but unknown parameter ${\bf{\theta}}^*$ from $n$ nonlinear observations of the form $y_i=f(\langle {\bf{x}}_i,{\bf{\theta}}^*\rangle)$ for $i=1,2,\ldots,n$. We develop a framework for characterizing time-data tradeoffs for a variety of parameter estimation algorithms when the nonlinear function $f$ is unknown. This framework includes many popular heuristics such as projected/proximal gradient descent and stochastic schemes. For example, we show that a projected gradient descent scheme converges at a linear rate to a reliable solution with a near minimal number of samples. We provide a sharp characterization of the convergence rate of such algorithms as a function of sample size, amount of a-prior knowledge available about the parameter and a measure of the nonlinearity of the function $f$. These results provide a precise understanding of the various tradeoffs involved between statistical and computational resources as well as a-prior side information available for such nonlinear parameter estimation problems. version:1
arxiv-1610-07104 | Independent Component Analysis by Entropy Maximization with Kernels | http://arxiv.org/abs/1610.07104 | id:1610.07104 author:Zois Boukouvalas, Rami Mowakeaa, Geng-Shen Fu, Tulay Adali category:stat.ML  published:2016-10-22 summary:Independent component analysis (ICA) is the most popular method for blind source separation (BSS) with a diverse set of applications, such as biomedical signal processing, video and image analysis, and communications. Maximum likelihood (ML), an optimal theoretical framework for ICA, requires knowledge of the true underlying probability density function (PDF) of the latent sources, which, in many applications, is unknown. ICA algorithms cast in the ML framework often deviate from its theoretical optimality properties due to poor estimation of the source PDF. Therefore, accurate estimation of source PDFs is critical in order to avoid model mismatch and poor ICA performance. In this paper, we propose a new and efficient ICA algorithm based on entropy maximization with kernels, (ICA-EMK), which uses both global and local measuring functions as constraints to dynamically estimate the PDF of the sources with reasonable complexity. In addition, the new algorithm performs optimization with respect to each of the cost function gradient directions separately, enabling parallel implementations on multi-core computers. We demonstrate the superior performance of ICA-EMK over competing ICA algorithms using simulated as well as real-world data. version:1
arxiv-1610-02581 | Variance-based regularization with convex objectives | http://arxiv.org/abs/1610.02581 | id:1610.02581 author:John Duchi, Hongseok Namkoong category:stat.ML math.ST stat.TH  published:2016-10-08 summary:We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error. Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator. In particular, we show that our procedure comes with certificates of optimality and achieves faster rates of convergence in more general settings than empirical risk minimization by virtue of trading off approximation and estimation error optimally. We give corroborating empirical evidence that suggests that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems. version:2
arxiv-1610-07091 | Automatic Identification of Sarcasm Target: An Introductory Approach | http://arxiv.org/abs/1610.07091 | id:1610.07091 author:Aditya Joshi, Pranav Goel, Pushpak Bhattacharyya, Mark Carman category:cs.CL  published:2016-10-22 summary:Past work in computational sarcasm deals primarily with sarcasm detection. In this paper, we introduce a novel, related problem: sarcasm target identification (\textit{i.e.}, extracting the target of ridicule in a sarcastic sentence). We present an introductory approach for sarcasm target identification. Our approach employs two types of extractors: one based on rules, and another consisting of a statistical classifier. To compare our approach, we use two baselines: a na\"ive baseline and another baseline based on work in sentiment target identification. We perform our experiments on book snippets and tweets, and show that our hybrid approach performs better than the two baselines and also, in comparison with using the two extractors individually. Our introductory approach establishes the viability of sarcasm target identification, and will serve as a baseline for future work. version:1
arxiv-1610-07086 | Deep image mining for diabetic retinopathy screening | http://arxiv.org/abs/1610.07086 | id:1610.07086 author:Gwenol√© Quellec, Katia Charri√®re, Yassine Boudi, B√©atrice Cochener, Mathieu Lamard category:cs.CV  published:2016-10-22 summary:Deep learning is quickly becoming the leading methodology for medical image analysis. Given a large medical archive, where each image is associated with a diagnosis, efficient pathology detectors or classifiers can be trained with virtually no expert knowledge about the target pathologies. However, deep learning algorithms, including the popular ConvNets, are black boxes: little is known about the local patterns analyzed by ConvNets to make a decision at the image level. A solution is proposed in this paper to create heatmaps showing which pixels in images play a role in the image-level predictions. In other words, a ConvNet trained for image-level classification can be used to detect lesions as well. A generalization of the backpropagation method is proposed in order to train ConvNets that produce high-quality heatmaps. The proposed solution is applied to diabetic retinopathy (DR) screening in a dataset of almost 90,000 fundus photographs from the 2015 Kaggle Diabetic Retinopathy competition. For the task of detecting referable DR, the proposed system outperforms state-of-the-art methods based on deep learning: $A_z$ = 0.954, as opposed to $A_z$ = 0.946 for Colas et al. (2016) in particular. Performance at the pixel level was evaluated in the DiaretDB1 dataset, where four types of lesions are manually segmented: microaneurysms, hemorrhages, exudates and cotton-wool spots. For all lesion types, the proposed detector, trained with image-level supervision, outperforms recent algorithms, even though they were trained with pixel-level supervision. This detector is part of the RetinOpTIC system for mobile eye pathology screening. Because it does not rely on expert knowledge or manual segmentation for detecting relevant patterns, the proposed solution can potentially discover new biomarkers in images, which makes it a promising image mining tool. version:1
arxiv-1610-07031 | Exercise Motion Classification from Large-Scale Wearable Sensor Data Using Convolutional Neural Networks | http://arxiv.org/abs/1610.07031 | id:1610.07031 author:Terry Taewoong Um, Vahid Babakeshizadeh, Dana Kulic category:cs.CV cs.LG  published:2016-10-22 summary:The ability to accurately observe human motion and identify human activities is essential for developing automatic rehabilitation and sports training systems. In this paper, large-scale exercise motion data obtained from a forearm-worn wearable sensor are classified with a convolutional neural network (CNN). Time series data consisting of accelerometer and orientation measurements are formatted as "images", allowing the CNN to automatically extract discriminative features. The resulting CNN classifies 50 gym exercises with 92.14% accuracy. A comparative study on the effects of image formatting and different CNN architectures is also presented. version:1
arxiv-1610-07008 | Optimization on Submanifolds of Convolution Kernels in CNNs | http://arxiv.org/abs/1610.07008 | id:1610.07008 author:Mete Ozay, Takayuki Okatani category:cs.CV  published:2016-10-22 summary:Kernel normalization methods have been employed to improve robustness of optimization methods to reparametrization of convolution kernels, covariate shift, and to accelerate training of Convolutional Neural Networks (CNNs). However, our understanding of theoretical properties of these methods has lagged behind their success in applications. We develop a geometric framework to elucidate underlying mechanisms of a diverse range of kernel normalization methods. Our framework enables us to expound and identify geometry of space of normalized kernels. We analyze and delineate how state-of-the-art kernel normalization methods affect the geometry of search spaces of the stochastic gradient descent (SGD) algorithms in CNNs. Following our theoretical results, we propose a SGD algorithm with assurance of almost sure convergence of the methods to a solution at single minimum of classification loss of CNNs. Experimental results show that the proposed method achieves state-of-the-art performance for major image classification benchmarks with CNNs. version:1
arxiv-1610-06998 | Ranking of classification algorithms in terms of mean-standard deviation using A-TOPSIS | http://arxiv.org/abs/1610.06998 | id:1610.06998 author:Andre G. C. Pacheco, Renato A. Krohling category:cs.LG  published:2016-10-22 summary:In classification problems when multiples algorithms are applied to different benchmarks a difficult issue arises, i.e., how can we rank the algorithms? In machine learning it is common run the algorithms several times and then a statistic is calculated in terms of means and standard deviations. In order to compare the performance of the algorithms, it is very common to employ statistical tests. However, these tests may also present limitations, since they consider only the means and not the standard deviations of the obtained results. In this paper, we present the so called A-TOPSIS, based on TOPSIS (Technique for Order Preference by Similarity to Ideal Solution), to solve the problem of ranking and comparing classification algorithms in terms of means and standard deviations. We use two case studies to illustrate the A-TOPSIS for ranking classification algorithms and the results show the suitability of A-TOPSIS to rank the algorithms. The presented approach is general and can be applied to compare the performance of stochastic algorithms in machine learning. Finally, to encourage researchers to use the A-TOPSIS for ranking algorithms we also presented in this work an easy-to-use A-TOPSIS web framework. version:1
arxiv-1610-06987 | Multitask Learning of Vegetation Biochemistry from Hyperspectral Data | http://arxiv.org/abs/1610.06987 | id:1610.06987 author:Utsav B. Gewali, Sildomar T. Monteiro category:cs.CV  published:2016-10-22 summary:Statistical models have been successful in accurately estimating the biochemical contents of vegetation from the reflectance spectra. However, their performance deteriorates when there is a scarcity of sizable amount of ground truth data for modeling the complex non-linear relationship occurring between the spectrum and the biochemical quantity. We propose a novel Gaussian process based multitask learning method for improving the prediction of a biochemical through the transfer of knowledge from the learned models for predicting related biochemicals. This method is most advantageous when there are few ground truth data for the biochemical of interest, but plenty of ground truth data for related biochemicals. The proposed multitask Gaussian process hypothesizes that the inter-relationship between the biochemical quantities is better modeled by using a combination of two or more covariance functions and inter-task correlation matrices. In the experiments, our method outperformed the current methods on two real-world datasets. version:1
arxiv-1610-06985 | Spectral Angle Based Unary Energy Functions for Spatial-Spectral Hyperspectral Classification using Markov Random Fields | http://arxiv.org/abs/1610.06985 | id:1610.06985 author:Utsav B. Gewali, Sildomar T. Monteiro category:cs.CV  published:2016-10-22 summary:In this paper, we propose and compare two spectral angle based approaches for spatial-spectral classification. Our methods use the spectral angle to generate unary energies in a grid-structured Markov random field defined over the pixel labels of a hyperspectral image. The first approach is to use the exponential spectral angle mapper (ESAM) kernel/covariance function, a spectral angle based function, with the support vector machine and the Gaussian process classifier. The second approach is to directly use the minimum spectral angle between the test pixel and the training pixels as the unary energy. We compare the proposed methods with the state-of-the-art Markov random field methods that use support vector machines and Gaussian processes with squared exponential kernel/covariance function. In our experiments with two datasets, it is seen that using minimum spectral angle as unary energy produces better or comparable results to the existing methods at a smaller running time. version:1
arxiv-1610-06972 | Learning Cost-Effective Treatment Regimes using Markov Decision Processes | http://arxiv.org/abs/1610.06972 | id:1610.06972 author:Himabindu Lakkaraju, Cynthia Rudin category:cs.AI cs.LG stat.ML  published:2016-10-21 summary:Decision makers, such as doctors and judges, make crucial decisions such as recommending treatments to patients, and granting bails to defendants on a daily basis. Such decisions typically involve weighting the potential benefits of taking an action against the costs involved. In this work, we aim to automate this task of learning \emph{cost-effective, interpretable and actionable treatment regimes}. We formulate this as a problem of learning a decision list -- a sequence of if-then-else rules -- which maps characteristics of subjects (eg., diagnostic test results of patients) to treatments. We propose a novel objective to construct a decision list which maximizes outcomes for the population, and minimizes overall costs. We model the problem of learning such a list as a Markov Decision Process (MDP) and employ a variant of the Upper Confidence Bound for Trees (UCT) strategy which leverages customized checks for pruning the search space effectively. Experimental results on real world observational data capturing judicial bail decisions and treatment recommendations for asthma patients demonstrate the effectiveness of our approach. version:1
arxiv-1610-06949 | Mean-Field Variational Inference for Gradient Matching with Gaussian Processes | http://arxiv.org/abs/1610.06949 | id:1610.06949 author:Nico S. Gorbach, Stefan Bauer, Joachim M. Buhmann category:stat.ML  published:2016-10-21 summary:Gradient matching with Gaussian processes is a promising tool for learning parameters of ordinary differential equations (ODE's). The essence of gradient matching is to model the prior over state variables as a Gaussian process which implies that the joint distribution given the ODE's and GP kernels is also Gaussian distributed. The state-derivatives are integrated out analytically since they are modelled as latent variables. However, the state variables themselves are also latent variables because they are contaminated by noise. Previous work sampled the state variables since integrating them out is \textit{not} analytically tractable. In this paper we use mean-field approximation to establish tight variational lower bounds that decouple state variables and are therefore, in contrast to the integral over state variables, analytically tractable and even concave for a restricted family of ODE's, including nonlinear and periodic ODE's. Such variational lower bounds facilitate "hill climbing" to determine the maximum a posteriori estimate of ODE parameters. An additional advantage of our approach over sampling methods is the determination of a proxy to the intractable posterior distribution over state variables given observations and the ODE's. version:1
arxiv-1610-06940 | Safety Verification of Deep Neural Networks | http://arxiv.org/abs/1610.06940 | id:1610.06940 author:Xiaowei Huang, Marta Kwiatkowska, Sen Wang, Min Wu category:cs.AI cs.LG stat.ML  published:2016-10-21 summary:Deep neural networks have achieved impressive experimental results in image classification, but can surprisingly be unstable with respect to adversarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it. With potential applications including perception modules and end-to-end controllers for self-driving cars, this raises concerns about their safety. We develop the first SMT-based automated verification framework for feed-forward multi-layer neural networks that works directly with the code of the network, exploring it layer by layer. We define safety for a region around a data point in a given layer by requiring that all points in the region are assigned the same class label. Working with a notion of a manipulation, a mapping between points that intuitively corresponds to a modification of an image, we employ discretisation to enable exhaustive search of the region. Our method can guarantee that adversarial examples are found for the given region and set of manipulations. If found, adversarial examples can be shown to human testers and/or used to fine-tune the network, and otherwise the network is declared safe for the given parameters. We implement the techniques using Z3 and evaluate them on state-of-the-art networks, including regularised and deep learning networks. version:1
arxiv-1610-06924 | Automatic Image De-fencing System | http://arxiv.org/abs/1610.06924 | id:1610.06924 author:Nakka Krishna Kanth category:cs.CV  published:2016-10-21 summary:Tourists and Wild-life photographers are often hindered in capturing their cherished images or videos by a fence that limits accessibility to the scene of interest. The situation has been exacerbated by growing concerns of security at public places and a need exists to provide a tool that can be used for post-processing such fenced videos to produce a de-fenced image. There are several challenges in this problem, we identify them as Robust detection of fence/occlusions and Estimating pixel motion of background scenes and Filling in the fence/occlusions by utilizing information in multiple frames of the input video. In this work, we aim to build an automatic post-processing tool that can efficiently rid the input video of occlusion artifacts like fences. Our work is distinguished by two major contributions. The first is the introduction of learning based technique to detect the fences patterns with complicated backgrounds. The second is the formulation of objective function and further minimization through loopy belief propagation to fill-in the fence pixels. We observe that grids of Histogram of oriented gradients descriptor using Support vector machines based classifier significantly outperforms detection accuracy of texels in a lattice. We present results of experiments using several real-world videos to demonstrate the effectiveness of the proposed fence detection and de-fencing algorithm. version:1
arxiv-1610-06918 | Learning to Protect Communications with Adversarial Neural Cryptography | http://arxiv.org/abs/1610.06918 | id:1610.06918 author:Mart√≠n Abadi, David G. Andersen category:cs.CR cs.LG  published:2016-10-21 summary:We ask whether neural networks can learn to use secret keys to protect information from other neural networks. Specifically, we focus on ensuring confidentiality properties in a multiagent system, and we specify those properties in terms of an adversary. Thus, a system may consist of neural networks named Alice and Bob, and we aim to limit what a third neural network named Eve learns from eavesdropping on the communication between Alice and Bob. We do not prescribe specific cryptographic algorithms to these neural networks; instead, we train end-to-end, adversarially. We demonstrate that the neural networks can learn how to perform forms of encryption and decryption, and also how to apply these operations selectively in order to meet confidentiality goals. version:1
arxiv-1610-06907 | Enhanced Object Detection via Fusion With Prior Beliefs from Image Classification | http://arxiv.org/abs/1610.06907 | id:1610.06907 author:Yilun Cao, Hyungtae Lee, Heesung Kwon category:cs.CV  published:2016-10-21 summary:In this paper, we introduce a novel fusion method that can enhance object detection performance by fusing decisions from two different types of computer vision tasks: object detection and image classification. In the proposed work, the class label of an image obtained from image classification is viewed as prior knowledge about existence or non-existence of certain objects. The prior knowledge is then fused with the decisions of object detection to improve detection accuracy by mitigating false positives of an object detector that are strongly contradicted with the prior knowledge. A recently introduced novel fusion approach called dynamic belief fusion (DBF) is used to fuse the detector output with the classification prior. Experimental results show that the detection performance of all the detection algorithms used in the proposed work is improved on benchmark datasets via the proposed fusion framework. version:1
arxiv-1610-06903 | Joint Deep Exploitation of Semantic Keywords and Visual Features for Malicious Crowd Image Classification | http://arxiv.org/abs/1610.06903 | id:1610.06903 author:Joel Levis, Hyungtae Lee, Heesung Kwon, James Michaelis, Michael Kolodny, Sungmin Eum category:cs.CV  published:2016-10-21 summary:General image classification approaches differentiate classes using strong distinguishing features but some classes cannot be easily separated because they contain very similar visual features. To deal with this problem, we can use keywords relevant to a particular class. To implement this concept we have newly constructed a malicious crowd dataset which contains crowd images with two events, benign and malicious, which look similar yet involve opposite semantic events. We also created a set of five malicious event-relevant keywords such as police and fire. In the evaluation, integrating malicious event classification with recognition output of these keywords enhances the overall performance on the malicious crowd dataset. version:1
arxiv-1610-06902 | Dictionary Learning Strategies for Compressed Fiber Sensing Using a Probabilistic Sparse Model | http://arxiv.org/abs/1610.06902 | id:1610.06902 author:Christian Weiss, Abdelhak M. Zoubir category:stat.ML  published:2016-10-21 summary:We present a sparse estimation and dictionary learning framework for compressed fiber sensing based on a probabilistic hierarchical sparse model. To handle severe dictionary coherence, selective shrinkage is achieved using a Weibull prior, which can be related to non-convex optimization with $p$-norm constraints for $0 < p < 1$. In addition, we leverage the specific dictionary structure to promote collective shrinkage based on a local similarity model. This is incorporated in form of a kernel function in the joint prior density of the sparse coefficients, thereby establishing a Markov random field-relation. Approximate inference is accomplished using a hybrid technique that combines Hamilton Monte Carlo and Gibbs sampling. To estimate the dictionary parameter, we pursue two strategies, relying on either a deterministic or a probabilistic model for the dictionary parameter. In the first strategy, the parameter is estimated based on alternating estimation. In the second strategy, it is jointly estimated along with the sparse coefficients. The performance is evaluated in comparison to an existing method in various scenarios using simulations and experimental data. version:1
arxiv-1610-06856 | Automated Big Text Security Classification | http://arxiv.org/abs/1610.06856 | id:1610.06856 author:Khudran Alzhrani, Ethan M. Rudd, Terrance E. Boult, C. Edward Chow category:cs.CR cs.AI cs.CL cs.CY  published:2016-10-21 summary:In recent years, traditional cybersecurity safeguards have proven ineffective against insider threats. Famous cases of sensitive information leaks caused by insiders, including the WikiLeaks release of diplomatic cables and the Edward Snowden incident, have greatly harmed the U.S. government's relationship with other governments and with its own citizens. Data Leak Prevention (DLP) is a solution for detecting and preventing information leaks from within an organization's network. However, state-of-art DLP detection models are only able to detect very limited types of sensitive information, and research in the field has been hindered due to the lack of available sensitive texts. Many researchers have focused on document-based detection with artificially labeled "confidential documents" for which security labels are assigned to the entire document, when in reality only a portion of the document is sensitive. This type of whole-document based security labeling increases the chances of preventing authorized users from accessing non-sensitive information within sensitive documents. In this paper, we introduce Automated Classification Enabled by Security Similarity (ACESS), a new and innovative detection model that penetrates the complexity of big text security classification/detection. To analyze the ACESS system, we constructed a novel dataset, containing formerly classified paragraphs from diplomatic cables made public by the WikiLeaks organization. To our knowledge this paper is the first to analyze a dataset that contains actual formerly sensitive information annotated at paragraph granularity. version:1
arxiv-1610-06815 | Deep Models for Engagement Assessment With Scarce Label Information | http://arxiv.org/abs/1610.06815 | id:1610.06815 author:Feng Li, Guangfan Zhang, Wei Wang, Roger Xu, Tom Schnell, Jonathan Wen, Frederic McKenzie, Jiang Li category:cs.CV  published:2016-10-21 summary:Task engagement is defined as loadings on energetic arousal (affect), task motivation, and concentration (cognition). It is usually challenging and expensive to label cognitive state data, and traditional computational models trained with limited label information for engagement assessment do not perform well because of overfitting. In this paper, we proposed two deep models (i.e., a deep classifier and a deep autoencoder) for engagement assessment with scarce label information. We recruited 15 pilots to conduct a 4-h flight simulation from Seattle to Chicago and recorded their electroencephalograph (EEG) signals during the simulation. Experts carefully examined the EEG signals and labeled 20 min of the EEG data for each pilot. The EEG signals were preprocessed and power spectral features were extracted. The deep models were pretrained by the unlabeled data and were fine-tuned by a different proportion of the labeled data (top 1%, 3%, 5%, 10%, 15%, and 20%) to learn new representations for engagement assessment. The models were then tested on the remaining labeled data. We compared performances of the new data representations with the original EEG features for engagement assessment. Experimental results show that the representations learned by the deep models yielded better accuracies for the six scenarios (77.09%, 80.45%, 83.32%, 85.74%, 85.78%, and 86.52%), based on different proportions of the labeled data for training, as compared with the corresponding accuracies (62.73%, 67.19%, 73.38%, 79.18%, 81.47%, and 84.92%) achieved by the original EEG features. Deep models are effective for engagement assessment especially when less label information was used for training. version:1
arxiv-1610-06811 | Convex Formulation for Kernel PCA and its Use in Semi-Supervised Learning | http://arxiv.org/abs/1610.06811 | id:1610.06811 author:Carlos M. Ala√≠z, Micha√´l Fanuel, Johan A. K. Suykens category:cs.LG stat.ML  published:2016-10-21 summary:In this paper, Kernel PCA is reinterpreted as the solution to a convex optimization problem. Actually, there is a constrained convex problem for each principal component, so that the constraints guarantee that the principal component is indeed a solution, and not a mere saddle point. Although these insights do not imply any algorithmic improvement, they can be used to further understand the method, formulate possible extensions and properly address them. As an example, a new convex optimization problem for semi-supervised classification is proposed, which seems particularly well-suited whenever the number of known labels is small. Our formulation resembles a Least Squares SVM problem with a regularization parameter multiplied by a negative sign, combined with a variational principle for Kernel PCA. Our primal optimization principle for semi-supervised learning is solved in terms of the Lagrange multipliers. Numerical experiments in several classification tasks illustrate the performance of the proposed model in problems with only a few labeled data. version:1
arxiv-1610-06806 | Robust training on approximated minimal-entropy set | http://arxiv.org/abs/1610.06806 | id:1610.06806 author:Tianpei Xie, Nasser. M. Narabadi, Alfred O. Hero category:cs.LG stat.ML I.1.2; H.1.1  published:2016-10-21 summary:In this paper, we propose a general framework to learn a robust large-margin binary classifier when corrupt measurements, called anomalies, caused by sensor failure might be present in the training set. The goal is to minimize the generalization error of the classifier on non-corrupted measurements while controlling the false alarm rate associated with anomalous samples. By incorporating a non-parametric regularizer based on an empirical entropy estimator, we propose a Geometric-Entropy-Minimization regularized Maximum Entropy Discrimination (GEM-MED) method to learn to classify and detect anomalies in a joint manner. We demonstrate using simulated data and a real multimodal data set. Our GEM-MED method can yield improved performance over previous robust classification methods in terms of both classification accuracy and anomaly detection rate. version:1
arxiv-1610-06781 | Vision-Based Reaching Using Modular Deep Networks: from Simulation to the Real World | http://arxiv.org/abs/1610.06781 | id:1610.06781 author:Fangyi Zhang, J√ºrgen Leitner, Ben Upcroft, Peter Corke category:cs.RO cs.AI cs.CV cs.LG cs.SY  published:2016-10-21 summary:In this paper we describe a deep network architecture that maps visual input to control actions for a robotic planar reaching task with 100% reliability in real-world trials. Our network is trained in simulation and fine-tuned with a limited number of real-world images. The policy search is guided by a kinematics-based controller (K-GPS), which works more effectively and efficiently than $\varepsilon$-Greedy. A critical insight in our system is the need to introduce a bottleneck in the network between the perception and control networks, and to initially train these networks independently. version:1
arxiv-1610-06761 | Maximally Divergent Intervals for Anomaly Detection | http://arxiv.org/abs/1610.06761 | id:1610.06761 author:Erik Rodner, Bj√∂rn Barz, Yanira Guanche, Milan Flach, Miguel Mahecha, Paul Bodesheim, Markus Reichstein, Joachim Denzler category:stat.ML cs.LG  published:2016-10-21 summary:We present new methods for batch anomaly detection in multivariate time series. Our methods are based on maximizing the Kullback-Leibler divergence between the data distribution within and outside an interval of the time series. An empirical analysis shows the benefits of our algorithms compared to methods that treat each time step independently from each other without optimizing with respect to all possible intervals. version:1
arxiv-1610-06756 | Fine-grained Recognition in the Noisy Wild: Sensitivity Analysis of Convolutional Neural Networks Approaches | http://arxiv.org/abs/1610.06756 | id:1610.06756 author:Erik Rodner, Marcel Simon, Robert B. Fisher, Joachim Denzler category:cs.CV  published:2016-10-21 summary:In this paper, we study the sensitivity of CNN outputs with respect to image transformations and noise in the area of fine-grained recognition. In particular, we answer the following questions (1) how sensitive are CNNs with respect to image transformations encountered during wild image capture?; (2) how can we predict CNN sensitivity?; and (3) can we increase the robustness of CNNs with respect to image degradations? To answer the first question, we provide an extensive empirical sensitivity analysis of commonly used CNN architectures (AlexNet, VGG19, GoogleNet) across various types of image degradations. This allows for predicting CNN performance for new domains comprised by images of lower quality or captured from a different viewpoint. We also show how the sensitivity of CNN outputs can be predicted for single images. Furthermore, we demonstrate that input layer dropout or pre-filtering during test time only reduces CNN sensitivity for high levels of degradation. Experiments for fine-grained recognition tasks reveal that VGG19 is more robust to severe image degradations than AlexNet and GoogleNet. However, small intensity noise can lead to dramatic changes in CNN performance even for VGG19. version:1
arxiv-1610-06740 | Model-based Outdoor Performance Capture | http://arxiv.org/abs/1610.06740 | id:1610.06740 author:Nadia Robertini, Dan Casas, Helge Rhodin, Hans-Peter Seidel, Christian Theobalt category:cs.CV  published:2016-10-21 summary:We propose a new model-based method to accurately reconstruct human performances captured outdoors in a multi-camera setup. Starting from a template of the actor model, we introduce a new unified implicit representation for both, articulated skeleton tracking and nonrigid surface shape refinement. Our method fits the template to unsegmented video frames in two stages - first, the coarse skeletal pose is estimated, and subsequently non-rigid surface shape and body pose are jointly refined. Particularly for surface shape refinement we propose a new combination of 3D Gaussians designed to align the projected model with likely silhouette contours without explicit segmentation or edge detection. We obtain reconstructions of much higher quality in outdoor settings than existing methods, and show that we are on par with state-of-the-art methods on indoor scenes for which they were designed version:1
arxiv-1610-06731 | Minimax Error of Interpolation and Optimal Design of Experiments for Variable Fidelity Data | http://arxiv.org/abs/1610.06731 | id:1610.06731 author:Evgeny Burnaev, Alexey Zaytsev category:stat.ML math.ST stat.AP stat.TH  published:2016-10-21 summary:Engineering problems often involve data sources of variable fidelity with different costs of obtaining an observation. In particular, one can use both a cheap low fidelity function (e.g. a computational experiment with a CFD code) and an expensive high fidelity function (e.g. a wind tunnel experiment) to generate a data sample in order to construct a regression model of a high fidelity function. The key question in this setting is how the sizes of the high and low fidelity data samples should be selected in order to stay within a given computational budget and maximize accuracy of the regression model prior to committing resources on data acquisition. In this paper we obtain minimax interpolation errors for single and variable fidelity scenarios for a multivariate Gaussian process regression. Evaluation of the minimax errors allows us to identify cases when the variable fidelity data provides better interpolation accuracy than the exclusively high fidelity data for the same computational budget. These results allow us to calculate the optimal shares of variable fidelity data samples under the given computational budget constraint. Real and synthetic data experiments suggest that using the obtained optimal shares often outperforms natural heuristics in terms of the regression accuracy. version:1
arxiv-1610-07857 | Hybrid clustering-classification neural network in the medical diagnostics of reactive arthritis | http://arxiv.org/abs/1610.07857 | id:1610.07857 author:Yevgeniy Bodyanskiy, Olena Vynokurova, Volodymyr Savvo, Tatiana Tverdokhlib, Pavlo Mulesa category:cs.LG cs.NE stat.ML  published:2016-10-21 summary:The hybrid clustering-classification neural network is proposed. This network allows increasing a quality of information processing under the condition of overlapping classes due to the rational choice of a learning rate parameter and introducing a special procedure of fuzzy reasoning in the clustering process, which occurs both with an external learning signal (supervised) and without the one (unsupervised). As similarity measure neighborhood function or membership one, cosine structures are used, which allow to provide a high flexibility due to self-learning-learning process and to provide some new useful properties. Many realized experiments have confirmed the efficiency of proposed hybrid clustering-classification neural network; also, this network was used for solving diagnostics task of reactive arthritis. version:1
arxiv-1610-06700 | End-to-End Training Approaches for Discriminative Segmental Models | http://arxiv.org/abs/1610.06700 | id:1610.06700 author:Hao Tang, Weiran Wang, Kevin Gimpel, Karen Livescu category:cs.CL cs.LG stat.ML  published:2016-10-21 summary:Recent work on discriminative segmental models has shown that they can achieve competitive speech recognition performance, using features based on deep neural frame classifiers. However, segmental models can be more challenging to train than standard frame-based approaches. While some segmental models have been successfully trained end to end, there is a lack of understanding of their training under different settings and with different losses. We investigate a model class based on recent successful approaches, consisting of a linear model that combines segmental features based on an LSTM frame classifier. Similarly to hybrid HMM-neural network models, segmental models of this class can be trained in two stages (frame classifier training followed by linear segmental model weight training), end to end (joint training of both frame classifier and linear weights), or with end-to-end fine-tuning after two-stage training. We study segmental models trained end to end with hinge loss, log loss, latent hinge loss, and marginal log loss. We consider several losses for the case where training alignments are available as well as where they are not. We find that in general, marginal log loss provides the most consistent strong performance without requiring ground-truth alignments. We also find that training with dropout is very important in obtaining good performance with end-to-end training. Finally, the best results are typically obtained by a combination of two-stage training and fine-tuning. version:1
arxiv-1610-02807 | Robust Bayesian Compressed sensing | http://arxiv.org/abs/1610.02807 | id:1610.02807 author:Qian Wan, Huiping Duan, Jun Fang, Hongbin Li category:stat.ML cs.LG  published:2016-10-10 summary:We consider the problem of robust compressed sensing whose objective is to recover a high-dimensional sparse signal from compressed measurements corrupted by outliers. A new sparse Bayesian learning method is developed for robust compressed sensing. The basic idea of the proposed method is to identify and remove the outliers from sparse signal recovery. To automatically identify the outliers, we employ a set of binary indicator hyperparameters to indicate which observations are outliers. These indicator hyperparameters are treated as random variables and assigned a beta process prior such that their values are confined to be binary. In addition, a Gaussian-inverse Gamma prior is imposed on the sparse signal to promote sparsity. Based on this hierarchical prior model, we develop a variational Bayesian method to estimate the indicator hyperparameters as well as the sparse signal. Simulation results show that the proposed method achieves a substantial performance improvement over existing robust compressed sensing techniques. version:2
arxiv-1610-06688 | Multispectral image denoising with optimized vector non-local mean filter | http://arxiv.org/abs/1610.06688 | id:1610.06688 author:Ahmed Ben Said, Rachid Hadjidj, Kamel Eddine Melkemi, Sebti Foufou category:cs.CV  published:2016-10-21 summary:Nowadays, many applications rely on images of high quality to ensure good performance in conducting their tasks. However, noise goes against this objective as it is an unavoidable issue in most applications. Therefore, it is essential to develop techniques to attenuate the impact of noise, while maintaining the integrity of relevant information in images. We propose in this work to extend the application of the Non-Local Means filter (NLM) to the vector case and apply it for denoising multispectral images. The objective is to benefit from the additional information brought by multispectral imaging systems. The NLM filter exploits the redundancy of information in an image to remove noise. A restored pixel is a weighted average of all pixels in the image. In our contribution, we propose an optimization framework where we dynamically fine tune the NLM filter parameters and attenuate its computational complexity by considering only pixels which are most similar to each other in computing a restored pixel. Filter parameters are optimized using Stein's Unbiased Risk Estimator (SURE) rather than using ad hoc means. Experiments have been conducted on multispectral images corrupted with additive white Gaussian noise and PSNR and similarity comparison with other approaches are provided to illustrate the efficiency of our approach in terms of both denoising performance and computation complexity. version:1
arxiv-1610-04841 | Translation Quality Estimation using Recurrent Neural Network | http://arxiv.org/abs/1610.04841 | id:1610.04841 author:Raj Nath Patel, Sasikumar M category:cs.CL  published:2016-10-16 summary:This paper describes our submission to the shared task on word/phrase level Quality Estimation (QE) in the First Conference on Statistical Machine Translation (WMT16). The objective of the shared task was to predict if the given word/phrase is a correct/incorrect (OK/BAD) translation in the given sentence. In this paper, we propose a novel approach for word level Quality Estimation using Recurrent Neural Network Language Model (RNN-LM) architecture. RNN-LMs have been found very effective in different Natural Language Processing (NLP) applications. RNN-LM is mainly used for vector space language modeling for different NLP problems. For this task, we modify the architecture of RNN-LM. The modified system predicts a label (OK/BAD) in the slot rather than predicting the word. The input to the system is a word sequence, similar to the standard RNN-LM. The approach is language independent and requires only the translated text for QE. To estimate the phrase level quality, we use the output of the word level QE system. version:2
arxiv-1610-06671 | Multi-view metric learning for multi-instance image classification | http://arxiv.org/abs/1610.06671 | id:1610.06671 author:Dewei Li, Yingjie Tian category:cs.CV  published:2016-10-21 summary:It is critical and meaningful to make image classification since it can help human in image retrieval and recognition, object detection, etc. In this paper, three-sides efforts are made to accomplish the task. First, visual features with bag-of-words representation, not single vector, are extracted to characterize the image. To improve the performance, the idea of multi-view learning is implemented and three kinds of features are provided, each one corresponds to a single view. The information from three views is complementary to each other, which can be unified together. Then a new distance function is designed for bags by computing the weighted sum of the distances between instances. The technique of metric learning is explored to construct a data-dependent distance metric to measure the relationships between instances, meanwhile between bags and images, more accurately. Last, a novel approach, called MVML, is proposed, which optimizes the joint probability that every image is similar with its nearest image. MVML learns multiple distance metrics, each one models a single view, to unifies the information from multiple views. The method can be solved by alternate optimization iteratively. Gradient ascent and positive semi-definite projection are utilized in the iterations. Distance comparisons verified that the new bag distance function is prior to previous functions. In model evaluation, numerical experiments show that MVML with multiple views performs better than single view condition, which demonstrates that our model can assemble the complementary information efficiently and measure the distance between images more precisely. Experiments on influence of parameters and instance number validate the consistency of the method. version:1
arxiv-1610-06669 | Scalable Pooled Time Series of Big Video Data from the Deep Web | http://arxiv.org/abs/1610.06669 | id:1610.06669 author:Chris Mattmann, Madhav Sharan category:cs.CV  published:2016-10-21 summary:We contribute a scalable implementation of Ryoo et al's Pooled Time Series algorithm from CVPR 2015. The updated algorithm has been evaluated on a large and diverse dataset of approximately 6800 videos collected from a crawl of the deep web related to human trafficking on DARPA's MEMEX effort. We describe the properties of Pooled Time Series and the motivation for using it to relate videos collected from the deep web. We highlight issues that we found while running Pooled Time Series on larger datasets and discuss solutions for those issues. Our solution centers are re-imagining Pooled Time Series as a Hadoop-based algorithm in which we compute portions of the eventual solution in parallel on large commodity clusters. We demonstrate that our new Hadoop-based algorithm works well on the 6800 video dataset and shares all of the properties described in the CVPR 2015 paper. We suggest avenues of future work in the project. version:1
arxiv-1610-06667 | Detecting Rainfall Onset Using Sky Images | http://arxiv.org/abs/1610.06667 | id:1610.06667 author:Soumyabrata Dev, Shilpa Manandhar, Yee Hui Lee, Stefan Winkler category:cs.CV physics.ao-ph  published:2016-10-21 summary:Ground-based sky cameras (popularly known as Whole Sky Imagers) are increasingly used now-a-days for continuous monitoring of the atmosphere. These imagers have higher temporal and spatial resolutions compared to conventional satellite images. In this paper, we use ground-based sky cameras to detect the onset of rainfall. These images contain additional information about cloud coverage and movement and are therefore useful for accurate rainfall nowcast. We validate our results using rain gauge measurement recordings and achieve an accuracy of 89% for correct detection of rainfall onset. version:1
arxiv-1610-06666 | Short-term prediction of localized cloud motion using ground-based sky imagers | http://arxiv.org/abs/1610.06666 | id:1610.06666 author:Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler category:cs.CV  published:2016-10-21 summary:Fine-scale short-term cloud motion prediction is needed for several applications, including solar energy generation and satellite communications. In tropical regions such as Singapore, clouds are mostly formed by convection; they are very localized, and evolve quickly. We capture hemispherical images of the sky at regular intervals of time using ground-based cameras. They provide a high resolution and localized cloud images. We use two successive frames to compute optical flow and predict the future location of clouds. We achieve good prediction accuracy for a lead time of up to 5 minutes. version:1
arxiv-1610-06665 | On the Convergence of Stochastic Gradient MCMC Algorithms with High-Order Integrators | http://arxiv.org/abs/1610.06665 | id:1610.06665 author:Changyou Chen, Nan Ding, Lawrence Carin category:stat.ML  published:2016-10-21 summary:Recent advances in Bayesian learning with large-scale data have witnessed emergence of stochastic gradient MCMC algorithms (SG-MCMC), such as stochastic gradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian MCMC (SGHMC), and the stochastic gradient thermostat. While finite-time convergence properties of the SGLD with a 1st-order Euler integrator have recently been studied, corresponding theory for general SG-MCMCs has not been explored. In this paper we consider general SG-MCMCs with high-order integrators, and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures. Our theoretical results show faster convergence rates and more accurate invariant measures for SG-MCMCs with higher-order integrators. For example, with the proposed efficient 2nd-order symmetric splitting integrator, the {\em mean square error} (MSE) of the posterior average for the SGHMC achieves an optimal convergence rate of $L^{-4/5}$ at $L$ iterations, compared to $L^{-2/3}$ for the SGHMC and SGLD with 1st-order Euler integrators. Furthermore, convergence results of decreasing-step-size SG-MCMCs are also developed, with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence. Experiments on both synthetic and real datasets verify our theory, and show advantages of the proposed method in two large-scale real applications. version:1
arxiv-1610-06664 | Stochastic Gradient MCMC with Stale Gradients | http://arxiv.org/abs/1610.06664 | id:1610.06664 author:Changyou Chen, Nan Ding, Chunyuan Li, Yizhe Zhang, Lawrence Carin category:stat.ML cs.LG  published:2016-10-21 summary:Stochastic gradient MCMC (SG-MCMC) has played an important role in large-scale Bayesian learning, with well-developed theoretical convergence properties. In such applications of SG-MCMC, it is becoming increasingly popular to employ distributed systems, where stochastic gradients are computed based on some outdated parameters, yielding what are termed stale gradients. While stale gradients could be directly used in SG-MCMC, their impact on convergence properties has not been well studied. In this paper we develop theory to show that while the bias and MSE of an SG-MCMC algorithm depend on the staleness of stochastic gradients, its estimation variance (relative to the expected estimate, based on a prescribed number of samples) is independent of it. In a simple Bayesian distributed system with SG-MCMC, where stale gradients are computed asynchronously by a set of workers, our theory indicates a linear speedup on the decrease of estimation variance w.r.t. the number of workers. Experiments on synthetic data and deep neural networks validate our theory, demonstrating the effectiveness and scalability of SG-MCMC with stale gradients. version:1
arxiv-1610-06633 | Novelty Learning via Collaborative Proximity Filtering | http://arxiv.org/abs/1610.06633 | id:1610.06633 author:Arun Kumar, Paul Schrater category:cs.HC cs.LG  published:2016-10-21 summary:The vast majority of recommender systems model preferences as static or slowly changing due to observable user experience. However, spontaneous changes in user preferences are ubiquitous in many domains like media consumption and key factors that drive changes in preferences are not directly observable. These latent sources of preference change pose new challenges. When systems do not track and adapt to users' tastes, users lose confidence and trust, increasing the risk of user churn. We meet these challenges by developing a model of novelty preferences that learns and tracks latent user tastes. We combine three innovations: a new measure of item similarity based on patterns of consumption co-occurrence; model for {\em spontaneous} changes in preferences; and a learning agent that tracks each user's dynamic preferences and learns individualized policies for variety. The resulting framework adaptively provides users with novelty tailored to their preferences for change per se. version:1
arxiv-1610-06920 | Bit-pragmatic Deep Neural Network Computing | http://arxiv.org/abs/1610.06920 | id:1610.06920 author:J. Albericio, P. Judd, A. Delm√°s, S. Sharify, A. Moshovos category:cs.LG cs.AI cs.AR cs.CV  published:2016-10-20 summary:We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragmatic (PRA), an architecture that exploits it improving performance and energy efficiency. The source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product [1]. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms using a) on-the-fly conversion of the multiplicator representation into an explicit list of powers of two, and b) hybrid bit-parallel multplicand/bit-serial multiplicator processing units. PRA exploits two sources of ineffectual computations: 1) the aforementioned zero product terms which are the result of the lack of explicitness in the multiplicator representation, and 2) the excess in the representation precision used for both multiplicants and multiplicators, e.g., [2]. Measurements demonstrate that for the convolutional layers, a straightforward variant of PRA improves performance by 2.6x over the DaDiaNao (DaDN) accelerator [3] and by 1.4x over STR [4]. Similarly, PRA improves energy efficiency by 28% and 10% on average compared to DaDN and STR. An improved cross lane synchronication scheme boosts performance improvements to 3.1x over DaDN. Finally, Pragmatic benefits persist even with an 8-bit quantized representation [5]. version:1
arxiv-1610-03899 | Generalization bound for kernel similarity learning | http://arxiv.org/abs/1610.03899 | id:1610.03899 author:Michael Rabadi category:stat.ML cs.LG  published:2016-10-12 summary:Similarity learning has received a large amount of interest and is an important tool for many scientific and industrial applications. In this framework, we wish to infer the distance (similarity) between points with respect to an arbitrary distance function $d$. Here, we formulate the problem as a regression from a feature space $\mathcal{X}$ to an arbitrary vector space $\mathcal{Y}$, where the Euclidean distance is proportional to $d$. We then give Rademacher complexity bounds on the generalization error. We find that with high probability, the complexity is bounded by the maximum of the radius of $\mathcal{X}$ and the radius of $\mathcal{Y}$. version:2
arxiv-1610-06603 | Combinatorial Multi-Armed Bandit with General Reward Functions | http://arxiv.org/abs/1610.06603 | id:1610.06603 author:Wei Chen, Wei Hu, Fu Li, Jian Li, Yu Liu, Pinyan Lu category:cs.LG cs.DS stat.ML  published:2016-10-20 summary:In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the $\max()$ function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (UCB) technique, do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve $O(\log T)$ distribution-dependent regret and $\tilde{O}(\sqrt{T})$ distribution-independent regret, where $T$ is the time horizon. We apply our results to the $K$-MAX problem and expected utility maximization problems. In particular, for $K$-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first $\tilde{O}(\sqrt T)$ bound on the $(1-\epsilon)$-approximation regret of its online problem, for any $\epsilon>0$. version:1
arxiv-1610-06551 | Nonlinear Structural Vector Autoregressive Models for Inferring Effective Brain Network Connectivity | http://arxiv.org/abs/1610.06551 | id:1610.06551 author:Yanning Shen, Brian Baingana, Georgios B. Giannakis category:stat.AP stat.ML  published:2016-10-20 summary:Structural equation models (SEMs) and vector autoregressive models (VARMs) are two broad families of approaches that have been shown useful in effective brain connectivity studies. While VARMs postulate that a given region of interest in the brain is directionally connected to another one by virtue of time-lagged influences, SEMs assert that causal dependencies arise due to contemporaneous effects, and may even be adopted when nodal measurements are not necessarily multivariate time series. To unify these complementary perspectives, linear structural vector autoregressive models (SVARMs) that leverage both contemporaneous and time-lagged nodal data have recently been put forth. Albeit simple and tractable, linear SVARMs are quite limited since they are incapable of modeling nonlinear dependencies between neuronal time series. To this end, the overarching goal of the present paper is to considerably broaden the span of linear SVARMs by capturing nonlinearities through kernels, which have recently emerged as a powerful nonlinear modeling framework in canonical machine learning tasks, e.g., regression, classification, and dimensionality reduction. The merits of kernel-based methods are extended here to the task of learning the effective brain connectivity, and an efficient regularized estimator is put forth to leverage the edge sparsity inherent to real-world complex networks. Judicious kernel choice from a preselected dictionary of kernels is also addressed using a data-driven approach. Extensive numerical tests on ECoG data captured through a study on epileptic seizures demonstrate that it is possible to unveil previously unknown causal links between brain regions of interest. version:1
arxiv-1610-06550 | Neural Machine Translation with Characters and Hierarchical Encoding | http://arxiv.org/abs/1610.06550 | id:1610.06550 author:Alexander Rosenberg Johansen, Jonas Meinertz Hansen, Elias Khazen Obeid, Casper Kaae S√∏nderby, Ole Winther category:cs.CL  published:2016-10-20 summary:Most existing Neural Machine Translation models use groups of characters or whole words as their unit of input and output. We propose a model with a hierarchical char2word encoder, that takes individual characters both as input and output. We first argue that this hierarchical representation of the character encoder reduces computational complexity, and show that it improves translation performance. Secondly, by qualitatively studying attention plots from the decoder we find that the model learns to compress common words into a single embedding whereas rare words, such as names and places, are represented character by character. version:1
arxiv-1610-06545 | Revisiting Classifier Two-Sample Tests for GAN Evaluation and Causal Discovery | http://arxiv.org/abs/1610.06545 | id:1610.06545 author:David Lopez-Paz, Maxime Oquab category:stat.ML  published:2016-10-20 summary:The goal of two-sample tests is to decide whether two probability distributions, denoted by $P$ and $Q$, are equal. One simple method to construct flexible two-sample tests is to use binary classifiers. More specifically, pair $n$ random samples drawn from $P$ with a positive label, and pair $n$ random samples drawn from $Q$ with a negative label. If the null hypothesis "$P = Q$" is true, the classification accuracy of a binary classifier on a hold-out subset of these data should remain near chance-level. Since the hold-out classification accuracy is an average of independent random variables under the null hypothesis, the two-sample test statistic follows a Binomial distribution. Furthermore, the decision boundary of our binary classifier provides interpretation on the differences between $P$ and $Q$. In particular this boundary can be useful to analyze which samples were correctly or incorrectly labeled by the classifier, with the least or most confidence. The goal of this paper is to revive the interest in classifier two-sample tests for a variety of applications, including independence testing, generative model evaluation, and causal discovery. To this end, we study their fundamentals, review prior literature on their applications, compare their performance against alternative state-of-the-art two-sample tests, and propose their use to evaluate generative adversarial network models applied to image synthesis. As a novel application of our research, we propose the use of conditional generative adversarial networks, together with classifier two-sample tests, to achieve state-of-the-art causal discovery. version:1
arxiv-1610-06542 | Lexicons and Minimum Risk Training for Neural Machine Translation: NAIST-CMU at WAT2016 | http://arxiv.org/abs/1610.06542 | id:1610.06542 author:Graham Neubig category:cs.CL  published:2016-10-20 summary:This year, the Nara Institute of Science and Technology (NAIST)/Carnegie Mellon University (CMU) submission to the Japanese-English translation track of the 2016 Workshop on Asian Translation was based on attentional neural machine translation (NMT) models. In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score. As a result, our system achieved the highest translation evaluation scores for the task. version:1
arxiv-1610-06540 | Jointly Learning to Align and Convert Graphemes to Phonemes with Neural Attention Models | http://arxiv.org/abs/1610.06540 | id:1610.06540 author:Shubham Toshniwal, Karen Livescu category:cs.CL cs.AI  published:2016-10-20 summary:We propose an attention-enabled encoder-decoder model for the problem of grapheme-to-phoneme conversion. Most previous work has tackled the problem via joint sequence models that require explicit alignments for training. In contrast, the attention-enabled encoder-decoder model allows for jointly learning to align and convert characters to phonemes. We explore different types of attention models, including global and local attention, and our best models achieve state-of-the-art results on three standard data sets (CMUDict, Pronlex, and NetTalk). version:1
arxiv-1610-06534 | Autonomous Racing using Learning Model Predictive Control | http://arxiv.org/abs/1610.06534 | id:1610.06534 author:Ugo Rosolia, Ashwin Carvalho, Francesco Borrelli category:cs.LG math.OC  published:2016-10-20 summary:A novel learning Model Predictive Control technique is applied to the autonomous racing problem. The goal of the controller is to minimize the time to complete a lap. The proposed control strategy uses the data from previous laps to improve its performance while satisfying safety requirements. Moreover, a system identification technique is proposed to estimate the vehicle dynamics. Simulation results with the high fidelity simulator software CarSim show the effectiveness of the proposed control scheme. version:1
arxiv-1610-06525 | ChoiceRank: Identifying Preferences from Node Traffic in Networks | http://arxiv.org/abs/1610.06525 | id:1610.06525 author:Lucas Maystre, Matthias Grossglauser category:stat.ML cs.LG cs.SI  published:2016-10-20 summary:Understanding how users navigate in a network is of high interest in many applications. We consider a setting where only aggregate node-level traffic is observed and tackle the task of learning edge transition probabilities. We cast it as a preference learning problem, and we study a model where choices follow Luce's axiom. In this case, the $O(n)$ marginal counts of node visits are a sufficient statistic for the $O(n^2)$ transition probabilities. We show how to make the inference problem well-posed regardless of the network's structure, and we present ChoiceRank, an iterative algorithm that scales to networks that contains billions of nodes and edges. We apply the model to a month-long clickstream of the English Wikipedia and one year of rides on New York City's bicycle-sharing system. In both cases, we successfully recover the transition probabilities using only the network structure and marginal (node-level) traffic data. version:1
arxiv-1610-06510 | Learning variable length units for SMT between related languages via Byte Pair Encoding | http://arxiv.org/abs/1610.06510 | id:1610.06510 author:Anoop Kunchukuttan, Pushpak Bhattacharyya category:cs.CL  published:2016-10-20 summary:We explore the use of segments learnt using Byte Pair Encoding (referred to as BPE units) as basic units for statistical machine translation between related languages and compare it with orthographic syllables, which are currently the best performing basic units for this translation task. BPE identifies the most frequent character sequences as basic units, while orthographic syllables are linguistically motivated pseudo-syllables. We show that BPE units outperform orthographic syllables as units of translation, showing up to 11% increase in BLEU scores. In addition, BPE can be applied to any writing system, while orthographic syllables can be used only for languages whose writing systems use vowel representations. We show that BPE units outperform word and morpheme level units for translation involving languages like Urdu, Japanese whose writing systems do not use vowels (either completely or partially). Across many language pairs, spanning multiple language families and types of writing systems, we show that translation with BPE segments outperforms orthographic syllables, especially for morphologically rich languages. version:1
arxiv-1610-06498 | Authorship Attribution Based on Life-Like Network Automata | http://arxiv.org/abs/1610.06498 | id:1610.06498 author:Jeaneth Machicao, Edilson A. Corr√™a Jr., Gisele H. B. Miranda, Diego R. Amancio, Odemir M. Bruno category:cs.CL  published:2016-10-20 summary:The authorship attribution is a problem of considerable practical and technical interest. Several methods have been designed to infer the authorship of disputed documents in multiple contexts. While traditional statistical methods based solely on word counts and related measurements have provided a simple, yet effective solution in particular cases; they are prone to manipulation. Recently, texts have been successfully modeled as networks, where words are represented by nodes linked according to textual similarity measurements. Such models are useful to identify informative topological patterns for the authorship recognition task. However, there is no consensus on which measurements should be used. Thus, we proposed a novel method to characterize text networks, by considering both topological and dynamical aspects of networks. Using concepts and methods from cellular automata theory, we devised a strategy to grasp informative spatio-temporal patterns from this model. Our experiments revealed an outperformance over traditional analysis relying only on topological measurements. Remarkably, we have found a dependence of pre-processing steps (such as the lemmatization) on the obtained results, a feature that has mostly been disregarded in related works. The optimized results obtained here pave the way for a better characterization of textual networks. version:1
arxiv-1610-06494 | An Image Dataset of Text Patches in Everyday Scenes | http://arxiv.org/abs/1610.06494 | id:1610.06494 author:Ahmed Ibrahim, A. Lynn Abbott, Mohamed E. Hussein category:cs.CV  published:2016-10-20 summary:This paper describes a dataset containing small images of text from everyday scenes. The purpose of the dataset is to support the development of new automated systems that can detect and analyze text. Although much research has been devoted to text detection and recognition in scanned documents, relatively little attention has been given to text detection in other types of images, such as photographs that are posted on social-media sites. This new dataset, known as COCO-Text-Patch, contains approximately 354,000 small images that are each labeled as "text" or "non-text". This dataset particularly addresses the problem of text verification, which is an essential stage in the end-to-end text detection and recognition pipeline. In order to evaluate the utility of this dataset, it has been used to train two deep convolution neural networks to distinguish text from non-text. One network is inspired by the GoogLeNet architecture, and the second one is based on CaffeNet. Accuracy levels of 90.2% and 90.9% were obtained using the two networks, respectively. All of the images, source code, and deep-learning trained models described in this paper will be publicly available version:1
arxiv-1610-06492 | Utilization of Deep Reinforcement Learning for saccadic-based object visual search | http://arxiv.org/abs/1610.06492 | id:1610.06492 author:Tomasz Kornuta, Kamil Rocki category:cs.CV cs.LG  published:2016-10-20 summary:The paper focuses on the problem of learning saccades enabling visual object search. The developed system combines reinforcement learning with a neural network for learning to predict the possible outcomes of its actions. We validated the solution in three types of environment consisting of (pseudo)-randomly generated matrices of digits. The experimental verification is followed by the discussion regarding elements required by systems mimicking the fovea movement and possible further research directions. version:1
arxiv-1610-06488 | An Evolving Neuro-Fuzzy System with Online Learning/Self-learning | http://arxiv.org/abs/1610.06488 | id:1610.06488 author:Yevgeniy V. Bodyanskiy, Oleksii K. Tyshchenko, Anastasiia O. Deineko category:cs.AI cs.NE  published:2016-10-20 summary:An architecture of a new neuro-fuzzy system is proposed. The basic idea of this approach is to tune both synaptic weights and membership functions with the help of the supervised learning and self-learning paradigms. The approach to solving the problem has to do with evolving online neuro-fuzzy systems that can process data under uncertainty conditions. The results prove the effectiveness of the developed architecture and the learning procedure. version:1
arxiv-1610-06486 | Adaptive Forecasting of Non-Stationary Nonlinear Time Series Based on the Evolving Weighted Neuro-Neo-Fuzzy-ANARX-Model | http://arxiv.org/abs/1610.06486 | id:1610.06486 author:Zhengbing Hu, Yevgeniy V. Bodyanskiy, Oleksii K. Tyshchenko, Olena O. Boiko category:cs.AI cs.NE  published:2016-10-20 summary:An evolving weighted neuro-neo-fuzzy-ANARX model and its learning procedures are introduced in the article. This system is basically used for time series forecasting. This system may be considered as a pool of elements that process data in a parallel manner. The proposed evolving system may provide online processing data streams. version:1
arxiv-1610-06485 | A Multidimensional Cascade Neuro-Fuzzy System with Neuron Pool Optimization in Each Cascade | http://arxiv.org/abs/1610.06485 | id:1610.06485 author:Yevgeniy V. Bodyanskiy, Oleksii K. Tyshchenko, Daria S. Kopaliani category:cs.AI cs.NE  published:2016-10-20 summary:A new architecture and learning algorithms for the multidimensional hybrid cascade neural network with neuron pool optimization in each cascade are proposed in this paper. The proposed system differs from the well-known cascade systems in its capability to process multidimensional time series in an online mode, which makes it possible to process non-stationary stochastic and chaotic signals with the required accuracy. Compared to conventional analogs, the proposed system provides computational simplicity and possesses both tracking and filtering capabilities. version:1
arxiv-1610-06484 | An Evolving Cascade System Based on A Set Of Neo Fuzzy Nodes | http://arxiv.org/abs/1610.06484 | id:1610.06484 author:Zhengbing Hu, Yevgeniy V. Bodyanskiy, Oleksii K. Tyshchenko, Olena O. Boiko category:cs.AI cs.NE  published:2016-10-20 summary:Neo-fuzzy elements are used as nodes for an evolving cascade system. The proposed system can tune both its parameters and architecture in an online mode. It can be used for solving a wide range of Data Mining tasks (namely time series forecasting). The evolving cascade system with neo-fuzzy nodes can process rather large data sets with high speed and effectiveness. version:1
arxiv-1610-06483 | An Extended Neo-Fuzzy Neuron and its Adaptive Learning Algorithm | http://arxiv.org/abs/1610.06483 | id:1610.06483 author:Yevgeniy V. Bodyanskiy, Oleksii K. Tyshchenko, Daria S. Kopaliani category:cs.AI cs.NE  published:2016-10-20 summary:A modification of the neo-fuzzy neuron is proposed (an extended neo-fuzzy neuron (ENFN)) that is characterized by improved approximating properties. An adaptive learning algorithm is proposed that has both tracking and smoothing properties. An ENFN distinctive feature is its computational simplicity compared to other artificial neural networks and neuro-fuzzy systems. version:1
arxiv-1610-06773 | Variational approximation of molecular kinetics from short off-equilibrium simulations | http://arxiv.org/abs/1610.06773 | id:1610.06773 author:Hao Wu, Feliks N√ºske, Fabian Paul, Stefan Klus, Peter Koltai, Frank No√© category:stat.ML physics.bio-ph physics.chem-ph q-bio.BM  published:2016-10-20 summary:Markov state models (MSMs) and Master equation models are popular approaches to approximate molecular kinetics, equilibria, metastable states, and reaction coordinates in terms of a state space discretization usually obtained by clustering. Recently, a powerful generalization of MSMs has been introduced, the variational approach of conformation dynamics (VAC) and its special case the time-lagged independent component analysis (TICA), which allow us to approximate molecular kinetics and reaction coordinates by linear combinations of smooth basis functions or order parameters. While MSMs can be learned from trajectories whose starting points are not sampled from an equilibrium ensemble, TICA and VAC have as yet not enjoyed this property, and thus previous TICA/VAC estimates have been strongly biased when used with ensembles of short trajectories. Here, we employ Koopman operator theory and ideas from dynamic mode decomposition (DMD) to show how TICA/VAC can be used to estimate the unbiased equilibrium distribution from short-trajectory data and further this result in order to construct unbiased estimators for expectations, covariance matrices, TICA/VAC eigenvectors, relaxation timescales, and reaction coordinates. version:1
arxiv-1610-06475 | ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras | http://arxiv.org/abs/1610.06475 | id:1610.06475 author:Raul Mur-Artal, Juan D. Tardos category:cs.RO cs.CV  published:2016-10-20 summary:We present ORB-SLAM2 a complete SLAM system for monocular, stereo and RGB-D cameras, including map reuse, loop closing and relocalization capabilities. The system works in real-time in standard CPUs in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our backend based on Bundle Adjustment with monocular and stereo observations allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches to map points that allow for zero-drift localization. The evaluation in 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields. version:1
arxiv-1610-06462 | Gaussian process modeling in approximate Bayesian computation to estimate horizontal gene transfer in bacteria | http://arxiv.org/abs/1610.06462 | id:1610.06462 author:Marko J√§rvenp√§√§, Michael Gutmann, Aki Vehtari, Pekka Marttinen category:stat.ML stat.AP stat.ME  published:2016-10-20 summary:Approximate Bayesian computation (ABC) can be used for model fitting when the likelihood function is intractable but simulating from the model is feasible. However, even a single evaluation of a complex model may take several hours, limiting the number of model evaluations available. Modeling the discrepancy between the simulated and observed data using a Gaussian process (GP) can be used to reduce the number of model evaluations required by ABC, but the sensitivity of this approach to a specific GP formulation has not been thoroughly investigated. We begin with a comprehensive empirical evaluation of using GPs in ABC, including various transformations of the discrepancies and two novel GP formulations. Our results indicate the choice of GP may significantly affect the accuracy of the estimated posterior distribution. Selection of an appropriate GP model is thus important. We define expected utility to measure the accuracy of classifying discrepancies below or above the ABC threshold, and show that by using this utility, the GP model selection step can be made automatic. Finally, based on the understanding gained with toy examples, we fit a population genetic model for bacteria, providing insight into horizontal gene transfer events within the population and from external origins. version:1
arxiv-1610-06461 | Efficient Estimation of Compressible State-Space Models with Application to Calcium Signal Deconvolution | http://arxiv.org/abs/1610.06461 | id:1610.06461 author:Abbas Kazemipour, Ji Liu, Patrick Kanold, Min Wu, Behtash Babadi category:stat.ML cs.CV cs.IT math.DS math.IT math.ST stat.TH  published:2016-10-20 summary:In this paper, we consider linear state-space models with compressible innovations and convergent transition matrices in order to model spatiotemporally sparse transient events. We perform parameter and state estimation using a dynamic compressed sensing framework and develop an efficient solution consisting of two nested Expectation-Maximization (EM) algorithms. Under suitable sparsity assumptions on the innovations, we prove recovery guarantees and derive confidence bounds for the state estimates. We provide simulation studies as well as application to spike deconvolution from calcium imaging data which verify our theoretical results and show significant improvement over existing algorithms. version:1
arxiv-1610-06454 | Reasoning with Memory Augmented Neural Networks for Language Comprehension | http://arxiv.org/abs/1610.06454 | id:1610.06454 author:Tsendsuren Munkhdalai, Hong Yu category:cs.CL cs.AI cs.NE stat.ML  published:2016-10-20 summary:Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets. version:1
arxiv-1610-06453 | Change-point Detection Methods for Body-Worn Video | http://arxiv.org/abs/1610.06453 | id:1610.06453 author:Stephanie Allen, David Madras, Ye Ye, Greg Zanotti category:cs.CV cs.LG stat.ML  published:2016-10-20 summary:Body-worn video (BWV) cameras are increasingly utilized by police departments to provide a record of police-public interactions. However, large-scale BWV deployment produces terabytes of data per week, necessitating the development of effective computational methods to identify salient changes in video. In work carried out at the 2016 RIPS program at IPAM, UCLA, we present a novel two-stage framework for video change-point detection. First, we employ state-of-the-art machine learning methods including convolutional neural networks and support vector machines for scene classification. We then develop and compare change-point detection algorithms utilizing mean squared-error minimization, forecasting methods, hidden Markov models, and maximum likelihood estimation to identify noteworthy changes. We test our framework on detection of vehicle exits and entrances in a BWV data set provided by the Los Angeles Police Department and achieve over 90% recall and nearly 70% precision -- demonstrating robustness to rapid scene changes, extreme luminance differences, and frequent camera occlusions. version:1
arxiv-1610-06449 | Exploiting inter-image similarity and ensemble of extreme learners for fixation prediction using deep features | http://arxiv.org/abs/1610.06449 | id:1610.06449 author:Hamed R. -Tavakoli, Ali Borji, Jorma Laaksonen, Esa Rahtu category:cs.CV cs.AI  published:2016-10-20 summary:This paper presents a novel fixation prediction and saliency modeling framework based on inter-image similarities and ensemble of Extreme Learning Machines (ELM). The proposed framework is inspired by two observations, 1) the contextual information of a scene along with low-level visual cues modulates attention, 2) the influence of scene memorability on eye movement patterns caused by the resemblance of a scene to a former visual experience. Motivated by such observations, we develop a framework that estimates the saliency of a given image using an ensemble of extreme learners, each trained on an image similar to the input image. That is, after retrieving a set of similar images for a given image, a saliency predictor is learnt from each of the images in the retrieved image set using an ELM, resulting in an ensemble. The saliency of the given image is then measured in terms of the mean of predicted saliency value by the ensemble's members. version:1
arxiv-1610-06447 | Regularized Optimal Transport and the Rot Mover's Distance | http://arxiv.org/abs/1610.06447 | id:1610.06447 author:Arnaud Dessein, Nicolas Papadakis, Jean-Luc Rouas category:stat.ML cs.LG  published:2016-10-20 summary:This paper presents a unified framework for smooth convex regularization of discrete optimal transport problems. In this context, the regularized optimal transport turns out to be equivalent to a matrix nearness problem with respect to Bregman divergences. Our framework thus naturally generalizes a previously proposed regularization based on the Boltzmann-Shannon entropy related to the Kullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We call the regularized optimal transport distance the rot mover's distance in reference to the classical earth mover's distance. We develop two generic schemes that we respectively call the alternate scaling algorithm and the non-negative alternate scaling algorithm, to compute efficiently the regularized optimal plans depending on whether the domain of the regularizer lies within the non-negative orthant or not. These schemes are based on Dykstra's algorithm with alternate Bregman projections, and further exploit the Newton-Raphson method for separable divergences. We enhance the separable case with a sparse extension to deal with high data dimensions. We also instantiate our proposed framework and discuss the inherent specificities for well-known regularizers and statistical divergences in the machine learning and information geometry communities. Finally, we demonstrate our methods with an experiment on synthetic input data that illustrates the effect of different regularizers and penalties on the output solutions. version:1
arxiv-1610-06434 | Kernel Alignment for Unsupervised Transfer Learning | http://arxiv.org/abs/1610.06434 | id:1610.06434 author:Ievgen Redko, Youn√®s Bennani category:stat.ML cs.LG  published:2016-10-20 summary:The ability of a human being to extrapolate previously gained knowledge to other domains inspired a new family of methods in machine learning called transfer learning. Transfer learning is often based on the assumption that objects in both target and source domains share some common feature and/or data space. In this paper, we propose a simple and intuitive approach that minimizes iteratively the distance between source and target task distributions by optimizing the kernel target alignment (KTA). We show that this procedure is suitable for transfer learning by relating it to Hilbert-Schmidt Independence Criterion (HSIC) and Quadratic Mutual Information (QMI) maximization. We run our method on benchmark computer vision data sets and show that it can outperform some state-of-art methods. version:1
arxiv-1610-05838 | CuMF_SGD: Fast and Scalable Matrix Factorization | http://arxiv.org/abs/1610.05838 | id:1610.05838 author:Xiaolong Xie, Wei Tan, Liana L. Fong, Yun Liang category:cs.LG cs.NA  published:2016-10-19 summary:Matrix factorization (MF) has been widely used in e.g., recommender systems, topic modeling and word embedding. Stochastic gradient descent (SGD) is popular in solving MF problems because it can deal with large data sets and is easy to do incremental learning. We observed that SGD for MF is memory bound. Meanwhile, single-node CPU systems with caching performs well only for small data sets; distributed systems have higher aggregated memory bandwidth but suffer from relatively slow network connection. This observation inspires us to accelerate MF by utilizing GPUs's high memory bandwidth and fast intra-node connection. We present cuMF_SGD, a CUDA-based SGD solution for large-scale MF problems. On a single CPU, we design two workload schedule schemes, i.e., batch-Hogwild! and wavefront-update that fully exploit the massive amount of cores. Especially, batch-Hogwild! as a vectorized version of Hogwild! overcomes the issue of memory discontinuity. We also develop highly-optimized kernels for SGD update, leveraging cache, warp-shuffle instructions and half-precision floats. We also design a partition scheme to utilize multiple GPUs while addressing the well-known convergence issue when parallelizing SGD. On three data sets with only one Maxwell or Pascal GPU, cuMF_SGD runs 3.1X-28.2X as fast compared with state-of-art CPU solutions on 1-64 CPU nodes. Evaluations also show that cuMF_SGD scales well on multiple GPUs in large data sets. version:2
arxiv-1610-06402 | A Growing Long-term Episodic & Semantic Memory | http://arxiv.org/abs/1610.06402 | id:1610.06402 author:Marc Pickett, Rami Al-Rfou, Louis Shao, Chris Tar category:cs.AI cs.LG cs.NE  published:2016-10-20 summary:The long-term memory of most connectionist systems lies entirely in the weights of the system. Since the number of weights is typically fixed, this bounds the total amount of knowledge that can be learned and stored. Though this is not normally a problem for a neural network designed for a specific task, such a bound is undesirable for a system that continually learns over an open range of domains. To address this, we describe a lifelong learning system that leverages a fast, though non-differentiable, content-addressable memory which can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge over many episodes for an unbounded number of domains. This opens the door for investigation into transfer learning, and leveraging prior knowledge that has been learned over a lifetime of experiences to new domains. version:1
arxiv-1610-06370 | Clinical Text Prediction with Numerically Grounded Conditional Language Models | http://arxiv.org/abs/1610.06370 | id:1610.06370 author:Georgios P. Spithourakis, Steffen E. Petersen, Sebastian Riedel category:cs.CL cs.HC cs.NE  published:2016-10-20 summary:Assisted text input techniques can save time and effort and improve text quality. In this paper, we investigate how grounded and conditional extensions to standard neural language models can bring improvements in the tasks of word prediction and completion. These extensions incorporate a structured knowledge base and numerical values from the text into the context used to predict the next word. Our automated evaluation on a clinical dataset shows extended models significantly outperform standard models. Our best system uses both conditioning and grounding, because of their orthogonal benefits. For word prediction with a list of 5 suggestions, it improves recall from 25.03% to 71.28% and for word completion it improves keystroke savings from 34.35% to 44.81%, where theoretical bound for this dataset is 58.78%. We also perform a qualitative investigation of how models with lower perplexity occasionally fare better at the tasks. We found that at test time numbers have more influence on the document level than on individual word probabilities. version:1
arxiv-1610-06368 | Retrieving challenging vessel connections in retinal images by line co-occurrence statistics | http://arxiv.org/abs/1610.06368 | id:1610.06368 author:Samaneh Abbasi-Sureshjani, Jiong Zhang, Remco Duits, Bart ter Haar Romeny category:cs.CV  published:2016-10-20 summary:Natural images contain often curvilinear structures, which might be disconnected, or partly occluded. Recovering the missing connection of disconnected structures is an open issue and needs appropriate geometric reasoning. We propose to find line co-occurrence statistics from the centerlines of blood vessels in retinal images and show its remarkable similarity to a well-known probabilistic model for the connectivity pattern in the primary visual cortex. Furthermore, the probabilistic model is trained from the data via statistics and used for automated grouping of interrupted vessels in a spectral clustering based approach. Several challenging image patches are investigated around junction points, where successful results indicate the perfect match of the trained model to the profiles of blood vessels in retinal images. Also, comparisons among several statistical models obtained from different datasets reveals their high similarity i.e., they are independent of the dataset. On top of that, the best approximation of the statistical model with the symmetrized extension of the probabilistic model on the projective line bundle is found with a least square error smaller than 2%. Apparently, the direction process on the projective line bundle is a good continuation model for vessels in retinal images. version:1
arxiv-1610-05108 | The xyz algorithm for fast interaction search in high-dimensional data | http://arxiv.org/abs/1610.05108 | id:1610.05108 author:Gian-Andrea Thanei, Nicolai Meinshausen, Rajen D. Shah category:stat.ML stat.CO 62-04  published:2016-10-17 summary:When performing regression on a dataset with $p$ variables, it is often of interest to go beyond using main linear effects and include interactions as products between individual variables. For small-scale problems, these interactions can be computed explicitly but this leads to a computational complexity of at least $\mathcal{O}(p^2)$ if done naively. This cost can be prohibitive if $p$ is very large. We introduce a new randomised algorithm that is able to discover interactions with high probability and under mild conditions has a runtime that is subquadratic in $p$. We show that strong interactions can be discovered in almost linear time, whilst finding weaker interactions requires $\mathcal{O}(p^\alpha)$ operations for $1 < \alpha < 2$ depending on their strength. The underlying idea is to transform interaction search into a closestpair problem which can be solved efficiently in subquadratic time. The algorithm is called $\mathit{xyz}$ and is implemented in the language R. We demonstrate its efficiency for application to genome-wide association studies, where more than $10^{11}$ interactions can be screened in under $280$ seconds with a single-core $1.2$ GHz CPU. version:2
arxiv-1610-06283 | Deep Neural Networks for Improved, Impromptu Trajectory Tracking of Quadrotors | http://arxiv.org/abs/1610.06283 | id:1610.06283 author:Qiyang Li, Jingxing Qian, Zining Zhu, Xuchan Bao, Mohamed K. Helwa, Angela P. Schoellig category:cs.RO cs.LG cs.NE cs.SY  published:2016-10-20 summary:Trajectory tracking control for quadrotors is important for applications ranging from surveying and inspection, to film making. However, designing and tuning classical controllers, such as proportional-integral-derivative (PID) controllers, to achieve high tracking precision can be time-consuming and difficult, due to hidden dynamics and other non-idealities. The Deep Neural Network (DNN), with its superior capability of approximating abstract, nonlinear functions, proposes a novel approach for enhancing trajectory tracking control. This paper presents a DNN-based algorithm that improves the tracking performance of a classical feedback controller. Given a desired trajectory, the DNNs provide a tailored input to the controller based on their gained experience. The input aims to achieve a unity map between the desired and the output trajectory. The motivation for this work is an interactive "fly-as-you-draw" application, in which a user draws a trajectory on a mobile device, and a quadrotor instantly flies that trajectory with the DNN-enhanced control system. Experimental results demonstrate that the proposed approach improves the tracking precision for user-drawn trajectories after the DNNs are trained on selected periodic trajectories, suggesting the method's potential in real-world applications. Tracking errors are reduced by around 40-50 % for both training and testing trajectories from users, highlighting the DNNs' capability of generalizing knowledge. version:1
arxiv-1610-06276 | Modeling Scalability of Distributed Machine Learning | http://arxiv.org/abs/1610.06276 | id:1610.06276 author:Alexander Ulanov, Andrey Simanovsky, Manish Marwah category:cs.LG cs.DC  published:2016-10-20 summary:Present day machine learning is computationally intensive and processes large amounts of data. It is implemented in a distributed fashion in order to address these scalability issues. The work is parallelized across a number of computing nodes. It is usually hard to estimate in advance how many nodes to use for a particular workload. We propose a simple framework for estimating the scalability of distributed machine learning algorithms. We measure the scalability by means of the speedup an algorithm achieves with more nodes. We propose time complexity models for gradient descent and graphical model inference. We validate our models with experiments on deep learning training and belief propagation. This framework was used to study the scalability of machine learning algorithms in Apache Spark. version:1
arxiv-1610-06272 | Lexicon Integrated CNN Models with Attention for Sentiment Analysis | http://arxiv.org/abs/1610.06272 | id:1610.06272 author:Bonggun Shin, Timothy Lee, Jinho D. Choi category:cs.CL  published:2016-10-20 summary:With the advent of word embeddings, lexicons are no longer fully utilized for sentiment analysis although they still provide important features in the traditional setting. This paper introduces a novel approach to sentiment analysis that integrates lexicon embeddings and an attention mechanism into Convolutional Neural Networks. Our approach performs separate convolutions for word and lexicon embeddings and provides a global view of the document using attention. Our models are experimented on both the SemEval'16 Task 4 dataset and the Stanford Sentiment Treebank, and show comparative or better results against the existing state-of-the-art systems. Our analysis shows that lexicon embeddings allow to build high-performing models with much smaller word embeddings, and the attention mechanism effectively dims out noisy words for sentiment analysis. version:1
arxiv-1610-06268 | Online Training of an Opto-Electronic Reservoir Computer Applied to Real-Time Channel Equalisation | http://arxiv.org/abs/1610.06268 | id:1610.06268 author:Piotr Antonik, Fran√ßois Duport, Michiel Hermans, Anteo Smerieri, Marc Haelterman, Serge Massar category:cs.ET cs.NE  published:2016-10-20 summary:Reservoir Computing is a bio-inspired computing paradigm for processing time dependent signals. The performance of its analogue implementation are comparable to other state of the art algorithms for tasks such as speech recognition or chaotic time series prediction, but these are often constrained by the offline training methods commonly employed. Here we investigated the online learning approach by training an opto-electronic reservoir computer using a simple gradient descent algorithm, programmed on an FPGA chip. Our system was applied to wireless communications, a quickly growing domain with an increasing demand for fast analogue devices to equalise the nonlinear distorted channels. We report error rates up to two orders of magnitude lower than previous implementations on this task. We show that our system is particularly well-suited for realistic channel equalisation by testing it on a drifting and a switching channels and obtaining good performances version:1
arxiv-1610-06266 | Adaptive Substring Extraction and Modified Local NBNN Scoring for Binary Feature-based Local Mobile Visual Search without False Positives | http://arxiv.org/abs/1610.06266 | id:1610.06266 author:Yusuke Uchida, Shigeyuki Sakazawa, Shin'ichi Satoh category:cs.CV  published:2016-10-20 summary:In this paper, we propose a stand-alone mobile visual search system based on binary features and the bag-of-visual words framework. The contribution of this study is three-fold: (1) We propose an adaptive substring extraction method that adaptively extracts informative bits from the original binary vector and stores them in the inverted index. These substrings are used to refine visual word-based matching. (2) A modified local NBNN scoring method is proposed in the context of image retrieval, which considers the density of binary features in scoring each feature matching. (3) In order to suppress false positives, we introduce a convexity check step that imposes a convexity constraint on the configuration of a transformed reference image. The proposed system improves retrieval accuracy by 11% compared with a conventional method without increasing the database size. Furthermore, our system with the convexity check does not lead to false positive results. version:1
arxiv-1610-06251 | DeepGraph: Graph Structure Predicts Network Growth | http://arxiv.org/abs/1610.06251 | id:1610.06251 author:Cheng Li, Xiaoxiao Guo, Qiaozhu Mei category:cs.SI cs.LG  published:2016-10-20 summary:The topological (or graph) structures of real-world networks are known to be predictive of multiple dynamic properties of the networks. Conventionally, a graph structure is represented using an adjacency matrix or a set of hand-crafted structural features. These representations either fail to highlight local and global properties of the graph or suffer from a severe loss of structural information. There lacks an effective graph representation, which hinges the realization of the predictive power of network structures. In this study, we propose to learn the represention of a graph, or the topological structure of a network, through a deep learning model. This end-to-end prediction model, named DeepGraph, takes the input of the raw adjacency matrix of a real-world network and outputs a prediction of the growth of the network. The adjacency matrix is first represented using a graph descriptor based on the heat kernel signature, which is then passed through a multi-column, multi-resolution convolutional neural network. Extensive experiments on five large collections of real-world networks demonstrate that the proposed prediction model significantly improves the effectiveness of existing methods, including linear or nonlinear regressors that use hand-crafted features, graph kernels, and competing deep learning methods. version:1
arxiv-1610-06249 | Multilevel Anomaly Detection for Mixed Data | http://arxiv.org/abs/1610.06249 | id:1610.06249 author:Kien Do, Truyen Tran, Svetha Venkatesh category:cs.LG cs.DB  published:2016-10-20 summary:Anomalies are those deviating from the norm. Unsupervised anomaly detection often translates to identifying low density regions. Major problems arise when data is high-dimensional and mixed of discrete and continuous attributes. We propose MIXMAD, which stands for MIXed data Multilevel Anomaly Detection, an ensemble method that estimates the sparse regions across multiple levels of abstraction of mixed data. The hypothesis is for domains where multiple data abstractions exist, a data point may be anomalous with respect to the raw representation or more abstract representations. To this end, our method sequentially constructs an ensemble of Deep Belief Nets (DBNs) with varying depths. Each DBN is an energy-based detector at a predefined abstraction level. At the bottom level of each DBN, there is a Mixed-variate Restricted Boltzmann Machine that models the density of mixed data. Predictions across the ensemble are finally combined via rank aggregation. The proposed MIXMAD is evaluated on high-dimensional realworld datasets of different characteristics. The results demonstrate that for anomaly detection, (a) multilevel abstraction of high-dimensional and mixed data is a sensible strategy, and (b) empirically, MIXMAD is superior to popular unsupervised detection methods for both homogeneous and mixed data. version:1
arxiv-1610-06235 | Enhancing ICA Performance by Exploiting Sparsity: Application to FMRI Analysis | http://arxiv.org/abs/1610.06235 | id:1610.06235 author:Zois Boukouvalas, Yuri Levin-Schwartz, Tulay Adali category:stat.ML  published:2016-10-19 summary:Independent component analysis (ICA) is a powerful method for blind source separation based on the assumption that sources are statistically independent. Though ICA has proven useful and has been employed in many applications, complete statistical independence can be too restrictive an assumption in practice. Additionally, important prior information about the data, such as sparsity, is usually available. Sparsity is a natural property of the data, a form of diversity, which, if incorporated into the ICA model, can relax the independence assumption, resulting in an improvement in the overall separation performance. In this work, we propose a new variant of ICA by entropy bound minimization (ICA-EBM)-a flexible, yet parameter-free algorithm-through the direct exploitation of sparsity. Using this new SparseICA-EBM algorithm, we study the synergy of independence and sparsity through simulations on synthetic as well as functional magnetic resonance imaging (fMRI)-like data. version:1
arxiv-1610-06227 | Cross-Lingual Syntactic Transfer with Limited Resources | http://arxiv.org/abs/1610.06227 | id:1610.06227 author:Mohammad Sadegh Rasooli, Michael Collins category:cs.CL  published:2016-10-19 summary:We describe a simple but effective method for cross-lingual syntactic transfer of dependency parsers, in the scenario where a large amount of translation data is not available.The method makes use of three steps: 1) a method for deriving cross-lingual word clusters, that can then be used in a multilingual parser; 2) a method for transferring lexical information from a target language to source language treebanks; 3) a method for integrating these steps with the density-driven annotation projection method of Rasooli and Collins(2015). Experiments show improvements over the state-of-the-art in several languages used in previous work (Rasooli and Collins, 2015;Zhang and Barzilay, 2015; Ammar et al.,2016), in a setting where the only source of translation data is the Bible, a considerably smaller corpus than the Europarl corpus used in previous work. Results using the Europarl corpus as a source of translation data show additional improvements over the results of Rasooli and Collins (2015). We conclude with results on 38 datasets (26 languages) from the Universal Dependencies corpora: 13 datasets(10 languages) have unlabeled attachment ac-curacies of 80% or higher; the average unlabeled accuracy on the 38 datasets is 74.8%. version:1
arxiv-1610-06210 | A Theme-Rewriting Approach for Generating Algebra Word Problems | http://arxiv.org/abs/1610.06210 | id:1610.06210 author:Rik Koncel-Kedziorski, Ioannis Konstas, Luke Zettlemoyer, Hannaneh Hajishirzi category:cs.CL  published:2016-10-19 summary:Texts present coherent stories that have a particular theme or overall setting, for example science fiction or western. In this paper, we present a text generation method called {\it rewriting} that edits existing human-authored narratives to change their theme without changing the underlying story. We apply the approach to math word problems, where it might help students stay more engaged by quickly transforming all of their homework assignments to the theme of their favorite movie without changing the math concepts that are being taught. Our rewriting method uses a two-stage decoding process, which proposes new words from the target theme and scores the resulting stories according to a number of factors defining aspects of syntactic, semantic, and thematic coherence. Experiments demonstrate that the final stories typically represent the new theme well while still testing the original math concepts, outperforming a number of baselines. We also release a new dataset of human-authored rewrites of math word problems in several themes. version:1
arxiv-1610-06204 | A Reinforcement Learning Approach to Sensor Planning for 3D Models | http://arxiv.org/abs/1610.06204 | id:1610.06204 author:Mustafa Devrim Kaba, Mustafa Gokhan Uzunbas, Ser Nam Lim category:cs.CV  published:2016-10-19 summary:We introduce a novel, fully automated solution method for sensor planning problem for 3D models. By modeling the human approach to the problem first, we put the problem into a reinforcement learning (RL) framework and successfully solve it using the well-known RL algorithms with function approximation. We compare our method with the greedy algorithm in various test cases and show that we can out-perform the baseline greedy algorithm in all cases. version:1
arxiv-1610-06194 | Robust and Parallel Bayesian Model Selection | http://arxiv.org/abs/1610.06194 | id:1610.06194 author:Michael Zhang, Henry Lam, Lizhen Lin category:stat.ML  published:2016-10-19 summary:Effective and accurate model selection that takes into account model uncertainty is an important but challenging problem in modern data analysis. One of the major challenges is the computational burden required to infer huge data sets which, in general, cannot be stored or processed on one machine. Moreover, in many real data modeling scenarios we may encounter the presence of outliers and contaminations that will damage the quality of our model and variable selection. We can overcome both of these problems through a simple "divide and conquer" strategy in which we divide the observations of the full data set equally into subsets and perform inference and model selections independently on each subset. After local subset inference, we can aggregate the optimal subset model or aggregate the local model/variable selection criteria to obtain a final model. We show that by aggregating with the geometric median, we obtain results that are robust to outliers and contamination of an unknown nature. version:1
arxiv-1610-06160 | Streaming Normalization: Towards Simpler and More Biologically-plausible Normalizations for Online and Recurrent Learning | http://arxiv.org/abs/1610.06160 | id:1610.06160 author:Qianli Liao, Kenji Kawaguchi, Tomaso Poggio category:cs.LG cs.NE  published:2016-10-19 summary:We systematically explored a spectrum of normalization algorithms related to Batch Normalization (BN) and propose a generalized formulation that simultaneously solves two major limitations of BN: (1) online learning and (2) recurrent learning. Our proposal is simpler and more biologically-plausible. Unlike previous approaches, our technique can be applied out of the box to all learning scenarios (e.g., online learning, batch learning, fully-connected, convolutional, feedforward, recurrent and mixed --- recurrent and convolutional) and compare favorably with existing approaches. We also propose Lp Normalization for normalizing by different orders of statistical moments. In particular, L1 normalization is well-performing, simple to implement, fast to compute, more biologically-plausible and thus ideal for GPU or hardware implementations. version:1
arxiv-1610-02287 | The Generalized Reparameterization Gradient | http://arxiv.org/abs/1610.02287 | id:1610.02287 author:Francisco J. R. Ruiz, Michalis K. Titsias, David M. Blei category:stat.ML  published:2016-10-07 summary:The reparameterization gradient has become a widely used method to obtain Monte Carlo gradients to optimize the variational objective. However, this technique does not easily apply to commonly used distributions such as beta or gamma without further approximations, and most practical applications of the reparameterization gradient fit Gaussian distributions. In this paper, we introduce the generalized reparameterization gradient, a method that extends the reparameterization gradient to a wider class of variational distributions. Generalized reparameterizations use invertible transformations of the latent variables which lead to transformed distributions that weakly depend on the variational parameters. This results in new Monte Carlo gradients that combine reparameterization gradients and score function gradients. We demonstrate our approach on variational inference for two complex probabilistic models. The generalized reparameterization is effective: even a single sample from the variational distribution is enough to obtain a low-variance gradient. version:3
arxiv-1610-06136 | POI: Multiple Object Tracking with High Performance Detection and Appearance Feature | http://arxiv.org/abs/1610.06136 | id:1610.06136 author:Fengwei Yu, Wenbo Li, Quanquan Li, Yu Liu, Xiaohua Shi, Junjie Yan category:cs.CV  published:2016-10-19 summary:Detection and learning based appearance feature play the central role in data association based multiple object tracking (MOT), but most recent MOT works usually ignore them and only focus on the hand-crafted feature and association algorithms. In this paper, we explore the high-performance detection and deep learning based appearance feature, and show that they lead to significantly better MOT results in both online and offline setting. We make our detection and appearance feature publicly available. In the following part, we first summarize the detection and appearance feature, and then introduce our tracker named Person of Interest (POI), which has both online and offline version. version:1
arxiv-1610-06106 | Efficiency of active learning for the allocation of workers on crowdsourced classification tasks | http://arxiv.org/abs/1610.06106 | id:1610.06106 author:Edoardo Manino, Long Tran-Thanh, Nicholas R. Jennings category:cs.HC cs.LG  published:2016-10-19 summary:Crowdsourcing has been successfully employed in the past as an effective and cheap way to execute classification tasks and has therefore attracted the attention of the research community. However, we still lack a theoretical understanding of how to collect the labels from the crowd in an optimal way. In this paper we focus on the problem of worker allocation and compare two active learning policies proposed in the empirical literature with a uniform allocation of the available budget. To this end we make a thorough mathematical analysis of the problem and derive a new bound on the performance of the system. Furthermore we run extensive simulations in a more realistic scenario and show that our theoretical results hold in practice. version:1
arxiv-1610-04154 | An Information Theoretic Feature Selection Framework for Big Data under Apache Spark | http://arxiv.org/abs/1610.04154 | id:1610.04154 author:Sergio Ram√≠rez-Gallego, H√©ctor Mouri√±o-Tal√≠n, David Mart√≠nez-Rego, Ver√≥nica Bol√≥n-Canedo, Jos√© Manuel Ben√≠tez, Amparo Alonso-Betanzos, Francisco Herrera category:cs.AI cs.DC cs.LG  published:2016-10-13 summary:With the advent of extremely high dimensional datasets, dimensionality reduction techniques are becoming mandatory. Among many techniques, feature selection has been growing in interest as an important tool to identify relevant features on huge datasets --both in number of instances and features--. The purpose of this work is to demonstrate that standard feature selection methods can be parallelized in Big Data platforms like Apache Spark, boosting both performance and accuracy. We thus propose a distributed implementation of a generic feature selection framework which includes a wide group of well-known Information Theoretic methods. Experimental results on a wide set of real-world datasets show that our distributed framework is capable of dealing with ultra-high dimensional datasets as well as those with a huge number of samples in a short period of time, outperforming the sequential version in all the cases studied. version:2
arxiv-1610-06072 | Learning to Learn Neural Networks | http://arxiv.org/abs/1610.06072 | id:1610.06072 author:Tom Bosc category:cs.LG stat.ML  published:2016-10-19 summary:Meta-learning consists in learning learning algorithms. We use a Long Short Term Memory (LSTM) based network to learn to compute on-line updates of the parameters of another neural network. These parameters are stored in the cell state of the LSTM. Our framework allows to compare learned algorithms to hand-made algorithms within the traditional train and test methodology. In an experiment, we learn a learning algorithm for a one-hidden layer Multi-Layer Perceptron (MLP) on non-linearly separable datasets. The learned algorithm is able to update parameters of both layers and generalise well on similar datasets. version:1
arxiv-1610-09932 | Support Vector Machines and Generalisation in HEP | http://arxiv.org/abs/1610.09932 | id:1610.09932 author:A. Bethani, A. J. Bevan, J. Hays, T. J. Stevenson category:physics.data-an cs.LG hep-ex  published:2016-10-19 summary:We review the concept of support vector machines (SVMs) and discuss examples of their use. One of the benefits of SVM algorithms, compared with neural networks and decision trees is that they can be less susceptible to over fitting than those other algorithms are to over training. This issue is related to the generalisation of a multivariate algorithm (MVA); a problem that has often been overlooked in particle physics. We discuss cross validation and how this can be used to improve the generalisation of a MVA in the context of High Energy Physics analyses. The examples presented use the Toolkit for Multivariate Analysis (TMVA) based on ROOT and describe our improvements to the SVM functionality and new tools introduced for cross validation within this framework. version:1
arxiv-1610-06053 | Chinese Restaurant Process for cognate clustering: A threshold free approach | http://arxiv.org/abs/1610.06053 | id:1610.06053 author:Taraka Rama category:cs.CL  published:2016-10-19 summary:In this paper, we introduce a threshold free approach, motivated from Chinese Restaurant Process, for the purpose of cognate clustering. We show that our approach yields similar results to a linguistically motivated cognate clustering system known as LexStat. Our Chinese Restaurant Process system is fast and does not require any threshold and can be applied to any language family of the world. version:1
arxiv-1610-06049 | Fast and Accurate Surface Normal Integration on Non-Rectangular Domains | http://arxiv.org/abs/1610.06049 | id:1610.06049 author:Martin B√§hr, Michael Breu√ü, Yvain Qu√©au, Ali Sharifi Boroujerdi, Jean-Denis Durou category:cs.NA cs.CV 68U10  published:2016-10-19 summary:The integration of surface normals for the purpose of computing the shape of a surface in 3D space is a classic problem in computer vision. However, even nowadays it is still a challenging task to devise a method that combines the flexibility to work on non-trivial computational domains with high accuracy, robustness and computational efficiency. By uniting a classic approach for surface normal integration with modern computational techniques we construct a solver that fulfils these requirements. Building upon the Poisson integration model we propose to use an iterative Krylov subspace solver as a core step in tackling the task. While such a method can be very efficient, it may only show its full potential when combined with a suitable numerical preconditioning and a problem-specific initialisation. We perform a thorough numerical study in order to identify an appropriate preconditioner for our purpose. To address the issue of a suitable initialisation we propose to compute this initial state via a recently developed fast marching integrator. Detailed numerical experiments illuminate the benefits of this novel combination. In addition, we show on real-world photometric stereo datasets that the developed numerical framework is flexible enough to tackle modern computer vision applications. version:1
arxiv-1610-06048 | K-Nearest Neighbor Classification Using Anatomized Data | http://arxiv.org/abs/1610.06048 | id:1610.06048 author:Koray Mancuhan, Chris Clifton category:cs.LG cs.CR cs.DB  published:2016-10-19 summary:This paper analyzes k nearest neighbor classification with training data anonymized using anatomy. Anatomy preserves all data values, but introduces uncertainty in the mapping between identifying and sensitive values. We first study the theoretical effect of the anatomized training data on the k nearest neighbor error rate bounds, nearest neighbor convergence rate, and Bayesian error. We then validate the derived bounds empirically. We show that 1) Learning from anatomized data approaches the limits of learning through the unprotected data (although requiring larger training data), and 2) nearest neighbor using anatomized data outperforms nearest neighbor on generalization-based anonymization. version:1
arxiv-1610-05652 | Vietnamese Named Entity Recognition using Token Regular Expressions and Bidirectional Inference | http://arxiv.org/abs/1610.05652 | id:1610.05652 author:Phuong Le-Hong category:cs.CL  published:2016-10-18 summary:This paper describes an efficient approach to improve the accuracy of a named entity recognition system for Vietnamese. The approach combines regular expressions over tokens and a bidirectional inference method in a sequence labelling model. The proposed method achieves an overall $F_1$ score of 89.66% on a test set of an evaluation campaign, organized in late 2016 by the Vietnamese Language and Speech Processing (VLSP) community. version:2
arxiv-1610-05400 | Going off the Grid: Iterative Model Selection for Biclustered Matrix Completion | http://arxiv.org/abs/1610.05400 | id:1610.05400 author:Eric Chi, Liuiyi Hu, Arvind K. Saibaba, Arvind U. K. Rao category:stat.CO stat.ML  published:2016-10-18 summary:We consider the problem of performing matrix completion with side information on row-by-row and column-by-column similarities. We build upon recent proposals for matrix estimation with smoothness constraints with respect to row and column graphs. We present a novel iterative procedure for directly minimizing an information criterion in order to select an appropriate amount row and column smoothing, namely perform model selection. We also discuss how to exploit the special structure of the problem to scale up the estimation and model selection procedure via the Hutchinson estimator. We present simulation results and an application to predicting associations in imaging-genomics studies. version:2
arxiv-1610-05985 | Robust Video Synchronization using Unsupervised Deep Learning | http://arxiv.org/abs/1610.05985 | id:1610.05985 author:Ido Freeman, Patrick Wieschollek, Hendrik P. A. Lensch category:cs.CV  published:2016-10-19 summary:Aligning video sequences is a fundamental yet still unsolved component for a wide range of applications in computer graphics and vision. Especially when targeting video clips containing an extensively varying appearance. Using recent advances in deep learning, we present a scalable and robust method for computing optimal non-linear temporal video alignments. The presented algorithm learns to retrieve and match similar video frames from input sequences without any human interaction or additional annotations in an unsupervised fashion. An iterative scheme is presented which leverages on the nature of the videos themselves in order to remove the need for labels. We incorporate a variation of Dijkstra's shortest-path algorithm for extracting meaningful training examples as well as a robust video alignment. While previous methods assume similar settings as weather conditions, season and illumination, our approach is able to robustly align videos regardless of such noise. This provides new ways of compositing non-seasonal video clips from data recorded months apart. version:1
arxiv-1610-05984 | Particle Swarm Optimization for Generating Fuzzy Reinforcement Learning Policies | http://arxiv.org/abs/1610.05984 | id:1610.05984 author:Daniel Hein, Alexander Hentschel, Thomas Runkler, Steffen Udluft category:cs.NE cs.AI cs.LG cs.SY  published:2016-10-19 summary:Fuzzy controllers are known to serve as efficient and interpretable system controllers for continuous state and action spaces. To date these controllers have been constructed by hand, or automatically trained either on expert generated problem specific cost functions or by incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not given in the majority of real world reinforcement learning (RL) problems. We introduce a new particle swarm reinforcement learning (PSRL) approach which is capable of constructing fuzzy RL policies solely by training parameters on world models produced from randomly generated samples of the real system. This approach relates self-organizing fuzzy controllers to model-based RL for the first time. PSRL can be used straightforward on any RL problem, which is demonstrated on three standard RL benchmarks, mountain car, cart pole balancing and cart pole swing up. Our experiments yielded high performing and well interpretable fuzzy policies. version:1
arxiv-1610-05956 | Clustering by connection center evolution | http://arxiv.org/abs/1610.05956 | id:1610.05956 author:Xiurui Geng, Hairong Tang category:stat.ML  published:2016-10-19 summary:The determination of cluster centers generally depends on the scale that we use to analyze the data to be clustered. Inappropriate scale usually leads to unreasonable cluster centers and thus unreasonable results. In this study, we first consider the similarity of elements in the data as the connectivity of nodes in an undirected graph, then present the concept of a connection center and regard it as the cluster center of the data. Based on this definition, the determination of cluster centers and the assignment of class are very simple, natural and effective. One more crucial finding is that the cluster centers of different scales can be obtained easily by the different powers of a similarity matrix and the change of power from small to large leads to the dynamic evolution of cluster centers from local (microscopic) to global (microscopic). Further, in this process of evolution, the number of categories changes discontinuously, which means that the presented method can automatically skip the unreasonable number of clusters, suggest appropriate observation scales and provide corresponding cluster results. version:1
arxiv-1610-05950 | Consistent Kernel Mean Estimation for Functions of Random Variables | http://arxiv.org/abs/1610.05950 | id:1610.05950 author:Carl-Johann Simon-Gabriel, Adam ≈öcibior, Ilya Tolstikhin, Bernhard Sch√∂lkopf category:stat.ML  published:2016-10-19 summary:We provide a theoretical foundation for non-parametric estimation of functions of random variables using kernel mean embeddings. We show that for any continuous function $f$, consistent estimators of the mean embedding of a random variable $X$ lead to consistent estimators of the mean embedding of $f(X)$. For Mat\'ern kernels and sufficiently smooth functions we also provide rates of convergence. Our results extend to functions of multiple random variables. If the variables are dependent, we require an estimator of the mean embedding of their joint distribution as a starting point; if they are independent, it is sufficient to have separate estimators of the mean embeddings of their marginal distributions. In either case, our results cover both mean embeddings based on i.i.d. samples as well as "reduced set" expansions in terms of dependent expansion points. The latter serves as a justification for using such expansions to limit memory resources when applying the approach as a basis for probabilistic programming. version:1
arxiv-1610-05949 | Visual-Inertial Monocular SLAM with Map Reuse | http://arxiv.org/abs/1610.05949 | id:1610.05949 author:Raul Mur-Artal, Juan D. Tardos category:cs.RO cs.CV  published:2016-10-19 summary:In recent years there have been excellent results in Visual-Inertial Odometry techniques, which aim to compute the incremental motion of the sensor with high accuracy and robustness. However these approaches lack the capability to close loops, and trajectory estimation accumulates drift even if the sensor is continually revisiting the same place. In this work we present a novel tightly-coupled Visual-Inertial Simultaneous Localization and Mapping system that is able to close loops and reuse its map to achieve zero-drift localization in already mapped areas. While our approach can be applied to any camera configuration, we address here the most general problem of a monocular camera, with its well-known scale ambiguity. We also propose a novel IMU initialization method, which computes the scale, the gravity direction, the velocity, and gyroscope and accelerometer biases, in a few seconds with high accuracy. We test our system in the 11 sequences of a recent micro-aerial vehicle public dataset achieving a typical scale factor error of 1% and centimeter precision. We compare to the state-of-the-art in visual-inertial odometry in sequences with revisiting, proving the better accuracy of our method due to map reuse and no drift accumulation. version:1
arxiv-1610-05948 | A Bayesian Approach to Estimation of Speaker Normalization Parameters | http://arxiv.org/abs/1610.05948 | id:1610.05948 author:Dhananjay Ram, Debasis Kundu, Rajesh M. Hegde category:cs.SD cs.CL stat.AP  published:2016-10-19 summary:In this work, a Bayesian approach to speaker normalization is proposed to compensate for the degradation in performance of a speaker independent speech recognition system. The speaker normalization method proposed herein uses the technique of vocal tract length normalization (VTLN). The VTLN parameters are estimated using a novel Bayesian approach which utilizes the Gibbs sampler, a special type of Markov Chain Monte Carlo method. Additionally the hyperparameters are estimated using maximum likelihood approach. This model is used assuming that human vocal tract can be modeled as a tube of uniform cross section. It captures the variation in length of the vocal tract of different speakers more effectively, than the linear model used in literature. The work has also investigated different methods like minimization of Mean Square Error (MSE) and Mean Absolute Error (MAE) for the estimation of VTLN parameters. Both single pass and two pass approaches are then used to build a VTLN based speech recognizer. Experimental results on recognition of vowels and Hindi phrases from a medium vocabulary indicate that the Bayesian method improves the performance by a considerable margin. version:1
arxiv-1610-05945 | A multi-task learning model for malware classification with useful file access pattern from API call sequence | http://arxiv.org/abs/1610.05945 | id:1610.05945 author:Xin Wang, Siu Ming Yiu category:cs.SD cs.CR cs.LG  published:2016-10-19 summary:Based on API call sequences, semantic-aware and machine learning (ML) based malware classifiers can be built for malware detection or classification. Previous works concentrate on crafting and extracting various features from malware binaries, disassembled binaries or API calls via static or dynamic analysis and resorting to ML to build classifiers. However, they tend to involve too much feature engineering and fail to provide interpretability. We solve these two problems with the recent advances in deep learning: 1) RNN-based autoencoders (RNN-AEs) can automatically learn low-dimensional representation of a malware from its raw API call sequence. 2) Multiple decoders can be trained under different supervisions to give more information, other than the class or family label of a malware. Inspired by the works of document classification and automatic sentence summarization, each API call sequence can be regarded as a sentence. In this paper, we make the first attempt to build a multi-task malware learning model based on API call sequences. The model consists of two decoders, one for malware classification and one for $\emph{file access pattern}$ (FAP) generation given the API call sequence of a malware. We base our model on the general seq2seq framework. Experiments show that our model can give competitive classification results as well as insightful FAP information. version:1
arxiv-1610-05929 | An automatic bad band preremoval algorithm for hyperspectral imagery | http://arxiv.org/abs/1610.05929 | id:1610.05929 author:Luyan Ji, Xiurui Geng, Yongchao Zhao, Fuxiang Wang category:cs.CV  published:2016-10-19 summary:For most hyperspectral remote sensing applications, removing bad bands, such as water absorption bands, is a required preprocessing step. Currently, the commonly applied method is by visual inspection, which is very time-consuming and it is easy to overlook some noisy bands. In this study, we find an inherent connection between target detection algorithms and the corrupted band removal. As an example, for the matched filter (MF), which is the most widely used target detection method for hyperspectral data, we present an automatic MF-based algorithm for bad band identification. The MF detector is a filter vector, and the resulting filter output is the sum of all bands weighted by the MF coefficients. Therefore, we can identify bad bands only by using the MF filter vector itself, the absolute value of whose entry accounts for the importance of each band for the target detection. For a specific target of interest, the bands with small MF weights correspond to the noisy or bad ones. Based on this fact, we develop an automatic bad band preremoval algorithm by utilizing the average absolute value of MF weights for multiple targets within a scene. Experiments with three well known hyperspectral datasets show that our method can always identify the water absorption and other low signal-to-noise (SNR) bands that are usually chosen as bad bands manually. version:1
arxiv-1610-05925 | Learning Determinantal Point Processes in Sublinear Time | http://arxiv.org/abs/1610.05925 | id:1610.05925 author:Christophe Dupuy, Francis Bach category:stat.ML cs.LG  published:2016-10-19 summary:We propose a new class of determinantal point processes (DPPs) which can be manipulated for inference and parameter learning in potentially sublinear time in the number of items. This class, based on a specific low-rank factorization of the marginal kernel, is particularly suited to a subclass of continuous DPPs and DPPs defined on exponentially many items. We apply this new class to modelling text documents as sampling a DPP of sentences, and propose a conditional maximum likelihood formulation to model topic proportions, which is made possible with no approximation for our class of DPPs. We present an application to document summarization with a DPP on $2^{500}$ items. version:1
arxiv-1610-05883 | A Robust 3D-2D Interactive Tool for Scene Segmentation and Annotation | http://arxiv.org/abs/1610.05883 | id:1610.05883 author:Duc Thanh Nguyen, Binh-Son Hua, Lap-Fai Yu, Sai-Kit Yeung category:cs.CV  published:2016-10-19 summary:Recent advances of 3D acquisition devices have enabled large-scale acquisition of 3D scene data. Such data, if completely and well annotated, can serve as useful ingredients for a wide spectrum of computer vision and graphics works such as data-driven modeling and scene understanding, object detection and recognition. However, annotating a vast amount of 3D scene data remains challenging due to the lack of an effective tool and/or the complexity of 3D scenes (e.g. clutter, varying illumination conditions). This paper aims to build a robust annotation tool that effectively and conveniently enables the segmentation and annotation of massive 3D data. Our tool works by coupling 2D and 3D information via an interactive framework, through which users can provide high-level semantic annotation for objects. We have experimented our tool and found that a typical indoor scene could be well segmented and annotated in less than 30 minutes by using the tool, as opposed to a few hours if done manually. Along with the tool, we created a dataset of over a hundred 3D scenes associated with complete annotations using our tool. The tool and dataset are available at www.scenenn.net. version:1
arxiv-1610-05872 | Making brain-machine interfaces robust to future neural variability | http://arxiv.org/abs/1610.05872 | id:1610.05872 author:David Sussillo, Sergey D. Stavisky, Jonathan C. Kao, Stephen I. Ryu, Krishna V. Shenoy category:q-bio.NC stat.ML  published:2016-10-19 summary:A major hurdle to clinical translation of brain-machine interfaces (BMIs) is that current decoders, which are trained from a small quantity of recent data, become ineffective when neural recording conditions subsequently change. We tested whether a decoder could be made more robust to future neural variability by training it to handle a variety of recording conditions sampled from months of previously collected data as well as synthetic training data perturbations. We developed a new multiplicative recurrent neural network BMI decoder that successfully learned a large variety of neural-to- kinematic mappings and became more robust with larger training datasets. When tested with a non-human primate preclinical BMI model, this decoder was robust under conditions that disabled a state-of-the-art Kalman filter based decoder. These results validate a new BMI strategy in which accumulated data history is effectively harnessed, and may facilitate reliable daily BMI use by reducing decoder retraining downtime. version:1
arxiv-1610-05861 | StuffNet: Using 'Stuff' to Improve Object Detection | http://arxiv.org/abs/1610.05861 | id:1610.05861 author:Samarth Brahmbhatt, Henrik I. Christensen, James Hays category:cs.CV  published:2016-10-19 summary:We propose a Convolutional Neural Network (CNN) based algorithm - StuffNet - for object detection. In addition to the standard convolutional features trained for region proposal and object detection [31], StuffNet uses convolutional features trained for segmentation of objects and 'stuff' (amorphous categories such as ground and water). Through experiments on Pascal VOC 2010, we show the importance of features learnt from stuff segmentation for improving object detection performance. StuffNet improves performance from 18.8% mAP to 23.9% mAP for small objects. We also devise a method to train StuffNet on datasets that do not have stuff segmentation labels. Through experiments on Pascal VOC 2007 and 2012, we demonstrate the effectiveness of this method and show that StuffNet also significantly improves object detection performance on such datasets. version:1
arxiv-1610-05858 | Bidirectional LSTM-CRF for Clinical Concept Extraction | http://arxiv.org/abs/1610.05858 | id:1610.05858 author:Raghavendra Chalapathy, Ehsan Zare Borzeshi, Massimo Piccardi category:cs.CL  published:2016-10-19 summary:Extraction of concepts present in patient clinical records is an essential step in clinical research. The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for clinical records presented concept extraction (CE) task, with aim to identify concepts (such as treatments, tests, problems) and classify them into predefined categories. State-of-the-art CE approaches heavily rely on hand crafted features and domain specific resources which are hard to collect and tune. For this reason, this paper employs bidirectional LSTM with CRF decoding initialized with general purpose off-the-shelf word embeddings for CE. The experimental results achieved on 2010 i2b2/VA reference standard corpora using bidirectional LSTM CRF ranks closely with top ranked systems. version:1
arxiv-1610-05854 | Mixed context networks for semantic segmentation | http://arxiv.org/abs/1610.05854 | id:1610.05854 author:Haiming Sun, Di Xie, Shiliang Pu category:cs.CV  published:2016-10-19 summary:Semantic segmentation is challenging as it requires both object-level information and pixel-level accuracy. Recently, FCN-based systems gained great improvement in this area. Unlike classification networks, combining features of different layers plays an important role in these dense prediction models, as these features contains information of different levels. A number of models have been proposed to show how to use these features. However, what is the best architecture to make use of features of different layers is still a question. In this paper, we propose a module, called mixed context network, and show that our presented system outperforms most existing semantic segmentation systems by making use of this module. version:1
arxiv-1610-05446 | Provably Good Early Detection of Diseases using Non-Sparse Covariance-Regularized Linear Discriminant Analysis | http://arxiv.org/abs/1610.05446 | id:1610.05446 author:Haoyi Xiong, Yanjie Fu, Wenqing Hu, Guanling Chen, Laura E. Barnes category:cs.LG  published:2016-10-18 summary:To improve the performance of Linear Discriminant Analysis (LDA) for early detection of diseases using Electronic Health Records (EHR) data, we propose \TheName{} -- a novel framework for \emph{\underline{E}HR based \underline{E}arly \underline{D}etection of \underline{D}iseases} on top of \emph{Covariance-Regularized} LDA models. Specifically, \TheName\ employs a \emph{non-sparse} inverse covariance matrix (or namely precision matrix) estimator derived from graphical lasso and incorporates the estimator into LDA classifiers to improve classification accuracy. Theoretical analysis on \TheName\ shows that it can bound the expected error rate of LDA classification, under certain assumptions. Finally, we conducted extensive experiments using a large-scale real-world EHR dataset -- CHSN. We compared our solution with other regularized LDA and downstream classifiers. The result shows \TheName\ outperforms all baselines and backups our theoretical analysis. version:2
arxiv-1610-05834 | Lensless Imaging with Compressive Ultrafast Sensing | http://arxiv.org/abs/1610.05834 | id:1610.05834 author:Guy Satat, Matthew Tancik, Ramesh Raskar category:cs.CV  published:2016-10-19 summary:Conventional imaging uses a set of lenses to form an image on the sensor plane. This pure hardware-based approach doesn't use any signal processing, nor the extra information in the time of arrival of photons to the sensor. Recently, modern compressive sensing techniques have been applied for lensless imaging. However, this computational approach tends to depend as much as possible on signal processing (for example, single pixel camera) and results in a long acquisition time. Here we propose using compressive ultrafast sensing for lensless imaging. We use extremely fast sensors (picosecond time resolution) to time tag photons as they arrive to an omnidirectional pixel. Thus, each measurement produces a time series where time is a function of the photon source location in the scene. This allows lensless imaging with significantly fewer measurements compared to regular single pixel imaging ($ 33 \times$ less measurements in our experiments). To achieve this goal, we developed a framework for using ultrafast pixels with compressive sensing, including an algorithm for ideal sensor placement, and an algorithm for optimized active illumination patterns. We show that efficient lensless imaging is possible with ultrafast imaging and compressive sensing. This paves the way for novel imaging architectures, and remote sensing in extreme situations where imaging with a lens is not possible. version:1
arxiv-1610-06848 | An Efficient Minibatch Acceptance Test for Metropolis-Hastings | http://arxiv.org/abs/1610.06848 | id:1610.06848 author:Haoyu Chen, Daniel Seita, Xinlei Pan, John Canny category:cs.LG stat.ML  published:2016-10-19 summary:We present a novel Metropolis-Hastings method for large datasets that uses small expected-size minibatches of data. Previous work on reducing the cost of Metropolis-Hastings tests yield variable data consumed per sample, with only constant factor reductions versus using the full dataset for each sample. Here we present a method that can be tuned to provide arbitrarily small batch sizes, by adjusting either proposal step size or temperature. Our test uses the noise-tolerant Barker acceptance test with a novel additive correction variable. The resulting test has similar cost to a normal SGD update. Our experiments demonstrate several order-of-magnitude speedups over previous work. version:1
arxiv-1610-05824 | Robot Vision Architecture for Autonomous Clothes Manipulation | http://arxiv.org/abs/1610.05824 | id:1610.05824 author:Li Sun, Gerardo Aragon-Camarasa, Simon Rogers, J. Paul Siebert category:cs.RO cs.CV  published:2016-10-18 summary:This paper presents a novel robot vision architecture for perceiving generic 3D clothes configurations. Our architecture is hierarchically structured, starting from low-level curvatures, across mid-level geometric shapes \& topology descriptions; and finally approaching high-level semantic surface structure descriptions. We demonstrate our robot vision architecture in a customised dual-arm industrial robot with our self-designed, off-the-self stereo vision system, carrying out autonomous grasping and dual-arm flattening. It is worth noting that the proposed dual-arm flattening approach is unique among the state-of-the-art robot autonomous system, which is the major contribution of this paper. The experimental results show that the proposed dual-arm flattening using stereo vision system remarkably outperforms the single-arm flattening and widely-cited Kinect-based sensing system for dexterous manipulation tasks. In addition, the proposed grasping approach achieves satisfactory performance on grasping various kind of garments, verifying the capability of proposed visual perception architecture to be adapted to more than one clothing manipulation tasks. version:1
arxiv-1610-05820 | Membership Inference Attacks against Machine Learning Models | http://arxiv.org/abs/1610.05820 | id:1610.05820 author:Reza Shokri, Marco Stronati, Vitaly Shmatikov category:cs.CR cs.LG stat.ML  published:2016-10-18 summary:We investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine whether the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference attack model to recognize differences in the target model's predictions on inputs that it trained on versus inputs that it did not use during training. We empirically evaluate our inference techniques on classification models trained by commercial "machine learning as a service" providers such as Google and Amazon. Using realistic datasets and classification tasks, we show that these models can be significantly vulnerable to membership inference attacks. version:1
