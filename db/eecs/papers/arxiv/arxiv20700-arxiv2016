arxiv-1609-00878 | A Probabilistic Optimum-Path Forest Classifier for Binary Classification Problems | http://arxiv.org/abs/1609.00878 | id:1609.00878 author:Silas E. N. Fernandes, Danillo R. Pereira, Caio C. O. Ramos, Andre N. Souza, Joao P. Papa category:cs.CV cs.LG stat.ML  published:2016-09-04 summary:Probabilistic-driven classification techniques extend the role of traditional approaches that output labels (usually integer numbers) only. Such techniques are more fruitful when dealing with problems where one is not interested in recognition/identification only, but also into monitoring the behavior of consumers and/or machines, for instance. Therefore, by means of probability estimates, one can take decisions to work better in a number of scenarios. In this paper, we propose a probabilistic-based Optimum Path Forest (OPF) classifier to handle with binary classification problems, and we show it can be more accurate than naive OPF in a number of datasets. In addition to being just more accurate or not, probabilistic OPF turns to be another useful tool to the scientific community. version:1
arxiv-1609-00866 | Fully Convolutional Neural Network for Fast Anomaly Detection in Crowded Scenes | http://arxiv.org/abs/1609.00866 | id:1609.00866 author:Mohammad Sabokrou, Mohsen Fayyaz, Mahmood Fathy, Reinhard klette category:cs.CV  published:2016-09-03 summary:We present an efficient method for detecting and localizing anomalies in videos showing crowded scenes. Research on {\it fully convolutional neural networks} (FCNs) has shown the potentials of this technology for object detection and localization, especially in images. We investigate how to involve temporal data, and how to transform a supervised FCN into an unsupervised one such that the resulting FCN ensures anomaly detection. Altogether, we propose an FCN-based architecture for anomaly detection and localization in crowded scenes videos. For reducing computations and, consequently, improving performance both with respect to speed and accuracy, we investigate the use of cascaded out-layer detection. Our architecture includes two main components, one for feature representation, and one for cascaded out-layer detection. Experimental results on Subway and UCSD benchmarks confirm that the detection and localization accuracy of our method is comparable to state-of-the-art methods, but at a significantly increased speed of 370 fps. version:1
arxiv-1609-00845 | Graph-Based Active Learning: A New Look at Expected Error Minimization | http://arxiv.org/abs/1609.00845 | id:1609.00845 author:Kwang-Sung Jun, Robert Nowak category:stat.ML cs.LG  published:2016-09-03 summary:In graph-based active learning, algorithms based on expected error minimization (EEM) have been popular and yield good empirical performance. The exact computation of EEM optimally balances exploration and exploitation. In practice, however, EEM-based algorithms employ various approximations due to the computational hardness of exact EEM. This can result in a lack of either exploration or exploitation, which can negatively impact the effectiveness of active learning. We propose a new algorithm TSA (Two-Step Approximation) that balances between exploration and exploitation efficiently while enjoying the same computational complexity as existing approximations. Finally, we empirically show the value of balancing between exploration and exploitation in both toy and real-world datasets where our method outperforms several state-of-the-art methods. version:1
arxiv-1609-00843 | An Online Universal Classifier for Binary, Multi-class and Multi-label Classification | http://arxiv.org/abs/1609.00843 | id:1609.00843 author:Meng Joo Er, Rajasekar Venkatesan, Ning Wang category:cs.LG cs.AI cs.NE  published:2016-09-03 summary:Classification involves the learning of the mapping function that associates input samples to corresponding target label. There are two major categories of classification problems: Single-label classification and Multi-label classification. Traditional binary and multi-class classifications are sub-categories of single-label classification. Several classifiers are developed for binary, multi-class and multi-label classification problems, but there are no classifiers available in the literature capable of performing all three types of classification. In this paper, a novel online universal classifier capable of performing all the three types of classification is proposed. Being a high speed online classifier, the proposed technique can be applied to streaming data applications. The performance of the developed classifier is evaluated using datasets from binary, multi-class and multi-label problems. The results obtained are compared with state-of-the-art techniques from each of the classification types. version:1
arxiv-1608-07249 | Benchmarking State-of-the-Art Deep Learning Software Tools | http://arxiv.org/abs/1608.07249 | id:1608.07249 author:Shaohuai Shi, Qiang Wang, Pengfei Xu, Xiaowen Chu category:cs.DC cs.LG  published:2016-08-25 summary:Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools coming to public. Training a deep network is usually a very time-consuming process. To address the huge computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training time. However, different tools exhibit different features and running performance when training different types of deep networks on different hardware platforms, which makes it difficult for end users to select an appropriate pair of software and hardware. In this paper, we aim to make a comparative study of the state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, TensorFlow, and Torch. We benchmark the running performance of these tools with three popular types of neural networks on two CPU platforms and three GPU platforms. Our contribution is two-fold. First, for deep learning end users, our benchmarking results can serve as a guide to selecting appropriate software tool and hardware platform. Second, for deep learning software developers, our in-depth analysis points out possible future directions to further optimize the training performance. version:3
arxiv-1608-06884 | Towards Bayesian Deep Learning: A Framework and Some Existing Methods | http://arxiv.org/abs/1608.06884 | id:1608.06884 author:Hao Wang, Dit-Yan Yeung category:stat.ML cs.CV cs.LG cs.NE  published:2016-08-24 summary:While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This paper proposes a general framework for Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this paper, we also discuss the relationship and differences between Bayesian deep learning and other related topics like Bayesian treatment of neural networks. version:2
arxiv-1609-00817 | Stochastic Learning of Multi-Instance Dictionary for Earth Mover's Distance based Histogram Comparison | http://arxiv.org/abs/1609.00817 | id:1609.00817 author:Jihong Fan, Ru-Ze Liang category:cs.CV  published:2016-09-03 summary:Dictionary plays an important role in multi-instance data representation. It maps bags of instances to histograms. Earth mover's distance (EMD) is the most effective histogram distance metric for the application of multi-instance retrieval. However, up to now, there is no existing multi-instance dictionary learning methods designed for EMD based histogram comparison. To fill this gap, we develop the first EMD-optimal dictionary learning method using stochastic optimization method. In the stochastic learning framework, we have one triplet of bags, including one basic bag, one positive bag, and one negative bag. These bags are mapped to histograms using a multi-instance dictionary. We argue that the EMD between the basic histogram and the positive histogram should be smaller than that between the basic histogram and the negative histogram. Base on this condition, we design a hinge loss. By minimizing this hinge loss and some regularization terms of the dictionary, we update the dictionary instances. The experiments over multi-instance retrieval applications shows its effectiveness when compared to other dictionary learning methods over the problems of medical image retrieval and natural language relation classification. version:1
arxiv-1609-00804 | Randomized Prediction Games for Adversarial Machine Learning | http://arxiv.org/abs/1609.00804 | id:1609.00804 author:Samuel Rota Bulò, Battista Biggio, Ignazio Pillai, Marcello Pelillo, Fabio Roli category:cs.LG cs.GT  published:2016-09-03 summary:In spam and malware detection, attackers exploit randomization to obfuscate malicious data and increase their chances of evading detection at test time; e.g., malware code is typically obfuscated using random strings or byte sequences to hide known exploits. Interestingly, randomization has also been proposed to improve security of learning algorithms against evasion attacks, as it results in hiding information about the classifier to the attacker. Recent work has proposed game-theoretical formulations to learn secure classifiers, by simulating different evasion attacks and modifying the classification function accordingly. However, both the classification function and the simulated data manipulations have been modeled in a deterministic manner, without accounting for any form of randomization. In this work, we overcome this limitation by proposing a randomized prediction game, namely, a non-cooperative game-theoretic formulation in which the classifier and the attacker make randomized strategy selections according to some probability distribution defined over the respective strategy set. We show that our approach allows one to improve the trade-off between attack detection and false alarms with respect to state-of-the-art secure classifiers, even against attacks that are different from those hypothesized during design, on application examples including handwritten digit recognition, spam and malware detection. version:1
arxiv-1609-00799 | Lexical-Morphological Modeling for Legal Text Analysis | http://arxiv.org/abs/1609.00799 | id:1609.00799 author:Danilo S. Carvalho, Minh-Tien Nguyen, Tran Xuan Chien, Minh Le Nguyen category:cs.IR cs.CL 14J30 (Primary) H.3  H.3.3  I.2.7  published:2016-09-03 summary:In the context of the Competition on Legal Information Extraction/Entailment (COLIEE), we propose a method comprising the necessary steps for finding relevant documents to a legal question and deciding on textual entailment evidence to provide a correct answer. The proposed method is based on the combination of several lexical and morphological characteristics, to build a language model and a set of features for Machine Learning algorithms. We provide a detailed study on the proposed method performance and failure cases, indicating that it is competitive with state-of-the-art approaches on Legal Information Retrieval and Question Answering, while not needing extensive training data nor depending on expert produced knowledge. The proposed method achieved significant results in the competition, indicating a substantial level of adequacy for the tasks addressed. version:1
arxiv-1609-00777 | End-to-End Reinforcement Learning of Dialogue Agents for Information Access | http://arxiv.org/abs/1609.00777 | id:1609.00777 author:Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao, Yun-Nung Chen, Faisal Ahmed, Li Deng category:cs.CL cs.LG  published:2016-09-03 summary:This paper proposes \emph{KB-InfoBot}---a dialogue agent that provides users with an entity from a knowledge base (KB) by interactively asking for its attributes. All components of the KB-InfoBot are trained in an end-to-end fashion using reinforcement learning. Goal-oriented dialogue systems typically need to interact with an external database to access real-world knowledge (e.g. movies playing in a city). Previous systems achieved this by issuing a symbolic query to the database and adding retrieved results to the dialogue state. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced "soft" posterior distribution over the KB that indicates which entities the user is interested in. We also provide a modified version of the episodic REINFORCE algorithm, which allows the KB-InfoBot to explore and learn both the policy for selecting dialogue acts and the posterior over the KB for retrieving the correct entities. Experimental results show that the end-to-end trained KB-InfoBot outperforms competitive rule-based baselines, as well as agents which are not end-to-end trainable. version:1
arxiv-1609-00770 | Stochastic Bouncy Particle Sampler | http://arxiv.org/abs/1609.00770 | id:1609.00770 author:Ari Pakman, Dar Gilboa, David Carlson, Liam Paninski category:stat.CO stat.ML  published:2016-09-03 summary:We introduce a novel stochastic version of the non-reversible, rejection-free Bouncy Particle Sampler (BPS), a Markov process whose sample trajectories are piecewise linear. The algorithm is based on simulating first arrival times in a doubly stochastic Poisson process using the thinning method, and allows efficient sampling of Bayesian posteriors in big datasets. We prove that in the BPS no bias is introduced by noisy evaluations of the log-likelihood gradient. On the other hand, we argue that efficiency considerations favor a small, controllable bias in the construction of the thinning proposals, in exchange for faster mixing. We introduce a simple regression-based proposal intensity for the thinning method that controls this trade-off. We illustrate the algorithm in several examples in which it outperforms both unbiased, but slowly mixing stochastic versions of BPS, as well as biased stochastic gradient-based samplers. version:1
arxiv-1609-00285 | Adaptive Acceleration of Sparse Coding via Matrix Factorization | http://arxiv.org/abs/1609.00285 | id:1609.00285 author:Joan Bruna, Thomas Moreau category:stat.ML  published:2016-09-01 summary:Sparse coding remains a core building block in many data analysis and machine learning pipelines. Typically it is solved by relying on generic optimization techniques, that are optimal in the class of first-order methods for non-smooth, convex functions, such as the Iterative Soft Thresholding Algorithm and its accelerated version (ISTA, FISTA). However, these methods don't exploit the particular structure of the problem at hand nor the input data distribution. An acceleration using neural networks was proposed in \citep{Gregor10}, coined LISTA, which showed empirically that one could achieve high quality estimates with few iterations by modifying the parameters of the proximal splitting appropriately. In this paper we study the reasons for such acceleration. Our mathematical analysis reveals that it is related to a specific matrix factorization of the Gram matrix of the dictionary, in which unitary transformations leverage near diagonalisation with small perturbations of the $\ell_1$ norm. When this factorization succeeds, we prove that the resulting splitting algorithm enjoys an improved convergence bound with respect to the non-adaptive version. Moreover, our analysis also shows that conditions for acceleration occur mostly at the beginning of the iterative process, consistent with numerical experiments. version:2
arxiv-1609-00672 | The Inflation Technique for Causal Inference with Latent Variables | http://arxiv.org/abs/1609.00672 | id:1609.00672 author:Elie Wolfe, Robert W. Spekkens, Tobias Fritz category:quant-ph math.ST stat.ME stat.ML stat.TH  published:2016-09-02 summary:The problem of causal inference is to determine if a given probability distribution on observed variables is compatible with some causal structure. The difficult case is when the structure includes latent variables. We here introduce the inflation technique for tackling this problem. An inflation of a causal structure is a new causal structure that can contain multiple copies of each of the original variables, but where the ancestry of each copy mirrors that of the original. For every distribution compatible with the original causal structure we identify a corresponding family of distributions, over certain subsets of inflation variables, which is compatible with the inflation structure. It follows that compatibility constraints at the inflation level can be translated to compatibility constraints at the level of the original causal structure; even if the former are weak, such as observable statistical independences implied by disjoint causal ancestry, the translated constraints can be strong. In particular, we can derive inequalities whose violation by a distribution witnesses that distribution's incompatibility with the causal structure (of which Bell inequalities and Pearl's instrumental inequality are prominent examples). We describe an algorithm for deriving all of the inequalities for the original causal structure that follow from ancestral independences in the inflation. Applied to an inflation of the Triangle scenario with binary variables, it yields inequalities that are stronger in at least some aspects than those obtainable by existing methods. We also describe an algorithm that derives a weaker set of inequalities but is much more efficient. Finally, we discuss which inflations version:1
arxiv-1608-05971 | STFCN: Spatio-Temporal FCN for Semantic Video Segmentation | http://arxiv.org/abs/1608.05971 | id:1608.05971 author:Mohsen Fayyaz, Mohammad Hajizadeh Saffar, Mohammad Sabokrou, Mahmood Fathy, Reinhard Klette, Fay Huang category:cs.CV  published:2016-08-21 summary:This paper presents a novel method to involve both spatial and temporal features for semantic video segmentation. Current work on convolutional neural networks(CNNs) has shown that CNNs provide advanced spatial features supporting a very good performance of solutions for both image and video analysis, especially for the semantic segmentation task. We investigate how involving temporal features also has a good effect on segmenting video data. We propose a module based on a long short-term memory (LSTM) architecture of a recurrent neural network for interpreting the temporal characteristics of video frames over time. Our system takes as input frames of a video and produces a correspondingly-sized output; for segmenting the video our method combines the use of three components: First, the regional spatial features of frames are extracted using a CNN; then, using LSTM the temporal features are added; finally, by deconvolving the spatio-temporal features we produce pixel-wise predictions. Our key insight is to build spatio-temporal convolutional networks (spatio-temporal CNNs) that have an end-to-end architecture for semantic video segmentation. We adapted fully some known convolutional network architectures (such as FCN-AlexNet and FCN-VGG16), and dilated convolution into our spatio-temporal CNNs. Our spatio-temporal CNNs achieve state-of-the-art semantic segmentation, as demonstrated for the Camvid and NYUDv2 datasets. version:2
arxiv-1609-00629 | SEBOOST - Boosting Stochastic Learning Using Subspace Optimization Techniques | http://arxiv.org/abs/1609.00629 | id:1609.00629 author:Elad Richardson, Rom Herskovitz, Boris Ginsburg, Michael Zibulevsky category:cs.CV cs.LG stat.ML  published:2016-09-02 summary:We present SEBOOST, a technique for boosting the performance of existing stochastic optimization methods. SEBOOST applies a secondary optimization process in the subspace spanned by the last steps and descent directions. The method was inspired by the SESOP optimization method for large-scale problems, and has been adapted for the stochastic learning framework. It can be applied on top of any existing optimization method with no need to tweak the internal algorithm. We show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters. As the boosting steps of SEBOOST are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden. We introduce two hyper-parameters that control the balance between the baseline method and the secondary optimization process. The method was evaluated on several deep learning tasks, demonstrating promising results. version:1
arxiv-1609-00626 | SynsetRank: Degree-adjusted Random Walk for Relation Identification | http://arxiv.org/abs/1609.00626 | id:1609.00626 author:Shinichi Nakajima, Sebastian Krause, Dirk Weissenborn, Nico Goernitz, Feiyu Xu category:cs.CL stat.AP  published:2016-09-02 summary:In relation extraction, a key process is to obtain good detectors that find relevant sentences describing the target relation. To minimize the necessity of labeled data for refining detectors, previous work successfully made use of BabelNet, a semantic graph structure expressing relationships between synsets, as side information or prior knowledge. The goal of this paper is to enhance the use of graph structure in the framework of random walk with a few adjustable parameters. Actually, a straightforward application of random walk degrades the performance even after parameter optimization. With the insight from this unsuccessful trial, we propose SynsetRank, which adjusts the initial probability so that high degree nodes influence the neighbors as strong as low degree nodes. In our experiment on 13 relations in the FB15K-237 dataset, SynsetRank significantly outperforms baselines and the plain random walk approach. version:1
arxiv-1609-00585 | Doubly stochastic large scale kernel learning with the empirical kernel map | http://arxiv.org/abs/1609.00585 | id:1609.00585 author:Nikolaas Steenbergen, Sebastian Schelter, Felix Bießmann category:cs.LG  published:2016-09-02 summary:With the rise of big data sets, the popularity of kernel methods declined and neural networks took over again. The main problem with kernel methods is that the kernel matrix grows quadratically with the number of data points. Most attempts to scale up kernel methods solve this problem by discarding data points or basis functions of some approximation of the kernel map. Here we present a simple yet effective alternative for scaling up kernel methods that takes into account the entire data set via doubly stochastic optimization of the emprical kernel map. The algorithm is straightforward to implement, in particular in parallel execution settings; it leverages the full power and versatility of classical kernel functions without the need to explicitly formulate a kernel map approximation. We provide empirical evidence that the algorithm works on large data sets. version:1
arxiv-1609-00577 | Generic Inference in Latent Gaussian Process Models | http://arxiv.org/abs/1609.00577 | id:1609.00577 author:Edwin V. Bonilla, Karl Krauth, Amir Dezfouli category:stat.ML  published:2016-09-02 summary:We develop an automated variational method for inference in models with Gaussian process (GP) priors and general likelihoods. The method supports multiple outputs and multiple latent functions and does not require detailed knowledge of the conditional likelihood, only needing its evaluation as a black-box function. Using a mixture of Gaussians as the variational distribution, we show that the evidence lower bound and its gradients can be estimated efficiently using empirical expectations over univariate Gaussian distributions. Furthermore, the method is scalable to large datasets which is achieved by using an augmented prior via the inducing-variable approach underpinning most sparse GP approximations, along with parallel computation and stochastic optimization. We evaluate our method with experiments on small datasets, medium-scale datasets and a large dataset, showing its competitiveness under different likelihood models and sparsity levels. Moreover, we analyze learning in our model under batch and stochastic settings, and study the effect of optimizing the inducing inputs. Finally, in the large-scale experiment, we investigate the problem of predicting airline delays and show that our method is on par with the state-of-the-art hard-coded approach for scalable GP regression. version:1
arxiv-1609-00565 | Skipping Word: A Character-Sequential Representation based Framework for Question Answering | http://arxiv.org/abs/1609.00565 | id:1609.00565 author:Lingxun Meng, Yan Li, Mengyi Liu, Peng Shu category:cs.CL  published:2016-09-02 summary:Recent works using artificial neural networks based on word distributed representation greatly boost the performance of various natural language learning tasks, especially question answering. Though, they also carry along with some attendant problems, such as corpus selection for embedding learning, dictionary transformation for different learning tasks, etc. In this paper, we propose to straightforwardly model sentences by means of character sequences, and then utilize convolutional neural networks to integrate character embedding learning together with point-wise answer selection training. Compared with deep models pre-trained on word embedding (WE) strategy, our character-sequential representation (CSR) based method shows a much simpler procedure and more stable performance across different benchmarks. Extensive experiments on two benchmark answer selection datasets exhibit the competitive performance compared with the state-of-the-art methods. version:1
arxiv-1609-00559 | Improving Correlation with Human Judgments by Embedding Second Order Vectors with Semantic Similarity | http://arxiv.org/abs/1609.00559 | id:1609.00559 author:Bridget T. McInnes, Ted Pedersen category:cs.CL  published:2016-09-02 summary:Vector space methods that measure semantic similarity and relatedness often rely on distributional information such as co--occurrence frequencies or statistical measures of association to weight the importance of particular co-occurrences. In this paper we extend these methods by embedding a measure of semantic similarity based on a human curated taxonomy into a second--order vector representation. This results in a measure of semantic relatedness that combines both the contextual information available in a corpus--based vector space representation with the semantic knowledge found in a biomedical ontology. Our results show that embedding semantic semantic similarity into a second order co--occurrence matrix improves correlation with human judgments for both similarity and relatedness. version:1
arxiv-1608-08851 | Efficient Two-Stream Motion and Appearance 3D CNNs for Video Classification | http://arxiv.org/abs/1608.08851 | id:1608.08851 author:Ali Diba, Ali Mohammad Pazandeh, Luc Van Gool category:cs.CV  published:2016-08-31 summary:The video and action classification have extremely evolved by deep neural networks specially with two stream CNN using RGB and optical flow as inputs and they present outstanding performance in terms of video analysis. One of the shortcoming of these methods is handling motion information extraction which is done out side of the CNNs and relatively time consuming also on GPUs. So proposing end-to-end methods which are exploring to learn motion representation, like 3D-CNN can achieve faster and accurate performance. We present some novel deep CNNs using 3D architecture to model actions and motion representation in an efficient way to be accurate and also as fast as real-time. Our new networks learn distinctive models to combine deep motion features into appearance model via learning optical flow features inside the network. version:2
arxiv-1609-00514 | On Horizontal and Vertical Separation in Hierarchical Text Classification | http://arxiv.org/abs/1609.00514 | id:1609.00514 author:Mostafa Dehghani, Hosein Azarbonyad, Jaap Kamps, Maarten Marx category:cs.IR cs.CL cs.IT math.IT 68P20  published:2016-09-02 summary:Hierarchy is a common and effective way of organizing data and representing their relationships at different levels of abstraction. However, hierarchical data dependencies cause difficulties in the estimation of "separable" models that can distinguish between the entities in the hierarchy. Extracting separable models of hierarchical entities requires us to take their relative position into account and to consider the different types of dependencies in the hierarchy. In this paper, we present an investigation of the effect of separability in text-based entity classification and argue that in hierarchical classification, a separation property should be established between entities not only in the same layer, but also in different layers. Our main findings are the followings. First, we analyse the importance of separability on the data representation in the task of classification and based on that, we introduce a "Strong Separation Principle" for optimizing expected effectiveness of classifiers decision based on separation property. Second, we present Hierarchical Significant Words Language Models (HSWLM) which capture all, and only, the essential features of hierarchical entities according to their relative position in the hierarchy resulting in horizontally and vertically separable models. Third, we validate our claims on real-world data and demonstrate that how HSWLM improves the accuracy of classification and how it provides transferable models over time. Although discussions in this paper focus on the classification problem, the models are applicable to any information access tasks on data that has, or can be mapped to, a hierarchical structure. version:1
arxiv-1609-00451 | Least Ambiguous Set-Valued Classifiers with Bounded Error Levels | http://arxiv.org/abs/1609.00451 | id:1609.00451 author:Mauricio Sadinle, Jing Lei, Larry Wasserman category:stat.ME cs.LG stat.ML  published:2016-09-02 summary:In most classification tasks there are observations that are ambiguous and therefore difficult to correctly label. Set-valued classification allows the classifiers to output a set of plausible labels rather than a single label, thereby giving a more appropriate and informative treatment to the labeling of ambiguous instances. We introduce a framework for multiclass set-valued classification, where the classifiers guarantee user-defined levels of coverage or confidence (the probability that the true label is contained in the set) while minimizing the ambiguity (the expected size of the output). We first derive oracle classifiers assuming the true distribution to be known. We show that the oracle classifiers are obtained from level sets of the functions that define the conditional probability of each class. Then we develop estimators with good asymptotic and finite sample properties. The proposed classifiers build on and refine many existing single-label classifiers. The optimal classifier can sometimes output the empty set. We provide two solutions to fix this issue that are suitable for various practical needs. version:1
arxiv-1609-00446 | Built-in Foreground/Background Prior for Weakly-Supervised Semantic Segmentation | http://arxiv.org/abs/1609.00446 | id:1609.00446 author:Fatemehsadat Saleh, Mohammad Sadegh Ali Akbarian, Mathieu Salzmann, Lars Petersson, Stephen Gould, Jose M. Alvarez category:cs.CV  published:2016-09-02 summary:Pixel-level annotations are expensive and time consuming to obtain. Hence, weak supervision using only image tags could have a significant impact in semantic segmentation. Recently, CNN-based methods have proposed to fine-tune pre-trained networks using image tags. Without additional information, this leads to poor localization accuracy. This problem, however, was alleviated by making use of objectness priors to generate foreground/background masks. Unfortunately these priors either require training pixel-level annotations/bounding boxes, or still yield inaccurate object boundaries. Here, we propose a novel method to extract markedly more accurate masks from the pre-trained network itself, forgoing external objectness modules. This is accomplished using the activations of the higher-level convolutional layers, smoothed by a dense CRF. We demonstrate that our method, based on these masks and a weakly-supervised loss, outperforms the state-of-the-art tag-based weakly-supervised semantic segmentation techniques. Furthermore, we introduce a new form of inexpensive weak supervision yielding an additional accuracy boost. version:1
arxiv-1609-00435 | Citation Classification for Behavioral Analysis of a Scientific Field | http://arxiv.org/abs/1609.00435 | id:1609.00435 author:David Jurgens, Srijan Kumar, Raine Hoover, Dan McFarland, Dan Jurafsky category:cs.CL cs.DL  published:2016-09-02 summary:Citations are an important indicator of the state of a scientific field, reflecting how authors frame their work, and influencing uptake by future scholars. However, our understanding of citation behavior has been limited to small-scale manual citation analysis. We perform the largest behavioral study of citations to date, analyzing how citations are both framed and taken up by scholars in one entire field: natural language processing. We introduce a new dataset of nearly 2,000 citations annotated for function and centrality, and use it to develop a state-of-the-art classifier and label the entire ACL Reference Corpus. We then study how citations are framed by authors and use both papers and online traces to track how citations are followed by readers. We demonstrate that authors are sensitive to discourse structure and publication venue when citing, that online readers follow temporal links to previous and future work rather than methodological links, and that how a paper cites related work is predictive of its citation count. Finally, we use changes in citation roles to show that the field of NLP is undergoing a significant increase in consensus. version:1
arxiv-1609-00719 | Peacock Bundles: Bundle Coloring for Graphs with Globality-Locality Trade-off | http://arxiv.org/abs/1609.00719 | id:1609.00719 author:Jaakko Peltonen, Ziyuan Lin category:cs.CG stat.ML  published:2016-09-02 summary:Bundling of graph edges (node-to-node connections) is a common technique to enhance visibility of overall trends in the edge structure of a large graph layout, and a large variety of bundling algorithms have been proposed. However, with strong bundling, it becomes hard to identify origins and destinations of individual edges. We propose a solution: we optimize edge coloring to differentiate bundled edges. We quantify strength of bundling in a flexible pairwise fashion between edges, and among bundled edges, we quantify how dissimilar their colors should be by dissimilarity of their origins and destinations. We solve the resulting nonlinear optimization, which is also interpretable as a novel dimensionality reduction task. In large graphs the necessary compromise is whether to differentiate colors sharply between locally occurring strongly bundled edges ("local bundles"), or also between the weakly bundled edges occurring globally over the graph ("global bundles"); we allow a user-set global-local tradeoff. We call the technique "peacock bundles". Experiments show the coloring clearly enhances comprehensibility of graph layouts with edge bundling. version:1
arxiv-1609-00425 | Identifying Dogmatism in Social Media: Signals and Models | http://arxiv.org/abs/1609.00425 | id:1609.00425 author:Ethan Fast, Eric Horvitz category:cs.CL cs.SI  published:2016-09-01 summary:We explore linguistic and behavioral features of dogmatism in social media and construct statistical models that can identify dogmatic comments. Our model is based on a corpus of Reddit posts, collected across a diverse set of conversational topics and annotated via paid crowdsourcing. We operationalize key aspects of dogmatism described by existing psychology theories (such as over-confidence), finding they have predictive power. We also find evidence for new signals of dogmatism, such as the tendency of dogmatic posts to refrain from signaling cognitive processes. When we use our predictive model to analyze millions of other Reddit posts, we find evidence that suggests dogmatism is a deeper personality trait, present for dogmatic users across many different domains, and that users who engage on dogmatic comments tend to show increases in dogmatic posts themselves. version:1
arxiv-1609-00368 | Ten Steps of EM Suffice for Mixtures of Two Gaussians | http://arxiv.org/abs/1609.00368 | id:1609.00368 author:Constantinos Daskalakis, Christos Tzamos, Manolis Zampetakis category:stat.ML cs.DS math.ST stat.TH  published:2016-09-01 summary:We provide global convergence guarantees for the expectation-maximization (EM) algorithm applied to mixtures of two Gaussians with known covariance matrices. We show that EM converges geometrically to the correct mean vectors, and provide simple, closed-form expressions for the convergence rate. As a simple illustration, we show that in one dimension ten steps of the EM algorithm initialized at $+ \infty$ result in less than 1% error estimation of the means. version:1
arxiv-1609-00361 | Autonomous driving challenge: To Infer the property of a dynamic object based on its motion pattern using recurrent neural network | http://arxiv.org/abs/1609.00361 | id:1609.00361 author:Mona Fathollahi, Rangachar Kasturi category:cs.CV  published:2016-09-01 summary:In autonomous driving applications a critical challenge is to identify action to take to avoid an obstacle on collision course. For example, when a heavy object is suddenly encountered it is critical to stop the vehicle or change the lane even if it causes other traffic disruptions. However,there are situations when it is preferable to collide with the object rather than take an action that would result in a much more serious accident than collision with the object. For example, a heavy object which falls from a truck should be avoided whereas a bouncing ball or a soft target such as a foam box need not be.We present a novel method to discriminate between the motion characteristics of these types of objects based on their physical properties such as bounciness, elasticity, etc.In this preliminary work, we use recurrent neural net-work with LSTM cells to train a classifier to classify objects based on their motion trajectories. We test the algorithm on synthetic data, and, as a proof of concept, demonstrate its effectiveness on a limited set of real-world data. version:1
arxiv-1609-00344 | Deep Learning Human Mind for Automated Visual Classification | http://arxiv.org/abs/1609.00344 | id:1609.00344 author:Concetto Spampinato, Simone Palazzo, Isaak Kavasidis, Daniela Giordano, Mubarak Shah, Nasim Souly category:cs.CV  published:2016-09-01 summary:What if we could effectively read the mind and transfer human visual capabilities to computer vision methods? In this paper, we aim at addressing this question by developing the first visual object classifier driven by human brain signals. In particular, we employ EEG data evoked by visual object stimuli combined with Recurrent Neural Networks (RNN) to learn a discriminative brain activity manifold of visual categories. Afterwards, we train a Convolutional Neural Network (CNN)-based regressor to project images onto the learned manifold, thus effectively allowing machines to employ human brain-based features for automated visual classification. We use a 32-channel EEG to record brain activity of seven subjects while looking at images of 40 ImageNet object classes. The proposed RNN based approach for discriminating object classes using brain signals reaches an average accuracy of about 40%, which outperforms existing methods attempting to learn EEG visual object representations. As for automated object categorization, our human brain-driven approach obtains competitive performance, comparable to those achieved by powerful CNN models, both on ImageNet and CalTech 101, thus demonstrating its classification and generalization capabilities. This gives us a real hope that, indeed, human mind can be read and transferred to machines. version:1
arxiv-1609-00288 | A Unified View of Multi-Label Performance Measures | http://arxiv.org/abs/1609.00288 | id:1609.00288 author:Xi-Zhu Wu, Zhi-Hua Zhou category:cs.LG  published:2016-09-01 summary:Multi-label classification deals with the problem where each instance is associated with multiple class labels. Because evaluation in multi-label classification is more complicated than single-label setting, a number of performance measures have been proposed. It is noticed that an algorithm usually performs differently on different measures. Therefore, it is important to understand which algorithms perform well on which measure(s) and why. In this paper, we propose a unified margin view to revisit eleven performance measures in multi-label classification. In particular, we define label-wise margin and instance-wise margin, and prove that through maximizing these margins, different corresponding performance measures will be optimized. Based on the defined margins, a max-margin approach called LIMO is designed and empirical results verify our theoretical findings. version:1
arxiv-1609-00278 | Semantic Image Based Geolocation Given a Map | http://arxiv.org/abs/1609.00278 | id:1609.00278 author:Arsalan Mousavian, Jana Kosecka category:cs.CV cs.RO  published:2016-09-01 summary:The problem visual place recognition is commonly used strategy for localization. Most successful appearance based methods typically rely on a large database of views endowed with local or global image descriptors and strive to retrieve the views of the same location. The quality of the results is often affected by the density of the reference views and the robustness of the image representation with respect to viewpoint variations, clutter and seasonal changes. In this work we present an approach for geo-locating a novel view and determining camera location and orientation using a map and a sparse set of geo-tagged reference views. We propose a novel technique for detection and identification of building facades from geo-tagged reference view using the map and geometry of the building facades. We compute the likelihood of camera location and orientation of the query images using the detected landmark (building) identities from reference views, 2D map of the environment, and geometry of building facades. We evaluate our approach for building identification and geo-localization on a new challenging outdoors urban dataset exhibiting large variations in appearance and viewpoint. version:1
arxiv-1609-00265 | Testing $k$-Monotonicity | http://arxiv.org/abs/1609.00265 | id:1609.00265 author:Clément L. Canonne, Elena Grigorescu, Siyao Guo, Akash Kumar, Karl Wimmer category:cs.DS cs.DM cs.LG  published:2016-09-01 summary:A Boolean $k$-monotone function defined over a finite poset domain ${\cal D}$ alternates between the values $0$ and $1$ at most $k$ times on any ascending chain in ${\cal D}$. Therefore, $k$-monotone functions are natural generalizations of the classical monotone functions, which are the $1$-monotone functions. Motivated by the recent interest in $k$-monotone functions in the context of circuit complexity and learning theory, and by the central role that monotonicity testing plays in the context of property testing, we initiate a systematic study of $k$-monotone functions, in the property testing model. In this model, the goal is to distinguish functions that are $k$-monotone (or are close to being $k$-monotone) from functions that are far from being $k$-monotone. Our results include the following: - We demonstrate a separation between testing $k$-monotonicity and testing monotonicity, on the hypercube domain $\{0,1\}^d$, for $k\geq 3$; - We demonstrate a separation between testing and learning on $\{0,1\}^d$, for $k=\omega(\log d)$: testing $k$-monotonicity can be performed with $2^{O(\sqrt d \cdot \log d\cdot \log{1/\varepsilon})}$ queries, while learning $k$-monotone functions requires $2^{\Omega(k\cdot \sqrt d\cdot{1/\varepsilon})}$ queries (Blais et al. (RANDOM 2015)). - We present a tolerant test for functions $f\colon[n]^d\to \{0,1\}$ with complexity independent of $n$, which makes progress on a problem left open by Berman et al. (STOC 2014). Our techniques exploit the testing-by-learning paradigm, use novel applications of Fourier analysis on the grid $[n]^d$, and draw connections to distribution testing techniques. version:1
arxiv-1609-00222 | Ternary Neural Networks for Resource-Efficient AI Applications | http://arxiv.org/abs/1609.00222 | id:1609.00222 author:Hande Alemdar, Nicholas Caldwell, Vincent Leroy, Adrien Prost-Boucle, Frédéric Pétrot category:cs.LG cs.AI cs.NE  published:2016-09-01 summary:The computation and storage requirements for Deep Neural Networks (DNNs) are usually high. This issue limit their deployability on ubiquitous computing devices such as smart phones or wearables. In this paper, we propose ternary neural networks (TNNs) in order to make deep learning more resource-efficient. We train these TNNs using a teacher-student approach. Using only ternary weights and ternary neurons, with a step activation function of two-thresholds, the student ternary network learns to mimic the behaviour of its teacher network. We propose a novel, layer-wise greedy methodology for training TNNs. During training, a ternary neural network inherently prunes the smaller weights by setting them to zero. This makes them even more compact thus more resource-friendly. We devise a purpose-built hardware design for TNNs and implement it on FPGA. The benchmark results with our purpose-built hardware running TNNs reveal that, with only 1.24 microjoules per image, we can achieve 97.76% accuracy with 5.37 microsecond latency and with a rate of 255K images per second on MNIST. version:1
arxiv-1609-00221 | Segmentation Free Object Discovery in Video | http://arxiv.org/abs/1609.00221 | id:1609.00221 author:Giovanni Cuffaro, Federico Becattini, Claudio Baecchi, Lorenzo Seidenari, Alberto Del Bimbo category:cs.CV  published:2016-09-01 summary:In this paper we present a simple yet effective approach to extend without supervision any object proposal from static images to videos. Unlike previous methods, these spatio-temporal proposals, to which we refer as tracks, are generated relying on little or no visual content by only exploiting bounding boxes spatial correlations through time. The tracks that we obtain are likely to represent objects and are a general-purpose tool to represent meaningful video content for a wide variety of tasks. For unannotated videos, tracks can be used to discover content without any supervision. As further contribution we also propose a novel and dataset-independent method to evaluate a generic object proposal based on the entropy of a classifier output response. We experiment on two competitive datasets, namely YouTube Objects and ILSVRC-2015 VID. version:1
arxiv-1609-00203 | Employing traditional machine learning algorithms for big data streams analysis: the case of object trajectory prediction | http://arxiv.org/abs/1609.00203 | id:1609.00203 author:Angelos Valsamis, Konstantinos Tserpes, Dimitrios Zissis, Dimosthenis Anagnostopoulos, Theodora Varvarigou category:cs.LG  published:2016-09-01 summary:In this paper, we model the trajectory of sea vessels and provide a service that predicts in near-real time the position of any given vessel in 4', 10', 20' and 40' time intervals. We explore the necessary tradeoffs between accuracy, performance and resource utilization are explored given the large volume and update rates of input data. We start with building models based on well-established machine learning algorithms using static datasets and multi-scan training approaches and identify the best candidate to be used in implementing a single-pass predictive approach, under real-time constraints. The results are measured in terms of accuracy and performance and are compared against the baseline kinematic equations. Results show that it is possible to efficiently model the trajectory of multiple vessels using a single model, which is trained and evaluated using an adequately large, static dataset, thus achieving a significant gain in terms of resource usage while not compromising accuracy. version:1
arxiv-1609-00162 | Transferring Object-Scene Convolutional Neural Networks for Event Recognition in Still Images | http://arxiv.org/abs/1609.00162 | id:1609.00162 author:Limin Wang, Zhe Wang, Yu Qiao, Luc Van Gool category:cs.CV  published:2016-09-01 summary:Event recognition in still images is an intriguing problem and has potential for real applications. This paper addresses the problem of event recognition by proposing a convolutional neural network that exploits knowledge of objects and scenes for event classification (OS2E-CNN). Intuitively, it stands to reason that there exists a correlation among the concepts of objects, scenes, and events. We empirically demonstrate that the recognition of objects and scenes substantially contributes to the recognition of events. Meanwhile, we propose an iterative selection method to identify a subset of object and scene classes, which help to more efficiently and effectively transfer their deep representations to event recognition. Specifically, we develop three types of transferring techniques: (1) initialization-based transferring, (2) knowledge-based transferring, and (3) data-based transferring. These newly designed transferring techniques exploit multi-task learning frameworks to incorporate extra knowledge from other networks and additional datasets into the training procedure of event CNNs. These multi-task learning frameworks turn out to be effective in reducing the effect of over-fitting and improving the generalization ability of the learned CNNs. With OS2E-CNN, we design a multi-ratio and multi-scale cropping strategy, and propose an end-to-end event recognition pipeline. We perform experiments on three event recognition benchmarks: the ChaLearn Cultural Event Recognition dataset, the Web Image Dataset for Event Recognition (WIDER), and the UIUC Sports Event dataset. The experimental results show that our proposed algorithm successfully adapts object and scene representations towards the event dataset and that it achieves the current state-of-the-art performance on these challenging datasets. version:1
arxiv-1609-00686 | Single photon in hierarchical architecture for physical reinforcement learning: Photon intelligence | http://arxiv.org/abs/1609.00686 | id:1609.00686 author:Makoto Naruse, Martin Berthel, Aurélien Drezet, Serge Huant, Hirokazu Hori, Song-Ju Kim category:cs.LG physics.optics quant-ph  published:2016-09-01 summary:Understanding and using natural processes for intelligent functionalities, referred to as natural intelligence, has recently attracted interest from a variety of fields, including post-silicon computing for artificial intelligence and decision making in the behavioural sciences. In a past study, we successfully used the wave-particle duality of single photons to solve the two-armed bandit problem, which constitutes the foundation of reinforcement learning and decision making. In this study, we propose and confirm a hierarchical architecture for single-photon-based reinforcement learning and decision making that verifies the scalability of the principle. Specifically, the four-armed bandit problem is solved given zero prior knowledge in a two-layer hierarchical architecture, where polarization is autonomously adapted in order to effect adequate decision making using single-photon measurements. In the hierarchical structure, the notion of layer-dependent decisions emerges. The optimal solutions in the coarse layer and in the fine layer, however, conflict with each other in some contradictive problems. We show that while what we call a tournament strategy resolves such contradictions, the probabilistic nature of single photons allows for the direct location of the optimal solution even for contradictive problems, hence manifesting the exploration ability of single photons. This study provides insights into photon intelligence in hierarchical architectures for future artificial intelligence as well as the potential of natural processes for intelligent functionalities. version:1
arxiv-1609-00153 | Weakly Supervised PatchNets: Describing and Aggregating Local Patches for Scene Recognition | http://arxiv.org/abs/1609.00153 | id:1609.00153 author:Zhe Wang, Limin Wang, Yali Wang, Bowen Zhang, Yu Qiao category:cs.CV  published:2016-09-01 summary:Conventional feature encoding scheme (e.g., Fisher vector) with local descriptors (e.g., SIFT) and recent deep convolutional neural networks (CNNs) are two classes of successful methods for image recognition. In this paper, we propose a hybrid representation, which leverages the great discriminative capacity of CNNs and the simplicity of descriptor encoding schema for image recognition, with a focus on scene recognition. To this end, we make three main contributions from the following aspects. First, we propose a patch-level and end-to-end architecture to model the appearance of local patches, called as PatchNet. PatchNet is essentially a customized network trained in a weakly supervised manner, which uses the image-level supervision to guide the patch-level feature extraction. Second, we present a hybrid visual representation, called as VSAD, by utilizing the robust feature representations of PatchNet to describe local patches and exploiting the semantic probabilities of PatchNet to aggregate these local patches into a global representation. Third, based on our VSAD representation, we propose a state-of-the-art scene recognition approach, which achieves excellent performance on two standard benchmarks: MIT Indoor67 (86.2%) and SUN397 (73.0%). version:1
arxiv-1609-00150 | Reward Augmented Maximum Likelihood for Neural Structured Prediction | http://arxiv.org/abs/1609.00150 | id:1609.00150 author:Mohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans category:cs.LG  published:2016-09-01 summary:A key problem in structured output prediction is direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient approach to incorporate task reward into a maximum likelihood framework. We establish a connection between the log-likelihood and regularized expected reward objectives, showing that at a zero temperature, they are approximately equivalent in the vicinity of the optimal solution. We show that optimal regularized expected reward is achieved when the conditional distribution of the outputs given the inputs is proportional to their exponentiated (temperature adjusted) rewards. Based on this observation, we optimize conditional log-probability of edited outputs that are sampled proportionally to their scaled exponentiated reward. We apply this framework to optimize edit distance in the output label space. Experiments on speech recognition and machine translation for neural sequence to sequence models show notable improvements over a maximum likelihood baseline by using edit distance augmented maximum likelihood. version:1
arxiv-1608-08968 | The Bayesian SLOPE | http://arxiv.org/abs/1608.08968 | id:1608.08968 author:Amir Sepehri category:stat.ME stat.ML  published:2016-08-31 summary:The SLOPE estimates regression coefficients by minimizing a regularized residual sum of squares using a sorted-$\ell_1$-norm penalty. The SLOPE combines testing and estimation in regression problems. It exhibits suitable variable selection and prediction properties, as well as minimax optimality. This paper introduces the Bayesian SLOPE procedure for linear regression. The classical SLOPE estimate is the posterior mode in the normal regression problem with an appropriate prior on the coefficients. The Bayesian SLOPE considers the full Bayesian model and has the advantage of offering credible sets and standard error estimates for the parameters. Moreover, the hierarchical Bayesian framework allows for full Bayesian and empirical Bayes treatment of the penalty coefficients; whereas it is not clear how to choose these coefficients when using the SLOPE on a general design matrix. A direct characterization of the posterior is provided which suggests a Gibbs sampler that does not involve latent variables. An efficient hybrid Gibbs sampler for the Bayesian SLOPE is introduced. Point estimation using the posterior mean is highlighted, which automatically facilitates the Bayesian prediction of future observations. These are demonstrated on real and synthetic data. version:2
arxiv-1609-00129 | Grid Loss: Detecting Occluded Faces | http://arxiv.org/abs/1609.00129 | id:1609.00129 author:Michael Opitz, Georg Waltner, Georg Poier, Horst Possegger, Horst Bischof category:cs.CV  published:2016-09-01 summary:Detection of partially occluded objects is a challenging computer vision problem. Standard Convolutional Neural Network (CNN) detectors fail if parts of the detection window are occluded, since not every sub-part of the window is discriminative on its own. To address this issue, we propose a novel loss layer for CNNs, named grid loss, which minimizes the error rate on sub-blocks of a convolution layer independently rather than over the whole feature map. This results in parts being more discriminative on their own, enabling the detector to recover if the detection window is partially occluded. By mapping our loss layer back to a regular fully connected layer, no additional computational cost is incurred at runtime compared to standard CNNs. We demonstrate our method for face detection on several public face detection benchmarks and show that our method outperforms regular CNNs, is suitable for realtime applications and achieves state-of-the-art performance. version:1
arxiv-1609-00116 | Neural Coarse-Graining: Extracting slowly-varying latent degrees of freedom with neural networks | http://arxiv.org/abs/1609.00116 | id:1609.00116 author:Nicholas Guttenberg, Martin Biehl, Ryota Kanai category:cs.AI cs.LG  published:2016-09-01 summary:We present a loss function for neural networks that encompasses an idea of trivial versus non-trivial predictions, such that the network jointly determines its own prediction goals and learns to satisfy them. This permits the network to choose sub-sets of a problem which are most amenable to its abilities to focus on solving, while discarding 'distracting' elements that interfere with its learning. To do this, the network first transforms the raw data into a higher-level categorical representation, and then trains a predictor from that new time series to its future. To prevent a trivial solution of mapping the signal to zero, we introduce a measure of non-triviality via a contrast between the prediction error of the learned model with a naive model of the overall signal statistics. The transform can learn to discard uninformative and unpredictable components of the signal in favor of the features which are both highly predictive and highly predictable. This creates a coarse-grained model of the time-series dynamics, focusing on predicting the slowly varying latent parameters which control the statistics of the time-series, rather than predicting the fast details directly. The result is a semi-supervised algorithm which is capable of extracting latent parameters, segmenting sections of time-series with differing statistics, and building a higher-level representation of the underlying dynamics from unlabeled data. version:1
arxiv-1609-00661 | Localization by Fusing a Group of Fingerprints via Multiple Antennas in Indoor Environment | http://arxiv.org/abs/1609.00661 | id:1609.00661 author:Xiansheng Guo, Nirwan Ansari category:stat.ML cs.IT math.IT  published:2016-09-01 summary:Most existing fingerprints-based indoor localization approaches are based on some single fingerprints, such as received signal strength (RSS), channel impulse response (CIR), and signal subspace. However, the localization accuracy obtained by the single fingerprint approach is rather susceptible to the changing environment, multi-path, and non-line-of-sight (NLOS) propagation. Furthermore, building the fingerprints is a very time consuming process. In this paper, we propose a novel localization framework by Fusing A Group Of fingerprinTs (FAGOT) via multiple antennas for the indoor environment. We first build a GrOup Of Fingerprints (GOOF), which includes five different fingerprints, namely, RSS, covariance matrix, signal subspace, fractional low order moment, and fourth-order cumulant, which are obtained by different transformations of the received signals from multiple antennas in the offline stage. Then, we design a parallel GOOF multiple classifiers based on AdaBoost (GOOF-AdaBoost) to train each of these fingerprints in parallel as five strong multiple classifiers. In the online stage, we input the corresponding transformations of the real measurements into these strong classifiers to obtain independent decisions. Finally, we propose an efficient combination fusion algorithm, namely, MUltiple Classifiers mUltiple Samples (MUCUS) fusion algorithm to improve the accuracy of localization by combining the predictions of multiple classifiers with different samples. As compared with the single fingerprint approaches, the prediction probability of our proposed approach is improved significantly. The process for building fingerprints can also be reduced drastically. We demonstrate the feasibility and performance of the proposed algorithm through extensive simulations as well as via real experimental data using a Universal Software Radio Peripheral (USRP) platform with four antennas. version:1
arxiv-1609-00096 | Image segmentation based on histogram of depth and an application in driver distraction detection | http://arxiv.org/abs/1609.00096 | id:1609.00096 author:Tran Hiep Dinh, Minh Trien Pham, Manh Duong Phung, Duc Manh Nguyen, Van Manh Hoang, Quang Vinh Tran category:cs.CV  published:2016-09-01 summary:This study proposes an approach to segment human object from a depth image based on histogram of depth values. The region of interest is first extracted based on a predefined threshold for histogram regions. A region growing process is then employed to separate multiple human bodies with the same depth interval. Our contribution is the identification of an adaptive growth threshold based on the detected histogram region. To demonstrate the effectiveness of the proposed method, an application in driver distraction detection was introduced. After successfully extracting the driver's position inside the car, we came up with a simple solution to track the driver motion. With the analysis of the difference between initial and current frame, a change of cluster position or depth value in the interested region, which cross the preset threshold, is considered as a distracted activity. The experiment results demonstrated the success of the algorithm in detecting typical distracted driving activities such as using phone for calling or texting, adjusting internal devices and drinking in real time. version:1
arxiv-1609-00086 | A novel online multi-label classifier for high-speed streaming data applications | http://arxiv.org/abs/1609.00086 | id:1609.00086 author:Rajasekar Venkatesan, Meng Joo Er, Mihika Dave, Mahardhika Pratama, Shiqian Wu category:cs.LG cs.AI cs.NE  published:2016-09-01 summary:In this paper, a high-speed online neural network classifier based on extreme learning machines for multi-label classification is proposed. In multi-label classification, each of the input data sample belongs to one or more than one of the target labels. The traditional binary and multi-class classification where each sample belongs to only one target class forms the subset of multi-label classification. Multi-label classification problems are far more complex than binary and multi-class classification problems, as both the number of target labels and each of the target labels corresponding to each of the input samples are to be identified. The proposed work exploits the high-speed nature of the extreme learning machines to achieve real-time multi-label classification of streaming data. A new threshold-based online sequential learning algorithm is proposed for high speed and streaming data classification of multi-label problems. The proposed method is experimented with six different datasets from different application domains such as multimedia, text, and biology. The hamming loss, accuracy, training time and testing time of the proposed technique is compared with nine different state-of-the-art methods. Experimental studies shows that the proposed technique outperforms the existing multi-label classifiers in terms of performance and speed. version:1
arxiv-1609-00085 | A novel progressive learning technique for multi-class classification | http://arxiv.org/abs/1609.00085 | id:1609.00085 author:Rajasekar Venkatesan, Meng Joo Er category:cs.LG cs.AI cs.NE  published:2016-09-01 summary:In this paper, a progressive learning technique for multi-class classification is proposed. This newly developed learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far. This technique is suitable for real-world applications where the number of classes is often unknown and online learning from real-time data is required. The consistency and the complexity of the progressive learning technique are analyzed. Several standard datasets are used to evaluate the performance of the developed technique. A comparative study shows that the developed technique is superior. version:1
arxiv-1609-00081 | All Fingers are not Equal: Intensity of References in Scientific Articles | http://arxiv.org/abs/1609.00081 | id:1609.00081 author:Tanmoy Chakraborty, Ramasuri Narayanam category:cs.CL cs.DL  published:2016-09-01 summary:Research accomplishment is usually measured by considering all citations with equal importance, thus ignoring the wide variety of purposes an article is being cited for. Here, we posit that measuring the intensity of a reference is crucial not only to perceive better understanding of research endeavor, but also to improve the quality of citation-based applications. To this end, we collect a rich annotated dataset with references labeled by the intensity, and propose a novel graph-based semi-supervised model, GraLap to label the intensity of references. Experiments with AAN datasets show a significant improvement compared to the baselines to achieve the true labels of the references (46% better correlation). Finally, we provide four applications to demonstrate how the knowledge of reference intensity leads to design better real-world applications. version:1
arxiv-1609-00074 | Neural Network Architecture Optimization through Submodularity and Supermodularity | http://arxiv.org/abs/1609.00074 | id:1609.00074 author:Junqi Jin, Ziang Yan, Kun Fu, Nan Jiang, Changshui Zhang category:stat.ML cs.LG  published:2016-09-01 summary:Deep learning models' architectures, including depth and width, are key factors influencing models' performance, such as test accuracy and computation time. This paper solves two problems: given computation time budget, choose an architecture to maximize accuracy, and given accuracy requirement, choose an architecture to minimize computation time. We convert this architecture optimization into a subset selection problem. With accuracy's submodularity and computation time's supermodularity, we propose efficient greedy optimization algorithms. The experiments demonstrate our algorithm's ability to find more accurate models or faster models. By analyzing architecture evolution with growing time budget, we discuss relationships among accuracy, time and architecture, and give suggestions on neural network architecture design. version:1
arxiv-1609-00072 | Attentional Push: Augmenting Salience with Shared Attention Modeling | http://arxiv.org/abs/1609.00072 | id:1609.00072 author:Siavash Gorji, James J. Clark category:cs.CV  published:2016-09-01 summary:We present a novel visual attention tracking technique based on Shared Attention modeling. Our proposed method models the viewer as a participant in the activity occurring in the scene. We go beyond image salience and instead of only computing the power of an image region to pull attention to it, we also consider the strength with which other regions of the image push attention to the region in question. We use the term Attentional Push to refer to the power of image regions to direct and manipulate the attention allocation of the viewer. An attention model is presented that incorporates the Attentional Push cues with standard image salience-based attention modeling algorithms to improve the ability to predict where viewers will fixate. Experimental evaluation validates significant improvements in predicting viewers' fixations using the proposed methodology in both static and dynamic imagery. version:1
arxiv-1609-00070 | How Much is 131 Million Dollars? Putting Numbers in Perspective with Compositional Descriptions | http://arxiv.org/abs/1609.00070 | id:1609.00070 author:Arun Tejasvi Chaganty, Percy Liang category:cs.CL  published:2016-09-01 summary:How much is 131 million US dollars? To help readers put such numbers in context, we propose a new task of automatically generating short descriptions known as perspectives, e.g. "$131 million is about the cost to employ everyone in Texas over a lunch period". First, we collect a dataset of numeric mentions in news articles, where each mention is labeled with a set of rated perspectives. We then propose a system to generate these descriptions consisting of two steps: formula construction and description generation. In construction, we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on familiarity, numeric proximity and semantic compatibility. In generation, we convert a formula into natural language using a sequence-to-sequence recurrent neural network. Our system obtains a 15.2% F1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation. version:1
arxiv-1608-08336 | Low-rank Multi-view Clustering in Third-Order Tensor Space | http://arxiv.org/abs/1608.08336 | id:1608.08336 author:Ming Yin, Junbin Gao, Shengli Xie, Yi Guo category:cs.CV  published:2016-08-30 summary:The plenty information from multiple views data as well as the complementary information among different views are usually beneficial to various tasks, e.g., clustering, classification, de-noising. Multi-view subspace clustering is based on the fact that the multi-view data are generated from a latent subspace. To recover the underlying subspace structure, the success of the sparse and/or low-rank subspace clustering has been witnessed recently. Despite some state-of-the-art subspace clustering approaches can numerically handle multi-view data, by simultaneously exploring all possible pairwise correlation within views, the high order statistics is often disregarded which can only be captured by simultaneously utilizing all views. As a consequence, the clustering performance for multi-view data is compromised. To address this issue, in this paper, a novel multi-view clustering method is proposed by using \textit{t-product} in third-order tensor space. Based on the circular convolution operation, multi-view data can be effectively represented by a \textit{t-linear} combination with sparse and low-rank penalty using "self-expressiveness". Our extensive experimental results on facial, object, digits image and text data demonstrate that the proposed method outperforms the state-of-the-art methods in terms of many criteria. version:2
arxiv-1609-00066 | A Review of Multivariate Distributions for Count Data Derived from the Poisson Distribution | http://arxiv.org/abs/1609.00066 | id:1609.00066 author:David I. Inouye, Eunho Yang, Genevera I. Allen, Pradeep Ravikumar category:stat.ME stat.ML  published:2016-08-31 summary:The Poisson distribution has been widely studied and used for modeling univariate count-valued data. Multivariate generalizations of the Poisson distribution that permit dependencies, however, have been far less popular. Yet, real-world high-dimensional count-valued data found in word counts, genomics, and crime statistics, for example, exhibit rich dependencies, and motivate the need for multivariate distributions that can appropriately model this data. We review multivariate distributions derived from the univariate Poisson, categorizing these models into three main classes: 1) where the marginal distributions are Poisson, 2) where the joint distribution is a mixture of Poissons, and 3) where the node-conditional distributions are derived from the Poisson. We discuss the development of multiple instances of these classes. Then, we extensively compare multiple models from each class on five real-world datasets from traffic accident data, crime statistics, biological next generation sequencing data and text corpora. These empirical experiments develop intuition about the comparative advantages and disadvantages of each class of multivariate distribution that was derived from the Poisson. Finally, we suggest new research directions as explored in the subsequent discussion section. (See arXiv paper comments for access to supplementary material.) version:1
arxiv-1609-00053 | Low memory implementation of Orthogonal Matching Pursuit like greedy algorithms: Analysis and Applications | http://arxiv.org/abs/1609.00053 | id:1609.00053 author:Laura Rebollo-Neira, Pradip Sasmal category:cs.CV cs.IT math.IT  published:2016-08-31 summary:The convergence analysis of a low memory implementation of the Orthogonal Matching Pursuit method, which is termed Self Projected Matching Pursuit, is presented. The approach is extended to improve the sparsity ratio of a signal representation when approximating the signal by partitioning. A backward strategy, for reducing terms in a signal decomposition, is discussed. The suitability of the methods, to be applied on cases where standard implementations of Orthogonal Matching Pursuit are not feasible due to memory requirements, is illustrated by producing high quality approximation of melodic music and X-Ray medical images. version:1
arxiv-1609-00048 | Randomized single-view algorithms for low-rank matrix approximation | http://arxiv.org/abs/1609.00048 | id:1609.00048 author:Joel A. Tropp, Alp Yurtsever, Madeleine Udell, Volkan Cevher category:cs.NA cs.DS math.NA stat.CO stat.ML  published:2016-08-31 summary:This paper develops a suite of algorithms for constructing low-rank approximations of an input matrix from a random linear image of the matrix, called a sketch. These methods can preserve structural properties of the input matrix, such as positive-semidefiniteness, and they can produce approximations with a user-specified rank. The algorithms are simple, accurate, numerically stable, and provably correct. Moreover, each method is accompanied by an informative error bound that allows users to select parameters a priori to achieve a given approximation quality. These claims are supported by computer experiments. version:1
arxiv-1609-00036 | Human Pose Estimation in Space and Time using 3D CNN | http://arxiv.org/abs/1609.00036 | id:1609.00036 author:Agne Grinciunaite, Amogh Gudi, Emrah Tasli, Marten den Uyl category:cs.CV cs.AI stat.ML  published:2016-08-31 summary:This paper explores the capabilities of convolutional neural networks to deal with a task that is easily manageable for humans: perceiving 3D pose of a human body from varying angles. However, in our approach, we are restricted to using a monocular vision system. For this purpose, we apply the convolutional neural networks approach on RGB videos and extend it to three dimensional convolutions. This is done via encoding the time dimension in videos as the 3rd dimension in convolutional space, and directly regressing to human body joint positions in 3D coordinate space. This research shows the ability of such a network to achieve state-of-the-art performance on the selected Human3.6M dataset, thus demonstrating the possibility of successfully representing a temporal data with an additional dimension in the convolutional operation. version:1
arxiv-1609-00017 | Radiation Search Operations using Scene Understanding with Autonomous UAV and UGV | http://arxiv.org/abs/1609.00017 | id:1609.00017 author:Gordon Christie, Adam Shoemaker, Kevin Kochersberger, Pratap Tokekar, Lance McLean, Alexander Leonessa category:cs.RO cs.CV  published:2016-08-31 summary:Autonomously searching for hazardous radiation sources requires the ability of the aerial and ground systems to understand the scene they are scouting. In this paper, we present systems, algorithms, and experiments to perform radiation search using unmanned aerial vehicles (UAV) and unmanned ground vehicles (UGV) by employing semantic scene segmentation. The aerial data is used to identify radiological points of interest, generate an orthophoto along with a digital elevation model (DEM) of the scene, and perform semantic segmentation to assign a category (e.g. road, grass) to each pixel in the orthophoto. We perform semantic segmentation by training a model on a dataset of images we collected and annotated, using the model to perform inference on images of the test area unseen to the model, and then refining the results with the DEM to better reason about category predictions at each pixel. We then use all of these outputs to plan a path for a UGV carrying a LiDAR to map the environment and avoid obstacles not present during the flight, and a radiation detector to collect more precise radiation measurements from the ground. Results of the analysis for each scenario tested favorably. We also note that our approach is general and has the potential to work for a variety of different sensing tasks. version:1
arxiv-1608-09014 | A Tutorial on Online Supervised Learning with Applications to Node Classification in Social Networks | http://arxiv.org/abs/1608.09014 | id:1608.09014 author:Alexander Rakhlin, Karthik Sridharan category:cs.LG stat.ML  published:2016-08-31 summary:We revisit the elegant observation of T. Cover '65 which, perhaps, is not as well-known to the broader community as it should be. The first goal of the tutorial is to explain---through the prism of this elementary result---how to solve certain sequence prediction problems by modeling sets of solutions rather than the unknown data-generating mechanism. We extend Cover's observation in several directions and focus on computational aspects of the proposed algorithms. The applicability of the methods is illustrated on several examples, including node classification in a network. The second aim of this tutorial is to demonstrate the following phenomenon: it is possible to predict as well as a combinatorial "benchmark" for which we have a certain multiplicative approximation algorithm, even if the exact computation of the benchmark given all the data is NP-hard. The proposed prediction methods, therefore, circumvent some of the computational difficulties associated with finding the best model given the data. These difficulties arise rather quickly when one attempts to develop a probabilistic model for graph-based or other problems with a combinatorial structure. version:1
arxiv-1608-09005 | Measuring the Quality of Exercises | http://arxiv.org/abs/1608.09005 | id:1608.09005 author:Paritosh Parmar, Brendan Tran Morris category:cs.CV  published:2016-08-31 summary:This work explores the problem of exercise quality measurement since it is essential for effective management of diseases like cerebral palsy (CP). This work examines the assessment of quality of large amplitude movement (LAM) exercises designed to treat CP in an automated fashion. Exercise data was collected by trained participants to generate ideal examples to use as a positive samples for machine learning. Following that, subjects were asked to deliberately make subtle errors during the exercise, such as restricting movements, as is commonly seen in cases of patients suffering from CP. The quality measurement problem was then posed as a classification to determine whether an example exercise was either "good" or "bad". Popular machine learning techniques for classification, including support vector machines (SVM), single and doublelayered neural networks (NN), boosted decision trees, and dynamic time warping (DTW), were compared. The AdaBoosted tree performed best with an accuracy of 94.68% demonstrating the feasibility of assessing exercise quality. version:1
arxiv-1608-09000 | Learning Syntactic Program Transformations from Examples | http://arxiv.org/abs/1608.09000 | id:1608.09000 author:Reudismam Rolim, Gustavo Soares, Loris D'Antoni, Oleksandr Polozov, Sumit Gulwani, Rohit Gheyi, Ryo Suzuki, Bjoern Hartmann category:cs.SE cs.LG cs.PL  published:2016-08-31 summary:IDEs, such as Visual Studio, automate common transformations, such as Rename and Extract Method refactorings. However, extending these catalogs of transformations is complex and time-consuming. A similar phenomenon appears in intelligent tutoring systems where instructors have to write cumbersome code transformations that describe "common faults" to fix similar student submissions to programming assignments. We present REFAZER, a technique for automatically generating program transformations. REFAZER builds on the observation that code edits performed by developers can be used as examples for learning transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-the-art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of edits used by students to fix incorrect programming assignment submissions, we learn transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 59 scenarios of repetitive edits taken from 3 C# open-source projects, REFAZER learns the intended program transformation in 83% of the cases. version:1
arxiv-1608-08984 | Towards Competitive Classifiers for Unbalanced Classification Problems: A Study on the Performance Scores | http://arxiv.org/abs/1608.08984 | id:1608.08984 author:Jonathan Ortigosa-Hernández, Iñaki Inza, Jose A. Lozano category:stat.ML cs.LG  published:2016-08-31 summary:Although a great methodological effort has been invested in proposing competitive solutions to the class-imbalance problem, little effort has been made in pursuing a theoretical understanding of this matter. In order to shed some light on this topic, we perform, through a novel framework, an exhaustive analysis of the adequateness of the most commonly used performance scores to assess this complex scenario. We conclude that using unweighted H\"older means with exponent $p \leq 1$ to average the recalls of all the classes produces adequate scores which are capable of determining whether a classifier is competitive. Then, we review the major solutions presented in the class-imbalance literature. Since any learning task can be defined as an optimisation problem where a loss function, usually connected to a particular score, is minimised, our goal, here, is to find whether the learning tasks found in the literature are also oriented to maximise the previously detected adequate scores. We conclude that they usually maximise the unweighted H\"older mean with $p = 1$ (a-mean). Finally, we provide bounds on the values of the studied performance scores which guarantee a classifier with a higher recall than the random classifier in each and every class. version:1
arxiv-1608-08974 | Interpreting Visual Question Answering Models | http://arxiv.org/abs/1608.08974 | id:1608.08974 author:Yash Goyal, Akrit Mohapatra, Devi Parikh, Dhruv Batra category:cs.CV cs.AI cs.CL cs.LG  published:2016-08-31 summary:Deep neural networks have shown striking progress and obtained state-of-the-art results in many AI research fields in the recent years. However, it is often unsatisfying to not know why they predict what they do. In this paper, we address the problem of interpreting Visual Question Answering (VQA) models. Specifically, we are interested in finding what part of the input (pixels in images or words in questions) the VQA model focuses on while answering the question. To tackle this problem, we use two visualization techniques -- guided backpropagation and occlusion -- to find important words in the question and important regions in the image. We then present qualitative and quantitative analyses of these importance maps. version:1
arxiv-1608-08967 | Robustness of classifiers: from adversarial to random noise | http://arxiv.org/abs/1608.08967 | id:1608.08967 author:Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard category:cs.LG cs.CV stat.ML  published:2016-08-31 summary:Several recent works have shown that state-of-the-art classifiers are vulnerable to worst-case (i.e., adversarial) perturbations of the datapoints. On the other hand, it has been empirically observed that these same classifiers are relatively robust to random noise. In this paper, we propose to study a \textit{semi-random} noise regime that generalizes both the random and worst-case noise regimes. We propose the first quantitative analysis of the robustness of nonlinear classifiers in this general noise regime. We establish precise theoretical bounds on the robustness of classifiers in this general regime, which depend on the curvature of the classifier's decision boundary. Our bounds confirm and quantify the empirical observations that classifiers satisfying curvature constraints are robust to random noise. Moreover, we quantify the robustness of classifiers in terms of the subspace dimension in the semi-random noise regime, and show that our bounds remarkably interpolate between the worst-case and random noise regimes. We perform experiments and show that the derived bounds provide very accurate estimates when applied to various state-of-the-art deep neural networks and datasets. This result suggests bounds on the curvature of the classifiers' decision boundaries that we support experimentally, and more generally offers important insights onto the geometry of high dimensional classification problems. version:1
arxiv-1608-08953 | Dynamic Allocation of Crowd Contributions for Sentiment Analysis during the 2016 U.S. Presidential Election | http://arxiv.org/abs/1608.08953 | id:1608.08953 author:Mehrnoosh Sameki, Mattia Gentil, Kate K. Mays, Lei Guo, Margrit Betke category:cs.HC cs.CL cs.SI  published:2016-08-31 summary:Opinions about the 2016 U.S. Presidential Candidates have been expressed in millions of tweets that are challenging to analyze automatically. Crowdsourcing the analysis of political tweets effectively is also difficult, due to large inter-rater disagreements when sarcasm is involved. Each tweet is typically analyzed by a fixed number of workers and majority voting. We here propose a crowdsourcing framework that instead uses a dynamic allocation of the number of workers. We explore two dynamic-allocation methods: (1) The number of workers queried to label a tweet is computed offline based on the predicted difficulty of discerning the sentiment of a particular tweet. (2) The number of crowd workers is determined online, during an iterative crowd sourcing process, based on inter-rater agreements between labels.We applied our approach to 1,000 twitter messages about the four U.S. presidential candidates Clinton, Cruz, Sanders, and Trump, collected during February 2016. We implemented the two proposed methods using decision trees that allocate more crowd efforts to tweets predicted to be sarcastic. We show that our framework outperforms the traditional static allocation scheme. It collects opinion labels from the crowd at a much lower cost while maintaining labeling accuracy. version:1
arxiv-1608-08940 | Hash2Vec, Feature Hashing for Word Embeddings | http://arxiv.org/abs/1608.08940 | id:1608.08940 author:Luis Argerich, Joaquín Torré Zaffaroni, Matías J Cano category:cs.CL cs.IR cs.LG  published:2016-08-31 summary:In this paper we propose the application of feature hashing to create word embeddings for natural language processing. Feature hashing has been used successfully to create document vectors in related tasks like document classification. In this work we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data. The results show that this algorithm, that does not need training, is able to capture the semantic meaning of words. We compare the results against GloVe showing that they are similar. As far as we know this is the first application of feature hashing to the word embeddings problem and the results indicate this is a scalable technique with practical results for NLP applications. version:1
arxiv-1608-08927 | The Generalized Smallest Grammar Problem | http://arxiv.org/abs/1608.08927 | id:1608.08927 author:Payam Siyari, Matthias Gallé category:cs.CL cs.AI cs.DS cs.IT math.IT  published:2016-08-31 summary:The Smallest Grammar Problem -- the problem of finding the smallest context-free grammar that generates exactly one given sequence -- has never been successfully applied to grammatical inference. We investigate the reasons and propose an extended formulation that seeks to minimize non-recursive grammars, instead of straight-line programs. In addition, we provide very efficient algorithms that approximate the minimization problem of this class of grammars. Our empirical evaluation shows that we are able to find smaller models than the current best approximations to the Smallest Grammar Problem on standard benchmarks, and that the inferred rules capture much better the syntactic structure of natural language. version:1
arxiv-1608-08925 | Learning to Personalize from Observational Data | http://arxiv.org/abs/1608.08925 | id:1608.08925 author:Nathan Kallus category:stat.ML cs.LG  published:2016-08-31 summary:We study the problem of learning to choose from m discrete treatment options (e.g., medical drugs) the one with best causal effect for a particular instance (e.g., patient) characterized by an observation of covariates. The training data consists of observations of covariates, treatment, and the outcome of the treatment. We recast the problem of learning to personalize from these observational data as a single learning task, which we use to develop four specific machine learning methods to directly address the personalization problem, two with a unique interpretability property. We also show how to validate personalization models on observational data, proposing the new coefficient of personalization as a unitless measure of effectiveness. We demonstrate the power of the new methods in two specific personalized medicine and policymaking applications and show they provide a significant advantage over standard approaches. version:1
arxiv-1609-00718 | Convolutional Neural Networks for Text Categorization: Shallow Word-level vs. Deep Character-level | http://arxiv.org/abs/1609.00718 | id:1609.00718 author:Rie Johnson, Tong Zhang category:cs.CL cs.LG stat.ML  published:2016-08-31 summary:This paper reports the performances of shallow word-level convolutional neural networks (CNN), our earlier work (2015), on the eight datasets with relatively large training data that were used for testing the very deep character-level CNN in Conneau et al. (2016). Our findings are as follows. The shallow word-level CNNs achieve better error rates than the error rates reported in Conneau et al., though the results should be interpreted with some consideration due to the unique pre-processing of Conneau et al. The shallow word-level CNN uses more parameters and therefore requires more storage than the deep character-level CNN; however, the shallow word-level CNN computes much faster. version:1
arxiv-1608-08905 | A Novel Online Real-time Classifier for Multi-label Data Streams | http://arxiv.org/abs/1608.08905 | id:1608.08905 author:Rajasekar Venkatesan, Meng Joo Er, Shiqian Wu, Mahardhika Pratama category:cs.LG cs.AI cs.NE  published:2016-08-31 summary:In this paper, a novel extreme learning machine based online multi-label classifier for real-time data streams is proposed. Multi-label classification is one of the actively researched machine learning paradigm that has gained much attention in the recent years due to its rapidly increasing real world applications. In contrast to traditional binary and multi-class classification, multi-label classification involves association of each of the input samples with a set of target labels simultaneously. There are no real-time online neural network based multi-label classifier available in the literature. In this paper, we exploit the inherent nature of high speed exhibited by the extreme learning machines to develop a novel online real-time classifier for multi-label data streams. The developed classifier is experimented with datasets from different application domains for consistency, performance and speed. The experimental studies show that the proposed method outperforms the existing state-of-the-art techniques in terms of speed and accuracy and can classify multi-label data streams in real-time. version:1
arxiv-1608-08898 | A High Speed Multi-label Classifier based on Extreme Learning Machines | http://arxiv.org/abs/1608.08898 | id:1608.08898 author:Meng Joo Er, Rajasekar Venkatesan, Ning Wang category:cs.LG cs.AI cs.NE  published:2016-08-31 summary:In this paper a high speed neural network classifier based on extreme learning machines for multi-label classification problem is proposed and dis-cussed. Multi-label classification is a superset of traditional binary and multi-class classification problems. The proposed work extends the extreme learning machine technique to adapt to the multi-label problems. As opposed to the single-label problem, both the number of labels the sample belongs to, and each of those target labels are to be identified for multi-label classification resulting in in-creased complexity. The proposed high speed multi-label classifier is applied to six benchmark datasets comprising of different application areas such as multi-media, text and biology. The training time and testing time of the classifier are compared with those of the state-of-the-arts methods. Experimental studies show that for all the six datasets, our proposed technique have faster execution speed and better performance, thereby outperforming all the existing multi-label clas-sification methods. version:1
arxiv-1608-08878 | Facial Surface Analysis using Iso-Geodesic Curves in Three Dimensional Face Recognition System | http://arxiv.org/abs/1608.08878 | id:1608.08878 author:Rachid Ahdid, El Mahdi Barrah, Said Safi, Bouzid Manaut category:cs.CV  published:2016-08-31 summary:In this paper, we present an automatic 3D face recognition system. This system is based on the representation of human faces surfaces as collections of Iso-Geodesic Curves (IGC) using 3D Fast Marching algorithm. To compare two facial surfaces, we compute a geodesic distance between a pair of facial curves using a Riemannian geometry. In the classifying step, we use: Neural Networks (NN), K-Nearest Neighbor (KNN) and Support Vector Machines (SVM). To test this method and evaluate its performance, a simulation series of experiments were performed on 3D Shape REtrieval Contest 2008 database (SHREC2008). version:1
arxiv-1608-08868 | Demographic Dialectal Variation in Social Media: A Case Study of African-American English | http://arxiv.org/abs/1608.08868 | id:1608.08868 author:Su Lin Blodgett, Lisa Green, Brendan O'Connor category:cs.CL  published:2016-08-31 summary:Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter. We propose a distantly supervised model to identify AAE-like language from demographics associated with geo-located messages, and we verify that this language follows well-known AAE linguistic phenomena. In addition, we analyze the quality of existing language identification and dependency parsing tools on AAE-like text, demonstrating that they perform poorly on such text compared to text associated with white speakers. We also provide an ensemble classifier for language identification which eliminates this disparity and release a new corpus of tweets containing AAE-like language. version:1
arxiv-1608-08852 | A Mathematical Framework for Feature Selection from Real-World Data with Non-Linear Observations | http://arxiv.org/abs/1608.08852 | id:1608.08852 author:Martin Genzel, Gitta Kutyniok category:stat.ML  published:2016-08-31 summary:In this paper, we study the challenge of feature selection based on a relatively small collection of sample pairs $\{(x_i, y_i)\}_{1 \leq i \leq m}$. The observations $y_i \in \mathbb{R}$ are thereby supposed to follow a noisy single-index model, depending on a certain set of signal variables. A major difficulty is that these variables usually cannot be observed directly, but rather arise as hidden factors in the actual data vectors $x_i \in \mathbb{R}^d$ (feature variables). We will prove that a successful variable selection is still possible in this setup, even when the applied estimator does not have any knowledge of the underlying model parameters and only takes the 'raw' samples $\{(x_i, y_i)\}_{1 \leq i \leq m}$ as input. The model assumptions of our results will be fairly general, allowing for non-linear observations, arbitrary convex signal structures as well as strictly convex loss functions. This is particularly appealing for practical purposes, since in many applications, already standard methods, e.g., the Lasso or logistic regression, yield surprisingly good outcomes. Apart from a general discussion of the practical scope of our theoretical findings, we will also derive a rigorous guarantee for a specific real-world problem, namely sparse feature extraction from (proteomics-based) mass spectrometry data. version:1
arxiv-1608-08831 | Spatio-colour Asplünd 's metric and Logarithmic Image Processing for Colour Images (LIPC) | http://arxiv.org/abs/1608.08831 | id:1608.08831 author:Guillaume Noyel, Michel Jourlin category:cs.CV  published:2016-08-31 summary:Aspl\"und 's metric, which is useful for pattern matching, consists in a double-sided probing, i.e. the over-graph and the sub-graph of a function are probed jointly. This paper extends the Aspl\"und 's metric we previously defined for colour and multivariate images using a marginal approach (i.e. component by component) to the first spatio-colour Aspl\"und 's metric based on the vectorial colour LIP model (LIPC). LIPC is a non-linear model with operations between colour images which are consistent with the human visual system. The defined colour metric is insensitive to lighting variations and a variant which is robust to noise is used for colour pattern matching. version:1
arxiv-1608-08792 | CliqueCNN: Deep Unsupervised Exemplar Learning | http://arxiv.org/abs/1608.08792 | id:1608.08792 author:Miguel A. Bautista, Artsiom Sanakoyeu, Ekaterina Sutter, Björn Ommer category:cs.CV  published:2016-08-31 summary:Exemplar learning is a powerful paradigm for discovering visual similarities in an unsupervised manner. In this context, however, the recent breakthrough in deep learning could not yet unfold its full potential. With only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of Convolutional Neural networks is impaired. Given weak estimates of local distance we propose a single optimization problem to extract batches of samples with mutually consistent relations. Conflicting relations are distributed over different batches and similar samples are grouped into compact cliques. Learning exemplar similarities is framed as a sequence of clique categorization tasks. The CNN then consolidates transitivity relations within and between cliques and learns a single representation for all samples without the need for labels. The proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification. version:1
arxiv-1608-08526 | Multi-Person Pose Estimation with Local Joint-to-Person Associations | http://arxiv.org/abs/1608.08526 | id:1608.08526 author:Umar Iqbal, Juergen Gall category:cs.CV  published:2016-08-30 summary:Despite of the recent success of neural networks for human pose estimation, current approaches are limited to pose estimation of a single person and cannot handle humans in groups or crowds. In this work, we propose a method that estimates the poses of multiple persons in an image in which a person can be occluded by another person or might be truncated. To this end, we consider multi-person pose estimation as a joint-to-person association problem. We construct a fully connected graph from a set of detected joint candidates in an image and resolve the joint-to-person association and outlier detection using integer linear programming. Since solving joint-to-person association jointly for all persons in an image is an NP-hard problem and even approximations are expensive, we solve the problem locally for each person. On the challenging MPII Human Pose Dataset for multiple persons, our approach achieves the accuracy of a state-of-the-art method, but it is 6,000 to 19,000 times faster. version:2
arxiv-1608-08782 | Training Deep Spiking Neural Networks using Backpropagation | http://arxiv.org/abs/1608.08782 | id:1608.08782 author:Jun Haeng Lee, Tobi Delbruck, Michael Pfeiffer category:cs.NE  published:2016-08-31 summary:Deep spiking neural networks (SNNs) hold great potential for improving the latency and energy efficiency of deep neural networks through event-based computation. However, training such networks is difficult due to the non-differentiable nature of asynchronous spike events. In this paper, we introduce a novel technique, which treats the membrane potentials of spiking neurons as differentiable signals, where discontinuities at spike times are only considered as noise. This enables an error backpropagation mechanism for deep SNNs, which works directly on spike signals and membrane potentials. Thus, compared with previous methods relying on indirect training and conversion, our technique has the potential to capture the statics of spikes more precisely. Our novel framework outperforms all previously reported results for SNNs on the permutation invariant MNIST benchmark, as well as the N-MNIST benchmark recorded with event-based vision sensors. version:1
arxiv-1608-08761 | hi-RF: Incremental Learning Random Forest for large-scale multi-class Data Classification | http://arxiv.org/abs/1608.08761 | id:1608.08761 author:Tingting Xie, Yuxing Peng, Changjian Wang category:cs.LG stat.ML  published:2016-08-31 summary:In recent years, dynamically growing data and incrementally growing number of classes pose new challenges to large-scale data classification research. Most traditional methods struggle to balance the precision and computational burden when data and its number of classes increased. However, some methods are with weak precision, and the others are time-consuming. In this paper, we propose an incremental learning method, namely, heterogeneous incremental Nearest Class Mean Random Forest (hi-RF), to handle this issue. It is a heterogeneous method that either replaces trees or updates trees leaves in the random forest adaptively, to reduce the computational time in comparable performance, when data of new classes arrive. Specifically, to keep the accuracy, one proportion of trees are replaced by new NCM decision trees; to reduce the computational load, the rest trees are updated their leaves probabilities only. Most of all, out-of-bag estimation and out-of-bag boosting are proposed to balance the accuracy and the computational efficiency. Fair experiments were conducted and demonstrated its comparable precision with much less computational time. version:1
arxiv-1608-08104 | Constraint matrix factorization for space variant PSFs field restoration | http://arxiv.org/abs/1608.08104 | id:1608.08104 author:F. M. Ngolè Mboula, J. -L. Starck, K. Okumura, J. Amiaux, P. Hudelot category:cs.CV astro-ph.IM 00  published:2016-08-29 summary:Context: in large-scale spatial surveys, the Point Spread Function (PSF) varies across the instrument field of view (FOV). Local measurements of the PSFs are given by the isolated stars images. Yet, these estimates may not be directly usable for post-processings because of the observational noise and potentially the aliasing. Aims: given a set of aliased and noisy stars images from a telescope, we want to estimate well-resolved and noise-free PSFs at the observed stars positions, in particular, exploiting the spatial correlation of the PSFs across the FOV. Contributions: we introduce RCA (Resolved Components Analysis) which is a noise-robust dimension reduction and super-resolution method based on matrix factorization. We propose an original way of using the PSFs spatial correlation in the restoration process through sparsity. The introduced formalism can be applied to correlated data sets with respect to any euclidean parametric space. Results: we tested our method on simulated monochromatic PSFs of Euclid telescope (launch planned for 2020). The proposed method outperforms existing PSFs restoration and dimension reduction methods. We show that a coupled sparsity constraint on individual PSFs and their spatial distribution yields a significant improvement on both the restored PSFs shapes and the PSFs subspace identification, in presence of aliasing. Perspectives: RCA can be naturally extended to account for the wavelength dependency of the PSFs. version:3
arxiv-1608-08738 | A Dictionary-based Approach to Racism Detection in Dutch Social Media | http://arxiv.org/abs/1608.08738 | id:1608.08738 author:Stéphan Tulkens, Lisa Hilte, Elise Lodewyckx, Ben Verhoeven, Walter Daelemans category:cs.CL  published:2016-08-31 summary:We present a dictionary-based approach to racism detection in Dutch social media comments, which were retrieved from two public Belgian social media sites likely to attract racist reactions. These comments were labeled as racist or non-racist by multiple annotators. For our approach, three discourse dictionaries were created: first, we created a dictionary by retrieving possibly racist and more neutral terms from the training data, and then augmenting these with more general words to remove some bias. A second dictionary was created through automatic expansion using a \texttt{word2vec} model trained on a large corpus of general Dutch text. Finally, a third dictionary was created by manually filtering out incorrect expansions. We trained multiple Support Vector Machines, using the distribution of words over the different categories in the dictionaries as features. The best-performing model used the manually cleaned dictionary and obtained an F-score of 0.46 for the racist class on a test set consisting of unseen Dutch comments, retrieved from the same sites used for the training set. The automated expansion of the dictionary only slightly boosted the model's performance, and this increase in performance was not statistically significant. The fact that the coverage of the expanded dictionaries did increase indicates that the words that were automatically added did occur in the corpus, but were not able to meaningfully impact performance. The dictionaries, code, and the procedure for requesting the corpus are available at: https://github.com/clips/hades version:1
arxiv-1608-08716 | Measuring Machine Intelligence Through Visual Question Answering | http://arxiv.org/abs/1608.08716 | id:1608.08716 author:C. Lawrence Zitnick, Aishwarya Agrawal, Stanislaw Antol, Margaret Mitchell, Dhruv Batra, Devi Parikh category:cs.AI cs.CL cs.CV cs.LG  published:2016-08-31 summary:As machines have become more intelligent, there has been a renewed interest in methods for measuring their intelligence. A common approach is to propose tasks for which a human excels, but one which machines find difficult. However, an ideal task should also be easy to evaluate and not be easily gameable. We begin with a case study exploring the recently popular task of image captioning and its limitations as a task for measuring machine intelligence. An alternative and more promising task is Visual Question Answering that tests a machine's ability to reason about language and vision. We describe a dataset unprecedented in size created for the task that contains over 760,000 human generated questions about images. Using around 10 million human generated answers, machines may be easily evaluated. version:1
arxiv-1608-08711 | Engagement Detection in Meetings | http://arxiv.org/abs/1608.08711 | id:1608.08711 author:Maria Frank, Ghassem Tofighi, Haisong Gu, Renate Fruchter category:cs.CV cs.HC  published:2016-08-31 summary:Group meetings are frequent business events aimed to develop and conduct project work, such as Big Room design and construction project meetings. To be effective in these meetings, participants need to have an engaged mental state. The mental state of participants however, is hidden from other participants, and thereby difficult to evaluate. Mental state is understood as an inner process of thinking and feeling, that is formed of a conglomerate of mental representations and propositional attitudes. There is a need to create transparency of these hidden states to understand, evaluate and influence them. Facilitators need to evaluate the meeting situation and adjust for higher engagement and productivity. This paper presents a framework that defines a spectrum of engagement states and an array of classifiers aimed to detect the engagement state of participants in real time. The Engagement Framework integrates multi-modal information from 2D and 3D imaging and sound. Engagement is detected and evaluated at participants and aggregated at group level. We use empirical data collected at the lab of Konica Minolta, Inc. to test initial applications of this framework. The paper presents examples of the tested engagement classifiers, which are based on research in psychology, communication, and human computer interaction. Their accuracy is illustrated in dyadic interaction for engagement detection. In closing we discuss the potential extension to complex group collaboration settings and future feedback implementations. version:1
arxiv-1608-08710 | Pruning Filters for Efficient ConvNets | http://arxiv.org/abs/1608.08710 | id:1608.08710 author:Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf category:cs.CV cs.LG  published:2016-08-31 summary:Convolutional Neural Networks (CNNs) are extensively used in image and video recognition, natural language processing and other machine learning applications. The success of CNNs in these areas corresponds with a significant increase in the number of parameters and computation costs. Recent approaches towards reducing these overheads involve pruning and compressing the weights of various layers without hurting the overall CNN performance. However, using model compression to generate sparse CNNs mostly reduces parameters from the fully connected layers and may not significantly reduce the final computation costs. In this paper, we present a compression technique for CNNs, where we prune the filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole planes in the network, together with their connecting convolution kernels, the computational costs are reduced significantly. In contrast to other techniques proposed for pruning networks, this approach does not result in sparse connectivity patterns. Hence, our techniques do not need the support of sparse convolution libraries and can work with the most efficient BLAS operations for matrix multiplications. In our results, we show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% while regaining close to the original accuracy by retraining the networks. version:1
arxiv-1608-08698 | Reconstructing parameters of spreading models from partial observations | http://arxiv.org/abs/1608.08698 | id:1608.08698 author:Andrey Y. Lokhov category:cs.SI cond-mat.dis-nn physics.soc-ph q-bio.PE stat.ML  published:2016-08-31 summary:Spreading processes are often modelled as a stochastic dynamics occurring on top of a given network with edge weights corresponding to the transmission probabilities. Knowledge of veracious transmission probabilities is essential for prediction, optimization, and control of diffusion dynamics. Unfortunately, in most cases the transmission rates are unknown and need to be reconstructed from the spreading data. Moreover, in realistic settings it is impossible to monitor the state of each node at every time, and thus the data is highly incomplete. We introduce an efficient dynamic message-passing algorithm, which is able to reconstruct parameters of the spreading model given only partial information on the activation times of nodes in the network. The method is generalizable to a large class of dynamic models, as well to the case of temporal graphs. version:1
arxiv-1608-08659 | Joint Estimation of Multiple Dependent Gaussian Graphical Models with Applications to Mouse Genomics | http://arxiv.org/abs/1608.08659 | id:1608.08659 author:Yuying Xie, Yufeng Liu, William Valdar category:stat.ML  published:2016-08-30 summary:Gaussian graphical models are widely used to represent conditional dependence among random variables. In this paper, we propose a novel estimator for data arising from a group of Gaussian graphical models that are themselves dependent. A motivating example is that of modeling gene expression collected on multiple tissues from the same individual: here the multivariate outcome is affected by dependencies acting not only at the level of the specific tissues, but also at the level of the whole body; existing methods that assume independence among graphs are not applicable in this case. To estimate multiple dependent graphs, we decompose the problem into two graphical layers: the systemic layer, which affects all outcomes and thereby induces cross- graph dependence, and the category-specific layer, which represents graph-specific variation. We propose a graphical EM technique that estimates both layers jointly, establish estimation consistency and selection sparsistency of the proposed estimator, and confirm by simulation that the EM method is superior to a simple one-step method. We apply our technique to mouse genomics data and obtain biologically plausible results. version:1
arxiv-1608-08614 | What makes ImageNet good for transfer learning? | http://arxiv.org/abs/1608.08614 | id:1608.08614 author:Minyoung Huh, Pulkit Agrawal, Alexei A. Efros category:cs.CV cs.AI cs.LG  published:2016-08-30 summary:The tremendous success of features learnt using the ImageNet classification task on a wide range of transfer tasks begs the question: what are the intrinsic properties of the ImageNet dataset that are critical for learning good, general-purpose features? This work provides an empirical investigation of various facets of this question: Is more pre-training data always better? How does feature quality depend on the number of training examples per class? Does adding more object classes improve performance? For the same data budget, how should the data be split into classes? Is fine-grained recognition necessary for learning good features? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class? version:1
arxiv-1608-08607 | Adaptive Two-Level Matching-Based Selection for Decomposition Multi-Objective Optimization | http://arxiv.org/abs/1608.08607 | id:1608.08607 author:Mengyuan Wu, Ke Li, Sam Kwong, Yu Zhou, Qingfu Zhang category:cs.NE  published:2016-08-30 summary:The balance between convergence and diversity is a key issue of evolutionary multi-objective optimization. The recently proposed stable matching-based selection provides a new perspective to handle this balance under the framework of decomposition multi-objective optimization. In particular, the stable matching between subproblems and solutions, which achieves an equilibrium between their mutual preferences, implicitly strikes a balance between the convergence and diversity. Nevertheless, the original stable matching model has a high risk of matching a solution with a unfavorable subproblem which finally leads to an imbalanced selection result. In this paper, we propose an adaptive two-level stable matching-based selection for decomposition multi-objective optimization. Specifically, borrowing the idea of stable matching with incomplete lists, we match each solution with one of its favorite subproblems by restricting the length of its preference list during the first-level stable matching. During the second-level stable matching, the remaining subproblems are thereafter matched with their favorite solutions according to the classic stable matching model. In particular, we develop an adaptive mechanism to automatically set the length of preference list for each solution according to its local competitiveness. The performance of our proposed method is validated and compared with several state-of-the-art evolutionary multi-objective optimization algorithms on 62 benchmark problem instances. Empirical results fully demonstrate the competitive performance of our proposed method on problems with complicated Pareto sets and those with more than three objectives. version:1
arxiv-1608-07187 | Semantics derived automatically from language corpora necessarily contain human biases | http://arxiv.org/abs/1608.07187 | id:1608.07187 author:Aylin Caliskan-Islam, Joanna J. Bryson, Arvind Narayanan category:cs.AI cs.CL cs.CY cs.LG  published:2016-08-25 summary:Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here. version:2
arxiv-1608-08574 | Applying Naive Bayes Classification to Google Play Apps Categorization | http://arxiv.org/abs/1608.08574 | id:1608.08574 author:Babatunde Olabenjo category:cs.LG cs.IR  published:2016-08-30 summary:There are over one million apps on Google Play Store and over half a million publishers. Having such a huge number of apps and developers can pose a challenge to app users and new publishers on the store. Discovering apps can be challenging if apps are not correctly published in the right category, and, in turn, reduce earnings for app developers. Additionally, with over 41 categories on Google Play Store, deciding on the right category to publish an app can be challenging for developers due to the number of categories they have to choose from. Machine Learning has been very useful, especially in classification problems such sentiment analysis, document classification and spam detection. These strategies can also be applied to app categorization on Google Play Store to suggest appropriate categories for app publishers using details from their application. In this project, we built two variations of the Naive Bayes classifier using open metadata from top developer apps on Google Play Store in other to classify new apps on the store. These classifiers are then evaluated using various evaluation methods and their results compared against each other. The results show that the Naive Bayes algorithm performs well for our classification problem and can potentially automate app categorization for Android app publishers on Google Play Store version:1
arxiv-1608-08515 | Language Detection For Short Text Messages In Social Media | http://arxiv.org/abs/1608.08515 | id:1608.08515 author:Ivana Balazevic, Mikio Braun, Klaus-Robert Müller category:cs.CL cs.AI  published:2016-08-30 summary:With the constant growth of the World Wide Web and the number of documents in different languages accordingly, the need for reliable language detection tools has increased as well. Platforms such as Twitter with predominantly short texts are becoming important information resources, which additionally imposes the need for short texts language detection algorithms. In this paper, we show how incorporating personalized user-specific information into the language detection algorithm leads to an important improvement of detection results. To choose the best algorithm for language detection for short text messages, we investigate several machine learning approaches. These approaches include the use of the well-known classifiers such as SVM and logistic regression, a dictionary based approach, and a probabilistic model based on modified Kneser-Ney smoothing. Furthermore, the extension of the probabilistic model to include additional user-specific information such as evidence accumulation per user and user interface language is explored, with the goal of improving the classification performance. The proposed approaches are evaluated on randomly collected Twitter data containing Latin as well as non-Latin alphabet languages and the quality of the obtained results is compared, followed by the selection of the best performing algorithm. This algorithm is then evaluated against two already existing general language detection tools: Chromium Compact Language Detector 2 (CLD2) and langid, where our method significantly outperforms the results achieved by both of the mentioned methods. Additionally, a preview of benefits and possible applications of having a reliable language detection algorithm is given. version:1
arxiv-1608-08471 | New Methods to Improve Large-Scale Microscopy Image Analysis with Prior Knowledge and Uncertainty | http://arxiv.org/abs/1608.08471 | id:1608.08471 author:Johannes Stegmaier category:cs.CV  published:2016-08-30 summary:Multidimensional imaging techniques provide powerful ways to examine various kinds of scientific questions. The routinely produced datasets in the terabyte-range, however, can hardly be analyzed manually and require an extensive use of automated image analysis. The present thesis introduces a new concept for the estimation and propagation of uncertainty involved in image analysis operators and new segmentation algorithms that are suitable for terabyte-scale analyses of 3D+t microscopy images. version:1
arxiv-1608-06622 | The discriminative Kalman filter for nonlinear and non-Gaussian sequential Bayesian filtering | http://arxiv.org/abs/1608.06622 | id:1608.06622 author:Michael C. Burkhart, David M. Brandman, Carlos E. Vargas-Irwin, Matthew T. Harrison category:stat.ML  published:2016-08-23 summary:The Kalman filter (KF) is used in a variety of applications for computing the posterior distribution of latent states in a state space model. The model requires a linear relationship between states and observations. Extensions to the Kalman filter have been proposed that incorporate linear approximations to nonlinear models, such as the extended Kalman filter (EKF) and the unscented Kalman filter (UKF). However, we argue that in cases where the dimensionality of observed variables greatly exceeds the dimensionality of state variables, a model for $p(\text{state} \text{observation})$ proves both easier to learn and more accurate for latent space estimation. We derive and validate what we call the discriminative Kalman filter (DKF): a closed-form discriminative version of Bayesian filtering that readily incorporates off-the-shelf discriminative learning techniques. Further, we demonstrate that given mild assumptions, highly non-linear models for $p(\text{state} \text{observation})$ can be specified. We motivate and validate on synthetic datasets and in neural decoding from non-human primates, showing substantial increases in decoding performance versus the standard Kalman filter. version:2
arxiv-1608-08435 | Multi-Label Classification Method Based on Extreme Learning Machines | http://arxiv.org/abs/1608.08435 | id:1608.08435 author:Rajasekar Venkatesan, Meng Joo Er category:cs.LG cs.AI cs.NE  published:2016-08-30 summary:In this paper, an Extreme Learning Machine (ELM) based technique for Multi-label classification problems is proposed and discussed. In multi-label classification, each of the input data samples belongs to one or more than one class labels. The traditional binary and multi-class classification problems are the subset of the multi-label problem with the number of labels corresponding to each sample limited to one. The proposed ELM based multi-label classification technique is evaluated with six different benchmark multi-label datasets from different domains such as multimedia, text and biology. A detailed comparison of the results is made by comparing the proposed method with the results from nine state of the arts techniques for five different evaluation metrics. The nine methods are chosen from different categories of multi-label methods. The comparative results shows that the proposed Extreme Learning Machine based multi-label classification technique is a better alternative than the existing state of the art methods for multi-label problems. version:1
arxiv-1608-08434 | Multi-Class Multi-Object Tracking using Changing Point Detection | http://arxiv.org/abs/1608.08434 | id:1608.08434 author:Byungjae Lee, Enkhbayar Erdenee, Songguo Jin, Phill Kyu Rhee category:cs.CV  published:2016-08-30 summary:This paper presents a robust multi-class multi-object tracking (MCMOT) formulated by a Bayesian filtering framework. Multi-object tracking for unlimited object classes is conducted by combining detection responses and changing point detection (CPD) algorithm. The CPD model is used to observe abrupt or abnormal changes due to a drift and an occlusion based spatiotemporal characteristics of track states. The ensemble of convolutional neural network (CNN) based object detector and Lucas-Kanede Tracker (KLT) based motion detector is employed to compute the likelihoods of foreground regions as the detection responses of different object classes. Extensive experiments are performed using lately introduced challenging benchmark videos; ImageNet VID and MOT benchmark dataset. The comparison to state-of-the-art video tracking techniques shows very encouraging results. version:1
arxiv-1608-07897 | Using k-nearest neighbors to construct cancelable minutiae templates | http://arxiv.org/abs/1608.07897 | id:1608.07897 author:Qinghai Gao category:cs.CV  published:2016-08-29 summary:Fingerprint is widely used in a variety of applications. Security measures have to be taken to protect the privacy of fingerprint data. Cancelable biometrics is proposed as an effective mechanism of using and protecting biometrics. In this paper we propose a new method of constructing cancelable fingerprint template by combining real template with synthetic template. Specifically, each user is given one synthetic minutia template generated with random number generator. Every minutia point from the real template is individually thrown into the synthetic template, from which its k-nearest neighbors are found. The verification template is constructed by combining an arbitrary set of the k-nearest neighbors. To prove the validity of the scheme, testing is carried out on three databases. The results show that the constructed templates satisfy the requirements of cancelable biometrics. version:2
arxiv-1608-08395 | Motion Representation with Acceleration Images | http://arxiv.org/abs/1608.08395 | id:1608.08395 author:Hirokatsu Kataoka, Yun He, Soma Shirakabe, Yutaka Satoh category:cs.CV cs.RO  published:2016-08-30 summary:Information of time differentiation is extremely important cue for a motion representation. We have applied first-order differential velocity from a positional information, moreover we believe that second-order differential acceleration is also a significant feature in a motion representation. However, an acceleration image based on a typical optical flow includes motion noises. We have not employed the acceleration image because the noises are too strong to catch an effective motion feature in an image sequence. On one hand, the recent convolutional neural networks (CNN) are robust against input noises. In this paper, we employ acceleration-stream in addition to the spatial- and temporal-stream based on the two-stream CNN. We clearly show the effectiveness of adding the acceleration stream to the two-stream CNN. version:1
arxiv-1608-08362 | Incremental Nonlinear System Identification and Adaptive Particle Filtering Using Gaussian Process | http://arxiv.org/abs/1608.08362 | id:1608.08362 author:Vahid Bastani, Lucio Marcenaro, Carlo Regazzoni category:stat.ML  published:2016-08-30 summary:An incremental/online state dynamic learning method is proposed for identification of the nonlinear Gaussian state space models. The method embeds the stochastic variational sparse Gaussian process as the probabilistic state dynamic model inside a particle filter framework. Model updating is done at measurement sample rate using stochastic gradient descent based optimization implemented in the state estimation filtering loop. The performance of the proposed method is compared with state-of-the-art Gaussian process based batch learning methods. Finally, it is shown that the state estimation performance significantly improves due to the online learning of state dynamics. version:1
arxiv-1608-08339 | American Sign Language fingerspelling recognition from video: Methods for unrestricted recognition and signer-independence | http://arxiv.org/abs/1608.08339 | id:1608.08339 author:Taehwan Kim category:cs.CL cs.CV  published:2016-08-30 summary:In this thesis, we study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL, and recognizing it is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work, we propose several types of recognition approaches, and explore the signer variation problem. Our best-performing models are segmental (semi-Markov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 8% letter error rates. The signer-independent setting is much more challenging, but with neural network adaptation we achieve up to 17% letter error rates. version:1
arxiv-1608-05745 | RETAIN: Interpretable Predictive Model in Healthcare using Reverse Time Attention Mechanism | http://arxiv.org/abs/1608.05745 | id:1608.05745 author:Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, Jimeng Sun category:cs.LG cs.AI cs.NE  published:2016-08-19 summary:Accuracy and interpretation are two goals of any successful predictive models. Most existing works have to suffer the tradeoff between the two by either picking complex black box models such as recurrent neural networks (RNN) or relying on less accurate traditional models with better interpretation such as logistic regression. To address this dilemma, we present REverse Time AttentIoN model (RETAIN) for analyzing EHR data that achieves high accuracy while remaining clinically interpretable. RETAIN is a two-level neural attention model that can find influential past visits and significant clinical variables within those visits (e.g,. key diagnoses). RETAIN mimics physician practice by attending the EHR data in a reverse time order so that more recent clinical visits will likely get higher attention. Experiments on a large real EHR dataset of 14 million visits from 263K patients over 8 years confirmed the comparable predictive accuracy and computational scalability to the state-of-the-art methods such as RNN. Finally, we demonstrate the clinical interpretation with concrete examples from RETAIN. version:2
arxiv-1608-08337 | Data Dependent Convergence for Distributed Stochastic Optimization | http://arxiv.org/abs/1608.08337 | id:1608.08337 author:Avleen S. Bijral category:math.OC cs.LG stat.ML  published:2016-08-30 summary:In this dissertation we propose alternative analysis of distributed stochastic gradient descent (SGD) algorithms that rely on spectral properties of the data covariance. As a consequence we can relate questions pertaining to speedups and convergence rates for distributed SGD to the data distribution instead of the regularity properties of the objective functions. More precisely we show that this rate depends on the spectral norm of the sample covariance matrix. An estimate of this norm can provide practitioners with guidance towards a potential gain in algorithm performance. For example many sparse datasets with low spectral norm prove to be amenable to gains in distributed settings. Towards establishing this data dependence we first study a distributed consensus-based SGD algorithm and show that the rate of convergence involves the spectral norm of the sample covariance matrix when the underlying data is assumed to be independent and identically distributed (homogenous). This dependence allows us to identify network regimes that prove to be beneficial for datasets with low sample covariance spectral norm. Existing consensus based analyses prove to be sub-optimal in the homogenous setting. Our analysis method also allows us to find data-dependent convergence rates as we limit the amount of communication. Spreading a fixed amount of data across more nodes slows convergence; in the asymptotic regime we show that adding more machines can help when minimizing twice-differentiable losses. Since the mini-batch results don't follow from the consensus results we propose a different data dependent analysis thereby providing theoretical validation for why certain datasets are more amenable to mini-batching. We also provide empirical evidence for results in this thesis. version:1
arxiv-1608-08334 | Egocentric Meets Surveillance | http://arxiv.org/abs/1608.08334 | id:1608.08334 author:Shervin Ardeshir, Ali Borji category:cs.CV  published:2016-08-30 summary:Thanks to the availability and increasing popularity of Egocentric cameras such as GoPro cameras, glasses, and etc. we have been provided with a plethora of videos captured from the first person perspective. Surveillance cameras and Unmanned Aerial Vehicles(also known as drones) also offer tremendous amount of videos, mostly with top-down or oblique view-point. Egocentric vision and top-view surveillance videos have been studied extensively in the past in the computer vision community. However, the relationship between the two has yet to be explored thoroughly. In this effort, we attempt to explore this relationship by approaching two questions. First, having a set of egocentric videos and a top-view video, can we verify if the top-view video contains all, or some of the egocentric viewers present in the egocentric set? And second, can we identify the egocentric viewers in the content of the top-view video? In other words, can we find the cameramen in the surveillance videos? These problems can become more challenging when the videos are not time-synchronous. Thus we formalize the problem in a way which handles and also estimates the unknown relative time-delays between the egocentric videos and the top-view video. We formulate the problem as a spectral graph matching instance, and jointly seek the optimal assignments and relative time-delays of the videos. As a result, we spatiotemporally localize the egocentric observers in the top-view video. We model each view (egocentric or top) using a graph, and compute the assignment and time-delays in an iterative-alternative fashion. version:1
arxiv-1608-08306 | Method for Improved DL CoMP Implementation in Heterogeneous Networks | http://arxiv.org/abs/1608.08306 | id:1608.08306 author:Faris B. Mismar category:stat.ML cs.NI  published:2016-08-30 summary:I propose a novel method for practical Joint Processing DL CoMP implementation in LTE/LTE-A systems using a supervised machine learning technique. DL CoMP has not been thoroughly studied in previous work although cluster formation and interference mitigation have been studied extensively. In this paper, I attempt to improve the cell-edge user data rate served by a heterogeneous network cluster by means of dynamically changing the DL SINR threshold at which DL CoMP is triggered. I do so by allowing the base stations to derive a threshold on the basis of machine learning inference. The simulation results show an improved user throughput at the cell edge of 40% and a 6.4% improvement to the average cell throughput compared to the baseline of static triggering. version:1
arxiv-1608-08305 | Utilizing Large Scale Vision and Text Datasets for Image Segmentation from Referring Expressions | http://arxiv.org/abs/1608.08305 | id:1608.08305 author:Ronghang Hu, Marcus Rohrbach, Subhashini Venugopalan, Trevor Darrell category:cs.CV  published:2016-08-30 summary:Image segmentation from referring expressions is a joint vision and language modeling task, where the input is an image and a textual expression describing a particular region in the image; and the goal is to localize and segment the specific image region based on the given expression. One major difficulty to train such language-based image segmentation systems is the lack of datasets with joint vision and text annotations. Although existing vision datasets such as MS COCO provide image captions, there are few datasets with region-level textual annotations for images, and these are often smaller in scale. In this paper, we explore how existing large scale vision-only and text-only datasets can be utilized to train models for image segmentation from referring expressions. We propose a method to address this problem, and show in experiments that our method can help this joint vision and language modeling task with vision-only and text-only data and outperforms previous results. version:1
arxiv-1608-08267 | Learning and Inferring Relations in Cortical Networks | http://arxiv.org/abs/1608.08267 | id:1608.08267 author:Peter U. Diehl, Matthew Cook category:cs.NE q-bio.NC  published:2016-08-29 summary:A pressing scientific challenge is to understand how brains work. Of particular interest is the neocortex,the part of the brain that is especially large in humans, capable of handling a wide variety of tasks including visual, auditory, language, motor, and abstract processing. These functionalities are processed in different self-organized regions of the neocortical sheet, and yet the anatomical structure carrying out the processing is relatively uniform across the sheet. We are at a loss to explain, simulate, or understand such a multi-functional homogeneous sheet-like computational structure - we do not have computational models which work in this way. Here we present an important step towards developing such models: we show how uniform modules of excitatory and inhibitory neurons can be connected bidirectionally in a network that, when exposed to input in the form of population codes, learns the input encodings as well as the relationships between the inputs. STDP learning rules lead the modules to self-organize into a relational network, which is able to infer missing inputs,restore noisy signals, decide between conflicting inputs, and combine cues to improve estimates. These networks show that it is possible for a homogeneous network of spiking units to self-organize so as to provide meaningful processing of its inputs. If such networks can be scaled up, they could provide an initial computational model relevant to the large scale anatomy of the neocortex. version:1
arxiv-1608-08266 | Visualizing and Understanding Sum-Product Networks | http://arxiv.org/abs/1608.08266 | id:1608.08266 author:Antonio Vergari, Nicola Di Mauro, Floriana Esposito category:cs.LG stat.ML  published:2016-08-29 summary:Sum-Product Networks (SPNs) are recently introduced deep tractable probabilistic models by which several kinds of inference queries can be answered exactly and in a tractable time. Up to now, they have been largely used as black box density estimators, assessed only by comparing their likelihood scores only. In this paper we explore and exploit the inner representations learned by SPNs. We do this with a threefold aim: first we want to get a better understanding of the inner workings of SPNs; secondly, we seek additional ways to evaluate one SPN model and compare it against other probabilistic models, providing diagnostic tools to practitioners; lastly, we want to empirically evaluate how good and meaningful the extracted representations are, as in a classic Representation Learning framework. In order to do so we revise their interpretation as deep neural networks and we propose to exploit several visualization techniques on their node activations and network outputs under different types of inference queries. To investigate these models as feature extractors, we plug some SPNs, learned in a greedy unsupervised fashion on image datasets, in supervised classification learning tasks. We extract several embedding types from node activations by filtering nodes by their type, by their associated feature abstraction level and by their scope. In a thorough empirical comparison we prove them to be competitive against those generated from popular feature extractors as Restricted Boltzmann Machines. Finally, we investigate embeddings generated from random probabilistic marginal queries as means to compare other tractable probabilistic models on a common ground, extending our experiments to Mixtures of Trees. version:1
arxiv-1608-08265 | About Learning in Recurrent Bistable Gradient Networks | http://arxiv.org/abs/1608.08265 | id:1608.08265 author:J. Fischer, S. Lackner category:cs.NE  published:2016-08-29 summary:Recurrent Bistable Gradient Networks are attractor based neural networks characterized by bistable dynamics of each single neuron. Coupled together using linear interaction determined by the interconnection weights, these networks do not suffer from spurious states or very limited capacity anymore. Vladimir Chinarov and Michael Menzinger, who invented these networks, trained them using Hebb's learning rule. We show, that this way of computing the weights leads to unwanted behaviour and limitations of the networks capabilities. Furthermore we evince, that using the first order of Hintons Contrastive Divergence algorithm leads to a quite promising recurrent neural network. These findings are tested by learning images of the MNIST database for handwritten numbers. version:1
arxiv-1608-08251 | Construction of Convex Sets on Quadrilateral Ordered Tiles or Graphs with Propagation Neighborhood Operations. Dales, Concavity Structures. Application to Gray Image Analysis of Human-Readable Shapes | http://arxiv.org/abs/1608.08251 | id:1608.08251 author:Igor Polkovnikov category:cs.CV  published:2016-08-29 summary:An effort has been made to show mathematicians some new ideas applied to image analysis. Gray images are presented as tilings. Based on topological properties of the tiling, a number of gray convex hulls: maximal, minimal, and oriented ones are constructed and some are proved. They are constructed with only one operation. Two tilings are used in the Constraint and Allowance types of operations. New type of concavity described: a dale. All operations are parallel, possible to realize clock-less. Convexities define what is the background. They are treated as separate gray objects. There are multiple relations among them and their descendants. Via that, topological size of concavities is proposed. Constructed with the same type of operations, Rays and Angles in a tiling define possible spatial relations. Notions like "strokes" are defined through concavities. Unusual effects on levelized gray objects are shown. It is illustrated how alphabet and complex hieroglyphs can be described through concavities and their relations. A hypothesis of living organisms image analysis is proposed. A number of examples with symbols and a human face are calculated with new Asynchwave C++ software library. version:1
arxiv-1608-08242 | Temporal Convolutional Networks: A Unified Approach to Action Segmentation | http://arxiv.org/abs/1608.08242 | id:1608.08242 author:Colin Lea, Rene Vidal, Austin Reiter, Gregory D. Hager category:cs.CV  published:2016-08-29 summary:The dominant paradigm for video-based action segmentation is composed of two steps: first, for each frame, compute low-level features using Dense Trajectories or a Convolutional Neural Network that encode spatiotemporal information locally, and second, input these features into a classifier that captures high-level temporal relationships, such as a Recurrent Neural Network (RNN). While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a unified approach, as demonstrated by our Temporal Convolutional Network (TCN), that hierarchically captures relationships at low-, intermediate-, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an RNN. version:1
arxiv-1608-08225 | Why does deep and cheap learning work so well? | http://arxiv.org/abs/1608.08225 | id:1608.08225 author:Henry W. Lin, Max Tegmark category:cond-mat.dis-nn cs.LG cs.NE stat.ML  published:2016-08-29 summary:We show how the success of deep learning depends not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can be approximated through "cheap learning" with exponentially fewer parameters than generic ones, because they have simplifying properties tracing back to the laws of physics. The exceptional simplicity of physics-based functions hinges on properties such as symmetry, locality, compositionality and polynomial log-probability, and we explore how these properties translate into exceptionally simple neural networks approximating both natural phenomena such as images and abstract representations thereof such as drawings. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to renormalization group procedures. Various "no-flattening theorems" show when these efficient deep networks cannot be accurately approximated by shallow ones without efficiency loss - even for linear networks. version:1
arxiv-1608-08188 | Visual Question: Predicting If a Crowd Will Agree on the Answer | http://arxiv.org/abs/1608.08188 | id:1608.08188 author:Danna Gurari, Kristen Grauman category:cs.AI cs.CL cs.CV cs.HC  published:2016-08-29 summary:Visual question answering (VQA) systems are emerging from a desire to empower users to ask any natural language question about visual content and receive a valid answer in response. However, close examination of the VQA problem reveals an unavoidable, entangled problem that multiple humans may or may not always agree on a single answer to a visual question. We train a model to automatically predict from a visual question whether a crowd would agree on a single answer. We then propose how to exploit this system in a novel application to efficiently allocate human effort to collect answers to visual questions. Specifically, we propose a crowdsourcing system that automatically solicits fewer human responses when answer agreement is expected and more human responses when answer disagreement is expected. Our system improves upon existing crowdsourcing systems, typically eliminating at least 20% of human effort with no loss to the information collected from the crowd. version:1
arxiv-1608-08182 | Data Poisoning Attacks on Factorization-Based Collaborative Filtering | http://arxiv.org/abs/1608.08182 | id:1608.08182 author:Bo Li, Yining Wang, Aarti Singh, Yevgeniy Vorobeychik category:cs.LG cs.CR cs.IR  published:2016-08-29 summary:Recommendation and collaborative filtering systems are important in modern information and e-commerce applications. As these systems are becoming increasingly popular in the industry, their outputs could affect business decision making, introducing incentives for an adversarial party to compromise the availability or integrity of such systems. We introduce a data poisoning attack on collaborative filtering systems. We demonstrate how a powerful attacker with full knowledge of the learner can generate malicious data so as to maximize his/her malicious objectives, while at the same time mimicking normal user behavior to avoid being detected. While the complete knowledge assumption seems extreme, it enables a robust assessment of the vulnerability of collaborative filtering schemes to highly motivated attacks. We present efficient solutions for two popular factorization-based collaborative filtering algorithms: the \emph{alternative minimization} formulation and the \emph{nuclear norm minimization} method. Finally, we test the effectiveness of our proposed algorithms on real-world data and discuss potential defensive strategies. version:1
arxiv-1608-08176 | What is Wrong with Topic Modeling? (and How to Fix it Using Search-based SE) | http://arxiv.org/abs/1608.08176 | id:1608.08176 author:Amritanshu Agrawal, Wei Fu, Tim Menzies category:cs.SE cs.AI cs.CL cs.IR  published:2016-08-29 summary:Topic Modeling finds human-readable structures in large sets of unstructured SE data. A widely used topic modeler is Latent Dirichlet Allocation. When run on SE data, LDA suffers from "order effects" i.e. different topics be generated if the training data was shuffled into a different order. Such order effects introduce a systematic error for any study that uses topics to make conclusions. This paper introduces LDADE, a Search-Based SE tool that tunes LDA's parameters using DE (Differential Evolution). LDADE has been tested on data from a programmer information exchange site (Stackoverflow), title and abstract text of thousands of SE papers, and software defect reports from NASA. Results were collected across different implementations of LDA (Python+Scikit-Learn, Scala+Spark); across different platforms (Linux, Macintosh) and for different kinds of LDAs (the traditional VEM method, or using Gibbs sampling). In all tests, the pattern was the same: LDADE's tunings dramatically reduces topic instability. The implications of this study for other software analytics tasks is now an open and pressing issue. In how many domains can search-based SE dramatically improve software analytics? version:1
arxiv-1608-08173 | Real-Time Visual Tracking: Promoting the Robustness of Correlation Filter Learning | http://arxiv.org/abs/1608.08173 | id:1608.08173 author:Yao Sui, Ziming Zhang, Guanghui Wang, Yafei Tang, Li Zhang category:cs.CV  published:2016-08-29 summary:Correlation filtering based tracking model has received lots of attention and achieved great success in real-time tracking, howev- er, the lost function in current correlation filtering paradigm could not reliably response to the appearance changes caused by occlusion and il- lumination variations. This study intends to promote the robustness of the correlation filter learning. By exploiting the anisotropy of the filter response, three sparsity related loss functions are proposed to alleviate the overfitting issue of previous methods and improve the overall tracking performance. As a result, three real-time trackers are implemented. Ex- tensive experiments in various challenging situations demonstrate that the robustness of the learned correlation filter has been greatly improved via the designed loss functions. In addition, the study reveals, from an ex- perimental perspective, how different loss functions essentially influence the tracking performance. An important conclusion is that the sensitivity of the peak values of the filter in successive frames is consistent with the tracking performance. This is a useful reference criterion in designing a robust correlation filter for visual tracking. version:1
arxiv-1608-08171 | Tracking Completion | http://arxiv.org/abs/1608.08171 | id:1608.08171 author:Yao Sui, Guanghui Wang, Yafei Tang, Li Zhang category:cs.CV  published:2016-08-29 summary:A fundamental component of modern trackers is an online learned tracking model, which is typically modeled either globally or lo- cally. The two kinds of models perform differently in terms of effectiveness and robustness under different challenging situations. This work exploits the advantages of both models. A subspace model, from a global perspec- tive, is learned from previously obtained targets via rank-minimization to address the tracking, and a pixel-level local observation is leveraged si- multaneously, from a local point of view, to augment the subspace model. A matrix completion method is employed to integrate the two models. Unlike previous tracking methods, which locate the target among all fully observed target candidates, the proposed approach first estimates an expected target via the matrix completion through partially observed target candidates, and then, identifies the target according to the esti- mation accuracy with respect to the target candidates. Specifically, the tracking is formulated as a problem of target appearance estimation. Extensive experiments on various challenging video sequences verify the effectiveness of the proposed approach and demonstrate that the pro- posed tracker outperforms other popular state-of-the-art trackers. version:1
arxiv-1608-08149 | ORBSLAM-based Endoscope Tracking and 3D Reconstruction | http://arxiv.org/abs/1608.08149 | id:1608.08149 author:Nader Mahmoud, Iñigo Cirauqui, Alexandre Hostettler, Christophe Doignon, Luc Soler, Jacques Marescaux, J. M. M. Montiel category:cs.CV  published:2016-08-29 summary:We aim to track the endoscope location inside the surgical scene and provide 3D reconstruction, in real-time, from the sole input of the image sequence captured by the monocular endoscope. This information offers new possibilities for developing surgical navigation and augmented reality applications. The main benefit of this approach is the lack of extra tracking elements which can disturb the surgeon performance in the clinical routine. It is our first contribution to exploit ORBSLAM, one of the best performing monocular SLAM algorithms, to estimate both of the endoscope location, and 3D structure of the surgical scene. However, the reconstructed 3D map poorly describe textureless soft organ surfaces such as liver. It is our second contribution to extend ORBSLAM to be able to reconstruct a semi-dense map of soft organs. Experimental results on in-vivo pigs, shows a robust endoscope tracking even with organs deformations and partial instrument occlusions. It also shows the reconstruction density, and accuracy against ground truth surface obtained from CT. version:1
arxiv-1608-08139 | Where is my Phone ? Personal Object Retrieval from Egocentric Images | http://arxiv.org/abs/1608.08139 | id:1608.08139 author:Cristian Reyes, Eva Mohedano, Kevin McGuinness, Noel E. O'Connor, Xavier Giro-i-Nieto category:cs.IR cs.CV H.3.3; I.4.9  published:2016-08-29 summary:This work presents a retrieval pipeline and evaluation scheme for the problem of finding the last appearance of personal objects in a large dataset of images captured from a wearable camera. Each personal object is modelled by a small set of images that define a query for a visual search engine.The retrieved results are reranked considering the temporal timestamps of the images to increase the relevance of the later detections. Finally, a temporal interleaving of the results is introduced for robustness against false detections. The Mean Reciprocal Rank is proposed as a metric to evaluate this problem. This application could help into developing personal assistants capable of helping users when they do not remember where they left their personal belongings. version:1
arxiv-1608-08128 | Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks | http://arxiv.org/abs/1608.08128 | id:1608.08128 author:Alberto Montes, Amaia Salvador, Xavier Giro-i-Nieto category:cs.CV I.4.8; I.5.4  published:2016-08-29 summary:This thesis explore different approaches using Convolutional and Recurrent Neural Networks to classify and temporally localize activities on videos, furthermore an implementation to achieve it has been proposed. As the first step, features have been extracted from video frames using an state of the art 3D Convolutional Neural Network. This features are fed in a recurrent neural network that solves the activity classification and temporally location tasks in a simple and flexible way. Different architectures and configurations have been tested in order to achieve the best performance and learning of the video dataset provided. In addition it has been studied different kind of post processing over the trained network's output to achieve a better results on the temporally localization of activities on the videos. The results provided by the neural network developed in this thesis have been submitted to the ActivityNet Challenge 2016 of the CVPR, achieving competitive results using a simple and flexible architecture. version:1
arxiv-1608-08063 | Wasserstein Discriminant Analysis | http://arxiv.org/abs/1608.08063 | id:1608.08063 author:Rémi Flamary, Marco Cuturi, Nicolas Courty, Alain Rakotomamonjy category:stat.ML cs.LG  published:2016-08-29 summary:Wasserstein Discriminant Analysis (WDA) is a new supervised method that can improve classification of high-dimensional data by computing a suitable linear map onto a lower dimensional subspace. Following the blueprint of classical Linear Discriminant Analysis (LDA), WDA selects the projection matrix that maximizes the ratio of two quantities: the dispersion of projected points coming from different classes, divided by the dispersion of projected points coming from the same class. To quantify dispersion, WDA uses regularized Wasserstein distances, rather than cross-variance measures which have been usually considered, notably in LDA. Thanks to the the underlying principles of optimal transport, WDA is able to capture both global (at distribution scale) and local (at samples scale) interactions between classes. Regularized Wasserstein distances can be computed using the Sinkhorn matrix scaling algorithm; We show that the optimization of WDA can be tackled using automatic differentiation of Sinkhorn iterations. Numerical experiments show promising results both in terms of prediction and visualization on toy examples and real life datasets such as MNIST and on deep features obtained from a subset of the Caltech dataset. version:1
arxiv-1608-08052 | Robust Discriminative Clustering with Sparse Regularizers | http://arxiv.org/abs/1608.08052 | id:1608.08052 author:Nicolas Flammarion, Balamurugan Palaniappan, Francis Bach category:stat.ML cs.LG  published:2016-08-29 summary:Clustering high-dimensional data often requires some form of dimensionality reduction, where clustered variables are separated from "noise-looking" variables. We cast this problem as finding a low-dimensional projection of the data which is well-clustered. This yields a one-dimensional projection in the simplest situation with two clusters, and extends naturally to a multi-label scenario for more than two clusters. In this paper, (a) we first show that this joint clustering and dimension reduction formulation is equivalent to previously proposed discriminative clustering frameworks, thus leading to convex relaxations of the problem, (b) we propose a novel sparse extension, which is still cast as a convex relaxation and allows estimation in higher dimensions, (c) we propose a natural extension for the multi-label scenario, (d) we provide a new theoretical analysis of the performance of these formulations with a simple probabilistic model, leading to scalings over the form $d=O(\sqrt{n})$ for the affine invariant case and $d=O(n)$ for the sparse case, where $n$ is the number of examples and $d$ the ambient dimension, and finally, (e) we propose an efficient iterative algorithm with running-time complexity proportional to $O(nd^2)$, improving on earlier algorithms which had quadratic complexity in the number of examples. version:1
arxiv-1608-08049 | Cortically-Inspired Spectral Clustering for Connectivity Analysis in Retinal Images: Curvature Integration | http://arxiv.org/abs/1608.08049 | id:1608.08049 author:Samaneh Abbasi-Sureshjani, Marta Favali, Giovanna Citti, Alessandro Sarti, Bart M. ter Haar Romeny category:cs.CV  published:2016-08-29 summary:Tree-like structures such as retinal images are widely studied in computer-aided diagnosis systems in large-scale screening programs. Despite several segmentation and tracking methods proposed in the literature, there still exist several limitations specifically when two or more curvilinear structures cross or bifurcate, or in the presence of interrupted lines or highly curved blood vessels. In this paper, we propose a novel approach based on multi-orientation scores augmented with a contextual affinity matrix, which both are inspired by the geometry of the primary visual cortex (V1) and their contextual connections. The connectivity is described with a four-dimensional kernel obtained as the fundamental solution of the Fokker-Planck equation modelling the cortical connectivity in the lifted space of positions, orientations and curvatures. It is further used in a self-tuning spectral clustering step to identify the main perceptual units in the stimuli. The proposed method has been validated on several easy and challenging structures in a set of artificial images and actual retinal patches. Supported by quantitative and qualitative results, the method is capable of overcoming the limitations of current state-of-the-art techniques. version:1
arxiv-1608-08029 | Edge Preserving and Multi-Scale Contextual Neural Network for Salient Object Detection | http://arxiv.org/abs/1608.08029 | id:1608.08029 author:Xiang Wang, Huimin Ma, Shaodi You, Xiaozhi Chen category:cs.CV  published:2016-08-29 summary:In this paper, we propose a novel edge preserving and multi-scale contextual neural network for salient object detection. The proposed framework is aiming to address two limits of the existing CNN based methods. First, region-based CNN methods lack sufficient context to accurately locate salient object since they deal with each region independently. Second, pixel-based CNN methods suffer from blurry boundaries due to the presence of convolutional and pooling layers. Motivated by these, we first propose an end-to-end edge-preserved neural network based on Fast R-CNN framework (named RegionNet) to efficiently generate saliency map with sharp object boundaries. Later, to further improve it, multi-scale spatial context is attached to RegionNet to consider the relationship between regions and the global scenes. Furthermore, our method can be generally applied to RGB-D saliency detection by depth refinement. The proposed framework achieves both clear detection boundary and multi-scale contextual robustness simultaneously for the first time, and thus achieves an optimized performance. Experiments on six RGB and two RGB-D benchmark datasets demonstrate that the proposed method outperforms previous methods by a large margin, in particular, we achieve relative improvement by 6.1% and 10.1% on F-measure on ECSSD and DUT-OMRON dataset, respectively. version:1
arxiv-1608-08021 | PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection | http://arxiv.org/abs/1608.08021 | id:1608.08021 author:Kye-Hyeon Kim, Yeongjae Cheon, Sanghoon Hong, Byungseok Roh, Minje Park category:cs.CV  published:2016-08-29 summary:This paper presents how we can achieve the state-of-the-art accuracy in multi-category object detection task while minimizing the computational cost by adapting and combining recent technical innovations. Following the common pipeline of "CNN feature extraction + region proposal + RoI classification", we mainly redesign the feature extraction part, since region proposal part is not computationally expensive and classification part can be efficiently compressed with common techniques like truncated SVD. Our design principle is "less channels with more layers" and adoption of some building blocks including concatenated ReLU, Inception, and HyperNet. The designed network is deep and thin and trained with the help of batch normalization, residual connections, and learning rate scheduling based on plateau detection. We obtained solid results on well-known object detection benchmarks: 81.8% mAP (mean average precision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place), while taking only 750ms/image on Intel i7-6700K CPU with a single core and 46ms/image on NVIDIA Titan X GPU. Theoretically, our network requires only 12.3% of the computational cost compared to ResNet-101, the winner on VOC2012. version:1
arxiv-1608-07433 | Mean Deviation Similarity Index: Efficient and Reliable Full-Reference Image Quality Evaluator | http://arxiv.org/abs/1608.07433 | id:1608.07433 author:Hossein Ziaei Nafchi, Atena Shahkolaei, Rachid Hedjam, Mohamed Cheriet category:cs.CV  published:2016-08-26 summary:Applications of perceptual image quality assessment (IQA) in image and video processing, such as image acquisition, image compression, image restoration and multimedia communication, have led to the development of many IQA metrics. In this paper, a reliable full reference IQA model is proposed that utilize gradient similarity (GS), chromaticity similarity (CS), and deviation pooling (DP). By considering the shortcomings of the commonly used GS to model human visual system (HVS), a new GS is proposed through a fusion technique that is more likely to follow HVS. We propose an efficient and effective formulation to calculate the joint similarity map of two chromatic channels for the purpose of measuring color changes. In comparison with a commonly used formulation in the literature, the proposed CS map is shown to be more efficient and provide comparable or better quality predictions. Motivated by a recent work that utilizes the standard deviation pooling, a general formulation of the DP is presented in this paper and used to compute a final score from the proposed GS and CS maps. This proposed formulation of DP benefits from the Minkowski pooling and a proposed power pooling as well. The experimental results on six datasets of natural images, a synthetic dataset, and a digitally retouched dataset show that the proposed index provides comparable or better quality predictions than the most recent and competing state-of-the-art IQA metrics in the literature, it is reliable and has low complexity. The MATLAB source code of the proposed metric is available at https://dl.dropboxusercontent.com/u/74505502/MDSI.m version:2
arxiv-1608-07997 | Correspondence Insertion for As-Projective-As-Possible Image Stitching | http://arxiv.org/abs/1608.07997 | id:1608.07997 author:William X. Liu, Tat-Jun Chin category:cs.CV  published:2016-08-29 summary:Spatially varying warps are increasingly popular for image alignment. In particular, as-projective-as-possible (APAP) warps have been proven effective for accurate panoramic stitching, especially in cases with significant depth parallax that defeat standard homographic warps. However, estimating spatially varying warps requires a sufficient number of feature matches. In image regions where feature detection or matching fail, the warp loses guidance and is unable to accurately model the true underlying warp, thus resulting in poor registration. In this paper, we propose a correspondence insertion method for APAP warps, with a focus on panoramic stitching. Our method automatically identifies misaligned regions, and inserts appropriate point correspondences to increase the flexibility of the warp and improve alignment. Unlike other warp varieties, the underlying projective regularization of APAP warps reduces overfitting and geometric distortion, despite increases to the warp complexity. Comparisons with recent techniques for parallax-tolerant image stitching demonstrate the effectiveness and simplicity of our approach. version:1
arxiv-1608-03773 | Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking | http://arxiv.org/abs/1608.03773 | id:1608.03773 author:Martin Danelljan, Andreas Robinson, Fahad Shahbaz Khan, Michael Felsberg category:cs.CV  published:2016-08-12 summary:Discriminative Correlation Filters (DCF) have demonstrated excellent performance for visual object tracking. The key to their success is the ability to efficiently exploit available negative data by including all shifted versions of a training sample. However, the underlying DCF formulation is restricted to single-resolution feature maps, significantly limiting its potential. In this paper, we go beyond the conventional DCF framework and introduce a novel formulation for training continuous convolution filters. We employ an implicit interpolation model to pose the learning problem in the continuous spatial domain. Our proposed formulation enables efficient integration of multi-resolution deep feature maps, leading to superior results on three object tracking benchmarks: OTB-2015 (+5.1% in mean OP), Temple-Color (+4.6% in mean OP), and VOT2015 (20% relative reduction in failure rate). Additionally, our approach is capable of sub-pixel localization, crucial for the task of accurate feature point tracking. We also demonstrate the effectiveness of our learning formulation in extensive feature point tracking experiments. Code and supplementary material are available at http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html. version:2
arxiv-1608-07986 | Manifold Langevin Monte Carlo with Partial Metric Updates | http://arxiv.org/abs/1608.07986 | id:1608.07986 author:Theodore Papamarkou, Eric B. Ford category:stat.ML  published:2016-08-29 summary:Manifold Markov chain Monte Carlo algorithms have been introduced to sample more effectively from challenging target densities exhibiting multiple modes or strong correlations. Such algorithms exploit the local geometry of the parameter space, thus enabling chains to achieve a faster convergence rate when measured in number of steps. However, often acquiring local geometric information increases computational complexity per step to the extent that sampling from high-dimensional targets becomes inefficient in terms of total computational time. This paper proposes a manifold Langevin Monte Carlo framework aimed at balancing the benefits of exploiting local geometry with computational requirements to achieve a high effective sample size for a given computational cost. The suggested strategy regulates the frequency of manifold-based updates via a schedule. An exponentially decaying schedule is put forward that enables more frequent updates of geometric information in early transient phases of the chain, while saving computational time in late stationary phases. Alternatively, a modulo schedule is introduced to allow for infrequent yet recurring geometric updates throughout the sampling course, acting to adaptively update proposals to learn the covariance structure of the parameter space. The average complexity can be manually set for either of these two schedules depending on the need for geometric exploitation posed by the underlying model. version:1
arxiv-1608-07973 | Capturing Deep Correlations with 2-Way Nets | http://arxiv.org/abs/1608.07973 | id:1608.07973 author:Aviv Eisenschtat, Lior Wolf category:cs.CV  published:2016-08-29 summary:We present a noval, bi-directional mapping neural network architecture for the task of matching vectors from two data-sources. Our approach employs two tied neural network channels to project two views into a common, maximally correlated space, using the euclidean loss. To achieve both maximally correlated projections we built an encoder-decoder framework composed of two parallel networks each captures the features of each of the views. We show a direct link between the correlation loss and euclidean loss enabling the use of euclidean loss for optimizing correlation maximization problem. To overcome common euclidean regression optimization problems, we incorporated batch-normalization layers and dropout layers adapted to the model at hand. We show state of the art results on a number of computer vision matching tasks including MNIST image matching and sentence-image matching on the flickr8k and flickr30k datasets. version:1
arxiv-1608-07955 | Magnetic skyrmion-based synaptic devices | http://arxiv.org/abs/1608.07955 | id:1608.07955 author:Yangqi Huang, Wang Kang, Xichao Zhang, Yan Zhou, Weisheng Zhao category:cs.ET cond-mat.str-el cs.NE  published:2016-08-29 summary:Magnetic skyrmions are promising candidates for next-generation information carriers, owing to their small size, topological stability, and ultralow depinning current density. A wide variety of skyrmionic device concepts and prototypes have been proposed, highlighting their potential applications. Here, we report on a bioinspired skyrmionic device with synaptic plasticity. The synaptic weight of the proposed device can be strengthened/weakened by positive/negative stimuli, mimicking the potentiation/depression process of a biological synapse. Both short-term plasticity(STP) and long-term potentiation(LTP) functionalities have been demonstrated for a spiking time-dependent plasticity(STDP) scheme. This proposal suggests new possibilities for synaptic devices for use in spiking neuromorphic computing applications. version:1
arxiv-1608-07951 | Approaching the Computational Color Constancy as a Classification Problem through Deep Learning | http://arxiv.org/abs/1608.07951 | id:1608.07951 author:Seoung Wug Oh, Seon Joo Kim category:cs.CV  published:2016-08-29 summary:Computational color constancy refers to the problem of computing the illuminant color so that the images of a scene under varying illumination can be normalized to an image under the canonical illumination. In this paper, we adopt a deep learning framework for the illumination estimation problem. The proposed method works under the assumption of uniform illumination over the scene and aims for the accurate illuminant color computation. Specifically, we trained the convolutional neural network to solve the problem by casting the color constancy problem as an illumination classification problem. We designed the deep learning architecture so that the output of the network can be directly used for computing the color of the illumination. Experimental results show that our deep network is able to extract useful features for the illumination estimation and our method outperforms all previous color constancy methods on multiple test datasets. version:1
arxiv-1608-07949 | Learning-Based Resource Allocation Scheme for TDD-Based CRAN System | http://arxiv.org/abs/1608.07949 | id:1608.07949 author:Sahar Imtiaz, Hadi Ghauch, M. Mahboob Ur Rahman, George Koudouridis, James Gross category:cs.NI cs.IT cs.LG math.IT  published:2016-08-29 summary:Explosive growth in the use of smart wireless devices has necessitated the provision of higher data rates and always-on connectivity, which are the main motivators for designing the fifth generation (5G) systems. To achieve higher system efficiency, massive antenna deployment with tight coordination is one potential strategy for designing 5G systems, but has two types of associated system overhead. First is the synchronization overhead, which can be reduced by implementing a cloud radio access network (CRAN)-based architecture design, that separates the baseband processing and radio access functionality to achieve better system synchronization. Second is the overhead for acquiring channel state information (CSI) of the users present in the system, which, however, increases tremendously when instantaneous CSI is used to serve high-mobility users. To serve a large number of users, a CRAN system with a dense deployment of remote radio heads (RRHs) is considered, such that each user has a line-of-sight (LOS) link with the corresponding RRH. Since, the trajectory of movement for high-mobility users is predictable; therefore, fairly accurate position estimates for those users can be obtained, and can be used for resource allocation to serve the considered users. The resource allocation is dependent upon various correlated system parameters, and these correlations can be learned using well-known \emph{machine learning} algorithms. This paper proposes a novel \emph{learning-based resource allocation scheme} for time division duplex (TDD) based 5G CRAN systems with dense RRH deployment, by using only the users' position estimates for resource allocation, thus avoiding the need for CSI acquisition. This reduces the overall system overhead significantly, while still achieving near-optimal system performance; thus, better (effective) system efficiency is achieved. (See the paper for full abstract) version:1
arxiv-1608-07934 | Relevant based structure learning for feature selection | http://arxiv.org/abs/1608.07934 | id:1608.07934 author:Hadi Zare, Mojtaba Niazi category:cs.LG stat.ML  published:2016-08-29 summary:Feature selection is an important task in many problems occurring in pattern recognition, bioinformatics, machine learning and data mining applications. The feature selection approach enables us to reduce the computation burden and the falling accuracy effect of dealing with huge number of features in typical learning problems. There is a variety of techniques for feature selection in supervised learning problems based on different selection metrics. In this paper, we propose a novel unified framework for feature selection built on the graphical models and information theoretic tools. The proposed approach exploits the structure learning among features to select more relevant and less redundant features to the predictive modeling problem according to a primary novel likelihood based criterion. In line with the selection of the optimal subset of features through the proposed method, it provides us the Bayesian network classifier without the additional cost of model training on the selected subset of features. The optimal properties of our method are established through empirical studies and computational complexity analysis. Furthermore the proposed approach is evaluated on a bunch of benchmark datasets based on the well-known classification algorithms. Extensive experiments confirm the significant improvement of the proposed approach compared to the earlier works. version:1
arxiv-1608-07929 | Discovering Patterns in Time-Varying Graphs: A Triclustering Approach | http://arxiv.org/abs/1608.07929 | id:1608.07929 author:Romain Guigourès, Marc Boullé, Fabrice Rossi category:stat.ML cs.SI physics.soc-ph  published:2016-08-29 summary:This paper introduces a novel technique to track structures in time varying graphs. The method uses a maximum a posteriori approach for adjusting a three-dimensional co-clustering of the source vertices, the destination vertices and the time, to the data under study, in a way that does not require any hyper-parameter tuning. The three dimensions are simultaneously segmented in order to build clusters of source vertices, destination vertices and time segments where the edge distributions across clusters of vertices follow the same evolution over the time segments. The main novelty of this approach lies in that the time segments are directly inferred from the evolution of the edge distribution between the vertices, thus not requiring the user to make any a priori quantization. Experiments conducted on artificial data illustrate the good behavior of the technique, and a study of a real-life data set shows the potential of the proposed approach for exploratory data analysis. version:1
arxiv-1608-07916 | Vehicle Detection from 3D Lidar Using Fully Convolutional Network | http://arxiv.org/abs/1608.07916 | id:1608.07916 author:Bo Li, Tianlei Zhang, Tian Xia category:cs.CV cs.RO  published:2016-08-29 summary:Convolutional network techniques have recently achieved great success in vision based detection tasks. This paper introduces the recent development of our research on transplanting the fully convolutional network technique to the detection tasks on 3D range scan data. Specifically, the scenario is set as the vehicle detection task from the range data of Velodyne 64E lidar. We proposes to present the data in a 2D point map and use a single 2D end-to-end fully convolutional network to predict the objectness confidence and the bounding boxes simultaneously. By carefully design the bounding box encoding, it is able to predict full 3D bounding boxes even using a 2D convolutional network. Experiments on the KITTI dataset shows the state-of-the-art performance of the proposed method. version:1
arxiv-1608-07905 | Machine Comprehension Using Match-LSTM and Answer Pointer | http://arxiv.org/abs/1608.07905 | id:1608.07905 author:Shuohang Wang, Jing Jiang category:cs.CL cs.AI  published:2016-08-29 summary:Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al.(2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our task. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al.(2016) using logistic regression and manually crafted features. version:1
arxiv-1608-07895 | Human-Algorithm Interaction Biases in the Big Data Cycle: A Markov Chain Iterated Learning Framework | http://arxiv.org/abs/1608.07895 | id:1608.07895 author:Olfa Nasraoui, Patrick Shafto category:cs.LG cs.HC  published:2016-08-29 summary:Early supervised machine learning algorithms have relied on reliable expert labels to build predictive models. However, the gates of data generation have recently been opened to a wider base of users who started participating increasingly with casual labeling, rating, annotating, etc. The increased online presence and participation of humans has led not only to a democratization of unchecked inputs to algorithms, but also to a wide democratization of the "consumption" of machine learning algorithms' outputs by general users. Hence, these algorithms, many of which are becoming essential building blocks of recommender systems and other information filters, started interacting with users at unprecedented rates. The result is machine learning algorithms that consume more and more data that is unchecked, or at the very least, not fitting conventional assumptions made by various machine learning algorithms. These include biased samples, biased labels, diverging training and testing sets, and cyclical interaction between algorithms, humans, information consumed by humans, and data consumed by algorithms. Yet, the continuous interaction between humans and algorithms is rarely taken into account in machine learning algorithm design and analysis. In this paper, we present a preliminary theoretical model and analysis of the mutual interaction between humans and algorithms, based on an iterated learning framework that is inspired from the study of human language evolution. We also define the concepts of human and algorithm blind spots and outline machine learning approaches to mend iterated bias through two novel notions: antidotes and reactive learning. version:1
arxiv-1608-07892 | Optimizing Recurrent Neural Networks Architectures under Time Constraints | http://arxiv.org/abs/1608.07892 | id:1608.07892 author:Junqi Jin, Ziang Yan, Kun Fu, Nan Jiang, Changshui Zhang category:stat.ML cs.LG  published:2016-08-29 summary:Recurrent neural network (RNN)'s architecture is a key factor influencing its performance. We propose algorithms to optimize hidden sizes under running time constraint. We convert the discrete optimization into a subset selection problem. By novel transformations, the objective function becomes submodular and constraint becomes supermodular. A greedy algorithm with bounds is suggested to solve the transformed problem. And we show how transformations influence the bounds. To speed up optimization, surrogate functions are proposed which balance exploration and exploitation. Experiments show that our algorithms can find more accurate models or faster models than manually tuned state-of-the-art and random search. We also compare popular RNN architectures using our algorithms. version:1
arxiv-1608-07888 | Online Monotone Optimization | http://arxiv.org/abs/1608.07888 | id:1608.07888 author:Ian Gemp, Sridhar Mahadevan category:cs.LG math.OC  published:2016-08-29 summary:This paper presents a new framework for analyzing and designing no-regret algorithms for dynamic (possibly adversarial) systems. The proposed framework generalizes the popular online convex optimization framework and extends it to its natural limit allowing it to capture a notion of regret that is intuitive for more general problems such as those encountered in game theory and variational inequalities. The framework hinges on a special choice of a system-wide loss function we have developed. Using this framework, we prove that a simple update scheme provides a no-regret algorithm for monotone systems. While previous results in game theory prove individual agents can enjoy unilateral no-regret guarantees, our result proves monotonicity sufficient for guaranteeing no-regret when considering the adjustments of multiple agent strategies in parallel. Furthermore, to our knowledge, this is the first framework to provide a suitable notion of regret for variational inequalities. Most importantly, our proposed framework ensures monotonicity a sufficient condition for employing multiple online learners safely in parallel. version:1
arxiv-1608-07876 | Human Action Recognition without Human | http://arxiv.org/abs/1608.07876 | id:1608.07876 author:Yun He, Soma Shirakabe, Yutaka Satoh, Hirokatsu Kataoka category:cs.CV cs.MM  published:2016-08-29 summary:The objective of this paper is to evaluate "human action recognition without human". Motion representation is frequently discussed in human action recognition. We have examined several sophisticated options, such as dense trajectories (DT) and the two-stream convolutional neural network (CNN). However, some features from the background could be too strong, as shown in some recent studies on human action recognition. Therefore, we considered whether a background sequence alone can classify human actions in current large-scale action datasets (e.g., UCF101). In this paper, we propose a novel concept for human action analysis that is named "human action recognition without human". An experiment clearly shows the effect of a background sequence for understanding an action label. version:1
arxiv-1608-07852 | Quantitative Analyses of Chinese Poetry of Tang and Song Dynasties: Using Changing Colors and Innovative Terms as Examples | http://arxiv.org/abs/1608.07852 | id:1608.07852 author:Chao-Lin Liu category:cs.CL cs.CY cs.DL cs.IR  published:2016-08-28 summary:Tang (618-907 AD) and Song (960-1279) dynasties are two very important periods in the development of Chinese literary. The most influential forms of the poetry in Tang and Song were Shi and Ci, respectively. Tang Shi and Song Ci established crucial foundations of the Chinese literature, and their influences in both literary works and daily lives of the Chinese communities last until today. We can analyze and compare the Complete Tang Shi and the Complete Song Ci from various viewpoints. In this presentation, we report our findings about the differences in their vocabularies. Interesting new words that started to appear in Song Ci and continue to be used in modern Chinese were identified. Colors are an important ingredient of the imagery in poetry, and we discuss the most frequent color words that appeared in Tang Shi and Song Ci. version:1
arxiv-1608-07836 | What to do about non-standard (or non-canonical) language in NLP | http://arxiv.org/abs/1608.07836 | id:1608.07836 author:Barbara Plank category:cs.CL  published:2016-08-28 summary:Real world data differs radically from the benchmark corpora we use in natural language processing (NLP). As soon as we apply our technologies to the real world, performance drops. The reason for this problem is obvious: NLP models are trained on samples from a limited set of canonical varieties that are considered standard, most prominently English newswire. However, there are many dimensions, e.g., socio-demographics, language, genre, sentence type, etc. on which texts can differ from the standard. The solution is not obvious: we cannot control for all factors, and it is not clear how to best go beyond the current practice of training on homogeneous data from a single domain and language. In this paper, I review the notion of canonicity, and how it shapes our community's approach to language. I argue for leveraging what I call fortuitous data, i.e., non-obvious data that is hitherto neglected, hidden in plain sight, or raw data that needs to be refined. If we embrace the variety of this heterogeneous data by combining it with proper algorithms, we will not only produce more robust models, but will also enable adaptive language technology capable of addressing natural language variation. version:1
arxiv-1608-07813 | Total variation reconstruction for compressive sensing using nonlocal Lagrangian multiplier | http://arxiv.org/abs/1608.07813 | id:1608.07813 author:Trinh Van Chien, Khanh Quoc Dinh, Viet Anh Nguyen, Byeungwoo Jeon category:cs.CV  published:2016-08-28 summary:Total variation has proved its effectiveness in solving inverse problems for compressive sensing. Besides, the nonlocal means filter used as regularization preserves texture better for recovered images, but it is quite complex to implement. In this paper, based on existence of both noise and image information in the Lagrangian multiplier, we propose a simple method in term of implementation called nonlocal Lagrangian multiplier (NLLM) in order to reduce noise and boost useful image information. Experimental results show that the proposed NLLM is superior both in subjective and objective qualities of recovered image over other recovery algorithms. version:1
arxiv-1608-06148 | Multiple objects tracking in surveillance video using color and Hu moments | http://arxiv.org/abs/1608.06148 | id:1608.06148 author:Chandrajit M, Girisha R, Vasudev T category:cs.CV  published:2016-08-22 summary:Multiple objects tracking finds its applications in many high level vision analysis like object behaviour interpretation and gait recognition. In this paper, a feature based method to track the multiple moving objects in surveillance video sequence is proposed. Object tracking is done by extracting the color and Hu moments features from the motion segmented object blob and establishing the association of objects in the successive frames of the video sequence based on Chi-Square dissimilarity measure and nearest neighbor classifier. The benchmark IEEE PETS and IEEE Change Detection datasets has been used to show the robustness of the proposed method. The proposed method is assessed quantitatively using the precision and recall accuracy metrics. Further, comparative evaluation with related works has been carried out to exhibit the efficacy of the proposed method. version:2
arxiv-1608-07807 | Cast and Self Shadow Segmentation in Video Sequences using Interval based Eigen Value Representation | http://arxiv.org/abs/1608.07807 | id:1608.07807 author:Chandrajit M, Girisha R, Vasudev T, Ashok C B category:cs.CV  published:2016-08-28 summary:Tracking of motion objects in the surveillance videos is useful for the monitoring and analysis. The performance of the surveillance system will deteriorate when shadows are detected as moving objects. Therefore, shadow detection and elimination usually benefits the next stages. To overcome this issue, a method for detection and elimination of shadows is proposed. This paper presents a method for segmenting moving objects in video sequences based on determining the Euclidian distance between two pixels considering neighborhood values in temporal domain. Further, a method that segments cast and self shadows in video sequences by computing the Eigen values for the neighborhood of each pixel is proposed. The dual-map for cast and self shadow pixels is represented based on the interval of Eigen values. The proposed methods are tested on the benchmark IEEE CHANGE DETECTION 2014 dataset. version:1
arxiv-1608-07802 | MindX: Denoising Mixed Impulse Poisson-Gaussian Noise Using Proximal Algorithms | http://arxiv.org/abs/1608.07802 | id:1608.07802 author:Mohamed Aly, Wolfgang Heidrich category:cs.CV math.OC  published:2016-08-28 summary:We present a novel algorithm for blind denoising of images corrupted by mixed impulse, Poisson, and Gaussian noises. The algorithm starts by applying the Anscombe variance-stabilizing transformation to convert the Poisson into white Gaussian noise. Then it applies a combinatorial optimization technique to denoise the mixed impulse Gaussian noise using proximal algorithms. The result is then processed by the inverse Anscombe transform. We compare our algorithm to state of the art methods on standard images, and show its superior performance in various noise conditions. version:1
arxiv-1608-08596 | A statistical model of tristimulus measurements within and between OLED displays | http://arxiv.org/abs/1608.08596 | id:1608.08596 author:Matti Raitoharju, Samu Kallio, Matti Pellikka category:cs.CV  published:2016-08-28 summary:We present an empirical model for noises in color measurements from OLED displays. According to measured data the noise is not isotropic in the XYZ space, instead most of the noise is along an axis that is parallel to a vector from origin to measured XYZ vector. The presented empirical model is simple and depends only on the measured XYZ values. Our tests show that the variations between multiple panels of the same type have similar distribution as the temporal noise in measurements from a single panel, but a larger magnitude. version:1
arxiv-1608-07775 | Hierarchical Attention Model for Improved Machine Comprehension of Spoken Content | http://arxiv.org/abs/1608.07775 | id:1608.07775 author:Wei Fang, Juei-Yang Hsu, Hung-yi Lee, Lin-Shan Lee category:cs.CL  published:2016-08-28 summary:Multimedia or spoken content presents more attractive information than plain text content, but the former is more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It's therefore highly attractive to develop machines which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, a new task of machine comprehension of spoken content was proposed recently. The initial goal was defined as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native languages are not English. An Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture was also proposed for this task, which considered only the sequential relationship within the speech utterances. In this paper, we propose a new Hierarchical Attention Model (HAM), which constructs multi-hopped attention mechanism over tree-structured rather than sequential representations for the utterances. Improved comprehension performance robust with respect to ASR errors were obtained. version:1
arxiv-1608-07739 | Bayesian selection for the regularization parameter in TVl0 denoising problems | http://arxiv.org/abs/1608.07739 | id:1608.07739 author:Jordan Frecon, Nelly Pustelnik, Nicolas Dobigeon, Herwig Wendt, Patrice Abry category:cs.LG stat.ML  published:2016-08-27 summary:Piecewise constant denoising can be solved either by deterministic optimization approaches, based on total variation (TV), or by stochastic Bayesian procedures. The former lead to low computational time but requires the selection of a regularization parameter, whose value significantly impacts the achieved solution, and whose automated selection remains an involved and challenging problem. Conversely, fully Bayesian formalisms encapsulate the regularization parameter selection into hierarchical models, at the price of large computational costs. This contribution proposes an operational strategy that combines hierarchical Bayesian and TVl0 formulations, with the double aim of automatically tuning the regularization parameter and of maintaining computational efficiency. The proposed procedure relies on formally connecting a Bayesian framework to a TVl0 minimization formulation. Behaviors and performance for the proposed piecewise constant denoising and regularization parameter tuning techniques are studied qualitatively and assessed quantitatively, and shown to compare favorably against those of a fully Bayesian hierarchical procedure, both in accuracy and in computational load. version:1
arxiv-1608-07738 | Testing APSyn against Vector Cosine on Similarity Estimation | http://arxiv.org/abs/1608.07738 | id:1608.07738 author:Enrico Santus, Emmanuele Chersoni, Alessandro Lenci, Chu-Ren Huang, Philippe Blache category:cs.CL  published:2016-08-27 summary:In Distributional Semantic Models (DSMs), Vector Cosine is widely used to estimate similarity between word vectors, although this measure was noticed to suffer from several shortcomings. The recent literature has proposed other methods which attempt to mitigate such biases. In this paper, we intend to investigate APSyn, a measure that computes the extent of the intersection between the most associated contexts of two target words, weighting it by context relevance. We evaluated this metric in a similarity estimation task on several popular test sets, and our results show that APSyn is in fact highly competitive, even with respect to the results reported in the literature for word embeddings. On top of it, APSyn addresses some of the weaknesses of Vector Cosine, performing well also on genuine similarity estimation. version:1
arxiv-1608-07734 | Learning Bayesian Networks without Assuming Missing at Random | http://arxiv.org/abs/1608.07734 | id:1608.07734 author:Tameem Adel, Cassio P. de Campos category:cs.AI stat.ML  published:2016-08-27 summary:We present new algorithms for learning Bayesian networks from data with missing values without the assumption that data are missing at random (MAR). An exact Bayesian network learning algorithm is obtained by recasting the problem into a standard Bayesian network learning problem without missing data. To the best of our knowledge, this is the first exact algorithm for this problem. As expected, the exact algorithm does not scale to large domains. We build on the exact method to create a new approximate algorithm using a hill-climbing technique. This algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available. We perform a wide range of experiments to demonstrate the benefits of learning Bayesian networks without assuming MAR. version:1
arxiv-1608-07724 | Learning Temporal Transformations From Time-Lapse Videos | http://arxiv.org/abs/1608.07724 | id:1608.07724 author:Yipin Zhou, Tamara L. Berg category:cs.CV  published:2016-08-27 summary:Based on life-long observations of physical, chemical, and biologic phenomena in the natural world, humans can often easily picture in their minds what an object will look like in the future. But, what about computers? In this paper, we learn computational models of object transformations from time-lapse videos. In particular, we explore the use of generative models to create depictions of objects at future times. These models explore several different prediction tasks: generating a future state given a single depiction of an object, generating a future state given two depictions of an object at different times, and generating future states recursively in a recurrent framework. We provide both qualitative and quantitative evaluations of the generated results, and also conduct a human evaluation to compare variations of our models. version:1
arxiv-1608-07720 | A Bi-LSTM-RNN Model for Relation Classification Using Low-Cost Sequence Features | http://arxiv.org/abs/1608.07720 | id:1608.07720 author:Fei Li, Meishan Zhang, Guohong Fu, Tao Qian, Donghong Ji category:cs.CL  published:2016-08-27 summary:Relation classification is associated with many potential applications in the artificial intelligence area. Recent approaches usually leverage neural networks based on structure features such as syntactic or dependency features to solve this problem. However, high-cost structure features make such approaches inconvenient to be directly used. In addition, structure features are probably domain-dependent. Therefore, this paper proposes a bi-directional long-short-term-memory recurrent-neural-network (Bi-LSTM-RNN) model based on low-cost sequence features to address relation classification. This model divides a sentence or text segment into five parts, namely two target entities and their three contexts. It learns the representations of entities and their contexts, and uses them to classify relations. We evaluate our model on two standard benchmark datasets in different domains, namely SemEval-2010 Task 8 and BioNLP-ST 2016 Task BB3. In the former dataset, our model achieves comparable performance compared with other models using sequence features. In the latter dataset, our model obtains the third best results compared with other models in the official evaluation. Moreover, we find that the context between two target entities plays the most important role in relation classification. Furthermore, statistic experiments show that the context between two target entities can be used as an approximate replacement of the shortest dependency path when dependency parsing is not used. version:1
arxiv-1608-07711 | 3D Object Proposals using Stereo Imagery for Accurate Object Class Detection | http://arxiv.org/abs/1608.07711 | id:1608.07711 author:Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Huimin Ma, Sanja Fidler, Raquel Urtasun category:cs.CV  published:2016-08-27 summary:The goal of this paper is to perform 3D object detection in the context of autonomous driving. Our method first aims at generating a set of high-quality 3D object proposals by exploiting stereo imagery. We formulate the problem as minimizing an energy function that encodes object size priors, placement of objects on the ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground. We then exploit a CNN on top of these proposals to perform object detection. In particular, we employ a convolutional neural net (CNN) that exploits context and depth information to jointly regress to 3D bounding box coordinates and object pose. Our experiments show significant performance gains over existing RGB and RGB-D object proposal methods on the challenging KITTI benchmark. When combined with the CNN, our approach outperforms all existing results in object detection and orientation estimation tasks for all three KITTI object classes. Furthermore, we experiment also with the setting where LIDAR information is available, and show that using both LIDAR and stereo leads to the best result. version:1
arxiv-1608-07710 | Random Forest for Label Ranking | http://arxiv.org/abs/1608.07710 | id:1608.07710 author:Yangming Zhou, Guoping Qiu category:cs.LG stat.ML  published:2016-08-27 summary:Label ranking aims to learn a mapping from instances to rankings over a finite number of predefined labels. Random forest is a powerful and one of the most successfully general-purpose machine learning algorithms of modern times. In the literature, there seems no research has yet been done in applying random forest to label ranking. In this paper, We present a powerful random forest label ranking method which uses random decision trees to retrieve nearest neighbors that are not only similar in the feature space but also in the ranking space. We have developed a novel two-step rank aggregation strategy to effectively aggregate neighboring rankings discovered by the random forest into a final predicted ranking. Compared with existing methods, the new random forest method has many advantages including its intrinsically scalable tree data structure, highly parallel-able computational architecture and much superior performances. We present extensive experimental results to demonstrate that our new method achieves the best predictive accuracy performances compared with state-of-the-art methods for datasets with complete ranking and datasets with only partial ranking information. version:1
arxiv-1608-07706 | Multi-Path Feedback Recurrent Neural Network for Scene Parsing | http://arxiv.org/abs/1608.07706 | id:1608.07706 author:Xiaojie Jin, Yunpeng Chen, Jiashi Feng, Zequn Jie, Shuicheng Yan category:cs.CV  published:2016-08-27 summary:In this paper, we consider the scene parsing problem. We propose a novel \textbf{M}ulti-\textbf{P}ath \textbf{F}eedback recurrent neural network (MPF-RNN) to enhance the capability of RNNs on modeling long-range context information at multiple levels and better distinguish pixels that are easy to confuse in pixel-wise classification. In contrast to CNNs without feedback and RNNs with only a single feedback path, MPF-RNN propagates the contextual features learned at top layers through weighted recurrent connections to \emph{multiple} bottom layers to help them learn better features with such "hindsight". Besides, we propose a new training strategy which considers the loss accumulated at multiple recurrent steps to improve performance of the MPF-RNN on parsing small objects as well as stabilize the training procedure. We empirically demonstrate that such an architecture with multiple feedback paths can effectively enhance the capability of deep neural networks in classifying pixels which are hard to distinguish without higher-level context information. With these two novel components, MPF-RNN provides new state-of-the-art results on four challenging scene parsing benchmarks, including SiftFlow, Barcelona, CamVid and Stanford Background. version:1
arxiv-1608-07690 | A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples | http://arxiv.org/abs/1608.07690 | id:1608.07690 author:Thomas Tanay, Lewis Griffin category:cs.LG stat.ML  published:2016-08-27 summary:Deep neural networks have been shown to suffer from a surprising weakness: their classification outputs can be changed by small, non-random perturbations of their inputs. This adversarial example phenomenon has been explained as originating from deep networks being "too linear" (Goodfellow et al., 2014). We show here that the linear explanation of adversarial examples presents a number of limitations: the formal argument is not convincing, linear classifiers do not always suffer from the phenomenon, and when they do their adversarial examples are different from the ones affecting deep networks. We propose a new perspective on the phenomenon. We argue that adversarial examples exist when the classification boundary lies close to the submanifold of sampled data, and present a mathematical analysis of this new perspective in the linear case. We define the notion of adversarial strength and show that it can be reduced to the deviation angle between the classifier considered and the nearest centroid classifier. Then, we show that the adversarial strength can be made arbitrarily high independently of the classification performance due to a mechanism that we call boundary tilting. This result leads us to defining a new taxonomy of adversarial examples. Finally, we show that the adversarial strength observed in practice is directly dependent on the level of regularisation used and the strongest adversarial examples, symptomatic of overfitting, can be avoided by using a proper level of regularisation. version:1
arxiv-1608-07685 | Knowledge Semantic Representation: A Generative Model for Interpretable Knowledge Graph Embedding | http://arxiv.org/abs/1608.07685 | id:1608.07685 author:Han Xiao, Minlie Huang, Xiaoyan Zhu category:cs.LG cs.AI  published:2016-08-27 summary:Knowledge representation is a critical topic in AI, and currently embedding as a key branch of knowledge representation takes the numerical form of entities and relations to joint the statistical models. However, most embedding methods merely concentrate on the triple fitting and ignore the explicit semantic expression, leading to an uninterpretable representation form. Thus, traditional embedding methods do not only degrade the performance, but also restrict many potential applications. For this end, this paper proposes a semantic representation method for knowledge graph \textbf{(KSR)}, which imposes a two-level hierarchical generative process that globally extracts many aspects and then locally assigns a specific category in each aspect for every triple. Because both the aspects and categories are semantics-relevant, the collection of categories in each aspect is treated as the semantic representation of this triple. Extensive experiments justify our model outperforms other state-of-the-art baselines in a substantial extent. version:1
arxiv-1608-07664 | Spatio-temporal Aware Non-negative Component Representation for Action Recognition | http://arxiv.org/abs/1608.07664 | id:1608.07664 author:Jianhong Wang, Tian Lan, Xu Zhang, Limin Luo category:cs.CV  published:2016-08-27 summary:This paper presents a novel mid-level representation for action recognition, named spatio-temporal aware non-negative component representation (STANNCR). The proposed STANNCR is based on action component and incorporates the spatial-temporal information. We first introduce a spatial-temporal distribution vector (STDV) to model the distributions of local feature locations in a compact and discriminative manner. Then we employ non-negative matrix factorization (NMF) to learn the action components and encode the video samples. The action component considers the correlations of visual words, which effectively bridge the sematic gap in action recognition. To incorporate the spatial-temporal cues for final representation, the STDV is used as the part of graph regularization for NMF. The fusion of spatial-temporal information makes the STANNCR more discriminative, and our fusion manner is more compact than traditional method of concatenating vectors. The proposed approach is extensively evaluated on three public datasets. The experimental results demonstrate the effectiveness of STANNCR for action recognition. version:1
arxiv-1608-07639 | Learning to generalize to new compositions in image understanding | http://arxiv.org/abs/1608.07639 | id:1608.07639 author:Yuval Atzmon, Jonathan Berant, Vahid Kezami, Amir Globerson, Gal Chechik category:cs.CV cs.AI cs.CL cs.LG  published:2016-08-27 summary:Recurrent neural networks have recently been used for learning to describe images using natural language. However, it has been observed that these models generalize poorly to scenes that were not observed during training, possibly depending too strongly on the statistics of the text in the training data. Here we propose to describe images using short structured representations, aiming to capture the crux of a description. These structured representations allow us to tease-out and evaluate separately two types of generalization: standard generalization to new images with similar scenes, and generalization to new combinations of known entities. We compare two learning approaches on the MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show, Attend and Tell), and a simple structured prediction model on top of a deep network. We find that the structured model generalizes to new compositions substantially better than the LSTM, ~7 times the accuracy of predicting structured representations. By providing a concrete method to quantify generalization for unseen combinations, we argue that structured representations and compositional splits are a useful benchmark for image captioning, and advocate compositional models that capture linguistic and visual structure. version:1
arxiv-1608-07636 | Learning Temporal Dependence from Time-Series Data with Latent Variables | http://arxiv.org/abs/1608.07636 | id:1608.07636 author:Hossein Hosseini, Sreeram Kannan, Baosen Zhang, Radha Poovendran category:cs.LG stat.ML  published:2016-08-27 summary:We consider the setting where a collection of time series, modeled as random processes, evolve in a causal manner, and one is interested in learning the graph governing the relationships of these processes. A special case of wide interest and applicability is the setting where the noise is Gaussian and relationships are Markov and linear. We study this setting with two additional features: firstly, each random process has a hidden (latent) state, which we use to model the internal memory possessed by the variables (similar to hidden Markov models). Secondly, each variable can depend on its latent memory state through a random lag (rather than a fixed lag), thus modeling memory recall with differing lags at distinct times. Under this setting, we develop an estimator and prove that under a genericity assumption, the parameters of the model can be learned consistently. We also propose a practical adaption of this estimator, which demonstrates significant performance gains in both synthetic and real-world datasets. version:1
arxiv-1608-07630 | Global analysis of Expectation Maximization for mixtures of two Gaussians | http://arxiv.org/abs/1608.07630 | id:1608.07630 author:Ji Xu, Daniel Hsu, Arian Maleki category:math.ST cs.LG stat.CO stat.ML stat.TH  published:2016-08-26 summary:Expectation Maximization (EM) is among the most popular algorithms for estimating parameters of statistical models. However, EM, which is an iterative algorithm based on the maximum likelihood principle, is generally only guaranteed to find stationary points of the likelihood objective, and these points may be far from any maximizer. This article addresses this disconnect between the statistical principles behind EM and its algorithmic properties. Specifically, it provides a global analysis of EM for specific models in which the observations comprise an i.i.d. sample from a mixture of two Gaussians. This is achieved by (i) studying the sequence of parameters from idealized execution of EM in the infinite sample limit, and fully characterizing the limit points of the sequence in terms of the initial parameters; and then (ii) based on this convergence analysis, establishing statistical consistency (or lack thereof) for the actual sequence of parameters produced by EM. version:1
arxiv-1608-07625 | Large Scale Behavioral Analytics via Topical Interaction | http://arxiv.org/abs/1608.07625 | id:1608.07625 author:Shih-Chieh Su category:cs.LG  published:2016-08-26 summary:We propose the split-diffuse (SD) algorithm that takes the output of an existing dimension reduction algorithm, and distributes the data points uniformly across the visualization space. The result, called the topic grids, is a set of grids on various topics which are generated from the free-form text content of any domain of interest. The topic grids efficiently utilizes the visualization space to provide visual summaries for massive data. Topical analysis, comparison and interaction can be performed on the topic grids in a more perceivable way. version:1
arxiv-1608-07619 | Interacting with Massive Behavioral Data | http://arxiv.org/abs/1608.07619 | id:1608.07619 author:Shih-Chieh Su category:cs.LG  published:2016-08-26 summary:In this short paper, we propose the split-diffuse (SD) algorithm that takes the output of an existing word embedding algorithm, and distributes the data points uniformly across the visualization space. The result improves the perceivability and the interactability by the human. We apply the SD algorithm to analyze the user behavior through access logs within the cyber security domain. The result, named the topic grids, is a set of grids on various topics generated from the logs. On the same set of grids, different behavioral metrics can be shown on different targets over different periods of time, to provide visualization and interaction to the human experts. Analysis, investigation, and other types of interaction can be performed on the topic grids more efficiently than on the output of existing dimension reduction methods. In addition to the cyber security domain, the topic grids can be further applied to other domains like e-commerce, credit card transaction, customer service to analyze the behavior in a large scale. version:1
arxiv-1608-07616 | Mitosis Detection in Intestinal Crypt Images with Hough Forest and Conditional Random Fields | http://arxiv.org/abs/1608.07616 | id:1608.07616 author:Gerda Bortsova, Michael Sterr, Lichao Wang, Fausto Milletari, Nassir Navab, Anika Böttcher, Heiko Lickert, Fabian Theis, Tingying Peng category:cs.CV  published:2016-08-26 summary:Intestinal enteroendocrine cells secrete hormones that are vital for the regulation of glucose metabolism but their differentiation from intestinal stem cells is not fully understood. Asymmetric stem cell divisions have been linked to intestinal stem cell homeostasis and secretory fate commitment. We monitored cell divisions using 4D live cell imaging of cultured intestinal crypts to characterize division modes by means of measurable features such as orientation or shape. A statistical analysis of these measurements requires annotation of mitosis events, which is currently a tedious and time-consuming task that has to be performed manually. To assist data processing, we developed a learning based method to automatically detect mitosis events. The method contains a dual-phase framework for joint detection of dividing cells (mothers) and their progeny (daughters). In the first phase we detect mother and daughters independently using Hough Forest whilst in the second phase we associate mother and daughters by modelling their joint probability as Conditional Random Field (CRF). The method has been evaluated on 32 movies and has achieved an AUC of 72%, which can be used in conjunction with manual correction and dramatically speed up the processing pipeline. version:1
arxiv-1608-02893 | Syntactically Informed Text Compression with Recurrent Neural Networks | http://arxiv.org/abs/1608.02893 | id:1608.02893 author:David Cox category:cs.LG cs.CL cs.IT math.IT E.4  published:2016-08-08 summary:We present a self-contained system for constructing natural language models for use in text compression. Our system improves upon previous neural network based models by utilizing recent advances in syntactic parsing -- Google's SyntaxNet -- to augment character-level recurrent neural networks. RNNs have proven exceptional in modeling sequence data such as text, as their architecture allows for modeling of long-term contextual information. version:2
arxiv-1608-07605 | Clustering and Community Detection with Imbalanced Clusters | http://arxiv.org/abs/1608.07605 | id:1608.07605 author:Cem Aksoylar, Jing Qian, Venkatesh Saligrama category:stat.ML cs.LG cs.SI  published:2016-08-26 summary:Spectral clustering methods which are frequently used in clustering and community detection applications are sensitive to the specific graph constructions particularly when imbalanced clusters are present. We show that ratio cut (RCut) or normalized cut (NCut) objectives are not tailored to imbalanced cluster sizes since they tend to emphasize cut sizes over cut values. We propose a graph partitioning problem that seeks minimum cut partitions under minimum size constraints on partitions to deal with imbalanced cluster sizes. Our approach parameterizes a family of graphs by adaptively modulating node degrees on a fixed node set, yielding a set of parameter dependent cuts reflecting varying levels of imbalance. The solution to our problem is then obtained by optimizing over these parameters. We present rigorous limit cut analysis results to justify our approach and demonstrate the superiority of our method through experiments on synthetic and real datasets for data clustering, semi-supervised learning and community detection. version:1
arxiv-1608-07597 | A Randomized Approach to Efficient Kernel Clustering | http://arxiv.org/abs/1608.07597 | id:1608.07597 author:Farhad Pourkamali-Anaraki, Stephen Becker category:stat.ML  published:2016-08-26 summary:Kernel-based K-means clustering has gained popularity due to its simplicity and the power of its implicit non-linear representation of the data. A dominant concern is the memory requirement since memory scales as the square of the number of data points. We provide a new analysis of a class of approximate kernel methods that have more modest memory requirements, and propose a specific one-pass randomized kernel approximation followed by standard K-means on the transformed data. The analysis and experiments suggest the method is accurate, while requiring drastically less memory than standard kernel K-means and significantly less memory than Nystrom based approximations. version:1
arxiv-1608-07536 | Leveraging over intact priors for boosting control and dexterity of prosthetic hands by amputees | http://arxiv.org/abs/1608.07536 | id:1608.07536 author:Valentina Gregori, Barbara Caputo category:cs.LG stat.ML  published:2016-08-26 summary:Non-invasive myoelectric prostheses require a long training time to obtain satisfactory control dexterity. These training times could possibly be reduced by leveraging over training efforts by previous subjects. So-called domain adaptation algorithms formalize this strategy and have indeed been shown to significantly reduce the amount of required training data for intact subjects for myoelectric movements classification. It is not clear, however, whether these results extend also to amputees and, if so, whether prior information from amputees and intact subjects is equally useful. To overcome this problem, we evaluated several domain adaptation algorithms on data coming from both amputees and intact subjects. Our findings indicate that: (1) the use of previous experience from other subjects allows us to reduce the training time by about an order of magnitude; (2) this improvement holds regardless of whether an amputee exploits previous information from other amputees or from intact subjects. version:1
arxiv-1608-06984 | Learning Human Search Strategies from a Crowdsourcing Game | http://arxiv.org/abs/1608.06984 | id:1608.06984 author:Thurston Sexton, Max Yi Ren category:cs.LG  published:2016-08-24 summary:There is evidence that humans can be more efficient than existing algorithms at searching for good solutions in high-dimensional and non-convex design or control spaces, potentially due to our prior knowledge and learning capability. This work attempts to quantify the search strategy of human beings to enhance a Bayesian optimization (BO) algorithm for an optimal design and control problem. We consider the sequence of human solutions (called a search trajectory) as generated from BO, and propose to recover the algorithmic parameters of BO through maximum likelihood estimation. The method is first verified through simulation studies and then applied to human solutions crowdsourced from a gamified design problem. We learn BO parameters from a player who achieved fast improvement in his/her solutions and show that applying the learned parameters to BO achieves better convergence than using a self-adaptive BO. The proposed method is different from inverse reinforcement learning in that it only requires a good search strategy, rather than near-optimal solutions from humans. version:2
arxiv-1608-07526 | Maximum Correntropy Unscented Filter | http://arxiv.org/abs/1608.07526 | id:1608.07526 author:Xi Liu, Badong Chen, Bin Xu, Zongze Wu, Paul Honeine category:stat.ML  published:2016-08-26 summary:The unscented transformation (UT) is an efficient method to solve the state estimation problem for a non-linear dynamic system, utilizing a derivative-free higher-order approximation by approximating a Gaussian distribution rather than approximating a non-linear function. Applying the UT to a Kalman filter type estimator leads to the well-known unscented Kalman filter (UKF). Although the UKF works very well in Gaussian noises, its performance may deteriorate significantly when the noises are non-Gaussian, especially when the system is disturbed by some heavy-tailed impulsive noises. To improve the robustness of the UKF against impulsive noises, a new filter for nonlinear systems is proposed in this work, namely the maximum correntropy unscented filter (MCUF). In MCUF, the UT is applied to obtain the prior estimates of the state and covariance matrix, and a robust statistical linearization regression based on the maximum correntropy criterion (MCC) is then used to obtain the posterior estimates of the state and covariance. The satisfying performance of the new algorithm is confirmed by two illustrative examples. version:1
arxiv-1608-07502 | Entity Embedding-based Anomaly Detection for Heterogeneous Categorical Events | http://arxiv.org/abs/1608.07502 | id:1608.07502 author:Ting Chen, Lu-An Tang, Yizhou Sun, Zhengzhang Chen, Kai Zhang category:cs.LG cs.CR stat.ML  published:2016-08-26 summary:Anomaly detection plays an important role in modern data-driven security applications, such as detecting suspicious access to a socket from a process. In many cases, such events can be described as a collection of categorical values that are considered as entities of different types, which we call heterogeneous categorical events. Due to the lack of intrinsic distance measures among entities, and the exponentially large event space, most existing work relies heavily on heuristics to calculate abnormal scores for events. Different from previous work, we propose a principled and unified probabilistic model APE (Anomaly detection via Probabilistic pairwise interaction and Entity embedding) that directly models the likelihood of events. In this model, we embed entities into a common latent space using their observed co-occurrence in different events. More specifically, we first model the compatibility of each pair of entities according to their embeddings. Then we utilize the weighted pairwise interactions of different entity types to define the event probability. Using Noise-Contrastive Estimation with "context-dependent" noise distribution, our model can be learned efficiently regardless of the large event space. Experimental results on real enterprise surveillance data show that our methods can accurately detect abnormal events compared to other state-of-the-art abnormal detection techniques. version:1
arxiv-1608-07494 | Estimating the Number of Clusters via Normalized Cluster Instability | http://arxiv.org/abs/1608.07494 | id:1608.07494 author:Jonas M. B. Haslbeck, Dirk U. Wulff category:stat.ML  published:2016-08-26 summary:We improve existing instability-based methods for the selection of the number of clusters $k$ in cluster analysis by normalizing instability. In contrast to existing instability methods which show good performance only for bounded sequence of small $k$s, our improved method achieves high estimation performance for $k$ across the whole sequence of possible $k$s. In addition, we compare for the first time model-based and model-free variants of $k$ selection via cluster instability and find that their performance is similar. We make our method available in the R-package cstab. version:1
arxiv-1608-07470 | A Fast Ellipse Detector Using Projective Invariant Pruning | http://arxiv.org/abs/1608.07470 | id:1608.07470 author:Qi Jia, Xin Fan, Zhongxuan Luo, Lianbo Song, Tie Qiu category:cs.CV cs.CG I.4.6; I.4.8  published:2016-08-26 summary:Detecting elliptical objects from an image is a central task in robot navigation and industrial diagnosis where the detection time is always a critical issue. Existing methods are hardly applicable to these real-time scenarios of limited hardware resource due to the huge number of fragment candidates (edges or arcs) for fitting ellipse equations. In this paper, we present a fast algorithm detecting ellipses with high accuracy. The algorithm leverage a newly developed projective invariant to significantly prune the undesired candidates and to pick out elliptical ones. The invariant is able to reflect the intrinsic geometry of a planar curve, giving the value of -1 on any three collinear points and +1 for any six points on an ellipse. Thus, we apply the pruning and picking by simply comparing these binary values. Moreover, the calculation of the invariant only involves the determinant of a 3*3 matrix. Extensive experiments on three challenging data sets with 650 images demonstrate that our detector runs 20%-50% faster than the state-of-the-art algorithms with the comparable or higher precision. version:1
arxiv-1608-07454 | Fine Hand Segmentation using Convolutional Neural Networks | http://arxiv.org/abs/1608.07454 | id:1608.07454 author:Tadej Vodopivec, Vincent Lepetit, Peter Peer category:cs.CV  published:2016-08-26 summary:We propose a method for extracting very accurate masks of hands in egocentric views. Our method is based on a novel Deep Learning architecture: In contrast with current Deep Learning methods, we do not use upscaling layers applied to a low-dimensional representation of the input image. Instead, we extract features with convolutional layers and map them directly to a segmentation mask with a fully connected layer. We show that this approach, when applied in a multi-scale fashion, is both accurate and efficient enough for real-time. We demonstrate it on a new dataset made of images captured in various environments, from the outdoors to offices. version:1
arxiv-1608-07444 | Who Leads the Clothing Fashion: Style, Color, or Texture? A Computational Study | http://arxiv.org/abs/1608.07444 | id:1608.07444 author:Qin Zou, Zheng Zhang, Qian Wang, Qingquan Li, Long Chen, Song Wang category:cs.CV  published:2016-08-26 summary:It is well known that clothing fashion is a distinctive and often habitual trend in the style in which a person dresses. Clothing fashions are usually expressed with visual stimuli such as style, color, and texture. However, it is not clear which visual stimulus places higher/lower influence on the updating of clothing fashion. In this study, computer vision and machine learning techniques are employed to analyze the influence of different visual stimuli on clothing-fashion updates. Specifically, a classification-based model is proposed to quantify the influence of different visual stimuli, in which each visual stimulus's influence is quantified by its corresponding accuracy in fashion classification. Experimental results demonstrate that, on clothing-fashion updates, the style holds a higher influence than the color, and the color holds a higher influence than the texture. version:1
arxiv-1608-02926 | A pragmatic theory of generic language | http://arxiv.org/abs/1608.02926 | id:1608.02926 author:Michael Henry Tessler, Noah D. Goodman category:cs.CL  published:2016-08-09 summary:Generalizations about categories are central to human understanding, and generic language (e.g. "Dogs bark.") provides a simple and ubiquitous way to communicate these generalizations. Yet the meaning of generic language is philosophically puzzling and has resisted precise formalization. We explore the idea that the core meaning of a generic sentence is simple but underspecified, and that general principles of pragmatic reasoning are responsible for establishing the precise meaning in context. Building on recent probabilistic models of language understanding, we provide a formal model for the evaluation and comprehension of generic sentences. This model explains the puzzling flexibility in usage of generics in terms of diverse prior beliefs about properties. We elicit these priors experimentally and show that the resulting model predictions explain almost all of the variance in human judgments for both common and novel generics. We probe the theory in more detail, and find that generic language depends in a fundamental way on subjective beliefs, not mere frequency. This theory provides the mathematical bridge between the words we use and the concepts they describe. version:2
arxiv-1608-07441 | Hard Negative Mining for Metric Learning Based Zero-Shot Classification | http://arxiv.org/abs/1608.07441 | id:1608.07441 author:Maxime Bucher, Stéphane Herbin, Frédéric Jurie category:cs.LG cs.AI cs.CV stat.ML  published:2016-08-26 summary:Zero-Shot learning has been shown to be an efficient strategy for domain adaptation. In this context, this paper builds on the recent work of Bucher et al. [1], which proposed an approach to solve Zero-Shot classification problems (ZSC) by introducing a novel metric learning based objective function. This objective function allows to learn an optimal embedding of the attributes jointly with a measure of similarity between images and attributes. This paper extends their approach by proposing several schemes to control the generation of the negative pairs, resulting in a significant improvement of the performance and giving above state-of-the-art results on three challenging ZSC datasets. version:1
arxiv-1608-07411 | An Octree-Based Approach towards Efficient Variational Range Data Fusion | http://arxiv.org/abs/1608.07411 | id:1608.07411 author:Wadim Kehl, Tobias Holl, Federico Tombari, Slobodan Ilic, Nassir Navab category:cs.CV  published:2016-08-26 summary:Volume-based reconstruction is usually expensive both in terms of memory consumption and runtime. Especially for sparse geometric structures, volumetric representations produce a huge computational overhead. We present an efficient way to fuse range data via a variational Octree-based minimization approach by taking the actual range data geometry into account. We transform the data into Octree-based truncated signed distance fields and show how the optimization can be conducted on the newly created structures. The main challenge is to uphold speed and a low memory footprint without sacrificing the solutions' accuracy during optimization. We explain how to dynamically adjust the optimizer's geometric structure via joining/splitting of Octree nodes and how to define the operators. We evaluate on various datasets and outline the suitability in terms of performance and geometric accuracy. version:1
arxiv-1608-07400 | Collaborative Filtering with Recurrent Neural Networks | http://arxiv.org/abs/1608.07400 | id:1608.07400 author:Robin Devooght, Hugues Bersini category:cs.IR cs.LG  published:2016-08-26 summary:We show that collaborative filtering can be viewed as a sequence prediction problem, and that given this interpretation, recurrent neural networks offer very competitive approach. In particular we study how the long short-term memory (LSTM) can be applied to collaborative filtering, and how it compares to standard nearest neighbors and matrix factorization methods on movie recommendation. We show that the LSTM is competitive in all aspects, and largely outperforms other methods in terms of item coverage and short term predictions. version:1
arxiv-1608-07373 | Applying Topological Persistence in Convolutional Neural Network for Music Audio Signals | http://arxiv.org/abs/1608.07373 | id:1608.07373 author:Jen-Yu Liu, Shyh-Kang Jeng, Yi-Hsuan Yang category:cs.NE cs.CG cs.MM cs.SD  published:2016-08-26 summary:Recent years have witnessed an increased interest in the application of persistent homology, a topological tool for data analysis, to machine learning problems. Persistent homology is known for its ability to numerically characterize the shapes of spaces induced by features or functions. On the other hand, deep neural networks have been shown effective in various tasks. To our best knowledge, however, existing neural network models seldom exploit shape information. In this paper, we investigate a way to use persistent homology in the framework of deep neural networks. Specifically, we propose to embed the so-called "persistence landscape," a rather new topological summary for data, into a convolutional neural network (CNN) for dealing with audio signals. Our evaluation on automatic music tagging, a multi-label classification task, shows that the resulting persistent convolutional neural network (PCNN) model can perform significantly better than state-of-the-art models in prediction accuracy. We also discuss the intuition behind the design of the proposed model, and offer insights into the features that it learns. version:1
arxiv-1608-07365 | Scalable Compression of Deep Neural Networks | http://arxiv.org/abs/1608.07365 | id:1608.07365 author:Xing Wang, Jie Liang category:cs.CV  published:2016-08-26 summary:Deep neural networks generally involve some layers with mil- lions of parameters, making them difficult to be deployed and updated on devices with limited resources such as mobile phones and other smart embedded systems. In this paper, we propose a scalable representation of the network parameters, so that different applications can select the most suitable bit rate of the network based on their own storage constraints. Moreover, when a device needs to upgrade to a high-rate network, the existing low-rate network can be reused, and only some incremental data are needed to be downloaded. We first hierarchically quantize the weights of a pre-trained deep neural network to enforce weight sharing. Next, we adaptively select the bits assigned to each layer given the total bit budget. After that, we retrain the network to fine-tune the quantized centroids. Experimental results show that our method can achieve scalable compression with graceful degradation in the performance. version:1
arxiv-1608-07338 | Fast Trajectory Simplification Algorithm for Natural User Interfaces in Robot Programming by Demonstration | http://arxiv.org/abs/1608.07338 | id:1608.07338 author:Daniel L. Marino, Milos Manic category:cs.CV  published:2016-08-25 summary:Trajectory simplification is a problem encountered in areas like Robot programming by demonstration, CAD/CAM, computer vision, and in GPS-based applications like traffic analysis. This problem entails reduction of the points in a given trajectory while keeping the relevant points which preserve important information. The benefits include storage reduction, computational expense, while making data more manageable. Common techniques formulate a minimization problem to be solved, where the solution is found iteratively under some error metric, which causes the algorithms to work in super-linear time. We present an algorithm called FastSTray, which selects the relevant points in the trajectory in linear time by following an open loop heuristic approach. While most current trajectory simplification algorithms are tailored for GPS trajectories, our approach focuses on smooth trajectories for robot programming by demonstration recorded using motion capture systems.Two variations of the algorithm are presented: 1. aims to preserve shape and temporal information; 2. preserves only shape information. Using the points in the simplified trajectory we use cubic splines to interpolate between these points and recreate the original trajectory. The presented algorithm was tested on trajectories recorded from a hand-tracking system. It was able to eliminate about 90% of the points in the original trajectories while maintaining errors between 0.78-2cm which corresponds to 1%-2.4% relative error with respect to the bounding box of the trajectories. version:1
arxiv-1608-06010 | Feedback-Controlled Sequential Lasso Screening | http://arxiv.org/abs/1608.06010 | id:1608.06010 author:Yun Wang, Xu Chen, Peter J. Ramadge category:cs.LG cs.AI cs.CV stat.ML  published:2016-08-21 summary:One way to solve lasso problems when the dictionary does not fit into available memory is to first screen the dictionary to remove unneeded features. Prior research has shown that sequential screening methods offer the greatest promise in this endeavor. Most existing work on sequential screening targets the context of tuning parameter selection, where one screens and solves a sequence of $N$ lasso problems with a fixed grid of geometrically spaced regularization parameters. In contrast, we focus on the scenario where a target regularization parameter has already been chosen via cross-validated model selection, and we then need to solve many lasso instances using this fixed value. In this context, we propose and explore a feedback controlled sequential screening scheme. Feedback is used at each iteration to select the next problem to be solved. This allows the sequence of problems to be adapted to the instance presented and the number of intermediate problems to be automatically selected. We demonstrate our feedback scheme using several datasets including a dictionary of approximate size 100,000 by 300,000. version:2
arxiv-1608-07328 | Fundamental Limits of Budget-Fidelity Trade-off in Label Crowdsourcing | http://arxiv.org/abs/1608.07328 | id:1608.07328 author:Farshad Lahouti, Babak Hassibi category:cs.LG cs.IT math.IT  published:2016-08-25 summary:Digital crowdsourcing (CS) is a modern approach to perform certain large projects using small contributions of a large crowd. In CS, a taskmaster typically breaks down the project into small batches of tasks and assigns them to so-called workers with imperfect skill levels. The crowdsourcer then collects and analyzes the results for inference and serving the purpose of the project. In this work, the CS problem, as a human-in-the-loop computation problem, is modeled and analyzed in an information theoretic rate-distortion framework. The purpose is to identify the ultimate fidelity that one can achieve by any form of query from the crowd and any decoding (inference) algorithm with a given budget. The results are established by a joint source channel (de)coding scheme, which represent the query scheme and inference, over parallel noisy channels, which model workers with imperfect skill levels. We also present and analyze a query scheme dubbed $k$-ary incidence coding and study optimized query pricing in this setting. version:1
arxiv-1608-06014 | The Symmetry of a Simple Optimization Problem in Lasso Screening | http://arxiv.org/abs/1608.06014 | id:1608.06014 author:Yun Wang, Peter J. Ramadge category:cs.LG cs.AI cs.CV stat.ML  published:2016-08-21 summary:Recently dictionary screening has been proposed as an effective way to improve the computational efficiency of solving the lasso problem, which is one of the most commonly used method for learning sparse representations. To address today's ever increasing large dataset, effective screening relies on a tight region bound on the solution to the dual lasso. Typical region bounds are in the form of an intersection of a sphere and multiple half spaces. One way to tighten the region bound is using more half spaces, which however, adds to the overhead of solving the high dimensional optimization problem in lasso screening. This paper reveals the interesting property that the optimization problem only depends on the projection of features onto the subspace spanned by the normals of the half spaces. This property converts an optimization problem in high dimension to much lower dimension, and thus sheds light on reducing the computation overhead of lasso screening based on tighter region bounds. version:2
arxiv-1608-07310 | Learning in concave games with imperfect information | http://arxiv.org/abs/1608.07310 | id:1608.07310 author:Panayotis Mertikopoulos category:math.OC cs.GT cs.LG  published:2016-08-25 summary:This paper examines the convergence properties of a class of learning schemes for concave N-person games - that is, games with convex action spaces and individually concave payoff functions. Specifically, we focus on a family of learning methods where players adjust their actions by taking small steps along their individual payoff gradients and then "mirror" the output back to their feasible action spaces. Assuming players only have access to gradient information that is accurate up to a zero-mean error with bounded variance, we show that when the process converges, its limit is a Nash equilibrium. We also introduce an equilibrium stability notion which we call variational stability (VS), and we show that stable equilibria are locally attracting with high probability whereas globally stable states are globally attracting with probability 1. Additionally, in finite games, we find that dominated strategies become extinct, strict equilibria are locally attracting with high probability, and the long-term average of the process converges to equilibrium in 2-player zero-sum games. Finally, we examine the scheme's convergence speed and we show that if the game admits a strict equilibrium and the players' mirror maps are surjective, then, with high probability, the process converges to equilibrium in a finite number of steps, no matter the level of uncertainty. version:1
arxiv-0507037 | Network Topology influences Synchronization and Intrinsic Read-out | http://arxiv.org/abs/q-bio/0507037 | id:0507037 author:Gabriele Scheler category:q-bio.NC cs.NE nlin.AO  published:2005-07-25 summary:What are the effects of neuromodulation on a large network model? Neuromodulation influences neural processing by presynaptic and postsynaptic regulation of synaptic efficacy and by ion channel regulation for dendritic excitability. We present a model, where regulation of synaptic efficacy changes the overall connectivity, or topology, of the network, and regulation of dendritic ion channels sets intrinsic excitability, or the gain of the neuron. We show that network topology influences synchronization, i. e. the correlations of spiking activity generated in the network and synchronization influences the read-out of intrinsic properties. Highly synchronous input drives neurons, such that differences in intrinsic properties disappear, while asynchronous input lets intrinsic properties determine output behavior. We conclude that neuromodulation may allow a network to alternate between a synchronized transmission mode and an asynchronous intrinsic read-out mode. version:2
arxiv-1608-07253 | Learning Latent Vector Spaces for Product Search | http://arxiv.org/abs/1608.07253 | id:1608.07253 author:Christophe Van Gysel, Maarten de Rijke, Evangelos Kanoulas category:cs.IR cs.AI cs.CL  published:2016-08-25 summary:We introduce a novel latent vector space model that jointly learns the latent representations of words, e-commerce products and a mapping between the two without the need for explicit annotations. The power of the model lies in its ability to directly model the discriminative relation between products and a particular word. We compare our method to existing latent vector space models (LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank setting. Our latent vector space model achieves its enhanced performance as it learns better product representations. Furthermore, the mapping from words to products and the representations of words benefit directly from the errors propagated back from the product representations during parameter estimation. We provide an in-depth analysis of the performance of our model and analyze the structure of the learned representations. version:1
arxiv-1608-07242 | Modeling and Propagating CNNs in a Tree Structure for Visual Tracking | http://arxiv.org/abs/1608.07242 | id:1608.07242 author:Hyeonseob Nam, Mooyeol Baek, Bohyung Han category:cs.CV  published:2016-08-25 summary:We present an online visual tracking algorithm by managing multiple target appearance models in a tree structure. The proposed algorithm employs Convolutional Neural Networks (CNNs) to represent target appearances, where multiple CNNs collaborate to estimate target states and determine the desirable paths for online model updates in the tree. By maintaining multiple CNNs in diverse branches of tree structure, it is convenient to deal with multi-modality in target appearances and preserve model reliability through smooth updates along tree paths. Since multiple CNNs share all parameters in convolutional layers, it takes advantage of multiple models with little extra cost by saving memory space and avoiding redundant network evaluations. The final target state is estimated by sampling target candidates around the state in the previous frame and identifying the best sample in terms of a weighted average score from a set of active CNNs. Our algorithm illustrates outstanding performance compared to the state-of-the-art techniques in challenging datasets such as online tracking benchmark and visual object tracking challenge. version:1
arxiv-1608-07241 | Formal Concept Analysis of Rodent Carriers of Zoonotic Disease | http://arxiv.org/abs/1608.07241 | id:1608.07241 author:Roman Ilin, Barbara A. Han category:stat.ML  published:2016-08-25 summary:The technique of Formal Concept Analysis is applied to a dataset describing the traits of rodents, with the goal of identifying zoonotic disease carriers,or those species carrying infections that can spillover to cause human disease. The concepts identified among these species together provide rules-of-thumb about the intrinsic biological features of rodents that carry zoonotic diseases, and offer utility for better targeting field surveillance efforts in the search for novel disease carriers in the wild. version:1
arxiv-1608-07179 | Minimizing Quadratic Functions in Constant Time | http://arxiv.org/abs/1608.07179 | id:1608.07179 author:Kohei Hayashi, Yuichi Yoshida category:cs.LG cs.DS stat.ML  published:2016-08-25 summary:A sampling-based optimization method for quadratic functions is proposed. Our method approximately solves the following $n$-dimensional quadratic minimization problem in constant time, which is independent of $n$: $z^*=\min_{\mathbf{v} \in \mathbb{R}^n}\langle\mathbf{v}, A \mathbf{v}\rangle + n\langle\mathbf{v}, \mathrm{diag}(\mathbf{d})\mathbf{v}\rangle + n\langle\mathbf{b}, \mathbf{v}\rangle$, where $A \in \mathbb{R}^{n \times n}$ is a matrix and $\mathbf{d},\mathbf{b} \in \mathbb{R}^n$ are vectors. Our theoretical analysis specifies the number of samples $k(\delta, \epsilon)$ such that the approximated solution $z$ satisfies $ z - z^* = O(\epsilon n^2)$ with probability $1-\delta$. The empirical performance (accuracy and runtime) is positively confirmed by numerical experiments. version:1
arxiv-1608-07159 | Active Robust Learning | http://arxiv.org/abs/1608.07159 | id:1608.07159 author:Hossein Ghafarian, Hadi Sadoghi Yazdi category:cs.LG math.OC  published:2016-08-25 summary:In many practical applications of learning algorithms, unlabeled data is cheap and abundant whereas labeled data is expensive. Active learning algorithms developed to achieve better performance with lower cost. Usually Representativeness and Informativeness are used in active learning algoirthms. Advanced recent active learning methods consider both of these criteria. Despite its vast literature, very few active learning methods consider noisy instances, i.e. label noisy and outlier instances. Also, these methods didn't consider accuracy in computing representativeness and informativeness. Based on the idea that inaccuracy in these measures and not taking noisy instances into consideration are two sides of a coin and are inherently related, a new loss function is proposed. This new loss function helps to decrease the effect of noisy instances while at the same time, reduces bias. We defined "instance complexity" as a new notion of complexity for instances of a learning problem. It is proved that noisy instances in the data if any, are the ones with maximum instance complexity. Based on this loss function which has two functions for classifying ordinary and noisy instances, a new classifier, named "Simple-Complex Classifier" is proposed. In this classifier there are a simple and a complex function, with the complex function responsible for selecting noisy instances. The resulting optimization problem for both learning and active learning is highly non-convex and very challenging. In order to solve it, a convex relaxation is proposed. version:1
arxiv-1608-07138 | Sympathy for the Details: Dense Trajectories and Hybrid Classification Architectures for Action Recognition | http://arxiv.org/abs/1608.07138 | id:1608.07138 author:César Roberto de Souza, Adrien Gaidon, Eleonora Vig, Antonio Manuel López category:cs.CV  published:2016-08-25 summary:Action recognition in videos is a challenging task due to the complexity of the spatio-temporal patterns to model and the difficulty to acquire and learn on large quantities of video data. Deep learning, although a breakthrough for image classification and showing promise for videos, has still not clearly superseded action recognition methods using hand-crafted features, even when training on massive datasets. In this paper, we introduce hybrid video classification architectures based on carefully designed unsupervised representations of hand-crafted spatio-temporal features classified by supervised deep networks. As we show in our experiments on five popular benchmarks for action recognition, our hybrid model combines the best of both worlds: it is data efficient (trained on 150 to 10000 short clips) and yet improves significantly on the state of the art, including recent deep models trained on millions of manually labelled images and videos. version:1
arxiv-1608-07115 | Aligning Packed Dependency Trees: a theory of composition for distributional semantics | http://arxiv.org/abs/1608.07115 | id:1608.07115 author:David Weir, Julie Weeds, Jeremy Reffin, Thomas Kober category:cs.CL  published:2016-08-25 summary:We present a new framework for compositional distributional semantics in which the distributional contexts of lexemes are expressed in terms of anchored packed dependency trees. We show that these structures have the potential to capture the full sentential contexts of a lexeme and provide a uniform basis for the composition of distributional knowledge in a way that captures both mutual disambiguation and generalization. version:1
arxiv-1608-03587 | The statistical trade-off between word order and word structure - large-scale evidence for the principle of least effort | http://arxiv.org/abs/1608.03587 | id:1608.03587 author:Alexander Koplenig, Peter Meyer, Sascha Wolfer, Carolin Mueller-Spitzer category:cs.CL  published:2016-08-11 summary:Languages employ different strategies to transmit structural and grammatical information. While, for example, grammatical dependency relationships in sentences are mainly conveyed by the ordering of the words for languages like Mandarin Chinese, or Vietnamese, the word ordering is much less restricted for languages such as Inupiatun or Quechua, as those languages (also) use the internal structure of words (e.g. inflectional morphology) to mark grammatical relationships in a sentence. Based on a quantitative analysis of more than 1,500 unique translations of different books of the Bible in more than 1,100 different languages that are spoken as a native language by approximately 6 billion people (more than 80% of the world population), we present large-scale evidence for a statistical trade-off between the amount of information conveyed by the ordering of words and the amount of information conveyed by internal word structure: languages that rely more strongly on word order information tend to rely less on word structure information and vice versa. In addition, we find that - despite differences in the way information is expressed - there is also evidence for a trade-off between different books of the biblical canon that recurs with little variation across languages: the more informative the word order of the book, the less informative its word structure and vice versa. We argue that this might suggest that, on the one hand, languages encode information in very different (but efficient) ways. On the other hand, content-related and stylistic features are statistically encoded in very similar ways. version:2
arxiv-1608-07094 | A Novel Term_Class Relevance Measure for Text Categorization | http://arxiv.org/abs/1608.07094 | id:1608.07094 author:D S Guru, Mahamad Suhil category:cs.IR cs.CL  published:2016-08-25 summary:In this paper, we introduce a new measure called Term_Class relevance to compute the relevancy of a term in classifying a document into a particular class. The proposed measure estimates the degree of relevance of a given term, in placing an unlabeled document to be a member of a known class, as a product of Class_Term weight and Class_Term density; where the Class_Term weight is the ratio of the number of documents of the class containing the term to the total number of documents containing the term and the Class_Term density is the relative density of occurrence of the term in the class to the total occurrence of the term in the entire population. Unlike the other existing term weighting schemes such as TF-IDF and its variants, the proposed relevance measure takes into account the degree of relative participation of the term across all documents of the class to the entire population. To demonstrate the significance of the proposed measure experimentation has been conducted on the 20 Newsgroups dataset. Further, the superiority of the novel measure is brought out through a comparative analysis. version:1
arxiv-1608-07076 | A Context-aware Natural Language Generator for Dialogue Systems | http://arxiv.org/abs/1608.07076 | id:1608.07076 author:Ondřej Dušek, Filip Jurčíček category:cs.CL I.2.7  published:2016-08-25 summary:We present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users' way of speaking, providing contextually appropriate responses. The generator is based on recurrent neural networks and the sequence-to-sequence approach. It is fully trainable from data which include preceding context along with responses to be generated. We show that the context-aware generator yields significant improvements over the baseline in both automatic metrics and a human pairwise preference test. version:1
arxiv-1608-07068 | Title Generation for User Generated Videos | http://arxiv.org/abs/1608.07068 | id:1608.07068 author:Kuo-Hao Zeng, Tseng-Hung Chen, Juan Carlos Niebles, Min Sun category:cs.CV cs.AI cs.MM  published:2016-08-25 summary:A great video title describes the most salient event compactly and captures the viewer's attention. In contrast, video captioning tends to generate sentences that describe the video as a whole. Although generating a video title automatically is a very useful task, it is much less addressed than video captioning. We address video title generation for the first time by proposing two methods that extend state-of-the-art video captioners to this new task. First, we make video captioners highlight sensitive by priming them with a highlight detector. Our framework allows for jointly training a model for title generation and video highlight localization. Second, we induce high sentence diversity in video captioners, so that the generated titles are also diverse and catchy. This means that a large number of sentences might be required to learn the sentence structure of titles. Hence, we propose a novel sentence augmentation method to train a captioner with additional sentence-only examples that come without corresponding videos. We collected a large-scale Video Titles in the Wild (VTW) dataset of 18100 automatically crawled user-generated videos and titles. On VTW, our methods consistently improve title prediction accuracy, and achieve the best performance in both automatic and human evaluation. Finally, our sentence augmentation method also outperforms the baselines on the M-VAD dataset. version:1
arxiv-1608-07051 | Learning Points and Routes to Recommend Trajectories | http://arxiv.org/abs/1608.07051 | id:1608.07051 author:Dawei Chen, Cheng Soon Ong, Lexing Xie category:cs.LG cs.IR  published:2016-08-25 summary:The problem of recommending tours to travellers is an important and broadly studied area. Suggested solutions include various approaches of points-of-interest (POI) recommendation and route planning. We consider the task of recommending a sequence of POIs, that simultaneously uses information about POIs and routes. Our approach unifies the treatment of various sources of information by representing them as features in machine learning algorithms, enabling us to learn from past behaviour. Information about POIs are used to learn a POI ranking model that accounts for the start and end points of tours. Data about previous trajectories are used for learning transition patterns between POIs that enable us to recommend probable routes. In addition, a probabilistic model is proposed to combine the results of POI ranking and the POI to POI transitions. We propose a new F$_1$ score on pairs of POIs that capture the order of visits. Empirical results show that our approach improves on recent methods, and demonstrate that combining points and routes enables better trajectory recommendations. version:1
arxiv-1608-06111 | An Incremental Parser for Abstract Meaning Representation | http://arxiv.org/abs/1608.06111 | id:1608.06111 author:Marco Damonte, Shay B. Cohen, Giorgio Satta category:cs.CL  published:2016-08-22 summary:We describe an incremental parser for mapping input text into Abstract Meaning Representation (AMR) graphs. Our parser is based on a transition system for dependency parsing, and allows to incrementally parse the sentence into an AMR graph, if all other components that are needed by the parser (such as the part-of-speech tagger and the dependency parser) are incremental as well. We show that our parser has an advantage over state-of-the-art AMR parsers on several metrics. version:2
arxiv-1608-07019 | Comparison among dimensionality reduction techniques based on Random Projection for cancer classification | http://arxiv.org/abs/1608.07019 | id:1608.07019 author:Haozhe Xie, Jie Li, Qiaosheng Zhang, Yadong Wang category:cs.LG stat.ML  published:2016-08-25 summary:Random Projection (RP) technique has been widely applied in many scenarios because it can reduce high-dimensional features into low-dimensional space within short time and meet the need of real-time analysis of massive data. There is an urgent need of dimensionality reduction with fast increase of big genomics data. However, the performance of RP is usually lower. We attempt to improve classification accuracy of RP through combining other reduction dimension methods such as Principle Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Feature Selection (FS). We compared classification accuracy and running time of different combination methods on three microarray datasets and a simulation dataset. Experimental results show a remarkable improvement of 14.77% in classification accuracy of FS followed by RP compared to RP on BC-TCGA dataset. LDA followed by RP also helps RP to yield a more discriminative subspace with an increase of 13.65% on classification accuracy on the same dataset. FS followed by RP outperforms other combination methods in classification accuracy on most of the datasets. version:1
arxiv-1608-07017 | Ambient Sound Provides Supervision for Visual Learning | http://arxiv.org/abs/1608.07017 | id:1608.07017 author:Andrew Owens, Jiajun Wu, Josh H. McDermott, William T. Freeman, Antonio Torralba category:cs.CV  published:2016-08-25 summary:The sound of crashing waves, the roar of fast-moving cars -- sound conveys important information about the objects in our surroundings. In this work, we show that ambient sounds can be used as a supervisory signal for learning visual models. To demonstrate this, we train a convolutional neural network to predict a statistical summary of the sound associated with a video frame. We show that, through this process, the network learns a representation that conveys information about objects and scenes. We evaluate this representation on several recognition tasks, finding that its performance is comparable to that of other state-of-the-art unsupervised learning methods. Finally, we show through visualizations that the network learns units that are selective to objects that are often associated with characteristic sounds. version:1
arxiv-1608-07005 | Multi-View Fuzzy Clustering with Minimax Optimization for Effective Clustering of Data from Multiple Sources | http://arxiv.org/abs/1608.07005 | id:1608.07005 author:Yangtao Wang, Lihui Chen category:cs.AI cs.LG stat.ML  published:2016-08-25 summary:Multi-view data clustering refers to categorizing a data set by making good use of related information from multiple representations of the data. It becomes important nowadays because more and more data can be collected in a variety of ways, in different settings and from different sources, so each data set can be represented by different sets of features to form different views of it. Many approaches have been proposed to improve clustering performance by exploring and integrating heterogeneous information underlying different views. In this paper, we propose a new multi-view fuzzy clustering approach called MinimaxFCM by using minimax optimization based on well-known Fuzzy c means. In MinimaxFCM the consensus clustering results are generated based on minimax optimization in which the maximum disagreements of different weighted views are minimized. Moreover, the weight of each view can be learned automatically in the clustering process. In addition, there is only one parameter to be set besides the fuzzifier. The detailed problem formulation, updating rules derivation, and the in-depth analysis of the proposed MinimaxFCM are provided here. Experimental studies on nine multi-view data sets including real world image and document data sets have been conducted. We observed that MinimaxFCM outperforms related multi-view clustering approaches in terms of clustering accuracy, demonstrating the great potential of MinimaxFCM for multi-view data analysis. version:1
arxiv-1608-07001 | Incremental Minimax Optimization based Fuzzy Clustering for Large Multi-view Data | http://arxiv.org/abs/1608.07001 | id:1608.07001 author:Yangtao Wang, Lihui Chen, Xiaoli Li category:cs.AI cs.LG stat.ML  published:2016-08-25 summary:Incremental clustering approaches have been proposed for handling large data when given data set is too large to be stored. The key idea of these approaches is to find representatives to represent each cluster in each data chunk and final data analysis is carried out based on those identified representatives from all the chunks. However, most of the incremental approaches are used for single view data. As large multi-view data generated from multiple sources becomes prevalent nowadays, there is a need for incremental clustering approaches to handle both large and multi-view data. In this paper we propose a new incremental clustering approach called incremental minimax optimization based fuzzy clustering (IminimaxFCM) to handle large multi-view data. In IminimaxFCM, representatives with multiple views are identified to represent each cluster by integrating multiple complementary views using minimax optimization. The detailed problem formulation, updating rules derivation, and the in-depth analysis of the proposed IminimaxFCM are provided. Experimental studies on several real world multi-view data sets have been conducted. We observed that IminimaxFCM outperforms related incremental fuzzy clustering in terms of clustering accuracy, demonstrating the great potential of IminimaxFCM for large multi-view data analysis. version:1
arxiv-1608-06993 | Densely Connected Convolutional Networks | http://arxiv.org/abs/1608.06993 | id:1608.06993 author:Gao Huang, Zhuang Liu, Kilian Q. Weinberger category:cs.CV cs.LG  published:2016-08-25 summary:Recent work has shown that convolutional networks can be substantially deeper, more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet), where each layer is directly connected to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections, one between each layer and its subsequent layer (treating the input as layer 0), our network has L(L+1)/2 direct connections. For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. Our proposed connectivity pattern has several compelling advantages: it alleviates the vanishing gradient problem and strengthens feature propagation; despite the increase in connections, it encourages feature reuse and leads to a substantial reduction of parameters; its models tend to generalize surprisingly well. We evaluate our proposed architecture on five highly competitive object recognition benchmark tasks. The DenseNet obtains significant improvements over the state-of-the-art on all five of them (e.g., yielding 3.74% test error on CIFAR-10, 19.25% on CIFAR-100 and 1.59% on SVHN). version:1
arxiv-1608-06985 | A 4D Light-Field Dataset and CNN Architectures for Material Recognition | http://arxiv.org/abs/1608.06985 | id:1608.06985 author:Ting-Chun Wang, Jun-Yan Zhu, Ebi Hiroaki, Manmohan Chandraker, Alexei A. Efros, Ravi Ramamoorthi category:cs.CV  published:2016-08-24 summary:We introduce a new light-field dataset of materials, and take advantage of the recent success of deep learning to perform material recognition on the 4D light-field. Our dataset contains 12 material categories, each with 100 images taken with a Lytro Illum, from which we extract about 30,000 patches in total. To the best of our knowledge, this is the first mid-size dataset for light-field images. Our main goal is to investigate whether the additional information in a light-field (such as multiple sub-aperture views and view-dependent reflectance effects) can aid material recognition. Since recognition networks have not been trained on 4D images before, we propose and compare several novel CNN architectures to train on light-field images. In our experiments, the best performing CNN architecture achieves a 7% boost compared with 2D image classification (70% to 77%). These results constitute important baselines that can spur further research in the use of CNNs for light-field applications. Upon publication, our dataset also enables other novel applications of light-fields, including object detection, image segmentation and view interpolation. version:1
arxiv-1608-02658 | Revisiting Causality Inference In Markov Chain | http://arxiv.org/abs/1608.02658 | id:1608.02658 author:Abbas Shojaee category:stat.ML cs.AI nlin.CD physics.data-an  published:2016-08-08 summary:Identifying causal relationships is a key premise of scientific research. Given the mass of observational data in many disciplines, new machine learning methods offer the possibility of using an empirical approach to identifying unappreciated causal relationships and to understanding causal behavior. Conventional methods of causality inference from observational data require a considerable length of time series data to capture cause and effect relationships. We believe that important causal relationships can be inferred from the composition of one-step transition rates (Markov Chains) to and from an event. Here we introduce 'Causality Inference using Composition of Transitions' (CICT), a computationally efficient method that reveals causal structure with high accuracy. We characterize the differences in causes, effects, and random events in the composition of their inputs and outputs. To demonstrate our method, we have used an administrative inpatient healthcare dataset to set up a graph network of patients transition between different diagnoses. Then we apply our method to patients transition graph, revealing deep and complex causal structure between clinical conditions. Our method is highly accurate in predicting whether a transition in a Markov chain is causal or random and performs well in identifying the direction of causality in bidirectional associations. Moreover, CICT brings in new information that enables unsupervised clustering methods to discriminate causality from randomness. Comprehensive performance analysis using C-statistics, goodness-of-fit statistics and decision analysis of predictive models, as well as comparison with the medical ground truth, validates our findings. version:2
arxiv-1608-06902 | Recurrent Neural Networks With Limited Numerical Precision | http://arxiv.org/abs/1608.06902 | id:1608.06902 author:Joachim Ott, Zhouhan Lin, Ying Zhang, Shih-Chii Liu, Yoshua Bengio category:cs.NE 62M45  published:2016-08-24 summary:Recurrent Neural Networks (RNNs) produce state-of-art performance on many machine learning tasks but their demand on resources in terms of memory and computational power are often high. Therefore, there is a great interest in optimizing the computations performed with these models especially when considering development of specialized low-power hardware for deep networks. One way of reducing the computational needs is to limit the numerical precision of the network weights and biases. This has led to different proposed rounding methods which have been applied so far to only Convolutional Neural Networks and Fully-Connected Networks. This paper addresses the question of how to best reduce weight precision during training in the case of RNNs. We present results from the use of different stochastic and deterministic reduced precision training methods applied to three major RNN types which are then tested on several datasets. The results show that the weight binarization methods do not work with the RNNs. However, the stochastic and deterministic ternarization, and pow2-ternarization methods gave rise to low-precision RNNs that produce similar and even higher accuracy on certain datasets therefore providing a path towards training more efficient implementations of RNNs in specialized hardware. version:1
arxiv-1608-06891 | Pose Estimation from Line Correspondences using Direct Linear Transformation | http://arxiv.org/abs/1608.06891 | id:1608.06891 author:Bronislav Přibyl, Pavel Zemčík, Martin Čadík category:cs.CV 68T45 I.4.8; I.4.1  published:2016-08-24 summary:This work is concerned with camera pose estimation from correspondences of 3D/2D lines, i.e. with the Perspective-n-Line (PnL) problem. We focus on large line sets, which can be efficiently solved by methods using linear formulation of PnL. We propose a novel method 'DLT-Combined-Lines' based on the Direct Linear Transformation (DLT) algorithm, which benefits from a new combination of two existing DLT methods for pose estimation. The method represents 2D structure by lines, and 3D structure by both points and lines. The redundant 3D information reduces the minimum of required line correspondences to 5. A cornerstone of the method is a combined projection matrix estimated by the DLT algorithm. It contains multiple estimates of camera rotation and translation, which can be recovered after enforcing constraints of the matrix. Multiplicity of the estimates is exploited to improve accuracy of the proposed method. For large line sets (10 and more), the method achieves state-of-the-art accuracy on synthetic data even under strong image noise. Moreover, it is the most accurate method on real world data, outperforming the state-of-the-art by a large margin. The proposed method is also highly computationally effective, estimating the pose of 1000 lines in 12 ms on a desktop computer. version:1
arxiv-1608-06879 | AIDE: Fast and Communication Efficient Distributed Optimization | http://arxiv.org/abs/1608.06879 | id:1608.06879 author:Sashank J. Reddi, Jakub Konečný, Peter Richtárik, Barnabás Póczós, Alex Smola category:math.OC cs.LG stat.ML  published:2016-08-24 summary:In this paper, we present two new communication-efficient methods for distributed minimization of an average of functions. The first algorithm is an inexact variant of the DANE algorithm that allows any local algorithm to return an approximate solution to a local subproblem. We show that such a strategy does not affect the theoretical guarantees of DANE significantly. In fact, our approach can be viewed as a robustification strategy since the method is substantially better behaved than DANE on data partition arising in practice. It is well known that DANE algorithm does not match the communication complexity lower bounds. To bridge this gap, we propose an accelerated variant of the first method, called AIDE, that not only matches the communication lower bounds but can also be implemented using a purely first-order oracle. Our empirical results show that AIDE is superior to other communication efficient algorithms in settings that naturally arise in machine learning applications. version:1
arxiv-1608-06863 | Kullback-Leibler Penalized Sparse Discriminant Analysis for Event-Related Potential Classification | http://arxiv.org/abs/1608.06863 | id:1608.06863 author:Victoria Peterson, Hugo Leonardo Rufiner, Ruben Daniel Spies category:cs.CV cs.LG stat.ML  published:2016-08-24 summary:A brain computer interface (BCI) is a system which provides direct communication between the mind of a person and the outside world by using only brain activity (EEG). The event-related potential (ERP)-based BCI problem consists of a binary pattern recognition. Linear discriminant analysis (LDA) is widely used to solve this type of classification problems, but it fails when the number of features is large relative to the number of observations. In this work we propose a penalized version of the sparse discriminant analysis (SDA), called Kullback-Leibler penalized sparse discriminant analysis (KLSDA). This method inherits both the discriminative feature selection and classification properties of SDA and it also improves SDA performance through the addition of Kullback-Leibler class discrepancy information. The KLSDA method is design to automatically select the optimal regularization parameters. Numerical experiments with two real ERP-EEG datasets show that this new method outperforms standard SDA. version:1
arxiv-1608-06807 | Efficient Training for Positive Unlabeled Learning | http://arxiv.org/abs/1608.06807 | id:1608.06807 author:Emanuele Sansone category:cs.LG  published:2016-08-24 summary:Positive unlabeled learning (PU learning) refers to the task of learning a binary classifier from only positive and unlabeled data [1]. This problem arises in various practical applications, like in multimedia/information retrieval [2], where the goal is to find samples in an unlabeled data set that are similar to the samples provided by a user, as well as for applications of outlier detection [3] or semi-supervised novelty detection [4]. The works in [5] and [6] have recently shown that PU learning can be formulated as a risk minimization problem. In particular, expressing the risk with a convex loss function, like the double Hinge loss, allows to achieve better classification performance than those ones obtained by using other loss functions. Nevertheless, the works have only focused in analysing the generalization performance obtained by using different loss functions, without considering the efficiency of training. In that regard, we propose a novel algorithm, which optimizes efficiently the risk minimization problem stated in [6]. In particular, we show that the storage complexity of our approach scales only linearly with the number of training samples. Concerning the training time, we show experimentally on different benchmark data sets that our algorithm exhibits the same quadratic behaviour of existing optimization algorithms implemented in highly-efficient libraries. The rest of the paper is organized as follows. In Section 2 we review the formulation of the PU learning problem and we enunciate for the first time the Representer theorem. In Section 3 we derive the convex formulation of the problem by using the double Hinge loss function. In Section 4 we propose an algorithm to solve the optimization problem and we finally conclude with the last section by describing the experimental evaluation. version:1
arxiv-1608-06800 | In the Saddle: Chasing Fast and Repeatable Features | http://arxiv.org/abs/1608.06800 | id:1608.06800 author:Javier Aldana-Iuit, Dmytro Mishkin, Ondrej Chum, Jiri Matas category:cs.CV  published:2016-08-24 summary:A novel similarity-covariant feature detector that extracts points whose neighbourhoods, when treated as a 3D intensity surface, have a saddle-like intensity profile. The saddle condition is verified efficiently by intensity comparisons on two concentric rings that must have exactly two dark-to-bright and two bright-to-dark transitions satisfying certain geometric constraints. Experiments show that the Saddle features are general, evenly spread and appearing in high density in a range of images. The Saddle detector is among the fastest proposed. In comparison with detector with similar speed, the Saddle features show superior matching performance on number of challenging datasets. version:1
arxiv-1608-06794 | Improving Sparse Word Representations with Distributional Inference for Semantic Composition | http://arxiv.org/abs/1608.06794 | id:1608.06794 author:Thomas Kober, Julie Weeds, Jeremy Reffin, David Weir category:cs.CL  published:2016-08-24 summary:Distributional models are derived from co-occurrences in a corpus, where only a small proportion of all possible plausible co-occurrences will be observed. This results in a very sparse vector space, requiring a mechanism for inferring missing knowledge. Most methods face this challenge in ways that render the resulting word representations uninterpretable, with the consequence that semantic composition becomes hard to model. In this paper we explore an alternative which involves explicitly inferring unobserved co-occurrences using the distributional neighbourhood. We show that distributional inference improves sparse word representations on several word similarity benchmarks and demonstrate that our model is competitive with the state-of-the-art for adjective-noun, noun-noun and verb-object compositions while being fully interpretable. version:1
arxiv-1608-06770 | Automatic Synchronization of Multi-User Photo Galleries | http://arxiv.org/abs/1608.06770 | id:1608.06770 author:E. Sansone, K. Apostolidis, N. Conci, G. Boato, V. Mezaris, F. G. B. De Natale category:cs.MM cs.CV  published:2016-08-24 summary:In this paper we address the issue of photo galleries synchronization, where pictures related to the same event are collected by different users. Existing solutions to address the problem are usually based on unrealistic assumptions, like time consistency across photo galleries, and often heavily rely on heuristics, limiting therefore the applicability to real-world scenarios. We propose a solution that achieves better generalization performance for the synchronization task compared to the available literature. The method is characterized by three stages: at first, deep convolutional neural network features are used to assess the visual similarity among the photos; then, pairs of similar photos are detected across different galleries and used to construct a graph; eventually, a probabilistic graphical model is used to estimate the temporal offset of each pair of galleries, by traversing the minimum spanning tree extracted from this graph. The experimental evaluation is conducted on four publicly available datasets covering different types of events, demonstrating the strength of our proposed method. A thorough discussion of the obtained results is provided for a critical assessment of the quality in synchronization. version:1
arxiv-1608-06761 | A Study of Vision based Human Motion Recognition and Analysis | http://arxiv.org/abs/1608.06761 | id:1608.06761 author:Geetanjali Vinayak Kale, Varsha Hemant Patil category:cs.CV  published:2016-08-24 summary:Vision based human motion recognition has fascinated many researchers due to its critical challenges and a variety of applications. The applications range from simple gesture recognition to complicated behaviour understanding in surveillance system. This leads to major development in the techniques related to human motion representation and recognition. This paper discusses applications, general framework of human motion recognition, and the details of each of its components. The paper emphasizes on human motion representation and the recognition methods along with their advantages and disadvantages. This study also discusses the selected literature, popular datasets, and concludes with the challenges in the domain along with a future direction. The human motion recognition domain has been active for more than two decades, and has provided a large amount of literature. A bird's eye view for new researchers in the domain is presented in the paper. version:1
arxiv-1608-06521 | Does V-NIR based Image Enhancement Come with Better Features? | http://arxiv.org/abs/1608.06521 | id:1608.06521 author:Vivek Sharma, Luc Van Gool category:cs.CV  published:2016-08-23 summary:Image enhancement using the visible (V) and near-infrared (NIR) usually enhances useful image details. The enhanced images are evaluated by observers perception, instead of quantitative feature evaluation. Thus, can we say that these enhanced images using NIR information has better features in comparison to the computed features in the Red, Green, and Blue color channels directly? In this work, we present a new method to enhance the visible images using NIR information via edge-preserving filters, and also investigate which method performs best from a image features standpoint. We then show that our proposed enhancement method produces more stable features than the existing state-of-the-art methods. version:2
arxiv-1608-06757 | Robust Named Entity Recognition in Idiosyncratic Domains | http://arxiv.org/abs/1608.06757 | id:1608.06757 author:Sebastian Arnold, Felix A. Gers, Torsten Kilias, Alexander Löser category:cs.CL  published:2016-08-24 summary:Named entity recognition often fails in idiosyncratic domains. That causes a problem for depending tasks, such as entity linking and relation extraction. We propose a generic and robust approach for high-recall named entity recognition. Our approach is easy to train and offers strong generalization over diverse domain-specific language, such as news documents (e.g. Reuters) or biomedical text (e.g. Medline). Our approach is based on deep contextual sequence learning and utilizes stacked bidirectional LSTM networks. Our model is trained with only few hundred labeled sentences and does not rely on further external knowledge. We report from our results F1 scores in the range of 84-94% on standard datasets. version:1
arxiv-1608-06718 | A Large-Scale Multilingual Disambiguation of Glosses | http://arxiv.org/abs/1608.06718 | id:1608.06718 author:José Camacho Collados, Claudio Delli Bovi, Alessandro Raganato, Roberto Navigli category:cs.CL  published:2016-08-24 summary:Linking concepts and named entities to knowledge bases has become a crucial Natural Language Understanding task. In this respect, recent works have shown the key advantage of exploiting textual definitions in various Natural Language Processing applications. However, to date there are no reliable large-scale corpora of sense-annotated textual definitions available to the research community. In this paper we present a large-scale high-quality corpus of disambiguated glosses in multiple languages, comprising sense annotations of both concepts and named entities from a unified sense inventory. Our approach for the construction and disambiguation of the corpus builds upon the structure of a large multilingual semantic network and a state-of-the-art disambiguation system; first, we gather complementary information of equivalent definitions across different languages to provide context for disambiguation, and then we combine it with a semantic similarity-based refinement. As a result we obtain a multilingual corpus of textual definitions featuring over 38 million definitions in 263 languages, and we make it freely available at http://lcl.uniroma1.it/disambiguated-glosses. Experiments on Open Information Extraction and Sense Clustering show how two state-of-the-art approaches improve their performance by integrating our disambiguated corpus into their pipeline. version:1
arxiv-1608-06716 | A Novel Approach for Shot Boundary Detection in Videos | http://arxiv.org/abs/1608.06716 | id:1608.06716 author:D. S. Guru, Mahamad Suhil, P. Lolika category:cs.CV  published:2016-08-24 summary:This paper presents a novel approach for video shot boundary detection. The proposed approach is based on split and merge concept. A fisher linear discriminant criterion is used to guide the process of both splitting and merging. For the purpose of capturing the between class and within class scatter we employ 2D2 FLD method which works on texture feature of regions in each frame of a video. Further to reduce the complexity of the process we propose to employ spectral clustering to group related regions together to a single there by achieving reduction in dimension. The proposed method is experimentally also validated on a cricket video. It is revealed that shots obtained by the proposed approach are highly cohesive and loosely coupled version:1
arxiv-1608-06713 | Transfer Learning for Endoscopic Image Classification | http://arxiv.org/abs/1608.06713 | id:1608.06713 author:Shoji Sonoyama, Toru Tamaki, Tsubasa Hirakawa, Bisser Raytchev, Kazufumi Kaneda, Tetsushi Koide, Shigeto Yoshida, Hiroshi Mieno, Shinji Tanaka category:cs.CV  published:2016-08-24 summary:In this paper we propose a method for transfer learning of endoscopic images. For transferring between features obtained from images taken by different (old and new) endoscopes, we extend the Max-Margin Domain Transfer (MMDT) proposed by Hoffman et al. in order to use L2 distance constraints as regularization, called Max-Margin Domain Transfer with L2 Distance Constraints (MMDTL2). Furthermore, we develop the dual formulation of the optimization problem in order to reduce the computation cost. Experimental results demonstrate that the proposed MMDTL2 outperforms MMDT for real data sets taken by different endoscopes. version:1
arxiv-1608-06709 | Computer-Aided Colorectal Tumor Classification in NBI Endoscopy Using CNN Features | http://arxiv.org/abs/1608.06709 | id:1608.06709 author:Toru Tamaki, Shoji Sonoyama, Tsubasa Hirakawa, Bisser Raytchev, Kazufumi Kaneda, Tetsushi Koide, Shigeto Yoshida, Hiroshi Mieno, Shinji Tanaka category:cs.CV  published:2016-08-24 summary:In this paper we report results for recognizing colorectal NBI endoscopic images by using features extracted from convolutional neural network (CNN). In this comparative study, we extract features from different layers from different CNN models, and then train linear SVM classifiers. Experimental results with 10-fold cross validations show that features from first few convolution layers are enough to achieve similar performance (i.e., recognition rate of 95%) with non-CNN local features such as Bag-of-Visual words, Fisher vector, and VLAD. version:1
arxiv-1608-06697 | Semantic descriptions of 24 evaluational adjectives, for application in sentiment analysis | http://arxiv.org/abs/1608.06697 | id:1608.06697 author:Cliff Goddard, Maite Taboada, Radoslava Trnavac category:cs.CL  published:2016-08-24 summary:We apply the Natural Semantic Metalanguage (NSM) approach (Goddard and Wierzbicka 2014) to the lexical-semantic analysis of English evaluational adjectives and compare the results with the picture developed in the Appraisal Framework (Martin and White 2005). The analysis is corpus-assisted, with examples mainly drawn from film and book reviews, and supported by collocational and statistical information from WordBanks Online. We propose NSM explications for 24 evaluational adjectives, arguing that they fall into five groups, each of which corresponds to a distinct semantic template. The groups can be sketched as follows: "First-person thought-plus-affect", e.g. wonderful; "Experiential", e.g. entertaining; "Experiential with bodily reaction", e.g. gripping; "Lasting impact", e.g. memorable; "Cognitive evaluation", e.g. complex, excellent. These groupings and semantic templates are compared with the classifications in the Appraisal Framework's system of Appreciation. In addition, we are particularly interested in sentiment analysis, the automatic identification of evaluation and subjectivity in text. We discuss the relevance of the two frameworks for sentiment analysis and other language technology applications. version:1
arxiv-1608-06669 | On Clustering and Embedding Manifolds using a Low Rank Neighborhood Approach | http://arxiv.org/abs/1608.06669 | id:1608.06669 author:Arun M. Saranathan, Mario Parente category:cs.CV  published:2016-08-23 summary:In the manifold learning community there has been an onus on the simultaneous clustering and embedding of multiple manifolds. Manifold clustering and embedding algorithms perform especially poorly when embedding highly nonlinear manifolds. In this paper we propose a novel algorithm for improved manifold clustering and embedding. Since a majority of these algorithms are graph based they use different strategies to ensure that only data-point belonging to the same manifold are chosen as neighbors. The new algorithm proposes the addition of a low-rank criterion on the neighborhood of each data-point to ensure that only data-points belonging to the same manifold are "prioritized" for neighbor selection. Following this a reconstruction matrix is calculated to express each data-point as an affine combination of its neighbors. If the low rank neighborhood criterion succeeds in prioritizing data-points belonging to same manifold as neighbors, the reconstruction matrix is (near) block diagonal. This reconstruction matrix can then be used for clustering and embedding. Over a variety of simulated and real data-sets the algorithm shows improvements on the state-of-the-art manifold clustering and embedding algorithms in terms of both clustering and embedding performance. version:1
arxiv-1608-06668 | Computerized Tomography with Total Variation and with Shearlets | http://arxiv.org/abs/1608.06668 | id:1608.06668 author:Edgar Garduño, Gabor T. Herman category:physics.med-ph cs.CV  published:2016-08-23 summary:To reduce the x-ray dose in computerized tomography (CT), many constrained optimization approaches have been proposed aiming at minimizing a regularizing function that measures lack of consistency with some prior knowledge about the object that is being imaged, subject to a (predetermined) level of consistency with the detected attenuation of x-rays. Proponents of the shearlet transform in the regularizing function claim that the reconstructions so obtained are better than those produced using TV for texture preservation (but may be worse for noise reduction). In this paper we report results related to this claim. In our reported experiments using simulated CT data collection of the head, reconstructions whose shearlet transform has a small $\ell_1$-norm are not more efficacious than reconstructions that have a small TV value. Our experiments for making such comparisons use the recently-developed superiorization methodology for both regularizing functions. Superiorization is an automated procedure for turning an iterative algorithm for producing images that satisfy a primary criterion (such as consistency with the observed measurements) into its superiorized version that will produce results that, according to the primary criterion are as good as those produced by the original algorithm, but in addition are superior to them according to a secondary (regularizing) criterion. The method presented for superiorization involving the $\ell_1$-norm of the shearlet transform is novel and is quite general: It can be used for any regularizing function that is defined as the $\ell_1$-norm of a transform specified by the application of a matrix. Because in the previous literature the split Bregman algorithm is used for similar purposes, a section is included comparing the results of the superiorization algorithm with the split Bregman algorithm. version:1
arxiv-1608-06665 | Deep learning is competing random forest in computational docking | http://arxiv.org/abs/1608.06665 | id:1608.06665 author:Mohamed Khamis, Walid Gomaa, Basem Galal category:q-bio.BM cs.LG I.2  published:2016-08-23 summary:Computational docking is the core process of computer-aided drug design; it aims at predicting the best orientation and conformation of a small drug molecule when bound to a target large protein receptor. The docking quality is typically measured by a scoring function: a mathematical predictive model that produces a score representing the binding free energy and hence the stability of the resulting complex molecule. We analyze the performance of both learning techniques on the scoring power, the ranking power, docking power, and screening power using the PDBbind 2013 database. For the scoring and ranking powers, the proposed learning scoring functions depend on a wide range of features (energy terms, pharmacophore, intermolecular) that entirely characterize the protein-ligand complexes. For the docking and screening powers, the proposed learning scoring functions depend on the intermolecular features of the RF-Score to utilize a larger number of training complexes. For the scoring power, the DL\_RF scoring function achieves Pearson's correlation coefficient between the predicted and experimentally measured binding affinities of 0.799 versus 0.758 of the RF scoring function. For the ranking power, the DL scoring function ranks the ligands bound to fixed target protein with accuracy 54% for the high-level ranking and with accuracy 78% for the low-level ranking while the RF scoring function achieves (46% and 62%) respectively. For the docking power, the DL\_RF scoring function has a success rate when the three best-scored ligand binding poses are considered within 2 \AA\ root-mean-square-deviation from the native pose of 36.0% versus 30.2% of the RF scoring function. For the screening power, the DL scoring function has an average enrichment factor and success rate at the top 1% level of (2.69 and 6.45%) respectively versus (1.61 and 4.84%) respectively of the RF scoring function. version:1
arxiv-1608-06664 | Topic Grids for Homogeneous Data Visualization | http://arxiv.org/abs/1608.06664 | id:1608.06664 author:Shih-Chieh Su, Joseph Vaughn, Jean-Laurent Huynh category:cs.LG cs.IR  published:2016-08-23 summary:We propose the topic grids to detect anomaly and analyze the behavior based on the access log content. Content-based behavioral risk is quantified in the high dimensional space where the topics are generated from the log. The topics are being projected homogeneously into a space that is perception- and interaction-friendly to the human experts. version:1
arxiv-1608-06656 | Lexical Query Modeling in Session Search | http://arxiv.org/abs/1608.06656 | id:1608.06656 author:Christophe Van Gysel, Evangelos Kanoulas, Maarten de Rijke category:cs.IR cs.CL  published:2016-08-23 summary:Lexical query modeling has been the leading paradigm for session search. In this paper, we analyze TREC session query logs and compare the performance of different lexical matching approaches for session search. Naive methods based on term frequency weighing perform on par with specialized session models. In addition, we investigate the viability of lexical query models in the setting of session search. We give important insights into the potential and limitations of lexical query modeling for session search and propose future directions for the field of session search. version:1
arxiv-1608-06651 | Unsupervised, Efficient and Semantic Expertise Retrieval | http://arxiv.org/abs/1608.06651 | id:1608.06651 author:Christophe Van Gysel, Maarten de Rijke, Marcel Worring category:cs.IR cs.AI cs.CL cs.LG  published:2016-08-23 summary:We introduce an unsupervised discriminative model for the task of retrieving experts in online document collections. We exclusively employ textual evidence and avoid explicit feature engineering by learning distributed word representations in an unsupervised way. We compare our model to state-of-the-art unsupervised statistical vector space and probabilistic generative approaches. Our proposed log-linear model achieves the retrieval performance levels of state-of-the-art document-centric methods with the low inference cost of so-called profile-centric approaches. It yields a statistically significant improved ranking over vector space and generative models in most cases, matching the performance of supervised methods on various benchmarks. That is, by using solely text we can do as well as methods that work with external evidence and/or relevance feedback. A contrastive analysis of rankings produced by discriminative and generative approaches shows that they have complementary strengths due to the ability of the unsupervised discriminative model to perform semantic matching. version:1
arxiv-1608-03533 | Sequence Graph Transform (SGT): A Feature Extraction Function for Sequence Data Mining | http://arxiv.org/abs/1608.03533 | id:1608.03533 author:Chitta Ranjan, Samaneh Ebrahimi, Kamran Paynabar category:stat.ML cs.LG  published:2016-08-11 summary:A ubiquitous presence of sequence data across fields, like, web, healthcare, bioinformatics, text mining, etc., has made sequence mining a vital research area. However, sequence mining is particularly challenging because of absence of an accurate and fast approach to find (dis)similarity between sequences. As a measure of (dis)similarity, mainstream data mining methods like k-means, kNN, regression, etc., have proved distance between data points in a euclidean space to be most effective. But a distance measure between sequences is not obvious due to their unstructuredness --- arbitrary strings of arbitrary length. We, therefore, propose a new function, called as Sequence Graph Transform (SGT), that extracts sequence features and embeds it in a finite-dimensional euclidean space. It is scalable due to a low computational complexity and has a universal applicability on any sequence problem. We theoretically show that SGT can capture both short and long patterns in sequences, and provides an accurate distance-based measure of (dis)similarity between them. This is also validated experimentally. Finally, we show its real world application for clustering, classification, search and visualization on different sequence problems. version:3
arxiv-1608-06608 | Infinite-Label Learning with Semantic Output Codes | http://arxiv.org/abs/1608.06608 | id:1608.06608 author:Yang Zhang, Rupam Acharyya, Ji Liu, Boqing Gong category:cs.LG  published:2016-08-23 summary:We develop a new statistical machine learning paradigm, named infinite-label learning, to annotate a data point with more than one relevant labels from a candidate set, which pools both the finite labels observed at training and a potentially infinite number of previously unseen labels. The infinite-label learning fundamentally expands the scope of conventional multi-label learning, and better models the practical requirements in various real-world applications, such as image tagging, ads-query association, and article categorization. However, how can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning, where the key is to represent a class/label by a vector of semantic codes, as opposed to treating them as atomic labels. We validate the infinite-label learning by a PAC bound in theory and some empirical studies on both synthetic and real data. version:1
arxiv-1608-06602 | Self-Averaging Expectation Propagation | http://arxiv.org/abs/1608.06602 | id:1608.06602 author:Burak Çakmak, Manfred Opper, Bernard H. Fleury, Ole Winther category:cs.IT cs.LG math.IT  published:2016-08-23 summary:We investigate the problem of approximate Bayesian inference for a general class of observation models by means of the expectation propagation (EP) framework for large systems under some statistical assumptions. Our approach tries to overcome the numerical bottleneck of EP caused by the inversion of large matrices. Assuming that the measurement matrices are realizations of specific types of ensembles we use the concept of freeness from random matrix theory to show that the EP cavity variances exhibit an asymptotic self-averaging property. They can be pre-computed using specific generating functions, i.e. the R- and/or S-transforms in free probability, which do not require matrix inversions. Our approach extends the framework of (generalized) approximate message passing -- assumes zero-mean iid entries of the measurement matrix -- to a general class of random matrix ensembles. The generalization is via a simple formulation of the R- and/or S-transforms of the limiting eigenvalue distribution of the Gramian of the measurement matrix. We demonstrate the performance of our approach on a signal recovery problem of nonlinear compressed sensing and compare it with that of EP. version:1
arxiv-1608-06582 | Approximation and inference methods for stochastic biochemical kinetics - a tutorial review | http://arxiv.org/abs/1608.06582 | id:1608.06582 author:David Schnoerr, Guido Sanguinetti, Ramon Grima category:q-bio.QM cond-mat.stat-mech physics.bio-ph q-bio.MN stat.ML  published:2016-08-23 summary:Stochastic fluctuations of molecule numbers are ubiquitous in biological systems. Important examples include gene expression and enzymatic processes in living cells. Such systems are typically modelled as chemical reaction networks whose dynamics are governed by the Chemical Master Equation. Despite its simple structure, no analytic solutions to the Chemical Master Equation are known for most systems. Moreover, stochastic simulations are computationally expensive, making systematic analysis and statistical inference a challenging task. Consequently, significant effort has been spent in recent decades on the development of efficient approximation and inference methods. This article gives an introduction to basic modelling concepts as well as an overview of state of the art methods. First, we motivate and introduce deterministic and stochastic models for chemical networks, and give an overview of simulation and exact solution methods. Next, we discuss several approximation methods, including the chemical Langevin equation, the system size expansion, moment closure approximations, time-scale separation approximations and hybrid methods. We discuss their various properties and review recent advances and remaining challenges for these methods. We present a comparison of several of these methods by means of a numerical case study and highlight various of their respective advantages and disadvantages. Finally, we discuss the problem of inference from experimental data in the Bayesian framework and review recent methods developed the literature. In summary, this review gives a self-contained introduction to modelling, approximations and inference methods for stochastic chemical kinetics. version:1
arxiv-1608-06581 | Fathom: Reference Workloads for Modern Deep Learning Methods | http://arxiv.org/abs/1608.06581 | id:1608.06581 author:Robert Adolf, Saketh Rama, Brandon Reagen, Gu-Yeon Wei, David Brooks category:cs.LG  published:2016-08-23 summary:Deep learning has been popularized by its recent successes on challenging artificial intelligence problems. One of the reasons for its dominance is also an ongoing challenge: the need for immense amounts of computational power. Hardware architects have responded by proposing a wide array of promising ideas, but to date, the majority of the work has focused on specific algorithms in somewhat narrow application domains. While their specificity does not diminish these approaches, there is a clear need for more flexible solutions. We believe the first step is to examine the characteristics of cutting edge models from across the deep learning community. Consequently, we have assembled Fathom: a collection of eight archetypal deep learning workloads for study. Each of these models comes from a seminal work in the deep learning community, ranging from the familiar deep convolutional neural network of Krizhevsky et al., to the more exotic memory networks from Facebook's AI research group. Fathom has been released online, and this paper focuses on understanding the fundamental performance characteristics of each model. We use a set of application-level modeling tools built around the TensorFlow deep learning framework in order to analyze the behavior of the Fathom workloads. We present a breakdown of where time is spent, the similarities between the performance profiles of our models, an analysis of behavior in inference and training, and the effects of parallelism on scaling. version:1
arxiv-1608-06558 | A Non-Local Conventional Approach for Noise Removal in 3D MRI | http://arxiv.org/abs/1608.06558 | id:1608.06558 author:Sona Morajab, Mehregan Mahdavi category:cs.CV  published:2016-08-23 summary:In this paper, a filtering approach for the 3D magnetic resonance imaging (MRI) assuming a Rician model for noise is addressed. Our denoising method is based on the Conventional Approach (CA) proposed to deal with the noise issue in the squared domain of the acquired magnitude MRI, where the noise distribution follows a Chi-square model rather than the Rician one. In the CA filtering method, the local samples around each voxel is used to estimate the unknown signal value. Intrinsically, such a method fails to achieve the best results where the underlying signal values have different statistical properties. On the contrary, our proposal takes advantage of the data redundancy and self-similarity properties of real MR images to improve the noise removal performance. In other words, in our approach, the statistical momentums of the given 3D MR volume are first calculated to explore the similar patches inside a defined search volume. Then, these patches are put together to obtain the noise-free value for each voxel under processing. The experimental results on the synthetic as well as the clinical MR data show our proposed method outperforms the other compared denoising filters. version:1
arxiv-1608-06557 | Neural Networks with Smooth Adaptive Activation Functions for Regression | http://arxiv.org/abs/1608.06557 | id:1608.06557 author:Le Hou, Dimitris Samaras, Tahsin M. Kurc, Yi Gao, Joel H. Saltz category:cs.CV  published:2016-08-23 summary:In Neural Networks (NN), Adaptive Activation Functions (AAF) have parameters that control the shapes of activation functions. These parameters are trained along with other parameters in the NN. AAFs have improved performance of Neural Networks (NN) in multiple classification tasks. In this paper, we propose and apply AAFs on feedforward NNs for regression tasks. We argue that applying AAFs in the regression (second-to-last) layer of a NN can significantly decrease the bias of the regression NN. However, using existing AAFs may lead to overfitting. To address this problem, we propose a Smooth Adaptive Activation Function (SAAF) with piecewise polynomial form which can approximate any continuous function to arbitrary degree of error. NNs with SAAFs can avoid overfitting by simply regularizing the parameters. In particular, an NN with SAAFs is Lipschitz continuous given a bounded magnitude of the NN parameters. We prove an upper-bound for model complexity in terms of fat-shattering dimension for any Lipschitz continuous regression model. Thus, regularizing the parameters in NNs with SAAFs avoids overfitting. We empirically evaluated NNs with SAAFs and achieved state-of-the-art results on multiple regression datasets. version:1
arxiv-1608-06549 | Using Semantic Similarity for Input Topic Identification in Crawling-based Web Application Testing | http://arxiv.org/abs/1608.06549 | id:1608.06549 author:Jun-Wei Lin, Farn Wang category:cs.SE cs.CL  published:2016-08-23 summary:To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, in existing crawlers, rules for identifying the topics of input text fields, such as login ids, passwords, emails, dates and phone numbers, have to be manually configured. Moreover, the rules for one application are very often not suitable for another. In addition, when several rules conflict and match an input text field to more than one topics, it can be difficult to determine which rule suggests a better match. This paper presents a natural-language approach to automatically identify the topics of encountered input fields during crawling by semantically comparing their similarities with the input fields in labeled corpus. In our evaluation with 100 real-world forms, the proposed approach demonstrated comparable performance to the rule-based one. Our experiments also show that the accuracy of the rule-based approach can be improved by up to 19% when integrated with our approach. version:1
arxiv-1608-06514 | Dynamic Multi-Objectives Optimization with a Changing Number of Objectives | http://arxiv.org/abs/1608.06514 | id:1608.06514 author:Renzhi Chen, Ke Li, Xin Yao category:cs.NE  published:2016-08-23 summary:Existing studies on dynamic multi-objective optimization focus on problems with time-dependent objective functions, while the ones with a changing number of objectives have rarely been considered in the literature. Instead of changing the shape or position of the Pareto-optimal front/set when having time-dependent objective functions, increasing or decreasing the number of objectives usually leads to the expansion or contraction of the dimension of the Pareto-optimal front/set manifold. Unfortunately, most existing dynamic handling techniques can hardly be adapted to this type of dynamics. In this paper, we report our attempt toward tackling the dynamic multi-objective optimization problems with a changing number of objectives. We implement a new two-archive evolutionary algorithm which maintains two co-evolving populations simultaneously. In particular, these two populations are complementary to each other: one concerns more about the convergence while the other concerns more about the diversity. The compositions of these two populations are adaptively reconstructed once the environment changes. In addition, these two populations interact with each other via a mating selection mechanism. Comprehensive experiments are conducted on various benchmark problems with a time-dependent number of objectives. Empirical results fully demonstrate the effectiveness of our proposed algorithm. version:1
arxiv-1608-06495 | Searching Action Proposals via Spatial Actionness Estimation and Temporal Path Inference and Tracking | http://arxiv.org/abs/1608.06495 | id:1608.06495 author:Nannan Li, Dan Xu, Zhenqiang Ying, Zhihao Li, Ge Li category:cs.CV  published:2016-08-23 summary:In this paper, we address the problem of searching action proposals in unconstrained video clips. Our approach starts from actionness estimation on frame-level bounding boxes, and then aggregates the bounding boxes belonging to the same actor across frames via linking, associating, tracking to generate spatial-temporal continuous action paths. To achieve the target, a novel actionness estimation method is firstly proposed by utilizing both human appearance and motion cues. Then, the association of the action paths is formulated as a maximum set coverage problem with the results of actionness estimation as a priori. To further promote the performance, we design an improved optimization objective for the problem and provide a greedy search algorithm to solve it. Finally, a tracking-by-detection scheme is designed to further refine the searched action paths. Extensive experiments on two challenging datasets, UCF-Sports and UCF-101, show that the proposed approach advances state-of-the-art proposal generation performance in terms of both accuracy and proposal quantity. version:1
arxiv-1608-06459 | Tracking Amendments to Legislation and Other Political Texts with a Novel Minimum-Edit-Distance Algorithm: DocuToads | http://arxiv.org/abs/1608.06459 | id:1608.06459 author:Henrik Hermansson, James P. Cross category:cs.CL cs.CY  published:2016-08-23 summary:Political scientists often find themselves tracking amendments to political texts. As different actors weigh in, texts change as they are drafted and redrafted, reflecting political preferences and power. This study provides a novel solution to the prob- lem of detecting amendments to political text based upon minimum edit distances. We demonstrate the usefulness of two language-insensitive, transparent, and efficient minimum-edit-distance algorithms suited for the task. These algorithms are capable of providing an account of the types (insertions, deletions, substitutions, and trans- positions) and substantive amount of amendments made between version of texts. To illustrate the usefulness and efficiency of the approach we replicate two existing stud- ies from the field of legislative studies. Our results demonstrate that minimum edit distance methods can produce superior measures of text amendments to hand-coded efforts in a fraction of the time and resource costs. version:1
arxiv-1608-06451 | Failure Detection for Facial Landmark Detectors | http://arxiv.org/abs/1608.06451 | id:1608.06451 author:Andreas Steger, Radu Timofte, Luc Van Gool category:cs.CV  published:2016-08-23 summary:Most face applications depend heavily on the accuracy of the face and facial landmarks detectors employed. Prediction of attributes such as gender, age, and identity usually completely fail when the faces are badly aligned due to inaccurate facial landmark detection. Despite the impressive recent advances in face and facial landmark detection, little study is on the recovery from and detection of failures or inaccurate predictions. In this work we study two top recent facial landmark detectors and devise confidence models for their outputs. We validate our failure detection approaches on standard benchmarks (AFLW, HELEN) and correctly identify more than 40% of the failures in the outputs of the landmark detectors. Moreover, with our failure detection we can achieve a 12% error reduction on a gender estimation application at the cost of a small increase in computation. version:1
arxiv-1608-06440 | A Delay-Tolerant Potential-Field-Based Network Implementation of an Integrated Navigation System | http://arxiv.org/abs/1608.06440 | id:1608.06440 author:Rachana Ashok Gupta, Ahmad A. Masoud, Mo-Yuen Chow category:cs.RO cs.CV cs.SY  published:2016-08-23 summary:Network controllers (NCs) are devices that are capable of converting dynamic, spatially extended, and functionally specialized modules into a taskable goal-oriented group called networked control system. This paper examines the practical aspects of designing and building an NC that uses the Internet as a communication medium. It focuses on finding compatible controller components that can be integrated via a host structure in a manner that makes it possible to network, in real-time, a webcam, an unmanned ground vehicle (UGV), and a remote computer server along with the necessary operator software interface. The aim is to deskill the UGV navigation process and yet maintain a robust performance. The structure of the suggested controller, its components, and the manner in which they are interfaced are described. Thorough experimental results along with performance assessment and comparisons to a previously implemented NC are provided. version:1
arxiv-1608-06434 | Convolutional Network for Attribute-driven and Identity-preserving Human Face Generation | http://arxiv.org/abs/1608.06434 | id:1608.06434 author:Mu Li, Wangmeng Zuo, David Zhang category:cs.CV  published:2016-08-23 summary:This paper focuses on the problem of generating human face pictures from specific attributes. The existing CNN-based face generation models, however, either ignore the identity of the generated face or fail to preserve the identity of the reference face image. Here we address this problem from the view of optimization, and suggest an optimization model to generate human face with the given attributes while keeping the identity of the reference image. The attributes can be obtained from the attribute-guided image or by tuning the attribute features of the reference image. With the deep convolutional network "VGG-Face", the loss is defined on the convolutional feature maps. We then apply the gradient decent algorithm to solve this optimization problem. The results validate the effectiveness of our method for attribute driven and identity-preserving face generation. version:1
arxiv-1608-06412 | Stability revisited: new generalisation bounds for the Leave-one-Out | http://arxiv.org/abs/1608.06412 | id:1608.06412 author:Alain Celisse, Benjamin Guedj category:stat.ML math.ST stat.TH  published:2016-08-23 summary:The present paper provides a new generic strategy leading to non-asymptotic theoretical guarantees on the Leave-one-Out procedure applied to a broad class of learning algorithms. This strategy relies on two main ingredients: the new notion of $L^q$ stability, and the strong use of moment inequalities. $L^q$ stability extends the ongoing notion of hypothesis stability while remaining weaker than the uniform stability. It leads to new PAC exponential generalisation bounds for Leave-one-Out under mild assumptions. In the literature, such bounds are available only for uniform stable algorithms under boundedness for instance. Our generic strategy is applied to the Ridge regression algorithm as a first step. version:1
arxiv-1608-06409 | Learning to Communicate: Channel Auto-encoders, Domain Specific Regularizers, and Attention | http://arxiv.org/abs/1608.06409 | id:1608.06409 author:Timothy J O'Shea, Kiran Karra, T. Charles Clancy category:cs.LG cs.IT cs.NI math.IT  published:2016-08-23 summary:We address the problem of learning efficient and adaptive ways to communicate binary information over an impaired channel. We treat the problem as reconstruction optimization through impairment layers in a channel autoencoder and introduce several new domain-specific regularizing layers to emulate common channel impairments. We also apply a radio transformer network based attention model on the input of the decoder to help recover canonical signal representations. We demonstrate some promising initial capacity results from this architecture and address several remaining challenges before such a system could become practical. version:1
arxiv-1608-06408 | Online Learning to Rank with Top-k Feedback | http://arxiv.org/abs/1608.06408 | id:1608.06408 author:Sougata Chaudhuri, Ambuj Tewari category:cs.LG  published:2016-08-23 summary:We consider two settings of online learning to rank where feedback is restricted to top ranked items. The problem is cast as an online game between a learner and sequence of users, over $T$ rounds. In both settings, the learners objective is to present ranked list of items to the users. The learner's performance is judged on the entire ranked list and true relevances of the items. However, the learner receives highly restricted feedback at end of each round, in form of relevances of only the top $k$ ranked items, where $k \ll m$. The first setting is \emph{non-contextual}, where the list of items to be ranked is fixed. The second setting is \emph{contextual}, where lists of items vary, in form of traditional query-document lists. No stochastic assumption is made on the generation process of relevances of items and contexts. We provide efficient ranking strategies for both the settings. The strategies achieve $O(T^{2/3})$ regret, where regret is based on popular ranking measures in first setting and ranking surrogates in second setting. We also provide impossibility results for certain ranking measures and a certain class of surrogates, when feedback is restricted to the top ranked item, i.e. $k=1$. We empirically demonstrate the performance of our algorithms on simulated and real world datasets. version:1
arxiv-1608-06627 | Artificial Neural Networks for Detection of Malaria in RBCs | http://arxiv.org/abs/1608.06627 | id:1608.06627 author:Purnima Pandit, A. Anand category:physics.med-ph cs.CV cs.NE 62M45  published:2016-08-23 summary:Malaria is one of the most common diseases caused by mosquitoes and is a great public health problem worldwide. Currently, for malaria diagnosis the standard technique is microscopic examination of a stained blood film. We propose use of Artificial Neural Networks (ANN) for the diagnosis of the disease in the red blood cell. For this purpose features / parameters are computed from the data obtained by the digital holographic images of the blood cells and is given as input to ANN which classifies the cell as the infected one or otherwise. version:1
arxiv-1608-06386 | Which techniques does your application use?: An information extraction framework for scientific articles | http://arxiv.org/abs/1608.06386 | id:1608.06386 author:Soham Dan, Sanyam Agarwal, Mayank Singh, Pawan Goyal, Animesh Mukherjee category:cs.CL  published:2016-08-23 summary:Every field of research consists of multiple application areas with various techniques routinely used to solve problems in these wide range of application areas. With the exponential growth in research volumes, it has become difficult to keep track of the ever-growing number of application areas as well as the corresponding problem solving techniques. In this paper, we consider the computational linguistics domain and present a novel information extraction system that automatically constructs a pool of all application areas in this domain and appropriately links them with corresponding problem solving techniques. Further, we categorize individual research articles based on their application area and the techniques proposed/used in the article. k-gram based discounting method along with handwritten rules and bootstrapped pattern learning is employed to extract application areas. Subsequently, a language modeling approach is proposed to characterize each article based on its application area. Similarly, regular expressions and high-scoring noun phrases are used for the extraction of the problem solving techniques. We propose a greedy approach to characterize each article based on the techniques. Towards the end, we present a table representing the most frequent techniques adopted for a particular application area. Finally, we propose three use cases presenting an extensive temporal analysis of the usage of techniques and application areas. version:1
arxiv-1608-06383 | Softplus Regressions and Convex Polytopes | http://arxiv.org/abs/1608.06383 | id:1608.06383 author:Mingyuan Zhou category:stat.ML stat.ME  published:2016-08-23 summary:To construct flexible nonlinear predictive distributions, the paper introduces a family of softplus function based regression models that convolve, stack, or combine both operations by convolving countably infinite stacked gamma distributions, whose scales depend on the covariates. Generalizing logistic regression that uses a single hyperplane to partition the covariate space into two halves, softplus regressions employ multiple hyperplanes to construct a confined space, related to a single convex polytope defined by the intersection of multiple half-spaces or a union of multiple convex polytopes, to separate one class from the other. The gamma process is introduced to support the convolution of countably infinite (stacked) covariate-dependent gamma distributions. For Bayesian inference, Gibbs sampling derived via novel data augmentation and marginalization techniques is used to deconvolve and/or demix the highly complex nonlinear predictive distribution. Example results demonstrate that softplus regressions provide flexible nonlinear decision boundaries, achieving classification accuracies comparable to that of kernel support vector machine while requiring significant less computation for out-of-sample prediction. version:1
arxiv-1608-06378 | Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine | http://arxiv.org/abs/1608.06378 | id:1608.06378 author:Bo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, Lin-Shan Lee category:cs.CL  published:2016-08-23 summary:Multimedia or spoken content presents more attractive information than plain text content, but it's more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It's highly attractive to develop a machine which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, we propose a new task of machine comprehension of spoken content. We define the initial goal as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native language is not English. We further propose an Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture for this task, achieving encouraging results in the initial tests. Initial results also have shown that word-level attention is probably more robust than sentence-level attention for this task with ASR errors. version:1
arxiv-1608-06374 | Deep Double Sparsity Encoder: Learning to Sparsify Not Only Features But Also Parameters | http://arxiv.org/abs/1608.06374 | id:1608.06374 author:Zhangyang Wang, Liang Zhang, Yingzhen Yang, Jiayu Zhou, Georgios B. Giannakis, Thomas S. Huang category:cs.LG cs.CV  published:2016-08-23 summary:This paper emphasizes the significance to jointly exploit the problem structure and the parameter structure, in the context of deep modeling. As a specific and interesting example, we describe the deep double sparsity encoder (DDSE), which is inspired by the double sparsity model for dictionary learning. DDSE simultaneously sparsities the output features and the learned model parameters, under one unified framework. In addition to its intuitive model interpretation, DDSE also possesses compact model size and low complexity. Extensive simulations compare DDSE with several carefully-designed baselines, and verify the consistently superior performance of DDSE. We further apply DDSE to the novel application domain of brain encoding, with promising preliminary results achieved. version:1
arxiv-1608-05477 | A Recurrent Encoder-Decoder Network for Sequential Face Alignment | http://arxiv.org/abs/1608.05477 | id:1608.05477 author:Xi Peng, Rogerio S. Feris, Xiaoyu Wang, Dimitris N. Metaxas category:cs.CV  published:2016-08-19 summary:We propose a novel recurrent encoder-decoder network model for real-time video-based face alignment. Our proposed model predicts 2D facial point maps regularized by a regression loss, while uniquely exploiting recurrent learning at both spatial and temporal dimensions. At the spatial level, we add a feedback loop connection between the combined output response map and the input, in order to enable iterative coarse-to-fine face alignment using a single network model. At the temporal level, we first decouple the features in the bottleneck of the network into temporal-variant factors, such as pose and expression, and temporal-invariant factors, such as identity information. Temporal recurrent learning is then applied to the decoupled temporal-variant features, yielding better generalization and significantly more accurate results at test time. We perform a comprehensive experimental analysis, showing the importance of each component of our proposed model, as well as superior results over the state-of-the-art in standard datasets. version:2
arxiv-1608-06338 | Large-scale Continuous Gesture Recognition Using Convolutional Neutral Networks | http://arxiv.org/abs/1608.06338 | id:1608.06338 author:Pichao Wang, Wanqing Li, Song Liu, Yuyao Zhang, Zhimin Gao, Philip Ogunbona category:cs.CV  published:2016-08-22 summary:This paper addresses the problem of continuous gesture recognition from sequences of depth maps using convolutional neutral networks (ConvNets). The proposed method first segments individual gestures from a depth sequence based on quantity of movement (QOM). For each segmented gesture, an Improved Depth Motion Map (IDMM), which converts the depth sequence into one image, is constructed and fed to a ConvNet for recognition. The IDMM effectively encodes both spatial and temporal information and allows the fine-tuning with existing ConvNet models for classification without introducing millions of parameters to learn. The proposed method is evaluated on the Large-scale Continuous Gesture Recognition of the ChaLearn Looking at People (LAP) challenge 2016. It achieved the performance of 0.2655 (Mean Jaccard Index) and ranked $3^{rd}$ place in this challenge. version:1
arxiv-1608-05143 | A Systematic Approach for Cross-source Point Cloud Registration by Preserving Macro and Micro Structures | http://arxiv.org/abs/1608.05143 | id:1608.05143 author:Xiaoshui Huang, Jian Zhang, Lixin Fan, Qiang Wu, Chun Yuan category:cs.CV  published:2016-08-18 summary:We propose a systematic approach for registering cross-source point clouds. The compelling need for cross-source point cloud registration is motivated by the rapid development of a variety of 3D sensing techniques, but many existing registration methods face critical challenges as a result of the large variations in cross-source point clouds. This paper therefore illustrates a novel registration method which successfully aligns two cross-source point clouds in the presence of significant missing data, large variations in point density, scale difference and so on. The robustness of the method is attributed to the extraction of macro and micro structures. Our work has three main contributions: (1) a systematic pipeline to deal with cross-source point cloud registration; (2) a graph construction method to maintain macro and micro structures; (3) a new graph matching method is proposed which considers the global geometric constraint to robustly register these variable graphs. Compared to most of the related methods, the experiments show that the proposed method successfully registers in cross-source datasets, while other methods have difficulty achieving satisfactory results. The proposed method also shows great ability in same-source datasets. version:2
arxiv-1608-06315 | LFADS - Latent Factor Analysis via Dynamical Systems | http://arxiv.org/abs/1608.06315 | id:1608.06315 author:David Sussillo, Rafal Jozefowicz, L. F. Abbott, Chethan Pandarinath category:cs.LG q-bio.NC stat.ML  published:2016-08-22 summary:Neuroscience is experiencing a data revolution in which many hundreds or thousands of neurons are recorded simultaneously. Currently, there is little consensus on how such data should be analyzed. Here we introduce LFADS (Latent Factor Analysis via Dynamical Systems), a method to infer latent dynamics from simultaneously recorded, single-trial, high-dimensional neural spiking data. LFADS is a sequential model based on a variational auto-encoder. By making a dynamical systems hypothesis regarding the generation of the observed data, LFADS reduces observed spiking to a set of low-dimensional temporal factors, per-trial initial conditions, and inferred inputs. We compare LFADS to existing methods on synthetic data and show that it significantly out-performs them in inferring neural firing rates and latent dynamics. version:1
arxiv-0707-1913 | Removing Manually-Generated Boilerplate from Electronic Texts: Experiments with Project Gutenberg e-Books | http://arxiv.org/abs/0707.1913 | id:0707.1913 author:Owen Kaser, Daniel Lemire category:cs.DL cs.CL  published:2007-07-13 summary:Collaborative work on unstructured or semi-structured documents, such as in literature corpora or source code, often involves agreed upon templates containing metadata. These templates are not consistent across users and over time. Rule-based parsing of these templates is expensive to maintain and tends to fail as new documents are added. Statistical techniques based on frequent occurrences have the potential to identify automatically a large fraction of the templates, thus reducing the burden on the programmers. We investigate the case of the Project Gutenberg corpus, where most documents are in ASCII format with preambles and epilogues that are often copied and pasted or manually typed. We show that a statistical approach can solve most cases though some documents require knowledge of English. We also survey various technical solutions that make our approach applicable to large data sets. version:3
arxiv-1608-06296 | Neural networks for the prediction organic chemistry reactions | http://arxiv.org/abs/1608.06296 | id:1608.06296 author:Jennifer N. Wei, David Duvenaud, Alán Aspuru-Guzik category:physics.chem-ph q-bio.QM stat.ML  published:2016-08-22 summary:Reaction prediction remains one of the great challenges for organic chemistry. Solving this problem computationally requires the programming of a vast amount of knowledge and intuition of the rules of organic chemistry and the development of algorithms for their application. It is desirable to develop algorithms that, like humans, "learn" from being exposed to examples of the application of the rules of organic chemistry. In this work, we introduce a novel algorithm for predicting the products of organic chemistry reactions using machine learning to first identify the reaction type. In particular, we trained deep convolutional neural networks to predict the outcome of reactions based example reactions, using a new reaction fingerprint model. Due to the flexibility of neural networks, the system can attempt to predict reactions outside the domain where it was trained. We test this capability on problems from a popular organic chemistry textbook. version:1
arxiv-1608-06253 | Multi-Dueling Bandits and Their Application to Online Ranker Evaluation | http://arxiv.org/abs/1608.06253 | id:1608.06253 author:Brian Brost, Yevgeny Seldin, Ingemar J. Cox, Christina Lioma category:cs.IR cs.LG stat.ML  published:2016-08-22 summary:New ranking algorithms are continually being developed and refined, necessitating the development of efficient methods for evaluating these rankers. Online ranker evaluation focuses on the challenge of efficiently determining, from implicit user feedback, which ranker out of a finite set of rankers is the best. Online ranker evaluation can be modeled by dueling ban- dits, a mathematical model for online learning under limited feedback from pairwise comparisons. Comparisons of pairs of rankers is performed by interleaving their result sets and examining which documents users click on. The dueling bandits model addresses the key issue of which pair of rankers to compare at each iteration, thereby providing a solution to the exploration-exploitation trade-off. Recently, methods for simultaneously comparing more than two rankers have been developed. However, the question of which rankers to compare at each iteration was left open. We address this question by proposing a generalization of the dueling bandits model that uses simultaneous comparisons of an unrestricted number of rankers. We evaluate our algorithm on synthetic data and several standard large-scale online ranker evaluation datasets. Our experimental results show that the algorithm yields orders of magnitude improvement in performance compared to stateof- the-art dueling bandit algorithms. version:1
arxiv-1608-06238 | Single-shot Adaptive Measurement for Quantum-enhanced Metrology | http://arxiv.org/abs/1608.06238 | id:1608.06238 author:Pantita Palittapongarnpim, Peter Wittek, Barry C. Sanders category:quant-ph stat.ML  published:2016-08-22 summary:Quantum-enhanced metrology aims to estimate an unknown parameter such that the precision scales better than the shot-noise bound. Single-shot adaptive quantum-enhanced metrology (AQEM) is a promising approach that uses feedback to tweak the quantum process according to previous measurement outcomes. Techniques and formalism for the adaptive case are quite different from the usual non-adaptive quantum metrology approach due to the causal relationship between measurements and outcomes. We construct a formal framework for AQEM by modeling the procedure as a decision-making process, and we derive the imprecision and the Cram\'{e}r-Rao lower bound with explicit dependence on the feedback policy. We also explain the reinforcement learning approach for generating quantum control policies, which is adopted due to the optimal policy being non-trivial to devise. Applying a learning algorithm based on differential evolution enables us to attain imprecision for adaptive interferometric phase estimation, which turns out to be SQL when non-entangled particles are used in the scheme. version:1
arxiv-1608-06235 | Adaptive Probabilistic Trajectory Optimization via Efficient Approximate Inference | http://arxiv.org/abs/1608.06235 | id:1608.06235 author:Yunpeng Pan, Xinyan Yan, Evangelos Theodorou, Byron Boots category:cs.RO cs.LG  published:2016-08-22 summary:Robotic systems must be able to quickly and robustly make decisions when operating in uncertain and dynamic environments. While Reinforcement Learning (RL) can be used to compute optimal policies with little prior knowledge about the environment, it suffers from slow convergence. An alternative approach is Model Predictive Control (MPC), which optimizes policies quickly, but also requires accurate models of the system dynamics and environment. In this paper we propose a new approach, adaptive probabilistic trajectory optimization, that combines the benefits of RL and MPC. Our method uses scalable approximate inference to learn and updates probabilistic models in an online incremental fashion while also computing optimal control policies via successive local approximations. We present two variations of our algorithm based on the Sparse Spectrum Gaussian Process (SSGP) model, and we test our algorithm on three learning tasks, demonstrating the effectiveness and efficiency of our approach. version:1
arxiv-1608-05581 | Unsupervised Feature Selection Based on the Morisita Estimator of Intrinsic Dimension | http://arxiv.org/abs/1608.05581 | id:1608.05581 author:Jean Golay, Mikhail Kanevski category:stat.ML cs.LG  published:2016-08-19 summary:This paper deals with a new filter algorithm for selecting the smallest subset of features carrying all the information content of a data set (i.e. for removing redundant features). It is a new version of the fractal dimension reduction algorithm following a sequential forward search strategy and it relies on the recently introduced Morisita estimator of Intrinsic Dimension (ID). Here, the ID is used to quantify dependencies between subsets of features, which allows the effective processing of highly non-linear data. The proposed algorithm is successfully tested on simulated and real world case studies. Different levels of sample size and noise are examined along with the variability of the results. In addition, a comprehensive procedure based on random forests shows that the data dimensionality is significantly reduced by the algorithm without information loss. version:2
arxiv-1608-06203 | Computational and Statistical Tradeoffs in Learning to Rank | http://arxiv.org/abs/1608.06203 | id:1608.06203 author:Ashish Khetan, Sewoong Oh category:cs.LG cs.IT math.IT stat.ML  published:2016-08-22 summary:For massive and heterogeneous modern datasets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited. In the application of learning to rank, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data. This allows the number of data points collected to be gracefully traded off against computational resources available, while guaranteeing the desired level of accuracy. Theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data. version:1
arxiv-1608-06197 | CrowdNet: A Deep Convolutional Network for Dense Crowd Counting | http://arxiv.org/abs/1608.06197 | id:1608.06197 author:Lokesh Boominathan, Srinivas S S Kruthiventi, R. Venkatesh Babu category:cs.CV  published:2016-08-22 summary:Our work proposes a novel deep learning framework for estimating crowd density from static images of highly dense crowds. We use a combination of deep and shallow, fully convolutional networks to predict the density map for a given crowd image. Such a combination is used for effectively capturing both the high-level semantic information (face/body detectors) and the low-level features (blob detectors), that are necessary for crowd counting under large scale variations. As most crowd datasets have limited training samples (<100 images) and deep learning based approaches require large amounts of training data, we perform multi-scale data augmentation. Augmenting the training samples in such a manner helps in guiding the CNN to learn scale invariant representations. Our method is tested on the challenging UCF_CC_50 dataset, and shown to outperform the state of the art methods. version:1
arxiv-1608-06192 | Efficient Continuous Relaxations for Dense CRF | http://arxiv.org/abs/1608.06192 | id:1608.06192 author:Alban Desmaison, Rudy Bunel, Pushmeet Kohli, Philip H. S. Torr, M. Pawan Kumar category:cs.CV  published:2016-08-22 summary:Dense conditional random fields (CRF) with Gaussian pairwise potentials have emerged as a popular framework for several computer vision applications such as stereo correspondence and semantic segmentation. By modeling long-range interactions, dense CRFs provide a more detailed labelling compared to their sparse counterparts. Variational inference in these dense models is performed using a filtering-based mean-field algorithm in order to obtain a fully-factorized distribution minimising the Kullback-Leibler divergence to the true distribution. In contrast to the continuous relaxation-based energy minimisation algorithms used for sparse CRFs, the mean-field algorithm fails to provide strong theoretical guarantees on the quality of its solutions. To address this deficiency, we show that it is possible to use the same filtering approach to speed-up the optimisation of several continuous relaxations. Specifically, we solve a convex quadratic programming (QP) relaxation using the efficient Frank-Wolfe algorithm. This also allows us to solve difference-of-convex relaxations via the iterative concave-convex procedure where each iteration requires solving a convex QP. Finally, we develop a novel divide-and-conquer method to compute the subgradients of a linear programming relaxation that provides the best theoretical bounds for energy minimisation. We demonstrate the advantage of continuous relaxations over the widely used mean-field algorithm on publicly available datasets. version:1
arxiv-1608-06154 | Multi-Sensor Prognostics using an Unsupervised Health Index based on LSTM Encoder-Decoder | http://arxiv.org/abs/1608.06154 | id:1608.06154 author:Pankaj Malhotra, Vishnu TV, Anusha Ramakrishnan, Gaurangi Anand, Lovekesh Vig, Puneet Agarwal, Gautam Shroff category:cs.LG cs.AI  published:2016-08-22 summary:Many approaches for estimation of Remaining Useful Life (RUL) of a machine, using its operational sensor data, make assumptions about how a system degrades or a fault evolves, e.g., exponential degradation. However, in many domains degradation may not follow a pattern. We propose a Long Short Term Memory based Encoder-Decoder (LSTM-ED) scheme to obtain an unsupervised health index (HI) for a system using multi-sensor time-series data. LSTM-ED is trained to reconstruct the time-series corresponding to healthy state of a system. The reconstruction error is used to compute HI which is then used for RUL estimation. We evaluate our approach on publicly available Turbofan Engine and Milling Machine datasets. We also present results on a real-world industry dataset from a pulverizer mill where we find significant correlation between LSTM-ED based HI and maintenance costs. version:1
arxiv-1608-06134 | Median-Based Generation of Synthetic Speech Durations using a Non-Parametric Approach | http://arxiv.org/abs/1608.06134 | id:1608.06134 author:Srikanth Ronanki, Oliver Watts, Simon King, Gustav Eje Henter category:cs.CL  published:2016-08-22 summary:This paper proposes a new approach to duration modelling for statistical parametric speech synthesis in which a recurrent statistical model is trained to output a phone transition probability at each timestep (acoustic frame). Unlike conventional approaches to duration modelling -- which assume that duration distributions have a particular form (e.g., a Gaussian) and use the mean of that distribution for synthesis -- our approach can in principle model any distribution supported on the non-negative integers. Generation from this model can be performed in many ways; here we consider output generation based on the median predicted duration. The median is more typical (more probable) than the conventional mean duration, is robust to training-data irregularities, and enables incremental generation. Furthermore, a frame-level approach to duration prediction is consistent with a longer-term goal of modelling durations and acoustic features together. Results indicate that the proposed method is competitive with baseline approaches in approximating the median duration of held-out natural speech. version:1
arxiv-1608-06132 | Reconstructing Neural Parameters and Synapses of arbitrary interconnected Neurons from their Simulated Spiking Activity | http://arxiv.org/abs/1608.06132 | id:1608.06132 author:J. Fischer, P. Manoonpong, S. Lackner category:cs.NE q-bio.NC  published:2016-08-22 summary:To understand the behavior of a neural circuit it is a presupposition that we have a model of the dynamical system describing this circuit. This model is determined by several parameters, including not only the synaptic weights, but also the parameters of each neuron. Existing works mainly concentrate on either the synaptic weights or the neural parameters. In this paper we present an algorithm to reconstruct all parameters including the synaptic weights of a spiking neuron model. The model based on works of Eugene M. Izhikevich (Izhikevich 2007) consists of two differential equations and covers different types of cortical neurons. It combines the dynamical properties of Hodgkin-Huxley-type dynamics with a high computational efficiency. The presented algorithm uses the recordings of the corresponding membrane potentials of the model for the reconstruction and consists of two main components. The first component is a rank based Genetic Algorithm (GA) which is used to find the neural parameters of the model. The second one is a Least Mean Squares approach which computes the synaptic weights of all interconnected neurons by minimizing the squared error between the calculated and the measured membrane potentials for each time step. In preparation for the reconstruction of the neural parameters and of the synaptic weights from real measured membrane potentials, promising results based on simulated data generated with a randomly parametrized Izhikevich model are presented. The reconstruction does not only converge to a global minimum of neural parameters, but also approximates the synaptic weights with high precision. version:1
arxiv-1608-06072 | Uniform Generalization, Concentration, and Adaptive Learning | http://arxiv.org/abs/1608.06072 | id:1608.06072 author:Ibrahim Alabdulmohsin category:cs.LG cs.IT math.IT stat.ML 68T05  94A15 I.2.6  published:2016-08-22 summary:One of the fundamental goals in any learning algorithm is to minimize its risk for overfitting. Mathematically, this implies that the learning algorithm enjoys a small generalization risk, which is defined either in expectation or in probability. Both types of generalization are commonly used in the literature. For instance, generalization in expectation has been used to analyze algorithms, such as ridge regression and SGD, whereas generalization in probability is used in the VC theory and the PAC-Bayesian framework, among others. Recently, however, a third notion of generalization has been studied, called uniform generalization, which requires that the generalization risk vanishes uniformly in expectation across all bounded parametric losses. It has been shown that uniform generalization is, in fact, equivalent to an algorithmic stability constraint, and that it recovers classical results in learning theory. However, the relationship between uniform generalization and concentration remained unknown. In this paper, we answer this question by proving that, while a generalization in expectation does not imply a generalization in probability, a uniform generalization in expectation does imply concentration. We establish a chain rule for uniform generalization and use it to derive a tight deviation bound. The chain rule also reveals that learning algorithms, which satisfy uniform generalization, are amenable to adaptive composition; thus improving upon earlier results that proposed stronger conditions, such as differential privacy, sample compression, or typical stability. version:1
arxiv-1608-06049 | Local Binary Convolutional Neural Networks | http://arxiv.org/abs/1608.06049 | id:1608.06049 author:Felix Juefei-Xu, Vishnu Naresh Boddeti, Marios Savvides category:cs.LG cs.CV  published:2016-08-22 summary:We propose local binary convolution (LBC), an efficient alternative to convolutional layers in standard convolutional neural networks (CNN). The design principles of LBC are motivated by local binary patterns (LBP). The LBC layer comprises of a set of fixed sparse pre-defined binary convolutional filters that are not updated during the training process, a non-linear activation function and a set of learnable linear weights. The linear weights combine the activated filter responses to approximate the corresponding activated filter responses of a standard convolutional layer. The LBC layer affords significant parameter savings, 9x to 169x in the number of learnable parameters compared to a standard convolutional layer. Furthermore, due to lower model complexity and sparse and binary nature of the weights also results in up to 9x to 169x savings in model size compared to a standard convolutional layer. We demonstrate both theoretically and experimentally that our local binary convolution layer is a good approximation of a standard convolutional layer. Empirically, CNNs with LBC layers, called local binary convolutional neural networks (LBCNN), reach state-of-the-art performance on a range of visual datasets (MNIST, SVHN, CIFAR-10, and a subset of ImageNet) while enjoying significant computational savings. version:1
arxiv-1608-06048 | Survey of resampling techniques for improving classification performance in unbalanced datasets | http://arxiv.org/abs/1608.06048 | id:1608.06048 author:Ajinkya More category:stat.AP cs.LG stat.ML  published:2016-08-22 summary:A number of classification problems need to deal with data imbalance between classes. Often it is desired to have a high recall on the minority class while maintaining a high precision on the majority class. In this paper, we review a number of resampling techniques proposed in literature to handle unbalanced datasets and study their effect on classification performance. version:1
arxiv-1608-06043 | Context Gates for Neural Machine Translation | http://arxiv.org/abs/1608.06043 | id:1608.06043 author:Zhaopeng Tu, Yang Liu, Zhengdong Lu, Xiaohua Liu, Hang Li category:cs.CL  published:2016-08-22 summary:In neural machine translation (NMT), generation of a target word depends on both source and target contexts. We find that source contexts have a direct impact on the adequacy of a translation while target contexts on the fluency. Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. Due to lack of effective control on the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. To address this problem, we propose to use context gates to dynamically control the ratios at which source and target contexts contribute to the generation of target words. In this way, we can enhance the adequacy of NMT while keeping the fluency unchanged. Experiments show that our approach significantly improves upon a standard attention-based NMT system by +2.3 BLEU points. version:1
arxiv-1608-06037 | Lets keep it simple: using simple architectures to outperform deeper architectures | http://arxiv.org/abs/1608.06037 | id:1608.06037 author:Seyyed Hossein HasanPour, Mohammad Rouhani, Javad Vahidi, Reza Saadati category:cs.CV cs.NE  published:2016-08-22 summary:There have been many new methods and tricks to make creating an architecture, more robust and perform better in the last several years. While these methods are proved to indeed contribute to better performance, they impose some restrictions as well, such as memory consumption that makes their application in deeper networks not always possible[1]. Having this in mind, in this work, we propose a simple architecture for convolutional neural network that not only achieves better performance and accuracy and imposes less computational cost than the latest well-known deeper architectures such as VGGNet, ResNet, in the several recognition benchmark datasets but can be trained more easily as well. We also introduce a new trick we call leap-out which along with the new architecture, let us achieve state of the art and very close to the state of the art it in benchmarking datasets such as Cifar10, cifar100 and mnist. version:1
arxiv-1608-06031 | Towards Instance Optimal Bounds for Best Arm Identification | http://arxiv.org/abs/1608.06031 | id:1608.06031 author:Lijie Chen, Jian Li, Mingda Qiao category:cs.LG cs.DS stat.ML  published:2016-08-22 summary:In the best arm identification (Best-1-Arm) problem, we are given $n$ stochastic bandit arms, each associated with a reward distribution with an unknown mean. We would like to identify the arm with the largest mean with probability $1-\delta$, using as few samples as possible. Understanding the sample complexity of Best-1-Arm has attracted significant attention since the last decade. However, the optimal sample complexity is still unknown. Recently, Chen and Li made an interesting conjecture, called gap-entropy conjecture, concerning the instance optimal sample complexity of Best-1-Arm. Given a Best-1-Arm instance, let $\mu_{[i]}$ denote the $i$th largest mean and $\Delta_{[i]}=\mu_{[1]}-\mu_{[i]}$ denote the corresponding gap. $H(I)=\sum_{i=2}^{n}\Delta_{[i]}^{-2}$ denotes the complexity of the instance. The gap-entropy conjecture states that for any instance $I$, $\Omega(H(I)\cdot(\log\delta^{-1}+\mathsf{Ent}(I)))$ is an instance lower bound, where $\mathsf{Ent}(I)$ is an entropy-like term completely determined by $\Delta_{[i]}$s, and there is a $\delta$-correct algorithm for Best-1-Arm with sample complexity $O(H(I)\cdot(\log\delta^{-1}+\mathsf{Ent}(I))+\Delta_{[2]}^{-2}\log\log\Delta_{[2]}^{-1})$. If the conjecture is true, we would have a complete understanding of the instance-wise sample complexity of Best-1-Arm. We make significant progress towards the resolution of the gap-entropy conjecture. For the upper bound, we provide a highly nontrivial $\delta$-correct algorithm which requires $$O(H(I)\cdot(\log\delta^{-1}+\mathsf{Ent}(I))+\Delta_{[2]}^{-2}\log\log \Delta_{[2]}^{-1}\operatorname*{polylog}(n,\delta^{-1}))$$ samples. For the lower bound, we show that for any Best-1-Arm instance with all gaps of the form $2^{-k}$, any $\delta$-correct monotone algorithm requires at least $\Omega(H(I)\cdot(\log\delta^{-1}+\mathsf{Ent}(I)))$ samples in expectation. version:1
arxiv-1608-06019 | Domain Separation Networks | http://arxiv.org/abs/1608.06019 | id:1608.06019 author:Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, Dumitru Erhan category:cs.CV  published:2016-08-22 summary:The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We suggest that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained not only to perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process. version:1
arxiv-1608-06007 | Distributed Probabilistic Bisection Search using Social Learning | http://arxiv.org/abs/1608.06007 | id:1608.06007 author:Athanasios Tsiligkaridis, Theodoros Tsiligkaridis category:cs.SI cs.LG cs.MA  published:2016-08-21 summary:We present a novel distributed probabilistic bisection algorithm using social learning with application to target localization. Each agent in the network first constructs a query about the target based on its local information and obtains a noisy response. Agents then perform a Bayesian update of their beliefs followed by a local averaging of the log beliefs. This two stage algorithm consisting of repeated querying and averaging runs until convergence. We derive bounds on the rate of convergence of the beliefs at the correct target location. Numerical simulations show that our method outperforms current state of the art methods. version:1
arxiv-1608-05995 | A Non-convex One-Pass Framework for Generalized Factorization Machines and Rank-One Matrix Sensing | http://arxiv.org/abs/1608.05995 | id:1608.05995 author:Ming Lin, Jieping Ye category:stat.ML cs.LG  published:2016-08-21 summary:We develop an efficient alternating framework for learning Factorization Machine (FM) on steaming data with provable guarantees. When the feature is $d$-dimension and the target second order coefficient matrix in FM is of rank $k$, our algorithm converges linearly, achieves $O(\epsilon)$ recovery error after retrieving $O(k^{3}d\log(1/\epsilon))$ training instances, consumes $O(kd)$ memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition. As special cases of FM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval. version:1
arxiv-1608-05983 | Deep Generative Models for Spectroscopic Analysis on Mars | http://arxiv.org/abs/1608.05983 | id:1608.05983 author:Ian Gemp, Ishan Durugkar, Mario Parente, M. Darby Dyar, Sridhar Mahadevan category:cs.LG stat.ML  published:2016-08-21 summary:Hyperspectral instruments (HSIs) measure the electromagnetic energy emitted by materials at high resolution (hundreds to thousands of channels) enabling material identification through spectroscopic analysis. Laser-induced breakdown spectroscopy (LIBS) is used by the ChemCam instrument on the Curiosity rover to measure the emission spectra of surface materials on Mars. From orbit, hyperspectral instruments (HSIs) on the CRISM instrument of the Mars Reconnaissance Orbiter (MRO) measure the electromagnetic energy emitted by materials at high resolution (hundreds to thousands of channels) enabling material identification through spectroscopic analysis. The data received are noisy, high-dimensional, and largely unlabeled. The ability to accurately predict elemental and material compositions of surface samples as well as to simulate spectra from hypothetical compositions, collectively known as hyperspectral unmixing, is invaluable to the exploration process. The nature of the problem allows us to construct deep (semi-supervised) generative models to accomplish both these tasks while making use of a large unlabeled dataset. Our main technical contribution is an invertibility trick where we train our model in reverse. version:1
arxiv-1608-05949 | Distributed Representations for Biological Sequence Analysis | http://arxiv.org/abs/1608.05949 | id:1608.05949 author:Dhananjay Kimothi, Akshay Soni, Pravesh Biyani, James M. Hogan category:cs.LG q-bio.QM  published:2016-08-21 summary:Biological sequence comparison is a key step in inferring the relatedness of various organisms and the functional similarity of their components. Thanks to the Next Generation Sequencing efforts, an abundance of sequence data is now available to be processed for a range of bioinformatics applications. Embedding a biological sequence over a nucleotide or amino acid alphabet in a lower dimensional vector space makes the data more amenable for use by current machine learning tools, provided the quality of embedding is high and it captures the most meaningful information of the original sequences. Motivated by recent advances in the text document embedding literature, we present a new method, called seq2vec, to represent a complete biological sequence in an Euclidean space. The new representation has the potential to capture the contextual information of the original sequence necessary for sequence comparison tasks. We test our embeddings with protein sequence classification and retrieval tasks and demonstrate encouraging outcomes. version:1
arxiv-1608-05934 | Spatial Modeling of Oil Exploration Areas Using Neural Networks and ANFIS in GIS | http://arxiv.org/abs/1608.05934 | id:1608.05934 author:Nouraddin Misagh, Mohammadreza Ashouri category:stat.ML  published:2016-08-21 summary:Exploration of hydrocarbon resources is a highly complicated and expensive process where various geological, geochemical and geophysical factors are developed then combined together. It is highly significant how to design the seismic data acquisition survey and locate the exploratory wells since incorrect or imprecise locations lead to waste of time and money during the operation. The objective of this study is to locate high-potential oil and gas field in 1: 250,000 sheet of Ahwaz including 20 oil fields to reduce both time and costs in exploration and production processes. In this regard, 17 maps were developed using GIS functions for factors including: minimum and maximum of total organic carbon (TOC), yield potential for hydrocarbons production (PP), Tmax peak, production index (PI), oxygen index (OI), hydrogen index (HI) as well as presence or proximity to high residual Bouguer gravity anomalies, proximity to anticline axis and faults, topography and curvature maps obtained from Asmari Formation subsurface contours. To model and to integrate maps, this study employed artificial neural network and adaptive neuro-fuzzy inference system (ANFIS) methods. The results obtained from model validation demonstrated that the 17x10x5 neural network with R=0.8948, RMS=0.0267, and kappa=0.9079 can be trained better than other models such as ANFIS and predicts the potential areas more accurately. However, this method failed to predict some oil fields and wrongly predict some areas as potential zones. version:1
arxiv-1608-05924 | Congruences and Concurrent Lines in Multi-View Geometry | http://arxiv.org/abs/1608.05924 | id:1608.05924 author:Jean Ponce, Bernd Sturmfels, Matthew Trager category:math.AG cs.CV cs.SC  published:2016-08-21 summary:We present a new framework for multi-view geometry in computer vision. A camera is a mapping between $\mathbb{P}^3$ and a line congruence. This model, which ignores image planes and measurements, is a natural abstraction of traditional pinhole cameras. It includes two-slit cameras, pushbroom cameras, catadioptric cameras, and many more. We study the concurrent lines variety, which consists of $n$-tuples of lines in $\mathbb{P}^3$ that intersect at a point. Combining its equations with those of various congruences, we derive constraints for corresponding images in multiple views. We also study photographic cameras which use image measurements and are modeled as rational maps from $\mathbb{P}^3$ to $\mathbb{P}^2$ or $\mathbb{P}^1\times \mathbb{P}^1$. version:1
arxiv-1608-05916 | Neural Networks and Chaos: Construction, Evaluation of Chaotic Networks, and Prediction of Chaos with Multilayer Feedforward Networks | http://arxiv.org/abs/1608.05916 | id:1608.05916 author:Jacques M. Bahi, Jean-François Couchot, Christophe Guyeux, Michel Salomon category:cs.NE math.DS nlin.CD  published:2016-08-21 summary:Many research works deal with chaotic neural networks for various fields of application. Unfortunately, up to now these networks are usually claimed to be chaotic without any mathematical proof. The purpose of this paper is to establish, based on a rigorous theoretical framework, an equivalence between chaotic iterations according to Devaney and a particular class of neural networks. On the one hand we show how to build such a network, on the other hand we provide a method to check if a neural network is a chaotic one. Finally, the ability of classical feedforward multilayer perceptrons to learn sets of data obtained from a dynamical system is regarded. Various Boolean functions are iterated on finite states. Iterations of some of them are proven to be chaotic as it is defined by Devaney. In that context, important differences occur in the training process, establishing with various neural networks that chaotic behaviors are far more difficult to learn. version:1
arxiv-1608-05910 | Validating search protocols for mining of health and disease events on Twitter | http://arxiv.org/abs/1608.05910 | id:1608.05910 author:Aditya Ramadona, Lutfan Lazuardi, Sulistyawati, Anwar Cahyono, Åsa Holmner, Hari Kusnanto, Joacim Röcklov category:stat.ML cs.CY cs.SI  published:2016-08-21 summary:In the year of 2016, there were more than 24 million Indonesian twitter users sharing news, events, as well as personal feelings and experiences on Twitter. This study seeks to validate a search protocol of health-related terms using real-time Twitter data which can later be used to understand if, and how, twitter can reveal information on the current health situation in Indonesia. In this validation study of mining protocols, we extracted geo-located conversations related to health and disease postings on Twitter using a set of pre-defined keywords, assessed the prevalence, frequency and timing of such content in these conversations, and validated how this search protocol was able to detect relevant disease tweets. Groups of words and phrases relevant to disease symptoms and health outcomes were used in a protocol developed in the Indonesian language in order to extract relevant content from geo-tagged Twitter feeds. A supervised learning algorithm using Classification and Regression Trees was used to validate search protocols of disease and health hits comparing to those identified by a team of human experts. The experts categorized tweets as positive or negative in respect to health events. The model fit was evaluated based on prediction performance. We observed 390 tweets from historical Twitter feeds and 1,145,649 tweets from Twitter stream feeds during the period July 26th to August 1st, 2016. Only twitter hits with health related keywords in the Indonesian language were obtained. The accuracy of predictions of mined hits versus expert validated hits using the CART algorithm showed good validity with AUC beyond 0.8. Our study shows that monitoring of public sentiment on Twitter, combined with contextual knowledge about the disease, can detect health and disease tweets and potentially be used as a valuable real-time proxy for health events over space and time. version:1
arxiv-1608-05895 | VoxResNet: Deep Voxelwise Residual Networks for Volumetric Brain Segmentation | http://arxiv.org/abs/1608.05895 | id:1608.05895 author:Hao Chen, Qi Dou, Lequan Yu, Pheng-Ann Heng category:cs.CV  published:2016-08-21 summary:Recently deep residual learning with residual units for training very deep neural networks advanced the state-of-the-art performance on 2D image recognition tasks, e.g., object detection and segmentation. However, how to fully leverage contextual representations for recognition tasks from volumetric data has not been well studied, especially in the field of medical image computing, where a majority of image modalities are in volumetric format. In this paper we explore the deep residual learning on the task of volumetric brain segmentation. There are at least two main contributions in our work. First, we propose a deep voxelwise residual network, referred as VoxResNet, which borrows the spirit of deep residual learning in 2D image recognition tasks, and is extended into a 3D variant for handling volumetric data. Second, an auto-context version of VoxResNet is proposed by seamlessly integrating the low-level image appearance features, implicit shape information and high-level context together for further improving the volumetric segmentation performance. Extensive experiments on the challenging benchmark of brain segmentation from magnetic resonance (MR) images corroborated the efficacy of our proposed method in dealing with volumetric data. We believe this work unravels the potential of 3D deep learning to advance the recognition performance on volumetric image segmentation. version:1
arxiv-1608-05889 | Online Feature Selection with Group Structure Analysis | http://arxiv.org/abs/1608.05889 | id:1608.05889 author:Jing Wang, Meng Wang, Peipei Li, Luoqi Liu, Zhongqiu Zhao, Xuegang Hu, Xindong Wu category:cs.CV cs.LG stat.ML  published:2016-08-21 summary:Online selection of dynamic features has attracted intensive interest in recent years. However, existing online feature selection methods evaluate features individually and ignore the underlying structure of feature stream. For instance, in image analysis, features are generated in groups which represent color, texture and other visual information. Simply breaking the group structure in feature selection may degrade performance. Motivated by this fact, we formulate the problem as an online group feature selection. The problem assumes that features are generated individually but there are group structure in the feature stream. To the best of our knowledge, this is the first time that the correlation among feature stream has been considered in the online feature selection process. To solve this problem, we develop a novel online group feature selection method named OGFS. Our proposed approach consists of two stages: online intra-group selection and online inter-group selection. In the intra-group selection, we design a criterion based on spectral analysis to select discriminative features in each group. In the inter-group selection, we utilize a linear regression model to select an optimal subset. This two-stage procedure continues until there are no more features arriving or some predefined stopping conditions are met. %Our method has been applied Finally, we apply our method to multiple tasks including image classification %, face verification and face verification. Extensive empirical studies performed on real-world and benchmark data sets demonstrate that our method outperforms other state-of-the-art online feature selection %method methods. version:1
arxiv-1608-05878 | The ground truth about metadata and community detection in networks | http://arxiv.org/abs/1608.05878 | id:1608.05878 author:Leto Peel, Daniel B. Larremore, Aaron Clauset category:cs.SI physics.data-an physics.soc-ph stat.ML  published:2016-08-20 summary:Across many scientific domains, there is common need to automatically extract a simplified view or a coarse-graining of how a complex system's components interact. This general task is called community detection in networks and is analogous to searching for clusters in independent vector data. It is common to evaluate the performance of community detection algorithms by their ability to find so-called \textit{ground truth} communities. This works well in synthetic networks with planted communities because such networks' links are formed explicitly based on the planted communities. However, there are no planted communities in real world networks. Instead, it is standard practice to treat some observed discrete-valued node attributes, or metadata, as ground truth. Here, we show that metadata are not the same as ground truth, and that treating them as such induces severe theoretical and practical problems. We prove that no algorithm can uniquely solve community detection, and we prove a general No Free Lunch theorem for community detection, which implies that no algorithm can perform better than any other across all inputs. However, node metadata still have value and a careful exploration of their relationship with network structure can yield insights of genuine worth. We illustrate this point by introducing two statistical techniques that can quantify the relationship between metadata and community structure for a broad class models. We demonstrate these techniques using both synthetic and real-world networks, and for multiple types of metadata and community structure. version:1
arxiv-1608-05859 | Using the Output Embedding to Improve Language Models | http://arxiv.org/abs/1608.05859 | id:1608.05859 author:Ofir Press, Lior Wolf category:cs.CL  published:2016-08-20 summary:We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. In addition, we offer a new method of regularizing the output embedding. These methods lead to a significant reduction in perplexity. version:1
arxiv-1608-05856 | Visual Processing by a Unified Schatten-$p$ Norm and $\ell_q$ Norm Regularized Principal Component Pursuit | http://arxiv.org/abs/1608.05856 | id:1608.05856 author:Jing Wang, Meng Wang, Xuegang Hu, Shuicheng Yan category:cs.CV  published:2016-08-20 summary:In this paper, we propose a non-convex formulation to recover the authentic structure from the corrupted real data. Typically, the specific structure is assumed to be low rank, which holds for a wide range of data, such as images and videos. Meanwhile, the corruption is assumed to be sparse. In the literature, such a problem is known as Robust Principal Component Analysis (RPCA), which usually recovers the low rank structure by approximating the rank function with a nuclear norm and penalizing the error by an $\ell_1$-norm. Although RPCA is a convex formulation and can be solved effectively, the introduced norms are not tight approximations, which may cause the solution to deviate from the authentic one. Therefore, we consider here a non-convex relaxation, consisting of a Schatten-$p$ norm and an $\ell_q$-norm that promote low rank and sparsity respectively. We derive a proximal iteratively reweighted algorithm (PIRA) to solve the problem. Our algorithm is based on an alternating direction method of multipliers, where in each iteration we linearize the underlying objective function that allows us to have a closed form solution. We demonstrate that solutions produced by the linearized approximation always converge and have a tighter approximation than the convex counterpart. Experimental results on benchmarks show encouraging results of our approach. version:1
arxiv-1608-05852 | Learning Word Embeddings from Intrinsic and Extrinsic Views | http://arxiv.org/abs/1608.05852 | id:1608.05852 author:Jifan Chen, Kan Chen, Xipeng Qiu, Qi Zhang, Xuanjing Huang, Zheng Zhang category:cs.CL cs.AI  published:2016-08-20 summary:While word embeddings are currently predominant for natural language processing, most of existing models learn them solely from their contexts. However, these context-based word embeddings are limited since not all words' meaning can be learned based on only context. Moreover, it is also difficult to learn the representation of the rare words due to data sparsity problem. In this work, we address these issues by learning the representations of words by integrating their intrinsic (descriptive) and extrinsic (contextual) information. To prove the effectiveness of our model, we evaluate it on four tasks, including word similarity, reverse dictionaries,Wiki link prediction, and document classification. Experiment results show that our model is powerful in both word and document modeling. version:1
arxiv-1608-05842 | Back to Basics: Unsupervised Learning of Optical Flow via Brightness Constancy and Motion Smoothness | http://arxiv.org/abs/1608.05842 | id:1608.05842 author:Jason J. Yu, Adam W. Harley, Konstantinos G. Derpanis category:cs.CV  published:2016-08-20 summary:Recently, convolutional networks (convnets) have proven useful for predicting optical flow. Much of this success is predicated on the availability of large datasets that require expensive and involved data acquisition and laborious la- beling. To bypass these challenges, we propose an unsuper- vised approach (i.e., without leveraging groundtruth flow) to train a convnet end-to-end for predicting optical flow be- tween two images. We use a loss function that combines a data term that measures photometric constancy over time with a spatial term that models the expected variation of flow across the image. Together these losses form a proxy measure for losses based on the groundtruth flow. Empiri- cally, we show that a strong convnet baseline trained with the proposed unsupervised approach outperforms the same network trained with supervision on the KITTI dataset. version:1
arxiv-1608-05812 | Analysis of Bayesian Classification based Approaches for Android Malware Detection | http://arxiv.org/abs/1608.05812 | id:1608.05812 author:Suleiman Y. Yerima, Sakir Sezer, Gavin McWilliams category:cs.CR cs.LG  published:2016-08-20 summary:Mobile malware has been growing in scale and complexity spurred by the unabated uptake of smartphones worldwide. Android is fast becoming the most popular mobile platform resulting in sharp increase in malware targeting the platform. Additionally, Android malware is evolving rapidly to evade detection by traditional signature-based scanning. Despite current detection measures in place, timely discovery of new malware is still a critical issue. This calls for novel approaches to mitigate the growing threat of zero-day Android malware. Hence, in this paper we develop and analyze proactive Machine Learning approaches based on Bayesian classification aimed at uncovering unknown Android malware via static analysis. The study, which is based on a large malware sample set of majority of the existing families, demonstrates detection capabilities with high accuracy. Empirical results and comparative analysis are presented offering useful insight towards development of effective static-analytic Bayesian classification based solutions for detecting unknown Android malware. version:1
arxiv-1608-05806 | Reweighting with Boosted Decision Trees | http://arxiv.org/abs/1608.05806 | id:1608.05806 author:A. Rogozhnikov category:physics.data-an hep-ex stat.ML  published:2016-08-20 summary:Machine learning tools are commonly used in modern high energy physics (HEP) experiments. Different models, such as boosted decision trees (BDT) and artificial neural networks (ANN), are widely used in analyses and even in the software triggers. In most cases, these are classification models used to select the "signal" events from data. Monte Carlo simulated events typically take part in training of these models. While the results of the simulation are expected to be close to real data, in practical cases there is notable disagreement between simulated and observed data. In order to use available simulation in training, corrections must be introduced to generated data. One common approach is reweighting - assigning weights to the simulated events. We present a novel method of event reweighting based on boosted decision trees. The problem of checking the quality of reweighting step in analyses is also discussed. version:1
arxiv-1608-05777 | Topic Sensitive Neural Headline Generation | http://arxiv.org/abs/1608.05777 | id:1608.05777 author:Lei Xu, Ziyun Wang, Ayana, Zhiyuan Liu, Maosong Sun category:cs.CL  published:2016-08-20 summary:Neural models have recently been used in text summarization including headline generation. The model can be trained using a set of document-headline pairs. However, the model does not explicitly consider topical similarities and differences of documents. We suggest to categorizing documents into various topics so that documents within the same topic are similar in content and share similar summarization patterns. Taking advantage of topic information of documents, we propose topic sensitive neural headline generation model. Our model can generate more accurate summaries guided by document topics. We test our model on LCSTS dataset, and experiments show that our method outperforms other baselines on each topic and achieves the state-of-art performance. version:1
arxiv-1608-07251 | Large-scale Collaborative Imaging Genetics Studies of Risk Genetic Factors for Alzheimer's Disease Across Multiple Institutions | http://arxiv.org/abs/1608.07251 | id:1608.07251 author:Qingyang Li, Tao Yang, Liang Zhan, Derrek Paul Hibar, Neda Jahanshad, Yalin Wang, Jieping Ye, Paul M. Thompson, Jie Wang category:cs.LG stat.ML  published:2016-08-19 summary:Genome-wide association studies (GWAS) offer new opportunities to identify genetic risk factors for Alzheimer's disease (AD). Recently, collaborative efforts across different institutions emerged that enhance the power of many existing techniques on individual institution data. However, a major barrier to collaborative studies of GWAS is that many institutions need to preserve individual data privacy. To address this challenge, we propose a novel distributed framework, termed Local Query Model (LQM) to detect risk SNPs for AD across multiple research institutions. To accelerate the learning process, we propose a Distributed Enhanced Dual Polytope Projection (D-EDPP) screening rule to identify irrelevant features and remove them from the optimization. To the best of our knowledge, this is the first successful run of the computationally intensive model selection procedure to learn a consistent model across different institutions without compromising their privacy while ranking the SNPs that may collectively affect AD. Empirical studies are conducted on 809 subjects with 5.9 million SNP features which are distributed across three individual institutions. D-EDPP achieved a 66-fold speed-up by effectively identifying irrelevant features. version:1
arxiv-1608-05754 | Fast estimation of approximate matrix ranks using spectral densities | http://arxiv.org/abs/1608.05754 | id:1608.05754 author:Shashanka Ubaru, Yousef Saad, Abd-Krim Seghouane category:cs.NA cs.LG math.NA  published:2016-08-19 summary:In many machine learning and data related applications, it is required to have the knowledge of approximate ranks of large data matrices at hand. In this paper, we present two computationally inexpensive techniques to estimate the approximate ranks of such large matrices. These techniques exploit approximate spectral densities, popular in physics, which are probability density distributions that measure the likelihood of finding eigenvalues of the matrix at a given point on the real line. Integrating the spectral density over an interval gives the eigenvalue count of the matrix in that interval. Therefore the rank can be approximated by integrating the spectral density over a carefully selected interval. Two different approaches are discussed to estimate the approximate rank, one based on Chebyshev polynomials and the other based on the Lanczos algorithm. In order to obtain the appropriate interval, it is necessary to locate a gap between the eigenvalues that correspond to noise and the relevant eigenvalues that contribute to the matrix rank. A method for locating this gap and selecting the interval of integration is proposed based on the plot of the spectral density. Numerical experiments illustrate the performance of these techniques on matrices from typical applications. version:1
arxiv-1608-05749 | Solving a Mixture of Many Random Linear Equations by Tensor Decomposition and Alternating Minimization | http://arxiv.org/abs/1608.05749 | id:1608.05749 author:Xinyang Yi, Constantine Caramanis, Sujay Sanghavi category:cs.LG cs.IT math.IT math.ST stat.ML stat.TH  published:2016-08-19 summary:We consider the problem of solving mixed random linear equations with $k$ components. This is the noiseless setting of mixed linear regression. The goal is to estimate multiple linear models from mixed samples in the case where the labels (which sample corresponds to which model) are not observed. We give a tractable algorithm for the mixed linear equation problem, and show that under some technical conditions, our algorithm is guaranteed to solve the problem exactly with sample complexity linear in the dimension, and polynomial in $k$, the number of components. Previous approaches have required either exponential dependence on $k$, or super-linear dependence on the dimension. The proposed algorithm is a combination of tensor decomposition and alternating minimization. Our analysis involves proving that the initialization provided by the tensor method allows alternating minimization, which is equivalent to EM in our setting, to converge to the global optimum at a linear rate. version:1
arxiv-1608-05747 | Space-Filling Curves as a Novel Crystal Structure Representation for Machine Learning Models | http://arxiv.org/abs/1608.05747 | id:1608.05747 author:Dipti Jasrasaria, Edward O. Pyzer-Knapp, Dmitrij Rappoport, Alan Aspuru-Guzik category:stat.ML physics.chem-ph  published:2016-08-19 summary:A fundamental problem in applying machine learning techniques for chemical problems is to find suitable representations for molecular and crystal structures. While the structure representations based on atom connectivities are prevalent for molecules, two-dimensional descriptors are not suitable for describing molecular crystals. In this work, we introduce the SFC-M family of feature representations, which are based on Morton space-filling curves, as an alternative means of representing crystal structures. Latent Semantic Indexing (LSI) was employed in a novel setting to reduce sparsity of feature representations. The quality of the SFC-M representations were assessed by using them in combination with artificial neural networks to predict Density Functional Theory (DFT) single point, Ewald summed, lattice, and many-body dispersion energies of 839 organic molecular crystal unit cells from the Cambridge Structural Database that consist of the elements C, H, N, and O. Promising initial results suggest that the SFC-M representations merit further exploration to improve its ability to predict solid-state properties of organic crystal structures version:1
arxiv-1608-06277 | Fundamental principles of cortical computation: unsupervised learning with prediction, compression and feedback | http://arxiv.org/abs/1608.06277 | id:1608.06277 author:Micah Richert, Dimitry Fisher, Filip Piekniewski, Eugene M. Izhikevich, Todd L. Hylton category:cs.CV q-bio.NC  published:2016-08-19 summary:There has been great progress in understanding of anatomical and functional microcircuitry of the primate cortex. However, the fundamental principles of cortical computation - the principles that allow the visual cortex to bind retinal spikes into representations of objects, scenes and scenarios - have so far remained elusive. In an attempt to come closer to understanding the fundamental principles of cortical computation, here we present a functional, phenomenological model of the primate visual cortex. The core part of the model describes four hierarchical cortical areas with feedforward, lateral, and recurrent connections. The three main principles implemented in the model are information compression, unsupervised learning by prediction, and use of lateral and top-down context. We show that the model reproduces key aspects of the primate ventral stream of visual processing including Simple and Complex cells in V1, increasingly complicated feature encoding, and increased separability of object representations in higher cortical areas. The model learns representations of the visual environment that allow for accurate classification and state-of-the-art visual tracking performance on novel objects. version:1
arxiv-1608-04381 | Automated Selection of Uniform Regions for CT Image Quality Detection | http://arxiv.org/abs/1608.04381 | id:1608.04381 author:Maitham D Naeemi, Adam M Alessio, Sohini Roychowdhury category:physics.med-ph cs.CV  published:2016-08-13 summary:CT images are widely used in pathology detection and follow-up treatment procedures. Accurate identification of pathological features requires diagnostic quality CT images with minimal noise and artifact variation. In this work, a novel Fourier-transform based metric for image quality (IQ) estimation is presented that correlates to additive CT image noise. In the proposed method, two windowed CT image subset regions are analyzed together to identify the extent of variation in the corresponding Fourier-domain spectrum. The two square windows are chosen such that their center pixels coincide and one window is a subset of the other. The Fourier-domain spectral difference between these two sub-sampled windows is then used to isolate spatial regions-of-interest (ROI) with low signal variation (ROI-LV) and high signal variation (ROI-HV), respectively. Finally, the spatial variance ($var$), standard deviation ($std$), coefficient of variance ($cov$) and the fraction of abdominal ROI pixels in ROI-LV ($\nu'(q)$), are analyzed with respect to CT image noise. For the phantom CT images, $var$ and $std$ correlate to CT image noise ($ r >0.76$ ($p\ll0.001$)), though not as well as $\nu'(q)$ ($r=0.96$ ($p\ll0.001$)). However, for the combined phantom and patient CT images, $var$ and $std$ do not correlate well with CT image noise ($ r <0.46$ ($p\ll0.001$)) as compared to $\nu'(q)$ ($r=0.95$ ($p\ll0.001$)). Thus, the proposed method and the metric, $\nu'(q)$, can be useful to quantitatively estimate CT image noise. version:2
arxiv-1608-05684 | Detecting Vanishing Points using Global Image Context in a Non-Manhattan World | http://arxiv.org/abs/1608.05684 | id:1608.05684 author:Menghua Zhai, Scott Workman, Nathan Jacobs category:cs.CV  published:2016-08-19 summary:We propose a novel method for detecting horizontal vanishing points and the zenith vanishing point in man-made environments. The dominant trend in existing methods is to first find candidate vanishing points, then remove outliers by enforcing mutual orthogonality. Our method reverses this process: we propose a set of horizon line candidates and score each based on the vanishing points it contains. A key element of our approach is the use of global image context, extracted with a deep convolutional network, to constrain the set of candidates under consideration. Our method does not make a Manhattan-world assumption and can operate effectively on scenes with only a single horizontal vanishing point. We evaluate our approach on three benchmark datasets and achieve state-of-the-art performance on each. In addition, our approach is significantly faster than the previous best method. version:1
arxiv-1608-05639 | Operator-Valued Bochner Theorem, Fourier Feature Maps for Operator-Valued Kernels, and Vector-Valued Learning | http://arxiv.org/abs/1608.05639 | id:1608.05639 author:Ha Quang Minh category:cs.LG  published:2016-08-19 summary:This paper presents a framework for computing random operator-valued feature maps for operator-valued positive definite kernels. This is a generalization of the random Fourier features for scalar-valued kernels to the operator-valued case. Our general setting is that of operator-valued kernels corresponding to RKHS of functions with values in a Hilbert space. We show that in general, for a given kernel, there are potentially infinitely many random feature maps, which can be bounded or unbounded. Most importantly, given a kernel, we present a general, closed form formula for computing a corresponding probability measure, which is required for the construction of the Fourier features, and which, unlike the scalar case, is not uniquely and automatically determined by the kernel. We also show that, under appropriate conditions, random bounded feature maps can always be computed. Furthermore, we show the uniform convergence, under the Hilbert-Schmidt norm, of the resulting approximate kernel to the exact kernel on any compact subset of Euclidean space. Our convergence requires differentiable kernels, an improvement over the twice-differentiability requirement in previous work in the scalar setting. We then show how operator-valued feature maps and their approximations can be employed in a general vector-valued learning framework. The mathematical formulation is illustrated by numerical examples on matrix-valued kernels. version:1
arxiv-1608-05610 | PAC-Bayesian Aggregation without Cross-Validation | http://arxiv.org/abs/1608.05610 | id:1608.05610 author:Niklas Thiemann, Christian Igel, Yevgeny Seldin category:cs.LG stat.ML  published:2016-08-19 summary:We propose a new PAC-Bayesian procedure for aggregating prediction models and a new way of constructing a hypothesis space with which the procedure works particularly well. The procedure is based on alternating minimization of a new PAC-Bayesian bound, which is convex in the posterior distribution used for aggregation and also convex in a trade-off parameter between empirical performance of the distribution and its complexity, measured by the Kullback-Leibler divergence to a prior. The hypothesis space is constructed by training a finite number of weak classifiers, where each classifier is trained on a small subsample of the data and validated on the corresponding complementary subset of the data. The weak classifiers are then weighted with respect to their validation performance through minimization of the PAC-Bayesian bound. We provide experimental results demonstrating that the proposed aggregation strategy is on par with the prediction accuracy of kernel SVMs tuned by cross-validation. The comparable accuracy is achieved at a much lower computation cost, since training many SVMs on small subsamples is significantly cheaper than training one SVM on the whole data due to super-quadratic training time of kernel SVMs. Remarkably, our prediction approach is based on minimization of a theoretical bound and does not require parameter cross-validation, as opposed to the majority of theoretical results that cannot be rigorously applied in practice. version:1
