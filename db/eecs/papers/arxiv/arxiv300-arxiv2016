arxiv-300-1 | An Unsupervised Dynamic Image Segmentation using Fuzzy Hopfield Neural Network based Genetic Algorithm | http://arxiv.org/pdf/1205.6572v1.pdf | author:Amiya Halder, Soumajit Pramanik category:cs.CV published:2012-05-30 summary:This paper proposes a Genetic Algorithm based segmentation method that canautomatically segment gray-scale images. The proposed method mainly consists ofspatial unsupervised grayscale image segmentation that divides an image intoregions. The aim of this algorithm is to produce precise segmentation of imagesusing intensity information along with neighborhood relationships. In thispaper, Fuzzy Hopfield Neural Network (FHNN) clustering helps in generating thepopulation of Genetic algorithm which there by automatically segments theimage. This technique is a powerful method for image segmentation and works forboth single and multiple-feature data with spatial information. Validity indexhas been utilized for introducing a robust technique for finding the optimumnumber of components in an image. Experimental results shown that the algorithmgenerates good quality segmented image.
arxiv-300-2 | A Brief Summary of Dictionary Learning Based Approach for Classification (revised) | http://arxiv.org/pdf/1205.6544v1.pdf | author:Shu Kong, Donghui Wang category:cs.CV cs.LG published:2012-05-30 summary:This note presents some representative methods which are based on dictionarylearning (DL) for classification. We do not review the sophisticated methods orframeworks that involve DL for classification, such as online DL and spatialpyramid matching (SPM), but rather, we concentrate on the direct DL-basedclassification methods. Here, the "so-called direct DL-based method" is theapproach directly deals with DL framework by adding some meaningful penaltyterms. By listing some representative methods, we can roughly divide them intotwo categories, i.e. (1) directly making the dictionary discriminative and (2)forcing the sparse coefficients discriminative to push the discrimination powerof the dictionary. From this taxonomy, we can expect some extensions of them asfuture researches.
arxiv-300-3 | A Brief Summary of Dictionary Learning Based Approach for Classification | http://arxiv.org/pdf/1205.6391v2.pdf | author:Kong Shu, Wang Donghui category:cs.CV published:2012-05-29 summary:This note presents some representative methods which are based on dictionarylearning (DL) for classification. We do not review the sophisticated methods orframeworks that involve DL for classification, such as online DL and spatialpyramid matching (SPM), but rather, we concentrate on the direct DL-basedclassification methods. Here, the "so-called direct DL-based method" is theapproach directly deals with DL framework by adding some meaningful penaltyterms. By listing some representative methods, we can roughly divide them intotwo categories, i.e. (1) directly making the dictionary discriminative and (2)forcing the sparse coefficients discriminative to push the discrimination powerof the dictionary. From this taxonomy, we can expect some extensions of them asfuture researches.
arxiv-300-4 | Finding Important Genes from High-Dimensional Data: An Appraisal of Statistical Tests and Machine-Learning Approaches | http://arxiv.org/pdf/1205.6523v1.pdf | author:Chamont Wang, Jana Gevertz, Chaur-Chin Chen, Leonardo Auslender category:stat.ML cs.LG q-bio.QM published:2012-05-30 summary:Over the past decades, statisticians and machine-learning researchers havedeveloped literally thousands of new tools for the reduction ofhigh-dimensional data in order to identify the variables most responsible for aparticular trait. These tools have applications in a plethora of settings,including data analysis in the fields of business, education, forensics, andbiology (such as microarray, proteomics, brain imaging), to name a few. In the present work, we focus our investigation on the limitations andpotential misuses of certain tools in the analysis of the benchmark coloncancer data (2,000 variables; Alon et al., 1999) and the prostate cancer data(6,033 variables; Efron, 2010, 2008). Our analysis demonstrates that modelsthat produce 100% accuracy measures often select different sets of genes andcannot stand the scrutiny of parameter estimates and model stability. Furthermore, we created a host of simulation datasets and "artificialdiseases" to evaluate the reliability of commonly used statistical and datamining tools. We found that certain widely used models can classify the datawith 100% accuracy without using any of the variables responsible for thedisease. With moderate sample size and suitable pre-screening, stochasticgradient boosting will be shown to be a superior model for gene selection andvariable screening from high-dimensional datasets.
arxiv-300-5 | Sparse Approximation via Penalty Decomposition Methods | http://arxiv.org/pdf/1205.2334v2.pdf | author:Zhaosong Lu, Yong Zhang category:cs.LG math.OC stat.CO stat.ML published:2012-05-10 summary:In this paper we consider sparse approximation problems, that is, general$l_0$ minimization problems with the $l_0$-"norm" of a vector being a part ofconstraints or objective function. In particular, we first study thefirst-order optimality conditions for these problems. We then propose penaltydecomposition (PD) methods for solving them in which a sequence of penaltysubproblems are solved by a block coordinate descent (BCD) method. Under somesuitable assumptions, we establish that any accumulation point of the sequencegenerated by the PD methods satisfies the first-order optimality conditions ofthe problems. Furthermore, for the problems in which the $l_0$ part is the onlynonconvex part, we show that such an accumulation point is a local minimizer ofthe problems. In addition, we show that any accumulation point of the sequencegenerated by the BCD method is a saddle point of the penalty subproblem.Moreover, for the problems in which the $l_0$ part is the only nonconvex part,we establish that such an accumulation point is a local minimizer of thepenalty subproblem. Finally, we test the performance of our PD methods byapplying them to sparse logistic regression, sparse inverse covarianceselection, and compressed sensing problems. The computational resultsdemonstrate that our methods generally outperform the existing methods in termsof solution quality and/or speed.
arxiv-300-6 | Penalty Decomposition Methods for Rank Minimization | http://arxiv.org/pdf/1008.5373v4.pdf | author:Zhaosong Lu, Yong Zhang category:math.OC cs.LG cs.NA cs.SY q-fin.CP q-fin.ST published:2010-08-31 summary:In this paper we consider general rank minimization problems with rankappearing in either objective function or constraint. We first establish that aclass of special rank minimization problems has closed-form solutions. Usingthis result, we then propose penalty decomposition methods for general rankminimization problems in which each subproblem is solved by a block coordinatedescend method. Under some suitable assumptions, we show that any accumulationpoint of the sequence generated by the penalty decomposition methods satisfiesthe first-order optimality conditions of a nonlinear reformulation of theproblems. Finally, we test the performance of our methods by applying them tothe matrix completion and nearest low-rank correlation matrix problems. Thecomputational results demonstrate that our methods are generally comparable orsuperior to the existing methods in terms of solution quality.
arxiv-300-7 | Effective Listings of Function Stop words for Twitter | http://arxiv.org/pdf/1205.6396v1.pdf | author:Murphy Choy category:cs.IR cs.CL published:2012-05-29 summary:Many words in documents recur very frequently but are essentially meaninglessas they are used to join words together in a sentence. It is commonlyunderstood that stop words do not contribute to the context or content oftextual documents. Due to their high frequency of occurrence, their presence intext mining presents an obstacle to the understanding of the content in thedocuments. To eliminate the bias effects, most text mining software orapproaches make use of stop words list to identify and remove those words.However, the development of such top words list is difficult and inconsistentbetween textual sources. This problem is further aggravated by sources such asTwitter which are highly repetitive or similar in nature. In this paper, wewill be examining the original work using term frequency, inverse documentfrequency and term adjacency for developing a stop words list for the Twitterdata source. We propose a new technique using combinatorial values as analternative measure to effectively list out stop words.
arxiv-300-8 | Learning high-dimensional directed acyclic graphs with latent and selection variables | http://arxiv.org/pdf/1104.5617v3.pdf | author:Diego Colombo, Marloes H. Maathuis, Markus Kalisch, Thomas S. Richardson category:stat.ME cs.LG math.ST stat.TH published:2011-04-29 summary:We consider the problem of learning causal information between randomvariables in directed acyclic graphs (DAGs) when allowing arbitrarily manylatent and selection variables. The FCI (Fast Causal Inference) algorithm hasbeen explicitly designed to infer conditional independence and causalinformation in such settings. However, FCI is computationally infeasible forlarge graphs. We therefore propose the new RFCI algorithm, which is much fasterthan FCI. In some situations the output of RFCI is slightly less informative,in particular with respect to conditional independence information. However, weprove that any causal information in the output of RFCI is correct in theasymptotic limit. We also define a class of graphs on which the outputs of FCIand RFCI are identical. We prove consistency of FCI and RFCI in sparsehigh-dimensional settings, and demonstrate in simulations that the estimationperformances of the algorithms are very similar. All software is implemented inthe R-package pcalg.
arxiv-300-9 | Checking Tests for Read-Once Functions over Arbitrary Bases | http://arxiv.org/pdf/1203.0631v3.pdf | author:Dmitry V. Chistikov category:cs.DM cs.CC cs.LG published:2012-03-03 summary:A Boolean function is called read-once over a basis B if it can be expressedby a formula over B where no variable appears more than once. A checking testfor a read-once function f over B depending on all its variables is a set ofinput vectors distinguishing f from all other read-once functions of the samevariables. We show that every read-once function f over B has a checking testcontaining O(n^l) vectors, where n is the number of relevant variables of f andl is the largest arity of functions in B. For some functions, this bound cannotbe improved by more than a constant factor. The employed technique involvesreconstructing f from its l-variable projections and provides a stronger formof Kuznetsov's classic theorem on read-once representations.
arxiv-300-10 | Potentials and Limits of Super-Resolution Algorithms and Signal Reconstruction from Sparse Data | http://arxiv.org/pdf/1205.6154v1.pdf | author:Gil Shabat category:physics.optics cs.CV math-ph math.MP published:2012-05-28 summary:A common distortion in videos is image instability in the form of chaotic(global and local displacements). Those instabilities can be used to enhanceimage resolution by using subpixel elastic registration. In this work, weinvestigate the performance of such methods over the ability to improve theresolution by accumulating several frames. The second part of this work dealswith reconstruction of discrete signals from a subset of samples underdifferent basis functions such as DFT, Haar, Walsh, Daubechies wavelets and CT(Radon) projections.
arxiv-300-11 | Semi-supervised logistic discrimination for functional data | http://arxiv.org/pdf/1102.4399v3.pdf | author:Shuichi Kawano, Sadanori Konishi category:stat.ME stat.ML published:2011-02-22 summary:Multi-class classification methods based on both labeled and unlabeledfunctional data sets are discussed. We present a semi-supervised logistic modelfor classification in the context of functional data analysis. Unknownparameters in our proposed model are estimated by regularization with the helpof EM algorithm. A crucial point in the modeling procedure is the choice of aregularization parameter involved in the semi-supervised functional logisticmodel. In order to select the adjusted parameter, we introduce model selectioncriteria from information-theoretic and Bayesian viewpoints. Monte Carlosimulations and a real data analysis are given to examine the effectiveness ofour proposed modeling strategy.
arxiv-300-12 | Positive words carry less information than negative words | http://arxiv.org/pdf/1110.4123v4.pdf | author:David Garcia, Antonios Garas, Frank Schweitzer category:cs.CL cs.IR physics.soc-ph published:2011-10-18 summary:We show that the frequency of word use is not only determined by the wordlength \cite{Zipf1935} and the average information content\cite{Piantadosi2011}, but also by its emotional content. We have analyzedthree established lexica of affective word usage in English, German, andSpanish, to verify that these lexica have a neutral, unbiased, emotionalcontent. Taking into account the frequency of word usage, we find that wordswith a positive emotional content are more frequently used. This lends supportto Pollyanna hypothesis \cite{Boucher1969} that there should be a positive biasin human expression. We also find that negative words contain more informationthan positive words, as the informativeness of a word increases uniformly withits valence decrease. Our findings support earlier conjectures about (i) therelation between word frequency and information content, and (ii) the impact ofpositive emotions on communication and social links.
arxiv-300-13 | Distributed Learning, Communication Complexity and Privacy | http://arxiv.org/pdf/1204.3514v3.pdf | author:Maria-Florina Balcan, Avrim Blum, Shai Fine, Yishay Mansour category:cs.LG cs.DS F.2.2; I.2.6 published:2012-04-16 summary:We consider the problem of PAC-learning from distributed data and analyzefundamental communication complexity questions involved. We provide generalupper and lower bounds on the amount of communication needed to learn well,showing that in addition to VC-dimension and covering number, quantities suchas the teaching-dimension and mistake-bound of a class play an important role.We also present tight results for a number of common concept classes includingconjunctions, parity functions, and decision lists. For linear separators, weshow that for non-concentrated distributions, we can use a version of thePerceptron algorithm to learn with much less communication than the number ofupdates given by the usual margin bound. We also show how boosting can beperformed in a generic manner in the distributed setting to achievecommunication with only logarithmic dependence on 1/epsilon for any conceptclass, and demonstrate how recent work on agnostic learning fromclass-conditional queries can be used to achieve low communication in agnosticsettings as well. We additionally present an analysis of privacy, consideringboth differential privacy and a notion of distributional privacy that isespecially appealing in this context.
arxiv-300-14 | Visualization of features of a series of measurements with one-dimensional cellular structure | http://arxiv.org/pdf/1205.4234v2.pdf | author:D. V. Lande category:cs.LG 68R published:2012-05-19 summary:This paper describes the method of visualization of periodic constituents andinstability areas in series of measurements, being based on the algorithm ofsmoothing out and concept of one-dimensional cellular automata. A method can beused at the analysis of temporal series, related to the volumes of thematicpublications in web-space.
arxiv-300-15 | Theory of Dependent Hierarchical Normalized Random Measures | http://arxiv.org/pdf/1205.4159v2.pdf | author:Changyou Chen, Wray Buntine, Nan Ding category:cs.LG math.ST stat.ML stat.TH published:2012-05-18 summary:This paper presents theory for Normalized Random Measures (NRMs), NormalizedGeneralized Gammas (NGGs), a particular kind of NRM, and Dependent HierarchicalNRMs which allow networks of dependent NRMs to be analysed. These have beenused, for instance, for time-dependent topic modelling. In this paper, we firstintroduce some mathematical background of completely random measures (CRMs) andtheir construction from Poisson processes, and then introduce NRMs and NGGs.Slice sampling is also introduced for posterior inference. The dependencyoperators in Poisson processes and for the corresponding CRMs and NRMs is thenintroduced and Posterior inference for the NGG presented. Finally, we givedependency and composition results when applying these operators to NRMs sothey can be used in a network with hierarchical and dependent relations.
arxiv-300-16 | Stochastic Belief Propagation: A Low-Complexity Alternative to the Sum-Product Algorithm | http://arxiv.org/pdf/1111.1020v2.pdf | author:Nima Noorshams, Martin J. Wainwright category:cs.IT math.IT stat.ML published:2011-11-04 summary:The sum-product or belief propagation (BP) algorithm is a widely-usedmessage-passing algorithm for computing marginal distributions in graphicalmodels with discrete variables. At the core of the BP message updates, whenapplied to a graphical model with pairwise interactions, lies a matrix-vectorproduct with complexity that is quadratic in the state dimension $d$, andrequires transmission of a $(d-1)$-dimensional vector of real numbers(messages) to its neighbors. Since various applications involve very largestate dimensions, such computation and communication complexities can beprohibitively complex. In this paper, we propose a low-complexity variant ofBP, referred to as stochastic belief propagation (SBP). As suggested by thename, it is an adaptively randomized version of the BP message updates in whicheach node passes randomly chosen information to each of its neighbors. The SBPmessage updates reduce the computational complexity (per iteration) fromquadratic to linear in $d$, without assuming any particular structure of thepotentials, and also reduce the communication complexity significantly,requiring only $\log{d}$ bits transmission per edge. Moreover, we establish anumber of theoretical guarantees for the performance of SBP, showing that itconverges almost surely to the BP fixed point for any tree-structured graph,and for graphs with cycles satisfying a contractivity condition. In addition,for these graphical models, we provide non-asymptotic upper bounds on theconvergence rate, showing that the $\ell_{\infty}$ norm of the error vectordecays no slower than $O(1/\sqrt{t})$ with the number of iterations $t$ ontrees and the mean square error decays as $O(1/t)$ for general graphs. Theseanalysis show that SBP can provably yield reductions in computational andcommunication complexities for various classes of graphical models.
arxiv-300-17 | Confusion Matrix Stability Bounds for Multiclass Classification | http://arxiv.org/pdf/1202.6221v2.pdf | author:Pierre Machart, Liva Ralaivola category:cs.LG published:2012-02-28 summary:In this paper, we provide new theoretical results on the generalizationproperties of learning algorithms for multiclass classification problems. Theoriginality of our work is that we propose to use the confusion matrix of aclassifier as a measure of its quality; our contribution is in the line of workwhich attempts to set up and study the statistical properties of new evaluationmeasures such as, e.g. ROC curves. In the confusion-based learning framework wepropose, we claim that a targetted objective is to minimize the size of theconfusion matrix C, measured through its operator norm C. We derivegeneralization bounds on the (size of the) confusion matrix in an extendedframework of uniform stability, adapted to the case of matrix valued loss.Pivotal to our study is a very recent matrix concentration inequality thatgeneralizes McDiarmid's inequality. As an illustration of the relevance of ourtheoretical results, we show how two SVM learning procedures can be proved tobe confusion-friendly. To the best of our knowledge, the present paper is thefirst that focuses on the confusion matrix from a theoretical point of view.
arxiv-300-18 | Locally Orderless Registration | http://arxiv.org/pdf/1205.5425v1.pdf | author:Sune Darkner, Jon Sporring category:cs.CV published:2012-05-24 summary:Image registration is an important tool for medical image analysis and isused to bring images into the same reference frame by warping the coordinatefield of one image, such that some similarity measure is minimized. We studysimilarity in image registration in the context of Locally Orderless Images(LOI), which is the natural way to study density estimates and reveals the 3fundamental scales: the measurement scale, the intensity scale, and theintegration scale. This paper has three main contributions: Firstly, we rephrase a large set ofpopular similarity measures into a common framework, which we refer to asLocally Orderless Registration, and which makes full use of the features oflocal histograms. Secondly, we extend the theoretical understanding of thelocal histograms. Thirdly, we use our framework to compare two state-of-the-artintensity density estimators for image registration: The Parzen Window (PW) andthe Generalized Partial Volume (GPV), and we demonstrate their differences on apopular similarity measure, Normalized Mutual Information (NMI). We conclude, that complicated similarity measures such as NMI may beevaluated almost as fast as simple measures such as Sum of Squared Distances(SSD) regardless of the choice of PW and GPV. Also, GPV is an asymmetricmeasure, and PW is our preferred choice.
arxiv-300-19 | Language-Constraint Reachability Learning in Probabilistic Graphs | http://arxiv.org/pdf/1205.5367v1.pdf | author:Claudio Taranto, Nicola Di Mauro, Floriana Esposito category:cs.AI cs.LG published:2012-05-24 summary:The probabilistic graphs framework models the uncertainty inherent inreal-world domains by means of probabilistic edges whose value quantifies thelikelihood of the edge existence or the strength of the link it represents. Thegoal of this paper is to provide a learning method to compute the most likelyrelationship between two nodes in a framework based on probabilistic graphs. Inparticular, given a probabilistic graph we adopted the language-constraintreachability method to compute the probability of possible interconnectionsthat may exists between two nodes. Each of these connections may be viewed asfeature, or a factor, between the two nodes and the corresponding probabilityas its weight. Each observed link is considered as a positive instance for itscorresponding link label. Given the training set of observed links aL2-regularized Logistic Regression has been adopted to learn a model able topredict unobserved link labels. The experiments on a real world collaborativefiltering problem proved that the proposed approach achieves better resultsthan that obtained adopting classical methods.
arxiv-300-20 | A hybrid clustering algorithm for data mining | http://arxiv.org/pdf/1205.5353v1.pdf | author:Ravindra Jain category:cs.DB cs.LG published:2012-05-24 summary:Data clustering is a process of arranging similar data into groups. Aclustering algorithm partitions a data set into several groups such that thesimilarity within a group is better than among groups. In this paper a hybridclustering algorithm based on K-mean and K-harmonic mean (KHM) is described.The proposed algorithm is tested on five different datasets. The research isfocused on fast and accurate clustering. Its performance is compared with thetraditional K-means & KHM algorithm. The result obtained from proposed hybridalgorithm is much better than the traditional K-mean & KHM algorithm.
arxiv-300-21 | Fuzzy - Rough Feature Selection With Î - Membership Function For Mammogram Classification | http://arxiv.org/pdf/1205.4336v2.pdf | author:K. Thangavel, R. Roselin category:cs.CV published:2012-05-19 summary:Breast cancer is the second leading cause for death among women and it isdiagnosed with the help of mammograms. Oncologists are miserably failed inidentifying the micro calcification at the early stage with the help of themammogram visually. In order to improve the performance of the breast cancerscreening, most of the researchers have proposed Computer Aided Diagnosis usingimage processing. In this study mammograms are preprocessed and features areextracted, then the abnormality is identified through the classification. Ifall the extracted features are used, most of the cases are misidentified. Hencefeature selection procedure is sought. In this paper, Fuzzy-Rough featureselection with {\pi} membership function is proposed. The selected features areused to classify the abnormalities with help of Ant-Miner and Weka tools. Theexperimental analysis shows that the proposed method improves the mammogramsclassification accuracy.
arxiv-300-22 | Neural Network Approach for Eye Detection | http://arxiv.org/pdf/1205.5097v1.pdf | author:Vijayalaxmi, P. Sudhakara Rao, S. Sreehari category:cs.CV published:2012-05-23 summary:Driving support systems, such as car navigation systems are becoming commonand they support driver in several aspects. Non-intrusive method of detectingFatigue and drowsiness based on eye-blink count and eye directed instructioncontrolhelps the driver to prevent from collision caused by drowsy driving. Eyedetection and tracking under various conditions such as illumination,background, face alignment and facial expression makes the problemcomplex.Neural Network based algorithm is proposed in this paper to detect theeyes efficiently. In the proposed algorithm, first the neural Network istrained to reject the non-eye regionbased on images with features of eyes andthe images with features of non-eye using Gabor filter and Support VectorMachines to reduce the dimension and classify efficiently. In the algorithm,first the face is segmented using L*a*btransform color space, then eyes aredetected using HSV and Neural Network approach. The algorithm is tested onnearly 100 images of different persons under different conditions and theresults are satisfactory with success rate of 98%.The Neural Network is trainedwith 50 non-eye images and 50 eye images with different angles using Gaborfilter. This paper is a part of research work on "Development of Non-Intrusivesystem for real-time Monitoring and Prediction of Driver Fatigue anddrowsiness" project sponsored by Department of Science & Technology, Govt. ofIndia, New Delhi at Vignan Institute of Technology and Sciences, Vignan Hills,Hyderabad.
arxiv-300-23 | On the practically interesting instances of MAXCUT | http://arxiv.org/pdf/1205.4893v1.pdf | author:Yonatan Bilu, Amit Daniely, Nati Linial, Michael Saks category:cs.CC cs.LG published:2012-05-22 summary:The complexity of a computational problem is traditionally quantified basedon the hardness of its worst case. This approach has many advantages and hasled to a deep and beautiful theory. However, from the practical perspective,this leaves much to be desired. In application areas, practically interestinginstances very often occupy just a tiny part of an algorithm's space ofinstances, and the vast majority of instances are simply irrelevant. Addressingthese issues is a major challenge for theoretical computer science which maymake theory more relevant to the practice of computer science. Following Bilu and Linial, we apply this perspective to MAXCUT, viewed as aclustering problem. Using a variety of techniques, we investigate practicallyinteresting instances of this problem. Specifically, we show how to solve inpolynomial time distinguished, metric, expanding and dense instances of MAXCUTunder mild stability assumptions. In particular, $(1+\epsilon)$-stability(which is optimal) suffices for metric and dense MAXCUT. We also show how tosolve in polynomial time $\Omega(\sqrt{n})$-stable instances of MAXCUT,substantially improving the best previously known result.
arxiv-300-24 | Clustering is difficult only when it does not matter | http://arxiv.org/pdf/1205.4891v1.pdf | author:Amit Daniely, Nati Linial, Michael Saks category:cs.LG cs.DS published:2012-05-22 summary:Numerous papers ask how difficult it is to cluster data. We suggest that themore relevant and interesting question is how difficult it is to cluster datasets {\em that can be clustered well}. More generally, despite the ubiquity andthe great importance of clustering, we still do not have a satisfactorymathematical theory of clustering. In order to properly understand clustering,it is clearly necessary to develop a solid theoretical basis for the area. Forexample, from the perspective of computational complexity theory the clusteringproblem seems very hard. Numerous papers introduce various criteria andnumerical measures to quantify the quality of a given clustering. The resultingconclusions are pessimistic, since it is computationally difficult to find anoptimal clustering of a given data set, if we go by any of these popularcriteria. In contrast, the practitioners' perspective is much more optimistic.Our explanation for this disparity of opinions is that complexity theoryconcentrates on the worst case, whereas in reality we only care for data setsthat can be clustered well. We introduce a theoretical framework of clustering in metric spaces thatrevolves around a notion of "good clustering". We show that if a goodclustering exists, then in many cases it can be efficiently found. Ourconclusion is that contrary to popular belief, clustering should not beconsidered a hard task.
arxiv-300-25 | Gray Level Co-Occurrence Matrices: Generalisation and Some New Features | http://arxiv.org/pdf/1205.4831v1.pdf | author:Bino Sebastian V, A. Unnikrishnan, Kannan Balakrishnan category:cs.CV published:2012-05-22 summary:Gray Level Co-occurrence Matrices (GLCM) are one of the earliest techniquesused for image texture analysis. In this paper we defined a new feature calledtrace extracted from the GLCM and its implications in texture analysis arediscussed in the context of Content Based Image Retrieval (CBIR). Thetheoretical extension of GLCM to n-dimensional gray scale images are alsodiscussed. The results indicate that trace features outperform Haralickfeatures when applied to CBIR.
arxiv-300-26 | Visual and semantic interpretability of projections of high dimensional data for classification tasks | http://arxiv.org/pdf/1205.4776v1.pdf | author:Ilknur Icke, Andrew Rosenberg category:cs.HC cs.LG published:2012-05-22 summary:A number of visual quality measures have been introduced in visual analyticsliterature in order to automatically select the best views of high dimensionaldata from a large number of candidate data projections. These methods generallyconcentrate on the interpretability of the visualization and pay littleattention to the interpretability of the projection axes. In this paper, weargue that interpretability of the visualizations and the featuretransformation functions are both crucial for visual exploration of highdimensional labeled data. We present a two-part user study to examine these tworelated but orthogonal aspects of interpretability. We first study how humansjudge the quality of 2D scatterplots of various datasets with varying number ofclasses and provide comparisons with ten automated measures, including a numberof visual quality measures and related measures from various machine learningfields. We then investigate how the user perception on interpretability ofmathematical expressions relate to various automated measures of complexitythat can be used to characterize data projection functions. We conclude with adiscussion of how automated measures of visual and semantic interpretability ofdata projections can be used together for exploratory analysis inclassification tasks.
arxiv-300-27 | Variance function estimation in high-dimensions | http://arxiv.org/pdf/1205.4770v1.pdf | author:Mladen Kolar, James Sharpnack category:stat.ML stat.ME published:2012-05-21 summary:We consider the high-dimensional heteroscedastic regression model, where themean and the log variance are modeled as a linear combination of inputvariables. Existing literature on high-dimensional linear regres- sion modelshas largely ignored non-constant error variances, even though they commonlyoccur in a variety of applications ranging from biostatis- tics to finance. Inthis paper we study a class of non-convex penalized pseudolikelihood estimatorsfor both the mean and variance parameters. We show that the HeteroscedasticIterative Penalized Pseudolikelihood Optimizer (HIPPO) achieves the oracleproperty, that is, we prove that the rates of convergence are the same as ifthe true model was known. We demonstrate numerical properties of the procedureon a simulation study and real world data.
arxiv-300-28 | Hypothesis testing using pairwise distances and associated kernels (with Appendix) | http://arxiv.org/pdf/1205.0411v2.pdf | author:Dino Sejdinovic, Arthur Gretton, Bharath Sriperumbudur, Kenji Fukumizu category:cs.LG stat.ME stat.ML published:2012-05-02 summary:We provide a unifying framework linking two classes of statistics used intwo-sample and independence testing: on the one hand, the energy distances anddistance covariances from the statistics literature; on the other, distancesbetween embeddings of distributions to reproducing kernel Hilbert spaces(RKHS), as established in machine learning. The equivalence holds when energydistances are computed with semimetrics of negative type, in which case akernel may be defined such that the RKHS distance between distributionscorresponds exactly to the energy distance. We determine the class ofprobability distributions for which kernels induced by semimetrics arecharacteristic (that is, for which embeddings of the distributions to an RKHSare injective). Finally, we investigate the performance of this family ofkernels in two-sample and independence tests: we show in particular that theenergy distance most commonly employed in statistics is just one member of aparametric family of kernels, and that other choices from this family can yieldmore powerful tests.
arxiv-300-29 | Approximate Dynamic Programming By Minimizing Distributionally Robust Bounds | http://arxiv.org/pdf/1205.1782v2.pdf | author:Marek Petrik category:stat.ML cs.LG published:2012-05-08 summary:Approximate dynamic programming is a popular method for solving large Markovdecision processes. This paper describes a new class of approximate dynamicprogramming (ADP) methods- distributionally robust ADP-that address the curseof dimensionality by minimizing a pessimistic bound on the policy loss. Thisapproach turns ADP into an optimization problem, for which we derive newmathematical program formulations and analyze its properties. DRADP improves onthe theoretical guarantees of existing ADP methods-it guarantees convergenceand L1 norm based error bounds. The empirical evaluation of DRADP shows thatthe theoretical guarantees translate well into good performance on benchmarkproblems.
arxiv-300-30 | Latent Multi-group Membership Graph Model | http://arxiv.org/pdf/1205.4546v1.pdf | author:Myunghwan Kim, Jure Leskovec category:cs.SI physics.soc-ph stat.ML published:2012-05-21 summary:We develop the Latent Multi-group Membership Graph (LMMG) model, a model ofnetworks with rich node feature structure. In the LMMG model, each node belongsto multiple groups and each latent group models the occurrence of links as wellas the node feature structure. The LMMG can be used to summarize the networkstructure, to predict links between the nodes, and to predict missing featuresof a node. We derive efficient inference and learning algorithms and evaluatethe predictive performance of the LMMG on several social and document networkdatasets.
arxiv-300-31 | Streaming Algorithms for Pattern Discovery over Dynamically Changing Event Sequences | http://arxiv.org/pdf/1205.4477v1.pdf | author:Debprakash Patnaik, Naren Ramakrishnan, Srivatsan Laxman, Badrish Chandramouli category:cs.LG cs.DB published:2012-05-21 summary:Discovering frequent episodes over event sequences is an important datamining task. In many applications, events constituting the data sequence arriveas a stream, at furious rates, and recent trends (or frequent episodes) canchange and drift due to the dynamical nature of the underlying event generationprocess. The ability to detect and track such the changing sets of frequentepisodes can be valuable in many application scenarios. Current methods forfrequent episode discovery are typically multipass algorithms, making themunsuitable in the streaming context. In this paper, we propose a new streamingalgorithm for discovering frequent episodes over a window of recent events inthe stream. Our algorithm processes events as they arrive, one batch at a time,while discovering the top frequent episodes over a window consisting of severalbatches in the immediate past. We derive approximation guarantees for ouralgorithm under the condition that frequent episodes are approximatelywell-separated from infrequent ones in every batch of the window. We presentextensive experimental evaluations of our algorithm on both real and syntheticdata. We also present comparisons with baselines and adaptations of streamingalgorithms from itemset mining literature.
arxiv-300-32 | Sparse Signal Recovery in the Presence of Intra-Vector and Inter-Vector Correlation | http://arxiv.org/pdf/1205.4471v1.pdf | author:Bhaskar D. Rao, Zhilin Zhang, Yuzhe Jin category:cs.IT cs.LG math.IT stat.ME stat.ML published:2012-05-20 summary:This work discusses the problem of sparse signal recovery when there iscorrelation among the values of non-zero entries. We examine intra-vectorcorrelation in the context of the block sparse model and inter-vectorcorrelation in the context of the multiple measurement vector model, as well astheir combination. Algorithms based on the sparse Bayesian learning arepresented and the benefits of incorporating correlation at the algorithm levelare discussed. The impact of correlation on the limits of support recovery isalso discussed highlighting the different impact intra-vector and inter-vectorcorrelations have on such limits.
arxiv-300-33 | Dynamic Domain Classification for Fractal Image Compression | http://arxiv.org/pdf/1206.4880v1.pdf | author:K. Revathy, M. Jayamohan category:cs.CV cs.GR published:2012-05-20 summary:Fractal image compression is attractive except for its high encoding timerequirements. The image is encoded as a set of contractive affinetransformations. The image is partitioned into non-overlapping range blocks,and a best matching domain block larger than the range block is identified.There are many attempts on improving the encoding time by reducing the size ofsearch pool for range-domain matching. But these methods are attempting toprepare a static domain pool that remains unchanged throughout the encodingprocess. This paper proposes dynamic preparation of separate domain pool foreach range block. This will result in significant reduction in the encodingtime. The domain pool for a particular range block can be selected based upon aparametric value. Here we use classification based on local fractal dimension.
arxiv-300-34 | Precision-biased Parsing and High-Quality Parse Selection | http://arxiv.org/pdf/1205.4387v1.pdf | author:Yoav Goldberg, Michael Elhadad category:cs.CL published:2012-05-20 summary:We introduce precision-biased parsing: a parsing task which favors precisionover recall by allowing the parser to abstain from decisions deemed uncertain.We focus on dependency-parsing and present an ensemble method which is capableof assigning parents to 84% of the text tokens while being over 96% accurate onthese tokens. We use the precision-biased parsing task to solve the relatedhigh-quality parse-selection task: finding a subset of high-quality (accurate)trees in a large collection of parsed text. We present a method for choosingover a third of the input trees while keeping unlabeled dependency parsingaccuracy of 97% on these trees. We also present a method which is not based onan ensemble but rather on directly predicting the risk associated withindividual parser decisions. In addition to its efficiency, this methoddemonstrates that a parsing system can provide reasonable estimates ofconfidence in its predictions without relying on ensembles or aggregate corpuscounts.
arxiv-300-35 | Complexity Analysis of the Lasso Regularization Path | http://arxiv.org/pdf/1205.0079v2.pdf | author:Julien Mairal, Bin Yu category:stat.ML cs.LG math.OC published:2012-05-01 summary:The regularization path of the Lasso can be shown to be piecewise linear,making it possible to "follow" and explicitly compute the entire path. Weanalyze in this paper this popular strategy, and prove that its worst casecomplexity is exponential in the number of variables. We then oppose thispessimistic result to an (optimistic) approximate analysis: We show that anapproximate path with at most O(1/sqrt(epsilon)) linear segments can always beobtained, where every point on the path is guaranteed to be optimal up to arelative epsilon-duality gap. We complete our theoretical analysis with apractical algorithm to compute these approximate paths.
arxiv-300-36 | Segmentation Similarity and Agreement | http://arxiv.org/pdf/1204.2847v2.pdf | author:Chris Fournier, Diana Inkpen category:cs.CL published:2012-04-12 summary:We propose a new segmentation evaluation metric, called segmentationsimilarity (S), that quantifies the similarity between two segmentations as theproportion of boundaries that are not transformed when comparing them usingedit distance, essentially using edit distance as a penalty function andscaling penalties by segmentation size. We propose several adaptedinter-annotator agreement coefficients which use S that are suitable forsegmentation. We show that S is configurable enough to suit a wide variety ofsegmentation evaluations, and is an improvement upon the state of the art. Wealso propose using inter-annotator agreement coefficients to evaluate automaticsegmenters in terms of human performance.
arxiv-300-37 | From Exact Learning to Computing Boolean Functions and Back Again | http://arxiv.org/pdf/1205.4349v1.pdf | author:Sergiu Goschin category:cs.LG cs.DM published:2012-05-19 summary:The goal of the paper is to relate complexity measures associated with theevaluation of Boolean functions (certificate complexity, decision treecomplexity) and learning dimensions used to characterize exact learning(teaching dimension, extended teaching dimension). The high level motivation isto discover non-trivial relations between exact learning of an unknown conceptand testing whether an unknown concept is part of a concept class or not.Concretely, the goal is to provide lower and upper bounds of complexitymeasures for one problem type in terms of the other.
arxiv-300-38 | ProPPA: A Fast Algorithm for $\ell_1$ Minimization and Low-Rank Matrix Completion | http://arxiv.org/pdf/1205.0088v2.pdf | author:Ranch Y. Q. Lai, Pong C. Yuen category:cs.LG math.OC published:2012-05-01 summary:We propose a Projected Proximal Point Algorithm (ProPPA) for solving a classof optimization problems. The algorithm iteratively computes the proximal pointof the last estimated solution projected into an affine space which itself isparallel and approaching to the feasible set. We provide convergence analysistheoretically supporting the general algorithm, and then apply it for solving$\ell_1$-minimization problems and the matrix completion problem. Theseproblems arise in many applications including machine learning, image andsignal processing. We compare our algorithm with the existing state-of-the-artalgorithms. Experimental results on solving these problems show that ouralgorithm is very efficient and competitive.
arxiv-300-39 | Task-specific Word-Clustering for Part-of-Speech Tagging | http://arxiv.org/pdf/1205.4298v1.pdf | author:Yoav Goldberg category:cs.CL published:2012-05-19 summary:While the use of cluster features became ubiquitous in core NLP tasks, mostcluster features in NLP are based on distributional similarity. We propose anew type of clustering criteria, specific to the task of part-of-speechtagging. Instead of distributional similarity, these clusters are based on thebeha vior of a baseline tagger when applied to a large corpus. These clusterfeatures provide similar gains in accuracy to those achieved bydistributional-similarity derived clusters. Using both types of clusterfeatures together further improve tagging accuracies. We show that the methodis effective for both the in-domain and out-of-domain scenarios for English,and for French, German and Italian. The effect is larger for out-of-domaintext.
arxiv-300-40 | Efficient Methods for Unsupervised Learning of Probabilistic Models | http://arxiv.org/pdf/1205.4295v1.pdf | author:Jascha Sohl-Dickstein category:cs.LG cs.AI cs.IT cs.NE math.IT published:2012-05-19 summary:In this thesis I develop a variety of techniques to train, evaluate, andsample from intractable and high dimensional probabilistic models. Abstractexceeds arXiv space limitations -- see PDF.
arxiv-300-41 | Two New Algorithms for Solving Covariance Graphical Lasso Based on Coordinate Descent and ECM | http://arxiv.org/pdf/1205.4120v1.pdf | author:Hao Wang category:stat.CO stat.ML published:2012-05-18 summary:Covariance graphical lasso applies a lasso penalty on the elements of thecovariance matrix. This method is useful because it not only produces sparseestimation of covariance matrix but also discovers marginal independencestructures by generating zeros in the covariance matrix. We propose and exploretwo new algorithms for solving the covariance graphical lasso problem. Our newalgorithms are based on coordinate descent and ECM. We show that these twoalgorithms are more attractive than the only existing competing algorithm ofBien and Tibshirani (2011) in terms of simplicity, speed and stability. We alsodiscuss convergence properties of our algorithms.
arxiv-300-42 | Adaptive experimental design for one-qubit state estimation with finite data based on a statistical update criterion | http://arxiv.org/pdf/1203.3391v2.pdf | author:Takanori Sugiyama, Peter S. Turner, Mio Murao category:quant-ph math.ST stat.ML stat.TH published:2012-03-15 summary:We consider 1-qubit mixed quantum state estimation by adaptively updatingmeasurements according to previously obtained outcomes and measurementsettings. Updates are determined by the average-variance-optimality(A-optimality) criterion, known in the classical theory of experimental designand applied here to quantum state estimation. In general, A-optimization is anonlinear minimization problem; however, we find an analytic solution for1-qubit state estimation using projective measurements, reducing computationaleffort. We compare numerically two adaptive and two nonadaptive schemes forfinite data sets and show that the A-optimality criterion gives more preciseestimates than standard quantum tomography.
arxiv-300-43 | Optimal Weights Mixed Filter for Removing Mixture of Gaussian and Impulse Noises | http://arxiv.org/pdf/1205.3999v1.pdf | author:Qiyu Jin, Ion Grama, Quansheng Liu category:cs.CV published:2012-05-17 summary:According to the character of Gaussian, we modify the Rank-Ordered AbsoluteDifferences (ROAD) to Rank-Ordered Absolute Differences of mixture of Gaussianand impulse noises (ROADG). It will be more effective to detect impulse noisewhen the impulse is mixed with Gaussian noise. Combining rightly the ROADG withOptimal Weights Filter (OWF), we obtain a new method to deal with the mixednoise, called Optimal Weights Mixed Filter (OWMF). The simulation results showthat the method is effective to remove the mixed noise.
arxiv-300-44 | Free Energy and the Generalized Optimality Equations for Sequential Decision Making | http://arxiv.org/pdf/1205.3997v1.pdf | author:Pedro A. Ortega, Daniel A. Braun category:stat.ML cs.AI cs.GT cs.SY published:2012-05-17 summary:The free energy functional has recently been proposed as a variationalprinciple for bounded rational decision-making, since it instantiates a naturaltrade-off between utility gains and information processing costs that can beaxiomatically derived. Here we apply the free energy principle to generaldecision trees that include both adversarial and stochastic environments. Wederive generalized sequential optimality equations that not only include theBellman optimality equations as a limit case, but also lead to well-knowndecision-rules such as Expectimax, Minimax and Expectiminimax. We show howthese decision-rules can be derived from a single free energy principle thatassigns a resource parameter to each node in the decision tree. These resourceparameters express a concrete computational cost that can be measured as theamount of samples that are needed from the distribution that belongs to eachnode. The free energy principle therefore provides the normative basis forgeneralized optimality equations that account for both adversarial andstochastic environments.
arxiv-300-45 | Neural Networks for Handwritten English Alphabet Recognition | http://arxiv.org/pdf/1205.3966v1.pdf | author:Yusuf Perwej, Ashish Chaturvedi category:cs.AI cs.CV published:2012-05-17 summary:This paper demonstrates the use of neural networks for developing a systemthat can recognize hand-written English alphabets. In this system, each Englishalphabet is represented by binary values that are used as input to a simplefeature extraction system, whose output is fed to our neural network system.
arxiv-300-46 | Background subtraction based on Local Shape | http://arxiv.org/pdf/1204.6326v2.pdf | author:Jean-Philippe Jodoin, Guillaume-Alexandre Bilodeau, Nicolas Saunier category:cs.CV published:2012-04-27 summary:We present a novel approach to background subtraction that is based on thelocal shape of small image regions. In our approach, an image region centeredon a pixel is mod-eled using the local self-similarity descriptor. We aim atobtaining a reliable change detection based on local shape change in an imagewhen foreground objects are moving. The method first builds a background modeland compares the local self-similarities between the background model and thesubsequent frames to distinguish background and foreground objects.Post-processing is then used to refine the boundaries of moving objects.Results show that this approach is promising as the foregrounds obtained arecom-plete, although they often include shadows.
arxiv-300-47 | Normalized Maximum Likelihood Coding for Exponential Family with Its Applications to Optimal Clustering | http://arxiv.org/pdf/1205.3549v2.pdf | author:So Hirai, Kenji Yamanishi category:cs.LG published:2012-05-16 summary:We are concerned with the issue of how to calculate the normalized maximumlikelihood (NML) code-length. There is a problem that the normalization term ofthe NML code-length may diverge when it is continuous and unbounded and astraightforward computation of it is highly expensive when the data domain isfinite . In previous works it has been investigated how to calculate the NMLcode-length for specific types of distributions. We first propose a generalmethod for computing the NML code-length for the exponential family. Then wespecifically focus on Gaussian mixture model (GMM), and propose a new efficientmethod for computing the NML to them. We develop it by generalizing Rissanen'sre-normalizing technique. Then we apply this method to the clustering issue, inwhich a clustering structure is modeled using a GMM, and the main task is toestimate the optimal number of clusters on the basis of the NML code-length. Wedemonstrate using artificial data sets the superiority of the NML-basedclustering over other criteria such as AIC, BIC in terms of the data sizerequired for high accuracy rate to be achieved.
arxiv-300-48 | Paraiso : An Automated Tuning Framework for Explicit Solvers of Partial Differential Equations | http://arxiv.org/pdf/1204.4779v2.pdf | author:Takayuki Muranushi category:astro-ph.IM cs.DC cs.NE published:2012-04-21 summary:We propose Paraiso, a domain specific language embedded in functionalprogramming language Haskell, for automated tuning of explicit solvers ofpartial differential equations (PDEs) on GPUs as well as multicore CPUs. InParaiso, one can describe PDE solving algorithms succinctly using tensorequations notation. Hydrodynamic properties, interpolation methods and otherbuilding blocks are described in abstract, modular, re-usable and combinableforms, which lets us generate versatile solvers from little set of Paraisosource codes. We demonstrate Paraiso by implementing a compressive hydrodynamics solver. Asingle source code less than 500 lines can be used to generate solvers ofarbitrary dimensions, for both multicore CPUs and GPUs. We demonstrate bothmanual annotation based tuning and evolutionary computing based automatedtuning of the program.
arxiv-300-49 | The ideal of the trifocal variety | http://arxiv.org/pdf/1205.3776v1.pdf | author:Chris Aholt, Luke Oeding category:math.AG cs.CV published:2012-05-16 summary:Techniques from representation theory, symbolic computational algebra, andnumerical algebraic geometry are used to find the minimal generators of theideal of the trifocal variety. An effective test for determining whether agiven tensor is a trifocal tensor is also given.
arxiv-300-50 | Efficient Topology-Controlled Sampling of Implicit Shapes | http://arxiv.org/pdf/1205.3766v1.pdf | author:Jason Chang, John W. Fisher III category:cs.CV published:2012-05-16 summary:Sampling from distributions of implicitly defined shapes enables analysis ofvarious energy functionals used for image segmentation. Recent work describes acomputationally efficient Metropolis-Hastings method for accomplishing thistask. Here, we extend that framework so that samples are accepted at everyiteration of the sampler, achieving an order of magnitude speed up inconvergence. Additionally, we show how to incorporate topological constraints.
arxiv-300-51 | Distribution of the search of evolutionary product unit neural networks for classification | http://arxiv.org/pdf/1205.3336v1.pdf | author:A. J. TallÃ³n-Ballesteros, P. A. GutiÃ©rrez-PeÃ±a, C. HervÃ¡s-MartÃ­nez category:cs.NE cs.AI cs.CV published:2012-05-15 summary:This paper deals with the distributed processing in the search for an optimumclassification model using evolutionary product unit neural networks. For thisdistributed search we used a cluster of computers. Our objective is to obtain amore efficient design than those net architectures which do not use adistributed process and which thus result in simpler designs. In order to getthe best classification models we use evolutionary algorithms to train anddesign neural networks, which require a very time consuming computation. Thereasons behind the need for this distribution are various. It is complicated totrain this type of nets because of the difficulty entailed in determining theirarchitecture due to the complex error surface. On the other hand, the use ofevolutionary algorithms involves running a great number of tests with differentseeds and parameters, thus resulting in a high computational cost
arxiv-300-52 | Arabic Language Learning Assisted by Computer, based on Automatic Speech Recognition | http://arxiv.org/pdf/1205.3316v1.pdf | author:Naim Terbeh, Mounir Zrigui category:cs.CL published:2012-05-15 summary:This work consists of creating a system of the Computer Assisted LanguageLearning (CALL) based on a system of Automatic Speech Recognition (ASR) for theArabic language using the tool CMU Sphinx3 [1], based on the approach of HMM.To this work, we have constructed a corpus of six hours of speech recordingswith a number of nine speakers. we find in the robustness to noise a groundsfor the choice of the HMM approach [2]. the results achieved are encouragingsince our corpus is made by only nine speakers, but they are always reasonsthat open the door for other improvement works.
arxiv-300-53 | A Comparative Study of Collaborative Filtering Algorithms | http://arxiv.org/pdf/1205.3193v1.pdf | author:Joonseok Lee, Mingxuan Sun, Guy Lebanon category:cs.IR stat.ML I.2.6; H.2.8 published:2012-05-14 summary:Collaborative filtering is a rapidly advancing research area. Every yearseveral new techniques are proposed and yet it is not clear which of thetechniques work best and under what conditions. In this paper we conduct astudy comparing several collaborative filtering techniques -- both classic andrecent state-of-the-art -- in a variety of experimental contexts. Specifically,we report conclusions controlling for number of items, number of users,sparsity level, performance criteria, and computational complexity. Ourconclusions identify what algorithms work well and in what conditions, andcontribute to both industrial deployment collaborative filtering algorithms andto the research community.
arxiv-300-54 | A Model-Driven Probabilistic Parser Generator | http://arxiv.org/pdf/1205.3183v1.pdf | author:Luis Quesada, Fernando Berzal, Francisco J. Cortijo category:cs.CL published:2012-05-14 summary:Existing probabilistic scanners and parsers impose hard constraints on theway lexical and syntactic ambiguities can be resolved. Furthermore, traditionalgrammar-based parsing tools are limited in the mechanisms they allow for takingcontext into account. In this paper, we propose a model-driven tool that allowsfor statistical language models with arbitrary probability estimators. Our workon model-driven probabilistic parsing is built on top of ModelCC, a model-basedparser generator, and enables the probabilistic interpretation and resolutionof anaphoric, cataphoric, and recursive references in the disambiguation ofabstract syntax graphs. In order to prove the expression power of ModelCC, wedescribe the design of a general-purpose natural language parser.
arxiv-300-55 | Multiple Identifications in Multi-Armed Bandits | http://arxiv.org/pdf/1205.3181v1.pdf | author:SÃ©bastien Bubeck, Tengyao Wang, Nitin Viswanathan category:cs.LG stat.ML published:2012-05-14 summary:We study the problem of identifying the top $m$ arms in a multi-armed banditgame. Our proposed solution relies on a new algorithm based on successiverejects of the seemingly bad arms, and successive accepts of the good ones.This algorithmic contribution allows to tackle other multiple identificationssettings that were previously out of reach. In particular we show that thisidea of successive accepts and rejects applies to the multi-bandit best armidentification problem.
arxiv-300-56 | b-Bit Minwise Hashing in Practice: Large-Scale Batch and Online Learning and Using GPUs for Fast Preprocessing with Simple Hash Functions | http://arxiv.org/pdf/1205.2958v1.pdf | author:Ping Li, Anshumali Shrivastava, Arnd Christian Konig category:cs.IR cs.DB cs.LG published:2012-05-14 summary:In this paper, we study several critical issues which must be tackled beforeone can apply b-bit minwise hashing to the volumes of data often usedindustrial applications, especially in the context of search. 1. (b-bit) Minwise hashing requires an expensive preprocessing step thatcomputes k (e.g., 500) minimal values after applying the correspondingpermutations for each data vector. We developed a parallelization scheme usingGPUs and observed that the preprocessing time can be reduced by a factor of20-80 and becomes substantially smaller than the data loading time. 2. One major advantage of b-bit minwise hashing is that it can substantiallyreduce the amount of memory required for batch learning. However, as onlinealgorithms become increasingly popular for large-scale learning in the contextof search, it is not clear if b-bit minwise yields significant improvements forthem. This paper demonstrates that $b$-bit minwise hashing provides aneffective data size/dimension reduction scheme and hence it can dramaticallyreduce the data loading time for each epoch of the online training process.This is significant because online learning often requires many (e.g., 10 to100) epochs to reach a sufficient accuracy. 3. Another critical issue is that for very large data sets it becomesimpossible to store a (fully) random permutation matrix, due to its spacerequirements. Our paper is the first study to demonstrate that $b$-bit minwisehashing implemented using simple hash functions, e.g., the 2-universal (2U) and4-universal (4U) hash families, can produce very similar learning results asusing fully random permutations. Experiments on datasets of up to 200GB arepresented.
arxiv-300-57 | Density Sensitive Hashing | http://arxiv.org/pdf/1205.2930v1.pdf | author:Yue Lin, Deng Cai, Cheng Li category:cs.IR cs.LG published:2012-05-14 summary:Nearest neighbors search is a fundamental problem in various research fieldslike machine learning, data mining and pattern recognition. Recently,hashing-based approaches, e.g., Locality Sensitive Hashing (LSH), are proved tobe effective for scalable high dimensional nearest neighbors search. Manyhashing algorithms found their theoretic root in random projection. Since thesealgorithms generate the hash tables (projections) randomly, a large number ofhash tables (i.e., long codewords) are required in order to achieve both highprecision and recall. To address this limitation, we propose a novel hashingalgorithm called {\em Density Sensitive Hashing} (DSH) in this paper. DSH canbe regarded as an extension of LSH. By exploring the geometric structure of thedata, DSH avoids the purely random projections selection and uses thoseprojective functions which best agree with the distribution of the data.Extensive experimental results on real-world data sets have shown that theproposed method achieves better performance compared to the state-of-the-arthashing approaches.
arxiv-300-58 | Texture Analysis And Characterization Using Probability Fractal Descriptors | http://arxiv.org/pdf/1205.2821v1.pdf | author:J. B. Florindo, O. M. Bruno category:cs.CV published:2012-05-13 summary:A gray-level image texture descriptors based on fractal dimension estimationis proposed in this work. The proposed method estimates the fractal dimensionusing probability (Voss) method. The descriptors are computed applying amultiscale transform to the fractal dimension curves of the texture image. Theproposed texture descriptor method is evaluated in a classification task ofwell known benchmark texture datasets. The results show the great performanceof the proposed method as a tool for texture images analysis andcharacterization.
arxiv-300-59 | Diffusion Adaptation Strategies for Distributed Optimization and Learning over Networks | http://arxiv.org/pdf/1111.0034v3.pdf | author:Jianshu Chen, Ali H. Sayed category:math.OC cs.IT cs.LG cs.SI math.IT physics.soc-ph published:2011-10-31 summary:We propose an adaptive diffusion mechanism to optimize a global cost functionin a distributed manner over a network of nodes. The cost function is assumedto consist of a collection of individual components. Diffusion adaptationallows the nodes to cooperate and diffuse information in real-time; it alsohelps alleviate the effects of stochastic gradient noise and measurement noisethrough a continuous learning process. We analyze the mean-square-errorperformance of the algorithm in some detail, including its transient andsteady-state behavior. We also apply the diffusion algorithm to two problems:distributed estimation with sparse parameters and distributed localization.Compared to well-studied incremental methods, diffusion methods do not requirethe use of a cyclic path over the nodes and are robust to node and linkfailure. Diffusion methods also endow networks with adaptation abilities thatenable the individual nodes to continue learning even when the cost functionchanges with time. Examples involving such dynamic cost functions with movingtargets are common in the context of biological networks.
arxiv-300-60 | Forecasting of Indian Rupee (INR) / US Dollar (USD) Currency Exchange Rate Using Artificial Neural Network | http://arxiv.org/pdf/1205.2797v1.pdf | author:Yusuf Perwej, Asif Perwej category:cs.NE published:2012-05-12 summary:A large part of the workforce, and growing every day, is originally fromIndia. India one of the second largest populations in the world, they have alot to offer in terms of jobs. The sheer number of IT workers makes them aformidable travelling force as well, easily picking up employment in Englishspeaking countries. The beginning of the economic crises since 2008 September,many Indians have return homeland, and this has had a substantial impression onthe Indian Rupee (INR) as liken to the US Dollar (USD). We are usingnumerational knowledge based techniques for forecasting has been proved highlysuccessful in present time. The purpose of this paper is to examine the effectsof several important neural network factors on model fitting and forecastingthe behaviours. In this paper, Artificial Neural Network has successfully beenused for exchange rate forecasting. This paper examines the effects of thenumber of inputs and hidden nodes and the size of the training sample on thein-sample and out-of-sample performance. The Indian Rupee (INR) / US Dollar(USD) is used for detailed examinations. The number of input nodes has agreater impact on performance than the number of hidden nodes, while a largenumber of observations do reduce forecast errors.
arxiv-300-61 | Robust Head Pose Estimation Using Contourlet Transform | http://arxiv.org/pdf/1204.5431v2.pdf | author:Mohammad Tofighi, Hashem Kalbkhani, Mahrokh G. Shayesteh, Mehdi Ghasemzadeh category:cs.CV published:2012-04-24 summary:Estimating pose of the head is an important preprocessing step in manypattern recognition and computer vision systems such as face recognition. Sincethe performance of the face recognition systems is greatly affected by theposes of the face, how to estimate the accurate pose of the face in human faceimage is still a challenging problem. In this paper, we represent a novelmethod for head pose estimation. To enhance the efficiency of the estimation weuse contourlet transform for feature extraction. Contourlet transform ismulti-resolution, multi-direction transform. In order to reduce the featurespace dimension and obtain appropriate features we use LDA (Linear DiscriminantAnalysis) and PCA (Principal Component Analysis) to remove ineffcient features.Then, we apply different classifiers such as k-nearest neighborhood (knn) andminimum distance. We use the public available FERET database to evaluate theperformance of proposed method. Simulation results indicate the superiorrobustness of the proposed method.
arxiv-300-62 | Are visual dictionaries generalizable? | http://arxiv.org/pdf/1205.2663v1.pdf | author:Otavio A. B. Penatti, Eduardo Valle, Ricardo da S. Torres category:cs.CV published:2012-05-11 summary:Mid-level features based on visual dictionaries are today a cornerstone ofsystems for classification and retrieval of images. Those state-of-the-artrepresentations depend crucially on the choice of a codebook (visualdictionary), which is usually derived from the dataset. In general-purpose,dynamic image collections (e.g., the Web), one cannot have the entirecollection in order to extract a representative dictionary. However, based onthe hypothesis that the dictionary reflects only the diversity of low-levelappearances and does not capture semantics, we argue that a dictionary based ona small subset of the data, or even on an entirely different dataset, is ableto produce a good representation, provided that the chosen images span adiverse enough portion of the low-level feature space. Our experiments confirmthat hypothesis, opening the opportunity to greatly alleviate the burden ingenerating the codebook, and confirming the feasibility of employing visualdictionaries in large-scale dynamic environments.
arxiv-300-63 | Penalty Decomposition Methods for $L0$-Norm Minimization | http://arxiv.org/pdf/1008.5372v2.pdf | author:Zhaosong Lu, Yong Zhang category:math.OC cs.CV cs.IT cs.LG cs.NA math.IT stat.ME published:2010-08-31 summary:In this paper we consider general l0-norm minimization problems, that is, theproblems with l0-norm appearing in either objective function or constraint. Inparticular, we first reformulate the l0-norm constrained problem as anequivalent rank minimization problem and then apply the penalty decomposition(PD) method proposed in [33] to solve the latter problem. By utilizing thespecial structures, we then transform all matrix operations of this method tovector operations and obtain a PD method that only involves vector operations.Under some suitable assumptions, we establish that any accumulation point ofthe sequence generated by the PD method satisfies a first-order optimalitycondition that is generally stronger than one natural optimality condition. Wefurther extend the PD method to solve the problem with the l0-norm appearing inobjective function. Finally, we test the performance of our PD methods byapplying them to compressed sensing, sparse logistic regression and sparseinverse covariance selection. The computational results demonstrate that ourmethods generally outperform the existing methods in terms of solution qualityand/or speed.
arxiv-300-64 | Visualization techniques for data mining of Latur district satellite imagery | http://arxiv.org/pdf/1104.3571v2.pdf | author:B. G. Kodge, P. S. Hiremath category:cs.CE cs.CV published:2011-03-28 summary:This study presents a new visualization tool for classification of satelliteimagery. Visualization of feature space allows exploration of patterns in theimage data and insight into the classification process and related uncertainty.Visual Data Mining provides added value to image classifications as the usercan be involved in the classification process providing increased confidence inand understanding of the results. In this study, we present a prototypevisualization tool for visual data mining (VDM) of satellite imagery. Thevisualization tool is showcased in a classification study of highresolutionimageries of Latur district in Maharashtra state of India.
arxiv-300-65 | Hajj and Umrah Event Recognition Datasets | http://arxiv.org/pdf/1205.2345v1.pdf | author:Hossam Zawbaa, Salah A. Aly category:cs.CV cs.CY published:2012-05-10 summary:In this note, new Hajj and Umrah Event Recognition datasets (HUER) arepresented. The demonstrated datasets are based on videos and images takenduring 2011-2012 Hajj and Umrah seasons. HUER is the first collection ofdatasets covering the six types of Hajj and Umrah ritual events (rotating inTawaf around Kabaa, performing Sa'y between Safa and Marwa, standing on themount of Arafat, staying overnight in Muzdalifah, staying two or three days inMina, and throwing Jamarat). The HUER datasets also contain video and imagedatabases for nine types of human actions during Hajj and Umrah (walking,drinking from Zamzam water, sleeping, smiling, eating, praying, sitting,shaving hairs and ablutions, reading the holy Quran and making duaa). Thespatial resolutions are 1280 x 720 pixels for images and 640 x 480 pixels forvideos and have lengths of 20 seconds in average with 30 frame per secondrates.
arxiv-300-66 | A Discussion on Parallelization Schemes for Stochastic Vector Quantization Algorithms | http://arxiv.org/pdf/1205.2282v1.pdf | author:Matthieu Durut, BenoÃ®t Patra, Fabrice Rossi category:stat.ML cs.DC cs.LG published:2012-05-10 summary:This paper studies parallelization schemes for stochastic Vector Quantizationalgorithms in order to obtain time speed-ups using distributed resources. Weshow that the most intuitive parallelization scheme does not lead to betterperformances than the sequential algorithm. Another distributed scheme istherefore introduced which obtains the expected speed-ups. Then, it is improvedto fit implementation on distributed architectures where communications areslow and inter-machines synchronization too costly. The schemes are tested withsimulated distributed architectures and, for the last one, with MicrosoftWindows Azure platform obtaining speed-ups up to 32 Virtual Machines.
arxiv-300-67 | Discrimination of English to other Indian languages (Kannada and Hindi) for OCR system | http://arxiv.org/pdf/1205.2164v1.pdf | author:Ankit Kumar, Tushar Patnaik, Vivek Kr Verma category:cs.CV published:2012-05-10 summary:India is a multilingual multi-script country. In every state of India thereare two languages one is state local language and the other is English. Forexample in Andhra Pradesh, a state in India, the document may contain textwords in English and Telugu script. For Optical Character Recognition (OCR) ofsuch a bilingual document, it is necessary to identify the script beforefeeding the text words to the OCRs of individual scripts. In this paper, we areintroducing a simple and efficient technique of script identification forKannada, English and Hindi text words of a printed document. The proposedapproach is based on the horizontal and vertical projection profile for thediscrimination of the three scripts. The feature extraction is done based onthe horizontal projection profile of each text words. We analysed 700 differentwords of Kannada, English and Hindi in order to extract the discriminationfeatures and for the development of knowledge base. We use the horizontalprojection profile of each text word and based on the horizontal projectionprofile we extract the appropriate features. The proposed system is tested on100 different document images containing more than 1000 text words of eachscript and a classification rate of 98.25%, 99.25% and 98.87% is achieved forKannada, English and Hindi respectively.
arxiv-300-68 | A Converged Algorithm for Tikhonov Regularized Nonnegative Matrix Factorization with Automatic Regularization Parameters Determination | http://arxiv.org/pdf/1205.2151v1.pdf | author:Andri Mirzal category:cs.LG published:2012-05-10 summary:We present a converged algorithm for Tikhonov regularized nonnegative matrixfactorization (NMF). We specially choose this regularization because it isknown that Tikhonov regularized least square (LS) is the more preferable formin solving linear inverse problems than the conventional LS. Because an NMFproblem can be decomposed into LS subproblems, it can be expected that Tikhonovregularized NMF will be the more appropriate approach in solving NMF problems.The algorithm is derived using additive update rules which have been shown tohave convergence guarantee. We equip the algorithm with a mechanism toautomatically determine the regularization parameters based on the L-curve, awell-known concept in the inverse problems community, but is rather unknown inthe NMF research. The introduction of this algorithm thus solves two inherentproblems in Tikhonov regularized NMF algorithm research, i.e., convergenceguarantee and regularization parameters determination.
arxiv-300-69 | Reduced Rank Vector Generalized Linear Models for Feature Extraction | http://arxiv.org/pdf/1007.3098v3.pdf | author:Yiyuan She category:stat.ML published:2010-07-19 summary:Supervised linear feature extraction can be achieved by fitting a reducedrank multivariate model. This paper studies rank penalized and rank constrainedvector generalized linear models. From the perspective of thresholding rules,we build a framework for fitting singular value penalized models and use it forfeature extraction. Through solving the rank constraint form of the problem, wepropose progressive feature space reduction for fast computation in highdimensions with little performance loss. A novel projective cross-validation isproposed for parameter tuning in such nonconvex setups. Real data applicationsare given to show the power of the methodology in supervised dimensionreduction and feature extraction.
arxiv-300-70 | Spatial Multiresolution Cluster Detection Method | http://arxiv.org/pdf/1205.2106v1.pdf | author:Lingsong Zhang, Zhengyuan Zhu category:stat.ME stat.CO stat.ML 62H11 published:2012-05-09 summary:A novel multi-resolution cluster detection (MCD) method is proposed toidentify irregularly shaped clusters in space. Multi-scale test statistic on asingle cell is derived based on likelihood ratio statistic for Bernoullisequence, Poisson sequence and Normal sequence. A neighborhood variabilitymeasure is defined to select the optimal test threshold. The MCD method iscompared with single scale testing methods controlling for false discovery rateand the spatial scan statistics using simulation and f-MRI data. The MCD methodis shown to be more effective for discovering irregularly shaped clusters, andthe implementation of this method does not require heavy computation, making itsuitable for cluster detection for large spatial data.
arxiv-300-71 | On the Identifiability of the Post-Nonlinear Causal Model | http://arxiv.org/pdf/1205.2599v1.pdf | author:Kun Zhang, Aapo Hyvarinen category:stat.ML cs.LG published:2012-05-09 summary:By taking into account the nonlinear effect of the cause, the inner noiseeffect, and the measurement distortion effect in the observed variables, thepost-nonlinear (PNL) causal model has demonstrated its excellent performance indistinguishing the cause from effect. However, its identifiability has not beenproperly addressed, and how to apply it in the case of more than two variablesis also a problem. In this paper, we conduct a systematic investigation on itsidentifiability in the two-variable case. We show that this model isidentifiable in most cases; by enumerating all possible situations in which themodel is not identifiable, we provide sufficient conditions for itsidentifiability. Simulations are given to support the theoretical results.Moreover, in the case of more than two variables, we show that the whole causalstructure can be found by applying the PNL causal model to each structure inthe Markov equivalent class and testing if the disturbance is independent ofthe direct causes for each variable. In this way the exhaustive search over allpossible causal structures is avoided.
arxiv-300-72 | A Uniqueness Theorem for Clustering | http://arxiv.org/pdf/1205.2600v1.pdf | author:Reza Bosagh Zadeh, Shai Ben-David category:cs.LG published:2012-05-09 summary:Despite the widespread use of Clustering, there is distressingly littlegeneral theory of clustering available. Questions like "What distinguishes aclustering of data from other data partitioning?", "Are there any principlesgoverning all clustering paradigms?", "How should a user choose an appropriateclustering algorithm for a particular task?", etc. are almost completelyunanswered by the existing body of clustering literature. We consider anaxiomatic approach to the theory of Clustering. We adopt the framework ofKleinberg, [Kle03]. By relaxing one of Kleinberg's clustering axioms, wesidestep his impossibility result and arrive at a consistent set of axioms. Wesuggest to extend these axioms, aiming to provide an axiomatic taxonomy ofclustering paradigms. Such a taxonomy should provide users some guidanceconcerning the choice of the appropriate clustering paradigm for a given task.The main result of this paper is a set of abstract properties that characterizethe Single-Linkage clustering function. This characterization result providesnew insight into the properties of desired data groupings that makeSingle-Linkage the appropriate choice. We conclude by considering a taxonomy ofclustering functions based on abstract properties that each satisfies.
arxiv-300-73 | The Entire Quantile Path of a Risk-Agnostic SVM Classifier | http://arxiv.org/pdf/1205.2602v1.pdf | author:Jin Yu, S. V. N. Vishwanatan, Jian Zhang category:cs.LG published:2012-05-09 summary:A quantile binary classifier uses the rule: Classify x as +1 if P(Y = 1X =x) >= t, and as -1 otherwise, for a fixed quantile parameter t {[0, 1]. It hasbeen shown that Support Vector Machines (SVMs) in the limit are quantileclassifiers with t = 1/2 . In this paper, we show that by using asymmetric costof misclassification SVMs can be appropriately extended to recover, in thelimit, the quantile binary classifier for any t. We then present a principledalgorithm to solve the extended SVM classifier for all values of tsimultaneously. This has two implications: First, one can recover the entireconditional distribution P(Y = 1X = x) = t for t {[0, 1]. Second, we can builda risk-agnostic SVM classifier where the cost of misclassification need not beknown apriori. Preliminary numerical experiments show the effectiveness of theproposed algorithm.
arxiv-300-74 | The Infinite Latent Events Model | http://arxiv.org/pdf/1205.2604v1.pdf | author:David Wingate, Noah Goodman, Daniel Roy, Joshua Tenenbaum category:stat.ML cs.LG published:2012-05-09 summary:We present the Infinite Latent Events Model, a nonparametric hierarchicalBayesian distribution over infinite dimensional Dynamic Bayesian Networks withbinary state representations and noisy-OR-like transitions. The distributioncan be used to learn structure in discrete timeseries data by simultaneouslyinferring a set of latent events, which events fired at each timestep, and howthose events are causally linked. We illustrate the model on a soundfactorization task, a network topology identification task, and a video gametask.
arxiv-300-75 | Herding Dynamic Weights for Partially Observed Random Field Models | http://arxiv.org/pdf/1205.2605v1.pdf | author:Max Welling category:cs.LG stat.ML published:2012-05-09 summary:Learning the parameters of a (potentially partially observable) random fieldmodel is intractable in general. Instead of focussing on a single optimalparameter value we propose to treat parameters as dynamical quantities. Weintroduce an algorithm to generate complex dynamics for parameters and (bothvisible and hidden) state vectors. We show that under certain conditionsaverages computed over trajectories of the proposed dynamical system convergeto averages computed over the data. Our "herding dynamics" does not requireexpensive operations such as exponentiation and is fully deterministic.
arxiv-300-76 | Exploring compact reinforcement-learning representations with linear regression | http://arxiv.org/pdf/1205.2606v1.pdf | author:Thomas J. Walsh, Istvan Szita, Carlos Diuk, Michael L. Littman category:cs.LG cs.AI published:2012-05-09 summary:This paper presents a new algorithm for online linear regression whoseefficiency guarantees satisfy the requirements of the KWIK (Knows What ItKnows) framework. The algorithm improves on the complexity bounds of thecurrent state-of-the-art procedure in this setting. We explore severalapplications of this algorithm for learning compact reinforcement-learningrepresentations. We show that KWIK linear regression can be used to learn thereward function of a factored MDP and the probabilities of action outcomes inStochastic STRIPS and Object Oriented MDPs, none of which have been proven tobe efficiently learnable in the RL setting before. We also combine KWIK linearregression with other KWIK learners to learn larger portions of these models,including experiments on learning factored MDP transition and reward functionstogether.
arxiv-300-77 | Temporal-Difference Networks for Dynamical Systems with Continuous Observations and Actions | http://arxiv.org/pdf/1205.2608v1.pdf | author:Christopher M. Vigorito category:cs.LG stat.ML published:2012-05-09 summary:Temporal-difference (TD) networks are a class of predictive staterepresentations that use well-established TD methods to learn models ofpartially observable dynamical systems. Previous research with TD networks hasdealt only with dynamical systems with finite sets of observations and actions.We present an algorithm for learning TD network representations of dynamicalsystems with continuous observations and actions. Our results show that thealgorithm is capable of learning accurate and robust models of several noisycontinuous dynamical systems. The algorithm presented here is the first fullyincremental method for learning a predictive representation of a continuousdynamical system.
arxiv-300-78 | Which Spatial Partition Trees are Adaptive to Intrinsic Dimension? | http://arxiv.org/pdf/1205.2609v1.pdf | author:Nakul Verma, Samory Kpotufe, Sanjoy Dasgupta category:stat.ML cs.LG published:2012-05-09 summary:Recent theory work has found that a special type of spatial partition tree -called a random projection tree - is adaptive to the intrinsic dimension of thedata from which it is built. Here we examine this same question, with acombination of theory and experiments, for a broader class of trees thatincludes k-d trees, dyadic trees, and PCA trees. Our motivation is to get afeel for (i) the kind of intrinsic low dimensional structure that can beempirically verified, (ii) the extent to which a spatial partition can exploitsuch structure, and (iii) the implications for standard statistical tasks suchas regression, vector quantization, and nearest neighbor search.
arxiv-300-79 | Probabilistic Structured Predictors | http://arxiv.org/pdf/1205.2610v1.pdf | author:Shankar Vembu, Thomas Gartner, Mario Boley category:cs.LG published:2012-05-09 summary:We consider MAP estimators for structured prediction with exponential familymodels. In particular, we concentrate on the case that efficient algorithms foruniform sampling from the output space exist. We show that under thisassumption (i) exact computation of the partition function remains a hardproblem, and (ii) the partition function and the gradient of the log partitionfunction can be approximated efficiently. Our main result is an approximationscheme for the partition function based on Markov Chain Monte Carlo theory. Wealso show that the efficient uniform sampling assumption holds in severalapplication settings that are of importance in machine learning.
arxiv-300-80 | Ordinal Boltzmann Machines for Collaborative Filtering | http://arxiv.org/pdf/1205.2611v1.pdf | author:Tran The Truyen, Dinh Q. Phung, Svetha Venkatesh category:cs.IR cs.LG published:2012-05-09 summary:Collaborative filtering is an effective recommendation technique wherein thepreference of an individual can potentially be predicted based on preferencesof other members. Early algorithms often relied on the strong locality in thepreference data, that is, it is enough to predict preference of a user on aparticular item based on a small subset of other users with similar tastes orof other items with similar properties. More recently, dimensionality reductiontechniques have proved to be equally competitive, and these are based on theco-occurrence patterns rather than locality. This paper explores and extends aprobabilistic model known as Boltzmann Machine for collaborative filteringtasks. It seamlessly integrates both the similarity and co-occurrence in aprincipled manner. In particular, we study parameterisation options to dealwith the ordinal nature of the preferences, and propose a joint modelling ofboth the user-based and item-based processes. Experiments on moderate andlarge-scale movie recommendation show that our framework rivals existingwell-known methods.
arxiv-300-81 | Computing Posterior Probabilities of Structural Features in Bayesian Networks | http://arxiv.org/pdf/1205.2612v1.pdf | author:Jin Tian, Ru He category:cs.LG stat.ML published:2012-05-09 summary:We study the problem of learning Bayesian network structures from data.Koivisto and Sood (2004) and Koivisto (2006) presented algorithms that cancompute the exact marginal posterior probability of a subnetwork, e.g., asingle edge, in O(n2n) time and the posterior probabilities for all n(n-1)potential edges in O(n2n) total time, assuming that the number of parents pernode or the indegree is bounded by a constant. One main drawback of theiralgorithms is the requirement of a special structure prior that is non uniformand does not respect Markov equivalence. In this paper, we develop an algorithmthat can compute the exact posterior probability of a subnetwork in O(3n) timeand the posterior probabilities for all n(n-1) potential edges in O(n3n) totaltime. Our algorithm also assumes a bounded indegree but allows generalstructure priors. We demonstrate the applicability of the algorithm on severaldata sets with up to 20 variables.
arxiv-300-82 | Products of Hidden Markov Models: It Takes N>1 to Tango | http://arxiv.org/pdf/1205.2614v1.pdf | author:Graham W Taylor, Geoffrey E. Hinton category:cs.LG stat.ML published:2012-05-09 summary:Products of Hidden Markov Models(PoHMMs) are an interesting class ofgenerative models which have received little attention since theirintroduction. This maybe in part due to their more computationally expensivegradient-based learning algorithm,and the intractability of computing the loglikelihood of sequences under the model. In this paper, we demonstrate how thepartition function can be estimated reliably via Annealed Importance Sampling.We perform experiments using contrastive divergence learning on rainfall dataand data captured from pairs of people dancing. Our results suggest thatadvances in learning and evaluation for undirected graphical models and recentincreases in available computing power make PoHMMs worth considering forcomplex time-series modeling tasks.
arxiv-300-83 | Modeling Discrete Interventional Data using Directed Cyclic Graphical Models | http://arxiv.org/pdf/1205.2617v1.pdf | author:Mark Schmidt, Kevin Murphy category:stat.ML cs.LG stat.ME published:2012-05-09 summary:We outline a representation for discrete multivariate distributions in termsof interventional potential functions that are globally normalized. Thisrepresentation can be used to model the effects of interventions, and theindependence properties encoded in this model can be represented as a directedgraph that allows cycles. In addition to discussing inference and sampling withthis representation, we give an exponential family parametrization that allowsparameter estimation to be stated as a convex optimization problem; we alsogive a convex relaxation of the task of simultaneous parameter and structurelearning using group l1-regularization. The model is evaluated on simulateddata and intracellular flow cytometry data.
arxiv-300-84 | BPR: Bayesian Personalized Ranking from Implicit Feedback | http://arxiv.org/pdf/1205.2618v1.pdf | author:Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme category:cs.IR cs.LG stat.ML published:2012-05-09 summary:Item recommendation is the task of predicting a personalized ranking on a setof items (e.g. websites, movies, products). In this paper, we investigate themost common scenario with implicit feedback (e.g. clicks, purchases). There aremany methods for item recommendation from implicit feedback like matrixfactorization (MF) or adaptive knearest-neighbor (kNN). Even though thesemethods are designed for the item prediction task of personalized ranking, noneof them is directly optimized for ranking. In this paper we present a genericoptimization criterion BPR-Opt for personalized ranking that is the maximumposterior estimator derived from a Bayesian analysis of the problem. We alsoprovide a generic learning algorithm for optimizing models with respect toBPR-Opt. The learning method is based on stochastic gradient descent withbootstrap sampling. We show how to apply our method to two state-of-the-artrecommender models: matrix factorization and adaptive kNN. Our experimentsindicate that for the task of personalized ranking our optimization methodoutperforms the standard learning techniques for MF and kNN. The results showthe importance of optimizing models for the right criterion.
arxiv-300-85 | Dynamic Behavioral Mixed-Membership Model for Large Evolving Networks | http://arxiv.org/pdf/1205.2056v1.pdf | author:Ryan Rossi, Brian Gallagher, Jennifer Neville, Keith Henderson category:cs.SI cs.LG physics.soc-ph stat.ML published:2012-05-09 summary:The majority of real-world networks are dynamic and extremely large (e.g.,Internet Traffic, Twitter, Facebook, ...). To understand the structuralbehavior of nodes in these large dynamic networks, it may be necessary to modelthe dynamics of behavioral roles representing the main connectivity patternsover time. In this paper, we propose a dynamic behavioral mixed-membershipmodel (DBMM) that captures the roles of nodes in the graph and how they evolveover time. Unlike other node-centric models, our model is scalable foranalyzing large dynamic networks. In addition, DBMM is flexible,parameter-free, has no functional form or parameterization, and isinterpretable (identifies explainable patterns). The performance resultsindicate our approach can be applied to very large networks while theexperimental results show that our model uncovers interesting patternsunderlying the dynamics of these networks.
arxiv-300-86 | Using the Gene Ontology Hierarchy when Predicting Gene Function | http://arxiv.org/pdf/1205.2622v1.pdf | author:Sara Mostafavi, Quaid Morris category:cs.LG cs.CE stat.ML published:2012-05-09 summary:The problem of multilabel classification when the labels are related througha hierarchical categorization scheme occurs in many application domains such ascomputational biology. For example, this problem arises naturally when tryingto automatically assign gene function using a controlled vocabularies like GeneOntology. However, most existing approaches for predicting gene functions solveindependent classification problems to predict genes that are involved in agiven function category, independently of the rest. Here, we propose two simplemethods for incorporating information about the hierarchical nature of thecategorization scheme. In the first method, we use information about a gene'sprevious annotation to set an initial prior on its label. In a second approach,we extend a graph-based semi-supervised learning algorithm for predicting genefunction in a hierarchy. We show that we can efficiently solve this problem bysolving a linear system of equations. We compare these approaches with aprevious label reconciliation-based approach. Results show that using thehierarchy information directly, compared to using reconciliation methods,improves gene function prediction.
arxiv-300-87 | Virtual Vector Machine for Bayesian Online Classification | http://arxiv.org/pdf/1205.2623v1.pdf | author:Thomas P. Minka, Rongjing Xiang, Yuan, Qi category:cs.LG stat.ML published:2012-05-09 summary:In a typical online learning scenario, a learner is required to process alarge data stream using a small memory buffer. Such a requirement is usually inconflict with a learner's primary pursuit of prediction accuracy. To addressthis dilemma, we introduce a novel Bayesian online classi cation algorithm,called the Virtual Vector Machine. The virtual vector machine allows you tosmoothly trade-off prediction accuracy with memory size. The virtual vectormachine summarizes the information contained in the preceding data stream by aGaussian distribution over the classi cation weights plus a constant number ofvirtual data points. The virtual data points are designed to add extranon-Gaussian information about the classi cation weights. To maintain theconstant number of virtual points, the virtual vector machine adds the currentreal data point into the virtual point set, merges two most similar virtualpoints into a new virtual point or deletes a virtual point that is far from thedecision boundary. The information lost in this process is absorbed into theGaussian distribution. The extra information provided by the virtual pointsleads to improved predictive accuracy over previous online classificationalgorithms.
arxiv-300-88 | Convexifying the Bethe Free Energy | http://arxiv.org/pdf/1205.2624v1.pdf | author:Ofer Meshi, Ariel Jaimovich, Amir Globerson, Nir Friedman category:cs.AI cs.LG published:2012-05-09 summary:The introduction of loopy belief propagation (LBP) revitalized theapplication of graphical models in many domains. Many recent works presentimprovements on the basic LBP algorithm in an attempt to overcome convergenceand local optima problems. Notable among these are convexified free energyapproximations that lead to inference procedures with provable convergence andquality properties. However, empirically LBP still outperforms most of itsconvex variants in a variety of settings, as we also demonstrate here.Motivated by this fact we seek convexified free energies that directlyapproximate the Bethe free energy. We show that the proposed approximationscompare favorably with state-of-the art convex free energy approximations.
arxiv-300-89 | Convergent message passing algorithms - a unifying view | http://arxiv.org/pdf/1205.2625v1.pdf | author:Talya Meltzer, Amir Globerson, Yair Weiss category:cs.AI cs.LG published:2012-05-09 summary:Message-passing algorithms have emerged as powerful techniques forapproximate inference in graphical models. When these algorithms converge, theycan be shown to find local (or sometimes even global) optima of variationalformulations to the inference problem. But many of the most popular algorithmsare not guaranteed to converge. This has lead to recent interest in convergentmessage-passing algorithms. In this paper, we present a unified view ofconvergent message-passing algorithms. We present a simple derivation of anabstract algorithm, tree-consistency bound optimization (TCBO) that is provablyconvergent in both its sum and max product forms. We then show that many of theexisting convergent algorithms are instances of our TCBO algorithm, and obtainnovel convergent algorithms "for free" by exchanging maximizations andsummations in existing algorithms. In particular, we show that Wainwright'snon-convergent sum-product algorithm for tree based variational bounds, isactually convergent with the right update order for the case where trees aremonotonic chains.
arxiv-300-90 | Group Sparse Priors for Covariance Estimation | http://arxiv.org/pdf/1205.2626v1.pdf | author:Benjamin Marlin, Mark Schmidt, Kevin Murphy category:stat.ML cs.LG published:2012-05-09 summary:Recently it has become popular to learn sparse Gaussian graphical models(GGMs) by imposing l1 or group l1,2 penalties on the elements of the precisionmatrix. Thispenalized likelihood approach results in a tractable convexoptimization problem. In this paper, we reinterpret these results as performingMAP estimation under a novel prior which we call the group l1 and l1,2positivedefinite matrix distributions. This enables us to build a hierarchicalmodel in which the l1 regularization terms vary depending on which group theentries are assigned to, which in turn allows us to learn block structuredsparse GGMs with unknown group assignments. Exact inference in thishierarchical model is intractable, due to the need to compute the normalizationconstant of these matrix distributions. However, we derive upper bounds on thepartition functions, which lets us use fast variational inference (optimizing alower bound on the joint posterior). We show that on two real world data sets(motion capture and financial data), our method which infers the blockstructure outperforms a method that uses a fixed block structure, which in turnoutperforms baseline methods that ignore block structure.
arxiv-300-91 | Domain Knowledge Uncertainty and Probabilistic Parameter Constraints | http://arxiv.org/pdf/1205.2627v1.pdf | author:Yi Mao, Guy Lebanon category:cs.LG stat.ML published:2012-05-09 summary:Incorporating domain knowledge into the modeling process is an effective wayto improve learning accuracy. However, as it is provided by humans, domainknowledge can only be specified with some degree of uncertainty. We propose toexplicitly model such uncertainty through probabilistic constraints over theparameter space. In contrast to hard parameter constraints, our approach iseffective also when the domain knowledge is inaccurate and generally results insuperior modeling accuracy. We focus on generative and conditional modelingwhere the parameters are assigned a Dirichlet or Gaussian prior and demonstratethe framework with experiments on both synthetic and real-world data.
arxiv-300-92 | Multiple Source Adaptation and the Renyi Divergence | http://arxiv.org/pdf/1205.2628v1.pdf | author:Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh category:cs.LG stat.ML published:2012-05-09 summary:This paper presents a novel theoretical study of the general problem ofmultiple source adaptation using the notion of Renyi divergence. Our resultsbuild on our previous work [12], but significantly broaden the scope of thatwork in several directions. We extend previous multiple source loss guaranteesbased on distribution weighted combinations to arbitrary target distributionsP, not necessarily mixtures of the source distributions, analyze both known andunknown target distribution cases, and prove a lower bound. We further extendour bounds to deal with the case where the learner receives an approximatedistribution for each source instead of the exact one, and show that similarloss guarantees can be achieved depending on the divergence between theapproximate and true distributions. We also analyze the case where the labelingfunctions of the source domains are somewhat different. Finally, we report theresults of experiments with both an artificial data set and a sentimentanalysis task, showing the performance benefits of the distribution weightedcombinations and the quality of our bounds based on the Renyi divergence.
arxiv-300-93 | Interpretation and Generalization of Score Matching | http://arxiv.org/pdf/1205.2629v1.pdf | author:Siwei Lyu category:cs.LG stat.ML published:2012-05-09 summary:Score matching is a recently developed parameter learning method that isparticularly effective to complicated high dimensional density models withintractable partition functions. In this paper, we study two issues that havenot been completely resolved for score matching. First, we provide a formallink between maximum likelihood and score matching. Our analysis shows thatscore matching finds model parameters that are more robust with noisy trainingdata. Second, we develop a generalization of score matching. Based on thisgeneralization, we further demonstrate an extension of score matching to modelsof discrete data.
arxiv-300-94 | Multi-Task Feature Learning Via Efficient l2,1-Norm Minimization | http://arxiv.org/pdf/1205.2631v1.pdf | author:Jun Liu, Shuiwang Ji, Jieping Ye category:cs.LG cs.CV stat.ML published:2012-05-09 summary:The problem of joint feature selection across a group of related tasks hasapplications in many areas including biomedical informatics and computervision. We consider the l2,1-norm regularized regression model for jointfeature selection from multiple tasks, which can be derived in theprobabilistic framework by assuming a suitable prior from the exponentialfamily. One appealing feature of the l2,1-norm regularization is that itencourages multiple predictors to share similar sparsity patterns. However, theresulting optimization problem is challenging to solve due to thenon-smoothness of the l2,1-norm regularization. In this paper, we propose toaccelerate the computation by reformulating it as two equivalent smooth convexoptimization problems which are then solved via the Nesterov's method-anoptimal first-order black-box method for smooth convex optimization. A keybuilding block in solving the reformulations is the Euclidean projection. Weshow that the Euclidean projection for the first reformulation can beanalytically computed, while the Euclidean projection for the second one can becomputed in linear time. Empirical evaluations on several data sets verify theefficiency of the proposed algorithms.
arxiv-300-95 | M-FISH Karyotyping - A New Approach Based on Watershed Transform | http://arxiv.org/pdf/1205.2031v1.pdf | author:K. S. Sreejini, A. Lijiya, V. K. Govindan category:cs.CV published:2012-05-09 summary:Karyotyping is a process in which chromosomes in a dividing cell are properlystained, identified and displayed in a standard format, which helps geneticistto study and diagnose genetic factors behind various genetic diseases and forstudying cancer. M-FISH (Multiplex Fluorescent In-Situ Hybridization) providescolor karyotyping. In this paper, an automated method for M-FISH chromosomesegmentation based on watershed transform followed by naive Bayesclassification of each region using the features, mean and standard deviation,is presented. Also, a post processing step is added to re-classify the smallchromosome segments to the neighboring larger segment for reducing the chancesof misclassification. The approach provided improved accuracy when compared tothe pixel-by-pixel approach. The approach was tested on 40 images from thedataset and achieved an accuracy of 84.21 %.
arxiv-300-96 | Improving Compressed Counting | http://arxiv.org/pdf/1205.2632v1.pdf | author:Ping Li category:cs.DS cs.LG stat.ML published:2012-05-09 summary:Compressed Counting (CC) [22] was recently proposed for estimating the athfrequency moments of data streams, where 0 < a <= 2. CC can be used forestimating Shannon entropy, which can be approximated by certain functions ofthe ath frequency moments as a -> 1. Monitoring Shannon entropy for anomalydetection (e.g., DDoS attacks) in large networks is an important task. Thispaper presents a new algorithm for improving CC. The improvement is mostsubstantial when a -> 1--. For example, when a = 0:99, the new algorithmreduces the estimation variance roughly by 100-fold. This new algorithm wouldmake CC considerably more practical for estimating Shannon entropy.Furthermore, the new algorithm is statistically optimal when a = 0.5.
arxiv-300-97 | Identifying confounders using additive noise models | http://arxiv.org/pdf/1205.2640v1.pdf | author:Dominik Janzing, Jonas Peters, Joris Mooij, Bernhard Schoelkopf category:stat.ML cs.LG published:2012-05-09 summary:We propose a method for inferring the existence of a latent common cause('confounder') of two observed random variables. The method assumes that thetwo effects of the confounder are (possibly nonlinear) functions of theconfounder plus independent, additive noise. We discuss under which conditionsthe model is identifiable (up to an arbitrary reparameterization of theconfounder) from the joint distribution of the effects. We state and prove atheoretical result that provides evidence for the conjecture that the model isgenerically identifiable under suitable technical conditions. In addition, wepropose a practical method to estimate the confounder from a finite i.i.d.sample of the effects and illustrate that the method works well on bothsimulated and real-world data.
arxiv-300-98 | Bayesian Discovery of Linear Acyclic Causal Models | http://arxiv.org/pdf/1205.2641v1.pdf | author:Patrik O. Hoyer, Antti Hyttinen category:stat.ML cs.LG stat.ME published:2012-05-09 summary:Methods for automated discovery of causal relationships fromnon-interventional data have received much attention recently. A widely usedand well understood model family is given by linear acyclic causal models(recursive structural equation models). For Gaussian data both constraint-basedmethods (Spirtes et al., 1993; Pearl, 2000) (which output a single equivalenceclass) and Bayesian score-based methods (Geiger and Heckerman, 1994) (whichassign relative scores to the equivalence classes) are available. On thecontrary, all current methods able to utilize non-Gaussianity in the data(Shimizu et al., 2006; Hoyer et al., 2008) always return only a single graph ora single equivalence class, and so are fundamentally unable to express thedegree of certainty attached to that output. In this paper we develop aBayesian score-based approach able to take advantage of non-Gaussianity whenestimating linear acyclic causal models, and we empirically demonstrate that,at least on very modest size networks, its accuracy is as good as or betterthan existing methods. We provide a complete code package (in R) whichimplements all algorithms and performs all of the analysis provided in thepaper, and hope that this will further the application of these methods tosolving causal inference problems.
arxiv-300-99 | New inference strategies for solving Markov Decision Processes using reversible jump MCMC | http://arxiv.org/pdf/1205.2643v1.pdf | author:Matthias Hoffman, Hendrik Kueck, Nando de Freitas, Arnaud Doucet category:cs.LG cs.SY math.OC stat.CO stat.ML published:2012-05-09 summary:In this paper we build on previous work which uses inferences techniques, inparticular Markov Chain Monte Carlo (MCMC) methods, to solve parameterizedcontrol problems. We propose a number of modifications in order to make thisapproach more practical in general, higher-dimensional spaces. We firstintroduce a new target distribution which is able to incorporate more rewardinformation from sampled trajectories. We also show how to break strongcorrelations between the policy parameters and sampled trajectories in order tosample more freely. Finally, we show how to incorporate these techniques in aprincipled manner to obtain estimates of the optimal policy.
arxiv-300-100 | Censored Exploration and the Dark Pool Problem | http://arxiv.org/pdf/1205.2646v1.pdf | author:Kuzman Ganchev, Michael Kearns, Yuriy Nevmyvaka, Jennifer Wortman Vaughan category:cs.LG cs.GT published:2012-05-09 summary:We introduce and analyze a natural algorithm for multi-venue exploration fromcensored data, which is motivated by the Dark Pool Problem of modernquantitative finance. We prove that our algorithm converges in polynomial timeto a near-optimal allocation policy; prior results for similar problems instochastic inventory control guaranteed only asymptotic convergence andexamined variants in which each venue could be treated independently. Ouranalysis bears a strong resemblance to that of efficient exploration/exploitation schemes in the reinforcement learning literature. We describe anextensive experimental evaluation of our algorithm on the Dark Pool Problemusing real trading data.
arxiv-300-101 | Learning Continuous-Time Social Network Dynamics | http://arxiv.org/pdf/1205.2648v1.pdf | author:Yu Fan, Christian R. Shelton category:cs.SI cs.LG physics.soc-ph stat.ML published:2012-05-09 summary:We demonstrate that a number of sociology models for social network dynamicscan be viewed as continuous time Bayesian networks (CTBNs). A sampling-basedapproximate inference method for CTBNs can be used as the basis of anexpectation-maximization procedure that achieves better accuracy in estimatingthe parameters of the model than the standard method of momentsalgorithmfromthe sociology literature. We extend the existing social networkmodels to allow for indirect and asynchronous observations of the links. AMarkov chain Monte Carlo sampling algorithm for this new model permitsestimation and inference. We provide results on both a synthetic network (forverification) and real social network data.
arxiv-300-102 | Correlated Non-Parametric Latent Feature Models | http://arxiv.org/pdf/1205.2650v1.pdf | author:Finale Doshi-Velez, Zoubin Ghahramani category:cs.LG stat.ML published:2012-05-09 summary:We are often interested in explaining data through a set of hidden factors orfeatures. When the number of hidden features is unknown, the Indian BuffetProcess (IBP) is a nonparametric latent feature model that does not bound thenumber of active features in dataset. However, the IBP assumes that all latentfeatures are uncorrelated, making it inadequate for many realworld problems. Weintroduce a framework for correlated nonparametric feature models, generalisingthe IBP. We use this framework to generate several specific models anddemonstrate applications on realworld datasets.
arxiv-300-103 | L2 Regularization for Learning Kernels | http://arxiv.org/pdf/1205.2653v1.pdf | author:Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh category:cs.LG stat.ML published:2012-05-09 summary:The choice of the kernel is critical to the success of many learningalgorithms but it is typically left to the user. Instead, the training data canbe used to learn the kernel by selecting it out of a given family, such as thatof non-negative linear combinations of p base kernels, constrained by a traceor L1 regularization. This paper studies the problem of learning kernels withthe same family of kernels but with an L2 regularization instead, and forregression problems. We analyze the problem of learning kernels with ridgeregression. We derive the form of the solution of the optimization problem andgive an efficient iterative algorithm for computing that solution. We present anovel theoretical analysis of the problem based on stability and give learningbounds for orthogonal kernels that contain only an additive term O(pp/m) whencompared to the standard kernel ridge regression stability bound. We alsoreport the results of experiments indicating that L1 regularization can lead tomodest improvements for a small number of kernels, but to performancedegradations in larger-scale cases. In contrast, L2 regularization neverdegrades performance and in fact achieves significant improvements with a largenumber of kernels.
arxiv-300-104 | Convex Coding | http://arxiv.org/pdf/1205.2656v1.pdf | author:David M. Bradley, J Andrew Bagnell category:cs.LG cs.IT math.IT stat.ML published:2012-05-09 summary:Inspired by recent work on convex formulations of clustering (Lashkari &Golland, 2008; Nowozin & Bakir, 2008) we investigate a new formulation of theSparse Coding Problem (Olshausen & Field, 1997). In sparse coding we attempt tosimultaneously represent a sequence of data-vectors sparsely (i.e. sparseapproximation (Tropp et al., 2006)) in terms of a 'code' defined by a set ofbasis elements, while also finding a code that enables such an approximation.As existing alternating optimization procedures for sparse coding aretheoretically prone to severe local minima problems, we propose a convexrelaxation of the sparse coding problem and derive a boosting-style algorithm,that (Nowozin & Bakir, 2008) serves as a convex 'master problem' which calls a(potentially non-convex) sub-problem to identify the next code element to add.Finally, we demonstrate the properties of our boosted coding algorithm on animage denoising task.
arxiv-300-105 | Multilingual Topic Models for Unaligned Text | http://arxiv.org/pdf/1205.2657v1.pdf | author:Jordan Boyd-Graber, David Blei category:cs.CL cs.IR cs.LG stat.ML published:2012-05-09 summary:We develop the multilingual topic model for unaligned text (MuTo), aprobabilistic model of text that is designed to analyze corpora composed ofdocuments in two languages. From these documents, MuTo uses stochastic EM tosimultaneously discover both a matching between the languages and multilinguallatent topics. We demonstrate that MuTo is able to find shared topics onreal-world multilingual corpora, successfully pairing related documents acrosslanguages. MuTo provides a new framework for creating multilingual topic modelswithout needing carefully curated parallel corpora and allows applicationsbuilt using the topic model formalism to be applied to a much wider class ofcorpora.
arxiv-300-106 | Optimization of Structured Mean Field Objectives | http://arxiv.org/pdf/1205.2658v1.pdf | author:Alexandre Bouchard-Cote, Michael I. Jordan category:stat.ML cs.LG published:2012-05-09 summary:In intractable, undirected graphical models, an intuitive way of creatingstructured mean field approximations is to select an acyclic tractablesubgraph. We show that the hardness of computing the objective function andgradient of the mean field objective qualitatively depends on a simple graphproperty. If the tractable subgraph has this property- we call such subgraphsv-acyclic-a very fast block coordinate ascent algorithm is possible. If not,optimization is harder, but we show a new algorithm based on the constructionof an auxiliary exponential family that can be used to make inference possiblein this case as well. We discuss the advantages and disadvantages of eachregime and compare the algorithms empirically.
arxiv-300-107 | Alternating Projections for Learning with Expectation Constraints | http://arxiv.org/pdf/1205.2660v1.pdf | author:Kedar Bellare, Gregory Druck, Andrew McCallum category:cs.LG stat.ML published:2012-05-09 summary:We present an objective function for learning with unlabeled data thatutilizes auxiliary expectation constraints. We optimize this objective functionusing a procedure that alternates between information and moment projections.Our method provides an alternate interpretation of the posterior regularizationframework (Graca et al., 2008), maintains uncertainty during optimizationunlike constraint-driven learning (Chang et al., 2007), and is more efficientthan generalized expectation criteria (Mann & McCallum, 2008). Applications ofthis framework include minimally supervised learning, semisupervised learning,and learning with constraints that are more expressive than the underlyingmodel. In experiments, we demonstrate comparable accuracy to generalizedexpectation criteria for minimally supervised learning, and use expressivestructural constraints to guide semi-supervised learning, providing a 3%-6%improvement over stateof-the-art constraint-driven learning.
arxiv-300-108 | REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs | http://arxiv.org/pdf/1205.2661v1.pdf | author:Peter L. Bartlett, Ambuj Tewari category:cs.LG published:2012-05-09 summary:We provide an algorithm that achieves the optimal regret rate in an unknownweakly communicating Markov Decision Process (MDP). The algorithm proceeds inepisodes where, in each episode, it picks a policy using regularization basedon the span of the optimal bias vector. For an MDP with S states and A actionswhose optimal bias vector has span bounded by H, we show a regret bound of~O(HSpAT). We also relate the span to various diameter-like quantitiesassociated with the MDP, demonstrating how our results improve on previousregret bounds.
arxiv-300-109 | On Smoothing and Inference for Topic Models | http://arxiv.org/pdf/1205.2662v1.pdf | author:Arthur Asuncion, Max Welling, Padhraic Smyth, Yee Whye Teh category:cs.LG stat.ML published:2012-05-09 summary:Latent Dirichlet analysis, or topic modeling, is a flexible latent variableframework for modeling high-dimensional sparse count data. Various learningalgorithms have been developed in recent years, including collapsed Gibbssampling, variational inference, and maximum a posteriori estimation, and thisvariety motivates the need for careful empirical comparisons. In this paper, wehighlight the close connections between these approaches. We find that the maindifferences are attributable to the amount of smoothing applied to the counts.When the hyperparameters are optimized, the differences in performance amongthe algorithms diminish significantly. The ability of these algorithms toachieve solutions of comparable accuracy gives us the freedom to selectcomputationally efficient approaches. Using the insights gained from thiscomparative study, we show how accurate topic models can be learned in severalseconds on text corpora with thousands of documents.
arxiv-300-110 | A Bayesian Sampling Approach to Exploration in Reinforcement Learning | http://arxiv.org/pdf/1205.2664v1.pdf | author:John Asmuth, Lihong Li, Michael L. Littman, Ali Nouri, David Wingate category:cs.LG published:2012-05-09 summary:We present a modular approach to reinforcement learning that uses a Bayesianrepresentation of the uncertainty over models. The approach, BOSS (Best ofSampled Set), drives exploration by sampling multiple models from the posteriorand selecting actions optimistically. It extends previous work by providing arule for deciding when to resample and how to combine the models. We show thatour algorithm achieves nearoptimal reward with high probability with a samplecomplexity that is low relative to the speed at which the posteriordistribution converges during learning. We demonstrate that BOSS performs quitefavorably compared to state-of-the-art reinforcement-learning approaches andillustrate its flexibility by pairing it with a non-parametric model thatgeneralizes across states.
arxiv-300-111 | Structured Input-Output Lasso, with Application to eQTL Mapping, and a Thresholding Algorithm for Fast Estimation | http://arxiv.org/pdf/1205.1989v1.pdf | author:Seunghak Lee, Eric P. Xing category:stat.ML q-bio.GN q-bio.QM stat.AP published:2012-05-09 summary:We consider the problem of learning a high-dimensional multi-task regressionmodel, under sparsity constraints induced by presence of grouping structures onthe input covariates and on the output predictors. This problem is primarilymotivated by expression quantitative trait locus (eQTL) mapping, of which thegoal is to discover genetic variations in the genome (inputs) that influencethe expression levels of multiple co-expressed genes (outputs), eitherepistatically, or pleiotropically, or both. A structured input-output lasso(SIOL) model based on an intricate l1/l2-norm penalty over the regressioncoefficient matrix is employed to enable discovery of complex sparseinput/output relationships; and a highly efficient new optimization algorithmcalled hierarchical group thresholding (HiGT) is developed to solve theresultant non-differentiable, non-separable, and ultra high-dimensionaloptimization problem. We show on both simulation and on a yeast eQTL datasetthat our model leads to significantly better recovery of the structured sparserelationships between the inputs and the outputs, and our algorithmsignificantly outperforms other optimization techniques under the same model.Additionally, we propose a novel approach for efficiently and effectivelydetecting input interactions by exploiting the prior knowledge available frombiological experiments.
arxiv-300-112 | Expressivity of Time-Varying Graphs and the Power of Waiting in Dynamic Networks | http://arxiv.org/pdf/1205.1975v1.pdf | author:Arnaud Casteigts, Paola Flocchini, Emmanuel Godard, Nicola Santoro, Masafumi Yamashita category:cs.DC cs.CL published:2012-05-09 summary:In infrastructure-less highly dynamic networks, computing and performing evenbasic tasks (such as routing and broadcasting) is a very challenging activitydue to the fact that connectivity does not necessarily hold, and the networkmay actually be disconnected at every time instant. Clearly the task ofdesigning protocols for these networks is less difficult if the environmentallows waiting (i.e., it provides the nodes with store-carry-forward-likemechanisms such as local buffering) than if waiting is not feasible. Noquantitative corroborations of this fact exist (e.g., no answer to thequestion: how much easier?). In this paper, we consider these qualitativequestions about dynamic networks, modeled as time-varying (or evolving) graphs,where edges exist only at some times. We examine the difficulty of the environment in terms of the expressivity ofthe corresponding time-varying graph; that is in terms of the languagegenerated by the feasible journeys in the graph. We prove that the set oflanguages $L_{nowait}$ when no waiting is allowed contains all computablelanguages. On the other end, using algebraic properties of quasi-orders, weprove that $L_{wait}$ is just the family of regular languages. In other words,we prove that, when waiting is no longer forbidden, the power of the acceptingautomaton (difficulty of the environment) drops drastically from being aspowerful as a Turing machine, to becoming that of a Finite-State machine. This(perhaps surprisingly large) gap is a measure of the computational power ofwaiting. We also study bounded waiting; that is when waiting is allowed at a node onlyfor at most $d$ time units. We prove the negative result that $L_{wait[d]} =L_{nowait}$; that is, the expressivity decreases only if the waiting is finitebut unpredictable (i.e., under the control of the protocol designer and not ofthe environment).
arxiv-300-113 | Graph Prediction in a Low-Rank and Autoregressive Setting | http://arxiv.org/pdf/1205.1406v2.pdf | author:Emile Richard, Pierre-Andre Savalle, Nicolas Vayatis category:stat.ML published:2012-05-07 summary:We study the problem of prediction for evolving graph data. We formulate theproblem as the minimization of a convex objective encouraging sparsity andlow-rank of the solution, that reflect natural graph properties. The convexformulation allows to obtain oracle inequalities and efficient solvers. Weprovide empirical results for our algorithm and comparison with competingmethods, and point out two open questions related to compressed sensing andalgebra of low-rank and sparse matrices.
arxiv-300-114 | Hamiltonian Monte Carlo with Reduced Momentum Flips | http://arxiv.org/pdf/1205.1939v1.pdf | author:Jascha Sohl-Dickstein category:cs.LG published:2012-05-09 summary:Hamiltonian Monte Carlo (or hybrid Monte Carlo) with partial momentumrefreshment explores the state space more slowly than it otherwise would due tothe momentum reversals which occur on proposal rejection. These causetrajectories to double back on themselves, leading to random walk behavior ontimescales longer than the typical rejection time, and leading to slowermixing. I present a technique by which the number of momentum reversals can bereduced. This is accomplished by maintaining the net exchange of probabilitybetween states with opposite momenta, but reducing the rate of exchange in bothdirections such that it is 0 in one direction. An experiment illustrates thesereduced momentum flips accelerating mixing for a particular distribution.
arxiv-300-115 | Hamiltonian Annealed Importance Sampling for partition function estimation | http://arxiv.org/pdf/1205.1925v1.pdf | author:Jascha Sohl-Dickstein, Benjamin J. Culpepper category:cs.LG published:2012-05-09 summary:We introduce an extension to annealed importance sampling that usesHamiltonian dynamics to rapidly estimate normalization constants. Wedemonstrate this method by computing log likelihoods in directed and undirectedprobabilistic image models. We compare the performance of linear generativemodels with both Gaussian and Laplace priors, product of experts models withLaplace and Student's t experts, the mc-RBM, and a bilinear generative model.We provide code to compare additional models.
arxiv-300-116 | The Natural Gradient by Analogy to Signal Whitening, and Recipes and Tricks for its Use | http://arxiv.org/pdf/1205.1828v1.pdf | author:Jascha Sohl-Dickstein category:cs.LG stat.ML published:2012-05-08 summary:The natural gradient allows for more efficient gradient descent by removingdependencies and biases inherent in a function's parameterization. Severalpapers present the topic thoroughly and precisely. It remains a very difficultidea to get your head around however. The intent of this note is to providesimple intuition for the natural gradient and its use. We review how an illconditioned parameter space can undermine learning, introduce the naturalgradient by analogy to the more widely understood concept of signal whitening,and present tricks and specific prescriptions for applying the natural gradientto learning problems.
arxiv-300-117 | A Novel Method For Speech Segmentation Based On Speakers' Characteristics | http://arxiv.org/pdf/1205.1794v1.pdf | author:Behrouz Abdolali, Hossein Sameti category:cs.AI cs.CL 92C55 C.3.4 published:2012-05-08 summary:Speech Segmentation is the process change point detection for partitioning aninput audio stream into regions each of which corresponds to only one audiosource or one speaker. One application of this system is in Speaker Diarizationsystems. There are several methods for speaker segmentation; however, most ofthe Speaker Diarization Systems use BIC-based Segmentation methods. The maingoal of this paper is to propose a new method for speaker segmentation withhigher speed than the current methods - e.g. BIC - and acceptable accuracy. Ourproposed method is based on the pitch frequency of the speech. The accuracy ofthis method is similar to the accuracy of common speaker segmentation methods.However, its computation cost is much less than theirs. We show that our methodis about 2.4 times faster than the BIC-based method, while the average accuracyof pitch-based method is slightly higher than that of the BIC-based method.
arxiv-300-118 | Graph-based Learning with Unbalanced Clusters | http://arxiv.org/pdf/1205.1496v2.pdf | author:Jing Qian, Venkatesh Saligrama, Manqi Zhao category:stat.ML cs.LG published:2012-05-07 summary:Graph construction is a crucial step in spectral clustering (SC) andgraph-based semi-supervised learning (SSL). Spectral methods applied onstandard graphs such as full-RBF, $\epsilon$-graphs and $k$-NN graphs can leadto poor performance in the presence of proximal and unbalanced data. This isbecause spectral methods based on minimizing RatioCut or normalized cut onthese graphs tend to put more importance on balancing cluster sizes overreducing cut values. We propose a novel graph construction technique and showthat the RatioCut solution on this new graph is able to handle proximal andunbalanced data. Our method is based on adaptively modulating the neighborhooddegrees in a $k$-NN graph, which tends to sparsify neighborhoods in low densityregions. Our method adapts to data with varying levels of unbalancedness andcan be naturally used for small cluster detection. We justify our ideas throughlimit cut analysis. Unsupervised and semi-supervised experiments on syntheticand real data sets demonstrate the superiority of our method.
arxiv-300-119 | A novel statistical fusion rule for image fusion and its comparison in non subsampled contourlet transform domain and wavelet domain | http://arxiv.org/pdf/1205.1648v1.pdf | author:Manu V T, Philomina Simon category:cs.CV math.ST stat.TH published:2012-05-08 summary:Image fusion produces a single fused image from a set of input images. A newmethod for image fusion is proposed based on Weighted Average Merging Method(WAMM) in the NonSubsampled Contourlet Transform (NSCT) domain. A performanceanalysis on various statistical fusion rules are also analysed both in NSCT andWavelet domain. Analysis has been made on medical images, remote sensing imagesand multi focus images. Experimental results shows that the proposed method,WAMM obtained better results in NSCT domain than the wavelet domain as itpreserves more edges and keeps the visual quality intact in the fused image.
arxiv-300-120 | DBC based Face Recognition using DWT | http://arxiv.org/pdf/1205.1644v1.pdf | author:H S Jagadeesh, K Suresh Babu, K B Raja category:cs.CV published:2012-05-08 summary:The applications using face biometric has proved its reliability in lastdecade. In this paper, we propose DBC based Face Recognition using DWT (DBC-FR) model. The Poly-U Near Infra Red (NIR) database images are scanned andcropped to get only the face part in pre-processing. The face part is resizedto 100*100 and DWT is applied to derive LL, LH, HL and HH subbands. The LLsubband of size 50*50 is converted into 100 cells with 5*5 dimention of eachcell. The Directional Binary Code (DBC) is applied on each 5*5 cell to derive100 features. The Euclidian distance measure is used to compare the features oftest image and database images. The proposed algorithm render better percentagerecognition rate compared to the existing algorithm.
arxiv-300-121 | Spectral Analysis of Projection Histogram for Enhancing Close matching character Recognition in Malayalam | http://arxiv.org/pdf/1205.1639v1.pdf | author:Sajilal Divakaran category:cs.CL cs.CV cs.IR published:2012-05-08 summary:The success rates of Optical Character Recognition (OCR) systems for printedMalayalam documents is quite impressive with the state of the art accuracylevels in the range of 85-95% for various. However for real applications,further enhancement of this accuracy levels are required. One of the bottlenecks in further enhancement of the accuracy is identified as close-matchingcharacters. In this paper, we delineate the close matching characters inMalayalam and report the development of a specialised classifier for theseclose-matching characters. The output of a state of the art of OCR is taken andcharacters falling into the close-matching character set is further fed intothis specialised classifier for enhancing the accuracy. The classifier is basedon support vector machine algorithm and uses feature vectors derived out ofspectral coefficients of projection histogram signals of close-matchingcharacters.
arxiv-300-122 | Parsing of Myanmar sentences with function tagging | http://arxiv.org/pdf/1205.1603v1.pdf | author:Win Win Thant, Tin Myat Htwe, Ni Lar Thein category:cs.CL published:2012-05-08 summary:This paper describes the use of Naive Bayes to address the task of assigningfunction tags and context free grammar (CFG) to parse Myanmar sentences. Partof the challenge of statistical function tagging for Myanmar sentences comesfrom the fact that Myanmar has free-phrase-order and a complex morphologicalsystem. Function tagging is a pre-processing step for parsing. In the task offunction tagging, we use the functional annotated corpus and tag Myanmarsentences with correct segmentation, POS (part-of-speech) tagging and chunkinginformation. We propose Myanmar grammar rules and apply context free grammar(CFG) to find out the parse tree of function tagged Myanmar sentences.Experiments show that our analysis achieves a good result with parsing ofsimple sentences and three types of complex sentences.
arxiv-300-123 | Real time facial expression recognition using a novel method | http://arxiv.org/pdf/1206.3559v1.pdf | author:Saumil Srivastava category:cs.CV published:2012-05-08 summary:This paper discusses a novel method for Facial Expression Recognition Systemwhich performs facial expression analysis in a near real time from a live webcam feed. Primary objectives were to get results in a near real time with lightinvariant, person independent and pose invariant way. The system is composed oftwo different entities trainer and evaluator. Each frame of video feed ispassed through a series of steps including haar classifiers, skin detection,feature extraction, feature points tracking, creating a learned Support VectorMachine model to classify emotions to achieve a tradeoff between accuracy andresult rate. A processing time of 100-120 ms per 10 frames was achieved withaccuracy of around 60%. We measure our accuracy in terms of variety ofinteraction and classification scenarios. We conclude by discussing relevanceof our work to human computer interaction and exploring further measures thatcan be taken.
arxiv-300-124 | Characterizing Ranked Chinese Syllable-to-Character Mapping Spectrum: A Bridge Between the Spoken and Written Chinese Language | http://arxiv.org/pdf/1205.1564v1.pdf | author:Wentian Li category:cs.CL stat.AP published:2012-05-08 summary:One important aspect of the relationship between spoken and written Chineseis the ranked syllable-to-character mapping spectrum, which is the ranked listof syllables by the number of characters that map to the syllable. Previously,this spectrum is analyzed for more than 400 syllables without distinguishingthe four intonations. In the current study, the spectrum with 1280 tonedsyllables is analyzed by logarithmic function, Beta rank function, andpiecewise logarithmic function. Out of the three fitting functions, thetwo-piece logarithmic function fits the data the best, both by the smallest sumof squared errors (SSE) and by the lowest Akaike information criterion (AIC)value. The Beta rank function is the close second. By sampling from a Poissondistribution whose parameter value is chosen from the observed data, weempirically estimate the $p$-value for testing thetwo-piece-logarithmic-function being better than the Beta rank functionhypothesis, to be 0.16. For practical purposes, the piecewise logarithmicfunction and the Beta rank function can be considered a tie.
arxiv-300-125 | Dynamic Multi-Relational Chinese Restaurant Process for Analyzing Influences on Users in Social Media | http://arxiv.org/pdf/1205.1456v1.pdf | author:Himabindu Lakkaraju, Indrajit Bhattacharya, Chiranjib Bhattacharyya category:cs.SI cs.LG physics.soc-ph published:2012-05-07 summary:We study the problem of analyzing influence of various factors affectingindividual messages posted in social media. The problem is challenging becauseof various types of influences propagating through the social media networkthat act simultaneously on any user. Additionally, the topic composition of theinfluencing factors and the susceptibility of users to these influences evolveover time. This problem has not studied before, and off-the-shelf models areunsuitable for this purpose. To capture the complex interplay of these variousfactors, we propose a new non-parametric model called the DynamicMulti-Relational Chinese Restaurant Process. This accounts for the user networkfor data generation and also allows the parameters to evolve over time.Designing inference algorithms for this model suited for large scalesocial-media data is another challenge. To this end, we propose a scalable andmulti-threaded inference algorithm based on online Gibbs Sampling. Extensiveevaluations on large-scale Twitter and Facebook data show that the extractedtopics when applied to authorship and commenting prediction outperformstate-of-the-art baselines. More importantly, our model produces valuableinsights on topic trends and user personality trends, beyond the capability ofexisting approaches.
arxiv-300-126 | Image Enhancement with Statistical Estimation | http://arxiv.org/pdf/1205.1365v1.pdf | author:Aroop Mukherjee, Soumen Kanrar category:cs.MM cs.CV published:2012-05-07 summary:Contrast enhancement is an important area of research for the image analysis.Over the decade, the researcher worked on this domain to develop an efficientand adequate algorithm. The proposed method will enhance the contrast of imageusing Binarization method with the help of Maximum Likelihood Estimation (MLE).The paper aims to enhance the image contrast of bimodal and multi-modal images.The proposed methodology use to collect mathematical information retrieves fromthe image. In this paper, we are using binarization method that generates thedesired histogram by separating image nodes. It generates the enhanced imageusing histogram specification with binarization method. The proposed method hasshowed an improvement in the image contrast enhancement compare with the otherimage.
arxiv-300-127 | Detecting Spammers via Aggregated Historical Data Set | http://arxiv.org/pdf/1205.1357v1.pdf | author:Eitan Menahem, Rami Puzis category:cs.CR cs.LG C.2.0; H.4.3 published:2012-05-07 summary:The battle between email service providers and senders of mass unsolicitedemails (Spam) continues to gain traction. Vast numbers of Spam emails are sentmainly from automatic botnets distributed over the world. One method formitigating Spam in a computationally efficient manner is fast and accurateblacklisting of the senders. In this work we propose a new sender reputationmechanism that is based on an aggregated historical data-set which encodes thebehavior of mail transfer agents over time. A historical data-set is createdfrom labeled logs of received emails. We use machine learning algorithms tobuild a model that predicts the \emph{spammingness} of mail transfer agents inthe near future. The proposed mechanism is targeted mainly at large enterprisesand email service providers and can be used for updating both the black and thewhite lists. We evaluate the proposed mechanism using 9.5M anonymized logentries obtained from the biggest Internet service provider in Europe.Experiments show that proposed method detects more than 94% of the Spam emailsthat escaped the blacklist (i.e., TPR), while having less than 0.5%false-alarms. Therefore, the effectiveness of the proposed method is muchhigher than of previously reported reputation mechanisms, which rely on emailslogs. In addition, the proposed method, when used for updating both the blackand white lists, eliminated the need in automatic content inspection of 4 outof 5 incoming emails, which resulted in dramatic reduction in the filteringcomputational load.
arxiv-300-128 | A United Image Force for Deformable Models and Direct Transforming Geometric Active Contorus to Snakes by Level Sets | http://arxiv.org/pdf/1201.1571v3.pdf | author:Hongyu Lu, Yutian Wang, Shanglian Bao category:cs.CV published:2012-01-07 summary:A uniform distribution of the image force field around the object fasts theconvergence speed of the segmentation process. However, to achieve this aim, itcauses the force constructed from the heat diffusion model unable to indicatethe object boundaries accurately. The image force based on electrostatic fieldmodel can perform an exact shape recovery. First, this study introduces afusion scheme of these two image forces, which is capable of extracting theobject boundary with high precision and fast speed. Until now, there is nosatisfied analysis about the relationship between Snakes and Geometric ActiveContours (GAC). The second contribution of this study addresses that the GACmodel can be deduced directly from Snakes model. It proves that each term inGAC and Snakes is correspondent and has similar function. However, the twomodels are expressed using different mathematics. Further, since losing theability of rotating the contour, adoption of level sets can limits the usage ofGAC in some circumstances.
arxiv-300-129 | Convex Relaxation for Combinatorial Penalties | http://arxiv.org/pdf/1205.1240v1.pdf | author:Guillaume Obozinski, Francis Bach category:stat.ML cs.LG published:2012-05-06 summary:In this paper, we propose an unifying view of several recently proposedstructured sparsity-inducing norms. We consider the situation of a modelsimultaneously (a) penalized by a set- function de ned on the support of theunknown parameter vector which represents prior knowledge on supports, and (b)regularized in Lp-norm. We show that the natural combinatorial optimizationproblems obtained may be relaxed into convex optimization problems andintroduce a notion, the lower combinatorial envelope of a set-function, thatcharacterizes the tightness of our relaxations. We moreover establish linkswith norms based on latent representations including the latent group Lasso andblock-coding, and with norms obtained from submodular functions.
arxiv-300-130 | A Hybrid Artificial Bee Colony Algorithm for Graph 3-Coloring | http://arxiv.org/pdf/1206.1012v1.pdf | author:Iztok Fister Jr., Iztok Fister, Janez Brest category:cs.NE published:2012-05-06 summary:The Artificial Bee Colony (ABC) is the name of an optimization algorithm thatwas inspired by the intelligent behavior of a honey bee swarm. It is widelyrecognized as a quick, reliable, and efficient methods for solving optimizationproblems. This paper proposes a hybrid ABC (HABC) algorithm for graph3-coloring, which is a well-known discrete optimization problem. The results ofHABC are compared with results of the well-known graph coloring algorithms oftoday, i.e. the Tabucol and Hybrid Evolutionary algorithm (HEA) and results ofthe traditional evolutionary algorithm with SAW method (EA-SAW). Extensiveexperimentations has shown that the HABC matched the competitive results of thebest graph coloring algorithms, and did better than the traditional heuristicsEA-SAW when solving equi-partite, flat, and random generated medium-sizedgraphs.
arxiv-300-131 | Volumetric Mapping of Genus Zero Objects via Mass Preservation | http://arxiv.org/pdf/1205.1225v1.pdf | author:Romeil Sandhu, Ayelet Dominitz, Yi Gao, Allen Tannenbaum category:cs.CG cs.CV published:2012-05-06 summary:In this work, we present a technique to map any genus zero solid object ontoa hexahedral decomposition of a solid cube. This problem appears in manyapplications ranging from finite element methods to visual tracking. From this,one can then hopefully utilize the proposed technique for shape analysis,registration, as well as other related computer graphics tasks. Moreimportantly, given that we seek to establish a one-to-one correspondence of aninput volume to that of a solid cube, our algorithm can naturally generate aquality hexahedral mesh as an output. In addition, we constrain the mappingitself to be volume preserving allowing for the possibility of further meshsimplification. We demonstrate our method both qualitatively and quantitativelyon various 3D solid models
arxiv-300-132 | Robust Recovery of Subspace Structures by Low-Rank Representation | http://arxiv.org/pdf/1010.2955v6.pdf | author:Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, Yi Ma category:cs.IT cs.CV cs.LG math.IT published:2010-10-14 summary:In this work we address the subspace recovery problem. Given a set of datasamples (vectors) approximately drawn from a union of multiple subspaces, ourgoal is to segment the samples into their respective subspaces and correct thepossible errors as well. To this end, we propose a novel method termed Low-RankRepresentation (LRR), which seeks the lowest-rank representation among all thecandidates that can represent the data samples as linear combinations of thebases in a given dictionary. It is shown that LRR well solves the subspacerecovery problem: when the data is clean, we prove that LRR exactly capturesthe true subspace structures; for the data contaminated by outliers, we provethat under certain conditions LRR can exactly recover the row space of theoriginal data and detect the outlier as well; for the data corrupted byarbitrary errors, LRR can also approximately recover the row space withtheoretical guarantees. Since the subspace membership is provably determined bythe row space, these further imply that LRR can perform robust subspacesegmentation and error correction, in an efficient way.
arxiv-300-133 | Solving Principal Component Pursuit in Linear Time via $l_1$ Filtering | http://arxiv.org/pdf/1108.5359v4.pdf | author:Risheng Liu, Zhouchen Lin, Siming Wei, Zhixun Su category:cs.NA cs.CV published:2011-08-26 summary:In the past decades, exactly recovering the intrinsic data structure fromcorrupted observations, which is known as robust principal component analysis(RPCA), has attracted tremendous interests and found many applications incomputer vision. Recently, this problem has been formulated as recovering alow-rank component and a sparse component from the observed data matrix. It isproved that under some suitable conditions, this problem can be exactly solvedby principal component pursuit (PCP), i.e., minimizing a combination of nuclearnorm and $l_1$ norm. Most of the existing methods for solving PCP requiresingular value decompositions (SVD) of the data matrix, resulting in a highcomputational complexity, hence preventing the applications of RPCA to verylarge scale computer vision problems. In this paper, we propose a novelalgorithm, called $l_1$ filtering, for \emph{exactly} solving PCP with an$O(r^2(m+n))$ complexity, where $m\times n$ is the size of data matrix and $r$is the rank of the matrix to recover, which is supposed to be much smaller than$m$ and $n$. Moreover, $l_1$ filtering is \emph{highly parallelizable}. It isthe first algorithm that can \emph{exactly} solve a nuclear norm minimizationproblem in \emph{linear time} (with respect to the data size). Experiments onboth synthetic data and real applications testify to the great advantage of$l_1$ filtering in speed over state-of-the-art algorithms.
arxiv-300-134 | TIGRESS: Trustful Inference of Gene REgulation using Stability Selection | http://arxiv.org/pdf/1205.1181v1.pdf | author:Anne-Claire Haury, Fantine Mordelet, Paola Vera-Licona, Jean-Philippe Vert category:stat.ML q-bio.QM published:2012-05-06 summary:Inferring the structure of gene regulatory networks (GRN) from geneexpression data has many applications, from the elucidation of complexbiological processes to the identification of potential drug targets. It ishowever a notoriously difficult problem, for which the many existing methodsreach limited accuracy. In this paper, we formulate GRN inference as a sparseregression problem and investigate the performance of a popular featureselection method, least angle regression (LARS) combined with stabilityselection. We introduce a novel, robust and accurate scoring technique forstability selection, which improves the performance of feature selection withLARS. The resulting method, which we call TIGRESS (Trustful Inference of GeneREgulation using Stability Selection), was ranked among the top methods in theDREAM5 gene network reconstruction challenge. We investigate in depth theinfluence of the various parameters of the method and show that a fineparameter tuning can lead to significant improvements and state-of-the-artperformance for GRN inference. TIGRESS reaches state-of-the-art performance onbenchmark data. This study confirms the potential of feature selectiontechniques for GRN inference. Code and data are available onhttp://cbio.ensmp.fr/~ahaury. Running TIGRESS online is possible onGenePattern: http://www.broadinstitute.org/cancer/software/genepattern/.
arxiv-300-135 | Rakeness in the design of Analog-to-Information Conversion of Sparse and Localized Signals | http://arxiv.org/pdf/1205.1144v1.pdf | author:Mauro Mangia, Riccardo Rovatti, Gianluca Setti category:cs.IT cs.CV math.IT published:2012-05-05 summary:Design of Random Modulation Pre-Integration systems based on therestricted-isometry property may be suboptimal when the energy of the signalsto be acquired is not evenly distributed, i.e. when they are both sparse andlocalized. To counter this, we introduce an additional design criterion, thatwe call rakeness, accounting for the amount of energy that the measurementscapture from the signal to be acquired. Hence, for localized signals a propersystem tuning increases the rakeness as well as the average SNR of the samplesused in its reconstruction. Yet, maximizing average SNR may go against the needof capturing all the components that are potentially non-zero in a sparsesignal, i.e., against the restricted isometry requirement ensuringreconstructability. What we propose is to administer the trade-off betweenrakeness and restricted isometry in a statistical way by laying down anoptimization problem. The solution of such an optimization problem is thestatistic of the process generating the random waveforms onto which the signalis projected to obtain the measurements. The formal definition of such aproblems is given as well as its solution for signals that are either localizedin frequency or in more generic domain. Sample applications, to ECG signals andsmall images of printed letters and numbers, show that rakeness-based designleads to non-negligible improvements in both cases.
arxiv-300-136 | Robot Navigation using Reinforcement Learning and Slow Feature Analysis | http://arxiv.org/pdf/1205.0986v1.pdf | author:Wendelin BÃ¶hmer category:cs.AI cs.NE published:2012-05-04 summary:The application of reinforcement learning algorithms onto real life problemsalways bears the challenge of filtering the environmental state out of rawsensor readings. While most approaches use heuristics, biology suggests thatthere must exist an unsupervised method to construct such filtersautomatically. Besides the extraction of environmental states, the filters haveto represent them in a fashion that support modern reinforcement algorithms.Many popular algorithms use a linear architecture, so one should aim at filtersthat have good approximation properties in combination with linear functions.This thesis wants to propose the unsupervised method slow feature analysis(SFA) for this task. Presented with a random sequence of sensor readings, SFAlearns a set of filters. With growing model complexity and training examples,the filters converge against trigonometric polynomial functions. These areknown to possess excellent approximation capabilities and should therforesupport the reinforcement algorithms well. We evaluate this claim on a robot.The task is to learn a navigational control in a simple environment using theleast square policy iteration (LSPI) algorithm. The only accessible sensor is ahead mounted video camera, but without meaningful filtering, video images arenot suited as LSPI input. We will show that filters learned by SFA, based on arandom walk video of the robot, allow the learned control to navigatesuccessfully in ca. 80% of the test trials.
arxiv-300-137 | Weighted Patterns as a Tool for Improving the Hopfield Model | http://arxiv.org/pdf/1205.0908v1.pdf | author:Iakov Karandashev, Boris Kryzhanovsky, Leonid Litinskii category:cs.LG cs.NE published:2012-05-04 summary:We generalize the standard Hopfield model to the case when a weight isassigned to each input pattern. The weight can be interpreted as the frequencyof the pattern occurrence at the input of the network. In the framework of thestatistical physics approach we obtain the saddle-point equation allowing us toexamine the memory of the network. In the case of unequal weights our modeldoes not lead to the catastrophic destruction of the memory due to itsoverfilling (that is typical for the standard Hopfield model). The real memoryconsists only of the patterns with weights exceeding a critical value that isdetermined by the weights distribution. We obtain the algorithm allowing us tofind this critical value for an arbitrary distribution of the weights, andanalyze in detail some particular weights distributions. It is shown that thememory decreases as compared to the case of the standard Hopfield model.However, in our model the network can learn online without the catastrophicdestruction of the memory.
arxiv-300-138 | Variable Selection for Latent Dirichlet Allocation | http://arxiv.org/pdf/1205.1053v1.pdf | author:Dongwoo Kim, Yeonseung Chung, Alice Oh category:cs.LG stat.ML published:2012-05-04 summary:In latent Dirichlet allocation (LDA), topics are multinomial distributionsover the entire vocabulary. However, the vocabulary usually contains many wordsthat are not relevant in forming the topics. We adopt a variable selectionmethod widely used in statistical modeling as a dimension reduction tool andcombine it with LDA. In this variable selection model for LDA (vsLDA), topicsare multinomial distributions over a subset of the vocabulary, and by excludingwords that are not informative for finding the latent topic structure of thecorpus, vsLDA finds topics that are more robust and discriminative. We comparethree models, vsLDA, LDA with symmetric priors, and LDA with asymmetric priors,on heldout likelihood, MCMC chain consistency, and document classification. Theperformance of vsLDA is better than symmetric LDA for likelihood andclassification, better than asymmetric LDA for consistency and classification,and about the same in the other comparisons.
arxiv-300-139 | Discretization of a matrix in the problem of quadratic functional binary minimization | http://arxiv.org/pdf/1205.0732v1.pdf | author:Boris Kryzhanovsky, Mikhail Kryzhanovsky, Magomed Malsagov category:cs.NE published:2012-05-03 summary:The capability of discretization of matrix elements in the problem ofquadratic functional minimization with linear member built on matrix inN-dimensional configuration space with discrete coordinates is researched. Itis shown, that optimal procedure of replacement matrix elements by the integerquantities with the limited number of gradations exist, and the efficient ofminimization does not reduce. Parameter depends on matrix properties, whichallows estimate the capability of using described procedure for given type ofmatrix, is found. Computational complexities of algorithm and RAM requirementsare reduced by 16 times, correct using of integer elements allows increaseminimization algorithm speed by the orders.
arxiv-300-140 | Rule-weighted and terminal-weighted context-free grammars have identical expressivity | http://arxiv.org/pdf/1205.0627v1.pdf | author:Yann Ponty category:cs.CL published:2012-05-03 summary:Two formalisms, both based on context-free grammars, have recently beenproposed as a basis for a non-uniform random generation of combinatorialobjects. The former, introduced by Denise et al, associates weights withletters, while the latter, recently explored by Weinberg et al in the contextof random generation, associates weights to transitions. In this short note, weuse a simple modification of the Greibach Normal Form transformation algorithm,due to Blum and Koch, to show the equivalent expressivities, in term of theirinduced distributions, of these two formalisms.
arxiv-300-141 | Bayesian clustering in decomposable graphs | http://arxiv.org/pdf/1005.5081v2.pdf | author:Luke Bornn, FranÃ§ois Caron category:stat.ME stat.AP stat.ML published:2010-05-27 summary:In this paper we propose a class of prior distributions on decomposablegraphs, allowing for improved modeling flexibility. While existing methodssolely penalize the number of edges, the proposed work empowers practitionersto control clustering, level of separation, and other features of the graph.Emphasis is placed on a particular prior distribution which derives itsmotivation from the class of product partition models; the properties of thisprior relative to existing priors is examined through theory and simulation. Wethen demonstrate the use of graphical models in the field of agriculture,showing how the proposed prior distribution alleviates the inflexibility ofprevious approaches in properly modeling the interactions between the yield ofdifferent crop varieties.
arxiv-300-142 | An Evolutionary Approach to Drug-Design Using a Novel Neighbourhood Based Genetic Algorithm | http://arxiv.org/pdf/1205.6412v1.pdf | author:Arnab Ghosh, Avishek Ghosh, Arkabandhu Chowdhury, Amit Konar category:cs.NE cs.CE published:2012-05-03 summary:The present work provides a new approach to evolve ligand structures whichrepresent possible drug to be docked to the active site of the target protein.The structure is represented as a tree where each non-empty node represents afunctional group. It is assumed that the active site configuration of thetarget protein is known with position of the essential residues. In this paperthe interaction energy of the ligands with the protein target is minimized.Moreover, the size of the tree is difficult to obtain and it will be differentfor different active sites. To overcome the difficulty, a variable tree sizeconfiguration is used for designing ligands. The optimization is done using anovel Neighbourhood Based Genetic Algorithm (NBGA) which uses dynamicneighbourhood topology. To get variable tree size, a variable-length version ofthe above algorithm is devised. To judge the merit of the algorithm, it isinitially applied on the well known Travelling Salesman Problem (TSP).
arxiv-300-143 | Multi-robot Cooperative Box-pushing problem using Multi-objective Particle Swarm Optimization Technique | http://arxiv.org/pdf/1206.5170v1.pdf | author:Arnab Ghosh, Avishek Ghosh, Arkabandhu Chowdhury, Amit Konar, R. Janarthanan category:cs.RO cs.NE published:2012-05-03 summary:The present work provides a new approach to solve the well-known multi-robotco-operative box pushing problem as a multi objective optimization problemusing modified Multi-objective Particle Swarm Optimization. The method proposedhere allows both turning and translation of the box, during shift to a desiredgoal position. We have employed local planning scheme to determine themagnitude of the forces applied by the two mobile robots perpendicularly atspecific locations on the box to align and translate it in each distinct stepof motion of the box, for minimization of both time and energy. Finally theresults are compared with the results obtained by solving the same problemusing Non-dominated Sorting Genetic Algorithm-II (NSGA-II). The proposed schemeis found to give better results compared to NSGA-II.
arxiv-300-144 | An Evolutionary Approach to Drug-Design Using Quantam Binary Particle Swarm Optimization Algorithm | http://arxiv.org/pdf/1206.4588v1.pdf | author:Avishek Ghosh, Arnab Ghosh, Arkabandhu Chowdhury, Jubin Hazra category:cs.NE cs.CE published:2012-05-03 summary:The present work provides a new approach to evolve ligand structures whichrepresent possible drug to be docked to the active site of the target protein.The structure is represented as a tree where each non-empty node represents afunctional group. It is assumed that the active site configuration of thetarget protein is known with position of the essential residues. In this paperthe interaction energy of the ligands with the protein target is minimized.Moreover, the size of the tree is difficult to obtain and it will be differentfor different active sites. To overcome the difficulty, a variable tree sizeconfiguration is used for designing ligands. The optimization is done using aquantum discrete PSO. The result using fixed length and variable lengthconfiguration are compared.
arxiv-300-145 | Greedy Multiple Instance Learning via Codebook Learning and Nearest Neighbor Voting | http://arxiv.org/pdf/1205.0610v1.pdf | author:Gang Chen, Jason Corso category:cs.LG I.5.1 published:2012-05-03 summary:Multiple instance learning (MIL) has attracted great attention recently inmachine learning community. However, most MIL algorithms are very slow andcannot be applied to large datasets. In this paper, we propose a greedystrategy to speed up the multiple instance learning process. Our contributionis two fold. First, we propose a density ratio model, and show that maximizinga density ratio function is the low bound of the DD model under certainconditions. Secondly, we make use of a histogram ratio between positive bagsand negative bags to represent the density ratio function and find codebooksseparately for positive bags and negative bags by a greedy strategy. Fortesting, we make use of a nearest neighbor strategy to classify new bags. Wetest our method on both small benchmark datasets and the large TRECVID MED11dataset. The experimental results show that our method yields comparableaccuracy to the current state of the art, while being up to at least one orderof magnitude faster.
arxiv-300-146 | Minimax Classifier for Uncertain Costs | http://arxiv.org/pdf/1205.0406v1.pdf | author:Rui Wang, Ke Tang category:cs.LG published:2012-05-02 summary:Many studies on the cost-sensitive learning assumed that a unique cost matrixis known for a problem. However, this assumption may not hold for manyreal-world problems. For example, a classifier might need to be applied inseveral circumstances, each of which associates with a different cost matrix.Or, different human experts have different opinions about the costs for a givenproblem. Motivated by these facts, this study aims to seek the minimaxclassifier over multiple cost matrices. In summary, we theoretically provedthat, no matter how many cost matrices are involved, the minimax problem can betackled by solving a number of standard cost-sensitive problems andsub-problems that involve only two cost matrices. As a result, a generalframework for achieving minimax classifier over multiple cost matrices issuggested and justified by preliminary empirical studies.
arxiv-300-147 | Parametric annealing: a stochastic search method for human pose tracking | http://arxiv.org/pdf/1204.6563v2.pdf | author:Prabhu Kaliamoorthi, Ramakrishna Kakarala category:cs.CV published:2012-04-30 summary:Model based methods to marker-free motion capture have a very highcomputational overhead that make them unattractive. In this paper we describe amethod that improves on existing global optimization techniques to trackingarticulated objects. Our method improves on the state-of-the-art AnnealedParticle Filter (APF) by reusing samples across annealing layers and by usingan adaptive parametric density for diffusion. We compare the proposed methodwith APF on a scalable problem and study how the two methods scale with thedimensionality, multi-modality and the range of search. Then we performsensitivity analysis on the parameters of our algorithm and show that ittolerates a wide range of parameter settings. We also show results on trackinghuman pose from the widely-used Human Eva I dataset. Our results show that theproposed method reduces the tracking error despite using less than 50% of thecomputational resources as APF. The tracked output also shows a significantqualitative improvement over APF as demonstrated through image and videoresults.
arxiv-300-148 | Hybrid Linear Modeling via Local Best-fit Flats | http://arxiv.org/pdf/1010.3460v2.pdf | author:Teng Zhang, Arthur Szlam, Yi Wang, Gilad Lerman category:cs.CV stat.ML published:2010-10-17 summary:We present a simple and fast geometric method for modeling data by a union ofaffine subspaces. The method begins by forming a collection of local best-fitaffine subspaces, i.e., subspaces approximating the data in localneighborhoods. The correct sizes of the local neighborhoods are determinedautomatically by the Jones' $\beta_2$ numbers (we prove under certain geometricconditions that our method finds the optimal local neighborhoods). Thecollection of subspaces is further processed by a greedy selection procedure ora spectral method to generate the final model. We discuss applications totracking-based motion segmentation and clustering of faces under differentilluminating conditions. We give extensive experimental evidence demonstratingthe state of the art accuracy and speed of the suggested algorithms on theseproblems and also on synthetic hybrid linear data as well as the MNISThandwritten digits data; and we demonstrate how to use our algorithms for fastdetermination of the number of affine subspaces.
arxiv-300-149 | Learning the Dependence Graph of Time Series with Latent Factors | http://arxiv.org/pdf/1106.1887v4.pdf | author:Ali Jalali, Sujay Sanghavi category:cs.LG published:2011-06-09 summary:This paper considers the problem of learning, from samples, the dependencystructure of a system of linear stochastic differential equations, when some ofthe variables are latent. In particular, we observe the time evolution of somevariables, and never observe other variables; from this, we would like to findthe dependency structure between the observed variables - separating out thespurious interactions caused by the (marginalizing out of the) latentvariables' time series. We develop a new method, based on convex optimization,to do so in the case when the number of latent variables is smaller than thenumber of observed ones. For the case when the dependency structure between theobserved variables is sparse, we theoretically establish a high-dimensionalscaling result for structure recovery. We verify our theoretical result withboth synthetic and real data (from the stock market).
arxiv-300-150 | Hybrid Batch Bayesian Optimization | http://arxiv.org/pdf/1202.5597v3.pdf | author:Javad Azimi, Ali Jalali, Xiaoli Fern category:cs.AI cs.LG published:2012-02-25 summary:Bayesian Optimization aims at optimizing an unknown non-convex/concavefunction that is costly to evaluate. We are interested in application scenarioswhere concurrent function evaluations are possible. Under such a setting, BOcould choose to either sequentially evaluate the function, one input at a timeand wait for the output of the function before making the next selection, orevaluate the function at a batch of multiple inputs at once. These twodifferent settings are commonly referred to as the sequential and batchsettings of Bayesian Optimization. In general, the sequential setting leads tobetter optimization performance as each function evaluation is selected withmore information, whereas the batch setting has an advantage in terms of thetotal experimental time (the number of iterations). In this work, our goal isto combine the strength of both settings. Specifically, we systematicallyanalyze Bayesian optimization using Gaussian process as the posterior estimatorand provide a hybrid algorithm that, based on the current state, dynamicallyswitches between a sequential policy and a batch policy with variable batchsizes. We provide theoretical justification for our algorithm and presentexperimental results on eight benchmark BO problems. The results show that ourmethod achieves substantial speedup (up to %78) compared to a pure sequentialpolicy, without suffering any significant performance loss.
arxiv-300-151 | A Singly-Exponential Time Algorithm for Computing Nonnegative Rank | http://arxiv.org/pdf/1205.0044v1.pdf | author:Ankur Moitra category:cs.DS cs.IR cs.LG published:2012-04-30 summary:Here, we give an algorithm for deciding if the nonnegative rank of a matrix$M$ of dimension $m \times n$ is at most $r$ which runs in time$(nm)^{O(r^2)}$. This is the first exact algorithm that runs in timesingly-exponential in $r$. This algorithm (and earlier algorithms) are built onmethods for finding a solution to a system of polynomial inequalities (if oneexists). Notably, the best algorithms for this task run in time exponential inthe number of variables but polynomial in all of the other parameters (thenumber of inequalities and the maximum degree). Hence these algorithms motivate natural algebraic questions whose solutionhave immediate {\em algorithmic} implications: How many variables do we need torepresent the decision problem, does $M$ have nonnegative rank at most $r$? Anaive formulation uses $nr + mr$ variables and yields an algorithm that isexponential in $n$ and $m$ even for constant $r$. (Arora, Ge, Kannan, Moitra,STOC 2012) recently reduced the number of variables to $2r^2 2^r$, and here weexponentially reduce the number of variables to $2r^2$ and this yields our mainalgorithm. In fact, the algorithm that we obtain is nearly-optimal (under theExponential Time Hypothesis) since an algorithm that runs in time $(nm)^{o(r)}$would yield a subexponential algorithm for 3-SAT . Our main result is based on establishing a normal form for nonnegative matrixfactorization - which in turn allows us to exploit algebraic dependence among alarge collection of linear transformations with variable entries. Additionally,we also demonstrate that nonnegative rank cannot be certified by even a verylarge submatrix of $M$, and this property also follows from the intuitiongained from viewing nonnegative rank through the lens of systems of polynomialinequalities.
arxiv-300-152 | You had me at hello: How phrasing affects memorability | http://arxiv.org/pdf/1203.6360v2.pdf | author:Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon Kleinberg, Lillian Lee category:cs.CL cs.SI physics.soc-ph I.2.7; J.4 published:2012-03-28 summary:Understanding the ways in which information achieves widespread publicawareness is a research question of significant interest. We consider whether,and how, the way in which the information is phrased --- the choice of wordsand sentence structure --- can affect this process. To this end, we develop ananalysis framework and build a corpus of movie quotes, annotated withmemorability information, in which we are able to control for both the speakerand the setting of the quotes. We find that there are significant differencesbetween memorable and non-memorable quotes in several key dimensions, evenafter controlling for situational and contextual factors. One is lexicaldistinctiveness: in aggregate, memorable quotes use less common word choices,but at the same time are built upon a scaffolding of common syntactic patterns.Another is that memorable quotes tend to be more general in ways that make themeasy to apply in new contexts --- that is, more portable. We also show how theconcept of "memorable language" can be extended across domains.
arxiv-300-153 | Elimination of Glass Artifacts and Object Segmentation | http://arxiv.org/pdf/1204.6653v1.pdf | author:Vini Katyal, Aviral, Deepesh Srivastava category:cs.CV published:2012-04-30 summary:Many images nowadays are captured from behind the glasses and may havecertain stains discrepancy because of glass and must be processed to makedifferentiation between the glass and objects behind it. This research paperproposes an algorithm to remove the damaged or corrupted part of the image andmake it consistent with other part of the image and to segment objects behindthe glass. The damaged part is removed using total variation inpainting methodand segmentation is done using kmeans clustering, anisotropic diffusion andwatershed transformation. The final output is obtained by interpolation. Thisalgorithm can be useful to applications in which some part of the images arecorrupted due to data transmission or needs to segment objects from an imagefor further processing.
arxiv-300-154 | Residual Belief Propagation for Topic Modeling | http://arxiv.org/pdf/1204.6610v1.pdf | author:Jia Zeng, Xiao-Qin Cao, Zhi-Qiang Liu category:cs.LG cs.IR published:2012-04-30 summary:Fast convergence speed is a desired property for training latent Dirichletallocation (LDA), especially in online and parallel topic modeling for massivedata sets. This paper presents a novel residual belief propagation (RBP)algorithm to accelerate the convergence speed for training LDA. The proposedRBP uses an informed scheduling scheme for asynchronous message passing, whichpasses fast-convergent messages with a higher priority to influence thoseslow-convergent messages at each learning iteration. Extensive empiricalstudies confirm that RBP significantly reduces the training time untilconvergence while achieves a much lower predictive perplexity than otherstate-of-the-art training algorithms for LDA, including variational Bayes (VB),collapsed Gibbs sampling (GS), loopy belief propagation (BP), and residual VB(RVB).
arxiv-300-155 | A Conjugate Property between Loss Functions and Uncertainty Sets in Classification Problems | http://arxiv.org/pdf/1204.6583v1.pdf | author:Takafumi Kanamori, Akiko Takeda, Taiji Suzuki category:stat.ML cs.LG published:2012-04-30 summary:In binary classification problems, mainly two approaches have been proposed;one is loss function approach and the other is uncertainty set approach. Theloss function approach is applied to major learning algorithms such as supportvector machine (SVM) and boosting methods. The loss function represents thepenalty of the decision function on the training samples. In the learningalgorithm, the empirical mean of the loss function is minimized to obtain theclassifier. Against a backdrop of the development of mathematical programming,nowadays learning algorithms based on loss functions are widely applied toreal-world data analysis. In addition, statistical properties of such learningalgorithms are well-understood based on a lots of theoretical works. On theother hand, the learning method using the so-called uncertainty set is used inhard-margin SVM, mini-max probability machine (MPM) and maximum margin MPM. Inthe learning algorithm, firstly, the uncertainty set is defined for each binarylabel based on the training samples. Then, the best separating hyperplanebetween the two uncertainty sets is employed as the decision function. This isregarded as an extension of the maximum-margin approach. The uncertainty setapproach has been studied as an application of robust optimization in the fieldof mathematical programming. The statistical properties of learning algorithmswith uncertainty sets have not been intensively studied. In this paper, weconsider the relation between the above two approaches. We point out that theuncertainty set is described by using the level set of the conjugate of theloss function. Based on such relation, we study statistical properties oflearning algorithms using uncertainty sets.
arxiv-300-156 | Recovery of Low-Rank Plus Compressed Sparse Matrices with Application to Unveiling Traffic Anomalies | http://arxiv.org/pdf/1204.6537v1.pdf | author:Morteza Mardani, Gonzalo Mateos, Georgios B. Giannakis category:cs.IT cs.NI math.IT stat.ML published:2012-04-30 summary:Given the superposition of a low-rank matrix plus the product of a known fatcompression matrix times a sparse matrix, the goal of this paper is toestablish deterministic conditions under which exact recovery of the low-rankand sparse components becomes possible. This fundamental identifiability issuearises with traffic anomaly detection in backbone networks, and subsumescompressed sensing as well as the timely low-rank plus sparse matrix recoverytasks encountered in matrix decomposition problems. Leveraging the ability of$\ell_1$- and nuclear norms to recover sparse and low-rank matrices, a convexprogram is formulated to estimate the unknowns. Analysis and simulationsconfirm that the said convex program can recover the unknowns for sufficientlylow-rank and sparse enough components, along with a compression matrixpossessing an isometry property when restricted to operate on sparse vectors.When the low-rank, sparse, and compression matrices are drawn from certainrandom ensembles, it is established that exact recovery is possible with highprobability. First-order algorithms are developed to solve the nonsmooth convexoptimization problem with provable iteration complexity guarantees. Insightfultests with synthetic and real network data corroborate the effectiveness of thenovel approach in unveiling traffic anomalies across flows and time, and itsability to outperform existing alternatives.
arxiv-300-157 | Dissimilarity Clustering by Hierarchical Multi-Level Refinement | http://arxiv.org/pdf/1204.6509v1.pdf | author:Brieuc Conan-Guez, Fabrice Rossi category:stat.ML cs.LG published:2012-04-29 summary:We introduce in this paper a new way of optimizing the natural extension ofthe quantization error using in k-means clustering to dissimilarity data. Theproposed method is based on hierarchical clustering analysis combined withmulti-level heuristic refinement. The method is computationally efficient andachieves better quantization errors than the
arxiv-300-158 | Active Contour with A Tangential Component | http://arxiv.org/pdf/1204.6458v1.pdf | author:Junyan Wang, Kap Luk Chan category:cs.CV published:2012-04-29 summary:Conventional edge-based active contours often require the normal component ofan edge indicator function on the optimal contours to approximate zero, whilethe tangential component can still be significant. In real images, the fullgradients of the edge indicator function along the object boundaries are oftensmall. Hence, the curve evolution of edge-based active contours can terminateearly before converging to the object boundaries with a careless contourinitialization. We propose a novel Geodesic Snakes (GeoSnakes) active contourthat requires the full gradients of the edge indicator to vanish at the optimalsolution. Besides, the conventional curve evolution approach for minimizingactive contour energy cannot fully solve the Euler-Lagrange (EL) equation ofour GeoSnakes active contour, causing a Pseudo Stationary Phenomenon (PSP). Toaddress the PSP problem, we propose an auxiliary curve evolution equation,named the equilibrium flow (EF) equation. Based on the EF and the conventionalcurve evolution, we obtain a solution to the full EL equation of GeoSnakesactive contour. Experimental results validate the proposed geometricalinterpretation of the early termination problem, and they also show that theproposed method overcomes the problem.
arxiv-300-159 | "I Wanted to Predict Elections with Twitter and all I got was this Lousy Paper" -- A Balanced Survey on Election Prediction using Twitter Data | http://arxiv.org/pdf/1204.6441v1.pdf | author:Daniel Gayo-Avello category:cs.CY cs.CL cs.SI physics.soc-ph published:2012-04-28 summary:Predicting X from Twitter is a popular fad within the Twitter researchsubculture. It seems both appealing and relatively easy. Among such kind ofstudies, electoral prediction is maybe the most attractive, and at this momentthere is a growing body of literature on such a topic. This is not only aninteresting research problem but, above all, it is extremely difficult.However, most of the authors seem to be more interested in claiming positiveresults than in providing sound and reproducible methods. It is also especiallyworrisome that many recent papers seem to only acknowledge those studiessupporting the idea of Twitter predicting elections, instead of conducting abalanced literature review showing both sides of the matter. After reading manyof such papers I have decided to write such a survey myself. Hence, in thispaper, every study relevant to the matter of electoral prediction using socialmedia is commented. From this review it can be concluded that the predictivepower of Twitter regarding elections has been greatly exaggerated, and thathard research problems still lie ahead.
arxiv-300-160 | A 3D Segmentation Method for Retinal Optical Coherence Tomography Volume Data | http://arxiv.org/pdf/1204.6385v1.pdf | author:Yankui Sun, Tian Zhang category:cs.CV physics.optics published:2012-04-28 summary:With the introduction of spectral-domain optical coherence tomography (OCT),much larger image datasets are routinely acquired compared to what was possibleusing the previous generation of time-domain OCT. Thus, the need for 3-Dsegmentation methods for processing such data is becoming increasinglyimportant. We present a new 3D segmentation method for retinal OCT volume data,which generates an enhanced volume data by using pixel intensity, boundaryposition information, intensity changes on both sides of the bordersimultaneously, and preliminary discrete boundary points are found from allA-Scans and then the smoothed boundary surface can be obtained after removing asmall quantity of error points. Our experiments show that this method isefficient, accurate and robust.
arxiv-300-161 | A Corpus-based Evaluation of a Domain-specific Text to Knowledge Mapping Prototype | http://arxiv.org/pdf/1204.6364v1.pdf | author:Rushdi Shams, Adel Elsayed, Quazi Mah-Zereen Akter category:cs.CL published:2012-04-28 summary:The aim of this paper is to evaluate a Text to Knowledge Mapping (TKM)Prototype. The prototype is domain-specific, the purpose of which is to mapinstructional text onto a knowledge domain. The context of the knowledge domainis DC electrical circuit. During development, the prototype has been testedwith a limited data set from the domain. The prototype reached a stage where itneeds to be evaluated with a representative linguistic data set called corpus.A corpus is a collection of text drawn from typical sources which can be usedas a test data set to evaluate NLP systems. As there is no available corpus forthe domain, we developed and annotated a representative corpus. The evaluationof the prototype considers two of its major components- lexical components andknowledge model. Evaluation on lexical components enriches the lexicalresources of the prototype like vocabulary and grammar structures. This leadsthe prototype to parse a reasonable amount of sentences in the corpus. Whiledealing with the lexicon was straight forward, the identification andextraction of appropriate semantic relations was much more involved. It wasnecessary, therefore, to manually develop a conceptual structure for the domainto formulate a domain-specific framework of semantic relations. The frameworkof semantic relationsthat has resulted from this study consisted of 55relations, out of which 42 have inverse relations. We also conducted rhetoricalanalysis on the corpus to prove its representativeness in conveying semantic.Finally, we conducted a topical and discourse analysis on the corpus to analyzethe coverage of discourse by the prototype.
arxiv-300-162 | A Corpus-based Evaluation of Lexical Components of a Domainspecific Text to Knowledge Mapping Prototype | http://arxiv.org/pdf/1204.6362v1.pdf | author:Rushdi Shams, Adel Elsayed category:cs.IR cs.CL published:2012-04-28 summary:The aim of this paper is to evaluate the lexical components of a Text toKnowledge Mapping (TKM) prototype. The prototype is domain-specific, thepurpose of which is to map instructional text onto a knowledge domain. Thecontext of the knowledge domain of the prototype is physics, specifically DCelectrical circuits. During development, the prototype has been tested with alimited data set from the domain. The prototype now reached a stage where itneeds to be evaluated with a representative linguistic data set called corpus.A corpus is a collection of text drawn from typical sources which can be usedas a test data set to evaluate NLP systems. As there is no available corpus forthe domain, we developed a representative corpus and annotated it withlinguistic information. The evaluation of the prototype considers one of itstwo main components- lexical knowledge base. With the corpus, the evaluationenriches the lexical knowledge resources like vocabulary and grammar structure.This leads the prototype to parse a reasonable amount of sentences in thecorpus.
arxiv-300-163 | PAC learnability under non-atomic measures: a problem by Vidyasagar | http://arxiv.org/pdf/1105.5669v3.pdf | author:Vladimir Pestov category:stat.ML 68T05 I.2.6 published:2011-05-27 summary:In response to a 1997 problem of M. Vidyasagar, we state a criterion for PAClearnability of a concept class $\mathscr C$ under the family of all non-atomic(diffuse) measures on the domain $\Omega$. The uniform Glivenko--Cantelliproperty with respect to non-atomic measures is no longer a necessarycondition, and consistent learnability cannot in general be expected. Ourcriterion is stated in terms of a combinatorial parameter $\VC({\mathscrC}\,{\mathrm{mod}}\,\omega_1)$ which we call the VC dimension of $\mathscr C$modulo countable sets. The new parameter is obtained by "thickening up" singlepoints in the definition of VC dimension to uncountable "clusters".Equivalently, $\VC(\mathscr C\modd\omega_1)\leq d$ if and only if everycountable subclass of $\mathscr C$ has VC dimension $\leq d$ outside acountable subset of $\Omega$. The new parameter can be also expressed as theclassical VC dimension of $\mathscr C$ calculated on a suitable subset of acompactification of $\Omega$. We do not make any measurability assumptions on$\mathscr C$, assuming instead the validity of Martin's Axiom (MA). Similarresults are obtained for function learning in terms of fat-shattering dimensionmodulo countable sets, but, just like in the classical distribution-free case,the finiteness of this parameter is sufficient but not necessary for PAClearnability under non-atomic measures.
arxiv-300-164 | The conduciveness of CA-rule graphs | http://arxiv.org/pdf/1204.6181v1.pdf | author:Valmir C. Barbosa category:nlin.CG cs.NE published:2012-04-27 summary:Given two subsets A and B of nodes in a directed graph, the conduciveness ofthe graph from A to B is the ratio representing how many of the edges outgoingfrom nodes in A are incoming to nodes in B. When the graph's nodes stand forthe possible solutions to certain problems of combinatorial optimization,choosing its edges appropriately has been shown to lead to conducivenessproperties that provide useful insight into the performance of algorithms tosolve those problems. Here we study the conduciveness of CA-rule graphs, thatis, graphs whose node set is the set of all CA rules given a cell's number ofpossible states and neighborhood size. We consider several different edge setsinterconnecting these nodes, both deterministic and random ones, and deriveanalytical expressions for the resulting graph's conduciveness toward ruleshaving a fixed number of non-quiescent entries. We demonstrate that one of therandom edge sets, characterized by allowing nodes to be sparsely interconnectedacross any Hamming distance between the corresponding rules, has the potentialof providing reasonable conduciveness toward the desired rules. We conjecturethat this may lie at the bottom of the best strategies known to date fordiscovering complex rules to solve specific problems, all of an evolutionarynature.
arxiv-300-165 | A consistent adjacency spectral embedding for stochastic blockmodel graphs | http://arxiv.org/pdf/1108.2228v3.pdf | author:Daniel L. Sussman, Minh Tang, Donniell E. Fishkind, Carey E. Priebe category:stat.ML published:2011-08-10 summary:We present a method to estimate block membership of nodes in a random graphgenerated by a stochastic blockmodel. We use an embedding procedure motivatedby the random dot product graph model, a particular example of the latentposition model. The embedding associates each node with a vector; these vectorsare clustered via minimization of a square error criterion. We prove that thismethod is consistent for assigning nodes to blocks, as only a negligible numberof nodes will be mis-assigned. We prove consistency of the method for directedand undirected graphs. The consistent block assignment makes possibleconsistent parameter estimation for a stochastic blockmodel. We extend theresult in the setting where the number of blocks grows slowly with the numberof nodes. Our method is also computationally feasible even for very largegraphs. We compare our method to Laplacian spectral clustering through analysisof simulated data and a graph derived from Wikipedia documents.
arxiv-300-166 | Distributed GraphLab: A Framework for Machine Learning in the Cloud | http://arxiv.org/pdf/1204.6078v1.pdf | author:Yucheng Low, Joseph Gonzalez, Aapo Kyrola, Danny Bickson, Carlos Guestrin, Joseph M. Hellerstein category:cs.DB cs.LG published:2012-04-26 summary:While high-level data parallel frameworks, like MapReduce, simplify thedesign and implementation of large-scale data processing systems, they do notnaturally or efficiently support many important data mining and machinelearning algorithms and can lead to inefficient learning systems. To help fillthis critical void, we introduced the GraphLab abstraction which naturallyexpresses asynchronous, dynamic, graph-parallel computation while ensuring dataconsistency and achieving a high degree of parallel performance in theshared-memory setting. In this paper, we extend the GraphLab framework to thesubstantially more challenging distributed setting while preserving strong dataconsistency guarantees. We develop graph based extensions to pipelined lockingand data versioning to reduce network congestion and mitigate the effect ofnetwork latency. We also introduce fault tolerance to the GraphLab abstractionusing the classic Chandy-Lamport snapshot algorithm and demonstrate how it canbe easily implemented by exploiting the GraphLab abstraction itself. Finally,we evaluate our distributed implementation of the GraphLab abstraction on alarge Amazon EC2 deployment and show 1-2 orders of magnitude performance gainsover Hadoop-based implementations.
arxiv-300-167 | Managing contextual artificial neural networks with a service-based mediator | http://arxiv.org/pdf/1204.0262v2.pdf | author:Greg Fish category:cs.NE 97R40 B.6.0 published:2012-04-01 summary:Today, a wide variety of probabilistic and expert AI systems used to analyzereal world inputs such as unstructured text, sounds, images, and statisticaldata. However, all these systems exist on different platforms, with differentimplementations, and with very different, often very specific goals in mind.This paper introduces a concept for a mediator framework for such systems andseeks to show several architectures which would support it, potential benefitsin combining the signals of disparate networks for formalized, high level logicand signal processing, and its possible academic and industrial uses.
arxiv-300-168 | Towards Analyzing Crossover Operators in Evolutionary Search via General Markov Chain Switching Theorem | http://arxiv.org/pdf/1111.0907v2.pdf | author:Yang Yu, Chao Qian, Zhi-Hua Zhou category:cs.NE published:2011-11-03 summary:Evolutionary algorithms (EAs), simulating the evolution process of naturalspecies, are used to solve optimization problems. Crossover (also calledrecombination), originated from simulating the chromosome exchange phenomena inzoogamy reproduction, is widely employed in EAs to generate offspringsolutions, of which the effectiveness has been examined empirically inapplications. However, due to the irregularity of crossover operators and thecomplicated interactions to mutation, crossover operators are hard to analyzeand thus have few theoretical results. Therefore, analyzing crossover not onlyhelps in understanding EAs, but also helps in developing novel techniques foranalyzing sophisticated metaheuristic algorithms. In this paper, we derive the General Markov Chain Switching Theorem (GMCST)to facilitate theoretical studies of crossover-enabled EAs. The theorem allowsus to analyze the running time of a sophisticated EA from an easy-to-analyzeEA. Using this tool, we analyze EAs with several crossover operators on theLeadingOnes and OneMax problems, which are noticeably two well studied problemsfor mutation-only EAs but with few results for crossover-enabled EAs. We firstderive the bounds of running time of the (2+2)-EA with crossover operators;then we study the running time gap between the mutation-only (2:2)-EA and the(2:2)-EA with crossover operators; finally, we develop strategies that applycrossover operators only when necessary, which improve from the mutation-onlyas well as the crossover-all-the-time (2:2)-EA. The theoretical results areverified by experiments.
arxiv-300-169 | Context-sensitive Spelling Correction Using Google Web 1T 5-Gram Information | http://arxiv.org/pdf/1204.5852v1.pdf | author:Youssef Bassil, Mohammad Alwani category:cs.CL published:2012-04-26 summary:In computing, spell checking is the process of detecting and sometimesproviding spelling suggestions for incorrectly spelled words in a text.Basically, a spell checker is a computer program that uses a dictionary ofwords to perform spell checking. The bigger the dictionary is, the higher isthe error detection rate. The fact that spell checkers are based on regulardictionaries, they suffer from data sparseness problem as they cannot capturelarge vocabulary of words including proper names, domain-specific terms,technical jargons, special acronyms, and terminologies. As a result, theyexhibit low error detection rate and often fail to catch major errors in thetext. This paper proposes a new context-sensitive spelling correction methodfor detecting and correcting non-word and real-word errors in digital textdocuments. The approach hinges around data statistics from Google Web 1T 5-gramdata set which consists of a big volume of n-gram word sequences, extractedfrom the World Wide Web. Fundamentally, the proposed method comprises an errordetector that detects misspellings, a candidate spellings generator based on acharacter 2-gram model that generates correction suggestions, and an errorcorrector that performs contextual error correction. Experiments conducted on aset of text documents from different domains and containing misspellings,showed an outstanding spelling error correction rate and a drastic reduction ofboth non-word and real-word errors. In a further study, the proposed algorithmis to be parallelized so as to lower the computational cost of the errordetection and correction processes.
arxiv-300-170 | Emergent Criticality Through Adaptive Information Processing in Boolean Networks | http://arxiv.org/pdf/1104.4141v2.pdf | author:Alireza Goudarzi, Christof Teuscher, Natali Gulbahce, Thimo Rohlf category:cs.NE nlin.AO published:2011-04-20 summary:We study information processing in populations of Boolean networks withevolving connectivity and systematically explore the interplay between thelearning capability, robustness, the network topology, and the task complexity.We solve a long-standing open question and find computationally that, for largesystem sizes $N$, adaptive information processing drives the networks to acritical connectivity $K_{c}=2$. For finite size networks, the connectivityapproaches the critical value with a power-law of the system size $N$. We showthat network learning and generalization are optimized near criticality, giventask complexity and the amount of information provided threshold values. Bothrandom and evolved networks exhibit maximal topological diversity near $K_{c}$.We hypothesize that this supports efficient exploration and robustness ofsolutions. Also reflected in our observation is that the variance of the valuesis maximal in critical network populations. Finally, we discuss implications ofour results for determining the optimal topology of adaptive dynamical networksthat solve computational tasks.
arxiv-300-171 | Geometry of Online Packing Linear Programs | http://arxiv.org/pdf/1204.5810v1.pdf | author:Marco Molinaro, R. Ravi category:cs.DS cs.LG published:2012-04-26 summary:We consider packing LP's with $m$ rows where all constraint coefficients arenormalized to be in the unit interval. The n columns arrive in random order andthe goal is to set the corresponding decision variables irrevocably when theyarrive so as to obtain a feasible solution maximizing the expected reward.Previous (1 - \epsilon)-competitive algorithms require the right-hand side ofthe LP to be Omega((m/\epsilon^2) log (n/\epsilon)), a bound that worsens withthe number of columns and rows. However, the dependence on the number ofcolumns is not required in the single-row case and known lower bounds for thegeneral case are also independent of n. Our goal is to understand whether the dependence on n is required in themulti-row case, making it fundamentally harder than the single-row version. Werefute this by exhibiting an algorithm which is (1 - \epsilon)-competitive aslong as the right-hand sides are Omega((m^2/\epsilon^2) log (m/\epsilon)). Ourtechniques refine previous PAC-learning based approaches which interpret theonline decisions as linear classifications of the columns based on sampled dualprices. The key ingredient of our improvement comes from a non-standardcovering argument together with the realization that only when the columns ofthe LP belong to few 1-d subspaces we can obtain small such covers; boundingthe size of the cover constructed also relies on the geometry of linearclassifiers. General packing LP's are handled by perturbing the input columns,which can be seen as making the learning problem more robust.
arxiv-300-172 | Quantitative Concept Analysis | http://arxiv.org/pdf/1204.5802v1.pdf | author:Dusko Pavlovic category:cs.LG math.CT 18D20, 06B23 I.2.6 published:2012-04-26 summary:Formal Concept Analysis (FCA) begins from a context, given as a binaryrelation between some objects and some attributes, and derives a lattice ofconcepts, where each concept is given as a set of objects and a set ofattributes, such that the first set consists of all objects that satisfy allattributes in the second, and vice versa. Many applications, though, providecontexts with quantitative information, telling not just whether an objectsatisfies an attribute, but also quantifying this satisfaction. Contexts inthis form arise as rating matrices in recommender systems, as occurrencematrices in text analysis, as pixel intensity matrices in digital imageprocessing, etc. Such applications have attracted a lot of attention, andseveral numeric extensions of FCA have been proposed. We propose the frameworkof proximity sets (proxets), which subsume partially ordered sets (posets) aswell as metric spaces. One feature of this approach is that it extracts fromquantified contexts quantified concepts, and thus allows full use of theavailable information. Another feature is that the categorical approach allowsanalyzing any universal properties that the classical FCA and the new versionsmay have, and thus provides structural guidance for aligning and combining theapproaches.
arxiv-300-173 | Oracle inequalities and minimax rates for non-local means and related adaptive kernel-based methods | http://arxiv.org/pdf/1112.4434v2.pdf | author:Ery Arias-Castro, Joseph Salmon, Rebecca Willett category:math.ST cs.CV cs.IT math.IT stat.TH published:2011-12-19 summary:This paper describes a novel theoretical characterization of the performanceof non-local means (NLM) for noise removal. NLM has proven effective in avariety of empirical studies, but little is understood fundamentally about howit performs relative to classical methods based on wavelets or how variousparameters (e.g., patch size) should be chosen. For cartoon images and imageswhich may contain thin features and regular textures, the error decay rates ofNLM are derived and compared with those of linear filtering, oracle estimators,variable-bandwidth kernel methods, Yaroslavsky's filter and waveletthresholding estimators. The trade-off between global and local search formatching patches is examined, and the bias reduction associated with the localpolynomial regression version of NLM is analyzed. The theoretical results arevalidated via simulations for 2D images corrupted by additive white Gaussiannoise.
arxiv-300-174 | Divide-and-Conquer Method for L1 Norm Matrix Factorization in the Presence of Outliers and Missing Data | http://arxiv.org/pdf/1202.5844v3.pdf | author:Deyu Meng, Zongben Xu category:cs.NA cs.CV published:2012-02-27 summary:The low-rank matrix factorization as a L1 norm minimization problem hasrecently attracted much attention due to its intrinsic robustness to thepresence of outliers and missing data. In this paper, we propose a new method,called the divide-and-conquer method, for solving this problem. The main ideais to break the original problem into a series of smallest possiblesub-problems, each involving only unique scalar parameter. Each of thesesubproblems is proved to be convex and has closed-form solution. By recursivelyoptimizing these small problems in an analytical way, efficient algorithm,entirely avoiding the time-consuming numerical optimization as an inner loop,for solving the original problem can naturally be constructed. Thecomputational complexity of the proposed algorithm is approximately linear inboth data size and dimensionality, making it possible to handle large-scale L1norm matrix factorization problems. The algorithm is also theoretically provedto be convergent. Based on a series of experiment results, it is substantiatedthat our method always achieves better results than the currentstate-of-the-art methods on $L1$ matrix factorization calculation in bothcomputational time and accuracy, especially on large-scale applications such asface recognition and structure from motion.
arxiv-300-175 | A New Approach of Improving CFA Image for Digital Camera's | http://arxiv.org/pdf/1204.5416v1.pdf | author:Manoj Kumar, Vikas Kaushik, Pradeep Singla category:cs.CV published:2012-04-24 summary:This paper work directly towards the improving the quality of the image forthe digital cameras and other visual capturing products. In this Paper, theauthors clearly defines the problems occurs in the CFA image. A differentmethodology for removing the noise is discuses in the paper for colorcorrection and color balancing of the image. At the same time, the authors alsoproposed a new methodology of providing denoisiing process before thedemosaickingfor the improving the image quality of CFA which is much efficientthen the other previous defined. The demosaicking process for producing thecolors in the image in a best way is also discuss.
arxiv-300-176 | Ecological Evaluation of Persuasive Messages Using Google AdWords | http://arxiv.org/pdf/1204.5369v1.pdf | author:Marco Guerini, Carlo Strapparava, Oliviero Stock category:cs.CL cs.SI I.2.7 published:2012-04-24 summary:In recent years there has been a growing interest in crowdsourcingmethodologies to be used in experimental research for NLP tasks. In particular,evaluation of systems and theories about persuasion is difficult to accommodatewithin existing frameworks. In this paper we present a new cheap and fastmethodology that allows fast experiment building and evaluation withfully-automated analysis at a low cost. The central idea is exploiting existingcommercial tools for advertising on the web, such as Google AdWords, to measuremessage impact in an ecological setting. The paper includes a description ofthe approach, tips for how to use AdWords for scientific research, and resultsof pilot experiments on the impact of affective text variations which confirmthe effectiveness of the approach.
arxiv-300-177 | Learning AMP Chain Graphs under Faithfulness | http://arxiv.org/pdf/1204.5357v1.pdf | author:Jose M. PeÃ±a category:stat.ML cs.AI math.ST stat.TH published:2012-04-24 summary:This paper deals with chain graphs under the alternativeAndersson-Madigan-Perlman (AMP) interpretation. In particular, we present aconstraint based algorithm for learning an AMP chain graph a given probabilitydistribution is faithful to. We also show that the extension of Meek'sconjecture to AMP chain graphs does not hold, which compromises the developmentof efficient and correct score+search learning algorithms under assumptionsweaker than faithfulness.
arxiv-300-178 | Hyperspectral Unmixing Overview: Geometrical, Statistical, and Sparse Regression-Based Approaches | http://arxiv.org/pdf/1202.6294v2.pdf | author:JosÃ© M. Bioucas-Dias, Antonio Plaza, Nicolas Dobigeon, Mario Parente, Qian Du, Paul Gader, Jocelyn Chanussot category:math.OC stat.AP stat.ML published:2012-02-28 summary:Imaging spectrometers measure electromagnetic energy scattered in theirinstantaneous field view in hundreds or thousands of spectral channels withhigher spectral resolution than multispectral cameras. Imaging spectrometersare therefore often referred to as hyperspectral cameras (HSCs). Higherspectral resolution enables material identification via spectroscopic analysis,which facilitates countless applications that require identifying materials inscenarios unsuitable for classical spectroscopic analysis. Due to low spatialresolution of HSCs, microscopic material mixing, and multiple scattering,spectra measured by HSCs are mixtures of spectra of materials in a scene. Thus,accurate estimation requires unmixing. Pixels are assumed to be mixtures of afew materials, called endmembers. Unmixing involves estimating all or some of:the number of endmembers, their spectral signatures, and their abundances ateach pixel. Unmixing is a challenging, ill-posed inverse problem because ofmodel inaccuracies, observation noise, environmental conditions, endmembervariability, and data set size. Researchers have devised and investigated manymodels searching for robust, stable, tractable, and accurate unmixingalgorithms. This paper presents an overview of unmixing methods from the timeof Keshava and Mustard's unmixing tutorial [1] to the present. Mixing modelsare first discussed. Signal-subspace, geometrical, statistical, sparsity-based,and spatial-contextual unmixing algorithms are described. Mathematical problemsand potential solutions are described. Algorithm characteristics areillustrated experimentally.
arxiv-300-179 | Time-dependent wave selection for information processing in excitable media | http://arxiv.org/pdf/1204.5345v1.pdf | author:William M. Stevens, Andrew Adamatzky, Ishrat Jahan, Ben de Lacy Costello category:nlin.PS cs.CL published:2012-04-24 summary:We demonstrate an improved technique for implementing logic circuits inlight-sensitive chemical excitable media. The technique makes use of theconstant-speed propagation of waves along defined channels in an excitablemedium based on the Belousov-Zhabotinsky reaction, along with the mutualannihilation of colliding waves. What distinguishes this work from previouswork in this area is that regions where channels meet at a junction canperiodically alternate between permitting the propagation of waves and blockingthem. These valve-like areas are used to select waves based on the length oftime that it takes waves to propagate from one valve to another. In anexperimental implementation, the channels which make up the circuit layout areprojected by a digital projector connected to a computer. Excitable channelsare projected as dark areas, unexcitable regions as light areas. Valvesalternate between dark and light: every valve has the same period and phase,with a 50% duty cycle. This scheme can be used to make logic gates based oncombinations of OR and AND-NOT operations, with few geometrical constraints.Because there are few geometrical constraints, compact circuits can beimplemented. Experimental results from an implementation of a 4-bit input,2-bit output integer square root circuit are given. This is the most complexlogic circuit that has been implemented in BZ excitable media to date.
arxiv-300-180 | ILexicOn: toward an ECD-compliant interlingual lexical ontology described with semantic web formalisms | http://arxiv.org/pdf/1204.5316v1.pdf | author:Maxime LefranÃ§ois, Fabien Gandon category:cs.CL cs.AI published:2012-04-24 summary:We are interested in bridging the world of natural language and the world ofthe semantic web in particular to support natural multilingual access to theweb of data. In this paper we introduce a new type of lexical ontology calledinterlingual lexical ontology (ILexicOn), which uses semantic web formalisms tomake each interlingual lexical unit class (ILUc) support the projection of itssemantic decomposition on itself. After a short overview of existing lexicalontologies, we briefly introduce the semantic web formalisms we use. We thenpresent the three layered architecture of our approach: i) the interlinguallexical meta-ontology (ILexiMOn); ii) the ILexicOn where ILUcs are formallydefined; iii) the data layer. We illustrate our approach with a standaloneILexicOn, and introduce and explain a concise human-readable notation torepresent ILexicOns. Finally, we show how semantic web formalisms enable theprojection of a semantic decomposition on the decomposed ILUc.
arxiv-300-181 | Black-box optimization benchmarking of IPOP-saACM-ES and BIPOP-saACM-ES on the BBOB-2012 noiseless testbed | http://arxiv.org/pdf/1206.5780v1.pdf | author:Ilya Loshchilov, Marc Schoenauer, MichÃ¨le Sebag category:cs.NE published:2012-04-24 summary:In this paper, we study the performance of IPOP-saACM-ES and BIPOP-saACM-ES,recently proposed self-adaptive surrogate-assisted Covariance Matrix AdaptationEvolution Strategies. Both algorithms were tested using restarts till a totalnumber of function evaluations of $10^6D$ was reached, where $D$ is thedimension of the function search space. We compared surrogate-assistedalgorithms with their surrogate-less versions IPOP-saACM-ES and BIPOP-saACM-ES,two algorithms with one of the best overall performance observed during theBBOB-2009 and BBOB-2010. The comparison shows that the surrogate-assistedversions outperform the original CMA-ES algorithms by a factor from 2 to 4 on 8out of 24 noiseless benchmark problems, showing the best results among allalgorithms of the BBOB-2009 and BBOB-2010 on Ellipsoid, Discus, Bent Cigar,Sharp Ridge and Sum of different powers functions.
arxiv-300-182 | Black-box optimization benchmarking of IPOP-saACM-ES on the BBOB-2012 noisy testbed | http://arxiv.org/pdf/1206.0974v1.pdf | author:Ilya Loshchilov, Marc Schoenauer, MichÃ¨le Sebag category:cs.NE published:2012-04-24 summary:In this paper, we study the performance of IPOP-saACM-ES, recently proposedself-adaptive surrogate-assisted Covariance Matrix Adaptation EvolutionStrategy. The algorithm was tested using restarts till a total number offunction evaluations of $10^6D$ was reached, where $D$ is the dimension of thefunction search space. The experiments show that the surrogate model controlallows IPOP-saACM-ES to be as robust as the original IPOP-aCMA-ES andoutperforms the latter by a factor from 2 to 3 on 6 benchmark problems withmoderate noise. On 15 out of 30 benchmark problems in dimension 20,IPOP-saACM-ES exceeds the records observed during BBOB-2009 and BBOB-2010.
arxiv-300-183 | Modeling, dependence, classification, united statistical science, many cultures | http://arxiv.org/pdf/1204.4699v3.pdf | author:Emanuel Parzen, Subhadeep Mukhopadhyay category:math.ST stat.ME stat.ML stat.TH 62Gxx published:2012-04-20 summary:Breiman (2001) proposed to statisticians awareness of two cultures: 1.Parametric modeling culture, pioneered by R.A.Fisher and Jerzy Neyman; 2.Algorithmic predictive culture, pioneered by machine learning research. Parzen (2001), as a part of discussing Breiman (2001), proposed thatresearchers be aware of many cultures, including the focus of our research: 3.Nonparametric, quantile based, information theoretic modeling. We provide aunification of many statistical methods for traditional small data sets andemerging big data sets in terms of comparison density, copula density, measureof dependence, correlation, information, new measures (called LP scorecomoments) that apply to long tailed distributions with out finite second ordermoments. A very important goal is to unify methods for discrete and continuousrandom variables. Our research extends these methods to modern high dimensionaldata modeling.
arxiv-300-184 | Lyapunov stochastic stability and control of robust dynamic coalitional games with transferable utilities | http://arxiv.org/pdf/1106.1933v2.pdf | author:Dario Bauso, Puduru Viswanadha Reddy, Tamer Basar category:cs.GT cs.LG cs.SY math.OC published:2011-06-09 summary:This paper considers a dynamic game with transferable utilities (TU), wherethe characteristic function is a continuous-time bounded mean ergodic process.A central planner interacts continuously over time with the players by choosingthe instantaneous allocations subject to budget constraints. Before the gamestarts, the central planner knows the nature of the process (bounded meanergodic), the bounded set from which the coalitions' values are sampled, andthe long run average coalitions' values. On the other hand, he has no knowledgeof the underlying probability function generating the coalitions' values. Ourgoal is to find allocation rules that use a measure of the extra reward that acoalition has received up to the current time by re-distributing the budgetamong the players. The objective is two-fold: i) guaranteeing convergence ofthe average allocations to the core (or a specific point in the core) of theaverage game, ii) driving the coalitions' excesses to an a priori given cone.The resulting allocation rules are robust as they guarantee the aforementionedconvergence properties despite the uncertain and time-varying nature of thecoaltions' values. We highlight three main contributions. First, we design anallocation rule based on full observation of the extra reward so that theaverage allocation approaches a specific point in the core of the average game,while the coalitions' excesses converge to an a priori given direction. Second,we design a new allocation rule based on partial observation on the extrareward so that the average allocation converges to the core of the averagegame, while the coalitions' excesses converge to an a priori given cone. Andthird, we establish connections to approachability theory and attainabilitytheory.
arxiv-300-185 | Knowledge revision in systems based on an informed tree search strategy : application to cartographic generalisation | http://arxiv.org/pdf/1204.4991v1.pdf | author:Patrick Taillandier, CÃ©cile DuchÃªne, Alexis Drogoul category:cs.AI cs.LG published:2012-04-23 summary:Many real world problems can be expressed as optimisation problems. Solvingthis kind of problems means to find, among all possible solutions, the one thatmaximises an evaluation function. One approach to solve this kind of problem isto use an informed search strategy. The principle of this kind of strategy isto use problem-specific knowledge beyond the definition of the problem itselfto find solutions more efficiently than with an uninformed strategy. This kindof strategy demands to define problem-specific knowledge (heuristics). Theefficiency and the effectiveness of systems based on it directly depend on theused knowledge quality. Unfortunately, acquiring and maintaining such knowledgecan be fastidious. The objective of the work presented in this paper is topropose an automatic knowledge revision approach for systems based on aninformed tree search strategy. Our approach consists in analysing the systemexecution logs and revising knowledge based on these logs by modelling therevision problem as a knowledge space exploration problem. We present anexperiment we carried out in an application domain where informed searchstrategies are often used: cartographic generalisation.
arxiv-300-186 | Objective Function Designing Led by User Preferences Acquisition | http://arxiv.org/pdf/1204.4990v1.pdf | author:Patrick Taillandier, Julien Gaffuri category:cs.LG cs.AI cs.HC published:2012-04-23 summary:Many real world problems can be defined as optimisation problems in which theaim is to maximise an objective function. The quality of obtained solution isdirectly linked to the pertinence of the used objective function. However,designing such function, which has to translate the user needs, is usuallyfastidious. In this paper, a method to help user objective functions designingis proposed. Our approach, which is highly interactive, is based on man machinedialogue and more particularly on the comparison of problem instance solutionsby the user. We propose an experiment in the domain of cartographicgeneralisation that shows promising results.
arxiv-300-187 | EHRs Connect Research and Practice: Where Predictive Modeling, Artificial Intelligence, and Clinical Decision Support Intersect | http://arxiv.org/pdf/1204.4927v1.pdf | author:Casey Bennett, Tom Doub, Rebecca Selove category:cs.AI cs.DB stat.ML published:2012-04-22 summary:Objectives: Electronic health records (EHRs) are only a first step incapturing and utilizing health-related data - the challenge is turning thatdata into useful information. Furthermore, EHRs are increasingly likely toinclude data relating to patient outcomes, functionality such as clinicaldecision support, and genetic information as well, and, as such, can be seen asrepositories of increasingly valuable information about patients' healthconditions and responses to treatment over time. Methods: We describe a casestudy of 423 patients treated by Centerstone within Tennessee and Indiana inwhich we utilized electronic health record data to generate predictivealgorithms of individual patient treatment response. Multiple models wereconstructed using predictor variables derived from clinical, financial andgeographic data. Results: For the 423 patients, 101 deteriorated, 223 improvedand in 99 there was no change in clinical condition. Based on modeling ofvarious clinical indicators at baseline, the highest accuracy in predictingindividual patient response ranged from 70-72% within the models tested. Interms of individual predictors, the Centerstone Assessment of Recovery Level -Adult (CARLA) baseline score was most significant in predicting outcome overtime (odds ratio 4.1 + 2.27). Other variables with consistently significantimpact on outcome included payer, diagnostic category, location and provisionof case management services. Conclusions: This approach represents a promisingavenue toward reducing the current gap between research and practice acrosshealthcare, developing data-driven clinical decision support based onreal-world populations, and serving as a component of embedded clinicalartificial intelligences that "learn" over time.
arxiv-300-188 | Quantum Interference in Cognition: Structural Aspects of the Brain | http://arxiv.org/pdf/1204.4914v1.pdf | author:Diederik Aerts, Sandro Sozzo category:cs.AI cs.CL quant-ph published:2012-04-22 summary:We identify the presence of typically quantum effects, namely 'superposition'and 'interference', in what happens when human concepts are combined, andprovide a quantum model in complex Hilbert space that represents faithfullyexperimental data measuring the situation of combining concepts. Our modelshows how 'interference of concepts' explains the effects of underextension andoverextension when two concepts combine to the disjunction of these twoconcepts. This result supports our earlier hypothesis that human thought has asuperposed two-layered structure, one layer consisting of 'classical logicalthought' and a superposed layer consisting of 'quantum conceptual thought'.Possible connections with recent findings of a 'grid-structure' for the brainare analyzed, and influences on the mind/brain relation, and consequences onapplied disciplines, such as artificial intelligence and quantum computation,are considered.
arxiv-300-189 | A Unified Multiscale Framework for Discrete Energy Minimization | http://arxiv.org/pdf/1204.4867v1.pdf | author:Shai Bagon, Meirav Galun category:cs.CV cs.DM published:2012-04-22 summary:Discrete energy minimization is a ubiquitous task in computer vision, yet isNP-hard in most cases. In this work we propose a multiscale framework forcoping with the NP-hardness of discrete optimization. Our approach utilizesalgebraic multiscale principles to efficiently explore the discrete solutionspace, yielding improved results on challenging, non-submodular energies forwhich current methods provide unsatisfactory approximations. In contrast topopular multiscale methods in computer vision, that builds an image pyramid,our framework acts directly on the energy to construct an energy pyramid.Deriving a multiscale scheme from the energy itself makes our frameworkapplication independent and widely applicable. Our framework gives rise to twocomplementary energy coarsening strategies: one in which coarser scales involvefewer variables, and a more revolutionary one in which the coarser scalesinvolve fewer discrete labels. We empirically evaluated our unified frameworkon a variety of both non-submodular and submodular energies, including energiesfrom Middlebury benchmark.
arxiv-300-190 | Learning XML Twig Queries | http://arxiv.org/pdf/1106.3725v3.pdf | author:SÅawomir Staworko, Piotr Wieczorek category:cs.DB cs.LG published:2011-06-19 summary:We investigate the problem of learning XML queries, path queries and treepattern queries, from examples given by the user. A learning algorithm takes onthe input a set of XML documents with nodes annotated by the user and returns aquery that selects the nodes in a manner consistent with the annotation. Westudy two learning settings that differ with the types of annotations. In thefirst setting the user may only indicate required nodes that the query mustreturn. In the second, more general, setting, the user may also indicateforbidden nodes that the query must not return. The query may or may not returnany node with no annotation. We formalize what it means for a class of queriesto be \emph{learnable}. One requirement is the existence of a learningalgorithm that is sound i.e., always returns a query consistent with theexamples given by the user. Furthermore, the learning algorithm should becomplete i.e., able to produce every query with a sufficiently rich example.Other requirements involve tractability of learning and its robustness tononessential examples. We show that the classes of simple path queries andpath-subsumption-free tree queries are learnable from positive examples. Thelearnability of the full class of tree pattern queries (and the full class ofpath queries) remains an open question. We show also that adding negativeexamples to the picture renders the learning unfeasible. Published in ICDT 2012, Berlin.
arxiv-300-191 | Energy-Efficient Building HVAC Control Using Hybrid System LBMPC | http://arxiv.org/pdf/1204.4717v1.pdf | author:Anil Aswani, Neal Master, Jay Taneja, Andrew Krioukov, David Culler, Claire Tomlin category:math.OC cs.LG cs.SY published:2012-04-20 summary:Improving the energy-efficiency of heating, ventilation, and air-conditioning(HVAC) systems has the potential to realize large economic and societalbenefits. This paper concerns the system identification of a hybrid systemmodel of a building-wide HVAC system and its subsequent control using a hybridsystem formulation of learning-based model predictive control (LBMPC). Here,the learning refers to model updates to the hybrid system model thatincorporate the heating effects due to occupancy, solar effects, outside airtemperature (OAT), and equipment, in addition to integrator dynamics inherentlypresent in low-level control. Though we make significant modelingsimplifications, our corresponding controller that uses this model is able toexperimentally achieve a large reduction in energy usage without anydegradations in occupant comfort. It is in this way that we justify themodeling simplifications that we have made. We conclude by presenting resultsfrom experiments on our building HVAC testbed, which show an average of 1.5MWhof energy savings per day (p = 0.002) with a 95% confidence interval of 1.0MWhto 2.1MWh of energy savings.
arxiv-300-192 | Efficient hierarchical clustering for continuous data | http://arxiv.org/pdf/1204.4708v1.pdf | author:Ricardo Henao, Joseph E. Lucas category:stat.ML published:2012-04-20 summary:We present an new sequential Monte Carlo sampler for coalescent basedBayesian hierarchical clustering. Our model is appropriate for modelingnon-i.i.d. data and offers a substantial reduction of computational cost whencompared to the original sampler without resorting to approximations. We alsopropose a quadratic complexity approximation that in practice shows almost noloss in performance compared to its counterpart. We show that as a byproduct ofour formulation, we obtain a greedy algorithm that exhibits performanceimprovement over other greedy algorithms, particularly in small data sets. Inorder to exploit the correlation structure of the data, we describe how toincorporate Gaussian process priors in the model as a flexible way to modelnon-i.i.d. data. Results on artificial and real data show significantimprovements over closely related approaches.
arxiv-300-193 | Structured sparsity through convex optimization | http://arxiv.org/pdf/1109.2397v2.pdf | author:Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski category:cs.LG stat.ML published:2011-09-12 summary:Sparse estimation methods are aimed at using or obtaining parsimoniousrepresentations of data or models. While naturally cast as a combinatorialoptimization problem, variable or feature selection admits a convex relaxationthrough the regularization by the $\ell_1$-norm. In this paper, we considersituations where we are not only interested in sparsity, but where somestructural prior knowledge is available as well. We show that the $\ell_1$-normcan then be extended to structured norms built on either disjoint oroverlapping groups of variables, leading to a flexible framework that can dealwith various structures. We present applications to unsupervised learning, forstructured sparse principal component analysis and hierarchical dictionarylearning, and to supervised learning in the context of non-linear variableselection.
arxiv-300-194 | A Fast and Effective Local Search Algorithm for Optimizing the Placement of Wind Turbines | http://arxiv.org/pdf/1204.4560v1.pdf | author:Markus Wagner, Jareth Day, Frank Neumann category:cs.NE published:2012-04-20 summary:The placement of wind turbines on a given area of land such that the windfarm produces a maximum amount of energy is a challenging optimization problem.In this article, we tackle this problem, taking into account wake effects thatare produced by the different turbines on the wind farm. We significantlyimprove upon existing results for the minimization of wake effects bydeveloping a new problem-specific local search algorithm. One key step in thespeed-up of our algorithm is the reduction in computation time needed to assessa given wind farm layout compared to previous approaches. Our new method allowsthe optimization of large real-world scenarios within a single night on astandard computer, whereas weeks on specialized computing servers were requiredfor previous approaches.
arxiv-300-195 | A Privacy-Aware Bayesian Approach for Combining Classifier and Cluster Ensembles | http://arxiv.org/pdf/1204.4521v1.pdf | author:Ayan Acharya, Eduardo R. Hruschka, Joydeep Ghosh category:cs.LG cs.CV stat.ML I.5.4 published:2012-04-20 summary:This paper introduces a privacy-aware Bayesian approach that combinesensembles of classifiers and clusterers to perform semi-supervised andtransductive learning. We consider scenarios where instances and theirclassification/clustering results are distributed across different data sitesand have sharing restrictions. As a special case, the privacy aware computationof the model when instances of the target data are distributed across differentdata sites, is also discussed. Experimental results show that the proposedapproach can provide good classification accuracies while adhering to thedata/model sharing constraints.
arxiv-300-196 | An Optimization Framework for Semi-Supervised and Transfer Learning using Multiple Classifiers and Clusterers | http://arxiv.org/pdf/1206.0994v1.pdf | author:Ayan Acharya, Eduardo R. Hruschka, Joydeep Ghosh, Sreangsu Acharyya category:cs.LG published:2012-04-20 summary:Unsupervised models can provide supplementary soft constraints to helpclassify new, "target" data since similar instances in the target set are morelikely to share the same class label. Such models can also help detect possibledifferences between training and target distributions, which is useful inapplications where concept drift may take place, as in transfer learningsettings. This paper describes a general optimization framework that takes asinput class membership estimates from existing classifiers learnt on previouslyencountered "source" data, as well as a similarity matrix from a clusterensemble operating solely on the target data to be classified, and yields aconsensus labeling of the target data. This framework admits a wide range ofloss functions and classification/clustering methods. It exploits properties ofBregman divergences in conjunction with Legendre duality to yield a principledand scalable approach. A variety of experiments show that the proposedframework can yield results substantially superior to those provided by populartransductive learning techniques or by naively applying classifiers learnt onthe original task to the target data.
arxiv-300-197 | Dynamic Template Tracking and Recognition | http://arxiv.org/pdf/1204.4476v1.pdf | author:Rizwan Chaudhry, Gregory Hager, Rene Vidal category:cs.CV cs.SY published:2012-04-19 summary:In this paper we address the problem of tracking non-rigid objects whoselocal appearance and motion changes as a function of time. This class ofobjects includes dynamic textures such as steam, fire, smoke, water, etc., aswell as articulated objects such as humans performing various actions. We modelthe temporal evolution of the object's appearance/motion using a LinearDynamical System (LDS). We learn such models from sample videos and use them asdynamic templates for tracking objects in novel videos. We pose the problem oftracking a dynamic non-rigid object in the current frame as a maximuma-posteriori estimate of the location of the object and the latent state of thedynamical system, given the current image features and the best estimate of thestate in the previous frame. The advantage of our approach is that we canspecify a-priori the type of texture to be tracked in the scene by usingpreviously trained models for the dynamics of these textures. Our frameworknaturally generalizes common tracking methods such as SSD and kernel-basedtracking from static templates to dynamic templates. We test our algorithm onsynthetic as well as real examples of dynamic textures and show that our simpledynamics-based trackers perform at par if not better than the state-of-the-art.Since our approach is general and applicable to any image feature, we alsoapply it to the problem of human action tracking and build action-specificoptical flow trackers that perform better than the state-of-the-art whentracking a human performing a particular action. Finally, since our approach isgenerative, we can use a-priori trained trackers for different texture oraction classes to simultaneously track and recognize the texture or action inthe video.
arxiv-300-198 | Probabilistic Recovery of Multiple Subspaces in Point Clouds by Geometric lp Minimization | http://arxiv.org/pdf/1002.1994v3.pdf | author:Gilad Lerman, Teng Zhang category:stat.ML published:2010-02-09 summary:We assume data independently sampled from a mixture distribution on the unitball of the D-dimensional Euclidean space with K+1 components: the firstcomponent is a uniform distribution on that ball representing outliers and theother K components are uniform distributions along K d-dimensional linearsubspaces restricted to that ball. We study both the simultaneous recovery ofall K underlying subspaces and the recovery of the best l0 subspace (i.e., withlargest number of points) by minimizing the lp-averaged distances of datapoints from d-dimensional subspaces of the D-dimensional space. Unlike other lpminimization problems, this minimization is non-convex for all p>0 and thusrequires different methods for its analysis. We show that if 0<p <= 1, thenboth all underlying subspaces and the best l0 subspace can be preciselyrecovered by lp minimization with overwhelming probability. This result extendsto additive homoscedastic uniform noise around the subspaces (i.e., uniformdistribution in a strip around them) and near recovery with an errorproportional to the noise level. On the other hand, if K>1 and p>1, then weshow that both all underlying subspaces and the best l0 subspace cannot berecovered and even nearly recovered. Further relaxations are also discussed. Weuse the results of this paper for partially justifying recent effectivealgorithms for modeling data by mixtures of multiple subspaces as well as fordiscussing the effect of using variants of lp minimizations in RANSAC-typestrategies for single subspace recovery.
arxiv-300-199 | Your Two Weeks of Fame and Your Grandmother's | http://arxiv.org/pdf/1204.4346v1.pdf | author:James Cook, Atish Das Sarma, Alex Fabrikant, Andrew Tomkins category:cs.DL cs.CL cs.SI physics.soc-ph J.4 published:2012-04-19 summary:Did celebrity last longer in 1929, 1992 or 2009? We investigate thephenomenon of fame by mining a collection of news articles that spans thetwentieth century, and also perform a side study on a collection of blog postsfrom the last 10 years. By analyzing mentions of personal names, we measureeach person's time in the spotlight, using two simple metrics that evaluate,roughly, the duration of a single news story about a person, and the overallduration of public interest in a person. We watched the distribution evolvefrom 1895 to 2010, expecting to find significantly shortening fame durations,per the much popularly bemoaned shortening of society's attention spans andquickening of media's news cycles. Instead, we conclusively demonstrate that,through many decades of rapid technological and societal change, through theappearance of Twitter, communication satellites, and the Internet, famedurations did not decrease, neither for the typical case nor for the extremelyfamous, with the last statistically significant fame duration decreases comingin the early 20th century, perhaps from the spread of telegraphy and telephony.Furthermore, while median fame durations stayed persistently constant, for themost famous of the famous, as measured by either volume or duration of mediaattention, fame durations have actually trended gently upward since the 1940s,with statistically significant increases on 40-year timescales. Similar studieshave been done with much shorter timescales specifically in the context ofinformation spreading on Twitter and similar social networking sites. To thebest of our knowledge, this is the first massive scale study of this naturethat spans over a century of archived data, thereby allowing us to trackchanges across decades.
arxiv-300-200 | The Stick-Breaking Construction of the Beta Process as a Poisson Process | http://arxiv.org/pdf/1109.0343v2.pdf | author:John Paisley, David Blei, Michael I. Jordan category:math.ST math.PR stat.ML stat.TH published:2011-09-02 summary:We show that the stick-breaking construction of the beta process due toPaisley, et al. (2010) can be obtained from the characterization of the betaprocess as a Poisson process. Specifically, we show that the mean measure ofthe underlying Poisson process is equal to that of the beta process. We usethis underlying representation to derive error bounds on truncated betaprocesses that are tighter than those in the literature. We also develop a newMCMC inference algorithm for beta processes, based in part on our new Poissonprocess construction.
arxiv-300-201 | The Discrete Infinite Logistic Normal Distribution | http://arxiv.org/pdf/1103.4789v3.pdf | author:John Paisley, Chong Wang, David Blei category:stat.ML published:2011-03-24 summary:We present the discrete infinite logistic normal distribution (DILN), aBayesian nonparametric prior for mixed membership models. DILN is ageneralization of the hierarchical Dirichlet process (HDP) that modelscorrelation structure between the weights of the atoms at the group level. Wederive a representation of DILN as a normalized collection of gamma-distributedrandom variables, and study its statistical properties. We considerapplications to topic modeling and derive a variational inference algorithm forapproximate posterior inference. We study the empirical performance of the DILNtopic model on four corpora, comparing performance with the HDP and thecorrelated topic model (CTM). To deal with large-scale data sets, we alsodevelop an online inference algorithm for DILN and compare with online HDP andonline LDA on the Nature magazine, which contains approximately 350,000articles.
arxiv-300-202 | Designing generalisation evaluation function through human-machine dialogue | http://arxiv.org/pdf/1204.4332v1.pdf | author:Patrick Taillandier, Julien Gaffuri category:cs.HC cs.LG published:2012-04-19 summary:Automated generalisation has known important improvements these last fewyears. However, an issue that still deserves more study concerns the automaticevaluation of generalised data. Indeed, many automated generalisation systemsrequire the utilisation of an evaluation function to automatically assessgeneralisation outcomes. In this paper, we propose a new approach dedicated tothe design of such a function. This approach allows an imperfectly definedevaluation function to be revised through a man-machine dialogue. The usergives its preferences to the system by comparing generalisation outcomes.Machine Learning techniques are then used to improve the evaluation function.An experiment carried out on buildings shows that our approach significantlyimproves generalisation evaluation functions defined by users.
arxiv-300-203 | Supervised feature evaluation by consistency analysis: application to measure sets used to characterise geographic objects | http://arxiv.org/pdf/1204.4329v1.pdf | author:Patrick Taillandier, Alexis Drogoul category:cs.LG published:2012-04-19 summary:Nowadays, supervised learning is commonly used in many domains. Indeed, manyworks propose to learn new knowledge from examples that translate the expectedbehaviour of the considered system. A key issue of supervised learning concernsthe description language used to represent the examples. In this paper, wepropose a method to evaluate the feature set used to describe them. Our methodis based on the computation of the consistency of the example base. We carriedout a case study in the domain of geomatic in order to evaluate the sets ofmeasures used to characterise geographic objects. The case study shows that ourmethod allows to give relevant evaluations of measure sets.
arxiv-300-204 | Learning in Riemannian Orbifolds | http://arxiv.org/pdf/1204.4294v1.pdf | author:Brijnesh J. Jain, Klaus Obermayer category:cs.LG cs.AI cs.CV published:2012-04-19 summary:Learning in Riemannian orbifolds is motivated by existing machine learningalgorithms that directly operate on finite combinatorial structures such aspoint patterns, trees, and graphs. These methods, however, lack statisticaljustification. This contribution derives consistency results for learningproblems in structured domains and thereby generalizes learning in vectorspaces and manifolds.
arxiv-300-205 | Speech Recognition: Increasing Efficiency of Support Vector Machines | http://arxiv.org/pdf/1204.4257v1.pdf | author:Aamir Khan, Muhammad Farhan, Asar Ali category:cs.CV published:2012-04-19 summary:With the advancement of communication and security technologies, it hasbecome crucial to have robustness of embedded biometric systems. This paperpresents the realization of such technologies which demands reliable anderror-free biometric identity verification systems. High dimensional patternsare not permitted due to eigen-decomposition in high dimensional feature spaceand degeneration of scattering matrices in small size sample. Generalization,dimensionality reduction and maximizing the margins are controlled byminimizing weight vectors. Results show good pattern by multimodal biometricsystem proposed in this paper. This paper is aimed at investigating a biometricidentity system using Support Vector Machines(SVMs) and Lindear DiscriminantAnalysis(LDA) with MFCCs and implementing such system in real-time usingSignalWAVE.
arxiv-300-206 | EP-GIG Priors and Applications in Bayesian Sparse Learning | http://arxiv.org/pdf/1204.4243v1.pdf | author:Zhihua Zhang, Shusen Wang, Dehua Liu, Michael I. Jordan category:stat.ML published:2012-04-19 summary:In this paper we propose a novel framework for the construction ofsparsity-inducing priors. In particular, we define such priors as a mixture ofexponential power distributions with a generalized inverse Gaussian density(EP-GIG). EP-GIG is a variant of generalized hyperbolic distributions, and thespecial cases include Gaussian scale mixtures and Laplace scale mixtures.Furthermore, Laplace scale mixtures can subserve a Bayesian framework forsparse learning with nonconvex penalization. The densities of EP-GIG can beexplicitly expressed. Moreover, the corresponding posterior distribution alsofollows a generalized inverse Gaussian distribution. These properties lead usto EM algorithms for Bayesian sparse learning. We show that these algorithmsbear an interesting resemblance to iteratively re-weighted $\ell_2$ or $\ell_1$methods. In addition, we present two extensions for grouped variable selectionand logistic regression.
arxiv-300-207 | Simultaneous Codeword Optimization (SimCO) for Dictionary Update and Learning | http://arxiv.org/pdf/1109.5302v3.pdf | author:Wei Dai, Tao Xu, Wenwu Wang category:cs.LG cs.IT math.IT published:2011-09-24 summary:We consider the data-driven dictionary learning problem. The goal is to seekan over-complete dictionary from which every training signal can be bestapproximated by a linear combination of only a few codewords. This task isoften achieved by iteratively executing two operations: sparse coding anddictionary update. In the literature, there are two benchmark mechanisms toupdate a dictionary. The first approach, such as the MOD algorithm, ischaracterized by searching for the optimal codewords while fixing the sparsecoefficients. In the second approach, represented by the K-SVD method, onecodeword and the related sparse coefficients are simultaneously updated whileall other codewords and coefficients remain unchanged. We propose a novelframework that generalizes the aforementioned two methods. The unique featureof our approach is that one can update an arbitrary set of codewords and thecorresponding sparse coefficients simultaneously: when sparse coefficients arefixed, the underlying optimization problem is similar to that in the MODalgorithm; when only one codeword is selected for update, it can be proved thatthe proposed algorithm is equivalent to the K-SVD method; and more importantly,our method allows us to update all codewords and all sparse coefficientssimultaneously, hence the term simultaneous codeword optimization (SimCO).Under the proposed framework, we design two algorithms, namely, primitive andregularized SimCO. We implement these two algorithms based on a simple gradientdescent mechanism. Simulations are provided to demonstrate the performance ofthe proposed algorithms, as compared with two baseline algorithms MOD andK-SVD. Results show that regularized SimCO is particularly appealing in termsof both learning performance and running speed.
arxiv-300-208 | Fuzzy Dynamical Genetic Programming in XCSF | http://arxiv.org/pdf/1204.4202v1.pdf | author:Richard J. Preen, Larry Bull category:cs.AI cs.LG cs.NE cs.SY I.2.6 published:2012-04-18 summary:A number of representation schemes have been presented for use withinLearning Classifier Systems, ranging from binary encodings to Neural Networks,and more recently Dynamical Genetic Programming (DGP). This paper presentsresults from an investigation into using a fuzzy DGP representation within theXCSF Learning Classifier System. In particular, asynchronous Fuzzy LogicNetworks are used to represent the traditional condition-action productionsystem rules. It is shown possible to use self-adaptive, open-ended evolutionto design an ensemble of such fuzzy dynamical systems within XCSF to solveseveral well-known continuous-valued test problems.
arxiv-300-209 | The Artificial Regression Market | http://arxiv.org/pdf/1204.4154v1.pdf | author:Nathan Lay, Adrian Barbu category:stat.ML math.ST stat.TH published:2012-04-18 summary:The Artificial Prediction Market is a recent machine learning technique formulti-class classification, inspired from the financial markets. It involves anumber of trained market participants that bet on the possible outcomes and arerewarded if they predict correctly. This paper generalizes the scope of theArtificial Prediction Markets to regression, where there are uncountably manypossible outcomes and the error is usually the MSE. For that, we introduce thereward kernel that rewards each participant based on its prediction error andwe derive the price equations. Using two reward kernels we obtain two differentlearning rules, one of which is approximated using Hermite-Gauss quadrature.The market setting makes it easy to aggregate specialized regressors that onlypredict when an observation falls into their specialization domain. Experimentsshow that regression markets based on the two learning rules outperform RandomForest Regression on many UCI datasets and are rarely outperformed.
arxiv-300-210 | Learning From An Optimization Viewpoint | http://arxiv.org/pdf/1204.4145v1.pdf | author:Karthik Sridharan category:cs.LG cs.GT published:2012-04-18 summary:In this dissertation we study statistical and online learning problems froman optimization viewpoint.The dissertation is divided into two parts : I. We first consider the question of learnability for statistical learningproblems in the general learning setting. The question of learnability is wellstudied and fully characterized for binary classification and for real valuedsupervised learning problems using the theory of uniform convergence. Howeverwe show that for the general learning setting uniform convergence theory failsto characterize learnability. To fill this void we use stability of learningalgorithms to fully characterize statistical learnability in the generalsetting. Next we consider the problem of online learning. Unlike thestatistical learning framework there is a dearth of generic tools that can beused to establish learnability and rates for online learning problems ingeneral. We provide online analogs to classical tools from statistical learningtheory like Rademacher complexity, covering numbers, etc. We further use thesetools to fully characterize learnability for online supervised learningproblems. II. In the second part, for general classes of convex learning problems, weprovide appropriate mirror descent (MD) updates for online and statisticallearning of these problems. Further, we show that the the MD is near optimalfor online convex learning and for most cases, is also near optimal forstatistical convex learning. We next consider the problem of convexoptimization and show that oracle complexity can be lower bounded by the socalled fat-shattering dimension of the associated linear class. Thus weestablish a strong connection between offline convex optimization problems andstatistical learning problems. We also show that for a large class of highdimensional optimization problems, MD is in fact near optimal even for convexoptimization.
arxiv-300-211 | Green's function based unparameterised multi-dimensional kernel density and likelihood ratio estimator | http://arxiv.org/pdf/1112.2093v2.pdf | author:Peter Kovesarki, Ian C. Brock, A. Elizabeth Nuncio Quiroz category:stat.ML math.ST stat.TH published:2011-12-09 summary:This paper introduces a probability density estimator based on Green'sfunction identities. A density model is constructed under the sole assumptionthat the probability density is differentiable. The method is implemented as abinary likelihood estimator for classification purposes, so issues such asmis-modeling and overtraining are also discussed. The identity behind thedensity estimator can be interpreted as a real-valued, non-scalar kernel methodwhich is able to reconstruct differentiable density functions.
arxiv-300-212 | Convolutional Neural Networks Applied to House Numbers Digit Classification | http://arxiv.org/pdf/1204.3968v1.pdf | author:Pierre Sermanet, Soumith Chintala, Yann LeCun category:cs.CV cs.LG cs.NE published:2012-04-18 summary:We classify digits of real-world house numbers using convolutional neuralnetworks (ConvNets). ConvNets are hierarchical feature learning neural networkswhose structure is biologically inspired. Unlike many popular vision approachesthat are hand-designed, ConvNets can automatically learn a unique set offeatures optimized for a given task. We augmented the traditional ConvNetarchitecture by learning multi-stage features and by using Lp pooling andestablish a new state-of-the-art of 94.85% accuracy on the SVHN dataset (45.2%error improvement). Furthermore, we analyze the benefits of different poolingmethods and multi-stage features in ConvNets. The source code and a tutorialare available at eblearn.sf.net.
arxiv-300-213 | Semi-Supervised learning with Density-Ratio Estimation | http://arxiv.org/pdf/1204.3965v1.pdf | author:Masanori Kawakita, Takafumi Kanamori category:stat.ML published:2012-04-18 summary:In this paper, we study statistical properties of semi-supervised learning,which is considered as an important problem in the community of machinelearning. In the standard supervised learning, only the labeled data isobserved. The classification and regression problems are formalized as thesupervised learning. In semi-supervised learning, unlabeled data is alsoobtained in addition to labeled data. Hence, exploiting unlabeled data isimportant to improve the prediction accuracy in semi-supervised learning. Thisproblems is regarded as a semiparametric estimation problem with missing data.Under the the discriminative probabilistic models, it had been considered thatthe unlabeled data is useless to improve the estimation accuracy. Recently, itwas revealed that the weighted estimator using the unlabeled data achievesbetter prediction accuracy in comparison to the learning method using onlylabeled data, especially when the discriminative probabilistic model ismisspecified. That is, the improvement under the semiparametric model withmissing data is possible, when the semiparametric model is misspecified. Inthis paper, we apply the density-ratio estimator to obtain the weight functionin the semi-supervised learning. The benefit of our approach is that theproposed estimator does not require well-specified probabilistic models for theprobability of the unlabeled data. Based on the statistical asymptotic theory,we prove that the estimation accuracy of our method outperforms the supervisedlearning using only labeled data. Some numerical experiments present theusefulness of our methods.
arxiv-300-214 | Regularized Partial Least Squares with an Application to NMR Spectroscopy | http://arxiv.org/pdf/1204.3942v1.pdf | author:Genevera I. Allen, Christine Peterson, Marina Vannucci, Mirjana Maletic-Savatic category:stat.ML published:2012-04-17 summary:High-dimensional data common in genomics, proteomics, and chemometrics oftencontains complicated correlation structures. Recently, partial least squares(PLS) and Sparse PLS methods have gained attention in these areas as dimensionreduction techniques in the context of supervised data analysis. We introduce aframework for Regularized PLS by solving a relaxation of the SIMPLSoptimization problem with penalties on the PLS loadings vectors. Our approachenjoys many advantages including flexibility, general penalties, easyinterpretation of results, and fast computation in high-dimensional settings.We also outline extensions of our methods leading to novel methods forNon-negative PLS and Generalized PLS, an adaption of PLS for structured data.We demonstrate the utility of our methods through simulations and a case studyon proton Nuclear Magnetic Resonance (NMR) spectroscopy data.
arxiv-300-215 | A Computational Analysis of Collective Discourse | http://arxiv.org/pdf/1204.3498v2.pdf | author:Vahed Qazvinian, Dragomir R. Radev category:cs.SI cs.CL physics.soc-ph published:2012-04-16 summary:This paper is focused on the computational analysis of collective discourse,a collective behavior seen in non-expert content contributions in online socialmedia. We collect and analyze a wide range of real-world collective discoursedatasets from movie user reviews to microblogs and news headlines to scientificcitations. We show that all these datasets exhibit diversity of perspective, aproperty seen in other collective systems and a criterion in wise crowds. Ourexperiments also confirm that the network of different perspectiveco-occurrences exhibits the small-world property with high clustering ofdifferent perspectives. Finally, we show that non-expert contributions incollective discourse can be used to answer simple questions that are otherwisehard to answer.
arxiv-300-216 | Fixed-Rank Representation for Unsupervised Visual Learning | http://arxiv.org/pdf/1203.2210v2.pdf | author:Risheng Liu, Zhouchen Lin, Fernando De la Torre, Zhixun Su category:cs.CV cs.NA published:2012-03-09 summary:Subspace clustering and feature extraction are two of the most commonly usedunsupervised learning techniques in computer vision and pattern recognition.State-of-the-art techniques for subspace clustering make use of recent advancesin sparsity and rank minimization. However, existing techniques arecomputationally expensive and may result in degenerate solutions that degradeclustering performance in the case of insufficient data sampling. To partiallysolve these problems, and inspired by existing work on matrix factorization,this paper proposes fixed-rank representation (FRR) as a unified framework forunsupervised visual learning. FRR is able to reveal the structure of multiplesubspaces in closed-form when the data is noiseless. Furthermore, we prove thatunder some suitable conditions, even with insufficient observations, FRR canstill reveal the true subspace memberships. To achieve robustness to outliersand noise, a sparse regularizer is introduced into the FRR framework. Beyondsubspace clustering, FRR can be used for unsupervised feature extraction. As anon-trivial byproduct, a fast numerical solver is developed for FRR.Experimental results on both synthetic data and real applications validate ourtheoretical analysis and demonstrate the benefits of FRR for unsupervisedvisual learning.
arxiv-300-217 | Indus script corpora, archaeo-metallurgy and Meluhha (Mleccha) | http://arxiv.org/pdf/1204.3800v1.pdf | author:Srinivasan Kalyanaraman category:cs.CL published:2012-04-17 summary:Jules Bloch's work on formation of the Marathi language has to be expandedfurther to provide for a study of evolution and formation of Indian languagesin the Indian language union (sprachbund). The paper analyses the stages in theevolution of early writing systems which began with the evolution of countingin the ancient Near East. A stage anterior to the stage of syllabicrepresentation of sounds of a language, is identified. Unique geometric shapesrequired for tokens to categorize objects became too large to handle toabstract hundreds of categories of goods and metallurgical processes during theproduction of bronze-age goods. About 3500 BCE, Indus script as a writingsystem was developed to use hieroglyphs to represent the 'spoken words'identifying each of the goods and processes. A rebus method of representingsimilar sounding words of the lingua franca of the artisans was used in Indusscript. This method is recognized and consistently applied for the linguafranca of the Indian sprachbund. That the ancient languages of India,constituted a sprachbund (or language union) is now recognized by manylinguists. The sprachbund area is proximate to the area where most of the Indusscript inscriptions were discovered, as documented in the corpora. Thathundreds of Indian hieroglyphs continued to be used in metallurgy is evidencedby their use on early punch-marked coins. This explains the combined use ofsyllabic scripts such as Brahmi and Kharoshti together with the hieroglyphs onRampurva copper bolt, and Sohgaura copper plate from about 6th centuryBCE.Indian hieroglyphs constitute a writing system for meluhha language and arerebus representations of archaeo-metallurgy lexemes. The rebus principle wasemployed by the early scripts and can legitimately be used to decipher theIndus script, after secure pictorial identification.
arxiv-300-218 | Statistical Multiresolution Estimation for Variational Imaging: With an Application in Poisson-Biophotonics | http://arxiv.org/pdf/1204.3748v1.pdf | author:Klaus Frick, Philipp Marnitz, Axel Munk category:stat.AP cs.CV published:2012-04-17 summary:In this paper we present a spatially-adaptive method for image reconstructionthat is based on the concept of statistical multiresolution estimation asintroduced in [Frick K, Marnitz P, and Munk A. "Statistical multiresolutionDantzig estimation in imaging: Fundamental concepts and algorithmic framework".Electron. J. Stat., 6:231-268, 2012]. It constitutes a variationalregularization technique that uses an supremum-type distance measure asdata-fidelity combined with a convex cost functional. The resulting convexoptimization problem is approached by a combination of an inexact alternatingdirection method of multipliers and Dykstra's projection algorithm. We describea novel method for balancing data-fit and regularity that is fully automaticand allows for a sound statistical interpretation. The performance of ourestimation approach is studied for various problems in imaging. Among others,this includes deconvolution problems that arise in Poisson nanoscalefluorescence microscopy.
arxiv-300-219 | Distributed Iterative Processing for Interference Channels with Receiver Cooperation | http://arxiv.org/pdf/1204.3742v1.pdf | author:Mihai-Alin Badiu, Carles Navarro ManchÃ³n, Vasile Bota, Bernard Henri Fleury category:cs.IT math.IT stat.ML published:2012-04-17 summary:We propose a framework for the derivation and evaluation of distributediterative algorithms for receiver cooperation in interference-limited wirelesssystems. Our approach views the processing within and collaboration betweenreceivers as the solution to an inference problem in the probabilistic model ofthe whole system. The probabilistic model is formulated to explicitlyincorporate the receivers' ability to share information of a predefined type.We employ a recently proposed unified message-passing tool to infer thevariables of interest in the factor graph representation of the probabilisticmodel. The exchange of information between receivers arises in the form ofpassing messages along some specific edges of the factor graph; the rate ofupdating and passing these messages determines the communication overheadassociated with cooperation. Simulation results illustrate the high performanceof the proposed algorithm even with a low number of message exchanges betweenreceivers.
arxiv-300-220 | Towards Real-Time Summarization of Scheduled Events from Twitter Streams | http://arxiv.org/pdf/1204.3731v1.pdf | author:Arkaitz Zubiaga, Damiano Spina, Enrique AmigÃ³, Julio Gonzalo category:cs.IR cs.CL cs.SI published:2012-04-17 summary:This paper explores the real-time summarization of scheduled events such assoccer games from torrential flows of Twitter streams. We propose and evaluatean approach that substantially shrinks the stream of tweets in real-time, andconsists of two steps: (i) sub-event detection, which determines if somethingnew has occurred, and (ii) tweet selection, which picks a representative tweetto describe each sub-event. We compare the summaries generated in threelanguages for all the soccer games in "Copa America 2011" to reference livereports offered by Yahoo! Sports journalists. We show that simple text analysismethods which do not involve external knowledge lead to summaries that cover84% of the sub-events on average, and 100% of key types of sub-events (such asgoals in soccer). Our approach should be straightforwardly applicable to otherkinds of scheduled events such as other sports, award ceremonies, keynotetalks, TV shows, etc.
arxiv-300-221 | Semi-Supervised Anomaly Detection - Towards Model-Independent Searches of New Physics | http://arxiv.org/pdf/1112.3329v3.pdf | author:Mikael Kuusela, Tommi Vatanen, Eric Malmi, Tapani Raiko, Timo Aaltonen, Yoshikazu Nagai category:hep-ex stat.AP stat.ML published:2011-12-14 summary:Most classification algorithms used in high energy physics fall under thecategory of supervised machine learning. Such methods require a training setcontaining both signal and background events and are prone to classificationerrors should this training data be systematically inaccurate for example dueto the assumed MC model. To complement such model-dependent searches, wepropose an algorithm based on semi-supervised anomaly detection techniques,which does not require a MC training sample for the signal data. We first modelthe background using a multivariate Gaussian mixture model. We then search fordeviations from this model by fitting to the observations a mixture of thebackground model and a number of additional Gaussians. This allows us toperform pattern recognition of any anomalous excess over the background. Weshow by a comparison to neural network classifiers that such an approach is alot more robust against misspecification of the signal MC than supervisedclassification. In cases where there is an unexpected signal, a neural networkmight fail to correctly identify it, while anomaly detection does not sufferfrom such a limitation. On the other hand, when there are no systematic errorsin the training data, both methods perform comparably.
arxiv-300-222 | Large-Scale Automatic Labeling of Video Events with Verbs Based on Event-Participant Interaction | http://arxiv.org/pdf/1204.3616v1.pdf | author:Andrei Barbu, Alexander Bridge, Dan Coroian, Sven Dickinson, Sam Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark Siskind, Jarrell Waggoner, Song Wang, Jinlian Wei, Yifan Yin, Zhiqi Zhang category:cs.CV cs.AI published:2012-04-16 summary:We present an approach to labeling short video clips with English verbs asevent descriptions. A key distinguishing aspect of this work is that it labelsvideos with verbs that describe the spatiotemporal interaction between eventparticipants, humans and objects interacting with each other, abstracting awayall object-class information and fine-grained image characteristics, andrelying solely on the coarse-grained motion of the event participants. We applyour approach to a large set of 22 distinct verb classes and a corpus of 2,584videos, yielding two surprising outcomes. First, a classification accuracy ofgreater than 70% on a 1-out-of-22 labeling task and greater than 85% on avariety of 1-out-of-10 subsets of this labeling task is independent of thechoice of which of two different time-series classifiers we employ. Second, weachieve this level of accuracy using a highly impoverished intermediaterepresentation consisting solely of the bounding boxes of one or two eventparticipants as a function of time. This indicates that successful eventrecognition depends more on the choice of appropriate features thatcharacterize the linguistic invariants of the event classes than on theparticular classifier algorithms.
arxiv-300-223 | Learning to Predict the Wisdom of Crowds | http://arxiv.org/pdf/1204.3611v1.pdf | author:Seyda Ertekin, Haym Hirsh, Cynthia Rudin category:cs.SI cs.LG published:2012-04-16 summary:The problem of "approximating the crowd" is that of estimating the crowd'smajority opinion by querying only a subset of it. Algorithms that approximatethe crowd can intelligently stretch a limited budget for a crowdsourcing task.We present an algorithm, "CrowdSense," that works in an online fashion todynamically sample subsets of labelers based on an exploration/exploitationcriterion. The algorithm produces a weighted combination of a subset of thelabelers' votes that approximates the crowd's opinion.
arxiv-300-224 | Kernels for Vector-Valued Functions: a Review | http://arxiv.org/pdf/1106.6251v2.pdf | author:Mauricio A. Alvarez, Lorenzo Rosasco, Neil D. Lawrence category:stat.ML cs.AI math.ST stat.TH published:2011-06-30 summary:Kernel methods are among the most popular techniques in machine learning.From a frequentist/discriminative perspective they play a central role inregularization theory as they provide a natural choice for the hypotheses spaceand the regularization functional through the notion of reproducing kernelHilbert spaces. From a Bayesian/generative perspective they are the key in thecontext of Gaussian processes, where the kernel function is also known as thecovariance function. Traditionally, kernel methods have been used in supervisedlearning problem with scalar outputs and indeed there has been a considerableamount of work devoted to designing and learning kernels. More recently therehas been an increasing interest in methods that deal with multiple outputs,motivated partly by frameworks like multitask learning. In this paper, wereview different methods to design or learn valid kernel functions for multipleoutputs, paying particular attention to the connection between probabilisticand functional methods.
arxiv-300-225 | Efficient Protocols for Distributed Classification and Optimization | http://arxiv.org/pdf/1204.3523v1.pdf | author:Hal Daume III, Jeff M. Phillips, Avishek Saha, Suresh Venkatasubramanian category:cs.LG stat.ML published:2012-04-16 summary:In distributed learning, the goal is to perform a learning task over datadistributed across multiple nodes with minimal (expensive) communication. Priorwork (Daume III et al., 2012) proposes a general model that bounds thecommunication required for learning classifiers while allowing for $\eps$training error on linearly separable data adversarially distributed acrossnodes. In this work, we develop key improvements and extensions to this basic model.Our first result is a two-party multiplicative-weight-update based protocolthat uses $O(d^2 \log{1/\eps})$ words of communication to classify distributeddata in arbitrary dimension $d$, $\eps$-optimally. This readily extends toclassification over $k$ nodes with $O(kd^2 \log{1/\eps})$ words ofcommunication. Our proposed protocol is simple to implement and is considerablymore efficient than baselines compared, as demonstrated by our empiricalresults. In addition, we illustrate general algorithm design paradigms for doingefficient learning over distributed data. We show how to solvefixed-dimensional and high dimensional linear programming efficiently in adistributed setting where constraints may be distributed across nodes. Sincemany learning problems can be viewed as convex optimization problems whereconstraints are generated by individual points, this models many typicaldistributed learning scenarios. Our techniques make use of a novel connectionfrom multipass streaming, as well as adapting the multiplicative-weight-updateframework more generally to a distributed setting. As a consequence, ourmethods extend to the wide range of problems solvable using these techniques.
arxiv-300-226 | The logic of quantum mechanics - Take II | http://arxiv.org/pdf/1204.3458v1.pdf | author:Bob Coecke category:quant-ph cs.CL cs.LO math.CT math.LO published:2012-04-16 summary:We put forward a new take on the logic of quantum mechanics, followingSchroedinger's point of view that it is composition which makes quantum theorywhat it is, rather than its particular propositional structure due to theexistence of superpositions, as proposed by Birkhoff and von Neumann. Thisgives rise to an intrinsically quantitative kind of logic, which truly deservesthe name `logic' in that it also models meaning in natural language, the latterbeing the origin of logic, that it supports automation, the most prominentpractical use of logic, and that it supports probabilistic inference.
arxiv-300-227 | Statistical analysis of emotions and opinions at Digg website | http://arxiv.org/pdf/1201.5484v2.pdf | author:Piotr Pohorecki, Julian Sienkiewicz, Marija Mitrovic, Georgios Paltoglou, Janusz A. Holyst category:physics.soc-ph cs.CL cs.SI published:2012-01-26 summary:We performed statistical analysis on data from the Digg.com website, whichenables its users to express their opinion on news stories by taking part inforum-like discussions as well as directly evaluate previous posts and storiesby assigning so called "diggs". Owing to fact that the content of each post hasbeen annotated with its emotional value, apart from the strictly structuralproperties, the study also includes an analysis of the average emotionalresponse of the posts commenting the main story. While analysing correlationsat the story level, an interesting relationship between the number of diggs andthe number of comments received by a story was found. The correlation betweenthe two quantities is high for data where small threads dominate andconsistently decreases for longer threads. However, while the correlation ofthe number of diggs and the average emotional response tends to grow for longerthreads, correlations between numbers of comments and the average emotionalresponse are almost zero. We also show that the initial set of comments givento a story has a substantial impact on the further "life" of the discussion:high negative average emotions in the first 10 comments lead to longer threadswhile the opposite situation results in shorter discussions. We also suggestpresence of two different mechanisms governing the evolution of the discussionand, consequently, its length.
arxiv-300-228 | Explaining Adaptation in Genetic Algorithms With Uniform Crossover: The Hyperclimbing Hypothesis | http://arxiv.org/pdf/1204.3436v1.pdf | author:Keki M. Burjorjee category:cs.NE cs.AI I.2.8; F.2 published:2012-04-16 summary:The hyperclimbing hypothesis is a hypothetical explanation for adaptation ingenetic algorithms with uniform crossover (UGAs). Hyperclimbing is anintuitive, general-purpose, non-local search heuristic applicable to discreteproduct spaces with rugged or stochastic cost functions. The strength of thisheuristic lie in its insusceptibility to local optima when the cost function isdeterministic, and its tolerance for noise when the cost function isstochastic. Hyperclimbing works by decimating a search space, i.e. byiteratively fixing the values of small numbers of variables. The hyperclimbinghypothesis holds that UGAs work by implementing efficient hyperclimbing. Proofof concept for this hypothesis comes from the use of a novel analytic techniqueinvolving the exploitation of algorithmic symmetry. We have also obtainedexperimental results that show that a simple tweak inspired by thehyperclimbing hypothesis dramatically improves the performance of a UGA onlarge, random instances of MAX-3SAT and the Sherrington Kirkpatrick SpinGlasses problem.
arxiv-300-229 | Stochastic Feature Mapping for PAC-Bayes Classification | http://arxiv.org/pdf/1204.2609v2.pdf | author:Xiong Li, Tai Sing Lee, Yuncai Liu category:cs.LG published:2012-04-12 summary:Probabilistic generative modeling of data distributions can potentiallyexploit hidden information which is useful for discriminative classification.This observation has motivated the development of approaches that couplegenerative and discriminative models for classification. In this paper, wepropose a new approach to couple generative and discriminative models in anunified framework based on PAC-Bayes risk theory. We first derive themodel-parameter-independent stochastic feature mapping from a practical MAPclassifier operating on generative models. Then we construct a linearstochastic classifier equipped with the feature mapping, and derive theexplicit PAC-Bayes risk bounds for such classifier for both supervised andsemi-supervised learning. Minimizing the risk bound, using an EM-like iterativeprocedure, results in a new posterior over hidden variables (E-step) and theupdate rules of model parameters (M-step). The derivation of the posterior isalways feasible due to the way of equipping feature mapping and the explicitform of bounding risk. The derived posterior allows the tuning of generativemodels and subsequently the feature mappings for better classification. Thederived update rules of the model parameters are same to those of the uncoupledmodels as the feature mapping is model-parameter-independent. Our experimentsshow that the coupling between data modeling generative model and thediscriminative classifier via a stochastic feature mapping in this frameworkleads to a general classification tool with state-of-the-art performance.
arxiv-300-230 | Group Invariant Scattering | http://arxiv.org/pdf/1101.2286v3.pdf | author:StÃ©phane Mallat category:math.FA cs.CV published:2011-01-12 summary:This paper constructs translation invariant operators on L2(R^d), which areLipschitz continuous to the action of diffeomorphisms. A scattering propagatoris a path ordered product of non-linear and non-commuting operators, each ofwhich computes the modulus of a wavelet transform. A local integration definesa windowed scattering transform, which is proved to be Lipschitz continuous tothe action of diffeomorphisms. As the window size increases, it converges to awavelet scattering transform which is translation invariant. Scatteringcoefficients also provide representations of stationary processes. Expectedvalues depend upon high order moments and can discriminate processes having thesame power spectrum. Scattering operators are extended on L2 (G), where G is acompact Lie group, and are invariant under the action of G. Combining ascattering on L2(R^d) and on Ld (SO(d)) defines a translation and rotationinvariant scattering on L2(R^d).
arxiv-300-231 | Neuroevolution Results in Emergence of Short-Term Memory for Goal-Directed Behavior | http://arxiv.org/pdf/1204.3221v1.pdf | author:Konstantin Lakhman, Mikhail Burtsev category:cs.NE cs.AI nlin.AO published:2012-04-14 summary:Animals behave adaptively in the environment with multiply competing goals.Understanding of the mechanisms underlying such goal-directed behavior remainsa challenge for neuroscience as well for adaptive system research. To addressthis problem we developed an evolutionary model of adaptive behavior in themultigoal stochastic environment. Proposed neuroevolutionary algorithm is basedon neuron's duplication as a basic mechanism of agent's recurrent neuralnetwork development. Results of simulation demonstrate that in the course ofevolution agents acquire the ability to store the short-term memory and,therefore, use it in behavioral strategies with alternative actions. We foundthat evolution discovered two mechanisms for short-term memory. The firstmechanism is integration of sensory signals and ongoing internal neuralactivity, resulting in emergence of cell groups specialized on alternativeactions. And the second mechanism is slow neurodynamical processes that makespossible to code the previous behavioral choice.
arxiv-300-232 | Selection of tuning parameters in bridge regression models via Bayesian information criterion | http://arxiv.org/pdf/1203.4326v3.pdf | author:Shuichi Kawano category:stat.ME stat.ML published:2012-03-20 summary:We consider the bridge linear regression modeling, which can produce a sparseor non-sparse model. A crucial point in the model building process is theselection of adjusted parameters including a regularization parameter and atuning parameter in bridge regression models. The choice of the adjustedparameters can be viewed as a model selection and evaluation problem. Wepropose a model selection criterion for evaluating bridge regression models interms of Bayesian approach. This selection criterion enables us to select theadjusted parameters objectively. We investigate the effectiveness of ourproposed modeling strategy through some numerical examples.
arxiv-300-233 | Block-Sparse Recovery via Convex Optimization | http://arxiv.org/pdf/1104.0654v3.pdf | author:Ehsan Elhamifar, Rene Vidal category:math.OC cs.CV cs.IT math.IT published:2011-04-04 summary:Given a dictionary that consists of multiple blocks and a signal that livesin the range space of only a few blocks, we study the problem of finding ablock-sparse representation of the signal, i.e., a representation that uses theminimum number of blocks. Motivated by signal/image processing and computervision applications, such as face recognition, we consider the block-sparserecovery problem in the case where the number of atoms in each block isarbitrary, possibly much larger than the dimension of the underlying subspace.To find a block-sparse representation of a signal, we propose two classes ofnon-convex optimization programs, which aim to minimize the number of nonzerocoefficient blocks and the number of nonzero reconstructed vectors from theblocks, respectively. Since both classes of problems are NP-hard, we proposeconvex relaxations and derive conditions under which each class of the convexprograms is equivalent to the original non-convex formulation. Our conditionsdepend on the notions of mutual and cumulative subspace coherence of adictionary, which are natural generalizations of existing notions of mutual andcumulative coherence. We evaluate the performance of the proposed convexprograms through simulations as well as real experiments on face recognition.We show that treating the face recognition problem as a block-sparse recoveryproblem improves the state-of-the-art results by 10% with only 25% of thetraining data.
arxiv-300-234 | Compensating Interpolation Distortion by Using New Optimized Modular Method | http://arxiv.org/pdf/1204.3618v1.pdf | author:Mohammad Tofighi, Ali Ayremlou, Farokh Marvasti category:cs.CV cs.MM published:2012-04-13 summary:A modular method was suggested before to recover a band limited signal fromthe sample and hold and linearly interpolated (or, in general, annth-order-hold) version of the regular samples. In this paper a novel approachfor compensating the distortion of any interpolation based on modular methodhas been proposed. In this method the performance of the modular method isoptimized by adding only some simply calculated coefficients. This approachcauses drastic improvement in terms of signal-to-noise ratios with fewermodules compared to the classical modular method. Simulation results clearlyconfirm the improvement of the proposed method and also its superior robustnessagainst additive noise.
arxiv-300-235 | Collaboration and Coordination in Secondary Networks for Opportunistic Spectrum Access | http://arxiv.org/pdf/1204.3005v1.pdf | author:Wassim Jouini, Marco Di Felice, Luciano Bononi, Christophe Moy category:stat.AP cs.NI stat.ML published:2012-04-13 summary:In this paper, we address the general case of a coordinated secondary networkwilling to exploit communication opportunities left vacant by a licensedprimary network. Since secondary users (SU) usually have no prior knowledge onthe environment, they need to learn the availability of each channel throughsensing techniques, which however can be prone to detection errors. We arguethat cooperation among secondary users can enable efficient learning andcoordination mechanisms in order to maximize the spectrum exploitation by SUs,while minimizing the impact on the primary network. To this goal, we providethree novel contributions in this paper. First, we formulate the spectrumselection in secondary networks as an instance of the Multi-Armed Bandit (MAB)problem, and we extend the analysis to the collaboration learning case, inwhich each SU learns the spectrum occupation, and shares this information withother SUs. We show that collaboration among SUs can mitigate the impact ofsensing errors on system performance, and improve the convergence of thelearning process to the optimal solution. Second, we integrate the learningalgorithms with two collaboration techniques based on modified versions of theHungarian algorithm and of the Round Robin algorithm that allows reducing theinterference among SUs. Third, we derive fundamental limits to the performanceof cooperative learning algorithms based on Upper Confidence Bound (UCB)policies in a symmetric scenario where all SU have the same perception of thequality of the resources. Extensive simulation results confirm theeffectiveness of our joint learning-collaboration algorithm in protecting theoperations of Primary Users (PUs), while maximizing the performance of SUs.
arxiv-300-236 | Image Restoration with Signal-dependent Camera Noise | http://arxiv.org/pdf/1204.2994v1.pdf | author:Ayan Chakrabarti, Todd Zickler category:cs.CV stat.AP published:2012-04-13 summary:This article describes a fast iterative algorithm for image denoising anddeconvolution with signal-dependent observation noise. We use an optimizationstrategy based on variable splitting that adapts traditional Gaussiannoise-based restoration algorithms to account for the observed image beingcorrupted by mixed Poisson-Gaussian noise and quantization errors.
arxiv-300-237 | Non-sparse Linear Representations for Visual Tracking with Online Reservoir Metric Learning | http://arxiv.org/pdf/1204.2912v1.pdf | author:Xi Li, Chunhua Shen, Qinfeng Shi, Anthony Dick, Anton van den Hengel category:cs.CV published:2012-04-13 summary:Most sparse linear representation-based trackers need to solve acomputationally expensive L1-regularized optimization problem. To address thisproblem, we propose a visual tracker based on non-sparse linearrepresentations, which admit an efficient closed-form solution withoutsacrificing accuracy. Moreover, in order to capture the correlation informationbetween different feature dimensions, we learn a Mahalanobis distance metric inan online fashion and incorporate the learned metric into the optimizationproblem for obtaining the linear representation. We show that online metriclearning using proximity comparison significantly improves the robustness ofthe tracking, especially on those sequences exhibiting drastic appearancechanges. Furthermore, in order to prevent the unbounded growth in the number oftraining samples for the metric learning, we design a time-weighted reservoirsampling method to maintain and update limited-sized foreground and backgroundsample buffers for balancing sample diversity and adaptability. Experimentalresults on challenging videos demonstrate the effectiveness and robustness ofthe proposed tracker.
arxiv-300-238 | Clustering using Max-norm Constrained Optimization | http://arxiv.org/pdf/1202.5598v4.pdf | author:Ali Jalali, Nathan Srebro category:cs.LG stat.ML published:2012-02-25 summary:We suggest using the max-norm as a convex surrogate constraint forclustering. We show how this yields a better exact cluster recovery guaranteethan previously suggested nuclear-norm relaxation, and study the effectivenessof our method, and other related convex relaxations, compared to otherclustering approaches.
arxiv-300-239 | Echoes of power: Language effects and power differences in social interaction | http://arxiv.org/pdf/1112.3670v3.pdf | author:Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bo Pang, Jon Kleinberg category:cs.SI cs.CL physics.soc-ph published:2011-12-15 summary:Understanding social interaction within groups is key to analyzing onlinecommunities. Most current work focuses on structural properties: who talks towhom, and how such interactions form larger network structures. Theinteractions themselves, however, generally take place in the form of naturallanguage --- either spoken or written --- and one could reasonably suppose thatsignals manifested in language might also provide information about roles,status, and other aspects of the group's dynamics. To date, however, findingsuch domain-independent language-based signals has been a challenge. Here, we show that in group discussions power differentials betweenparticipants are subtly revealed by how much one individual immediately echoesthe linguistic style of the person they are responding to. Starting from thisobservation, we propose an analysis framework based on linguistic coordinationthat can be used to shed light on power relationships and that worksconsistently across multiple types of power --- including a more "static" formof power based on status differences, and a more "situational" form of power inwhich one individual experiences a type of dependence on another. Using thisframework, we study how conversational behavior can reveal power relationshipsin two very different settings: discussions among Wikipedians and argumentsbefore the U.S. Supreme Court.
arxiv-300-240 | Watersheds, waterfalls, on edge or node weighted graphs | http://arxiv.org/pdf/1204.2837v1.pdf | author:Fernand Meyer category:cs.CV cs.DM 68U10, 05C85 published:2012-04-12 summary:We present an algebraic approach to the watershed adapted to edge or nodeweighted graphs. Starting with the flooding adjunction, we introduce theflooding graphs, for which node and edge weights may be deduced one from theother. Each node weighted or edge weighted graph may be transformed in aflooding graph, showing that there is no superiority in using one or the other,both being equivalent. We then introduce pruning operators extract subgraphs ofincreasing steepness. For an increasing steepness, the number of neverascending paths becomes smaller and smaller. This reduces the watershed zone,where catchment basins overlap. A last pruning operator called scissorassociates to each node outside the regional minima one and only one edge. Thecatchment basins of this new graph do not overlap and form a watershedpartition. Again, with an increasing steepness, the number of distinctwatershed partitions contained in a graph becomes smaller and smaller.Ultimately, for natural image, an infinite steepness leads to a uniquesolution, as it is not likely that two absolutely identical non ascending pathsof infinite steepness connect a node with two distinct minima. It happens thatnon ascending paths of a given steepness are the geodesics of lexicographicdistance functions of a given depth. This permits to extract the watershedpartitions as skeletons by zone of influence of the minima for suchlexicographic distances. The waterfall hierarchy is obtained by a sequence ofoperations. The first constructs the minimum spanning forest which spans aninitial watershed partition. The contraction of the trees into one nodeproduces a reduced graph which may be submitted to the same treatment. Theprocess is iterated until only one region remains. The union of the edges ofall forests produced constitutes a minimum spanning tree of the initial graph.
arxiv-300-241 | Premise Selection for Mathematics by Corpus Analysis and Kernel Methods | http://arxiv.org/pdf/1108.3446v2.pdf | author:Jesse Alama, Tom Heskes, Daniel KÃ¼hlwein, Evgeni Tsivtsivadze, Josef Urban category:cs.LG cs.AI 68T05 I.2.6; I.2.3 published:2011-08-17 summary:Smart premise selection is essential when using automated reasoning as a toolfor large-theory formal proof development. A good method for premise selectionin complex mathematical libraries is the application of machine learning tolarge corpora of proofs. This work develops learning-based premise selection intwo ways. First, a newly available minimal dependency analysis of existinghigh-level formal mathematical proofs is used to build a large knowledge baseof proof dependencies, providing precise data for ATP-based re-verification andfor training premise selection algorithms. Second, a new machine learningalgorithm for premise selection based on kernel methods is proposed andimplemented. To evaluate the impact of both techniques, a benchmark consistingof 2078 large-theory mathematical problems is constructed,extending the olderMPTP Challenge benchmark. The combined effect of the techniques results in a50% improvement on the benchmark over the Vampire/SInE state-of-the-art systemfor automated reasoning in large theories.
arxiv-300-242 | Estimating the Prevalence of Deception in Online Review Communities | http://arxiv.org/pdf/1204.2804v1.pdf | author:Myle Ott, Claire Cardie, Jeff Hancock category:cs.SI cs.CL cs.CY published:2012-04-12 summary:Consumers' purchase decisions are increasingly influenced by user-generatedonline reviews. Accordingly, there has been growing concern about the potentialfor posting "deceptive opinion spam" -- fictitious reviews that have beendeliberately written to sound authentic, to deceive the reader. But while thispractice has received considerable public attention and concern, relativelylittle is known about the actual prevalence, or rate, of deception in onlinereview communities, and less still about the factors that influence it. We propose a generative model of deception which, in conjunction with adeception classifier, we use to explore the prevalence of deception in sixpopular online review communities: Expedia, Hotels.com, Orbitz, Priceline,TripAdvisor, and Yelp. We additionally propose a theoretical model of onlinereviews based on economic signaling theory, in which consumer reviews diminishthe inherent information asymmetry between consumers and producers, by actingas a signal to a product's true, unknown quality. We find that deceptiveopinion spam is a growing problem overall, but with different growth ratesacross communities. These rates, we argue, are driven by the differentsignaling costs associated with deception for each review community, e.g.,posting requirements. When measures are taken to increase signaling cost, e.g.,filtering reviews written by first-time reviewers, deception prevalence iseffectively reduced.
arxiv-300-243 | Seeing Unseeability to See the Unseeable | http://arxiv.org/pdf/1204.2801v1.pdf | author:Siddharth Narayanaswamy, Andrei Barbu, Jeffrey Mark Siskind category:cs.CV cs.AI cs.RO published:2012-04-12 summary:We present a framework that allows an observer to determine occluded portionsof a structure by finding the maximum-likelihood estimate of those occludedportions consistent with visible image evidence and a consistency model. Doingthis requires determining which portions of the structure are occluded in thefirst place. Since each process relies on the other, we determine a solution toboth problems in tandem. We extend our framework to determine confidence ofone's assessment of which portions of an observed structure are occluded, andthe estimate of that occluded structure, by determining the sensitivity ofone's assessment to potential new observations. We further extend our frameworkto determine a robotic action whose execution would allow a new observationthat would maximally increase one's confidence.
arxiv-300-244 | Video In Sentences Out | http://arxiv.org/pdf/1204.2742v1.pdf | author:Andrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven Dickinson, Sanja Fidler, Aaron Michaux, Sam Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark Siskind, Jarrell Waggoner, Song Wang, Jinlian Wei, Yifan Yin, Zhiqi Zhang category:cs.CV cs.AI published:2012-04-12 summary:We present a system that produces sentential descriptions of video: who didwhat to whom, and where and how they did it. Action class is rendered as averb, participant objects as noun phrases, properties of those objects asadjectival modifiers in those noun phrases,spatial relations between thoseparticipants as prepositional phrases, and characteristics of the event asprepositional-phrase adjuncts and adverbial modifiers. Extracting theinformation needed to render these linguistic entities requires an approach toevent recognition that recovers object tracks, the track-to-role assignments,and changing body posture.
arxiv-300-245 | Simultaneous Object Detection, Tracking, and Event Recognition | http://arxiv.org/pdf/1204.2741v1.pdf | author:Andrei Barbu, Aaron Michaux, Siddharth Narayanaswamy, Jeffrey Mark Siskind category:cs.CV cs.AI published:2012-04-12 summary:The common internal structure and algorithmic organization of objectdetection, detection-based tracking, and event recognition facilitates ageneral approach to integrating these three components. This supportsmultidirectional information flow between these components allowing objectdetection to influence tracking and event recognition and event recognition toinfluence tracking and object detection. The performance of the combination canexceed the performance of the components in isolation. This can be done withlinear asymptotic complexity.
arxiv-300-246 | Positive Semidefinite Metric Learning Using Boosting-like Algorithms | http://arxiv.org/pdf/1104.4704v2.pdf | author:Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel category:cs.CV published:2011-04-25 summary:The success of many machine learning and pattern recognition methods reliesheavily upon the identification of an appropriate distance metric on the inputdata. It is often beneficial to learn such a metric from the input trainingdata, instead of using a default one such as the Euclidean distance. In thiswork, we propose a boosting-based technique, termed BoostMetric, for learning aquadratic Mahalanobis distance metric. Learning a valid Mahalanobis distancemetric requires enforcing the constraint that the matrix parameter to themetric remains positive definite. Semidefinite programming is often used toenforce this constraint, but does not scale well and easy to implement.BoostMetric is instead based on the observation that any positive semidefinitematrix can be decomposed into a linear combination of trace-one rank-onematrices. BoostMetric thus uses rank-one positive semidefinite matrices as weaklearners within an efficient and scalable boosting-based learning process. Theresulting methods are easy to implement, efficient, and can accommodate varioustypes of constraints. We extend traditional boosting algorithms in that itsweak learner is a positive semidefinite matrix with trace and rank being onerather than a classifier or regressor. Experiments on various datasetsdemonstrate that the proposed algorithms compare favorably to thosestate-of-the-art methods in terms of classification accuracy and running time.
arxiv-300-247 | Detecting lateral genetic material transfer | http://arxiv.org/pdf/1204.2601v1.pdf | author:C. CalderÃ³n, L. Delaye, V. Mireles, P. Miramontes category:cs.NE cs.AI q-bio.GN published:2012-04-12 summary:The bioinformatical methods to detect lateral gene transfer events are mainlybased on functional coding DNA characteristics. In this paper, we propose theuse of DNA traits not depending on protein coding requirements. We introduceseveral semilocal variables that depend on DNA primary sequence and thatreflect thermodynamic as well as physico-chemical magnitudes that are able totell apart the genome of different organisms. After combining these variablesin a neural classificator, we obtain results whose power of resolution go asfar as to detect the exchange of genomic material between bacteria that arephylogenetically close.
arxiv-300-248 | Probabilistic Latent Tensor Factorization Model for Link Pattern Prediction in Multi-relational Networks | http://arxiv.org/pdf/1204.2588v1.pdf | author:Sheng Gao, Ludovic Denoyer, Patrick Gallinari category:cs.SI cs.LG stat.ML 15A69 H.2.8; J.4 published:2012-04-11 summary:This paper aims at the problem of link pattern prediction in collections ofobjects connected by multiple relation types, where each type may play adistinct role. While common link analysis models are limited to single-typelink prediction, we attempt here to capture the correlations among differentrelation types and reveal the impact of various relation types on performancequality. For that, we define the overall relations between object pairs as a\textit{link pattern} which consists in interaction pattern and connectionstructure in the network, and then use tensor formalization to jointly modeland predict the link patterns, which we refer to as \textit{Link PatternPrediction} (LPP) problem. To address the issue, we propose a ProbabilisticLatent Tensor Factorization (PLTF) model by introducing another latent factorfor multiple relation types and furnish the Hierarchical Bayesian treatment ofthe proposed probabilistic model to avoid overfitting for solving the LPPproblem. To learn the proposed model we develop an efficient Markov Chain MonteCarlo sampling method. Extensive experiments are conducted on several realworld datasets and demonstrate significant improvements over several existingstate-of-the-art methods.
arxiv-300-249 | Modeling Relational Data via Latent Factor Blockmodel | http://arxiv.org/pdf/1204.2581v1.pdf | author:Sheng Gao, Ludovic Denoyer, Patrick Gallinari category:cs.DS cs.LG stat.ML 15A83 H.2.8; J.4 published:2012-04-11 summary:In this paper we address the problem of modeling relational data, whichappear in many applications such as social network analysis, recommendersystems and bioinformatics. Previous studies either consider latent featurebased models but disregarding local structure in the network, or focusexclusively on capturing local structure of objects based on latent blockmodelswithout coupling with latent characteristics of objects. To combine thebenefits of the previous work, we propose a novel model that can simultaneouslyincorporate the effect of latent features and covariates if any, as well as theeffect of latent structure that may exist in the data. To achieve this, wemodel the relation graph as a function of both latent feature factors andlatent cluster memberships of objects to collectively discover globallypredictive intrinsic properties of objects and capture latent block structurein the network to improve prediction performance. We also develop anoptimization transfer algorithm based on the generalized EM-style strategy tolearn the latent factors. We prove the efficacy of our proposed model throughthe link prediction task and cluster analysis task, and extensive experimentson the synthetic data and several real world datasets suggest that our proposedLFBM model outperforms the other state of the art approaches in the evaluatedtasks.
arxiv-300-250 | Concept Modeling with Superwords | http://arxiv.org/pdf/1204.2523v1.pdf | author:Khalid El-Arini, Emily B. Fox, Carlos Guestrin category:stat.ML cs.CL cs.IR cs.LG published:2012-04-11 summary:In information retrieval, a fundamental goal is to transform a document intoconcepts that are representative of its content. The term "representative" isin itself challenging to define, and various tasks require differentgranularities of concepts. In this paper, we aim to model concepts that aresparse over the vocabulary, and that flexibly adapt their content based onother relevant semantic information such as textual structure or associatedimage features. We explore a Bayesian nonparametric model based on nested betaprocesses that allows for inferring an unknown number of strictly sparseconcepts. The resulting model provides an inherently different representationof concepts than a standard LDA (or HDP) based topic model, and allows fordirect incorporation of semantic features. We demonstrate the utility of thisrepresentation on multilingual blog data and the Congressional Record.
arxiv-300-251 | A Simple Explanation of A Spectral Algorithm for Learning Hidden Markov Models | http://arxiv.org/pdf/1204.2477v1.pdf | author:Matthew James Johnson category:stat.ME cs.LG stat.ML published:2012-04-11 summary:A simple linear algebraic explanation of the algorithm in "A SpectralAlgorithm for Learning Hidden Markov Models" (COLT 2009). Most of the contentis in Figure 2; the text just makes everything precise in four nearly-trivialclaims.
arxiv-300-252 | Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing | http://arxiv.org/pdf/1204.2436v1.pdf | author:Nicolas Gillis category:stat.ML math.NA math.OC published:2012-04-11 summary:Nonnegative matrix factorization (NMF) has become a very popular technique inmachine learning because it automatically extracts meaningful features througha sparse and part-based representation. However, NMF has the drawback of beinghighly ill-posed, that is, there typically exist many different but equivalentfactorizations. In this paper, we introduce a completely new way to obtainingmore well-posed NMF problems whose solutions are sparser. Our technique isbased on the preprocessing of the nonnegative input data matrix, and relies onthe theory of M-matrices and the geometric interpretation of NMF. This approachprovably leads to optimal and sparse solutions under the separabilityassumption of Donoho and Stodden (NIPS, 2003), and, for rank-three matrices,makes the number of exact factorizations finite. We illustrate theeffectiveness of our technique on several image datasets.
arxiv-300-253 | Distributed Multi-view Matching in Networks with Limited Communications | http://arxiv.org/pdf/1111.4840v4.pdf | author:Eduardo Montijano, Rosario Aragues, Carlos Sagues category:cs.CV cs.MA cs.RO published:2011-11-21 summary:We address the problem of distributed matching of features in networks withvision systems. Every camera in the network has limited communicationcapabilities and can only exchange local matches with its neighbors. We proposea distributed algorithm that takes these local matches and computes globalcorrespondences by a proper propagation in the network. When the algorithmfinishes, each camera knows the global correspondences between its features andthe features of all the cameras in the network. The presence of spuriousintroduced by the local matcher may produce inconsistent globalcorrespondences, which are association paths between features from the samecamera. The contributions of this work are the propagation of the local matchesand the detection and resolution of these inconsistencies by deleting localmatches. Our resolution algorithm considers the quality of each local match,when this information is provided by the local matcher. We formally prove thatafter executing the algorithm, the network finishes with a global dataassociation free of inconsistencies. We provide a fully decentralized solutionto the problem which does not rely on any particular communication topology.Simulations and experimental results with real images show the performance ofthe method considering different features, matching functions and scenarios.
arxiv-300-254 | Self-Adaptive Surrogate-Assisted Covariance Matrix Adaptation Evolution Strategy | http://arxiv.org/pdf/1204.2356v1.pdf | author:Ilya Loshchilov, Marc Schoenauer, MichÃ¨le Sebag category:cs.NE published:2012-04-11 summary:This paper presents a novel mechanism to adapt surrogate-assistedpopulation-based algorithms. This mechanism is applied to ACM-ES, a recentlyproposed surrogate-assisted variant of CMA-ES. The resulting algorithm,saACM-ES, adjusts online the lifelength of the current surrogate model (thenumber of CMA-ES generations before learning a new surrogate) and the surrogatehyper-parameters. Both heuristics significantly improve the quality of thesurrogate model, yielding a significant speed-up of saACM-ES compared to theACM-ES and CMA-ES baselines. The empirical validation of saACM-ES on theBBOB-2012 noiseless testbed demonstrates the efficiency and the scalabilityw.r.t the problem dimension and the population size of the proposed approach,that reaches new best results on some of the benchmark problems.
arxiv-300-255 | Feature Extraction Methods for Color Image Similarity | http://arxiv.org/pdf/1204.2336v1.pdf | author:R. Venkata Ramana Chary, D. Rajya Lakshmi, K. V. N. Sunitha category:cs.CV published:2012-04-11 summary:Many User interactive systems are proposed all methods are trying toimplement as a user friendly and various approaches proposed but most of thesystems not reached to the use specifications like user friendly systems withuser interest, all proposed method implemented basic techniques some areimproved methods also propose but not reaching to the user specifications. Inthis proposed paper we concentrated on image retrieval system with in earlydays many user interactive systems performed with basic concepts but suchsystems are not reaching to the user specifications and not attracted to theuser so a lot of research interest in recent years with new specifications,recent approaches have user is interested in friendly interacted methods areexpecting, many are concentrated for improvement in all methods. In thisproposed system we focus on the retrieval of images within a large imagecollection based on color projections and different mathematical approaches areintroduced and applied for retrieval of images. before Appling proposed methodsimages are sub grouping using threshold values, in this paper R G B colorcombinations considered for retrieval of images, in proposed methods areimplemented and results are included, through results it is observed that weobtaining efficient results comparatively previous and existing.
arxiv-300-256 | Automated Generation of Cross-Domain Analogies via Evolutionary Computation | http://arxiv.org/pdf/1204.2335v1.pdf | author:Atilim Gunes Baydin, Ramon Lopez de Mantaras, Santiago Ontanon category:cs.NE nlin.AO published:2012-04-11 summary:Analogy plays an important role in creativity, and is extensively used inscience as well as art. In this paper we introduce a technique for theautomated generation of cross-domain analogies based on a novel evolutionaryalgorithm (EA). Unlike existing work in computational analogy-making restrictedto creating analogies between two given cases, our approach, for a given case,is capable of creating an analogy along with the novel analogous case itself.Our algorithm is based on the concept of "memes", which are units of culture,or knowledge, undergoing variation and selection under a fitness measure, andrepresents evolving pieces of knowledge as semantic networks. Using a fitnessfunction based on Gentner's structure mapping theory of analogies, wedemonstrate the feasibility of spontaneously generating semantic networks thatare analogous to a given base network.
arxiv-300-257 | Robust Nonnegative Matrix Factorization via $L_1$ Norm Regularization | http://arxiv.org/pdf/1204.2311v1.pdf | author:Bin Shen, Luo Si, Rongrong Ji, Baodi Liu category:cs.LG cs.CV stat.ML published:2012-04-11 summary:Nonnegative Matrix Factorization (NMF) is a widely used technique in manyapplications such as face recognition, motion segmentation, etc. Itapproximates the nonnegative data in an original high dimensional space with alinear representation in a low dimensional space by using the product of twononnegative matrices. In many applications data are often partially corruptedwith large additive noise. When the positions of noise are known, some existingvariants of NMF can be applied by treating these corrupted entries as missingvalues. However, the positions are often unknown in many real worldapplications, which prevents the usage of traditional NMF or other existingvariants of NMF. This paper proposes a Robust Nonnegative Matrix Factorization(RobustNMF) algorithm that explicitly models the partial corruption as largeadditive noise without requiring the information of positions of noise. Inpractice, large additive noise can be used to model outliers. In particular,the proposed method jointly approximates the clean data matrix with the productof two nonnegative matrices and estimates the positions and values ofoutliers/noise. An efficient iterative optimization algorithm with a solidtheoretical justification has been proposed to learn the desired matrixfactorization. Experimental results demonstrate the advantages of the proposedalgorithm.
arxiv-300-258 | Ubiquitous WLAN/Camera Positioning using Inverse Intensity Chromaticity Space-based Feature Detection and Matching: A Preliminary Result | http://arxiv.org/pdf/1204.2294v1.pdf | author:Wan Mohd Yaakob Wan Bejuri, Mohd Murtadha Mohamad, Maimunah Sapri, Mohd Adly Rosly category:cs.CV published:2012-04-10 summary:This paper present our new intensity chromaticity space-based featuredetection and matching algorithm. This approach utilizes hybridization ofwireless local area network and camera internal sensor which to receive signalstrength from a access point and the same time retrieve interest pointinformation from hallways. This information is combined by model fittingapproach in order to find the absolute of user target position. No conventionalsearching algorithm is required, thus it is expected reducing the computationalcomplexity. Finally we present pre-experimental results to illustrate theperformance of the localization system for an indoor environment set-up.
arxiv-300-259 | Evolutionary Computation in Astronomy and Astrophysics: A Review | http://arxiv.org/pdf/1202.2523v2.pdf | author:JosÃ© A. GarcÃ­a GutiÃ©rrez, Carlos Cotta, Antonio J. FernÃ¡ndez-Leiva category:cs.AI astro-ph.IM cs.NE published:2012-02-12 summary:In general Evolutionary Computation (EC) includes a number of optimizationmethods inspired by biological mechanisms of evolution. The methods cataloguedin this area use the Darwinian principles of life evolution to producealgorithms that returns high quality solutions to hard-to-solve optimizationproblems. The main strength of EC is precisely that they provide good solutionseven if the computational resources (e.g., running time) are limited. Astronomyand Astrophysics are two fields that often require optimizing problems of highcomplexity or analyzing a huge amount of data and the so-called completeoptimization methods are inherently limited by the size of the problem/data.For instance, reliable analysis of large amounts of data is central to modernastrophysics and astronomical sciences in general. EC techniques perform wellwhere other optimization methods are inherently limited (as complete methodsapplied to NP-hard problems), and in the last ten years, numerous proposalshave come up that apply with greater or lesser success methodologies ofevolutional computation to common engineering problems. Some of these problems,such as the estimation of non-lineal parameters, the development of automaticlearning techniques, the implementation of control systems, or the resolutionof multi-objective optimization problems, have had (and have) a specialrepercussion in the fields. For these reasons EC emerges as a feasiblealternative for traditional methods. In this paper, we discuss some promisingapplications in this direction and a number of recent works in this area; thepaper also includes a general description of EC to provide a global perspectiveto the reader and gives some guidelines of application of EC techniques forfuture research
arxiv-300-260 | Affine Image Registration Transformation Estimation Using a Real Coded Genetic Algorithm with SBX | http://arxiv.org/pdf/1204.2139v1.pdf | author:Mosab Bazargani, AntÃ³nio dos Anjos, Fernando G. Lobo, Ali Mollahosseini, Hamid Reza Shahbazkia category:cs.NE published:2012-04-10 summary:This paper describes the application of a real coded genetic algorithm (GA)to align two or more 2-D images by means of image registration. The proposedsearch strategy is a transformation parameters-based approach involving theaffine transform. The real coded GA uses Simulated Binary Crossover (SBX), aparent-centric recombination operator that has shown to deliver a goodperformance in many optimization problems in the continuous domain. Inaddition, we propose a new technique for matching points between a warped andstatic images by using a randomized ordering when visiting the points duringthe matching procedure. This new technique makes the evaluation of theobjective function somewhat noisy, but GAs and other population-based searchalgorithms have been shown to cope well with noisy fitness evaluations. Theresults obtained are competitive to those obtained by state-of-the-artclassical methods in image registration, confirming the usefulness of theproposed noisy objective function and the suitability of SBX as a recombinationoperator for this type of problem.
arxiv-300-261 | The steepest watershed: from graphs to images | http://arxiv.org/pdf/1204.2134v1.pdf | author:Fernand Meyer category:cs.CV 68U10, 05C85 published:2012-04-10 summary:The watershed is a powerful tool for segmenting objects whose contours appearas crest lines on a gradient image. The watershed transform associates to atopographic surface a partition into catchment basins, defined as attractionzones of a drop of water falling on the relief and following a line of steepestdescent. Unfortunately, catchment basins may overlap and do not form apartition. Moreover, current watershed algorithms, being shortsighted, do notcorrectly estimate the steepness of the downwards trajectories and overestimatethe overlapping zones of catchment basins. An arbitrary division of these zonesbetween adjacent catchment basin results in a poor localization of thecontours. We propose an algorithm without myopia, which considers the totallength of a trajectory for estimating its steepness. We first considertopographic surfaces defined on node weighted graphs. The graphs are pruned inorder to eliminate all downwards trajectories which are not the steepest. Aniterative algorithm with simple neighborhood operations performs the pruningand constructs the catchment basins. The algorithm is then adapted to gray toneimages. The graph structure itself is encoded as an image thanks to the fixedneighborhood structure of grids. A pair of adaptative erosions and dilationsprune the graph and extend the catchment basins. As a result one obtains aprecise detection of the catchment basins and a graph of the steepesttrajectories. A last iterative algorithm allows to follow selected downwardstrajectories in order to detect particular structures such as rivers or thalweglines of the topographic surface.
arxiv-300-262 | Image-based Vehicle Classification System | http://arxiv.org/pdf/1204.2114v1.pdf | author:Jun Yee Ng, Yong Haur Tay category:cs.CV published:2012-04-10 summary:Electronic toll collection (ETC) system has been a common trend used for tollcollection on toll road nowadays. The implementation of electronic tollcollection allows vehicles to travel at low or full speed during the tollpayment, which help to avoid the traffic delay at toll road. One of the majorcomponents of an electronic toll collection is the automatic vehicle detectionand classification (AVDC) system which is important to classify the vehicle sothat the toll is charged according to the vehicle classes. Vision-based vehicleclassification system is one type of vehicle classification system which adoptcamera as the input sensing device for the system. This type of system hasadvantage over the rest for it is cost efficient as low cost camera is used.The implementation of vision-based vehicle classification system requires lowerinitial investment cost and very suitable for the toll collection trendmigration in Malaysia from single ETC system to full-scale multi-lane free flow(MLFF). This project includes the development of an image-based vehicleclassification system as an effort to seek for a robust vision-based vehicleclassification system. The techniques used in the system includescale-invariant feature transform (SIFT) technique, Canny's edge detector,K-means clustering as well as Euclidean distance matching. In this project, aunique way to image description as matching medium is proposed. Thisdistinctiveness of method is analogous to the human DNA concept which is highlyunique. The system is evaluated on open datasets and return promising results.
arxiv-300-263 | Automatic facial feature extraction and expression recognition based on neural network | http://arxiv.org/pdf/1204.2073v1.pdf | author:S. P. Khandait, R. C. Thool, P. D. Khandait category:cs.CV published:2012-04-10 summary:In this paper, an approach to the problem of automatic facial featureextraction from a still frontal posed image and classification and recognitionof facial expression and hence emotion and mood of a person is presented. Feedforward back propagation neural network is used as a classifier for classifyingthe expressions of supplied face into seven basic categories like surprise,neutral, sad, disgust, fear, happy and angry. For face portion segmentation andlocalization, morphological image processing operations are used. Permanentfacial features like eyebrows, eyes, mouth and nose are extracted using SUSANedge detection operator, facial geometry, edge projection analysis. Experimentsare carried out on JAFFE facial expression database and gives betterperformance in terms of 100% accuracy for training set and 95.26% accuracy fortest set.
arxiv-300-264 | Compressed Beamforming in Ultrasound Imaging | http://arxiv.org/pdf/1202.6037v2.pdf | author:Noam Wagner, Yonina C. Eldar, Zvi Friedman category:cs.IT cs.CV math.IT published:2012-02-09 summary:Emerging sonography techniques often require increasing the number oftransducer elements involved in the imaging process. Consequently, largeramounts of data must be acquired and processed. The significant growth in theamounts of data affects both machinery size and power consumption. Within theclassical sampling framework, state of the art systems reduce processing ratesby exploiting the bandpass bandwidth of the detected signals. It has beenrecently shown, that a much more significant sample-rate reduction may beobtained, by treating ultrasound signals within the Finite Rate of Innovationframework. These ideas follow the spirit of Xampling, which combines classicmethods from sampling theory with recent developments in Compressed Sensing.Applying such low-rate sampling schemes to individual transducer elements,which detect energy reflected from biological tissues, is limited by the noisynature of the signals. This often results in erroneous parameter extraction,bringing forward the need to enhance the SNR of the low-rate samples. In ourwork, we achieve SNR enhancement, by beamforming the sub-Nyquist samplesobtained from multiple elements. We refer to this process as "compressedbeamforming". Applying it to cardiac ultrasound data, we successfully imagemacroscopic perturbations, while achieving a nearly eight-fold reduction insample-rate, compared to standard techniques.
arxiv-300-265 | SVD-EBP Algorithm for Iris Pattern Recognition | http://arxiv.org/pdf/1204.2062v1.pdf | author:Babasaheb G. Patil, Shaila Subbaraman category:cs.CV published:2012-04-10 summary:This paper proposes a neural network approach based on Error Back Propagation(EBP) for classification of different eye images. To reduce the complexity oflayered neural network the dimensions of input vectors are optimized usingSingular Value Decomposition (SVD). The main of this work is to provide forbest method for feature extraction and classification. The details of thiscombined system named as SVD-EBP system, and results thereof are presented inthis paper. Keywords- Singular value decomposition(SVD), Error back Propagation(EBP).
arxiv-300-266 | A Fuzzy Similarity Based Concept Mining Model for Text Classification | http://arxiv.org/pdf/1204.2061v1.pdf | author:Shalini Puri category:cs.IR cs.LG published:2012-04-10 summary:Text Classification is a challenging and a red hot field in the currentscenario and has great importance in text categorization applications. A lot ofresearch work has been done in this field but there is a need to categorize acollection of text documents into mutually exclusive categories by extractingthe concepts or features using supervised learning paradigm and differentclassification algorithms. In this paper, a new Fuzzy Similarity Based ConceptMining Model (FSCMM) is proposed to classify a set of text documents into pre -defined Category Groups (CG) by providing them training and preparing on thesentence, document and integrated corpora levels along with feature reduction,ambiguity removal on each level to achieve high system performance. FuzzyFeature Category Similarity Analyzer (FFCSA) is used to analyze each extractedfeature of Integrated Corpora Feature Vector (ICFV) with the correspondingcategories or classes. This model uses Support Vector Machine Classifier (SVMC)to classify correctly the training data patterns into two groups; i. e., + 1and - 1, thereby producing accurate and correct results. The proposed modelworks efficiently and effectively with great performance and high - accuracyresults.
arxiv-300-267 | A technical study and analysis on fuzzy similarity based models for text classification | http://arxiv.org/pdf/1204.2058v1.pdf | author:Shalini Puri, Sona Kaushik category:cs.IR cs.LG published:2012-04-10 summary:In this new and current era of technology, advancements and techniques,efficient and effective text document classification is becoming a challengingand highly required area to capably categorize text documents into mutuallyexclusive categories. Fuzzy similarity provides a way to find the similarity offeatures among various documents. In this paper, a technical review on variousfuzzy similarity based models is given. These models are discussed and comparedto frame out their use and necessity. A tour of different methodologies isprovided which is based upon fuzzy similarity related concerns. It shows thathow text and web documents are categorized efficiently into differentcategories. Various experimental results of these models are also discussed.The technical comparisons among each model's parameters are shown in the formof a 3-D chart. Such study and technical review provide a strong base ofresearch work done on fuzzy similarity based text document categorization.
arxiv-300-268 | Coherence Functions with Applications in Large-Margin Classification Methods | http://arxiv.org/pdf/1204.2049v1.pdf | author:Zhihua Zhang, Guang Dai, Michael I. Jordan category:stat.ML published:2012-04-10 summary:Support vector machines (SVMs) naturally embody sparseness due to their useof hinge loss functions. However, SVMs can not directly estimate conditionalclass probabilities. In this paper we propose and study a family of coherencefunctions, which are convex and differentiable, as surrogates of the hingefunction. The coherence function is derived by using the maximum-entropyprinciple and is characterized by a temperature parameter. It bridges the hingefunction and the logit function in logistic regression. The limit of thecoherence function at zero temperature corresponds to the hinge function, andthe limit of the minimizer of its expected error is the minimizer of theexpected error of the hinge loss. We refer to the use of the coherence functionin large-margin classification as C-learning, and we present efficientcoordinate descent algorithms for the training of regularized ${\calC}$-learning models.
arxiv-300-269 | Learning Topic Models - Going beyond SVD | http://arxiv.org/pdf/1204.1956v2.pdf | author:Sanjeev Arora, Rong Ge, Ankur Moitra category:cs.LG cs.DS cs.IR published:2012-04-09 summary:Topic Modeling is an approach used for automatic comprehension andclassification of data in a variety of settings, and perhaps the canonicalapplication is in uncovering thematic structure in a corpus of documents. Anumber of foundational works both in machine learning and in theory havesuggested a probabilistic model for documents, whereby documents arise as aconvex combination of (i.e. distribution on) a small number of topic vectors,each topic vector being a distribution on words (i.e. a vector ofword-frequencies). Similar models have since been used in a variety ofapplication areas; the Latent Dirichlet Allocation or LDA model of Blei et al.is especially popular. Theoretical studies of topic modeling focus on learning the model'sparameters assuming the data is actually generated from it. Existing approachesfor the most part rely on Singular Value Decomposition(SVD), and consequentlyhave one of two limitations: these works need to either assume that eachdocument contains only one topic, or else can only recover the span of thetopic vectors instead of the topic vectors themselves. This paper formally justifies Nonnegative Matrix Factorization(NMF) as a maintool in this context, which is an analog of SVD where all vectors arenonnegative. Using this tool we give the first polynomial-time algorithm forlearning topic models without the above two limitations. The algorithm uses afairly mild assumption about the underlying topic matrix called separability,which is usually found to hold in real-life data. A compelling feature of ouralgorithm is that it generalizes to models that incorporate topic-topiccorrelations, such as the Correlated Topic Model and the Pachinko AllocationModel. We hope that this paper will motivate further theoretical results that useNMF as a replacement for SVD - just as NMF has come to replace SVD in manyapplications.
arxiv-300-270 | Non-asymptotic Oracle Inequalities for the High-Dimensional Cox Regression via Lasso | http://arxiv.org/pdf/1204.1992v1.pdf | author:Shengchun Kong, Bin Nan category:math.ST stat.ML stat.TH published:2012-04-09 summary:We consider the finite sample properties of the regularized high-dimensionalCox regression via lasso. Existing literature focuses on linear models orgeneralized linear models with Lipschitz loss functions, where the empiricalrisk functions are the summations of independent and identically distributed(iid) losses. The summands in the negative log partial likelihood function forcensored survival data, however, are neither iid nor Lipschitz. We firstapproximate the negative log partial likelihood function by a sum of iidnon-Lipschitz terms, then derive the non-asymptotic oracle inequalities for thelasso penalized Cox regression using pointwise arguments to tackle thedifficulty caused by the lack of iid and Lipschitz property.
arxiv-300-271 | Matrix Completion from Noisy Entries | http://arxiv.org/pdf/0906.2027v2.pdf | author:Raghunandan H. Keshavan, Andrea Montanari, Sewoong Oh category:cs.LG stat.ML published:2009-06-11 summary:Given a matrix M of low-rank, we consider the problem of reconstructing itfrom noisy observations of a small, random subset of its entries. The problemarises in a variety of applications, from collaborative filtering (the `Netflixproblem') to structure-from-motion and positioning. We study a low complexityalgorithm introduced by Keshavan et al.(2009), based on a combination ofspectral techniques and manifold optimization, that we call here OptSpace. Weprove performance guarantees that are order-optimal in a number ofcircumstances.
arxiv-300-272 | Knapsack based Optimal Policies for Budget-Limited Multi-Armed Bandits | http://arxiv.org/pdf/1204.1909v1.pdf | author:Long Tran-Thanh, Archie Chapman, Alex Rogers, Nicholas R. Jennings category:cs.AI cs.LG published:2012-04-09 summary:In budget-limited multi-armed bandit (MAB) problems, the learner's actionsare costly and constrained by a fixed budget. Consequently, an optimalexploitation policy may not be to pull the optimal arm repeatedly, as is thecase in other variants of MAB, but rather to pull the sequence of differentarms that maximises the agent's total reward within the budget. This differencefrom existing MABs means that new approaches to maximising the total reward arerequired. Given this, we develop two pulling policies, namely: (i) KUBE; and(ii) fractional KUBE. Whereas the former provides better performance up to 40%in our experimental settings, the latter is computationally less expensive. Wealso prove logarithmic upper bounds for the regret of both policies, and showthat these bounds are asymptotically optimal (i.e. they only differ from thebest possible regret by a constant factor).
arxiv-300-273 | Analysis of Thompson Sampling for the multi-armed bandit problem | http://arxiv.org/pdf/1111.1797v3.pdf | author:Shipra Agrawal, Navin Goyal category:cs.LG cs.DS 68W40, 68Q25 F.2.0 published:2011-11-08 summary:The multi-armed bandit problem is a popular model for studyingexploration/exploitation trade-off in sequential decision problems. Manyalgorithms are now available for this well-studied problem. One of the earliestalgorithms, given by W. R. Thompson, dates back to 1933. This algorithm,referred to as Thompson Sampling, is a natural Bayesian algorithm. The basicidea is to choose an arm to play according to its probability of being the bestarm. Thompson Sampling algorithm has experimentally been shown to be close tooptimal. In addition, it is efficient to implement and exhibits severaldesirable properties such as small regret for delayed feedback. However,theoretical understanding of this algorithm was quite limited. In this paper,for the first time, we show that Thompson Sampling algorithm achieveslogarithmic expected regret for the multi-armed bandit problem. More precisely,for the two-armed bandit problem, the expected regret in time $T$ is$O(\frac{\ln T}{\Delta} + \frac{1}{\Delta^3})$. And, for the $N$-armed banditproblem, the expected regret in time $T$ is $O([(\sum_{i=2}^N\frac{1}{\Delta_i^2})^2] \ln T)$. Our bounds are optimal but for the dependenceon $\Delta_i$ and the constant factors in big-Oh.
arxiv-300-274 | Skin-color based videos categorization | http://arxiv.org/pdf/1204.1811v1.pdf | author:Rehanullah Khan, Asad Maqsood, Zeeshan Khan, Muhammad Ishaq, Arsalan Arif category:cs.CV cs.AI published:2012-04-09 summary:On dedicated websites, people can upload videos and share it with the rest ofthe world. Currently these videos are cat- egorized manually by the help of theuser community. In this paper, we propose a combination of color spaces withthe Bayesian network approach for robust detection of skin color followed by anautomated video categorization. Exper- imental results show that our method canachieve satisfactory performance for categorizing videos based on skin color.
arxiv-300-275 | Estimation of causal orders in a linear non-Gaussian acyclic model: a method robust against latent confounders | http://arxiv.org/pdf/1204.1795v1.pdf | author:Tatsuya Tashiro, Shohei Shimizu, Aapo Hyvarinen, Takashi Washio category:stat.ML published:2012-04-09 summary:We consider to learn a causal ordering of variables in a linear non-Gaussianacyclic model called LiNGAM. Several existing methods have been shown toconsistently estimate a causal ordering assuming that all the model assumptionsare correct. But, the estimation results could be distorted if some assumptionsactually are violated. In this paper, we propose a new algorithm for learningcausal orders that is robust against one typical violation of the modelassumptions: latent confounders. We demonstrate the effectiveness of our methodusing artificial data.
arxiv-300-276 | Period-halving Bifurcation of a Neuronal Recurrence Equation | http://arxiv.org/pdf/1110.3586v3.pdf | author:RenÃ© Ndoundam category:cs.NE math.DS nlin.CD published:2011-10-17 summary:We study the sequences generated by neuronal recurrence equations of the form$x(n) = {\bf 1}[\sum_{j=1}^{h} a_{j} x(n-j)- \theta]$. From a neuronalrecurrence equation of memory size $h$ which describes a cycle of length$\rho(m) \times lcm(p_0, p_1,..., p_{-1+\rho(m)})$, we construct a set of$\rho(m)$ neuronal recurrence equations whose dynamics describe respectivelythe transient of length $O(\rho(m) \times lcm(p_0, ..., p_{d}))$ and the cycleof length $O(\rho(m) \times lcm(p_{d+1}, ..., p_{-1+\rho(m)}))$ if $0 \leq d\leq -2+\rho(m)$ and 1 if $d=\rho(m)-1$. This result shows the exponential time of the convergence of neuronalrecurrence equation to fixed points and the existence of the period-halvingbifurcation.
arxiv-300-277 | Efficient Design of Triplet Based Spike-Timing Dependent Plasticity | http://arxiv.org/pdf/1204.1706v1.pdf | author:Mostafa Rahimi Azghadi, Said Al-Sarawi, Nicolangelo Iannella, Derek Abbott category:cs.NE published:2012-04-08 summary:Spike-Timing Dependent Plasticity (STDP) is believed to play an importantrole in learning and the formation of computational function in the brain. Theclassical model of STDP which considers the timing between pairs ofpre-synaptic and post-synaptic spikes (p-STDP) is incapable of reproducingsynaptic weight changes similar to those seen in biological experiments whichinvestigate the effect of either higher order spike trains (e.g. triplet andquadruplet of spikes), or, simultaneous effect of the rate and timing of spikepairs on synaptic plasticity. In this paper, we firstly investigate synapticweight changes using a p-STDP circuit and show how it fails to reproduce thementioned complex biological experiments. We then present a new STDP VLSIcircuit which acts based on the timing among triplets of spikes (t-STDP) thatis able to reproduce all the mentioned experimental results. We believe thatour new STDP VLSI circuit improves upon previous circuits, whose learningcapacity exceeds current designs due to its capability of mimicking theoutcomes of biological experiments more closely; thus plays a significant rolein future VLSI implementation of neuromorphic systems.
arxiv-300-278 | Multi-Level Coding Efficiency with Improved Quality for Image Compression based on AMBTC | http://arxiv.org/pdf/1204.1704v1.pdf | author:K. Somasundaram, S. Vimala category:cs.CV published:2012-04-08 summary:In this paper, we have proposed an extended version of Absolute Moment BlockTruncation Coding (AMBTC) to compress images. Generally the elements of abitplane used in the variants of Block Truncation Coding (BTC) are of size 1bit. But it has been extended to two bits in the proposed method. Number ofstatistical moments preserved to reconstruct the compressed has also beenraised from 2 to 4. Hence, the quality of the reconstructed images has beenimproved significantly from 33.62 to 38.12 with the increase in bpp by 1. Theincreased bpp (3) is further reduced to 1.75in multiple levels: in one level,by dropping 4 elements of the bitplane in such a away that the pixel values ofthe dropped elements can easily be interpolated with out much of loss in thequality, in level two, eight elements are dropped and reconstructed later andin level three, the size of the statistical moments is reduced. The experimentswere carried over standard images of varying intensities. In all the cases, theproposed method outperforms the existing AMBTC technique in terms of both PSNRand bpp.
arxiv-300-279 | The threshold EM algorithm for parameter learning in bayesian network with incomplete data | http://arxiv.org/pdf/1204.1681v1.pdf | author:Fradj Ben Lamine, Karim Kalti, Mohamed Ali Mahjoub category:cs.AI cs.LG stat.ML published:2012-04-07 summary:Bayesian networks (BN) are used in a big range of applications but they haveone issue concerning parameter learning. In real application, training data arealways incomplete or some nodes are hidden. To deal with this problem manylearning parameter algorithms are suggested foreground EM, Gibbs sampling andRBE algorithms. In order to limit the search space and escape from local maximaproduced by executing EM algorithm, this paper presents a learning parameteralgorithm that is a fusion of EM and RBE algorithms. This algorithmincorporates the range of a parameter into the EM algorithm. This range iscalculated by the first step of RBE algorithm allowing a regularization of eachparameter in bayesian network after the maximization step of the EM algorithm.The threshold EM algorithm is applied in brain tumor diagnosis and show someadvantages and disadvantages over the EM algorithm.
arxiv-300-280 | Clustering and Bayesian network for image of faces classification | http://arxiv.org/pdf/1204.1679v1.pdf | author:Khlifia Jayech, Mohamed Ali Mahjoub category:cs.CV cs.AI published:2012-04-07 summary:In a content based image classification system, target images are sorted byfeature similarities with respect to the query (CBIR). In this paper, wepropose to use new approach combining distance tangent, k-means algorithm andBayesian network for image classification. First, we use the technique oftangent distance to calculate several tangent spaces representing the sameimage. The objective is to reduce the error in the classification phase.Second, we cut the image in a whole of blocks. For each block, we compute avector of descriptors. Then, we use K-means to cluster the low-level featuresincluding color and texture information to build a vector of labels for eachimage. Finally, we apply five variants of Bayesian networks classifiers(Na\"ive Bayes, Global Tree Augmented Na\"ive Bayes (GTAN), Global ForestAugmented Na\"ive Bayes (GFAN), Tree Augmented Na\"ive Bayes for each class(TAN), and Forest Augmented Na\"ive Bayes for each class (FAN) to classify theimage of faces using the vector of labels. In order to validate the feasibilityand effectively, we compare the results of GFAN to FAN and to the othersclassifiers (NB, GTAN, TAN). The results demonstrate FAN outperforms than GFAN,NB, GTAN and TAN in the overall classification accuracy.
arxiv-300-281 | A New Approach for Arabic Handwritten Postal Addresses Recognition | http://arxiv.org/pdf/1204.1678v1.pdf | author:Moncef Charfi, Monji Kherallah, Abdelkarim El Baati, Adel M. Alimi category:cs.CV published:2012-04-07 summary:In this paper, we propose an automatic analysis system for the Arabichandwriting postal addresses recognition, by using the beta elliptical model.Our system is divided into different steps: analysis, pre-processing andclassification. The first operation is the filtering of image. In the second,we remove the border print, stamps and graphics. After locating the address onthe envelope, the address segmentation allows the extraction of postal code andcity name separately. The pre-processing system and the modeling approach arebased on two basic steps. The first step is the extraction of the temporalorder in the image of the handwritten trajectory. The second step is based onthe use of Beta-Elliptical model for the representation of handwritten script.The recognition system is based on Graph-matching algorithm. Our modeling andrecognition approaches were validated by using the postal code and city namesextracted from the Tunisian postal envelopes data. The recognition rateobtained is about 98%.
arxiv-300-282 | Automatic liver segmentation method in CT images | http://arxiv.org/pdf/1204.1634v1.pdf | author:Oussema zayane, besma jouini, Mohamed Ali Mahjoub category:cs.CV published:2012-04-07 summary:The aim of this work is to develop a method for automatic segmentation of theliver based on a priori knowledge of the image, such as location and shape ofthe liver.
arxiv-300-283 | New approach using Bayesian Network to improve content based image classification systems | http://arxiv.org/pdf/1204.1631v1.pdf | author:Khlifia jayech, mohamed ali mahjoub category:cs.CV cs.IR published:2012-04-07 summary:This paper proposes a new approach based on augmented naive Bayes for imageclassification. Initially, each image is cutting in a whole of blocks. For eachblock, we compute a vector of descriptors. Then, we propose to carry out aclassification of the vectors of descriptors to build a vector of labels foreach image. Finally, we propose three variants of Bayesian Networks such asNaive Bayesian Network (NB), Tree Augmented Naive Bayes (TAN) and ForestAugmented Naive Bayes (FAN) to classify the image using the vector of labels.The results showed a marked improvement over the FAN, NB and TAN.
arxiv-300-284 | Image segmentation by adaptive distance based on EM algorithm | http://arxiv.org/pdf/1204.1629v1.pdf | author:Mohamed Ali Mahjoub, karim kalti category:cs.CV published:2012-04-07 summary:This paper introduces a Bayesian image segmentation algorithm based on finitemixtures. An EM algorithm is developed to estimate parameters of the Gaussianmixtures. The finite mixture is a flexible and powerful probabilistic modelingtool. It can be used to provide a model-based clustering in the field ofpattern recognition. However, the application of finite mixtures to imagesegmentation presents some difficulties; especially it's sensible to noise. Inthis paper we propose a variant of this method which aims to resolve thisproblem. Our approach proceeds by the characterization of pixels by twofeatures: the first one describes the intrinsic properties of the pixel and thesecond characterizes the neighborhood of pixel. Then the classification is madeon the base on adaptive distance which privileges the one or the other featuresaccording to the spatial position of the pixel in the image. The obtainedresults have shown a significant improvement of our approach compared to thestandard version of EM algorithm.
arxiv-300-285 | UCB Algorithm for Exponential Distributions | http://arxiv.org/pdf/1204.1624v1.pdf | author:Wassim Jouini, Christophe Moy category:stat.ML cs.LG published:2012-04-07 summary:We introduce in this paper a new algorithm for Multi-Armed Bandit (MAB)problems. A machine learning paradigm popular within Cognitive Network relatedtopics (e.g., Spectrum Sensing and Allocation). We focus on the case where therewards are exponentially distributed, which is common when dealing withRayleigh fading channels. This strategy, named Multiplicative Upper ConfidenceBound (MUCB), associates a utility index to every available arm, and thenselects the arm with the highest index. For every arm, the associated index isequal to the product of a multiplicative factor by the sample mean of therewards collected by this arm. We show that the MUCB policy has a lowcomplexity and is order optimal.
arxiv-300-286 | Discrimination between Arabic and Latin from bilingual documents | http://arxiv.org/pdf/1204.1615v1.pdf | author:Sofiene Haboubi, Samia Maddouri, Hamid Amiri category:cs.CV cs.CL cs.IR published:2012-04-07 summary:2011 International Conference on Communications, Computing and ControlApplications (CCCA)
arxiv-300-287 | Vision-based Human Gender Recognition: A Survey | http://arxiv.org/pdf/1204.1611v1.pdf | author:Choon Boon Ng, Yong Haur Tay, Bok Min Goi category:cs.CV published:2012-04-07 summary:Gender is an important demographic attribute of people. This paper provides asurvey of human gender recognition in computer vision. A review of approachesexploiting information from face and whole body (either from a still image orgait sequence) is presented. We highlight the challenges faced and survey therepresentative methods of these approaches. Based on the results, goodperformance have been achieved for datasets captured under controlledenvironments, but there is still much work that can be done to improve therobustness of gender recognition under real-life environments.
arxiv-300-288 | Randomized Smoothing for Stochastic Optimization | http://arxiv.org/pdf/1103.4296v2.pdf | author:John C. Duchi, Peter L. Bartlett, Martin J. Wainwright category:math.OC stat.ML published:2011-03-22 summary:We analyze convergence rates of stochastic optimization procedures fornon-smooth convex optimization problems. By combining randomized smoothingtechniques with accelerated gradient methods, we obtain convergence rates ofstochastic optimization procedures, both in expectation and with highprobability, that have optimal dependence on the variance of the gradientestimates. To the best of our knowledge, these are the first variance-basedrates for non-smooth optimization. We give several applications of our resultsto statistical estimation problems, and provide experimental results thatdemonstrate the effectiveness of the proposed algorithms. We also describe howa combination of our algorithm with recent work on decentralized optimizationyields a distributed stochastic optimization algorithm that is order-optimal.
arxiv-300-289 | A Machine Learning Approach For Opinion Holder Extraction In Arabic Language | http://arxiv.org/pdf/1206.1011v1.pdf | author:Mohamed Elarnaoty, Samir AbdelRahman, Aly Fahmy category:cs.IR cs.LG published:2012-04-06 summary:Opinion mining aims at extracting useful subjective information from reliableamounts of text. Opinion mining holder recognition is a task that has not beenconsidered yet in Arabic Language. This task essentially requires deepunderstanding of clauses structures. Unfortunately, the lack of a robust,publicly available, Arabic parser further complicates the research. This paperpresents a leading research for the opinion holder extraction in Arabic newsindependent from any lexical parsers. We investigate constructing acomprehensive feature set to compensate the lack of parsing structuraloutcomes. The proposed feature set is tuned from English previous works coupledwith our proposed semantic field and named entities features. Our featureanalysis is based on Conditional Random Fields (CRF) and semi-supervisedpattern recognition techniques. Different research models are evaluated viacross-validation experiments achieving 54.03 F-measure. We publicly release ourown research outcome corpus and lexicon for opinion mining community toencourage further research.
arxiv-300-290 | Learning Fuzzy Î²-Certain and Î²-Possible rules from incomplete quantitative data by rough sets | http://arxiv.org/pdf/1204.1467v1.pdf | author:Ali Soltan Mohammadi, L. Asadzadeh, D. D. Rezaee category:cs.DS cs.LG published:2012-04-06 summary:The rough-set theory proposed by Pawlak, has been widely used in dealing withdata classification problems. The original rough-set model is, however, quitesensitive to noisy data. Tzung thus proposed deals with the problem ofproducing a set of fuzzy certain and fuzzy possible rules from quantitativedata with a predefined tolerance degree of uncertainty and misclassification.This model allowed, which combines the variable precision rough-set model andthe fuzzy set theory, is thus proposed to solve this problem. This paper thusdeals with the problem of producing a set of fuzzy certain and fuzzy possiblerules from incomplete quantitative data with a predefined tolerance degree ofuncertainty and misclassification. A new method, incomplete quantitative datafor rough-set model and the fuzzy set theory, is thus proposed to solve thisproblem. It first transforms each quantitative value into a fuzzy set oflinguistic terms using membership functions and then finding incompletequantitative data with lower and the fuzzy upper approximations. It secondcalculates the fuzzy {\beta}-lower and the fuzzy {\beta}-upper approximations.The certain and possible rules are then generated based on these fuzzyapproximations. These rules can then be used to classify unknown objects.
arxiv-300-291 | Fast projections onto mixed-norm balls with applications | http://arxiv.org/pdf/1204.1437v1.pdf | author:Suvrit Sra category:stat.ML cs.LG math.OC published:2012-04-06 summary:Joint sparsity offers powerful structural cues for feature selection,especially for variables that are expected to demonstrate a "grouped" behavior.Such behavior is commonly modeled via group-lasso, multitask lasso, and relatedmethods where feature selection is effected via mixed-norms. Several mixed-normbased sparse models have received substantial attention, and for some casesefficient algorithms are also available. Surprisingly, several constrainedsparse models seem to be lacking scalable algorithms. We address thisdeficiency by presenting batch and online (stochastic-gradient) optimizationmethods, both of which rely on efficient projections onto mixed-norm balls. Weillustrate our methods by applying them to the multitask lasso. We conclude bymentioning some open problems.
arxiv-300-292 | Continuous Markov Random Fields for Robust Stereo Estimation | http://arxiv.org/pdf/1204.1393v1.pdf | author:Koichiro Yamaguchi, Tamir Hazan, David McAllester, Raquel Urtasun category:cs.CV I.2.10; I.4.8 published:2012-04-06 summary:In this paper we present a novel slanted-plane MRF model which reasonsjointly about occlusion boundaries as well as depth. We formulate the problemas the one of inference in a hybrid MRF composed of both continuous (i.e.,slanted 3D planes) and discrete (i.e., occlusion boundaries) random variables.This allows us to define potentials encoding the ownership of the pixels thatcompose the boundary between segments, as well as potentials encoding whichjunctions are physically possible. Our approach outperforms thestate-of-the-art on Middlebury high resolution imagery as well as in the morechallenging KITTI dataset, while being more efficient than existing slantedplane MRF-based methods, taking on average 2 minutes to perform inference onhigh resolution imagery.
arxiv-300-293 | Learning to relate images: Mapping units, complex cells and simultaneous eigenspaces | http://arxiv.org/pdf/1110.0107v2.pdf | author:Roland Memisevic category:cs.CV cs.AI nlin.AO stat.ML published:2011-10-01 summary:A fundamental operation in many vision tasks, including motion understanding,stereopsis, visual odometry, or invariant recognition, is establishingcorrespondences between images or between images and data from othermodalities. We present an analysis of the role that multiplicative interactionsplay in learning such correspondences, and we show how learning and inferringrelationships between images can be viewed as detecting rotations in theeigenspaces shared among a set of orthogonal matrices. We review a variety ofrecent multiplicative sparse coding methods in light of this observation. Wealso review how the squaring operation performed by energy models and by modelsof complex cells can be thought of as a way to implement multiplicativeinteractions. This suggests that the main utility of including complex cells incomputational models of vision may be that they can encode relations notinvariances.
arxiv-300-294 | Mouse Simulation Using Two Coloured Tapes | http://arxiv.org/pdf/1204.1277v1.pdf | author:Vikram Kumar, Kamran Niyazi, Swapnil Mahe, Swapnil Vyawahare category:cs.AI cs.CV published:2012-04-05 summary:In this paper, we present a novel approach for Human Computer Interaction(HCI) where, we control cursor movement using a real-time camera. Currentmethods involve changing mouse parts such as adding more buttons or changingthe position of the tracking ball. Instead, our method is to use a camera andcomputer vision technology, such as image segmentation and gesture recognition,to control mouse tasks (left and right clicking, double-clicking, andscrolling) and we show how it can perform everything as current mouse devicescan. The software will be developed in JAVA language. Recognition and poseestimation in this system are user independent and robust as we will be usingcolour tapes on our finger to perform actions. The software can be used as anintuitive input interface to applications that require multi-dimensionalcontrol e.g. computer games etc.
arxiv-300-295 | Tight Sample Complexity of Large-Margin Learning | http://arxiv.org/pdf/1011.5053v2.pdf | author:Sivan Sabato, Nathan Srebro, Naftali Tishby category:cs.LG math.PR math.ST stat.ML stat.TH published:2010-11-23 summary:We obtain a tight distribution-specific characterization of the samplecomplexity of large-margin classification with L_2 regularization: We introducethe \gamma-adapted-dimension, which is a simple function of the spectrum of adistribution's covariance matrix, and show distribution-specific upper andlower bounds on the sample complexity, both governed by the\gamma-adapted-dimension of the source distribution. We conclude that this newquantity tightly characterizes the true sample complexity of large-marginclassification. The bounds hold for a rich family of sub-Gaussiandistributions.
arxiv-300-296 | A Complete Workflow for Development of Bangla OCR | http://arxiv.org/pdf/1204.1198v1.pdf | author:Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas category:cs.CV published:2012-04-05 summary:Developing a Bangla OCR requires bunch of algorithm and methods. There weremany effort went on for developing a Bangla OCR. But all of them failed toprovide an error free Bangla OCR. Each of them has some lacking. We discussedabout the problem scope of currently existing Bangla OCR's. In this paper, wepresent the basic steps required for developing a Bangla OCR and a completeworkflow for development of a Bangla OCR with mentioning all the possiblealgorithms required.
arxiv-300-297 | An Implementation of Intrusion Detection System Using Genetic Algorithm | http://arxiv.org/pdf/1204.1336v1.pdf | author:Mohammad Sazzadul Hoque, Md. Abdul Mukit, Md. Abu Naser Bikas category:cs.CR cs.NE cs.NI published:2012-04-05 summary:Nowadays it is very important to maintain a high level security to ensuresafe and trusted communication of information between various organizations.But secured data communication over internet and any other network is alwaysunder threat of intrusions and misuses. So Intrusion Detection Systems havebecome a needful component in terms of computer and network security. There arevarious approaches being utilized in intrusion detections, but unfortunatelyany of the systems so far is not completely flawless. So, the quest ofbetterment continues. In this progression, here we present an IntrusionDetection System (IDS), by applying genetic algorithm (GA) to efficientlydetect various types of network intrusions. Parameters and evolution processesfor GA are discussed in details and implemented. This approach uses evolutiontheory to information evolution in order to filter the traffic data and thusreduce the complexity. To implement and measure the performance of our systemwe used the KDD99 benchmark dataset and obtained reasonable detection rate.
arxiv-300-298 | Principal Component Analysis-Linear Discriminant Analysis Feature Extractor for Pattern Recognition | http://arxiv.org/pdf/1204.1177v1.pdf | author:Aamir Khan, Hasan Farooq category:cs.CV published:2012-04-05 summary:Robustness of embedded biometric systems is of prime importance with theemergence of fourth generation communication devices and advancement insecurity systems This paper presents the realization of such technologies whichdemands reliable and error-free biometric identity verification systems. Highdimensional patterns are not permitted due to eigen-decomposition in highdimensional image space and degeneration of scattering matrices in small sizesample. Generalization, dimensionality reduction and maximizing the margins arecontrolled by minimizing weight vectors. Results show good pattern bymultimodal biometric system proposed in this paper. This paper is aimed atinvestigating a biometric identity system using Principal Component Analysisand Lindear Discriminant Analysis with K-Nearest Neighbor and implementing suchsystem in real-time using SignalWAVE.
arxiv-300-299 | A Topic Modeling Toolbox Using Belief Propagation | http://arxiv.org/pdf/1201.0838v2.pdf | author:Jia Zeng category:cs.LG published:2012-01-04 summary:Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian modelfor probabilistic topic modeling, which attracts worldwide interests andtouches on many important applications in text mining, computer vision andcomputational biology. This paper introduces a topic modeling toolbox (TMBP)based on the belief propagation (BP) algorithms. TMBP toolbox is implemented byMEX C++/Matlab/Octave for either Windows 7 or Linux. Compared with existingtopic modeling packages, the novelty of this toolbox lies in the BP algorithmsfor learning LDA-based topic models. The current version includes BP algorithmsfor latent Dirichlet allocation (LDA), author-topic models (ATM), relationaltopic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing projectand more BP-based algorithms for various topic models will be added in the nearfuture. Interested users may also extend BP algorithms for learning morecomplicated topic models. The source codes are freely available under the GNUGeneral Public Licence, Version 1.0 at https://mloss.org/software/view/399/.
arxiv-300-300 | Efficient Fruit Defect Detection and Glare removal Algorithm by anisotropic diffusion and 2D Gabor filter | http://arxiv.org/pdf/1204.0767v2.pdf | author:Vini Katyal, Deepesh Srivastava category:cs.CV published:2012-04-03 summary:This paper focuses on fruit defect detection and glare removal usingmorphological operations, Glare removal can be considered as an importantpreprocessing step as uneven lighting may introduce it in images, which hamperthe results produced through segmentation by Gabor filters .The problem ofglare in images is very pronounced sometimes due to the unusual reflectancefrom the camera sensor or stray light entering, this method counteracts thisproblem and makes the defect detection much more pronounced. Anisotropicdiffusion is used for further smoothening of the images and removing the highenergy regions in an image for better defect detection and makes the defectsmore retrievable. Our algorithm is robust and scalable the employability of aparticular mask for glare removal has been checked and proved useful forcounteracting.this problem, anisotropic diffusion further enhances the defectswith its use further Optimal Gabor filter at various orientations is used fordefect detection.
