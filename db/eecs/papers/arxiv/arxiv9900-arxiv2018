arxiv-1505-00388 | Order-Revealing Encryption and the Hardness of Private Learning | http://arxiv.org/abs/1505.00388 | id:1505.00388 author:Mark Bun, Mark Zhandry category:cs.CR cs.CC cs.LG  published:2015-05-03 summary:An order-revealing encryption scheme gives a public procedure by which two ciphertexts can be compared to reveal the ordering of their underlying plaintexts. We show how to use order-revealing encryption to separate computationally efficient PAC learning from efficient $(\epsilon, \delta)$-differentially private PAC learning. That is, we construct a concept class that is efficiently PAC learnable, but for which every efficient learner fails to be differentially private. This answers a question of Kasiviswanathan et al. (FOCS '08, SIAM J. Comput. '11). To prove our result, we give a generic transformation from an order-revealing encryption scheme into one with strongly correct comparison, which enables the consistent comparison of ciphertexts that are not obtained as the valid encryption of any message. We believe this construction may be of independent interest. version:1
arxiv-1505-00384 | Making Sense of Hidden Layer Information in Deep Networks by Learning Hierarchical Targets | http://arxiv.org/abs/1505.00384 | id:1505.00384 author:Abhinav Tushar category:cs.NE cs.LG  published:2015-05-03 summary:This paper proposes an architecture for deep neural networks with hidden layer branches that learn targets of lower hierarchy than final layer targets. The branches provide a channel for enforcing useful information in hidden layer which helps in attaining better accuracy, both for the final layer and hidden layers. The shared layers modify their weights using the gradients of all cost functions higher than the branching layer. This model provides a flexible inference system with many levels of targets which is modular and can be used efficiently in situations requiring different levels of results according to complexity. This paper applies the idea to a text classification task on 20 Newsgroups data set with two level of hierarchical targets and a comparison is made with training without the use of hidden layer branches. version:1
arxiv-1412-6577 | Modeling Compositionality with Multiplicative Recurrent Neural Networks | http://arxiv.org/abs/1412.6577 | id:1412.6577 author:Ozan İrsoy, Claire Cardie category:cs.LG cs.CL stat.ML  published:2014-12-20 summary:We present the multiplicative recurrent neural network as a general model for compositional meaning in language, and evaluate it on the task of fine-grained sentiment analysis. We establish a connection to the previously investigated matrix-space models for compositionality, and show they are special cases of the multiplicative recurrent net. Our experiments show that these models perform comparably or better than Elman-type additive recurrent neural networks and outperform matrix-space models on a standard fine-grained sentiment analysis corpus. Furthermore, they yield comparable results to structural deep models on the recently published Stanford Sentiment Treebank without the need for generating parse trees. version:3
arxiv-1505-00353 | Joint Multi-Leaf Segmentation, Alignment and Tracking from Fluorescence Plant Videos | http://arxiv.org/abs/1505.00353 | id:1505.00353 author:Xi Yin, Xiaoming Liu, Jin Chen, David M. Kramer category:cs.CV  published:2015-05-02 summary:This paper proposes a novel framework for fluorescence plant video processing. Biologists are interested in the leaf level photosynthetic analysis within a plant. A prerequisite for such analysis is to segment all leaves, estimate their structures and track them over time. We treat this as a joint multi-leaf segmentation, alignment, and tracking problem. First, leaf segmentation and alignment are applied on the last frame of a plant video to find a number of well-aligned leaf candidates. Second, leaf tracking is applied on the remaining frames with leaf candidate transformation from the previous frame. We form two optimization problems with shared terms in their objective functions for leaf alignment and tracking respectively. Gradient descent is used to solve the proposed optimization problems. A quantitative evaluation framework is formulated to evaluate the performance of our algorithm with three metrics. Two models are learned to predict the alignment accuracy and detect tracking failure respectively. We also study the limitation of our proposed alignment and tracking framework. Experimental results show the effectiveness, efficiency, and robustness of the proposed method. version:1
arxiv-1505-00315 | Learning Temporal Embeddings for Complex Video Analysis | http://arxiv.org/abs/1505.00315 | id:1505.00315 author:Vignesh Ramanathan, Kevin Tang, Greg Mori, Li Fei-Fei category:cs.CV  published:2015-05-02 summary:In this paper, we propose to learn temporal embeddings of video frames for complex video analysis. Large quantities of unlabeled video data can be easily obtained from the Internet. These videos possess the implicit weak label that they are sequences of temporally and semantically coherent images. We leverage this information to learn temporal embeddings for video frames by associating frames with the temporal context that they appear in. To do this, we propose a scheme for incorporating temporal context based on past and future frames in videos, and compare this to other contextual representations. In addition, we show how data augmentation using multi-resolution samples and hard negatives helps to significantly improve the quality of the learned embeddings. We evaluate various design decisions for learning temporal embeddings, and show that our embeddings can improve performance for multiple video tasks such as retrieval, classification, and temporal order recovery in unconstrained Internet video. version:1
arxiv-1505-00314 | Deconstructing Principal Component Analysis Using a Data Reconciliation Perspective | http://arxiv.org/abs/1505.00314 | id:1505.00314 author:Shankar Narasimhan, Nirav Bhatt category:cs.LG cs.SY stat.ME I.2  published:2015-05-02 summary:Data reconciliation (DR) and Principal Component Analysis (PCA) are two popular data analysis techniques in process industries. Data reconciliation is used to obtain accurate and consistent estimates of variables and parameters from erroneous measurements. PCA is primarily used as a method for reducing the dimensionality of high dimensional data and as a preprocessing technique for denoising measurements. These techniques have been developed and deployed independently of each other. The primary purpose of this article is to elucidate the close relationship between these two seemingly disparate techniques. This leads to a unified framework for applying PCA and DR. Further, we show how the two techniques can be deployed together in a collaborative and consistent manner to process data. The framework has been extended to deal with partially measured systems and to incorporate partial knowledge available about the process model. version:1
arxiv-1502-00186 | Advanced Mean Field Theory of Restricted Boltzmann Machine | http://arxiv.org/abs/1502.00186 | id:1502.00186 author:Haiping Huang, Taro Toyoizumi category:cond-mat.stat-mech cs.LG q-bio.NC stat.ML  published:2015-02-01 summary:Learning in restricted Boltzmann machine is typically hard due to the computation of gradients of log-likelihood function. To describe the network state statistics of the restricted Boltzmann machine, we develop an advanced mean field theory based on the Bethe approximation. Our theory provides an efficient message passing based method that evaluates not only the partition function (free energy) but also its gradients without requiring statistical sampling. The results are compared with those obtained by the computationally expensive sampling based method. version:3
arxiv-1505-00308 | Multi-Object Classification and Unsupervised Scene Understanding Using Deep Learning Features and Latent Tree Probabilistic Models | http://arxiv.org/abs/1505.00308 | id:1505.00308 author:Tejaswi Nimmagadda, Anima Anandkumar category:cs.CV cs.LG  published:2015-05-02 summary:Deep learning has shown state-of-art classification performance on datasets such as ImageNet, which contain a single object in each image. However, multi-object classification is far more challenging. We present a unified framework which leverages the strengths of multiple machine learning methods, viz deep learning, probabilistic models and kernel methods to obtain state-of-art performance on Microsoft COCO, consisting of non-iconic images. We incorporate contextual information in natural images through a conditional latent tree probabilistic model (CLTM), where the object co-occurrences are conditioned on the extracted fc7 features from pre-trained Imagenet CNN as input. We learn the CLTM tree structure using conditional pairwise probabilities for object co-occurrences, estimated through kernel methods, and we learn its node and edge potentials by training a new 3-layer neural network, which takes fc7 features as input. Object classification is carried out via inference on the learnt conditional tree model, and we obtain significant gain in precision-recall and F-measures on MS-COCO, especially for difficult object categories. Moreover, the latent variables in the CLTM capture scene information: the images with top activations for a latent node have common themes such as being a grasslands or a food scene, and on on. In addition, we show that a simple k-means clustering of the inferred latent nodes alone significantly improves scene classification performance on the MIT-Indoor dataset, without the need for any retraining, and without using scene labels during training. Thus, we present a unified framework for multi-object classification and unsupervised scene understanding. version:1
arxiv-1505-00296 | Object-Scene Convolutional Neural Networks for Event Recognition in Images | http://arxiv.org/abs/1505.00296 | id:1505.00296 author:Limin Wang, Zhe Wang, Wenbin Du, Yu Qiao category:cs.CV  published:2015-05-02 summary:Event recognition from still images is of great importance for image understanding. However, compared with event recognition in videos, there are much fewer research works on event recognition in images. This paper addresses the issue of event recognition from images and proposes an effective method with deep neural networks. Specifically, we design a new architecture, called Object-Scene Convolutional Neural Network (OS-CNN). This architecture is decomposed into object net and scene net, which extract useful information for event understanding from the perspective of objects and scene context, respectively. Meanwhile, we investigate different network architectures for OS-CNN design, and adapt the deep (AlexNet) and very-deep (GoogLeNet) networks to the task of event recognition. Furthermore, we find that the deep and very-deep networks are complementary to each other. Finally, based on the proposed OS-CNN and comparative study of different network architectures, we come up with a solution of five-stream CNN for the track of cultural event recognition at the ChaLearn Looking at People (LAP) challenge 2015. Our method obtains the performance of 85.5% and ranks the $1^{st}$ place in this challenge. version:1
arxiv-1505-00294 | Monotonous (Semi-)Nonnegative Matrix Factorization | http://arxiv.org/abs/1505.00294 | id:1505.00294 author:Nirav Bhatt, Arun Ayyar category:cs.LG stat.ML I.2  published:2015-05-01 summary:Nonnegative matrix factorization (NMF) factorizes a non-negative matrix into product of two non-negative matrices, namely a signal matrix and a mixing matrix. NMF suffers from the scale and ordering ambiguities. Often, the source signals can be monotonous in nature. For example, in source separation problem, the source signals can be monotonously increasing or decreasing while the mixing matrix can have nonnegative entries. NMF methods may not be effective for such cases as it suffers from the ordering ambiguity. This paper proposes an approach to incorporate notion of monotonicity in NMF, labeled as monotonous NMF. An algorithm based on alternating least-squares is proposed for recovering monotonous signals from a data matrix. Further, the assumption on mixing matrix is relaxed to extend monotonous NMF for data matrix with real numbers as entries. The approach is illustrated using synthetic noisy data. The results obtained by monotonous NMF are compared with standard NMF algorithms in the literature, and it is shown that monotonous NMF estimates source signals well in comparison to standard NMF algorithms when the underlying sources signals are monotonous. version:1
arxiv-1505-00277 | Grounded Discovery of Coordinate Term Relationships between Software Entities | http://arxiv.org/abs/1505.00277 | id:1505.00277 author:Dana Movshovitz-Attias, William W. Cohen category:cs.CL cs.AI cs.LG cs.SE  published:2015-05-01 summary:We present an approach for the detection of coordinate-term relationships between entities from the software domain, that refer to Java classes. Usually, relations are found by examining corpus statistics associated with text entities. In some technical domains, however, we have access to additional information about the real-world objects named by the entities, suggesting that coupling information about the "grounded" entities with corpus statistics might lead to improved methods for relation discovery. To this end, we develop a similarity measure for Java classes using distributional information about how they are used in software, which we combine with corpus statistics on the distribution of contexts in which the classes appear in text. Using our approach, cross-validation accuracy on this dataset can be improved dramatically, from around 60% to 88%. Human labeling results show that our classifier has an F1 score of 86% over the top 1000 predicted pairs. version:1
arxiv-1505-00276 | Joint Object and Part Segmentation using Deep Learned Potentials | http://arxiv.org/abs/1505.00276 | id:1505.00276 author:Peng Wang, Xiaohui Shen, Zhe Lin, Scott Cohen, Brian Price, Alan Yuille category:cs.CV  published:2015-05-01 summary:Segmenting semantic objects from images and parsing them into their respective semantic parts are fundamental steps towards detailed object understanding in computer vision. In this paper, we propose a joint solution that tackles semantic object and part segmentation simultaneously, in which higher object-level context is provided to guide part segmentation, and more detailed part-level localization is utilized to refine object segmentation. Specifically, we first introduce the concept of semantic compositional parts (SCP) in which similar semantic parts are grouped and shared among different objects. A two-channel fully convolutional network (FCN) is then trained to provide the SCP and object potentials at each pixel. At the same time, a compact set of segments can also be obtained from the SCP predictions of the network. Given the potentials and the generated segments, in order to explore long-range context, we finally construct an efficient fully connected conditional random field (FCRF) to jointly predict the final object and part labels. Extensive evaluation on three different datasets shows that our approach can mutually enhance the performance of object and part segmentation, and outperforms the current state-of-the-art on both tasks. version:1
arxiv-1505-00249 | Image Segmentation by Size-Dependent Single Linkage Clustering of a Watershed Basin Graph | http://arxiv.org/abs/1505.00249 | id:1505.00249 author:Aleksandar Zlateski, H. Sebastian Seung category:cs.CV  published:2015-05-01 summary:We present a method for hierarchical image segmentation that defines a disaffinity graph on the image, over-segments it into watershed basins, defines a new graph on the basins, and then merges basins with a modified, size-dependent version of single linkage clustering. The quasilinear runtime of the method makes it suitable for segmenting large images. We illustrate the method on the challenging problem of segmenting 3D electron microscopic brain images. version:1
arxiv-1504-07857 | Probabilistic Depth Image Registration incorporating Nonvisual Information | http://arxiv.org/abs/1504.07857 | id:1504.07857 author:Manuel Wüthrich, Peter Pastor, Ludovic Righetti, Aude Billard, Stefan Schaal category:cs.RO cs.CV  published:2015-04-29 summary:In this paper, we derive a probabilistic registration algorithm for object modeling and tracking. In many robotics applications, such as manipulation tasks, nonvisual information about the movement of the object is available, which we will combine with the visual information. Furthermore we do not only consider observations of the object, but we also take space into account which has been observed to not be part of the object. Furthermore we are computing a posterior distribution over the relative alignment and not a point estimate as typically done in for example Iterative Closest Point (ICP). To our knowledge no existing algorithm meets these three conditions and we thus derive a novel registration algorithm in a Bayesian framework. Experimental results suggest that the proposed methods perform favorably in comparison to PCL implementations of feature mapping and ICP, especially if nonvisual information is available. version:2
arxiv-1505-00218 | Volumetric Bias in Segmentation and Reconstruction: Secrets and Solutions | http://arxiv.org/abs/1505.00218 | id:1505.00218 author:Yuri Boykov, Hossam Isack, Carl Olsson, Ismail Ben Ayed category:cs.CV  published:2015-05-01 summary:Many standard optimization methods for segmentation and reconstruction compute ML model estimates for appearance or geometry of segments, e.g. Zhu-Yuille 1996, Torr 1998, Chan-Vese 2001, GrabCut 2004, Delong et al. 2012. We observe that the standard likelihood term in these formulations corresponds to a generalized probabilistic K-means energy. In learning it is well known that this energy has a strong bias to clusters of equal size, which can be expressed as a penalty for KL divergence from a uniform distribution of cardinalities. However, this volumetric bias has been mostly ignored in computer vision. We demonstrate significant artifacts in standard segmentation and reconstruction methods due to this bias. Moreover, we propose binary and multi-label optimization techniques that either (a) remove this bias or (b) replace it by a KL divergence term for any given target volume distribution. Our general ideas apply to many continuous or discrete energy formulations in segmentation, stereo, and other reconstruction problems. version:1
arxiv-1212-5423 | Topic Extraction and Bundling of Related Scientific Articles | http://arxiv.org/abs/1212.5423 | id:1212.5423 author:Shameem A Puthiya Parambath category:cs.IR cs.DL stat.ML  published:2012-12-21 summary:Automatic classification of scientific articles based on common characteristics is an interesting problem with many applications in digital library and information retrieval systems. Properly organized articles can be useful for automatic generation of taxonomies in scientific writings, textual summarization, efficient information retrieval etc. Generating article bundles from a large number of input articles, based on the associated features of the articles is tedious and computationally expensive task. In this report we propose an automatic two-step approach for topic extraction and bundling of related articles from a set of scientific articles in real-time. For topic extraction, we make use of Latent Dirichlet Allocation (LDA) topic modeling techniques and for bundling, we make use of hierarchical agglomerative clustering techniques. We run experiments to validate our bundling semantics and compare it with existing models in use. We make use of an online crowdsourcing marketplace provided by Amazon called Amazon Mechanical Turk to carry out experiments. We explain our experimental setup and empirical results in detail and show that our method is advantageous over existing ones. version:2
arxiv-1505-00193 | Segmentation and Restoration of Images on Surfaces by Parametric Active Contours with Topology Changes | http://arxiv.org/abs/1505.00193 | id:1505.00193 author:Heike Benninghoff, Harald Garcke category:cs.CV math.AP math.NA  published:2015-05-01 summary:In this article, a new method for segmentation and restoration of images on two-dimensional surfaces is given. Active contour models for image segmentation are extended to images on surfaces. The evolving curves on the surfaces are mathematically described using a parametric approach. For image restoration, a diffusion equation with Neumann boundary conditions is solved in a postprocessing step in the individual regions. Numerical schemes are presented which allow to efficiently compute segmentations and denoised versions of images on surfaces. Also topology changes of the evolving curves are detected and performed using a fast sub-routine. Finally, several experiments are presented where the developed methods are applied on different artificial and real images defined on different surfaces. version:1
arxiv-1505-00171 | SynthCam3D: Semantic Understanding With Synthetic Indoor Scenes | http://arxiv.org/abs/1505.00171 | id:1505.00171 author:Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, Roberto Cipolla category:cs.CV  published:2015-05-01 summary:We are interested in automatic scene understanding from geometric cues. To this end, we aim to bring semantic segmentation in the loop of real-time reconstruction. Our semantic segmentation is built on a deep autoencoder stack trained exclusively on synthetic depth data generated from our novel 3D scene library, SynthCam3D. Importantly, our network is able to segment real world scenes without any noise modelling. We present encouraging preliminary results. version:1
arxiv-1505-00161 | Embedding Semantic Relations into Word Representations | http://arxiv.org/abs/1505.00161 | id:1505.00161 author:Danushka Bollegala, Takanori Maehara, Ken-ichi Kawarabayashi category:cs.CL  published:2015-05-01 summary:Learning representations for semantic relations is important for various tasks such as analogy detection, relational search, and relation classification. Although there have been several proposals for learning representations for individual words, learning word representations that explicitly capture the semantic relations between words remains under developed. We propose an unsupervised method for learning vector representations for words such that the learnt representations are sensitive to the semantic relations that exist between two words. First, we extract lexical patterns from the co-occurrence contexts of two words in a corpus to represent the semantic relations that exist between those two words. Second, we represent a lexical pattern as the weighted sum of the representations of the words that co-occur with that lexical pattern. Third, we train a binary classifier to detect relationally similar vs. non-similar lexical pattern pairs. The proposed method is unsupervised in the sense that the lexical pattern pairs we use as train data are automatically sampled from a corpus, without requiring any manual intervention. Our proposed method statistically significantly outperforms the current state-of-the-art word representations on three benchmark datasets for proportional analogy detection, demonstrating its ability to accurately capture the semantic relations among words. version:1
arxiv-1505-00146 | Thompson Sampling for Budgeted Multi-armed Bandits | http://arxiv.org/abs/1505.00146 | id:1505.00146 author:Yingce Xia, Haifang Li, Tao Qin, Nenghai Yu, Tie-Yan Liu category:cs.LG  published:2015-05-01 summary:Thompson sampling is one of the earliest randomized algorithms for multi-armed bandits (MAB). In this paper, we extend the Thompson sampling to Budgeted MAB, where there is random cost for pulling an arm and the total cost is constrained by a budget. We start with the case of Bernoulli bandits, in which the random rewards (costs) of an arm are independently sampled from a Bernoulli distribution. To implement the Thompson sampling algorithm in this case, at each round, we sample two numbers from the posterior distributions of the reward and cost for each arm, obtain their ratio, select the arm with the maximum ratio, and then update the posterior distributions. We prove that the distribution-dependent regret bound of this algorithm is $O(\ln B)$, where $B$ denotes the budget. By introducing a Bernoulli trial, we further extend this algorithm to the setting that the rewards (costs) are drawn from general distributions, and prove that its regret bound remains almost the same. Our simulation results demonstrate the effectiveness of the proposed algorithm. version:1
arxiv-1505-00145 | Quality Control in Crowdsourced Object Segmentation | http://arxiv.org/abs/1505.00145 | id:1505.00145 author:Ferran Cabezas, Axel Carlier, Amaia Salvador, Xavier Giró-i-Nieto, Vincent Charvillat category:cs.CV cs.HC  published:2015-05-01 summary:This paper explores processing techniques to deal with noisy data in crowdsourced object segmentation tasks. We use the data collected with "Click'n'Cut", an online interactive segmentation tool, and we perform several experiments towards improving the segmentation results. First, we introduce different superpixel-based techniques to filter users' traces, and assess their impact on the segmentation result. Second, we present different criteria to detect and discard the traces from potential bad users, resulting in a remarkable increase in performance. Finally, we show a novel superpixel-based segmentation algorithm which does not require any prior filtering and is based on weighting each user's contribution according to his/her level of expertise. version:1
arxiv-1412-6623 | Word Representations via Gaussian Embedding | http://arxiv.org/abs/1412.6623 | id:1412.6623 author:Luke Vilnis, Andrew McCallum category:cs.CL cs.LG  published:2014-12-20 summary:Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation. version:4
arxiv-1505-00138 | Compositional Distributional Semantics with Compact Closed Categories and Frobenius Algebras | http://arxiv.org/abs/1505.00138 | id:1505.00138 author:Dimitri Kartsaklis category:cs.CL cs.AI math.CT math.QA quant-ph  published:2015-05-01 summary:This thesis contributes to ongoing research related to the categorical compositional model for natural language of Coecke, Sadrzadeh and Clark in three ways: Firstly, I propose a concrete instantiation of the abstract framework based on Frobenius algebras (joint work with Sadrzadeh). The theory improves shortcomings of previous proposals, extends the coverage of the language, and is supported by experimental work that improves existing results. The proposed framework describes a new class of compositional models that find intuitive interpretations for a number of linguistic phenomena. Secondly, I propose and evaluate in practice a new compositional methodology which explicitly deals with the different levels of lexical ambiguity (joint work with Pulman). A concrete algorithm is presented, based on the separation of vector disambiguation from composition in an explicit prior step. Extensive experimental work shows that the proposed methodology indeed results in more accurate composite representations for the framework of Coecke et al. in particular and every other class of compositional models in general. As a last contribution, I formalize the explicit treatment of lexical ambiguity in the context of the categorical framework by resorting to categorical quantum mechanics (joint work with Coecke). In the proposed extension, the concept of a distributional vector is replaced with that of a density matrix, which compactly represents a probability distribution over the potential different meanings of the specific word. Composition takes the form of quantum measurements, leading to interesting analogies between quantum physics and linguistics. version:1
arxiv-1505-00110 | The Cross-Depiction Problem: Computer Vision Algorithms for Recognising Objects in Artwork and in Photographs | http://arxiv.org/abs/1505.00110 | id:1505.00110 author:Hongping Cai, Qi Wu, Tadeo Corradi, Peter Hall category:cs.CV 68745 I.2.10  published:2015-05-01 summary:The cross-depiction problem is that of recognising visual objects regardless of whether they are photographed, painted, drawn, etc. It is a potentially significant yet under-researched problem. Emulating the remarkable human ability to recognise objects in an astonishingly wide variety of depictive forms is likely to advance both the foundations and the applications of Computer Vision. In this paper we benchmark classification, domain adaptation, and deep learning methods; demonstrating that none perform consistently well in the cross-depiction problem. Given the current interest in deep learning, the fact such methods exhibit the same behaviour as all but one other method: they show a significant fall in performance over inhomogeneous databases compared to their peak performance, which is always over data comprising photographs only. Rather, we find the methods that have strong models of spatial relations between parts tend to be more robust and therefore conclude that such information is important in modelling object classes regardless of appearance details. version:1
arxiv-1505-00075 | A Cooperative Framework for Fireworks Algorithm | http://arxiv.org/abs/1505.00075 | id:1505.00075 author:Shaoqiu Zheng, Junzhi Li, Andreas Janecek, Ying Tan category:cs.NE  published:2015-05-01 summary:This paper presents a cooperative framework for fireworks algorithm (CoFFWA). A detailed analysis of existing fireworks algorithm (FWA) and its recently developed variants has revealed that (i) the selection strategy lead to the contribution of the firework with the best fitness (core firework) for the optimization overwhelms the contributions of the rest of fireworks (non-core fireworks) in the explosion operator, (ii) the Gaussian mutation operator is not as effective as it is designed to be. To overcome these limitations, the CoFFWA is proposed, which can greatly enhance the exploitation ability of non-core fireworks by using independent selection operator and increase the exploration capacity by crowdness-avoiding cooperative strategy among the fireworks. Experimental results on the CEC2013 benchmark functions suggest that CoFFWA outperforms the state-of-the-art FWA variants, artificial bee colony, differential evolution, the standard particle swarm optimization (SPSO) in 2007 and the most recent SPSO in 2011 in term of convergence performance. version:1
arxiv-1505-00040 | Overlapping and Non-overlapping Camera Layouts for Robot Pose Estimation | http://arxiv.org/abs/1505.00040 | id:1505.00040 author:Mohammad Ehab Ragab category:cs.CV  published:2015-04-30 summary:We study the use of overlapping and non-overlapping camera layouts in estimating the ego-motion of a moving robot. To estimate the location and orientation of the robot, we investigate using four cameras as non-overlapping individuals, and as two stereo pairs. The pros and cons of the two approaches are elucidated. The cameras work independently and can have larger field of view in the non-overlapping layout. However, a scale factor ambiguity should be dealt with. On the other hand, stereo systems provide more accuracy but require establishing feature correspondence with more computational demand. For both approaches, the extended Kalman filter is used as a real-time recursive estimator. The approaches studied are verified with synthetic and real experiments alike. version:1
arxiv-1504-07907 | A Flexible Tensor Block Coordinate Ascent Scheme for Hypergraph Matching | http://arxiv.org/abs/1504.07907 | id:1504.07907 author:Quynh Nguyen, Antoine Gautier, Matthias Hein category:cs.CV  published:2015-04-29 summary:The estimation of correspondences between two images resp. point sets is a core problem in computer vision. One way to formulate the problem is graph matching leading to the quadratic assignment problem which is NP-hard. Several so called second order methods have been proposed to solve this problem. In recent years hypergraph matching leading to a third order problem became popular as it allows for better integration of geometric information. For most of these third order algorithms no theoretical guarantees are known. In this paper we propose a general framework for tensor block coordinate ascent methods for hypergraph matching. We propose two algorithms which both come along with the guarantee of monotonic ascent in the matching score on the set of discrete assignment matrices. In the experiments we show that our new algorithms outperform previous work both in terms of achieving better matching scores and matching accuracy. This holds in particular for very challenging settings where one has a high number of outliers and other forms of noise. version:2
arxiv-1403-6095 | Simultaneous sparse estimation of canonical vectors in the p>>N setting | http://arxiv.org/abs/1403.6095 | id:1403.6095 author:Irina Gaynanova, James G. Booth, Martin T. Wells category:stat.ME stat.ML  published:2014-03-24 summary:This article considers the problem of sparse estimation of canonical vectors in linear discriminant analysis when $p\gg N$. Several methods have been proposed in the literature that estimate one canonical vector in the two-group case. However, $G-1$ canonical vectors can be considered if the number of groups is $G$. In the multi-group context, it is common to estimate canonical vectors in a sequential fashion. Moreover, separate prior estimation of the covariance structure is often required. We propose a novel methodology for direct estimation of canonical vectors. In contrast to existing techniques, the proposed method estimates all canonical vectors at once, performs variable selection across all the vectors and comes with theoretical guarantees on the variable selection and classification consistency. First, we highlight the fact that in the $N>p$ setting the canonical vectors can be expressed in a closed form up to an orthogonal transformation. Secondly, we propose an extension of this form to the $p\gg N$ setting and achieve feature selection by using a group penalty. The resulting optimization problem is convex and can be solved using a block-coordinate descent algorithm. The practical performance of the method is evaluated through simulation studies as well as real data applications. version:4
arxiv-1501-01239 | On the Relationship between Sum-Product Networks and Bayesian Networks | http://arxiv.org/abs/1501.01239 | id:1501.01239 author:Han Zhao, Mazen Melibari, Pascal Poupart category:cs.AI stat.ML  published:2015-01-06 summary:In this paper, we establish some theoretical connections between Sum-Product Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be converted into a BN in linear time and space in terms of the network size. The key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent the local conditional probability distributions at each node in the resulting BN by exploiting context-specific independence (CSI). The generated BN has a simple directed bipartite graphical structure. We show that by applying the Variable Elimination algorithm (VE) to the generated BN with ADD representations, we can recover the original SPN where the SPN can be viewed as a history record or caching of the VE inference process. To help state the proof clearly, we introduce the notion of {\em normal} SPN and present a theoretical analysis of the consistency and decomposability properties. We conclude the paper with some discussion of the implications of the proof and establish a connection between the depth of an SPN and a lower bound of the tree-width of its corresponding BN. version:2
arxiv-1505-00192 | Application of S-Transform on Hyper kurtosis based Modified Duo Histogram Equalized DIC images for Pre-cancer Detection | http://arxiv.org/abs/1505.00192 | id:1505.00192 author:Sabyasachi Mukhopadhyay, Soham Mandal, Sawon Pratiher, Ritwik Barman, M. Venkatesh, Nirmalya Ghosh, Prasanta K. Panigrahi category:cs.CV  published:2015-04-30 summary:Our proposed hyper kurtosis based histogram equalized DIC images enhances the contrast by preserving the brightness. The evolution and development of precancerous activity among tissues are studied through S-transform (ST). The significant variations of amplitude spectra can be observed due to increased medium roughness from normal tissue were observed in time-frequency domain. The randomness and inhomogeneity of the tissue structures among human normal and different grades of DIC tissues is recognized by ST based timefrequency analysis. This study offers a simpler and better way to recognize the substantial changes among different stages of DIC tissues, which are reflected by spatial information containing within the inhomogeneity structures of different types of tissue. version:1
arxiv-1504-08219 | Hierarchical Subquery Evaluation for Active Learning on a Graph | http://arxiv.org/abs/1504.08219 | id:1504.08219 author:Oisin Mac Aodha, Neill D. F. Campbell, Jan Kautz, Gabriel J. Brostow category:cs.CV cs.LG stat.ML  published:2015-04-30 summary:To train good supervised and semi-supervised object classifiers, it is critical that we not waste the time of the human experts who are providing the training labels. Existing active learning strategies can have uneven performance, being efficient on some datasets but wasteful on others, or inconsistent just between runs on the same dataset. We propose perplexity based graph construction and a new hierarchical subquery evaluation algorithm to combat this variability, and to release the potential of Expected Error Reduction. Under some specific circumstances, Expected Error Reduction has been one of the strongest-performing informativeness criteria for active learning. Until now, it has also been prohibitively costly to compute for sizeable datasets. We demonstrate our highly practical algorithm, comparing it to other active learning measures on classification datasets that vary in sparsity, dimensionality, and size. Our algorithm is consistent over multiple runs and achieves high accuracy, while querying the human expert for labels at a frequency that matches their desired time budget. version:1
arxiv-1504-08215 | Lateral Connections in Denoising Autoencoders Support Supervised Learning | http://arxiv.org/abs/1504.08215 | id:1504.08215 author:Antti Rasmus, Harri Valpola, Tapani Raiko category:cs.LG cs.NE stat.ML  published:2015-04-30 summary:We show how a deep denoising autoencoder with lateral connections can be used as an auxiliary unsupervised learning task to support supervised learning. The proposed model is trained to minimize simultaneously the sum of supervised and unsupervised cost functions by back-propagation, avoiding the need for layer-wise pretraining. It improves the state of the art significantly in the permutation-invariant MNIST classification task. version:1
arxiv-1504-08183 | Texts in, meaning out: neural language models in semantic similarity task for Russian | http://arxiv.org/abs/1504.08183 | id:1504.08183 author:Andrey Kutuzov, Igor Andreev category:cs.CL  published:2015-04-30 summary:Distributed vector representations for natural language vocabulary get a lot of attention in contemporary computational linguistics. This paper summarizes the experience of applying neural network language models to the task of calculating semantic similarity for Russian. The experiments were performed in the course of Russian Semantic Similarity Evaluation track, where our models took from the 2nd to the 5th position, depending on the task. We introduce the tools and corpora used, comment on the nature of the shared task and describe the achieved results. It was found out that Continuous Skip-gram and Continuous Bag-of-words models, previously successfully applied to English material, can be used for semantic modeling of Russian as well. Moreover, we show that texts in Russian National Corpus (RNC) provide an excellent training material for such models, outperforming other, much larger corpora. It is especially true for semantic relatedness tasks (although stacking models trained on larger corpora on top of RNC models improves performance even more). High-quality semantic vectors learned in such a way can be used in a variety of linguistic tasks and promise an exciting field for further study. version:1
arxiv-1505-01065 | Proceedings of The 39th Annual Workshop of the Austrian Association for Pattern Recognition (OAGM), 2015 | http://arxiv.org/abs/1505.01065 | id:1505.01065 author:Sebastian Hegenbart, Roland Kwitt, Andreas Uhl category:cs.CV  published:2015-04-30 summary:The 39th annual workshop of the Austrian Association for Pattern Recognition (OAGM/AAPR) provides a platform for presentation and discussion of research progress as well as research projects within the OAGM/AAPR community. version:1
arxiv-1504-08102 | Detecting and ordering adjectival scalemates | http://arxiv.org/abs/1504.08102 | id:1504.08102 author:Emiel van Miltenburg category:cs.CL  published:2015-04-30 summary:This paper presents a pattern-based method that can be used to infer adjectival scales, such as <lukewarm, warm, hot>, from a corpus. Specifically, the proposed method uses lexical patterns to automatically identify and order pairs of scalemates, followed by a filtering phase in which unrelated pairs are discarded. For the filtering phase, several different similarity measures are implemented and compared. The model presented in this paper is evaluated using the current standard, along with a novel evaluation set, and shown to be at least as good as the current state-of-the-art. version:1
arxiv-1203-2995 | Marginal multi-Bernoulli filters: RFS derivation of MHT, JIPDA and association-based MeMBer | http://arxiv.org/abs/1203.2995 | id:1203.2995 author:Jason L. Williams category:cs.SY cs.CV  published:2012-03-14 summary:Recent developments in random finite sets (RFSs) have yielded a variety of tracking methods that avoid data association. This paper derives a form of the full Bayes RFS filter and observes that data association is implicitly present, in a data structure similar to MHT. Subsequently, algorithms are obtained by approximating the distribution of associations. Two algorithms result: one nearly identical to JIPDA, and another related to the MeMBer filter. Both improve performance in challenging environments. version:5
arxiv-1411-6880 | An Automated Images-to-Graphs Framework for High Resolution Connectomics | http://arxiv.org/abs/1411.6880 | id:1411.6880 author:William Gray Roncal, Dean M. Kleissas, Joshua T. Vogelstein, Priya Manavalan, Kunal Lillaney, Michael Pekala, Randal Burns, R. Jacob Vogelstein, Carey E. Priebe, Mark A. Chevillet, Gregory D. Hager category:q-bio.QM cs.CV  published:2014-11-25 summary:Reconstructing a map of neuronal connectivity is a critical challenge in contemporary neuroscience. Recent advances in high-throughput serial section electron microscopy (EM) have produced massive 3D image volumes of nanoscale brain tissue for the first time. The resolution of EM allows for individual neurons and their synaptic connections to be directly observed. Recovering neuronal networks by manually tracing each neuronal process at this scale is unmanageable, and therefore researchers are developing automated image processing modules. Thus far, state-of-the-art algorithms focus only on the solution to a particular task (e.g., neuron segmentation or synapse identification). In this manuscript we present the first fully automated images-to-graphs pipeline (i.e., a pipeline that begins with an imaged volume of neural tissue and produces a brain graph without any human interaction). To evaluate overall performance and select the best parameters and methods, we also develop a metric to assess the quality of the output graphs. We evaluate a set of algorithms and parameters, searching possible operating points to identify the best available brain graph for our assessment metric. Finally, we deploy a reference end-to-end version of the pipeline on a large, publicly available data set. This provides a baseline result and framework for community analysis and future algorithm development and testing. All code and data derivatives have been made publicly available toward eventually unlocking new biofidelic computational primitives and understanding of neuropathologies. version:2
arxiv-1412-4729 | Translating Videos to Natural Language Using Deep Recurrent Neural Networks | http://arxiv.org/abs/1412.4729 | id:1412.4729 author:Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko category:cs.CV cs.CL  published:2014-12-15 summary:Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation. version:3
arxiv-1504-05133 | Exploiting Local Features from Deep Networks for Image Retrieval | http://arxiv.org/abs/1504.05133 | id:1504.05133 author:Joe Yue-Hei Ng, Fan Yang, Larry S. Davis category:cs.CV  published:2015-04-20 summary:Deep convolutional neural networks have been successfully applied to image classification tasks. When these same networks have been applied to image retrieval, the assumption has been made that the last layers would give the best performance, as they do in classification. We show that for instance-level image retrieval, lower layers often perform better than the last layers in convolutional neural networks. We present an approach for extracting convolutional features from different layers of the networks, and adopt VLAD encoding to encode features into a single vector for each image. We investigate the effect of different layers and scales of input images on the performance of convolutional features using the recent deep networks OxfordNet and GoogLeNet. Experiments demonstrate that intermediate layers or higher layers with finer scales produce better results for image retrieval, compared to the last layer. When using compressed 128-D VLAD descriptors, our method obtains state-of-the-art results and outperforms other VLAD and CNN based approaches on two out of three test datasets. Our work provides guidance for transferring deep networks trained on image classification to image retrieval tasks. version:2
arxiv-1504-08050 | Detecting Concept-level Emotion Cause in Microblogging | http://arxiv.org/abs/1504.08050 | id:1504.08050 author:Shuangyong Song, Yao Meng category:cs.CL cs.AI 68P20 H.2.8  published:2015-04-30 summary:In this paper, we propose a Concept-level Emotion Cause Model (CECM), instead of the mere word-level models, to discover causes of microblogging users' diversified emotions on specific hot event. A modified topic-supervised biterm topic model is utilized in CECM to detect emotion topics' in event-related tweets, and then context-sensitive topical PageRank is utilized to detect meaningful multiword expressions as emotion causes. Experimental results on a dataset from Sina Weibo, one of the largest microblogging websites in China, show CECM can better detect emotion causes than baseline methods. version:1
arxiv-1407-7559 | Toward a multilevel representation of protein molecules: comparative approaches to the aggregation/folding propensity problem | http://arxiv.org/abs/1407.7559 | id:1407.7559 author:Lorenzo Livi, Alessandro Giuliani, Antonello Rizzi category:cs.CE cs.LG q-bio.BM q-bio.MN I.2.6; K.3.2  published:2014-07-28 summary:This paper builds upon the fundamental work of Niwa et al. [34], which provides the unique possibility to analyze the relative aggregation/folding propensity of the elements of the entire Escherichia coli (E. coli) proteome in a cell-free standardized microenvironment. The hardness of the problem comes from the superposition between the driving forces of intra- and inter-molecule interactions and it is mirrored by the evidences of shift from folding to aggregation phenotypes by single-point mutations [10]. Here we apply several state-of-the-art classification methods coming from the field of structural pattern recognition, with the aim to compare different representations of the same proteins gathered from the Niwa et al. data base; such representations include sequences and labeled (contact) graphs enriched with chemico-physical attributes. By this comparison, we are able to identify also some interesting general properties of proteins. Notably, (i) we suggest a threshold around 250 residues discriminating "easily foldable" from "hardly foldable" molecules consistent with other independent experiments, and (ii) we highlight the relevance of contact graph spectra for folding behavior discrimination and characterization of the E. coli solubility data. The soundness of the experimental results presented in this paper is proved by the statistically relevant relationships discovered among the chemico-physical description of proteins and the developed cost matrix of substitution used in the various discrimination systems. version:3
arxiv-1504-08025 | Technical Note on Equivalence Between Recurrent Neural Network Time Series Models and Variational Bayesian Models | http://arxiv.org/abs/1504.08025 | id:1504.08025 author:Jascha Sohl-Dickstein, Diederik P. Kingma category:cs.LG  published:2015-04-29 summary:We observe that the standard log likelihood training objective for a Recurrent Neural Network (RNN) model of time series data is equivalent to a variational Bayesian training objective, given the proper choice of generative and inference models. This perspective may motivate extensions to both RNNs and variational Bayesian models. We propose one such extension, where multiple particles are used for the hidden state of an RNN, allowing a natural representation of uncertainty or multimodality. version:1
arxiv-1504-08023 | Anticipating the future by watching unlabeled video | http://arxiv.org/abs/1504.08023 | id:1504.08023 author:Carl Vondrick, Hamed Pirsiavash, Antonio Torralba category:cs.CV  published:2015-04-29 summary:In many computer vision applications, machines will need to reason beyond the present, and predict the future. This task is challenging because it requires leveraging extensive commonsense knowledge of the world that is difficult to write down. We believe that a promising resource for efficiently obtaining this knowledge is through the massive amounts of readily available unlabeled video. In this paper, we present a large scale framework that capitalizes on temporal structure in unlabeled video to learn to anticipate both actions and objects in the future. The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future. We experimentally validate this idea on two challenging "in the wild" video datasets, and our results suggest that learning with unlabeled videos significantly helps forecast actions and anticipate objects. version:1
arxiv-1504-08022 | A Deep Learning Model for Structured Outputs with High-order Interaction | http://arxiv.org/abs/1504.08022 | id:1504.08022 author:Hongyu Guo, Xiaodan Zhu, Martin Renqiang Min category:cs.LG cs.NE  published:2015-04-29 summary:Many real-world applications are associated with structured data, where not only input but also output has interplay. However, typical classification and regression models often lack the ability of simultaneously exploring high-order interaction within input and that within output. In this paper, we present a deep learning model aiming to generate a powerful nonlinear functional mapping from structured input to structured output. More specifically, we propose to integrate high-order hidden units, guided discriminative pretraining, and high-order auto-encoders for this purpose. We evaluate the model with three datasets, and obtain state-of-the-art performances among competitive methods. Our current work focuses on structured output regression, which is a less explored area, although the model can be extended to handle structured label classification. version:1
arxiv-1504-08021 | Who Spoke What? A Latent Variable Framework for the Joint Decoding of Multiple Speakers and their Keywords | http://arxiv.org/abs/1504.08021 | id:1504.08021 author:Harshavardhan Sundar, Thippur V. Sreenivas category:cs.SD cs.LG  published:2015-04-29 summary:In this paper, we present a latent variable (LV) framework to identify all the speakers and their keywords given a multi-speaker mixture signal. We introduce two separate LVs to denote active speakers and the keywords uttered. The dependency of a spoken keyword on the speaker is modeled through a conditional probability mass function. The distribution of the mixture signal is expressed in terms of the LV mass functions and speaker-specific-keyword models. The proposed framework admits stochastic models, representing the probability density function of the observation vectors given that a particular speaker uttered a specific keyword, as speaker-specific-keyword models. The LV mass functions are estimated in a Maximum Likelihood framework using the Expectation Maximization (EM) algorithm. The active speakers and their keywords are detected as modes of the joint distribution of the two LVs. In mixture signals, containing two speakers uttering the keywords simultaneously, the proposed framework achieves an accuracy of 82% for detecting both the speakers and their respective keywords, using Student's-t mixture models as speaker-specific-keyword models. version:1
arxiv-1504-07967 | Improved repeatability measures for evaluating performance of feature detectors | http://arxiv.org/abs/1504.07967 | id:1504.07967 author:Shoaib Ehsan, Nadia Kanwal, Adrian F. Clark, Klaus D. McDonald-Maier category:cs.CV cs.PF  published:2015-04-29 summary:The most frequently employed measure for performance characterisation of local feature detectors is repeatability, but it has been observed that this does not necessarily mirror actual performance. Presented are improved repeatability formulations which correlate much better with the true performance of feature detectors. Comparative results for several state-of-the-art feature detectors are presented using these measures; it is found that Hessian-based detectors are generally superior at identifying features when images are subject to various geometric and photometric transformations. version:1
arxiv-1504-07962 | Hardware based Scale- and Rotation-Invariant Feature Extraction: A Retrospective Analysis and Future Directions | http://arxiv.org/abs/1504.07962 | id:1504.07962 author:Shoaib Ehsan, Adrian F. Clark, Klaus D. McDonald-Maier category:cs.CV  published:2015-04-29 summary:Computer Vision techniques represent a class of algorithms that are highly computation and data intensive in nature. Generally, performance of these algorithms in terms of execution speed on desktop computers is far from real-time. Since real-time performance is desirable in many applications, special-purpose hardware is required in most cases to achieve this goal. Scale- and rotation-invariant local feature extraction is a low level computer vision task with very high computational complexity. The state-of-the-art algorithms that currently exist in this domain, like SIFT and SURF, suffer from slow execution speeds and at best can only achieve rates of 2-3 Hz on modern desktop computers. Hardware-based scale- and rotation-invariant local feature extraction is an emerging trend enabling real-time performance for these computationally complex algorithms. This paper takes a retrospective look at the advances made so far in this field, discusses the hardware design strategies employed and results achieved, identifies current research gaps and suggests future research directions. version:1
arxiv-1504-07958 | Exploring Integral Image Word Length Reduction Techniques for SURF Detector | http://arxiv.org/abs/1504.07958 | id:1504.07958 author:Shoaib Ehsan, Klaus D. McDonald-Maier category:cs.CV  published:2015-04-29 summary:Speeded Up Robust Features (SURF) is a state of the art computer vision algorithm that relies on integral image representation for performing fast detection and description of image features that are scale and rotation invariant. Integral image representation, however, has major draw back of large binary word length that leads to substantial increase in memory size. When designing a dedicated hardware to achieve real-time performance for the SURF algorithm, it is imperative to consider the adverse effects of integral image on memory size, bus width and computational resources. With the objective of minimizing hardware resources, this paper presents a novel implementation concept of a reduced word length integral image based SURF detector. It evaluates two existing word length reduction techniques for the particular case of SURF detector and extends one of these to achieve more reduction in word length. This paper also introduces a novel method to achieve integral image word length reduction for SURF detector. version:1
arxiv-1411-5086 | Designing Optimal Mortality Risk Prediction Scores that Preserve Clinical Knowledge | http://arxiv.org/abs/1411.5086 | id:1411.5086 author:Natalia M. Arzeno, Karla A. Lawson, Sarah V. Duzinski, Haris Vikalo category:stat.ML stat.AP  published:2014-11-19 summary:Many in-hospital mortality risk prediction scores dichotomize predictive variables to simplify the score calculation. However, hard thresholding in these additive stepwise scores of the form "add x points if variable v is above/below threshold t" may lead to critical failures. In this paper, we seek to develop risk prediction scores that preserve clinical knowledge embedded in features and structure of the existing additive stepwise scores while addressing limitations caused by variable dichotomization. To this end, we propose a novel score structure that relies on a transformation of predictive variables by means of nonlinear logistic functions facilitating smooth differentiation between critical and normal values of the variables. We develop an optimization framework for inferring parameters of the logistic functions for a given patient population via cyclic block coordinate descent. The parameters may readily be updated as the patient population and standards of care evolve. We tested the proposed methodology on two populations: (1) brain trauma patients admitted to the intensive care unit of the Dell Children's Medical Center of Central Texas between 2007 and 2012, and (2) adult ICU patient data from the MIMIC II database. The results are compared with those obtained by the widely used PRISM III and SOFA scores. The prediction power of a score is evaluated using area under ROC curve, Youden's index, and precision-recall balance in a cross-validation study. The results demonstrate that the new framework enables significant performance improvements over PRISM III and SOFA in terms of all three criteria. version:2
arxiv-1504-07918 | Robust hyperspectral image classification with rejection fields | http://arxiv.org/abs/1504.07918 | id:1504.07918 author:Filipe Condessa, Jose Bioucas-Dias, Jelena Kovacevic category:cs.CV 68  published:2015-04-29 summary:In this paper we present a novel method for robust hyperspectral image classification using context and rejection. Hyperspectral image classification is generally an ill-posed image problem where pixels may belong to unknown classes, and obtaining representative and complete training sets is costly. Furthermore, the need for high classification accuracies is frequently greater than the need to classify the entire image. We approach this problem with a robust classification method that combines classification with context with classification with rejection. A rejection field that will guide the rejection is derived from the classification with contextual information obtained by using the SegSALSA algorithm. We validate our method in real hyperspectral data and show that the performance gains obtained from the rejection fields are equivalent to an increase the dimension of the training sets. version:1
arxiv-1504-07901 | Comparative study of image registration techniques for bladder video-endoscopy | http://arxiv.org/abs/1504.07901 | id:1504.07901 author:Achraf Ben-Hamadou, Charles Soussen, Walter Blondel, Christian Daul, Didier Wolf category:cs.CV  published:2015-04-29 summary:Bladder cancer is widely spread in the world. Many adequate diagnosis techniques exist. Video-endoscopy remains the standard clinical procedure for visual exploration of the bladder internal surface. However, video-endoscopy presents the limit that the imaged area for each image is about nearly 1cm2. And, lesions are, typically, spread over several images. The aim of this contribution is to assess the performance of two mosaicing algorithms leading to the construction of panoramic maps (one unique image) of bladder walls. The quantitative comparison study is performed on a set of real endoscopic exam data and on simulated data relative to bladder phantom. version:1
arxiv-1504-07874 | Visual Information Retrieval in Endoscopic Video Archives | http://arxiv.org/abs/1504.07874 | id:1504.07874 author:Jennifer Roldan-Carlos, Mathias Lux, Xavier Giró-i-Nieto, Pia Muñoz, Nektarios Anagnostopoulos category:cs.IR cs.CV cs.MM  published:2015-04-29 summary:In endoscopic procedures, surgeons work with live video streams from the inside of their subjects. A main source for documentation of procedures are still frames from the video, identified and taken during the surgery. However, with growing demands and technical means, the streams are saved to storage servers and the surgeons need to retrieve parts of the videos on demand. In this submission we present a demo application allowing for video retrieval based on visual features and late fusion, which allows surgeons to re-find shots taken during the procedure. version:1
arxiv-1504-07865 | ASTROMLSKIT: A New Statistical Machine Learning Toolkit: A Platform for Data Analytics in Astronomy | http://arxiv.org/abs/1504.07865 | id:1504.07865 author:Snehanshu Saha, Surbhi Agrawal, Manikandan. R, Kakoli Bora, Swati Routh, Anand Narasimhamurthy category:cs.CE astro-ph.IM cs.LG  published:2015-04-29 summary:Astroinformatics is a new impact area in the world of astronomy, occasionally called the final frontier, where several astrophysicists, statisticians and computer scientists work together to tackle various data intensive astronomical problems. Exponential growth in the data volume and increased complexity of the data augments difficult questions to the existing challenges. Classical problems in Astronomy are compounded by accumulation of astronomical volume of complex data, rendering the task of classification and interpretation incredibly laborious. The presence of noise in the data makes analysis and interpretation even more arduous. Machine learning algorithms and data analytic techniques provide the right platform for the challenges posed by these problems. A diverse range of open problem like star-galaxy separation, detection and classification of exoplanets, classification of supernovae is discussed. The focus of the paper is the applicability and efficacy of various machine learning algorithms like K Nearest Neighbor (KNN), random forest (RF), decision tree (DT), Support Vector Machine (SVM), Na\"ive Bayes and Linear Discriminant Analysis (LDA) in analysis and inference of the decision theoretic problems in Astronomy. The machine learning algorithms, integrated into ASTROMLSKIT, a toolkit developed in the course of the work, have been used to analyze HabCat data and supernovae data. Accuracy has been found to be appreciably good. version:1
arxiv-1504-07858 | Intelligent Health Recommendation System for Computer Users | http://arxiv.org/abs/1504.07858 | id:1504.07858 author:Qi Guo, Zixuan Wang, Ming Li, Hamid Aghajan category:cs.CV  published:2015-04-29 summary:The time people spend in front of computers has been increasing steadily due to the role computers play in modern society. Individuals who sit in front of computers for an extended period of time, specifically with improper postures may incur various health issues. In this work, individuals' behaviors in front of computers are studied using web cameras. By means of non-rigid face tracking system, data are analyzed to determine the 3D head pose, blink rate and yawn frequency of computer users. When combining these visual cues, a system of intelligent personal assistants for computer users is proposed. version:1
arxiv-1504-07843 | On the universal structure of human lexical semantics | http://arxiv.org/abs/1504.07843 | id:1504.07843 author:Hyejin Youn, Logan Sutton, Eric Smith, Cristopher Moore, Jon F. Wilkins, Ian Maddieson, William Croft, Tanmoy Bhattacharya category:physics.soc-ph cs.CL  published:2015-04-29 summary:How universal is human conceptual structure? The way concepts are organized in the human brain may reflect distinct features of cultural, historical, and environmental background in addition to properties universal to human cognition. Semantics, or meaning expressed through language, provides direct access to the underlying conceptual structure, but meaning is notoriously difficult to measure, let alone parameterize. Here we provide an empirical measure of semantic proximity between concepts using cross-linguistic dictionaries. Across languages carefully selected from a phylogenetically and geographically stratified sample of genera, translations of words reveal cases where a particular language uses a single polysemous word to express concepts represented by distinct words in another. We use the frequency of polysemies linking two concepts as a measure of their semantic proximity, and represent the pattern of such linkages by a weighted network. This network is highly uneven and fragmented: certain concepts are far more prone to polysemy than others, and there emerge naturally interpretable clusters loosely connected to each other. Statistical analysis shows such structural properties are consistent across different language groups, largely independent of geography, environment, and literacy. It is therefore possible to conclude the conceptual structure connecting basic vocabulary studied is primarily due to universal features of human cognition and language use. version:1
arxiv-1501-00901 | Learning to Recognize Pedestrian Attribute | http://arxiv.org/abs/1501.00901 | id:1501.00901 author:Yubin Deng, Ping Luo, Chen Change Loy, Xiaoou Tang category:cs.CV  published:2015-01-05 summary:Learning to recognize pedestrian attributes at far distance is a challenging problem in visual surveillance since face and body close-shots are hardly available; instead, only far-view image frames of pedestrian are given. In this study, we present an alternative approach that exploits the context of neighboring pedestrian images for improved attribute inference compared to the conventional SVM-based method. In addition, we conduct extensive experiments to evaluate the informativeness of background and foreground features for attribute recognition. Experiments are based on our newly released pedestrian attribute dataset, which is by far the largest and most diverse of its kind. version:2
arxiv-1504-07720 | Dual Averaging on Compactly-Supported Distributions And Application to No-Regret Learning on a Continuum | http://arxiv.org/abs/1504.07720 | id:1504.07720 author:Walid Krichene category:cs.LG math.OC  published:2015-04-29 summary:We consider an online learning problem on a continuum. A decision maker is given a compact feasible set $S$, and is faced with the following sequential problem: at iteration~$t$, the decision maker chooses a distribution $x^{(t)} \in \Delta(S)$, then a loss function $\ell^{(t)} : S \to \mathbb{R}_+$ is revealed, and the decision maker incurs expected loss $\langle \ell^{(t)}, x^{(t)} \rangle = \mathbb{E}_{s \sim x^{(t)}} \ell^{(t)}(s)$. We view the problem as an online convex optimization problem on the space $\Delta(S)$ of Lebesgue-continnuous distributions on $S$. We prove a general regret bound for the Dual Averaging method on $L^2(S)$, then prove that dual averaging with $\omega$-potentials (a class of strongly convex regularizers) achieves sublinear regret when $S$ is uniformly fat (a condition weaker than convexity). version:1
arxiv-1504-01777 | Heterogeneous Tensor Decomposition for Clustering via Manifold Optimization | http://arxiv.org/abs/1504.01777 | id:1504.01777 author:Yanfeng Sun, Junbin Gao, Xia Hong, Bamdev Mishra, Baocai Yin category:cs.CV  published:2015-04-07 summary:Tensors or multiarray data are generalizations of matrices. Tensor clustering has become a very important research topic due to the intrinsically rich structures in real-world multiarray datasets. Subspace clustering based on vectorizing multiarray data has been extensively researched. However, vectorization of tensorial data does not exploit complete structure information. In this paper, we propose a subspace clustering algorithm without adopting any vectorization process. Our approach is based on a novel heterogeneous Tucker decomposition model. In contrast to existing techniques, we propose a new clustering algorithm that alternates between different modes of the proposed heterogeneous tensor model. All but the last mode have closed-form updates. Updating the last mode reduces to optimizing over the so-called multinomial manifold, for which we investigate second order Riemannian geometry and propose a trust-region algorithm. Numerical experiments show that our proposed algorithm compete effectively with state-of-the-art clustering algorithms that are based on tensor factorization. version:2
arxiv-1411-4491 | Joint cross-domain classification and subspace learning for unsupervised adaptation | http://arxiv.org/abs/1411.4491 | id:1411.4491 author:Basura Fernando, Tatiana Tommasi, Tinne Tuytelaars category:cs.CV cs.LG  published:2014-11-17 summary:Domain adaptation aims at adapting the knowledge acquired on a source domain to a new different but related target domain. Several approaches have beenproposed for classification tasks in the unsupervised scenario, where no labeled target data are available. Most of the attention has been dedicated to searching a new domain-invariant representation, leaving the definition of the prediction function to a second stage. Here we propose to learn both jointly. Specifically we learn the source subspace that best matches the target subspace while at the same time minimizing a regularized misclassification loss. We provide an alternating optimization technique based on stochastic sub-gradient descent to solve the learning problem and we demonstrate its performance on several domain adaptation tasks. version:3
arxiv-1504-07678 | Leveraging Deep Neural Networks and Knowledge Graphs for Entity Disambiguation | http://arxiv.org/abs/1504.07678 | id:1504.07678 author:Hongzhao Huang, Larry Heck, Heng Ji category:cs.CL  published:2015-04-28 summary:Entity Disambiguation aims to link mentions of ambiguous entities to a knowledge base (e.g., Wikipedia). Modeling topical coherence is crucial for this task based on the assumption that information from the same semantic context tends to belong to the same topic. This paper presents a novel deep semantic relatedness model (DSRM) based on deep neural networks (DNN) and semantic knowledge graphs (KGs) to measure entity semantic relatedness for topical coherence modeling. The DSRM is directly trained on large-scale KGs and it maps heterogeneous types of knowledge of an entity from KGs to numerical feature vectors in a latent space such that the distance between two semantically-related entities is minimized. Compared with the state-of-the-art relatedness approach proposed by (Milne and Witten, 2008a), the DSRM obtains 19.4% and 24.5% reductions in entity disambiguation errors on two publicly available datasets respectively. version:1
arxiv-1504-07676 | Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers | http://arxiv.org/abs/1504.07676 | id:1504.07676 author:Abraham J. Wyner, Matthew Olson, Justin Bleich, David Mease category:stat.ML cs.LG stat.ME  published:2015-04-28 summary:There is a large literature explaining why AdaBoost is a successful classifier. The literature on AdaBoost focuses on classifier margins and boosting's interpretation as the optimization of an exponential likelihood function. These existing explanations, however, have been pointed out to be incomplete. A random forest is another popular ensemble method for which there is substantially less explanation in the literature. We introduce a novel perspective on AdaBoost and random forests that proposes that the two algorithms work for similar reasons. While both classifiers achieve similar predictive accuracy, random forests cannot be conceived as a direct optimization procedure. Rather, random forests is a self-averaging, interpolating algorithm which creates what we denote as a "spikey-smooth" classifier, and we view AdaBoost in the same light. We conjecture that both AdaBoost and random forests succeed because of this mechanism. We provide a number of examples and some theoretical justification to support this explanation. In the process, we question the conventional wisdom that suggests that boosting algorithms for classification require regularization or early stopping and should be limited to low complexity classes of learners, such as decision stumps. We conclude that boosting should be used like random forests: with large decision trees and without direct regularization or early stopping. version:1
arxiv-1504-07662 | Evaluation of Explore-Exploit Policies in Multi-result Ranking Systems | http://arxiv.org/abs/1504.07662 | id:1504.07662 author:Dragomir Yankov, Pavel Berkhin, Lihong Li category:cs.LG  published:2015-04-28 summary:We analyze the problem of using Explore-Exploit techniques to improve precision in multi-result ranking systems such as web search, query autocompletion and news recommendation. Adopting an exploration policy directly online, without understanding its impact on the production system, may have unwanted consequences - the system may sustain large losses, create user dissatisfaction, or collect exploration data which does not help improve ranking quality. An offline framework is thus necessary to let us decide what policy and how we should apply in a production environment to ensure positive outcome. Here, we describe such an offline framework. Using the framework, we study a popular exploration policy - Thompson sampling. We show that there are different ways of implementing it in multi-result ranking systems, each having different semantic interpretation and leading to different results in terms of sustained click-through-rate (CTR) loss and expected model improvement. In particular, we demonstrate that Thompson sampling can act as an online learner optimizing CTR, which in some cases can lead to an interesting outcome: lift in CTR during exploration. The observation is important for production systems as it suggests that one can get both valuable exploration data to improve ranking performance on the long run, and at the same time increase CTR while exploration lasts. version:1
arxiv-1504-07648 | Nearly Optimal Deterministic Algorithm for Sparse Walsh-Hadamard Transform | http://arxiv.org/abs/1504.07648 | id:1504.07648 author:Mahdi Cheraghchi, Piotr Indyk category:cs.IT cs.CC cs.LG math.FA math.IT  published:2015-04-28 summary:For every fixed constant $\alpha > 0$, we design an algorithm for computing the $k$-sparse Walsh-Hadamard transform of an $N$-dimensional vector $x \in \mathbb{R}^N$ in time $k^{1+\alpha} (\log N)^{O(1)}$. Specifically, the algorithm is given query access to $x$ and computes a $k$-sparse $\tilde{x} \in \mathbb{R}^N$ satisfying $\ \tilde{x} - \hat{x}\ _1 \leq c \ \hat{x} - H_k(\hat{x})\ _1$, for an absolute constant $c > 0$, where $\hat{x}$ is the transform of $x$ and $H_k(\hat{x})$ is its best $k$-sparse approximation. Our algorithm is fully deterministic and only uses non-adaptive queries to $x$ (i.e., all queries are determined and performed in parallel when the algorithm starts). An important technical tool that we use is a construction of nearly optimal and linear lossless condensers which is a careful instantiation of the GUV condenser (Guruswami, Umans, Vadhan, JACM 2009). Moreover, we design a deterministic and non-adaptive $\ell_1/\ell_1$ compressed sensing scheme based on general lossless condensers that is equipped with a fast reconstruction algorithm running in time $k^{1+\alpha} (\log N)^{O(1)}$ (for the GUV-based condenser) and is of independent interest. Our scheme significantly simplifies and improves an earlier expander-based construction due to Berinde, Gilbert, Indyk, Karloff, Strauss (Allerton 2008). Our methods use linear lossless condensers in a black box fashion; therefore, any future improvement on explicit constructions of such condensers would immediately translate to improved parameters in our framework (potentially leading to $k (\log N)^{O(1)}$ reconstruction time with a reduced exponent in the poly-logarithmic factor, and eliminating the extra parameter $\alpha$). Finally, by allowing the algorithm to use randomness, while still using non-adaptive queries, the running time of the algorithm can be improved to $\tilde{O}(k \log^3 N)$. version:1
arxiv-1504-07643 | A novel variational model for image registration using Gaussian curvature | http://arxiv.org/abs/1504.07643 | id:1504.07643 author:Mazlinda Ibrahim, Ke Chen, Carlos Brito-Loeza category:math.NA cs.CV 65F10  68U10  62H35  published:2015-04-28 summary:Image registration is one important task in many image processing applications. It aims to align two or more images so that useful information can be extracted through comparison, combination or superposition. This is achieved by constructing an optimal trans- formation which ensures that the template image becomes similar to a given reference image. Although many models exist, designing a model capable of modelling large and smooth deformation field continues to pose a challenge. This paper proposes a novel variational model for image registration using the Gaussian curvature as a regulariser. The model is motivated by the surface restoration work in geometric processing [Elsey and Esedoglu, Multiscale Model. Simul., (2009), pp. 1549-1573]. An effective numerical solver is provided for the model using an augmented Lagrangian method. Numerical experiments can show that the new model outperforms three competing models based on, respectively, a linear curvature [Fischer and Modersitzki, J. Math. Imaging Vis., (2003), pp. 81- 85], the mean curvature [Chumchob, Chen and Brito, Multiscale Model. Simul., (2011), pp. 89-128] and the diffeomorphic demon model [Vercauteren at al., NeuroImage, (2009), pp. 61-72] in terms of robustness and accuracy. version:1
arxiv-1504-07614 | Or's of And's for Interpretable Classification, with Application to Context-Aware Recommender Systems | http://arxiv.org/abs/1504.07614 | id:1504.07614 author:Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, Perry MacNeille category:cs.LG  published:2015-04-28 summary:We present a machine learning algorithm for building classifiers that are comprised of a small number of disjunctions of conjunctions (or's of and's). An example of a classifier of this form is as follows: If X satisfies (x1 = 'blue' AND x3 = 'middle') OR (x1 = 'blue' AND x2 = '<15') OR (x1 = 'yellow'), then we predict that Y=1, ELSE predict Y=0. An attribute-value pair is called a literal and a conjunction of literals is called a pattern. Models of this form have the advantage of being interpretable to human experts, since they produce a set of conditions that concisely describe a specific class. We present two probabilistic models for forming a pattern set, one with a Beta-Binomial prior, and the other with Poisson priors. In both cases, there are prior parameters that the user can set to encourage the model to have a desired size and shape, to conform with a domain-specific definition of interpretability. We provide two scalable MAP inference approaches: a pattern level search, which involves association rule mining, and a literal level search. We show stronger priors reduce computation. We apply the Bayesian Or's of And's (BOA) model to predict user behavior with respect to in-vehicle context-aware personalized recommender systems. version:1
arxiv-1411-5057 | Fast Iteratively Reweighted Least Squares Algorithms for Analysis-Based Sparsity Reconstruction | http://arxiv.org/abs/1411.5057 | id:1411.5057 author:Chen Chen, Junzhou Huang, Lei He, Hongsheng Li category:cs.CV  published:2014-11-18 summary:In this paper, we propose a novel algorithm for analysis-based sparsity reconstruction. It can solve the generalized problem by structured sparsity regularization with an orthogonal basis and total variation regularization. The proposed algorithm is based on the iterative reweighted least squares (IRLS) model, which is further accelerated by the preconditioned conjugate gradient method. The convergence rate of the proposed algorithm is almost the same as that of the traditional IRLS algorithms, that is, exponentially fast. Moreover, with the specifically devised preconditioner, the computational cost for each iteration is significantly less than that of traditional IRLS algorithms, which enables our approach to handle large scale problems. In addition to the fast convergence, it is straightforward to apply our method to standard sparsity, group sparsity, overlapping group sparsity and TV based problems. Experiments are conducted on a practical application: compressive sensing magnetic resonance imaging. Extensive results demonstrate that the proposed algorithm achieves superior performance over 14 state-of-the-art algorithms in terms of both accuracy and computational cost. version:3
arxiv-1504-07590 | A Robust Lane Detection and Departure Warning System | http://arxiv.org/abs/1504.07590 | id:1504.07590 author:Mrinal Haloi, Dinesh Babu Jayagopi category:cs.CV 68T45  published:2015-04-28 summary:In this work, we have developed a robust lane detection and departure warning technique. Our system is based on single camera sensor. For lane detection a modified Inverse Perspective Mapping using only a few extrinsic camera parameters and illuminant Invariant techniques is used. Lane markings are represented using a combination of 2nd and 4th order steerable filters, robust to shadowing. Effect of shadowing and extra sun light are removed using Lab color space, and illuminant invariant representation. Lanes are assumed to be cubic curves and fitted using robust RANSAC. This method can reliably detect lanes of the road and its boundary. This method has been experimented in Indian road conditions under different challenging situations and the result obtained were very good. For lane departure angle an optical flow based method were used. version:1
arxiv-1504-03083 | Joint Learning of Distributed Representations for Images and Texts | http://arxiv.org/abs/1504.03083 | id:1504.03083 author:Xiaodong He, Rupesh Srivastava, Jianfeng Gao, Li Deng category:cs.CV  published:2015-04-13 summary:This technical report provides extra details of the deep multimodal similarity model (DMSM) which was proposed in (Fang et al. 2015, arXiv:1411.4952). The model is trained via maximizing global semantic similarity between images and their captions in natural language using the public Microsoft COCO database, which consists of a large set of images and their corresponding captions. The learned representations attempt to capture the combination of various visual concepts and cues. version:2
arxiv-1504-07575 | Becoming the Expert - Interactive Multi-Class Machine Teaching | http://arxiv.org/abs/1504.07575 | id:1504.07575 author:Edward Johns, Oisin Mac Aodha, Gabriel J. Brostow category:cs.CV cs.LG stat.ML  published:2015-04-28 summary:Compared to machines, humans are extremely good at classifying images into categories, especially when they possess prior knowledge of the categories at hand. If this prior information is not available, supervision in the form of teaching images is required. To learn categories more quickly, people should see important and representative images first, followed by less important images later - or not at all. However, image-importance is individual-specific, i.e. a teaching image is important to a student if it changes their overall ability to discriminate between classes. Further, students keep learning, so while image-importance depends on their current knowledge, it also varies with time. In this work we propose an Interactive Machine Teaching algorithm that enables a computer to teach challenging visual concepts to a human. Our adaptive algorithm chooses, online, which labeled images from a teaching set should be shown to the student as they learn. We show that a teaching strategy that probabilistically models the student's ability and progress, based on their correct and incorrect answers, produces better 'experts'. We present results using real human participants across several varied and challenging real-world datasets. version:1
arxiv-1504-07571 | Can Machines Truly Think | http://arxiv.org/abs/1504.07571 | id:1504.07571 author:Murat Okandan category:cs.AI cs.NE  published:2015-04-28 summary:Can machines truly think? This question and its answer have many implications that depend, in large part, on any number of assumptions underlying how the issue has been addressed or considered previously. A crucial question, and one that is almost taken for granted, is the starting point for this discussion: Can "thought" be achieved or emulated by algorithmic procedures? version:1
arxiv-1504-06665 | Using Syntax-Based Machine Translation to Parse English into Abstract Meaning Representation | http://arxiv.org/abs/1504.06665 | id:1504.06665 author:Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, Jonathan May category:cs.CL cs.AI I.2.7  published:2015-04-24 summary:We present a parser for Abstract Meaning Representation (AMR). We treat English-to-AMR conversion within the framework of string-to-tree, syntax-based machine translation (SBMT). To make this work, we transform the AMR structure into a form suitable for the mechanics of SBMT and useful for modeling. We introduce an AMR-specific language model and add data and features drawn from semantic resources. Our resulting AMR parser improves upon state-of-the-art results by 7 Smatch points. version:2
arxiv-1504-07553 | Differentially Private Release and Learning of Threshold Functions | http://arxiv.org/abs/1504.07553 | id:1504.07553 author:Mark Bun, Kobbi Nissim, Uri Stemmer, Salil Vadhan category:cs.CR cs.LG  published:2015-04-28 summary:We prove new upper and lower bounds on the sample complexity of $(\epsilon, \delta)$ differentially private algorithms for releasing approximate answers to threshold functions. A threshold function $c_x$ over a totally ordered domain $X$ evaluates to $c_x(y) = 1$ if $y \le x$, and evaluates to $0$ otherwise. We give the first nontrivial lower bound for releasing thresholds with $(\epsilon,\delta)$ differential privacy, showing that the task is impossible over an infinite domain $X$, and moreover requires sample complexity $n \ge \Omega(\log^* X )$, which grows with the size of the domain. Inspired by the techniques used to prove this lower bound, we give an algorithm for releasing thresholds with $n \le 2^{(1+ o(1))\log^* X }$ samples. This improves the previous best upper bound of $8^{(1 + o(1))\log^* X }$ (Beimel et al., RANDOM '13). Our sample complexity upper and lower bounds also apply to the tasks of learning distributions with respect to Kolmogorov distance and of properly PAC learning thresholds with differential privacy. The lower bound gives the first separation between the sample complexity of properly learning a concept class with $(\epsilon,\delta)$ differential privacy and learning without privacy. For properly learning thresholds in $\ell$ dimensions, this lower bound extends to $n \ge \Omega(\ell \cdot \log^* X )$. To obtain our results, we give reductions in both directions from releasing and properly learning thresholds and the simpler interior point problem. Given a database $D$ of elements from $X$, the interior point problem asks for an element between the smallest and largest elements in $D$. We introduce new recursive constructions for bounding the sample complexity of the interior point problem, as well as further reductions and techniques for proving impossibility results for other basic problems in differential privacy. version:1
arxiv-1504-07488 | Speeding Up Neural Networks for Large Scale Classification using WTA Hashing | http://arxiv.org/abs/1504.07488 | id:1504.07488 author:Amir H. Bakhtiary, Agata Lapedriza, David Masip category:cs.CV  published:2015-04-28 summary:In this paper we propose to use the Winner Takes All hashing technique to speed up forward propagation and backward propagation in fully connected layers in convolutional neural networks. The proposed technique reduces significantly the computational complexity, which in turn, allows us to train layers with a large number of kernels with out the associated time penalty. As a consequence we are able to train convolutional neural network on a very large number of output classes with only a small increase in the computational cost. To show the effectiveness of the technique we train a new output layer on a pretrained network using both the regular multiplicative approach and our proposed hashing methodology. Our results showed no drop in performance and demonstrate, with our implementation, a 7 fold speed up during the training. version:1
arxiv-1412-5673 | Entity-Augmented Distributional Semantics for Discourse Relations | http://arxiv.org/abs/1412.5673 | id:1412.5673 author:Yangfeng Ji, Jacob Eisenstein category:cs.CL cs.LG  published:2014-12-17 summary:Discourse relations bind smaller linguistic elements into coherent texts. However, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked sentences. A more subtle challenge is that it is not enough to represent the meaning of each sentence of a discourse relation, because the relation may depend on links between lower-level elements, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted not only from the distributional representations of the sentences, but also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank. version:3
arxiv-1504-07460 | Identifying Reliable Annotations for Large Scale Image Segmentation | http://arxiv.org/abs/1504.07460 | id:1504.07460 author:Alexander Kolesnikov, Christoph H. Lampert category:cs.CV  published:2015-04-28 summary:Challenging computer vision tasks, in particular semantic image segmentation, require large training sets of annotated images. While obtaining the actual images is often unproblematic, creating the necessary annotation is a tedious and costly process. Therefore, one often has to work with unreliable annotation sources, such as Amazon Mechanical Turk or (semi-)automatic algorithmic techniques. In this work, we present a Gaussian process (GP) based technique for simultaneously identifying which images of a training set have unreliable annotation and learning a segmentation model in which the negative effect of these images is suppressed. Alternatively, the model can also just be used to identify the most reliably annotated images from the training set, which can then be used for training any other segmentation method. By relying on "deep features" in combination with a linear covariance function, our GP can be learned and its hyperparameter determined efficiently using only matrix operations and gradient-based optimization. This makes our method scalable even to large datasets with several million training instances. version:1
arxiv-1504-07459 | CommentWatcher: An Open Source Web-based platform for analyzing discussions on web forums | http://arxiv.org/abs/1504.07459 | id:1504.07459 author:Marian-Andrei Rizoiu, Adrien Guille, Julien Velcin category:cs.CL cs.SI H.3.5; I.2.7; H.3.5  published:2015-04-28 summary:We present CommentWatcher, an open source tool aimed at analyzing discussions on web forums. Constructed as a web platform, CommentWatcher features automatic mass fetching of user posts from forum on multiple sites, extracting topics, visualizing the topics as an expression cloud and exploring their temporal evolution. The underlying social network of users is simultaneously constructed using the citation relations between users and visualized as a graph structure. Our platform addresses the issues of the diversity and dynamics of structures of webpages hosting the forums by implementing a parser architecture that is independent of the HTML structure of webpages. This allows easy on-the-fly adding of new websites. Two types of users are targeted: end users who seek to study the discussed topics and their temporal evolution, and researchers in need of establishing a forum benchmark dataset and comparing the performances of analysis tools. version:1
arxiv-1504-07442 | Embedded Platforms for Computer Vision-based Advanced Driver Assistance Systems: a Survey | http://arxiv.org/abs/1504.07442 | id:1504.07442 author:Gorka Velez, Oihana Otaegui category:cs.CV  published:2015-04-28 summary:Computer Vision, either alone or combined with other technologies such as radar or Lidar, is one of the key technologies used in Advanced Driver Assistance Systems (ADAS). Its role understanding and analysing the driving scene is of great importance as it can be noted by the number of ADAS applications that use this technology. However, porting a vision algorithm to an embedded automotive system is still very challenging, as there must be a trade-off between several design requisites. Furthermore, there is not a standard implementation platform, so different alternatives have been proposed by both the scientific community and the industry. This paper aims to review the requisites and the different embedded implementation platforms that can be used for Computer Vision-based ADAS, with a critical analysis and an outlook to future trends. version:1
arxiv-1412-2693 | Provable Methods for Training Neural Networks with Sparse Connectivity | http://arxiv.org/abs/1412.2693 | id:1412.2693 author:Hanie Sedghi, Anima Anandkumar category:cs.LG cs.NE stat.ML  published:2014-12-08 summary:We provide novel guaranteed approaches for training feedforward neural networks with sparse connectivity. We leverage on the techniques developed previously for learning linear networks and show that they can also be effectively adopted to learn non-linear networks. We operate on the moments involving label and the score function of the input, and show that their factorization provably yields the weight matrix of the first layer of a deep network under mild conditions. In practice, the output of our method can be employed as effective initializers for gradient descent. version:4
arxiv-1504-07395 | Lexical Translation Model Using a Deep Neural Network Architecture | http://arxiv.org/abs/1504.07395 | id:1504.07395 author:Thanh-Le Ha, Jan Niehues, Alex Waibel category:cs.CL cs.LG cs.NE  published:2015-04-28 summary:In this paper we combine the advantages of a model using global source sentence contexts, the Discriminative Word Lexicon, and neural networks. By using deep neural networks instead of the linear maximum entropy model in the Discriminative Word Lexicon models, we are able to leverage dependencies between different source words due to the non-linearity. Furthermore, the models for different target words can share parameters and therefore data sparsity problems are effectively reduced. By using this approach in a state-of-the-art translation system, we can improve the performance by up to 0.5 BLEU points for three different language pairs on the TED translation task. version:1
arxiv-1504-07389 | Building Classifiers to Predict the Start of Glucose-Lowering Pharmacotherapy Using Belgian Health Expenditure Data | http://arxiv.org/abs/1504.07389 | id:1504.07389 author:Marc Claesen, Frank De Smet, Pieter Gillard, Chantal Mathieu, Bart De Moor category:stat.ML cs.IR I.5.4; J.3  published:2015-04-28 summary:Early diagnosis is important for type 2 diabetes (T2D) to improve patient prognosis, prevent complications and reduce long-term treatment costs. We present a novel risk profiling approach based exclusively on health expenditure data that is available to Belgian mutual health insurers. We used expenditure data related to drug purchases and medical provisions to construct models that predict whether a patient will start glucose-lowering pharmacotherapy in the coming years, based on that patient's recent medical expenditure history. The design and implementation of the modeling strategy are discussed in detail and several learning methods are benchmarked for our application. Our best performing model obtains between 74.9% and 76.8% area under the ROC curve, which is comparable to state-of-the-art risk prediction approaches for T2D based on questionnaires. In contrast to other methods, our approach can be implemented on a population-wide scale at virtually no extra operational cost. Possibly, our approach can be further improved by additional information about some risk factors of T2D that is unavailable in health expenditure data. version:1
arxiv-1412-7022 | Audio Source Separation with Discriminative Scattering Networks | http://arxiv.org/abs/1412.7022 | id:1412.7022 author:Pablo Sprechmann, Joan Bruna, Yann LeCun category:cs.SD cs.LG  published:2014-12-22 summary:In this report we describe an ongoing line of research for solving single-channel source separation problems. Many monaural signal decomposition techniques proposed in the literature operate on a feature space consisting of a time-frequency representation of the input data. A challenge faced by these approaches is to effectively exploit the temporal dependencies of the signals at scales larger than the duration of a time-frame. In this work we propose to tackle this problem by modeling the signals using a time-frequency representation with multiple temporal resolutions. The proposed representation consists of a pyramid of wavelet scattering operators, which generalizes Constant Q Transforms (CQT) with extra layers of convolution and complex modulus. We first show that learning standard models with this multi-resolution setting improves source separation results over fixed-resolution methods. As study case, we use Non-Negative Matrix Factorizations (NMF) that has been widely considered in many audio application. Then, we investigate the inclusion of the proposed multi-resolution setting into a discriminative training regime. We discuss several alternatives using different deep neural network architectures. version:3
arxiv-1504-07329 | Combined A*-Ants Algorithm: A New Multi-Parameter Vehicle Navigation Scheme | http://arxiv.org/abs/1504.07329 | id:1504.07329 author:Hojjat Salehinejad, Hossein Nezamabadi-pour, Saeid Saryazdi, Fereydoun Farrahi-Moghaddam category:cs.NE  published:2015-04-28 summary:In this paper a multi-parameter A*(A- star)-ants based algorithm is proposed in order to find the best optimized multi-parameter path between two desired points in regions. This algorithm recognizes paths, according to user desired parameters using electronic maps. The proposed algorithm is a combination of A* and ants algorithm in which the proposed A* algorithm is the prologue to the suggested ant based algorithm .In fact, this A* algorithm invigorates some paths pheromones in ants algorithm. As one of implementations of this method, this algorithm was applied on a part of Kerman city, Iran as a multi-parameter vehicle navigator. It finds the best optimized multi-parameter direction between two desired junctions based on city traveler parameters. Comparison results between the proposed method and ants algorithm demonstrates efficiency and lower cost function results of the proposed method versus ants algorithm. version:1
arxiv-1504-07327 | Toward Smart Power Grids: Communication Network Design for Power Grids Synchronization | http://arxiv.org/abs/1504.07327 | id:1504.07327 author:Hojjat Salehinejad, Farhad Pouladi, Siamak Talebi category:cs.NE  published:2015-04-28 summary:In smart power grids, keeping the synchronicity of generators and the corresponding controls is of great importance. To do so, a simple model is employed in terms of swing equation to represent the interactions among dynamics of generators and feedback control. In case of having a communication network available, the control can be done based on the transmitted measurements by the communication network. The stability of system is denoted by the largest eigenvalue of the weighted sum of the Laplacian matrices of the communication infrastructure and power network. In this work, we use graph theory to model the communication network as a graph problem. Then, Ant Colony System (ACS) is employed for optimum design of above graph for synchronization of power grids. Performance evaluation of the proposed method for the 39-bus New England power system versus methods such as exhaustive search and Rayleigh quotient approximation indicates feasibility and effectiveness of our method for even large scale smart power grids. version:1
arxiv-1504-07324 | Reader-Aware Multi-Document Summarization via Sparse Coding | http://arxiv.org/abs/1504.07324 | id:1504.07324 author:Piji Li, Lidong Bing, Wai Lam, Hang Li, Yi Liao category:cs.CL cs.AI  published:2015-04-28 summary:We propose a new MDS paradigm called reader-aware multi-document summarization (RA-MDS). Specifically, a set of reader comments associated with the news reports are also collected. The generated summaries from the reports for the event should be salient according to not only the reports but also the reader comments. To tackle this RA-MDS problem, we propose a sparse-coding-based method that is able to calculate the salience of the text units by jointly considering news reports and reader comments. Another reader-aware characteristic of our framework is to improve linguistic quality via entity rewriting. The rewriting consideration is jointly assessed together with other summarization requirements under a unified optimization model. To support the generation of compressive summaries via optimization, we explore a finer syntactic unit, namely, noun/verb phrase. In this work, we also generate a data set for conducting RA-MDS. Extensive experiments on this data set and some classical data sets demonstrate the effectiveness of our proposed approach. version:1
arxiv-1504-07313 | Private Disclosure of Information in Health Tele-monitoring | http://arxiv.org/abs/1504.07313 | id:1504.07313 author:Daniel Aranki, Ruzena Bajcsy category:cs.CR cs.AI cs.IT cs.LG math.IT  published:2015-04-28 summary:We present a novel framework, called Private Disclosure of Information (PDI), which is aimed to prevent an adversary from inferring certain sensitive information about subjects using the data that they disclosed during communication with an intended recipient. We show cases where it is possible to achieve perfect privacy regardless of the adversary's auxiliary knowledge while preserving full utility of the information to the intended recipient and provide sufficient conditions for such cases. We also demonstrate the applicability of PDI on a real-world data set that simulates a health tele-monitoring scenario. version:1
arxiv-1504-07284 | Mid-level Elements for Object Detection | http://arxiv.org/abs/1504.07284 | id:1504.07284 author:Aayush Bansal, Abhinav Shrivastava, Carl Doersch, Abhinav Gupta category:cs.CV  published:2015-04-27 summary:Building on the success of recent discriminative mid-level elements, we propose a surprisingly simple approach for object detection which performs comparable to the current state-of-the-art approaches on PASCAL VOC comp-3 detection challenge (no external data). Through extensive experiments and ablation analysis, we show how our approach effectively improves upon the HOG-based pipelines by adding an intermediate mid-level representation for the task of object detection. This representation is easily interpretable and allows us to visualize what our object detector "sees". We also discuss the insights our approach shares with CNN-based methods, such as sharing representation between categories helps. version:1
arxiv-1412-3596 | EgoSampling: Fast-Forward and Stereo for Egocentric Videos | http://arxiv.org/abs/1412.3596 | id:1412.3596 author:Yair Poleg, Tavi Halperin, Chetan Arora, Shmuel Peleg category:cs.CV cs.MM  published:2014-12-11 summary:While egocentric cameras like GoPro are gaining popularity, the videos they capture are long, boring, and difficult to watch from start to end. Fast forwarding (i.e. frame sampling) is a natural choice for faster video browsing. However, this accentuates the shake caused by natural head motion, making the fast forwarded video useless. We propose EgoSampling, an adaptive frame sampling that gives more stable fast forwarded videos. Adaptive frame sampling is formulated as energy minimization, whose optimal solution can be found in polynomial time. In addition, egocentric video taken while walking suffers from the left-right movement of the head as the body weight shifts from one leg to another. We turn this drawback into a feature: Stereo video can be created by sampling the frames from the left most and right most head positions of each step, forming approximate stereo-pairs. version:2
arxiv-1407-2538 | Learning Deep Structured Models | http://arxiv.org/abs/1407.2538 | id:1407.2538 author:Liang-Chieh Chen, Alexander G. Schwing, Alan L. Yuille, Raquel Urtasun category:cs.LG  published:2014-07-09 summary:Many problems in real-world applications involve predicting several random variables which are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such relationships. The goal of this paper is to combine MRFs with deep learning algorithms to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as multi-class classification of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains. version:3
arxiv-1504-07278 | Optimal Convergence Rate in Feed Forward Neural Networks using HJB Equation | http://arxiv.org/abs/1504.07278 | id:1504.07278 author:Vipul Arora, Laxmidhar Behera, Ajay Pratap Yadav category:cs.NE  published:2015-04-27 summary:A control theoretic approach is presented in this paper for both batch and instantaneous updates of weights in feed-forward neural networks. The popular Hamilton-Jacobi-Bellman (HJB) equation has been used to generate an optimal weight update law. The remarkable contribution in this paper is that closed form solutions for both optimal cost and weight update can be achieved for any feed-forward network using HJB equation in a simple yet elegant manner. The proposed approach has been compared with some of the existing best performing learning algorithms. It is found as expected that the proposed approach is faster in convergence in terms of computational time. Some of the benchmark test data such as 8-bit parity, breast cancer and credit approval, as well as 2D Gabor function have been used to validate our claims. The paper also discusses issues related to global optimization. The limitations of popular deterministic weight update laws are critiqued and the possibility of global optimization using HJB formulation is discussed. It is hoped that the proposed algorithm will bring in a lot of interest in researchers working in developing fast learning algorithms and global optimization. version:1
arxiv-1504-07272 | Surrogate regret bounds for generalized classification performance metrics | http://arxiv.org/abs/1504.07272 | id:1504.07272 author:Wojciech Kotłowski, Krzysztof Dembczyński category:cs.LG  published:2015-04-27 summary:We consider optimization of generalized performance metrics for binary classification by means of surrogate loss. We focus on a class of metrics, which are linear-fractional functions of the false positive and false negative rates (examples of which include $F_{\beta}$-measure, Jaccard similarity coefficient, AM measure, and many others). Our analysis concerns the following two-step procedure. First, a real-valued function $f$ is learned by minimizing a surrogate loss for binary classification on the training sample. It is assumed that the surrogate loss is a strongly proper composite loss function (examples of which include logistic loss, squared-error loss, exponential loss, etc.). Then, given $f$, a threshold $\hat{\theta}$ is tuned on a separate validation sample, by direct optimization of the target performance measure. We show that the regret of the resulting classifier (obtained from thresholding $f$ on $\hat{\theta}$) measured with respect to the target metric is upperbounded by the regret of $f$ measured with respect to the surrogate loss. Our finding is further analyzed in a computational study on both synthetic and real data sets. version:1
arxiv-1504-07269 | Dynamic Body VSLAM with Semantic Constraints | http://arxiv.org/abs/1504.07269 | id:1504.07269 author:N. Dinesh Reddy, Prateek Singhal, Visesh Chari, K. Madhava Krishna category:cs.CV  published:2015-04-27 summary:Image based reconstruction of urban environments is a challenging problem that deals with optimization of large number of variables, and has several sources of errors like the presence of dynamic objects. Since most large scale approaches make the assumption of observing static scenes, dynamic objects are relegated to the noise modeling section of such systems. This is an approach of convenience since the RANSAC based framework used to compute most multiview geometric quantities for static scenes naturally confine dynamic objects to the class of outlier measurements. However, reconstructing dynamic objects along with the static environment helps us get a complete picture of an urban environment. Such understanding can then be used for important robotic tasks like path planning for autonomous navigation, obstacle tracking and avoidance, and other areas. In this paper, we propose a system for robust SLAM that works in both static and dynamic environments. To overcome the challenge of dynamic objects in the scene, we propose a new model to incorporate semantic constraints into the reconstruction algorithm. While some of these constraints are based on multi-layered dense CRFs trained over appearance as well as motion cues, other proposed constraints can be expressed as additional terms in the bundle adjustment optimization process that does iterative refinement of 3D structure and camera / object motion trajectories. We show results on the challenging KITTI urban dataset for accuracy of motion segmentation and reconstruction of the trajectory and shape of moving objects relative to ground truth. We are able to show average relative error reduction by a significant amount for moving object trajectory reconstruction relative to state-of-the-art methods like VISO 2, as well as standard bundle adjustment algorithms. version:1
arxiv-1504-07259 | Image Segmentation and Restoration Using Parametric Contours With Free Endpoints | http://arxiv.org/abs/1504.07259 | id:1504.07259 author:Heike Benninghoff, Harald Garcke category:cs.CV math.AP math.NA  published:2015-04-27 summary:In this paper, we introduce a novel approach for active contours with free endpoints. A scheme is presented for image segmentation and restoration based on a discrete version of the Mumford-Shah functional where the contours can be both closed and open curves. Additional to a flow of the curves in normal direction, evolution laws for the tangential flow of the endpoints are derived. Using a parametric approach to describe the evolving contours together with an edge-preserving denoising, we obtain a fast method for image segmentation and restoration. The analytical and numerical schemes are presented followed by numerical experiments with artificial test images and with a real medical image. version:1
arxiv-1504-07235 | Sign Stable Random Projections for Large-Scale Learning | http://arxiv.org/abs/1504.07235 | id:1504.07235 author:Ping Li category:stat.ML cs.LG stat.CO  published:2015-04-27 summary:We study the use of "sign $\alpha$-stable random projections" (where $0<\alpha\leq 2$) for building basic data processing tools in the context of large-scale machine learning applications (e.g., classification, regression, clustering, and near-neighbor search). After the processing by sign stable random projections, the inner products of the processed data approximate various types of nonlinear kernels depending on the value of $\alpha$. Thus, this approach provides an effective strategy for approximating nonlinear learning algorithms essentially at the cost of linear learning. When $\alpha =2$, it is known that the corresponding nonlinear kernel is the arc-cosine kernel. When $\alpha=1$, the procedure approximates the arc-cos-$\chi^2$ kernel (under certain condition). When $\alpha\rightarrow0+$, it corresponds to the resemblance kernel. From practitioners' perspective, the method of sign $\alpha$-stable random projections is ready to be tested for large-scale learning applications, where $\alpha$ can be simply viewed as a tuning parameter. What is missing in the literature is an extensive empirical study to show the effectiveness of sign stable random projections, especially for $\alpha\neq 2$ or 1. The paper supplies such a study on a wide variety of classification datasets. In particular, we compare shoulder-by-shoulder sign stable random projections with the recently proposed "0-bit consistent weighted sampling (CWS)" (Li 2015). version:1
arxiv-1504-05070 | Self-Adaptive Hierarchical Sentence Model | http://arxiv.org/abs/1504.05070 | id:1504.05070 author:Han Zhao, Zhengdong Lu, Pascal Poupart category:cs.CL cs.LG cs.NE  published:2015-04-20 summary:The ability to accurately model a sentence at varying stages (e.g., word-phrase-sentence) plays a central role in natural language processing. As an effort towards this goal we propose a self-adaptive hierarchical sentence model (AdaSent). AdaSent effectively forms a hierarchy of representations from words to phrases and then to sentences through recursive gated local composition of adjacent segments. We design a competitive mechanism (through gating networks) to allow the representations of the same sentence to be engaged in a particular learning task (e.g., classification), therefore effectively mitigating the gradient vanishing problem persistent in other recursive models. Both qualitative and quantitative analysis shows that AdaSent can automatically form and select the representations suitable for the task at hand during training, yielding superior classification performance over competitor models on 5 benchmark data sets. version:2
arxiv-1504-07159 | Combining Local Appearance and Holistic View: Dual-Source Deep Neural Networks for Human Pose Estimation | http://arxiv.org/abs/1504.07159 | id:1504.07159 author:Xiaochuan Fan, Kang Zheng, Yuewei Lin, Song Wang category:cs.CV  published:2015-04-27 summary:We propose a new learning-based method for estimating 2D human pose from a single image, using Dual-Source Deep Convolutional Neural Networks (DS-CNN). Recently, many methods have been developed to estimate human pose by using pose priors that are estimated from physiologically inspired graphical models or learned from a holistic perspective. In this paper, we propose to integrate both the local (body) part appearance and the holistic view of each local part for more accurate human pose estimation. Specifically, the proposed DS-CNN takes a set of image patches (category-independent object proposals for training and multi-scale sliding windows for testing) as the input and then learns the appearance of each local part by considering their holistic views in the full body. Using DS-CNN, we achieve both joint detection, which determines whether an image patch contains a body joint, and joint localization, which finds the exact location of the joint in the image patch. Finally, we develop an algorithm to combine these joint detection/localization results from all the image patches for estimating the human pose. The experimental results show the effectiveness of the proposed method by comparing to the state-of-the-art human-pose estimation methods based on pose priors that are estimated from physiologically inspired graphical models or learned from a holistic perspective. version:1
arxiv-1412-5744 | Stochastic Descent Analysis of Representation Learning Algorithms | http://arxiv.org/abs/1412.5744 | id:1412.5744 author:Richard M. Golden category:stat.ML cs.LG  published:2014-12-18 summary:Although stochastic approximation learning methods have been widely used in the machine learning literature for over 50 years, formal theoretical analyses of specific machine learning algorithms are less common because stochastic approximation theorems typically possess assumptions which are difficult to communicate and verify. This paper presents a new stochastic approximation theorem for state-dependent noise with easily verifiable assumptions applicable to the analysis and design of important deep learning algorithms including: adaptive learning, contrastive divergence learning, stochastic descent expectation maximization, and active learning. version:6
arxiv-1504-07082 | Shape Representation and Classification through Pattern Spectrum and Local Binary Pattern - A Decision Level Fusion Approach | http://arxiv.org/abs/1504.07082 | id:1504.07082 author:B. H. Shekar, Bharathi Pilar category:cs.CV  published:2015-04-27 summary:In this paper, we present a decision level fused local Morphological Pattern Spectrum(PS) and Local Binary Pattern (LBP) approach for an efficient shape representation and classification. This method makes use of Earth Movers Distance(EMD) as the measure in feature matching and shape retrieval process. The proposed approach has three major phases : Feature Extraction, Construction of hybrid spectrum knowledge base and Classification. In the first phase, feature extraction of the shape is done using pattern spectrum and local binary pattern method. In the second phase, the histograms of both pattern spectrum and local binary pattern are fused and stored in the knowledge base. In the third phase, the comparison and matching of the features, which are represented in the form of histograms, is done using Earth Movers Distance(EMD) as metric. The top-n shapes are retrieved for each query shape. The accuracy is tested by means of standard Bulls eye score method. The experiments are conducted on publicly available shape datasets like Kimia-99, Kimia-216 and MPEG-7. The comparative study is also provided with the well known approaches to exhibit the retrieval accuracy of the proposed approach. version:1
arxiv-1504-07071 | Exploring semantically-related concepts from Wikipedia: the case of SeRE | http://arxiv.org/abs/1504.07071 | id:1504.07071 author:Daniel Hienert, Dennis Wegener, Siegfried Schomisch category:cs.CL cs.IR  published:2015-04-27 summary:In this paper we present our web application SeRE designed to explore semantically related concepts. Wikipedia and DBpedia are rich data sources to extract related entities for a given topic, like in- and out-links, broader and narrower terms, categorisation information etc. We use the Wikipedia full text body to compute the semantic relatedness for extracted terms, which results in a list of entities that are most relevant for a topic. For any given query, the user interface of SeRE visualizes these related concepts, ordered by semantic relatedness; with snippets from Wikipedia articles that explain the connection between those two entities. In a user study we examine how SeRE can be used to find important entities and their relationships for a given topic and to answer the question of how the classification system can be used for filtering. version:1
arxiv-1410-5605 | Attentive monitoring of multiple video streams driven by a Bayesian foraging strategy | http://arxiv.org/abs/1410.5605 | id:1410.5605 author:Paolo Napoletano, Giuseppe Boccignone, Francesco Tisato category:cs.CV  published:2014-10-21 summary:In this paper we shall consider the problem of deploying attention to subsets of the video streams for collating the most relevant data and information of interest related to a given task. We formalize this monitoring problem as a foraging problem. We propose a probabilistic framework to model observer's attentive behavior as the behavior of a forager. The forager, moment to moment, focuses its attention on the most informative stream/camera, detects interesting objects or activities, or switches to a more profitable stream. The approach proposed here is suitable to be exploited for multi-stream video summarization. Meanwhile, it can serve as a preliminary step for more sophisticated video surveillance, e.g. activity and behavior analysis. Experimental results achieved on the UCR Videoweb Activities Dataset, a publicly available dataset, are presented to illustrate the utility of the proposed technique. version:3
arxiv-1606-00414 | On a Possible Similarity between Gene and Semantic Networks | http://arxiv.org/abs/1606.00414 | id:1606.00414 author:Nicolas Turenne category:cs.CL cs.CE cs.SI  published:2015-04-27 summary:In several domains such as linguistics, molecular biology or social sciences, holistic effects are hardly well-defined by modeling with single units, but more and more studies tend to understand macro structures with the help of meaningful and useful associations in fields such as social networks, systems biology or semantic web. A stochastic multi-agent system offers both accurate theoretical framework and operational computing implementations to model large-scale associations, their dynamics and patterns extraction. We show that clustering around a target object in a set of associations of object prove some similarity in specific data and two case studies about gene-gene and term-term relationships leading to an idea of a common organizing principle of cognition with random and deterministic effects. version:1
arxiv-1504-07028 | SegSALSA-STR: A convex formulation to supervised hyperspectral image segmentation using hidden fields and structure tensor regularization | http://arxiv.org/abs/1504.07028 | id:1504.07028 author:Filipe Condessa, Jose Bioucas-Dias, Jelena Kovacevic category:cs.CV 68  published:2015-04-27 summary:We present a supervised hyperspectral image segmentation algorithm based on a convex formulation of a marginal maximum a posteriori segmentation with hidden fields and structure tensor regularization: Segmentation via the Constraint Split Augmented Lagrangian Shrinkage by Structure Tensor Regularization (SegSALSA-STR). This formulation avoids the generally discrete nature of segmentation problems and the inherent NP-hardness of the integer optimization associated. We extend the Segmentation via the Constraint Split Augmented Lagrangian Shrinkage (SegSALSA) algorithm by generalizing the vectorial total variation prior using a structure tensor prior constructed from a patch-based Jacobian. The resulting algorithm is convex, time-efficient and highly parallelizable. This shows the potential of combining hidden fields with convex optimization through the inclusion of different regularizers. The SegSALSA-STR algorithm is validated in the segmentation of real hyperspectral images. version:1
arxiv-1504-07021 | On-Board Vision Processing For Small UAVs: Time to Rethink Strategy | http://arxiv.org/abs/1504.07021 | id:1504.07021 author:Shoaib Ehsan, Klaus D. McDonald-Maier category:cs.CV cs.RO  published:2015-04-27 summary:The ultimate research goal for unmanned aerial vehicles (UAVs) is to facilitate autonomy of operation. Research in the last decade has highlighted the potential of vision sensing in this regard. Although vital for accomplishment of missions assigned to any type of unmanned aerial vehicles, vision sensing is more critical for small aerial vehicles due to lack of high precision inertial sensors. In addition, uncertainty of GPS signal in indoor and urban environments calls for more reliance on vision sensing for such small vehicles. With off-line processing does not offer an attractive option in terms of autonomy, these vehicles have been challenging platforms to implement vision processing onboard due to their strict payload capacity and power budget. The strict constraints drive the need for new vision processing architectures for small unmanned aerial vehicles. Recent research has shown encouraging results with FPGA based hardware architectures. This paper reviews the bottle necks involved in implementing vision processing on-board, advocates the potential of hardware based solutions to tackle strict constraints of small unmanned aerial vehicles and finally analyzes feasibility of ASICs, Structured ASICs and FPGAs for use on future systems. version:1
arxiv-1504-07004 | An Active Learning Based Approach For Effective Video Annotation And Retrieval | http://arxiv.org/abs/1504.07004 | id:1504.07004 author:Moitreya Chatterjee, Anton Leuski category:cs.MM cs.IR cs.LG H.3.3; H.5.1  published:2015-04-27 summary:Conventional multimedia annotation/retrieval systems such as Normalized Continuous Relevance Model (NormCRM) [16] require a fully labeled training data for a good performance. Active Learning, by determining an order for labeling the training data, allows for a good performance even before the training data is fully annotated. In this work we propose an active learning algorithm, which combines a novel measure of sample uncertainty with a novel clustering-based approach for determining sample density and diversity and integrate it with NormCRM. The clusters are also iteratively refined to ensure both feature and label-level agreement among samples. We show that our approach outperforms multiple baselines both on a recent, open character animation dataset and on the popular TRECVID corpus at both the tasks of annotation and text-based retrieval of videos. version:1
arxiv-1504-07000 | Accelerated nonlinear discriminant analysis | http://arxiv.org/abs/1504.07000 | id:1504.07000 author:Nikolaos Gkalelis, Vasileios Mezaris category:cs.LG  published:2015-04-27 summary:In this paper, a novel nonlinear discriminant analysis is proposed. Experimental results show that the new method provides state of the art performance when combined with LSVM in terms of training time and accuracy. version:1
arxiv-1504-06993 | Compression Artifacts Reduction by a Deep Convolutional Network | http://arxiv.org/abs/1504.06993 | id:1504.06993 author:Chao Dong, Yubin Deng, Chen Change Loy, Xiaoou Tang category:cs.CV I.4.5; I.2.6  published:2015-04-27 summary:Lossy compression introduces complex compression artifacts, particularly the blocking artifacts, ringing effects and blurring. Existing algorithms either focus on removing blocking artifacts and produce blurred output, or restores sharpened images that are accompanied with ringing effects. Inspired by the deep convolutional networks (DCN) on super-resolution, we formulate a compact and efficient network for seamless attenuation of different compression artifacts. We also demonstrate that a deeper model can be effectively trained with the features learned in a shallow network. Following a similar "easy to hard" idea, we systematically investigate several practical transfer settings and show the effectiveness of transfer learning in low-level vision problems. Our method shows superior performance than the state-of-the-arts both on the benchmark datasets and the real-world use case (i.e. Twitter). In addition, we show that our method can be applied as pre-processing to facilitate other low-level vision routines when they take compressed images as input. version:1
arxiv-1504-06964 | Modeling Recovery Curves With Application to Prostatectomy | http://arxiv.org/abs/1504.06964 | id:1504.06964 author:Fulton Wang, Tyler H. McCormick, Cynthia Rudin, John Gore category:stat.ME stat.AP stat.ML  published:2015-04-27 summary:We propose a Bayesian model that predicts recovery curves based on information available before the disruptive event. A recovery curve of interest is the quantified sexual function of prostate cancer patients after prostatectomy surgery. We illustrate the utility of our model as a pre-treatment medical decision aid, producing personalized predictions that are both interpretable and accurate. We uncover covariate relationships that agree with and supplement that in existing medical literature. version:1
arxiv-1504-06936 | Concept Extraction to Identify Adverse Drug Reactions in Medical Forums: A Comparison of Algorithms | http://arxiv.org/abs/1504.06936 | id:1504.06936 author:Alejandro Metke-Jimenez, Sarvnaz Karimi category:cs.AI cs.CL cs.IR  published:2015-04-27 summary:Social media is becoming an increasingly important source of information to complement traditional pharmacovigilance methods. In order to identify signals of potential adverse drug reactions, it is necessary to first identify medical concepts in the social media text. Most of the existing studies use dictionary-based methods which are not evaluated independently from the overall signal detection task. We compare different approaches to automatically identify and normalise medical concepts in consumer reviews in medical forums. Specifically, we implement several dictionary-based methods popular in the relevant literature, as well as a method we suggest based on a state-of-the-art machine learning method for entity recognition. MetaMap, a popular biomedical concept extraction tool, is used as a baseline. Our evaluations were performed in a controlled setting on a common corpus which is a collection of medical forum posts annotated with concepts and linked to controlled vocabularies such as MedDRA and SNOMED CT. To our knowledge, our study is the first to systematically examine the effect of popular concept extraction methods in the area of signal detection for adverse reactions. We show that the choice of algorithm or controlled vocabulary has a significant impact on concept extraction, which will impact the overall signal detection process. We also show that our proposed machine learning approach significantly outperforms all the other methods in identification of both adverse reactions and drugs, even when trained with a relatively small set of annotated text. version:1
arxiv-1504-06921 | Detection and Recognition of Malaysian Special License Plate Based On SIFT Features | http://arxiv.org/abs/1504.06921 | id:1504.06921 author:Hooi Sin Ng, Yong Haur Tay, Kim Meng Liang, Hamam Mokayed, Hock Woon Hon category:cs.CV  published:2015-04-27 summary:Automated car license plate recognition systems are developed and applied for purpose of facilitating the surveillance, law enforcement, access control and intelligent transportation monitoring with least human intervention. In this paper, an algorithm based on SIFT feature points clustering and matching is proposed to address the issue of recognizing Malaysian special plates. These special plates do not follow the format of standard car plates as they may contain italic, cursive, connected and small letters. The algorithm is tested with 150 Malaysian special plate images under different environment and the promising experimental results demonstrate that the proposed algorithm is relatively robust. version:1
arxiv-1503-02364 | Neural Responding Machine for Short-Text Conversation | http://arxiv.org/abs/1503.02364 | id:1503.02364 author:Lifeng Shang, Zhengdong Lu, Hang Li category:cs.CL cs.AI cs.NE  published:2015-03-09 summary:We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models. version:2
arxiv-1504-06897 | Linear Spatial Pyramid Matching Using Non-convex and non-negative Sparse Coding for Image Classification | http://arxiv.org/abs/1504.06897 | id:1504.06897 author:Chengqiang Bao, Liangtian He, Yilun Wang category:cs.CV cs.LG 68T45 I.5.2  published:2015-04-27 summary:Recently sparse coding have been highly successful in image classification mainly due to its capability of incorporating the sparsity of image representation. In this paper, we propose an improved sparse coding model based on linear spatial pyramid matching(SPM) and Scale Invariant Feature Transform (SIFT ) descriptors. The novelty is the simultaneous non-convex and non-negative characters added to the sparse coding model. Our numerical experiments show that the improved approach using non-convex and non-negative sparse coding is superior than the original ScSPM[1] on several typical databases. version:1
arxiv-1504-06877 | Bayesian kernel-based system identification with quantized output data | http://arxiv.org/abs/1504.06877 | id:1504.06877 author:Giulio Bottegal, Gianluigi Pillonetto, Håkan Hjalmarsson category:cs.SY stat.ML  published:2015-04-26 summary:In this paper we introduce a novel method for linear system identification with quantized output data. We model the impulse response as a zero-mean Gaussian process whose covariance (kernel) is given by the recently proposed stable spline kernel, which encodes information on regularity and exponential stability. This serves as a starting point to cast our system identification problem into a Bayesian framework. We employ Markov Chain Monte Carlo (MCMC) methods to provide an estimate of the system. In particular, we show how to design a Gibbs sampler which quickly converges to the target distribution. Numerical simulations show a substantial improvement in the accuracy of the estimates over state-of-the-art kernel-based methods when employed in identification of systems with quantized data. version:1
arxiv-1504-06868 | Autonomy and Reliability of Continuous Active Learning for Technology-Assisted Review | http://arxiv.org/abs/1504.06868 | id:1504.06868 author:Gordon V. Cormack, Maura R. Grossman category:cs.IR cs.LG  published:2015-04-26 summary:We enhance the autonomy of the continuous active learning method shown by Cormack and Grossman (SIGIR 2014) to be effective for technology-assisted review, in which documents from a collection are retrieved and reviewed, using relevance feedback, until substantially all of the relevant documents have been reviewed. Autonomy is enhanced through the elimination of topic-specific and dataset-specific tuning parameters, so that the sole input required by the user is, at the outset, a short query, topic description, or single relevant document; and, throughout the review, ongoing relevance assessments of the retrieved documents. We show that our enhancements consistently yield superior results to Cormack and Grossman's version of continuous active learning, and other methods, not only on average, but on the vast majority of topics from four separate sets of tasks: the legal datasets examined by Cormack and Grossman, the Reuters RCV1-v2 subject categories, the TREC 6 AdHoc task, and the construction of the TREC 2002 filtering test collection. version:1
arxiv-1504-06864 | Fast Dictionary Matching for Content-based Image Retrieval | http://arxiv.org/abs/1504.06864 | id:1504.06864 author:Patryk Najgebauer, Janusz Rygal, Tomasz Nowak, Jakub Romanowski, Leszek Rutkowski, Sviatoslav Voloshynovskiy, Rafal Scherer category:cs.CV  published:2015-04-26 summary:This paper describes a method for searching for common sets of descriptors between collections of images. The presented method operates on local interest keypoints, which are generated using the SURF algorithm. The use of a dictionary of descriptors allowed achieving good performance of the content-based image retrieval. The method can be used to initially determine a set of similar pairs of keypoints between images. For this purpose, we use a certain level of tolerance between values of descriptors, as values of feature descriptors are almost never equal but similar between different images. After that, the method compares the structure of rotation and location of interest points in one image with the point structure in other images. Thus, we were able to find similar areas in images and determine the level of similarity between them, even when images contain different scenes. version:1
arxiv-1504-06859 | When Hillclimbers Beat Genetic Algorithms in Multimodal Optimization | http://arxiv.org/abs/1504.06859 | id:1504.06859 author:Fernando G. Lobo, Mosab Bazargani category:cs.NE  published:2015-04-26 summary:It has been shown in the past that a multistart hillclimbing strategy compares favourably to a standard genetic algorithm with respect to solving instances of the multimodal problem generator. We extend that work and verify if the utilization of diversity preservation techniques in the genetic algorithm changes the outcome of the comparison. We do so under two scenarios: (1) when the goal is to find the global optimum, (2) when the goal is to find all optima. A mathematical analysis is performed for the multistart hillclimbing algorithm and a through empirical study is conducted for solving instances of the multimodal problem generator with increasing number of optima, both with the hillclimbing strategy as well as with genetic algorithms with niching. Although niching improves the performance of the genetic algorithm, it is still inferior to the multistart hillclimbing strategy on this class of problems. An idealized niching strategy is also presented and it is argued that its performance should be close to a lower bound of what any evolutionary algorithm can do on this class of problems. version:1
arxiv-1503-00623 | Unregularized Online Learning Algorithms with General Loss Functions | http://arxiv.org/abs/1503.00623 | id:1503.00623 author:Yiming Ying, Ding-Xuan Zhou category:cs.LG stat.ML  published:2015-03-02 summary:In this paper, we consider unregularized online learning algorithms in a Reproducing Kernel Hilbert Spaces (RKHS). Firstly, we derive explicit convergence rates of the unregularized online learning algorithms for classification associated with a general gamma-activating loss (see Definition 1 in the paper). Our results extend and refine the results in Ying and Pontil (2008) for the least-square loss and the recent result in Bach and Moulines (2011) for the loss function with a Lipschitz-continuous gradient. Moreover, we establish a very general condition on the step sizes which guarantees the convergence of the last iterate of such algorithms. Secondly, we establish, for the first time, the convergence of the unregularized pairwise learning algorithm with a general loss function and derive explicit rates under the assumption of polynomially decaying step sizes. Concrete examples are used to illustrate our main results. The main techniques are tools from convex analysis, refined inequalities of Gaussian averages, and an induction approach. version:2
arxiv-1504-06848 | Maximum a Posteriori Estimation by Search in Probabilistic Programs | http://arxiv.org/abs/1504.06848 | id:1504.06848 author:David Tolpin, Frank Wood category:cs.AI stat.ML  published:2015-04-26 summary:We introduce an approximate search algorithm for fast maximum a posteriori probability estimation in probabilistic programs, which we call Bayesian ascent Monte Carlo (BaMC). Probabilistic programs represent probabilistic models with varying number of mutually dependent finite, countable, and continuous random variables. BaMC is an anytime MAP search algorithm applicable to any combination of random variables and dependencies. We compare BaMC to other MAP estimation algorithms and show that BaMC is faster and more robust on a range of probabilistic models. version:1
arxiv-1504-06825 | Comparison of Training Methods for Deep Neural Networks | http://arxiv.org/abs/1504.06825 | id:1504.06825 author:Patrick O. Glauner category:cs.LG cs.AI  published:2015-04-26 summary:This report describes the difficulties of training neural networks and in particular deep neural networks. It then provides a literature review of training methods for deep neural networks, with a focus on pre-training. It focuses on Deep Belief Networks composed of Restricted Boltzmann Machines and Stacked Autoencoders and provides an outreach on further and alternative approaches. It also includes related practical recommendations from the literature on training them. In the second part, initial experiments using some of the covered methods are performed on two databases. In particular, experiments are performed on the MNIST hand-written digit dataset and on facial emotion data from a Kaggle competition. The results are discussed in the context of results reported in other research papers. An error rate lower than the best contribution to the Kaggle competition is achieved using an optimized Stacked Autoencoder. version:1
arxiv-1504-06817 | Analysis of Nuclear Norm Regularization for Full-rank Matrix Completion | http://arxiv.org/abs/1504.06817 | id:1504.06817 author:Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou category:cs.LG stat.ML  published:2015-04-26 summary:In this paper, we provide a theoretical analysis of the nuclear-norm regularized least squares for full-rank matrix completion. Although similar formulations have been examined by previous studies, their results are unsatisfactory because only additive upper bounds are provided. Under the assumption that the top eigenspaces of the target matrix are incoherent, we derive a relative upper bound for recovering the best low-rank approximation of the unknown matrix. Our relative upper bound is tighter than previous additive bounds of other methods if the mass of the target matrix is concentrated on its top eigenspaces, and also implies perfect recovery if it is low-rank. The analysis is built upon the optimality condition of the regularized formulation and existing guarantees for low-rank matrix completion. To the best of our knowledge, this is first time such a relative bound is proved for the regularized formulation of matrix completion. version:1
arxiv-1504-06798 | Overlapping Community Detection by Online Cluster Aggregation | http://arxiv.org/abs/1504.06798 | id:1504.06798 author:Mark Kozdoba, Shie Mannor category:cs.LG cs.SI physics.soc-ph  published:2015-04-26 summary:We present a new online algorithm for detecting overlapping communities. The main ingredients are a modification of an online k-means algorithm and a new approach to modelling overlap in communities. An evaluation on large benchmark graphs shows that the quality of discovered communities compares favorably to several methods in the recent literature, while the running time is significantly improved. version:1
arxiv-1411-6067 | Viewpoints and Keypoints | http://arxiv.org/abs/1411.6067 | id:1411.6067 author:Shubham Tulsiani, Jitendra Malik category:cs.CV  published:2014-11-22 summary:We characterize the problem of pose estimation for rigid objects in terms of determining viewpoint to explain coarse pose and keypoint prediction to capture the finer details. We address both these tasks in two different settings - the constrained setting with known bounding boxes and the more challenging detection setting where the aim is to simultaneously detect and correctly estimate pose of objects. We present Convolutional Neural Network based architectures for these and demonstrate that leveraging viewpoint estimates can substantially improve local appearance based keypoint predictions. In addition to achieving significant improvements over state-of-the-art in the above tasks, we analyze the error modes and effect of object characteristics on performance to guide future efforts towards this goal. version:2
arxiv-1412-8439 | Spy vs. Spy: Rumor Source Obfuscation | http://arxiv.org/abs/1412.8439 | id:1412.8439 author:Giulia Fanti, Peter Kairouz, Sewoong Oh, Pramod Viswanath category:cs.SI cs.LG  published:2014-12-29 summary:Anonymous messaging platforms, such as Secret and Whisper, have emerged as important social media for sharing one's thoughts without the fear of being judged by friends, family, or the public. Further, such anonymous platforms are crucial in nations with authoritarian governments; the right to free expression and sometimes the personal safety of the author of the message depend on anonymity. Whether for fear of judgment or personal endangerment, it is crucial to keep anonymous the identity of the user who initially posted a sensitive message. In this paper, we consider an adversary who observes a snapshot of the spread of a message at a certain time. Recent advances in rumor source detection shows that the existing messaging protocols are vulnerable against such an adversary. We introduce a novel messaging protocol, which we call adaptive diffusion, and show that it spreads the messages fast and achieves a perfect obfuscation of the source when the underlying contact network is an infinite regular tree: all users with the message are nearly equally likely to have been the origin of the message. Experiments on a sampled Facebook network show that it effectively hides the location of the source even when the graph is finite, irregular and has cycles. We further consider a stronger adversarial model where a subset of colluding users track the reception of messages. We show that the adaptive diffusion provides a strong protection of the anonymity of the source even under this scenario. version:3
arxiv-1502-04423 | Exploring Transfer Function Nonlinearity in Echo State Networks | http://arxiv.org/abs/1502.04423 | id:1502.04423 author:Alireza Goudarzi, Alireza Shabani, Darko Stefanovic category:cs.NE  published:2015-02-16 summary:Supralinear and sublinear pre-synaptic and dendritic integration is considered to be responsible for nonlinear computation power of biological neurons, emphasizing the role of nonlinear integration as opposed to nonlinear output thresholding. How, why, and to what degree the transfer function nonlinearity helps biologically inspired neural network models is not fully understood. Here, we study these questions in the context of echo state networks (ESN). ESN is a simple neural network architecture in which a fixed recurrent network is driven with an input signal, and the output is generated by a readout layer from the measurements of the network states. ESN architecture enjoys efficient training and good performance on certain signal-processing tasks, such as system identification and time series prediction. ESN performance has been analyzed with respect to the connectivity pattern in the network structure and the input bias. However, the effects of the transfer function in the network have not been studied systematically. Here, we use an approach tanh on the Taylor expansion of a frequently used transfer function, the hyperbolic tangent function, to systematically study the effect of increasing nonlinearity of the transfer function on the memory, nonlinear capacity, and signal processing performance of ESN. Interestingly, we find that a quadratic approximation is enough to capture the computational power of ESN with tanh function. The results of this study apply to both software and hardware implementation of ESN. version:2
arxiv-1504-02833 | Hierarchical Composition of Memristive Networks for Real-Time Computing | http://arxiv.org/abs/1504.02833 | id:1504.02833 author:Jens Bürger, Alireza Goudarzi, Darko Stefanovic, Christof Teuscher category:cs.ET cond-mat.dis-nn cs.NE  published:2015-04-11 summary:Advances in materials science have led to physical instantiations of self-assembled networks of memristive devices and demonstrations of their computational capability through reservoir computing. Reservoir computing is an approach that takes advantage of collective system dynamics for real-time computing. A dynamical system, called a reservoir, is excited with a time-varying signal and observations of its states are used to reconstruct a desired output signal. However, such a monolithic assembly limits the computational power due to signal interdependency and the resulting correlated readouts. Here, we introduce an approach that hierarchically composes a set of interconnected memristive networks into a larger reservoir. We use signal amplification and restoration to reduce reservoir state correlation, which improves the feature extraction from the input signals. Using the same number of output signals, such a hierarchical composition of heterogeneous small networks outperforms monolithic memristive networks by at least 20% on waveform generation tasks. On the NARMA-10 task, we reduce the error by up to a factor of 2 compared to homogeneous reservoirs with sigmoidal neurons, whereas single memristive networks are unable to produce the correct result. Hierarchical composition is key for solving more complex tasks with such novel nano-scale hardware. version:2
arxiv-1502-00718 | Product Reservoir Computing: Time-Series Computation with Multiplicative Neurons | http://arxiv.org/abs/1502.00718 | id:1502.00718 author:Alireza Goudarzi, Alireza Shabani, Darko Stefanovic category:cs.NE  published:2015-02-03 summary:Echo state networks (ESN), a type of reservoir computing (RC) architecture, are efficient and accurate artificial neural systems for time series processing and learning. An ESN consists of a core of recurrent neural networks, called a reservoir, with a small number of tunable parameters to generate a high-dimensional representation of an input, and a readout layer which is easily trained using regression to produce a desired output from the reservoir states. Certain computational tasks involve real-time calculation of high-order time correlations, which requires nonlinear transformation either in the reservoir or the readout layer. Traditional ESN employs a reservoir with sigmoid or tanh function neurons. In contrast, some types of biological neurons obey response curves that can be described as a product unit rather than a sum and threshold. Inspired by this class of neurons, we introduce a RC architecture with a reservoir of product nodes for time series computation. We find that the product RC shows many properties of standard ESN such as short-term memory and nonlinear capacity. On standard benchmarks for chaotic prediction tasks, the product RC maintains the performance of a standard nonlinear ESN while being more amenable to mathematical analysis. Our study provides evidence that such networks are powerful in highly nonlinear tasks owing to high-order statistics generated by the recurrent product node reservoir. version:2
arxiv-1411-5752 | Hypercolumns for Object Segmentation and Fine-grained Localization | http://arxiv.org/abs/1411.5752 | id:1411.5752 author:Bharath Hariharan, Pablo Arbeláez, Ross Girshick, Jitendra Malik category:cs.CV  published:2014-11-21 summary:Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as feature representation. However, the information in this layer may be too coarse to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation[22], where we improve state-of-the-art from 49.7[22] mean AP^r to 60.0, keypoint localization, where we get a 3.3 point boost over[20] and part labeling, where we show a 6.6 point gain over a strong baseline. version:2
arxiv-1504-06740 | SIFT Vs SURF: Quantifying the Variation in Transformations | http://arxiv.org/abs/1504.06740 | id:1504.06740 author:Siddharth Srivastava category:cs.CV  published:2015-04-25 summary:This paper studies the robustness of SIFT and SURF against different image transforms (rigid body, similarity, affine and projective) by quantitatively analyzing the variations in the extent of transformations. Previous studies have been comparing the two techniques on absolute transformations rather than the specific amount of deformation caused by the transformation. The paper establishes an exhaustive empirical analysis of such deformations and matching capability of SIFT and SURF with variations in matching parameters and the amount of tolerance. This is helpful in choosing the specific use case for applying these techniques. version:1
arxiv-1504-06719 | Adaptive Locally Affine-Invariant Shape Matching | http://arxiv.org/abs/1504.06719 | id:1504.06719 author:Smit Marvaniya, Raj Gupta, Anurag Mittal category:cs.CV  published:2015-04-25 summary:Matching deformable objects using their shapes is an important problem in computer vision since shape is perhaps the most distinguishable characteristic of an object. The problem is difficult due to many factors such as intra-class variations, local deformations, articulations, viewpoint changes and missed and extraneous contour portions due to errors in shape extraction. While small local deformations has been handled in the literature by allowing some leeway in the matching of individual contour points via methods such as Chamfer distance and Hausdorff distance, handling more severe deformations and articulations has been done by applying local geometric corrections such as similarity or affine. However, determining which portions of the shape should be used for the geometric corrections is very hard, although some methods have been tried. In this paper, we address this problem by an efficient search for the group of contour segments to be clustered together for a geometric correction using Dynamic Programming by essentially searching for the segmentations of two shapes that lead to the best matching between them. At the same time, we allow portions of the contours to remain unmatched to handle missing and extraneous contour portions. Experiments indicate that our method outperforms other algorithms, especially when the shapes to be matched are more complex. version:1
arxiv-1504-06701 | A Prior Distribution over Directed Acyclic Graphs for Sparse Bayesian Networks | http://arxiv.org/abs/1504.06701 | id:1504.06701 author:Felix L. Rios, John M. Noble, Timo J. T. Koski category:stat.ML  published:2015-04-25 summary:The main contribution of this article is a new prior distribution over directed acyclic graphs, which gives larger weight to sparse graphs. This distribution is intended for structured Bayesian networks, where the structure is given by an ordered block model. That is, the nodes of the graph are objects which fall into categories (or blocks); the blocks have a natural ordering. The presence of a relationship between two objects is denoted by an arrow, from the object of lower category to the object of higher category. The models considered here were introduced in Kemp et al. (2004) for relational data and extended to multivariate data in Mansinghka et al. (2006). The prior over graph structures presented here has an explicit formula. The number of nodes in each layer of the graph follow a Hoppe Ewens urn model. We consider the situation where the nodes of the graph represent random variables, whose joint probability distribution factorises along the DAG. We describe Monte Carlo schemes for finding the optimal aposteriori structure given a data matrix and compare the performance with Mansinghka et al. (2006) and also with the uniform prior. version:1
arxiv-1504-06681 | Online Convex Optimization Using Predictions | http://arxiv.org/abs/1504.06681 | id:1504.06681 author:Niangjun Chen, Anish Agarwal, Adam Wierman, Siddharth Barman, Lachlan L. H. Andrew category:cs.LG  published:2015-04-25 summary:Making use of predictions is a crucial, but under-explored, area of online algorithms. This paper studies a class of online optimization problems where we have external noisy predictions available. We propose a stochastic prediction error model that generalizes prior models in the learning and stochastic control communities, incorporates correlation among prediction errors, and captures the fact that predictions improve as time passes. We prove that achieving sublinear regret and constant competitive ratio for online algorithms requires the use of an unbounded prediction window in adversarial settings, but that under more realistic stochastic prediction error models it is possible to use Averaging Fixed Horizon Control (AFHC) to simultaneously achieve sublinear regret and constant competitive ratio in expectation using only a constant-sized prediction window. Furthermore, we show that the performance of AFHC is tightly concentrated around its mean. version:1
arxiv-1504-06678 | Differential Recurrent Neural Networks for Action Recognition | http://arxiv.org/abs/1504.06678 | id:1504.06678 author:Vivek Veeriah, Naifan Zhuang, Guo-Jun Qi category:cs.CV  published:2015-04-25 summary:The long short-term memory (LSTM) neural network is capable of processing complex sequential information since it utilizes special gating schemes for learning representations from long input sequences. It has the potential to model any sequential time-series data, where the current hidden state has to be considered in the context of the past hidden states. This property makes LSTM an ideal choice to learn the complex dynamics of various actions. Unfortunately, the conventional LSTMs do not consider the impact of spatio-temporal dynamics corresponding to the given salient motion patterns, when they gate the information that ought to be memorized through time. To address this problem, we propose a differential gating scheme for the LSTM neural network, which emphasizes on the change in information gain caused by the salient motions between the successive frames. This change in information gain is quantified by Derivative of States (DoS), and thus the proposed LSTM model is termed as differential Recurrent Neural Network (dRNN). We demonstrate the effectiveness of the proposed model by automatically recognizing actions from the real-world 2D and 3D human action datasets. Our study is one of the first works towards demonstrating the potential of learning complex time-series representations via high-order derivatives of states. version:1
arxiv-1412-0477 | Recovering Spatiotemporal Correspondence between Deformable Objects by Exploiting Consistent Foreground Motion in Video | http://arxiv.org/abs/1412.0477 | id:1412.0477 author:Luca Del Pero, Susanna Ricco, Rahul Sukthankar, Vittorio Ferrari category:cs.CV  published:2014-12-01 summary:Given unstructured videos of deformable objects, we automatically recover spatiotemporal correspondences to map one object to another (such as animals in the wild). While traditional methods based on appearance fail in such challenging conditions, we exploit consistency in object motion between instances. Our approach discovers pairs of short video intervals where the object moves in a consistent manner and uses these candidates as seeds for spatial alignment. We model the spatial correspondence between the point trajectories on the object in one interval to those in the other using a time-varying Thin Plate Spline deformation model. On a large dataset of tiger and horse videos, our method automatically aligns thousands of pairs of frames to a high accuracy, and outperforms the popular SIFT Flow algorithm. version:2
arxiv-1504-06658 | Inferring Missing Entity Type Instances for Knowledge Base Completion: New Dataset and Methods | http://arxiv.org/abs/1504.06658 | id:1504.06658 author:Arvind Neelakantan, Ming-Wei Chang category:cs.CL stat.ML  published:2015-04-24 summary:Most of previous work in knowledge base (KB) completion has focused on the problem of relation extraction. In this work, we focus on the task of inferring missing entity type instances in a KB, a fundamental task for KB competition yet receives little attention. Due to the novelty of this task, we construct a large-scale dataset and design an automatic evaluation methodology. Our knowledge base completion method uses information within the existing KB and external information from Wikipedia. We show that individual methods trained with a global objective that considers unobserved cells from both the entity and the type side gives consistently higher quality predictions compared to baseline methods. We also perform manual evaluation on a small subset of the data to verify the effectiveness of our knowledge base completion methods and the correctness of our proposed automatic evaluation method. version:1
arxiv-1504-06654 | Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space | http://arxiv.org/abs/1504.06654 | id:1504.06654 author:Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, Andrew McCallum category:cs.CL stat.ML  published:2015-04-24 summary:There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type ignoring polysemy and thus jeopardizing their usefulness for downstream tasks. We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours. version:1
arxiv-1504-06650 | Learning Dictionaries for Named Entity Recognition using Minimal Supervision | http://arxiv.org/abs/1504.06650 | id:1504.06650 author:Arvind Neelakantan, Michael Collins category:cs.CL stat.ML  published:2015-04-24 summary:This paper describes an approach for automatic construction of dictionaries for Named Entity Recognition (NER) using large amounts of unlabeled data and a few seed examples. We use Canonical Correlation Analysis (CCA) to obtain lower dimensional embeddings (representations) for candidate phrases and classify these phrases using a small number of labeled examples. Our method achieves 16.5% and 11.3% F-1 score improvement over co-training on disease and virus NER respectively. We also show that by adding candidate phrase embeddings as features in a sequence tagger gives better performance compared to using word embeddings. version:1
arxiv-1407-4139 | Subjectivity, Bayesianism, and Causality | http://arxiv.org/abs/1407.4139 | id:1407.4139 author:Pedro A. Ortega category:cs.AI stat.ME stat.ML  published:2014-07-15 summary:Bayesian probability theory is one of the most successful frameworks to model reasoning under uncertainty. Its defining property is the interpretation of probabilities as degrees of belief in propositions about the state of the world relative to an inquiring subject. This essay examines the notion of subjectivity by drawing parallels between Lacanian theory and Bayesian probability theory, and concludes that the latter must be enriched with causal interventions to model agency. The central contribution of this work is an abstract model of the subject that accommodates causal interventions in a measure-theoretic formalisation. This formalisation is obtained through a game-theoretic Ansatz based on modelling the inside and outside of the subject as an extensive-form game with imperfect information between two players. Finally, I illustrate the expressiveness of this model with an example of causal induction. version:4
arxiv-1504-06591 | Object Level Deep Feature Pooling for Compact Image Representation | http://arxiv.org/abs/1504.06591 | id:1504.06591 author:Konda Reddy Mopuri, R. Venkatesh Babu category:cs.CV  published:2015-04-24 summary:Convolutional Neural Network (CNN) features have been successfully employed in recent works as an image descriptor for various vision tasks. But the inability of the deep CNN features to exhibit invariance to geometric transformations and object compositions poses a great challenge for image search. In this work, we demonstrate the effectiveness of the objectness prior over the deep CNN features of image regions for obtaining an invariant image representation. The proposed approach represents the image as a vector of pooled CNN features describing the underlying objects. This representation provides robustness to spatial layout of the objects in the scene and achieves invariance to general geometric transformations, such as translation, rotation and scaling. The proposed approach also leads to a compact representation of the scene, making each image occupy a smaller memory footprint. Experiments show that the proposed representation achieves state of the art retrieval results on a set of challenging benchmark image datasets, while maintaining a compact representation. version:1
arxiv-1504-06587 | Semantic Motion Segmentation Using Dense CRF Formulation | http://arxiv.org/abs/1504.06587 | id:1504.06587 author:N. Dinesh Reddy, Prateek Singhal, K. Madhava Krishna category:cs.CV  published:2015-04-24 summary:While the literature has been fairly dense in the areas of scene understanding and semantic labeling there have been few works that make use of motion cues to embellish semantic performance and vice versa. In this paper, we address the problem of semantic motion segmentation, and show how semantic and motion priors augments performance. We pro- pose an algorithm that jointly infers the semantic class and motion labels of an object. Integrating semantic, geometric and optical ow based constraints into a dense CRF-model we infer both the object class as well as motion class, for each pixel. We found improvement in performance using a fully connected CRF as compared to a standard clique-based CRFs. For inference, we use a Mean Field approximation based algorithm. Our method outperforms recently pro- posed motion detection algorithms and also improves the semantic labeling compared to the state-of-the-art Automatic Labeling Environment algorithm on the challenging KITTI dataset especially for object classes such as pedestrians and cars that are critical to an outdoor robotic navigation scenario. version:1
arxiv-1504-06567 | Cultural Event Recognition with Visual ConvNets and Temporal Models | http://arxiv.org/abs/1504.06567 | id:1504.06567 author:Amaia Salvador, Matthias Zeppelzauer, Daniel Manchon-Vizuete, Andrea Calafell, Xavier Giro-i-Nieto category:cs.CV cs.CY  published:2015-04-24 summary:This paper presents our contribution to the ChaLearn Challenge 2015 on Cultural Event Classification. The challenge in this task is to automatically classify images from 50 different cultural events. Our solution is based on the combination of visual features extracted from convolutional neural networks with temporal information using a hierarchical classifier scheme. We extract visual features from the last three fully connected layers of both CaffeNet (pretrained with ImageNet) and our fine tuned version for the ChaLearn challenge. We propose a late fusion strategy that trains a separate low-level SVM on each of the extracted neural codes. The class predictions of the low-level SVMs form the input to a higher level SVM, which gives the final event scores. We achieve our best result by adding a temporal refinement step into our classification scheme, which is applied directly to the output of each low-level SVM. Our approach penalizes high classification scores based on visual features when their time stamp does not match well an event-specific temporal distribution learned from the training and validation data. Our system achieved the second best result in the ChaLearn Challenge 2015 on Cultural Event Classification with a mean average precision of 0.767 on the test set. version:1
arxiv-1503-00783 | Joint calibration of Ensemble of Exemplar SVMs | http://arxiv.org/abs/1503.00783 | id:1503.00783 author:Davide Modolo, Alexander Vezhnevets, Olga Russakovsky, Vittorio Ferrari category:cs.CV  published:2015-03-02 summary:We present a method for calibrating the Ensemble of Exemplar SVMs model. Unlike the standard approach, which calibrates each SVM independently, our method optimizes their joint performance as an ensemble. We formulate joint calibration as a constrained optimization problem and devise an efficient optimization algorithm to find its global optimum. The algorithm dynamically discards parts of the solution space that cannot contain the optimum early on, making the optimization computationally feasible. We experiment with EE-SVM trained on state-of-the-art CNN descriptors. Results on the ILSVRC 2014 and PASCAL VOC 2007 datasets show that (i) our joint calibration procedure outperforms independent calibration on the task of classifying windows as belonging to an object class or not; and (ii) this improved window classifier leads to better performance on the object detection task. version:2
arxiv-1408-2539 | Optimum Statistical Estimation with Strategic Data Sources | http://arxiv.org/abs/1408.2539 | id:1408.2539 author:Yang Cai, Constantinos Daskalakis, Christos H. Papadimitriou category:stat.ML cs.GT cs.LG  published:2014-08-11 summary:We propose an optimum mechanism for providing monetary incentives to the data sources of a statistical estimator such as linear regression, so that high quality data is provided at low cost, in the sense that the sum of payments and estimation error is minimized. The mechanism applies to a broad range of estimators, including linear and polynomial regression, kernel regression, and, under some additional assumptions, ridge regression. It also generalizes to several objectives, including minimizing estimation error subject to budget constraints. Besides our concrete results for regression problems, we contribute a mechanism design framework through which to design and analyze statistical estimators whose examples are supplied by workers with cost for labeling said examples. version:2
arxiv-1504-06553 | A Bayesian approach for structure learning in oscillating regulatory networks | http://arxiv.org/abs/1504.06553 | id:1504.06553 author:D Trejo, AJ Millar, G Sanguinetti category:stat.ML q-bio.QM  published:2015-04-24 summary:Oscillations lie at the core of many biological processes, from the cell cycle, to circadian oscillations and developmental processes. Time-keeping mechanisms are essential to enable organisms to adapt to varying conditions in environmental cycles, from day/night to seasonal. Transcriptional regulatory networks are one of the mechanisms behind these biological oscillations. However, while identifying cyclically expressed genes from time series measurements is relatively easy, determining the structure of the interaction network underpinning the oscillation is a far more challenging problem. Here, we explicitly leverage the oscillatory nature of the transcriptional signals and present a method for reconstructing network interactions tailored to this special but important class of genetic circuits. Our method is based on projecting the signal onto a set of oscillatory basis functions using a Discrete Fourier Transform. We build a Bayesian Hierarchical model within a frequency domain linear model in order to enforce sparsity and incorporate prior knowledge about the network structure. Experiments on real and simulated data show that the method can lead to substantial improvements over competing approaches if the oscillatory assumption is met, and remains competitive also in cases it is not. version:1
arxiv-1504-06544 | Sampling Correctors | http://arxiv.org/abs/1504.06544 | id:1504.06544 author:Clément Canonne, Themis Gouleakis, Ronitt Rubinfeld category:cs.DS cs.LG math.PR  published:2015-04-24 summary:In many situations, sample data is obtained from a noisy or imperfect source. In order to address such corruptions, this paper introduces the concept of a sampling corrector. Such algorithms use structure that the distribution is purported to have, in order to allow one to make "on-the-fly" corrections to samples drawn from probability distributions. These algorithms then act as filters between the noisy data and the end user. We show connections between sampling correctors, distribution learning algorithms, and distribution property testing algorithms. We show that these connections can be utilized to expand the applicability of known distribution learning and property testing algorithms as well as to achieve improved algorithms for those tasks. As a first step, we show how to design sampling correctors using proper learning algorithms. We then focus on the question of whether algorithms for sampling correctors can be more efficient in terms of sample complexity than learning algorithms for the analogous families of distributions. When correcting monotonicity, we show that this is indeed the case when also granted query access to the cumulative distribution function. We also obtain sampling correctors for monotonicity without this stronger type of access, provided that the distribution be originally very close to monotone (namely, at a distance $O(1/\log^2 n)$). In addition to that, we consider a restricted error model that aims at capturing "missing data" corruptions. In this model, we show that distributions that are close to monotone have sampling correctors that are significantly more efficient than achievable by the learning approach. We also consider the question of whether an additional source of independent random bits is required by sampling correctors to implement the correction process. version:1
arxiv-1411-7883 | Articulated motion discovery using pairs of trajectories | http://arxiv.org/abs/1411.7883 | id:1411.7883 author:Luca Del Pero, Susanna Ricco, Rahul Sukthankar, Vittorio Ferrari category:cs.CV  published:2014-11-28 summary:We propose an unsupervised approach for discovering characteristic motion patterns in videos of highly articulated objects performing natural, unscripted behaviors, such as tigers in the wild. We discover consistent patterns in a bottom-up manner by analyzing the relative displacements of large numbers of ordered trajectory pairs through time, such that each trajectory is attached to a different moving part on the object. The pairs of trajectories descriptor relies entirely on motion and is more discriminative than state-of-the-art features that employ single trajectories. Our method generates temporal video intervals, each automatically trimmed to one instance of the discovered behavior, and clusters them by type (e.g., running, turning head, drinking water). We present experiments on two datasets: dogs from YouTube-Objects and a new dataset of National Geographic tiger videos. Results confirm that our proposed descriptor outperforms existing appearance- and trajectory-based descriptors (e.g., HOG and DTFs) on both datasets and enables us to segment unconstrained animal video into intervals containing single behaviors. version:3
arxiv-1504-06507 | Local Variation as a Statistical Hypothesis Test | http://arxiv.org/abs/1504.06507 | id:1504.06507 author:Michael Baltaxe, Peter Meer, Michael Lindenbaum category:cs.CV  published:2015-04-24 summary:The goal of image oversegmentation is to divide an image into several pieces, each of which should ideally be part of an object. One of the simplest and yet most effective oversegmentation algorithms is known as local variation (LV) (Felzenszwalb and Huttenlocher 2004). In this work, we study this algorithm and show that algorithms similar to LV can be devised by applying different statistical models and decisions, thus providing further theoretical justification and a well-founded explanation for the unexpected high performance of the LV approach. Some of these algorithms are based on statistics of natural images and on a hypothesis testing decision; we denote these algorithms probabilistic local variation (pLV). The best pLV algorithm, which relies on censored estimation, presents state-of-the-art results while keeping the same computational complexity of the LV algorithm. version:1
arxiv-1504-06494 | Discriminative Switching Linear Dynamical Systems applied to Physiological Condition Monitoring | http://arxiv.org/abs/1504.06494 | id:1504.06494 author:Konstantinos Georgatzis, Christopher K. I. Williams category:cs.LG  published:2015-04-24 summary:We present a Discriminative Switching Linear Dynamical System (DSLDS) applied to patient monitoring in Intensive Care Units (ICUs). Our approach is based on identifying the state-of-health of a patient given their observed vital signs using a discriminative classifier, and then inferring their underlying physiological values conditioned on this status. The work builds on the Factorial Switching Linear Dynamical System (FSLDS) (Quinn et al., 2009) which has been previously used in a similar setting. The FSLDS is a generative model, whereas the DSLDS is a discriminative model. We demonstrate on two real-world datasets that the DSLDS is able to outperform the FSLDS in most cases of interest, and that an $\alpha$-mixture of the two models achieves higher performance than either of the two models separately. version:1
arxiv-1412-6553 | Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition | http://arxiv.org/abs/1412.6553 | id:1412.6553 author:Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, Victor Lempitsky category:cs.CV cs.LG  published:2014-12-19 summary:We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process. We evaluate this approach on two CNNs and show that it is competitive with previous approaches, leading to higher obtained CPU speedups at the cost of lower accuracy drops for the smaller of the two networks. Thus, for the 36-class character classification CNN, our approach obtains a 8.5x CPU speedup of the whole network with only minor accuracy drop (1% from 91% to 90%). For the standard ImageNet architecture (AlexNet), the approach speeds up the second convolution layer by a factor of 4x at the cost of $1\%$ increase of the overall top-5 classification error. version:3
arxiv-1504-03439 | Image Denoising Using Low Rank Minimization With Modified Noise Estimation | http://arxiv.org/abs/1504.03439 | id:1504.03439 author:Zahid Hussain Shamsi, Hyun Sook Oh, Dai-Gyoung Kim category:cs.CV  published:2015-04-14 summary:Recently, the application of low rank minimization to image denoising has shown remarkable denoising results which are equivalent or better than those of the existing state-of-the-art algorithms. However, due to iterative nature of low rank optimization, estimation of residual noise is an essential requirement after each iteration. Currently, this noise is estimated by using the filtered noise in the previous iteration without considering the geometric structure of the given image. This estimate may be affected in the presence of moderate and severe levels of noise. To obtain a more reliable estimate of residual noise, we propose a modified algorithm (GWNNM) which includes the contribution of the geometric structure of an image to the existing noise estimation. Furthermore, the proposed algorithm exploits the difference of large and small singular values to enhance the edges and textures during the denoising process. Consequently, the proposed modifications achieve significant improvements in the denoising results of the existing low rank optimization algorithms. version:2
arxiv-1504-06434 | Situational Object Boundary Detection | http://arxiv.org/abs/1504.06434 | id:1504.06434 author:Jasper Uijlings, Vittorio Ferrari category:cs.CV  published:2015-04-24 summary:Intuitively, the appearance of true object boundaries varies from image to image. Hence the usual monolithic approach of training a single boundary predictor and applying it to all images regardless of their content is bound to be suboptimal. In this paper we therefore propose situational object boundary detection: We first define a variety of situations and train a specialized object boundary detector for each of them using [Dollar and Zitnick 2013]. Then given a test image, we classify it into these situations using its context, which we model by global image appearance. We apply the corresponding situational object boundary detectors, and fuse them based on the classification probabilities. In experiments on ImageNet, Microsoft COCO, and Pascal VOC 2012 segmentation we show that our situational object boundary detection gives significant improvements over a monolithic approach. Additionally, our method substantially outperforms [Hariharan et al. 2011] on semantic contour detection on their SBD dataset. version:1
arxiv-1503-05034 | $gen$CNN: A Convolutional Architecture for Word Sequence Prediction | http://arxiv.org/abs/1503.05034 | id:1503.05034 author:Mingxuan Wang, Zhengdong Lu, Hang Li, Wenbin Jiang, Qun Liu category:cs.CL  published:2015-03-17 summary:We propose a novel convolutional architecture, named $gen$CNN, for word sequence prediction. Different from previous work on neural network-based language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. Instead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy specifically designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and $n$-best re-ranking in machine translation show that $gen$CNN outperforms the state-of-the-arts with big margins. version:2
arxiv-1411-6228 | From Image-level to Pixel-level Labeling with Convolutional Networks | http://arxiv.org/abs/1411.6228 | id:1411.6228 author:Pedro O. Pinheiro, Ronan Collobert category:cs.CV  published:2014-11-23 summary:We are interested in inferring object segmentation by leveraging only object class information, and by considering only minimal priors on the object segmentation task. This problem could be viewed as a kind of weakly supervised segmentation task, and naturally fits the Multiple Instance Learning (MIL) framework: every training image is known to have (or not) at least one pixel corresponding to the image class label, and the segmentation task can be rewritten as inferring the pixels belonging to the class of the object (given one image, and its object class). We propose a Convolutional Neural Network-based model, which is constrained during training to put more weight on pixels which are important for classifying the image. We show that at test time, the model has learned to discriminate the right pixels well enough, such that it performs very well on an existing segmentation benchmark, by adding only few smoothing priors. Our system is trained using a subset of the Imagenet dataset and the segmentation experiments are performed on the challenging Pascal VOC dataset (with no fine-tuning of the model on Pascal VOC). Our model beats the state of the art results in weakly supervised object segmentation task by a large margin. We also compare the performance of our model with state of the art fully-supervised segmentation approaches. version:3
arxiv-1504-06394 | Social Trust Prediction via Max-norm Constrained 1-bit Matrix Completion | http://arxiv.org/abs/1504.06394 | id:1504.06394 author:Jing Wang, Jie Shen, Huan Xu category:cs.SI cs.LG stat.ML  published:2015-04-24 summary:Social trust prediction addresses the significant problem of exploring interactions among users in social networks. Naturally, this problem can be formulated in the matrix completion framework, with each entry indicating the trustness or distrustness. However, there are two challenges for the social trust problem: 1) the observed data are with sign (1-bit) measurements; 2) they are typically sampled non-uniformly. Most of the previous matrix completion methods do not well handle the two issues. Motivated by the recent progress of max-norm, we propose to solve the problem with a 1-bit max-norm constrained formulation. Since max-norm is not easy to optimize, we utilize a reformulation of max-norm which facilitates an efficient projected gradient decent algorithm. We demonstrate the superiority of our formulation on two benchmark datasets. version:1
arxiv-1504-06391 | On the Stability of Online Language Features: How Much Text do you Need to know a Person? | http://arxiv.org/abs/1504.06391 | id:1504.06391 author:Eben M. Haber category:cs.CL  published:2015-04-24 summary:In recent years, numerous studies have inferred personality and other traits from people's online writing. While these studies are encouraging, more information is needed in order to use these techniques with confidence. How do linguistic features vary across different online media, and how much text is required to have a representative sample for a person? In this paper, we examine several large sets of online, user-generated text, drawn from Twitter, email, blogs, and online discussion forums. We examine and compare population-wide results for the linguistic measure LIWC, and the inferred traits of Big5 Personality and Basic Human Values. We also empirically measure the stability of these traits across different sized samples for each individual. Our results highlight the importance of tuning models to each online medium, and include guidelines for the minimum amount of text required for a representative result. version:1
arxiv-1504-04792 | Visual Recognition Using Directional Distribution Distance | http://arxiv.org/abs/1504.04792 | id:1504.04792 author:Jianxin Wu, Bin-Bin Gao, Guoqing Liu category:cs.CV  published:2015-04-19 summary:In computer vision, an entity such as an image or video is often represented as a set of instance vectors, which can be SIFT, motion, or deep learning feature vectors extracted from different parts of that entity. Thus, it is essential to design efficient and effective methods to compare two sets of instance vectors. Existing methods such as FV, VLAD or Super Vectors have achieved excellent results. However, this paper shows that these methods are designed based on a generative perspective, and a discriminative method can be more effective in categorizing images or videos. The proposed D3 (discriminative distribution distance) method effectively compares two sets as two distributions, and proposes a directional total variation distance (DTVD) to measure how separated are they. Furthermore, a robust classifier-based method is proposed to estimate DTVD robustly. The D3 method is evaluated in action and image recognition tasks and has achieved excellent accuracy and speed. D3 also has a synergy with FV. The combination of D3 and FV has advantages over D3, FV, and VLAD. version:2
arxiv-1409-3879 | Unsupervised learning of clutter-resistant visual representations from natural videos | http://arxiv.org/abs/1409.3879 | id:1409.3879 author:Qianli Liao, Joel Z. Leibo, Tomaso Poggio category:cs.CV cs.LG  published:2014-09-12 summary:Populations of neurons in inferotemporal cortex (IT) maintain an explicit code for object identity that also tolerates transformations of object appearance e.g., position, scale, viewing angle [1, 2, 3]. Though the learning rules are not known, recent results [4, 5, 6] suggest the operation of an unsupervised temporal-association-based method e.g., Foldiak's trace rule [7]. Such methods exploit the temporal continuity of the visual world by assuming that visual experience over short timescales will tend to have invariant identity content. Thus, by associating representations of frames from nearby times, a representation that tolerates whatever transformations occurred in the video may be achieved. Many previous studies verified that such rules can work in simple situations without background clutter, but the presence of visual clutter has remained problematic for this approach. Here we show that temporal association based on large class-specific filters (templates) avoids the problem of clutter. Our system learns in an unsupervised way from natural videos gathered from the internet, and is able to perform a difficult unconstrained face recognition task on natural images: Labeled Faces in the Wild [8]. version:2
arxiv-1504-06366 | Use of Ensembles of Fourier Spectra in Capturing Recurrent Concepts in Data Streams | http://arxiv.org/abs/1504.06366 | id:1504.06366 author:Sripirakas Sakthithasan, Russel Pears, Albert Bifet, Bernhard Pfahringer category:cs.AI cs.LG  published:2015-04-23 summary:In this research, we apply ensembles of Fourier encoded spectra to capture and mine recurring concepts in a data stream environment. Previous research showed that compact versions of Decision Trees can be obtained by applying the Discrete Fourier Transform to accurately capture recurrent concepts in a data stream. However, in highly volatile environments where new concepts emerge often, the approach of encoding each concept in a separate spectrum is no longer viable due to memory overload and thus in this research we present an ensemble approach that addresses this problem. Our empirical results on real world data and synthetic data exhibiting varying degrees of recurrence reveal that the ensemble approach outperforms the single spectrum approach in terms of classification accuracy, memory and execution time. version:1
arxiv-1504-06363 | On the Runtime of Randomized Local Search and Simple Evolutionary Algorithms for Dynamic Makespan Scheduling | http://arxiv.org/abs/1504.06363 | id:1504.06363 author:Frank Neumann, Carsten Witt category:cs.DS cs.NE  published:2015-04-23 summary:Evolutionary algorithms have been frequently used for dynamic optimization problems. With this paper, we contribute to the theoretical understanding of this research area. We present the first computational complexity analysis of evolutionary algorithms for a dynamic variant of a classical combinatorial optimization problem, namely makespan scheduling. We study the model of a strong adversary which is allowed to change one job at regular intervals. Furthermore, we investigate the setting of random changes. Our results show that randomized local search and a simple evolutionary algorithm are very effective in dynamically tracking changes made to the problem instance. version:1
arxiv-1504-06341 | Strategic Teaching and Learning in Games | http://arxiv.org/abs/1504.06341 | id:1504.06341 author:Burkhard C. Schipper category:cs.GT cs.AI cs.LG  published:2015-04-23 summary:It is known that there are uncoupled learning heuristics leading to Nash equilibrium in all finite games. Why should players use such learning heuristics and where could they come from? We show that there is no uncoupled learning heuristic leading to Nash equilibrium in all finite games that a player has an incentive to adopt, that would be evolutionary stable or that could "learn itself". Rather, a player has an incentive to strategically teach such a learning opponent in order secure at least the Stackelberg leader payoff. The impossibility result remains intact when restricted to the classes of generic games, two-player games, potential games, games with strategic complements or 2x2 games, in which learning is known to be "nice". More generally, it also applies to uncoupled learning heuristics leading to correlated equilibria, rationalizable outcomes, iterated admissible outcomes, or minimal curb sets. A possibility result restricted to "strategically trivial" games fails if some generic games outside this class are considered as well. version:1
arxiv-1504-06329 | Analysis of Stopping Active Learning based on Stabilizing Predictions | http://arxiv.org/abs/1504.06329 | id:1504.06329 author:Michael Bloodgood, John Grothendieck category:cs.LG cs.CL stat.ML  published:2015-04-23 summary:Within the natural language processing (NLP) community, active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new NLP systems and technologies. This paper presents the first theoretical analysis of stopping active learning based on stabilizing predictions (SP). The analysis has revealed three elements that are central to the success of the SP method: (1) bounds on Cohen's Kappa agreement between successively trained models impose bounds on differences in F-measure performance of the models; (2) since the stop set does not have to be labeled, it can be made large in practice, helping to guarantee that the results transfer to previously unseen streams of examples at test/application time; and (3) good (low variance) sample estimates of Kappa between successive models can be obtained. Proofs of relationships between the level of Kappa agreement and the difference in performance between consecutive models are presented. Specifically, if the Kappa agreement between two models exceeds a threshold T (where $T>0$), then the difference in F-measure performance between those models is bounded above by $\frac{4(1-T)}{T}$ in all cases. If precision of the positive conjunction of the models is assumed to be $p$, then the bound can be tightened to $\frac{4(1-T)}{(p+1)T}$. version:1
arxiv-1504-06305 | Regularization-free estimation in trace regression with symmetric positive semidefinite matrices | http://arxiv.org/abs/1504.06305 | id:1504.06305 author:Martin Slawski, Ping Li, Matthias Hein category:stat.ML cs.LG stat.ME  published:2015-04-23 summary:Over the past few years, trace regression models have received considerable attention in the context of matrix completion, quantum state tomography, and compressed sensing. Estimation of the underlying matrix from regularization-based approaches promoting low-rankedness, notably nuclear norm regularization, have enjoyed great popularity. In the present paper, we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite (\textsf{spd}) and the design satisfies certain conditions. In this situation, simple least squares estimation subject to an \textsf{spd} constraint may perform as well as regularization-based approaches with a proper choice of the regularization parameter, which entails knowledge of the noise level and/or tuning. By contrast, constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity. version:1
arxiv-1504-06274 | A new approach for physiological time series | http://arxiv.org/abs/1504.06274 | id:1504.06274 author:Dong Mao, Yang Wang, Qiang Wu category:cs.LG stat.ML  published:2015-04-23 summary:We developed a new approach for the analysis of physiological time series. An iterative convolution filter is used to decompose the time series into various components. Statistics of these components are extracted as features to characterize the mechanisms underlying the time series. Motivated by the studies that show many normal physiological systems involve irregularity while the decrease of irregularity usually implies the abnormality, the statistics for "outliers" in the components are used as features measuring irregularity. Support vector machines are used to select the most relevant features that are able to differentiate the time series from normal and abnormal systems. This new approach is successfully used in the study of congestive heart failure by heart beat interval time series. version:1
arxiv-1504-06266 | Evolving Fuzzy Image Segmentation with Self-Configuration | http://arxiv.org/abs/1504.06266 | id:1504.06266 author:Ahmed Othman, Hamid R. Tizhoosh, Farzad Khalvati category:cs.CV  published:2015-04-23 summary:Current image segmentation techniques usually require that the user tune several parameters in order to obtain maximum segmentation accuracy, a computationally inefficient approach, especially when a large number of images must be processed sequentially in daily practice. The use of evolving fuzzy systems for designing a method that automatically adjusts parameters to segment medical images according to the quality expectation of expert users has been proposed recently (Evolving fuzzy image segmentation EFIS). However, EFIS suffers from a few limitations when used in practice mainly due to some fixed parameters. For instance, EFIS depends on auto-detection of the object of interest for feature calculation, a task that is highly application-dependent. This shortcoming limits the applicability of EFIS, which was proposed with the ultimate goal of offering a generic but adjustable segmentation scheme. In this paper, a new version of EFIS is proposed to overcome these limitations. The new EFIS, called self-configuring EFIS (SC-EFIS), uses available training data to self-estimate the parameters that are fixed in EFIS. As well, the proposed SC-EFIS relies on a feature selection process that does not require auto-detection of an ROI. The proposed SC-EFIS was evaluated using the same segmentation algorithms and the same dataset as for EFIS. The results show that SC-EFIS can provide the same results as EFIS but with a higher level of automation. version:1
arxiv-1412-7755 | Multiple Object Recognition with Visual Attention | http://arxiv.org/abs/1412.7755 | id:1412.7755 author:Jimmy Ba, Volodymyr Mnih, Koray Kavukcuoglu category:cs.LG cs.CV cs.NE  published:2014-12-24 summary:We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation. version:2
arxiv-1504-06243 | Person Re-identification with Correspondence Structure Learning | http://arxiv.org/abs/1504.06243 | id:1504.06243 author:Yang Shen, Weiyao Lin, Junchi Yan, Mingliang Xu, Jianxin Wu, Jingdong Wang category:cs.CV  published:2015-04-23 summary:This paper addresses the problem of handling spatial misalignments due to camera-view changes or human-pose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images. We further introduce a global-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Experimental results on various datasets demonstrate the effectiveness of our approach. version:1
arxiv-1504-06206 | An Elastic Image Registration Approach for Wireless Capsule Endoscope Localization | http://arxiv.org/abs/1504.06206 | id:1504.06206 author:Isabel N. Figueiredo, Carlos Leal, Luís Pinto, Pedro N. Figueiredo, Richard Tsai category:cs.CV  published:2015-04-23 summary:Wireless Capsule Endoscope (WCE) is an innovative imaging device that permits physicians to examine all the areas of the Gastrointestinal (GI) tract. It is especially important for the small intestine, where traditional invasive endoscopies cannot reach. Although WCE represents an extremely important advance in medical imaging, a major drawback that remains unsolved is the WCE precise location in the human body during its operating time. This is mainly due to the complex physiological environment and the inherent capsule effects during its movement. When an abnormality is detected, in the WCE images, medical doctors do not know precisely where this abnormality is located relative to the intestine and therefore they can not proceed efficiently with the appropriate therapy. The primary objective of the present paper is to give a contribution to WCE localization, using image-based methods. The main focus of this work is on the description of a multiscale elastic image registration approach, its experimental application on WCE videos, and comparison with a multiscale affine registration. The proposed approach includes registrations that capture both rigid-like and non-rigid deformations, due respectively to the rigid-like WCE movement and the elastic deformation of the small intestine originated by the GI peristaltic movement. Under this approach a qualitative information about the WCE speed can be obtained, as well as the WCE location and orientation via projective geometry. The results of the experimental tests with real WCE video frames show the good performance of the proposed approach, when elastic deformations of the small intestine are involved in successive frames, and its superiority with respect to a multiscale affine image registration, which accounts for rigid-like deformations only and discards elastic deformations. version:1
arxiv-1412-1123 | DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection | http://arxiv.org/abs/1412.1123 | id:1412.1123 author:Gedas Bertasius, Jianbo Shi, Lorenzo Torresani category:cs.CV  published:2014-12-02 summary:Contour detection has been a fundamental component in many image segmentation and object detection systems. Most previous work utilizes low-level features such as texture or saliency to detect contours and then use them as cues for a higher-level task such as object detection. However, we claim that recognizing objects and predicting contours are two mutually related tasks. Contrary to traditional approaches, we show that we can invert the commonly established pipeline: instead of detecting contours with low-level cues for a higher-level recognition task, we exploit object-related features as high-level cues for contour detection. We achieve this goal by means of a multi-scale deep network that consists of five convolutional layers and a bifurcated fully-connected sub-network. The section from the input layer to the fifth convolutional layer is fixed and directly lifted from a pre-trained network optimized over a large-scale object classification task. This section of the network is applied to four different scales of the image input. These four parallel and identical streams are then attached to a bifurcated sub-network consisting of two independently-trained branches. One branch learns to predict the contour likelihood (with a classification objective) whereas the other branch is trained to learn the fraction of human labelers agreeing about the contour presence at a given point (with a regression criterion). We show that without any feature engineering our multi-scale deep learning approach achieves state-of-the-art results in contour detection. version:3
arxiv-1504-06165 | Collectively Embedding Multi-Relational Data for Predicting User Preferences | http://arxiv.org/abs/1504.06165 | id:1504.06165 author:Nitish Gupta, Sameer Singh category:cs.LG cs.IR  published:2015-04-23 summary:Matrix factorization has found incredible success and widespread application as a collaborative filtering based approach to recommendations. Unfortunately, incorporating additional sources of evidence, especially ones that are incomplete and noisy, is quite difficult to achieve in such models, however, is often crucial for obtaining further gains in accuracy. For example, additional information about businesses from reviews, categories, and attributes should be leveraged for predicting user preferences, even though this information is often inaccurate and partially-observed. Instead of creating customized methods that are specific to each type of evidences, in this paper we present a generic approach to factorization of relational data that collectively models all the relations in the database. By learning a set of embeddings that are shared across all the relations, the model is able to incorporate observed information from all the relations, while also predicting all the relations of interest. Our evaluation on multiple Amazon and Yelp datasets demonstrates effective utilization of additional information for held-out preference prediction, but further, we present accurate models even for the cold-starting businesses and products for which we do not observe any ratings or reviews. We also illustrate the capability of the model in imputing missing information and jointly visualizing words, categories, and attribute factors. version:1
arxiv-1504-06151 | Robust Principal Component Analysis on Graphs | http://arxiv.org/abs/1504.06151 | id:1504.06151 author:Nauman Shahid, Vassilis Kalofolias, Xavier Bresson, Michael Bronstein, Pierre Vandergheynst category:cs.CV  published:2015-04-23 summary:Principal Component Analysis (PCA) is the most widely used tool for linear dimensionality reduction and clustering. Still it is highly sensitive to outliers and does not scale well with respect to the number of data samples. Robust PCA solves the first issue with a sparse penalty term. The second issue can be handled with the matrix factorization model, which is however non-convex. Besides, PCA based clustering can also be enhanced by using a graph of data similarity. In this article, we introduce a new model called "Robust PCA on Graphs" which incorporates spectral graph regularization into the Robust PCA framework. Our proposed model benefits from 1) the robustness of principal components to occlusions and missing values, 2) enhanced low-rank recovery, 3) improved clustering property due to the graph smoothness assumption on the low-rank matrix, and 4) convexity of the resulting optimization problem. Extensive experiments on 8 benchmark, 3 video and 2 artificial datasets with corruptions clearly reveal that our model outperforms 10 other state-of-the-art models in its clustering and low-rank recovery tasks. version:1
arxiv-1504-06133 | Sparse Radial Sampling LBP for Writer Identification | http://arxiv.org/abs/1504.06133 | id:1504.06133 author:Anguelos Nicolaou, Andrew D. Bagdanov, Marcus Liwicki, Dimosthenis Karatzas category:cs.CV  published:2015-04-23 summary:In this paper we present the use of Sparse Radial Sampling Local Binary Patterns, a variant of Local Binary Patterns (LBP) for text-as-texture classification. By adapting and extending the standard LBP operator to the particularities of text we get a generic text-as-texture classification scheme and apply it to writer identification. In experiments on CVL and ICDAR 2013 datasets, the proposed feature-set demonstrates State-Of-the-Art (SOA) performance. Among the SOA, the proposed method is the only one that is based on dense extraction of a single local feature descriptor. This makes it fast and applicable at the earliest stages in a DIA pipeline without the need for segmentation, binarization, or extraction of multiple features. version:1
arxiv-1501-00642 | Unsupervised Feature Learning for Dense Correspondences across Scenes | http://arxiv.org/abs/1501.00642 | id:1501.00642 author:Chao Zhang, Chunhua Shen, Tingzhi Shen category:cs.CV  published:2015-01-04 summary:We propose a fast, accurate matching method for estimating dense pixel correspondences across scenes. It is a challenging problem to estimate dense pixel correspondences between images depicting different scenes or instances of the same object category. While most such matching methods rely on hand-crafted features such as SIFT, we learn features from a large amount of unlabeled image patches using unsupervised learning. Pixel-layer features are obtained by encoding over the dictionary, followed by spatial pooling to obtain patch-layer features. The learned features are then seamlessly embedded into a multi-layer match- ing framework. We experimentally demonstrate that the learned features, together with our matching model, outperforms state-of-the-art methods such as the SIFT flow, coherency sensitive hashing and the recent deformable spatial pyramid matching methods both in terms of accuracy and computation efficiency. Furthermore, we evaluate the performance of a few different dictionary learning and feature encoding methods in the proposed pixel correspondences estimation framework, and analyse the impact of dictionary learning and feature encoding with respect to the final matching performance. version:2
arxiv-1504-06080 | svcR: An R Package for Support Vector Clustering improved with Geometric Hashing applied to Lexical Pattern Discovery | http://arxiv.org/abs/1504.06080 | id:1504.06080 author:Nicolas Turenne category:cs.LG cs.CL  published:2015-04-23 summary:We present a new R package which takes a numerical matrix format as data input, and computes clusters using a support vector clustering method (SVC). We have implemented an original 2D-grid labeling approach to speed up cluster extraction. In this sense, SVC can be seen as an efficient cluster extraction if clusters are separable in a 2-D map. Secondly we showed that this SVC approach using a Jaccard-Radial base kernel can help to classify well enough a set of terms into ontological classes and help to define regular expression rules for information extraction in documents; our case study concerns a set of terms and documents about developmental and molecular biology. version:1
arxiv-1504-06078 | x.ent: R Package for Entities and Relations Extraction based on Unsupervised Learning and Document Structure | http://arxiv.org/abs/1504.06078 | id:1504.06078 author:Nicolas Turenne, Tien Phan category:cs.CL cs.AI  published:2015-04-23 summary:Relation extraction with accurate precision is still a challenge when processing full text databases. We propose an approach based on cooccurrence analysis in each document for which we used document organization to improve accuracy of relation extraction. This approach is implemented in a R package called \emph{x.ent}. Another facet of extraction relies on use of extracted relation into a querying system for expert end-users. Two datasets had been used. One of them gets interest from specialists of epidemiology in plant health. For this dataset usage is dedicated to plant-disease exploration through agricultural information news. An open-data platform exploits exports from \emph{x.ent} and is publicly available. version:1
arxiv-1504-06077 | Open Data Platform for Knowledge Access in Plant Health Domain : VESPA Mining | http://arxiv.org/abs/1504.06077 | id:1504.06077 author:Nicolas Turenne, Mathieu Andro, Roselyne Corbière, Tien T. Phan category:cs.IR cs.CL  published:2015-04-23 summary:Important data are locked in ancient literature. It would be uneconomic to produce these data again and today or to extract them without the help of text mining technologies. Vespa is a text mining project whose aim is to extract data on pest and crops interactions, to model and predict attacks on crops, and to reduce the use of pesticides. A few attempts proposed an agricultural information access. Another originality of our work is to parse documents with a dependency of the document architecture. version:1
arxiv-1406-4729 | Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition | http://arxiv.org/abs/1406.4729 | id:1406.4729 author:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun category:cs.CV  published:2014-06-18 summary:Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image. This requirement is "artificial" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, "spatial pyramid pooling", to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102x faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition. version:4
arxiv-1504-06066 | Object Detection Networks on Convolutional Feature Maps | http://arxiv.org/abs/1504.06066 | id:1504.06066 author:Shaoqing Ren, Kaiming He, Ross Girshick, Xiangyu Zhang, Jian Sun category:cs.CV  published:2015-04-23 summary:Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep ConvNet architectures. The object classifier, however, has not received much attention and most state-of-the-art systems (like R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important. We take inspiration from traditional object classifiers, such as DPM, and experiment with deep networks that have part-like filters and reason over latent variables. We discover that on pre-trained convolutional feature maps, even randomly initialized deep classifiers produce excellent results, while the improvement due to fine-tuning is secondary; on HOG features, deep classifiers outperform DPMs and produce the best HOG-only results without external data. We believe these findings provide new insight for developing object detection systems. Our framework, called Networks on Convolutional feature maps (NoC), achieves outstanding results on the PASCAL VOC 2007 (73.3% mAP) and 2012 (68.8% mAP) benchmarks. version:1
arxiv-1504-06055 | Understanding and Diagnosing Visual Tracking Systems | http://arxiv.org/abs/1504.06055 | id:1504.06055 author:Naiyan Wang, Jianping Shi, Dit-Yan Yeung, Jiaya Jia category:cs.CV  published:2015-04-23 summary:Several benchmark datasets for visual tracking research have been proposed in recent years. Despite their usefulness, whether they are sufficient for understanding and diagnosing the strengths and weaknesses of different trackers remains questionable. To address this issue, we propose a framework by breaking a tracker down into five constituent parts, namely, motion model, feature extractor, observation model, model updater, and ensemble post-processor. We then conduct ablative experiments on each component to study how it affects the overall result. Surprisingly, our findings are discrepant with some common beliefs in the visual tracking research community. We find that the feature extractor plays the most important role in a tracker. On the other hand, although the observation model is the focus of many studies, we find that it often brings no significant improvement. Moreover, the motion model and model updater contain many details that could affect the result. Also, the ensemble post-processor can improve the result substantially when the constituent trackers have high diversity. Based on our findings, we put together some very elementary building blocks to give a basic tracker which is competitive in performance to the state-of-the-art trackers. We believe our framework can provide a solid baseline when conducting controlled experiments for visual tracking research. version:1
arxiv-1501-04587 | Transferring Rich Feature Hierarchies for Robust Visual Tracking | http://arxiv.org/abs/1501.04587 | id:1501.04587 author:Naiyan Wang, Siyi Li, Abhinav Gupta, Dit-Yan Yeung category:cs.CV cs.NE  published:2015-01-19 summary:Convolutional neural network (CNN) models have demonstrated great success in various computer vision tasks including image classification and object detection. However, some equally important tasks such as visual tracking remain relatively unexplored. We believe that a major hurdle that hinders the application of CNN to visual tracking is the lack of properly labeled training data. While existing applications that liberate the power of CNN often need an enormous amount of training data in the order of millions, visual tracking applications typically have only one labeled example in the first frame of each video. We address this research issue here by pre-training a CNN offline and then transferring the rich feature hierarchies learned to online tracking. The CNN is also fine-tuned during online tracking to adapt to the appearance of the tracked target specified in the first video frame. To fit the characteristics of object tracking, we first pre-train the CNN to recognize what is an object, and then propose to generate a probability map instead of producing a simple class label. Using two challenging open benchmarks for performance evaluation, our proposed tracker has demonstrated substantial improvement over other state-of-the-art trackers. version:2
arxiv-1504-06043 | Stability of Stochastic Approximations with `Controlled Markov' Noise and Temporal Difference Learning | http://arxiv.org/abs/1504.06043 | id:1504.06043 author:Arunselvan Ramaswamy, Shalabh Bhatnagar category:cs.SY stat.ML  published:2015-04-23 summary:In this paper we present a `stability theorem' for stochastic approximation (SA) algorithms with `controlled Markov' noise. Such algorithms were first studied by Borkar in 2006. Specifically, sufficient conditions are presented which guarantee the stability of the iterates. Further, under these conditions the iterates are shown to track a solution to the differential inclusion defined in terms of the ergodic occupation measures associated with the `controlled Markov' process. As an application to our main result we present an improvement to a general form of temporal difference learning algorithms. Specifically, we present sufficient conditions for their stability and convergence using our framework. This paper builds on the works of Borkar as well as Benveniste, Metivier and Priouret. version:1
arxiv-1504-06036 | Edge Detection Based on Global and Local Parameters of the Image | http://arxiv.org/abs/1504.06036 | id:1504.06036 author:Andrew F. C. Brustolin category:cs.CV I.4.6  published:2015-04-23 summary:This paper presents an edge detection method based on global and local parameters of the image, which produces satisfactory results on the edge detection of complex images and has a simple structure for execution. The local and global parameters of the image are arithmetic means and standard deviations, the former acquired from a three sized window representing five pixels, the latter acquired from the entire row or column. We obtain the differences of grayscale intensities between two adjacent pixels and the sum of the modulus of these differences from the horizontal and vertical scans of the image. Using these obtained values, we calculate the local and global parameters. After the gathering of the local and global parameters, we compare each sum of the modulus of differences with its own local and global parameter. In the case of the comparison is true, the consecutive pixel to the modulus sum of differences index is marked as an edge. We present the results of the tests with grayscale images using different parameters and discuss the advantages and disadvantages of each parameter value and algorithm structure chosen on the edge processing. There is a comparison of results between this papers detector and Canny, where we evaluate the quality of the presented detector. version:1
arxiv-1504-03068 | Review Mining for Feature Based Opinion Summarization and Visualization | http://arxiv.org/abs/1504.03068 | id:1504.03068 author:Ahmad Kamal category:cs.IR cs.CL  published:2015-04-13 summary:The application and usage of opinion mining, especially for business intelligence, product recommendation, targeted marketing etc. have fascinated many research attentions around the globe. Various research efforts attempted to mine opinions from customer reviews at different levels of granularity, including word-, sentence-, and document-level. However, development of a fully automatic opinion mining and sentiment analysis system is still elusive. Though the development of opinion mining and sentiment analysis systems are getting momentum, most of them attempt to perform document-level sentiment analysis, classifying a review document as positive, negative, or neutral. Such document-level opinion mining approaches fail to provide insight about users sentiment on individual features of a product or service. Therefore, it seems to be a great help for both customers and manufacturers, if the reviews could be processed at a finer-grained level and presented in a summarized form through some visual means, highlighting individual features of a product and users sentiment expressed over them. In this paper, the design of a unified opinion mining and sentiment analysis framework is presented at the intersection of both machine learning and natural language processing approaches. Also, design of a novel feature-level review summarization scheme is proposed to visualize mined features, opinions and their polarity values in a comprehendible way. version:2
arxiv-1504-06026 | Graphical Fermat's Principle and Triangle-Free Graph Estimation | http://arxiv.org/abs/1504.06026 | id:1504.06026 author:Junwei Lu, Han Liu category:stat.ML  published:2015-04-23 summary:We consider the problem of estimating undirected triangle-free graphs of high dimensional distributions. Triangle-free graphs form a rich graph family which allows arbitrary loopy structures but 3-cliques. For inferential tractability, we propose a graphical Fermat's principle to regularize the distribution family. Such principle enforces the existence of a distribution-dependent pseudo-metric such that any two nodes have a smaller distance than that of two other nodes who have a geodesic path include these two nodes. Guided by this principle, we show that a greedy strategy is able to recover the true graph. The resulting algorithm only requires a pairwise distance matrix as input and is computationally even more efficient than calculating the minimum spanning tree. We consider graph estimation problems under different settings, including discrete and nonparametric distribution families. Thorough numerical results are provided to illustrate the usefulness of the proposed method. version:1
arxiv-1504-05277 | Deep Spatial Pyramid: The Devil is Once Again in the Details | http://arxiv.org/abs/1504.05277 | id:1504.05277 author:Bin-Bin Gao, Xiu-Shen Wei, Jianxin Wu, Weiyao Lin category:cs.CV  published:2015-04-21 summary:In this paper we show that by carefully making good choices for various detailed but important factors in a visual recognition framework using deep learning features, one can achieve a simple, efficient, yet highly accurate image classification system. We first list 5 important factors, based on both existing researches and ideas proposed in this paper. These important detailed factors include: 1) $\ell_2$ matrix normalization is more effective than unnormalized or $\ell_2$ vector normalization, 2) the proposed natural deep spatial pyramid is very effective, and 3) a very small $K$ in Fisher Vectors surprisingly achieves higher accuracy than normally used large $K$ values. Along with other choices (convolutional activations and multiple scales), the proposed DSP framework is not only intuitive and efficient, but also achieves excellent classification accuracy on many benchmark datasets. For example, DSP's accuracy on SUN397 is 59.78%, significantly higher than previous state-of-the-art (53.86%). version:2
arxiv-1504-05994 | On the relation between Gaussian process quadratures and sigma-point methods | http://arxiv.org/abs/1504.05994 | id:1504.05994 author:Simo Särkkä, Jouni Hartikainen, Lennart Svensson, Fredrik Sandblom category:stat.ME math.DS stat.ML  published:2015-04-22 summary:This article is concerned with Gaussian process quadratures, which are numerical integration methods based on Gaussian process regression methods, and sigma-point methods, which are used in advanced non-linear Kalman filtering and smoothing algorithms. We show that many sigma-point methods can be interpreted as Gaussian quadrature based methods with suitably selected covariance functions. We show that this interpretation also extends to more general multivariate Gauss--Hermite integration methods and related spherical cubature rules. Additionally, we discuss different criteria for selecting the sigma-point locations: exactness for multivariate polynomials up to a given order, minimum average error, and quasi-random point sets. The performance of the different methods is tested in numerical experiments. version:1
arxiv-1503-04567 | Learning Mixed Membership Community Models in Social Tagging Networks through Tensor Methods | http://arxiv.org/abs/1503.04567 | id:1503.04567 author:Anima Anandkumar, Hanie Sedghi category:cs.LG cs.SI stat.ML  published:2015-03-16 summary:Community detection in graphs has been extensively studied both in theory and in applications. However, detecting communities in hypergraphs is more challenging. In this paper, we propose a tensor decomposition approach for guaranteed learning of communities in a special class of hypergraphs modeling social tagging systems or folksonomies. A folksonomy is a tripartite 3-uniform hypergraph consisting of (user, tag, resource) hyperedges. We posit a probabilistic mixed membership community model, and prove that the tensor method consistently learns the communities under efficient sample complexity and separation requirements. version:2
arxiv-1412-0650 | A review of "Mem-computing NP-complete problems in polynomial time using polynomial resources" (arXiv:1411.4798) | http://arxiv.org/abs/1412.0650 | id:1412.0650 author:Igor L. Markov category:cs.ET cs.NE  published:2014-11-29 summary:The reviewed paper describes an analog device that empirically solves small instances of the NP-complete Subset Sum Problem (SSP). The authors claim that this device can solve the SSP in polynomial time using polynomial space, in principle, and observe no exponential scaling in resource requirements. We point out that (a) the properties ascribed by the authors to their device are insufficient to solve NP-complete problems in poly-time, (b) runtime analysis offered does not cover the spectral measurement step, (c) the overall technique requires exponentially increasing resources when scaled up because of the spectral measurement step. version:3
arxiv-1311-5954 | Robust Vertex Classification | http://arxiv.org/abs/1311.5954 | id:1311.5954 author:Li Chen, Cencheng Shen, Joshua Vogelstein, Carey Priebe category:stat.ML  published:2013-11-23 summary:For random graphs distributed according to stochastic blockmodels, a special case of latent position graphs, adjacency spectral embedding followed by appropriate vertex classification is asymptotically Bayes optimal; but this approach requires knowledge of and critically depends on the model dimension. In this paper, we propose a sparse representation vertex classifier which does not require information about the model dimension. This classifier represents a test vertex as a sparse combination of the vertices in the training set and uses the recovered coefficients to classify the test vertex. We prove consistency of our proposed classifier for stochastic blockmodels, and demonstrate that the sparse representation classifier can predict vertex labels with higher accuracy than adjacency spectral embedding approaches via both simulation studies and real data experiments. Our results demonstrate the robustness and effectiveness of our proposed vertex classifier when the model dimension is unknown. version:2
arxiv-1502-02606 | The Power of Randomization: Distributed Submodular Maximization on Massive Datasets | http://arxiv.org/abs/1502.02606 | id:1502.02606 author:Rafael da Ponte Barbosa, Alina Ene, Huy L. Nguyen, Justin Ward category:cs.LG cs.AI cs.DC  published:2015-02-09 summary:A wide variety of problems in machine learning, including exemplar clustering, document summarization, and sensor placement, can be cast as constrained submodular maximization problems. Unfortunately, the resulting submodular optimization problems are often too large to be solved on a single machine. We develop a simple distributed algorithm that is embarrassingly parallel and it achieves provable, constant factor, worst-case approximation guarantees. In our experiments, we demonstrate its efficiency in large problems with different kinds of constraints with objective values always close to what is achievable in the centralized setting. version:2
arxiv-1402-5284 | Convergence results for projected line-search methods on varieties of low-rank matrices via Łojasiewicz inequality | http://arxiv.org/abs/1402.5284 | id:1402.5284 author:Reinhold Schneider, André Uschmajew category:math.OC cs.LG math.NA  published:2014-02-21 summary:The aim of this paper is to derive convergence results for projected line-search methods on the real-algebraic variety $\mathcal{M}_{\le k}$ of real $m \times n$ matrices of rank at most $k$. Such methods extend Riemannian optimization methods, which are successfully used on the smooth manifold $\mathcal{M}_k$ of rank-$k$ matrices, to its closure by taking steps along gradient-related directions in the tangent cone, and afterwards projecting back to $\mathcal{M}_{\le k}$. Considering such a method circumvents the difficulties which arise from the nonclosedness and the unbounded curvature of $\mathcal{M}_k$. The pointwise convergence is obtained for real-analytic functions on the basis of a \L{}ojasiewicz inequality for the projection of the antigradient to the tangent cone. If the derived limit point lies on the smooth part of $\mathcal{M}_{\le k}$, i.e. in $\mathcal{M}_k$, this boils down to more or less known results, but with the benefit that asymptotic convergence rate estimates (for specific step-sizes) can be obtained without an a priori curvature bound, simply from the fact that the limit lies on a smooth manifold. At the same time, one can give a convincing justification for assuming critical points to lie in $\mathcal{M}_k$: if $X$ is a critical point of $f$ on $\mathcal{M}_{\le k}$, then either $X$ has rank $k$, or $\nabla f(X) = 0$. version:3
arxiv-1504-05880 | Spectral Norm of Random Kernel Matrices with Applications to Privacy | http://arxiv.org/abs/1504.05880 | id:1504.05880 author:Shiva Prasad Kasiviswanathan, Mark Rudelson category:stat.ML cs.CR cs.LG F.2.1  published:2015-04-22 summary:Kernel methods are an extremely popular set of techniques used for many important machine learning and data analysis applications. In addition to having good practical performances, these methods are supported by a well-developed theory. Kernel methods use an implicit mapping of the input data into a high dimensional feature space defined by a kernel function, i.e., a function returning the inner product between the images of two data points in the feature space. Central to any kernel method is the kernel matrix, which is built by evaluating the kernel function on a given sample dataset. In this paper, we initiate the study of non-asymptotic spectral theory of random kernel matrices. These are n x n random matrices whose (i,j)th entry is obtained by evaluating the kernel function on $x_i$ and $x_j$, where $x_1,...,x_n$ are a set of n independent random high-dimensional vectors. Our main contribution is to obtain tight upper bounds on the spectral norm (largest eigenvalue) of random kernel matrices constructed by commonly used kernel functions based on polynomials and Gaussian radial basis. As an application of these results, we provide lower bounds on the distortion needed for releasing the coefficients of kernel ridge regression under attribute privacy, a general privacy notion which captures a large class of privacy definitions. Kernel ridge regression is standard method for performing non-parametric regression that regularly outperforms traditional regression approaches in various domains. Our privacy distortion lower bounds are the first for any kernel technique, and our analysis assumes realistic scenarios for the input, unlike all previous lower bounds for other release problems which only hold under very restrictive input settings. version:1
arxiv-1504-05854 | On-the-fly Approximation of Multivariate Total Variation Minimization | http://arxiv.org/abs/1504.05854 | id:1504.05854 author:Jordan Frecon, Nelly Pustelnik, Patrice Abry, Laurent Condat category:cs.LG cs.NA math.OC  published:2015-04-22 summary:In the context of change-point detection, addressed by Total Variation minimization strategies, an efficient on-the-fly algorithm has been designed leading to exact solutions for univariate data. In this contribution, an extension of such an on-the-fly strategy to multivariate data is investigated. The proposed algorithm relies on the local validation of the Karush-Kuhn-Tucker conditions on the dual problem. Showing that the non-local nature of the multivariate setting precludes to obtain an exact on-the-fly solution, we devise an on-the-fly algorithm delivering an approximate solution, whose quality is controlled by a practitioner-tunable parameter, acting as a trade-off between quality and computational cost. Performance assessment shows that high quality solutions are obtained on-the-fly while benefiting of computational costs several orders of magnitude lower than standard iterative procedures. The proposed algorithm thus provides practitioners with an efficient multivariate change-point detection on-the-fly procedure. version:1
arxiv-1504-05811 | Learning of Behavior Trees for Autonomous Agents | http://arxiv.org/abs/1504.05811 | id:1504.05811 author:Michele Colledanchise, Ramviyas Parasuraman, Petter Ögren category:cs.RO cs.AI cs.LG  published:2015-04-22 summary:Definition of an accurate system model for Automated Planner (AP) is often impractical, especially for real-world problems. Conversely, off-the-shelf planners fail to scale up and are domain dependent. These drawbacks are inherited from conventional transition systems such as Finite State Machines (FSMs) that describes the action-plan execution generated by the AP. On the other hand, Behavior Trees (BTs) represent a valid alternative to FSMs presenting many advantages in terms of modularity, reactiveness, scalability and domain-independence. In this paper, we propose a model-free AP framework using Genetic Programming (GP) to derive an optimal BT for an autonomous agent to achieve a given goal in unknown (but fully observable) environments. We illustrate the proposed framework using experiments conducted with an open source benchmark Mario AI for automated generation of BTs that can play the game character Mario to complete a certain level at various levels of difficulty to include enemies and obstacles. version:1
arxiv-1504-05809 | LOAD: Local Orientation Adaptive Descriptor for Texture and Material Classification | http://arxiv.org/abs/1504.05809 | id:1504.05809 author:Xianbiao Qi, Guoying Zhao, Linlin Shen, Qingquan Li, Matti Pietikainen category:cs.CV  published:2015-04-22 summary:In this paper, we propose a novel local feature, called Local Orientation Adaptive Descriptor (LOAD), to capture regional texture in an image. In LOAD, we proposed to define point description on an Adaptive Coordinate System (ACS), adopt a binary sequence descriptor to capture relationships between one point and its neighbors and use multi-scale strategy to enhance the discriminative power of the descriptor. The proposed LOAD enjoys not only discriminative power to capture the texture information, but also has strong robustness to illumination variation and image rotation. Extensive experiments on benchmark data sets of texture classification and real-world material recognition show that the proposed LOAD yields the state-of-the-art performance. It is worth to mention that we achieve a 65.4\% classification accuracy-- which is, to the best of our knowledge, the highest record by far --on Flickr Material Database by using a single feature. Moreover, by combining LOAD with the feature extracted by Convolutional Neural Networks (CNN), we obtain significantly better performance than both the LOAD and CNN. This result confirms that the LOAD is complementary to the learning-based features. version:1
arxiv-1504-05767 | Rounding Methods for Neural Networks with Low Resolution Synaptic Weights | http://arxiv.org/abs/1504.05767 | id:1504.05767 author:Lorenz K. Muller, Giacomo Indiveri category:cs.NE  published:2015-04-22 summary:Neural network algorithms simulated on standard computing platforms typically make use of high resolution weights, with floating-point notation. However, for dedicated hardware implementations of such algorithms, fixed-point synaptic weights with low resolution are preferable. The basic approach of reducing the resolution of the weights in these algorithms by standard rounding methods incurs drastic losses in performance. To reduce the resolution further, in the extreme case even to binary weights, more advanced techniques are necessary. To this end, we propose two methods for mapping neural network algorithms with high resolution weights to corresponding algorithms that work with low resolution weights and demonstrate that their performance is substantially better than standard rounding. We further use these methods to investigate the performance of three common neural network algorithms under fixed memory size of the weight matrix with different weight resolutions. We show that dedicated hardware systems, whose technology dictates very low weight resolutions (be they electronic or biological) could in principle implement the algorithms we study. version:1
arxiv-1504-05766 | Honeybees-inspired heuristic algorithms for numerical optimisation | http://arxiv.org/abs/1504.05766 | id:1504.05766 author:Muharrem Düğenci category:cs.NE  published:2015-04-22 summary:Swarm intelligence is all about developing collective behaviours to solve complex, ill-structured and large-scale problems. Efficiency in collective behaviours depends on how to harmonise the individual contributions so that a complementary collective effort can be achieved to offer a useful solution. The main points in organising the harmony remains as managing the diversification and intensification actions appropriately, where the efficiency of collective behaviours depends on blending these two actions appropriately. In this study, two swarm intelligence algorithms inspired of natural honeybee colonies have been overviewed with many respects and two new revisions and a hybrid version have been studied to improve the efficiencies in solving numerical optimisation problems, which are well-known hard benchmarks. Consequently, the revisions and especially the hybrid algorithm proposed have outperformed the two original bee algorithms in solving these very hard numerical optimisation benchmarks. version:1
arxiv-1412-7272 | Learning Non-deterministic Representations with Energy-based Ensembles | http://arxiv.org/abs/1412.7272 | id:1412.7272 author:Maruan Al-Shedivat, Emre Neftci, Gert Cauwenberghs category:cs.LG cs.NE  published:2014-12-23 summary:The goal of a generative model is to capture the distribution underlying the data, typically through latent variables. After training, these variables are often used as a new representation, more effective than the original features in a variety of learning tasks. However, the representations constructed by contemporary generative models are usually point-wise deterministic mappings from the original feature space. Thus, even with representations robust to class-specific transformations, statistically driven models trained on them would not be able to generalize when the labeled data is scarce. Inspired by the stochasticity of the synaptic connections in the brain, we introduce Energy-based Stochastic Ensembles. These ensembles can learn non-deterministic representations, i.e., mappings from the feature space to a family of distributions in the latent space. These mappings are encoded in a distribution over a (possibly infinite) collection of models. By conditionally sampling models from the ensemble, we obtain multiple representations for every input example and effectively augment the data. We propose an algorithm similar to contrastive divergence for training restricted Boltzmann stochastic ensembles. Finally, we demonstrate the concept of the stochastic representations on a synthetic dataset as well as test them in the one-shot learning scenario on MNIST. version:2
arxiv-1504-05665 | Rebuilding Factorized Information Criterion: Asymptotically Accurate Marginal Likelihood | http://arxiv.org/abs/1504.05665 | id:1504.05665 author:Kohei Hayashi, Shin-ichi Maeda, Ryohei Fujimaki category:cs.LG stat.ML  published:2015-04-22 summary:Factorized information criterion (FIC) is a recently developed approximation technique for the marginal log-likelihood, which provides an automatic model selection framework for a few latent variable models (LVMs) with tractable inference algorithms. This paper reconsiders FIC and fills theoretical gaps of previous FIC studies. First, we reveal the core idea of FIC that allows generalization for a broader class of LVMs, including continuous LVMs, in contrast to previous FICs, which are applicable only to binary LVMs. Second, we investigate the model selection mechanism of the generalized FIC. Our analysis provides a formal justification of FIC as a model selection criterion for LVMs and also a systematic procedure for pruning redundant latent variables that have been removed heuristically in previous studies. Third, we provide an interpretation of FIC as a variational free energy and uncover a few previously-unknown their relationships. A demonstrative study on Bayesian principal component analysis is provided and numerical experiments support our theoretical results. version:1
arxiv-1504-05632 | Self-Tuned Deep Super Resolution | http://arxiv.org/abs/1504.05632 | id:1504.05632 author:Zhangyang Wang, Yingzhen Yang, Zhaowen Wang, Shiyu Chang, Wei Han, Jianchao Yang, Thomas S. Huang category:cs.LG cs.CV  published:2015-04-22 summary:Deep learning has been successfully applied to image super resolution (SR). In this paper, we propose a deep joint super resolution (DJSR) model to exploit both external and self similarities for SR. A Stacked Denoising Convolutional Auto Encoder (SDCAE) is first pre-trained on external examples with proper data augmentations. It is then fine-tuned with multi-scale self examples from each input, where the reliability of self examples is explicitly taken into account. We also enhance the model performance by sub-model training and selection. The DJSR model is extensively evaluated and compared with state-of-the-arts, and show noticeable performance improvements both quantitatively and perceptually on a wide range of images. version:1
arxiv-1504-05451 | Adaptive Compressive Tracking via Online Vector Boosting Feature Selection | http://arxiv.org/abs/1504.05451 | id:1504.05451 author:Qingshan Liu, Jing Yang, Kaihua Zhang, Yi Wu category:cs.CV  published:2015-04-21 summary:Recently, the compressive tracking (CT) method has attracted much attention due to its high efficiency, but it cannot well deal with the large scale target appearance variations due to its data-independent random projection matrix that results in less discriminative features. To address this issue, in this paper we propose an adaptive CT approach, which selects the most discriminative features to design an effective appearance model. Our method significantly improves CT in three aspects: Firstly, the most discriminative features are selected via an online vector boosting method. Secondly, the object representation is updated in an effective online manner, which preserves the stable features while filtering out the noisy ones. Finally, a simple and effective trajectory rectification approach is adopted that can make the estimated location more accurate. Extensive experiments on the CVPR2013 tracking benchmark demonstrate the superior performance of our algorithm compared over state-of-the-art tracking algorithms. version:2
arxiv-1504-05623 | Median and Mode Ellipse Parameterization for Robust Contour Fitting | http://arxiv.org/abs/1504.05623 | id:1504.05623 author:Michael A. Greminger category:cs.CV  published:2015-04-22 summary:Problems that require the parameterization of closed contours arise frequently in computer vision applications. This article introduces a new curve parameterization algorithm that is able to fit a closed curve to a set of points while being robust to the presence of outliers and occlusions in the data. This robustness property makes this algorithm applicable to computer vision applications where misclassification of features may lead to outliers. The algorithm starts by fitting ellipses to numerous five point subsets from the source data. The closed curve is parameterized by determining the median perimeter of the set of ellipses. The resulting curve is not an ellipse, allowing arbitrary closed contours to be parameterized. The use of the modal perimeter rather than the median perimeter is also explored. A detailed comparison is made between the proposed curve fitting algorithm and existing robust ellipse fitting algorithms. Finally, the utility of the algorithm for computer vision applications is demonstrated through the parameterization of the boundary of fuel droplets during combustion. The performance of the proposed algorithm and the performance of existing algorithms are compared to a ground truth segmentation of the fuel droplet images, which demonstrates improved performance for both area quantification and edge deviation. version:1
arxiv-1504-05619 | Learning Opposites with Evolving Rules | http://arxiv.org/abs/1504.05619 | id:1504.05619 author:Hamid R. Tizhoosh, Shahryar Rahnamayan category:cs.NE cs.LG  published:2015-04-21 summary:The idea of opposition-based learning was introduced 10 years ago. Since then a noteworthy group of researchers has used some notions of oppositeness to improve existing optimization and learning algorithms. Among others, evolutionary algorithms, reinforcement agents, and neural networks have been reportedly extended into their opposition-based version to become faster and/or more accurate. However, most works still use a simple notion of opposites, namely linear (or type- I) opposition, that for each $x\in[a,b]$ assigns its opposite as $\breve{x}_I=a+b-x$. This, of course, is a very naive estimate of the actual or true (non-linear) opposite $\breve{x}_{II}$, which has been called type-II opposite in literature. In absence of any knowledge about a function $y=f(\mathbf{x})$ that we need to approximate, there seems to be no alternative to the naivety of type-I opposition if one intents to utilize oppositional concepts. But the question is if we can receive some level of accuracy increase and time savings by using the naive opposite estimate $\breve{x}_I$ according to all reports in literature, what would we be able to gain, in terms of even higher accuracies and more reduction in computational complexity, if we would generate and employ true opposites? This work introduces an approach to approximate type-II opposites using evolving fuzzy rules when we first perform opposition mining. We show with multiple examples that learning true opposites is possible when we mine the opposites from the training data to subsequently approximate $\breve{x}_{II}=f(\mathbf{x},y)$. version:1
arxiv-1504-05539 | Temporal-Difference Networks | http://arxiv.org/abs/1504.05539 | id:1504.05539 author:Richard S. Sutton, Brian Tanner category:cs.LG  published:2015-04-21 summary:We introduce a generalization of temporal-difference (TD) learning to networks of interrelated predictions. Rather than relating a single prediction to itself at a later time, as in conventional TD methods, a TD network relates each prediction in a set of predictions to other predictions in the set at a later time. TD networks can represent and apply TD learning to a much wider class of predictions than has previously been possible. Using a random-walk example, we show that these networks can be used to learn to predict by a fixed interval, which is not possible with conventional TD methods. Secondly, we show that if the inter-predictive relationships are made conditional on action, then the usual learning-efficiency advantage of TD methods over Monte Carlo (supervised learning) methods becomes particularly pronounced. Thirdly, we demonstrate that TD networks can learn predictive state representations that enable exact solution of a non-Markov problem. A very broad range of inter-predictive temporal relationships can be expressed in these networks. Overall we argue that TD networks represent a substantial extension of the abilities of TD methods and bring us closer to the goal of representing world knowledge in entirely predictive, grounded terms. version:1
arxiv-1504-05524 | A robust and efficient video representation for action recognition | http://arxiv.org/abs/1504.05524 | id:1504.05524 author:Heng Wang, Dan Oneata, Jakob Verbeek, Cordelia Schmid category:cs.CV  published:2015-04-21 summary:This paper introduces a state-of-the-art video representation and applies it to efficient action recognition and detection. We first propose to improve the popular dense trajectory features by explicit camera motion estimation. More specifically, we extract feature point matches between frames using SURF descriptors and dense optical flow. The matches are used to estimate a homography with RANSAC. To improve the robustness of homography estimation, a human detector is employed to remove outlier matches from the human body as human motion is not constrained by the camera. Trajectories consistent with the homography are considered as due to camera motion, and thus removed. We also use the homography to cancel out camera motion from the optical flow. This results in significant improvement on motion-based HOF and MBH descriptors. We further explore the recent Fisher vector as an alternative feature encoding approach to the standard bag-of-words histogram, and consider different ways to include spatial layout information in these encodings. We present a large and varied set of evaluations, considering (i) classification of short basic actions on six datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. We find that our improved trajectory features significantly outperform previous dense trajectories, and that Fisher vectors are superior to bag-of-words encodings for video recognition tasks. In all three tasks, we show substantial improvements over the state-of-the-art results. version:1
arxiv-1504-05517 | Online Learning Algorithm for Time Series Forecasting Suitable for Low Cost Wireless Sensor Networks Nodes | http://arxiv.org/abs/1504.05517 | id:1504.05517 author:Juan Pardo, Francisco Zamora-Martinez, Paloma Botella-Rocamora category:cs.NI cs.LG cs.SY  published:2015-04-21 summary:Time series forecasting is an important predictive methodology which can be applied to a wide range of problems. Particularly, forecasting the indoor temperature permits an improved utilization of the HVAC (Heating, Ventilating and Air Conditioning) systems in a home and thus a better energy efficiency. With such purpose the paper describes how to implement an Artificial Neural Network (ANN) algorithm in a low cost system-on-chip to develop an autonomous intelligent wireless sensor network. The present paper uses a Wireless Sensor Networks (WSN) to monitor and forecast the indoor temperature in a smart home, based on low resources and cost microcontroller technology as the 8051MCU. An on-line learning approach, based on Back-Propagation (BP) algorithm for ANNs, has been developed for real-time time series learning. It performs the model training with every new data that arrive to the system, without saving enormous quantities of data to create a historical database as usual, i.e., without previous knowledge. Consequently to validate the approach a simulation study through a Bayesian baseline model have been tested in order to compare with a database of a real application aiming to see the performance and accuracy. The core of the paper is a new algorithm, based on the BP one, which has been described in detail, and the challenge was how to implement a computational demanding algorithm in a simple architecture with very few hardware resources. version:1
arxiv-1504-05487 | Deep Convolutional Neural Networks Based on Semi-Discrete Frames | http://arxiv.org/abs/1504.05487 | id:1504.05487 author:Thomas Wiatowski, Helmut Bölcskei category:cs.LG cs.IT math.FA math.IT stat.ML  published:2015-04-21 summary:Deep convolutional neural networks have led to breakthrough results in practical feature extraction applications. The mathematical analysis of these networks was pioneered by Mallat, 2012. Specifically, Mallat considered so-called scattering networks based on identical semi-discrete wavelet frames in each network layer, and proved translation-invariance as well as deformation stability of the resulting feature extractor. The purpose of this paper is to develop Mallat's theory further by allowing for different and, most importantly, general semi-discrete frames (such as, e.g., Gabor frames, wavelets, curvelets, shearlets, ridgelets) in distinct network layers. This allows to extract wider classes of features than point singularities resolved by the wavelet transform. Our generalized feature extractor is proven to be translation-invariant, and we develop deformation stability results for a larger class of deformations than those considered by Mallat. For Mallat's wavelet-based feature extractor, we get rid of a number of technical conditions. The mathematical engine behind our results is continuous frame theory, which allows us to completely detach the invariance and deformation stability proofs from the particular algebraic structure of the underlying frames. version:1
arxiv-1504-05473 | Can FCA-based Recommender System Suggest a Proper Classifier? | http://arxiv.org/abs/1504.05473 | id:1504.05473 author:Yury Kashnitsky, Dmitry I. Ignatov category:cs.IR cs.LG stat.ML 62-07  published:2015-04-21 summary:The paper briefly introduces multiple classifier systems and describes a new algorithm, which improves classification accuracy by means of recommendation of a proper algorithm to an object classification. This recommendation is done assuming that a classifier is likely to predict the label of the object correctly if it has correctly classified its neighbors. The process of assigning a classifier to each object is based on Formal Concept Analysis. We explain the idea of the algorithm with a toy example and describe our first experiments with real-world datasets. version:1
arxiv-1412-6502 | Detecting Epileptic Seizures from EEG Data using Neural Networks | http://arxiv.org/abs/1412.6502 | id:1412.6502 author:Siddharth Pramod, Adam Page, Tinoosh Mohsenin, Tim Oates category:cs.LG cs.NE q-bio.NC  published:2014-12-19 summary:We explore the use of neural networks trained with dropout in predicting epileptic seizures from electroencephalographic data (scalp EEG). The input to the neural network is a 126 feature vector containing 9 features for each of the 14 EEG channels obtained over 1-second, non-overlapping windows. The models in our experiments achieved high sensitivity and specificity on patient records not used in the training process. This is demonstrated using leave-one-out-cross-validation across patient records, where we hold out one patient's record as the test set and use all other patients' records for training; repeating this procedure for all patients in the database. version:4
arxiv-1504-05434 | A local approach to estimation in discrete loglinear models | http://arxiv.org/abs/1504.05434 | id:1504.05434 author:Helene Massam, Nanwei Wang category:stat.ML 62H17  62M40  published:2015-04-21 summary:We consider two connected aspects of maximum likelihood estimation of the parameter for high-dimensional discrete graphical models: the existence of the maximum likelihood estimate (mle) and its computation. When the data is sparse, there are many zeros in the contingency table and the maximum likelihood estimate of the parameter may not exist. Fienberg and Rinaldo (2012) have shown that the mle does not exists iff the data vector belongs to a face of the so-called marginal cone spanned by the rows of the design matrix of the model. Identifying these faces in high-dimension is challenging. In this paper, we take a local approach : we show that one such face, albeit possibly not the smallest one, can be identified by looking at a collection of marginal graphical models generated by induced subgraphs $G_i,i=1,\ldots,k$ of $G$. This is our first contribution. Our second contribution concerns the composite maximum likelihood estimate. When the dimension of the problem is large, estimating the parameters of a given graphical model through maximum likelihood is challenging, if not impossible. The traditional approach to this problem has been local with the use of composite likelihood based on local conditional likelihoods. A more recent development is to have the components of the composite likelihood be marginal likelihoods centred around each $v$. We first show that the estimates obtained by consensus through local conditional and marginal likelihoods are identical. We then study the asymptotic properties of the composite maximum likelihood estimate when both the dimension of the model and the sample size $N$ go to infinity. version:1
arxiv-1504-05408 | Effective Discriminative Feature Selection with Non-trivial Solutions | http://arxiv.org/abs/1504.05408 | id:1504.05408 author:Hong Tao, Chenping Hou, Feiping Nie, Yuanyuan Jiao, Dongyun Yi category:cs.LG  published:2015-04-21 summary:Feature selection and feature transformation, the two main ways to reduce dimensionality, are often presented separately. In this paper, a feature selection method is proposed by combining the popular transformation based dimensionality reduction method Linear Discriminant Analysis (LDA) and sparsity regularization. We impose row sparsity on the transformation matrix of LDA through ${\ell}_{2,1}$-norm regularization to achieve feature selection, and the resultant formulation optimizes for selecting the most discriminative features and removing the redundant ones simultaneously. The formulation is extended to the ${\ell}_{2,p}$-norm regularized case: which is more likely to offer better sparsity when $0<p<1$. Thus the formulation is a better approximation to the feature selection problem. An efficient algorithm is developed to solve the ${\ell}_{2,p}$-norm based optimization problem and it is proved that the algorithm converges when $0<p\le 2$. Systematical experiments are conducted to understand the work of the proposed method. Promising experimental results on various types of real-world data sets demonstrate the effectiveness of our algorithm. version:1
arxiv-1504-05392 | Nonparametric Testing for Heterogeneous Correlation | http://arxiv.org/abs/1504.05392 | id:1504.05392 author:Stephen Bamattre, Rex Hu, Joseph S. Verducci category:stat.ML  published:2015-04-21 summary:In the presence of weak overall correlation, it may be useful to investigate if the correlation is significantly and substantially more pronounced over a subpopulation. Two different testing procedures are compared. Both are based on the rankings of the values of two variables from a data set with a large number n of observations. The first maintains its level against Gaussian copulas; the second adapts to general alternatives in the sense that that the number of parameters used in the test grows with n. An analysis of wine quality illustrates how the methods detect heterogeneity of association between chemical properties of the wine, which are attributable to a mix of different cultivars. version:1
arxiv-1504-05369 | Key-Pose Prediction in Cyclic Human Motion | http://arxiv.org/abs/1504.05369 | id:1504.05369 author:Dan Zecha, Rainer Lienhart category:cs.CV  published:2015-04-21 summary:In this paper we study the problem of estimating innercyclic time intervals within repetitive motion sequences of top-class swimmers in a swimming channel. Interval limits are given by temporal occurrences of key-poses, i.e. distinctive postures of the body. A key-pose is defined by means of only one or two specific features of the complete posture. It is often difficult to detect such subtle features directly. We therefore propose the following method: Given that we observe the swimmer from the side, we build a pictorial structure of poselets to robustly identify random support poses within the regular motion of a swimmer. We formulate a maximum likelihood model which predicts a key-pose given the occurrences of multiple support poses within one stroke. The maximum likelihood can be extended with prior knowledge about the temporal location of a key-pose in order to improve the prediction recall. We experimentally show that our models reliably and robustly detect key-poses with a high precision and that their performance can be improved by extending the framework with additional camera views. version:1
arxiv-1412-6830 | Learning Activation Functions to Improve Deep Neural Networks | http://arxiv.org/abs/1412.6830 | id:1412.6830 author:Forest Agostinelli, Matthew Hoffman, Peter Sadowski, Pierre Baldi category:cs.NE cs.CV cs.LG stat.ML  published:2014-12-21 summary:Artificial neural networks typically have a fixed, non-linear activation function at each neuron. We have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes. version:3
arxiv-1504-02462 | A Group Theoretic Perspective on Unsupervised Deep Learning | http://arxiv.org/abs/1504.02462 | id:1504.02462 author:Arnab Paul, Suresh Venkatasubramanian category:cs.LG cs.NE stat.ML  published:2015-04-08 summary:Why does Deep Learning work? What representations does it capture? How do higher-order representations emerge? We study these questions from the perspective of group theory, thereby opening a new approach towards a theory of Deep learning. One factor behind the recent resurgence of the subject is a key algorithmic step called {\em pretraining}: first search for a good generative model for the input samples, and repeat the process one layer at a time. We show deeper implications of this simple principle, by establishing a connection with the interplay of orbits and stabilizers of group actions. Although the neural networks themselves may not form groups, we show the existence of {\em shadow} groups whose elements serve as close approximations. Over the shadow groups, the pre-training step, originally introduced as a mechanism to better initialize a network, becomes equivalent to a search for features with minimal orbits. Intuitively, these features are in a way the {\em simplest}. Which explains why a deep learning network learns simple features first. Next, we show how the same principle, when repeated in the deeper layers, can capture higher order representations, and why representation complexity increases as the layers get deeper. version:3
arxiv-1412-0694 | Streaming Variational Inference for Bayesian Nonparametric Mixture Models | http://arxiv.org/abs/1412.0694 | id:1412.0694 author:Alex Tank, Nicholas J. Foti, Emily B. Fox category:stat.ML  published:2014-12-01 summary:In theory, Bayesian nonparametric (BNP) models are well suited to streaming data scenarios due to their ability to adapt model complexity with the observed data. Unfortunately, such benefits have not been fully realized in practice; existing inference algorithms are either not applicable to streaming applications or not extensible to BNP models. For the special case of Dirichlet processes, streaming inference has been considered. However, there is growing interest in more flexible BNP models building on the class of normalized random measures (NRMs). We work within this general framework and present a streaming variational inference algorithm for NRM mixture models. Our algorithm is based on assumed density filtering (ADF), leading straightforwardly to expectation propagation (EP) for large-scale batch inference as well. We demonstrate the efficacy of the algorithm on clustering documents in large, streaming text corpora. version:3
arxiv-1504-05308 | Automatic Face Recognition from Video | http://arxiv.org/abs/1504.05308 | id:1504.05308 author:Ognjen Arandjelovic category:cs.CV  published:2015-04-21 summary:The objective of this work is to automatically recognize faces from video sequences in a realistic, unconstrained setup in which illumination conditions are extreme and greatly changing, viewpoint and user motion pattern have a wide variability, and video input is of low quality. At the centre of focus are face appearance manifolds: this thesis presents a significant advance of their understanding and application in the sphere of face recognition. The two main contributions are the Generic Shape-Illumination Manifold recognition algorithm and the Anisotropic Manifold Space clustering. The Generic Shape-Illumination Manifold is evaluated on a large data corpus acquired in real-world conditions and its performance is shown to greatly exceed that of state-of-the-art methods in the literature and the best performing commercial software. Empirical evaluation of the Anisotropic Manifold Space clustering on a popular situation comedy is also described with excellent preliminary results. version:1
arxiv-1504-05302 | The adaptable buffer algorithm for high quantile estimation in non-stationary data streams | http://arxiv.org/abs/1504.05302 | id:1504.05302 author:Ognjen Arandjelovic, Duc-Son Pham, Svetha Venkatesh category:cs.CV  published:2015-04-21 summary:The need to estimate a particular quantile of a distribution is an important problem which frequently arises in many computer vision and signal processing applications. For example, our work was motivated by the requirements of many semi-automatic surveillance analytics systems which detect abnormalities in close-circuit television (CCTV) footage using statistical models of low-level motion features. In this paper we specifically address the problem of estimating the running quantile of a data stream with non-stationary stochasticity when the memory for storing observations is limited. We make several major contributions: (i) we derive an important theoretical result which shows that the change in the quantile of a stream is constrained regardless of the stochastic properties of data, (ii) we describe a set of high-level design goals for an effective estimation algorithm that emerge as a consequence of our theoretical findings, (iii) we introduce a novel algorithm which implements the aforementioned design goals by retaining a sample of data values in a manner adaptive to changes in the distribution of data and progressively narrowing down its focus in the periods of quasi-stationary stochasticity, and (iv) we present a comprehensive evaluation of the proposed algorithm and compare it with the existing methods in the literature on both synthetic data sets and three large `real-world' streams acquired in the course of operation of an existing commercial surveillance system. Our findings convincingly demonstrate that the proposed method is highly successful and vastly outperforms the existing alternatives, especially when the target quantile is high valued and the available buffer capacity severely limited. version:1
arxiv-1504-05299 | Groupwise registration of aerial images | http://arxiv.org/abs/1504.05299 | id:1504.05299 author:Ognjen Arandjelovic, Duc-Son Pham, Svetha Venkatesh category:cs.CV  published:2015-04-21 summary:This paper addresses the task of time separated aerial image registration. The ability to solve this problem accurately and reliably is important for a variety of subsequent image understanding applications. The principal challenge lies in the extent and nature of transient appearance variation that a land area can undergo, such as that caused by the change in illumination conditions, seasonal variations, or the occlusion by non-persistent objects (people, cars). Our work introduces several novelties: (i) unlike all previous work on aerial image registration, we approach the problem using a set-based paradigm; (ii) we show how local, pair-wise constraints can be used to enforce a globally good registration using a constraints graph structure; (iii) we show how a simple holistic representation derived from raw aerial images can be used as a basic building block of the constraints graph in a manner which achieves both high registration accuracy and speed. We demonstrate: (i) that the proposed method outperforms the state-of-the-art for pair-wise registration already, achieving greater accuracy and reliability, while at the same time reducing the computational cost of the task; and (ii) that the increase in the number of available images in a set consistently reduces the average registration error. version:1
arxiv-1504-05298 | Viewpoint distortion compensation in practical surveillance systems | http://arxiv.org/abs/1504.05298 | id:1504.05298 author:Ognjen Arandjelovic, Duc-Son Pham, Svetha Venkatesh category:cs.CV  published:2015-04-21 summary:Our aim is to estimate the perspective-effected geometric distortion of a scene from a video feed. In contrast to all previous work we wish to achieve this using from low-level, spatio-temporally local motion features used in commercial semi-automatic surveillance systems. We: (i) describe a dense algorithm which uses motion features to estimate the perspective distortion at each image locus and then polls all such local estimates to arrive at the globally best estimate, (ii) present an alternative coarse algorithm which subdivides the image frame into blocks, and uses motion features to derive block-specific motion characteristics and constrain the relationships between these characteristics, with the perspective estimate emerging as a result of a global optimization scheme, and (iii) report the results of an evaluation using nine large sets acquired using existing close-circuit television (CCTV) cameras. Our findings demonstrate that both of the proposed methods are successful, their accuracy matching that of human labelling using complete visual data. version:1
arxiv-1504-05289 | Distance-based species tree estimation: information-theoretic trade-off between number of loci and sequence length under the coalescent | http://arxiv.org/abs/1504.05289 | id:1504.05289 author:Elchanan Mossel, Sebastien Roch category:math.PR cs.LG math.ST q-bio.PE stat.TH  published:2015-04-21 summary:We consider the reconstruction of a phylogeny from multiple genes under the multispecies coalescent. We establish a connection with the sparse signal detection problem, where one seeks to distinguish between a distribution and a mixture of the distribution and a sparse signal. Using this connection, we derive an information-theoretic trade-off between the number of genes, $m$, needed for an accurate reconstruction and the sequence length, $k$, of the genes. Specifically, we show that to detect a branch of length $f$, one needs $m = \Theta(1/[f^{2} \sqrt{k}])$. version:1
arxiv-1504-05287 | Decomposing Overcomplete 3rd Order Tensors using Sum-of-Squares Algorithms | http://arxiv.org/abs/1504.05287 | id:1504.05287 author:Rong Ge, Tengyu Ma category:cs.DS cs.LG stat.ML  published:2015-04-21 summary:Tensor rank and low-rank tensor decompositions have many applications in learning and complexity theory. Most known algorithms use unfoldings of tensors and can only handle rank up to $n^{\lfloor p/2 \rfloor}$ for a $p$-th order tensor in $\mathbb{R}^{n^p}$. Previously no efficient algorithm can decompose 3rd order tensors when the rank is super-linear in the dimension. Using ideas from sum-of-squares hierarchy, we give the first quasi-polynomial time algorithm that can decompose a random 3rd order tensor decomposition when the rank is as large as $n^{3/2}/\textrm{polylog} n$. We also give a polynomial time algorithm for certifying the injective norm of random low rank tensors. Our tensor decomposition algorithm exploits the relationship between injective norm and the tensor components. The proof relies on interesting tools for decoupling random variables to prove better matrix concentration bounds, which can be useful in other settings. version:1
arxiv-1503-04269 | An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning | http://arxiv.org/abs/1503.04269 | id:1503.04269 author:Richard S. Sutton, A. Rupam Mahmood, Martha White category:cs.LG  published:2015-03-14 summary:In this paper we introduce the idea of improving the performance of parametric temporal-difference (TD) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps. In particular, we show that varying the emphasis of linear TD($\lambda$)'s updates in a particular way causes its expected update to become stable under off-policy training. The only prior model-free TD methods to achieve this with per-step computation linear in the number of function approximation parameters are the gradient-TD family of methods including TDC, GTD($\lambda$), and GQ($\lambda$). Compared to these methods, our _emphatic TD($\lambda$)_ is simpler and easier to use; it has only one learned parameter vector and one step-size parameter. Our treatment includes general state-dependent discounting and bootstrapping functions, and a way of specifying varying degrees of interest in accurately valuing different states. version:2
arxiv-1411-4555 | Show and Tell: A Neural Image Caption Generator | http://arxiv.org/abs/1411.4555 | id:1411.4555 author:Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan category:cs.CV  published:2014-11-17 summary:Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art. version:2
arxiv-1504-05241 | Convolutional Neural Network-Based Image Representation for Visual Loop Closure Detection | http://arxiv.org/abs/1504.05241 | id:1504.05241 author:Yi Hou, Hong Zhang, Shilin Zhou category:cs.RO cs.CV  published:2015-04-20 summary:Deep convolutional neural networks (CNN) have recently been shown in many computer vision and pattern recog- nition applications to outperform by a significant margin state- of-the-art solutions that use traditional hand-crafted features. However, this impressive performance is yet to be fully exploited in robotics. In this paper, we focus one specific problem that can benefit from the recent development of the CNN technology, i.e., we focus on using a pre-trained CNN model as a method of generating an image representation appropriate for visual loop closure detection in SLAM (simultaneous localization and mapping). We perform a comprehensive evaluation of the outputs at the intermediate layers of a CNN as image descriptors, in comparison with state-of-the-art image descriptors, in terms of their ability to match images for detecting loop closures. The main conclusions of our study include: (a) CNN-based image representations perform comparably to state-of-the-art hand- crafted competitors in environments without significant lighting change, (b) they outperform state-of-the-art competitors when lighting changes significantly, and (c) they are also significantly faster to extract than the state-of-the-art hand-crafted features even on a conventional CPU and are two orders of magnitude faster on an entry-level GPU. version:1
arxiv-1502-02710 | Scalable Multilabel Prediction via Randomized Methods | http://arxiv.org/abs/1502.02710 | id:1502.02710 author:Nikos Karampatziakis, Paul Mineiro category:cs.LG  published:2015-02-09 summary:Modeling the dependence between outputs is a fundamental challenge in multilabel classification. In this work we show that a generic regularized nonlinearity mapping independent predictions to joint predictions is sufficient to achieve state-of-the-art performance on a variety of benchmark problems. Crucially, we compute the joint predictions without ever obtaining any independent predictions, while incorporating low-rank and smoothness regularization. We achieve this by leveraging randomized algorithms for matrix decomposition and kernel approximation. Furthermore, our techniques are applicable to the multiclass setting. We apply our method to a variety of multiclass and multilabel data sets, obtaining state-of-the-art results. version:2
arxiv-1502-02766 | Multi-view Face Detection Using Deep Convolutional Neural Networks | http://arxiv.org/abs/1502.02766 | id:1502.02766 author:Sachin Sudhakar Farfade, Mohammad Saberian, Li-Jia Li category:cs.CV I.4  published:2015-02-10 summary:In this paper we consider the problem of multi-view face detection. While there has been significant research on this problem, current state-of-the-art approaches for this task require annotation of facial landmarks, e.g. TSM [25], or annotation of face poses [28, 22]. They also require training dozens of models to fully capture faces in all orientations, e.g. 22 models in HeadHunter method [22]. In this paper we propose Deep Dense Face Detector (DDFD), a method that does not require pose/landmark annotation and is able to detect faces in a wide range of orientations using a single model based on deep convolutional neural networks. The proposed method has minimal complexity; unlike other recent deep learning object detection methods [9], it does not require additional components such as segmentation, bounding-box regression, or SVM classifiers. Furthermore, we analyzed scores of the proposed face detector for faces in different orientations and found that 1) the proposed method is able to detect faces from different angles and can handle occlusion to some extent, 2) there seems to be a correlation between dis- tribution of positive examples in the training set and scores of the proposed face detector. The latter suggests that the proposed methods performance can be further improved by using better sampling strategies and more sophisticated data augmentation techniques. Evaluations on popular face detection benchmark datasets show that our single-model face detector algorithm has similar or better performance compared to the previous methods, which are more complex and require annotations of either different poses or facial landmarks. version:3
arxiv-1504-05158 | Multi-swarm PSO algorithm for the Quadratic Assignment Problem: a massive parallel implementation on the OpenCL platform | http://arxiv.org/abs/1504.05158 | id:1504.05158 author:Piotr Szwed, Wojciech Chmiel category:cs.NE  published:2015-04-20 summary:This paper presents a multi-swarm PSO algorithm for the Quadratic Assignment Problem (QAP) implemented on OpenCL platform. Our work was motivated by results of time efficiency tests performed for single-swarm algorithm implementation that showed clearly that the benefits of a parallel execution platform can be fully exploited, if the processed population is large. The described algorithm can be executed in two modes: with independent swarms or with migration. We discuss the algorithm construction, as well as we report results of tests performed on several problem instances from the QAPLIB library. During the experiments the algorithm was configured to process large populations. This allowed us to collect statistical data related to values of goal function reached by individual particles. We use them to demonstrate on two test cases that although single particles seem to behave chaotically during the optimization process, when the whole population is analyzed, the probability that a particle will select a near-optimal solution grows. version:1
arxiv-1406-3407 | Restricted Boltzmann Machine for Classification with Hierarchical Correlated Prior | http://arxiv.org/abs/1406.3407 | id:1406.3407 author:Gang Chen, Sargur H. Srihari category:cs.LG 68T10 I.2.6  published:2014-06-13 summary:Restricted Boltzmann machines (RBM) and its variants have become hot research topics recently, and widely applied to many classification problems, such as character recognition and document categorization. Often, classification RBM ignores the interclass relationship or prior knowledge of sharing information among classes. In this paper, we are interested in RBM with the hierarchical prior over classes. We assume parameters for nearby nodes are correlated in the hierarchical tree, and further the parameters at each node of the tree be orthogonal to those at its ancestors. We propose a hierarchical correlated RBM for classification problem, which generalizes the classification RBM with sharing information among different classes. In order to reduce the redundancy between node parameters in the hierarchy, we also introduce orthogonal restrictions to our objective function. We test our method on challenge datasets, and show promising results compared to competitive baselines. version:2
arxiv-1504-05143 | Network Plasticity as Bayesian Inference | http://arxiv.org/abs/1504.05143 | id:1504.05143 author:David Kappel, Stefan Habenschuss, Robert Legenstein, Wolfgang Maass category:cs.NE q-bio.NC  published:2015-04-20 summary:General results from statistical learning theory suggest to understand not only brain computations, but also brain plasticity as probabilistic inference. But a model for that has been missing. We propose that inherently stochastic features of synaptic plasticity and spine motility enable cortical networks of neurons to carry out probabilistic inference by sampling from a posterior distribution of network configurations. This model provides a viable alternative to existing models that propose convergence of parameters to maximum likelihood values. It explains how priors on weight distributions and connection probabilities can be merged optimally with learned experience, how cortical networks can generalize learned information so well to novel experiences, and how they can compensate continuously for unforeseen disturbances of the network. The resulting new theory of network plasticity explains from a functional perspective a number of experimental data on stochastic aspects of synaptic plasticity that previously appeared to be quite puzzling. version:1
arxiv-1504-05122 | Optimal Nudging: Solving Average-Reward Semi-Markov Decision Processes as a Minimal Sequence of Cumulative Tasks | http://arxiv.org/abs/1504.05122 | id:1504.05122 author:Reinaldo Uribe Muriel, Fernando Lozando, Charles Anderson category:cs.LG cs.AI  published:2015-04-20 summary:This paper describes a novel method to solve average-reward semi-Markov decision processes, by reducing them to a minimal sequence of cumulative reward problems. The usual solution methods for this type of problems update the gain (optimal average reward) immediately after observing the result of taking an action. The alternative introduced, optimal nudging, relies instead on setting the gain to some fixed value, which transitorily makes the problem a cumulative-reward task, solving it by any standard reinforcement learning method, and only then updating the gain in a way that minimizes uncertainty in a minmax sense. The rule for optimal gain update is derived by exploiting the geometric features of the w-l space, a simple mapping of the space of policies. The total number of cumulative reward tasks that need to be solved is shown to be small. Some experiments are presented to explore the features of the algorithm and to compare its performance with other approaches. version:1
arxiv-1504-05095 | Hybrid Genetic Algorithm and Lasso Test Approach for Inferring Well Supported Phylogenetic Trees based on Subsets of Chloroplastic Core Genes | http://arxiv.org/abs/1504.05095 | id:1504.05095 author:Bassam AlKindy, Christophe Guyeux, Jean-François Couchot, Michel Salomon, Christian Parisod, Jacques M. Bahi category:cs.AI cs.NE q-bio.PE q-bio.QM  published:2015-04-20 summary:The amount of completely sequenced chloroplast genomes increases rapidly every day, leading to the possibility to build large scale phylogenetic trees of plant species. Considering a subset of close plant species defined according to their chloroplasts, the phylogenetic tree that can be inferred by their core genes is not necessarily well supported, due to the possible occurrence of "problematic" genes (i.e., homoplasy, incomplete lineage sorting, horizontal gene transfers, etc.) which may blur phylogenetic signal. However, a trustworthy phylogenetic tree can still be obtained if the number of problematic genes is low, the problem being to determine the largest subset of core genes that produces the best supported tree. To discard problematic genes and due to the overwhelming number of possible combinations, we propose an hybrid approach that embeds both genetic algorithms and statistical tests. Given a set of organisms, the result is a pipeline of many stages for the production of well supported phylogenetic trees. The proposal has been applied to different cases of plant families, leading to encouraging results for these families. version:1
arxiv-1504-05059 | Nonparametric Nearest Neighbor Random Process Clustering | http://arxiv.org/abs/1504.05059 | id:1504.05059 author:Michael Tschannen, Helmut Bölcskei category:stat.ML cs.IT cs.LG math.IT  published:2015-04-20 summary:We consider the problem of clustering noisy finite-length observations of stationary ergodic random processes according to their nonparametric generative models without prior knowledge of the model statistics and the number of generative models. Two algorithms, both using the L1-distance between estimated power spectral densities (PSDs) as a measure of dissimilarity, are analyzed. The first algorithm, termed nearest neighbor process clustering (NNPC), to the best of our knowledge, is new and relies on partitioning the nearest neighbor graph of the observations via spectral clustering. The second algorithm, simply referred to as k-means (KM), consists of a single k-means iteration with farthest point initialization and was considered before in the literature, albeit with a different measure of dissimilarity and with asymptotic performance results only. We show that both NNPC and KM succeed with high probability under noise and even when the generative process PSDs overlap significantly, all provided that the observation length is sufficiently large. Our results quantify the tradeoff between the overlap of the generative process PSDs, the noise variance, and the observation length. Finally, we present numerical performance results for synthetic and real data. version:1
arxiv-1504-05035 | F-SVM: Combination of Feature Transformation and SVM Learning via Convex Relaxation | http://arxiv.org/abs/1504.05035 | id:1504.05035 author:Xiaohe Wu, Wangmeng Zuo, Yuanyuan Zhu, Liang Lin category:cs.LG cs.CV  published:2015-04-20 summary:The generalization error bound of support vector machine (SVM) depends on the ratio of radius and margin, while standard SVM only considers the maximization of the margin but ignores the minimization of the radius. Several approaches have been proposed to integrate radius and margin for joint learning of feature transformation and SVM classifier. However, most of them either require the form of the transformation matrix to be diagonal, or are non-convex and computationally expensive. In this paper, we suggest a novel approximation for the radius of minimum enclosing ball (MEB) in feature space, and then propose a convex radius-margin based SVM model for joint learning of feature transformation and SVM classifier, i.e., F-SVM. An alternating minimization method is adopted to solve the F-SVM model, where the feature transformation is updatedvia gradient descent and the classifier is updated by employing the existing SVM solver. By incorporating with kernel principal component analysis, F-SVM is further extended for joint learning of nonlinear transformation and classifier. Experimental results on the UCI machine learning datasets and the LFW face datasets show that F-SVM outperforms the standard SVM and the existing radius-margin based SVMs, e.g., RMM, R-SVM+ and R-SVM+{\mu}. version:1
arxiv-1412-6645 | Weakly Supervised Multi-Embeddings Learning of Acoustic Models | http://arxiv.org/abs/1412.6645 | id:1412.6645 author:Gabriel Synnaeve, Emmanuel Dupoux category:cs.SD cs.CL cs.LG I.2.6; I.2.7; I.5.1  published:2014-12-20 summary:We trained a Siamese network with multi-task same/different information on a speech dataset, and found that it was possible to share a network for both tasks without a loss in performance. The first task was to discriminate between two same or different words, and the second was to discriminate between two same or different talkers. version:3
arxiv-1410-4355 | Multi-Level Anomaly Detection on Time-Varying Graph Data | http://arxiv.org/abs/1410.4355 | id:1410.4355 author:Robert A. Bridges, John Collins, Erik M. Ferragut, Jason Laska, Blair D. Sullivan category:cs.SI cs.LG stat.ML  published:2014-10-16 summary:This work presents a novel modeling and analysis framework for graph sequences which addresses the challenge of detecting and contextualizing anomalies in labelled, streaming graph data. We introduce a generalization of the BTER model of Seshadhri et al. by adding flexibility to community structure, and use this model to perform multi-scale graph anomaly detection. Specifically, probability models describing coarse subgraphs are built by aggregating probabilities at finer levels, and these closely related hierarchical models simultaneously detect deviations from expectation. This technique provides insight into a graph's structure and internal context that may shed light on a detected event. Additionally, this multi-scale analysis facilitates intuitive visualizations by allowing users to narrow focus from an anomalous graph to particular subgraphs or nodes causing the anomaly. For evaluation, two hierarchical anomaly detectors are tested against a baseline Gaussian method on a series of sampled graphs. We demonstrate that our graph statistics-based approach outperforms both a distribution-based detector and the baseline in a labeled setting with community structure, and it accurately detects anomalies in synthetic and real-world datasets at the node, subgraph, and graph levels. To illustrate the accessibility of information made possible via this technique, the anomaly detector and an associated interactive visualization tool are tested on NCAA football data, where teams and conferences that moved within the league are identified with perfect recall, and precision greater than 0.786. version:4
arxiv-1502-06919 | Low Rank Matrix Completion with Exponential Family Noise | http://arxiv.org/abs/1502.06919 | id:1502.06919 author:Jean Lafond category:math.ST stat.ML stat.TH  published:2015-02-24 summary:The matrix completion problem consists in reconstructing a matrix from a sample of entries, possibly observed with noise. A popular class of estimator, known as nuclear norm penalized estimators, are based on minimizing the sum of a data fitting term and a nuclear norm penalization. Here, we investigate the case where the noise distribution belongs to the exponential family and is sub-exponential. Our framework alllows for a general sampling scheme. We first consider an estimator defined as the minimizer of the sum of a log-likelihood term and a nuclear norm penalization and prove an upper bound on the Frobenius prediction risk. The rate obtained improves on previous works on matrix completion for exponential family. When the sampling distribution is known, we propose another estimator and prove an oracle inequality w.r.t. the Kullback-Leibler prediction risk, which translates immediatly into an upper bound on the Frobenius prediction risk. Finally, we show that all the rates obtained are minimax optimal up to a logarithmic factor. version:2
arxiv-1504-04943 | Weakly Supervised Fine-Grained Image Categorization | http://arxiv.org/abs/1504.04943 | id:1504.04943 author:Yu Zhang, Xiu-shen Wei, Jianxin Wu, Jianfei Cai, Jiangbo Lu, Viet-Anh Nguyen, Minh N. Do category:cs.CV  published:2015-04-20 summary:In this paper, we categorize fine-grained images without using any object / part annotation neither in the training nor in the testing stage, a step towards making it suitable for deployments. Fine-grained image categorization aims to classify objects with subtle distinctions. Most existing works heavily rely on object / part detectors to build the correspondence between object parts by using object or object part annotations inside training images. The need for expensive object annotations prevents the wide usage of these methods. Instead, we propose to select useful parts from multi-scale part proposals in objects, and use them to compute a global image representation for categorization. This is specially designed for the annotation-free fine-grained categorization task, because useful parts have shown to play an important role in existing annotation-dependent works but accurate part detectors can be hardly acquired. With the proposed image representation, we can further detect and visualize the key (most discriminative) parts in objects of different classes. In the experiment, the proposed annotation-free method achieves better accuracy than that of state-of-the-art annotation-free and most existing annotation-dependent methods on two challenging datasets, which shows that it is not always necessary to use accurate object / part annotations in fine-grained image categorization. version:1
arxiv-1504-04923 | Learning discriminative trajectorylet detector sets for accurate skeleton-based action recognition | http://arxiv.org/abs/1504.04923 | id:1504.04923 author:Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, Anton von den Hengel category:cs.CV  published:2015-04-20 summary:The introduction of low-cost RGB-D sensors has promoted the research in skeleton-based human action recognition. Devising a representation suitable for characterising actions on the basis of noisy skeleton sequences remains a challenge, however. We here provide two insights into this challenge. First, we show that the discriminative information of a skeleton sequence usually resides in a short temporal interval and we propose a simple-but-effective local descriptor called trajectorylet to capture the static and kinematic information within this interval. Second, we further propose to encode each trajectorylet with a discriminative trajectorylet detector set which is selected from a large number of candidate detectors trained through exemplar-SVMs. The action-level representation is obtained by pooling trajectorylet encodings. Evaluating on standard datasets acquired from the Kinect sensor, it is demonstrated that our method obtains superior results over existing approaches under various experimental setups. version:1
arxiv-1504-04909 | Illuminating search spaces by mapping elites | http://arxiv.org/abs/1504.04909 | id:1504.04909 author:Jean-Baptiste Mouret, Jeff Clune category:cs.AI cs.NE cs.RO q-bio.PE  published:2015-04-20 summary:Many fields use search algorithms, which automatically explore a search space to find high-performing solutions: chemists search through the space of molecules to discover new drugs; engineers search for stronger, cheaper, safer designs, scientists search for models that best explain data, etc. The goal of search algorithms has traditionally been to return the single highest-performing solution in a search space. Here we describe a new, fundamentally different type of algorithm that is more useful because it provides a holistic view of how high-performing solutions are distributed throughout a search space. It creates a map of high-performing solutions at each point in a space defined by dimensions of variation that a user gets to choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) algorithm illuminates search spaces, allowing researchers to understand how interesting attributes of solutions combine to affect performance, either positively or, equally of interest, negatively. For example, a drug company may wish to understand how performance changes as the size of molecules and their cost-to-produce vary. MAP-Elites produces a large diversity of high-performing, yet qualitatively different solutions, which can be more helpful than a single, high-performing solution. Interestingly, because MAP-Elites explores more of the search space, it also tends to find a better overall solution than state-of-the-art search algorithms. We demonstrate the benefits of this new algorithm in three different problem domains ranging from producing modular neural networks to designing simulated and real soft robots. Because MAP- Elites (1) illuminates the relationship between performance and dimensions of interest in solutions, (2) returns a set of high-performing, yet diverse solutions, and (3) improves finding a single, best solution, it will advance science and engineering. version:1
arxiv-1411-6660 | Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition | http://arxiv.org/abs/1411.6660 | id:1411.6660 author:Zhenzhong Lan, Ming Lin, Xuanchong Li, Alexander G. Hauptmann, Bhiksha Raj category:cs.CV  published:2014-11-24 summary:Most state-of-the-art action feature extractors involve differential operators, which act as highpass filters and tend to attenuate low frequency action information. This attenuation introduces bias to the resulting features and generates ill-conditioned feature matrices. The Gaussian Pyramid has been used as a feature enhancing technique that encodes scale-invariant characteristics into the feature space in an attempt to deal with this attenuation. However, at the core of the Gaussian Pyramid is a convolutional smoothing operation, which makes it incapable of generating new features at coarse scales. In order to address this problem, we propose a novel feature enhancing technique called Multi-skIp Feature Stacking (MIFS), which stacks features extracted using a family of differential filters parameterized with multiple time skips and encodes shift-invariance into the frequency space. MIFS compensates for information lost from using differential operators by recapturing information at coarse scales. This recaptured information allows us to match actions at different speeds and ranges of motion. We prove that MIFS enhances the learnability of differential-based features exponentially. The resulting feature matrices from MIFS have much smaller conditional numbers and variances than those from conventional methods. Experimental results show significantly improved performance on challenging action recognition and event detection tasks. Specifically, our method exceeds the state-of-the-arts on Hollywood2, UCF101 and UCF50 datasets and is comparable to state-of-the-arts on HMDB51 and Olympics Sports datasets. MIFS can also be used as a speedup strategy for feature extraction with minimal or no accuracy cost. version:4
arxiv-1504-04871 | DEEP-CARVING: Discovering Visual Attributes by Carving Deep Neural Nets | http://arxiv.org/abs/1504.04871 | id:1504.04871 author:Sukrit Shankar, Vikas K. Garg, Roberto Cipolla category:cs.CV  published:2015-04-19 summary:Most of the approaches for discovering visual attributes in images demand significant supervision, which is cumbersome to obtain. In this paper, we aim to discover visual attributes in a weakly supervised setting that is commonly encountered with contemporary image search engines. Deep Convolutional Neural Networks (CNNs) have enjoyed remarkable success in vision applications recently. However, in a weakly supervised scenario, widely used CNN training procedures do not learn a robust model for predicting multiple attribute labels simultaneously. The primary reason is that the attributes highly co-occur within the training data. To ameliorate this limitation, we propose Deep-Carving, a novel training procedure with CNNs, that helps the net efficiently carve itself for the task of multiple attribute prediction. During training, the responses of the feature maps are exploited in an ingenious way to provide the net with multiple pseudo-labels (for training images) for subsequent iterations. The process is repeated periodically after a fixed number of iterations, and enables the net carve itself iteratively for efficiently disentangling features. Additionally, we contribute a noun-adjective pairing inspired Natural Scenes Attributes Dataset to the research community, CAMIT - NSAD, containing a number of co-occurring attributes within a noun category. We describe, in detail, salient aspects of this dataset. Our experiments on CAMIT-NSAD and the SUN Attributes Dataset, with weak supervision, clearly demonstrate that the Deep-Carved CNNs consistently achieve considerable improvement in the precision of attribute prediction over popular baseline methods. version:1
arxiv-1412-6514 | Score Function Features for Discriminative Learning | http://arxiv.org/abs/1412.6514 | id:1412.6514 author:Majid Janzamin, Hanie Sedghi, Anima Anandkumar category:cs.LG stat.ML  published:2014-12-19 summary:Feature learning forms the cornerstone for tackling challenging learning problems in domains such as speech, computer vision and natural language processing. In this paper, we consider a novel class of matrix and tensor-valued features, which can be pre-trained using unlabeled samples. We present efficient algorithms for extracting discriminative information, given these pre-trained features and labeled samples for any related task. Our class of features are based on higher-order score functions, which capture local variations in the probability density function of the input. We establish a theoretical framework to characterize the nature of discriminative information that can be extracted from score-function features, when used in conjunction with labeled samples. We employ efficient spectral decomposition algorithms (on matrices and tensors) for extracting discriminative components. The advantage of employing tensor-valued features is that we can extract richer discriminative information in the form of an overcomplete representations. Thus, we present a novel framework for employing generative models of the input for discriminative learning. version:2
arxiv-1502-05767 | Automatic differentiation in machine learning: a survey | http://arxiv.org/abs/1502.05767 | id:1502.05767 author:Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, Jeffrey Mark Siskind category:cs.SC cs.LG 68W30  65D25  68T05 G.1.4; I.2.6  published:2015-02-20 summary:Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD) is a technique for calculating derivatives of numeric functions expressed as computer programs efficiently and accurately, used in fields such as computational fluid dynamics, nuclear engineering, and atmospheric sciences. Despite its advantages and use in other fields, machine learning practitioners have been little influenced by AD and make scant use of available tools. We survey the intersection of AD and machine learning, cover applications where AD has the potential to make a big impact, and report on some recent developments in the adoption of this technique. We aim to dispel some misconceptions that we contend have impeded the use of AD within the machine learning community. version:2
arxiv-1504-04850 | Exploring Bayesian Models for Multi-level Clustering of Hierarchically Grouped Sequential Data | http://arxiv.org/abs/1504.04850 | id:1504.04850 author:Adway Mitra category:cs.LG cs.AI  published:2015-04-19 summary:A wide range of Bayesian models have been proposed for data that is divided hierarchically into groups. These models aim to cluster the data at different levels of grouping, by assigning a mixture component to each datapoint, and a mixture distribution to each group. Multi-level clustering is facilitated by the sharing of these components and distributions by the groups. In this paper, we introduce the concept of Degree of Sharing (DoS) for the mixture components and distributions, with an aim to analyze and classify various existing models. Next we introduce a generalized hierarchical Bayesian model, of which the existing models can be shown to be special cases. Unlike most of these models, our model takes into account the sequential nature of the data, and various other temporal structures at different levels while assigning mixture components and distributions. We show one specialization of this model aimed at hierarchical segmentation of news transcripts, and present a Gibbs Sampling based inference algorithm for it. We also show experimentally that the proposed model outperforms existing models for the same task. version:1
arxiv-1504-04802 | Gradual Classical Logic for Attributed Objects - Extended in Re-Presentation | http://arxiv.org/abs/1504.04802 | id:1504.04802 author:Ryuta Arisaka category:cs.AI cs.CL cs.LO  published:2015-04-19 summary:Our understanding about things is conceptual. By stating that we reason about objects, it is in fact not the objects but concepts referring to them that we manipulate. Now, so long just as we acknowledge infinitely extending notions such as space, time, size, colour, etc, - in short, any reasonable quality - into which an object is subjected, it becomes infeasible to affirm atomicity in the concept referring to the object. However, formal/symbolic logics typically presume atomic entities upon which other expressions are built. Can we reflect our intuition about the concept onto formal/symbolic logics at all? I assure that we can, but the usual perspective about the atomicity needs inspected. In this work, I present gradual logic which materialises the observation that we cannot tell apart whether a so-regarded atomic entity is atomic or is just atomic enough not to be considered non-atomic. The motivation is to capture certain phenomena that naturally occur around concepts with attributes, including presupposition and contraries. I present logical particulars of the logic, which is then mapped onto formal semantics. Two linguistically interesting semantics will be considered. Decidability is shown. version:1
arxiv-1501-06272 | Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval | http://arxiv.org/abs/1501.06272 | id:1501.06272 author:Fang Zhao, Yongzhen Huang, Liang Wang, Tieniu Tan category:cs.CV cs.LG  published:2015-01-26 summary:With the rapid growth of web images, hashing has received increasing interests in large scale image retrieval. Research efforts have been devoted to learning compact binary codes that preserve semantic similarity based on labels. However, most of these hashing methods are designed to handle simple binary similarity. The complex multilevel semantic structure of images associated with multiple labels have not yet been well explored. Here we propose a deep semantic ranking based method for learning hash functions that preserve multilevel semantic similarity between multi-label images. In our approach, deep convolutional neural network is incorporated into hash functions to jointly learn feature representations and mappings from them to hash codes, which avoids the limitation of semantic representation power of hand-crafted features. Meanwhile, a ranking list that encodes the multilevel similarity information is employed to guide the learning of such deep hash functions. An effective scheme based on surrogate loss is used to solve the intractable optimization problem of nonsmooth and multivariate ranking measures involved in the learning procedure. Experimental results show the superiority of our proposed approach over several state-of-the-art hashing methods in term of ranking evaluation metrics when tested on multi-label image datasets. version:2
arxiv-1504-04788 | Compressing Neural Networks with the Hashing Trick | http://arxiv.org/abs/1504.04788 | id:1504.04788 author:Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, Yixin Chen category:cs.LG cs.NE  published:2015-04-19 summary:As deep nets are increasingly used in applications suited for mobile devices, a fundamental dilemma becomes apparent: the trend in deep learning is to grow models to absorb ever-increasing data set sizes; however mobile devices are designed with very little memory and cannot store such large models. We present a novel network architecture, HashedNets, that exploits inherent redundancy in neural networks to achieve drastic reductions in model sizes. HashedNets uses a low-cost hash function to randomly group connection weights into hash buckets, and all connections within the same hash bucket share a single parameter value. These parameters are tuned to adjust to the HashedNets weight sharing architecture with standard backprop during training. Our hashing procedure introduces no additional memory overhead, and we demonstrate on several benchmark data sets that HashedNets shrink the storage requirements of neural networks substantially while mostly preserving generalization performance. version:1
arxiv-1503-01557 | Supervised Discrete Hashing | http://arxiv.org/abs/1503.01557 | id:1503.01557 author:Fumin Shen, Chunhua Shen, Wei Liu, Heng Tao Shen category:cs.CV  published:2015-03-05 summary:This paper has been withdrawn by the authour. version:3
arxiv-1411-7346 | A Chasm Between Identity and Equivalence Testing with Conditional Queries | http://arxiv.org/abs/1411.7346 | id:1411.7346 author:Jayadev Acharya, Clément L. Canonne, Gautam Kamath category:cs.DS cs.CC cs.LG math.PR math.ST stat.TH  published:2014-11-26 summary:A recent model for property testing of probability distributions enables tremendous savings in the sample complexity of testing algorithms, by allowing them to condition the sampling on subsets of the domain. In particular, Canonne, Ron, and Servedio showed that, in this setting, testing identity of an unknown distribution $D$ (i.e., whether $D=D^*$ for an explicitly known $D^*$) can be done with a constant number of samples, independent of the support size $n$ -- in contrast to the required $\sqrt{n}$ in the standard sampling model. However, it was unclear whether the same held for the case of testing equivalence, where both distributions are unknown. Indeed, while Canonne, Ron, and Servedio established a $\mathrm{poly}\log(n)$-query upper bound for equivalence testing, very recently brought down to $\tilde O(\log\log n)$ by Falahatgar et al., whether a dependence on the domain size $n$ is necessary was still open, and explicitly posed by Fischer at the Bertinoro Workshop on Sublinear Algorithms. In this work, we answer the question in the positive, showing that any testing algorithm for equivalence must make $\Omega(\sqrt{\log\log n})$ queries in the conditional sampling model. Interestingly, this demonstrates an intrinsic qualitative gap between identity and equivalence testing, absent in the standard sampling model (where both problems have sampling complexity $n^{\Theta(1)}$). Turning to another question, we investigate the complexity of support size estimation. We provide a doubly-logarithmic upper bound for the adaptive version of this problem, generalizing work of Ron and Tsur to our weaker model. We also establish a logarithmic lower bound for the non-adaptive version of this problem. This latter result carries on to the related problem of non-adaptive uniformity testing, an exponential improvement over previous results that resolves an open question of Chakraborty et al. version:2
arxiv-1504-04770 | Online Inference for Relation Extraction with a Reduced Feature Set | http://arxiv.org/abs/1504.04770 | id:1504.04770 author:Maxim Rabinovich, Cédric Archambeau category:cs.CL cs.LG  published:2015-04-18 summary:Access to web-scale corpora is gradually bringing robust automatic knowledge base creation and extension within reach. To exploit these large unannotated---and extremely difficult to annotate---corpora, unsupervised machine learning methods are required. Probabilistic models of text have recently found some success as such a tool, but scalability remains an obstacle in their application, with standard approaches relying on sampling schemes that are known to be difficult to scale. In this report, we therefore present an empirical assessment of the sublinear time sparse stochastic variational inference (SSVI) scheme applied to RelLDA. We demonstrate that online inference leads to relatively strong qualitative results but also identify some of its pathologies---and those of the model---which will need to be overcome if SSVI is to be used for large-scale relation extraction. version:1
arxiv-1504-04763 | Understanding the Fisher Vector: a multimodal part model | http://arxiv.org/abs/1504.04763 | id:1504.04763 author:David Novotný, Diane Larlus, Florent Perronnin, Andrea Vedaldi category:cs.CV  published:2015-04-18 summary:Fisher Vectors and related orderless visual statistics have demonstrated excellent performance in object detection, sometimes superior to established approaches such as the Deformable Part Models. However, it remains unclear how these models can capture complex appearance variations using visual codebooks of limited sizes and coarse geometric information. In this work, we propose to interpret Fisher-Vector-based object detectors as part-based models. Through the use of several visualizations and experiments, we show that this is a useful insight to explain the good performance of the model. Furthermore, we reveal for the first time several interesting properties of the FV, including its ability to work well using only a small subset of input patches and visual words. Finally, we discuss the relation of the FV and DPM detectors, pointing out differences and commonalities between them. version:1
arxiv-1504-04756 | Time Resolution Dependence of Information Measures for Spiking Neurons: Atoms, Scaling, and Universality | http://arxiv.org/abs/1504.04756 | id:1504.04756 author:Sarah E. Marzen, Michael R. DeWeese, James P. Crutchfield category:q-bio.NC cond-mat.dis-nn cs.NE math.PR nlin.CD  published:2015-04-18 summary:The mutual information between stimulus and spike-train response is commonly used to monitor neural coding efficiency, but neuronal computation broadly conceived requires more refined and targeted information measures of input-output joint processes. A first step towards that larger goal is to develop information measures for individual output processes, including information generation (entropy rate), stored information (statistical complexity), predictable information (excess entropy), and active information accumulation (bound information rate). We calculate these for spike trains generated by a variety of noise-driven integrate-and-fire neurons as a function of time resolution and for alternating renewal processes. We show that their time-resolution dependence reveals coarse-grained structural properties of interspike interval statistics; e.g., $\tau$-entropy rates that diverge less quickly than the firing rate indicate interspike interval correlations. We also find evidence that the excess entropy and regularized statistical complexity of different types of integrate-and-fire neurons are universal in the continuous-time limit in the sense that they do not depend on mechanism details. This suggests a surprising simplicity in the spike trains generated by these model neurons. Interestingly, neurons with gamma-distributed ISIs and neurons whose spike trains are alternating renewal processes do not fall into the same universality class. These results lead to two conclusions. First, the dependence of information measures on time resolution reveals mechanistic details about spike train generation. Second, information measures can be used as model selection tools for analyzing spike train processes. version:1
arxiv-1504-04751 | A Knowledge-poor Pronoun Resolution System for Turkish | http://arxiv.org/abs/1504.04751 | id:1504.04751 author:Dilek Küçük, Meltem Turhan Yöndem category:cs.CL  published:2015-04-18 summary:A pronoun resolution system which requires limited syntactic knowledge to identify the antecedents of personal and reflexive pronouns in Turkish is presented. As in its counterparts for languages like English, Spanish and French, the core of the system is the constraints and preferences determined empirically. In the evaluation phase, it performed considerably better than the baseline algorithm used for comparison. The system is significant for its being the first fully specified knowledge-poor computational framework for pronoun resolution in Turkish where Turkish possesses different structural properties from the languages for which knowledge-poor systems had been developed. version:1
arxiv-1504-04740 | On the consistency of Multithreshold Entropy Linear Classifier | http://arxiv.org/abs/1504.04740 | id:1504.04740 author:Wojciech Marian Czarnecki category:cs.LG stat.ML  published:2015-04-18 summary:Multithreshold Entropy Linear Classifier (MELC) is a recent classifier idea which employs information theoretic concept in order to create a multithreshold maximum margin model. In this paper we analyze its consistency over multithreshold linear models and show that its objective function upper bounds the amount of misclassified points in a similar manner like hinge loss does in support vector machines. For further confirmation we also conduct some numerical experiments on five datasets. version:1
arxiv-1504-04739 | Fast optimization of Multithreshold Entropy Linear Classifier | http://arxiv.org/abs/1504.04739 | id:1504.04739 author:Rafal Jozefowicz, Wojciech Marian Czarnecki category:cs.LG stat.ML  published:2015-04-18 summary:Multithreshold Entropy Linear Classifier (MELC) is a density based model which searches for a linear projection maximizing the Cauchy-Schwarz Divergence of dataset kernel density estimation. Despite its good empirical results, one of its drawbacks is the optimization speed. In this paper we analyze how one can speed it up through solving an approximate problem. We analyze two methods, both similar to the approximate solutions of the Kernel Density Estimation querying and provide adaptive schemes for selecting a crucial parameters based on user-specified acceptable error. Furthermore we show how one can exploit well known conjugate gradients and L-BFGS optimizers despite the fact that the original optimization problem should be solved on the sphere. All above methods and modifications are tested on 10 real life datasets from UCI repository to confirm their practical usability. version:1
arxiv-1504-04716 | Gap Analysis of Natural Language Processing Systems with respect to Linguistic Modality | http://arxiv.org/abs/1504.04716 | id:1504.04716 author:Vishal Shukla category:cs.CL cs.AI  published:2015-04-18 summary:Modality is one of the important components of grammar in linguistics. It lets speaker to express attitude towards, or give assessment or potentiality of state of affairs. It implies different senses and thus has different perceptions as per the context. This paper presents an account showing the gap in the functionality of the current state of art Natural Language Processing (NLP) systems. The contextual nature of linguistic modality is studied. In this paper, the works and logical approaches employed by Natural Language Processing systems dealing with modality are reviewed. It sees human cognition and intelligence as multi-layered approach that can be implemented by intelligent systems for learning. Lastly, current flow of research going on within this field is talked providing futurology. version:1
arxiv-1406-5266 | Web-Scale Training for Face Identification | http://arxiv.org/abs/1406.5266 | id:1406.5266 author:Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf category:cs.CV  published:2014-06-20 summary:Scaling machine learning methods to very large datasets has attracted considerable attention in recent years, thanks to easy access to ubiquitous sensing and data from the web. We study face recognition and show that three distinct properties have surprising effects on the transferability of deep convolutional networks (CNN): (1) The bottleneck of the network serves as an important transfer learning regularizer, and (2) in contrast to the common wisdom, performance saturation may exist in CNN's (as the number of training samples grows); we propose a solution for alleviating this by replacing the naive random subsampling of the training set with a bootstrapping process. Moreover, (3) we find a link between the representation norm and the ability to discriminate in a target domain, which sheds lights on how such networks represent faces. Based on these discoveries, we are able to improve face recognition accuracy on the widely used LFW benchmark, both in the verification (1:1) and identification (1:N) protocols, and directly compare, for the first time, with the state of the art Commercially-Off-The-Shelf system and show a sizable leap in performance. version:2
arxiv-1412-4044 | Adaptive Stochastic Gradient Descent on the Grassmannian for Robust Low-Rank Subspace Recovery and Clustering | http://arxiv.org/abs/1412.4044 | id:1412.4044 author:Jun He, Yue Zhang category:stat.ML cs.CV cs.NA math.OC  published:2014-12-12 summary:In this paper, we present GASG21 (Grassmannian Adaptive Stochastic Gradient for $L_{2,1}$ norm minimization), an adaptive stochastic gradient algorithm to robustly recover the low-rank subspace from a large matrix. In the presence of column outliers, we reformulate the batch mode matrix $L_{2,1}$ norm minimization with rank constraint problem as a stochastic optimization approach constrained on Grassmann manifold. For each observed data vector, the low-rank subspace $\mathcal{S}$ is updated by taking a gradient step along the geodesic of Grassmannian. In order to accelerate the convergence rate of the stochastic gradient method, we choose to adaptively tune the constant step-size by leveraging the consecutive gradients. Furthermore, we demonstrate that with proper initialization, the K-subspaces extension, K-GASG21, can robustly cluster a large number of corrupted data vectors into a union of subspaces. Numerical experiments on synthetic and real data demonstrate the efficiency and accuracy of the proposed algorithms even with heavy column outliers corruption. version:2
arxiv-1504-04686 | Local, Private, Efficient Protocols for Succinct Histograms | http://arxiv.org/abs/1504.04686 | id:1504.04686 author:Raef Bassily, Adam Smith category:cs.CR cs.DS cs.LG F.2.0  published:2015-04-18 summary:We give efficient protocols and matching accuracy lower bounds for frequency estimation in the local model for differential privacy. In this model, individual users randomize their data themselves, sending differentially private reports to an untrusted server that aggregates them. We study protocols that produce a succinct histogram representation of the data. A succinct histogram is a list of the most frequent items in the data (often called "heavy hitters") along with estimates of their frequencies; the frequency of all other items is implicitly estimated as 0. If there are $n$ users whose items come from a universe of size $d$, our protocols run in time polynomial in $n$ and $\log(d)$. With high probability, they estimate the accuracy of every item up to error $O\left(\sqrt{\log(d)/(\epsilon^2n)}\right)$ where $\epsilon$ is the privacy parameter. Moreover, we show that this much error is necessary, regardless of computational efficiency, and even for the simple setting where only one item appears with significant frequency in the data set. Previous protocols (Mishra and Sandler, 2006; Hsu, Khanna and Roth, 2012) for this task either ran in time $\Omega(d)$ or had much worse error (about $\sqrt[6]{\log(d)/(\epsilon^2n)}$), and the only known lower bound on error was $\Omega(1/\sqrt{n})$. We also adapt a result of McGregor et al (2010) to the local setting. In a model with public coins, we show that each user need only send 1 bit to the server. For all known local protocols (including ours), the transformation preserves computational efficiency. version:1
arxiv-1408-3727 | Inverse Reinforcement Learning with Multi-Relational Chains for Robot-Centered Smart Home | http://arxiv.org/abs/1408.3727 | id:1408.3727 author:Kun Li, Max Q. -H. Meng category:cs.RO cs.LG  published:2014-08-16 summary:In a robot-centered smart home, the robot observes the home states with its own sensors, and then it can change certain object states according to an operator's commands for remote operations, or imitate the operator's behaviors in the house for autonomous operations. To model the robot's imitation of the operator's behaviors in a dynamic indoor environment, we use multi-relational chains to describe the changes of environment states, and apply inverse reinforcement learning to encoding the operator's behaviors with a learned reward function. We implement this approach with a mobile robot, and do five experiments to include increasing training days, object numbers, and action types. Besides, a baseline method by directly recording the operator's behaviors is also implemented, and comparison is made on the accuracy of home state evaluation and the accuracy of robot action selection. The results show that the proposed approach handles dynamic environment well, and guides the robot's actions in the house more accurately. version:5
arxiv-1504-04666 | Unsupervised Dependency Parsing: Let's Use Supervised Parsers | http://arxiv.org/abs/1504.04666 | id:1504.04666 author:Phong Le, Willem Zuidema category:cs.CL cs.LG  published:2015-04-18 summary:We present a self-training approach to unsupervised dependency parsing that reuses existing supervised and unsupervised parsing algorithms. Our approach, called `iterated reranking' (IR), starts with dependency trees generated by an unsupervised parser, and iteratively improves these trees using the richer probability models used in supervised parsing that are in turn trained on these trees. Our system achieves 1.8% accuracy higher than the state-of-the-part parser of Spitkovsky et al. (2013) on the WSJ corpus. version:1
arxiv-1503-02510 | Compositional Distributional Semantics with Long Short Term Memory | http://arxiv.org/abs/1503.02510 | id:1503.02510 author:Phong Le, Willem Zuidema category:cs.CL cs.AI cs.LG  published:2015-03-09 summary:We are proposing an extension of the recursive neural network that makes use of a variant of the long short-term memory architecture. The extension allows information low in parse trees to be stored in a memory register (the `memory cell') and used much later higher up in the parse tree. This provides a solution to the vanishing gradient problem and allows the network to capture long range dependencies. Experimental results show that our composition outperformed the traditional neural-network composition on the Stanford Sentiment Treebank. version:2
arxiv-1504-04660 | A spectral optical flow method for determining velocities from digital imagery | http://arxiv.org/abs/1504.04660 | id:1504.04660 author:Neal Hurlburt, Steve Jaffey category:cs.CV astro-ph.IM  published:2015-04-17 summary:We present a method for determining surface flows from solar images based upon optical flow techniques. We apply the method to sets of images obtained by a variety of solar imagers to assess its performance. The {\tt opflow3d} procedure is shown to extract accurate velocity estimates when provided perfect test data and quickly generates results consistent with completely distinct methods when applied on global scales. We also validate it in detail by comparing it to an established method when applied to high-resolution datasets and find that it provides comparable results without the need to tune, filter or otherwise preprocess the images before its application. version:1
arxiv-1504-04658 | Deep Karaoke: Extracting Vocals from Musical Mixtures Using a Convolutional Deep Neural Network | http://arxiv.org/abs/1504.04658 | id:1504.04658 author:Andrew J. R. Simpson, Gerard Roma, Mark D. Plumbley category:cs.SD cs.LG cs.NE 68Txx  published:2015-04-17 summary:Identification and extraction of singing voice from within musical mixtures is a key challenge in source separation and machine audition. Recently, deep neural networks (DNN) have been used to estimate 'ideal' binary masks for carefully controlled cocktail party speech separation problems. However, it is not yet known whether these methods are capable of generalizing to the discrimination of voice and non-voice in the context of musical mixtures. Here, we trained a convolutional DNN (of around a billion parameters) to provide probabilistic estimates of the ideal binary mask for separation of vocal sounds from real-world musical mixtures. We contrast our DNN results with more traditional linear methods. Our approach may be useful for automatic removal of vocal sounds from musical mixtures for 'karaoke' type applications. version:1
arxiv-1504-04651 | Biometrics for Child Vaccination and Welfare: Persistence of Fingerprint Recognition for Infants and Toddlers | http://arxiv.org/abs/1504.04651 | id:1504.04651 author:Anil K. Jain, Sunpreet S. Arora, Lacey Best-Rowden, Kai Cao, Prem Sewak Sudhish, Anjoo Bhatnagar category:cs.CV  published:2015-04-17 summary:With a number of emerging applications requiring biometric recognition of children (e.g., tracking child vaccination schedules, identifying missing children and preventing newborn baby swaps in hospitals), investigating the temporal stability of biometric recognition accuracy for children is important. The persistence of recognition accuracy of three of the most commonly used biometric traits (fingerprints, face and iris) has been investigated for adults. However, persistence of biometric recognition accuracy has not been studied systematically for children in the age group of 0-4 years. Given that very young children are often uncooperative and do not comprehend or follow instructions, in our opinion, among all biometric modalities, fingerprints are the most viable for recognizing children. This is primarily because it is easier to capture fingerprints of young children compared to other biometric traits, e.g., iris, where a child needs to stare directly towards the camera to initiate iris capture. In this report, we detail our initiative to investigate the persistence of fingerprint recognition for children in the age group of 0-4 years. Based on preliminary results obtained for the data collected in the first phase of our study, use of fingerprints for recognition of 0-4 year-old children appears promising. version:1
arxiv-1504-04646 | Performance Evaluation of Machine Learning Algorithms in Post-operative Life Expectancy in the Lung Cancer Patients | http://arxiv.org/abs/1504.04646 | id:1504.04646 author:Kwetishe Joro Danjuma category:cs.LG  published:2015-04-17 summary:The nature of clinical data makes it difficult to quickly select, tune and apply machine learning algorithms to clinical prognosis. As a result, a lot of time is spent searching for the most appropriate machine learning algorithms applicable in clinical prognosis that contains either binary-valued or multi-valued attributes. The study set out to identify and evaluate the performance of machine learning classification schemes applied in clinical prognosis of post-operative life expectancy in the lung cancer patients. Multilayer Perceptron, J48, and the Naive Bayes algorithms were used to train and test models on Thoracic Surgery datasets obtained from the University of California Irvine machine learning repository. Stratified 10-fold cross-validation was used to evaluate baseline performance accuracy of the classifiers. The comparative analysis shows that multilayer perceptron performed best with classification accuracy of 82.3%, J48 came out second with classification accuracy of 81.8%, and Naive Bayes came out the worst with classification accuracy of 74.4%. The quality and outcome of the chosen machine learning algorithms depends on the ingenuity of the clinical miner. version:1
arxiv-1504-03101 | Convex Learning of Multiple Tasks and their Structure | http://arxiv.org/abs/1504.03101 | id:1504.03101 author:Carlo Ciliberto, Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco category:cs.LG  published:2015-04-13 summary:Reducing the amount of human supervision is a key problem in machine learning and a natural approach is that of exploiting the relations (structure) among different tasks. This is the idea at the core of multi-task learning. In this context a fundamental question is how to incorporate the tasks structure in the learning problem.We tackle this question by studying a general computational framework that allows to encode a-priori knowledge of the tasks structure in the form of a convex penalty; in this setting a variety of previously proposed methods can be recovered as special cases, including linear and non-linear approaches. Within this framework, we show that tasks and their structure can be efficiently learned considering a convex optimization problem that can be approached by means of block coordinate methods such as alternating minimization and for which we prove convergence to the global minimum. version:2
arxiv-1412-6607 | Visual Scene Representations: Contrast, Scaling and Occlusion | http://arxiv.org/abs/1412.6607 | id:1412.6607 author:Stefano Soatto, Jingming Dong, Nikolaos Karianakis category:cs.CV  published:2014-12-20 summary:We study the structure of representations, defined as approximations of minimal sufficient statistics that are maximal invariants to nuisance factors, for visual data subject to scaling and occlusion of line-of-sight. We derive analytical expressions for such representations and show that, under certain restrictive assumptions, they are related to features commonly in use in the computer vision community. This link highlights the condition tacitly assumed by these descriptors, and also suggests ways to improve and generalize them. This new interpretation draws connections to the classical theories of sampling, hypothesis testing and group invariance. version:5
arxiv-1410-8151 | A comparison of dense region detectors for image search and fine-grained classification | http://arxiv.org/abs/1410.8151 | id:1410.8151 author:Ahmet Iscen, Giorgos Tolias, Philippe-Henri Gosselin, Hervé Jégou category:cs.CV  published:2014-10-29 summary:We consider a pipeline for image classification or search based on coding approaches like Bag of Words or Fisher vectors. In this context, the most common approach is to extract the image patches regularly in a dense manner on several scales. This paper proposes and evaluates alternative choices to extract patches densely. Beyond simple strategies derived from regular interest region detectors, we propose approaches based on super-pixels, edges, and a bank of Zernike filters used as detectors. The different approaches are evaluated on recent image retrieval and fine-grain classification benchmarks. Our results show that the regular dense detector is outperformed by other methods in most situations, leading us to improve the state of the art in comparable setups on standard retrieval and fined-grain benchmarks. As a byproduct of our study, we show that existing methods for blob and super-pixel extraction achieve high accuracy if the patches are extracted along the edges and not around the detected regions. version:3
arxiv-1504-04599 | Testing Closeness With Unequal Sized Samples | http://arxiv.org/abs/1504.04599 | id:1504.04599 author:Bhaswar B. Bhattacharya, Gregory Valiant category:cs.LG cs.IT math.IT math.ST stat.ML stat.TH  published:2015-04-17 summary:We consider the problem of closeness testing for two discrete distributions in the practically relevant setting of \emph{unequal} sized samples drawn from each of them. Specifically, given a target error parameter $\varepsilon > 0$, $m_1$ independent draws from an unknown distribution $p,$ and $m_2$ draws from an unknown distribution $q$, we describe a test for distinguishing the case that $p=q$ from the case that $ p-q _1 \geq \varepsilon$. If $p$ and $q$ are supported on at most $n$ elements, then our test is successful with high probability provided $m_1\geq n^{2/3}/\varepsilon^{4/3}$ and $m_2 = \Omega(\max\{\frac{n}{\sqrt m_1\varepsilon^2}, \frac{\sqrt n}{\varepsilon^2}\});$ we show that this tradeoff is optimal throughout this range, to constant factors. These results extend the recent work of Chan et al. who established the sample complexity when the two samples have equal sizes, and tightens the results of Acharya et al. by polynomials factors in both $n$ and $\varepsilon$. As a consequence, we obtain an algorithm for estimating the mixing time of a Markov chain on $n$ states up to a $\log n$ factor that uses $\tilde{O}(n^{3/2} \tau_{mix})$ queries to a "next node" oracle, improving upon the $\tilde{O}(n^{5/3}\tau_{mix})$ query algorithm of Batu et al. Finally, we note that the core of our testing algorithm is a relatively simple statistic that seems to perform well in practice, both on synthetic data and on natural language data. version:1
arxiv-1504-04588 | The Nataf-Beta Random Field Classifier: An Extension of the Beta Conjugate Prior to Classification Problems | http://arxiv.org/abs/1504.04588 | id:1504.04588 author:James-A. Goulet category:cs.LG I.5.2  published:2015-04-17 summary:This paper presents the Nataf-Beta Random Field Classifier, a discriminative approach that extends the applicability of the Beta conjugate prior to classification problems. The approach's key feature is to model the probability of a class conditional on attribute values as a random field whose marginals are Beta distributed, and where the parameters of marginals are themselves described by random fields. Although the classification accuracy of the approach proposed does not statistically outperform the best accuracies reported in the literature, it ranks among the top tier for the six benchmark datasets tested. The Nataf-Beta Random Field Classifier is suited as a general purpose classification approach for real-continuous and real-integer attribute value problems. version:1
arxiv-1305-1495 | GReTA - a novel Global and Recursive Tracking Algorithm in three dimensions | http://arxiv.org/abs/1305.1495 | id:1305.1495 author:Alessandro Attanasi, Andrea Cavagna, Lorenzo Del Castello, Irene Giardina, Asja Jelic, Stefania Melillo, Leonardo Parisi, Fabio Pellacini, Edward Shen, Edmondo Silvestri, Massimiliano Viale category:q-bio.QM cs.CV  published:2013-05-07 summary:Tracking multiple moving targets allows quantitative measure of the dynamic behavior in systems as diverse as animal groups in biology, turbulence in fluid dynamics and crowd and traffic control. In three dimensions, tracking several targets becomes increasingly hard since optical occlusions are very likely, i.e. two featureless targets frequently overlap for several frames. Occlusions are particularly frequent in biological groups such as bird flocks, fish schools, and insect swarms, a fact that has severely limited collective animal behavior field studies in the past. This paper presents a 3D tracking method that is robust in the case of severe occlusions. To ensure robustness, we adopt a global optimization approach that works on all objects and frames at once. To achieve practicality and scalability, we employ a divide and conquer formulation, thanks to which the computational complexity of the problem is reduced by orders of magnitude. We tested our algorithm with synthetic data, with experimental data of bird flocks and insect swarms and with public benchmark datasets, and show that our system yields high quality trajectories for hundreds of moving targets with severe overlap. The results obtained on very heterogeneous data show the potential applicability of our method to the most diverse experimental situations. version:3
arxiv-1504-04548 | Color Constancy Using CNNs | http://arxiv.org/abs/1504.04548 | id:1504.04548 author:Simone Bianco, Claudio Cusano, Raimondo Schettini category:cs.CV  published:2015-04-17 summary:In this work we describe a Convolutional Neural Network (CNN) to accurately predict the scene illumination. Taking image patches as input, the CNN works in the spatial domain without using hand-crafted features that are employed by most previous methods. The network consists of one convolutional layer with max pooling, one fully connected layer and three output nodes. Within the network structure, feature learning and regression are integrated into one optimization process, which leads to a more effective model for estimating scene illumination. This approach achieves state-of-the-art performance on a standard dataset of RAW images. Preliminary experiments on images with spatially varying illumination demonstrate the stability of the local illuminant estimation ability of our CNN. version:1
arxiv-1504-04531 | Hyperspectral pansharpening: a review | http://arxiv.org/abs/1504.04531 | id:1504.04531 author:Laetitia Loncan, Luis B. Almeida, José M. Bioucas-Dias, Xavier Briottet, Jocelyn Chanussot, Nicolas Dobigeon, Sophie Fabre, Wenzhi Liao, Giorgio A. Licciardi, Miguel Simões, Jean-Yves Tourneret, Miguel A. Veganzones, Gemine Vivone, Qi Wei, Naoto Yokoya category:cs.CV physics.data-an stat.AP  published:2015-04-17 summary:Pansharpening aims at fusing a panchromatic image with a multispectral one, to generate an image with the high spatial resolution of the former and the high spectral resolution of the latter. In the last decade, many algorithms have been presented in the literature for pansharpening using multispectral data. With the increasing availability of hyperspectral systems, these methods are now being adapted to hyperspectral images. In this work, we compare new pansharpening techniques designed for hyperspectral data with some of the state of the art methods for multispectral pansharpening, which have been adapted for hyperspectral data. Eleven methods from different classes (component substitution, multiresolution analysis, hybrid, Bayesian and matrix factorization) are analyzed. These methods are applied to three datasets and their effectiveness and robustness are evaluated with widely used performance indicators. In addition, all the pansharpening techniques considered in this paper have been implemented in a MATLAB toolbox that is made available to the community. version:1
arxiv-1503-08195 | Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet Representations, and Forecast Rankings | http://arxiv.org/abs/1503.08195 | id:1503.08195 author:Werner Ehm, Tilmann Gneiting, Alexander Jordan, Fabian Krüger category:math.ST stat.ME stat.ML stat.TH  published:2015-03-27 summary:In the practice of point prediction, it is desirable that forecasters receive a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. When evaluating and comparing competing forecasts, it is then critical that the scoring function used for these purposes be consistent for the functional at hand, in the sense that the expected score is minimized when following the directive. We show that any scoring function that is consistent for a quantile or an expectile functional, respectively, can be represented as a mixture of extremal scoring functions that form a linearly parameterized family. Scoring functions for the mean value and probability forecasts of binary events constitute important examples. The quantile and expectile functionals along with the respective extremal scoring functions admit appealing economic interpretations in terms of thresholds in decision making. The Choquet type mixture representations give rise to simple checks of whether a forecast dominates another in the sense that it is preferable under any consistent scoring function. In empirical settings it suffices to compare the average scores for only a finite number of extremal elements. Plots of the average scores with respect to the extremal scoring functions, which we call Murphy diagrams, permit detailed comparisons of the relative merits of competing forecasts. version:2
arxiv-1408-5661 | Asymptotic Accuracy of Bayesian Estimation for a Single Latent Variable | http://arxiv.org/abs/1408.5661 | id:1408.5661 author:Keisuke Yamazaki category:stat.ML cs.LG  published:2014-08-25 summary:In data science and machine learning, hierarchical parametric models, such as mixture models, are often used. They contain two kinds of variables: observable variables, which represent the parts of the data that can be directly measured, and latent variables, which represent the underlying processes that generate the data. Although there has been an increase in research on the estimation accuracy for observable variables, the theoretical analysis of estimating latent variables has not been thoroughly investigated. In a previous study, we determined the accuracy of a Bayes estimation for the joint probability of the latent variables in a dataset, and we proved that the Bayes method is asymptotically more accurate than the maximum-likelihood method. However, the accuracy of the Bayes estimation for a single latent variable remains unknown. In the present paper, we derive the asymptotic expansions of the error functions, which are defined by the Kullback-Leibler divergence, for two types of single-variable estimations when the statistical regularity is satisfied. Our results indicate that the accuracies of the Bayes and maximum-likelihood methods are asymptotically equivalent and clarify that the Bayes method is only advantageous for multivariable estimations. version:3
arxiv-1502-05678 | VIP: Finding Important People in Images | http://arxiv.org/abs/1502.05678 | id:1502.05678 author:Clint Solomon Mathialagan, Andrew C. Gallagher, Dhruv Batra category:cs.CV  published:2015-02-19 summary:People preserve memories of events such as birthdays, weddings, or vacations by capturing photos, often depicting groups of people. Invariably, some individuals in the image are more important than others given the context of the event. This paper analyzes the concept of the importance of individuals in group photographs. We address two specific questions -- Given an image, who are the most important individuals in it? Given multiple images of a person, which image depicts the person in the most important role? We introduce a measure of importance of people in images and investigate the correlation between importance and visual saliency. We find that not only can we automatically predict the importance of people from purely visual cues, incorporating this predicted importance results in significant improvement in applications such as im2text (generating sentences that describe images of groups of people). version:2
arxiv-1504-04421 | Feasibility Preserving Constraint-Handling Strategies for Real Parameter Evolutionary Optimization | http://arxiv.org/abs/1504.04421 | id:1504.04421 author:Nikhil Padhye, Pulkit Mittal, Kalyanmoy Deb category:cs.NE  published:2015-04-17 summary:Evolutionary Algorithms (EAs) are being routinely applied for a variety of optimization tasks, and real-parameter optimization in the presence of constraints is one such important area. During constrained optimization EAs often create solutions that fall outside the feasible region; hence a viable constraint- handling strategy is needed. This paper focuses on the class of constraint-handling strategies that repair infeasible solutions by bringing them back into the search space and explicitly preserve feasibility of the solutions. Several existing constraint-handling strategies are studied, and two new single parameter constraint-handling methodologies based on parent-centric and inverse parabolic probability (IP) distribution are proposed. The existing and newly proposed constraint-handling methods are first studied with PSO, DE, GAs, and simulation results on four scalable test-problems under different location settings of the optimum are presented. The newly proposed constraint-handling methods exhibit robustness in terms of performance and also succeed on search spaces comprising up-to 500 variables while locating the optimum within an error of 10$^{-10}$. The working principle of the IP based methods is also demonstrated on (i) some generic constrained optimization problems, and (ii) a classic `Weld' problem from structural design and mechanics. The successful performance of the proposed methods clearly exhibits their efficacy as a generic constrained-handling strategy for a wide range of applications. version:1
arxiv-1504-01716 | An Empirical Evaluation of Deep Learning on Highway Driving | http://arxiv.org/abs/1504.01716 | id:1504.01716 author:Brody Huval, Tao Wang, Sameep Tandon, Jeff Kiske, Will Song, Joel Pazhayampallil, Mykhaylo Andriluka, Pranav Rajpurkar, Toki Migimatsu, Royce Cheng-Yue, Fernando Mujica, Adam Coates, Andrew Y. Ng category:cs.RO cs.CV  published:2015-04-07 summary:Numerous groups have applied a variety of deep learning techniques to computer vision problems in highway perception scenarios. In this paper, we presented a number of empirical evaluations of recent deep learning advances. Computer vision, combined with deep learning, has the potential to bring about a relatively inexpensive, robust solution to autonomous driving. To prepare deep learning for industry uptake and practical applications, neural networks will require large data sets that represent all possible driving environments and scenarios. We collect a large data set of highway data and apply deep learning and computer vision algorithms to problems such as car and lane detection. We show how existing convolutional neural networks (CNNs) can be used to perform lane and vehicle detection while running at frame rates required for a real-time system. Our results lend credence to the hypothesis that deep learning holds promise for autonomous driving. version:3
arxiv-1412-5083 | Random Forests Can Hash | http://arxiv.org/abs/1412.5083 | id:1412.5083 author:Qiang Qiu, Guillermo Sapiro, Alex Bronstein category:cs.CV cs.IR cs.LG stat.ML  published:2014-12-16 summary:Hash codes are a very efficient data representation needed to be able to cope with the ever growing amounts of data. We introduce a random forest semantic hashing scheme with information-theoretic code aggregation, showing for the first time how random forest, a technique that together with deep learning have shown spectacular results in classification, can also be extended to large-scale retrieval. Traditional random forest fails to enforce the consistency of hashes generated from each tree for the same class data, i.e., to preserve the underlying similarity, and it also lacks a principled way for code aggregation across trees. We start with a simple hashing scheme, where independently trained random trees in a forest are acting as hashing functions. We the propose a subspace model as the splitting function, and show that it enforces the hash consistency in a tree for data from the same class. We also introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code, producing a near-optimal unique hash for each class. Experiments on large-scale public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art hashing methods for retrieval tasks. version:3
arxiv-1412-7753 | Learning Longer Memory in Recurrent Neural Networks | http://arxiv.org/abs/1412.7753 | id:1412.7753 author:Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, Marc'Aurelio Ranzato category:cs.NE cs.LG  published:2014-12-24 summary:Recurrent neural network is a powerful model that learns temporal patterns in sequential data. For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modification of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming kind of a longer term memory. We evaluate our model in language modeling experiments, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997). version:2
arxiv-1504-04406 | Non-Uniform Stochastic Average Gradient Method for Training Conditional Random Fields | http://arxiv.org/abs/1504.04406 | id:1504.04406 author:Mark Schmidt, Reza Babanezhad, Mohamed Osama Ahmed, Aaron Defazio, Ann Clifton, Anoop Sarkar category:stat.ML cs.LG math.OC stat.CO  published:2015-04-16 summary:We apply stochastic average gradient (SAG) algorithms for training conditional random fields (CRFs). We describe a practical implementation that uses structure in the CRF gradient to reduce the memory requirement of this linearly-convergent stochastic gradient method, propose a non-uniform sampling scheme that substantially improves practical performance, and analyze the rate of convergence of the SAGA variant under non-uniform sampling. Our experimental results reveal that our method often significantly outperforms existing methods in terms of the training objective, and performs as well or better than optimally-tuned stochastic gradient methods in terms of test error. version:1
arxiv-1503-07921 | Breaking the News: First Impressions Matter on Online News | http://arxiv.org/abs/1503.07921 | id:1503.07921 author:Julio Reis, Fabrıcio Benevenuto, Pedro O. S. Vaz de Melo, Raquel Prates, Haewoon Kwak, Jisun An category:cs.CY cs.CL  published:2015-03-26 summary:A growing number of people are changing the way they consume news, replacing the traditional physical newspapers and magazines by their virtual online versions or/and weblogs. The interactivity and immediacy present in online news are changing the way news are being produced and exposed by media corporations. News websites have to create effective strategies to catch people's attention and attract their clicks. In this paper we investigate possible strategies used by online news corporations in the design of their news headlines. We analyze the content of 69,907 headlines produced by four major global media corporations during a minimum of eight consecutive months in 2014. In order to discover strategies that could be used to attract clicks, we extracted features from the text of the news headlines related to the sentiment polarity of the headline. We discovered that the sentiment of the headline is strongly related to the popularity of the news and also with the dynamics of the posted comments on that particular news. version:2
arxiv-1412-6881 | On Learning Vector Representations in Hierarchical Label Spaces | http://arxiv.org/abs/1412.6881 | id:1412.6881 author:Jinseok Nam, Johannes Fürnkranz category:cs.LG cs.CL stat.ML  published:2014-12-22 summary:An important problem in multi-label classification is to capture label patterns or underlying structures that have an impact on such patterns. This paper addresses one such problem, namely how to exploit hierarchical structures over labels. We present a novel method to learn vector representations of a label space given a hierarchy of labels and label co-occurrence patterns. Our experimental results demonstrate qualitatively that the proposed method is able to learn regularities among labels by exploiting a label hierarchy as well as label co-occurrences. It highlights the importance of the hierarchical information in order to obtain regularities which facilitate analogical reasoning over a label space. We also experimentally illustrate the dependency of the learned representations on the label hierarchy. version:3
arxiv-1412-6614 | In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning | http://arxiv.org/abs/1412.6614 | id:1412.6614 author:Behnam Neyshabur, Ryota Tomioka, Nathan Srebro category:cs.LG cs.AI cs.CV stat.ML  published:2014-12-20 summary:We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multilayer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning. version:4
arxiv-1409-0940 | High-performance Kernel Machines with Implicit Distributed Optimization and Randomization | http://arxiv.org/abs/1409.0940 | id:1409.0940 author:Vikas Sindhwani, Haim Avron category:stat.ML cs.DC cs.LG  published:2014-09-03 summary:In order to fully utilize "big data", it is often required to use "big models". Such models tend to grow with the complexity and size of the training data, and do not make strong parametric assumptions upfront on the nature of the underlying statistical dependencies. Kernel methods fit this need well, as they constitute a versatile and principled statistical methodology for solving a wide range of non-parametric modelling problems. However, their high computational costs (in storage and time) pose a significant barrier to their widespread adoption in big data applications. We propose an algorithmic framework and high-performance implementation for massive-scale training of kernel-based statistical models, based on combining two key technical ingredients: (i) distributed general purpose convex optimization, and (ii) the use of randomization to improve the scalability of kernel methods. Our approach is based on a block-splitting variant of the Alternating Directions Method of Multipliers, carefully reconfigured to handle very large random feature matrices, while exploiting hybrid parallelism typically found in modern clusters of multicore machines. Our implementation supports a variety of statistical learning tasks by enabling several loss functions, regularization schemes, kernels, and layers of randomized approximations for both dense and sparse datasets, in a highly extensible framework. We evaluate the ability of our framework to learn models on data from applications, and provide a comparison against existing sequential and parallel libraries. version:3
arxiv-1504-04317 | Towards a relation extraction framework for cyber-security concepts | http://arxiv.org/abs/1504.04317 | id:1504.04317 author:Corinne L. Jones, Robert A. Bridges, Kelly Huffer, John Goodall category:cs.IR cs.CL cs.CR H.3.3  published:2015-04-16 summary:In order to assist security analysts in obtaining information pertaining to their network, such as novel vulnerabilities, exploits, or patches, information retrieval methods tailored to the security domain are needed. As labeled text data is scarce and expensive, we follow developments in semi-supervised Natural Language Processing and implement a bootstrapping algorithm for extracting security entities and their relationships from text. The algorithm requires little input data, specifically, a few relations or patterns (heuristics for identifying relations), and incorporates an active learning component which queries the user on the most important decisions to prevent drifting from the desired relations. Preliminary testing on a small corpus shows promising results, obtaining precision of .82. version:1
arxiv-1406-2751 | Reweighted Wake-Sleep | http://arxiv.org/abs/1406.2751 | id:1406.2751 author:Jörg Bornschein, Yoshua Bengio category:cs.LG  published:2014-06-11 summary:Training deep directed graphical models with many hidden variables and performing inference remains a major challenge. Helmholtz machines and deep belief networks are such models, and the wake-sleep algorithm has been proposed to train them. The wake-sleep algorithm relies on training not just the directed generative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of latent given visible. We propose a novel interpretation of the wake-sleep algorithm which suggests that better estimators of the gradient can be obtained by sampling latent variables multiple times from the inference network. This view is based on importance sampling as an estimator of the likelihood, with the approximate inference network as a proposal distribution. This interpretation is confirmed experimentally, showing that better likelihood can be achieved with this reweighted wake-sleep procedure. Based on this interpretation, we propose that a sigmoidal belief network is not sufficiently powerful for the layers of the inference network in order to recover a good estimator of the posterior distribution of latent variables. Our experiments show that using a more powerful layer model, such as NADE, yields substantially better generative models. version:4
arxiv-1503-04287 | Metric Localization using Google Street View | http://arxiv.org/abs/1503.04287 | id:1503.04287 author:Pratik Agarwal, Wolfram Burgard, Luciano Spinello category:cs.RO cs.CV  published:2015-03-14 summary:Accurate metrical localization is one of the central challenges in mobile robotics. Many existing methods aim at localizing after building a map with the robot. In this paper, we present a novel approach that instead uses geotagged panoramas from the Google Street View as a source of global positioning. We model the problem of localization as a non-linear least squares estimation in two phases. The first estimates the 3D position of tracked feature points from short monocular camera sequences. The second computes the rigid body transformation between the Street View panoramas and the estimated points. The only input of this approach is a stream of monocular camera images and odometry estimates. We quantified the accuracy of the method by running the approach on a robotic platform in a parking lot by using visual fiducials as ground truth. Additionally, we applied the approach in the context of personal localization in a real urban scenario by using data from a Google Tango tablet. version:2
arxiv-1411-3815 | Predictive Encoding of Contextual Relationships for Perceptual Inference, Interpolation and Prediction | http://arxiv.org/abs/1411.3815 | id:1411.3815 author:Mingmin Zhao, Chengxu Zhuang, Yizhou Wang, Tai Sing Lee category:cs.LG cs.CV cs.NE  published:2014-11-14 summary:We propose a new neurally-inspired model that can learn to encode the global relationship context of visual events across time and space and to use the contextual information to modulate the analysis by synthesis process in a predictive coding framework. The model learns latent contextual representations by maximizing the predictability of visual events based on local and global contextual information through both top-down and bottom-up processes. In contrast to standard predictive coding models, the prediction error in this model is used to update the contextual representation but does not alter the feedforward input for the next layer, and is thus more consistent with neurophysiological observations. We establish the computational feasibility of this model by demonstrating its ability in several aspects. We show that our model can outperform state-of-art performances of gated Boltzmann machines (GBM) in estimation of contextual information. Our model can also interpolate missing events or predict future events in image sequences while simultaneously estimating contextual information. We show it achieves state-of-art performances in terms of prediction accuracy in a variety of tasks and possesses the ability to interpolate missing frames, a function that is lacking in GBM. version:6
arxiv-1502-04569 | Image Specificity | http://arxiv.org/abs/1502.04569 | id:1502.04569 author:Mainak Jas, Devi Parikh category:cs.CV  published:2015-02-16 summary:For some images, descriptions written by multiple people are consistent with each other. But for other images, descriptions across people vary considerably. In other words, some images are specific $-$ they elicit consistent descriptions from different people $-$ while other images are ambiguous. Applications involving images and text can benefit from an understanding of which images are specific and which ones are ambiguous. For instance, consider text-based image retrieval. If a query description is moderately similar to the caption (or reference description) of an ambiguous image, that query may be considered a decent match to the image. But if the image is very specific, a moderate similarity between the query and the reference description may not be sufficient to retrieve the image. In this paper, we introduce the notion of image specificity. We present two mechanisms to measure specificity given multiple descriptions of an image: an automated measure and a measure that relies on human judgement. We analyze image specificity with respect to image content and properties to better understand what makes an image specific. We then train models to automatically predict the specificity of an image from image features alone without requiring textual descriptions of the image. Finally, we show that modeling image specificity leads to improvements in a text-based image retrieval application. version:2
arxiv-1412-6249 | Purine: A bi-graph based deep learning framework | http://arxiv.org/abs/1412.6249 | id:1412.6249 author:Min Lin, Shuo Li, Xuan Luo, Shuicheng Yan category:cs.NE cs.LG  published:2014-12-19 summary:In this paper, we introduce a novel deep learning framework, termed Purine. In Purine, a deep network is expressed as a bipartite graph (bi-graph), which is composed of interconnected operators and data tensors. With the bi-graph abstraction, networks are easily solvable with event-driven task dispatcher. We then demonstrate that different parallelism schemes over GPUs and/or CPUs on single or multiple PCs can be universally implemented by graph composition. This eases researchers from coding for various parallelization schemes, and the same dispatcher can be used for solving variant graphs. Scheduled by the task dispatcher, memory transfers are fully overlapped with other computations, which greatly reduce the communication overhead and help us achieve approximate linear acceleration. version:5
arxiv-1504-04216 | Genetic algorithm implementation for effective document subject search | http://arxiv.org/abs/1504.04216 | id:1504.04216 author:V. K. Ivanov, P. I. Meskin category:cs.IR cs.NE  published:2015-04-16 summary:This paper describes the software implementation of genetic algorithm for identifying and selecting most relevant results received during sequentially executed subject search operations. Simulated evolutionary process generates sustainable and effective population of search queries, forms search pattern of documents or semantic core, creates relevant sets of required documents, allows automatic classification of search results. The paper discusses the features of subject search, justifies the use of a genetic algorithm, describes arguments of the fitness function and describes basic steps and parameters of the algorithm. version:1
arxiv-1506-06046 | Face Prediction Model for an Automatic Age-invariant Face Recognition System | http://arxiv.org/abs/1506.06046 | id:1506.06046 author:Poonam Yadav category:cs.CV cs.NE  published:2015-04-16 summary:Automated face recognition and identification softwares are becoming part of our daily life; it finds its abode not only with Facebook's auto photo tagging, Apple's iPhoto, Google's Picasa, Microsoft's Kinect, but also in Homeland Security Department's dedicated biometric face detection systems. Most of these automatic face identification systems fail where the effects of aging come into the picture. Little work exists in the literature on the subject of face prediction that accounts for aging, which is a vital part of the computer face recognition systems. In recent years, individual face components' (e.g. eyes, nose, mouth) features based matching algorithms have emerged, but these approaches are still not efficient. Therefore, in this work we describe a Face Prediction Model (FPM), which predicts human face aging or growth related image variation using Principle Component Analysis (PCA) and Artificial Neural Network (ANN) learning techniques. The FPM captures the facial changes, which occur with human aging and predicts the facial image with a few years of gap with an acceptable accuracy of face matching from 76 to 86%. version:1
arxiv-1503-01224 | Temporal Pyramid Pooling Based Convolutional Neural Networks for Action Recognition | http://arxiv.org/abs/1503.01224 | id:1503.01224 author:Peng Wang, Yuanzhouhan Cao, Chunhua Shen, Lingqiao Liu, Heng Tao Shen category:cs.CV  published:2015-03-04 summary:Encouraged by the success of Convolutional Neural Networks (CNNs) in image classification, recently much effort is spent on applying CNNs to video based action recognition problems. One challenge is that video contains a varying number of frames which is incompatible to the standard input format of CNNs. Existing methods handle this issue either by directly sampling a fixed number of frames or bypassing this issue by introducing a 3D convolutional layer which conducts convolution in spatial-temporal domain. To solve this issue, here we propose a novel network structure which allows an arbitrary number of frames as the network input. The key of our solution is to introduce a module consisting of an encoding layer and a temporal pyramid pooling layer. The encoding layer maps the activation from previous layers to a feature vector suitable for pooling while the temporal pyramid pooling layer converts multiple frame-level activations into a fixed-length video-level representation. In addition, we adopt a feature concatenation layer which combines appearance information and motion information. Compared with the frame sampling strategy, our method avoids the risk of missing any important frames. Compared with the 3D convolutional method which requires a huge video dataset for network training, our model can be learned on a small target dataset because we can leverage the off-the-shelf image-level CNN for model parameter initialization. Experiments on two challenging datasets, Hollywood2 and HMDB51, demonstrate that our method achieves superior performance over state-of-the-art methods while requiring much fewer training data. version:2
arxiv-1504-04184 | Multichannel sparse recovery of complex-valued signals using Huber's criterion | http://arxiv.org/abs/1504.04184 | id:1504.04184 author:Esa Ollila category:cs.IT math.IT stat.CO stat.ML  published:2015-04-16 summary:In this paper, we generalize Huber's criterion to multichannel sparse recovery problem of complex-valued measurements where the objective is to find good recovery of jointly sparse unknown signal vectors from the given multiple measurement vectors which are different linear combinations of the same known elementary vectors. This requires careful characterization of robust complex-valued loss functions as well as Huber's criterion function for the multivariate sparse regression problem. We devise a greedy algorithm based on simultaneous normalized iterative hard thresholding (SNIHT) algorithm. Unlike the conventional SNIHT method, our algorithm, referred to as HUB-SNIHT, is robust under heavy-tailed non-Gaussian noise conditions, yet has a negligible performance loss compared to SNIHT under Gaussian noise. Usefulness of the method is illustrated in source localization application with sensor arrays. version:1
arxiv-1412-6418 | Inducing Semantic Representation from Text by Jointly Predicting and Factorizing Relations | http://arxiv.org/abs/1412.6418 | id:1412.6418 author:Ivan Titov, Ehsan Khoddam category:cs.CL cs.LG stat.ML  published:2014-12-19 summary:In this work, we propose a new method to integrate two recent lines of work: unsupervised induction of shallow semantics (e.g., semantic roles) and factorization of relations in text and knowledge bases. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the language. version:3
arxiv-1412-7110 | Learning linearly separable features for speech recognition using convolutional neural networks | http://arxiv.org/abs/1412.7110 | id:1412.7110 author:Dimitri Palaz, Mathew Magimai Doss, Ronan Collobert category:cs.LG cs.CL cs.NE  published:2014-12-22 summary:Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. These features are extracted based on prior knowledge such as, speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. This system was shown to yield similar or better performance than HMM/ANN based system on phoneme recognition task and on large scale continuous speech recognition task, using less parameters. Motivated by these studies, we investigate the use of simple linear classifier in the CNN-based framework. Thus, the network learns linearly separable features from raw speech. We show that such system yields similar or better performance than MLP based system using cepstral-based features as input. version:6
arxiv-1504-04114 | Actively Learning to Attract Followers on Twitter | http://arxiv.org/abs/1504.04114 | id:1504.04114 author:Nir Levine, Timothy A. Mann, Shie Mannor category:stat.ML cs.LG cs.SI  published:2015-04-16 summary:Twitter, a popular social network, presents great opportunities for on-line machine learning research. However, previous research has focused almost entirely on learning from passively collected data. We study the problem of learning to acquire followers through normative user behavior, as opposed to the mass following policies applied by many bots. We formalize the problem as a contextual bandit problem, in which we consider retweeting content to be the action chosen and each tweet (content) is accompanied by context. We design reward signals based on the change in followers. The result of our month long experiment with 60 agents suggests that (1) aggregating experience across agents can adversely impact prediction accuracy and (2) the Twitter community's response to different actions is non-stationary. Our findings suggest that actively learning on-line can provide deeper insights about how to attract followers than machine learning over passively collected data alone. version:1
arxiv-1504-04103 | Faster Algorithms for Testing under Conditional Sampling | http://arxiv.org/abs/1504.04103 | id:1504.04103 author:Moein Falahatgar, Ashkan Jafarpour, Alon Orlitsky, Venkatadheeraj Pichapathi, Ananda Theertha Suresh category:cs.DS cs.CC cs.LG math.ST stat.TH  published:2015-04-16 summary:There has been considerable recent interest in distribution-tests whose run-time and sample requirements are sublinear in the domain-size $k$. We study two of the most important tests under the conditional-sampling model where each query specifies a subset $S$ of the domain, and the response is a sample drawn from $S$ according to the underlying distribution. For identity testing, which asks whether the underlying distribution equals a specific given distribution or $\epsilon$-differs from it, we reduce the known time and sample complexities from $\tilde{\mathcal{O}}(\epsilon^{-4})$ to $\tilde{\mathcal{O}}(\epsilon^{-2})$, thereby matching the information theoretic lower bound. For closeness testing, which asks whether two distributions underlying observed data sets are equal or different, we reduce existing complexity from $\tilde{\mathcal{O}}(\epsilon^{-4} \log^5 k)$ to an even sub-logarithmic $\tilde{\mathcal{O}}(\epsilon^{-5} \log \log k)$ thus providing a better bound to an open problem in Bertinoro Workshop on Sublinear Algorithms [Fisher, 2004]. version:1
arxiv-1410-2386 | Bayesian Robust Tensor Factorization for Incomplete Multiway Data | http://arxiv.org/abs/1410.2386 | id:1410.2386 author:Qibin Zhao, Guoxu Zhou, Liqing Zhang, Andrzej Cichocki, Shun-ichi Amari category:cs.CV cs.LG  published:2014-10-09 summary:We propose a generative model for robust tensor factorization in the presence of both missing data and outliers. The objective is to explicitly infer the underlying low-CP-rank tensor capturing the global information and a sparse tensor capturing the local information (also considered as outliers), thus providing the robust predictive distribution over missing entries. The low-CP-rank tensor is modeled by multilinear interactions between multiple latent factors on which the column sparsity is enforced by a hierarchical prior, while the sparse tensor is modeled by a hierarchical view of Student-$t$ distribution that associates an individual hyperparameter with each element independently. For model learning, we develop an efficient closed-form variational inference under a fully Bayesian treatment, which can effectively prevent the overfitting problem and scales linearly with data size. In contrast to existing related works, our method can perform model selection automatically and implicitly without need of tuning parameters. More specifically, it can discover the groundtruth of CP rank and automatically adapt the sparsity inducing priors to various types of outliers. In addition, the tradeoff between the low-rank approximation and the sparse representation can be optimized in the sense of maximum model evidence. The extensive experiments and comparisons with many state-of-the-art algorithms on both synthetic and real-world datasets demonstrate the superiorities of our method from several perspectives. version:2
arxiv-1504-03834 | Comparisons of wavelet functions in QRS signal to noise ratio enhancement and detection accuracy | http://arxiv.org/abs/1504.03834 | id:1504.03834 author:Pornchai Phukpattaranont category:cs.CV cs.CE  published:2015-04-15 summary:We compare the capability of wavelet functions used for noise removal in preprocessing step of a QRS detection algorithm in the electrocardiogram (ECG) signal. The QRS signal to noise ratio enhancement and the detection accuracy of each wavelet function are evaluated using three measures: (1) the ratio of the maximum beat amplitude to the minimum beat amplitude (RMM), (2) the mean of absolute of time error (MATE), and (3) the figure of merit (FOM). Three wavelet functions from previous well-known publications are explored, i.e., Bior1.3, Db10, and Mexican hat wavelet functions. Results evaluated with the ECG signal from MIT-BIH arrhythmia database show that the Mexican hat wavelet function is better than the others. While the scale 8 of Mexican hat wavelet function can provide the best enhancement in QRS signal to noise ratio, the scale 4 of Mexican hat wavelet function can provide the best detection accuracy. These results may be combined and may enable the use of a single fixed threshold for all ECG records leading to the reduction in computational complexity of the QRS detection algorithm. version:2
arxiv-1504-04090 | Segmentation of Subspaces in Sequential Data | http://arxiv.org/abs/1504.04090 | id:1504.04090 author:Stephen Tierney, Yi Guo, Junbin Gao category:cs.CV  published:2015-04-16 summary:We propose Ordered Subspace Clustering (OSC) to segment data drawn from a sequentially ordered union of subspaces. Similar to Sparse Subspace Clustering (SSC) we formulate the problem as one of finding a sparse representation but include an additional penalty term to take care of sequential data. We test our method on data drawn from infrared hyper spectral, video and motion capture data. Experiments show that our method, OSC, outperforms the state of the art methods: Spatial Subspace Clustering (SpatSC), Low-Rank Representation (LRR) and SSC. version:1
