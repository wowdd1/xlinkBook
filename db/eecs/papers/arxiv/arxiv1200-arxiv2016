arxiv-1200-1 | Learning Robust Low-Rank Representations | http://arxiv.org/pdf/1209.6393v1.pdf | author:Pablo Sprechmann, Alex M. Bronstein, Guillermo Sapiro category:cs.LG math.OC published:2012-09-27 summary:In this paper we present a comprehensive framework for learning robustlow-rank representations by combining and extending recent ideas for learningfast sparse coding regressors with structured non-convex optimizationtechniques. This approach connects robust principal component analysis (RPCA)with dictionary learning techniques and allows its approximation via trainableencoders. We propose an efficient feed-forward architecture derived from anoptimization algorithm designed to exactly solve robust low dimensionalprojections. This architecture, in combination with different trainingobjective functions, allows the regressors to be used as online approximants ofthe exact offline RPCA problem or as RPCA-based neural networks. Simplemodifications of these encoders can handle challenging extensions, such as theinclusion of geometric data transformations. We present several examples withreal data from image, audio, and video processing. When used to approximateRPCA, our basic implementation shows several orders of magnitude speedupcompared to the exact solvers with almost no performance degradation. We showthe strength of the inclusion of learning to the RPCA approach on a musicsource separation application, where the encoders outperform the exact RPCAalgorithms, which are already reported to produce state-of-the-art results on abenchmark database. Our preliminary implementation on an iPad showsfaster-than-real-time performance with minimal latency.
arxiv-1200-2 | Trading Regret for Efficiency: Online Convex Optimization with Long Term Constraints | http://arxiv.org/pdf/1111.6082v3.pdf | author:Mehrdad Mahdavi, Rong Jin, Tianbao Yang category:cs.LG published:2011-11-25 summary:In this paper we propose a framework for solving constrained online convexoptimization problem. Our motivation stems from the observation that mostalgorithms proposed for online convex optimization require a projection ontothe convex set $\mathcal{K}$ from which the decisions are made. While forsimple shapes (e.g. Euclidean ball) the projection is straightforward, forarbitrary complex sets this is the main computational challenge and may beinefficient in practice. In this paper, we consider an alternative onlineconvex optimization problem. Instead of requiring decisions belong to$\mathcal{K}$ for all rounds, we only require that the constraints which definethe set $\mathcal{K}$ be satisfied in the long run. We show that our frameworkcan be utilized to solve a relaxed version of online learning with sideconstraints addressed in \cite{DBLP:conf/colt/MannorT06} and\cite{DBLP:conf/aaai/KvetonYTM08}. By turning the problem into an onlineconvex-concave optimization problem, we propose an efficient algorithm whichachieves $\tilde{\mathcal{O}}(\sqrt{T})$ regret bound and$\tilde{\mathcal{O}}(T^{3/4})$ bound for the violation of constraints. Then wemodify the algorithm in order to guarantee that the constraints are satisfiedin the long run. This gain is achieved at the price of getting$\tilde{\mathcal{O}}(T^{3/4})$ regret bound. Our second algorithm is based onthe Mirror Prox method \citep{nemirovski-2005-prox} to solve variationalinequalities which achieves $\tilde{\mathcal{\mathcal{O}}}(T^{2/3})$ bound forboth regret and the violation of constraints when the domain $\K$ can bedescribed by a finite number of linear constraints. Finally, we extend theresult to the setting where we only have partial access to the convex set$\mathcal{K}$ and propose a multipoint bandit feedback algorithm with the samebounds in expectation as our first algorithm.
arxiv-1200-3 | Sparse Ising Models with Covariates | http://arxiv.org/pdf/1209.6342v1.pdf | author:Jie Cheng, Elizaveta Levina, Pei Wang, Ji Zhu category:stat.ML cs.LG published:2012-09-27 summary:There has been a lot of work fitting Ising models to multivariate binary datain order to understand the conditional dependency relationships between thevariables. However, additional covariates are frequently recorded together withthe binary data, and may influence the dependence relationships. Motivated bysuch a dataset on genomic instability collected from tumor samples of severaltypes, we propose a sparse covariate dependent Ising model to study both theconditional dependency within the binary data and its relationship with theadditional covariates. This results in subject-specific Ising models, where thesubject's covariates influence the strength of association between the genes.As in all exploratory data analysis, interpretability of results is important,and we use L1 penalties to induce sparsity in the fitted graphs and in thenumber of selected covariates. Two algorithms to fit the model are proposed andcompared on a set of simulated data, and asymptotic results are established.The results on the tumor dataset and their biological significance arediscussed in detail.
arxiv-1200-4 | Mirror Descent Meets Fixed Share (and feels no regret) | http://arxiv.org/pdf/1202.3323v2.pdf | author:Nicolò Cesa-Bianchi, Pierre Gaillard, Gabor Lugosi, Gilles Stoltz category:cs.LG stat.ML published:2012-02-15 summary:Mirror descent with an entropic regularizer is known to achieve shiftingregret bounds that are logarithmic in the dimension. This is done using eithera carefully designed projection or by a weight sharing technique. Via a novelunified analysis, we show that these two approaches deliver essentiallyequivalent bounds on a notion of regret generalizing shifting, adaptive,discounted, and other related regrets. Our analysis also captures and extendsthe generalized weight sharing technique of Bousquet and Warmuth, and can berefined in several ways, including improvements for small losses and adaptivetuning of parameters.
arxiv-1200-5 | More Is Better: Large Scale Partially-supervised Sentiment Classification - Appendix | http://arxiv.org/pdf/1209.6329v1.pdf | author:Yoav Haimovitch, Koby Crammer, Shie Mannor category:cs.LG published:2012-09-27 summary:We describe a bootstrapping algorithm to learn from partially labeled data,and the results of an empirical study for using it to improve performance ofsentiment classification using up to 15 million unlabeled Amazon productreviews. Our experiments cover semi-supervised learning, domain adaptation andweakly supervised learning. In some cases our methods were able to reduce testerror by more than half using such large amount of data. NOTICE: This is only the supplementary material.
arxiv-1200-6 | Reclassification formula that provides to surpass K-means method | http://arxiv.org/pdf/1209.6204v1.pdf | author:M. Kharinov category:cs.CV cs.DS published:2012-09-27 summary:The paper presents a formula for the reclassification of multidimensionaldata points (columns of real numbers, "objects", "vectors", etc.). This formuladescribes the change in the total squared error caused by reclassification ofdata points from one cluster into another and prompts the way to calculate thesequence of optimal partitions, which are characterized by a minimum value ofthe total squared error E (weighted sum of within-class variance,within-cluster sum of squares WCSS etc.), i.e. the sum of squared distancesfrom each data point to its cluster center. At that source data points aretreated with repetitions allowed, and resulting clusters from differentpartitions, in general case, overlap each other. The final partitions arecharacterized by "equilibrium" stability with respect to the reclassificationof the data points, where the term "stability" means that any prescribedreclassification of data points does not increase the total squared error E. Itis important that conventional K-means method, in general case, providesgeneration of instable partitions with overstated values of the total squarederror E. The proposed method, based on the formula of reclassification, is moreefficient than K-means method owing to converting of any partition into stableone, as well as involving into the process of reclassification of certain setsof data points, in contrast to the classification of individual data pointsaccording to K-means method.
arxiv-1200-7 | Noise Influence on the Fuzzy-Linguistic Partitioning of Iris Code Space | http://arxiv.org/pdf/1209.6190v1.pdf | author:Iulia M. Motoc, Cristina M. Noaica, Robert Badea, Claudiu G. Ghica category:cs.CV 68U10 I.5 published:2012-09-27 summary:This paper analyses the set of iris codes stored or used in an irisrecognition system as an f-granular space. The f-granulation is given byidentifying in the iris code space the extensions of the fuzzy concepts wolves,goats, lambs and sheep (previously introduced by Doddington as 'animals' of thebiometric menagerie) - which together form a partitioning of the iris codespace. The main question here is how objective (stable / stationary) thispartitioning is when the iris segments are subject to noisy acquisition. Inorder to prove that the f-granulation of iris code space with respect to thefuzzy concepts that define the biometric menagerie is unstable in noisyconditions (is sensitive to noise), three types of noise (localvar, motionblur, salt and pepper) have been alternatively added to the iris segmentsextracted from University of Bath Iris Image Database. The results of 180exhaustive (all-to-all) iris recognition tests are presented and commentedhere.
arxiv-1200-8 | The Biometric Menagerie - A Fuzzy and Inconsistent Concept | http://arxiv.org/pdf/1209.6189v1.pdf | author:Nicolaie Popescu-Bodorin, Valentina E. Balas, Iulia M. Motoc category:cs.CV 68U10 I.5 published:2012-09-27 summary:This paper proves that in iris recognition, the concepts of sheep, goats,lambs and wolves - as proposed by Doddington and Yager in the so-calledBiometric Menagerie, are at most fuzzy and at least not quite well defined.They depend not only on the users or on their biometric templates, but also onthe parameters that calibrate the iris recognition system. This paper showsthat, in the case of iris recognition, the extensions of these concepts havevery unsharp and unstable (non-stationary) boundaries. The membership of a userto these categories is more often expressed as a degree (as a fuzzy value)rather than as a crisp value. Moreover, they are defined by fuzzy Sugeno rulesinstead of classical (crisp) definitions. For these reasons, we said that theBiometric Menagerie proposed by Doddington and Yager could be at most a fuzzyconcept of biometry, but even this status is conditioned by improving itsdefinition. All of these facts are confirmed experimentally in a series of 12exhaustive iris recognition tests undertaken for University of Bath Iris ImageDatabase while using three different iris code dimensions (256x16, 128x8 and64x4), two different iris texture encoders (Log-Gabor and Haar-Hilbert) and twodifferent types of safety models.
arxiv-1200-9 | The Horse Raced Past: Gardenpath Processing in Dynamical Systems | http://arxiv.org/pdf/1203.0145v2.pdf | author:Peter beim Graben category:cs.CL published:2012-03-01 summary:I pinpoint an interesting similarity between a recent account to rationalparsing and the treatment of sequential decisions problems in a dynamicalsystems approach. I argue that expectation-driven search heuristics aiming atfast computation resembles a high-risk decision strategy in favor of largetransition velocities. Hale's rational parser, combining generalizedleft-corner parsing with informed $\mathrm{A}^*$ search to resolve processingconflicts, explains gardenpath effects in natural sentence processing bymisleading estimates of future processing costs that are to be minimized. Onthe other hand, minimizing the duration of cognitive computations intime-continuous dynamical systems can be described by combining vector spacerepresentations of cognitive states by means of filler/role decompositions andsubsequent tensor product representations with the paradigm of stableheteroclinic sequences. Maximizing transition velocities according to ahigh-risk decision strategy could account for a fast race even between statesthat are apparently remote in representation space.
arxiv-1200-10 | Face Alignment Using Active Shape Model And Support Vector Machine | http://arxiv.org/pdf/1209.6151v1.pdf | author:Thai Hoang Le, Truong Nhat Vo category:cs.CV published:2012-09-27 summary:The Active Shape Model (ASM) is one of the most popular local texture modelsfor face alignment. It applies in many fields such as locating facial featuresin the image, face synthesis, etc. However, the experimental results show thatthe accuracy of the classical ASM for some applications is not high. This papersuggests some improvements on the classical ASM to increase the performance ofthe model in the application: face alignment. Four of our major improvementsinclude: i) building a model combining Sobel filter and the 2-D profile insearching face in image; ii) applying Canny algorithm for the enhancement edgeon image; iii) Support Vector Machine (SVM) is used to classify landmarks onface, in order to determine exactly location of these landmarks support forASM; iv)automatically adjust 2-D profile in the multi-level model based on thesize of the input image. The experimental results on Caltech face database andTechnical University of Denmark database (imm_face) show that our proposedimprovement leads to far better performance.
arxiv-1200-11 | Movie Popularity Classification based on Inherent Movie Attributes using C4.5,PART and Correlation Coefficient | http://arxiv.org/pdf/1209.6070v1.pdf | author:Khalid Ibnal Asad, Tanvir Ahmed, Md. Saiedur Rahman category:cs.LG cs.DB cs.IR H.2.8 published:2012-09-26 summary:Abundance of movie data across the internet makes it an obvious candidate formachine learning and knowledge discovery. But most researches are directedtowards bi-polar classification of movie or generation of a movierecommendation system based on reviews given by viewers on various internetsites. Classification of movie popularity based solely on attributes of a moviei.e. actor, actress, director rating, language, country and budget etc. hasbeen less highlighted due to large number of attributes that are associatedwith each movie and their differences in dimensions. In this paper, we proposeclassification scheme of pre-release movie popularity based on inherentattributes using C4.5 and PART classifier algorithm and define the relationbetween attributes of post release movies using correlation coefficient.
arxiv-1200-12 | Reproduction of Images by Gamut Mapping and Creation of New Test Charts in Prepress Process | http://arxiv.org/pdf/1209.6037v1.pdf | author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV published:2012-09-26 summary:With the advent of digital images the problem of keeping picturevisualization uniformity arises because each printing or scanning device hasits own color chart. So, universal color profiles are made by ICC to bringuniformity in various types of devices. Keeping that color profile in mindvarious new color charts are created and calibrated with the help of standardIT8 test charts available in the market. The main objective to colorreproduction is to produce the identical picture at device output. For thatprinciples for gamut mapping has been designed
arxiv-1200-13 | The Issue-Adjusted Ideal Point Model | http://arxiv.org/pdf/1209.6004v1.pdf | author:Sean M. Gerrish, David M. Blei category:stat.ML cs.LG stat.AP published:2012-09-26 summary:We develop a model of issue-specific voting behavior. This model can be usedto explore lawmakers' personal voting patterns of voting by issue area,providing an exploratory window into how the language of the law is correlatedwith political support. We derive approximate posterior inference algorithmsbased on variational methods. Across 12 years of legislative data, wedemonstrate both improvement in heldout prediction performance and the model'sutility in interpreting an inherently multi-dimensional space.
arxiv-1200-14 | Bayesian Mixture Models for Frequent Itemset Discovery | http://arxiv.org/pdf/1209.6001v1.pdf | author:Ruefei He, Jonathan Shapiro category:cs.LG cs.IR stat.ML published:2012-09-26 summary:In binary-transaction data-mining, traditional frequent itemset mining oftenproduces results which are not straightforward to interpret. To overcome thisproblem, probability models are often used to produce more compact andconclusive results, albeit with some loss of accuracy. Bayesian statistics havebeen widely used in the development of probability models in machine learningin recent years and these methods have many advantages, including theirabilities to avoid overfitting. In this paper, we develop two Bayesian mixturemodels with the Dirichlet distribution prior and the Dirichlet process (DP)prior to improve the previous non-Bayesian mixture model developed fortransaction dataset mining. We implement the inference of both mixture modelsusing two methods: a collapsed Gibbs sampling scheme and a variationalapproximation algorithm. Experiments in several benchmark problems have shownthat both mixture models achieve better performance than a non-Bayesian mixturemodel. The variational algorithm is the faster of the two approaches while theGibbs sampling method achieves a more accurate results. The Dirichlet processmixture model can automatically grow to a proper complexity for a betterapproximation. Once the model is built, it can be very fast to query and runanalysis on (typically 10 times faster than Eclat, as we will show in theexperiment section). However, these approaches also show that mixture modelsunderestimate the probabilities of frequent itemsets. Consequently, thesemodels have a higher sensitivity but a lower specificity.
arxiv-1200-15 | Subset Selection for Gaussian Markov Random Fields | http://arxiv.org/pdf/1209.5991v1.pdf | author:Satyaki Mahalanabis, Daniel Stefankovic category:cs.LG stat.ML 68Q32 published:2012-09-26 summary:Given a Gaussian Markov random field, we consider the problem of selecting asubset of variables to observe which minimizes the total expected squaredprediction error of the unobserved variables. We first show that finding anexact solution is NP-hard even for a restricted class of Gaussian Markov randomfields, called Gaussian free fields, which arise in semi-supervised learningand computer vision. We then give a simple greedy approximation algorithm forGaussian free fields on arbitrary graphs. Finally, we give a message passingalgorithm for general Gaussian Markov random fields on bounded tree-widthgraphs.
arxiv-1200-16 | PlaceRaider: Virtual Theft in Physical Spaces with Smartphones | http://arxiv.org/pdf/1209.5982v1.pdf | author:Robert Templeman, Zahid Rahman, David Crandall, Apu Kapadia category:cs.CR cs.CV published:2012-09-26 summary:As smartphones become more pervasive, they are increasingly targeted bymalware. At the same time, each new generation of smartphone featuresincreasingly powerful onboard sensor suites. A new strain of sensor malware hasbeen developing that leverages these sensors to steal information from thephysical environment (e.g., researchers have recently demonstrated how malwarecan listen for spoken credit card numbers through the microphone, or feelkeystroke vibrations using the accelerometer). Yet the possibilities of whatmalware can see through a camera have been understudied. This paper introducesa novel visual malware called PlaceRaider, which allows remote attackers toengage in remote reconnaissance and what we call virtual theft. Throughcompletely opportunistic use of the camera on the phone and other sensors,PlaceRaider constructs rich, three dimensional models of indoor environments.Remote burglars can thus download the physical space, study the environmentcarefully, and steal virtual objects from the environment (such as financialdocuments, information on computer monitors, and personally identifiableinformation). Through two human subject studies we demonstrate theeffectiveness of using mobile devices as powerful surveillance and virtualtheft platforms, and we suggest several possible defenses against visualmalware.
arxiv-1200-17 | Alternating Direction Methods for Latent Variable Gaussian Graphical Model Selection | http://arxiv.org/pdf/1206.1275v2.pdf | author:Shiqian Ma, Lingzhou Xue, Hui Zou category:math.OC stat.ML published:2012-06-06 summary:Chandrasekaran, Parrilo and Willsky (2010) proposed a convex optimizationproblem to characterize graphical model selection in the presence of unobservedvariables. This convex optimization problem aims to estimate an inversecovariance matrix that can be decomposed into a sparse matrix minus a low-rankmatrix from sample data. Solving this convex optimization problem is verychallenging, especially for large problems. In this paper, we propose twoalternating direction methods for solving this problem. The first method is toapply the classical alternating direction method of multipliers to solve theproblem as a consensus problem. The second method is a proximal gradient basedalternating direction method of multipliers. Our methods exploit and takeadvantage of the special structure of the problem and thus can solve largeproblems very efficiently. Global convergence result is established for theproposed methods. Numerical results on both synthetic data and gene expressiondata show that our methods usually solve problems with one million variables inone to two minutes, and are usually five to thirty five times faster than astate-of-the-art Newton-CG proximal point algorithm.
arxiv-1200-18 | Optimal Weighting of Multi-View Data with Low Dimensional Hidden States | http://arxiv.org/pdf/1209.5477v2.pdf | author:Yichao Lu, Dean P. Foster category:stat.ML cs.LG published:2012-09-25 summary:In Natural Language Processing (NLP) tasks, data often has the following twoproperties: First, data can be chopped into multi-views which has beensuccessfully used for dimension reduction purposes. For example, in topicclassification, every paper can be chopped into the title, the main text andthe references. However, it is common that some of the views are less noisierthan other views for supervised learning problems. Second, unlabeled data areeasy to obtain while labeled data are relatively rare. For example, articlesoccurred on New York Times in recent 10 years are easy to grab but having themclassified as 'Politics', 'Finance' or 'Sports' need human labor. Hence lessnoisy features are preferred before running supervised learning methods. Inthis paper we propose an unsupervised algorithm which optimally weightsfeatures from different views when these views are generated from a lowdimensional hidden state, which occurs in widely used models like MixtureGaussian Model, Hidden Markov Model (HMM) and Latent Dirichlet Allocation(LDA).
arxiv-1200-19 | Refinability of splines from lattice Voronoi cells | http://arxiv.org/pdf/1209.5826v1.pdf | author:Jorg Peters category:math.NA cs.CV 41A15, 65D07 published:2012-09-26 summary:Splines can be constructed by convolving the indicator function of theVoronoi cell of a lattice. This paper presents simple criteria that imply thatonly a small subset of such spline families can be refined: essentially thewell-known box splines and tensor-product splines. Among the many non-refinableconstructions are hex-splines and their generalization to non-Cartesianlattices. An example shows how non-refinable splines can exhibit increasedapproximation error upon refinement of the lattice.
arxiv-1200-20 | Natural Language Processing - A Survey | http://arxiv.org/pdf/1209.6238v1.pdf | author:Kevin Mote category:cs.CL published:2012-09-25 summary:The utility and power of Natural Language Processing (NLP) seems destined tochange our technological society in profound and fundamental ways. Howeverthere are, to date, few accessible descriptions of the science of NLP that havebeen written for a popular audience, or even for an audience of intelligent,but uninitiated scientists. This paper aims to provide just such an overview.In short, the objective of this article is to describe the purpose, proceduresand practical applications of NLP in a clear, balanced, and readable way. Wewill examine the most recent literature describing the methods and processes ofNLP, analyze some of the challenges that researchers are faced with, andbriefly survey some of the current and future applications of this science toIT research in general.
arxiv-1200-21 | Environmental Sounds Spectrogram Classification using Log-Gabor Filters and Multiclass Support Vector Machines | http://arxiv.org/pdf/1209.5756v1.pdf | author:Sameh Souli, Zied Lachiri category:cs.CV published:2012-09-25 summary:This paper presents novel approaches for efficient feature extraction usingenvironmental sound magnitude spectrogram. We propose approach based on thevisual domain. This approach included three methods. The first method is basedon extraction for each spectrogram a single log-Gabor filter followed by mutualinformation procedure. In the second method, the spectrogram is passed by thesame steps of the first method but with an averaged bank of 12 log-Gaborfilter. The third method consists of spectrogram segmentation into threepatches, and after that for each spectrogram patch we applied the secondmethod. The classification results prove that the second method is the mostefficient in our environmental sound classification system.
arxiv-1200-22 | Feature selection with test cost constraint | http://arxiv.org/pdf/1209.5601v1.pdf | author:Fan Min, Qinghua Hu, William Zhu category:cs.AI cs.LG published:2012-09-25 summary:Feature selection is an important preprocessing step in machine learning anddata mining. In real-world applications, costs, including money, time and otherresources, are required to acquire the features. In some cases, there is a testcost constraint due to limited resources. We shall deliberately select aninformative and cheap feature subset for classification. This paper proposesthe feature selection with test cost constraint problem for this issue. The newproblem has a simple form while described as a constraint satisfaction problem(CSP). Backtracking is a general algorithm for CSP, and it is efficient insolving the new problem on medium-sized data. As the backtracking algorithm isnot scalable to large datasets, a heuristic algorithm is also developed.Experimental results show that the heuristic algorithm can find the optimalsolution in most cases. We also redefine some existing feature selectionproblems in rough sets, especially in decision-theoretic rough sets, from theviewpoint of CSP. These new definitions provide insight to some new researchdirections.
arxiv-1200-23 | Supervised Blockmodelling | http://arxiv.org/pdf/1209.5561v1.pdf | author:Leto Peel category:cs.LG cs.SI stat.ML published:2012-09-25 summary:Collective classification models attempt to improve classificationperformance by taking into account the class labels of related instances.However, they tend not to learn patterns of interactions between classes and/ormake the assumption that instances of the same class link to each other(assortativity assumption). Blockmodels provide a solution to these issues,being capable of modelling assortative and disassortative interactions, andlearning the pattern of interactions in the form of a summary network. TheSupervised Blockmodel provides good classification performance using linkstructure alone, whilst simultaneously providing an interpretable summary ofnetwork interactions to allow a better understanding of the data. This workexplores three variants of supervised blockmodels of varying complexity andtests them on four structurally different real world networks.
arxiv-1200-24 | Towards a learning-theoretic analysis of spike-timing dependent plasticity | http://arxiv.org/pdf/1209.5549v1.pdf | author:David Balduzzi, Michel Besserve category:q-bio.NC cs.LG stat.ML published:2012-09-25 summary:This paper suggests a learning-theoretic perspective on how synapticplasticity benefits global brain functioning. We introduce a model, theselectron, that (i) arises as the fast time constant limit of leakyintegrate-and-fire neurons equipped with spiking timing dependent plasticity(STDP) and (ii) is amenable to theoretical analysis. We show that the selectronencodes reward estimates into spikes and that an error bound on spikes iscontrolled by a spiking margin and the sum of synaptic weights. Moreover, theefficacy of spikes (their usefulness to other reward maximizing selectrons)also depends on total synaptic strength. Finally, based on our analysis, wepropose a regularized version of STDP, and show the regularization improves therobustness of neuronal learning when faced with multiple stimuli.
arxiv-1200-25 | High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity | http://arxiv.org/pdf/1109.3714v4.pdf | author:Po-Ling Loh, Martin J. Wainwright category:math.ST cs.IT math.IT stat.ML stat.TH published:2011-09-16 summary:Although the standard formulations of prediction problems involvefully-observed and noiseless data drawn in an i.i.d. manner, many applicationsinvolve noisy and/or missing data, possibly involving dependence, as well. Westudy these issues in the context of high-dimensional sparse linear regression,and propose novel estimators for the cases of noisy, missing and/or dependentdata. Many standard approaches to noisy or missing data, such as those usingthe EM algorithm, lead to optimization problems that are inherently nonconvex,and it is difficult to establish theoretical guarantees on practicalalgorithms. While our approach also involves optimizing nonconvex programs, weare able to both analyze the statistical error associated with any globaloptimum, and more surprisingly, to prove that a simple algorithm based onprojected gradient descent will converge in polynomial time to a smallneighborhood of the set of all global minimizers. On the statistical side, weprovide nonasymptotic bounds that hold with high probability for the cases ofnoisy, missing and/or dependent data. On the computational side, we prove thatunder the same types of conditions required for statistical consistency, theprojected gradient descent algorithm is guaranteed to converge at a geometricrate to a near-global minimizer. We illustrate these theoretical predictionswith simulations, showing close agreement with the predicted scalings.
arxiv-1200-26 | Segmentation of Breast Regions in Mammogram Based on Density: A Review | http://arxiv.org/pdf/1209.5494v1.pdf | author:Nafiza Saidin, Harsa Amylia Mat Sakim, Umi Kalthum Ngah, Ibrahim Lutfi Shuaib category:cs.CV published:2012-09-25 summary:The focus of this paper is to review approaches for segmentation of breastregions in mammograms according to breast density. Studies based on densityhave been undertaken because of the relationship between breast cancer anddensity. Breast cancer usually occurs in the fibroglandular area of breasttissue, which appears bright on mammograms and is described as breast density.Most of the studies are focused on the classification methods for glandulartissue detection. Others highlighted on the segmentation methods forfibroglandular tissue, while few researchers performed segmentation of thebreast anatomical regions based on density. There have also been works on thesegmentation of other specific parts of breast regions such as either detectionof nipple position, skin-air interface or pectoral muscles. The problems on theevaluation performance of the segmentation results in relation to ground truthare also discussed in this paper.
arxiv-1200-27 | Learning a Common Substructure of Multiple Graphical Gaussian Models | http://arxiv.org/pdf/1203.0117v3.pdf | author:Satoshi Hara, Takashi Washio category:stat.ML published:2012-03-01 summary:Properties of data are frequently seen to vary depending on the sampledsituations, which usually changes along a time evolution or owing toenvironmental effects. One way to analyze such data is to find invariances, orrepresentative features kept constant over changes. The aim of this paper is toidentify one such feature, namely interactions or dependencies among variablesthat are common across multiple datasets collected under different conditions.To that end, we propose a common substructure learning (CSSL) framework basedon a graphical Gaussian model. We further present a simple learning algorithmbased on the Dual Augmented Lagrangian and the Alternating Direction Method ofMultipliers. We confirm the performance of CSSL over other existing techniquesin finding unchanging dependency structures in multiple datasets throughnumerical simulations on synthetic data and through a real world application toanomaly detection in automobile sensors.
arxiv-1200-28 | Model based neuro-fuzzy ASR on Texas processor | http://arxiv.org/pdf/1209.5417v1.pdf | author:Hesam Ekhtiyar, Mehdi Sheida, Somaye Sobati Moghadam category:cs.CV published:2012-09-24 summary:In this paper an algorithm for recognizing speech has been proposed. Therecognized speech is used to execute related commands which use the MFCC andtwo kind of classifiers, first one uses MLP and second one uses fuzzy inferencesystem as a classifier. The experimental results demonstrate the high gain andefficiency of the proposed algorithm. We have implemented this system based ongraphical design and tested on a fix point digital signal processor (DSP) of600 MHz, with reference DM6437-EVM of Texas instrument.
arxiv-1200-29 | Autoregressive short-term prediction of turning points using support vector regression | http://arxiv.org/pdf/1209.0127v2.pdf | author:Ran El-Yaniv, Alexandra Faynburd category:cs.LG cs.CE cs.NE published:2012-09-01 summary:This work is concerned with autoregressive prediction of turning points infinancial price sequences. Such turning points are critical local extremapoints along a series, which mark the start of new swings. Predicting thefuture time of such turning points or even their early or late identificationslightly before or after the fact has useful applications in economics andfinance. Building on recently proposed neural network model for turning pointprediction, we propose and study a new autoregressive model for predictingturning points of small swings. Our method relies on a known turning pointindicator, a Fourier enriched representation of price histories, and supportvector regression. We empirically examine the performance of the proposedmethod over a long history of the Dow Jones Industrial average. Our study showsthat the proposed method is superior to the previous neural network model, interms of trading performance of a simple trading application and also exhibitsa quantifiable advantage over the buy-and-hold benchmark.
arxiv-1200-30 | Conditional validity of inductive conformal predictors | http://arxiv.org/pdf/1209.2673v2.pdf | author:Vladimir Vovk category:cs.LG 68T05, 62G15 published:2012-09-12 summary:Conformal predictors are set predictors that are automatically valid in thesense of having coverage probability equal to or exceeding a given confidencelevel. Inductive conformal predictors are a computationally efficient versionof conformal predictors satisfying the same property of validity. However,inductive conformal predictors have been only known to control unconditionalcoverage probability. This paper explores various versions of conditionalvalidity and various ways to achieve them using inductive conformal predictorsand their modifications.
arxiv-1200-31 | Developing Improved Greedy Crossover to Solve Symmetric Traveling Salesman Problem | http://arxiv.org/pdf/1209.5339v1.pdf | author:Hassan Ismkhan, Kamran Zamanifar category:cs.NE published:2012-09-24 summary:The Traveling Salesman Problem (TSP) is one of the most famous optimizationproblems. Greedy crossover designed by Greffenstette et al, can be used whileSymmetric TSP (STSP) is resolved by Genetic Algorithm (GA). Researchers haveproposed several versions of greedy crossover. Here we propose improved versionof it. We compare our greedy crossover with some of recent crossovers, we useour greedy crossover and some recent crossovers in GA then compare crossoverson speed and accuracy.
arxiv-1200-32 | BPRS: Belief Propagation Based Iterative Recommender System | http://arxiv.org/pdf/1209.5335v1.pdf | author:Erman Ayday, Arash Einolghozati, Faramarz Fekri category:cs.LG published:2012-09-24 summary:In this paper we introduce the first application of the Belief Propagation(BP) algorithm in the design of recommender systems. We formulate therecommendation problem as an inference problem and aim to compute the marginalprobability distributions of the variables which represent the ratings to bepredicted. However, computing these marginal probability functions iscomputationally prohibitive for large-scale systems. Therefore, we utilize theBP algorithm to efficiently compute these functions. Recommendations for eachactive user are then iteratively computed by probabilistic message passing. Asopposed to the previous recommender algorithms, BPRS does not require solvingthe recommendation problem for all the users if it wishes to update therecommendations for only a single active. Further, BPRS computes therecommendations for each user with linear complexity and without requiring atraining period. Via computer simulations (using the 100K MovieLens dataset),we verify that BPRS iteratively reduces the error in the predicted ratings ofthe users until it converges. Finally, we confirm that BPRS is comparable tothe state of art methods such as Correlation-based neighborhood model (CorNgbr)and Singular Value Decomposition (SVD) in terms of rating and precisionaccuracy. Therefore, we believe that the BP-based recommendation algorithm is anew promising approach which offers a significant advantage on scalabilitywhile providing competitive accuracy for the recommender systems.
arxiv-1200-33 | Towards Large-scale and Ultrahigh Dimensional Feature Selection via Feature Generation | http://arxiv.org/pdf/1209.5260v1.pdf | author:Mingkui Tan, Ivor W. Tsang, Li Wang category:cs.LG published:2012-09-24 summary:In many real-world applications such as text mining, it is desirable toselect the most relevant features or variables to improve the generalizationability, or to provide a better interpretation of the prediction models. {Inthis paper, a novel adaptive feature scaling (AFS) scheme is proposed byintroducing a feature scaling {vector $\d \in [0, 1]^m$} to alleviate the biasproblem brought by the scaling bias of the diverse features.} By reformulatingthe resultant AFS model to semi-infinite programming problem, a novel featuregenerating method is presented to identify the most relevant features forclassification problems. In contrast to the traditional feature selectionmethods, the new formulation has the advantage of solving extremelyhigh-dimensional and large-scale problems. With an exact solution to theworst-case analysis in the identification of relevant features, the proposedfeature generating scheme converges globally. More importantly, the proposedscheme facilitates the group selection with or without special structures.Comprehensive experiments on a wide range of synthetic and real-world datasetsdemonstrate that the proposed method {achieves} better or competitiveperformance compared with the existing methods on (group) feature selection interms of generalization performance and training efficiency. The C++ and MATLABimplementations of our algorithm can be available at\emph{http://c2inet.sce.ntu.edu.sg/Mingkui/robust-FGM.rar}.
arxiv-1200-34 | On Move Pattern Trends in a Large Go Games Corpus | http://arxiv.org/pdf/1209.5251v1.pdf | author:Petr Baudiš, Josef Moudřík category:cs.AI cs.LG published:2012-09-24 summary:We process a large corpus of game records of the board game of Go and proposea way of extracting summary information on played moves. We then apply severalbasic data-mining methods on the summary information to identify the mostdifferentiating features within the summary information, and discuss theircorrespondence with traditional Go knowledge. We show statistically significantmappings of the features to player attributes such as playing strength orinformally perceived "playing style" (e.g. territoriality or aggressivity),describe accurate classifiers for these attributes, and propose applicationsincluding seeding real-work ranks of internet players, aiding in Go study andtuning of Go-playing programs, or contribution to Go-theoretical discussion onthe scope of "playing style".
arxiv-1200-35 | Spike Timing Dependent Competitive Learning in Recurrent Self Organizing Pulsed Neural Networks Case Study: Phoneme and Word Recognition | http://arxiv.org/pdf/1209.5245v1.pdf | author:Tarek Behi, Najet Arous, Noureddine Ellouze category:cs.CV cs.AI q-bio.NC published:2012-09-24 summary:Synaptic plasticity seems to be a capital aspect of the dynamics of neuralnetworks. It is about the physiological modifications of the synapse, whichhave like consequence a variation of the value of the synaptic weight. Theinformation encoding is based on the precise timing of single spike events thatis based on the relative timing of the pre- and post-synaptic spikes, localsynapse competitions within a single neuron and global competition via lateralconnections. In order to classify temporal sequences, we present in this paperhow to use a local hebbian learning, spike-timing dependent plasticity forunsupervised competitive learning, preserving self-organizing maps of spikingneurons. In fact we present three variants of self-organizing maps (SOM) withspike-timing dependent Hebbian learning rule, the Leaky Integrators Neurons(LIN), the Spiking_SOM and the recurrent Spiking_SOM (RSSOM) models. The casestudy of the proposed SOM variants is phoneme classification and wordrecognition in continuous speech and speaker independent.
arxiv-1200-36 | Making a Science of Model Search | http://arxiv.org/pdf/1209.5111v1.pdf | author:J. Bergstra, D. Yamins, D. D. Cox category:cs.CV cs.NE published:2012-09-23 summary:Many computer vision algorithms depend on a variety of parameter choices andsettings that are typically hand-tuned in the course of evaluating thealgorithm. While such parameter tuning is often presented as being incidentalto the algorithm, correctly setting these parameter choices is frequentlycritical to evaluating a method's full potential. Compounding matters, theseparameters often must be re-tuned when the algorithm is applied to a newproblem domain, and the tuning process itself often depends on personalexperience and intuition in ways that are hard to describe. Since theperformance of a given technique depends on both the fundamental quality of thealgorithm and the details of its tuning, it can be difficult to determinewhether a given technique is genuinely better, or simply better tuned. In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools to replacehand-tuning with a reproducible and unbiased optimization process. Our approachis to expose the underlying expression graph of how a performance metric (e.g.classification accuracy on validation examples) is computed from parametersthat govern not only how individual processing steps are applied, but evenwhich processing steps are included. A hyper parameter optimization algorithmtransforms this graph into a program for optimizing that performance metric.Our approach yields state of the art results on three disparate computer visionproblems: a face-matching verification task (LFW), a face identification task(PubFig83) and an object recognition task (CIFAR-10), using a single algorithm.More broadly, we argue that the formalization of a meta-model supports moreobjective, reproducible, and quantitative evaluation of computer visionalgorithms, and that it can serve as a valuable tool for guiding algorithmdevelopment.
arxiv-1200-37 | An Implementation of Computer Graphics as Prepress Image Enhancement Process | http://arxiv.org/pdf/1209.5041v1.pdf | author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV published:2012-09-23 summary:The production of a printed product involves three stages: prepress, theprinting process (press) itself, and finishing (post press). There are varioustypes of equipments (printers, scanners) and various qualities image arepresent in the market. These give different color rendering each time duringreproduction. So, a color key tool has been developed keeping Color ManagementScheme (CMS) in mind so that during reproduction no color rendering takes placeirrespective of use of any device and resolution level has also been improved.
arxiv-1200-38 | Image Classification and Optimized Image Reproduction | http://arxiv.org/pdf/1209.5040v1.pdf | author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV published:2012-09-23 summary:By taking into account the properties and limitations of the human visualsystem, images can be more efficiently compressed, colors more accuratelyreproduced, prints better rendered. To show all these advantages in this papernew adapted color charts have been created based on technical and visual imagecategory analysis. A number of tests have been carried out using extreme imageswith their key information strictly in dark and light areas. It was shown thatthe image categorization using the adapted color charts improves the analysisof relevant image information with regard to both the image gradation and thedetail reproduction. The images with key information in hi-key areas were alsotest printed using the adapted color charts.
arxiv-1200-39 | Creation of Digital Test Form for Prepress Department | http://arxiv.org/pdf/1209.5039v1.pdf | author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV published:2012-09-23 summary:The main problem in colour management in prepress department is lack ofavailability of literature on colour management and knowledge gap betweenprepress department and press department. So a digital test from has beencreated by Adobe Photoshop to analyse the ICC profile and to create a newprofile and this analysed data is used to study about various grey scale of RGBand CMYK images. That helps in conversion of image from RGB to CMYK in prepressdepartment.
arxiv-1200-40 | Fast Randomized Model Generation for Shapelet-Based Time Series Classification | http://arxiv.org/pdf/1209.5038v1.pdf | author:Daniel Gordon, Danny Hendler, Lior Rokach category:cs.LG published:2012-09-23 summary:Time series classification is a field which has drawn much attention over thepast decade. A new approach for classification of time series usesclassification trees based on shapelets. A shapelet is a subsequence extractedfrom one of the time series in the dataset. A disadvantage of this approach isthe time required for building the shapelet-based classification tree. Thesearch for the best shapelet requires examining all subsequences of all lengthsfrom all time series in the training set. A key goal of this work was to find an evaluation order of the shapeletsspace which enables fast convergence to an accurate model. The comparativeanalysis we conducted clearly indicates that a random evaluation order yieldsthe best results. Our empirical analysis of the distribution of high-qualityshapelets within the shapelets space provides insights into why randomizedshapelets sampling is superior to alternative evaluation orders. We present an algorithm for randomized model generation for shapelet-basedclassification that converges extremely quickly to a model with surprisinglyhigh accuracy after evaluating only an exceedingly small fraction of theshapelets space.
arxiv-1200-41 | Nominal Association Vector and Matrix | http://arxiv.org/pdf/1109.2553v2.pdf | author:Wenxue Huang, Yong Shi, Xiaogang Wang category:stat.ML published:2011-09-12 summary:When response variables are nominal and populations are cross-classified withrespect to multiple polytomies, questions often arise about the degree ofassociation of the responses with explanatory variables. When populations areknown, we introduce a nominal association vector and matrix to evaluate thedependence of a response variable with an explanatory variable. These measuresprovide detailed evaluations of nominal associations at both local and globallevels. We also define a general class of global association measures whichembraces the well known association measure by Goodman-Kruskal (1954). Theproposed association matrix also gives rise to the expected generalizedconfusion matrix in classification. The hierarchy of equivalence relationsdefined by the association vector and matrix are also shown.
arxiv-1200-42 | A Bayesian Nonparametric Approach to Image Super-resolution | http://arxiv.org/pdf/1209.5019v1.pdf | author:Gungor Polatkan, Mingyuan Zhou, Lawrence Carin, David Blei, Ingrid Daubechies category:cs.LG stat.ML published:2012-09-22 summary:Super-resolution methods form high-resolution images from low-resolutionimages. In this paper, we develop a new Bayesian nonparametric model forsuper-resolution. Our method uses a beta-Bernoulli process to learn a set ofrecurring visual patterns, called dictionary elements, from the data. Becauseit is nonparametric, the number of elements found is also determined from thedata. We test the results on both benchmark and natural images, comparing withseveral other models from the research literature. We perform large-scale humanevaluation experiments to assess the visual quality of the results. In a firstimplementation, we use Gibbs sampling to approximate the posterior. However,this algorithm is not feasible for large-scale data. To circumvent this, wethen develop an online variational Bayes (VB) algorithm. This algorithm findshigh quality dictionaries in a fraction of the time needed by the Gibbssampler.
arxiv-1200-43 | Is the k-NN classifier in high dimensions affected by the curse of dimensionality? | http://arxiv.org/pdf/1110.4347v3.pdf | author:Vladimir Pestov category:stat.ML 62H30, 68H05 I.2.6 published:2011-10-19 summary:There is an increasing body of evidence suggesting that exact nearestneighbour search in high-dimensional spaces is affected by the curse ofdimensionality at a fundamental level. Does it necessarily mean that the sameis true for k nearest neighbours based learning algorithms such as the k-NNclassifier? We analyse this question at a number of levels and show that theanswer is different at each of them. As our first main observation, we show theconsistency of a k approximate nearest neighbour classifier. However, theperformance of the classifier in very high dimensions is provably unstable. Asour second main observation, we point out that the existing model forstatistical learning is oblivious of dimension of the domain and so everylearning problem admits a universally consistent deterministic reduction to theone-dimensional case by means of a Borel isomorphism.
arxiv-1200-44 | Bellman Error Based Feature Generation using Random Projections on Sparse Spaces | http://arxiv.org/pdf/1207.5554v3.pdf | author:Mahdi Milani Fard, Yuri Grinberg, Amir-massoud Farahmand, Joelle Pineau, Doina Precup category:cs.LG stat.ML published:2012-07-23 summary:We address the problem of automatic generation of features for value functionapproximation. Bellman Error Basis Functions (BEBFs) have been shown to improvethe error of policy evaluation with function approximation, with a convergencerate similar to that of value iteration. We propose a simple, fast and robustalgorithm based on random projections to generate BEBFs for sparse featurespaces. We provide a finite sample analysis of the proposed method, and provethat projections logarithmic in the dimension of the original space are enoughto guarantee contraction in the error. Empirical results demonstrate thestrength of this method.
arxiv-1200-45 | A Note on the SPICE Method | http://arxiv.org/pdf/1209.4887v1.pdf | author:Cristian R. Rojas, Dimitrios Katselis, Håkan Hjalmarsson category:stat.ML cs.SY published:2012-09-21 summary:In this article, we analyze the SPICE method developed in [1], and establishits connections with other standard sparse estimation methods such as the Lassoand the LAD-Lasso. This result positions SPICE as a computationally efficienttechnique for the calculation of Lasso-type estimators. Conversely, thisconnection is very useful for establishing the asymptotic properties of SPICEunder several problem scenarios and for suggesting suitable modifications incases where the naive version of SPICE would not work.
arxiv-1200-46 | Scaling Multidimensional Inference for Structured Gaussian Processes | http://arxiv.org/pdf/1209.4120v2.pdf | author:Elad Gilboa, Yunus Saatçi, John P. Cunningham category:stat.ML published:2012-09-18 summary:Exact Gaussian Process (GP) regression has O(N^3) runtime for data size N,making it intractable for large N. Many algorithms for improving GP scalingapproximate the covariance with lower rank matrices. Other work has exploitedstructure inherent in particular covariance functions, including GPs withimplied Markov structure, and equispaced inputs (both enable O(N) runtime).However, these GP advances have not been extended to the multidimensional inputsetting, despite the preponderance of multidimensional applications. This paperintroduces and tests novel extensions of structured GPs to multidimensionalinputs. We present new methods for additive GPs, showing a novel connectionbetween the classic backfitting method and the Bayesian framework. To achieveoptimal accuracy-complexity tradeoff, we extend this model with a novel variantof projection pursuit regression. Our primary result -- projection pursuitGaussian Process Regression -- shows orders of magnitude speedup whilepreserving high accuracy. The natural second and third steps includenon-Gaussian observations and higher dimensional equispaced grid methods. Weintroduce novel techniques to address both of these necessary directions. Wethoroughly illustrate the power of these three advances on several datasets,achieving close performance to the naive Full GP at orders of magnitude lesscost.
arxiv-1200-47 | On Constrained Spectral Clustering and Its Applications | http://arxiv.org/pdf/1201.5338v2.pdf | author:Xiang Wang, Buyue Qian, Ian Davidson category:cs.LG stat.ML H.2.8 published:2012-01-25 summary:Constrained clustering has been well-studied for algorithms such as $K$-meansand hierarchical clustering. However, how to satisfy many constraints in thesealgorithmic settings has been shown to be intractable. One alternative toencode many constraints is to use spectral clustering, which remains adeveloping area. In this paper, we propose a flexible framework for constrainedspectral clustering. In contrast to some previous efforts that implicitlyencode Must-Link and Cannot-Link constraints by modifying the graph Laplacianor constraining the underlying eigenspace, we present a more natural andprincipled formulation, which explicitly encodes the constraints as part of aconstrained optimization problem. Our method offers several practicaladvantages: it can encode the degree of belief in Must-Link and Cannot-Linkconstraints; it guarantees to lower-bound how well the given constraints aresatisfied using a user-specified threshold; it can be solved deterministicallyin polynomial time through generalized eigendecomposition. Furthermore, byinheriting the objective function from spectral clustering and encoding theconstraints explicitly, much of the existing analysis of unconstrained spectralclustering techniques remains valid for our formulation. We validate theeffectiveness of our approach by empirical results on both artificial and realdatasets. We also demonstrate an innovative use of encoding large number ofconstraints: transfer learning via constraints.
arxiv-1200-48 | Probabilistic Auto-Associative Models and Semi-Linear PCA | http://arxiv.org/pdf/1209.4551v1.pdf | author:Serge Iovleff category:stat.AP stat.ML published:2012-09-20 summary:Auto-Associative models cover a large class of methods used in data analysis.In this paper, we describe the generals properties of these models when theprojection component is linear and we propose and test an easy to implementProbabilistic Semi-Linear Auto- Associative model in a Gaussian setting. Weshow it is a generalization of the PCA model to the semi-linear case. Numericalexperiments on simulated datasets and a real astronomical application highlightthe interest of this approach
arxiv-1200-49 | The Future of Neural Networks | http://arxiv.org/pdf/1209.4855v1.pdf | author:Sachin Lakra, T. V. Prasad, G. Ramakrishna category:cs.NE published:2012-09-20 summary:The paper describes some recent developments in neural networks and discussesthe applicability of neural networks in the development of a machine thatmimics the human brain. The paper mentions a new architecture, the pulsedneural network that is being considered as the next generation of neuralnetworks. The paper also explores the use of memristors in the development of abrain-like computer called the MoNETA. A new model, multi/infinite dimensionalneural networks, are a recent development in the area of advanced neuralnetworks. The paper concludes that the need of neural networks in thedevelopment of human-like technology is essential and may be non-expendable forit.
arxiv-1200-50 | A Neuro-Fuzzy Technique for Implementing the Half-Adder Circuit Using the CANFIS Model | http://arxiv.org/pdf/1209.4895v1.pdf | author:Sachin Lakra, T. V. Prasad, Deepak Sharma, Shree Harsh Atrey, Anubhav Sharma category:cs.NE published:2012-09-20 summary:A Neural Network, in general, is not considered to be a good solver ofmathematical and binary arithmetic problems. However, networks have beendeveloped for such problems as the XOR circuit. This paper presents a techniquefor the implementation of the Half-adder circuit using the CoActive Neuro-FuzzyInference System (CANFIS) Model and attempts to solve the problem using theNeuroSolutions 5 Simulator. The paper gives the experimental results along withthe interpretations and possible applications of the technique.
arxiv-1200-51 | Stemmer for Serbian language | http://arxiv.org/pdf/1209.4471v1.pdf | author:Nikola Milošević category:cs.CL cs.IR published:2012-09-20 summary:In linguistic morphology and information retrieval, stemming is the processfor reducing inflected (or sometimes derived) words to their stem, base or rootform; generally a written word form. In this work is presented suffix strippingstemmer for Serbian language, one of the highly inflectional languages.
arxiv-1200-52 | An Efficient Color Face Verification Based on 2-Directional 2-Dimensional Feature Extraction | http://arxiv.org/pdf/1209.4420v1.pdf | author:Lan-Ting LI category:cs.CV published:2012-09-20 summary:A novel and uniform framework for face verification is presented in thispaper. First of all, a 2-directional 2-dimensional feature extraction method isadopted to extract client-specific template - 2D discrimant projection matrix.Then the face skin color information is utilized as an additive feature toenhance decision making strategy that makes use of not only 2D grey feature butalso 2D skin color feature. A fusion decision of both is applied to experimentthe performance on the XM2VTS database according to Lausanne protocol.Experimental results show that the framework achieves high verificationaccuracy and verification speed.
arxiv-1200-53 | Head Frontal-View Identification Using Extended LLE | http://arxiv.org/pdf/1209.4419v1.pdf | author:Chao Wang category:cs.CV published:2012-09-20 summary:Automatic head frontal-view identification is challenging due to appearancevariations caused by pose changes, especially without any training samples. Inthis paper, we present an unsupervised algorithm for identifying frontal viewamong multiple facial images under various yaw poses (derived from the sameperson). Our approach is based on Locally Linear Embedding (LLE), with theassumption that with yaw pose being the only variable, the facial images shouldlie in a smooth and low dimensional manifold. We horizontally flip the facialimages and present two K-nearest neighbor protocols for the original images andthe flipped images, respectively. In the proposed extended LLE, for any facialimage (original or flipped one), we search (1) the Ko nearest neighbors amongthe original facial images and (2) the Kf nearest neighbors among the flippedfacial images to construct the same neighborhood graph. The extended LLEeliminates the differences (because of background, face position and scale inthe whole image and some asymmetry of left-right face) between the originalfacial image and the flipped facial image at the same yaw pose so that theflipped facial images can be used effectively. Our approach does not need anytraining samples as prior information. The experimental results show that thefrontal view of head can be identified reliably around the lowest point of thepose manifold for multiple facial images, especially the cropped facial images(little background and centered face).
arxiv-1200-54 | Spectral Graph Cut from a Filtering Point of View | http://arxiv.org/pdf/1205.4450v2.pdf | author:Chengxi Ye, Yuxu Lin, Mingli Song, Chun Chen, David W. Jacobs category:cs.CV published:2012-05-20 summary:We analyze spectral graph theory based image segmentation algorithms and showthere is a natural connection with edge preserving filtering. Based on thisconnection we show that the normalized cut algorithm is equivalent to repeatedapplication of bilateral filtering. Then, using this interpretation we presentand implement a fast normalized cut algorithm. Experiments show that ourimplementation can solve the original optimization problem with a 10x-100xspeedup. Furthermore, we show this connection makes possible a new model forsegmentation called conditioned normalized cut that easily incorporates imagepatches in color and demonstrate how this problem can be solved with edgepreserving filtering.
arxiv-1200-55 | Image Super-Resolution via Sparse Bayesian Modeling of Natural Images | http://arxiv.org/pdf/1209.4317v1.pdf | author:Haichao Zhang, David Wipf, Yanning Zhang category:cs.CV published:2012-09-19 summary:Image super-resolution (SR) is one of the long-standing and active topics inimage processing community. A large body of works for image super resolutionformulate the problem with Bayesian modeling techniques and then obtain itsMaximum-A-Posteriori (MAP) solution, which actually boils down to a regularizedregression task over separable regularization term. Although straightforward,this approach cannot exploit the full potential offered by the probabilisticmodeling, as only the posterior mode is sought. Also, the separable property ofthe regularization term can not capture any correlations between the sparsecoefficients, which sacrifices much on its modeling accuracy. We propose aBayesian image SR algorithm via sparse modeling of natural images. The sparsityproperty of the latent high resolution image is exploited by introducing latentvariables into the high-order Markov Random Field (MRF) which capture thecontent adaptive variance by pixel-wise adaptation. The high-resolution imageis estimated via Empirical Bayesian estimation scheme, which is substantiallyfaster than our previous approach based on Markov Chain Monte Carlo sampling[1]. It is shown that the actual cost function for the proposed approachactually incorporates a non-factorial regularization term over the sparsecoefficients. Experimental results indicate that the proposed method cangenerate competitive or better results than \emph{state-of-the-art} SRalgorithms.
arxiv-1200-56 | Alpha/Beta Divergences and Tweedie Models | http://arxiv.org/pdf/1209.4280v1.pdf | author:Y. Kenan Yilmaz, A. Taylan Cemgil category:stat.ML cs.IT math.IT math.ST stat.TH published:2012-09-19 summary:We describe the underlying probabilistic interpretation of alpha and betadivergences. We first show that beta divergences are inherently tied to Tweediedistributions, a particular type of exponential family, known as exponentialdispersion models. Starting from the variance function of a Tweedie model, weoutline how to get alpha and beta divergences as special cases of Csisz\'ar's$f$ and Bregman divergences. This result directly generalizes the well-knownrelationship between the Gaussian distribution and least squares estimation toTweedie models and beta divergence minimization.
arxiv-1200-57 | Writing Reusable Digital Geometry Algorithms in a Generic Image Processing Framework | http://arxiv.org/pdf/1209.4233v1.pdf | author:Roland Levillain, Thierry Géraud, Laurent Najman category:cs.MS cs.CV published:2012-09-18 summary:Digital Geometry software should reflect the generality of the underlyingmathe- matics: mapping the latter to the former requires genericity. Bydesigning generic solutions, one can effectively reuse digital geometry datastructures and algorithms. We propose an image processing framework focused onthe Generic Programming paradigm in which an algorithm on the paper can beturned into a single code, written once and usable with various input types.This approach enables users to design and implement new methods at a lowercost, try cross-domain experiments and help generalize results
arxiv-1200-58 | Feature Specific Sentiment Analysis for Product Reviews | http://arxiv.org/pdf/1209.2352v2.pdf | author:Subhabrata Mukherjee, Pushpak Bhattacharyya category:cs.IR cs.CL published:2012-09-11 summary:In this paper, we present a novel approach to identify feature specificexpressions of opinion in product reviews with different features and mixedemotions. The objective is realized by identifying a set of potential featuresin the review and extracting opinion expressions about those features byexploiting their associations. Capitalizing on the view that more closelyassociated words come together to express an opinion about a certain feature,dependency parsing is used to identify relations between the opinionexpressions. The system learns the set of significant relations to be used bydependency parsing and a threshold parameter which allows us to merge closelyassociated opinion expressions. The data requirement is minimal as this is aone time learning of the domain independent parameters. The associations arerepresented in the form of a graph which is partitioned to finally retrieve theopinion expression describing the user specified feature. We show that thesystem achieves a high accuracy across all domains and performs at par withstate-of-the-art systems despite its data limitations.
arxiv-1200-59 | WikiSent : Weakly Supervised Sentiment Analysis Through Extractive Summarization With Wikipedia | http://arxiv.org/pdf/1209.2493v2.pdf | author:Subhabrata Mukherjee, Pushpak Bhattacharyya category:cs.IR cs.CL published:2012-09-12 summary:This paper describes a weakly supervised system for sentiment analysis in themovie review domain. The objective is to classify a movie review into apolarity class, positive or negative, based on those sentences bearing opinionon the movie alone. The irrelevant text, not directly related to the revieweropinion on the movie, is left out of analysis. Wikipedia incorporates the worldknowledge of movie-specific features in the system which is used to obtain anextractive summary of the review, consisting of the reviewer's opinions aboutthe specific aspects of the movie. This filters out the concepts which areirrelevant or objective with respect to the given movie. The proposed system,WikiSent, does not require any labeled data for training. The only weaksupervision arises out of the usage of resources like WordNet, Part-of-SpeechTagger and Sentiment Lexicons by virtue of their construction. WikiSentachieves a considerable accuracy improvement over the baseline and has a betteror comparable accuracy to the existing semi-supervised and unsupervised systemsin the domain, on the same dataset. We also perform a general movie reviewtrend analysis using WikiSent to find the trend in movie-making and the publicacceptance in terms of movie genre, year of release and polarity.
arxiv-1200-60 | TwiSent: A Multistage System for Analyzing Sentiment in Twitter | http://arxiv.org/pdf/1209.2495v2.pdf | author:Subhabrata Mukherjee, Akshat Malu, A. R. Balamurali, Pushpak Bhattacharyya category:cs.IR cs.CL published:2012-09-12 summary:In this paper, we present TwiSent, a sentiment analysis system for Twitter.Based on the topic searched, TwiSent collects tweets pertaining to it andcategorizes them into the different polarity classes positive, negative andobjective. However, analyzing micro-blog posts have many inherent challengescompared to the other text genres. Through TwiSent, we address the problems of1) Spams pertaining to sentiment analysis in Twitter, 2) Structural anomaliesin the text in the form of incorrect spellings, nonstandard abbreviations,slangs etc., 3) Entity specificity in the context of the topic searched and 4)Pragmatics embedded in text. The system performance is evaluated on manuallyannotated gold standard data and on an automatically annotated tweet set basedon hashtags. It is a common practise to show the efficacy of a supervisedsystem on an automatically annotated dataset. However, we show that such asystem achieves lesser classification accurcy when tested on generic twitterdataset. We also show that our system performs much better than an existingsystem.
arxiv-1200-61 | Leveraging Sentiment to Compute Word Similarity | http://arxiv.org/pdf/1209.2341v2.pdf | author:A. R. Balamurali, Subhabrata Mukherjee, Akshat Malu, Pushpak Bhattacharyya category:cs.IR cs.CL published:2012-09-11 summary:In this paper, we introduce a new WordNet based similarity metric, SenSim,which incorporates sentiment content (i.e., degree of positive or negativesentiment) of the words being compared to measure the similarity between them.The proposed metric is based on the hypothesis that knowing the sentiment isbeneficial in measuring the similarity. To verify this hypothesis, we measureand compare the annotator agreement for 2 annotation strategies: 1) sentimentinformation of a pair of words is considered while annotating and 2) sentimentinformation of a pair of words is not considered while annotating.Inter-annotator correlation scores show that the agreement is better when thetwo annotators consider sentiment information while assigning a similarityscore to a pair of words. We use this hypothesis to measure the similaritybetween a pair of words. Specifically, we represent each word as a vectorcontaining sentiment scores of all the content words in the WordNet gloss ofthe sense of that word. These sentiment scores are derived from a sentimentlexicon. We then measure the cosine similarity between the two vectors. Weperform both intrinsic and extrinsic evaluation of SenSim and compare theperformance with other widely usedWordNet similarity metrics.
arxiv-1200-62 | Robust Online Hamiltonian Learning | http://arxiv.org/pdf/1207.1655v2.pdf | author:Christopher E. Granade, Christopher Ferrie, Nathan Wiebe, D. G. Cory category:quant-ph cs.LG published:2012-07-06 summary:In this work we combine two distinct machine learning methodologies,sequential Monte Carlo and Bayesian experimental design, and apply them to theproblem of inferring the dynamical parameters of a quantum system. We designthe algorithm with practicality in mind by including parameters that controltrade-offs between the requirements on computational and experimentalresources. The algorithm can be implemented online (during experimental datacollection), avoiding the need for storage and post-processing. Mostimportantly, our algorithm is capable of learning Hamiltonian parameters evenwhen the parameters change from experiment-to-experiment, and also whenadditional noise processes are present and unknown. The algorithm alsonumerically estimates the Cramer-Rao lower bound, certifying its ownperformance.
arxiv-1200-63 | Generalized Canonical Correlation Analysis for Disparate Data Fusion | http://arxiv.org/pdf/1209.3761v1.pdf | author:Ming Sun, Carey E. Priebe, Minh Tang category:stat.ML cs.LG published:2012-09-17 summary:Manifold matching works to identify embeddings of multiple disparate dataspaces into the same low-dimensional space, where joint inference can bepursued. It is an enabling methodology for fusion and inference from multipleand massive disparate data sources. In this paper we focus on a method calledCanonical Correlation Analysis (CCA) and its generalization GeneralizedCanonical Correlation Analysis (GCCA), which belong to the more general ReducedRank Regression (RRR) framework. We present an efficiency investigation of CCAand GCCA under different training conditions for a particular text documentclassification task.
arxiv-1200-64 | Submodularity in Batch Active Learning and Survey Problems on Gaussian Random Fields | http://arxiv.org/pdf/1209.3694v1.pdf | author:Yifei Ma, Roman Garnett, Jeff Schneider category:cs.LG cs.AI cs.DS published:2012-09-17 summary:Many real-world datasets can be represented in the form of a graph whose edgeweights designate similarities between instances. A discrete Gaussian randomfield (GRF) model is a finite-dimensional Gaussian process (GP) whose priorcovariance is the inverse of a graph Laplacian. Minimizing the trace of thepredictive covariance Sigma (V-optimality) on GRFs has proven successful inbatch active learning classification problems with budget constraints. However,its worst-case bound has been missing. We show that the V-optimality on GRFs asa function of the batch query set is submodular and hence its greedy selectionalgorithm guarantees an (1-1/e) approximation ratio. Moreover, GRF models havethe absence-of-suppressor (AofS) condition. For active survey problems, wepropose a similar survey criterion which minimizes 1'(Sigma)1. In practice,V-optimality criterion performs better than GPs with mutual information gaincriteria and allows nonuniform costs for different nodes.
arxiv-1200-65 | Off-grid Direction of Arrival Estimation Using Sparse Bayesian Inference | http://arxiv.org/pdf/1108.5838v4.pdf | author:Zai Yang, Lihua Xie, Cishen Zhang category:stat.AP cs.IT math.IT stat.ML published:2011-08-30 summary:Direction of arrival (DOA) estimation is a classical problem in signalprocessing with many practical applications. Its research has recently beenadvanced owing to the development of methods based on sparse signalreconstruction. While these methods have shown advantages over conventionalones, there are still difficulties in practical situations where true DOAs arenot on the discretized sampling grid. To deal with such an off-grid DOAestimation problem, this paper studies an off-grid model that takes intoaccount effects of the off-grid DOAs and has a smaller modeling error. Aniterative algorithm is developed based on the off-grid model from a Bayesianperspective while joint sparsity among different snapshots is exploited byassuming a Laplace prior for signals at all snapshots. The new approach appliesto both single snapshot and multi-snapshot cases. Numerical simulations showthat the proposed algorithm has improved accuracy in terms of mean squaredestimation error. The algorithm can maintain high estimation accuracy evenunder a very coarse sampling grid.
arxiv-1200-66 | Nonconvex proximal splitting: batch and incremental algorithms | http://arxiv.org/pdf/1109.0258v2.pdf | author:Suvrit Sra category:math.OC stat.ML published:2011-09-01 summary:Within the unmanageably large class of nonconvex optimization, we considerthe rich subclass of nonsmooth problems that have composite objectives---thisalready includes the extensively studied convex, composite objective problemsas a special case. For this subclass, we introduce a powerful, new frameworkthat permits asymptotically non-vanishing perturbations. In particular, wedevelop perturbation-based batch and incremental (online like) nonconvexproximal splitting algorithms. To our knowledge, this is the first time thatsuch perturbation-based nonconvex splitting algorithms are being proposed andanalyzed. While the main contribution of the paper is the theoreticalframework, we complement our results by presenting some empirical results onmatrix factorization.
arxiv-1200-67 | Practical recommendations for gradient-based training of deep architectures | http://arxiv.org/pdf/1206.5533v2.pdf | author:Yoshua Bengio category:cs.LG published:2012-06-24 summary:Learning algorithms related to artificial neural networks and in particularfor Deep Learning may seem to involve many bells and whistles, calledhyper-parameters. This chapter is meant as a practical guide withrecommendations for some of the most commonly used hyper-parameters, inparticular in the context of learning algorithms based on back-propagatedgradient and gradient-based optimization. It also discusses how to deal withthe fact that more interesting results can be obtained when allowing one toadjust many hyper-parameters. Overall, it describes elements of the practiceused to successfully and efficiently train and debug large-scale and often deepmulti-layer neural networks. It closes with open questions about the trainingdifficulties observed with deeper architectures.
arxiv-1200-68 | A hybrid ACO approach to the Matrix Bandwidth Minimization Problem | http://arxiv.org/pdf/1208.5333v2.pdf | author:Camelia-M. Pintea, Camelia Chira, Gloria-C. Crisan category:cs.AI cs.NE 68T20 published:2012-08-27 summary:The evolution of the human society raises more and more difficult endeavors.For some of the real-life problems, the computing time-restriction enhancestheir complexity. The Matrix Bandwidth Minimization Problem (MBMP) seeks for asimultaneous permutation of the rows and the columns of a square matrix inorder to keep its nonzero entries close to the main diagonal. The MBMP is ahighly investigated P-complete problem, as it has broad applications inindustry, logistics, artificial intelligence or information recovery. Thispaper describes a new attempt to use the Ant Colony Optimization framework intackling MBMP. The introduced model is based on the hybridization of the AntColony System technique with new local search mechanisms. Computationalexperiments confirm a good performance of the proposed algorithm for theconsidered set of MBMP instances.
arxiv-1200-69 | Proceedings of the 29th International Conference on Machine Learning (ICML-12) | http://arxiv.org/pdf/1207.4676v2.pdf | author:John Langford, Joelle Pineau category:cs.LG stat.ML published:2012-07-19 summary:This is an index to the papers that appear in the Proceedings of the 29thInternational Conference on Machine Learning (ICML-12). The conference was heldin Edinburgh, Scotland, June 27th - July 3rd, 2012.
arxiv-1200-70 | A Hajj And Umrah Location Classification System For Video Crowded Scenes | http://arxiv.org/pdf/1209.3433v1.pdf | author:Hossam M. Zawbaa, Salah A. Aly, Adnan A. Gutub category:cs.CV cs.CY cs.LG published:2012-09-15 summary:In this paper, a new automatic system for classifying ritual locations indiverse Hajj and Umrah video scenes is investigated. This challenging subjecthas mostly been ignored in the past due to several problems one of which is thelack of realistic annotated video datasets. HUER Dataset is defined to modelsix different Hajj and Umrah ritual locations[26]. The proposed Hajj and Umrah ritual location classifying system consists offour main phases: Preprocessing, segmentation, feature extraction, and locationclassification phases. The shot boundary detection and background/foregroudsegmentation algorithms are applied to prepare the input video scenes into theKNN, ANN, and SVM classifiers. The system improves the state of art results onHajj and Umrah location classifications, and successfully recognizes the sixHajj rituals with more than 90% accuracy. The various demonstrated experimentsshow the promising results.
arxiv-1200-71 | Hierarchical Object Parsing from Structured Noisy Point Clouds | http://arxiv.org/pdf/1108.3605v2.pdf | author:Adrian Barbu category:cs.CV published:2011-08-18 summary:Object parsing and segmentation from point clouds are challenging tasksbecause the relevant data is available only as thin structures along objectboundaries or other features, and is corrupted by large amounts of noise. Tohandle this kind of data, flexible shape models are desired that can accuratelyfollow the object boundaries. Popular models such as Active Shape and ActiveAppearance models lack the necessary flexibility for this task, while recentapproaches such as the Recursive Compositional Models make modelsimplifications in order to obtain computational guarantees. This paperinvestigates a hierarchical Bayesian model of shape and appearance in agenerative setting. The input data is explained by an object parsing layer,which is a deformation of a hidden PCA shape model with Gaussian prior. Thepaper also introduces a novel efficient inference algorithm that uses informeddata-driven proposals to initialize local searches for the hidden variables.Applied to the problem of object parsing from structured point clouds such asedge detection images, the proposed approach obtains state of the art parsingerrors on two standard datasets without using any intensity information.
arxiv-1200-72 | Further Optimal Regret Bounds for Thompson Sampling | http://arxiv.org/pdf/1209.3353v1.pdf | author:Shipra Agrawal, Navin Goyal category:cs.LG cs.DS stat.ML 68W40, 68Q25 F.2.0 published:2012-09-15 summary:Thompson Sampling is one of the oldest heuristics for multi-armed banditproblems. It is a randomized algorithm based on Bayesian ideas, and hasrecently generated significant interest after several studies demonstrated itto have better empirical performance compared to the state of the art methods.In this paper, we provide a novel regret analysis for Thompson Sampling thatsimultaneously proves both the optimal problem-dependent bound of$(1+\epsilon)\sum_i \frac{\ln T}{\Delta_i}+O(\frac{N}{\epsilon^2})$ and thefirst near-optimal problem-independent bound of $O(\sqrt{NT\ln T})$ on theexpected regret of this algorithm. Our near-optimal problem-independent boundsolves a COLT 2012 open problem of Chapelle and Li. The optimalproblem-dependent regret bound for this problem was first proven recently byKaufmann et al. [ALT 2012]. Our novel martingale-based analysis techniques areconceptually simple, easily extend to distributions other than the Betadistribution, and also extend to the more general contextual bandits setting[Manuscript, Agrawal and Goyal, 2012].
arxiv-1200-73 | Link Prediction in Graphs with Autoregressive Features | http://arxiv.org/pdf/1209.3230v1.pdf | author:Emile Richard, Stephane Gaiffas, Nicolas Vayatis category:stat.ML published:2012-09-14 summary:In the paper, we consider the problem of link prediction in time-evolvinggraphs. We assume that certain graph features, such as the node degree, followa vector autoregressive (VAR) model and we propose to use this information toimprove the accuracy of prediction. Our strategy involves a joint optimizationprocedure over the space of adjacency matrices and VAR matrices which takesinto account both sparsity and low rank properties of the matrices. Oracleinequalities are derived and illustrate the trade-offs in the choice ofsmoothing parameters when modeling the joint effect of sparsity and low rankproperty. The estimate is computed efficiently using proximal methods through ageneralized forward-backward agorithm.
arxiv-1200-74 | Multiclass Learning with Simplex Coding | http://arxiv.org/pdf/1209.1360v2.pdf | author:Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco, Jean-Jacques Slotine category:stat.ML cs.LG published:2012-09-06 summary:In this paper we discuss a novel framework for multiclass learning, definedby a suitable coding/decoding strategy, namely the simplex coding, that allowsto generalize to multiple classes a relaxation approach commonly used in binaryclassification. In this framework, a relaxation error analysis can be developedavoiding constraints on the considered hypotheses class. Moreover, we show thatin this setting it is possible to derive the first provably consistentregularized method with training/tuning complexity which is independent to thenumber of classes. Tools from convex analysis are introduced that can be usedbeyond the scope of this paper.
arxiv-1200-75 | Agent-based Exploration of Wirings of Biological Neural Networks: Position Paper | http://arxiv.org/pdf/1209.3150v1.pdf | author:Önder Gürcan, Oğuz Dikenelli, Kemal S. Türker category:cs.NE q-bio.NC published:2012-09-14 summary:The understanding of human central nervous system depends on knowledge of itswiring. However, there are still gaps in our understanding of its wiring due totechnical difficulties. While some information is coming out from humanexperiments, medical research is lacking of simulation models to put currentfindings together to obtain the global picture and to predict hypotheses tolead future experiments. Agent-based modeling and simulation (ABMS) is a strongcandidate for the simulation model. In this position paper, we discuss thecurrent status of "neural wiring" and "ABMS in biological systems". Inparticular, we discuss that the ABMS context provides features required forexploration of biological neural wiring.
arxiv-1200-76 | Analog readout for optical reservoir computers | http://arxiv.org/pdf/1209.3129v1.pdf | author:Anteo Smerieri, François Duport, Yvan Paquot, Benjamin Schrauwen, Marc Haelterman, Serge Massar category:cs.ET cs.LG cs.NE physics.optics published:2012-09-14 summary:Reservoir computing is a new, powerful and flexible machine learningtechnique that is easily implemented in hardware. Recently, by using atime-multiplexed architecture, hardware reservoir computers have reachedperformance comparable to digital implementations. Operating speeds allowingfor real time information operation have been reached using optoelectronicsystems. At present the main performance bottleneck is the readout layer whichuses slow, digital postprocessing. We have designed an analog readout suitablefor time-multiplexed optoelectronic reservoir computers, capable of working inreal time. The readout has been built and tested experimentally on a standardbenchmark task. Its performance is better than non-reservoir methods, withample room for further improvement. The present work thereby overcomes one ofthe major limitations for the future development of hardware reservoircomputers.
arxiv-1200-77 | Beyond Stemming and Lemmatization: Ultra-stemming to Improve Automatic Text Summarization | http://arxiv.org/pdf/1209.3126v1.pdf | author:Juan-Manuel Torres-Moreno category:cs.IR cs.CL published:2012-09-14 summary:In Automatic Text Summarization, preprocessing is an important phase toreduce the space of textual representation. Classically, stemming andlemmatization have been widely used for normalizing words. However, even usingnormalization on large texts, the curse of dimensionality can disturb theperformance of summarizers. This paper describes a new method for normalizationof words to further reduce the space of representation. We propose to reduceeach word to its initial letters, as a form of Ultra-stemming. The results showthat Ultra-stemming not only preserve the content of summaries produced by thisrepresentation, but often the performances of the systems can be dramaticallyimproved. Summaries on trilingual corpora were evaluated automatically withFresa. Results confirm an increase in the performance, regardless of summarizersystem used.
arxiv-1200-78 | Detection and Classification of Viewer Age Range Smart Signs at TV Broadcast | http://arxiv.org/pdf/1209.3113v1.pdf | author:Baran Tander, Atilla Özmen, Murat Başkan category:cs.CV published:2012-09-14 summary:In this paper, the identification and classification of Viewer Age RangeSmart Signs, designed by the Radio and Television Supreme Council of Turkey, togive age range information for the TV viewers, are realized. Therefore, theautomatic detection at the broadcast will be possible, enabling themanufacturing of TV receivers which are sensible to these signs. The mostimportant step at this process is the pattern recognition. Since the symbolsthat must be identified are circular, various circle detection techniques canbe employed. In our study, first, two different circle segmentation methods forstill images are analyzed, their advantages and drawbacks are discussed. Apopular neural network structure called Multilayer Perceptron is employed forthe classification. Afterwards, the same procedures are carried out forstreaming video. All of the steps depicted above are realized on a standard PC.
arxiv-1200-79 | Signal Recovery in Unions of Subspaces with Applications to Compressive Imaging | http://arxiv.org/pdf/1209.3079v1.pdf | author:Nikhil Rao, Benjamin Recht, Robert Nowak category:stat.ML math.OC published:2012-09-14 summary:In applications ranging from communications to genetics, signals can bemodeled as lying in a union of subspaces. Under this model, signal coefficientsthat lie in certain subspaces are active or inactive together. The potentialsubspaces are known in advance, but the particular set of subspaces that areactive (i.e., in the signal support) must be learned from measurements. We showthat exploiting knowledge of subspaces can further reduce the number ofmeasurements required for exact signal recovery, and derive universal boundsfor the number of measurements needed. The bound is universal in the sense thatit only depends on the number of subspaces under consideration, and theirorientation relative to each other. The particulars of the subspaces (e.g.,compositions, dimensions, extents, overlaps, etc.) does not affect the resultswe obtain. In the process, we derive sample complexity bounds for the specialcase of the group lasso with overlapping groups (the latent group lasso), whichis used in a variety of applications. Finally, we also show that wavelettransform coefficients of images can be modeled as lying in groups, and hencecan be efficiently recovered using group lasso methods.
arxiv-1200-80 | Parametric Local Metric Learning for Nearest Neighbor Classification | http://arxiv.org/pdf/1209.3056v1.pdf | author:Jun Wang, Adam Woznica, Alexandros Kalousis category:cs.LG published:2012-09-13 summary:We study the problem of learning local metrics for nearest neighborclassification. Most previous works on local metric learning learn a number oflocal unrelated metrics. While this "independence" approach delivers anincreased flexibility its downside is the considerable risk of overfitting. Wepresent a new parametric local metric learning method in which we learn asmooth metric matrix function over the data manifold. Using an approximationerror bound of the metric matrix function we learn local metrics as linearcombinations of basis metrics defined on anchor points over different regionsof the instance space. We constrain the metric matrix function by imposing onthe linear combinations manifold regularization which makes the learned metricmatrix function vary smoothly along the geodesics of the data manifold. Ourmetric learning method has excellent performance both in terms of predictivepower and scalability. We experimented with several large-scale classificationproblems, tens of thousands of instances, and compared it with several state ofthe art metric learning methods, both global and local, as well as to SVM withautomatic kernel selection, all of which it outperforms in a significantmanner.
arxiv-1200-81 | Community Detection in the Labelled Stochastic Block Model | http://arxiv.org/pdf/1209.2910v1.pdf | author:Simon Heimlicher, Marc Lelarge, Laurent Massoulié category:cs.SI cs.LG math.PR physics.soc-ph published:2012-09-13 summary:We consider the problem of community detection from observed interactionsbetween individuals, in the context where multiple types of interaction arepossible. We use labelled stochastic block models to represent the observeddata, where labels correspond to interaction types. Focusing on a two-communityscenario, we conjecture a threshold for the problem of reconstructing thehidden communities in a way that is correlated with the true partition. Tosubstantiate the conjecture, we prove that the given threshold correctlyidentifies a transition on the behaviour of belief propagation from insensitiveto sensitive. We further prove that the same threshold corresponds to thetransition in a related inference problem on a tree model from infeasible tofeasible. Finally, numerical results using belief propagation for communitydetection give further support to the conjecture.
arxiv-1200-82 | A Novel Approach of Harris Corner Detection of Noisy Images using Adaptive Wavelet Thresholding Technique | http://arxiv.org/pdf/1209.2903v1.pdf | author:Nilanjan Dey, Pradipti Nandi, Nilanjana Barman category:cs.CV published:2012-09-13 summary:In this paper we propose a method of corner detection for obtaining featureswhich is required to track and recognize objects within a noisy image. Cornerdetection of noisy images is a challenging task in image processing. Naturalimages often get corrupted by noise during acquisition and transmission. ThoughCorner detection of these noisy images does not provide desired results, hencede-noising is required. Adaptive wavelet thresholding approach is applied forthe same.
arxiv-1200-83 | Generalized sequential tree-reweighted message passing | http://arxiv.org/pdf/1205.6352v4.pdf | author:Vladimir Kolmogorov, Thomas Schoenemann category:cs.CV published:2012-05-29 summary:This paper addresses the problem of approximate MAP-MRF inference in generalgraphical models. Following [36], we consider a family of linear programmingrelaxations of the problem where each relaxation is specified by a set ofnested pairs of factors for which the marginalization constraint needs to beenforced. We develop a generalization of the TRW-S algorithm [9] for thisproblem, where we use a decomposition into junction chains, monotonic w.r.t.some ordering on the nodes. This generalizes the monotonic chains in [9] in anatural way. We also show how to deal with nested factors in an efficient way.Experiments show an improvement over min-sum diffusion, MPLP and subgradientascent algorithms on a number of computer vision and natural languageprocessing problems.
arxiv-1200-84 | Hirarchical Digital Image Inpainting Using Wavelets | http://arxiv.org/pdf/1209.2816v1.pdf | author:S. Padmavathi, B. Priyalakshmi. Dr. K. P. Soman category:cs.CV published:2012-09-13 summary:Inpainting is the technique of reconstructing unknown or damaged portions ofan image in a visually plausible way. Inpainting algorithm automatically fillsthe damaged region in an image using the information available in undamagedregion. Propagation of structure and texture information becomes a challenge asthe size of damaged area increases. In this paper, a hierarchical inpaintingalgorithm using wavelets is proposed. The hierarchical method tries to keep themask size smaller while wavelets help in handling the high pass structureinformation and low pass texture information separately. The performance of theproposed algorithm is tested using different factors. The results of ouralgorithm are compared with existing methods such as interpolation, diffusionand exemplar techniques.
arxiv-1200-85 | Improving Energy Efficiency in Femtocell Networks: A Hierarchical Reinforcement Learning Framework | http://arxiv.org/pdf/1209.2790v1.pdf | author:Xianfu Chen, Honggang Zhang, Tao Chen, Mika Lasanen category:cs.LG published:2012-09-13 summary:This paper investigates energy efficiency for two-tier femtocell networksthrough combining game theory and stochastic learning. With the Stackelberggame formulation, a hierarchical reinforcement learning framework is applied tostudy the joint average utility maximization of macrocells and femtocellssubject to the minimum signal-to-interference-plus-noise-ratio requirements.The macrocells behave as the leaders and the femtocells are followers duringthe learning procedure. At each time step, the leaders commit to dynamicstrategies based on the best responses of the followers, while the followerscompete against each other with no further information but the leaders'strategy information. In this paper, we propose two learning algorithms toschedule each cell's stochastic power levels, leading by the macrocells.Numerical experiments are presented to validate the proposed studies and showthat the two learning algorithms substantially improve the energy efficiency ofthe femtocell networks.
arxiv-1200-86 | Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL | http://arxiv.org/pdf/1209.2784v1.pdf | author:Nishant A. Mehta, Dongryeol Lee, Alexander G. Gray category:cs.LG stat.ML published:2012-09-13 summary:Since its inception, the modus operandi of multi-task learning (MTL) has beento minimize the task-wise mean of the empirical risks. We introduce ageneralized loss-compositional paradigm for MTL that includes a spectrum offormulations as a subfamily. One endpoint of this spectrum is minimax MTL: anew MTL formulation that minimizes the maximum of the tasks' empirical risks.Via a certain relaxation of minimax MTL, we obtain a continuum of MTLformulations spanning minimax MTL and classical MTL. The full paradigm itselfis loss-compositional, operating on the vector of empirical risks. Itincorporates minimax MTL, its relaxations, and many new MTL formulations asspecial cases. We show theoretically that minimax MTL tends to avoid worst caseoutcomes on newly drawn test tasks in the learning to learn (LTL) test setting.The results of several MTL formulations on synthetic and real problems in theMTL and LTL test settings are encouraging.
arxiv-1200-87 | Low Complexity Damped Gauss-Newton Algorithms for CANDECOMP/PARAFAC | http://arxiv.org/pdf/1205.2584v2.pdf | author:Anh Huy Phan, Petr Tichavský, Andrzej Cichocki category:cs.NA cs.LG math.OC published:2012-05-11 summary:The damped Gauss-Newton (dGN) algorithm for CANDECOMP/PARAFAC (CP)decomposition can handle the challenges of collinearity of factors anddifferent magnitudes of factors; nevertheless, for factorization of an $N$-Dtensor of size $I_1\times I_N$ with rank $R$, the algorithm is computationallydemanding due to construction of large approximate Hessian of size $(RT \timesRT)$ and its inversion where $T = \sum_n I_n$. In this paper, we propose a fastimplementation of the dGN algorithm which is based on novel expressions of theinverse approximate Hessian in block form. The new implementation has lowercomputational complexity, besides computation of the gradient (this part iscommon to both methods), requiring the inversion of a matrix of size$NR^2\times NR^2$, which is much smaller than the whole approximate Hessian, if$T \gg NR$. In addition, the implementation has lower memory requirements,because neither the Hessian nor its inverse never need to be stored in theirentirety. A variant of the algorithm working with complex valued data isproposed as well. Complexity and performance of the proposed algorithm iscompared with those of dGN and ALS with line search on examples of difficultbenchmark tensors.
arxiv-1200-88 | Multi-track Map Matching | http://arxiv.org/pdf/1209.2759v1.pdf | author:Adel Javanmard, Maya Haridasan, Li Zhang category:cs.LG cs.DS stat.AP published:2012-09-13 summary:We study algorithms for matching user tracks, consisting of time-orderedlocation points, to paths in the road network. Previous work has focused on thescenario where the location data is linearly ordered and consists of fairlydense and regular samples. In this work, we consider the \emph{multi-track mapmatching}, where the location data comes from different trips on the sameroute, each with very sparse samples. This captures the realistic scenariowhere users repeatedly travel on regular routes and samples are sparselycollected, either due to energy consumption constraints or because samples areonly collected when the user actively uses a service. In the multi-trackproblem, the total set of combined locations is only partially ordered, ratherthan globally ordered as required by previous map-matching algorithms. Wepropose two methods, the iterative projection scheme and the graph Laplacianscheme, to solve the multi-track problem by using a single-track map-matchingsubroutine. We also propose a boosting technique which may be applied to eitherapproach to improve the accuracy of the estimated paths. In addition, in orderto deal with variable sampling rates in single-track map matching, we propose amethod based on a particular regularized cost function that can be adapted fordifferent sampling rates and measurement errors. We evaluate the effectivenessof our techniques for reconstructing tracks under several differentconfigurations of sampling error and sampling rate.
arxiv-1200-89 | An Invariance Principle for Polytopes | http://arxiv.org/pdf/0912.4884v2.pdf | author:Prahladh Harsha, Adam Klivans, Raghu Meka category:cs.CC cs.CG cs.DM cs.LG math.PR published:2009-12-24 summary:Let X be randomly chosen from {-1,1}^n, and let Y be randomly chosen from thestandard spherical Gaussian on R^n. For any (possibly unbounded) polytope Pformed by the intersection of k halfspaces, we prove that Pr [X belongs to P] - Pr [Y belongs to P] < log^{8/5}k * Delta, where Deltais a parameter that is small for polytopes formed by the intersection of"regular" halfspaces (i.e., halfspaces with low influence). The novelty of ourinvariance principle is the polylogarithmic dependence on k. Previously, onlybounds that were at least linear in k were known. We give two importantapplications of our main result: (1) A polylogarithmic in k bound on theBoolean noise sensitivity of intersections of k "regular" halfspaces (previouswork gave bounds linear in k). (2) A pseudorandom generator (PRG) with seedlength O((log n)*poly(log k,1/delta)) that delta-fools all polytopes with kfaces with respect to the Gaussian distribution. We also obtain PRGs withsimilar parameters that fool polytopes formed by intersection of regularhalfspaces over the hypercube. Using our PRG constructions, we obtain the firstdeterministic quasi-polynomial time algorithms for approximately counting thenumber of solutions to a broad class of integer programs, including densecovering problems and contingency tables.
arxiv-1200-90 | Comparison Study for Clonal Selection Algorithm and Genetic Algorithm | http://arxiv.org/pdf/1209.2717v1.pdf | author:Ezgi Deniz Ulker, Sadik Ulker category:cs.NE published:2012-09-12 summary:Two metaheuristic algorithms namely Artificial Immune Systems (AIS) andGenetic Algorithms are classified as computational systems inspired bytheoretical immunology and genetics mechanisms. In this work we examine thecomparative performances of two algorithms. A special selection algorithm,Clonal Selection Algorithm (CLONALG), which is a subset of Artificial ImmuneSystems, and Genetic Algorithms are tested with certain benchmark functions. Itis shown that depending on type of a function Clonal Selection Algorithm andGenetic Algorithm have better performance over each other.
arxiv-1200-91 | Visual Tracking with Similarity Matching Ratio | http://arxiv.org/pdf/1209.2696v1.pdf | author:Aysegul Dundar, Jonghoon Jin, Eugenio Culurciello category:cs.CV cs.RO published:2012-09-12 summary:This paper presents a novel approach to visual tracking: Similarity MatchingRatio (SMR). The traditional approach of tracking is minimizing some measuresof the difference between the template and a patch from the frame. Thisapproach is vulnerable to outliers and drastic appearance changes and anextensive study is focusing on making the approach more tolerant to them.However, this often results in longer, corrective algo- rithms which do notsolve the original problem. This paper proposes a novel approach to thedefinition of the tracking problems, SMR, which turns the differences into aprobability measure. Only pixel differences below a threshold count towardsdeciding the match, the rest are ignored. This approach makes the SMR trackerrobust to outliers and points that dramaticaly change appearance. The SMRtracker is tested on challenging video sequences and achieved state-of-the-artperformance.
arxiv-1200-92 | Regret Bounds for Restless Markov Bandits | http://arxiv.org/pdf/1209.2693v1.pdf | author:Ronald Ortner, Daniil Ryabko, Peter Auer, Rémi Munos category:cs.LG math.OC stat.ML published:2012-09-12 summary:We consider the restless Markov bandit problem, in which the state of eacharm evolves according to a Markov process independently of the learner'sactions. We suggest an algorithm that after $T$ steps achieves$\tilde{O}(\sqrt{T})$ regret with respect to the best policy that knows thedistributions of all arms. No assumptions on the Markov chains are made exceptthat they are irreducible. In addition, we show that index-based policies arenecessarily suboptimal for the considered problem.
arxiv-1200-93 | Sparsity and `Something Else': An Approach to Encrypted Image Folding | http://arxiv.org/pdf/0909.2017v5.pdf | author:James Bowley, Laura Rebollo-Neira category:cs.CV cs.IT math.IT published:2009-09-10 summary:A property of sparse representations in relation to their capacity forinformation storage is discussed. It is shown that this feature can be used foran application that we term Encrypted Image Folding. The proposed procedure isrealizable through any suitable transformation. In particular, in this paper weillustrate the approach by recourse to the Discrete Cosine Transform and acombination of redundant Cosine and Dirac dictionaries. The main advantage ofthe proposed technique is that both storage and encryption can be achievedsimultaneously using simple processing steps.
arxiv-1200-94 | Sparse Representation of Astronomical Images | http://arxiv.org/pdf/1209.2657v1.pdf | author:Laura Rebollo-Neira, James Bowley category:math-ph cs.CV math.MP published:2012-09-12 summary:Sparse representation of astronomical images is discussed. It is shown that asignificant gain in sparsity is achieved when particular mixed dictionaries areused for approximating these types of images with greedy selection strategies.Experiments are conducted to confirm: i)Effectiveness at producing sparserepresentations. ii)Competitiveness, with respect to the time required toprocess large images.The latter is a consequence of the suitability of theproposed dictionaries for approximating images in partitions of smallblocks.This feature makes it possible to apply the effective greedy selectiontechnique Orthogonal Matching Pursuit, up to some block size. For blocksexceeding that size a refinement of the original Matching Pursuit approach isconsidered. The resulting method is termed Self Projected Matching Pursuit,because is shown to be effective for implementing, via Matching Pursuit itself,the optional back-projection intermediate steps in that approach.
arxiv-1200-95 | Positivity and Transportation | http://arxiv.org/pdf/1209.2655v1.pdf | author:Marco Cuturi category:stat.ML math.CO published:2012-09-12 summary:We prove in this paper that the weighted volume of the set of integraltransportation matrices between two integral histograms r and c of equal sum isa positive definite kernel of r and c when the set of considered weights formsa positive definite matrix. The computation of this quantity, despite being thesubject of a significant research effort in algebraic statistics, remains anintractable challenge for histograms of even modest dimensions. We propose analternative kernel which, rather than considering all matrices of thetransportation polytope, only focuses on a sub-sample of its vertices known asits Northwestern corner solutions. The resulting kernel is positive definiteand can be computed with a number of operations O(R^2d) that grows linearly inthe complexity of the dimension d, where R^2, the total amount of sampledvertices, is a parameter that controls the complexity of the kernel.
arxiv-1200-96 | Probabilities on Sentences in an Expressive Logic | http://arxiv.org/pdf/1209.2620v1.pdf | author:Marcus Hutter, John W. Lloyd, Kee Siong Ng, William T. B. Uther category:cs.LO cs.AI cs.LG math.LO math.PR published:2012-09-12 summary:Automated reasoning about uncertain knowledge has many applications. Onedifficulty when developing such systems is the lack of a completelysatisfactory integration of logic and probability. We address this problemdirectly. Expressive languages like higher-order logic are ideally suited forrepresenting and reasoning about structured knowledge. Uncertain knowledge canbe modeled by using graded probabilities rather than binary truth-values. Themain technical problem studied in this paper is the following: Given a set ofsentences, each having some probability of being true, what probability shouldbe ascribed to other (query) sentences? A natural wish-list, among others, isthat the probability distribution (i) is consistent with the knowledge base,(ii) allows for a consistent inference procedure and in particular (iii)reduces to deductive logic in the limit of probabilities being 0 and 1, (iv)allows (Bayesian) inductive reasoning and (v) learning in the limit and inparticular (vi) allows confirmation of universally quantifiedhypotheses/sentences. We translate this wish-list into technical requirementsfor a prior probability and show that probabilities satisfying all our criteriaexist. We also give explicit constructions and several generalcharacterizations of probabilities that satisfy some or all of the criteria andvarious (counter) examples. We also derive necessary and sufficient conditionsfor extending beliefs about finitely many sentences to suitable probabilitiesover all sentences, and in particular least dogmatic or least biased ones. Weconclude with a brief outlook on how the developed theory might be used andapproximated in autonomous reasoning agents. Our theory is a step towards aglobally consistent and empirically satisfactory unification of probability andlogic.
arxiv-1200-97 | Cultural Algorithm Toolkit for Multi-objective Rule Mining | http://arxiv.org/pdf/1209.2948v1.pdf | author:Sujatha Srinivasan, Sivakumar Ramakrishnan category:cs.NE cs.AI published:2012-09-12 summary:Cultural algorithm is a kind of evolutionary algorithm inspired from societalevolution and is composed of a belief space, a population space and a protocolthat enables exchange of knowledge between these sources. Knowledge created inthe population space is accepted into the belief space while this collectiveknowledge from these sources is combined to influence the decisions of theindividual agents in solving problems. Classification rules comes underdescriptive knowledge discovery in data mining and are the most sought out byusers since they represent highly comprehensible form of knowledge. The ruleshave certain properties which make them useful forms of actionable knowledge tousers. The rules are evaluated using these properties namely the rule metrics.In the current study a Cultural Algorithm Toolkit for Classification RuleMining (CAT-CRM) is proposed which allows the user to control three differentset of parameters namely the evolutionary parameters, the rule parameters aswell as agent parameters and hence can be used for experimenting with anevolutionary system, a rule mining system or an agent based social system.Results of experiments conducted to observe the effect of different number andtype of metrics on the performance of the algorithm on bench mark data sets isreported.
arxiv-1200-98 | Training a Feed-forward Neural Network with Artificial Bee Colony Based Backpropagation Method | http://arxiv.org/pdf/1209.2548v1.pdf | author:Sudarshan Nandy, Partha Pratim Sarkar, Achintya Das category:cs.NE cs.AI published:2012-09-12 summary:Back-propagation algorithm is one of the most widely used and populartechniques to optimize the feed forward neural network training. Natureinspired meta-heuristic algorithms also provide derivative-free solution tooptimize complex problem. Artificial bee colony algorithm is a nature inspiredmeta-heuristic algorithm, mimicking the foraging or food source searchingbehaviour of bees in a bee colony and this algorithm is implemented in severalapplications for an improved optimized outcome. The proposed method in thispaper includes an improved artificial bee colony algorithm basedback-propagation neural network training method for fast and improvedconvergence rate of the hybrid neural network learning method. The result isanalysed with the genetic algorithm based back-propagation method, and it isanother hybridized procedure of its kind. Analysis is performed over standarddata sets, reflecting the light of efficiency of proposed method in terms ofconvergence speed and rate.
arxiv-1200-99 | Initial Version of State Transition Algorithm | http://arxiv.org/pdf/1208.0228v2.pdf | author:Xiaojun Zhou, Chunhua Yang, Weihua Gui category:math.OC cs.NE published:2012-08-01 summary:In terms of the concepts of state and state transition, a new algorithm-StateTransition Algorithm (STA) is proposed in order to probe into classical andintelligent optimization algorithms. On the basis of state and statetransition, it becomes much simpler and easier to understand. As for continuousfunction optimization problems, three special operators named rotation,translation and expansion are presented. While for discrete functionoptimization problems, an operator called general elementary transformation isintroduced. Finally, with 4 common benchmark continuous functions and adiscrete problem used to test the performance of STA, the experiment shows thatSTA is a promising algorithm due to its good search capability.
arxiv-1200-100 | Wavelet Based Image Coding Schemes : A Recent Survey | http://arxiv.org/pdf/1209.2515v1.pdf | author:V. J. Rehna, M. K. Jeya Kumar category:cs.CV published:2012-09-12 summary:A variety of new and powerful algorithms have been developed for imagecompression over the years. Among them the wavelet-based image compressionschemes have gained much popularity due to their overlapping nature whichreduces the blocking artifacts that are common phenomena in JPEG compressionand multiresolution character which leads to superior energy compaction withhigh quality reconstructed images. This paper provides a detailed survey onsome of the popular wavelet coding techniques such as the Embedded ZerotreeWavelet (EZW) coding, Set Partitioning in Hierarchical Tree (SPIHT) coding, theSet Partitioned Embedded Block (SPECK) Coder, and the Embedded Block Codingwith Optimized Truncation (EBCOT) algorithm. Other wavelet-based codingtechniques like the Wavelet Difference Reduction (WDR) and the Adaptive ScannedWavelet Difference Reduction (ASWDR) algorithms, the Space FrequencyQuantization (SFQ) algorithm, the Embedded Predictive Wavelet Image Coder(EPWIC), Compression with Reversible Embedded Wavelet (CREW), the Stack-Run(SR) coding and the recent Geometric Wavelet (GW) coding are also discussed.Based on the review, recommendations and discussions are presented foralgorithm development and implementation.
arxiv-1200-101 | Multimodal diffusion geometry by joint diagonalization of Laplacians | http://arxiv.org/pdf/1209.2295v2.pdf | author:Davide Eynard, Klaus Glashoff, Michael M. Bronstein, Alexander M. Bronstein category:cs.CV cs.AI published:2012-09-11 summary:We construct an extension of diffusion geometry to multiple modalitiesthrough joint approximate diagonalization of Laplacian matrices. This naturallyextends classical data analysis tools based on spectral geometry, such asdiffusion maps and spectral clustering. We provide several synthetic and realexamples of manifold learning, retrieval, and clustering demonstrating that thejoint diffusion geometry frequently better captures the inherent structure ofmulti-modal data. We also show that many previous attempts to constructmultimodal spectral clustering can be seen as particular cases of jointapproximate diagonalization of the Laplacians.
arxiv-1200-102 | Performance Evaluation of Predictive Classifiers For Knowledge Discovery From Engineering Materials Data Sets | http://arxiv.org/pdf/1209.2501v1.pdf | author:Hemanth K. S Doreswamy category:cs.LG published:2012-09-12 summary:In this paper, naive Bayesian and C4.5 Decision Tree Classifiers(DTC) aresuccessively applied on materials informatics to classify the engineeringmaterials into different classes for the selection of materials that suit theinput design specifications. Here, the classifiers are analyzed individuallyand their performance evaluation is analyzed with confusion matrix predictiveparameters and standard measures, the classification results are analyzed ondifferent class of materials. Comparison of classifiers has found that naiveBayesian classifier is more accurate and better than the C4.5 DTC. Theknowledge discovered by the naive bayesian classifier can be employed fordecision making in materials selection in manufacturing industries.
arxiv-1200-103 | Query Complexity of Derivative-Free Optimization | http://arxiv.org/pdf/1209.2434v1.pdf | author:Kevin G. Jamieson, Robert D. Nowak, Benjamin Recht category:stat.ML cs.LG published:2012-09-11 summary:This paper provides lower bounds on the convergence rate of Derivative FreeOptimization (DFO) with noisy function evaluations, exposing a fundamental andunavoidable gap between the performance of algorithms with access to gradientsand those with access to only function evaluations. However, there aresituations in which DFO is unavoidable, and for such situations we propose anew DFO algorithm that is proved to be near optimal for the class of stronglyconvex objective functions. A distinctive feature of the algorithm is that ituses only Boolean-valued function comparisons, rather than functionevaluations. This makes the algorithm useful in an even wider range ofapplications, such as optimization based on paired comparisons from humansubjects, for example. We also show that regardless of whether DFO is based onnoisy function evaluations or Boolean-valued function comparisons, theconvergence rate is the same.
arxiv-1200-104 | Identification of Fertile Translations in Medical Comparable Corpora: a Morpho-Compositional Approach | http://arxiv.org/pdf/1209.2400v1.pdf | author:Estelle Delpech, Béatrice Daille, Emmanuel Morin, Claire Lemaire category:cs.CL published:2012-09-11 summary:This paper defines a method for lexicon in the biomedical domain fromcomparable corpora. The method is based on compositional translation andexploits morpheme-level translation equivalences. It can generate translationsfor a large variety of morphologically constructed words and can also generate'fertile' translations. We show that fertile translations increase the overallquality of the extracted lexicon for English to French translation.
arxiv-1200-105 | On the Equivalence between Herding and Conditional Gradient Algorithms | http://arxiv.org/pdf/1203.4523v2.pdf | author:Francis Bach, Simon Lacoste-Julien, Guillaume Obozinski category:cs.LG math.OC stat.ML published:2012-03-20 summary:We show that the herding procedure of Welling (2009) takes exactly the formof a standard convex optimization algorithm--namely a conditional gradientalgorithm minimizing a quadratic moment discrepancy. This link enables us toinvoke convergence results from convex optimization and to consider fasteralternatives for the task of approximating integrals in a reproducing kernelHilbert space. We study the behavior of the different variants throughnumerical simulations. The experiments indicate that while we can improve overherding on the task of approximating integrals, the original herding algorithmtends to approach more often the maximum entropy distribution, shedding morelight on the learning bias behind herding.
arxiv-1200-106 | Modeling controversies in the press: the case of the abnormal bees' death | http://arxiv.org/pdf/1209.2163v1.pdf | author:Alexandre Delanoë, Serge Galam category:physics.soc-ph cs.CL published:2012-09-10 summary:The controversy about the cause(s) of abnormal death of bee colonies inFrance is investigated through an extensive analysis of the french speakingpress. A statistical analysis of textual data is first performed on the lexiconused by journalists to describe the facts and to present associatedinformations during the period 1998-2010. Three states are identified toexplain the phenomenon. The first state asserts a unique cause, the second onefocuses on multifactor causes and the third one states the absence of currentproof. Assigning each article to one of the three states, we are able to followthe associated opinion dynamics among the journalists over 13 years. Then, weapply the Galam sequential probabilistic model of opinion dynamic to thosedata. Assuming journalists are either open mind or inflexible about theirrespective opinions, the results are reproduced precisely provided we accountfor a series of annual changes in the proportions of respective inflexibles.The results shed a new counter intuitive light on the various pressure supposedto apply on the journalists by either chemical industries or beekeepers andexperts or politicians. The obtained dynamics of respective inflexibles showsthe possible effect of lobbying, the inertia of the debate and the netadvantage gained by the first whistleblowers.
arxiv-1200-107 | Distance Dependent Infinite Latent Feature Models | http://arxiv.org/pdf/1110.5454v2.pdf | author:Samuel J. Gershman, Peter I. Frazier, David M. Blei category:stat.ML math.ST stat.TH published:2011-10-25 summary:Latent feature models are widely used to decompose data into a small numberof components. Bayesian nonparametric variants of these models, which use theIndian buffet process (IBP) as a prior over latent features, allow the numberof features to be determined from the data. We present a generalization of theIBP, the distance dependent Indian buffet process (dd-IBP), for modelingnon-exchangeable data. It relies on distances defined between data points,biasing nearby data to share more features. The choice of distance measureallows for many kinds of dependencies, including temporal and spatial. Further,the original IBP is a special case of the dd-IBP. In this paper, we develop thedd-IBP and theoretically characterize its feature-sharing properties. We derivea Markov chain Monte Carlo sampler for a linear Gaussian model with a dd-IBPprior and study its performance on several non-exchangeable data sets.
arxiv-1200-108 | A Bayesian Boosting Model | http://arxiv.org/pdf/1209.1996v1.pdf | author:Alexander Lorbert, David M. Blei, Robert E. Schapire, Peter J. Ramadge category:stat.ML published:2012-09-10 summary:We offer a novel view of AdaBoost in a statistical setting. We propose aBayesian model for binary classification in which label noise is modeledhierarchically. Using variational inference to optimize a dynamic evidencelower bound, we derive a new boosting-like algorithm called VIBoost. We showits close connections to AdaBoost and give experimental results from fourdatasets.
arxiv-1200-109 | Parsimonious Mahalanobis Kernel for the Classification of High Dimensional Data | http://arxiv.org/pdf/1206.4481v2.pdf | author:M. Fauvel, A. Villa, J. Chanussot, J. A. Benediktsson category:cs.NA cs.LG published:2012-06-20 summary:The classification of high dimensional data with kernel methods is consideredin this article. Exploit- ing the emptiness property of high dimensionalspaces, a kernel based on the Mahalanobis distance is proposed. The computationof the Mahalanobis distance requires the inversion of a covariance matrix. Inhigh dimensional spaces, the estimated covariance matrix is ill-conditioned andits inversion is unstable or impossible. Using a parsimonious statisticalmodel, namely the High Dimensional Discriminant Analysis model, the specificsignal and noise subspaces are estimated for each considered class making theinverse of the class specific covariance matrix explicit and stable, leading tothe definition of a parsimonious Mahalanobis kernel. A SVM based framework isused for selecting the hyperparameters of the parsimonious Mahalanobis kernelby optimizing the so-called radius-margin bound. Experimental results on threehigh dimensional data sets show that the proposed kernel is suitable forclassifying high dimensional data, providing better classification accuraciesthan the conventional Gaussian kernel.
arxiv-1200-110 | A Comparative Study of Efficient Initialization Methods for the K-Means Clustering Algorithm | http://arxiv.org/pdf/1209.1960v1.pdf | author:M. Emre Celebi, Hassan A. Kingravi, Patricio A. Vela category:cs.LG cs.CV I.5.3; H.2.8 published:2012-09-10 summary:K-means is undoubtedly the most widely used partitional clustering algorithm.Unfortunately, due to its gradient descent nature, this algorithm is highlysensitive to the initial placement of the cluster centers. Numerousinitialization methods have been proposed to address this problem. In thispaper, we first present an overview of these methods with an emphasis on theircomputational efficiency. We then compare eight commonly used linear timecomplexity initialization methods on a large and diverse collection of datasets using various performance criteria. Finally, we analyze the experimentalresults using non-parametric statistical tests and provide recommendations forpractitioners. We demonstrate that popular initialization methods often performpoorly and that there are in fact strong alternatives to these methods.
arxiv-1200-111 | A spatio-spectral hybridization for edge preservation and noisy image restoration via local parametric mixtures and Lagrangian relaxation | http://arxiv.org/pdf/1209.1826v1.pdf | author:Kinjal Basu, Debapriya Sengupta category:stat.ME cs.CV stat.AP published:2012-09-09 summary:This paper investigates a fully unsupervised statistical method for edgepreserving image restoration and compression using a spatial decompositionscheme. Smoothed maximum likelihood is used for local estimation of edge pixelsfrom mixture parametric models of local templates. For the complementary smoothpart the traditional L2-variational problem is solved in the Fourier domainwith Thin Plate Spline (TPS) regularization. It is well known that naiveFourier compression of the whole image fails to restore a piece-wise smoothnoisy image satisfactorily due to Gibbs phenomenon. Images are interpreted asrelative frequency histograms of samples from bi-variate densities where thesample sizes might be unknown. The set of discontinuities is assumed to becompletely unsupervised Lebesgue-null, compact subset of the plane in thecontinuous formulation of the problem. Proposed spatial decomposition uses awidely used topological concept, partition of unity. The decision on edge pixelneighborhoods are made based on the multiple testing procedure of Holms.Statistical summary of the ?final output is decomposed into two layers ofinformation extraction, one for the subset of edge pixels and the other for thesmooth region. Robustness is also demonstrated by applying the technique onnoisy degradation of clean images.
arxiv-1200-112 | An Empirical Study of MAUC in Multi-class Problems with Uncertain Cost Matrices | http://arxiv.org/pdf/1209.1800v1.pdf | author:Rui Wang, Ke Tang category:cs.LG published:2012-09-09 summary:Cost-sensitive learning relies on the availability of a known and fixed costmatrix. However, in some scenarios, the cost matrix is uncertain duringtraining, and re-train a classifier after the cost matrix is specified wouldnot be an option. For binary classification, this issue can be successfullyaddressed by methods maximizing the Area Under the ROC Curve (AUC) metric.Since the AUC can measure performance of base classifiers independent of costduring training, and a larger AUC is more likely to lead to a smaller totalcost in testing using the threshold moving method. As an extension of AUC tomulti-class problems, MAUC has attracted lots of attentions and been widelyused. Although MAUC also measures performance of base classifiers independentof cost, it is unclear whether a larger MAUC of classifiers is more likely tolead to a smaller total cost. In fact, it is also unclear what kinds ofpost-processing methods should be used in multi-class problems to convert baseclassifiers into discrete classifiers such that the total cost is as small aspossible. In the paper, we empirically explore the relationship between MAUCand the total cost of classifiers by applying two categories of post-processingmethods. Our results suggest that a larger MAUC is also beneficial.Interestingly, simple calibration methods that convert the output matrix intoposterior probabilities perform better than existing sophisticated postre-optimization methods.
arxiv-1200-113 | On the Use of Lee's Protocol for Speckle-Reducing Techniques | http://arxiv.org/pdf/1209.1788v1.pdf | author:Elsa E. Moschetti, M. Gabriela Palacio, Mery Picco, Oscar H. Bustos, Alejandro C. Frery category:cs.CV published:2012-09-09 summary:This paper presents two new MAP (Maximum a Posteriori) filters for specklenoise reduction and a Monte Carlo procedure for the assessment of theirperformance. In order to quantitatively evaluate the results obtained usingthese new filters, with respect to classical ones, a Monte Carlo extension ofLee's protocol is proposed. This extension of the protocol shows that itsoriginal version leads to inconsistencies that hamper its use as a generalprocedure for filter assessment. Some solutions for these inconsistencies areproposed, and a consistent comparison of speckle-reducing filters is provided.
arxiv-1200-114 | Difference of Normals as a Multi-Scale Operator in Unorganized Point Clouds | http://arxiv.org/pdf/1209.1759v1.pdf | author:Yani Ioannou, Babak Taati, Robin Harrap, Michael Greenspan category:cs.CV published:2012-09-08 summary:A novel multi-scale operator for unorganized 3D point clouds is introduced.The Difference of Normals (DoN) provides a computationally efficient,multi-scale approach to processing large unorganized 3D point clouds. Theapplication of DoN in the multi-scale filtering of two different real-worldoutdoor urban LIDAR scene datasets is quantitatively and qualitativelydemonstrated. In both datasets the DoN operator is shown to segment large 3Dpoint clouds into scale-salient clusters, such as cars, people, and lamp poststowards applications in semi-automatic annotation, and as a pre-processing stepin automatic object recognition. The application of the operator tosegmentation is evaluated on a large public dataset of outdoor LIDAR sceneswith ground truth annotations.
arxiv-1200-115 | Information content versus word length in random typing | http://arxiv.org/pdf/1209.1751v1.pdf | author:Ramon Ferrer-i-Cancho, Fermín Moscoso del Prado Martín category:cs.CL published:2012-09-08 summary:Recently, it has been claimed that a linear relationship between a measure ofinformation content and word length is expected from word length optimizationand it has been shown that this linearity is supported by a strong correlationbetween information content and word length in many languages (Piantadosi etal. 2011, PNAS 108, 3825-3826). Here, we study in detail some connectionsbetween this measure and standard information theory. The relationship betweenthe measure and word length is studied for the popular random typing processwhere a text is constructed by pressing keys at random from a keyboardcontaining letters and a space behaving as a word delimiter. Although thisrandom process does not optimize word lengths according to information content,it exhibits a linear relationship between information content and word length.The exact slope and intercept are presented for three major variants of therandom typing process. A strong correlation between information content andword length can simply arise from the units making a word (e.g., letters) andnot necessarily from the interplay between a word and its context as proposedby Piantadosi et al. In itself, the linear relation does not entail the resultsof any optimization process.
arxiv-1200-116 | Design of Spectrum Sensing Policy for Multi-user Multi-band Cognitive Radio Network | http://arxiv.org/pdf/1209.1739v1.pdf | author:Jan Oksanen, Jarmo Lundén, Visa Koivunen category:cs.LG cs.NI published:2012-09-08 summary:Finding an optimal sensing policy for a particular access policy and sensingscheme is a laborious combinatorial problem that requires the system modelparameters to be known. In practise the parameters or the model itself may notbe completely known making reinforcement learning methods appealing. In thispaper a non-parametric reinforcement learning-based method is developed forsensing and accessing multi-band radio spectrum in multi-user cognitive radionetworks. A suboptimal sensing policy search algorithm is proposed for aparticular multi-user multi-band access policy and the randomizedChair-Varshney rule. The randomized Chair-Varshney rule is used to reduce theprobability of false alarms under a constraint on the probability of detectionthat protects the primary user. The simulation results show that the proposedmethod achieves a sum profit (e.g. data rate) close to the optimal sensingpolicy while achieving the desired probability of detection.
arxiv-1200-117 | Load Distribution Composite Design Pattern for Genetic Algorithm-Based Autonomic Computing Systems | http://arxiv.org/pdf/1209.1734v1.pdf | author:Vishnuvardhan Mannava, T. Ramesh category:cs.SE cs.DC cs.NE published:2012-09-08 summary:Current autonomic computing systems are ad hoc solutions that are designedand implemented from the scratch. When designing software, in most cases two ormore patterns are to be composed to solve a bigger problem. A composite designpatterns shows a synergy that makes the composition more than just the sum ofits parts which leads to ready-made software architectures. As far as we know,there are no studies on composition of design patterns for autonomic computingdomain. In this paper we propose pattern-oriented software architecture forself-optimization in autonomic computing system using design patternscomposition and multi objective evolutionary algorithms that software designersand/or programmers can exploit to drive their work. Main objective of thesystem is to reduce the load in the server by distributing the population toclients. We used Case Based Reasoning, Database Access, and Master Slave designpatterns. We evaluate the effectiveness of our architecture with and withoutdesign patterns compositions. The use of composite design patterns in thearchitecture and quantitative measurements are presented. A simple UML classdiagram is used to describe the architecture.
arxiv-1200-118 | Bandits with heavy tail | http://arxiv.org/pdf/1209.1727v1.pdf | author:Sébastien Bubeck, Nicolò Cesa-Bianchi, Gábor Lugosi category:stat.ML cs.LG published:2012-09-08 summary:The stochastic multi-armed bandit problem is well understood when the rewarddistributions are sub-Gaussian. In this paper we examine the bandit problemunder the weaker assumption that the distributions have moments of order1+\epsilon, for some $\epsilon \in (0,1]$. Surprisingly, moments of order 2(i.e., finite variance) are sufficient to obtain regret bounds of the sameorder as under sub-Gaussian reward distributions. In order to achieve suchregret, we define sampling strategies based on refined estimators of the meansuch as the truncated empirical mean, Catoni's M-estimator, and themedian-of-means estimator. We also derive matching lower bounds that also showthat the best achievable regret deteriorates when \epsilon <1.
arxiv-1200-119 | Bayesian Nonparametric Hidden Semi-Markov Models | http://arxiv.org/pdf/1203.1365v2.pdf | author:Matthew J. Johnson, Alan S. Willsky category:stat.ME stat.AP stat.ML published:2012-03-07 summary:There is much interest in the Hierarchical Dirichlet Process Hidden MarkovModel (HDP-HMM) as a natural Bayesian nonparametric extension of the ubiquitousHidden Markov Model for learning from sequential and time-series data. However,in many settings the HDP-HMM's strict Markovian constraints are undesirable,particularly if we wish to learn or encode non-geometric state durations. Wecan extend the HDP-HMM to capture such structure by drawing uponexplicit-duration semi-Markovianity, which has been developed mainly in theparametric frequentist setting, to allow construction of highly interpretablemodels that admit natural prior information on state durations. In this paper we introduce the explicit-duration Hierarchical DirichletProcess Hidden semi-Markov Model (HDP-HSMM) and develop sampling algorithms forefficient posterior inference. The methods we introduce also provide newmethods for sampling inference in the finite Bayesian HSMM. Our modular Gibbssampling methods can be embedded in samplers for larger hierarchical Bayesianmodels, adding semi-Markov chain modeling as another tool in the Bayesianinference toolbox. We demonstrate the utility of the HDP-HSMM and our inferencemethods on both synthetic and real experiments.
arxiv-1200-120 | Runtime Guarantees for Regression Problems | http://arxiv.org/pdf/1110.1358v2.pdf | author:Hui Han Chin, Aleksander Madry, Gary Miller, Richard Peng category:cs.DS cs.CV published:2011-10-06 summary:We study theoretical runtime guarantees for a class of optimization problemsthat occur in a wide variety of inference problems. these problems aremotivated by the lasso framework and have applications in machine learning andcomputer vision. Our work shows a close connection between these problems and core questionsin algorithmic graph theory. While this connection demonstrates thedifficulties of obtaining runtime guarantees, it also suggests an approach ofusing techniques originally developed for graph algorithms. We then show that most of these problems can be formulated as a grouped leastsquares problem, and give efficient algorithms for this formulation. Ouralgorithms rely on routines for solving quadratic minimization problems, whichin turn are equivalent to solving linear systems. Finally we present someexperimental results on applying our approximation algorithm to imageprocessing problems.
arxiv-1200-121 | Wavelet Based QRS Complex Detection of ECG Signal | http://arxiv.org/pdf/1209.1563v1.pdf | author:Sayantan Mukhopadhyay, Shouvik Biswas, Anamitra Bardhan Roy, Nilanjan Dey category:cs.CV published:2012-09-07 summary:The Electrocardiogram (ECG) is a sensitive diagnostic tool that is used todetect various cardiovascular diseases by measuring and recording theelectrical activity of the heart in exquisite detail. A wide range of heartcondition is determined by thorough examination of the features of the ECGreport. Automatic extraction of time plane features is important foridentification of vital cardiac diseases. This paper presents amulti-resolution wavelet transform based system for detection 'P', 'Q', 'R','S', 'T' peaks complex from original ECG signal. 'R-R' time lapse is animportant minutia of the ECG signal that corresponds to the heartbeat of theconcerned person. Abrupt increase in height of the 'R' wave or changes in themeasurement of the 'R-R' denote various anomalies of human heart. Similarly'P-P', 'Q-Q', 'S-S', 'T-T' also corresponds to different anomalies of heart andtheir peak amplitude also envisages other cardiac diseases. In this proposedmethod the 'PQRST' peaks are marked and stored over the entire signal and thetime interval between two consecutive 'R' peaks and other peaks interval aremeasured to detect anomalies in behavior of heart, if any. The peaks areachieved by the composition of Daubeheissub bands wavelet of original ECGsignal. The accuracy of the 'PQRST' complex detection and interval measurementis achieved up to 100% with high exactitude by processing and thresholding theoriginal ECG signal.
arxiv-1200-122 | A Comparative Study between Moravec and Harris Corner Detection of Noisy Images Using Adaptive Wavelet Thresholding Technique | http://arxiv.org/pdf/1209.1558v1.pdf | author:Nilanjan Dey, Pradipti Nandi, Nilanjana Barman, Debolina Das, Subhabrata Chakraborty category:cs.CV published:2012-09-07 summary:In this paper a comparative study between Moravec and Harris Corner Detectionhas been done for obtaining features required to track and recognize objectswithin a noisy image. Corner detection of noisy images is a challenging task inimage processing. Natural images often get corrupted by noise duringacquisition and transmission. As Corner detection of these noisy images doesnot provide desired results, hence de-noising is required. Adaptive waveletthresholding approach is applied for the same.
arxiv-1200-123 | On spatial selectivity and prediction across conditions with fMRI | http://arxiv.org/pdf/1209.1450v1.pdf | author:Yannick Schwartz, Gaël Varoquaux, Bertrand Thirion category:stat.ML cs.LG published:2012-09-07 summary:Researchers in functional neuroimaging mostly use activation coordinates toformulate their hypotheses. Instead, we propose to use the full statisticalimages to define regions of interest (ROIs). This paper presents two machinelearning approaches, transfer learning and selection transfer, that arecompared upon their ability to identify the common patterns between brainactivation maps related to two functional tasks. We provide some preliminaryquantification of these similarities, and show that selection transfer makes itpossible to set a spatial scale yielding ROIs that are more specific to thecontext of interest than with transfer learning. In particular, selectiontransfer outlines well known regions such as the Visual Word Form Area whendiscriminating between different visual tasks.
arxiv-1200-124 | Estimating Densities with Non-Parametric Exponential Families | http://arxiv.org/pdf/1206.5036v2.pdf | author:Lin Yuan, Sergey Kirshner, Robert Givan category:stat.ML cs.LG published:2012-06-22 summary:We propose a novel approach for density estimation with exponential familiesfor the case when the true density may not fall within the chosen family. Ourapproach augments the sufficient statistics with features designed toaccumulate probability mass in the neighborhood of the observed points,resulting in a non-parametric model similar to kernel density estimators. Weshow that under mild conditions, the resulting model uses only the sufficientstatistics if the density is within the chosen exponential family, andasymptotically, it approximates densities outside of the chosen exponentialfamily. Using the proposed approach, we modify the exponential random graphmodel, commonly used for modeling small-size graph distributions, to addressthe well-known issue of model degeneracy.
arxiv-1200-125 | Wavelet Based Normal and Abnormal Heart Sound Identification using Spectrogram Analysis | http://arxiv.org/pdf/1209.1224v1.pdf | author:Nilanjan Dey, Achintya Das, Sheli Sinha Chaudhuri category:cs.CV published:2012-09-06 summary:The present work proposes a computer-aided normal and abnormal heart soundidentification based on Discrete Wavelet Transform (DWT), it being useful fortele-diagnosis of heart diseases. Due to the presence of Cumulative Frequencycomponents in the spectrogram, DWT is applied on the spectro-gram up to n levelto extract the features from the individual approximation components. Onedimensional feature vector is obtained by evaluating the Row Mean of theapproximation components of these spectrograms. For this present approach, theset of spectrograms has been considered as the database, rather than raw soundsamples. Minimum Euclidean distance is computed between feature vector of thetest sample and the feature vectors of the stored samples to identify the heartsound. By applying this algorithm, almost 82% of accuracy was achieved.
arxiv-1200-126 | FCM Based Blood Vessel Segmentation Method for Retinal Images | http://arxiv.org/pdf/1209.1181v1.pdf | author:Nilanjan Dey, Anamitra Bardhan Roy, Moumita Pal, Achintya Das category:cs.CV published:2012-09-06 summary:Segmentation of blood vessels in retinal images provides early diagnosis ofdiseases like glaucoma, diabetic retinopathy and macular degeneration. Amongthese diseases occurrence of Glaucoma is most frequent and has serious ocularconsequences that can even lead to blindness, if it is not detected early. Theclinical criteria for the diagnosis of glaucoma include intraocular pressuremeasurement, optic nerve head evaluation, retinal nerve fiber layer and visualfield defects. This form of blood vessel segmentation helps in early detectionfor ophthalmic diseases, and potentially reduces the risk of blindness. Thelow-contrast images at the retina owing to narrow blood vessels of the retinaare difficult to extract. These low contrast images are, however useful inrevealing certain systemic diseases. Motivated by the goals of improvingdetection of such vessels, this present work proposes an algorithm forsegmentation of blood vessels and compares the results between expertophthalmologist hand-drawn ground-truths and segmented image(i.e. the output ofthe present work).Sensitivity, specificity, positive predictive value (PPV),positive likelihood ratio (PLR) and accuracy are used to evaluate overallperformance.It is found that this work segments blood vessels successfully withsensitivity, specificity, PPV, PLR and accuracy of 99.62%, 54.66%, 95.08%,219.72 and 95.03%, respectively.
arxiv-1200-127 | Restricting exchangeable nonparametric distributions | http://arxiv.org/pdf/1209.1145v1.pdf | author:Sinead Williamson, Zoubin Ghahramani, Steven N. MacEachern, Eric P. Xing category:stat.ME stat.ML published:2012-09-05 summary:Distributions over exchangeable matrices with infinitely many columns, suchas the Indian buffet process, are useful in constructing nonparametric latentvariable models. However, the distribution implied by such models over thenumber of features exhibited by each data point may be poorly- suited for manymodeling tasks. In this paper, we propose a class of exchangeable nonparametricpriors obtained by restricting the domain of existing models. Such models allowus to specify the distribution over the number of features per data point, andcan achieve better performance on data sets where the number of features is notwell-modeled by the original distribution.
arxiv-1200-128 | A Method of Moments for Mixture Models and Hidden Markov Models | http://arxiv.org/pdf/1203.0683v3.pdf | author:Animashree Anandkumar, Daniel Hsu, Sham M. Kakade category:cs.LG stat.ML published:2012-03-03 summary:Mixture models are a fundamental tool in applied statistics and machinelearning for treating data taken from multiple subpopulations. The currentpractice for estimating the parameters of such models relies on local searchheuristics (e.g., the EM algorithm) which are prone to failure, and existingconsistent methods are unfavorable due to their high computational and samplecomplexity which typically scale exponentially with the number of mixturecomponents. This work develops an efficient method of moments approach toparameter estimation for a broad class of high-dimensional mixture models withmany components, including multi-view mixtures of Gaussians (such as mixturesof axis-aligned Gaussians) and hidden Markov models. The new method leads torigorous unsupervised learning results for mixture models that were notachieved by previous works; and, because of its simplicity, it offers a viablealternative to EM for practical deployment.
arxiv-1200-129 | Video Data Visualization System: Semantic Classification And Personalization | http://arxiv.org/pdf/1209.1125v1.pdf | author:Jamel Slimi, Anis Ben Ammar, Adel M. Alimi category:cs.IR cs.CV cs.MM published:2012-09-05 summary:We present in this paper an intelligent video data visualization tool, basedon semantic classification, for retrieving and exploring a large scale corpusof videos. Our work is based on semantic classification resulting from semanticanalysis of video. The obtained classes will be projected in the visualizationspace. The graph is represented by nodes and edges, the nodes are the keyframesof video documents and the edges are the relation between documents and theclasses of documents. Finally, we construct the user's profile, based on theinteraction with the system, to render the system more adequate to itsreferences.
arxiv-1200-130 | Statistically adaptive learning for a general class of cost functions (SA L-BFGS) | http://arxiv.org/pdf/1209.0029v3.pdf | author:Stephen Purpura, Dustin Hillard, Mark Hubenthal, Jim Walsh, Scott Golder, Scott Smith category:cs.LG stat.ML published:2012-08-31 summary:We present a system that enables rapid model experimentation for tera-scalemachine learning with trillions of non-zero features, billions of trainingexamples, and millions of parameters. Our contribution to the literature is anew method (SA L-BFGS) for changing batch L-BFGS to perform in near real-timeby using statistical tools to balance the contributions of previous weights,old training examples, and new training examples to achieve fast convergencewith few iterations. The result is, to our knowledge, the most scalable andflexible linear learning system reported in the literature, beating standardpractice with the current best system (Vowpal Wabbit and AllReduce). Using theKDD Cup 2012 data set from Tencent, Inc. we provide experimental results toverify the performance of this method.
arxiv-1200-131 | Learning Probability Measures with respect to Optimal Transport Metrics | http://arxiv.org/pdf/1209.1077v1.pdf | author:Guillermo D. Canas, Lorenzo Rosasco category:cs.LG stat.ML K.3.2 published:2012-09-05 summary:We study the problem of estimating, in the sense of optimal transportmetrics, a measure which is assumed supported on a manifold embedded in aHilbert space. By establishing a precise connection between optimal transportmetrics, optimal quantization, and learning theory, we derive new probabilisticbounds for the performance of a classic algorithm in unsupervised learning(k-means), when used to produce a probability measure derived from the data. Inthe course of the analysis, we arrive at new lower bounds, as well asprobabilistic upper bounds on the convergence rate of the empirical law oflarge numbers, which, unlike existing bounds, are applicable to a wide class ofmeasures.
arxiv-1200-132 | Contextually Guided Semantic Labeling and Search for 3D Point Clouds | http://arxiv.org/pdf/1111.5358v3.pdf | author:Abhishek Anand, Hema Swetha Koppula, Thorsten Joachims, Ashutosh Saxena category:cs.RO cs.AI cs.CV published:2011-11-22 summary:RGB-D cameras, which give an RGB image to- gether with depths, are becomingincreasingly popular for robotic perception. In this paper, we address the taskof detecting commonly found objects in the 3D point cloud of indoor scenesobtained from such cameras. Our method uses a graphical model that capturesvarious features and contextual relations, including the local visualappearance and shape cues, object co-occurence relationships and geometricrelationships. With a large number of object classes and relations, the model'sparsimony becomes important and we address that by using multiple types of edgepotentials. We train the model using a maximum-margin learning approach. In ourexperiments over a total of 52 3D scenes of homes and offices (composed fromabout 550 views), we get a performance of 84.06% and 73.38% in labeling officeand home scenes respectively for 17 object classes each. We also present amethod for a robot to search for an object using the learned model and thecontextual information available from the current labelings of the scene. Weapplied this algorithm successfully on a mobile robot for the task of finding12 object classes in 10 different offices and achieved a precision of 97.56%with 78.43% recall.
arxiv-1200-133 | Optimal measures and Markov transition kernels | http://arxiv.org/pdf/1012.0366v7.pdf | author:Roman V. Belavkin category:math.OC cs.CC cs.IT math-ph math.FA math.IT math.MP stat.ML published:2010-12-02 summary:We study optimal solutions to an abstract optimization problem for measures,which is a generalization of classical variational problems in informationtheory and statistical physics. In the classical problems, information andrelative entropy are defined using the Kullback-Leibler divergence, and forthis reason optimal measures belong to a one-parameter exponential family.Measures within such a family have the property of mutual absolute continuity.Here we show that this property characterizes other families of optimalpositive measures if a functional representing information has a strictlyconvex dual. Mutual absolute continuity of optimal probability measures allowsus to strictly separate deterministic and non-deterministic Markov transitionkernels, which play an important role in theories of decisions, estimation,control, communication and computation. We show that deterministic transitionsare strictly sub-optimal, unless information resource with a strictly convexdual is unconstrained. For illustration, we construct an example where, unlikenon-deterministic, any deterministic kernel either has negatively infiniteexpected utility (unbounded expected error) or communicates infiniteinformation.
arxiv-1200-134 | Visual Exploration of Simulated and Measured Blood Flow | http://arxiv.org/pdf/1209.0999v1.pdf | author:Anna Vilanova, Bernhard Preim, Roy van Pelt, Rocco Gasteiger, Mathias Neugebauer, Thomas Wischgoll category:cs.GR cs.CV published:2012-09-05 summary:Morphology of cardiovascular tissue is influenced by the unsteady behavior ofthe blood flow and vice versa. Therefore, the pathogenesis of severalcardiovascular diseases is directly affected by the blood-flow dynamics.Understanding flow behavior is of vital importance to understand thecardiovascular system and potentially harbors a considerable value for bothdiagnosis and risk assessment. The analysis of hemodynamic characteristicsinvolves qualitative and quantitative inspection of the blood-flow field.Visualization plays an important role in the qualitative exploration, as wellas the definition of relevant quantitative measures and its validation. Thereare two main approaches to obtain information about the blood flow: simulationby computational fluid dynamics, and in-vivo measurements. Although research onblood flow simulation has been performed for decades, many open problems remainconcerning accuracy and patient-specific solutions. Possibilities for realmeasurement of blood flow have recently increased considerably by newdevelopments in magnetic resonance imaging which enable the acquisition of 3Dquantitative measurements of blood-flow velocity fields. This chapter presentsthe visualization challenges for both simulation and real measurements ofunsteady blood-flow fields.
arxiv-1200-135 | Sparse Reward Processes | http://arxiv.org/pdf/1201.2555v2.pdf | author:Christos Dimitrakakis category:cs.LG stat.ML published:2012-01-12 summary:We introduce a class of learning problems where the agent is presented with aseries of tasks. Intuitively, if there is relation among those tasks, then theinformation gained during execution of one task has value for the execution ofanother task. Consequently, the agent is intrinsically motivated to explore itsenvironment beyond the degree necessary to solve the current task it has athand. We develop a decision theoretic setting that generalises standardreinforcement learning tasks and captures this intuition. More precisely, weconsider a multi-stage stochastic game between a learning agent and anopponent. We posit that the setting is a good model for the problem oflife-long learning in uncertain environments, where while resources must bespent learning about currently important tasks, there is also the need toallocate effort towards learning about aspects of the world which are notrelevant at the moment. This is due to the fact that unpredictable futureevents may lead to a change of priorities for the decision maker. Thus, in somesense, the model "explains" the necessity of curiosity. Apart from introducingthe general formalism, the paper provides algorithms. These are evaluatedexperimentally in some exemplary domains. In addition, performance bounds areproven for some cases of this problem.
arxiv-1200-136 | Structuring Relevant Feature Sets with Multiple Model Learning | http://arxiv.org/pdf/1209.0913v1.pdf | author:Jun Wang, Alexandros Kalousis category:cs.LG published:2012-09-05 summary:Feature selection is one of the most prominent learning tasks, especially inhigh-dimensional datasets in which the goal is to understand the mechanismsthat underly the learning dataset. However most of them typically deliver justa flat set of relevant features and provide no further information on what kindof structures, e.g. feature groupings, might underly the set of relevantfeatures. In this paper we propose a new learning paradigm in which our goal isto uncover the structures that underly the set of relevant features for a givenlearning problem. We uncover two types of features sets, non-replaceablefeatures that contain important information about the target variable andcannot be replaced by other features, and functionally similar features setsthat can be used interchangeably in learned models, given the presence of thenon-replaceable features, with no change in the predictive performance. To doso we propose a new learning algorithm that learns a number of disjoint modelsusing a model disjointness regularization constraint together with a constrainton the predictive agreement of the disjoint models. We explore the behavior ofour approach on a number of high-dimensional datasets, and show that, asexpected by their construction, these satisfy a number of properties. Namely,model disjointness, a high predictive agreement, and a similar predictiveperformance to models learned on the full set of relevant features. The abilityto structure the set of relevant features in such a manner can become avaluable tool in different applications of scientific knowledge discovery.
arxiv-1200-137 | Improving the K-means algorithm using improved downhill simplex search | http://arxiv.org/pdf/1209.0853v1.pdf | author:Ehsan Saboori, Shafigh Parsazad, Anoosheh Sadeghi category:cs.LG published:2012-09-05 summary:The k-means algorithm is one of the well-known and most popular clusteringalgorithms. K-means seeks an optimal partition of the data by minimizing thesum of squared error with an iterative optimization procedure, which belongs tothe category of hill climbing algorithms. As we know hill climbing searches arefamous for converging to local optimums. Since k-means can converge to a localoptimum, different initial points generally lead to different convergencecancroids, which makes it important to start with a reasonable initialpartition in order to achieve high quality clustering solutions. However, intheory, there exist no efficient and universal methods for determining suchinitial partitions. In this paper we tried to find an optimum initialpartitioning for k-means algorithm. To achieve this goal we proposed a newimproved version of downhill simplex search, and then we used it in order tofind an optimal result for clustering approach and then compare this algorithmwith Genetic Algorithm base (GA), Genetic K-Means (GKM), Improved GeneticK-Means (IGKM) and k-means algorithms.
arxiv-1200-138 | Multiresolution Gaussian Processes | http://arxiv.org/pdf/1209.0833v1.pdf | author:Emily B. Fox, David B. Dunson category:stat.ME stat.ML published:2012-09-05 summary:We propose a multiresolution Gaussian process to capture long-range,non-Markovian dependencies while allowing for abrupt changes. Themultiresolution GP hierarchically couples a collection of smooth GPs, eachdefined over an element of a random nested partition. Long-range dependenciesare captured by the top-level GP while the partition points define the abruptchanges. Due to the inherent conjugacy of the GPs, one can analyticallymarginalize the GPs and compute the conditional likelihood of the observationsgiven the partition tree. This property allows for efficient inference of thepartition itself, for which we employ graph-theoretic techniques. We apply themultiresolution GP to the analysis of Magnetoencephalography (MEG) recordingsof brain activity.
arxiv-1200-139 | Estimating the Hessian by Back-propagating Curvature | http://arxiv.org/pdf/1206.6464v2.pdf | author:James Martens, Ilya Sutskever, Kevin Swersky category:cs.LG stat.ML published:2012-06-27 summary:In this work we develop Curvature Propagation (CP), a general technique forefficiently computing unbiased approximations of the Hessian of any functionthat is computed using a computational graph. At the cost of roughly twogradient evaluations, CP can give a rank-1 approximation of the whole Hessian,and can be repeatedly applied to give increasingly precise unbiased estimatesof any or all of the entries of the Hessian. Of particular interest is thediagonal of the Hessian, for which no general approach is known to exist thatis both efficient and accurate. We show in experiments that CP turns out towork well in practice, giving very accurate estimates of the Hessian of neuralnetworks, for example, with a relatively small amount of work. We also apply CPto Score Matching, where a diagonal of a Hessian plays an integral role in theScore Matching objective, and where it is usually computed exactly usinginefficient algorithms which do not scale to larger and more complex models.
arxiv-1200-140 | Synthesis of Stochastic Flow Networks | http://arxiv.org/pdf/1209.0724v1.pdf | author:Hongchao Zhou, Ho-Lin Chen, Jehoshua Bruck category:cs.IT cs.NE math.IT math.PR published:2012-09-04 summary:A stochastic flow network is a directed graph with incoming edges (inputs)and outgoing edges (outputs), tokens enter through the input edges, travelstochastically in the network, and can exit the network through the outputedges. Each node in the network is a splitter, namely, a token can enter a nodethrough an incoming edge and exit on one of the output edges according to apredefined probability distribution. Stochastic flow networks can be easilyimplemented by DNA-based chemical reactions, with promising applications inmolecular computing and stochastic computing. In this paper, we address afundamental synthesis question: Given a finite set of possible splitters and anarbitrary rational probability distribution, design a stochastic flow network,such that every token that enters the input edge will exit the outputs with theprescribed probability distribution. The problem of probability transformation dates back to von Neumann's 1951work and was followed, among others, by Knuth and Yao in 1976. Most existingworks have been focusing on the "simulation" of target distributions. In thispaper, we design optimal-sized stochastic flow networks for "synthesizing"target distributions. It shows that when each splitter has two outgoing edgesand is unbiased, an arbitrary rational probability \frac{a}{b} with a\leq b\leq2^n can be realized by a stochastic flow network of size n that is optimal.Compared to the other stochastic systems, feedback (cycles in networks)strongly improves the expressibility of stochastic flow networks.
arxiv-1200-141 | Isoelastic Agents and Wealth Updates in Machine Learning Markets | http://arxiv.org/pdf/1206.6443v2.pdf | author:Amos Storkey, Jono Millin, Krzysztof Geras category:cs.LG cs.GT stat.ML published:2012-06-27 summary:Recently, prediction markets have shown considerable promise for developingflexible mechanisms for machine learning. In this paper, agents with isoelasticutilities are considered. It is shown that the costs associated withhomogeneous markets of agents with isoelastic utilities produce equilibriumprices corresponding to alpha-mixtures, with a particular form of mixingcomponent relating to each agent's wealth. We also demonstrate that wealthaccumulation for logarithmic and other isoelastic agents (through payoffs onprediction of training targets) can implement both Bayesian model updates andmixture weight updates by imposing different market payoff structures. Aniterative algorithm is given for market equilibrium computation. We demonstratethat inhomogeneous markets of agents with isoelastic utilities outperform stateof the art aggregate classifiers such as random forests, as well as singleclassifiers (neural networks, decision trees) on a number of machine learningbenchmarks, and show that isoelastic combination methods are generally betterthan their logarithmic counterparts.
arxiv-1200-142 | A Split-Merge Framework for Comparing Clusterings | http://arxiv.org/pdf/1206.6475v2.pdf | author:Qiaoliang Xiang, Qi Mao, Kian Ming Chai, Hai Leong Chieu, Ivor Tsang, Zhendong Zhao category:cs.LG stat.ML published:2012-06-27 summary:Clustering evaluation measures are frequently used to evaluate theperformance of algorithms. However, most measures are not properly normalizedand ignore some information in the inherent structure of clusterings. We modelthe relation between two clusterings as a bipartite graph and propose a generalcomponent-based decomposition formula based on the components of the graph.Most existing measures are examples of this formula. In order to satisfyconsistency in the component, we further propose a split-merge framework forcomparing clusterings of different data sets. Our framework gives measures thatare conditionally normalized, and it can make use of data point information,such as feature vectors and pairwise distances. We use an entropy-basedinstance of the framework and a coreference resolution data set to demonstrateempirically the utility of our framework over other measures.
arxiv-1200-143 | Efficient EM Training of Gaussian Mixtures with Missing Data | http://arxiv.org/pdf/1209.0521v1.pdf | author:Olivier Delalleau, Aaron Courville, Yoshua Bengio category:cs.LG stat.ML published:2012-09-04 summary:In data-mining applications, we are frequently faced with a large fraction ofmissing entries in the data matrix, which is problematic for most discriminantmachine learning algorithms. A solution that we explore in this paper is theuse of a generative model (a mixture of Gaussians) to compute the conditionalexpectation of the missing variables given the observed variables. Sincetraining a Gaussian mixture with many different patterns of missing values canbe computationally very expensive, we introduce a spanning-tree based algorithmthat significantly speeds up training in these conditions. We also observe thatgood results can be obtained by using the generative model to fill-in themissing values for a separate discriminant learning algorithm.
arxiv-1200-144 | Monotonicity of Fitness Landscapes and Mutation Rate Control | http://arxiv.org/pdf/1209.0514v1.pdf | author:Roman V. Belavkin, Alastair Channon, Elizabeth Aston, John Aston, Rok Krasovec, Christopher G. Knight category:q-bio.PE cs.IT cs.NE math.IT math.OC published:2012-09-04 summary:The typical view in evolutionary biology is that mutation rates areminimised. Contrary to that view, studies in combinatorial optimisation andsearch have shown a clear advantage of using variable mutation rates as acontrol parameter to optimise the performance of evolutionary algorithms.Ronald Fisher's work is the basis of much biological theory in this area. Heused Euclidean geometry of continuous, infinite phenotypic spaces to study therelation between mutation size and expected fitness of the offspring. Here wedevelop a general theory of optimal mutation rate control that is based on thealternative geometry of discrete and finite spaces of DNA sequences. We definethe monotonic properties of fitness landscapes, which allows us to relatefitness to the topology of genotypes and mutation size. First, we consider thecase of a perfectly monotonic fitness landscape, in which the optimal mutationrate control functions can be derived exactly or approximately depending onadditional constraints of the problem. Then we consider the general case ofnon-monotonic landscapes. We use the ideas of local and weak monotonicity toshow that optimal mutation rate control functions exist in any such landscapeand that they resemble control functions in a monotonic landscape at least insome neighbourhood of a fitness maximum. Generally, optimal mutation ratesincrease when fitness decreases, and the increase of mutation rate is morerapid in landscapes that are less monotonic (more rugged). We demonstrate theserelationships by obtaining and analysing approximately optimal mutation ratecontrol functions in 115 complete landscapes of binding scores between DNAsequences and transcription factors. We discuss the relevance of these findingsto living organisms, including the phenomenon of stress-induced mutagenesis.
arxiv-1200-145 | Learning Parameterized Skills | http://arxiv.org/pdf/1206.6398v2.pdf | author:Bruno Da Silva, George Konidaris, Andrew Barto category:cs.LG stat.ML published:2012-06-27 summary:We introduce a method for constructing skills capable of solving tasks drawnfrom a distribution of parameterized reinforcement learning problems. Themethod draws example tasks from a distribution of interest and uses thecorresponding learned policies to estimate the topology of thelower-dimensional piecewise-smooth manifold on which the skill policies lie.This manifold models how policy parameters change as task parameters vary. Themethod identifies the number of charts that compose the manifold and thenapplies non-linear regression in each chart to construct a parameterized skillby predicting policy parameters from task parameters. We evaluate our method onan underactuated simulated robotic arm tasked with learning to accurately throwdarts at a parameterized target location.
arxiv-1200-146 | Proximal methods for the latent group lasso penalty | http://arxiv.org/pdf/1209.0368v1.pdf | author:Silvia Villa, Lorenzo Rosasco, Sofia Mosci, Alessandro Verri category:math.OC cs.LG stat.ML 65K10, 90C25 published:2012-09-03 summary:We consider a regularized least squares problem, with regularization bystructured sparsity-inducing norms, which extend the usual $\ell_1$ and thegroup lasso penalty, by allowing the subsets to overlap. Such regularizationslead to nonsmooth problems that are difficult to optimize, and we propose inthis paper a suitable version of an accelerated proximal method to solve them.We prove convergence of a nested procedure, obtained composing an acceleratedproximal method with an inner algorithm for computing the proximity operator.By exploiting the geometrical properties of the penalty, we devise a new activeset strategy, thanks to which the inner iteration is relatively fast, thusguaranteeing good computational performances of the overall algorithm. Ourapproach allows to deal with high dimensional problems without pre-processingfor dimensionality reduction, leading to better computational and predictionperformances with respect to the state-of-the art methods, as shown empiricallyboth on toy and real data.
arxiv-1200-147 | Seeded Graph Matching | http://arxiv.org/pdf/1209.0367v1.pdf | author:Donniell E. Fishkind, Sancar Adali, Carey E. Priebe category:stat.ML published:2012-09-03 summary:Graph inference is a burgeoning field in the applied and theoreticalstatistics communities, as well as throughout the wider world of science,engineering, business, etc. Given two graphs on the same number of vertices,the graph matching problem is to find a bijection between the two vertex setswhich minimizes the number of adjacency disagreements between the two graphs.The seeded graph matching problem is the graph matching problem with anadditional constraint that the bijection assigns some particular vertices ofone vertex set to respective particular vertices of the other vertex set.Solving the (seeded) graph matching problem will enable methodologies for manygraph inference tasks, but the problem is NP-hard. We modify thestate-of-the-art approximate graph matching algorithm of Vogelstein et al.(2012) to make it a fast approximate seeded graph matching algorithm. Wedemonstrate the effectiveness of our algorithm - and the potential for dramaticperformance improvement from incorporating just a few seeds - via simulationand real data experiments.
arxiv-1200-148 | Ranking-Based Black-Box Complexity | http://arxiv.org/pdf/1102.1140v3.pdf | author:Benjamin Doerr, Carola Winzen category:cs.NE cs.CC cs.DS published:2011-02-06 summary:Randomized search heuristics such as evolutionary algorithms, simulatedannealing, and ant colony optimization are a broadly used class ofgeneral-purpose algorithms. Analyzing them via classical methods of theoreticalcomputer science is a growing field. While several strong runtime analysisresults have appeared in the last 20 years, a powerful complexity theory forsuch algorithms is yet to be developed. We enrich the existing notions ofblack-box complexity by the additional restriction that not the actualobjective values, but only the relative quality of the previously evaluatedsolutions may be taken into account by the black-box algorithm. Many randomizedsearch heuristics belong to this class of algorithms. We show that the new ranking-based model gives more realistic complexityestimates for some problems. For example, the class of all binary-valuefunctions has a black-box complexity of $O(\log n)$ in the previous black-boxmodels, but has a ranking-based complexity of $\Theta(n)$. For the class of all OneMax functions, we present a ranking-based black-boxalgorithm that has a runtime of $\Theta(n / \log n)$, which shows that theOneMax problem does not become harder with the additional ranking-basednessrestriction.
arxiv-1200-149 | Robopinion: Opinion Mining Framework Inspired by Autonomous Robot Navigation | http://arxiv.org/pdf/1209.0249v1.pdf | author:M. A. El-Dosuky, M. Z. Rashad, T. T. Hamza, A. H. EL-Bassiouny category:cs.CL cs.IR published:2012-09-03 summary:Data association methods are used by autonomous robots to find matchesbetween the current landmarks and the new set of observed features. We seek aframework for opinion mining to benefit from advancements in autonomous robotnavigation in both research and development
arxiv-1200-150 | Binary hidden Markov models and varieties | http://arxiv.org/pdf/1206.0500v3.pdf | author:Andrew J. Critch category:math.AG stat.ML 14Q15 published:2012-06-03 summary:The technological applications of hidden Markov models have been extremelydiverse and successful, including natural language processing, gesturerecognition, gene sequencing, and Kalman filtering of physical measurements.HMMs are highly non-linear statistical models, and just as linear models areamenable to linear algebraic techniques, non-linear models are amenable tocommutative algebra and algebraic geometry. This paper closely examines HMMs in which all the hidden random variables arebinary. Its main contributions are (1) a birational parametrization for everysuch HMM, with an explicit inverse for recovering the hidden parameters interms of observables, (2) a semialgebraic model membership test for every suchHMM, and (3) minimal defining equations for the 4-node fully binary model,comprising 21 quadrics and 29 cubics, which were computed using Grobner basesin the cumulant coordinates of Sturmfels and Zwiernik. The new model parametersin (1) are rationally identifiable in the sense of Sullivant, Garcia-Puente,and Spielvogel, and each model's Zariski closure is therefore a rationalprojective variety of dimension 5. Grobner basis computations for the model andits graph are found to be considerably faster using these parameters. In thecase of two hidden states, item (2) supersedes a previous algorithm ofSchonhuth which is only generically defined, and the defining equations (3)yield new invariants for HMMs of all lengths $\geq 4$. Such invariants havebeen used successfully in model selection problems in phylogenetics, and onecan hope for similar applications in the case of HMMs.
arxiv-1200-151 | Transductive Ordinal Regression | http://arxiv.org/pdf/1102.2808v5.pdf | author:Chun-Wei Seah, Ivor W. Tsang, Yew-Soon Ong category:cs.LG published:2011-02-14 summary:Ordinal regression is commonly formulated as a multi-class problem withordinal constraints. The challenge of designing accurate classifiers forordinal regression generally increases with the number of classes involved, dueto the large number of labeled patterns that are needed. The availability ofordinal class labels, however, is often costly to calibrate or difficult toobtain. Unlabeled patterns, on the other hand, often exist in much greaterabundance and are freely available. To take benefits from the abundance ofunlabeled patterns, we present a novel transductive learning paradigm forordinal regression in this paper, namely Transductive Ordinal Regression (TOR).The key challenge of the present study lies in the precise estimation of boththe ordinal class label of the unlabeled data and the decision functions of theordinal classes, simultaneously. The core elements of the proposed TOR includean objective function that caters to several commonly used loss functionscasted in transductive settings, for general ordinal regression. A labelswapping scheme that facilitates a strictly monotonic decrease in the objectivefunction value is also introduced. Extensive numerical studies on commonly usedbenchmark datasets including the real world sentiment prediction problem arethen presented to showcase the characteristics and efficacies of the proposedtransductive ordinal regression. Further, comparisons to recentstate-of-the-art ordinal regression methods demonstrate the introducedtransductive learning paradigm for ordinal regression led to the robust andimproved performance.
arxiv-1200-152 | Ranked bandits in metric spaces: learning optimally diverse rankings over large document collections | http://arxiv.org/pdf/1005.5197v2.pdf | author:Aleksandrs Slivkins, Filip Radlinski, Sreenivas Gollapudi category:cs.LG cs.DS published:2010-05-28 summary:Most learning to rank research has assumed that the utility of differentdocuments is independent, which results in learned ranking functions thatreturn redundant results. The few approaches that avoid this have ratherunsatisfyingly lacked theoretical foundations, or do not scale. We present alearning-to-rank formulation that optimizes the fraction of satisfied users,with several scalable algorithms that explicitly takes document similarity andranking context into account. Our formulation is a non-trivial commongeneralization of two multi-armed bandit models from the literature: "rankedbandits" (Radlinski et al., ICML 2008) and "Lipschitz bandits" (Kleinberg etal., STOC 2008). We present theoretical justifications for this approach, aswell as a near-optimal algorithm. Our evaluation adds optimizations thatimprove empirical performance, and shows that our algorithms learn orders ofmagnitude more quickly than previous approaches.
arxiv-1200-153 | FASTSUBS: An Efficient and Exact Procedure for Finding the Most Likely Lexical Substitutes Based on an N-gram Language Model | http://arxiv.org/pdf/1205.5407v2.pdf | author:Deniz Yuret category:cs.CL published:2012-05-24 summary:Lexical substitutes have found use in areas such as paraphrasing, textsimplification, machine translation, word sense disambiguation, and part ofspeech induction. However the computational complexity of accuratelyidentifying the most likely substitutes for a word has made large scaleexperiments difficult. In this paper I introduce a new search algorithm,FASTSUBS, that is guaranteed to find the K most likely lexical substitutes fora given word in a sentence based on an n-gram language model. The computationis sub-linear in both K and the vocabulary size V. An implementation of thealgorithm and a dataset with the top 100 substitutes of each token in the WSJsection of the Penn Treebank are available at http://goo.gl/jzKH0.
arxiv-1200-154 | Comparison of the C4.5 and a Naive Bayes Classifier for the Prediction of Lung Cancer Survivability | http://arxiv.org/pdf/1206.1121v2.pdf | author:George Dimitoglou, James A. Adams, Carol M. Jim category:cs.LG published:2012-06-06 summary:Numerous data mining techniques have been developed to extract informationand identify patterns and predict trends from large data sets. In this study,two classification techniques, the J48 implementation of the C4.5 algorithm anda Naive Bayes classifier are applied to predict lung cancer survivability froman extensive data set with fifteen years of patient records. The purpose of theproject is to verify the predictive effectiveness of the two techniques onreal, historical data. Besides the performance outcome that renders J48marginally better than the Naive Bayes technique, there is a detaileddescription of the data and the required pre-processing activities. Theperformance results confirm expectations while some of the issues that appearedduring experimentation, underscore the value of having domain-specificunderstanding to leverage any domain-specific characteristics inherent in thedata.
arxiv-1200-155 | Learning implicitly in reasoning in PAC-Semantics | http://arxiv.org/pdf/1209.0056v1.pdf | author:Brendan Juba category:cs.AI cs.DS cs.LG cs.LO published:2012-09-01 summary:We consider the problem of answering queries about formulas of propositionallogic based on background knowledge partially represented explicitly as otherformulas, and partially represented as partially obscured examplesindependently drawn from a fixed probability distribution, where the queriesare answered with respect to a weaker semantics than usual -- PAC-Semantics,introduced by Valiant (2000) -- that is defined using the distribution ofexamples. We describe a fairly general, efficient reduction to limited versionsof the decision problem for a proof system (e.g., bounded space treelikeresolution, bounded degree polynomial calculus, etc.) from correspondingversions of the reasoning problem where some of the background knowledge is notexplicitly given as formulas, only learnable from the examples. Crucially, wedo not generate an explicit representation of the knowledge extracted from theexamples, and so the "learning" of the background knowledge is only doneimplicitly. As a consequence, this approach can utilize formulas as backgroundknowledge that are not perfectly valid over the distribution---essentially theanalogue of agnostic learning here.
arxiv-1200-156 | A Session Based Blind Watermarking Technique within the NROI of Retinal Fundus Images for Authentication Using DWT, Spread Spectrum and Harris Corner Detection | http://arxiv.org/pdf/1209.0053v1.pdf | author:Nilanjan Dey, Moumita Pal, Achintya Das category:cs.CV cs.CY published:2012-09-01 summary:Digital Retinal Fundus Images helps to detect various ophthalmic diseases bydetecting morphological changes in optical cup, optical disc and macula.Present work proposes a method for the authentication of medical images basedon Discrete Wavelet Transformation (DWT) and Spread Spectrum. Proper selectionof the Non Region of Interest (NROI) for watermarking is crucial, as the areaunder concern has to be the least required portion conveying any medicalinformation. Proposed method discusses both the selection of least impact areaand the blind watermarking technique. Watermark is embedded within theHigh-High (HH) sub band. During embedding, watermarked image is dispersedwithin the band using a pseudo random sequence and a Session key. Watermarkedimage is extracted using the session key and the size of the image. In thisapproach the generated watermarked image having an acceptable level ofimperceptibility and distortion is compared to the Original retinal image basedon Peak Signal to Noise Ratio (PSNR) and correlation value.
arxiv-1200-157 | Steps Towards a Theory of Visual Information: Active Perception, Signal-to-Symbol Conversion and the Interplay Between Sensing and Control | http://arxiv.org/pdf/1110.2053v3.pdf | author:Stefano Soatto category:cs.CV published:2011-10-10 summary:This manuscript describes the elements of a theory of information tailored tocontrol and decision tasks and specifically to visual data. The concept ofActionable Information is described, that relates to a notion of informationchampioned by J. Gibson, and a notion of "complete information" that relates tothe minimal sufficient statistics of a complete representation. It is shownthat the "actionable information gap" between the two can be reduced byexercising control on the sensing process. Thus, senging, control andinformation are inextricably tied. This has consequences in the so-called"signal-to-symbol barrier" problem, as well as in the analysis and design ofactive sensing systems. It has ramifications in vision-based control,navigation, 3-D reconstruction and rendering, as well as detection,localization, recognition and categorization of objects and scenes in livevideo. This manuscript has been developed from a set of lecture notes for a summercourse at the First International Computer Vision Summer School (ICVSS) inScicli, Italy, in July of 2008. They were later expanded and amended forsubsequent lectures in the same School in July 2009. Starting on November 1,2009, they were further expanded for a special topics course, CS269, taught atUCLA in the Spring term of 2010.
arxiv-1200-158 | Vector Field k-Means: Clustering Trajectories by Fitting Multiple Vector Fields | http://arxiv.org/pdf/1208.5801v2.pdf | author:Nivan Ferreira, James T. Klosowski, Carlos Scheidegger, Claudio Silva category:cs.LG published:2012-08-28 summary:Scientists study trajectory data to understand trends in movement patterns,such as human mobility for traffic analysis and urban planning. There is apressing need for scalable and efficient techniques for analyzing this data anddiscovering the underlying patterns. In this paper, we introduce a noveltechnique which we call vector-field $k$-means. The central idea of our approach is to use vector fields to induce asimilarity notion between trajectories. Other clustering algorithms seek arepresentative trajectory that best describes each cluster, much like $k$-meansidentifies a representative "center" for each cluster. Vector-field $k$-means,on the other hand, recognizes that in all but the simplest examples, no singletrajectory adequately describes a cluster. Our approach is based on the premisethat movement trends in trajectory data can be modeled as flows within multiplevector fields, and the vector field itself is what defines each of theclusters. We also show how vector-field $k$-means connects techniques forscalar field design on meshes and $k$-means clustering. We present an algorithm that finds a locally optimal clustering oftrajectories into vector fields, and demonstrate how vector-field $k$-means canbe used to mine patterns from trajectory data. We present experimental evidenceof its effectiveness and efficiency using several datasets, includinghistorical hurricane data, GPS tracks of people and vehicles, and anonymouscall records from a large phone company. We compare our results to previoustrajectory clustering techniques, and find that our algorithm performs fasterin practice than the current state-of-the-art in trajectory clustering, in someexamples by a large margin.
arxiv-1200-159 | Combinatorial Gradient Fields for 2D Images with Empirically Convergent Separatrices | http://arxiv.org/pdf/1208.6523v1.pdf | author:Jan Reininghaus, David Günther, Ingrid Hotz, Tino Weinkauf, Hans Peter Seidel category:cs.CV cs.CG cs.DM 68U10 published:2012-08-31 summary:This paper proposes an efficient probabilistic method that computescombinatorial gradient fields for two dimensional image data. In contrast toexisting algorithms, this approach yields a geometric Morse-Smale complex thatconverges almost surely to its continuous counterpart when the image resolutionis increased. This approach is motivated using basic ideas from probabilitytheory and builds upon an algorithm from discrete Morse theory with a strongmathematical foundation. While a formal proof is only hinted at, we do providea thorough numerical evaluation of our method and compare it to establishedalgorithms.
arxiv-1200-160 | A two-stage denoising filter: the preprocessed Yaroslavsky filter | http://arxiv.org/pdf/1208.6516v1.pdf | author:Joseph Salmon, Rebecca Willett, Ery Arias-Castro category:cs.CV math.ST stat.TH published:2012-08-31 summary:This paper describes a simple image noise removal method which combines apreprocessing step with the Yaroslavsky filter for strong numerical, visual,and theoretical performance on a broad class of images. The framework developedis a two-stage approach. In the first stage the image is filtered with aclassical denoising method (e.g., wavelet or curvelet thresholding). In thesecond stage a modification of the Yaroslavsky filter is performed on theoriginal noisy image, where the weights of the filters are governed by pixelsimilarities in the denoised image from the first stage. Similar prefilteringideas have proved effective previously in the literature, and this paperprovides theoretical guarantees and important insight into why prefiltering canbe effective. Empirically, this simple approach achieves very good performancefor cartoon images, and can be computed much more quickly than currentpatch-based denoising algorithms.
arxiv-1200-161 | A prototype for projecting HPSG syntactic lexica towards LMF | http://arxiv.org/pdf/1207.5328v3.pdf | author:Kais Haddar, Héla Fehri, Laurent Romary category:cs.CL published:2012-07-23 summary:The comparative evaluation of Arabic HPSG grammar lexica requires a deepstudy of their linguistic coverage. The complexity of this task results mainlyfrom the heterogeneity of the descriptive components within those lexica(underlying linguistic resources and different data categories, for example).It is therefore essential to define more homogeneous representations, which inturn will enable us to compare them and eventually merge them. In this context,we present a method for comparing HPSG lexica based on a rule system. Thismethod is implemented within a prototype for the projection from Arabic HPSG toa normalised pivot language compliant with LMF (ISO 24613 - Lexical MarkupFramework) and serialised using a TEI (Text Encoding Initiative) basedrepresentation. The design of this system is based on an initial study of theHPSG formalism looking at its adequacy for the representation of Arabic, andfrom this, we identify the appropriate feature structures corresponding to eachArabic lexical category and their possible LMF counterparts.
arxiv-1200-162 | A Widely Applicable Bayesian Information Criterion | http://arxiv.org/pdf/1208.6338v1.pdf | author:Sumio Watanabe category:cs.LG stat.ML published:2012-08-31 summary:A statistical model or a learning machine is called regular if the map takinga parameter to a probability distribution is one-to-one and if its Fisherinformation matrix is always positive definite. If otherwise, it is calledsingular. In regular statistical models, the Bayes free energy, which isdefined by the minus logarithm of Bayes marginal likelihood, can beasymptotically approximated by the Schwarz Bayes information criterion (BIC),whereas in singular models such approximation does not hold. Recently, it was proved that the Bayes free energy of a singular model isasymptotically given by a generalized formula using a birational invariant, thereal log canonical threshold (RLCT), instead of half the number of parametersin BIC. Theoretical values of RLCTs in several statistical models are now beingdiscovered based on algebraic geometrical methodology. However, it has beendifficult to estimate the Bayes free energy using only training samples,because an RLCT depends on an unknown true distribution. In the present paper, we define a widely applicable Bayesian informationcriterion (WBIC) by the average log likelihood function over the posteriordistribution with the inverse temperature $1/\log n$, where $n$ is the numberof training samples. We mathematically prove that WBIC has the same asymptoticexpansion as the Bayes free energy, even if a statistical model is singular forand unrealizable by a statistical model. Since WBIC can be numericallycalculated without any information about a true distribution, it is ageneralized version of BIC onto singular statistical models.
arxiv-1200-163 | Comparative Study and Optimization of Feature-Extraction Techniques for Content based Image Retrieval | http://arxiv.org/pdf/1208.6335v1.pdf | author:Aman Chadha, Sushmit Mallik, Ravdeep Johar category:cs.CV published:2012-08-30 summary:The aim of a Content-Based Image Retrieval (CBIR) system, also known as Queryby Image Content (QBIC), is to help users to retrieve relevant images based ontheir contents. CBIR technologies provide a method to find images in largedatabases by using unique descriptors from a trained image. The imagedescriptors include texture, color, intensity and shape of the object inside animage. Several feature-extraction techniques viz., Average RGB, Color Moments,Co-occurrence, Local Color Histogram, Global Color Histogram and GeometricMoment have been critically compared in this paper. However, individually thesetechniques result in poor performance. So, combinations of these techniqueshave also been evaluated and results for the most efficient combination oftechniques have been presented and optimized for each class of image query. Wealso propose an improvement in image retrieval performance by introducing theidea of Query modification through image cropping. It enables the user toidentify a region of interest and modify the initial query to refine andpersonalize the image retrieval results.
arxiv-1200-164 | An Improved Bound for the Nystrom Method for Large Eigengap | http://arxiv.org/pdf/1209.0001v1.pdf | author:Mehrdad Mahdavi, Tianbao Yang, Rong Jin category:cs.LG cs.NA stat.ML published:2012-08-30 summary:We develop an improved bound for the approximation error of the Nystr\"{o}mmethod under the assumption that there is a large eigengap in the spectrum ofkernel matrix. This is based on the empirical observation that the eigengap hasa significant impact on the approximation error of the Nystr\"{o}m method. Ourapproach is based on the concentration inequality of integral operator and thetheory of matrix perturbation. Our analysis shows that when there is a largeeigengap, we can improve the approximation error of the Nystr\"{o}m method from$O(N/m^{1/4})$ to $O(N/m^{1/2})$ when measured in Frobenius norm, where $N$ isthe size of the kernel matrix, and $m$ is the number of sampled columns.
arxiv-1200-165 | Link Prediction via Generalized Coupled Tensor Factorisation | http://arxiv.org/pdf/1208.6231v1.pdf | author:Beyza Ermiş, Evrim Acar, A. Taylan Cemgil category:cs.LG published:2012-08-30 summary:This study deals with the missing link prediction problem: the problem ofpredicting the existence of missing connections between entities of interest.We address link prediction using coupled analysis of relational datasetsrepresented as heterogeneous data, i.e., datasets in the form of matrices andhigher-order tensors. We propose to use an approach based on probabilisticinterpretation of tensor factorisation models, i.e., Generalised Coupled TensorFactorisation, which can simultaneously fit a large class of tensor models tohigher-order tensors/matrices with com- mon latent factors using different lossfunctions. Numerical experiments demonstrate that joint analysis of data frommultiple sources via coupled factorisation improves the link predictionperformance and the selection of right loss function and tensor model iscrucial for accurately predicting missing links.
arxiv-1200-166 | Benchmarking recognition results on word image datasets | http://arxiv.org/pdf/1208.6137v1.pdf | author:Deepak Kumar, M N Anil Prasad, A G Ramakrishnan category:cs.CV published:2012-08-30 summary:We have benchmarked the maximum obtainable recognition accuracy on variousword image datasets using manual segmentation and a currently availablecommercial OCR. We have developed a Matlab program, with graphical userinterface, for semi-automated pixel level segmentation of word images. Wediscuss the advantages of pixel level annotation. We have covered fivedatabases adding up to over 3600 word images. These word images have beencropped from camera captured scene, born-digital and street view images. Werecognize the segmented word image using the trial version of Nuance OmnipageOCR. We also discuss, how the degradations introduced during acquisition orinaccuracies introduced during creation of word images affect the recognitionof the word present in the image. Word images for different kinds ofdegradations and correction for slant and curvy nature of words are alsodiscussed. The word recognition rates obtained on ICDAR 2003, Sign evaluation,Street view, Born-digital and ICDAR 2011 datasets are 83.9%, 89.3%, 79.6%,88.5% and 86.7% respectively.
arxiv-1200-167 | Average word length dynamics as indicator of cultural changes in society | http://arxiv.org/pdf/1208.6109v1.pdf | author:Vladimir V. Bochkarev, Anna V. Shevlyakova, Valery D. Solovyev category:cs.CL 91F20 J.5 published:2012-08-30 summary:Dynamics of average length of words in Russian and English is analysed in thearticle. Words belonging to the diachronic text corpus Google Books Ngram anddated back to the last two centuries are studied. It was found out that averageword length slightly increased in the 19th century, and then it was growingrapidly most of the 20th century and started decreasing over the period fromthe end of the 20th - to the beginning of the 21th century. Words whichcontributed mostly to increase or decrease of word average length wereidentified. At that, content words and functional words are analysedseparately. Long content words contribute mostly to word average length ofword. As it was shown, these words reflect the main tendencies of socialdevelopment and thus, are used frequently. Change of frequency of personalpronouns also contributes significantly to change of average word length. Theother parameters connected with average length of word were also analysed.
arxiv-1200-168 | Message passing with relaxed moment matching | http://arxiv.org/pdf/1204.4166v2.pdf | author:Yuan Qi, Yandong Guo category:cs.LG stat.CO stat.ML published:2012-04-18 summary:Bayesian learning is often hampered by large computational expense. As apowerful generalization of popular belief propagation, expectation propagation(EP) efficiently approximates the exact Bayesian computation. Nevertheless, EPcan be sensitive to outliers and suffer from divergence for difficult cases. Toaddress this issue, we propose a new approximate inference approach, relaxedexpectation propagation (REP). It relaxes the moment matching requirement ofexpectation propagation by adding a relaxation factor into the KL minimization.We penalize this relaxation with a $l_1$ penalty. As a result, when twodistributions in the relaxed KL divergence are similar, the relaxation factorwill be penalized to zero and, therefore, we obtain the original momentmatching; In the presence of outliers, these two distributions aresignificantly different and the relaxation factor will be used to reduce thecontribution of the outlier. Based on this penalized KL minimization, REP isrobust to outliers and can greatly improve the posterior approximation qualityover EP. To examine the effectiveness of REP, we apply it to Gaussian processclassification, a task known to be suitable to EP. Our classification resultson synthetic and UCI benchmark datasets demonstrate significant improvement ofREP over EP and Power EP--in terms of algorithmic stability, estimationaccuracy and predictive performance.
arxiv-1200-169 | Multivariate information measures: an experimentalist's perspective | http://arxiv.org/pdf/1111.6857v5.pdf | author:Nicholas Timme, Wesley Alford, Benjamin Flecker, John M. Beggs category:cs.IT cs.LG math.IT stat.AP 94A15 published:2011-11-28 summary:Information theory is widely accepted as a powerful tool for analyzingcomplex systems and it has been applied in many disciplines. Recently, somecentral components of information theory - multivariate information measures -have found expanded use in the study of several phenomena. These informationmeasures differ in subtle yet significant ways. Here, we will review theinformation theory behind each measure, as well as examine the differencesbetween these measures by applying them to several simple model systems. Inaddition to these systems, we will illustrate the usefulness of the informationmeasures by analyzing neural spiking data from a dissociated culture throughearly stages of its development. We hope that this work will aid otherresearchers as they seek the best multivariate information measure for theirspecific research goals and system. Finally, we have made software availableonline which allows the user to calculate all of the information measuresdiscussed within this paper.
arxiv-1200-170 | Practical Bayesian Optimization of Machine Learning Algorithms | http://arxiv.org/pdf/1206.2944v2.pdf | author:Jasper Snoek, Hugo Larochelle, Ryan P. Adams category:stat.ML cs.LG published:2012-06-13 summary:Machine learning algorithms frequently require careful tuning of modelhyperparameters, regularization terms, and optimization parameters.Unfortunately, this tuning is often a "black art" that requires expertexperience, unwritten rules of thumb, or sometimes brute-force search. Muchmore appealing is the idea of developing automatic approaches which canoptimize the performance of a given learning algorithm to the task at hand. Inthis work, we consider the automatic tuning problem within the framework ofBayesian optimization, in which a learning algorithm's generalizationperformance is modeled as a sample from a Gaussian process (GP). The tractableposterior distribution induced by the GP leads to efficient use of theinformation gathered by previous experiments, enabling optimal choices aboutwhat parameters to try next. Here we show how the effects of the Gaussianprocess prior and the associated inference procedure can have a large impact onthe success or failure of Bayesian optimization. We show that thoughtfulchoices can lead to results that exceed expert-level performance in tuningmachine learning algorithms. We also describe new algorithms that take intoaccount the variable cost (duration) of learning experiments and that canleverage the presence of multiple cores for parallel experimentation. We showthat these proposed algorithms improve on previous automatic procedures and canreach or surpass human expert-level optimization on a diverse set ofcontemporary algorithms including latent Dirichlet allocation, structured SVMsand convolutional neural networks.
arxiv-1200-171 | The expected performance of stellar parametrization with Gaia spectrophotometry | http://arxiv.org/pdf/1207.6005v2.pdf | author:C. Liu, C. A. L. Bailer-Jones, R. Sordo, A. Vallenari, R. Borrachero, X. Luri, P. Sartoretti category:astro-ph.IM astro-ph.GA stat.ML published:2012-07-25 summary:Gaia will obtain astrometry and spectrophotometry for essentially all sourcesin the sky down to a broad band magnitude limit of G=20, an expected yield of10^9 stars. Its main scientific objective is to reveal the formation andevolution of our Galaxy through chemo-dynamical analysis. In addition toinferring positions, parallaxes and proper motions from the astrometry, we mustalso infer the astrophysical parameters of the stars from thespectrophotometry, the BP/RP spectrum. Here we investigate the performance ofthree different algorithms (SVM, ILIUM, Aeneas) for estimating the effectivetemperature, line-of-sight interstellar extinction, metallicity and surfacegravity of A-M stars over a wide range of these parameters and over the fullmagnitude range Gaia will observe (G=6-20mag). One of the algorithms, Aeneas,infers the posterior probability density function over all parameters, and canoptionally take into account the parallax and the Hertzsprung-Russell diagramto improve the estimates. For all algorithms the accuracy of estimation dependson G and on the value of the parameters themselves, so a broad summary ofperformance is only approximate. For stars at G=15 with less than twomagnitudes extinction, we expect to be able to estimate Teff to within 1%, loggto 0.1-0.2dex, and [Fe/H] (for FGKM stars) to 0.1-0.2dex, just using the BP/RPspectrum (mean absolute error statistics are quoted). Performance degrades atlarger extinctions, but not always by a large amount. Extinction can beestimated to an accuracy of 0.05-0.2mag for stars across the full parameterrange with a priori unknown extinction between 0 and 10mag. Performancedegrades at fainter magnitudes, but even at G=19 we can estimate logg to betterthan 0.2dex for all spectral types, and [Fe/H] to within 0.35dex for FGKMstars, for extinctions below 1mag.
arxiv-1200-172 | Are You Imitating Me? Unsupervised Sparse Modeling for Group Activity Analysis from a Single Video | http://arxiv.org/pdf/1208.5451v1.pdf | author:Zhongwei Tang, Alexey Castrodad, Mariano Tepper, Guillermo Sapiro category:cs.CV 94A08 published:2012-08-27 summary:A framework for unsupervised group activity analysis from a single video ishere presented. Our working hypothesis is that human actions lie on a union oflow-dimensional subspaces, and thus can be efficiently modeled as sparse linearcombinations of atoms from a learned dictionary representing the action'sprimitives. Contrary to prior art, and with the primary goal of spatio-temporalaction grouping, in this work only one single video segment is available forboth unsupervised learning and analysis without any prior training information.After extracting simple features at a single spatio-temporal scale, we learn adictionary for each individual in the video during each short time lapse. Thesedictionaries allow us to compare the individuals' actions by producing anaffinity matrix which contains sufficient discriminative information about theactions in the scene leading to grouping with simple and efficient tools. Withdiverse publicly available real videos, we demonstrate the effectiveness of theproposed framework and its robustness to cluttered backgrounds, changes ofhuman appearance, and action variability.
arxiv-1200-173 | A Missing and Found Recognition System for Hajj and Umrah | http://arxiv.org/pdf/1208.5365v1.pdf | author:Salah A. Aly category:cs.CV cs.CY published:2012-08-27 summary:This note describes an integrated recognition system for identifying missingand found objects as well as missing, dead, and found people during Hajj andUmrah seasons in the two Holy cities of Makkah and Madina in the Kingdom ofSaudi Arabia. It is assumed that the total estimated number of pilgrims willreach 20 millions during the next decade. The ultimate goal of this system isto integrate facial recognition and object identification solutions into theHajj and Umrah rituals. The missing and found computerized system is part ofthe CrowdSensing system for Hajj and Umrah crowd estimation, management andsafety.
arxiv-1200-174 | Sensitive Ants in Solving the Generalized Vehicle Routing Problem | http://arxiv.org/pdf/1208.5341v1.pdf | author:Camelia-M. Pintea, Camelia Chira, D. Dumitrescu, Petrica C. Pop category:cs.AI cs.NE 68T20 published:2012-08-27 summary:The idea of sensitivity in ant colony systems has been exploited in hybridant-based models with promising results for many combinatorial optimizationproblems. Heterogeneity is induced in the ant population by endowing individualants with a certain level of sensitivity to the pheromone trail. The variablepheromone sensitivity within the same population of ants can potentiallyintensify the search while in the same time inducing diversity for theexploration of the environment. The performance of sensitive ant models isinvestigated for solving the generalized vehicle routing problem. Numericalresults and comparisons are discussed and analysed with a focus on emphasizingany particular aspects and potential benefits related to hybrid ant-basedmodels.
arxiv-1200-175 | New results of ant algorithms for the Linear Ordering Problem | http://arxiv.org/pdf/1208.5340v1.pdf | author:Camelia-M. Pintea, Camelia Chira, D. Dumitrescu category:cs.AI cs.NE 68T20 published:2012-08-27 summary:Ant-based algorithms are successful tools for solving complex problems. Oneof these problems is the Linear Ordering Problem (LOP). The paper shows newresults on some LOP instances, using Ant Colony System (ACS) and the Step-BackSensitive Ant Model (SB-SAM).
arxiv-1200-176 | Fast L1-Minimization Algorithms For Robust Face Recognition | http://arxiv.org/pdf/1007.3753v4.pdf | author:Allen Y. Yang, Zihan Zhou, Arvind Ganesh, S. Shankar Sastry, Yi Ma category:cs.CV cs.NA published:2010-07-21 summary:L1-minimization refers to finding the minimum L1-norm solution to anunderdetermined linear system b=Ax. Under certain conditions as described incompressive sensing theory, the minimum L1-norm solution is also the sparsestsolution. In this paper, our study addresses the speed and scalability of itsalgorithms. In particular, we focus on the numerical implementation of asparsity-based classification framework in robust face recognition, wheresparse representation is sought to recover human identities from veryhigh-dimensional facial images that may be corrupted by illumination, facialdisguise, and pose variation. Although the underlying numerical problem is alinear program, traditional algorithms are known to suffer poor scalability forlarge-scale applications. We investigate a new solution based on a classicalconvex optimization framework, known as Augmented Lagrangian Methods (ALM). Thenew convex solvers provide a viable solution to real-world, time-criticalapplications such as face recognition. We conduct extensive experiments tovalidate and compare the performance of the ALM algorithms against severalpopular L1-minimization solvers, including interior-point method, Homotopy,FISTA, SESOP-PCD, approximate message passing (AMP) and TFOCS. To aid peerevaluation, the code for all the algorithms has been made publicly available.
arxiv-1200-177 | Bayesian Network Structure Learning with Permutation Tests | http://arxiv.org/pdf/1101.5184v3.pdf | author:Marco Scutari, Adriana Brogini category:stat.ML stat.ME published:2011-01-27 summary:In literature there are several studies on the performance of Bayesiannetwork structure learning algorithms. The focus of these studies is almostalways the heuristics the learning algorithms are based on, i.e. themaximisation algorithms (in score-based algorithms) or the techniques forlearning the dependencies of each variable (in constraint-based algorithms). Inthis paper we investigate how the use of permutation tests instead ofparametric ones affects the performance of Bayesian network structure learningfrom discrete data. Shrinkage tests are also covered to provide a broadoverview of the techniques developed in current literature.
arxiv-1200-178 | New Analysis and Algorithm for Learning with Drifting Distributions | http://arxiv.org/pdf/1205.4343v2.pdf | author:Mehryar Mohri, Andres Munoz Medina category:cs.LG stat.ML published:2012-05-19 summary:We present a new analysis of the problem of learning with driftingdistributions in the batch setting using the notion of discrepancy. We provelearning bounds based on the Rademacher complexity of the hypothesis set andthe discrepancy of distributions both for a drifting PAC scenario and atracking scenario. Our bounds are always tighter and in some casessubstantially improve upon previous ones based on the $L_1$ distance. We alsopresent a generalization of the standard on-line to batch conversion to thedrifting scenario in terms of the discrepancy and arbitrary convex combinationsof hypotheses. We introduce a new algorithm exploiting these learningguarantees, which we show can be formulated as a simple QP. Finally, we reportthe results of preliminary experiments demonstrating the benefits of thisalgorithm.
arxiv-1200-179 | The proximal point method for a hybrid model in image restoration | http://arxiv.org/pdf/1110.1804v2.pdf | author:Zhi-Feng Pang, Li-Lian Wang, Yu-Fei Yang category:cs.CV cs.IT math.IT math.OC published:2011-10-09 summary:Models including two $L^1$ -norm terms have been widely used in imagerestoration. In this paper we first propose the alternating direction method ofmultipliers (ADMM) to solve this class of models. Based on ADMM, we thenpropose the proximal point method (PPM), which is more efficient than ADMM.Following the operator theory, we also give the convergence analysis of theproposed methods. Furthermore, we use the proposed methods to solve a class ofhybrid models combining the ROF model with the LLT model. Some numericalresults demonstrate the viability and efficiency of the proposed methods.
arxiv-1200-180 | Graph Degree Linkage: Agglomerative Clustering on a Directed Graph | http://arxiv.org/pdf/1208.5092v1.pdf | author:Wei Zhang, Xiaogang Wang, Deli Zhao, Xiaoou Tang category:cs.CV cs.SI stat.ML published:2012-08-25 summary:This paper proposes a simple but effective graph-based agglomerativealgorithm, for clustering high-dimensional data. We explore the different rolesof two fundamental concepts in graph theory, indegree and outdegree, in thecontext of clustering. The average indegree reflects the density near a sample,and the average outdegree characterizes the local geometry around a sample.Based on such insights, we define the affinity measure of clusters via theproduct of average indegree and average outdegree. The product-based affinitymakes our algorithm robust to noise. The algorithm has three main advantages:good performance, easy implementation, and high computational efficiency. Wetest the algorithm on two fundamental computer vision problems: imageclustering and object matching. Extensive experiments demonstrate that itoutperforms the state-of-the-arts in both applications.
arxiv-1200-181 | Multi-Task Averaging | http://arxiv.org/pdf/1107.4390v4.pdf | author:Sergey Feldman, Bela A. Frigyik, Maya R. Gupta category:stat.ML stat.ME published:2011-07-21 summary:We present a multi-task learning approach to jointly estimate the means ofmultiple independent data sets. The proposed multi-task averaging (MTA)algorithm results in a convex combination of the single-task maximum likelihoodestimates. We derive the optimal minimum risk estimator and the minimaxestimator, and show that these estimators can be efficiently estimated.Simulations and real data experiments demonstrate that MTA estimators oftenoutperform both single-task and James-Stein estimators.
arxiv-1200-182 | Non-Local Euclidean Medians | http://arxiv.org/pdf/1207.3056v2.pdf | author:Kunal N. Chaudhury, Amit Singer category:cs.CV cs.DS published:2012-07-12 summary:In this letter, we note that the denoising performance of Non-Local Means(NLM) at large noise levels can be improved by replacing the mean by theEuclidean median. We call this new denoising algorithm the Non-Local EuclideanMedians (NLEM). At the heart of NLEM is the observation that the median is morerobust to outliers than the mean. In particular, we provide a simple geometricinsight that explains why NLEM performs better than NLM in the vicinity ofedges, particularly at large noise levels. NLEM can be efficiently implementedusing iteratively reweighted least squares, and its computational complexity iscomparable to that of NLM. We provide some preliminary results to study theproposed algorithm and to compare it with NLM.
arxiv-1200-183 | WESD - Weighted Spectral Distance for Measuring Shape Dissimilarity | http://arxiv.org/pdf/1208.5016v1.pdf | author:Ender Konukoglu, Ben Glocker, Antonio Criminisi, Kilian M. Pohl category:cs.CV published:2012-08-24 summary:This article presents a new distance for measuring shape dissimilaritybetween objects. Recent publications introduced the use of eigenvalues of theLaplace operator as compact shape descriptors. Here, we revisit the eigenvaluesto define a proper distance, called Weighted Spectral Distance (WESD), forquantifying shape dissimilarity. The definition of WESD is derived throughanalysing the heat-trace. This analysis provides the proposed distance anintuitive meaning and mathematically links it to the intrinsic geometry ofobjects. We analyse the resulting distance definition, present and prove itsimportant theoretical properties. Some of these properties include: i) WESD isdefined over the entire sequence of eigenvalues yet it is guaranteed toconverge, ii) it is a pseudometric, iii) it is accurately approximated with afinite number of eigenvalues, and iv) it can be mapped to the [0,1) interval.Lastly, experiments conducted on synthetic and real objects are presented.These experiments highlight the practical benefits of WESD for applications invision and medical image analysis.
arxiv-1200-184 | The Mysterious Optimality of Naive Bayes: Estimation of the Probability in the System of "Classifiers" | http://arxiv.org/pdf/cs/0202020v3.pdf | author:Oleg Kupervasser, Alexsander Vardy category:cs.CV cs.AI published:2002-02-17 summary:Bayes Classifiers are widely used currently for recognition, identificationand knowledge discovery. The fields of application are, for example, imageprocessing, medicine, chemistry (QSAR). But by mysterious way the Naive BayesClassifier usually gives a very nice and good presentation of a recognition. Itcan not be improved considerably by more complex models of Bayes Classifier. Wedemonstrate here a very nice and simple proof of the Naive Bayes Classifieroptimality, that can explain this interesting fact.The derivation in thecurrent paper is based on arXiv:cs/0202020v1
arxiv-1200-185 | Ensemble Models with Trees and Rules | http://arxiv.org/pdf/1112.3699v8.pdf | author:Deniz Akdemir category:stat.ML published:2011-12-16 summary:In this article, we have proposed several approaches for post processing alarge ensemble of prediction models or rules. The results from our simulationsshow that the post processing methods we have considered here are promising. Wehave used the techniques developed here for estimation of quantitative traitsfrom markers, on the benchmark "Bostob Housing"data set and in somesimulations. In most cases, the produced models had better predictionperformance than, for example, the ones produced by the random forest or therulefit algorithms.
arxiv-1200-186 | The Segmentation Fusion Method On10 Multi-Sensors | http://arxiv.org/pdf/1208.4842v1.pdf | author:Firouz Abdullah Al-Wassai, N. V. Kalyankar category:cs.CV published:2012-08-23 summary:The most significant problem may be undesirable effects for the spectralsignatures of fused images as well as the benefits of using fused images mostlycompared to their source images were acquired at the same time by one sensor.They may or may not be suitable for the fusion of other images. It becomestherefore increasingly important to investigate techniques that allowmulti-sensor, multi-date image fusion to make final conclusions can be drawn onthe most suitable method of fusion. So, In this study we present a new methodSegmentation Fusion method (SF) for remotely sensed images is presented byconsidering the physical characteristics of sensors, which uses a feature levelprocessing paradigm. In a particularly, attempts to test the proposed methodperformance on 10 multi-sensor images and comparing it with different fusiontechniques for estimating the quality and degree of information improvementquantitatively by using various spatial and spectral metrics.
arxiv-1200-187 | Optimized Look-Ahead Tree Policies: A Bridge Between Look-Ahead Tree Policies and Direct Policy Search | http://arxiv.org/pdf/1208.4773v1.pdf | author:Tobias Jung, Louis Wehenkel, Damien Ernst, Francis Maes category:cs.SY cs.AI cs.LG published:2012-08-23 summary:Direct policy search (DPS) and look-ahead tree (LT) policies are two widelyused classes of techniques to produce high performance policies for sequentialdecision-making problems. To make DPS approaches work well, one crucial issueis to select an appropriate space of parameterized policies with respect to thetargeted problem. A fundamental issue in LT approaches is that, to take gooddecisions, such policies must develop very large look-ahead trees which mayrequire excessive online computational resources. In this paper, we propose anew hybrid policy learning scheme that lies at the intersection of DPS and LT,in which the policy is an algorithm that develops a small look-ahead tree in adirected way, guided by a node scoring function that is learned through DPS.The LT-based representation is shown to be a versatile way of representingpolicies in a DPS scheme, while at the same time, DPS enables to significantlyreduce the size of the look-ahead trees that are required to take high-qualitydecisions. We experimentally compare our method with two other state-of-the-art DPStechniques and four common LT policies on four benchmark domains and show thatit combines the advantages of the two techniques from which it originates. Inparticular, we show that our method: (1) produces overall better performingpolicies than both pure DPS and pure LT policies, (2) requires a substantiallysmaller number of policy evaluations than other DPS techniques, (3) is easy totune and (4) results in policies that are quite robust with respect toperturbations of the initial conditions.
arxiv-1200-188 | Introduction of the weight edition errors in the Levenshtein distance | http://arxiv.org/pdf/1208.4503v1.pdf | author:Gueddah Hicham category:cs.CL published:2012-08-22 summary:In this paper, we present a new approach dedicated to correcting the spellingerrors of the Arabic language. This approach corrects typographical errors likeinserting, deleting, and permutation. Our method is inspired from theLevenshtein algorithm, and allows a finer and better scheduling thanLevenshtein. The results obtained are very satisfactory and encouraging, whichshows the interest of our new approach.
arxiv-1200-189 | Submodular Functions: Learnability, Structure, and Optimization | http://arxiv.org/pdf/1008.2159v3.pdf | author:Maria-Florina Balcan, Nicholas J. A. Harvey category:cs.DS cs.DM cs.LG published:2010-08-12 summary:Submodular functions are discrete functions that model laws of diminishingreturns and enjoy numerous algorithmic applications. They have been used inmany areas, including combinatorial optimization, machine learning, andeconomics. In this work we study submodular functions from a learning theoreticangle. We provide algorithms for learning submodular functions, as well aslower bounds on their learnability. In doing so, we uncover several novelstructural results revealing ways in which submodular functions can be bothsurprisingly structured and surprisingly unstructured. We provide severalconcrete implications of our work in other domains including algorithmic gametheory and combinatorial optimization. At a technical level, this research combines ideas from many areas, includinglearning theory (distributional learning and PAC-style analyses), combinatoricsand optimization (matroids and submodular functions), and pseudorandomness(lossless expander graphs).
arxiv-1200-190 | A non-parametric mixture model for topic modeling over time | http://arxiv.org/pdf/1208.4411v1.pdf | author:Avinava Dubey, Ahmed Hefny, Sinead Williamson, Eric P. Xing category:stat.ML published:2012-08-22 summary:A single, stationary topic model such as latent Dirichlet allocation isinappropriate for modeling corpora that span long time periods, as thepopularity of topics is likely to change over time. A number of models thatincorporate time have been proposed, but in general they either exhibit limitedforms of temporal variation, or require computationally expensive inferencemethods. In this paper we propose non-parametric Topics over Time (npTOT), amodel for time-varying topics that allows an unbounded number of topics andexible distribution over the temporal variations in those topics' popularity.We develop a collapsed Gibbs sampler for the proposed model and compare againstexisting models on synthetic and real document sets.
arxiv-1200-191 | A Unified Approach for Modeling and Recognition of Individual Actions and Group Activities | http://arxiv.org/pdf/1208.4398v1.pdf | author:Qiang Qiu, Rama Chellappa category:cs.CV stat.ML published:2012-08-21 summary:Recognizing group activities is challenging due to the difficulties inisolating individual entities, finding the respective roles played by theindividuals and representing the complex interactions among the participants.Individual actions and group activities in videos can be represented in acommon framework as they share the following common feature: both are composedof a set of low-level features describing motions, e.g., optical flow for eachpixel or a trajectory for each feature point, according to a set of compositionconstraints in both temporal and spatial dimensions. In this paper, we presenta unified model to assess the similarity between two given individual or groupactivities. Our approach avoids explicit extraction of individual actors,identifying and representing the inter-person interactions. With the proposedapproach, retrieval from a video database can be performed throughQuery-by-Example; and activities can be recognized by querying videoscontaining known activities. The suggested video matching process can beperformed in an unsupervised manner. We demonstrate the performance of ourapproach by recognizing a set of human actions and football plays.
arxiv-1200-192 | An Online Character Recognition System to Convert Grantha Script to Malayalam | http://arxiv.org/pdf/1208.4316v1.pdf | author:Sreeraj. M, Sumam Mary Idicula category:cs.CV published:2012-08-21 summary:This paper presents a novel approach to recognize Grantha, an ancient scriptin South India and converting it to Malayalam, a prevalent language in SouthIndia using online character recognition mechanism. The motivation behind thiswork owes its credit to (i) developing a mechanism to recognize Grantha scriptin this modern world and (ii) affirming the strong connection among Grantha andMalayalam. A framework for the recognition of Grantha script using onlinecharacter recognition is designed and implemented. The features extracted fromthe Grantha script comprises mainly of time-domain features based on writingdirection and curvature. The recognized characters are mapped to correspondingMalayalam characters. The framework was tested on a bed of medium lengthmanuscripts containing 9-12 sample lines and printed pages of a book titledSoundarya Lahari writtenin Grantha by Sri Adi Shankara to recognize the wordsand sentences. The manuscript recognition rates with the system are for Granthaas 92.11%, Old Malayalam 90.82% and for new Malayalam script 89.56%. Therecognition rates of pages of the printed book are for Grantha as 96.16%, OldMalayalam script 95.22% and new Malayalam script as 92.32% respectively. Theseresults show the efficiency of the developed system.
arxiv-1200-193 | Learning LiNGAM based on data with more variables than observations | http://arxiv.org/pdf/1208.4183v1.pdf | author:Shohei Shimizu category:stat.ML published:2012-08-21 summary:A very important topic in systems biology is developing statistical methodsthat automatically find causal relations in gene regulatory networks with noprior knowledge of causal connectivity. Many methods have been developed fortime series data. However, discovery methods based on steady-state data areoften necessary and preferable since obtaining time series data can be moreexpensive and/or infeasible for many biological systems. A conventionalapproach is causal Bayesian networks. However, estimation of Bayesian networksis ill-posed. In many cases it cannot uniquely identify the underlying causalnetwork and only gives a large class of equivalent causal networks that cannotbe distinguished between based on the data distribution. We propose a newdiscovery algorithm for uniquely identifying the underlying causal network ofgenes. To the best of our knowledge, the proposed method is the first algorithmfor learning gene networks based on a fully identifiable causal model calledLiNGAM. We here compare our algorithm with competing algorithms usingartificially-generated data, although it is definitely better to test it basedon real microarray gene expression data.
arxiv-1200-194 | Semi-supervised Clustering Ensemble by Voting | http://arxiv.org/pdf/1208.4138v1.pdf | author:Ashraf Mohammed Iqbal, Abidalrahman Moh'd, Zahoor Khan category:cs.LG stat.ML published:2012-08-20 summary:Clustering ensemble is one of the most recent advances in unsupervisedlearning. It aims to combine the clustering results obtained using differentalgorithms or from different runs of the same clustering algorithm for the samedata set, this is accomplished using on a consensus function, the efficiencyand accuracy of this method has been proven in many works in literature. In thefirst part of this paper we make a comparison among current approaches toclustering ensemble in literature. All of these approaches consist of two mainsteps: the ensemble generation and consensus function. In the second part ofthe paper, we suggest engaging supervision in the clustering ensemble procedureto get more enhancements on the clustering results. Supervision can be appliedin two places: either by using semi-supervised algorithms in the clusteringensemble generation step or in the form of a feedback used by the consensusfunction stage. Also, we introduce a flexible two parameter weightingmechanism, the first parameter describes the compatibility between the datasetsunder study and the semi-supervised clustering algorithms used to generate thebase partitions, the second parameter is used to provide the user feedback onthe these partitions. The two parameters are engaged in a "relabeling andvoting" based consensus function to produce the final clustering.
arxiv-1200-195 | Recent Technological Advances in Natural Language Processing and Artificial Intelligence | http://arxiv.org/pdf/1208.4079v1.pdf | author:Nishal Pradeepkumar Shah category:cs.CL I.2.7 published:2012-08-20 summary:A recent advance in computer technology has permitted scientists to implementand test algorithms that were known from quite some time (or not) but whichwere computationally expensive. Two such projects are IBM's Jeopardy as a partof its DeepQA project [1] and Wolfram's Wolframalpha[2]. Both these methodsimplement natural language processing (another goal of AI scientists) and tryto answer questions as asked by the user. Though the goal of the two projectsis similar, both of them have a different procedure at it's core. In thefollowing sections, the mechanism and history of IBM's Jeopardy and Wolframalpha has been explained followed by the implications of these projects inrealizing Ray Kurzweil's [3] dream of passing the Turing test by 2029. A recipeof taking the above projects to a new level is also explained.
arxiv-1200-196 | Learning sparse messages in networks of neural cliques | http://arxiv.org/pdf/1208.4009v1.pdf | author:Behrooz Kamary Aliabadi, Claude Berrou, Vincent Gripon, Xiaoran Jiang category:cs.NE published:2012-08-20 summary:An extension to a recently introduced binary neural network is proposed inorder to allow the learning of sparse messages, in large numbers and with highmemory efficiency. This new network is justified both in biological andinformational terms. The learning and retrieval rules are detailed andillustrated by various simulation results.
arxiv-1200-197 | Performance Tuning Of J48 Algorithm For Prediction Of Soil Fertility | http://arxiv.org/pdf/1208.3943v1.pdf | author:Jay Gholap category:cs.LG cs.DB cs.PF stat.ML published:2012-08-20 summary:Data mining involves the systematic analysis of large data sets, and datamining in agricultural soil datasets is exciting and modern research area. Theproductive capacity of a soil depends on soil fertility. Achieving andmaintaining appropriate levels of soil fertility, is of utmost importance ifagricultural land is to remain capable of nourishing crop production. In thisresearch, Steps for building a predictive model of soil fertility have beenexplained. This paper aims at predicting soil fertility class using decision treealgorithms in data mining . Further, it focuses on performance tuning of J48decision tree algorithm with the help of meta-techniques such as attributeselection and boosting.
arxiv-1200-198 | High-dimensional structure estimation in Ising models: Local separation criterion | http://arxiv.org/pdf/1107.1736v4.pdf | author:Animashree Anandkumar, Vincent Y. F. Tan, Furong Huang, Alan S. Willsky category:stat.ML cs.LG math.ST stat.TH published:2011-07-08 summary:We consider the problem of high-dimensional Ising (graphical) modelselection. We propose a simple algorithm for structure estimation based on thethresholding of the empirical conditional variation distances. We introduce anovel criterion for tractable graph families, where this method is efficient,based on the presence of sparse local separators between node pairs in theunderlying graph. For such graphs, the proposed algorithm has a samplecomplexity of $n=\Omega(J_{\min}^{-2}\log p)$, where $p$ is the number ofvariables, and $J_{\min}$ is the minimum (absolute) edge potential in themodel. We also establish nonasymptotic necessary and sufficient conditions forstructure estimation.
arxiv-1200-199 | Input Scheme for Hindi Using Phonetic Mapping | http://arxiv.org/pdf/1209.1300v1.pdf | author:Nisheeth Joshi, Iti Mathur category:cs.CL published:2012-08-19 summary:Written Communication on Computers requires knowledge of writing text for thedesired language using Computer. Mostly people do not use any other languagebesides English. This creates a barrier. To resolve this issue we havedeveloped a scheme to input text in Hindi using phonetic mapping scheme. Usingthis scheme we generate intermediate code strings and match them withpronunciations of input text. Our system show significant success over otherinput systems available.
arxiv-1200-200 | Evaluation of Computational Grammar Formalisms for Indian Languages | http://arxiv.org/pdf/1209.1301v1.pdf | author:Nisheeth Joshi, Iti Mathur category:cs.CL published:2012-08-19 summary:Natural Language Parsing has been the most prominent research area since thegenesis of Natural Language Processing. Probabilistic Parsers are beingdeveloped to make the process of parser development much easier, accurate andfast. In Indian context, identification of which Computational GrammarFormalism is to be used is still a question which needs to be answered. In thispaper we focus on this problem and try to analyze different formalisms forIndian languages.
arxiv-1200-201 | A practical approach to language complexity: a Wikipedia case study | http://arxiv.org/pdf/1204.2765v2.pdf | author:Taha Yasseri, András Kornai, János Kertész category:cs.CL physics.soc-ph published:2012-04-12 summary:In this paper we present statistical analysis of English texts fromWikipedia. We try to address the issue of language complexity empirically bycomparing the simple English Wikipedia (Simple) to comparable samples of themain English Wikipedia (Main). Simple is supposed to use a more simplifiedlanguage with a limited vocabulary, and editors are explicitly requested tofollow this guideline, yet in practice the vocabulary richness of both samplesare at the same level. Detailed analysis of longer units (n-grams of words andpart of speech tags) shows that the language of Simple is less complex thanthat of Main primarily due to the use of shorter sentences, as opposed todrastically simplified syntax or vocabulary. Comparing the two languagevarieties by the Gunning readability index supports this conclusion. We alsoreport on the topical dependence of language complexity, e.g. that the languageis more advanced in conceptual articles compared to person-based (biographical)and object-based articles. Finally, we investigate the relation betweenconflict and language complexity by analyzing the content of the talk pagesassociated to controversial and peacefully developing articles, concluding thatcontroversy has the effect of reducing language complexity.
arxiv-1200-202 | Unsupervised Discovery of Mid-Level Discriminative Patches | http://arxiv.org/pdf/1205.3137v2.pdf | author:Saurabh Singh, Abhinav Gupta, Alexei A. Efros category:cs.CV cs.AI cs.LG published:2012-05-14 summary:The goal of this paper is to discover a set of discriminative patches whichcan serve as a fully unsupervised mid-level visual representation. The desiredpatches need to satisfy two requirements: 1) to be representative, they need tooccur frequently enough in the visual world; 2) to be discriminative, they needto be different enough from the rest of the visual world. The patches couldcorrespond to parts, objects, "visual phrases", etc. but are not restricted tobe any one of them. We pose this as an unsupervised discriminative clusteringproblem on a huge dataset of image patches. We use an iterative procedure whichalternates between clustering and training discriminative classifiers, whileapplying careful cross-validation at each step to prevent overfitting. Thepaper experimentally demonstrates the effectiveness of discriminative patchesas an unsupervised mid-level visual representation, suggesting that it could beused in place of visual words for many tasks. Furthermore, discriminativepatches can also be used in a supervised regime, such as scene classification,where they demonstrate state-of-the-art performance on the MIT Indoor-67dataset.
arxiv-1200-203 | Image Super-Resolution via Dual-Dictionary Learning And Sparse Representation | http://arxiv.org/pdf/1208.3723v1.pdf | author:Jian Zhang, Chen Zhao, Ruiqin Xiong, Siwei Ma, Debin Zhao category:cs.CV published:2012-08-18 summary:Learning-based image super-resolution aims to reconstruct high-frequency (HF)details from the prior model trained by a set of high- and low-resolution imagepatches. In this paper, HF to be estimated is considered as a combination oftwo components: main high-frequency (MHF) and residual high-frequency (RHF),and we propose a novel image super-resolution method via dual-dictionarylearning and sparse representation, which consists of the main dictionarylearning and the residual dictionary learning, to recover MHF and RHFrespectively. Extensive experimental results on test images validate that byemploying the proposed two-layer progressive scheme, more image details can berecovered and much better results can be achieved than the state-of-the-artalgorithms in terms of both PSNR and visual perception.
arxiv-1200-204 | An improvement direction for filter selection techniques using information theory measures and quadratic optimization | http://arxiv.org/pdf/1208.3689v1.pdf | author:Waad Bouaguel, Ghazi Bel Mufti category:cs.LG cs.IT math.IT published:2012-08-17 summary:Filter selection techniques are known for their simplicity and efficiency.However this kind of methods doesn't take into consideration the featuresinter-redundancy. Consequently the un-removed redundant features remain in thefinal classification model, giving lower generalization performance. In thispaper we propose to use a mathematical optimization method that reducesinter-features redundancy and maximize relevance between each feature and thetarget variable.
arxiv-1200-205 | Information-theoretic Dictionary Learning for Image Classification | http://arxiv.org/pdf/1208.3687v1.pdf | author:Qiang Qiu, Vishal M. Patel, Rama Chellappa category:cs.CV cs.IT math.IT stat.ML published:2012-08-17 summary:We present a two-stage approach for learning dictionaries for objectclassification tasks based on the principle of information maximization. Theproposed method seeks a dictionary that is compact, discriminative, andgenerative. In the first stage, dictionary atoms are selected from an initialdictionary by maximizing the mutual information measure on dictionarycompactness, discrimination and reconstruction. In the second stage, theselected dictionary atoms are updated for improved reconstructive anddiscriminative power using a simple gradient ascent algorithm on mutualinformation. Experiments using real datasets demonstrate the effectiveness ofour approach for image classification tasks.
arxiv-1200-206 | Modeling and Control of CSTR using Model based Neural Network Predictive Control | http://arxiv.org/pdf/1208.3600v1.pdf | author:Piyush Shrivastava category:cs.AI cs.NE nlin.AO published:2012-08-17 summary:This paper presents a predictive control strategy based on neural networkmodel of the plant is applied to Continuous Stirred Tank Reactor (CSTR). Thissystem is a highly nonlinear process; therefore, a nonlinear predictive method,e.g., neural network predictive control, can be a better match to govern thesystem dynamics. In the paper, the NN model and the way in which it can be usedto predict the behavior of the CSTR process over a certain prediction horizonare described, and some comments about the optimization procedure are made.Predictive control algorithm is applied to control the concentration in acontinuous stirred tank reactor (CSTR), whose parameters are optimallydetermined by solving quadratic performance index using the optimizationalgorithm. An efficient control of the product concentration in cstr can beachieved only through accurate model. Here an attempt is made to alleviate themodeling difficulties using Artificial Intelligent technique such as NeuralNetwork. Simulation results demonstrate the feasibility and effectiveness ofthe NNMPC technique.
arxiv-1200-207 | Leveraging Subjective Human Annotation for Clustering Historic Newspaper Articles | http://arxiv.org/pdf/1208.3530v1.pdf | author:Haimonti Dutta, William Chan, Deepak Shankargouda, Manoj Pooleery, Axinia Radeva, Kyle Rego, Boyi Xie, Rebecca Passonneau, Austin Lee, Barbara Taranto category:cs.IR cs.CL cs.DL published:2012-08-17 summary:The New York Public Library is participating in the Chronicling Americainitiative to develop an online searchable database of historically significantnewspaper articles. Microfilm copies of the newspapers are scanned and highresolution Optical Character Recognition (OCR) software is run on them. Thetext from the OCR provides a wealth of data and opinion for researchers andhistorians. However, categorization of articles provided by the OCR engine isrudimentary and a large number of the articles are labeled editorial withoutfurther grouping. Manually sorting articles into fine-grained categories istime consuming if not impossible given the size of the corpus. This paperstudies techniques for automatic categorization of newspaper articles so as toenhance search and retrieval on the archive. We explore unsupervised (e.g.KMeans) and semi-supervised (e.g. constrained clustering) learning algorithmsto develop article categorization schemes geared towards the needs ofend-users. A pilot study was designed to understand whether there was unanimousagreement amongst patrons regarding how articles can be categorized. It wasfound that the task was very subjective and consequently automated algorithmsthat could deal with subjective labels were used. While the small scale pilotstudy was extremely helpful in designing machine learning algorithms, a muchlarger system needs to be developed to collect annotations from users of thearchive. The "BODHI" system currently being developed is a step in thatdirection, allowing users to correct wrongly scanned OCR and providing keywordsand tags for newspaper articles used frequently. On successful implementationof the beta version of this system, we hope that it can be integrated withexisting software being developed for the Chronicling America project.
arxiv-1200-208 | Bayesian and L1 Approaches to Sparse Unsupervised Learning | http://arxiv.org/pdf/1106.1157v3.pdf | author:Shakir Mohamed, Katherine Heller, Zoubin Ghahramani category:cs.LG cs.AI stat.ML published:2011-06-06 summary:The use of L1 regularisation for sparse learning has generated immenseresearch interest, with successful application in such diverse areas as signalacquisition, image coding, genomics and collaborative filtering. While existingwork highlights the many advantages of L1 methods, in this paper we find thatL1 regularisation often dramatically underperforms in terms of predictiveperformance when compared with other methods for inferring sparsity. We focuson unsupervised latent variable models, and develop L1 minimising factormodels, Bayesian variants of "L1", and Bayesian models with a stronger L0-likesparsity induced through spike-and-slab distributions. These spike-and-slabBayesian factor models encourage sparsity while accounting for uncertainty in aprincipled manner and avoiding unnecessary shrinkage of non-zero values. Wedemonstrate on a number of data sets that in practice spike-and-slab Bayesianmethods outperform L1 minimisation, even on a computational budget. We thushighlight the need to re-assess the wide use of L1 methods in sparsity-reliantapplications, particularly when we care about generalising to previously unseendata, and provide an alternative that, over many varying conditions, providesimproved generalisation performance.
arxiv-1200-209 | Contour Completion Around a Fixation Point | http://arxiv.org/pdf/1208.3512v1.pdf | author:Toshiro Kubota category:cs.CV published:2012-08-16 summary:The paper presents two edge grouping algorithms for finding a closed contourstarting from a particular edge point and enclosing a fixation point. Bothalgorithms search a shortest simple cycle in \textit{an angularly orderedgraph} derived from an edge image where a vertex is an end point of a contourfragment and an undirected arc is drawn between a pair of end-points whosevisual angle from the fixation point is less than a threshold value, which isset to $\pi/2$ in our experiments. The first algorithm restricts the searchspace by disregarding arcs that cross the line extending from the fixationpoint to the starting point. The second algorithm improves the solution of thefirst algorithm in a greedy manner. The algorithms were tested with a largenumber of natural images with manually placed fixation and starting points. Theresults are promising.
arxiv-1200-210 | Automated Marble Plate Classification System Based On Different Neural Network Input Training Sets and PLC Implementation | http://arxiv.org/pdf/1208.6310v1.pdf | author:Irina Topalova category:cs.NE cs.LG published:2012-08-16 summary:The process of sorting marble plates according to their surface texture is animportant task in the automated marble plate production. Nowadays someinspection systems in marble industry that automate the classification tasksare too expensive and are compatible only with specific technological equipmentin the plant. In this paper a new approach to the design of an Automated MarblePlate Classification System (AMPCS),based on different neural network inputtraining sets is proposed, aiming at high classification accuracy using simpleprocessing and application of only standard devices. It is based on training aclassification MLP neural network with three different input training sets:extracted texture histograms, Discrete Cosine and Wavelet Transform over thehistograms. The algorithm is implemented in a PLC for real-time operation. Theperformance of the system is assessed with each one of the input training sets.The experimental test results regarding classification accuracy and quickoperation are represented and discussed.
arxiv-1200-211 | Color Image Compression Algorithm Based on the DCT Blocks | http://arxiv.org/pdf/1208.3133v1.pdf | author:Walaa M. Abd-Elhafiez, Wajeb Gharibi category:cs.CV published:2012-08-15 summary:This paper presents the performance of different blockbased discrete cosinetransform (DCT) algorithms for compressing color image. In this RGB componentof color image are converted to YCbCr before DCT transform is applied. Y isluminance component;Cb and Cr are chrominance components of the image. Themodification of the image data is done based on the classification of imageblocks to edge blocks and non-edge blocks, then the edge block of the image iscompressed with low compression and the nonedge blocks is compressed with highcompression. The analysis results have indicated that the performance of thesuggested method is much better, where the constructed images are lessdistorted and compressed with higher factor.
arxiv-1200-212 | Parallelization of Maximum Entropy POS Tagging for Bahasa Indonesia with MapReduce | http://arxiv.org/pdf/1208.3047v1.pdf | author:Arif Nurwidyantoro, Edi Winarko category:cs.DC cs.CL published:2012-08-15 summary:In this paper, MapReduce programming model is used to parallelize trainingand tagging proceess in Maximum Entropy part of speech tagging for BahasaIndonesia. In training process, MapReduce model is implemented dictionary,tagtoken, and feature creation. In tagging process, MapReduce is implemented totag lines of document in parallel. The training experiments showed that totaltraining time using MapReduce is faster, but its result reading time inside theprocess slow down the total training time. The tagging experiments usingdifferent number of map and reduce process showed that MapReduce implementationcould speedup the tagging process. The fastest tagging result is showed bytagging process using 1,000,000 word corpus and 30 map process.
arxiv-1200-213 | Performance Analysis Of Neuro Genetic Algorithm Applied On Detecting Proportion Of Components In Manhole Gas Mixture | http://arxiv.org/pdf/1209.1048v1.pdf | author:Varun Kumar Ojha, Paramartha Dutta, Hiranmay Saha category:cs.NE cs.CV published:2012-08-15 summary:The article presents performance analysis of a real valued neuro geneticalgorithm applied for the detection of proportion of the gases found in manholegas mixture. The neural network (NN) trained using genetic algorithm (GA) leadsto concept of neuro genetic algorithm, which is used for implementing anintelligent sensory system for the detection of component gases present inmanhole gas mixture Usually a manhole gas mixture contains several toxic gaseslike Hydrogen Sulfide, Ammonia, Methane, Carbon Dioxide, Nitrogen Oxide, andCarbon Monoxide. A semiconductor based gas sensor array used for sensingmanhole gas components is an integral part of the proposed intelligent system.It consists of many sensor elements, where each sensor element is responsiblefor sensing particular gas component. Multiple sensors of different gases usedfor detecting gas mixture of multiple gases, results in cross-sensitivity. Thecross-sensitivity is a major issue and the problem is viewed as patternrecognition problem. The objective of this article is to present performanceanalysis of the real valued neuro genetic algorithm which is applied formultiple gas detection.
arxiv-1200-214 | Efficient Algorithm for Extremely Large Multi-task Regression with Massive Structured Sparsity | http://arxiv.org/pdf/1208.3014v1.pdf | author:Seunghak Lee, Eric P. Xing category:stat.ML q-bio.QM published:2012-08-15 summary:We develop a highly scalable optimization method called "hierarchicalgroup-thresholding" for solving a multi-task regression model with complexstructured sparsity constraints on both input and output spaces. Despite therecent emergence of several efficient optimization algorithms for tacklingcomplex sparsity-inducing regularizers, true scalability in practicalhigh-dimensional problems where a huge amount (e.g., millions) of sparsitypatterns need to be enforced remains an open challenge, because all existingalgorithms must deal with ALL such patterns exhaustively in every iteration,which is computationally prohibitive. Our proposed algorithm addresses thescalability problem by screening out multiple groups of coefficientssimultaneously and systematically. We employ a hierarchical tree representationof group constraints to accelerate the process of removing irrelevantconstraints by taking advantage of the inclusion relationships between groupsparsities, thereby avoiding dealing with all constraints in every optimizationstep, and necessitating optimization operation only on a small number ofoutstanding coefficients. In our experiments, we demonstrate the efficiency ofour method on simulation datasets, and in an application of detecting geneticvariants associated with gene expression traits.
arxiv-1200-215 | More than Word Frequencies: Authorship Attribution via Natural Frequency Zoned Word Distribution Analysis | http://arxiv.org/pdf/1208.3001v1.pdf | author:Zhili Chen, Liusheng Huang, Wei Yang, Peng Meng, Haibo Miao category:cs.CL published:2012-08-15 summary:With such increasing popularity and availability of digital text data,authorships of digital texts can not be taken for granted due to the ease ofcopying and parsing. This paper presents a new text style analysis callednatural frequency zoned word distribution analysis (NFZ-WDA), and then a basicauthorship attribution scheme and an open authorship attribution scheme fordigital texts based on the analysis. NFZ-WDA is based on the observation thatall authors leave distinct intrinsic word usage traces on texts written by themand these intrinsic styles can be identified and employed to analyze theauthorship. The intrinsic word usage styles can be estimated through theanalysis of word distribution within a text, which is more than normal wordfrequency analysis and can be expressed as: which groups of words are used inthe text; how frequently does each group of words occur; how are theoccurrences of each group of words distributed in the text. Next, the basicauthorship attribution scheme and the open authorship attribution schemeprovide solutions for both closed and open authorship attribution problems.Through analysis and extensive experimental studies, this paper demonstratesthe efficiency of the proposed method for authorship attribution.
arxiv-1200-216 | Using Program Synthesis for Social Recommendations | http://arxiv.org/pdf/1208.2925v1.pdf | author:Alvin Cheung, Armando Solar-Lezama, Samuel Madden category:cs.LG cs.DB cs.PL cs.SI physics.soc-ph published:2012-08-14 summary:This paper presents a new approach to select events of interest to a user ina social media setting where events are generated by the activities of theuser's friends through their mobile devices. We argue that given the uniquerequirements of the social media setting, the problem is best viewed as aninductive learning problem, where the goal is to first generalize from theusers' expressed "likes" and "dislikes" of specific events, then to produce aprogram that can be manipulated by the system and distributed to the collectiondevices to collect only data of interest. The key contribution of this paper isa new algorithm that combines existing machine learning techniques with newprogram synthesis technology to learn users' preferences. We show that whencompared with the more standard approaches, our new algorithm provides up toorder-of-magnitude reductions in model training time, and significantly higherprediction accuracies for our target application. The approach also improves onstandard machine learning techniques in that it produces clear programs thatcan be manipulated to optimize data collection and filtering.
arxiv-1200-217 | Predictive Information Rate in Discrete-time Gaussian Processes | http://arxiv.org/pdf/1206.0304v2.pdf | author:Samer A. Abdallah, Mark D. Plumbley category:stat.ML math.ST stat.TH published:2012-06-01 summary:We derive expressions for the predicitive information rate (PIR) for theclass of autoregressive Gaussian processes AR(N), both in terms of theprediction coefficients and in terms of the power spectral density. The latterresult suggests a duality between the PIR and the multi-information rate forprocesses with mutually inverse power spectra (i.e. with poles and zeros of thetransfer function exchanged). We investigate the behaviour of the PIR inrelation to the multi-information rate for some simple examples, which suggest,somewhat counter-intuitively, that the PIR is maximised for very `smooth' ARprocesses whose power spectra have multiple poles at zero frequency. We alsoobtain results for moving average Gaussian processes which are consistent withthe duality conjectured earlier. One consequence of this is that the PIR isunbounded for MA(N) processes.
arxiv-1200-218 | Metric distances derived from cosine similarity and Pearson and Spearman correlations | http://arxiv.org/pdf/1208.3145v1.pdf | author:Stijn van Dongen, Anton J. Enright category:stat.ME cs.LG published:2012-08-14 summary:We investigate two classes of transformations of cosine similarity andPearson and Spearman correlations into metric distances, utilising the simpletool of metric-preserving functions. The first class puts anti-correlatedobjects maximally far apart. Previously known transforms fall within thisclass. The second class collates correlated and anti-correlated objects. Anexample of such a transformation that yields a metric distance is the sinefunction when applied to centered data.
arxiv-1200-219 | Analysis of a Statistical Hypothesis Based Learning Mechanism for Faster crawling | http://arxiv.org/pdf/1208.2808v1.pdf | author:Sudarshan Nandy, Partha Pratim Sarkar, Achintya Das category:cs.LG cs.IR published:2012-08-14 summary:The growth of world-wide-web (WWW) spreads its wings from an intangiblequantities of web-pages to a gigantic hub of web information which graduallyincreases the complexity of crawling process in a search engine. A searchengine handles a lot of queries from various parts of this world, and theanswers of it solely depend on the knowledge that it gathers by means ofcrawling. The information sharing becomes a most common habit of the society,and it is done by means of publishing structured, semi-structured andunstructured resources on the web. This social practice leads to an exponentialgrowth of web-resource, and hence it became essential to crawl for continuousupdating of web-knowledge and modification of several existing resources in anysituation. In this paper one statistical hypothesis based learning mechanism isincorporated for learning the behavior of crawling speed in differentenvironment of network, and for intelligently control of the speed of crawler.The scaling technique is used to compare the performance proposed method withthe standard crawler. The high speed performance is observed after scaling, andthe retrieval of relevant web-resource in such a high speed is analyzed.
arxiv-1200-220 | A Method for Selecting Noun Sense using Co-occurrence Relation in English-Korean Translation | http://arxiv.org/pdf/1208.2777v1.pdf | author:Hyonil Kim, Changil Choe category:cs.CL published:2012-08-14 summary:The sense analysis is still critical problem in machine translation system,especially such as English-Korean translation which the syntactical differentbetween source and target languages is very great. We suggest a method forselecting the noun sense using contextual feature in English-KoreanTranslation.
arxiv-1200-221 | Detecting Events and Patterns in Large-Scale User Generated Textual Streams with Statistical Learning Methods | http://arxiv.org/pdf/1208.2873v1.pdf | author:Vasileios Lampos category:cs.LG cs.CL cs.IR cs.SI stat.AP stat.ML published:2012-08-13 summary:A vast amount of textual web streams is influenced by events or phenomenaemerging in the real world. The social web forms an excellent modern paradigm,where unstructured user generated content is published on a regular basis andin most occasions is freely distributed. The present Ph.D. Thesis deals withthe problem of inferring information - or patterns in general - about eventsemerging in real life based on the contents of this textual stream. We showthat it is possible to extract valuable information about social phenomena,such as an epidemic or even rainfall rates, by automatic analysis of thecontent published in Social Media, and in particular Twitter, using StatisticalMachine Learning methods. An important intermediate task regards the formationand identification of features which characterise a target event; we select anduse those textual features in several linear, non-linear and hybrid inferenceapproaches achieving a significantly good performance in terms of the appliedloss function. By examining further this rich data set, we also propose methodsfor extracting various types of mood signals revealing how affective norms - atleast within the social web's population - evolve during the day and howsignificant events emerging in the real world are influencing them. Lastly, wepresent some preliminary findings showing several spatiotemporalcharacteristics of this textual information as well as the potential of usingit to tackle tasks such as the prediction of voting intentions.
arxiv-1200-222 | Stable Segmentation of Digital Image | http://arxiv.org/pdf/1208.2655v1.pdf | author:M. Kharinov category:cs.CV published:2012-08-13 summary:In the paper the optimal image segmentation by means of piecewise constantapproximations is considered. The optimality is defined by a minimum value ofthe total squared error or by equivalent value of standard deviation of theapproximation from the image. The optimal approximations are definedindependently on the method of their obtaining and might be generated indifferent algorithms. We investigate the computation of the optimalapproximation on the grounds of stability with respect to a given set ofmodifications. To obtain the optimal approximation the Mumford-Shuh model isgeneralized and developed, which in the computational part is combined with theOtsu method in multi-thresholding version. The proposed solution is provedanalytically and experimentally on the example of the standard image.
arxiv-1200-223 | A Plea for Neutral Comparison Studies in Computational Sciences | http://arxiv.org/pdf/1208.2651v1.pdf | author:Anne-Laure Boulesteix, Manuel J. A. Eugster category:stat.CO cs.CV stat.ME stat.ML published:2012-08-13 summary:In a context where most published articles are devoted to the development of"new methods", comparison studies are generally appreciated by readers butsurprisingly given poor consideration by many scientific journals. Inconnection with recent articles on over-optimism and epistemology published inBioinformatics, this letter stresses the importance of neutral comparisonstudies for the objective evaluation of existing methods and the establishmentof standards by drawing parallels with clinical research.
arxiv-1200-224 | Multi-Instance Learning with Any Hypothesis Class | http://arxiv.org/pdf/1107.2021v3.pdf | author:Sivan Sabato, Naftali Tishby category:cs.LG stat.ML published:2011-07-11 summary:In the supervised learning setting termed Multiple-Instance Learning (MIL),the examples are bags of instances, and the bag label is a function of thelabels of its instances. Typically, this function is the Boolean OR. Thelearner observes a sample of bags and the bag labels, but not the instancelabels that determine the bag labels. The learner is then required to emit aclassification rule for bags based on the sample. MIL has numerousapplications, and many heuristic algorithms have been used successfully on thisproblem, each adapted to specific settings or applications. In this work weprovide a unified theoretical analysis for MIL, which holds for any underlyinghypothesis class, regardless of a specific application or problem domain. Weshow that the sample complexity of MIL is only poly-logarithmically dependenton the size of the bag, for any underlying hypothesis class. In addition, weintroduce a new PAC-learning algorithm for MIL, which uses a regular supervisedlearning algorithm as an oracle. We prove that efficient PAC-learning for MILcan be generated from any efficient non-MIL supervised learning algorithm thathandles one-sided error. The computational complexity of the resultingalgorithm is only polynomially dependent on the bag size.
arxiv-1200-225 | Nonparametric sparsity and regularization | http://arxiv.org/pdf/1208.2572v1.pdf | author:Lorenzo Rosasco, Silvia Villa, Sofia Mosci, Matteo Santoro, Alessandro verri category:stat.ML cs.LG math.OC published:2012-08-13 summary:In this work we are interested in the problems of supervised learning andvariable selection when the input-output dependence is described by a nonlinearfunction depending on a few variables. Our goal is to consider a sparsenonparametric model, hence avoiding linear or additive models. The key idea isto measure the importance of each variable in the model by making use ofpartial derivatives. Based on this intuition we propose a new notion ofnonparametric sparsity and a corresponding least squares regularization scheme.Using concepts and results from the theory of reproducing kernel Hilbert spacesand proximal methods, we show that the proposed learning algorithm correspondsto a minimization problem which can be provably solved by an iterativeprocedure. The consistency properties of the obtained estimator are studiedboth in terms of prediction and selection performance. An extensive empiricalanalysis shows that the proposed method performs favorably with respect to thestate-of-the-art methods.
arxiv-1200-226 | Design of Low Noise Amplifiers Using Particle Swarm Optimization | http://arxiv.org/pdf/1208.6028v1.pdf | author:Sadik Ulker category:cs.NE published:2012-08-13 summary:This short paper presents a work on the design of low noise microwaveamplifiers using particle swarm optimization (PSO) technique. Particle SwarmOptimization is used as a method that is applied to a single stage amplifiercircuit to meet two criteria: desired gain and desired low noise. The aim is toget the best optimized design using the predefined constraints for gain and lownoise values. The code is written to apply the algorithm to meet the desiredgoals and the obtained results are verified using different simulators. Theresults obtained show that PSO can be applied very efficiently for this kind ofdesign problems with multiple constraints.
arxiv-1200-227 | Path Integral Control by Reproducing Kernel Hilbert Space Embedding | http://arxiv.org/pdf/1208.2523v1.pdf | author:Konrad Rawlik, Marc Toussaint, Sethu Vijayakumar category:cs.LG stat.ML published:2012-08-13 summary:We present an embedding of stochastic optimal control problems, of the socalled path integral form, into reproducing kernel Hilbert spaces. Usingconsistent, sample based estimates of the embedding leads to a model free,non-parametric approach for calculation of an approximate solution to thecontrol problem. This formulation admits a decomposition of the problem into aninvariant and task dependent component. Consequently, we make much moreefficient use of the sample data compared to previous sample based approachesin this domain, e.g., by allowing sample re-use across tasks. Numericalexamples on test problems, which illustrate the sample efficiency, areprovided.
arxiv-1200-228 | An Efficient Genetic Programming System with Geometric Semantic Operators and its Application to Human Oral Bioavailability Prediction | http://arxiv.org/pdf/1208.2437v1.pdf | author:Mauro Castelli, Luca Manzoni, Leonardo Vanneschi category:cs.NE published:2012-08-12 summary:Very recently new genetic operators, called geometric semantic operators,have been defined for genetic programming. Contrarily to standard geneticoperators, which are uniquely based on the syntax of the individuals, these newoperators are based on their semantics, meaning with it the set of input-outputpairs on training data. Furthermore, these operators present the interestingproperty of inducing a unimodal fitness landscape for every problem thatconsists in finding a match between given input and output data (for instanceregression and classification). Nevertheless, the current definition of theseoperators has a serious limitation: they impose an exponential growth in thesize of the individuals in the population, so their use is impossible inpractice. This paper is intended to overcome this limitation, presenting a newgenetic programming system that implements geometric semantic operators in anextremely efficient way. To demonstrate the power of the proposed system, weuse it to solve a complex real-life application in the field ofpharmacokinetic: the prediction of the human oral bioavailability of potentialnew drugs. Besides the excellent performances on training data, which wereexpected because the fitness landscape is unimodal, we also report an excellentgeneralization ability of the proposed system, at least for the studiedapplication. In fact, it outperforms standard genetic programming and a wideset of other well-known machine learning methods.
arxiv-1200-229 | How to sample if you must: on optimal functional sampling | http://arxiv.org/pdf/1208.2417v1.pdf | author:Assaf Hallak, Shie Mannor category:stat.ML cs.LG published:2012-08-12 summary:We examine a fundamental problem that models various active sampling setups,such as network tomography. We analyze sampling of a multivariate normaldistribution with an unknown expectation that needs to be estimated: in oursetup it is possible to sample the distribution from a given set of linearfunctionals, and the difficulty addressed is how to optimally select thecombinations to achieve low estimation error. Although this problem is in theheart of the field of optimal design, no efficient solutions for the case withmany functionals exist. We present some bounds and an efficient sub-optimalsolution for this problem for more structured sets such as binary functionalsthat are induced by graph walks.
arxiv-1200-230 | Feasibility of Genetic Algorithm for Textile Defect Classification Using Neural Network | http://arxiv.org/pdf/1208.6025v1.pdf | author:Md. Tarek Habib, Rahat Hossain Faisal, M. Rokonuzzaman category:cs.NE published:2012-08-11 summary:The global market for textile industry is highly competitive nowadays.Quality control in production process in textile industry has been a key factorfor retaining existence in such competitive market. Automated textileinspection systems are very useful in this respect, because manual inspectionis time consuming and not accurate enough. Hence, automated textile inspectionsystems have been drawing plenty of attention of the researchers of differentcountries in order to replace manual inspection. Defect detection and defectclassification are the two major problems that are posed by the research ofautomated textile inspection systems. In this paper, we perform an extensiveinvestigation on the applicability of genetic algorithm (GA) in the context oftextile defect classification using neural network (NN). We observe the effectof tuning different network parameters and explain the reasons. We empiricallyfind a suitable NN model in the context of textile defect classification. Wecompare the performance of this model with that of the classification modelsimplemented by others.
arxiv-1200-231 | A Large Population Size Can Be Unhelpful in Evolutionary Algorithms | http://arxiv.org/pdf/1208.2345v1.pdf | author:Tianshi Chen, Ke Tang, Guoliang Chen, Xin Yao category:cs.NE published:2012-08-11 summary:The utilization of populations is one of the most important features ofevolutionary algorithms (EAs). There have been many studies analyzing theimpact of different population sizes on the performance of EAs. However, mostof such studies are based computational experiments, except for a few cases.The common wisdom so far appears to be that a large population would increasethe population diversity and thus help an EA. Indeed, increasing the populationsize has been a commonly used strategy in tuning an EA when it did not performas well as expected for a given problem. He and Yao (2002) showed theoreticallythat for some problem instance classes, a population can help to reduce theruntime of an EA from exponential to polynomial time. This paper analyzes therole of population further in EAs and shows rigorously that large populationsmay not always be useful. Conditions, under which large populations can beharmful, are discussed in this paper. Although the theoretical analysis wascarried out on one multi-modal problem using a specific type of EAs, it hasmuch wider implications. The analysis has revealed certain problemcharacteristics, which can be either the problem considered here or otherproblems, that lead to the disadvantages of large population sizes. Theanalytical approach developed in this paper can also be applied to analyzingEAs on other problems.
arxiv-1200-232 | Energy Efficient Wireless Communication using Genetic Algorithm Guided Faster Light Weight Digital Signature Algorithm (GADSA) | http://arxiv.org/pdf/1208.2333v1.pdf | author:Arindam Sarkar, J. K. Mandal category:cs.CR cs.NE published:2012-08-11 summary:In this paper GA based light weight faster version of Digital SignatureAlgorithm (GADSA) in wireless communication has been proposed. Various geneticoperators like crossover and mutation are used to optimizing amount of modularmultiplication. Roulette Wheel selection mechanism helps to select bestchromosome which in turn helps in faster computation and minimizes the timerequirements for DSA. Minimization of number of modular multiplication itself aNP-hard problem that means there is no polynomial time deterministic algorithmfor this purpose. This paper deals with this problem using GA basedoptimization algorithm for minimization of the modular multiplication. ProposedGADSA initiates with an initial population comprises of set of valid andcomplete set of individuals. Some operators are used to generate feasible validoffspring from the existing one. Among several exponents the best solutionreached by GADSA is compared with some of the existing techniques. Extensivesimulations shows competitive results for the proposed GADSA.
arxiv-1200-233 | Learning pseudo-Boolean k-DNF and Submodular Functions | http://arxiv.org/pdf/1208.2294v1.pdf | author:Sofya Raskhodnikova, Grigory Yaroslavtsev category:cs.LG cs.DM cs.DS published:2012-08-10 summary:We prove that any submodular function f: {0,1}^n -> {0,1,...,k} can berepresented as a pseudo-Boolean 2k-DNF formula. Pseudo-Boolean DNFs are anatural generalization of DNF representation for functions with integer range.Each term in such a formula has an associated integral constant. We show thatan analog of Hastad's switching lemma holds for pseudo-Boolean k-DNFs if allconstants associated with the terms of the formula are bounded. This allows us to generalize Mansour's PAC-learning algorithm for k-DNFs topseudo-Boolean k-DNFs, and hence gives a PAC-learning algorithm with membershipqueries under the uniform distribution for submodular functions of the formf:{0,1}^n -> {0,1,...,k}. Our algorithm runs in time polynomial in n, k^{O(k\log k / \epsilon)}, 1/\epsilon and log(1/\delta) and works even in theagnostic setting. The line of previous work on learning submodular functions[Balcan, Harvey (STOC '11), Gupta, Hardt, Roth, Ullman (STOC '11), Cheraghchi,Klivans, Kothari, Lee (SODA '12)] implies only n^{O(k)} query complexity forlearning submodular functions in this setting, for fixed epsilon and delta. Our learning algorithm implies a property tester for submodularity offunctions f:{0,1}^n -> {0, ..., k} with query complexity polynomial in n fork=O((\log n/ \loglog n)^{1/2}) and constant proximity parameter \epsilon.
arxiv-1200-234 | Balancing Lifetime and Classification Accuracy of Wireless Sensor Networks | http://arxiv.org/pdf/1208.2278v1.pdf | author:Kush R. Varshney, Peter M. van de Ven category:cs.NI stat.ML published:2012-08-10 summary:Wireless sensor networks are composed of distributed sensors that can be usedfor signal detection or classification. The likelihood functions of thehypotheses are often not known in advance, and decision rules have to belearned via supervised learning. A specific such algorithm is Fisherdiscriminant analysis (FDA), the classification accuracy of which has beenpreviously studied in the context of wireless sensor networks. Previous work,however, does not take into account the communication protocol or batterylifetime of the sensor networks; in this paper we extend the existing studiesby proposing a model that captures the relationship between battery lifetimeand classification accuracy. In order to do so we combine the FDA with a modelthat captures the dynamics of the Carrier-Sense Multiple-Access (CSMA)algorithm, the random-access algorithm used to regulate communications insensor networks. This allows us to study the interaction between theclassification accuracy, battery lifetime and effort put towards learning, aswell as the impact of the back-off rates of CSMA on the accuracy. Wecharacterize the tradeoff between the length of the training stage andaccuracy, and show that accuracy is non-monotone in the back-off rate due tochanges in the training sample size and overfitting.
arxiv-1200-235 | Curved Space Optimization: A Random Search based on General Relativity Theory | http://arxiv.org/pdf/1208.2214v1.pdf | author:Fereydoun Farrahi Moghaddam, Reza Farrahi Moghaddam, Mohamed Cheriet category:cs.NE published:2012-08-10 summary:Designing a fast and efficient optimization method with local optimaavoidance capability on a variety of optimization problems is still an openproblem for many researchers. In this work, the concept of a new globaloptimization method with an open implementation area is introduced as a CurvedSpace Optimization (CSO) method, which is a simple probabilistic optimizationmethod enhanced by concepts of general relativity theory. To address globaloptimization challenges such as performance and convergence, this new method isdesigned based on transformation of a random search space into a new searchspace based on concepts of space-time curvature in general relativity theory.In order to evaluate the performance of our proposed method, an implementationof CSO is deployed and its results are compared on benchmark functions withstate-of-the art optimization methods. The results show that the performance ofCSO is promising on unimodal and multimodal benchmark functions with differentsearch space dimension sizes.
arxiv-1200-236 | Brain tumor MRI image classification with feature selection and extraction using linear discriminant analysis | http://arxiv.org/pdf/1208.2128v1.pdf | author:V. P. Gladis Pushpa Rathi, S. Palani category:cs.CV cs.LG published:2012-08-10 summary:Feature extraction is a method of capturing visual content of an image. Thefeature extraction is the process to represent raw image in its reduced form tofacilitate decision making such as pattern classification. We have tried toaddress the problem of classification MRI brain images by creating a robust andmore accurate classifier which can act as an expert assistant to medicalpractitioners. The objective of this paper is to present a novel method offeature selection and extraction. This approach combines the Intensity,Texture, shape based features and classifies the tumor as white matter, Graymatter, CSF, abnormal and normal area. The experiment is performed on 140 tumorcontained brain MR images from the Internet Brain Segmentation Repository. Theproposed technique has been carried out over a larger database as compare toany previous work and is more robust and effective. PCA and Linear DiscriminantAnalysis (LDA) were applied on the training sets. The Support Vector Machine(SVM) classifier served as a comparison of nonlinear techniques Vs linear ones.PCA and LDA methods are used to reduce the number of features used. The featureselection using the proposed technique is more beneficial as it analyses thedata according to grouping class variable and gives reduced feature set withhigh classification accuracy.
arxiv-1200-237 | A Fast Projected Fixed-Point Algorithm for Large Graph Matching | http://arxiv.org/pdf/1207.1114v3.pdf | author:Yao Lu, Kaizhu Huang, Cheng-Lin Liu category:cs.CV published:2012-07-03 summary:We propose a fast approximate algorithm for large graph matching. A newprojected fixed-point method is defined and a new doubly stochastic projectionis adopted to derive the algorithm. Previous graph matching algorithms sufferfrom high computational complexity and therefore do not have good scalabilitywith respect to graph size. For matching two weighted graphs of $n$ nodes, ouralgorithm has time complexity only $O(n^3)$ per iteration and space complexity$O(n^2)$. In addition to its scalability, our algorithm is easy to implement,robust, and able to match undirected weighted attributed graphs of differentsizes. While the convergence rate of previous iterative graph matchingalgorithms is unknown, our algorithm is theoretically guaranteed to converge ata linear rate. Extensive experiments on large synthetic and real graphs (morethan 1,000 nodes) were conducted to evaluate the performance of variousalgorithms. Results show that in most cases our proposed algorithm achievesbetter performance than previous state-of-the-art algorithms in terms of bothspeed and accuracy in large graph matching. In particular, with high accuracy,our algorithm takes only a few seconds (in a PC) to match two graphs of 1,000nodes.
arxiv-1200-238 | A study on non-destructive method for detecting Toxin in pepper using Neural networks | http://arxiv.org/pdf/1208.2092v1.pdf | author:M. Rajalakshmi, P. Subashini category:cs.NE cs.CV published:2012-08-10 summary:Mycotoxin contamination in certain agricultural systems have been a seriousconcern for human and animal health. Mycotoxins are toxic substances producedmostly as secondary metabolites by fungi that grow on seeds and feed in thefield, or in storage. The food-borne Mycotoxins likely to be of greatestsignificance for human health in tropical developing countries are Aflatoxinsand Fumonisins. Chili pepper is also prone to Aflatoxin contamination duringharvesting, production and storage periods.Various methods used for detectionof Mycotoxins give accurate results, but they are slow, expensive anddestructive. Destructive method is testing a material that degrades the sampleunder investigation. Whereas, non-destructive testing will, after testing,allow the part to be used for its intended purpose. Ultrasonic methods,Multispectral image processing methods, Terahertz methods, X-ray andThermography have been very popular in nondestructive testing andcharacterization of materials and health monitoring. Image processing methodsare used to improve the visual quality of the pictures and to extract usefulinformation from them. In this proposed work, the chili pepper samples will becollected, and the X-ray, multispectral images of the samples will be processedusing image processing methods. The term "Computational Intelligence" referredas simulation of human intelligence on computers. It is also called as"Artificial Intelligence" (AI) approach. The techniques used in AI approach areNeural network, Fuzzy logic and evolutionary computation. Finally, thecomputational intelligence method will be used in addition to image processingto provide best, high performance and accurate results for detecting theMycotoxin level in the samples collected.
arxiv-1200-239 | Stereo Acoustic Perception based on Real Time Video Acquisition for Navigational Assistance | http://arxiv.org/pdf/1208.1880v1.pdf | author:Supreeth K. Rao, Arpitha Prasad B., Anushree R. Shetty, Chinmai, R. Bhakthavathsalam, Rajeshwari Hegde category:cs.CV cs.MM cs.SD published:2012-08-09 summary:A smart navigation system (an Electronic Travel Aid) based on an objectdetection mechanism has been designed to detect the presence of obstacles thatimmediately impede the path, by means of real time video processing. Thealgorithm can be used for any general purpose navigational aid. This paper isdiscussed, keeping in mind the navigation of the visually impaired, and is notlimited to the same. A video camera feeds images of the surroundings to a Da-Vinci Digital Media Processor, DM642, which works on the video, frame by frame.The processor carries out image processing techniques whose result containsinformation about the object in terms of image pixels. The algorithm aims toselect the object which, among all others, poses maximum threat to thenavigation. A database containing a total of three sounds is constructed.Hence, each image translates to a beep, where every beep informs the navigatorof the obstacles directly in front of him. This paper implements an algorithmthat is more efficient as compared to its predecessors.
arxiv-1200-240 | Scaling Multiple-Source Entity Resolution using Statistically Efficient Transfer Learning | http://arxiv.org/pdf/1208.1860v1.pdf | author:Sahand Negahban, Benjamin I. P. Rubinstein, Jim Gemmell category:cs.DB cs.LG published:2012-08-09 summary:We consider a serious, previously-unexplored challenge facing almost allapproaches to scaling up entity resolution (ER) to multiple data sources: theprohibitive cost of labeling training data for supervised learning ofsimilarity scores for each pair of sources. While there exists a richliterature describing almost all aspects of pairwise ER, this new challenge isarising now due to the unprecedented ability to acquire and store data fromonline sources, features driven by ER such as enriched search verticals, andthe uniqueness of noisy and missing data characteristics for each source. Weshow on real-world and synthetic data that for state-of-the-art techniques, thereality of heterogeneous sources means that the number of labeled training datamust scale quadratically in the number of sources, just to maintain constantprecision/recall. We address this challenge with a brand new transfer learningalgorithm which requires far less training data (or equivalently, achievessuperior accuracy with the same data) and is trained using fast convexoptimization. The intuition behind our approach is to adaptively sharestructure learned about one scoring problem with all other scoring problemssharing a data source in common. We demonstrate that our theoreticallymotivated approach incurs no runtime cost while it can maintain constantprecision/recall with the cost of labeling increasing only linearly with thenumber of sources.
arxiv-1200-241 | Margin Distribution Controlled Boosting | http://arxiv.org/pdf/1208.1846v1.pdf | author:Guangxu Guo, Songcan Chen category:cs.LG published:2012-08-09 summary:Schapire's margin theory provides a theoretical explanation to the success ofboosting-type methods and manifests that a good margin distribution (MD) oftraining samples is essential for generalization. However the statement that aMD is good is vague, consequently, many recently developed algorithms try togenerate a MD in their goodness senses for boosting generalization. Unliketheir indirect control over MD, in this paper, we propose an alternativeboosting algorithm termed Margin distribution Controlled Boosting (MCBoost)which directly controls the MD by introducing and optimizing a key adjustablemargin parameter. MCBoost's optimization implementation adopts the columngeneration technique to ensure fast convergence and small number of weakclassifiers involved in the final MCBooster. We empirically demonstrate: 1)AdaBoost is actually also a MD controlled algorithm and its iteration numberacts as a parameter controlling the distribution and 2) the generalizationperformance of MCBoost evaluated on UCI benchmark datasets is validated betterthan those of AdaBoost, L2Boost, LPBoost, AdaBoost-CG and MDBoost.
arxiv-1200-242 | Metric Learning across Heterogeneous Domains by Respectively Aligning Both Priors and Posteriors | http://arxiv.org/pdf/1208.1829v1.pdf | author:Qiang Qian, Songcan Chen category:cs.LG published:2012-08-09 summary:In this paper, we attempts to learn a single metric across two heterogeneousdomains where source domain is fully labeled and has many samples while targetdomain has only a few labeled samples but abundant unlabeled samples. To thebest of our knowledge, this task is seldom touched. The proposed learning modelhas a simple underlying motivation: all the samples in both the source and thetarget domains are mapped into a common space, where both their priorsP(sample)s and their posteriors P(labelsample)s are forced to be respectivelyaligned as much as possible. We show that the two mappings, from both thesource domain and the target domain to the common space, can be reparameterizedinto a single positive semi-definite(PSD) matrix. Then we develop an efficientBregman Projection algorithm to optimize the PDS matrix over which a LogDetfunction is used to regularize. Furthermore, we also show that this model canbe easily kernelized and verify its effectiveness in crosslanguage retrievaltask and cross-domain object recognition task.
arxiv-1200-243 | Self-Organizing Time Map: An Abstraction of Temporal Multivariate Patterns | http://arxiv.org/pdf/1208.1819v1.pdf | author:Peter Sarlin category:cs.LG cs.DS published:2012-08-09 summary:This paper adopts and adapts Kohonen's standard Self-Organizing Map (SOM) forexploratory temporal structure analysis. The Self-Organizing Time Map (SOTM)implements SOM-type learning to one-dimensional arrays for individual timeunits, preserves the orientation with short-term memory and arranges the arraysin an ascending order of time. The two-dimensional representation of the SOTMattempts thus twofold topology preservation, where the horizontal directionpreserves time topology and the vertical direction data topology. This enablesdiscovering the occurrence and exploring the properties of temporal structuralchanges in data. For representing qualities and properties of SOTMs, we adaptmeasures and visualizations from the standard SOM paradigm, as well asintroduce a measure of temporal structural changes. The functioning of theSOTM, and its visualizations and quality and property measures, are illustratedon artificial toy data. The usefulness of the SOTM in a real-world setting isshown on poverty, welfare and development indicators.
arxiv-1200-244 | ARMA Time-Series Modeling with Graphical Models | http://arxiv.org/pdf/1207.4162v2.pdf | author:Bo Thiesson, David Maxwell Chickering, David Heckerman, Christopher Meek category:stat.AP cs.LG stat.ME published:2012-07-11 summary:We express the classic ARMA time-series model as a directed graphical model.In doing so, we find that the deterministic relationships in the model make iteffectively impossible to use the EM algorithm for learning model parameters.To remedy this problem, we replace the deterministic relationships withGaussian distributions having a small variance, yielding the stochastic ARMA(ARMA) model. This modification allows us to use the EM algorithm to learnparmeters and to forecast,even in situations where some data is missing. Thismodification, in conjunction with the graphicalmodel approach, also allows usto include cross predictors in situations where there are multiple times seriesand/or additional nontemporal covariates. More surprising,experiments suggestthat the move to stochastic ARMA yields improved accuracy through bettersmoothing. We demonstrate improvements afforded by cross prediction and bettersmoothing on real data.
arxiv-1200-245 | An Efficient Automatic Attendance System Using Fingerprint Reconstruction Technique | http://arxiv.org/pdf/1208.1672v1.pdf | author:Josphineleela Ramakrishnan, M. Ramakrishnan category:cs.CV published:2012-08-08 summary:Biometric time and attendance system is one of the most successfulapplications of biometric technology. One of the main advantage of a biometrictime and attendance system is it avoids "buddy-punching". Buddy punching was amajor loophole which will be exploiting in the traditional time attendancesystems. Fingerprint recognition is an established field today, but stillidentifying individual from a set of enrolled fingerprints is a time takingprocess. Most fingerprint-based biometric systems store the minutiae templateof a user in the database. It has been traditionally assumed that the minutiaetemplate of a user does not reveal any information about the originalfingerprint. This belief has now been shown to be false; several algorithmshave been proposed that can reconstruct fingerprint images from minutiaetemplates. In this paper, a novel fingerprint reconstruction algorithm isproposed to reconstruct the phase image, which is then converted into thegrayscale image. The proposed reconstruction algorithm reconstructs the phaseimage from minutiae. The proposed reconstruction algorithm is used to automatethe whole process of taking attendance, manually which is a laborious andtroublesome work and waste a lot of time, with its managing and maintaining therecords for a period of time is also a burdensome task. The proposedreconstruction algorithm has been evaluated with respect to the success ratesof type-I attack (match the reconstructed fingerprint against the originalfingerprint) and type-II attack (match the reconstructed fingerprint againstdifferent impressions of the original fingerprint) using a commercialfingerprint recognition system. Given the reconstructed image from ouralgorithm, we show that both types of attacks can be effectively launchedagainst a fingerprint recognition system.
arxiv-1200-246 | Performance Measurement and Method Analysis (PMMA) for Fingerprint Reconstruction | http://arxiv.org/pdf/1208.1670v1.pdf | author:Josphineleela Ramakrishnan, Ramakrishnan Malaisamy category:cs.CV published:2012-08-08 summary:Fingerprint reconstruction is one of the most well-known and publicizedbiometrics. Because of their uniqueness and consistency over time, fingerprintshave been used for identification over a century, more recently becomingautomated due to advancements in computed capabilities. Fingerprintreconstruction is popular because of the inherent ease of acquisition, thenumerous sources (e.g. ten fingers) available for collection, and theirestablished use and collections by law enforcement and immigration.Fingerprints have always been the most practical and positive means ofidentification. Offenders, being well aware of this, have been coming up withways to escape identification by that means. Erasing left over fingerprints,using gloves, fingerprint forgery; are certain examples of methods tried bythem, over the years. Failing to prevent themselves, they moved to an extent ofmutilating their finger skin pattern, to remain unidentified. This article isbased upon obliteration of finger ridge patterns and discusses some known casesin relation to the same, in chronological order; highlighting the reasons whyoffenders go to an extent of performing such act. The paper gives an overviewof different methods and performance measurement of the fingerprintreconstruction.
arxiv-1200-247 | A Survey of Recent View-based 3D Model Retrieval Methods | http://arxiv.org/pdf/1208.3670v1.pdf | author:Qiong Liu category:cs.CV published:2012-08-08 summary:Extensive research efforts have been dedicated to 3D model retrieval inrecent decades. Recently, view-based methods have attracted much researchattention due to the high discriminative property of multi-views for 3D objectrepresentation. In this report, we summarize the view-based 3D model methodsand provide the further research trends. This paper focuses on the scheme formatching between multiple views of 3D models and the application ofbag-of-visual-words method in 3D model retrieval. For matching between multipleviews, the many-to-many matching, probabilistic matching and semisupervisedlearning methods are introduced. For bag-of-visual-words application in 3Dmodel retrieval, we first briefly review the bag-of-visual-words works onmultimedia and computer vision tasks, where the visual dictionary has beendetailed introduced. Then a series of 3D model retrieval methods by usingbag-of-visual-words description are surveyed in this paper. At last, wesummarize the further research content in view-based 3D model retrieval.
arxiv-1200-248 | Re-initialization Free Level Set Evolution via Reaction Diffusion | http://arxiv.org/pdf/1112.1496v3.pdf | author:Kaihua Zhang, Lei Zhang, Huihui Song, David Zhang category:cs.CV published:2011-12-07 summary:This paper presents a novel reaction-diffusion (RD) method for implicitactive contours, which is completely free of the costly re-initializationprocedure in level set evolution (LSE). A diffusion term is introduced intoLSE, resulting in a RD-LSE equation, to which a piecewise constant solution canbe derived. In order to have a stable numerical solution of the RD based LSE,we propose a two-step splitting method (TSSM) to iteratively solve the RD-LSEequation: first iterating the LSE equation, and then solving the diffusionequation. The second step regularizes the level set function obtained in thefirst step to ensure stability, and thus the complex and costlyre-initialization procedure is completely eliminated from LSE. By successfullyapplying diffusion to LSE, the RD-LSE model is stable by means of the simplefinite difference method, which is very easy to implement. The proposed RDmethod can be generalized to solve the LSE for both variational level setmethod and PDE-based level set method. The RD-LSE method shows very goodperformance on boundary anti-leakage, and it can be readily extended to highdimensional level set method. The extensive and promising experimental resultson synthetic and real images validate the effectiveness of the proposed RD-LSEapproach.
arxiv-1200-249 | Guess Who Rated This Movie: Identifying Users Through Subspace Clustering | http://arxiv.org/pdf/1208.1544v1.pdf | author:Amy Zhang, Nadia Fawaz, Stratis Ioannidis, Andrea Montanari category:cs.LG published:2012-08-07 summary:It is often the case that, within an online recommender system, multipleusers share a common account. Can such shared accounts be identified solely onthe basis of the user- provided ratings? Once a shared account is identified,can the different users sharing it be identified as well? Whenever such useridentification is feasible, it opens the way to possible improvements inpersonalized recommendations, but also raises privacy concerns. We develop amodel for composite accounts based on unions of linear subspaces, and usesubspace clustering for carrying out the identification task. We show that asignificant fraction of such accounts is identifiable in a reliable manner, andillustrate potential uses for personalized recommendation.
arxiv-1200-250 | The Graphical Lasso: New Insights and Alternatives | http://arxiv.org/pdf/1111.5479v2.pdf | author:Rahul Mazumder, Trevor Hastie category:stat.ML cs.LG published:2011-11-23 summary:The graphical lasso \citep{FHT2007a} is an algorithm for learning thestructure in an undirected Gaussian graphical model, using $\ell_1$regularization to control the number of zeros in the precision matrix${\B\Theta}={\B\Sigma}^{-1}$ \citep{BGA2008,yuan_lin_07}. The {\texttt R}package \GL\ \citep{FHT2007a} is popular, fast, and allows one to efficientlybuild a path of models for different values of the tuning parameter.Convergence of \GL\ can be tricky; the converged precision matrix might not bethe inverse of the estimated covariance, and occasionally it fails to convergewith warm starts. In this paper we explain this behavior, and propose newalgorithms that appear to outperform \GL. By studying the "normal equations" we see that, \GL\ is solving the {\emdual} of the graphical lasso penalized likelihood, by block coordinate ascent;a result which can also be found in \cite{BGA2008}. In this dual, the target of estimation is $\B\Sigma$, the covariance matrix,rather than the precision matrix $\B\Theta$. We propose similar primalalgorithms \PGL\ and \DPGL, that also operate by block-coordinate descent,where $\B\Theta$ is the optimization target. We study all of these algorithms,and in particular different approaches to solving their coordinatesub-problems. We conclude that \DPGL\ is superior from several points of view.
arxiv-1200-251 | Acceleration of the shiftable O(1) algorithm for bilateral filtering and non-local means | http://arxiv.org/pdf/1203.5128v2.pdf | author:Kunal N. Chaudhury category:cs.CV cs.DC published:2012-03-22 summary:A direct implementation of the bilateral filter [1] requires O(\sigma_s^2)operations per pixel, where \sigma_s is the (effective) width of the spatialkernel. A fast implementation of the bilateral filter was recently proposed in[2] that required O(1) operations per pixel with respect to \sigma_s. This wasdone by using trigonometric functions for the range kernel of the bilateralfilter, and by exploiting their so-called shiftability property. In particular,a fast implementation of the Gaussian bilateral filter was realized byapproximating the Gaussian range kernel using raised cosines. Later, it wasdemonstrated in [3] that this idea could be extended to a larger class offilters, including the popular non-local means filter [4]. As already observedin [2], a flip side of this approach was that the run time depended on thewidth \sigma_r of the range kernel. For an image with (local) intensityvariations in the range [0,T], the run time scaled as O(T^2/\sigma^2_r) with\sigma_r. This made it difficult to implement narrow range kernels,particularly for images with large dynamic range. We discuss this problem inthis note, and propose some simple steps to accelerate the implementation ingeneral, and for small \sigma_r in particular. [1] C. Tomasi and R. Manduchi, "Bilateral filtering for gray and colorimages", Proc. IEEE International Conference on Computer Vision, 1998. [2] K.N. Chaudhury, Daniel Sage, and M. Unser, "Fast O(1) bilateral filteringusing trigonometric range kernels", IEEE Transactions on Image Processing,2011. [3] K.N. Chaudhury, "Constant-time filtering using shiftable kernels", IEEESignal Processing Letters, 2011. [4] A. Buades, B. Coll, and J.M. Morel, "A review of image denoisingalgorithms, with a new one", Multiscale Modeling and Simulation, 2005.
arxiv-1200-252 | Color Assessment and Transfer for Web Pages | http://arxiv.org/pdf/1208.1679v1.pdf | author:Ou Wu category:cs.HC cs.CV cs.GR H.4.m; H.2.8 published:2012-08-07 summary:Colors play a particularly important role in both designing and accessing Webpages. A well-designed color scheme improves Web pages' visual aesthetic andfacilitates user interactions. As far as we know, existing color assessmentstudies focus on images; studies on color assessment and editing for Web pagesare rare. This paper investigates color assessment for Web pages based onexisting online color theme-rating data sets and applies this assessment to Webcolor edit. This study consists of three parts. First, we study the extractionof a Web page's color theme. Second, we construct color assessment models thatscore the color compatibility of a Web page by leveraging machine learningtechniques. Third, we incorporate the learned color assessment model into a newapplication, namely, color transfer for Web pages. Our study combinestechniques from computer graphics, Web mining, computer vision, and machinelearning. Experimental results suggest that our constructed color assessmentmodels are effective, and useful in the color transfer for Web pages, which hasreceived little attention in both Web mining and computer graphics communities.
arxiv-1200-253 | Data Selection for Semi-Supervised Learning | http://arxiv.org/pdf/1208.1315v1.pdf | author:Shafigh Parsazad, Ehsan Saboori, Amin Allahyar category:cs.LG published:2012-08-07 summary:The real challenge in pattern recognition task and machine learning processis to train a discriminator using labeled data and use it to distinguishbetween future data as accurate as possible. However, most of the problems inthe real world have numerous data, which labeling them is a cumbersome or evenan impossible matter. Semi-supervised learning is one approach to overcomethese types of problems. It uses only a small set of labeled with the companyof huge remain and unlabeled data to train the discriminator. Insemi-supervised learning, it is very essential that which data is labeled anddepend on position of data it effectiveness changes. In this paper, we proposedan evolutionary approach called Artificial Immune System (AIS) to determinewhich data is better to be labeled to get the high quality data. Theexperimental results represent the effectiveness of this algorithm in findingthese data points.
arxiv-1200-254 | Structured Prediction Cascades | http://arxiv.org/pdf/1208.3279v1.pdf | author:David Weiss, Benjamin Sapp, Ben Taskar category:stat.ML cs.LG published:2012-08-06 summary:Structured prediction tasks pose a fundamental trade-off between the need formodel complexity to increase predictive power and the limited computationalresources for inference in the exponentially-sized output spaces such modelsrequire. We formulate and develop the Structured Prediction Cascadearchitecture: a sequence of increasingly complex models that progressivelyfilter the space of possible outputs. The key principle of our approach is thateach model in the cascade is optimized to accurately filter and refine thestructured output state space of the next model, speeding up both learning andinference in the next layer of the cascade. We learn cascades by optimizing anovel convex loss function that controls the trade-off between the filteringefficiency and the accuracy of the cascade, and provide generalization boundsfor both accuracy and efficiency. We also extend our approach to intractablemodels using tree-decomposition ensembles, and provide algorithms and theoryfor this setting. We evaluate our approach on several large-scale problems,achieving state-of-the-art performance in handwriting recognition and humanpose recognition. We find that structured prediction cascades allow tremendousspeedups and the use of previously intractable features and models in bothsettings.
arxiv-1200-255 | One Permutation Hashing for Efficient Search and Learning | http://arxiv.org/pdf/1208.1259v1.pdf | author:Ping Li, Art Owen, Cun-Hui Zhang category:cs.LG cs.IR cs.IT math.IT stat.CO stat.ML published:2012-08-06 summary:Recently, the method of b-bit minwise hashing has been applied to large-scalelinear learning and sublinear time near-neighbor search. The major drawback ofminwise hashing is the expensive preprocessing cost, as the method requiresapplying (e.g.,) k=200 to 500 permutations on the data. The testing time canalso be expensive if a new data point (e.g., a new document or image) has notbeen processed, which might be a significant issue in user-facing applications. We develop a very simple solution based on one permutation hashing.Conceptually, given a massive binary data matrix, we permute the columns onlyonce and divide the permuted columns evenly into k bins; and we simply store,for each data vector, the smallest nonzero location in each bin. Theinteresting probability analysis (which is validated by experiments) revealsthat our one permutation scheme should perform very similarly to the original(k-permutation) minwise hashing. In fact, the one permutation scheme can beeven slightly more accurate, due to the "sample-without-replacement" effect. Our experiments with training linear SVM and logistic regression on thewebspam dataset demonstrate that this one permutation hashing scheme canachieve the same (or even slightly better) accuracies compared to the originalk-permutation scheme. To test the robustness of our method, we also experimentwith the small news20 dataset which is very sparse and has merely on average500 nonzeros in each data vector. Interestingly, our one permutation schemenoticeably outperforms the k-permutation scheme when k is not too small on thenews20 dataset. In summary, our method can achieve at least the same accuracyas the original k-permutation scheme, at merely 1/k of the originalpreprocessing cost.
arxiv-1200-256 | Sequential Estimation Methods from Inclusion Principle | http://arxiv.org/pdf/1208.1056v1.pdf | author:Xinjia Chen category:math.ST cs.LG math.PR stat.TH published:2012-08-05 summary:In this paper, we propose new sequential estimation methods based oninclusion principle. The main idea is to reformulate the estimation problems asconstructing sequential random intervals and use confidence sequences tocontrol the associated coverage probabilities. In contrast to existingasymptotic sequential methods, our estimation procedures rigorously guaranteethe pre-specified levels of confidence.
arxiv-1200-257 | Detection of Deviations in Mobile Applications Network Behavior | http://arxiv.org/pdf/1208.0564v2.pdf | author:L. Chekina, D. Mimran, L. Rokach, Y. Elovici, B. Shapira category:cs.CR cs.LG published:2012-07-27 summary:In this paper a novel system for detecting meaningful deviations in a mobileapplication's network behavior is proposed. The main goal of the proposedsystem is to protect mobile device users and cellular infrastructure companiesfrom malicious applications. The new system is capable of: (1) identifyingmalicious attacks or masquerading applications installed on a mobile device,and (2) identifying republishing of popular applications injected with amalicious code. The detection is performed based on the application's networktraffic patterns only. For each application two types of models are learned.The first model, local, represents the personal traffic pattern for each userusing an application and is learned on the device. The second model,collaborative, represents traffic patterns of numerous users using anapplication and is learned on the system server. Machine-learning methods areused for learning and detection purposes. This paper focuses on methodsutilized for local (i.e., on mobile device) learning and detection ofdeviations from the normal application's behavior. These methods wereimplemented and evaluated on Android devices. The evaluation experimentsdemonstrate that: (1) various applications have specific network trafficpatterns and certain application categories can be distinguishable by theirnetwork patterns, (2) different levels of deviations from normal behavior canbe detected accurately, and (3) local learning is feasible and has a lowperformance overhead on mobile devices.
arxiv-1200-258 | APRIL: Active Preference-learning based Reinforcement Learning | http://arxiv.org/pdf/1208.0984v1.pdf | author:Riad Akrour, Marc Schoenauer, Michèle Sebag category:cs.LG published:2012-08-05 summary:This paper focuses on reinforcement learning (RL) with limited priorknowledge. In the domain of swarm robotics for instance, the expert can hardlydesign a reward function or demonstrate the target behavior, forbidding the useof both standard RL and inverse reinforcement learning. Although with a limitedexpertise, the human expert is still often able to emit preferences and rankthe agent demonstrations. Earlier work has presented an iterativepreference-based RL framework: expert preferences are exploited to learn anapproximate policy return, thus enabling the agent to achieve direct policysearch. Iteratively, the agent selects a new candidate policy and demonstratesit; the expert ranks the new demonstration comparatively to the previous bestone; the expert's ranking feedback enables the agent to refine the approximatepolicy return, and the process is iterated. In this paper, preference-basedreinforcement learning is combined with active ranking in order to decrease thenumber of ranking queries to the expert needed to yield a satisfactory policy.Experiments on the mountain car and the cancer treatment testbeds witness thata couple of dozen rankings enable to learn a competent policy.
arxiv-1200-259 | Human Activity Learning using Object Affordances from RGB-D Videos | http://arxiv.org/pdf/1208.0967v1.pdf | author:Hema Swetha Koppula, Rudhir Gupta, Ashutosh Saxena category:cs.CV published:2012-08-04 summary:Human activities comprise several sub-activities performed in a sequence andinvolve interactions with various objects. This makes reasoning about theobject affordances a central task for activity recognition. In this work, weconsider the problem of jointly labeling the object affordances and humanactivities from RGB-D videos. We frame the problem as a Markov Random Fieldwhere the nodes represent objects and sub-activities, and the edges representthe relationships between object affordances, their relations withsub-activities, and their evolution over time. We formulate the learningproblem using a structural SVM approach, where labeling over various alternatetemporal segmentations are considered as latent variables. We tested our methodon a dataset comprising 120 activity videos collected from four subjects, andobtained an end-to-end precision of 81.8% and recall of 80.0% for labeling theactivities.
arxiv-1200-260 | Dynamic Matrix Factorization: A State Space Approach | http://arxiv.org/pdf/1110.2098v3.pdf | author:John Z. Sun, Kush R. Varshney, Karthik Subbian category:cs.LG published:2011-10-10 summary:Matrix factorization from a small number of observed entries has recentlygarnered much attention as the key ingredient of successful recommendationsystems. One unresolved problem in this area is how to adapt current methods tohandle changing user preferences over time. Recent proposals to address thisissue are heuristic in nature and do not fully exploit the time-dependentstructure of the problem. As a principled and general temporal formulation, wepropose a dynamical state space model of matrix factorization. Our proposalbuilds upon probabilistic matrix factorization, a Bayesian model with Gaussianpriors. We utilize results in state tracking, such as the Kalman filter, toprovide accurate recommendations in the presence of both process andmeasurement noise. We show how system parameters can be learned viaexpectation-maximization and provide comparisons to current publishedtechniques.
arxiv-1200-261 | Provably Safe and Robust Learning-Based Model Predictive Control | http://arxiv.org/pdf/1107.2487v2.pdf | author:Anil Aswani, Humberto Gonzalez, S. Shankar Sastry, Claire Tomlin category:math.OC cs.LG cs.SY math.ST stat.TH published:2011-07-13 summary:Controller design faces a trade-off between robustness and performance, andthe reliability of linear controllers has caused many practitioners to focus onthe former. However, there is renewed interest in improving system performanceto deal with growing energy constraints. This paper describes a learning-basedmodel predictive control (LBMPC) scheme that provides deterministic guaranteeson robustness, while statistical identification tools are used to identifyricher models of the system in order to improve performance; the benefits ofthis framework are that it handles state and input constraints, optimizessystem performance with respect to a cost function, and can be designed to usea wide variety of parametric or nonparametric statistical tools. The maininsight of LBMPC is that safety and performance can be decoupled underreasonable conditions in an optimization framework by maintaining two models ofthe system. The first is an approximate model with bounds on its uncertainty,and the second model is updated by statistical methods. LBMPC improvesperformance by choosing inputs that minimize a cost subject to the learneddynamics, and it ensures safety and robustness by checking whether these sameinputs keep the approximate model stable when it is subject to uncertainty.Furthermore, we show that if the system is sufficiently excited, then the LBMPCcontrol action probabilistically converges to that of an MPC computed using thetrue dynamics.
arxiv-1200-262 | Statistical Results on Filtering and Epi-convergence for Learning-Based Model Predictive Control | http://arxiv.org/pdf/1208.0864v1.pdf | author:Anil Aswani, Humberto Gonzalez, S. Shankar Sastry, Claire Tomlin category:math.OC cs.LG cs.SY published:2012-08-03 summary:Learning-based model predictive control (LBMPC) is a technique that providesdeterministic guarantees on robustness, while statistical identification toolsare used to identify richer models of the system in order to improveperformance. This technical note provides proofs that elucidate the reasons forour choice of measurement model, as well as giving proofs concerning thestochastic convergence of LBMPC. The first part of this note discussessimultaneous state estimation and statistical identification (or learning) ofunmodeled dynamics, for dynamical systems that can be described by ordinarydifferential equations (ODE's). The second part provides proofs concerning theepi-convergence of different statistical estimators that can be used with thelearning-based model predictive control (LBMPC) technique. In particular, weprove results on the statistical properties of a nonparametric estimator thatwe have designed to have the correct deterministic and stochastic propertiesfor numerical implementation when used in conjunction with LBMPC.
arxiv-1200-263 | Cross-conformal predictors | http://arxiv.org/pdf/1208.0806v1.pdf | author:Vladimir Vovk category:stat.ML cs.LG 62G15 published:2012-08-03 summary:This note introduces the method of cross-conformal prediction, which is ahybrid of the methods of inductive conformal prediction and cross-validation,and studies its validity and predictive efficiency empirically.
arxiv-1200-264 | A Novel Approach of Color Image Hiding using RGB Color planes and DWT | http://arxiv.org/pdf/1208.0803v1.pdf | author:Nilanjan Dey, Anamitra Bardhan Roy, Sayantan Dey category:cs.CR cs.CV published:2012-08-03 summary:This work proposes a wavelet based Steganographic technique for the colorimage. The true color cover image and the true color secret image both aredecomposed into three separate color planes namely R, G and B. Each plane ofthe images is decomposed into four sub bands using DWT. Each color plane of thesecret image is hidden by alpha blending technique in the corresponding subbands of the respective color planes of the original image. During embedding,secret image is dispersed within the original image depending upon the alphavalue. Extraction of the secret image varies according to the alpha value. Inthis approach the stego image generated is of acceptable level ofimperceptibility and distortion compared to the cover image and the overallsecurity is high.
arxiv-1200-265 | A phase-sensitive method for filtering on the sphere | http://arxiv.org/pdf/1208.0385v2.pdf | author:Ramakrishna Kakarala, Philip Ogunbona category:math.RT cs.CV published:2012-08-02 summary:This paper examines filtering on a sphere, by first examining the roles ofspherical harmonic magnitude and phase. We show that phase is more importantthan magnitude in determining the structure of a spherical function. We examinethe properties of linear phase shifts in the spherical harmonic domain, whichsuggest a mechanism for constructing finite-impulse-response (FIR) filters. Weshow that those filters have desirable properties, such as being associative,mapping spherical functions to spherical functions, allowing directionalfiltering, and being defined by relatively simple equations. We provideexamples of the filters for both spherical and manifold data.
arxiv-1200-266 | Evolutionary Inference for Function-valued Traits: Gaussian Process Regression on Phylogenies | http://arxiv.org/pdf/1004.4668v3.pdf | author:Nick S. Jones, John Moriarty category:q-bio.QM cs.LG stat.ML published:2010-04-26 summary:Biological data objects often have both of the following features: (i) theyare functions rather than single numbers or vectors, and (ii) they arecorrelated due to phylogenetic relationships. In this paper we give a flexiblestatistical model for such data, by combining assumptions from phylogeneticswith Gaussian processes. We describe its use as a nonparametric Bayesian priordistribution, both for prediction (placing posterior distributions on ancestralfunctions) and model selection (comparing rates of evolution across aphylogeny, or identifying the most likely phylogenies consistent with theobserved data). Our work is integrative, extending the popular phylogeneticBrownian Motion and Ornstein-Uhlenbeck models to functional data and Bayesianinference, and extending Gaussian Process regression to phylogenies. We providea brief illustration of the application of our method.
arxiv-1200-267 | Fast and Accurate Algorithms for Re-Weighted L1-Norm Minimization | http://arxiv.org/pdf/1208.0651v1.pdf | author:M. Salman Asif, Justin Romberg category:stat.CO cs.IT math.IT stat.ML published:2012-08-03 summary:To recover a sparse signal from an underdetermined system, we often solve aconstrained L1-norm minimization problem. In many cases, the signal sparsityand the recovery performance can be further improved by replacing the L1 normwith a "weighted" L1 norm. Without any prior information about nonzero elementsof the signal, the procedure for selecting weights is iterative in nature.Common approaches update the weights at every iteration using the solution of aweighted L1 problem from the previous iteration. In this paper, we present two homotopy-based algorithms that efficientlysolve reweighted L1 problems. First, we present an algorithm that quicklyupdates the solution of a weighted L1 problem as the weights change. Since thesolution changes only slightly with small changes in the weights, we develop ahomotopy algorithm that replaces the old weights with the new ones in a smallnumber of computationally inexpensive steps. Second, we propose an algorithmthat solves a weighted L1 problem by adaptively selecting the weights whileestimating the signal. This algorithm integrates the reweighting into everystep along the homotopy path by changing the weights according to the changesin the solution and its support, allowing us to achieve a high quality signalreconstruction by solving a single homotopy problem. We compare the performanceof both algorithms, in terms of reconstruction accuracy and computationalcomplexity, against state-of-the-art solvers and show that our methods havesmaller computational cost. In addition, we will show that the adaptiveselection of the weights inside the homotopy often yields reconstructions ofhigher quality.
arxiv-1200-268 | Ancestral Inference from Functional Data: Statistical Methods and Numerical Examples | http://arxiv.org/pdf/1208.0628v1.pdf | author:Pantelis Z. Hadjipantelis, Nick S. Jones, John Moriarty, David Springate, Christopher G. Knight category:stat.ML q-bio.PE q-bio.QM published:2012-08-02 summary:Many biological characteristics of evolutionary interest are not scalarvariables but continuous functions. Here we use phylogenetic Gaussian processregression to model the evolution of simulated function-valued traits. Givenfunction-valued data only from the tips of an evolutionary tree and utilisingindependent principal component analysis (IPCA) as a method for dimensionreduction, we construct distributional estimates of ancestral function-valuedtraits, and estimate parameters describing their evolutionary dynamics.
arxiv-1200-269 | A hybrid artificial immune system and Self Organising Map for network intrusion detection | http://arxiv.org/pdf/1208.0541v1.pdf | author:Simon T. Powers, Jun He category:cs.NE cs.CR published:2012-08-02 summary:Network intrusion detection is the problem of detecting unauthorised use of,or access to, computer systems over a network. Two broad approaches exist totackle this problem: anomaly detection and misuse detection. An anomalydetection system is trained only on examples of normal connections, and thushas the potential to detect novel attacks. However, many anomaly detectionsystems simply report the anomalous activity, rather than analysing it furtherin order to report higher-level information that is of more use to a securityofficer. On the other hand, misuse detection systems recognise known attackpatterns, thereby allowing them to provide more detailed information about anintrusion. However, such systems cannot detect novel attacks. A hybrid system is presented in this paper with the aim of combining theadvantages of both approaches. Specifically, anomalous network connections areinitially detected using an artificial immune system. Connections that areflagged as anomalous are then categorised using a Kohonen Self Organising Map,allowing higher-level information, in the form of cluster membership, to beextracted. Experimental results on the KDD 1999 Cup dataset show a low falsepositive rate and a detection and classification rate for Denial-of-Service andUser-to-Root attacks that is higher than those in a sample of other works.
arxiv-1200-270 | Optimization hardness as transient chaos in an analog approach to constraint satisfaction | http://arxiv.org/pdf/1208.0526v1.pdf | author:Maria Ercsey-Ravasz, Zoltan Toroczkai category:cs.CC cs.NE math.DS nlin.CD F.2.3; F.1.0 published:2012-08-02 summary:Boolean satisfiability [1] (k-SAT) is one of the most studied optimizationproblems, as an efficient (that is, polynomial-time) solution to k-SAT (for$k\geq 3$) implies efficient solutions to a large number of hard optimizationproblems [2,3]. Here we propose a mapping of k-SAT into a deterministiccontinuous-time dynamical system with a unique correspondence between itsattractors and the k-SAT solution clusters. We show that beyond a constraintdensity threshold, the analog trajectories become transiently chaotic [4-7],and the boundaries between the basins of attraction [8] of the solutionclusters become fractal [7-9], signaling the appearance of optimizationhardness [10]. Analytical arguments and simulations indicate that the systemalways finds solutions for satisfiable formulae even in the frozen regimes ofrandom 3-SAT [11] and of locked occupation problems [12] (considered among thehardest algorithmic benchmarks); a property partly due to the system'shyperbolic [4,13] character. The system finds solutions in polynomialcontinuous-time, however, at the expense of exponential fluctuations in itsenergy function.
arxiv-1200-271 | Efficient Optimization of Performance Measures by Classifier Adaptation | http://arxiv.org/pdf/1012.0930v3.pdf | author:Nan Li, Ivor W. Tsang, Zhi-Hua Zhou category:cs.LG cs.AI published:2010-12-04 summary:In practical applications, machine learning algorithms are often needed tolearn classifiers that optimize domain specific performance measures.Previously, the research has focused on learning the needed classifier inisolation, yet learning nonlinear classifier for nonlinear and nonsmoothperformance measures is still hard. In this paper, rather than learning theneeded classifier by optimizing specific performance measure directly, wecircumvent this problem by proposing a novel two-step approach called as CAPO,namely to first train nonlinear auxiliary classifiers with existing learningmethods, and then to adapt auxiliary classifiers for specific performancemeasures. In the first step, auxiliary classifiers can be obtained efficientlyby taking off-the-shelf learning algorithms. For the second step, we show thatthe classifier adaptation problem can be reduced to a quadratic programproblem, which is similar to linear SVMperf and can be efficiently solved. Byexploiting nonlinear auxiliary classifiers, CAPO can generate nonlinearclassifier which optimizes a large variety of performance measures includingall the performance measure based on the contingency table and AUC, whilstkeeping high computational efficiency. Empirical studies show that CAPO iseffective and of high computational efficiency, and even it is more efficientthan linear SVMperf.
arxiv-1200-272 | Multidimensional Membership Mixture Models | http://arxiv.org/pdf/1208.0402v1.pdf | author:Yun Jiang, Marcus Lim, Ashutosh Saxena category:cs.LG stat.ML published:2012-08-02 summary:We present the multidimensional membership mixture (M3) models where everydimension of the membership represents an independent mixture model and eachdata point is generated from the selected mixture components jointly. This ishelpful when the data has a certain shared structure. For example, three uniquemeans and three unique variances can effectively form a Gaussian mixture modelwith nine components, while requiring only six parameters to fully describe it.In this paper, we present three instantiations of M3 models (together with thelearning and inference algorithms): infinite, finite, and hybrid, depending onwhether the number of mixtures is fixed or not. They are built upon Dirichletprocess mixture models, latent Dirichlet allocation, and a combinationrespectively. We then consider two applications: topic modeling and learning 3Dobject arrangements. Our experiments show that our M3 models achieve betterperformance using fewer topics than many classic topic models. We also observethat topics from the different dimensions of M3 models are meaningful andorthogonal to each other.
arxiv-1200-273 | Fast Planar Correlation Clustering for Image Segmentation | http://arxiv.org/pdf/1208.0378v1.pdf | author:Julian Yarkony, Alexander T. Ihler, Charless C. Fowlkes category:cs.CV cs.DS cs.LG stat.ML published:2012-08-02 summary:We describe a new optimization scheme for finding high-quality correlationclusterings in planar graphs that uses weighted perfect matching as asubroutine. Our method provides lower-bounds on the energy of the optimalcorrelation clustering that are typically fast to compute and tight inpractice. We demonstrate our algorithm on the problem of image segmentationwhere this approach outperforms existing global optimization techniques inminimizing the objective and is competitive with the state of the art inproducing high-quality segmentations.
arxiv-1200-274 | Artificial Neural Network Based Prediction of Optimal Pseudo-Damping and Meta-Damping in Oscillatory Fractional Order Dynamical Systems | http://arxiv.org/pdf/1208.0318v1.pdf | author:Saptarshi Das, Indranil Pan, Khrist Sur, Shantanu Das category:cs.SY cs.NE published:2012-08-01 summary:This paper investigates typical behaviors like damped oscillations infractional order (FO) dynamical systems. Such response occurs due to thepresence of, what is conceived as, pseudo-damping and meta-damping in somespecial class of FO systems. Here, approximation of such damped oscillation inFO systems with the conventional notion of integer order damping and timeconstant has been carried out using Genetic Algorithm (GA). Next, a multilayerfeed-forward Artificial Neural Network (ANN) has been trained using the GAbased results to predict the optimal pseudo and meta-damping from knowledge ofthe maximum order or number of terms in the FO dynamical system.
arxiv-1200-275 | The bistable brain: a neuronal model with symbiotic interactions | http://arxiv.org/pdf/1208.0223v1.pdf | author:Ricardo Lopez-Ruiz, Daniele Fournier-Prunaret category:nlin.CD cs.NE math.DS published:2012-08-01 summary:In general, the behavior of large and complex aggregates of elementarycomponents can not be understood nor extrapolated from the properties of a fewcomponents. The brain is a good example of this type of networked systems wheresome patterns of behavior are observed independently of the topology and of thenumber of coupled units. Following this insight, we have studied the dynamicsof different aggregates of logistic maps according to a particular {\itsymbiotic} coupling scheme that imitates the neuronal excitation coupling. Allthese aggregates show some common dynamical properties, concretely a bistablebehavior that is reported here with a certain detail. Thus, the qualitativerelationship with neural systems is suggested through a naive model of many ofsuch networked logistic maps whose behavior mimics the waking-sleepingbistability displayed by brain systems. Due to its relevance, some regions ofmultistability are determined and sketched for all these logistic models.
arxiv-1200-276 | Adaptation of pedagogical resources description standard (LOM) with the specificity of Arabic language | http://arxiv.org/pdf/1208.0200v1.pdf | author:Asma Boudhief, Mohsen Maraoui, Mounir Zrigui category:cs.CL published:2012-08-01 summary:In this article we focus firstly on the principle of pedagogical indexing andcharacteristics of Arabic language and secondly on the possibility of adaptingthe standard for describing learning resources used (the LOM and itsApplication Profiles) with learning conditions such as the educational levelsof students and their levels of understanding,... the educational context withtaking into account the representative elements of text, text length, ... inparticular, we put in relief the specificity of the Arabic language which is acomplex language, characterized by its flexion, its voyellation andagglutination.
arxiv-1200-277 | Oracle inequalities for computationally adaptive model selection | http://arxiv.org/pdf/1208.0129v1.pdf | author:Alekh Agarwal, Peter L. Bartlett, John C. Duchi category:stat.ML cs.LG published:2012-08-01 summary:We analyze general model selection procedures using penalized empirical lossminimization under computational constraints. While classical model selectionapproaches do not consider computational aspects of performing model selection,we argue that any practical model selection procedure must not only trade offestimation and approximation error, but also the computational effort requiredto compute empirical minimizers for different function classes. We provide aframework for analyzing such problems, and we give algorithms for modelselection under a computational budget. These algorithms satisfy oracleinequalities that show that the risk of the selected model is not much worsethan if we had devoted all of our omputational budget to the optimal functionclass.
arxiv-1200-278 | Ergodic Mirror Descent | http://arxiv.org/pdf/1105.4681v3.pdf | author:John C. Duchi, Alekh Agarwal, Mikael Johansson, Michael I. Jordan category:math.OC stat.ML published:2011-05-24 summary:We generalize stochastic subgradient descent methods to situations in whichwe do not receive independent samples from the distribution over which weoptimize, but instead receive samples that are coupled over time. We show thatas long as the source of randomness is suitably ergodic---it converges quicklyenough to a stationary distribution---the method enjoys strong convergenceguarantees, both in expectation and with high probability. This result hasimplications for stochastic optimization in high-dimensional spaces,peer-to-peer distributed optimization schemes, decision problems with dependentdata, and stochastic optimization problems over combinatorial spaces.
arxiv-1200-279 | Learning a peptide-protein binding affinity predictor with kernel ridge regression | http://arxiv.org/pdf/1207.7253v1.pdf | author:Sébastien Giguère, Mario Marchand, François Laviolette, Alexandre Drouin, Jacques Corbeil category:q-bio.QM cs.LG q-bio.BM stat.ML 92B05 published:2012-07-31 summary:We propose a specialized string kernel for small bio-molecules, peptides andpseudo-sequences of binding interfaces. The kernel incorporatesphysico-chemical properties of amino acids and elegantly generalize eightkernels, such as the Oligo, the Weighted Degree, the Blended Spectrum, and theRadial Basis Function. We provide a low complexity dynamic programmingalgorithm for the exact computation of the kernel and a linear time algorithmfor it's approximation. Combined with kernel ridge regression and SupCK, anovel binding pocket kernel, the proposed kernel yields biologically relevantand good prediction accuracy on the PepX database. For the first time, amachine learning predictor is capable of accurately predicting the bindingaffinity of any peptide to any protein. The method was also applied to bothsingle-target and pan-specific Major Histocompatibility Complex class IIbenchmark datasets and three Quantitative Structure Affinity Model benchmarkdatasets. On all benchmarks, our method significantly (p-value < 0.057) outperforms thecurrent state-of-the-art methods at predicting peptide-protein bindingaffinities. The proposed approach is flexible and can be applied to predict anyquantitative biological activity. The method should be of value to a largesegment of the research community with the potential to acceleratepeptide-based drug and vaccine development.
arxiv-1200-280 | On the information-theoretic structure of distributed measurements | http://arxiv.org/pdf/1107.1222v2.pdf | author:David Balduzzi category:cs.IT cs.DC cs.NE math.CT math.IT nlin.CG published:2011-07-06 summary:The internal structure of a measuring device, which depends on what itscomponents are and how they are organized, determines how it categorizes itsinputs. This paper presents a geometric approach to studying the internalstructure of measurements performed by distributed systems such asprobabilistic cellular automata. It constructs the quale, a family of sectionsof a suitably defined presheaf, whose elements correspond to the measurementsperformed by all subsystems of a distributed system. Using the quale wequantify (i) the information generated by a measurement; (ii) the extent towhich a measurement is context-dependent; and (iii) whether a measurement isdecomposable into independent submeasurements, which turns out to be equivalentto context-dependence. Finally, we show that only indecomposable measurementsare more informative than the sum of their submeasurements.
arxiv-1200-281 | PAC-Bayesian Inequalities for Martingales | http://arxiv.org/pdf/1110.6886v3.pdf | author:Yevgeny Seldin, François Laviolette, Nicolò Cesa-Bianchi, John Shawe-Taylor, Peter Auer category:cs.LG cs.IT math.IT stat.ML published:2011-10-31 summary:We present a set of high-probability inequalities that control theconcentration of weighted averages of multiple (possibly uncountably many)simultaneously evolving and interdependent martingales. Our results extend thePAC-Bayesian analysis in learning theory from the i.i.d. setting to martingalesopening the way for its application to importance weighted sampling,reinforcement learning, and other interactive learning domains, as well as manyother domains in probability theory and statistics, where martingales areencountered. We also present a comparison inequality that bounds the expectation of aconvex function of a martingale difference sequence shifted to the [0,1]interval by the expectation of the same function of independent Bernoullivariables. This inequality is applied to derive a tighter analog ofHoeffding-Azuma's inequality.
arxiv-1200-282 | The Divergence of Reinforcement Learning Algorithms with Value-Iteration and Function Approximation | http://arxiv.org/pdf/1107.4606v2.pdf | author:Michael Fairbank, Eduardo Alonso category:cs.LG published:2011-07-22 summary:This paper gives specific divergence examples of value-iteration for severalmajor Reinforcement Learning and Adaptive Dynamic Programming algorithms, whenusing a function approximator for the value function. These divergence examplesdiffer from previous divergence examples in the literature, in that they areapplicable for a greedy policy, i.e. in a "value iteration" scenario. Perhapssurprisingly, with a greedy policy, it is also possible to get divergence forthe algorithms TD(1) and Sarsa(1). In addition to these divergences, we alsoachieve divergence for the Adaptive Dynamic Programming algorithms HDP, DHP andGDHP.
arxiv-1200-283 | A Survey Of Activity Recognition And Understanding The Behavior In Video Survelliance | http://arxiv.org/pdf/1207.6774v1.pdf | author:A. R. Revathi, Dhananjay Kumar category:cs.CV published:2012-07-29 summary:This paper presents a review of human activity recognition and behaviourunderstanding in video sequence. The key objective of this paper is to providea general review on the overall process of a surveillance system used in thecurrent trend. Visual surveillance system is directed on automaticidentification of events of interest, especially on tracking and classificationof moving objects. The processing step of the video surveillance systemincludes the following stages: Surrounding model, object representation, objecttracking, activity recognition and behaviour understanding. It describestechniques that use to define a general set of activities that are applicableto a wide range of scenes and environments in video sequence.
arxiv-1200-284 | Universally Consistent Latent Position Estimation and Vertex Classification for Random Dot Product Graphs | http://arxiv.org/pdf/1207.6745v1.pdf | author:Daniel L. Sussman, Minh Tang, Carey E. Priebe category:stat.ML math.ST stat.TH published:2012-07-29 summary:In this work we show that, using the eigen-decomposition of the adjacencymatrix, we can consistently estimate latent positions for random dot productgraphs provided the latent positions are i.i.d. from some distribution. Ifclass labels are observed for a number of vertices tending to infinity, then weshow that the remaining vertices can be classified with error converging toBayes optimal using the $k$-nearest-neighbors classification rule. We evaluatethe proposed methods on simulated data and a graph derived from Wikipedia.
arxiv-1200-285 | Exploring Promising Stepping Stones by Combining Novelty Search with Interactive Evolution | http://arxiv.org/pdf/1207.6682v1.pdf | author:Brian G. Woolley, Kenneth O. Stanley category:cs.NE I.2.6 published:2012-07-28 summary:The field of evolutionary computation is inspired by the achievements ofnatural evolution, in which there is no final objective. Yet the pursuit ofobjectives is ubiquitous in simulated evolution. A significant problem is thatobjective approaches assume that intermediate stepping stones will increasinglyresemble the final objective when in fact they often do not. The consequence isthat while solutions may exist, searching for such objectives may not discoverthem. This paper highlights the importance of leveraging human insight duringsearch as an alternative to articulating explicit objectives. In particular, anew approach called novelty-assisted interactive evolutionary computation(NA-IEC) combines human intuition with novelty search for the first time tofacilitate the serendipitous discovery of agent behaviors. In this approach,the human user directs evolution by selecting what is interesting from theon-screen population of behaviors. However, unlike in typical IEC, the user cannow request that the next generation be filled with novel descendants. Theexperimental results demonstrate that combining human insight with noveltysearch finds solutions significantly faster and at lower genomic complexitiesthan fully-automated processes, including pure novelty search, suggesting animportant role for human users in the search for solutions.
arxiv-1200-286 | High Dimensional Semiparametric Gaussian Copula Graphical Models | http://arxiv.org/pdf/1202.2169v3.pdf | author:Han Liu, Fang Han, Ming Yuan, John Lafferty, Larry Wasserman category:stat.ML published:2012-02-10 summary:In this paper, we propose a semiparametric approach, named nonparanormalskeptic, for efficiently and robustly estimating high dimensional undirectedgraphical models. To achieve modeling flexibility, we consider Gaussian Copulagraphical models (or the nonparanormal) as proposed by Liu et al. (2009). Toachieve estimation robustness, we exploit nonparametric rank-based correlationcoefficient estimators, including Spearman's rho and Kendall's tau. In highdimensional settings, we prove that the nonparanormal skeptic achieves theoptimal parametric rate of convergence in both graph and parameter estimation.This celebrating result suggests that the Gaussian copula graphical models canbe used as a safe replacement of the popular Gaussian graphical models, evenwhen the data are truly Gaussian. Besides theoretical analysis, we also conductthorough numerical simulations to compare different estimators for their graphrecovery performance under both ideal and noisy settings. The proposed methodsare then applied on a large-scale genomic dataset to illustrate their empiricalusefulness. The R language software package huge implementing the proposedmethods is available on the Comprehensive R Archive Network: http://cran.r-project.org/.
arxiv-1200-287 | Supervised Laplacian Eigenmaps with Applications in Clinical Diagnostics for Pediatric Cardiology | http://arxiv.org/pdf/1207.7035v1.pdf | author:Thomas Perry, Hongyuan Zha, Patricio Frias, Dadan Zeng, Mark Braunstein category:cs.LG published:2012-07-27 summary:Electronic health records contain rich textual data which possess criticalpredictive information for machine-learning based diagnostic aids. However manytraditional machine learning methods fail to simultaneously integrate bothvector space data and text. We present a supervised method using Laplacianeigenmaps to augment existing machine-learning methods with low-dimensionalrepresentations of textual predictors which preserve the local similarities.The proposed implementation performs alternating optimization using gradientdescent. For the evaluation we applied our method to over 2,000 patient recordsfrom a large single-center pediatric cardiology practice to predict if patientswere diagnosed with cardiac disease. Our method was compared with latentsemantic indexing, latent Dirichlet allocation, and local Fisher discriminantanalysis. The results were assessed using AUC, MCC, specificity, andsensitivity. Results indicate supervised Laplacian eigenmaps was the highestperforming method in our study, achieving 0.782 and 0.374 for AUC and MCCrespectively. SLE showed an increase in 8.16% in AUC and 20.6% in MCC over thebaseline which excluded textual data and a 2.69% and 5.35% increase in AUC andMCC respectively over unsupervised Laplacian eigenmaps. This method allows manyexisting machine learning predictors to effectively and efficiently utilize thepotential of textual predictors.
arxiv-1200-288 | A New Training Algorithm for Kanerva's Sparse Distributed Memory | http://arxiv.org/pdf/1207.5774v3.pdf | author:Lou Marvin Caraig category:cs.CV cs.LG cs.NE published:2012-07-22 summary:The Sparse Distributed Memory proposed by Pentii Kanerva (SDM in short) wasthought to be a model of human long term memory. The architecture of the SDMpermits to store binary patterns and to retrieve them using partially matchingpatterns. However Kanerva's model is especially efficient only in handlingrandom data. The purpose of this article is to introduce a new approach oftraining Kanerva's SDM that can handle efficiently non-random data, and toprovide it the capability to recognize inverted patterns. This approach uses asignal model which is different from the one proposed for different purposes byHely, Willshaw and Hayes in [4]. This article additionally suggests a differentway of creating hard locations in the memory despite the Kanerva's staticmodel.
arxiv-1200-289 | Identifying Users From Their Rating Patterns | http://arxiv.org/pdf/1207.6379v1.pdf | author:José Bento, Nadia Fawaz, Andrea Montanari, Stratis Ioannidis category:cs.IR cs.LG stat.ML published:2012-07-26 summary:This paper reports on our analysis of the 2011 CAMRa Challenge dataset (Track2) for context-aware movie recommendation systems. The train dataset comprises4,536,891 ratings provided by 171,670 users on 23,974$ movies, as well as thehousehold groupings of a subset of the users. The test dataset comprises 5,450ratings for which the user label is missing, but the household label isprovided. The challenge required to identify the user labels for the ratings inthe test set. Our main finding is that temporal information (time labels of theratings) is significantly more useful for achieving this objective than theuser preferences (the actual ratings). Using a model that leverages on thisfact, we are able to identify users within a known household with an accuracyof approximately 96% (i.e. misclassification rate around 4%).
arxiv-1200-290 | A novel Hopfield neural network approach for minimizing total weighted tardiness of jobs scheduled on identical machines | http://arxiv.org/pdf/1208.4583v1.pdf | author:N. Fogarasi, K. Tornai, J. Levendovszky category:cs.NE 90C27 G.1.6 published:2012-07-26 summary:This paper explores fast, polynomial time heuristic approximate solutions tothe NP-hard problem of scheduling jobs on N identical machines. The jobs areindependent and are allowed to be stopped and restarted on another machine at alater time. They have well-de?ned deadlines, and relative priorities quantifiedby non-negative real weights. The objective is to find schedules which minimizethe total weighted tardiness (TWT) of all jobs. We show how this problem can bemapped into quadratic form and present a polynomial time heuristic solutionbased on the Hop?eld Neural Network (HNN) approach. It is demonstrated, throughthe results of extensive numerical simulations, that this solution outperformsother popular heuristic methods. The proposed heuristic is both theoreticallyand empirically shown to be scalable to large problem sizes (over 100 jobs tobe scheduled), which makes it applicable to grid computing scheduling, arisingin fields such as computational biology, chemistry and finance.
arxiv-1200-291 | On When and How to use SAT to Mine Frequent Itemsets | http://arxiv.org/pdf/1207.6253v1.pdf | author:Rui Henriques, Inês Lynce, Vasco Manquinho category:cs.AI cs.DB cs.LG published:2012-07-26 summary:A new stream of research was born in the last decade with the goal of miningitemsets of interest using Constraint Programming (CP). This has promoted anatural way to combine complex constraints in a highly flexible manner.Although CP state-of-the-art solutions formulate the task using Booleanvariables, the few attempts to adopt propositional Satisfiability (SAT)provided an unsatisfactory performance. This work deepens the study on when andhow to use SAT for the frequent itemset mining (FIM) problem by definingdifferent encodings with multiple task-driven enumeration options and searchstrategies. Although for the majority of the scenarios SAT-based solutionsappear to be non-competitive with CP peers, results show a variety ofinteresting cases where SAT encodings are the best option.
arxiv-1200-292 | Fast global convergence of gradient methods for high-dimensional statistical recovery | http://arxiv.org/pdf/1104.4824v3.pdf | author:Alekh Agarwal, Sahand N. Negahban, Martin J. Wainwright category:stat.ML cs.IT math.IT published:2011-04-25 summary:Many statistical $M$-estimators are based on convex optimization problemsformed by the combination of a data-dependent loss function with a norm-basedregularizer. We analyze the convergence rates of projected gradient andcomposite gradient methods for solving such problems, working within ahigh-dimensional framework that allows the data dimension $\pdim$ to grow with(and possibly exceed) the sample size $\numobs$. This high-dimensionalstructure precludes the usual global assumptions---namely, strong convexity andsmoothness conditions---that underlie much of classical optimization analysis.We define appropriately restricted versions of these conditions, and show thatthey are satisfied with high probability for various statistical models. Underthese conditions, our theory guarantees that projected gradient descent has aglobally geometric rate of convergence up to the \emph{statistical precision}of the model, meaning the typical distance between the true unknown parameter$\theta^*$ and an optimal solution $\hat{\theta}$. This result is substantiallysharper than previous convergence results, which yielded sublinear convergence,or linear convergence only up to the noise level. Our analysis applies to awide range of $M$-estimators and statistical models, including sparse linearregression using Lasso ($\ell_1$-regularized regression); group Lasso for blocksparsity; log-linear models with regularization; low-rank matrix recovery usingnuclear norm regularization; and matrix decomposition. Overall, our analysisreveals interesting connections between statistical precision and computationalefficiency in high-dimensional estimation.
arxiv-1200-293 | Optimal Sampling Points in Reproducing Kernel Hilbert Spaces | http://arxiv.org/pdf/1207.5871v1.pdf | author:Rui Wang, Haizhang Zhang category:cs.IT math.IT stat.ML published:2012-07-25 summary:The recent developments of basis pursuit and compressed sensing seek toextract information from as few samples as possible. In such applications,since the number of samples is restricted, one should deploy the samplingpoints wisely. We are motivated to study the optimal distribution of finitesampling points. Formulation under the framework of optimal reconstructionyields a minimization problem. In the discrete case, we estimate the distancebetween the optimal subspace resulting from a general Karhunen-Loeve transformand the kernel space to obtain another algorithm that is computationallyfavorable. Numerical experiments are then presented to illustrate theperformance of the algorithms for the searching of optimal sampling points.
arxiv-1200-294 | Improved Bound for the Nystrom's Method and its Application to Kernel Classification | http://arxiv.org/pdf/1111.2262v4.pdf | author:Rong Jin, Tianbao Yang, Mehrdad Mahdavi, Yu-Feng Li, Zhi-Hua Zhou category:cs.LG cs.NA published:2011-11-09 summary:We develop two approaches for analyzing the approximation error bound for theNystr\"{o}m method, one based on the concentration inequality of integraloperator, and one based on the compressive sensing theory. We show that theapproximation error, measured in the spectral norm, can be improved from$O(N/\sqrt{m})$ to $O(N/m^{1 - \rho})$ in the case of large eigengap, where $N$is the total number of data points, $m$ is the number of sampled data points,and $\rho \in (0, 1/2)$ is a positive constant that characterizes the eigengap.When the eigenvalues of the kernel matrix follow a $p$-power law, our analysisbased on compressive sensing theory further improves the bound to $O(N/m^{p -1})$ under an incoherence assumption, which explains why the Nystr\"{o}m methodworks well for kernel matrix with skewed eigenvalues. We present a kernelclassification approach based on the Nystr\"{o}m method and derive itsgeneralization performance using the improved bound. We show that when theeigenvalues of kernel matrix follow a $p$-power law, we can reduce the numberof support vectors to $N^{2p/(p^2 - 1)}$, a number less than $N$ when $p >1+\sqrt{2}$, without seriously sacrificing its generalization performance.
arxiv-1200-295 | Conditional mean embeddings as regressors - supplementary | http://arxiv.org/pdf/1205.4656v2.pdf | author:Steffen Grünewälder, Guy Lever, Luca Baldassarre, Sam Patterson, Arthur Gretton, Massimilano Pontil category:cs.LG stat.ML published:2012-05-21 summary:We demonstrate an equivalence between reproducing kernel Hilbert space (RKHS)embeddings of conditional distributions and vector-valued regressors. Thisconnection introduces a natural regularized loss function which the RKHSembeddings minimise, providing an intuitive understanding of the embeddings anda justification for their use. Furthermore, the equivalence allows theapplication of vector-valued regression methods and results to the problem oflearning conditional distributions. Using this link we derive a sparse versionof the embedding by considering alternative formulations. Further, by applyingconvergence results for vector-valued regression to the embedding problem wederive minimax convergence rates which are O(\log(n)/n) -- compared to currentstate of the art rates of O(n^{-1/4}) -- and are valid under milder and moreintuitive assumptions. These minimax upper rates coincide with lower rates upto a logarithmic factor, showing that the embedding method achieves nearlyoptimal rates. We study our sparse embedding algorithm in a reinforcementlearning task where the algorithm shows significant improvement in sparsityover an incomplete Cholesky decomposition.
arxiv-1200-296 | VOI-aware MCTS | http://arxiv.org/pdf/1207.5589v1.pdf | author:David Tolpin, Solomon Eyal Shimony category:cs.AI cs.LG published:2012-07-24 summary:UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in gamesand Markov decision processes, is based on UCB1, a sampling policy for theMulti-armed Bandit problem (MAB) that minimizes the cumulative regret. However,search differs from MAB in that in MCTS it is usually only the final "arm pull"(the actual move selection) that collects a reward, rather than all "armpulls". In this paper, an MCTS sampling policy based on Value of Information(VOI) estimates of rollouts is suggested. Empirical evaluation of the policyand comparison to UCB1 and UCT is performed on random MAB instances as well ason Computer Go.
arxiv-1200-297 | Evolving Musical Counterpoint: The Chronopoint Musical Evolution System | http://arxiv.org/pdf/1207.5560v1.pdf | author:Jeffrey Power Jacobs, James Reggia category:cs.SD cs.AI cs.NE published:2012-07-23 summary:Musical counterpoint, a musical technique in which two or more independentmelodies are played simultaneously with the goal of creating harmony, has beenaround since the baroque era. However, to our knowledge computationalgeneration of aesthetically pleasing linear counterpoint based on subjectivefitness assessment has not been explored by the evolutionary computationcommunity (although generation using objective fitness has been attempted inquite a few cases). The independence of contrapuntal melodies and thesubjective nature of musical aesthetics provide an excellent platform for theapplication of genetic algorithms. In this paper, a genetic algorithm approachto generating contrapuntal melodies is explained, with a description of thevarious musical heuristics used and of how variable-length chromosome stringsare used to avoid generating "jerky" rhythms and melodic phrases, as well ashow subjectivity is incorporated into the algorithm's fitness measures. Next,results from empirical testing of the algorithm are presented, with a focus onhow a user's musical sophistication influences their experience. Lastly,further musical and compositional applications of the algorithm are discussedalong with planned future work on the algorithm.
arxiv-1200-298 | A note on the lack of symmetry in the graphical lasso | http://arxiv.org/pdf/1111.2667v2.pdf | author:Benjamin T. Rolfs, Bala Rajaratnam category:stat.ML stat.CO published:2011-11-11 summary:The graphical lasso (glasso) is a widely-used fast algorithm for estimatingsparse inverse covariance matrices. The glasso solves an L1 penalized maximumlikelihood problem and is available as an R library on CRAN. The output fromthe glasso, a regularized covariance matrix estimate a sparse inversecovariance matrix estimate, not only identify a graphical model but can alsoserve as intermediate inputs into multivariate procedures such as PCA, LDA,MANOVA, and others. The glasso indeed produces a covariance matrix estimatewhich solves the L1 penalized optimization problem in a dual sense; however,the method for producing the inverse covariance matrix estimator after thisoptimization is inexact and may produce asymmetric estimates. This problem isexacerbated when the amount of L1 regularization that is applied is small,which in turn is more likely to occur if the true underlying inverse covariancematrix is not sparse. The lack of symmetry can potentially have consequences.First, it implies that the covariance and inverse covariance estimates are notnumerical inverses of one another, and second, asymmetry can possibly lead tonegative or complex eigenvalues,rendering many multivariate procedures whichmay depend on the inverse covariance estimator unusable. We demonstrate thisproblem, explain its causes, and propose possible remedies.
arxiv-1200-299 | MCTS Based on Simple Regret | http://arxiv.org/pdf/1207.5536v1.pdf | author:David Tolpin, Solomon Eyal Shimony category:cs.AI cs.LG published:2012-07-23 summary:UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in gamesand Markov decision processes, is based on UCB, a sampling policy for theMulti-armed Bandit problem (MAB) that minimizes the cumulative regret. However,search differs from MAB in that in MCTS it is usually only the final "arm pull"(the actual move selection) that collects a reward, rather than all "armpulls". Therefore, it makes more sense to minimize the simple regret, asopposed to the cumulative regret. We begin by introducing policies formulti-armed bandits with lower finite-time and asymptotic simple regret thanUCB, using it to develop a two-stage scheme (SR+CR) for MCTS which outperformsUCT empirically. Optimizing the sampling process is itself a metareasoning problem, a solutionof which can use value of information (VOI) techniques. Although the theory ofVOI for search exists, applying it to MCTS is non-trivial, as typical myopicassumptions fail. Lacking a complete working VOI theory for MCTS, wenevertheless propose a sampling scheme that is "aware" of VOI, achieving analgorithm that in empirical evaluation outperforms both UCT and the otherproposed algorithms.
arxiv-1200-300 | Nonlinear spectral unmixing of hyperspectral images using Gaussian processes | http://arxiv.org/pdf/1207.5451v1.pdf | author:Yoann Altmann, Nicolas Dobigeon, Steve McLaughlin, Jean-Yves Tourneret category:stat.ML stat.AP published:2012-07-23 summary:This paper presents an unsupervised algorithm for nonlinear unmixing ofhyperspectral images. The proposed model assumes that the pixel reflectancesresult from a nonlinear function of the abundance vectors associated with thepure spectral components. We assume that the spectral signatures of the purecomponents and the nonlinear function are unknown. The first step of theproposed method consists of the Bayesian estimation of the abundance vectorsfor all the image pixels and the nonlinear function relating the abundancevectors to the observations. The endmembers are subsequently estimated usingGaussian process regression. The performance of the unmixing strategy isevaluated with simulations conducted on synthetic and real data.
