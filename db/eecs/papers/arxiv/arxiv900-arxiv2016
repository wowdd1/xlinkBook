arxiv-0912-4884 | An Invariance Principle for Polytopes | http://arxiv.org/abs/0912.4884 | id:0912.4884 author:Prahladh Harsha, Adam Klivans, Raghu Meka category:cs.CC cs.CG cs.DM cs.LG math.PR  published:2009-12-24 summary:Let X be randomly chosen from {-1,1}^n, and let Y be randomly chosen from the standard spherical Gaussian on R^n. For any (possibly unbounded) polytope P formed by the intersection of k halfspaces, we prove that Pr [X belongs to P] - Pr [Y belongs to P] < log^{8/5}k * Delta, where Delta is a parameter that is small for polytopes formed by the intersection of "regular" halfspaces (i.e., halfspaces with low influence). The novelty of our invariance principle is the polylogarithmic dependence on k. Previously, only bounds that were at least linear in k were known. We give two important applications of our main result: (1) A polylogarithmic in k bound on the Boolean noise sensitivity of intersections of k "regular" halfspaces (previous work gave bounds linear in k). (2) A pseudorandom generator (PRG) with seed length O((log n)*poly(log k,1/delta)) that delta-fools all polytopes with k faces with respect to the Gaussian distribution. We also obtain PRGs with similar parameters that fool polytopes formed by intersection of regular halfspaces over the hypercube. Using our PRG constructions, we obtain the first deterministic quasi-polynomial time algorithms for approximately counting the number of solutions to a broad class of integer programs, including dense covering problems and contingency tables. version:2
arxiv-1209-2717 | Comparison Study for Clonal Selection Algorithm and Genetic Algorithm | http://arxiv.org/abs/1209.2717 | id:1209.2717 author:Ezgi Deniz Ulker, Sadik Ulker category:cs.NE  published:2012-09-12 summary:Two metaheuristic algorithms namely Artificial Immune Systems (AIS) and Genetic Algorithms are classified as computational systems inspired by theoretical immunology and genetics mechanisms. In this work we examine the comparative performances of two algorithms. A special selection algorithm, Clonal Selection Algorithm (CLONALG), which is a subset of Artificial Immune Systems, and Genetic Algorithms are tested with certain benchmark functions. It is shown that depending on type of a function Clonal Selection Algorithm and Genetic Algorithm have better performance over each other. version:1
arxiv-1209-2696 | Visual Tracking with Similarity Matching Ratio | http://arxiv.org/abs/1209.2696 | id:1209.2696 author:Aysegul Dundar, Jonghoon Jin, Eugenio Culurciello category:cs.CV cs.RO  published:2012-09-12 summary:This paper presents a novel approach to visual tracking: Similarity Matching Ratio (SMR). The traditional approach of tracking is minimizing some measures of the difference between the template and a patch from the frame. This approach is vulnerable to outliers and drastic appearance changes and an extensive study is focusing on making the approach more tolerant to them. However, this often results in longer, corrective algo- rithms which do not solve the original problem. This paper proposes a novel approach to the definition of the tracking problems, SMR, which turns the differences into a probability measure. Only pixel differences below a threshold count towards deciding the match, the rest are ignored. This approach makes the SMR tracker robust to outliers and points that dramaticaly change appearance. The SMR tracker is tested on challenging video sequences and achieved state-of-the-art performance. version:1
arxiv-1209-2693 | Regret Bounds for Restless Markov Bandits | http://arxiv.org/abs/1209.2693 | id:1209.2693 author:Ronald Ortner, Daniil Ryabko, Peter Auer, Rémi Munos category:cs.LG math.OC stat.ML  published:2012-09-12 summary:We consider the restless Markov bandit problem, in which the state of each arm evolves according to a Markov process independently of the learner's actions. We suggest an algorithm that after $T$ steps achieves $\tilde{O}(\sqrt{T})$ regret with respect to the best policy that knows the distributions of all arms. No assumptions on the Markov chains are made except that they are irreducible. In addition, we show that index-based policies are necessarily suboptimal for the considered problem. version:1
arxiv-0909-2017 | Sparsity and `Something Else': An Approach to Encrypted Image Folding | http://arxiv.org/abs/0909.2017 | id:0909.2017 author:James Bowley, Laura Rebollo-Neira category:cs.CV cs.IT math.IT  published:2009-09-10 summary:A property of sparse representations in relation to their capacity for information storage is discussed. It is shown that this feature can be used for an application that we term Encrypted Image Folding. The proposed procedure is realizable through any suitable transformation. In particular, in this paper we illustrate the approach by recourse to the Discrete Cosine Transform and a combination of redundant Cosine and Dirac dictionaries. The main advantage of the proposed technique is that both storage and encryption can be achieved simultaneously using simple processing steps. version:5
arxiv-1209-2657 | Sparse Representation of Astronomical Images | http://arxiv.org/abs/1209.2657 | id:1209.2657 author:Laura Rebollo-Neira, James Bowley category:math-ph cs.CV math.MP  published:2012-09-12 summary:Sparse representation of astronomical images is discussed. It is shown that a significant gain in sparsity is achieved when particular mixed dictionaries are used for approximating these types of images with greedy selection strategies. Experiments are conducted to confirm: i)Effectiveness at producing sparse representations. ii)Competitiveness, with respect to the time required to process large images.The latter is a consequence of the suitability of the proposed dictionaries for approximating images in partitions of small blocks.This feature makes it possible to apply the effective greedy selection technique Orthogonal Matching Pursuit, up to some block size. For blocks exceeding that size a refinement of the original Matching Pursuit approach is considered. The resulting method is termed Self Projected Matching Pursuit, because is shown to be effective for implementing, via Matching Pursuit itself, the optional back-projection intermediate steps in that approach. version:1
arxiv-1209-2655 | Positivity and Transportation | http://arxiv.org/abs/1209.2655 | id:1209.2655 author:Marco Cuturi category:stat.ML math.CO  published:2012-09-12 summary:We prove in this paper that the weighted volume of the set of integral transportation matrices between two integral histograms r and c of equal sum is a positive definite kernel of r and c when the set of considered weights forms a positive definite matrix. The computation of this quantity, despite being the subject of a significant research effort in algebraic statistics, remains an intractable challenge for histograms of even modest dimensions. We propose an alternative kernel which, rather than considering all matrices of the transportation polytope, only focuses on a sub-sample of its vertices known as its Northwestern corner solutions. The resulting kernel is positive definite and can be computed with a number of operations O(R^2d) that grows linearly in the complexity of the dimension d, where R^2, the total amount of sampled vertices, is a parameter that controls the complexity of the kernel. version:1
arxiv-1209-2620 | Probabilities on Sentences in an Expressive Logic | http://arxiv.org/abs/1209.2620 | id:1209.2620 author:Marcus Hutter, John W. Lloyd, Kee Siong Ng, William T. B. Uther category:cs.LO cs.AI cs.LG math.LO math.PR  published:2012-09-12 summary:Automated reasoning about uncertain knowledge has many applications. One difficulty when developing such systems is the lack of a completely satisfactory integration of logic and probability. We address this problem directly. Expressive languages like higher-order logic are ideally suited for representing and reasoning about structured knowledge. Uncertain knowledge can be modeled by using graded probabilities rather than binary truth-values. The main technical problem studied in this paper is the following: Given a set of sentences, each having some probability of being true, what probability should be ascribed to other (query) sentences? A natural wish-list, among others, is that the probability distribution (i) is consistent with the knowledge base, (ii) allows for a consistent inference procedure and in particular (iii) reduces to deductive logic in the limit of probabilities being 0 and 1, (iv) allows (Bayesian) inductive reasoning and (v) learning in the limit and in particular (vi) allows confirmation of universally quantified hypotheses/sentences. We translate this wish-list into technical requirements for a prior probability and show that probabilities satisfying all our criteria exist. We also give explicit constructions and several general characterizations of probabilities that satisfy some or all of the criteria and various (counter) examples. We also derive necessary and sufficient conditions for extending beliefs about finitely many sentences to suitable probabilities over all sentences, and in particular least dogmatic or least biased ones. We conclude with a brief outlook on how the developed theory might be used and approximated in autonomous reasoning agents. Our theory is a step towards a globally consistent and empirically satisfactory unification of probability and logic. version:1
arxiv-1209-2948 | Cultural Algorithm Toolkit for Multi-objective Rule Mining | http://arxiv.org/abs/1209.2948 | id:1209.2948 author:Sujatha Srinivasan, Sivakumar Ramakrishnan category:cs.NE cs.AI  published:2012-09-12 summary:Cultural algorithm is a kind of evolutionary algorithm inspired from societal evolution and is composed of a belief space, a population space and a protocol that enables exchange of knowledge between these sources. Knowledge created in the population space is accepted into the belief space while this collective knowledge from these sources is combined to influence the decisions of the individual agents in solving problems. Classification rules comes under descriptive knowledge discovery in data mining and are the most sought out by users since they represent highly comprehensible form of knowledge. The rules have certain properties which make them useful forms of actionable knowledge to users. The rules are evaluated using these properties namely the rule metrics. In the current study a Cultural Algorithm Toolkit for Classification Rule Mining (CAT-CRM) is proposed which allows the user to control three different set of parameters namely the evolutionary parameters, the rule parameters as well as agent parameters and hence can be used for experimenting with an evolutionary system, a rule mining system or an agent based social system. Results of experiments conducted to observe the effect of different number and type of metrics on the performance of the algorithm on bench mark data sets is reported. version:1
arxiv-1209-2548 | Training a Feed-forward Neural Network with Artificial Bee Colony Based Backpropagation Method | http://arxiv.org/abs/1209.2548 | id:1209.2548 author:Sudarshan Nandy, Partha Pratim Sarkar, Achintya Das category:cs.NE cs.AI  published:2012-09-12 summary:Back-propagation algorithm is one of the most widely used and popular techniques to optimize the feed forward neural network training. Nature inspired meta-heuristic algorithms also provide derivative-free solution to optimize complex problem. Artificial bee colony algorithm is a nature inspired meta-heuristic algorithm, mimicking the foraging or food source searching behaviour of bees in a bee colony and this algorithm is implemented in several applications for an improved optimized outcome. The proposed method in this paper includes an improved artificial bee colony algorithm based back-propagation neural network training method for fast and improved convergence rate of the hybrid neural network learning method. The result is analysed with the genetic algorithm based back-propagation method, and it is another hybridized procedure of its kind. Analysis is performed over standard data sets, reflecting the light of efficiency of proposed method in terms of convergence speed and rate. version:1
arxiv-1208-0228 | Initial Version of State Transition Algorithm | http://arxiv.org/abs/1208.0228 | id:1208.0228 author:Xiaojun Zhou, Chunhua Yang, Weihua Gui category:math.OC cs.NE  published:2012-08-01 summary:In terms of the concepts of state and state transition, a new algorithm-State Transition Algorithm (STA) is proposed in order to probe into classical and intelligent optimization algorithms. On the basis of state and state transition, it becomes much simpler and easier to understand. As for continuous function optimization problems, three special operators named rotation, translation and expansion are presented. While for discrete function optimization problems, an operator called general elementary transformation is introduced. Finally, with 4 common benchmark continuous functions and a discrete problem used to test the performance of STA, the experiment shows that STA is a promising algorithm due to its good search capability. version:2
arxiv-1209-2515 | Wavelet Based Image Coding Schemes : A Recent Survey | http://arxiv.org/abs/1209.2515 | id:1209.2515 author:V. J. Rehna, M. K. Jeya Kumar category:cs.CV  published:2012-09-12 summary:A variety of new and powerful algorithms have been developed for image compression over the years. Among them the wavelet-based image compression schemes have gained much popularity due to their overlapping nature which reduces the blocking artifacts that are common phenomena in JPEG compression and multiresolution character which leads to superior energy compaction with high quality reconstructed images. This paper provides a detailed survey on some of the popular wavelet coding techniques such as the Embedded Zerotree Wavelet (EZW) coding, Set Partitioning in Hierarchical Tree (SPIHT) coding, the Set Partitioned Embedded Block (SPECK) Coder, and the Embedded Block Coding with Optimized Truncation (EBCOT) algorithm. Other wavelet-based coding techniques like the Wavelet Difference Reduction (WDR) and the Adaptive Scanned Wavelet Difference Reduction (ASWDR) algorithms, the Space Frequency Quantization (SFQ) algorithm, the Embedded Predictive Wavelet Image Coder (EPWIC), Compression with Reversible Embedded Wavelet (CREW), the Stack-Run (SR) coding and the recent Geometric Wavelet (GW) coding are also discussed. Based on the review, recommendations and discussions are presented for algorithm development and implementation. version:1
arxiv-1209-2295 | Multimodal diffusion geometry by joint diagonalization of Laplacians | http://arxiv.org/abs/1209.2295 | id:1209.2295 author:Davide Eynard, Klaus Glashoff, Michael M. Bronstein, Alexander M. Bronstein category:cs.CV cs.AI  published:2012-09-11 summary:We construct an extension of diffusion geometry to multiple modalities through joint approximate diagonalization of Laplacian matrices. This naturally extends classical data analysis tools based on spectral geometry, such as diffusion maps and spectral clustering. We provide several synthetic and real examples of manifold learning, retrieval, and clustering demonstrating that the joint diffusion geometry frequently better captures the inherent structure of multi-modal data. We also show that many previous attempts to construct multimodal spectral clustering can be seen as particular cases of joint approximate diagonalization of the Laplacians. version:2
arxiv-1209-2501 | Performance Evaluation of Predictive Classifiers For Knowledge Discovery From Engineering Materials Data Sets | http://arxiv.org/abs/1209.2501 | id:1209.2501 author:Hemanth K. S Doreswamy category:cs.LG  published:2012-09-12 summary:In this paper, naive Bayesian and C4.5 Decision Tree Classifiers(DTC) are successively applied on materials informatics to classify the engineering materials into different classes for the selection of materials that suit the input design specifications. Here, the classifiers are analyzed individually and their performance evaluation is analyzed with confusion matrix predictive parameters and standard measures, the classification results are analyzed on different class of materials. Comparison of classifiers has found that naive Bayesian classifier is more accurate and better than the C4.5 DTC. The knowledge discovered by the naive bayesian classifier can be employed for decision making in materials selection in manufacturing industries. version:1
arxiv-1209-2434 | Query Complexity of Derivative-Free Optimization | http://arxiv.org/abs/1209.2434 | id:1209.2434 author:Kevin G. Jamieson, Robert D. Nowak, Benjamin Recht category:stat.ML cs.LG  published:2012-09-11 summary:This paper provides lower bounds on the convergence rate of Derivative Free Optimization (DFO) with noisy function evaluations, exposing a fundamental and unavoidable gap between the performance of algorithms with access to gradients and those with access to only function evaluations. However, there are situations in which DFO is unavoidable, and for such situations we propose a new DFO algorithm that is proved to be near optimal for the class of strongly convex objective functions. A distinctive feature of the algorithm is that it uses only Boolean-valued function comparisons, rather than function evaluations. This makes the algorithm useful in an even wider range of applications, such as optimization based on paired comparisons from human subjects, for example. We also show that regardless of whether DFO is based on noisy function evaluations or Boolean-valued function comparisons, the convergence rate is the same. version:1
arxiv-1209-2400 | Identification of Fertile Translations in Medical Comparable Corpora: a Morpho-Compositional Approach | http://arxiv.org/abs/1209.2400 | id:1209.2400 author:Estelle Delpech, Béatrice Daille, Emmanuel Morin, Claire Lemaire category:cs.CL  published:2012-09-11 summary:This paper defines a method for lexicon in the biomedical domain from comparable corpora. The method is based on compositional translation and exploits morpheme-level translation equivalences. It can generate translations for a large variety of morphologically constructed words and can also generate 'fertile' translations. We show that fertile translations increase the overall quality of the extracted lexicon for English to French translation. version:1
arxiv-1203-4523 | On the Equivalence between Herding and Conditional Gradient Algorithms | http://arxiv.org/abs/1203.4523 | id:1203.4523 author:Francis Bach, Simon Lacoste-Julien, Guillaume Obozinski category:cs.LG math.OC stat.ML  published:2012-03-20 summary:We show that the herding procedure of Welling (2009) takes exactly the form of a standard convex optimization algorithm--namely a conditional gradient algorithm minimizing a quadratic moment discrepancy. This link enables us to invoke convergence results from convex optimization and to consider faster alternatives for the task of approximating integrals in a reproducing kernel Hilbert space. We study the behavior of the different variants through numerical simulations. The experiments indicate that while we can improve over herding on the task of approximating integrals, the original herding algorithm tends to approach more often the maximum entropy distribution, shedding more light on the learning bias behind herding. version:2
arxiv-1209-2163 | Modeling controversies in the press: the case of the abnormal bees' death | http://arxiv.org/abs/1209.2163 | id:1209.2163 author:Alexandre Delanoë, Serge Galam category:physics.soc-ph cs.CL  published:2012-09-10 summary:The controversy about the cause(s) of abnormal death of bee colonies in France is investigated through an extensive analysis of the french speaking press. A statistical analysis of textual data is first performed on the lexicon used by journalists to describe the facts and to present associated informations during the period 1998-2010. Three states are identified to explain the phenomenon. The first state asserts a unique cause, the second one focuses on multifactor causes and the third one states the absence of current proof. Assigning each article to one of the three states, we are able to follow the associated opinion dynamics among the journalists over 13 years. Then, we apply the Galam sequential probabilistic model of opinion dynamic to those data. Assuming journalists are either open mind or inflexible about their respective opinions, the results are reproduced precisely provided we account for a series of annual changes in the proportions of respective inflexibles. The results shed a new counter intuitive light on the various pressure supposed to apply on the journalists by either chemical industries or beekeepers and experts or politicians. The obtained dynamics of respective inflexibles shows the possible effect of lobbying, the inertia of the debate and the net advantage gained by the first whistleblowers. version:1
arxiv-1110-5454 | Distance Dependent Infinite Latent Feature Models | http://arxiv.org/abs/1110.5454 | id:1110.5454 author:Samuel J. Gershman, Peter I. Frazier, David M. Blei category:stat.ML math.ST stat.TH  published:2011-10-25 summary:Latent feature models are widely used to decompose data into a small number of components. Bayesian nonparametric variants of these models, which use the Indian buffet process (IBP) as a prior over latent features, allow the number of features to be determined from the data. We present a generalization of the IBP, the distance dependent Indian buffet process (dd-IBP), for modeling non-exchangeable data. It relies on distances defined between data points, biasing nearby data to share more features. The choice of distance measure allows for many kinds of dependencies, including temporal and spatial. Further, the original IBP is a special case of the dd-IBP. In this paper, we develop the dd-IBP and theoretically characterize its feature-sharing properties. We derive a Markov chain Monte Carlo sampler for a linear Gaussian model with a dd-IBP prior and study its performance on several non-exchangeable data sets. version:2
arxiv-1209-1996 | A Bayesian Boosting Model | http://arxiv.org/abs/1209.1996 | id:1209.1996 author:Alexander Lorbert, David M. Blei, Robert E. Schapire, Peter J. Ramadge category:stat.ML  published:2012-09-10 summary:We offer a novel view of AdaBoost in a statistical setting. We propose a Bayesian model for binary classification in which label noise is modeled hierarchically. Using variational inference to optimize a dynamic evidence lower bound, we derive a new boosting-like algorithm called VIBoost. We show its close connections to AdaBoost and give experimental results from four datasets. version:1
arxiv-1206-4481 | Parsimonious Mahalanobis Kernel for the Classification of High Dimensional Data | http://arxiv.org/abs/1206.4481 | id:1206.4481 author:M. Fauvel, A. Villa, J. Chanussot, J. A. Benediktsson category:cs.NA cs.LG  published:2012-06-20 summary:The classification of high dimensional data with kernel methods is considered in this article. Exploit- ing the emptiness property of high dimensional spaces, a kernel based on the Mahalanobis distance is proposed. The computation of the Mahalanobis distance requires the inversion of a covariance matrix. In high dimensional spaces, the estimated covariance matrix is ill-conditioned and its inversion is unstable or impossible. Using a parsimonious statistical model, namely the High Dimensional Discriminant Analysis model, the specific signal and noise subspaces are estimated for each considered class making the inverse of the class specific covariance matrix explicit and stable, leading to the definition of a parsimonious Mahalanobis kernel. A SVM based framework is used for selecting the hyperparameters of the parsimonious Mahalanobis kernel by optimizing the so-called radius-margin bound. Experimental results on three high dimensional data sets show that the proposed kernel is suitable for classifying high dimensional data, providing better classification accuracies than the conventional Gaussian kernel. version:2
arxiv-1209-1960 | A Comparative Study of Efficient Initialization Methods for the K-Means Clustering Algorithm | http://arxiv.org/abs/1209.1960 | id:1209.1960 author:M. Emre Celebi, Hassan A. Kingravi, Patricio A. Vela category:cs.LG cs.CV I.5.3; H.2.8  published:2012-09-10 summary:K-means is undoubtedly the most widely used partitional clustering algorithm. Unfortunately, due to its gradient descent nature, this algorithm is highly sensitive to the initial placement of the cluster centers. Numerous initialization methods have been proposed to address this problem. In this paper, we first present an overview of these methods with an emphasis on their computational efficiency. We then compare eight commonly used linear time complexity initialization methods on a large and diverse collection of data sets using various performance criteria. Finally, we analyze the experimental results using non-parametric statistical tests and provide recommendations for practitioners. We demonstrate that popular initialization methods often perform poorly and that there are in fact strong alternatives to these methods. version:1
arxiv-1209-1826 | A spatio-spectral hybridization for edge preservation and noisy image restoration via local parametric mixtures and Lagrangian relaxation | http://arxiv.org/abs/1209.1826 | id:1209.1826 author:Kinjal Basu, Debapriya Sengupta category:stat.ME cs.CV stat.AP  published:2012-09-09 summary:This paper investigates a fully unsupervised statistical method for edge preserving image restoration and compression using a spatial decomposition scheme. Smoothed maximum likelihood is used for local estimation of edge pixels from mixture parametric models of local templates. For the complementary smooth part the traditional L2-variational problem is solved in the Fourier domain with Thin Plate Spline (TPS) regularization. It is well known that naive Fourier compression of the whole image fails to restore a piece-wise smooth noisy image satisfactorily due to Gibbs phenomenon. Images are interpreted as relative frequency histograms of samples from bi-variate densities where the sample sizes might be unknown. The set of discontinuities is assumed to be completely unsupervised Lebesgue-null, compact subset of the plane in the continuous formulation of the problem. Proposed spatial decomposition uses a widely used topological concept, partition of unity. The decision on edge pixel neighborhoods are made based on the multiple testing procedure of Holms. Statistical summary of the ?final output is decomposed into two layers of information extraction, one for the subset of edge pixels and the other for the smooth region. Robustness is also demonstrated by applying the technique on noisy degradation of clean images. version:1
arxiv-1209-1800 | An Empirical Study of MAUC in Multi-class Problems with Uncertain Cost Matrices | http://arxiv.org/abs/1209.1800 | id:1209.1800 author:Rui Wang, Ke Tang category:cs.LG  published:2012-09-09 summary:Cost-sensitive learning relies on the availability of a known and fixed cost matrix. However, in some scenarios, the cost matrix is uncertain during training, and re-train a classifier after the cost matrix is specified would not be an option. For binary classification, this issue can be successfully addressed by methods maximizing the Area Under the ROC Curve (AUC) metric. Since the AUC can measure performance of base classifiers independent of cost during training, and a larger AUC is more likely to lead to a smaller total cost in testing using the threshold moving method. As an extension of AUC to multi-class problems, MAUC has attracted lots of attentions and been widely used. Although MAUC also measures performance of base classifiers independent of cost, it is unclear whether a larger MAUC of classifiers is more likely to lead to a smaller total cost. In fact, it is also unclear what kinds of post-processing methods should be used in multi-class problems to convert base classifiers into discrete classifiers such that the total cost is as small as possible. In the paper, we empirically explore the relationship between MAUC and the total cost of classifiers by applying two categories of post-processing methods. Our results suggest that a larger MAUC is also beneficial. Interestingly, simple calibration methods that convert the output matrix into posterior probabilities perform better than existing sophisticated post re-optimization methods. version:1
arxiv-1209-1788 | On the Use of Lee's Protocol for Speckle-Reducing Techniques | http://arxiv.org/abs/1209.1788 | id:1209.1788 author:Elsa E. Moschetti, M. Gabriela Palacio, Mery Picco, Oscar H. Bustos, Alejandro C. Frery category:cs.CV  published:2012-09-09 summary:This paper presents two new MAP (Maximum a Posteriori) filters for speckle noise reduction and a Monte Carlo procedure for the assessment of their performance. In order to quantitatively evaluate the results obtained using these new filters, with respect to classical ones, a Monte Carlo extension of Lee's protocol is proposed. This extension of the protocol shows that its original version leads to inconsistencies that hamper its use as a general procedure for filter assessment. Some solutions for these inconsistencies are proposed, and a consistent comparison of speckle-reducing filters is provided. version:1
arxiv-1209-1759 | Difference of Normals as a Multi-Scale Operator in Unorganized Point Clouds | http://arxiv.org/abs/1209.1759 | id:1209.1759 author:Yani Ioannou, Babak Taati, Robin Harrap, Michael Greenspan category:cs.CV  published:2012-09-08 summary:A novel multi-scale operator for unorganized 3D point clouds is introduced. The Difference of Normals (DoN) provides a computationally efficient, multi-scale approach to processing large unorganized 3D point clouds. The application of DoN in the multi-scale filtering of two different real-world outdoor urban LIDAR scene datasets is quantitatively and qualitatively demonstrated. In both datasets the DoN operator is shown to segment large 3D point clouds into scale-salient clusters, such as cars, people, and lamp posts towards applications in semi-automatic annotation, and as a pre-processing step in automatic object recognition. The application of the operator to segmentation is evaluated on a large public dataset of outdoor LIDAR scenes with ground truth annotations. version:1
arxiv-1209-1751 | Information content versus word length in random typing | http://arxiv.org/abs/1209.1751 | id:1209.1751 author:Ramon Ferrer-i-Cancho, Fermín Moscoso del Prado Martín category:physics.data-an cond-mat.stat-mech cs.CL  published:2012-09-08 summary:Recently, it has been claimed that a linear relationship between a measure of information content and word length is expected from word length optimization and it has been shown that this linearity is supported by a strong correlation between information content and word length in many languages (Piantadosi et al. 2011, PNAS 108, 3825-3826). Here, we study in detail some connections between this measure and standard information theory. The relationship between the measure and word length is studied for the popular random typing process where a text is constructed by pressing keys at random from a keyboard containing letters and a space behaving as a word delimiter. Although this random process does not optimize word lengths according to information content, it exhibits a linear relationship between information content and word length. The exact slope and intercept are presented for three major variants of the random typing process. A strong correlation between information content and word length can simply arise from the units making a word (e.g., letters) and not necessarily from the interplay between a word and its context as proposed by Piantadosi et al. In itself, the linear relation does not entail the results of any optimization process. version:1
arxiv-1209-1739 | Design of Spectrum Sensing Policy for Multi-user Multi-band Cognitive Radio Network | http://arxiv.org/abs/1209.1739 | id:1209.1739 author:Jan Oksanen, Jarmo Lundén, Visa Koivunen category:cs.LG cs.NI  published:2012-09-08 summary:Finding an optimal sensing policy for a particular access policy and sensing scheme is a laborious combinatorial problem that requires the system model parameters to be known. In practise the parameters or the model itself may not be completely known making reinforcement learning methods appealing. In this paper a non-parametric reinforcement learning-based method is developed for sensing and accessing multi-band radio spectrum in multi-user cognitive radio networks. A suboptimal sensing policy search algorithm is proposed for a particular multi-user multi-band access policy and the randomized Chair-Varshney rule. The randomized Chair-Varshney rule is used to reduce the probability of false alarms under a constraint on the probability of detection that protects the primary user. The simulation results show that the proposed method achieves a sum profit (e.g. data rate) close to the optimal sensing policy while achieving the desired probability of detection. version:1
arxiv-1209-1734 | Load Distribution Composite Design Pattern for Genetic Algorithm-Based Autonomic Computing Systems | http://arxiv.org/abs/1209.1734 | id:1209.1734 author:Vishnuvardhan Mannava, T. Ramesh category:cs.SE cs.DC cs.NE  published:2012-09-08 summary:Current autonomic computing systems are ad hoc solutions that are designed and implemented from the scratch. When designing software, in most cases two or more patterns are to be composed to solve a bigger problem. A composite design patterns shows a synergy that makes the composition more than just the sum of its parts which leads to ready-made software architectures. As far as we know, there are no studies on composition of design patterns for autonomic computing domain. In this paper we propose pattern-oriented software architecture for self-optimization in autonomic computing system using design patterns composition and multi objective evolutionary algorithms that software designers and/or programmers can exploit to drive their work. Main objective of the system is to reduce the load in the server by distributing the population to clients. We used Case Based Reasoning, Database Access, and Master Slave design patterns. We evaluate the effectiveness of our architecture with and without design patterns compositions. The use of composite design patterns in the architecture and quantitative measurements are presented. A simple UML class diagram is used to describe the architecture. version:1
arxiv-1209-1727 | Bandits with heavy tail | http://arxiv.org/abs/1209.1727 | id:1209.1727 author:Sébastien Bubeck, Nicolò Cesa-Bianchi, Gábor Lugosi category:stat.ML cs.LG  published:2012-09-08 summary:The stochastic multi-armed bandit problem is well understood when the reward distributions are sub-Gaussian. In this paper we examine the bandit problem under the weaker assumption that the distributions have moments of order 1+\epsilon, for some $\epsilon \in (0,1]$. Surprisingly, moments of order 2 (i.e., finite variance) are sufficient to obtain regret bounds of the same order as under sub-Gaussian reward distributions. In order to achieve such regret, we define sampling strategies based on refined estimators of the mean such as the truncated empirical mean, Catoni's M-estimator, and the median-of-means estimator. We also derive matching lower bounds that also show that the best achievable regret deteriorates when \epsilon <1. version:1
arxiv-1203-1365 | Bayesian Nonparametric Hidden Semi-Markov Models | http://arxiv.org/abs/1203.1365 | id:1203.1365 author:Matthew J. Johnson, Alan S. Willsky category:stat.ME stat.AP stat.ML  published:2012-03-07 summary:There is much interest in the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) as a natural Bayesian nonparametric extension of the ubiquitous Hidden Markov Model for learning from sequential and time-series data. However, in many settings the HDP-HMM's strict Markovian constraints are undesirable, particularly if we wish to learn or encode non-geometric state durations. We can extend the HDP-HMM to capture such structure by drawing upon explicit-duration semi-Markovianity, which has been developed mainly in the parametric frequentist setting, to allow construction of highly interpretable models that admit natural prior information on state durations. In this paper we introduce the explicit-duration Hierarchical Dirichlet Process Hidden semi-Markov Model (HDP-HSMM) and develop sampling algorithms for efficient posterior inference. The methods we introduce also provide new methods for sampling inference in the finite Bayesian HSMM. Our modular Gibbs sampling methods can be embedded in samplers for larger hierarchical Bayesian models, adding semi-Markov chain modeling as another tool in the Bayesian inference toolbox. We demonstrate the utility of the HDP-HSMM and our inference methods on both synthetic and real experiments. version:2
arxiv-1110-1358 | Runtime Guarantees for Regression Problems | http://arxiv.org/abs/1110.1358 | id:1110.1358 author:Hui Han Chin, Aleksander Madry, Gary Miller, Richard Peng category:cs.DS cs.CV  published:2011-10-06 summary:We study theoretical runtime guarantees for a class of optimization problems that occur in a wide variety of inference problems. these problems are motivated by the lasso framework and have applications in machine learning and computer vision. Our work shows a close connection between these problems and core questions in algorithmic graph theory. While this connection demonstrates the difficulties of obtaining runtime guarantees, it also suggests an approach of using techniques originally developed for graph algorithms. We then show that most of these problems can be formulated as a grouped least squares problem, and give efficient algorithms for this formulation. Our algorithms rely on routines for solving quadratic minimization problems, which in turn are equivalent to solving linear systems. Finally we present some experimental results on applying our approximation algorithm to image processing problems. version:2
arxiv-1209-1563 | Wavelet Based QRS Complex Detection of ECG Signal | http://arxiv.org/abs/1209.1563 | id:1209.1563 author:Sayantan Mukhopadhyay, Shouvik Biswas, Anamitra Bardhan Roy, Nilanjan Dey category:cs.CV  published:2012-09-07 summary:The Electrocardiogram (ECG) is a sensitive diagnostic tool that is used to detect various cardiovascular diseases by measuring and recording the electrical activity of the heart in exquisite detail. A wide range of heart condition is determined by thorough examination of the features of the ECG report. Automatic extraction of time plane features is important for identification of vital cardiac diseases. This paper presents a multi-resolution wavelet transform based system for detection 'P', 'Q', 'R', 'S', 'T' peaks complex from original ECG signal. 'R-R' time lapse is an important minutia of the ECG signal that corresponds to the heartbeat of the concerned person. Abrupt increase in height of the 'R' wave or changes in the measurement of the 'R-R' denote various anomalies of human heart. Similarly 'P-P', 'Q-Q', 'S-S', 'T-T' also corresponds to different anomalies of heart and their peak amplitude also envisages other cardiac diseases. In this proposed method the 'PQRST' peaks are marked and stored over the entire signal and the time interval between two consecutive 'R' peaks and other peaks interval are measured to detect anomalies in behavior of heart, if any. The peaks are achieved by the composition of Daubeheissub bands wavelet of original ECG signal. The accuracy of the 'PQRST' complex detection and interval measurement is achieved up to 100% with high exactitude by processing and thresholding the original ECG signal. version:1
arxiv-1209-1558 | A Comparative Study between Moravec and Harris Corner Detection of Noisy Images Using Adaptive Wavelet Thresholding Technique | http://arxiv.org/abs/1209.1558 | id:1209.1558 author:Nilanjan Dey, Pradipti Nandi, Nilanjana Barman, Debolina Das, Subhabrata Chakraborty category:cs.CV  published:2012-09-07 summary:In this paper a comparative study between Moravec and Harris Corner Detection has been done for obtaining features required to track and recognize objects within a noisy image. Corner detection of noisy images is a challenging task in image processing. Natural images often get corrupted by noise during acquisition and transmission. As Corner detection of these noisy images does not provide desired results, hence de-noising is required. Adaptive wavelet thresholding approach is applied for the same. version:1
arxiv-1209-1450 | On spatial selectivity and prediction across conditions with fMRI | http://arxiv.org/abs/1209.1450 | id:1209.1450 author:Yannick Schwartz, Gaël Varoquaux, Bertrand Thirion category:stat.ML cs.LG  published:2012-09-07 summary:Researchers in functional neuroimaging mostly use activation coordinates to formulate their hypotheses. Instead, we propose to use the full statistical images to define regions of interest (ROIs). This paper presents two machine learning approaches, transfer learning and selection transfer, that are compared upon their ability to identify the common patterns between brain activation maps related to two functional tasks. We provide some preliminary quantification of these similarities, and show that selection transfer makes it possible to set a spatial scale yielding ROIs that are more specific to the context of interest than with transfer learning. In particular, selection transfer outlines well known regions such as the Visual Word Form Area when discriminating between different visual tasks. version:1
arxiv-1206-5036 | Estimating Densities with Non-Parametric Exponential Families | http://arxiv.org/abs/1206.5036 | id:1206.5036 author:Lin Yuan, Sergey Kirshner, Robert Givan category:stat.ML cs.LG  published:2012-06-22 summary:We propose a novel approach for density estimation with exponential families for the case when the true density may not fall within the chosen family. Our approach augments the sufficient statistics with features designed to accumulate probability mass in the neighborhood of the observed points, resulting in a non-parametric model similar to kernel density estimators. We show that under mild conditions, the resulting model uses only the sufficient statistics if the density is within the chosen exponential family, and asymptotically, it approximates densities outside of the chosen exponential family. Using the proposed approach, we modify the exponential random graph model, commonly used for modeling small-size graph distributions, to address the well-known issue of model degeneracy. version:2
arxiv-1209-1224 | Wavelet Based Normal and Abnormal Heart Sound Identification using Spectrogram Analysis | http://arxiv.org/abs/1209.1224 | id:1209.1224 author:Nilanjan Dey, Achintya Das, Sheli Sinha Chaudhuri category:cs.CV  published:2012-09-06 summary:The present work proposes a computer-aided normal and abnormal heart sound identification based on Discrete Wavelet Transform (DWT), it being useful for tele-diagnosis of heart diseases. Due to the presence of Cumulative Frequency components in the spectrogram, DWT is applied on the spectro-gram up to n level to extract the features from the individual approximation components. One dimensional feature vector is obtained by evaluating the Row Mean of the approximation components of these spectrograms. For this present approach, the set of spectrograms has been considered as the database, rather than raw sound samples. Minimum Euclidean distance is computed between feature vector of the test sample and the feature vectors of the stored samples to identify the heart sound. By applying this algorithm, almost 82% of accuracy was achieved. version:1
arxiv-1209-1181 | FCM Based Blood Vessel Segmentation Method for Retinal Images | http://arxiv.org/abs/1209.1181 | id:1209.1181 author:Nilanjan Dey, Anamitra Bardhan Roy, Moumita Pal, Achintya Das category:cs.CV  published:2012-09-06 summary:Segmentation of blood vessels in retinal images provides early diagnosis of diseases like glaucoma, diabetic retinopathy and macular degeneration. Among these diseases occurrence of Glaucoma is most frequent and has serious ocular consequences that can even lead to blindness, if it is not detected early. The clinical criteria for the diagnosis of glaucoma include intraocular pressure measurement, optic nerve head evaluation, retinal nerve fiber layer and visual field defects. This form of blood vessel segmentation helps in early detection for ophthalmic diseases, and potentially reduces the risk of blindness. The low-contrast images at the retina owing to narrow blood vessels of the retina are difficult to extract. These low contrast images are, however useful in revealing certain systemic diseases. Motivated by the goals of improving detection of such vessels, this present work proposes an algorithm for segmentation of blood vessels and compares the results between expert ophthalmologist hand-drawn ground-truths and segmented image(i.e. the output of the present work).Sensitivity, specificity, positive predictive value (PPV), positive likelihood ratio (PLR) and accuracy are used to evaluate overall performance.It is found that this work segments blood vessels successfully with sensitivity, specificity, PPV, PLR and accuracy of 99.62%, 54.66%, 95.08%, 219.72 and 95.03%, respectively. version:1
arxiv-1209-1145 | Restricting exchangeable nonparametric distributions | http://arxiv.org/abs/1209.1145 | id:1209.1145 author:Sinead Williamson, Zoubin Ghahramani, Steven N. MacEachern, Eric P. Xing category:stat.ME stat.ML  published:2012-09-05 summary:Distributions over exchangeable matrices with infinitely many columns, such as the Indian buffet process, are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly- suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution. version:1
arxiv-1203-0683 | A Method of Moments for Mixture Models and Hidden Markov Models | http://arxiv.org/abs/1203.0683 | id:1203.0683 author:Animashree Anandkumar, Daniel Hsu, Sham M. Kakade category:cs.LG stat.ML  published:2012-03-03 summary:Mixture models are a fundamental tool in applied statistics and machine learning for treating data taken from multiple subpopulations. The current practice for estimating the parameters of such models relies on local search heuristics (e.g., the EM algorithm) which are prone to failure, and existing consistent methods are unfavorable due to their high computational and sample complexity which typically scale exponentially with the number of mixture components. This work develops an efficient method of moments approach to parameter estimation for a broad class of high-dimensional mixture models with many components, including multi-view mixtures of Gaussians (such as mixtures of axis-aligned Gaussians) and hidden Markov models. The new method leads to rigorous unsupervised learning results for mixture models that were not achieved by previous works; and, because of its simplicity, it offers a viable alternative to EM for practical deployment. version:3
arxiv-1209-1125 | Video Data Visualization System: Semantic Classification And Personalization | http://arxiv.org/abs/1209.1125 | id:1209.1125 author:Jamel Slimi, Anis Ben Ammar, Adel M. Alimi category:cs.IR cs.CV cs.MM  published:2012-09-05 summary:We present in this paper an intelligent video data visualization tool, based on semantic classification, for retrieving and exploring a large scale corpus of videos. Our work is based on semantic classification resulting from semantic analysis of video. The obtained classes will be projected in the visualization space. The graph is represented by nodes and edges, the nodes are the keyframes of video documents and the edges are the relation between documents and the classes of documents. Finally, we construct the user's profile, based on the interaction with the system, to render the system more adequate to its references. version:1
arxiv-1209-0029 | Statistically adaptive learning for a general class of cost functions (SA L-BFGS) | http://arxiv.org/abs/1209.0029 | id:1209.0029 author:Stephen Purpura, Dustin Hillard, Mark Hubenthal, Jim Walsh, Scott Golder, Scott Smith category:cs.LG stat.ML  published:2012-08-31 summary:We present a system that enables rapid model experimentation for tera-scale machine learning with trillions of non-zero features, billions of training examples, and millions of parameters. Our contribution to the literature is a new method (SA L-BFGS) for changing batch L-BFGS to perform in near real-time by using statistical tools to balance the contributions of previous weights, old training examples, and new training examples to achieve fast convergence with few iterations. The result is, to our knowledge, the most scalable and flexible linear learning system reported in the literature, beating standard practice with the current best system (Vowpal Wabbit and AllReduce). Using the KDD Cup 2012 data set from Tencent, Inc. we provide experimental results to verify the performance of this method. version:3
arxiv-1209-1077 | Learning Probability Measures with respect to Optimal Transport Metrics | http://arxiv.org/abs/1209.1077 | id:1209.1077 author:Guillermo D. Canas, Lorenzo Rosasco category:cs.LG stat.ML K.3.2  published:2012-09-05 summary:We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic upper bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures. version:1
arxiv-1111-5358 | Contextually Guided Semantic Labeling and Search for 3D Point Clouds | http://arxiv.org/abs/1111.5358 | id:1111.5358 author:Abhishek Anand, Hema Swetha Koppula, Thorsten Joachims, Ashutosh Saxena category:cs.RO cs.AI cs.CV  published:2011-11-22 summary:RGB-D cameras, which give an RGB image to- gether with depths, are becoming increasingly popular for robotic perception. In this paper, we address the task of detecting commonly found objects in the 3D point cloud of indoor scenes obtained from such cameras. Our method uses a graphical model that captures various features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the model's parsimony becomes important and we address that by using multiple types of edge potentials. We train the model using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and offices (composed from about 550 views), we get a performance of 84.06% and 73.38% in labeling office and home scenes respectively for 17 object classes each. We also present a method for a robot to search for an object using the learned model and the contextual information available from the current labelings of the scene. We applied this algorithm successfully on a mobile robot for the task of finding 12 object classes in 10 different offices and achieved a precision of 97.56% with 78.43% recall. version:3
arxiv-1012-0366 | Optimal measures and Markov transition kernels | http://arxiv.org/abs/1012.0366 | id:1012.0366 author:Roman V. Belavkin category:math.OC cs.CC cs.IT math-ph math.FA math.IT math.MP stat.ML  published:2010-12-02 summary:We study optimal solutions to an abstract optimization problem for measures, which is a generalization of classical variational problems in information theory and statistical physics. In the classical problems, information and relative entropy are defined using the Kullback-Leibler divergence, and for this reason optimal measures belong to a one-parameter exponential family. Measures within such a family have the property of mutual absolute continuity. Here we show that this property characterizes other families of optimal positive measures if a functional representing information has a strictly convex dual. Mutual absolute continuity of optimal probability measures allows us to strictly separate deterministic and non-deterministic Markov transition kernels, which play an important role in theories of decisions, estimation, control, communication and computation. We show that deterministic transitions are strictly sub-optimal, unless information resource with a strictly convex dual is unconstrained. For illustration, we construct an example where, unlike non-deterministic, any deterministic kernel either has negatively infinite expected utility (unbounded expected error) or communicates infinite information. version:7
arxiv-1209-0999 | Visual Exploration of Simulated and Measured Blood Flow | http://arxiv.org/abs/1209.0999 | id:1209.0999 author:Anna Vilanova, Bernhard Preim, Roy van Pelt, Rocco Gasteiger, Mathias Neugebauer, Thomas Wischgoll category:cs.GR cs.CV  published:2012-09-05 summary:Morphology of cardiovascular tissue is influenced by the unsteady behavior of the blood flow and vice versa. Therefore, the pathogenesis of several cardiovascular diseases is directly affected by the blood-flow dynamics. Understanding flow behavior is of vital importance to understand the cardiovascular system and potentially harbors a considerable value for both diagnosis and risk assessment. The analysis of hemodynamic characteristics involves qualitative and quantitative inspection of the blood-flow field. Visualization plays an important role in the qualitative exploration, as well as the definition of relevant quantitative measures and its validation. There are two main approaches to obtain information about the blood flow: simulation by computational fluid dynamics, and in-vivo measurements. Although research on blood flow simulation has been performed for decades, many open problems remain concerning accuracy and patient-specific solutions. Possibilities for real measurement of blood flow have recently increased considerably by new developments in magnetic resonance imaging which enable the acquisition of 3D quantitative measurements of blood-flow velocity fields. This chapter presents the visualization challenges for both simulation and real measurements of unsteady blood-flow fields. version:1
arxiv-1201-2555 | Sparse Reward Processes | http://arxiv.org/abs/1201.2555 | id:1201.2555 author:Christos Dimitrakakis category:cs.LG stat.ML  published:2012-01-12 summary:We introduce a class of learning problems where the agent is presented with a series of tasks. Intuitively, if there is relation among those tasks, then the information gained during execution of one task has value for the execution of another task. Consequently, the agent is intrinsically motivated to explore its environment beyond the degree necessary to solve the current task it has at hand. We develop a decision theoretic setting that generalises standard reinforcement learning tasks and captures this intuition. More precisely, we consider a multi-stage stochastic game between a learning agent and an opponent. We posit that the setting is a good model for the problem of life-long learning in uncertain environments, where while resources must be spent learning about currently important tasks, there is also the need to allocate effort towards learning about aspects of the world which are not relevant at the moment. This is due to the fact that unpredictable future events may lead to a change of priorities for the decision maker. Thus, in some sense, the model "explains" the necessity of curiosity. Apart from introducing the general formalism, the paper provides algorithms. These are evaluated experimentally in some exemplary domains. In addition, performance bounds are proven for some cases of this problem. version:2
arxiv-1209-0913 | Structuring Relevant Feature Sets with Multiple Model Learning | http://arxiv.org/abs/1209.0913 | id:1209.0913 author:Jun Wang, Alexandros Kalousis category:cs.LG  published:2012-09-05 summary:Feature selection is one of the most prominent learning tasks, especially in high-dimensional datasets in which the goal is to understand the mechanisms that underly the learning dataset. However most of them typically deliver just a flat set of relevant features and provide no further information on what kind of structures, e.g. feature groupings, might underly the set of relevant features. In this paper we propose a new learning paradigm in which our goal is to uncover the structures that underly the set of relevant features for a given learning problem. We uncover two types of features sets, non-replaceable features that contain important information about the target variable and cannot be replaced by other features, and functionally similar features sets that can be used interchangeably in learned models, given the presence of the non-replaceable features, with no change in the predictive performance. To do so we propose a new learning algorithm that learns a number of disjoint models using a model disjointness regularization constraint together with a constraint on the predictive agreement of the disjoint models. We explore the behavior of our approach on a number of high-dimensional datasets, and show that, as expected by their construction, these satisfy a number of properties. Namely, model disjointness, a high predictive agreement, and a similar predictive performance to models learned on the full set of relevant features. The ability to structure the set of relevant features in such a manner can become a valuable tool in different applications of scientific knowledge discovery. version:1
arxiv-1209-0853 | Improving the K-means algorithm using improved downhill simplex search | http://arxiv.org/abs/1209.0853 | id:1209.0853 author:Ehsan Saboori, Shafigh Parsazad, Anoosheh Sadeghi category:cs.LG  published:2012-09-05 summary:The k-means algorithm is one of the well-known and most popular clustering algorithms. K-means seeks an optimal partition of the data by minimizing the sum of squared error with an iterative optimization procedure, which belongs to the category of hill climbing algorithms. As we know hill climbing searches are famous for converging to local optimums. Since k-means can converge to a local optimum, different initial points generally lead to different convergence cancroids, which makes it important to start with a reasonable initial partition in order to achieve high quality clustering solutions. However, in theory, there exist no efficient and universal methods for determining such initial partitions. In this paper we tried to find an optimum initial partitioning for k-means algorithm. To achieve this goal we proposed a new improved version of downhill simplex search, and then we used it in order to find an optimal result for clustering approach and then compare this algorithm with Genetic Algorithm base (GA), Genetic K-Means (GKM), Improved Genetic K-Means (IGKM) and k-means algorithms. version:1
arxiv-1209-0833 | Multiresolution Gaussian Processes | http://arxiv.org/abs/1209.0833 | id:1209.0833 author:Emily B. Fox, David B. Dunson category:stat.ME stat.ML  published:2012-09-05 summary:We propose a multiresolution Gaussian process to capture long-range, non-Markovian dependencies while allowing for abrupt changes. The multiresolution GP hierarchically couples a collection of smooth GPs, each defined over an element of a random nested partition. Long-range dependencies are captured by the top-level GP while the partition points define the abrupt changes. Due to the inherent conjugacy of the GPs, one can analytically marginalize the GPs and compute the conditional likelihood of the observations given the partition tree. This property allows for efficient inference of the partition itself, for which we employ graph-theoretic techniques. We apply the multiresolution GP to the analysis of Magnetoencephalography (MEG) recordings of brain activity. version:1
arxiv-1206-6464 | Estimating the Hessian by Back-propagating Curvature | http://arxiv.org/abs/1206.6464 | id:1206.6464 author:James Martens, Ilya Sutskever, Kevin Swersky category:cs.LG stat.ML  published:2012-06-27 summary:In this work we develop Curvature Propagation (CP), a general technique for efficiently computing unbiased approximations of the Hessian of any function that is computed using a computational graph. At the cost of roughly two gradient evaluations, CP can give a rank-1 approximation of the whole Hessian, and can be repeatedly applied to give increasingly precise unbiased estimates of any or all of the entries of the Hessian. Of particular interest is the diagonal of the Hessian, for which no general approach is known to exist that is both efficient and accurate. We show in experiments that CP turns out to work well in practice, giving very accurate estimates of the Hessian of neural networks, for example, with a relatively small amount of work. We also apply CP to Score Matching, where a diagonal of a Hessian plays an integral role in the Score Matching objective, and where it is usually computed exactly using inefficient algorithms which do not scale to larger and more complex models. version:2
arxiv-1209-0724 | Synthesis of Stochastic Flow Networks | http://arxiv.org/abs/1209.0724 | id:1209.0724 author:Hongchao Zhou, Ho-Lin Chen, Jehoshua Bruck category:cs.IT cs.NE math.IT math.PR  published:2012-09-04 summary:A stochastic flow network is a directed graph with incoming edges (inputs) and outgoing edges (outputs), tokens enter through the input edges, travel stochastically in the network, and can exit the network through the output edges. Each node in the network is a splitter, namely, a token can enter a node through an incoming edge and exit on one of the output edges according to a predefined probability distribution. Stochastic flow networks can be easily implemented by DNA-based chemical reactions, with promising applications in molecular computing and stochastic computing. In this paper, we address a fundamental synthesis question: Given a finite set of possible splitters and an arbitrary rational probability distribution, design a stochastic flow network, such that every token that enters the input edge will exit the outputs with the prescribed probability distribution. The problem of probability transformation dates back to von Neumann's 1951 work and was followed, among others, by Knuth and Yao in 1976. Most existing works have been focusing on the "simulation" of target distributions. In this paper, we design optimal-sized stochastic flow networks for "synthesizing" target distributions. It shows that when each splitter has two outgoing edges and is unbiased, an arbitrary rational probability \frac{a}{b} with a\leq b\leq 2^n can be realized by a stochastic flow network of size n that is optimal. Compared to the other stochastic systems, feedback (cycles in networks) strongly improves the expressibility of stochastic flow networks. version:1
arxiv-1206-6443 | Isoelastic Agents and Wealth Updates in Machine Learning Markets | http://arxiv.org/abs/1206.6443 | id:1206.6443 author:Amos Storkey, Jono Millin, Krzysztof Geras category:cs.LG cs.GT stat.ML  published:2012-06-27 summary:Recently, prediction markets have shown considerable promise for developing flexible mechanisms for machine learning. In this paper, agents with isoelastic utilities are considered. It is shown that the costs associated with homogeneous markets of agents with isoelastic utilities produce equilibrium prices corresponding to alpha-mixtures, with a particular form of mixing component relating to each agent's wealth. We also demonstrate that wealth accumulation for logarithmic and other isoelastic agents (through payoffs on prediction of training targets) can implement both Bayesian model updates and mixture weight updates by imposing different market payoff structures. An iterative algorithm is given for market equilibrium computation. We demonstrate that inhomogeneous markets of agents with isoelastic utilities outperform state of the art aggregate classifiers such as random forests, as well as single classifiers (neural networks, decision trees) on a number of machine learning benchmarks, and show that isoelastic combination methods are generally better than their logarithmic counterparts. version:2
arxiv-1206-6475 | A Split-Merge Framework for Comparing Clusterings | http://arxiv.org/abs/1206.6475 | id:1206.6475 author:Qiaoliang Xiang, Qi Mao, Kian Ming Chai, Hai Leong Chieu, Ivor Tsang, Zhendong Zhao category:cs.LG stat.ML  published:2012-06-27 summary:Clustering evaluation measures are frequently used to evaluate the performance of algorithms. However, most measures are not properly normalized and ignore some information in the inherent structure of clusterings. We model the relation between two clusterings as a bipartite graph and propose a general component-based decomposition formula based on the components of the graph. Most existing measures are examples of this formula. In order to satisfy consistency in the component, we further propose a split-merge framework for comparing clusterings of different data sets. Our framework gives measures that are conditionally normalized, and it can make use of data point information, such as feature vectors and pairwise distances. We use an entropy-based instance of the framework and a coreference resolution data set to demonstrate empirically the utility of our framework over other measures. version:2
arxiv-1209-0521 | Efficient EM Training of Gaussian Mixtures with Missing Data | http://arxiv.org/abs/1209.0521 | id:1209.0521 author:Olivier Delalleau, Aaron Courville, Yoshua Bengio category:cs.LG stat.ML  published:2012-09-04 summary:In data-mining applications, we are frequently faced with a large fraction of missing entries in the data matrix, which is problematic for most discriminant machine learning algorithms. A solution that we explore in this paper is the use of a generative model (a mixture of Gaussians) to compute the conditional expectation of the missing variables given the observed variables. Since training a Gaussian mixture with many different patterns of missing values can be computationally very expensive, we introduce a spanning-tree based algorithm that significantly speeds up training in these conditions. We also observe that good results can be obtained by using the generative model to fill-in the missing values for a separate discriminant learning algorithm. version:1
arxiv-1209-0514 | Monotonicity of Fitness Landscapes and Mutation Rate Control | http://arxiv.org/abs/1209.0514 | id:1209.0514 author:Roman V. Belavkin, Alastair Channon, Elizabeth Aston, John Aston, Rok Krasovec, Christopher G. Knight category:q-bio.PE cs.IT cs.NE math.IT math.OC  published:2012-09-04 summary:The typical view in evolutionary biology is that mutation rates are minimised. Contrary to that view, studies in combinatorial optimisation and search have shown a clear advantage of using variable mutation rates as a control parameter to optimise the performance of evolutionary algorithms. Ronald Fisher's work is the basis of much biological theory in this area. He used Euclidean geometry of continuous, infinite phenotypic spaces to study the relation between mutation size and expected fitness of the offspring. Here we develop a general theory of optimal mutation rate control that is based on the alternative geometry of discrete and finite spaces of DNA sequences. We define the monotonic properties of fitness landscapes, which allows us to relate fitness to the topology of genotypes and mutation size. First, we consider the case of a perfectly monotonic fitness landscape, in which the optimal mutation rate control functions can be derived exactly or approximately depending on additional constraints of the problem. Then we consider the general case of non-monotonic landscapes. We use the ideas of local and weak monotonicity to show that optimal mutation rate control functions exist in any such landscape and that they resemble control functions in a monotonic landscape at least in some neighbourhood of a fitness maximum. Generally, optimal mutation rates increase when fitness decreases, and the increase of mutation rate is more rapid in landscapes that are less monotonic (more rugged). We demonstrate these relationships by obtaining and analysing approximately optimal mutation rate control functions in 115 complete landscapes of binding scores between DNA sequences and transcription factors. We discuss the relevance of these findings to living organisms, including the phenomenon of stress-induced mutagenesis. version:1
arxiv-1206-6398 | Learning Parameterized Skills | http://arxiv.org/abs/1206.6398 | id:1206.6398 author:Bruno Da Silva, George Konidaris, Andrew Barto category:cs.LG stat.ML  published:2012-06-27 summary:We introduce a method for constructing skills capable of solving tasks drawn from a distribution of parameterized reinforcement learning problems. The method draws example tasks from a distribution of interest and uses the corresponding learned policies to estimate the topology of the lower-dimensional piecewise-smooth manifold on which the skill policies lie. This manifold models how policy parameters change as task parameters vary. The method identifies the number of charts that compose the manifold and then applies non-linear regression in each chart to construct a parameterized skill by predicting policy parameters from task parameters. We evaluate our method on an underactuated simulated robotic arm tasked with learning to accurately throw darts at a parameterized target location. version:2
arxiv-1209-0368 | Proximal methods for the latent group lasso penalty | http://arxiv.org/abs/1209.0368 | id:1209.0368 author:Silvia Villa, Lorenzo Rosasco, Sofia Mosci, Alessandro Verri category:math.OC cs.LG stat.ML 65K10  90C25  published:2012-09-03 summary:We consider a regularized least squares problem, with regularization by structured sparsity-inducing norms, which extend the usual $\ell_1$ and the group lasso penalty, by allowing the subsets to overlap. Such regularizations lead to nonsmooth problems that are difficult to optimize, and we propose in this paper a suitable version of an accelerated proximal method to solve them. We prove convergence of a nested procedure, obtained composing an accelerated proximal method with an inner algorithm for computing the proximity operator. By exploiting the geometrical properties of the penalty, we devise a new active set strategy, thanks to which the inner iteration is relatively fast, thus guaranteeing good computational performances of the overall algorithm. Our approach allows to deal with high dimensional problems without pre-processing for dimensionality reduction, leading to better computational and prediction performances with respect to the state-of-the art methods, as shown empirically both on toy and real data. version:1
arxiv-1209-0367 | Seeded Graph Matching | http://arxiv.org/abs/1209.0367 | id:1209.0367 author:Donniell E. Fishkind, Sancar Adali, Carey E. Priebe category:stat.ML  published:2012-09-03 summary:Graph inference is a burgeoning field in the applied and theoretical statistics communities, as well as throughout the wider world of science, engineering, business, etc. Given two graphs on the same number of vertices, the graph matching problem is to find a bijection between the two vertex sets which minimizes the number of adjacency disagreements between the two graphs. The seeded graph matching problem is the graph matching problem with an additional constraint that the bijection assigns some particular vertices of one vertex set to respective particular vertices of the other vertex set. Solving the (seeded) graph matching problem will enable methodologies for many graph inference tasks, but the problem is NP-hard. We modify the state-of-the-art approximate graph matching algorithm of Vogelstein et al. (2012) to make it a fast approximate seeded graph matching algorithm. We demonstrate the effectiveness of our algorithm - and the potential for dramatic performance improvement from incorporating just a few seeds - via simulation and real data experiments. version:1
arxiv-1102-1140 | Ranking-Based Black-Box Complexity | http://arxiv.org/abs/1102.1140 | id:1102.1140 author:Benjamin Doerr, Carola Winzen category:cs.NE cs.CC cs.DS  published:2011-02-06 summary:Randomized search heuristics such as evolutionary algorithms, simulated annealing, and ant colony optimization are a broadly used class of general-purpose algorithms. Analyzing them via classical methods of theoretical computer science is a growing field. While several strong runtime analysis results have appeared in the last 20 years, a powerful complexity theory for such algorithms is yet to be developed. We enrich the existing notions of black-box complexity by the additional restriction that not the actual objective values, but only the relative quality of the previously evaluated solutions may be taken into account by the black-box algorithm. Many randomized search heuristics belong to this class of algorithms. We show that the new ranking-based model gives more realistic complexity estimates for some problems. For example, the class of all binary-value functions has a black-box complexity of $O(\log n)$ in the previous black-box models, but has a ranking-based complexity of $\Theta(n)$. For the class of all OneMax functions, we present a ranking-based black-box algorithm that has a runtime of $\Theta(n / \log n)$, which shows that the OneMax problem does not become harder with the additional ranking-basedness restriction. version:3
arxiv-1209-0249 | Robopinion: Opinion Mining Framework Inspired by Autonomous Robot Navigation | http://arxiv.org/abs/1209.0249 | id:1209.0249 author:M. A. El-Dosuky, M. Z. Rashad, T. T. Hamza, A. H. EL-Bassiouny category:cs.CL cs.IR  published:2012-09-03 summary:Data association methods are used by autonomous robots to find matches between the current landmarks and the new set of observed features. We seek a framework for opinion mining to benefit from advancements in autonomous robot navigation in both research and development version:1
arxiv-1206-0500 | Binary hidden Markov models and varieties | http://arxiv.org/abs/1206.0500 | id:1206.0500 author:Andrew J. Critch category:math.AG stat.ML 14Q15  published:2012-06-03 summary:The technological applications of hidden Markov models have been extremely diverse and successful, including natural language processing, gesture recognition, gene sequencing, and Kalman filtering of physical measurements. HMMs are highly non-linear statistical models, and just as linear models are amenable to linear algebraic techniques, non-linear models are amenable to commutative algebra and algebraic geometry. This paper closely examines HMMs in which all the hidden random variables are binary. Its main contributions are (1) a birational parametrization for every such HMM, with an explicit inverse for recovering the hidden parameters in terms of observables, (2) a semialgebraic model membership test for every such HMM, and (3) minimal defining equations for the 4-node fully binary model, comprising 21 quadrics and 29 cubics, which were computed using Grobner bases in the cumulant coordinates of Sturmfels and Zwiernik. The new model parameters in (1) are rationally identifiable in the sense of Sullivant, Garcia-Puente, and Spielvogel, and each model's Zariski closure is therefore a rational projective variety of dimension 5. Grobner basis computations for the model and its graph are found to be considerably faster using these parameters. In the case of two hidden states, item (2) supersedes a previous algorithm of Schonhuth which is only generically defined, and the defining equations (3) yield new invariants for HMMs of all lengths $\geq 4$. Such invariants have been used successfully in model selection problems in phylogenetics, and one can hope for similar applications in the case of HMMs. version:3
arxiv-1102-2808 | Transductive Ordinal Regression | http://arxiv.org/abs/1102.2808 | id:1102.2808 author:Chun-Wei Seah, Ivor W. Tsang, Yew-Soon Ong category:cs.LG  published:2011-02-14 summary:Ordinal regression is commonly formulated as a multi-class problem with ordinal constraints. The challenge of designing accurate classifiers for ordinal regression generally increases with the number of classes involved, due to the large number of labeled patterns that are needed. The availability of ordinal class labels, however, is often costly to calibrate or difficult to obtain. Unlabeled patterns, on the other hand, often exist in much greater abundance and are freely available. To take benefits from the abundance of unlabeled patterns, we present a novel transductive learning paradigm for ordinal regression in this paper, namely Transductive Ordinal Regression (TOR). The key challenge of the present study lies in the precise estimation of both the ordinal class label of the unlabeled data and the decision functions of the ordinal classes, simultaneously. The core elements of the proposed TOR include an objective function that caters to several commonly used loss functions casted in transductive settings, for general ordinal regression. A label swapping scheme that facilitates a strictly monotonic decrease in the objective function value is also introduced. Extensive numerical studies on commonly used benchmark datasets including the real world sentiment prediction problem are then presented to showcase the characteristics and efficacies of the proposed transductive ordinal regression. Further, comparisons to recent state-of-the-art ordinal regression methods demonstrate the introduced transductive learning paradigm for ordinal regression led to the robust and improved performance. version:5
arxiv-1005-5197 | Ranked bandits in metric spaces: learning optimally diverse rankings over large document collections | http://arxiv.org/abs/1005.5197 | id:1005.5197 author:Aleksandrs Slivkins, Filip Radlinski, Sreenivas Gollapudi category:cs.LG cs.DS  published:2010-05-28 summary:Most learning to rank research has assumed that the utility of different documents is independent, which results in learned ranking functions that return redundant results. The few approaches that avoid this have rather unsatisfyingly lacked theoretical foundations, or do not scale. We present a learning-to-rank formulation that optimizes the fraction of satisfied users, with several scalable algorithms that explicitly takes document similarity and ranking context into account. Our formulation is a non-trivial common generalization of two multi-armed bandit models from the literature: "ranked bandits" (Radlinski et al., ICML 2008) and "Lipschitz bandits" (Kleinberg et al., STOC 2008). We present theoretical justifications for this approach, as well as a near-optimal algorithm. Our evaluation adds optimizations that improve empirical performance, and shows that our algorithms learn orders of magnitude more quickly than previous approaches. version:2
arxiv-1205-5407 | FASTSUBS: An Efficient and Exact Procedure for Finding the Most Likely Lexical Substitutes Based on an N-gram Language Model | http://arxiv.org/abs/1205.5407 | id:1205.5407 author:Deniz Yuret category:cs.CL  published:2012-05-24 summary:Lexical substitutes have found use in areas such as paraphrasing, text simplification, machine translation, word sense disambiguation, and part of speech induction. However the computational complexity of accurately identifying the most likely substitutes for a word has made large scale experiments difficult. In this paper I introduce a new search algorithm, FASTSUBS, that is guaranteed to find the K most likely lexical substitutes for a given word in a sentence based on an n-gram language model. The computation is sub-linear in both K and the vocabulary size V. An implementation of the algorithm and a dataset with the top 100 substitutes of each token in the WSJ section of the Penn Treebank are available at http://goo.gl/jzKH0. version:2
arxiv-1206-1121 | Comparison of the C4.5 and a Naive Bayes Classifier for the Prediction of Lung Cancer Survivability | http://arxiv.org/abs/1206.1121 | id:1206.1121 author:George Dimitoglou, James A. Adams, Carol M. Jim category:cs.LG  published:2012-06-06 summary:Numerous data mining techniques have been developed to extract information and identify patterns and predict trends from large data sets. In this study, two classification techniques, the J48 implementation of the C4.5 algorithm and a Naive Bayes classifier are applied to predict lung cancer survivability from an extensive data set with fifteen years of patient records. The purpose of the project is to verify the predictive effectiveness of the two techniques on real, historical data. Besides the performance outcome that renders J48 marginally better than the Naive Bayes technique, there is a detailed description of the data and the required pre-processing activities. The performance results confirm expectations while some of the issues that appeared during experimentation, underscore the value of having domain-specific understanding to leverage any domain-specific characteristics inherent in the data. version:2
arxiv-1209-0056 | Learning implicitly in reasoning in PAC-Semantics | http://arxiv.org/abs/1209.0056 | id:1209.0056 author:Brendan Juba category:cs.AI cs.DS cs.LG cs.LO  published:2012-09-01 summary:We consider the problem of answering queries about formulas of propositional logic based on background knowledge partially represented explicitly as other formulas, and partially represented as partially obscured examples independently drawn from a fixed probability distribution, where the queries are answered with respect to a weaker semantics than usual -- PAC-Semantics, introduced by Valiant (2000) -- that is defined using the distribution of examples. We describe a fairly general, efficient reduction to limited versions of the decision problem for a proof system (e.g., bounded space treelike resolution, bounded degree polynomial calculus, etc.) from corresponding versions of the reasoning problem where some of the background knowledge is not explicitly given as formulas, only learnable from the examples. Crucially, we do not generate an explicit representation of the knowledge extracted from the examples, and so the "learning" of the background knowledge is only done implicitly. As a consequence, this approach can utilize formulas as background knowledge that are not perfectly valid over the distribution---essentially the analogue of agnostic learning here. version:1
arxiv-1209-0053 | A Session Based Blind Watermarking Technique within the NROI of Retinal Fundus Images for Authentication Using DWT, Spread Spectrum and Harris Corner Detection | http://arxiv.org/abs/1209.0053 | id:1209.0053 author:Nilanjan Dey, Moumita Pal, Achintya Das category:cs.CV cs.CY  published:2012-09-01 summary:Digital Retinal Fundus Images helps to detect various ophthalmic diseases by detecting morphological changes in optical cup, optical disc and macula. Present work proposes a method for the authentication of medical images based on Discrete Wavelet Transformation (DWT) and Spread Spectrum. Proper selection of the Non Region of Interest (NROI) for watermarking is crucial, as the area under concern has to be the least required portion conveying any medical information. Proposed method discusses both the selection of least impact area and the blind watermarking technique. Watermark is embedded within the High-High (HH) sub band. During embedding, watermarked image is dispersed within the band using a pseudo random sequence and a Session key. Watermarked image is extracted using the session key and the size of the image. In this approach the generated watermarked image having an acceptable level of imperceptibility and distortion is compared to the Original retinal image based on Peak Signal to Noise Ratio (PSNR) and correlation value. version:1
arxiv-1110-2053 | Steps Towards a Theory of Visual Information: Active Perception, Signal-to-Symbol Conversion and the Interplay Between Sensing and Control | http://arxiv.org/abs/1110.2053 | id:1110.2053 author:Stefano Soatto category:cs.CV  published:2011-10-10 summary:This manuscript describes the elements of a theory of information tailored to control and decision tasks and specifically to visual data. The concept of Actionable Information is described, that relates to a notion of information championed by J. Gibson, and a notion of "complete information" that relates to the minimal sufficient statistics of a complete representation. It is shown that the "actionable information gap" between the two can be reduced by exercising control on the sensing process. Thus, senging, control and information are inextricably tied. This has consequences in the so-called "signal-to-symbol barrier" problem, as well as in the analysis and design of active sensing systems. It has ramifications in vision-based control, navigation, 3-D reconstruction and rendering, as well as detection, localization, recognition and categorization of objects and scenes in live video. This manuscript has been developed from a set of lecture notes for a summer course at the First International Computer Vision Summer School (ICVSS) in Scicli, Italy, in July of 2008. They were later expanded and amended for subsequent lectures in the same School in July 2009. Starting on November 1, 2009, they were further expanded for a special topics course, CS269, taught at UCLA in the Spring term of 2010. version:3
arxiv-1208-5801 | Vector Field k-Means: Clustering Trajectories by Fitting Multiple Vector Fields | http://arxiv.org/abs/1208.5801 | id:1208.5801 author:Nivan Ferreira, James T. Klosowski, Carlos Scheidegger, Claudio Silva category:cs.LG  published:2012-08-28 summary:Scientists study trajectory data to understand trends in movement patterns, such as human mobility for traffic analysis and urban planning. There is a pressing need for scalable and efficient techniques for analyzing this data and discovering the underlying patterns. In this paper, we introduce a novel technique which we call vector-field $k$-means. The central idea of our approach is to use vector fields to induce a similarity notion between trajectories. Other clustering algorithms seek a representative trajectory that best describes each cluster, much like $k$-means identifies a representative "center" for each cluster. Vector-field $k$-means, on the other hand, recognizes that in all but the simplest examples, no single trajectory adequately describes a cluster. Our approach is based on the premise that movement trends in trajectory data can be modeled as flows within multiple vector fields, and the vector field itself is what defines each of the clusters. We also show how vector-field $k$-means connects techniques for scalar field design on meshes and $k$-means clustering. We present an algorithm that finds a locally optimal clustering of trajectories into vector fields, and demonstrate how vector-field $k$-means can be used to mine patterns from trajectory data. We present experimental evidence of its effectiveness and efficiency using several datasets, including historical hurricane data, GPS tracks of people and vehicles, and anonymous call records from a large phone company. We compare our results to previous trajectory clustering techniques, and find that our algorithm performs faster in practice than the current state-of-the-art in trajectory clustering, in some examples by a large margin. version:2
arxiv-1208-6523 | Combinatorial Gradient Fields for 2D Images with Empirically Convergent Separatrices | http://arxiv.org/abs/1208.6523 | id:1208.6523 author:Jan Reininghaus, David Günther, Ingrid Hotz, Tino Weinkauf, Hans Peter Seidel category:cs.CV cs.CG cs.DM 68U10  published:2012-08-31 summary:This paper proposes an efficient probabilistic method that computes combinatorial gradient fields for two dimensional image data. In contrast to existing algorithms, this approach yields a geometric Morse-Smale complex that converges almost surely to its continuous counterpart when the image resolution is increased. This approach is motivated using basic ideas from probability theory and builds upon an algorithm from discrete Morse theory with a strong mathematical foundation. While a formal proof is only hinted at, we do provide a thorough numerical evaluation of our method and compare it to established algorithms. version:1
arxiv-1208-6516 | A two-stage denoising filter: the preprocessed Yaroslavsky filter | http://arxiv.org/abs/1208.6516 | id:1208.6516 author:Joseph Salmon, Rebecca Willett, Ery Arias-Castro category:cs.CV math.ST stat.TH  published:2012-08-31 summary:This paper describes a simple image noise removal method which combines a preprocessing step with the Yaroslavsky filter for strong numerical, visual, and theoretical performance on a broad class of images. The framework developed is a two-stage approach. In the first stage the image is filtered with a classical denoising method (e.g., wavelet or curvelet thresholding). In the second stage a modification of the Yaroslavsky filter is performed on the original noisy image, where the weights of the filters are governed by pixel similarities in the denoised image from the first stage. Similar prefiltering ideas have proved effective previously in the literature, and this paper provides theoretical guarantees and important insight into why prefiltering can be effective. Empirically, this simple approach achieves very good performance for cartoon images, and can be computed much more quickly than current patch-based denoising algorithms. version:1
arxiv-1207-5328 | A prototype for projecting HPSG syntactic lexica towards LMF | http://arxiv.org/abs/1207.5328 | id:1207.5328 author:Kais Haddar, Héla Fehri, Laurent Romary category:cs.CL  published:2012-07-23 summary:The comparative evaluation of Arabic HPSG grammar lexica requires a deep study of their linguistic coverage. The complexity of this task results mainly from the heterogeneity of the descriptive components within those lexica (underlying linguistic resources and different data categories, for example). It is therefore essential to define more homogeneous representations, which in turn will enable us to compare them and eventually merge them. In this context, we present a method for comparing HPSG lexica based on a rule system. This method is implemented within a prototype for the projection from Arabic HPSG to a normalised pivot language compliant with LMF (ISO 24613 - Lexical Markup Framework) and serialised using a TEI (Text Encoding Initiative) based representation. The design of this system is based on an initial study of the HPSG formalism looking at its adequacy for the representation of Arabic, and from this, we identify the appropriate feature structures corresponding to each Arabic lexical category and their possible LMF counterparts. version:3
arxiv-1208-6338 | A Widely Applicable Bayesian Information Criterion | http://arxiv.org/abs/1208.6338 | id:1208.6338 author:Sumio Watanabe category:cs.LG stat.ML  published:2012-08-31 summary:A statistical model or a learning machine is called regular if the map taking a parameter to a probability distribution is one-to-one and if its Fisher information matrix is always positive definite. If otherwise, it is called singular. In regular statistical models, the Bayes free energy, which is defined by the minus logarithm of Bayes marginal likelihood, can be asymptotically approximated by the Schwarz Bayes information criterion (BIC), whereas in singular models such approximation does not hold. Recently, it was proved that the Bayes free energy of a singular model is asymptotically given by a generalized formula using a birational invariant, the real log canonical threshold (RLCT), instead of half the number of parameters in BIC. Theoretical values of RLCTs in several statistical models are now being discovered based on algebraic geometrical methodology. However, it has been difficult to estimate the Bayes free energy using only training samples, because an RLCT depends on an unknown true distribution. In the present paper, we define a widely applicable Bayesian information criterion (WBIC) by the average log likelihood function over the posterior distribution with the inverse temperature $1/\log n$, where $n$ is the number of training samples. We mathematically prove that WBIC has the same asymptotic expansion as the Bayes free energy, even if a statistical model is singular for and unrealizable by a statistical model. Since WBIC can be numerically calculated without any information about a true distribution, it is a generalized version of BIC onto singular statistical models. version:1
arxiv-1208-6335 | Comparative Study and Optimization of Feature-Extraction Techniques for Content based Image Retrieval | http://arxiv.org/abs/1208.6335 | id:1208.6335 author:Aman Chadha, Sushmit Mallik, Ravdeep Johar category:cs.CV  published:2012-08-30 summary:The aim of a Content-Based Image Retrieval (CBIR) system, also known as Query by Image Content (QBIC), is to help users to retrieve relevant images based on their contents. CBIR technologies provide a method to find images in large databases by using unique descriptors from a trained image. The image descriptors include texture, color, intensity and shape of the object inside an image. Several feature-extraction techniques viz., Average RGB, Color Moments, Co-occurrence, Local Color Histogram, Global Color Histogram and Geometric Moment have been critically compared in this paper. However, individually these techniques result in poor performance. So, combinations of these techniques have also been evaluated and results for the most efficient combination of techniques have been presented and optimized for each class of image query. We also propose an improvement in image retrieval performance by introducing the idea of Query modification through image cropping. It enables the user to identify a region of interest and modify the initial query to refine and personalize the image retrieval results. version:1
arxiv-1209-0001 | An Improved Bound for the Nystrom Method for Large Eigengap | http://arxiv.org/abs/1209.0001 | id:1209.0001 author:Mehrdad Mahdavi, Tianbao Yang, Rong Jin category:cs.LG cs.NA stat.ML  published:2012-08-30 summary:We develop an improved bound for the approximation error of the Nystr\"{o}m method under the assumption that there is a large eigengap in the spectrum of kernel matrix. This is based on the empirical observation that the eigengap has a significant impact on the approximation error of the Nystr\"{o}m method. Our approach is based on the concentration inequality of integral operator and the theory of matrix perturbation. Our analysis shows that when there is a large eigengap, we can improve the approximation error of the Nystr\"{o}m method from $O(N/m^{1/4})$ to $O(N/m^{1/2})$ when measured in Frobenius norm, where $N$ is the size of the kernel matrix, and $m$ is the number of sampled columns. version:1
arxiv-1208-6231 | Link Prediction via Generalized Coupled Tensor Factorisation | http://arxiv.org/abs/1208.6231 | id:1208.6231 author:Beyza Ermiş, Evrim Acar, A. Taylan Cemgil category:cs.LG  published:2012-08-30 summary:This study deals with the missing link prediction problem: the problem of predicting the existence of missing connections between entities of interest. We address link prediction using coupled analysis of relational datasets represented as heterogeneous data, i.e., datasets in the form of matrices and higher-order tensors. We propose to use an approach based on probabilistic interpretation of tensor factorisation models, i.e., Generalised Coupled Tensor Factorisation, which can simultaneously fit a large class of tensor models to higher-order tensors/matrices with com- mon latent factors using different loss functions. Numerical experiments demonstrate that joint analysis of data from multiple sources via coupled factorisation improves the link prediction performance and the selection of right loss function and tensor model is crucial for accurately predicting missing links. version:1
arxiv-1208-6137 | Benchmarking recognition results on word image datasets | http://arxiv.org/abs/1208.6137 | id:1208.6137 author:Deepak Kumar, M N Anil Prasad, A G Ramakrishnan category:cs.CV  published:2012-08-30 summary:We have benchmarked the maximum obtainable recognition accuracy on various word image datasets using manual segmentation and a currently available commercial OCR. We have developed a Matlab program, with graphical user interface, for semi-automated pixel level segmentation of word images. We discuss the advantages of pixel level annotation. We have covered five databases adding up to over 3600 word images. These word images have been cropped from camera captured scene, born-digital and street view images. We recognize the segmented word image using the trial version of Nuance Omnipage OCR. We also discuss, how the degradations introduced during acquisition or inaccuracies introduced during creation of word images affect the recognition of the word present in the image. Word images for different kinds of degradations and correction for slant and curvy nature of words are also discussed. The word recognition rates obtained on ICDAR 2003, Sign evaluation, Street view, Born-digital and ICDAR 2011 datasets are 83.9%, 89.3%, 79.6%, 88.5% and 86.7% respectively. version:1
arxiv-1208-6109 | Average word length dynamics as indicator of cultural changes in society | http://arxiv.org/abs/1208.6109 | id:1208.6109 author:Vladimir V. Bochkarev, Anna V. Shevlyakova, Valery D. Solovyev category:cs.CL 91F20 J.5  published:2012-08-30 summary:Dynamics of average length of words in Russian and English is analysed in the article. Words belonging to the diachronic text corpus Google Books Ngram and dated back to the last two centuries are studied. It was found out that average word length slightly increased in the 19th century, and then it was growing rapidly most of the 20th century and started decreasing over the period from the end of the 20th - to the beginning of the 21th century. Words which contributed mostly to increase or decrease of word average length were identified. At that, content words and functional words are analysed separately. Long content words contribute mostly to word average length of word. As it was shown, these words reflect the main tendencies of social development and thus, are used frequently. Change of frequency of personal pronouns also contributes significantly to change of average word length. The other parameters connected with average length of word were also analysed. version:1
arxiv-1204-4166 | Message passing with relaxed moment matching | http://arxiv.org/abs/1204.4166 | id:1204.4166 author:Yuan Qi, Yandong Guo category:cs.LG stat.CO stat.ML  published:2012-04-18 summary:Bayesian learning is often hampered by large computational expense. As a powerful generalization of popular belief propagation, expectation propagation (EP) efficiently approximates the exact Bayesian computation. Nevertheless, EP can be sensitive to outliers and suffer from divergence for difficult cases. To address this issue, we propose a new approximate inference approach, relaxed expectation propagation (REP). It relaxes the moment matching requirement of expectation propagation by adding a relaxation factor into the KL minimization. We penalize this relaxation with a $l_1$ penalty. As a result, when two distributions in the relaxed KL divergence are similar, the relaxation factor will be penalized to zero and, therefore, we obtain the original moment matching; In the presence of outliers, these two distributions are significantly different and the relaxation factor will be used to reduce the contribution of the outlier. Based on this penalized KL minimization, REP is robust to outliers and can greatly improve the posterior approximation quality over EP. To examine the effectiveness of REP, we apply it to Gaussian process classification, a task known to be suitable to EP. Our classification results on synthetic and UCI benchmark datasets demonstrate significant improvement of REP over EP and Power EP--in terms of algorithmic stability, estimation accuracy and predictive performance. version:2
arxiv-1111-6857 | Multivariate information measures: an experimentalist's perspective | http://arxiv.org/abs/1111.6857 | id:1111.6857 author:Nicholas Timme, Wesley Alford, Benjamin Flecker, John M. Beggs category:cs.IT cs.LG math.IT physics.data-an stat.AP 94A15  published:2011-11-28 summary:Information theory is widely accepted as a powerful tool for analyzing complex systems and it has been applied in many disciplines. Recently, some central components of information theory - multivariate information measures - have found expanded use in the study of several phenomena. These information measures differ in subtle yet significant ways. Here, we will review the information theory behind each measure, as well as examine the differences between these measures by applying them to several simple model systems. In addition to these systems, we will illustrate the usefulness of the information measures by analyzing neural spiking data from a dissociated culture through early stages of its development. We hope that this work will aid other researchers as they seek the best multivariate information measure for their specific research goals and system. Finally, we have made software available online which allows the user to calculate all of the information measures discussed within this paper. version:5
arxiv-1206-2944 | Practical Bayesian Optimization of Machine Learning Algorithms | http://arxiv.org/abs/1206.2944 | id:1206.2944 author:Jasper Snoek, Hugo Larochelle, Ryan P. Adams category:stat.ML cs.LG  published:2012-06-13 summary:Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks. version:2
arxiv-1207-6005 | The expected performance of stellar parametrization with Gaia spectrophotometry | http://arxiv.org/abs/1207.6005 | id:1207.6005 author:C. Liu, C. A. L. Bailer-Jones, R. Sordo, A. Vallenari, R. Borrachero, X. Luri, P. Sartoretti category:astro-ph.IM astro-ph.GA physics.data-an stat.ML  published:2012-07-25 summary:Gaia will obtain astrometry and spectrophotometry for essentially all sources in the sky down to a broad band magnitude limit of G=20, an expected yield of 10^9 stars. Its main scientific objective is to reveal the formation and evolution of our Galaxy through chemo-dynamical analysis. In addition to inferring positions, parallaxes and proper motions from the astrometry, we must also infer the astrophysical parameters of the stars from the spectrophotometry, the BP/RP spectrum. Here we investigate the performance of three different algorithms (SVM, ILIUM, Aeneas) for estimating the effective temperature, line-of-sight interstellar extinction, metallicity and surface gravity of A-M stars over a wide range of these parameters and over the full magnitude range Gaia will observe (G=6-20mag). One of the algorithms, Aeneas, infers the posterior probability density function over all parameters, and can optionally take into account the parallax and the Hertzsprung-Russell diagram to improve the estimates. For all algorithms the accuracy of estimation depends on G and on the value of the parameters themselves, so a broad summary of performance is only approximate. For stars at G=15 with less than two magnitudes extinction, we expect to be able to estimate Teff to within 1%, logg to 0.1-0.2dex, and [Fe/H] (for FGKM stars) to 0.1-0.2dex, just using the BP/RP spectrum (mean absolute error statistics are quoted). Performance degrades at larger extinctions, but not always by a large amount. Extinction can be estimated to an accuracy of 0.05-0.2mag for stars across the full parameter range with a priori unknown extinction between 0 and 10mag. Performance degrades at fainter magnitudes, but even at G=19 we can estimate logg to better than 0.2dex for all spectral types, and [Fe/H] to within 0.35dex for FGKM stars, for extinctions below 1mag. version:2
arxiv-1208-5451 | Are You Imitating Me? Unsupervised Sparse Modeling for Group Activity Analysis from a Single Video | http://arxiv.org/abs/1208.5451 | id:1208.5451 author:Zhongwei Tang, Alexey Castrodad, Mariano Tepper, Guillermo Sapiro category:cs.CV 94A08 I.4.7; I.5.3; I.6.4  published:2012-08-27 summary:A framework for unsupervised group activity analysis from a single video is here presented. Our working hypothesis is that human actions lie on a union of low-dimensional subspaces, and thus can be efficiently modeled as sparse linear combinations of atoms from a learned dictionary representing the action's primitives. Contrary to prior art, and with the primary goal of spatio-temporal action grouping, in this work only one single video segment is available for both unsupervised learning and analysis without any prior training information. After extracting simple features at a single spatio-temporal scale, we learn a dictionary for each individual in the video during each short time lapse. These dictionaries allow us to compare the individuals' actions by producing an affinity matrix which contains sufficient discriminative information about the actions in the scene leading to grouping with simple and efficient tools. With diverse publicly available real videos, we demonstrate the effectiveness of the proposed framework and its robustness to cluttered backgrounds, changes of human appearance, and action variability. version:1
arxiv-1208-5365 | A Missing and Found Recognition System for Hajj and Umrah | http://arxiv.org/abs/1208.5365 | id:1208.5365 author:Salah A. Aly category:cs.CV cs.CY  published:2012-08-27 summary:This note describes an integrated recognition system for identifying missing and found objects as well as missing, dead, and found people during Hajj and Umrah seasons in the two Holy cities of Makkah and Madina in the Kingdom of Saudi Arabia. It is assumed that the total estimated number of pilgrims will reach 20 millions during the next decade. The ultimate goal of this system is to integrate facial recognition and object identification solutions into the Hajj and Umrah rituals. The missing and found computerized system is part of the CrowdSensing system for Hajj and Umrah crowd estimation, management and safety. version:1
arxiv-1208-5341 | Sensitive Ants in Solving the Generalized Vehicle Routing Problem | http://arxiv.org/abs/1208.5341 | id:1208.5341 author:Camelia-M. Pintea, Camelia Chira, D. Dumitrescu, Petrica C. Pop category:cs.AI cs.NE 68T20  published:2012-08-27 summary:The idea of sensitivity in ant colony systems has been exploited in hybrid ant-based models with promising results for many combinatorial optimization problems. Heterogeneity is induced in the ant population by endowing individual ants with a certain level of sensitivity to the pheromone trail. The variable pheromone sensitivity within the same population of ants can potentially intensify the search while in the same time inducing diversity for the exploration of the environment. The performance of sensitive ant models is investigated for solving the generalized vehicle routing problem. Numerical results and comparisons are discussed and analysed with a focus on emphasizing any particular aspects and potential benefits related to hybrid ant-based models. version:1
arxiv-1208-5340 | New results of ant algorithms for the Linear Ordering Problem | http://arxiv.org/abs/1208.5340 | id:1208.5340 author:Camelia-M. Pintea, Camelia Chira, D. Dumitrescu category:cs.AI cs.NE 68T20  published:2012-08-27 summary:Ant-based algorithms are successful tools for solving complex problems. One of these problems is the Linear Ordering Problem (LOP). The paper shows new results on some LOP instances, using Ant Colony System (ACS) and the Step-Back Sensitive Ant Model (SB-SAM). version:1
arxiv-1007-3753 | Fast L1-Minimization Algorithms For Robust Face Recognition | http://arxiv.org/abs/1007.3753 | id:1007.3753 author:Allen Y. Yang, Zihan Zhou, Arvind Ganesh, S. Shankar Sastry, Yi Ma category:cs.CV cs.NA  published:2010-07-21 summary:L1-minimization refers to finding the minimum L1-norm solution to an underdetermined linear system b=Ax. Under certain conditions as described in compressive sensing theory, the minimum L1-norm solution is also the sparsest solution. In this paper, our study addresses the speed and scalability of its algorithms. In particular, we focus on the numerical implementation of a sparsity-based classification framework in robust face recognition, where sparse representation is sought to recover human identities from very high-dimensional facial images that may be corrupted by illumination, facial disguise, and pose variation. Although the underlying numerical problem is a linear program, traditional algorithms are known to suffer poor scalability for large-scale applications. We investigate a new solution based on a classical convex optimization framework, known as Augmented Lagrangian Methods (ALM). The new convex solvers provide a viable solution to real-world, time-critical applications such as face recognition. We conduct extensive experiments to validate and compare the performance of the ALM algorithms against several popular L1-minimization solvers, including interior-point method, Homotopy, FISTA, SESOP-PCD, approximate message passing (AMP) and TFOCS. To aid peer evaluation, the code for all the algorithms has been made publicly available. version:4
arxiv-1101-5184 | Bayesian Network Structure Learning with Permutation Tests | http://arxiv.org/abs/1101.5184 | id:1101.5184 author:Marco Scutari, Adriana Brogini category:stat.ML stat.ME  published:2011-01-27 summary:In literature there are several studies on the performance of Bayesian network structure learning algorithms. The focus of these studies is almost always the heuristics the learning algorithms are based on, i.e. the maximisation algorithms (in score-based algorithms) or the techniques for learning the dependencies of each variable (in constraint-based algorithms). In this paper we investigate how the use of permutation tests instead of parametric ones affects the performance of Bayesian network structure learning from discrete data. Shrinkage tests are also covered to provide a broad overview of the techniques developed in current literature. version:3
arxiv-1205-4343 | New Analysis and Algorithm for Learning with Drifting Distributions | http://arxiv.org/abs/1205.4343 | id:1205.4343 author:Mehryar Mohri, Andres Munoz Medina category:cs.LG stat.ML  published:2012-05-19 summary:We present a new analysis of the problem of learning with drifting distributions in the batch setting using the notion of discrepancy. We prove learning bounds based on the Rademacher complexity of the hypothesis set and the discrepancy of distributions both for a drifting PAC scenario and a tracking scenario. Our bounds are always tighter and in some cases substantially improve upon previous ones based on the $L_1$ distance. We also present a generalization of the standard on-line to batch conversion to the drifting scenario in terms of the discrepancy and arbitrary convex combinations of hypotheses. We introduce a new algorithm exploiting these learning guarantees, which we show can be formulated as a simple QP. Finally, we report the results of preliminary experiments demonstrating the benefits of this algorithm. version:2
arxiv-1110-1804 | The proximal point method for a hybrid model in image restoration | http://arxiv.org/abs/1110.1804 | id:1110.1804 author:Zhi-Feng Pang, Li-Lian Wang, Yu-Fei Yang category:cs.CV cs.IT math.IT math.OC  published:2011-10-09 summary:Models including two $L^1$ -norm terms have been widely used in image restoration. In this paper we first propose the alternating direction method of multipliers (ADMM) to solve this class of models. Based on ADMM, we then propose the proximal point method (PPM), which is more efficient than ADMM. Following the operator theory, we also give the convergence analysis of the proposed methods. Furthermore, we use the proposed methods to solve a class of hybrid models combining the ROF model with the LLT model. Some numerical results demonstrate the viability and efficiency of the proposed methods. version:2
arxiv-1208-5092 | Graph Degree Linkage: Agglomerative Clustering on a Directed Graph | http://arxiv.org/abs/1208.5092 | id:1208.5092 author:Wei Zhang, Xiaogang Wang, Deli Zhao, Xiaoou Tang category:cs.CV cs.SI stat.ML  published:2012-08-25 summary:This paper proposes a simple but effective graph-based agglomerative algorithm, for clustering high-dimensional data. We explore the different roles of two fundamental concepts in graph theory, indegree and outdegree, in the context of clustering. The average indegree reflects the density near a sample, and the average outdegree characterizes the local geometry around a sample. Based on such insights, we define the affinity measure of clusters via the product of average indegree and average outdegree. The product-based affinity makes our algorithm robust to noise. The algorithm has three main advantages: good performance, easy implementation, and high computational efficiency. We test the algorithm on two fundamental computer vision problems: image clustering and object matching. Extensive experiments demonstrate that it outperforms the state-of-the-arts in both applications. version:1
arxiv-1107-4390 | Multi-Task Averaging | http://arxiv.org/abs/1107.4390 | id:1107.4390 author:Sergey Feldman, Bela A. Frigyik, Maya R. Gupta category:stat.ML stat.ME  published:2011-07-21 summary:We present a multi-task learning approach to jointly estimate the means of multiple independent data sets. The proposed multi-task averaging (MTA) algorithm results in a convex combination of the single-task maximum likelihood estimates. We derive the optimal minimum risk estimator and the minimax estimator, and show that these estimators can be efficiently estimated. Simulations and real data experiments demonstrate that MTA estimators often outperform both single-task and James-Stein estimators. version:4
arxiv-1207-3056 | Non-Local Euclidean Medians | http://arxiv.org/abs/1207.3056 | id:1207.3056 author:Kunal N. Chaudhury, Amit Singer category:cs.CV cs.DS  published:2012-07-12 summary:In this letter, we note that the denoising performance of Non-Local Means (NLM) at large noise levels can be improved by replacing the mean by the Euclidean median. We call this new denoising algorithm the Non-Local Euclidean Medians (NLEM). At the heart of NLEM is the observation that the median is more robust to outliers than the mean. In particular, we provide a simple geometric insight that explains why NLEM performs better than NLM in the vicinity of edges, particularly at large noise levels. NLEM can be efficiently implemented using iteratively reweighted least squares, and its computational complexity is comparable to that of NLM. We provide some preliminary results to study the proposed algorithm and to compare it with NLM. version:2
arxiv-1208-5016 | WESD - Weighted Spectral Distance for Measuring Shape Dissimilarity | http://arxiv.org/abs/1208.5016 | id:1208.5016 author:Ender Konukoglu, Ben Glocker, Antonio Criminisi, Kilian M. Pohl category:cs.CV  published:2012-08-24 summary:This article presents a new distance for measuring shape dissimilarity between objects. Recent publications introduced the use of eigenvalues of the Laplace operator as compact shape descriptors. Here, we revisit the eigenvalues to define a proper distance, called Weighted Spectral Distance (WESD), for quantifying shape dissimilarity. The definition of WESD is derived through analysing the heat-trace. This analysis provides the proposed distance an intuitive meaning and mathematically links it to the intrinsic geometry of objects. We analyse the resulting distance definition, present and prove its important theoretical properties. Some of these properties include: i) WESD is defined over the entire sequence of eigenvalues yet it is guaranteed to converge, ii) it is a pseudometric, iii) it is accurately approximated with a finite number of eigenvalues, and iv) it can be mapped to the [0,1) interval. Lastly, experiments conducted on synthetic and real objects are presented. These experiments highlight the practical benefits of WESD for applications in vision and medical image analysis. version:1
arxiv-0202020 | The Mysterious Optimality of Naive Bayes: Estimation of the Probability in the System of "Classifiers" | http://arxiv.org/abs/cs/0202020 | id:0202020 author:Oleg Kupervasser, Alexsander Vardy category:cs.CV cs.AI  published:2002-02-17 summary:Bayes Classifiers are widely used currently for recognition, identification and knowledge discovery. The fields of application are, for example, image processing, medicine, chemistry (QSAR). But by mysterious way the Naive Bayes Classifier usually gives a very nice and good presentation of a recognition. It can not be improved considerably by more complex models of Bayes Classifier. We demonstrate here a very nice and simple proof of the Naive Bayes Classifier optimality, that can explain this interesting fact.The derivation in the current paper is based on arXiv:cs/0202020v1 version:3
arxiv-1112-3699 | Ensemble Models with Trees and Rules | http://arxiv.org/abs/1112.3699 | id:1112.3699 author:Deniz Akdemir category:stat.ML  published:2011-12-16 summary:In this article, we have proposed several approaches for post processing a large ensemble of prediction models or rules. The results from our simulations show that the post processing methods we have considered here are promising. We have used the techniques developed here for estimation of quantitative traits from markers, on the benchmark "Bostob Housing"data set and in some simulations. In most cases, the produced models had better prediction performance than, for example, the ones produced by the random forest or the rulefit algorithms. version:8
arxiv-1208-4842 | The Segmentation Fusion Method On10 Multi-Sensors | http://arxiv.org/abs/1208.4842 | id:1208.4842 author:Firouz Abdullah Al-Wassai, N. V. Kalyankar category:cs.CV  published:2012-08-23 summary:The most significant problem may be undesirable effects for the spectral signatures of fused images as well as the benefits of using fused images mostly compared to their source images were acquired at the same time by one sensor. They may or may not be suitable for the fusion of other images. It becomes therefore increasingly important to investigate techniques that allow multi-sensor, multi-date image fusion to make final conclusions can be drawn on the most suitable method of fusion. So, In this study we present a new method Segmentation Fusion method (SF) for remotely sensed images is presented by considering the physical characteristics of sensors, which uses a feature level processing paradigm. In a particularly, attempts to test the proposed method performance on 10 multi-sensor images and comparing it with different fusion techniques for estimating the quality and degree of information improvement quantitatively by using various spatial and spectral metrics. version:1
arxiv-1208-4773 | Optimized Look-Ahead Tree Policies: A Bridge Between Look-Ahead Tree Policies and Direct Policy Search | http://arxiv.org/abs/1208.4773 | id:1208.4773 author:Tobias Jung, Louis Wehenkel, Damien Ernst, Francis Maes category:cs.SY cs.AI cs.LG  published:2012-08-23 summary:Direct policy search (DPS) and look-ahead tree (LT) policies are two widely used classes of techniques to produce high performance policies for sequential decision-making problems. To make DPS approaches work well, one crucial issue is to select an appropriate space of parameterized policies with respect to the targeted problem. A fundamental issue in LT approaches is that, to take good decisions, such policies must develop very large look-ahead trees which may require excessive online computational resources. In this paper, we propose a new hybrid policy learning scheme that lies at the intersection of DPS and LT, in which the policy is an algorithm that develops a small look-ahead tree in a directed way, guided by a node scoring function that is learned through DPS. The LT-based representation is shown to be a versatile way of representing policies in a DPS scheme, while at the same time, DPS enables to significantly reduce the size of the look-ahead trees that are required to take high-quality decisions. We experimentally compare our method with two other state-of-the-art DPS techniques and four common LT policies on four benchmark domains and show that it combines the advantages of the two techniques from which it originates. In particular, we show that our method: (1) produces overall better performing policies than both pure DPS and pure LT policies, (2) requires a substantially smaller number of policy evaluations than other DPS techniques, (3) is easy to tune and (4) results in policies that are quite robust with respect to perturbations of the initial conditions. version:1
arxiv-1208-4503 | Introduction of the weight edition errors in the Levenshtein distance | http://arxiv.org/abs/1208.4503 | id:1208.4503 author:Gueddah Hicham category:cs.CL  published:2012-08-22 summary:In this paper, we present a new approach dedicated to correcting the spelling errors of the Arabic language. This approach corrects typographical errors like inserting, deleting, and permutation. Our method is inspired from the Levenshtein algorithm, and allows a finer and better scheduling than Levenshtein. The results obtained are very satisfactory and encouraging, which shows the interest of our new approach. version:1
arxiv-1008-2159 | Submodular Functions: Learnability, Structure, and Optimization | http://arxiv.org/abs/1008.2159 | id:1008.2159 author:Maria-Florina Balcan, Nicholas J. A. Harvey category:cs.DS cs.DM cs.LG  published:2010-08-12 summary:Submodular functions are discrete functions that model laws of diminishing returns and enjoy numerous algorithmic applications. They have been used in many areas, including combinatorial optimization, machine learning, and economics. In this work we study submodular functions from a learning theoretic angle. We provide algorithms for learning submodular functions, as well as lower bounds on their learnability. In doing so, we uncover several novel structural results revealing ways in which submodular functions can be both surprisingly structured and surprisingly unstructured. We provide several concrete implications of our work in other domains including algorithmic game theory and combinatorial optimization. At a technical level, this research combines ideas from many areas, including learning theory (distributional learning and PAC-style analyses), combinatorics and optimization (matroids and submodular functions), and pseudorandomness (lossless expander graphs). version:3
arxiv-1208-4411 | A non-parametric mixture model for topic modeling over time | http://arxiv.org/abs/1208.4411 | id:1208.4411 author:Avinava Dubey, Ahmed Hefny, Sinead Williamson, Eric P. Xing category:stat.ML  published:2012-08-22 summary:A single, stationary topic model such as latent Dirichlet allocation is inappropriate for modeling corpora that span long time periods, as the popularity of topics is likely to change over time. A number of models that incorporate time have been proposed, but in general they either exhibit limited forms of temporal variation, or require computationally expensive inference methods. In this paper we propose non-parametric Topics over Time (npTOT), a model for time-varying topics that allows an unbounded number of topics and exible distribution over the temporal variations in those topics' popularity. We develop a collapsed Gibbs sampler for the proposed model and compare against existing models on synthetic and real document sets. version:1
arxiv-1208-4398 | A Unified Approach for Modeling and Recognition of Individual Actions and Group Activities | http://arxiv.org/abs/1208.4398 | id:1208.4398 author:Qiang Qiu, Rama Chellappa category:cs.CV stat.ML  published:2012-08-21 summary:Recognizing group activities is challenging due to the difficulties in isolating individual entities, finding the respective roles played by the individuals and representing the complex interactions among the participants. Individual actions and group activities in videos can be represented in a common framework as they share the following common feature: both are composed of a set of low-level features describing motions, e.g., optical flow for each pixel or a trajectory for each feature point, according to a set of composition constraints in both temporal and spatial dimensions. In this paper, we present a unified model to assess the similarity between two given individual or group activities. Our approach avoids explicit extraction of individual actors, identifying and representing the inter-person interactions. With the proposed approach, retrieval from a video database can be performed through Query-by-Example; and activities can be recognized by querying videos containing known activities. The suggested video matching process can be performed in an unsupervised manner. We demonstrate the performance of our approach by recognizing a set of human actions and football plays. version:1
arxiv-1208-4316 | An Online Character Recognition System to Convert Grantha Script to Malayalam | http://arxiv.org/abs/1208.4316 | id:1208.4316 author:Sreeraj. M, Sumam Mary Idicula category:cs.CV  published:2012-08-21 summary:This paper presents a novel approach to recognize Grantha, an ancient script in South India and converting it to Malayalam, a prevalent language in South India using online character recognition mechanism. The motivation behind this work owes its credit to (i) developing a mechanism to recognize Grantha script in this modern world and (ii) affirming the strong connection among Grantha and Malayalam. A framework for the recognition of Grantha script using online character recognition is designed and implemented. The features extracted from the Grantha script comprises mainly of time-domain features based on writing direction and curvature. The recognized characters are mapped to corresponding Malayalam characters. The framework was tested on a bed of medium length manuscripts containing 9-12 sample lines and printed pages of a book titled Soundarya Lahari writtenin Grantha by Sri Adi Shankara to recognize the words and sentences. The manuscript recognition rates with the system are for Grantha as 92.11%, Old Malayalam 90.82% and for new Malayalam script 89.56%. The recognition rates of pages of the printed book are for Grantha as 96.16%, Old Malayalam script 95.22% and new Malayalam script as 92.32% respectively. These results show the efficiency of the developed system. version:1
arxiv-1208-4183 | Learning LiNGAM based on data with more variables than observations | http://arxiv.org/abs/1208.4183 | id:1208.4183 author:Shohei Shimizu category:stat.ML  published:2012-08-21 summary:A very important topic in systems biology is developing statistical methods that automatically find causal relations in gene regulatory networks with no prior knowledge of causal connectivity. Many methods have been developed for time series data. However, discovery methods based on steady-state data are often necessary and preferable since obtaining time series data can be more expensive and/or infeasible for many biological systems. A conventional approach is causal Bayesian networks. However, estimation of Bayesian networks is ill-posed. In many cases it cannot uniquely identify the underlying causal network and only gives a large class of equivalent causal networks that cannot be distinguished between based on the data distribution. We propose a new discovery algorithm for uniquely identifying the underlying causal network of genes. To the best of our knowledge, the proposed method is the first algorithm for learning gene networks based on a fully identifiable causal model called LiNGAM. We here compare our algorithm with competing algorithms using artificially-generated data, although it is definitely better to test it based on real microarray gene expression data. version:1
arxiv-1208-4138 | Semi-supervised Clustering Ensemble by Voting | http://arxiv.org/abs/1208.4138 | id:1208.4138 author:Ashraf Mohammed Iqbal, Abidalrahman Moh'd, Zahoor Khan category:cs.LG stat.ML  published:2012-08-20 summary:Clustering ensemble is one of the most recent advances in unsupervised learning. It aims to combine the clustering results obtained using different algorithms or from different runs of the same clustering algorithm for the same data set, this is accomplished using on a consensus function, the efficiency and accuracy of this method has been proven in many works in literature. In the first part of this paper we make a comparison among current approaches to clustering ensemble in literature. All of these approaches consist of two main steps: the ensemble generation and consensus function. In the second part of the paper, we suggest engaging supervision in the clustering ensemble procedure to get more enhancements on the clustering results. Supervision can be applied in two places: either by using semi-supervised algorithms in the clustering ensemble generation step or in the form of a feedback used by the consensus function stage. Also, we introduce a flexible two parameter weighting mechanism, the first parameter describes the compatibility between the datasets under study and the semi-supervised clustering algorithms used to generate the base partitions, the second parameter is used to provide the user feedback on the these partitions. The two parameters are engaged in a "relabeling and voting" based consensus function to produce the final clustering. version:1
arxiv-1208-4079 | Recent Technological Advances in Natural Language Processing and Artificial Intelligence | http://arxiv.org/abs/1208.4079 | id:1208.4079 author:Nishal Pradeepkumar Shah category:cs.CL I.2.7  published:2012-08-20 summary:A recent advance in computer technology has permitted scientists to implement and test algorithms that were known from quite some time (or not) but which were computationally expensive. Two such projects are IBM's Jeopardy as a part of its DeepQA project [1] and Wolfram's Wolframalpha[2]. Both these methods implement natural language processing (another goal of AI scientists) and try to answer questions as asked by the user. Though the goal of the two projects is similar, both of them have a different procedure at it's core. In the following sections, the mechanism and history of IBM's Jeopardy and Wolfram alpha has been explained followed by the implications of these projects in realizing Ray Kurzweil's [3] dream of passing the Turing test by 2029. A recipe of taking the above projects to a new level is also explained. version:1
arxiv-1208-4009 | Learning sparse messages in networks of neural cliques | http://arxiv.org/abs/1208.4009 | id:1208.4009 author:Behrooz Kamary Aliabadi, Claude Berrou, Vincent Gripon, Xiaoran Jiang category:cs.NE  published:2012-08-20 summary:An extension to a recently introduced binary neural network is proposed in order to allow the learning of sparse messages, in large numbers and with high memory efficiency. This new network is justified both in biological and informational terms. The learning and retrieval rules are detailed and illustrated by various simulation results. version:1
arxiv-1208-3943 | Performance Tuning Of J48 Algorithm For Prediction Of Soil Fertility | http://arxiv.org/abs/1208.3943 | id:1208.3943 author:Jay Gholap category:cs.LG cs.DB cs.PF stat.ML  published:2012-08-20 summary:Data mining involves the systematic analysis of large data sets, and data mining in agricultural soil datasets is exciting and modern research area. The productive capacity of a soil depends on soil fertility. Achieving and maintaining appropriate levels of soil fertility, is of utmost importance if agricultural land is to remain capable of nourishing crop production. In this research, Steps for building a predictive model of soil fertility have been explained. This paper aims at predicting soil fertility class using decision tree algorithms in data mining . Further, it focuses on performance tuning of J48 decision tree algorithm with the help of meta-techniques such as attribute selection and boosting. version:1
arxiv-1107-1736 | High-dimensional structure estimation in Ising models: Local separation criterion | http://arxiv.org/abs/1107.1736 | id:1107.1736 author:Animashree Anandkumar, Vincent Y. F. Tan, Furong Huang, Alan S. Willsky category:stat.ML cs.LG math.ST stat.TH  published:2011-07-08 summary:We consider the problem of high-dimensional Ising (graphical) model selection. We propose a simple algorithm for structure estimation based on the thresholding of the empirical conditional variation distances. We introduce a novel criterion for tractable graph families, where this method is efficient, based on the presence of sparse local separators between node pairs in the underlying graph. For such graphs, the proposed algorithm has a sample complexity of $n=\Omega(J_{\min}^{-2}\log p)$, where $p$ is the number of variables, and $J_{\min}$ is the minimum (absolute) edge potential in the model. We also establish nonasymptotic necessary and sufficient conditions for structure estimation. version:4
arxiv-1209-1300 | Input Scheme for Hindi Using Phonetic Mapping | http://arxiv.org/abs/1209.1300 | id:1209.1300 author:Nisheeth Joshi, Iti Mathur category:cs.CL  published:2012-08-19 summary:Written Communication on Computers requires knowledge of writing text for the desired language using Computer. Mostly people do not use any other language besides English. This creates a barrier. To resolve this issue we have developed a scheme to input text in Hindi using phonetic mapping scheme. Using this scheme we generate intermediate code strings and match them with pronunciations of input text. Our system show significant success over other input systems available. version:1
arxiv-1209-1301 | Evaluation of Computational Grammar Formalisms for Indian Languages | http://arxiv.org/abs/1209.1301 | id:1209.1301 author:Nisheeth Joshi, Iti Mathur category:cs.CL  published:2012-08-19 summary:Natural Language Parsing has been the most prominent research area since the genesis of Natural Language Processing. Probabilistic Parsers are being developed to make the process of parser development much easier, accurate and fast. In Indian context, identification of which Computational Grammar Formalism is to be used is still a question which needs to be answered. In this paper we focus on this problem and try to analyze different formalisms for Indian languages. version:1
arxiv-1204-2765 | A practical approach to language complexity: a Wikipedia case study | http://arxiv.org/abs/1204.2765 | id:1204.2765 author:Taha Yasseri, András Kornai, János Kertész category:cs.CL physics.data-an physics.soc-ph  published:2012-04-12 summary:In this paper we present statistical analysis of English texts from Wikipedia. We try to address the issue of language complexity empirically by comparing the simple English Wikipedia (Simple) to comparable samples of the main English Wikipedia (Main). Simple is supposed to use a more simplified language with a limited vocabulary, and editors are explicitly requested to follow this guideline, yet in practice the vocabulary richness of both samples are at the same level. Detailed analysis of longer units (n-grams of words and part of speech tags) shows that the language of Simple is less complex than that of Main primarily due to the use of shorter sentences, as opposed to drastically simplified syntax or vocabulary. Comparing the two language varieties by the Gunning readability index supports this conclusion. We also report on the topical dependence of language complexity, e.g. that the language is more advanced in conceptual articles compared to person-based (biographical) and object-based articles. Finally, we investigate the relation between conflict and language complexity by analyzing the content of the talk pages associated to controversial and peacefully developing articles, concluding that controversy has the effect of reducing language complexity. version:2
arxiv-1205-3137 | Unsupervised Discovery of Mid-Level Discriminative Patches | http://arxiv.org/abs/1205.3137 | id:1205.3137 author:Saurabh Singh, Abhinav Gupta, Alexei A. Efros category:cs.CV cs.AI cs.LG  published:2012-05-14 summary:The goal of this paper is to discover a set of discriminative patches which can serve as a fully unsupervised mid-level visual representation. The desired patches need to satisfy two requirements: 1) to be representative, they need to occur frequently enough in the visual world; 2) to be discriminative, they need to be different enough from the rest of the visual world. The patches could correspond to parts, objects, "visual phrases", etc. but are not restricted to be any one of them. We pose this as an unsupervised discriminative clustering problem on a huge dataset of image patches. We use an iterative procedure which alternates between clustering and training discriminative classifiers, while applying careful cross-validation at each step to prevent overfitting. The paper experimentally demonstrates the effectiveness of discriminative patches as an unsupervised mid-level visual representation, suggesting that it could be used in place of visual words for many tasks. Furthermore, discriminative patches can also be used in a supervised regime, such as scene classification, where they demonstrate state-of-the-art performance on the MIT Indoor-67 dataset. version:2
arxiv-1208-3723 | Image Super-Resolution via Dual-Dictionary Learning And Sparse Representation | http://arxiv.org/abs/1208.3723 | id:1208.3723 author:Jian Zhang, Chen Zhao, Ruiqin Xiong, Siwei Ma, Debin Zhao category:cs.CV  published:2012-08-18 summary:Learning-based image super-resolution aims to reconstruct high-frequency (HF) details from the prior model trained by a set of high- and low-resolution image patches. In this paper, HF to be estimated is considered as a combination of two components: main high-frequency (MHF) and residual high-frequency (RHF), and we propose a novel image super-resolution method via dual-dictionary learning and sparse representation, which consists of the main dictionary learning and the residual dictionary learning, to recover MHF and RHF respectively. Extensive experimental results on test images validate that by employing the proposed two-layer progressive scheme, more image details can be recovered and much better results can be achieved than the state-of-the-art algorithms in terms of both PSNR and visual perception. version:1
arxiv-1208-3689 | An improvement direction for filter selection techniques using information theory measures and quadratic optimization | http://arxiv.org/abs/1208.3689 | id:1208.3689 author:Waad Bouaguel, Ghazi Bel Mufti category:cs.LG cs.IT math.IT  published:2012-08-17 summary:Filter selection techniques are known for their simplicity and efficiency. However this kind of methods doesn't take into consideration the features inter-redundancy. Consequently the un-removed redundant features remain in the final classification model, giving lower generalization performance. In this paper we propose to use a mathematical optimization method that reduces inter-features redundancy and maximize relevance between each feature and the target variable. version:1
arxiv-1208-3687 | Information-theoretic Dictionary Learning for Image Classification | http://arxiv.org/abs/1208.3687 | id:1208.3687 author:Qiang Qiu, Vishal M. Patel, Rama Chellappa category:cs.CV cs.IT math.IT stat.ML  published:2012-08-17 summary:We present a two-stage approach for learning dictionaries for object classification tasks based on the principle of information maximization. The proposed method seeks a dictionary that is compact, discriminative, and generative. In the first stage, dictionary atoms are selected from an initial dictionary by maximizing the mutual information measure on dictionary compactness, discrimination and reconstruction. In the second stage, the selected dictionary atoms are updated for improved reconstructive and discriminative power using a simple gradient ascent algorithm on mutual information. Experiments using real datasets demonstrate the effectiveness of our approach for image classification tasks. version:1
arxiv-1208-3600 | Modeling and Control of CSTR using Model based Neural Network Predictive Control | http://arxiv.org/abs/1208.3600 | id:1208.3600 author:Piyush Shrivastava category:cs.AI cs.NE nlin.AO  published:2012-08-17 summary:This paper presents a predictive control strategy based on neural network model of the plant is applied to Continuous Stirred Tank Reactor (CSTR). This system is a highly nonlinear process; therefore, a nonlinear predictive method, e.g., neural network predictive control, can be a better match to govern the system dynamics. In the paper, the NN model and the way in which it can be used to predict the behavior of the CSTR process over a certain prediction horizon are described, and some comments about the optimization procedure are made. Predictive control algorithm is applied to control the concentration in a continuous stirred tank reactor (CSTR), whose parameters are optimally determined by solving quadratic performance index using the optimization algorithm. An efficient control of the product concentration in cstr can be achieved only through accurate model. Here an attempt is made to alleviate the modeling difficulties using Artificial Intelligent technique such as Neural Network. Simulation results demonstrate the feasibility and effectiveness of the NNMPC technique. version:1
arxiv-1208-3530 | Leveraging Subjective Human Annotation for Clustering Historic Newspaper Articles | http://arxiv.org/abs/1208.3530 | id:1208.3530 author:Haimonti Dutta, William Chan, Deepak Shankargouda, Manoj Pooleery, Axinia Radeva, Kyle Rego, Boyi Xie, Rebecca Passonneau, Austin Lee, Barbara Taranto category:cs.IR cs.CL cs.DL  published:2012-08-17 summary:The New York Public Library is participating in the Chronicling America initiative to develop an online searchable database of historically significant newspaper articles. Microfilm copies of the newspapers are scanned and high resolution Optical Character Recognition (OCR) software is run on them. The text from the OCR provides a wealth of data and opinion for researchers and historians. However, categorization of articles provided by the OCR engine is rudimentary and a large number of the articles are labeled editorial without further grouping. Manually sorting articles into fine-grained categories is time consuming if not impossible given the size of the corpus. This paper studies techniques for automatic categorization of newspaper articles so as to enhance search and retrieval on the archive. We explore unsupervised (e.g. KMeans) and semi-supervised (e.g. constrained clustering) learning algorithms to develop article categorization schemes geared towards the needs of end-users. A pilot study was designed to understand whether there was unanimous agreement amongst patrons regarding how articles can be categorized. It was found that the task was very subjective and consequently automated algorithms that could deal with subjective labels were used. While the small scale pilot study was extremely helpful in designing machine learning algorithms, a much larger system needs to be developed to collect annotations from users of the archive. The "BODHI" system currently being developed is a step in that direction, allowing users to correct wrongly scanned OCR and providing keywords and tags for newspaper articles used frequently. On successful implementation of the beta version of this system, we hope that it can be integrated with existing software being developed for the Chronicling America project. version:1
arxiv-1106-1157 | Bayesian and L1 Approaches to Sparse Unsupervised Learning | http://arxiv.org/abs/1106.1157 | id:1106.1157 author:Shakir Mohamed, Katherine Heller, Zoubin Ghahramani category:cs.LG cs.AI stat.ML  published:2011-06-06 summary:The use of L1 regularisation for sparse learning has generated immense research interest, with successful application in such diverse areas as signal acquisition, image coding, genomics and collaborative filtering. While existing work highlights the many advantages of L1 methods, in this paper we find that L1 regularisation often dramatically underperforms in terms of predictive performance when compared with other methods for inferring sparsity. We focus on unsupervised latent variable models, and develop L1 minimising factor models, Bayesian variants of "L1", and Bayesian models with a stronger L0-like sparsity induced through spike-and-slab distributions. These spike-and-slab Bayesian factor models encourage sparsity while accounting for uncertainty in a principled manner and avoiding unnecessary shrinkage of non-zero values. We demonstrate on a number of data sets that in practice spike-and-slab Bayesian methods outperform L1 minimisation, even on a computational budget. We thus highlight the need to re-assess the wide use of L1 methods in sparsity-reliant applications, particularly when we care about generalising to previously unseen data, and provide an alternative that, over many varying conditions, provides improved generalisation performance. version:3
arxiv-1208-3512 | Contour Completion Around a Fixation Point | http://arxiv.org/abs/1208.3512 | id:1208.3512 author:Toshiro Kubota category:cs.CV  published:2012-08-16 summary:The paper presents two edge grouping algorithms for finding a closed contour starting from a particular edge point and enclosing a fixation point. Both algorithms search a shortest simple cycle in \textit{an angularly ordered graph} derived from an edge image where a vertex is an end point of a contour fragment and an undirected arc is drawn between a pair of end-points whose visual angle from the fixation point is less than a threshold value, which is set to $\pi/2$ in our experiments. The first algorithm restricts the search space by disregarding arcs that cross the line extending from the fixation point to the starting point. The second algorithm improves the solution of the first algorithm in a greedy manner. The algorithms were tested with a large number of natural images with manually placed fixation and starting points. The results are promising. version:1
arxiv-1208-6310 | Automated Marble Plate Classification System Based On Different Neural Network Input Training Sets and PLC Implementation | http://arxiv.org/abs/1208.6310 | id:1208.6310 author:Irina Topalova category:cs.NE cs.LG  published:2012-08-16 summary:The process of sorting marble plates according to their surface texture is an important task in the automated marble plate production. Nowadays some inspection systems in marble industry that automate the classification tasks are too expensive and are compatible only with specific technological equipment in the plant. In this paper a new approach to the design of an Automated Marble Plate Classification System (AMPCS),based on different neural network input training sets is proposed, aiming at high classification accuracy using simple processing and application of only standard devices. It is based on training a classification MLP neural network with three different input training sets: extracted texture histograms, Discrete Cosine and Wavelet Transform over the histograms. The algorithm is implemented in a PLC for real-time operation. The performance of the system is assessed with each one of the input training sets. The experimental test results regarding classification accuracy and quick operation are represented and discussed. version:1
arxiv-1208-3133 | Color Image Compression Algorithm Based on the DCT Blocks | http://arxiv.org/abs/1208.3133 | id:1208.3133 author:Walaa M. Abd-Elhafiez, Wajeb Gharibi category:cs.CV  published:2012-08-15 summary:This paper presents the performance of different blockbased discrete cosine transform (DCT) algorithms for compressing color image. In this RGB component of color image are converted to YCbCr before DCT transform is applied. Y is luminance component;Cb and Cr are chrominance components of the image. The modification of the image data is done based on the classification of image blocks to edge blocks and non-edge blocks, then the edge block of the image is compressed with low compression and the nonedge blocks is compressed with high compression. The analysis results have indicated that the performance of the suggested method is much better, where the constructed images are less distorted and compressed with higher factor. version:1
arxiv-1208-3047 | Parallelization of Maximum Entropy POS Tagging for Bahasa Indonesia with MapReduce | http://arxiv.org/abs/1208.3047 | id:1208.3047 author:Arif Nurwidyantoro, Edi Winarko category:cs.DC cs.CL  published:2012-08-15 summary:In this paper, MapReduce programming model is used to parallelize training and tagging proceess in Maximum Entropy part of speech tagging for Bahasa Indonesia. In training process, MapReduce model is implemented dictionary, tagtoken, and feature creation. In tagging process, MapReduce is implemented to tag lines of document in parallel. The training experiments showed that total training time using MapReduce is faster, but its result reading time inside the process slow down the total training time. The tagging experiments using different number of map and reduce process showed that MapReduce implementation could speedup the tagging process. The fastest tagging result is showed by tagging process using 1,000,000 word corpus and 30 map process. version:1
arxiv-1209-1048 | Performance Analysis Of Neuro Genetic Algorithm Applied On Detecting Proportion Of Components In Manhole Gas Mixture | http://arxiv.org/abs/1209.1048 | id:1209.1048 author:Varun Kumar Ojha, Paramartha Dutta, Hiranmay Saha category:cs.NE cs.CV  published:2012-08-15 summary:The article presents performance analysis of a real valued neuro genetic algorithm applied for the detection of proportion of the gases found in manhole gas mixture. The neural network (NN) trained using genetic algorithm (GA) leads to concept of neuro genetic algorithm, which is used for implementing an intelligent sensory system for the detection of component gases present in manhole gas mixture Usually a manhole gas mixture contains several toxic gases like Hydrogen Sulfide, Ammonia, Methane, Carbon Dioxide, Nitrogen Oxide, and Carbon Monoxide. A semiconductor based gas sensor array used for sensing manhole gas components is an integral part of the proposed intelligent system. It consists of many sensor elements, where each sensor element is responsible for sensing particular gas component. Multiple sensors of different gases used for detecting gas mixture of multiple gases, results in cross-sensitivity. The cross-sensitivity is a major issue and the problem is viewed as pattern recognition problem. The objective of this article is to present performance analysis of the real valued neuro genetic algorithm which is applied for multiple gas detection. version:1
arxiv-1208-3014 | Efficient Algorithm for Extremely Large Multi-task Regression with Massive Structured Sparsity | http://arxiv.org/abs/1208.3014 | id:1208.3014 author:Seunghak Lee, Eric P. Xing category:stat.ML q-bio.QM  published:2012-08-15 summary:We develop a highly scalable optimization method called "hierarchical group-thresholding" for solving a multi-task regression model with complex structured sparsity constraints on both input and output spaces. Despite the recent emergence of several efficient optimization algorithms for tackling complex sparsity-inducing regularizers, true scalability in practical high-dimensional problems where a huge amount (e.g., millions) of sparsity patterns need to be enforced remains an open challenge, because all existing algorithms must deal with ALL such patterns exhaustively in every iteration, which is computationally prohibitive. Our proposed algorithm addresses the scalability problem by screening out multiple groups of coefficients simultaneously and systematically. We employ a hierarchical tree representation of group constraints to accelerate the process of removing irrelevant constraints by taking advantage of the inclusion relationships between group sparsities, thereby avoiding dealing with all constraints in every optimization step, and necessitating optimization operation only on a small number of outstanding coefficients. In our experiments, we demonstrate the efficiency of our method on simulation datasets, and in an application of detecting genetic variants associated with gene expression traits. version:1
arxiv-1208-3001 | More than Word Frequencies: Authorship Attribution via Natural Frequency Zoned Word Distribution Analysis | http://arxiv.org/abs/1208.3001 | id:1208.3001 author:Zhili Chen, Liusheng Huang, Wei Yang, Peng Meng, Haibo Miao category:cs.CL  published:2012-08-15 summary:With such increasing popularity and availability of digital text data, authorships of digital texts can not be taken for granted due to the ease of copying and parsing. This paper presents a new text style analysis called natural frequency zoned word distribution analysis (NFZ-WDA), and then a basic authorship attribution scheme and an open authorship attribution scheme for digital texts based on the analysis. NFZ-WDA is based on the observation that all authors leave distinct intrinsic word usage traces on texts written by them and these intrinsic styles can be identified and employed to analyze the authorship. The intrinsic word usage styles can be estimated through the analysis of word distribution within a text, which is more than normal word frequency analysis and can be expressed as: which groups of words are used in the text; how frequently does each group of words occur; how are the occurrences of each group of words distributed in the text. Next, the basic authorship attribution scheme and the open authorship attribution scheme provide solutions for both closed and open authorship attribution problems. Through analysis and extensive experimental studies, this paper demonstrates the efficiency of the proposed method for authorship attribution. version:1
arxiv-1208-2925 | Using Program Synthesis for Social Recommendations | http://arxiv.org/abs/1208.2925 | id:1208.2925 author:Alvin Cheung, Armando Solar-Lezama, Samuel Madden category:cs.LG cs.DB cs.PL cs.SI physics.soc-ph  published:2012-08-14 summary:This paper presents a new approach to select events of interest to a user in a social media setting where events are generated by the activities of the user's friends through their mobile devices. We argue that given the unique requirements of the social media setting, the problem is best viewed as an inductive learning problem, where the goal is to first generalize from the users' expressed "likes" and "dislikes" of specific events, then to produce a program that can be manipulated by the system and distributed to the collection devices to collect only data of interest. The key contribution of this paper is a new algorithm that combines existing machine learning techniques with new program synthesis technology to learn users' preferences. We show that when compared with the more standard approaches, our new algorithm provides up to order-of-magnitude reductions in model training time, and significantly higher prediction accuracies for our target application. The approach also improves on standard machine learning techniques in that it produces clear programs that can be manipulated to optimize data collection and filtering. version:1
arxiv-1206-0304 | Predictive Information Rate in Discrete-time Gaussian Processes | http://arxiv.org/abs/1206.0304 | id:1206.0304 author:Samer A. Abdallah, Mark D. Plumbley category:stat.ML math.ST stat.TH  published:2012-06-01 summary:We derive expressions for the predicitive information rate (PIR) for the class of autoregressive Gaussian processes AR(N), both in terms of the prediction coefficients and in terms of the power spectral density. The latter result suggests a duality between the PIR and the multi-information rate for processes with mutually inverse power spectra (i.e. with poles and zeros of the transfer function exchanged). We investigate the behaviour of the PIR in relation to the multi-information rate for some simple examples, which suggest, somewhat counter-intuitively, that the PIR is maximised for very `smooth' AR processes whose power spectra have multiple poles at zero frequency. We also obtain results for moving average Gaussian processes which are consistent with the duality conjectured earlier. One consequence of this is that the PIR is unbounded for MA(N) processes. version:2
arxiv-1208-3145 | Metric distances derived from cosine similarity and Pearson and Spearman correlations | http://arxiv.org/abs/1208.3145 | id:1208.3145 author:Stijn van Dongen, Anton J. Enright category:stat.ME cs.LG  published:2012-08-14 summary:We investigate two classes of transformations of cosine similarity and Pearson and Spearman correlations into metric distances, utilising the simple tool of metric-preserving functions. The first class puts anti-correlated objects maximally far apart. Previously known transforms fall within this class. The second class collates correlated and anti-correlated objects. An example of such a transformation that yields a metric distance is the sine function when applied to centered data. version:1
arxiv-1208-2808 | Analysis of a Statistical Hypothesis Based Learning Mechanism for Faster crawling | http://arxiv.org/abs/1208.2808 | id:1208.2808 author:Sudarshan Nandy, Partha Pratim Sarkar, Achintya Das category:cs.LG cs.IR  published:2012-08-14 summary:The growth of world-wide-web (WWW) spreads its wings from an intangible quantities of web-pages to a gigantic hub of web information which gradually increases the complexity of crawling process in a search engine. A search engine handles a lot of queries from various parts of this world, and the answers of it solely depend on the knowledge that it gathers by means of crawling. The information sharing becomes a most common habit of the society, and it is done by means of publishing structured, semi-structured and unstructured resources on the web. This social practice leads to an exponential growth of web-resource, and hence it became essential to crawl for continuous updating of web-knowledge and modification of several existing resources in any situation. In this paper one statistical hypothesis based learning mechanism is incorporated for learning the behavior of crawling speed in different environment of network, and for intelligently control of the speed of crawler. The scaling technique is used to compare the performance proposed method with the standard crawler. The high speed performance is observed after scaling, and the retrieval of relevant web-resource in such a high speed is analyzed. version:1
arxiv-1208-2777 | A Method for Selecting Noun Sense using Co-occurrence Relation in English-Korean Translation | http://arxiv.org/abs/1208.2777 | id:1208.2777 author:Hyonil Kim, Changil Choe category:cs.CL  published:2012-08-14 summary:The sense analysis is still critical problem in machine translation system, especially such as English-Korean translation which the syntactical different between source and target languages is very great. We suggest a method for selecting the noun sense using contextual feature in English-Korean Translation. version:1
arxiv-1208-2873 | Detecting Events and Patterns in Large-Scale User Generated Textual Streams with Statistical Learning Methods | http://arxiv.org/abs/1208.2873 | id:1208.2873 author:Vasileios Lampos category:cs.LG cs.CL cs.IR cs.SI stat.AP stat.ML  published:2012-08-13 summary:A vast amount of textual web streams is influenced by events or phenomena emerging in the real world. The social web forms an excellent modern paradigm, where unstructured user generated content is published on a regular basis and in most occasions is freely distributed. The present Ph.D. Thesis deals with the problem of inferring information - or patterns in general - about events emerging in real life based on the contents of this textual stream. We show that it is possible to extract valuable information about social phenomena, such as an epidemic or even rainfall rates, by automatic analysis of the content published in Social Media, and in particular Twitter, using Statistical Machine Learning methods. An important intermediate task regards the formation and identification of features which characterise a target event; we select and use those textual features in several linear, non-linear and hybrid inference approaches achieving a significantly good performance in terms of the applied loss function. By examining further this rich data set, we also propose methods for extracting various types of mood signals revealing how affective norms - at least within the social web's population - evolve during the day and how significant events emerging in the real world are influencing them. Lastly, we present some preliminary findings showing several spatiotemporal characteristics of this textual information as well as the potential of using it to tackle tasks such as the prediction of voting intentions. version:1
arxiv-1208-2655 | Stable Segmentation of Digital Image | http://arxiv.org/abs/1208.2655 | id:1208.2655 author:M. Kharinov category:cs.CV  published:2012-08-13 summary:In the paper the optimal image segmentation by means of piecewise constant approximations is considered. The optimality is defined by a minimum value of the total squared error or by equivalent value of standard deviation of the approximation from the image. The optimal approximations are defined independently on the method of their obtaining and might be generated in different algorithms. We investigate the computation of the optimal approximation on the grounds of stability with respect to a given set of modifications. To obtain the optimal approximation the Mumford-Shuh model is generalized and developed, which in the computational part is combined with the Otsu method in multi-thresholding version. The proposed solution is proved analytically and experimentally on the example of the standard image. version:1
arxiv-1208-2651 | A Plea for Neutral Comparison Studies in Computational Sciences | http://arxiv.org/abs/1208.2651 | id:1208.2651 author:Anne-Laure Boulesteix, Manuel J. A. Eugster category:stat.CO cs.CV stat.ME stat.ML  published:2012-08-13 summary:In a context where most published articles are devoted to the development of "new methods", comparison studies are generally appreciated by readers but surprisingly given poor consideration by many scientific journals. In connection with recent articles on over-optimism and epistemology published in Bioinformatics, this letter stresses the importance of neutral comparison studies for the objective evaluation of existing methods and the establishment of standards by drawing parallels with clinical research. version:1
arxiv-1107-2021 | Multi-Instance Learning with Any Hypothesis Class | http://arxiv.org/abs/1107.2021 | id:1107.2021 author:Sivan Sabato, Naftali Tishby category:cs.LG stat.ML  published:2011-07-11 summary:In the supervised learning setting termed Multiple-Instance Learning (MIL), the examples are bags of instances, and the bag label is a function of the labels of its instances. Typically, this function is the Boolean OR. The learner observes a sample of bags and the bag labels, but not the instance labels that determine the bag labels. The learner is then required to emit a classification rule for bags based on the sample. MIL has numerous applications, and many heuristic algorithms have been used successfully on this problem, each adapted to specific settings or applications. In this work we provide a unified theoretical analysis for MIL, which holds for any underlying hypothesis class, regardless of a specific application or problem domain. We show that the sample complexity of MIL is only poly-logarithmically dependent on the size of the bag, for any underlying hypothesis class. In addition, we introduce a new PAC-learning algorithm for MIL, which uses a regular supervised learning algorithm as an oracle. We prove that efficient PAC-learning for MIL can be generated from any efficient non-MIL supervised learning algorithm that handles one-sided error. The computational complexity of the resulting algorithm is only polynomially dependent on the bag size. version:3
arxiv-1208-2572 | Nonparametric sparsity and regularization | http://arxiv.org/abs/1208.2572 | id:1208.2572 author:Lorenzo Rosasco, Silvia Villa, Sofia Mosci, Matteo Santoro, Alessandro verri category:stat.ML cs.LG math.OC  published:2012-08-13 summary:In this work we are interested in the problems of supervised learning and variable selection when the input-output dependence is described by a nonlinear function depending on a few variables. Our goal is to consider a sparse nonparametric model, hence avoiding linear or additive models. The key idea is to measure the importance of each variable in the model by making use of partial derivatives. Based on this intuition we propose a new notion of nonparametric sparsity and a corresponding least squares regularization scheme. Using concepts and results from the theory of reproducing kernel Hilbert spaces and proximal methods, we show that the proposed learning algorithm corresponds to a minimization problem which can be provably solved by an iterative procedure. The consistency properties of the obtained estimator are studied both in terms of prediction and selection performance. An extensive empirical analysis shows that the proposed method performs favorably with respect to the state-of-the-art methods. version:1
arxiv-1208-6028 | Design of Low Noise Amplifiers Using Particle Swarm Optimization | http://arxiv.org/abs/1208.6028 | id:1208.6028 author:Sadik Ulker category:cs.NE  published:2012-08-13 summary:This short paper presents a work on the design of low noise microwave amplifiers using particle swarm optimization (PSO) technique. Particle Swarm Optimization is used as a method that is applied to a single stage amplifier circuit to meet two criteria: desired gain and desired low noise. The aim is to get the best optimized design using the predefined constraints for gain and low noise values. The code is written to apply the algorithm to meet the desired goals and the obtained results are verified using different simulators. The results obtained show that PSO can be applied very efficiently for this kind of design problems with multiple constraints. version:1
arxiv-1208-2523 | Path Integral Control by Reproducing Kernel Hilbert Space Embedding | http://arxiv.org/abs/1208.2523 | id:1208.2523 author:Konrad Rawlik, Marc Toussaint, Sethu Vijayakumar category:cs.LG stat.ML  published:2012-08-13 summary:We present an embedding of stochastic optimal control problems, of the so called path integral form, into reproducing kernel Hilbert spaces. Using consistent, sample based estimates of the embedding leads to a model free, non-parametric approach for calculation of an approximate solution to the control problem. This formulation admits a decomposition of the problem into an invariant and task dependent component. Consequently, we make much more efficient use of the sample data compared to previous sample based approaches in this domain, e.g., by allowing sample re-use across tasks. Numerical examples on test problems, which illustrate the sample efficiency, are provided. version:1
arxiv-1208-2437 | An Efficient Genetic Programming System with Geometric Semantic Operators and its Application to Human Oral Bioavailability Prediction | http://arxiv.org/abs/1208.2437 | id:1208.2437 author:Mauro Castelli, Luca Manzoni, Leonardo Vanneschi category:cs.NE  published:2012-08-12 summary:Very recently new genetic operators, called geometric semantic operators, have been defined for genetic programming. Contrarily to standard genetic operators, which are uniquely based on the syntax of the individuals, these new operators are based on their semantics, meaning with it the set of input-output pairs on training data. Furthermore, these operators present the interesting property of inducing a unimodal fitness landscape for every problem that consists in finding a match between given input and output data (for instance regression and classification). Nevertheless, the current definition of these operators has a serious limitation: they impose an exponential growth in the size of the individuals in the population, so their use is impossible in practice. This paper is intended to overcome this limitation, presenting a new genetic programming system that implements geometric semantic operators in an extremely efficient way. To demonstrate the power of the proposed system, we use it to solve a complex real-life application in the field of pharmacokinetic: the prediction of the human oral bioavailability of potential new drugs. Besides the excellent performances on training data, which were expected because the fitness landscape is unimodal, we also report an excellent generalization ability of the proposed system, at least for the studied application. In fact, it outperforms standard genetic programming and a wide set of other well-known machine learning methods. version:1
arxiv-1208-2417 | How to sample if you must: on optimal functional sampling | http://arxiv.org/abs/1208.2417 | id:1208.2417 author:Assaf Hallak, Shie Mannor category:stat.ML cs.LG  published:2012-08-12 summary:We examine a fundamental problem that models various active sampling setups, such as network tomography. We analyze sampling of a multivariate normal distribution with an unknown expectation that needs to be estimated: in our setup it is possible to sample the distribution from a given set of linear functionals, and the difficulty addressed is how to optimally select the combinations to achieve low estimation error. Although this problem is in the heart of the field of optimal design, no efficient solutions for the case with many functionals exist. We present some bounds and an efficient sub-optimal solution for this problem for more structured sets such as binary functionals that are induced by graph walks. version:1
arxiv-1208-6025 | Feasibility of Genetic Algorithm for Textile Defect Classification Using Neural Network | http://arxiv.org/abs/1208.6025 | id:1208.6025 author:Md. Tarek Habib, Rahat Hossain Faisal, M. Rokonuzzaman category:cs.NE  published:2012-08-11 summary:The global market for textile industry is highly competitive nowadays. Quality control in production process in textile industry has been a key factor for retaining existence in such competitive market. Automated textile inspection systems are very useful in this respect, because manual inspection is time consuming and not accurate enough. Hence, automated textile inspection systems have been drawing plenty of attention of the researchers of different countries in order to replace manual inspection. Defect detection and defect classification are the two major problems that are posed by the research of automated textile inspection systems. In this paper, we perform an extensive investigation on the applicability of genetic algorithm (GA) in the context of textile defect classification using neural network (NN). We observe the effect of tuning different network parameters and explain the reasons. We empirically find a suitable NN model in the context of textile defect classification. We compare the performance of this model with that of the classification models implemented by others. version:1
arxiv-1208-2345 | A Large Population Size Can Be Unhelpful in Evolutionary Algorithms | http://arxiv.org/abs/1208.2345 | id:1208.2345 author:Tianshi Chen, Ke Tang, Guoliang Chen, Xin Yao category:cs.NE  published:2012-08-11 summary:The utilization of populations is one of the most important features of evolutionary algorithms (EAs). There have been many studies analyzing the impact of different population sizes on the performance of EAs. However, most of such studies are based computational experiments, except for a few cases. The common wisdom so far appears to be that a large population would increase the population diversity and thus help an EA. Indeed, increasing the population size has been a commonly used strategy in tuning an EA when it did not perform as well as expected for a given problem. He and Yao (2002) showed theoretically that for some problem instance classes, a population can help to reduce the runtime of an EA from exponential to polynomial time. This paper analyzes the role of population further in EAs and shows rigorously that large populations may not always be useful. Conditions, under which large populations can be harmful, are discussed in this paper. Although the theoretical analysis was carried out on one multi-modal problem using a specific type of EAs, it has much wider implications. The analysis has revealed certain problem characteristics, which can be either the problem considered here or other problems, that lead to the disadvantages of large population sizes. The analytical approach developed in this paper can also be applied to analyzing EAs on other problems. version:1
arxiv-1208-2333 | Energy Efficient Wireless Communication using Genetic Algorithm Guided Faster Light Weight Digital Signature Algorithm (GADSA) | http://arxiv.org/abs/1208.2333 | id:1208.2333 author:Arindam Sarkar, J. K. Mandal category:cs.CR cs.NE  published:2012-08-11 summary:In this paper GA based light weight faster version of Digital Signature Algorithm (GADSA) in wireless communication has been proposed. Various genetic operators like crossover and mutation are used to optimizing amount of modular multiplication. Roulette Wheel selection mechanism helps to select best chromosome which in turn helps in faster computation and minimizes the time requirements for DSA. Minimization of number of modular multiplication itself a NP-hard problem that means there is no polynomial time deterministic algorithm for this purpose. This paper deals with this problem using GA based optimization algorithm for minimization of the modular multiplication. Proposed GADSA initiates with an initial population comprises of set of valid and complete set of individuals. Some operators are used to generate feasible valid offspring from the existing one. Among several exponents the best solution reached by GADSA is compared with some of the existing techniques. Extensive simulations shows competitive results for the proposed GADSA. version:1
arxiv-1208-2294 | Learning pseudo-Boolean k-DNF and Submodular Functions | http://arxiv.org/abs/1208.2294 | id:1208.2294 author:Sofya Raskhodnikova, Grigory Yaroslavtsev category:cs.LG cs.DM cs.DS  published:2012-08-10 summary:We prove that any submodular function f: {0,1}^n -> {0,1,...,k} can be represented as a pseudo-Boolean 2k-DNF formula. Pseudo-Boolean DNFs are a natural generalization of DNF representation for functions with integer range. Each term in such a formula has an associated integral constant. We show that an analog of Hastad's switching lemma holds for pseudo-Boolean k-DNFs if all constants associated with the terms of the formula are bounded. This allows us to generalize Mansour's PAC-learning algorithm for k-DNFs to pseudo-Boolean k-DNFs, and hence gives a PAC-learning algorithm with membership queries under the uniform distribution for submodular functions of the form f:{0,1}^n -> {0,1,...,k}. Our algorithm runs in time polynomial in n, k^{O(k \log k / \epsilon)}, 1/\epsilon and log(1/\delta) and works even in the agnostic setting. The line of previous work on learning submodular functions [Balcan, Harvey (STOC '11), Gupta, Hardt, Roth, Ullman (STOC '11), Cheraghchi, Klivans, Kothari, Lee (SODA '12)] implies only n^{O(k)} query complexity for learning submodular functions in this setting, for fixed epsilon and delta. Our learning algorithm implies a property tester for submodularity of functions f:{0,1}^n -> {0, ..., k} with query complexity polynomial in n for k=O((\log n/ \loglog n)^{1/2}) and constant proximity parameter \epsilon. version:1
arxiv-1208-2278 | Balancing Lifetime and Classification Accuracy of Wireless Sensor Networks | http://arxiv.org/abs/1208.2278 | id:1208.2278 author:Kush R. Varshney, Peter M. van de Ven category:cs.NI stat.ML  published:2012-08-10 summary:Wireless sensor networks are composed of distributed sensors that can be used for signal detection or classification. The likelihood functions of the hypotheses are often not known in advance, and decision rules have to be learned via supervised learning. A specific such algorithm is Fisher discriminant analysis (FDA), the classification accuracy of which has been previously studied in the context of wireless sensor networks. Previous work, however, does not take into account the communication protocol or battery lifetime of the sensor networks; in this paper we extend the existing studies by proposing a model that captures the relationship between battery lifetime and classification accuracy. In order to do so we combine the FDA with a model that captures the dynamics of the Carrier-Sense Multiple-Access (CSMA) algorithm, the random-access algorithm used to regulate communications in sensor networks. This allows us to study the interaction between the classification accuracy, battery lifetime and effort put towards learning, as well as the impact of the back-off rates of CSMA on the accuracy. We characterize the tradeoff between the length of the training stage and accuracy, and show that accuracy is non-monotone in the back-off rate due to changes in the training sample size and overfitting. version:1
arxiv-1208-2214 | Curved Space Optimization: A Random Search based on General Relativity Theory | http://arxiv.org/abs/1208.2214 | id:1208.2214 author:Fereydoun Farrahi Moghaddam, Reza Farrahi Moghaddam, Mohamed Cheriet category:cs.NE  published:2012-08-10 summary:Designing a fast and efficient optimization method with local optima avoidance capability on a variety of optimization problems is still an open problem for many researchers. In this work, the concept of a new global optimization method with an open implementation area is introduced as a Curved Space Optimization (CSO) method, which is a simple probabilistic optimization method enhanced by concepts of general relativity theory. To address global optimization challenges such as performance and convergence, this new method is designed based on transformation of a random search space into a new search space based on concepts of space-time curvature in general relativity theory. In order to evaluate the performance of our proposed method, an implementation of CSO is deployed and its results are compared on benchmark functions with state-of-the art optimization methods. The results show that the performance of CSO is promising on unimodal and multimodal benchmark functions with different search space dimension sizes. version:1
arxiv-1208-2128 | Brain tumor MRI image classification with feature selection and extraction using linear discriminant analysis | http://arxiv.org/abs/1208.2128 | id:1208.2128 author:V. P. Gladis Pushpa Rathi, S. Palani category:cs.CV cs.LG  published:2012-08-10 summary:Feature extraction is a method of capturing visual content of an image. The feature extraction is the process to represent raw image in its reduced form to facilitate decision making such as pattern classification. We have tried to address the problem of classification MRI brain images by creating a robust and more accurate classifier which can act as an expert assistant to medical practitioners. The objective of this paper is to present a novel method of feature selection and extraction. This approach combines the Intensity, Texture, shape based features and classifies the tumor as white matter, Gray matter, CSF, abnormal and normal area. The experiment is performed on 140 tumor contained brain MR images from the Internet Brain Segmentation Repository. The proposed technique has been carried out over a larger database as compare to any previous work and is more robust and effective. PCA and Linear Discriminant Analysis (LDA) were applied on the training sets. The Support Vector Machine (SVM) classifier served as a comparison of nonlinear techniques Vs linear ones. PCA and LDA methods are used to reduce the number of features used. The feature selection using the proposed technique is more beneficial as it analyses the data according to grouping class variable and gives reduced feature set with high classification accuracy. version:1
arxiv-1207-1114 | A Fast Projected Fixed-Point Algorithm for Large Graph Matching | http://arxiv.org/abs/1207.1114 | id:1207.1114 author:Yao Lu, Kaizhu Huang, Cheng-Lin Liu category:cs.CV  published:2012-07-03 summary:We propose a fast approximate algorithm for large graph matching. A new projected fixed-point method is defined and a new doubly stochastic projection is adopted to derive the algorithm. Previous graph matching algorithms suffer from high computational complexity and therefore do not have good scalability with respect to graph size. For matching two weighted graphs of $n$ nodes, our algorithm has time complexity only $O(n^3)$ per iteration and space complexity $O(n^2)$. In addition to its scalability, our algorithm is easy to implement, robust, and able to match undirected weighted attributed graphs of different sizes. While the convergence rate of previous iterative graph matching algorithms is unknown, our algorithm is theoretically guaranteed to converge at a linear rate. Extensive experiments on large synthetic and real graphs (more than 1,000 nodes) were conducted to evaluate the performance of various algorithms. Results show that in most cases our proposed algorithm achieves better performance than previous state-of-the-art algorithms in terms of both speed and accuracy in large graph matching. In particular, with high accuracy, our algorithm takes only a few seconds (in a PC) to match two graphs of 1,000 nodes. version:3
arxiv-1208-2092 | A study on non-destructive method for detecting Toxin in pepper using Neural networks | http://arxiv.org/abs/1208.2092 | id:1208.2092 author:M. Rajalakshmi, P. Subashini category:cs.NE cs.CV  published:2012-08-10 summary:Mycotoxin contamination in certain agricultural systems have been a serious concern for human and animal health. Mycotoxins are toxic substances produced mostly as secondary metabolites by fungi that grow on seeds and feed in the field, or in storage. The food-borne Mycotoxins likely to be of greatest significance for human health in tropical developing countries are Aflatoxins and Fumonisins. Chili pepper is also prone to Aflatoxin contamination during harvesting, production and storage periods.Various methods used for detection of Mycotoxins give accurate results, but they are slow, expensive and destructive. Destructive method is testing a material that degrades the sample under investigation. Whereas, non-destructive testing will, after testing, allow the part to be used for its intended purpose. Ultrasonic methods, Multispectral image processing methods, Terahertz methods, X-ray and Thermography have been very popular in nondestructive testing and characterization of materials and health monitoring. Image processing methods are used to improve the visual quality of the pictures and to extract useful information from them. In this proposed work, the chili pepper samples will be collected, and the X-ray, multispectral images of the samples will be processed using image processing methods. The term "Computational Intelligence" referred as simulation of human intelligence on computers. It is also called as "Artificial Intelligence" (AI) approach. The techniques used in AI approach are Neural network, Fuzzy logic and evolutionary computation. Finally, the computational intelligence method will be used in addition to image processing to provide best, high performance and accurate results for detecting the Mycotoxin level in the samples collected. version:1
arxiv-1208-1880 | Stereo Acoustic Perception based on Real Time Video Acquisition for Navigational Assistance | http://arxiv.org/abs/1208.1880 | id:1208.1880 author:Supreeth K. Rao, Arpitha Prasad B., Anushree R. Shetty, Chinmai, R. Bhakthavathsalam, Rajeshwari Hegde category:cs.CV cs.MM cs.SD  published:2012-08-09 summary:A smart navigation system (an Electronic Travel Aid) based on an object detection mechanism has been designed to detect the presence of obstacles that immediately impede the path, by means of real time video processing. The algorithm can be used for any general purpose navigational aid. This paper is discussed, keeping in mind the navigation of the visually impaired, and is not limited to the same. A video camera feeds images of the surroundings to a Da- Vinci Digital Media Processor, DM642, which works on the video, frame by frame. The processor carries out image processing techniques whose result contains information about the object in terms of image pixels. The algorithm aims to select the object which, among all others, poses maximum threat to the navigation. A database containing a total of three sounds is constructed. Hence, each image translates to a beep, where every beep informs the navigator of the obstacles directly in front of him. This paper implements an algorithm that is more efficient as compared to its predecessors. version:1
arxiv-1208-1860 | Scaling Multiple-Source Entity Resolution using Statistically Efficient Transfer Learning | http://arxiv.org/abs/1208.1860 | id:1208.1860 author:Sahand Negahban, Benjamin I. P. Rubinstein, Jim Gemmell category:cs.DB cs.LG H.2; I.2.6; I.5.4  published:2012-08-09 summary:We consider a serious, previously-unexplored challenge facing almost all approaches to scaling up entity resolution (ER) to multiple data sources: the prohibitive cost of labeling training data for supervised learning of similarity scores for each pair of sources. While there exists a rich literature describing almost all aspects of pairwise ER, this new challenge is arising now due to the unprecedented ability to acquire and store data from online sources, features driven by ER such as enriched search verticals, and the uniqueness of noisy and missing data characteristics for each source. We show on real-world and synthetic data that for state-of-the-art techniques, the reality of heterogeneous sources means that the number of labeled training data must scale quadratically in the number of sources, just to maintain constant precision/recall. We address this challenge with a brand new transfer learning algorithm which requires far less training data (or equivalently, achieves superior accuracy with the same data) and is trained using fast convex optimization. The intuition behind our approach is to adaptively share structure learned about one scoring problem with all other scoring problems sharing a data source in common. We demonstrate that our theoretically motivated approach incurs no runtime cost while it can maintain constant precision/recall with the cost of labeling increasing only linearly with the number of sources. version:1
arxiv-1208-1846 | Margin Distribution Controlled Boosting | http://arxiv.org/abs/1208.1846 | id:1208.1846 author:Guangxu Guo, Songcan Chen category:cs.LG  published:2012-08-09 summary:Schapire's margin theory provides a theoretical explanation to the success of boosting-type methods and manifests that a good margin distribution (MD) of training samples is essential for generalization. However the statement that a MD is good is vague, consequently, many recently developed algorithms try to generate a MD in their goodness senses for boosting generalization. Unlike their indirect control over MD, in this paper, we propose an alternative boosting algorithm termed Margin distribution Controlled Boosting (MCBoost) which directly controls the MD by introducing and optimizing a key adjustable margin parameter. MCBoost's optimization implementation adopts the column generation technique to ensure fast convergence and small number of weak classifiers involved in the final MCBooster. We empirically demonstrate: 1) AdaBoost is actually also a MD controlled algorithm and its iteration number acts as a parameter controlling the distribution and 2) the generalization performance of MCBoost evaluated on UCI benchmark datasets is validated better than those of AdaBoost, L2Boost, LPBoost, AdaBoost-CG and MDBoost. version:1
arxiv-1208-1829 | Metric Learning across Heterogeneous Domains by Respectively Aligning Both Priors and Posteriors | http://arxiv.org/abs/1208.1829 | id:1208.1829 author:Qiang Qian, Songcan Chen category:cs.LG  published:2012-08-09 summary:In this paper, we attempts to learn a single metric across two heterogeneous domains where source domain is fully labeled and has many samples while target domain has only a few labeled samples but abundant unlabeled samples. To the best of our knowledge, this task is seldom touched. The proposed learning model has a simple underlying motivation: all the samples in both the source and the target domains are mapped into a common space, where both their priors P(sample)s and their posteriors P(label sample)s are forced to be respectively aligned as much as possible. We show that the two mappings, from both the source domain and the target domain to the common space, can be reparameterized into a single positive semi-definite(PSD) matrix. Then we develop an efficient Bregman Projection algorithm to optimize the PDS matrix over which a LogDet function is used to regularize. Furthermore, we also show that this model can be easily kernelized and verify its effectiveness in crosslanguage retrieval task and cross-domain object recognition task. version:1
arxiv-1208-1819 | Self-Organizing Time Map: An Abstraction of Temporal Multivariate Patterns | http://arxiv.org/abs/1208.1819 | id:1208.1819 author:Peter Sarlin category:cs.LG cs.DS  published:2012-08-09 summary:This paper adopts and adapts Kohonen's standard Self-Organizing Map (SOM) for exploratory temporal structure analysis. The Self-Organizing Time Map (SOTM) implements SOM-type learning to one-dimensional arrays for individual time units, preserves the orientation with short-term memory and arranges the arrays in an ascending order of time. The two-dimensional representation of the SOTM attempts thus twofold topology preservation, where the horizontal direction preserves time topology and the vertical direction data topology. This enables discovering the occurrence and exploring the properties of temporal structural changes in data. For representing qualities and properties of SOTMs, we adapt measures and visualizations from the standard SOM paradigm, as well as introduce a measure of temporal structural changes. The functioning of the SOTM, and its visualizations and quality and property measures, are illustrated on artificial toy data. The usefulness of the SOTM in a real-world setting is shown on poverty, welfare and development indicators. version:1
arxiv-1207-4162 | ARMA Time-Series Modeling with Graphical Models | http://arxiv.org/abs/1207.4162 | id:1207.4162 author:Bo Thiesson, David Maxwell Chickering, David Heckerman, Christopher Meek category:stat.AP cs.LG stat.ME  published:2012-07-11 summary:We express the classic ARMA time-series model as a directed graphical model. In doing so, we find that the deterministic relationships in the model make it effectively impossible to use the EM algorithm for learning model parameters. To remedy this problem, we replace the deterministic relationships with Gaussian distributions having a small variance, yielding the stochastic ARMA (ARMA) model. This modification allows us to use the EM algorithm to learn parmeters and to forecast,even in situations where some data is missing. This modification, in conjunction with the graphicalmodel approach, also allows us to include cross predictors in situations where there are multiple times series and/or additional nontemporal covariates. More surprising,experiments suggest that the move to stochastic ARMA yields improved accuracy through better smoothing. We demonstrate improvements afforded by cross prediction and better smoothing on real data. version:2
arxiv-1208-1672 | An Efficient Automatic Attendance System Using Fingerprint Reconstruction Technique | http://arxiv.org/abs/1208.1672 | id:1208.1672 author:Josphineleela Ramakrishnan, M. Ramakrishnan category:cs.CV  published:2012-08-08 summary:Biometric time and attendance system is one of the most successful applications of biometric technology. One of the main advantage of a biometric time and attendance system is it avoids "buddy-punching". Buddy punching was a major loophole which will be exploiting in the traditional time attendance systems. Fingerprint recognition is an established field today, but still identifying individual from a set of enrolled fingerprints is a time taking process. Most fingerprint-based biometric systems store the minutiae template of a user in the database. It has been traditionally assumed that the minutiae template of a user does not reveal any information about the original fingerprint. This belief has now been shown to be false; several algorithms have been proposed that can reconstruct fingerprint images from minutiae templates. In this paper, a novel fingerprint reconstruction algorithm is proposed to reconstruct the phase image, which is then converted into the grayscale image. The proposed reconstruction algorithm reconstructs the phase image from minutiae. The proposed reconstruction algorithm is used to automate the whole process of taking attendance, manually which is a laborious and troublesome work and waste a lot of time, with its managing and maintaining the records for a period of time is also a burdensome task. The proposed reconstruction algorithm has been evaluated with respect to the success rates of type-I attack (match the reconstructed fingerprint against the original fingerprint) and type-II attack (match the reconstructed fingerprint against different impressions of the original fingerprint) using a commercial fingerprint recognition system. Given the reconstructed image from our algorithm, we show that both types of attacks can be effectively launched against a fingerprint recognition system. version:1
arxiv-1208-1670 | Performance Measurement and Method Analysis (PMMA) for Fingerprint Reconstruction | http://arxiv.org/abs/1208.1670 | id:1208.1670 author:Josphineleela Ramakrishnan, Ramakrishnan Malaisamy category:cs.CV  published:2012-08-08 summary:Fingerprint reconstruction is one of the most well-known and publicized biometrics. Because of their uniqueness and consistency over time, fingerprints have been used for identification over a century, more recently becoming automated due to advancements in computed capabilities. Fingerprint reconstruction is popular because of the inherent ease of acquisition, the numerous sources (e.g. ten fingers) available for collection, and their established use and collections by law enforcement and immigration. Fingerprints have always been the most practical and positive means of identification. Offenders, being well aware of this, have been coming up with ways to escape identification by that means. Erasing left over fingerprints, using gloves, fingerprint forgery; are certain examples of methods tried by them, over the years. Failing to prevent themselves, they moved to an extent of mutilating their finger skin pattern, to remain unidentified. This article is based upon obliteration of finger ridge patterns and discusses some known cases in relation to the same, in chronological order; highlighting the reasons why offenders go to an extent of performing such act. The paper gives an overview of different methods and performance measurement of the fingerprint reconstruction. version:1
arxiv-1208-3670 | A Survey of Recent View-based 3D Model Retrieval Methods | http://arxiv.org/abs/1208.3670 | id:1208.3670 author:Qiong Liu category:cs.CV  published:2012-08-08 summary:Extensive research efforts have been dedicated to 3D model retrieval in recent decades. Recently, view-based methods have attracted much research attention due to the high discriminative property of multi-views for 3D object representation. In this report, we summarize the view-based 3D model methods and provide the further research trends. This paper focuses on the scheme for matching between multiple views of 3D models and the application of bag-of-visual-words method in 3D model retrieval. For matching between multiple views, the many-to-many matching, probabilistic matching and semisupervised learning methods are introduced. For bag-of-visual-words application in 3D model retrieval, we first briefly review the bag-of-visual-words works on multimedia and computer vision tasks, where the visual dictionary has been detailed introduced. Then a series of 3D model retrieval methods by using bag-of-visual-words description are surveyed in this paper. At last, we summarize the further research content in view-based 3D model retrieval. version:1
arxiv-1112-1496 | Re-initialization Free Level Set Evolution via Reaction Diffusion | http://arxiv.org/abs/1112.1496 | id:1112.1496 author:Kaihua Zhang, Lei Zhang, Huihui Song, David Zhang category:cs.CV  published:2011-12-07 summary:This paper presents a novel reaction-diffusion (RD) method for implicit active contours, which is completely free of the costly re-initialization procedure in level set evolution (LSE). A diffusion term is introduced into LSE, resulting in a RD-LSE equation, to which a piecewise constant solution can be derived. In order to have a stable numerical solution of the RD based LSE, we propose a two-step splitting method (TSSM) to iteratively solve the RD-LSE equation: first iterating the LSE equation, and then solving the diffusion equation. The second step regularizes the level set function obtained in the first step to ensure stability, and thus the complex and costly re-initialization procedure is completely eliminated from LSE. By successfully applying diffusion to LSE, the RD-LSE model is stable by means of the simple finite difference method, which is very easy to implement. The proposed RD method can be generalized to solve the LSE for both variational level set method and PDE-based level set method. The RD-LSE method shows very good performance on boundary anti-leakage, and it can be readily extended to high dimensional level set method. The extensive and promising experimental results on synthetic and real images validate the effectiveness of the proposed RD-LSE approach. version:3
arxiv-1208-1544 | Guess Who Rated This Movie: Identifying Users Through Subspace Clustering | http://arxiv.org/abs/1208.1544 | id:1208.1544 author:Amy Zhang, Nadia Fawaz, Stratis Ioannidis, Andrea Montanari category:cs.LG  published:2012-08-07 summary:It is often the case that, within an online recommender system, multiple users share a common account. Can such shared accounts be identified solely on the basis of the user- provided ratings? Once a shared account is identified, can the different users sharing it be identified as well? Whenever such user identification is feasible, it opens the way to possible improvements in personalized recommendations, but also raises privacy concerns. We develop a model for composite accounts based on unions of linear subspaces, and use subspace clustering for carrying out the identification task. We show that a significant fraction of such accounts is identifiable in a reliable manner, and illustrate potential uses for personalized recommendation. version:1
arxiv-1111-5479 | The Graphical Lasso: New Insights and Alternatives | http://arxiv.org/abs/1111.5479 | id:1111.5479 author:Rahul Mazumder, Trevor Hastie category:stat.ML cs.LG  published:2011-11-23 summary:The graphical lasso \citep{FHT2007a} is an algorithm for learning the structure in an undirected Gaussian graphical model, using $\ell_1$ regularization to control the number of zeros in the precision matrix ${\B\Theta}={\B\Sigma}^{-1}$ \citep{BGA2008,yuan_lin_07}. The {\texttt R} package \GL\ \citep{FHT2007a} is popular, fast, and allows one to efficiently build a path of models for different values of the tuning parameter. Convergence of \GL\ can be tricky; the converged precision matrix might not be the inverse of the estimated covariance, and occasionally it fails to converge with warm starts. In this paper we explain this behavior, and propose new algorithms that appear to outperform \GL. By studying the "normal equations" we see that, \GL\ is solving the {\em dual} of the graphical lasso penalized likelihood, by block coordinate ascent; a result which can also be found in \cite{BGA2008}. In this dual, the target of estimation is $\B\Sigma$, the covariance matrix, rather than the precision matrix $\B\Theta$. We propose similar primal algorithms \PGL\ and \DPGL, that also operate by block-coordinate descent, where $\B\Theta$ is the optimization target. We study all of these algorithms, and in particular different approaches to solving their coordinate sub-problems. We conclude that \DPGL\ is superior from several points of view. version:2
arxiv-1203-5128 | Acceleration of the shiftable O(1) algorithm for bilateral filtering and non-local means | http://arxiv.org/abs/1203.5128 | id:1203.5128 author:Kunal N. Chaudhury category:cs.CV cs.DC  published:2012-03-22 summary:A direct implementation of the bilateral filter [1] requires O(\sigma_s^2) operations per pixel, where \sigma_s is the (effective) width of the spatial kernel. A fast implementation of the bilateral filter was recently proposed in [2] that required O(1) operations per pixel with respect to \sigma_s. This was done by using trigonometric functions for the range kernel of the bilateral filter, and by exploiting their so-called shiftability property. In particular, a fast implementation of the Gaussian bilateral filter was realized by approximating the Gaussian range kernel using raised cosines. Later, it was demonstrated in [3] that this idea could be extended to a larger class of filters, including the popular non-local means filter [4]. As already observed in [2], a flip side of this approach was that the run time depended on the width \sigma_r of the range kernel. For an image with (local) intensity variations in the range [0,T], the run time scaled as O(T^2/\sigma^2_r) with \sigma_r. This made it difficult to implement narrow range kernels, particularly for images with large dynamic range. We discuss this problem in this note, and propose some simple steps to accelerate the implementation in general, and for small \sigma_r in particular. [1] C. Tomasi and R. Manduchi, "Bilateral filtering for gray and color images", Proc. IEEE International Conference on Computer Vision, 1998. [2] K.N. Chaudhury, Daniel Sage, and M. Unser, "Fast O(1) bilateral filtering using trigonometric range kernels", IEEE Transactions on Image Processing, 2011. [3] K.N. Chaudhury, "Constant-time filtering using shiftable kernels", IEEE Signal Processing Letters, 2011. [4] A. Buades, B. Coll, and J.M. Morel, "A review of image denoising algorithms, with a new one", Multiscale Modeling and Simulation, 2005. version:2
arxiv-1208-1679 | Color Assessment and Transfer for Web Pages | http://arxiv.org/abs/1208.1679 | id:1208.1679 author:Ou Wu category:cs.HC cs.CV cs.GR H.4.m; H.2.8  published:2012-08-07 summary:Colors play a particularly important role in both designing and accessing Web pages. A well-designed color scheme improves Web pages' visual aesthetic and facilitates user interactions. As far as we know, existing color assessment studies focus on images; studies on color assessment and editing for Web pages are rare. This paper investigates color assessment for Web pages based on existing online color theme-rating data sets and applies this assessment to Web color edit. This study consists of three parts. First, we study the extraction of a Web page's color theme. Second, we construct color assessment models that score the color compatibility of a Web page by leveraging machine learning techniques. Third, we incorporate the learned color assessment model into a new application, namely, color transfer for Web pages. Our study combines techniques from computer graphics, Web mining, computer vision, and machine learning. Experimental results suggest that our constructed color assessment models are effective, and useful in the color transfer for Web pages, which has received little attention in both Web mining and computer graphics communities. version:1
arxiv-1208-1315 | Data Selection for Semi-Supervised Learning | http://arxiv.org/abs/1208.1315 | id:1208.1315 author:Shafigh Parsazad, Ehsan Saboori, Amin Allahyar category:cs.LG  published:2012-08-07 summary:The real challenge in pattern recognition task and machine learning process is to train a discriminator using labeled data and use it to distinguish between future data as accurate as possible. However, most of the problems in the real world have numerous data, which labeling them is a cumbersome or even an impossible matter. Semi-supervised learning is one approach to overcome these types of problems. It uses only a small set of labeled with the company of huge remain and unlabeled data to train the discriminator. In semi-supervised learning, it is very essential that which data is labeled and depend on position of data it effectiveness changes. In this paper, we proposed an evolutionary approach called Artificial Immune System (AIS) to determine which data is better to be labeled to get the high quality data. The experimental results represent the effectiveness of this algorithm in finding these data points. version:1
arxiv-1208-3279 | Structured Prediction Cascades | http://arxiv.org/abs/1208.3279 | id:1208.3279 author:David Weiss, Benjamin Sapp, Ben Taskar category:stat.ML cs.LG  published:2012-08-06 summary:Structured prediction tasks pose a fundamental trade-off between the need for model complexity to increase predictive power and the limited computational resources for inference in the exponentially-sized output spaces such models require. We formulate and develop the Structured Prediction Cascade architecture: a sequence of increasingly complex models that progressively filter the space of possible outputs. The key principle of our approach is that each model in the cascade is optimized to accurately filter and refine the structured output state space of the next model, speeding up both learning and inference in the next layer of the cascade. We learn cascades by optimizing a novel convex loss function that controls the trade-off between the filtering efficiency and the accuracy of the cascade, and provide generalization bounds for both accuracy and efficiency. We also extend our approach to intractable models using tree-decomposition ensembles, and provide algorithms and theory for this setting. We evaluate our approach on several large-scale problems, achieving state-of-the-art performance in handwriting recognition and human pose recognition. We find that structured prediction cascades allow tremendous speedups and the use of previously intractable features and models in both settings. version:1
arxiv-1208-1259 | One Permutation Hashing for Efficient Search and Learning | http://arxiv.org/abs/1208.1259 | id:1208.1259 author:Ping Li, Art Owen, Cun-Hui Zhang category:cs.LG cs.IR cs.IT math.IT stat.CO stat.ML  published:2012-08-06 summary:Recently, the method of b-bit minwise hashing has been applied to large-scale linear learning and sublinear time near-neighbor search. The major drawback of minwise hashing is the expensive preprocessing cost, as the method requires applying (e.g.,) k=200 to 500 permutations on the data. The testing time can also be expensive if a new data point (e.g., a new document or image) has not been processed, which might be a significant issue in user-facing applications. We develop a very simple solution based on one permutation hashing. Conceptually, given a massive binary data matrix, we permute the columns only once and divide the permuted columns evenly into k bins; and we simply store, for each data vector, the smallest nonzero location in each bin. The interesting probability analysis (which is validated by experiments) reveals that our one permutation scheme should perform very similarly to the original (k-permutation) minwise hashing. In fact, the one permutation scheme can be even slightly more accurate, due to the "sample-without-replacement" effect. Our experiments with training linear SVM and logistic regression on the webspam dataset demonstrate that this one permutation hashing scheme can achieve the same (or even slightly better) accuracies compared to the original k-permutation scheme. To test the robustness of our method, we also experiment with the small news20 dataset which is very sparse and has merely on average 500 nonzeros in each data vector. Interestingly, our one permutation scheme noticeably outperforms the k-permutation scheme when k is not too small on the news20 dataset. In summary, our method can achieve at least the same accuracy as the original k-permutation scheme, at merely 1/k of the original preprocessing cost. version:1
arxiv-1208-1056 | Sequential Estimation Methods from Inclusion Principle | http://arxiv.org/abs/1208.1056 | id:1208.1056 author:Xinjia Chen category:math.ST cs.LG math.PR stat.TH  published:2012-08-05 summary:In this paper, we propose new sequential estimation methods based on inclusion principle. The main idea is to reformulate the estimation problems as constructing sequential random intervals and use confidence sequences to control the associated coverage probabilities. In contrast to existing asymptotic sequential methods, our estimation procedures rigorously guarantee the pre-specified levels of confidence. version:1
arxiv-1208-0564 | Detection of Deviations in Mobile Applications Network Behavior | http://arxiv.org/abs/1208.0564 | id:1208.0564 author:L. Chekina, D. Mimran, L. Rokach, Y. Elovici, B. Shapira category:cs.CR cs.LG  published:2012-07-27 summary:In this paper a novel system for detecting meaningful deviations in a mobile application's network behavior is proposed. The main goal of the proposed system is to protect mobile device users and cellular infrastructure companies from malicious applications. The new system is capable of: (1) identifying malicious attacks or masquerading applications installed on a mobile device, and (2) identifying republishing of popular applications injected with a malicious code. The detection is performed based on the application's network traffic patterns only. For each application two types of models are learned. The first model, local, represents the personal traffic pattern for each user using an application and is learned on the device. The second model, collaborative, represents traffic patterns of numerous users using an application and is learned on the system server. Machine-learning methods are used for learning and detection purposes. This paper focuses on methods utilized for local (i.e., on mobile device) learning and detection of deviations from the normal application's behavior. These methods were implemented and evaluated on Android devices. The evaluation experiments demonstrate that: (1) various applications have specific network traffic patterns and certain application categories can be distinguishable by their network patterns, (2) different levels of deviations from normal behavior can be detected accurately, and (3) local learning is feasible and has a low performance overhead on mobile devices. version:2
arxiv-1208-0984 | APRIL: Active Preference-learning based Reinforcement Learning | http://arxiv.org/abs/1208.0984 | id:1208.0984 author:Riad Akrour, Marc Schoenauer, Michèle Sebag category:cs.LG  published:2012-08-05 summary:This paper focuses on reinforcement learning (RL) with limited prior knowledge. In the domain of swarm robotics for instance, the expert can hardly design a reward function or demonstrate the target behavior, forbidding the use of both standard RL and inverse reinforcement learning. Although with a limited expertise, the human expert is still often able to emit preferences and rank the agent demonstrations. Earlier work has presented an iterative preference-based RL framework: expert preferences are exploited to learn an approximate policy return, thus enabling the agent to achieve direct policy search. Iteratively, the agent selects a new candidate policy and demonstrates it; the expert ranks the new demonstration comparatively to the previous best one; the expert's ranking feedback enables the agent to refine the approximate policy return, and the process is iterated. In this paper, preference-based reinforcement learning is combined with active ranking in order to decrease the number of ranking queries to the expert needed to yield a satisfactory policy. Experiments on the mountain car and the cancer treatment testbeds witness that a couple of dozen rankings enable to learn a competent policy. version:1
arxiv-1208-0967 | Human Activity Learning using Object Affordances from RGB-D Videos | http://arxiv.org/abs/1208.0967 | id:1208.0967 author:Hema Swetha Koppula, Rudhir Gupta, Ashutosh Saxena category:cs.CV  published:2012-08-04 summary:Human activities comprise several sub-activities performed in a sequence and involve interactions with various objects. This makes reasoning about the object affordances a central task for activity recognition. In this work, we consider the problem of jointly labeling the object affordances and human activities from RGB-D videos. We frame the problem as a Markov Random Field where the nodes represent objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolution over time. We formulate the learning problem using a structural SVM approach, where labeling over various alternate temporal segmentations are considered as latent variables. We tested our method on a dataset comprising 120 activity videos collected from four subjects, and obtained an end-to-end precision of 81.8% and recall of 80.0% for labeling the activities. version:1
arxiv-1110-2098 | Dynamic Matrix Factorization: A State Space Approach | http://arxiv.org/abs/1110.2098 | id:1110.2098 author:John Z. Sun, Kush R. Varshney, Karthik Subbian category:cs.LG  published:2011-10-10 summary:Matrix factorization from a small number of observed entries has recently garnered much attention as the key ingredient of successful recommendation systems. One unresolved problem in this area is how to adapt current methods to handle changing user preferences over time. Recent proposals to address this issue are heuristic in nature and do not fully exploit the time-dependent structure of the problem. As a principled and general temporal formulation, we propose a dynamical state space model of matrix factorization. Our proposal builds upon probabilistic matrix factorization, a Bayesian model with Gaussian priors. We utilize results in state tracking, such as the Kalman filter, to provide accurate recommendations in the presence of both process and measurement noise. We show how system parameters can be learned via expectation-maximization and provide comparisons to current published techniques. version:3
arxiv-1107-2487 | Provably Safe and Robust Learning-Based Model Predictive Control | http://arxiv.org/abs/1107.2487 | id:1107.2487 author:Anil Aswani, Humberto Gonzalez, S. Shankar Sastry, Claire Tomlin category:math.OC cs.LG cs.SY math.ST stat.TH  published:2011-07-13 summary:Controller design faces a trade-off between robustness and performance, and the reliability of linear controllers has caused many practitioners to focus on the former. However, there is renewed interest in improving system performance to deal with growing energy constraints. This paper describes a learning-based model predictive control (LBMPC) scheme that provides deterministic guarantees on robustness, while statistical identification tools are used to identify richer models of the system in order to improve performance; the benefits of this framework are that it handles state and input constraints, optimizes system performance with respect to a cost function, and can be designed to use a wide variety of parametric or nonparametric statistical tools. The main insight of LBMPC is that safety and performance can be decoupled under reasonable conditions in an optimization framework by maintaining two models of the system. The first is an approximate model with bounds on its uncertainty, and the second model is updated by statistical methods. LBMPC improves performance by choosing inputs that minimize a cost subject to the learned dynamics, and it ensures safety and robustness by checking whether these same inputs keep the approximate model stable when it is subject to uncertainty. Furthermore, we show that if the system is sufficiently excited, then the LBMPC control action probabilistically converges to that of an MPC computed using the true dynamics. version:2
arxiv-1208-0864 | Statistical Results on Filtering and Epi-convergence for Learning-Based Model Predictive Control | http://arxiv.org/abs/1208.0864 | id:1208.0864 author:Anil Aswani, Humberto Gonzalez, S. Shankar Sastry, Claire Tomlin category:math.OC cs.LG cs.SY  published:2012-08-03 summary:Learning-based model predictive control (LBMPC) is a technique that provides deterministic guarantees on robustness, while statistical identification tools are used to identify richer models of the system in order to improve performance. This technical note provides proofs that elucidate the reasons for our choice of measurement model, as well as giving proofs concerning the stochastic convergence of LBMPC. The first part of this note discusses simultaneous state estimation and statistical identification (or learning) of unmodeled dynamics, for dynamical systems that can be described by ordinary differential equations (ODE's). The second part provides proofs concerning the epi-convergence of different statistical estimators that can be used with the learning-based model predictive control (LBMPC) technique. In particular, we prove results on the statistical properties of a nonparametric estimator that we have designed to have the correct deterministic and stochastic properties for numerical implementation when used in conjunction with LBMPC. version:1
arxiv-1208-0806 | Cross-conformal predictors | http://arxiv.org/abs/1208.0806 | id:1208.0806 author:Vladimir Vovk category:stat.ML cs.LG 62G15  published:2012-08-03 summary:This note introduces the method of cross-conformal prediction, which is a hybrid of the methods of inductive conformal prediction and cross-validation, and studies its validity and predictive efficiency empirically. version:1
arxiv-1208-0803 | A Novel Approach of Color Image Hiding using RGB Color planes and DWT | http://arxiv.org/abs/1208.0803 | id:1208.0803 author:Nilanjan Dey, Anamitra Bardhan Roy, Sayantan Dey category:cs.CR cs.CV  published:2012-08-03 summary:This work proposes a wavelet based Steganographic technique for the color image. The true color cover image and the true color secret image both are decomposed into three separate color planes namely R, G and B. Each plane of the images is decomposed into four sub bands using DWT. Each color plane of the secret image is hidden by alpha blending technique in the corresponding sub bands of the respective color planes of the original image. During embedding, secret image is dispersed within the original image depending upon the alpha value. Extraction of the secret image varies according to the alpha value. In this approach the stego image generated is of acceptable level of imperceptibility and distortion compared to the cover image and the overall security is high. version:1
arxiv-1208-0385 | A phase-sensitive method for filtering on the sphere | http://arxiv.org/abs/1208.0385 | id:1208.0385 author:Ramakrishna Kakarala, Philip Ogunbona category:math.RT cs.CV  published:2012-08-02 summary:This paper examines filtering on a sphere, by first examining the roles of spherical harmonic magnitude and phase. We show that phase is more important than magnitude in determining the structure of a spherical function. We examine the properties of linear phase shifts in the spherical harmonic domain, which suggest a mechanism for constructing finite-impulse-response (FIR) filters. We show that those filters have desirable properties, such as being associative, mapping spherical functions to spherical functions, allowing directional filtering, and being defined by relatively simple equations. We provide examples of the filters for both spherical and manifold data. version:2
arxiv-1004-4668 | Evolutionary Inference for Function-valued Traits: Gaussian Process Regression on Phylogenies | http://arxiv.org/abs/1004.4668 | id:1004.4668 author:Nick S. Jones, John Moriarty category:q-bio.QM cs.LG physics.data-an stat.ML  published:2010-04-26 summary:Biological data objects often have both of the following features: (i) they are functions rather than single numbers or vectors, and (ii) they are correlated due to phylogenetic relationships. In this paper we give a flexible statistical model for such data, by combining assumptions from phylogenetics with Gaussian processes. We describe its use as a nonparametric Bayesian prior distribution, both for prediction (placing posterior distributions on ancestral functions) and model selection (comparing rates of evolution across a phylogeny, or identifying the most likely phylogenies consistent with the observed data). Our work is integrative, extending the popular phylogenetic Brownian Motion and Ornstein-Uhlenbeck models to functional data and Bayesian inference, and extending Gaussian Process regression to phylogenies. We provide a brief illustration of the application of our method. version:3
arxiv-1208-0651 | Fast and Accurate Algorithms for Re-Weighted L1-Norm Minimization | http://arxiv.org/abs/1208.0651 | id:1208.0651 author:M. Salman Asif, Justin Romberg category:stat.CO cs.IT math.IT stat.ML  published:2012-08-03 summary:To recover a sparse signal from an underdetermined system, we often solve a constrained L1-norm minimization problem. In many cases, the signal sparsity and the recovery performance can be further improved by replacing the L1 norm with a "weighted" L1 norm. Without any prior information about nonzero elements of the signal, the procedure for selecting weights is iterative in nature. Common approaches update the weights at every iteration using the solution of a weighted L1 problem from the previous iteration. In this paper, we present two homotopy-based algorithms that efficiently solve reweighted L1 problems. First, we present an algorithm that quickly updates the solution of a weighted L1 problem as the weights change. Since the solution changes only slightly with small changes in the weights, we develop a homotopy algorithm that replaces the old weights with the new ones in a small number of computationally inexpensive steps. Second, we propose an algorithm that solves a weighted L1 problem by adaptively selecting the weights while estimating the signal. This algorithm integrates the reweighting into every step along the homotopy path by changing the weights according to the changes in the solution and its support, allowing us to achieve a high quality signal reconstruction by solving a single homotopy problem. We compare the performance of both algorithms, in terms of reconstruction accuracy and computational complexity, against state-of-the-art solvers and show that our methods have smaller computational cost. In addition, we will show that the adaptive selection of the weights inside the homotopy often yields reconstructions of higher quality. version:1
arxiv-1208-0628 | Ancestral Inference from Functional Data: Statistical Methods and Numerical Examples | http://arxiv.org/abs/1208.0628 | id:1208.0628 author:Pantelis Z. Hadjipantelis, Nick S. Jones, John Moriarty, David Springate, Christopher G. Knight category:stat.ML q-bio.PE q-bio.QM  published:2012-08-02 summary:Many biological characteristics of evolutionary interest are not scalar variables but continuous functions. Here we use phylogenetic Gaussian process regression to model the evolution of simulated function-valued traits. Given function-valued data only from the tips of an evolutionary tree and utilising independent principal component analysis (IPCA) as a method for dimension reduction, we construct distributional estimates of ancestral function-valued traits, and estimate parameters describing their evolutionary dynamics. version:1
arxiv-1208-0541 | A hybrid artificial immune system and Self Organising Map for network intrusion detection | http://arxiv.org/abs/1208.0541 | id:1208.0541 author:Simon T. Powers, Jun He category:cs.NE cs.CR  published:2012-08-02 summary:Network intrusion detection is the problem of detecting unauthorised use of, or access to, computer systems over a network. Two broad approaches exist to tackle this problem: anomaly detection and misuse detection. An anomaly detection system is trained only on examples of normal connections, and thus has the potential to detect novel attacks. However, many anomaly detection systems simply report the anomalous activity, rather than analysing it further in order to report higher-level information that is of more use to a security officer. On the other hand, misuse detection systems recognise known attack patterns, thereby allowing them to provide more detailed information about an intrusion. However, such systems cannot detect novel attacks. A hybrid system is presented in this paper with the aim of combining the advantages of both approaches. Specifically, anomalous network connections are initially detected using an artificial immune system. Connections that are flagged as anomalous are then categorised using a Kohonen Self Organising Map, allowing higher-level information, in the form of cluster membership, to be extracted. Experimental results on the KDD 1999 Cup dataset show a low false positive rate and a detection and classification rate for Denial-of-Service and User-to-Root attacks that is higher than those in a sample of other works. version:1
arxiv-1208-0526 | Optimization hardness as transient chaos in an analog approach to constraint satisfaction | http://arxiv.org/abs/1208.0526 | id:1208.0526 author:Maria Ercsey-Ravasz, Zoltan Toroczkai category:cs.CC cs.NE math.DS nlin.CD physics.comp-ph F.2.3; F.1.0  published:2012-08-02 summary:Boolean satisfiability [1] (k-SAT) is one of the most studied optimization problems, as an efficient (that is, polynomial-time) solution to k-SAT (for $k\geq 3$) implies efficient solutions to a large number of hard optimization problems [2,3]. Here we propose a mapping of k-SAT into a deterministic continuous-time dynamical system with a unique correspondence between its attractors and the k-SAT solution clusters. We show that beyond a constraint density threshold, the analog trajectories become transiently chaotic [4-7], and the boundaries between the basins of attraction [8] of the solution clusters become fractal [7-9], signaling the appearance of optimization hardness [10]. Analytical arguments and simulations indicate that the system always finds solutions for satisfiable formulae even in the frozen regimes of random 3-SAT [11] and of locked occupation problems [12] (considered among the hardest algorithmic benchmarks); a property partly due to the system's hyperbolic [4,13] character. The system finds solutions in polynomial continuous-time, however, at the expense of exponential fluctuations in its energy function. version:1
arxiv-1012-0930 | Efficient Optimization of Performance Measures by Classifier Adaptation | http://arxiv.org/abs/1012.0930 | id:1012.0930 author:Nan Li, Ivor W. Tsang, Zhi-Hua Zhou category:cs.LG cs.AI  published:2010-12-04 summary:In practical applications, machine learning algorithms are often needed to learn classifiers that optimize domain specific performance measures. Previously, the research has focused on learning the needed classifier in isolation, yet learning nonlinear classifier for nonlinear and nonsmooth performance measures is still hard. In this paper, rather than learning the needed classifier by optimizing specific performance measure directly, we circumvent this problem by proposing a novel two-step approach called as CAPO, namely to first train nonlinear auxiliary classifiers with existing learning methods, and then to adapt auxiliary classifiers for specific performance measures. In the first step, auxiliary classifiers can be obtained efficiently by taking off-the-shelf learning algorithms. For the second step, we show that the classifier adaptation problem can be reduced to a quadratic program problem, which is similar to linear SVMperf and can be efficiently solved. By exploiting nonlinear auxiliary classifiers, CAPO can generate nonlinear classifier which optimizes a large variety of performance measures including all the performance measure based on the contingency table and AUC, whilst keeping high computational efficiency. Empirical studies show that CAPO is effective and of high computational efficiency, and even it is more efficient than linear SVMperf. version:3
arxiv-1208-0402 | Multidimensional Membership Mixture Models | http://arxiv.org/abs/1208.0402 | id:1208.0402 author:Yun Jiang, Marcus Lim, Ashutosh Saxena category:cs.LG stat.ML  published:2012-08-02 summary:We present the multidimensional membership mixture (M3) models where every dimension of the membership represents an independent mixture model and each data point is generated from the selected mixture components jointly. This is helpful when the data has a certain shared structure. For example, three unique means and three unique variances can effectively form a Gaussian mixture model with nine components, while requiring only six parameters to fully describe it. In this paper, we present three instantiations of M3 models (together with the learning and inference algorithms): infinite, finite, and hybrid, depending on whether the number of mixtures is fixed or not. They are built upon Dirichlet process mixture models, latent Dirichlet allocation, and a combination respectively. We then consider two applications: topic modeling and learning 3D object arrangements. Our experiments show that our M3 models achieve better performance using fewer topics than many classic topic models. We also observe that topics from the different dimensions of M3 models are meaningful and orthogonal to each other. version:1
arxiv-1208-0378 | Fast Planar Correlation Clustering for Image Segmentation | http://arxiv.org/abs/1208.0378 | id:1208.0378 author:Julian Yarkony, Alexander T. Ihler, Charless C. Fowlkes category:cs.CV cs.DS cs.LG stat.ML  published:2012-08-02 summary:We describe a new optimization scheme for finding high-quality correlation clusterings in planar graphs that uses weighted perfect matching as a subroutine. Our method provides lower-bounds on the energy of the optimal correlation clustering that are typically fast to compute and tight in practice. We demonstrate our algorithm on the problem of image segmentation where this approach outperforms existing global optimization techniques in minimizing the objective and is competitive with the state of the art in producing high-quality segmentations. version:1
arxiv-1208-0318 | Artificial Neural Network Based Prediction of Optimal Pseudo-Damping and Meta-Damping in Oscillatory Fractional Order Dynamical Systems | http://arxiv.org/abs/1208.0318 | id:1208.0318 author:Saptarshi Das, Indranil Pan, Khrist Sur, Shantanu Das category:cs.SY cs.NE  published:2012-08-01 summary:This paper investigates typical behaviors like damped oscillations in fractional order (FO) dynamical systems. Such response occurs due to the presence of, what is conceived as, pseudo-damping and meta-damping in some special class of FO systems. Here, approximation of such damped oscillation in FO systems with the conventional notion of integer order damping and time constant has been carried out using Genetic Algorithm (GA). Next, a multilayer feed-forward Artificial Neural Network (ANN) has been trained using the GA based results to predict the optimal pseudo and meta-damping from knowledge of the maximum order or number of terms in the FO dynamical system. version:1
arxiv-1208-0223 | The bistable brain: a neuronal model with symbiotic interactions | http://arxiv.org/abs/1208.0223 | id:1208.0223 author:Ricardo Lopez-Ruiz, Daniele Fournier-Prunaret category:nlin.CD cs.NE math.DS  published:2012-08-01 summary:In general, the behavior of large and complex aggregates of elementary components can not be understood nor extrapolated from the properties of a few components. The brain is a good example of this type of networked systems where some patterns of behavior are observed independently of the topology and of the number of coupled units. Following this insight, we have studied the dynamics of different aggregates of logistic maps according to a particular {\it symbiotic} coupling scheme that imitates the neuronal excitation coupling. All these aggregates show some common dynamical properties, concretely a bistable behavior that is reported here with a certain detail. Thus, the qualitative relationship with neural systems is suggested through a naive model of many of such networked logistic maps whose behavior mimics the waking-sleeping bistability displayed by brain systems. Due to its relevance, some regions of multistability are determined and sketched for all these logistic models. version:1
arxiv-1208-0200 | Adaptation of pedagogical resources description standard (LOM) with the specificity of Arabic language | http://arxiv.org/abs/1208.0200 | id:1208.0200 author:Asma Boudhief, Mohsen Maraoui, Mounir Zrigui category:cs.CL  published:2012-08-01 summary:In this article we focus firstly on the principle of pedagogical indexing and characteristics of Arabic language and secondly on the possibility of adapting the standard for describing learning resources used (the LOM and its Application Profiles) with learning conditions such as the educational levels of students and their levels of understanding,... the educational context with taking into account the representative elements of text, text length, ... in particular, we put in relief the specificity of the Arabic language which is a complex language, characterized by its flexion, its voyellation and agglutination. version:1
arxiv-1208-0129 | Oracle inequalities for computationally adaptive model selection | http://arxiv.org/abs/1208.0129 | id:1208.0129 author:Alekh Agarwal, Peter L. Bartlett, John C. Duchi category:stat.ML cs.LG  published:2012-08-01 summary:We analyze general model selection procedures using penalized empirical loss minimization under computational constraints. While classical model selection approaches do not consider computational aspects of performing model selection, we argue that any practical model selection procedure must not only trade off estimation and approximation error, but also the computational effort required to compute empirical minimizers for different function classes. We provide a framework for analyzing such problems, and we give algorithms for model selection under a computational budget. These algorithms satisfy oracle inequalities that show that the risk of the selected model is not much worse than if we had devoted all of our omputational budget to the optimal function class. version:1
arxiv-1105-4681 | Ergodic Mirror Descent | http://arxiv.org/abs/1105.4681 | id:1105.4681 author:John C. Duchi, Alekh Agarwal, Mikael Johansson, Michael I. Jordan category:math.OC stat.ML  published:2011-05-24 summary:We generalize stochastic subgradient descent methods to situations in which we do not receive independent samples from the distribution over which we optimize, but instead receive samples that are coupled over time. We show that as long as the source of randomness is suitably ergodic---it converges quickly enough to a stationary distribution---the method enjoys strong convergence guarantees, both in expectation and with high probability. This result has implications for stochastic optimization in high-dimensional spaces, peer-to-peer distributed optimization schemes, decision problems with dependent data, and stochastic optimization problems over combinatorial spaces. version:3
arxiv-1207-7253 | Learning a peptide-protein binding affinity predictor with kernel ridge regression | http://arxiv.org/abs/1207.7253 | id:1207.7253 author:Sébastien Giguère, Mario Marchand, François Laviolette, Alexandre Drouin, Jacques Corbeil category:q-bio.QM cs.LG q-bio.BM stat.ML 92B05  published:2012-07-31 summary:We propose a specialized string kernel for small bio-molecules, peptides and pseudo-sequences of binding interfaces. The kernel incorporates physico-chemical properties of amino acids and elegantly generalize eight kernels, such as the Oligo, the Weighted Degree, the Blended Spectrum, and the Radial Basis Function. We provide a low complexity dynamic programming algorithm for the exact computation of the kernel and a linear time algorithm for it's approximation. Combined with kernel ridge regression and SupCK, a novel binding pocket kernel, the proposed kernel yields biologically relevant and good prediction accuracy on the PepX database. For the first time, a machine learning predictor is capable of accurately predicting the binding affinity of any peptide to any protein. The method was also applied to both single-target and pan-specific Major Histocompatibility Complex class II benchmark datasets and three Quantitative Structure Affinity Model benchmark datasets. On all benchmarks, our method significantly (p-value < 0.057) outperforms the current state-of-the-art methods at predicting peptide-protein binding affinities. The proposed approach is flexible and can be applied to predict any quantitative biological activity. The method should be of value to a large segment of the research community with the potential to accelerate peptide-based drug and vaccine development. version:1
arxiv-1107-1222 | On the information-theoretic structure of distributed measurements | http://arxiv.org/abs/1107.1222 | id:1107.1222 author:David Balduzzi category:cs.IT cs.DC cs.NE math.CT math.IT nlin.CG  published:2011-07-06 summary:The internal structure of a measuring device, which depends on what its components are and how they are organized, determines how it categorizes its inputs. This paper presents a geometric approach to studying the internal structure of measurements performed by distributed systems such as probabilistic cellular automata. It constructs the quale, a family of sections of a suitably defined presheaf, whose elements correspond to the measurements performed by all subsystems of a distributed system. Using the quale we quantify (i) the information generated by a measurement; (ii) the extent to which a measurement is context-dependent; and (iii) whether a measurement is decomposable into independent submeasurements, which turns out to be equivalent to context-dependence. Finally, we show that only indecomposable measurements are more informative than the sum of their submeasurements. version:2
arxiv-1110-6886 | PAC-Bayesian Inequalities for Martingales | http://arxiv.org/abs/1110.6886 | id:1110.6886 author:Yevgeny Seldin, François Laviolette, Nicolò Cesa-Bianchi, John Shawe-Taylor, Peter Auer category:cs.LG cs.IT math.IT stat.ML  published:2011-10-31 summary:We present a set of high-probability inequalities that control the concentration of weighted averages of multiple (possibly uncountably many) simultaneously evolving and interdependent martingales. Our results extend the PAC-Bayesian analysis in learning theory from the i.i.d. setting to martingales opening the way for its application to importance weighted sampling, reinforcement learning, and other interactive learning domains, as well as many other domains in probability theory and statistics, where martingales are encountered. We also present a comparison inequality that bounds the expectation of a convex function of a martingale difference sequence shifted to the [0,1] interval by the expectation of the same function of independent Bernoulli variables. This inequality is applied to derive a tighter analog of Hoeffding-Azuma's inequality. version:3
arxiv-1107-4606 | The Divergence of Reinforcement Learning Algorithms with Value-Iteration and Function Approximation | http://arxiv.org/abs/1107.4606 | id:1107.4606 author:Michael Fairbank, Eduardo Alonso category:cs.LG  published:2011-07-22 summary:This paper gives specific divergence examples of value-iteration for several major Reinforcement Learning and Adaptive Dynamic Programming algorithms, when using a function approximator for the value function. These divergence examples differ from previous divergence examples in the literature, in that they are applicable for a greedy policy, i.e. in a "value iteration" scenario. Perhaps surprisingly, with a greedy policy, it is also possible to get divergence for the algorithms TD(1) and Sarsa(1). In addition to these divergences, we also achieve divergence for the Adaptive Dynamic Programming algorithms HDP, DHP and GDHP. version:2
arxiv-1207-6774 | A Survey Of Activity Recognition And Understanding The Behavior In Video Survelliance | http://arxiv.org/abs/1207.6774 | id:1207.6774 author:A. R. Revathi, Dhananjay Kumar category:cs.CV  published:2012-07-29 summary:This paper presents a review of human activity recognition and behaviour understanding in video sequence. The key objective of this paper is to provide a general review on the overall process of a surveillance system used in the current trend. Visual surveillance system is directed on automatic identification of events of interest, especially on tracking and classification of moving objects. The processing step of the video surveillance system includes the following stages: Surrounding model, object representation, object tracking, activity recognition and behaviour understanding. It describes techniques that use to define a general set of activities that are applicable to a wide range of scenes and environments in video sequence. version:1
arxiv-1207-6745 | Universally Consistent Latent Position Estimation and Vertex Classification for Random Dot Product Graphs | http://arxiv.org/abs/1207.6745 | id:1207.6745 author:Daniel L. Sussman, Minh Tang, Carey E. Priebe category:stat.ML math.ST stat.TH  published:2012-07-29 summary:In this work we show that, using the eigen-decomposition of the adjacency matrix, we can consistently estimate latent positions for random dot product graphs provided the latent positions are i.i.d. from some distribution. If class labels are observed for a number of vertices tending to infinity, then we show that the remaining vertices can be classified with error converging to Bayes optimal using the $k$-nearest-neighbors classification rule. We evaluate the proposed methods on simulated data and a graph derived from Wikipedia. version:1
arxiv-1207-6682 | Exploring Promising Stepping Stones by Combining Novelty Search with Interactive Evolution | http://arxiv.org/abs/1207.6682 | id:1207.6682 author:Brian G. Woolley, Kenneth O. Stanley category:cs.NE I.2.6  published:2012-07-28 summary:The field of evolutionary computation is inspired by the achievements of natural evolution, in which there is no final objective. Yet the pursuit of objectives is ubiquitous in simulated evolution. A significant problem is that objective approaches assume that intermediate stepping stones will increasingly resemble the final objective when in fact they often do not. The consequence is that while solutions may exist, searching for such objectives may not discover them. This paper highlights the importance of leveraging human insight during search as an alternative to articulating explicit objectives. In particular, a new approach called novelty-assisted interactive evolutionary computation (NA-IEC) combines human intuition with novelty search for the first time to facilitate the serendipitous discovery of agent behaviors. In this approach, the human user directs evolution by selecting what is interesting from the on-screen population of behaviors. However, unlike in typical IEC, the user can now request that the next generation be filled with novel descendants. The experimental results demonstrate that combining human insight with novelty search finds solutions significantly faster and at lower genomic complexities than fully-automated processes, including pure novelty search, suggesting an important role for human users in the search for solutions. version:1
arxiv-1202-2169 | High Dimensional Semiparametric Gaussian Copula Graphical Models | http://arxiv.org/abs/1202.2169 | id:1202.2169 author:Han Liu, Fang Han, Ming Yuan, John Lafferty, Larry Wasserman category:stat.ML  published:2012-02-10 summary:In this paper, we propose a semiparametric approach, named nonparanormal skeptic, for efficiently and robustly estimating high dimensional undirected graphical models. To achieve modeling flexibility, we consider Gaussian Copula graphical models (or the nonparanormal) as proposed by Liu et al. (2009). To achieve estimation robustness, we exploit nonparametric rank-based correlation coefficient estimators, including Spearman's rho and Kendall's tau. In high dimensional settings, we prove that the nonparanormal skeptic achieves the optimal parametric rate of convergence in both graph and parameter estimation. This celebrating result suggests that the Gaussian copula graphical models can be used as a safe replacement of the popular Gaussian graphical models, even when the data are truly Gaussian. Besides theoretical analysis, we also conduct thorough numerical simulations to compare different estimators for their graph recovery performance under both ideal and noisy settings. The proposed methods are then applied on a large-scale genomic dataset to illustrate their empirical usefulness. The R language software package huge implementing the proposed methods is available on the Comprehensive R Archive Network: http://cran. r-project.org/. version:3
arxiv-1207-7035 | Supervised Laplacian Eigenmaps with Applications in Clinical Diagnostics for Pediatric Cardiology | http://arxiv.org/abs/1207.7035 | id:1207.7035 author:Thomas Perry, Hongyuan Zha, Patricio Frias, Dadan Zeng, Mark Braunstein category:cs.LG  published:2012-07-27 summary:Electronic health records contain rich textual data which possess critical predictive information for machine-learning based diagnostic aids. However many traditional machine learning methods fail to simultaneously integrate both vector space data and text. We present a supervised method using Laplacian eigenmaps to augment existing machine-learning methods with low-dimensional representations of textual predictors which preserve the local similarities. The proposed implementation performs alternating optimization using gradient descent. For the evaluation we applied our method to over 2,000 patient records from a large single-center pediatric cardiology practice to predict if patients were diagnosed with cardiac disease. Our method was compared with latent semantic indexing, latent Dirichlet allocation, and local Fisher discriminant analysis. The results were assessed using AUC, MCC, specificity, and sensitivity. Results indicate supervised Laplacian eigenmaps was the highest performing method in our study, achieving 0.782 and 0.374 for AUC and MCC respectively. SLE showed an increase in 8.16% in AUC and 20.6% in MCC over the baseline which excluded textual data and a 2.69% and 5.35% increase in AUC and MCC respectively over unsupervised Laplacian eigenmaps. This method allows many existing machine learning predictors to effectively and efficiently utilize the potential of textual predictors. version:1
arxiv-1207-5774 | A New Training Algorithm for Kanerva's Sparse Distributed Memory | http://arxiv.org/abs/1207.5774 | id:1207.5774 author:Lou Marvin Caraig category:cs.CV cs.LG cs.NE  published:2012-07-22 summary:The Sparse Distributed Memory proposed by Pentii Kanerva (SDM in short) was thought to be a model of human long term memory. The architecture of the SDM permits to store binary patterns and to retrieve them using partially matching patterns. However Kanerva's model is especially efficient only in handling random data. The purpose of this article is to introduce a new approach of training Kanerva's SDM that can handle efficiently non-random data, and to provide it the capability to recognize inverted patterns. This approach uses a signal model which is different from the one proposed for different purposes by Hely, Willshaw and Hayes in [4]. This article additionally suggests a different way of creating hard locations in the memory despite the Kanerva's static model. version:3
arxiv-1207-6379 | Identifying Users From Their Rating Patterns | http://arxiv.org/abs/1207.6379 | id:1207.6379 author:José Bento, Nadia Fawaz, Andrea Montanari, Stratis Ioannidis category:cs.IR cs.LG stat.ML  published:2012-07-26 summary:This paper reports on our analysis of the 2011 CAMRa Challenge dataset (Track 2) for context-aware movie recommendation systems. The train dataset comprises 4,536,891 ratings provided by 171,670 users on 23,974$ movies, as well as the household groupings of a subset of the users. The test dataset comprises 5,450 ratings for which the user label is missing, but the household label is provided. The challenge required to identify the user labels for the ratings in the test set. Our main finding is that temporal information (time labels of the ratings) is significantly more useful for achieving this objective than the user preferences (the actual ratings). Using a model that leverages on this fact, we are able to identify users within a known household with an accuracy of approximately 96% (i.e. misclassification rate around 4%). version:1
arxiv-1208-4583 | A novel Hopfield neural network approach for minimizing total weighted tardiness of jobs scheduled on identical machines | http://arxiv.org/abs/1208.4583 | id:1208.4583 author:N. Fogarasi, K. Tornai, J. Levendovszky category:cs.NE 90C27 G.1.6  published:2012-07-26 summary:This paper explores fast, polynomial time heuristic approximate solutions to the NP-hard problem of scheduling jobs on N identical machines. The jobs are independent and are allowed to be stopped and restarted on another machine at a later time. They have well-de?ned deadlines, and relative priorities quantified by non-negative real weights. The objective is to find schedules which minimize the total weighted tardiness (TWT) of all jobs. We show how this problem can be mapped into quadratic form and present a polynomial time heuristic solution based on the Hop?eld Neural Network (HNN) approach. It is demonstrated, through the results of extensive numerical simulations, that this solution outperforms other popular heuristic methods. The proposed heuristic is both theoretically and empirically shown to be scalable to large problem sizes (over 100 jobs to be scheduled), which makes it applicable to grid computing scheduling, arising in fields such as computational biology, chemistry and finance. version:1
arxiv-1207-6253 | On When and How to use SAT to Mine Frequent Itemsets | http://arxiv.org/abs/1207.6253 | id:1207.6253 author:Rui Henriques, Inês Lynce, Vasco Manquinho category:cs.AI cs.DB cs.LG  published:2012-07-26 summary:A new stream of research was born in the last decade with the goal of mining itemsets of interest using Constraint Programming (CP). This has promoted a natural way to combine complex constraints in a highly flexible manner. Although CP state-of-the-art solutions formulate the task using Boolean variables, the few attempts to adopt propositional Satisfiability (SAT) provided an unsatisfactory performance. This work deepens the study on when and how to use SAT for the frequent itemset mining (FIM) problem by defining different encodings with multiple task-driven enumeration options and search strategies. Although for the majority of the scenarios SAT-based solutions appear to be non-competitive with CP peers, results show a variety of interesting cases where SAT encodings are the best option. version:1
arxiv-1104-4824 | Fast global convergence of gradient methods for high-dimensional statistical recovery | http://arxiv.org/abs/1104.4824 | id:1104.4824 author:Alekh Agarwal, Sahand N. Negahban, Martin J. Wainwright category:stat.ML cs.IT math.IT  published:2011-04-25 summary:Many statistical $M$-estimators are based on convex optimization problems formed by the combination of a data-dependent loss function with a norm-based regularizer. We analyze the convergence rates of projected gradient and composite gradient methods for solving such problems, working within a high-dimensional framework that allows the data dimension $\pdim$ to grow with (and possibly exceed) the sample size $\numobs$. This high-dimensional structure precludes the usual global assumptions---namely, strong convexity and smoothness conditions---that underlie much of classical optimization analysis. We define appropriately restricted versions of these conditions, and show that they are satisfied with high probability for various statistical models. Under these conditions, our theory guarantees that projected gradient descent has a globally geometric rate of convergence up to the \emph{statistical precision} of the model, meaning the typical distance between the true unknown parameter $\theta^*$ and an optimal solution $\hat{\theta}$. This result is substantially sharper than previous convergence results, which yielded sublinear convergence, or linear convergence only up to the noise level. Our analysis applies to a wide range of $M$-estimators and statistical models, including sparse linear regression using Lasso ($\ell_1$-regularized regression); group Lasso for block sparsity; log-linear models with regularization; low-rank matrix recovery using nuclear norm regularization; and matrix decomposition. Overall, our analysis reveals interesting connections between statistical precision and computational efficiency in high-dimensional estimation. version:3
arxiv-1207-5871 | Optimal Sampling Points in Reproducing Kernel Hilbert Spaces | http://arxiv.org/abs/1207.5871 | id:1207.5871 author:Rui Wang, Haizhang Zhang category:cs.IT math.IT stat.ML  published:2012-07-25 summary:The recent developments of basis pursuit and compressed sensing seek to extract information from as few samples as possible. In such applications, since the number of samples is restricted, one should deploy the sampling points wisely. We are motivated to study the optimal distribution of finite sampling points. Formulation under the framework of optimal reconstruction yields a minimization problem. In the discrete case, we estimate the distance between the optimal subspace resulting from a general Karhunen-Loeve transform and the kernel space to obtain another algorithm that is computationally favorable. Numerical experiments are then presented to illustrate the performance of the algorithms for the searching of optimal sampling points. version:1
arxiv-1111-2262 | Improved Bound for the Nystrom's Method and its Application to Kernel Classification | http://arxiv.org/abs/1111.2262 | id:1111.2262 author:Rong Jin, Tianbao Yang, Mehrdad Mahdavi, Yu-Feng Li, Zhi-Hua Zhou category:cs.LG cs.NA  published:2011-11-09 summary:We develop two approaches for analyzing the approximation error bound for the Nystr\"{o}m method, one based on the concentration inequality of integral operator, and one based on the compressive sensing theory. We show that the approximation error, measured in the spectral norm, can be improved from $O(N/\sqrt{m})$ to $O(N/m^{1 - \rho})$ in the case of large eigengap, where $N$ is the total number of data points, $m$ is the number of sampled data points, and $\rho \in (0, 1/2)$ is a positive constant that characterizes the eigengap. When the eigenvalues of the kernel matrix follow a $p$-power law, our analysis based on compressive sensing theory further improves the bound to $O(N/m^{p - 1})$ under an incoherence assumption, which explains why the Nystr\"{o}m method works well for kernel matrix with skewed eigenvalues. We present a kernel classification approach based on the Nystr\"{o}m method and derive its generalization performance using the improved bound. We show that when the eigenvalues of kernel matrix follow a $p$-power law, we can reduce the number of support vectors to $N^{2p/(p^2 - 1)}$, a number less than $N$ when $p > 1+\sqrt{2}$, without seriously sacrificing its generalization performance. version:4
arxiv-1205-4656 | Conditional mean embeddings as regressors - supplementary | http://arxiv.org/abs/1205.4656 | id:1205.4656 author:Steffen Grünewälder, Guy Lever, Luca Baldassarre, Sam Patterson, Arthur Gretton, Massimilano Pontil category:cs.LG stat.ML  published:2012-05-21 summary:We demonstrate an equivalence between reproducing kernel Hilbert space (RKHS) embeddings of conditional distributions and vector-valued regressors. This connection introduces a natural regularized loss function which the RKHS embeddings minimise, providing an intuitive understanding of the embeddings and a justification for their use. Furthermore, the equivalence allows the application of vector-valued regression methods and results to the problem of learning conditional distributions. Using this link we derive a sparse version of the embedding by considering alternative formulations. Further, by applying convergence results for vector-valued regression to the embedding problem we derive minimax convergence rates which are O(\log(n)/n) -- compared to current state of the art rates of O(n^{-1/4}) -- and are valid under milder and more intuitive assumptions. These minimax upper rates coincide with lower rates up to a logarithmic factor, showing that the embedding method achieves nearly optimal rates. We study our sparse embedding algorithm in a reinforcement learning task where the algorithm shows significant improvement in sparsity over an incomplete Cholesky decomposition. version:2
arxiv-1207-5589 | VOI-aware MCTS | http://arxiv.org/abs/1207.5589 | id:1207.5589 author:David Tolpin, Solomon Eyal Shimony category:cs.AI cs.LG  published:2012-07-24 summary:UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in games and Markov decision processes, is based on UCB1, a sampling policy for the Multi-armed Bandit problem (MAB) that minimizes the cumulative regret. However, search differs from MAB in that in MCTS it is usually only the final "arm pull" (the actual move selection) that collects a reward, rather than all "arm pulls". In this paper, an MCTS sampling policy based on Value of Information (VOI) estimates of rollouts is suggested. Empirical evaluation of the policy and comparison to UCB1 and UCT is performed on random MAB instances as well as on Computer Go. version:1
arxiv-1207-5560 | Evolving Musical Counterpoint: The Chronopoint Musical Evolution System | http://arxiv.org/abs/1207.5560 | id:1207.5560 author:Jeffrey Power Jacobs, James Reggia category:cs.SD cs.AI cs.NE  published:2012-07-23 summary:Musical counterpoint, a musical technique in which two or more independent melodies are played simultaneously with the goal of creating harmony, has been around since the baroque era. However, to our knowledge computational generation of aesthetically pleasing linear counterpoint based on subjective fitness assessment has not been explored by the evolutionary computation community (although generation using objective fitness has been attempted in quite a few cases). The independence of contrapuntal melodies and the subjective nature of musical aesthetics provide an excellent platform for the application of genetic algorithms. In this paper, a genetic algorithm approach to generating contrapuntal melodies is explained, with a description of the various musical heuristics used and of how variable-length chromosome strings are used to avoid generating "jerky" rhythms and melodic phrases, as well as how subjectivity is incorporated into the algorithm's fitness measures. Next, results from empirical testing of the algorithm are presented, with a focus on how a user's musical sophistication influences their experience. Lastly, further musical and compositional applications of the algorithm are discussed along with planned future work on the algorithm. version:1
arxiv-1111-2667 | A note on the lack of symmetry in the graphical lasso | http://arxiv.org/abs/1111.2667 | id:1111.2667 author:Benjamin T. Rolfs, Bala Rajaratnam category:stat.ML stat.CO  published:2011-11-11 summary:The graphical lasso (glasso) is a widely-used fast algorithm for estimating sparse inverse covariance matrices. The glasso solves an L1 penalized maximum likelihood problem and is available as an R library on CRAN. The output from the glasso, a regularized covariance matrix estimate a sparse inverse covariance matrix estimate, not only identify a graphical model but can also serve as intermediate inputs into multivariate procedures such as PCA, LDA, MANOVA, and others. The glasso indeed produces a covariance matrix estimate which solves the L1 penalized optimization problem in a dual sense; however, the method for producing the inverse covariance matrix estimator after this optimization is inexact and may produce asymmetric estimates. This problem is exacerbated when the amount of L1 regularization that is applied is small, which in turn is more likely to occur if the true underlying inverse covariance matrix is not sparse. The lack of symmetry can potentially have consequences. First, it implies that the covariance and inverse covariance estimates are not numerical inverses of one another, and second, asymmetry can possibly lead to negative or complex eigenvalues,rendering many multivariate procedures which may depend on the inverse covariance estimator unusable. We demonstrate this problem, explain its causes, and propose possible remedies. version:2
arxiv-1207-5536 | MCTS Based on Simple Regret | http://arxiv.org/abs/1207.5536 | id:1207.5536 author:David Tolpin, Solomon Eyal Shimony category:cs.AI cs.LG  published:2012-07-23 summary:UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in games and Markov decision processes, is based on UCB, a sampling policy for the Multi-armed Bandit problem (MAB) that minimizes the cumulative regret. However, search differs from MAB in that in MCTS it is usually only the final "arm pull" (the actual move selection) that collects a reward, rather than all "arm pulls". Therefore, it makes more sense to minimize the simple regret, as opposed to the cumulative regret. We begin by introducing policies for multi-armed bandits with lower finite-time and asymptotic simple regret than UCB, using it to develop a two-stage scheme (SR+CR) for MCTS which outperforms UCT empirically. Optimizing the sampling process is itself a metareasoning problem, a solution of which can use value of information (VOI) techniques. Although the theory of VOI for search exists, applying it to MCTS is non-trivial, as typical myopic assumptions fail. Lacking a complete working VOI theory for MCTS, we nevertheless propose a sampling scheme that is "aware" of VOI, achieving an algorithm that in empirical evaluation outperforms both UCT and the other proposed algorithms. version:1
arxiv-1207-5451 | Nonlinear spectral unmixing of hyperspectral images using Gaussian processes | http://arxiv.org/abs/1207.5451 | id:1207.5451 author:Yoann Altmann, Nicolas Dobigeon, Steve McLaughlin, Jean-Yves Tourneret category:stat.ML physics.data-an stat.AP  published:2012-07-23 summary:This paper presents an unsupervised algorithm for nonlinear unmixing of hyperspectral images. The proposed model assumes that the pixel reflectances result from a nonlinear function of the abundance vectors associated with the pure spectral components. We assume that the spectral signatures of the pure components and the nonlinear function are unknown. The first step of the proposed method consists of the Bayesian estimation of the abundance vectors for all the image pixels and the nonlinear function relating the abundance vectors to the observations. The endmembers are subsequently estimated using Gaussian process regression. The performance of the unmixing strategy is evaluated with simulations conducted on synthetic and real data. version:1
arxiv-1202-1467 | Message-Passing Algorithms for Channel Estimation and Decoding Using Approximate Inference | http://arxiv.org/abs/1202.1467 | id:1202.1467 author:Mihai-Alin Badiu, Gunvor Elisabeth Kirkelund, Carles Navarro Manchón, Erwin Riegler, Bernard Henri Fleury category:cs.IT math.IT stat.ML  published:2012-02-07 summary:We design iterative receiver schemes for a generic wireless communication system by treating channel estimation and information decoding as an inference problem in graphical models. We introduce a recently proposed inference framework that combines belief propagation (BP) and the mean field (MF) approximation and includes these algorithms as special cases. We also show that the expectation propagation and expectation maximization algorithms can be embedded in the BP-MF framework with slight modifications. By applying the considered inference algorithms to our probabilistic model, we derive four different message-passing receiver schemes. Our numerical evaluation demonstrates that the receiver based on the BP-MF framework and its variant based on BP-EM yield the best compromise between performance, computational complexity and numerical stability among all candidate algorithms. version:2
arxiv-1207-5409 | FST Based Morphological Analyzer for Hindi Language | http://arxiv.org/abs/1207.5409 | id:1207.5409 author:Deepak Kumar, Manjeet Singh, Seema Shukla category:cs.CL cs.IR  published:2012-07-23 summary:Hindi being a highly inflectional language, FST (Finite State Transducer) based approach is most efficient for developing a morphological analyzer for this language. The work presented in this paper uses the SFST (Stuttgart Finite State Transducer) tool for generating the FST. A lexicon of root words is created. Rules are then added for generating inflectional and derivational words from these root words. The Morph Analyzer developed was used in a Part Of Speech (POS) Tagger based on Stanford POS Tagger. The system was first trained using a manually tagged corpus and MAXENT (Maximum Entropy) approach of Stanford POS tagger was then used for tagging input sentences. The morphological analyzer gives approximately 97% correct results. POS tagger gives an accuracy of approximately 87% for the sentences that have the words known to the trained model file, and 80% accuracy for the sentences that have the words unknown to the trained model file. version:1
arxiv-1207-5371 | Towards a theory of statistical tree-shape analysis | http://arxiv.org/abs/1207.5371 | id:1207.5371 author:Aasa Feragen, Pechin Lo, Marleen de Bruijne, Mads Nielsen, Francois Lauze category:stat.ME cs.CV math.MG  published:2012-07-23 summary:In order to develop statistical methods for shapes with a tree-structure, we construct a shape space framework for tree-like shapes and study metrics on the shape space. This shape space has singularities, corresponding to topological transitions in the represented trees. We study two closely related metrics on the shape space, TED and QED. QED is a quotient Euclidean distance arising naturally from the shape space formulation, while TED is the classical tree edit distance. Using Gromov's metric geometry we gain new insight into the geometries defined by TED and QED. We show that the new metric QED has nice geometric properties which facilitate statistical analysis, such as existence and local uniqueness of geodesics and averages. TED, on the other hand, does not share the geometric advantages of QED, but has nice algorithmic properties. We provide a theoretical framework and experimental results on synthetic data trees as well as airway trees from pulmonary CT scans. This way, we effectively illustrate that our framework has both the theoretical and qualitative properties necessary to build a theory of statistical tree-shape analysis. version:1
arxiv-1207-5342 | A Robust Signal Classification Scheme for Cognitive Radio | http://arxiv.org/abs/1207.5342 | id:1207.5342 author:Hanwen Cao, Jürgen Peissig category:cs.IT cs.LG cs.NI math.IT C.2.1  published:2012-07-23 summary:This paper presents a robust signal classification scheme for achieving comprehensive spectrum sensing of multiple coexisting wireless systems. It is built upon a group of feature-based signal detection algorithms enhanced by the proposed dimension cancelation (DIC) method for mitigating the noise uncertainty problem. The classification scheme is implemented on our testbed consisting real-world wireless devices. The simulation and experimental performances agree with each other well and shows the e?ectiveness and robustness of the proposed scheme. version:1
arxiv-1207-5326 | Guarantees of Augmented Trace Norm Models in Tensor Recovery | http://arxiv.org/abs/1207.5326 | id:1207.5326 author:Ziqiang Shi, Jiqing Han, Tieran Zheng, Shiwen Deng, Ji Li category:cs.IT cs.CV math.IT  published:2012-07-23 summary:This paper studies the recovery guarantees of the models of minimizing $\ \mathcal{X}\ _*+\frac{1}{2\alpha}\ \mathcal{X}\ _F^2$ where $\mathcal{X}$ is a tensor and $\ \mathcal{X}\ _*$ and $\ \mathcal{X}\ _F$ are the trace and Frobenius norm of respectively. We show that they can efficiently recover low-rank tensors. In particular, they enjoy exact guarantees similar to those known for minimizing $\ \mathcal{X}\ _*$ under the conditions on the sensing operator such as its null-space property, restricted isometry property, or spherical section property. To recover a low-rank tensor $\mathcal{X}^0$, minimizing $\ \mathcal{X}\ _*+\frac{1}{2\alpha}\ \mathcal{X}\ _F^2$ returns the same solution as minimizing $\ \mathcal{X}\ _*$ almost whenever $\alpha\geq10\mathop {\max}\limits_{i}\ X^0_{(i)}\ _2$. version:1
arxiv-1207-5208 | Meta-Learning of Exploration/Exploitation Strategies: The Multi-Armed Bandit Case | http://arxiv.org/abs/1207.5208 | id:1207.5208 author:Francis Maes, Damien Ernst, Louis Wehenkel category:cs.AI cs.LG stat.ML  published:2012-07-22 summary:The exploration/exploitation (E/E) dilemma arises naturally in many subfields of Science. Multi-armed bandit problems formalize this dilemma in its canonical form. Most current research in this field focuses on generic solutions that can be applied to a wide range of problems. However, in practice, it is often the case that a form of prior information is available about the specific class of target problems. Prior knowledge is rarely used in current solutions due to the lack of a systematic approach to incorporate it into the E/E strategy. To address a specific class of E/E problems, we propose to proceed in three steps: (i) model prior knowledge in the form of a probability distribution over the target class of E/E problems; (ii) choose a large hypothesis space of candidate E/E strategies; and (iii), solve an optimization problem to find a candidate E/E strategy of maximal average performance over a sample of problems drawn from the prior distribution. We illustrate this meta-learning approach with two different hypothesis spaces: one where E/E strategies are numerically parameterized and another where E/E strategies are represented as small symbolic formulas. We propose appropriate optimization algorithms for both cases. Our experiments, with two-armed Bernoulli bandit problems and various playing budgets, show that the meta-learnt E/E strategies outperform generic strategies of the literature (UCB1, UCB1-Tuned, UCB-v, KL-UCB and epsilon greedy); they also evaluate the robustness of the learnt E/E strategies, by tests carried out on arms whose rewards follow a truncated Gaussian distribution. version:1
arxiv-1203-1269 | A Short Note on Gaussian Process Modeling for Large Datasets using Graphics Processing Units | http://arxiv.org/abs/1203.1269 | id:1203.1269 author:Mark Franey, Pritam Ranjan, Hugh Chipman category:stat.CO stat.ML  published:2012-03-06 summary:The graphics processing unit (GPU) has emerged as a powerful and cost effective processor for general performance computing. GPUs are capable of an order of magnitude more floating-point operations per second as compared to modern central processing units (CPUs), and thus provide a great deal of promise for computationally intensive statistical applications. Fitting complex statistical models with a large number of parameters and/or for large datasets is often very computationally expensive. In this study, we focus on Gaussian process (GP) models -- statistical models commonly used for emulating expensive computer simulators. We demonstrate that the computational cost of implementing GP models can be significantly reduced by using a CPU+GPU heterogeneous computing system over an analogous implementation on a traditional computing system with no GPU acceleration. Our small study suggests that GP models are fertile ground for further implementation on CPU+GPU systems. version:2
arxiv-1207-5136 | Causal Inference on Time Series using Structural Equation Models | http://arxiv.org/abs/1207.5136 | id:1207.5136 author:Jonas Peters, Dominik Janzing, Bernhard Schölkopf category:stat.ML cs.LG stat.ME  published:2012-07-21 summary:Causal inference uses observations to infer the causal structure of the data generating system. We study a class of functional models that we call Time Series Models with Independent Noise (TiMINo). These models require independent residual time series, whereas traditional methods like Granger causality exploit the variance of residuals. There are two main contributions: (1) Theoretical: By restricting the model class (e.g. to additive noise) we can provide a more general identifiability result than existing ones. This result incorporates lagged and instantaneous effects that can be nonlinear and do not need to be faithful, and non-instantaneous feedbacks between the time series. (2) Practical: If there are no feedback loops between time series, we propose an algorithm based on non-linear independence tests of time series. When the data are causally insufficient, or the data generating process does not satisfy the model assumptions, this algorithm may still give partial results, but mostly avoids incorrect answers. An extension to (non-instantaneous) feedbacks is possible, but not discussed. It outperforms existing methods on artificial and real data. Code can be provided upon request. version:1
arxiv-1207-5113 | Piecewise Linear Patch Reconstruction for Segmentation and Description of Non-smooth Image Structures | http://arxiv.org/abs/1207.5113 | id:1207.5113 author:Junyan Wang, Kap Luk Chan category:cs.CV  published:2012-07-21 summary:In this paper, we propose a unified energy minimization model for the segmentation of non-smooth image structures. The energy of piecewise linear patch reconstruction is considered as an objective measure of the quality of the segmentation of non-smooth structures. The segmentation is achieved by minimizing the single energy without any separate process of feature extraction. We also prove that the error of segmentation is bounded by the proposed energy functional, meaning that minimizing the proposed energy leads to reducing the error of segmentation. As a by-product, our method produces a dictionary of optimized orthonormal descriptors for each segmented region. The unique feature of our method is that it achieves the simultaneous segmentation and description for non-smooth image structures under the same optimization framework. The experiments validate our theoretical claims and show the clear superior performance of our methods over other related methods for segmentation of various image textures. We show that our model can be coupled with the piecewise smooth model to handle both smooth and non-smooth structures, and we demonstrate that the proposed model is capable of coping with multiple different regions through the one-against-all strategy. version:1
arxiv-1207-5091 | Learning Probabilistic Systems from Tree Samples | http://arxiv.org/abs/1207.5091 | id:1207.5091 author:Anvesh Komuravelli, Corina S. Pasareanu, Edmund M. Clarke category:cs.LO cs.LG  published:2012-07-21 summary:We consider the problem of learning a non-deterministic probabilistic system consistent with a given finite set of positive and negative tree samples. Consistency is defined with respect to strong simulation conformance. We propose learning algorithms that use traditional and a new "stochastic" state-space partitioning, the latter resulting in the minimum number of states. We then use them to solve the problem of "active learning", that uses a knowledgeable teacher to generate samples as counterexamples to simulation equivalence queries. We show that the problem is undecidable in general, but that it becomes decidable under a suitable condition on the teacher which comes naturally from the way samples are generated from failed simulation checks. The latter problem is shown to be undecidable if we impose an additional condition on the learner to always conjecture a "minimum state" hypothesis. We therefore propose a semi-algorithm using stochastic partitions. Finally, we apply the proposed (semi-) algorithms to infer intermediate assumptions in an automated assume-guarantee verification framework for probabilistic systems. version:1
arxiv-1110-5097 | Absolute Uniqueness of Phase Retrieval with Random Illumination | http://arxiv.org/abs/1110.5097 | id:1110.5097 author:Albert Fannjiang category:physics.optics cs.CV math-ph math.MP  published:2011-10-23 summary:Random illumination is proposed to enforce absolute uniqueness and resolve all types of ambiguity, trivial or nontrivial, from phase retrieval. Almost sure irreducibility is proved for any complex-valued object of a full rank support. While the new irreducibility result can be viewed as a probabilistic version of the classical result by Bruck, Sodin and Hayes, it provides a novel perspective and an effective method for phase retrieval. In particular, almost sure uniqueness, up to a global phase, is proved for complex-valued objects under general two-point conditions. Under a tight sector constraint absolute uniqueness is proved to hold with probability exponentially close to unity as the object sparsity increases. Under a magnitude constraint with random amplitude illumination, uniqueness modulo global phase is proved to hold with probability exponentially close to unity as object sparsity increases. For general complex-valued objects without any constraint, almost sure uniqueness up to global phase is established with two sets of Fourier magnitude data under two independent illuminations. Numerical experiments suggest that random illumination essentially alleviates most, if not all, numerical problems commonly associated with the standard phasing algorithms. version:7
arxiv-1207-5064 | A Novel Metric Approach Evaluation For The Spatial Enhancement Of Pan-Sharpened Images | http://arxiv.org/abs/1207.5064 | id:1207.5064 author:Firouz Abdullah Al-Wassai, Dr. N. V. Kalyankar category:cs.CV  published:2012-07-20 summary:Various and different methods can be used to produce high-resolution multispectral images from high-resolution panchromatic image (PAN) and low-resolution multispectral images (MS), mostly on the pixel level. The Quality of image fusion is an essential determinant of the value of processing images fusion for many applications. Spatial and spectral qualities are the two important indexes that used to evaluate the quality of any fused image. However, the jury is still out of fused image's benefits if it compared with its original images. In addition, there is a lack of measures for assessing the objective quality of the spatial resolution for the fusion methods. So, an objective quality of the spatial resolution assessment for fusion images is required. Therefore, this paper describes a new approach proposed to estimate the spatial resolution improve by High Past Division Index (HPDI) upon calculating the spatial-frequency of the edge regions of the image and it deals with a comparison of various analytical techniques for evaluating the Spatial quality, and estimating the colour distortion added by image fusion including: MG, SG, FCC, SD, En, SNR, CC and NRMSE. In addition, this paper devotes to concentrate on the comparison of various image fusion techniques based on pixel and feature fusion technique. version:1
arxiv-1207-5058 | Parameter and Structure Learning in Nested Markov Models | http://arxiv.org/abs/1207.5058 | id:1207.5058 author:Ilya Shpitser, Thomas S. Richardson, James M. Robins, Robin Evans category:stat.ML math.ST stat.TH  published:2012-07-20 summary:The constraints arising from DAG models with latent variables can be naturally represented by means of acyclic directed mixed graphs (ADMGs). Such graphs contain directed and bidirected arrows, and contain no directed cycles. DAGs with latent variables imply independence constraints in the distribution resulting from a 'fixing' operation, in which a joint distribution is divided by a conditional. This operation generalizes marginalizing and conditioning. Some of these constraints correspond to identifiable 'dormant' independence constraints, with the well known 'Verma constraint' as one example. Recently, models defined by a set of the constraints arising after fixing from a DAG with latents, were characterized via a recursive factorization and a nested Markov property. In addition, a parameterization was given in the discrete case. In this paper we use this parameterization to describe a parameter fitting algorithm, and a search and score structure learning algorithm for these nested Markov models. We apply our algorithms to a variety of datasets. version:1
arxiv-1207-5007 | Multisegmentation through wavelets: Comparing the efficacy of Daubechies vs Coiflets | http://arxiv.org/abs/1207.5007 | id:1207.5007 author:Madhur Srivastava, Yashwant Yashu, Satish K. Singh, Prasanta K. Panigrahi category:cs.CV  published:2012-07-20 summary:In this paper, we carry out a comparative study of the efficacy of wavelets belonging to Daubechies and Coiflet family in achieving image segmentation through a fast statistical algorithm.The fact that wavelets belonging to Daubechies family optimally capture the polynomial trends and those of Coiflet family satisfy mini-max condition, makes this comparison interesting. In the context of the present algorithm, it is found that the performance of Coiflet wavelets is better, as compared to Daubechies wavelet. version:1
arxiv-1111-7190 | Developing Embodied Multisensory Dialogue Agents | http://arxiv.org/abs/1111.7190 | id:1111.7190 author:Michał B. Paradowski category:cs.AI cs.CL  published:2011-11-29 summary:A few decades of work in the AI field have focused efforts on developing a new generation of systems which can acquire knowledge via interaction with the world. Yet, until very recently, most such attempts were underpinned by research which predominantly regarded linguistic phenomena as separated from the brain and body. This could lead one into believing that to emulate linguistic behaviour, it suffices to develop 'software' operating on abstract representations that will work on any computational machine. This picture is inaccurate for several reasons, which are elucidated in this paper and extend beyond sensorimotor and semantic resonance. Beginning with a review of research, I list several heterogeneous arguments against disembodied language, in an attempt to draw conclusions for developing embodied multisensory agents which communicate verbally and non-verbally with their environment. Without taking into account both the architecture of the human brain, and embodiment, it is unrealistic to replicate accurately the processes which take place during language acquisition, comprehension, production, or during non-linguistic actions. While robots are far from isomorphic with humans, they could benefit from strengthened associative connections in the optimization of their processes and their reactivity and sensitivity to environmental stimuli, and in situated human-machine interaction. The concept of multisensory integration should be extended to cover linguistic input and the complementary information combined from temporally coincident sensory impressions. version:3
arxiv-1207-4931 | Motion Planning Of an Autonomous Mobile Robot Using Artificial Neural Network | http://arxiv.org/abs/1207.4931 | id:1207.4931 author:G. N. Tripathi, V. Rihani category:cs.RO cs.AI cs.LG cs.NE  published:2012-07-20 summary:The paper presents the electronic design and motion planning of a robot based on decision making regarding its straight motion and precise turn using Artificial Neural Network (ANN). The ANN helps in learning of robot so that it performs motion autonomously. The weights calculated are implemented in microcontroller. The performance has been tested to be excellent. version:1
arxiv-1207-3031 | Distributed Strongly Convex Optimization | http://arxiv.org/abs/1207.3031 | id:1207.3031 author:Konstantinos I. Tsianos, Michael G. Rabbat category:cs.DC cs.LG stat.ML  published:2012-07-12 summary:A lot of effort has been invested into characterizing the convergence rates of gradient based algorithms for non-linear convex optimization. Recently, motivated by large datasets and problems in machine learning, the interest has shifted towards distributed optimization. In this work we present a distributed algorithm for strongly convex constrained optimization. Each node in a network of n computers converges to the optimum of a strongly convex, L-Lipchitz continuous, separable objective at a rate O(log (sqrt(n) T) / T) where T is the number of iterations. This rate is achieved in the online setting where the data is revealed one at a time to the nodes, and in the batch setting where each node has access to its full local dataset from the start. The same convergence rate is achieved in expectation when the subgradients used at each node are corrupted with additive zero-mean noise. version:2
arxiv-1004-0378 | Facial Expression Representation and Recognition Using 2DHLDA, Gabor Wavelets, and Ensemble Learning | http://arxiv.org/abs/1004.0378 | id:1004.0378 author:Mahmoud Khademi, Mohammad H. Kiapour, Mehran Safayani, Mohammad T. Manzuri, M. Shojaei category:cs.CV cs.LG I.5  published:2010-04-02 summary:In this paper, a novel method for representation and recognition of the facial expressions in two-dimensional image sequences is presented. We apply a variation of two-dimensional heteroscedastic linear discriminant analysis (2DHLDA) algorithm, as an efficient dimensionality reduction technique, to Gabor representation of the input sequence. 2DHLDA is an extension of the two-dimensional linear discriminant analysis (2DLDA) approach and it removes the equal within-class covariance. By applying 2DHLDA in two directions, we eliminate the correlations between both image columns and image rows. Then, we perform a one-dimensional LDA on the new features. This combined method can alleviate the small sample size problem and instability encountered by HLDA. Also, employing both geometric and appearance features and using an ensemble learning scheme based on data fusion, we create a classifier which can efficiently classify the facial expressions. The proposed method is robust to illumination changes and it can properly represent temporal information as well as subtle changes in facial muscles. We provide experiments on Cohn-Kanade database that show the superiority of the proposed method. KEYWORDS: two-dimensional heteroscedastic linear discriminant analysis (2DHLDA), subspace learning, facial expression analysis, Gabor wavelets, ensemble learning. version:7
arxiv-1010-4951 | Local Component Analysis for Nonparametric Bayes Classifier | http://arxiv.org/abs/1010.4951 | id:1010.4951 author:Mahmoud Khademi, Mohammad T. Manzuri-Shalmani, Meharn safayani category:cs.CV cs.LG  published:2010-10-24 summary:The decision boundaries of Bayes classifier are optimal because they lead to maximum probability of correct decision. It means if we knew the prior probabilities and the class-conditional densities, we could design a classifier which gives the lowest probability of error. However, in classification based on nonparametric density estimation methods such as Parzen windows, the decision regions depend on the choice of parameters such as window width. Moreover, these methods suffer from curse of dimensionality of the feature space and small sample size problem which severely restricts their practical applications. In this paper, we address these problems by introducing a novel dimension reduction and classification method based on local component analysis. In this method, by adopting an iterative cross-validation algorithm, we simultaneously estimate the optimal transformation matrices (for dimension reduction) and classifier parameters based on local information. The proposed method can classify the data with complicated boundary and also alleviate the course of dimensionality dilemma. Experiments on real data show the superiority of the proposed algorithm in term of classification accuracies for pattern classification applications like age, facial expression and character recognition. Keywords: Bayes classifier, curse of dimensionality dilemma, Parzen window, pattern classification, subspace learning. version:2
arxiv-1207-4814 | Automorphism Groups of Graphical Models and Lifted Variational Inference | http://arxiv.org/abs/1207.4814 | id:1207.4814 author:Hung Hai Bui, Tuyen N. Huynh, Sebastian Riedel category:cs.AI cs.LG math.CO stat.CO stat.ML  published:2012-07-19 summary:Using the theory of group action, we first introduce the concept of the automorphism group of an exponential family or a graphical model, thus formalizing the general notion of symmetry of a probabilistic model. This automorphism group provides a precise mathematical framework for lifted inference in the general exponential family. Its group action partitions the set of random variables and feature functions into equivalent classes (called orbits) having identical marginals and expectations. Then the inference problem is effectively reduced to that of computing marginals or expectations for each class, thus avoiding the need to deal with each individual variable or feature. We demonstrate the usefulness of this general framework in lifting two classes of variational approximation for MAP inference: local LP relaxation and local LP relaxation with cycle constraints; the latter yields the first lifted inference that operate on a bound tighter than local constraints. Initial experimental results demonstrate that lifted MAP inference with cycle constraints achieved the state of the art performance, obtaining much better objective function values than local approximation while remaining relatively efficient. version:1
arxiv-1207-4748 | Hierarchical Clustering using Randomly Selected Similarities | http://arxiv.org/abs/1207.4748 | id:1207.4748 author:Brian Eriksson category:stat.ML cs.IT cs.LG math.IT  published:2012-07-19 summary:The problem of hierarchical clustering items from pairwise similarities is found across various scientific disciplines, from biology to networking. Often, applications of clustering techniques are limited by the cost of obtaining similarities between pairs of items. While prior work has been developed to reconstruct clustering using a significantly reduced set of pairwise similarities via adaptive measurements, these techniques are only applicable when choice of similarities are available to the user. In this paper, we examine reconstructing hierarchical clustering under similarity observations at-random. We derive precise bounds which show that a significant fraction of the hierarchical clustering can be recovered using fewer than all the pairwise similarities. We find that the correct hierarchical clustering down to a constant fraction of the total number of items (i.e., clusters sized O(N)) can be found using only O(N log N) randomly selected pairwise similarities in expectation. version:1
arxiv-1207-4674 | Models of Disease Spectra | http://arxiv.org/abs/1207.4674 | id:1207.4674 author:Iead Rezek, Christian Beckmann category:stat.ML  published:2012-07-19 summary:Case vs control comparisons have been the classical approach to the study of neurological diseases. However, most patients will not fall cleanly into either group. Instead, clinicians will typically find patients that cannot be classified as having clearly progressed into the disease state. For those subjects, very little can be said about their brain function on the basis of analyses of group differences. To describe the intermediate brain function requires models that interpolate between the disease states. We have chosen Gaussian Processes (GP) regression to obtain a continuous spectrum of brain activation and to extract the unknown disease progression profile. Our models incorporate spatial distribution of measures of activation, e.g. the correlation of an fMRI trace with an input stimulus, and so constitute ultra-high multi-variate GP regressors. We applied GPs to model fMRI image phenotypes across Alzheimer's Disease (AD) behavioural measures, e.g. MMSE, ACE etc. scores, and obtained predictions at non-observed MMSE/ACE values. The overall model confirmed the known reduction in the spatial extent of activity in response to reading versus false-font stimulation. The predictive uncertainty indicated the worsening confidence intervals at behavioural scores distance from those used for GP training. Thus, the model indicated the type of patient (what behavioural score) that would need to included in the training data to improve models predictions. version:1
arxiv-1205-4217 | Thompson Sampling: An Asymptotically Optimal Finite Time Analysis | http://arxiv.org/abs/1205.4217 | id:1205.4217 author:Emilie Kaufmann, Nathaniel Korda, Rémi Munos category:stat.ML cs.LG  published:2012-05-18 summary:The question of the optimality of Thompson Sampling for solving the stochastic multi-armed bandit problem had been open since 1933. In this paper we answer it positively for the case of Bernoulli rewards by providing the first finite-time analysis that matches the asymptotic rate given in the Lai and Robbins lower bound for the cumulative regret. The proof is accompanied by a numerical comparison with other optimal policies, experiments that have been lacking in the literature until now for the Bernoulli case. version:2
arxiv-1106-0823 | Recovering Epipolar Geometry from Images of Smooth Surfaces | http://arxiv.org/abs/1106.0823 | id:1106.0823 author:Oleg Kupervasser category:cs.CV cs.AI 68T45  published:2011-06-04 summary:We present four methods for recovering the epipolar geometry from images of smooth surfaces. In the existing methods for recovering epipolar geometry corresponding feature points are used that cannot be found in such images. The first method is based on finding corresponding characteristic points created by illumination (ICPM - illumination characteristic points' method (PM)). The second method is based on correspondent tangency points created by tangents from epipoles to outline of smooth bodies (OTPM - outline tangent PM). These two methods are exact and give correct results for real images, because positions of the corresponding illumination characteristic points and corresponding outline are known with small errors. But the second method is limited either to special type of scenes or to restricted camera motion. We also consider two more methods which are termed CCPM (curve characteristic PM) and CTPM (curve tangent PM), for searching epipolar geometry for images of smooth bodies based on a set of level curves with constant illumination intensity. The CCPM method is based on searching correspondent points on isophoto curves with the help of correlation of curvatures between these lines. The CTPM method is based on property of the tangential to isophoto curve epipolar line to map into the tangential to correspondent isophoto curves epipolar line. The standard method (SM) based on knowledge of pairs of the almost exact correspondent points. The methods have been implemented and tested by SM on pairs of real images. Unfortunately, the last two methods give us only a finite subset of solutions including "good" solution. Exception is "epipoles in infinity". The main reason is inaccuracy of assumption of constant brightness for smooth bodies. But outline and illumination characteristic points are not influenced by this inaccuracy. So, the first pair of methods gives exact results. version:4
arxiv-1207-4632 | Clustering of Local Optima in Combinatorial Fitness Landscapes | http://arxiv.org/abs/1207.4632 | id:1207.4632 author:Gabriela Ochoa, Sébastien Verel, Fabio Daolio, Marco Tomassini category:cs.NE cs.AI  published:2012-07-19 summary:Using the recently proposed model of combinatorial landscapes: local optima networks, we study the distribution of local optima in two classes of instances of the quadratic assignment problem. Our results indicate that the two problem instance classes give rise to very different configuration spaces. For the so-called real-like class, the optima networks possess a clear modular structure, while the networks belonging to the class of random uniform instances are less well partitionable into clusters. We briefly discuss the consequences of the findings for heuristically searching the corresponding problem spaces. version:1
arxiv-1207-4631 | Analyzing the Effect of Objective Correlation on the Efficient Set of MNK-Landscapes | http://arxiv.org/abs/1207.4631 | id:1207.4631 author:Sébastien Verel, Arnaud Liefooghe, Laetitia Jourdan, Clarisse Dhaenens category:cs.NE cs.AI  published:2012-07-19 summary:In multiobjective combinatorial optimization, there exists two main classes of metaheuristics, based either on multiple aggregations, or on a dominance relation. As in the single objective case, the structure of the search space can explain the difficulty for multiobjective metaheuristics, and guide the design of such methods. In this work we analyze the properties of multiobjective combinatorial search spaces. In particular, we focus on the features related the efficient set, and we pay a particular attention to the correlation between objectives. Few benchmark takes such objective correlation into account. Here, we define a general method to design multiobjective problems with correlation. As an example, we extend the well-known multiobjective NK-landscapes. By measuring different properties of the search space, we show the importance of considering the objective correlation on the design of metaheuristics. version:1
arxiv-1207-4629 | On the Neutrality of Flowshop Scheduling Fitness Landscapes | http://arxiv.org/abs/1207.4629 | id:1207.4629 author:Marie-Eleonore Marmion, Clarisse Dhaenens, Laetitia Jourdan, Arnaud Liefooghe, Sébastien Verel category:cs.NE cs.AI  published:2012-07-19 summary:Solving efficiently complex problems using metaheuristics, and in particular local searches, requires incorporating knowledge about the problem to solve. In this paper, the permutation flowshop problem is studied. It is well known that in such problems, several solutions may have the same fitness value. As this neutrality property is an important one, it should be taken into account during the design of optimization methods. Then in the context of the permutation flowshop, a deep landscape analysis focused on the neutrality property is driven and propositions on the way to use this neutrality to guide efficiently the search are given. version:1
arxiv-1207-4628 | On the Effect of Connectedness for Biobjective Multiple and Long Path Problems | http://arxiv.org/abs/1207.4628 | id:1207.4628 author:Sébastien Verel, Arnaud Liefooghe, Jérémie Humeau, Laetitia Jourdan, Clarisse Dhaenens category:cs.NE cs.AI  published:2012-07-19 summary:Recently, the property of connectedness has been claimed to give a strong motivation on the design of local search techniques for multiobjective combinatorial optimization (MOCO). Indeed, when connectedness holds, a basic Pareto local search, initialized with at least one non-dominated solution, allows to identify the efficient set exhaustively. However, this becomes quickly infeasible in practice as the number of efficient solutions typically grows exponentially with the instance size. As a consequence, we generally have to deal with a limited-size approximation, where a good sample set has to be found. In this paper, we propose the biobjective multiple and long path problems to show experimentally that, on the first problems, even if the efficient set is connected, a local search may be outperformed by a simple evolutionary algorithm in the sampling of the efficient set. At the opposite, on the second problems, a local search algorithm may successfully approximate a disconnected efficient set. Then, we argue that connectedness is not the single property to study for the design of local search heuristics for MOCO. This work opens new discussions on a proper definition of the multiobjective fitness landscape. version:1
arxiv-1207-4626 | The Road to VEGAS: Guiding the Search over Neutral Networks | http://arxiv.org/abs/1207.4626 | id:1207.4626 author:Marie-Eleonore Marmion, Clarisse Dhaenens, Laetitia Jourdan, Arnaud Liefooghe, Sébastien Verel category:cs.NE  published:2012-07-19 summary:VEGAS (Varying Evolvability-Guided Adaptive Search) is a new methodology proposed to deal with the neutrality property of some optimization problems. ts main feature is to consider the whole neutral network rather than an arbitrary solution. Moreover, VEGAS is designed to escape from plateaus based on the evolvability of solution and a multi-armed bandit. Experiments are conducted on NK-landscapes with neutrality. Results show the importance of considering the whole neutral network and of guiding the search cleverly. The impact of the level of neutrality and of the exploration-exploitation trade-off are deeply analyzed. version:1
arxiv-1207-4625 | Appropriate Nouns with Obligatory Modifiers | http://arxiv.org/abs/1207.4625 | id:1207.4625 author:E. Laporte category:cs.CL  published:2012-07-19 summary:The notion of appropriate sequence as introduced by Z. Harris provides a powerful syntactic way of analysing the detailed meaning of various sentences, including ambiguous ones. In an adjectival sentence like 'The leather was yellow', the introduction of an appropriate noun, here 'colour', specifies which quality the adjective describes. In some other adjectival sentences with an appropriate noun, that noun plays the same part as 'colour' and seems to be relevant to the description of the adjective. These appropriate nouns can usually be used in elementary sentences like 'The leather had some colour', but in many cases they have a more or less obligatory modifier. For example, you can hardly mention that an object has a colour without qualifying that colour at all. About 300 French nouns are appropriate in at least one adjectival sentence and have an obligatory modifier. They enter in a number of sentence structures related by several syntactic transformations. The appropriateness of the noun and the fact that the modifier is obligatory are reflected in these transformations. The description of these syntactic phenomena provides a basis for a classification of these nouns. It also concerns the lexical properties of thousands of predicative adjectives, and in particular the relations between the sentence without the noun : 'The leather was yellow' and the adjectival sentence with the noun : 'The colour of the leather was yellow'. version:1
arxiv-1207-4597 | Local stability of Belief Propagation algorithm with multiple fixed points | http://arxiv.org/abs/1207.4597 | id:1207.4597 author:Victorin Martin, Jean-Marc Lasgouttes, Cyril Furtlehner category:stat.ML cs.LG  published:2012-07-19 summary:A number of problems in statistical physics and computer science can be expressed as the computation of marginal probabilities over a Markov random field. Belief propagation, an iterative message-passing algorithm, computes exactly such marginals when the underlying graph is a tree. But it has gained its popularity as an efficient way to approximate them in the more general case, even if it can exhibits multiple fixed points and is not guaranteed to converge. In this paper, we express a new sufficient condition for local stability of a belief propagation fixed point in terms of the graph structure and the beliefs values at the fixed point. This gives credence to the usual understanding that Belief Propagation performs better on sparse graphs. version:1
arxiv-1207-4463 | Protein Function Prediction Based on Kernel Logistic Regression with 2-order Graphic Neighbor Information | http://arxiv.org/abs/1207.4463 | id:1207.4463 author:Jingwei Liu category:q-bio.QM cs.LG q-bio.MN  published:2012-07-18 summary:To enhance the accuracy of protein-protein interaction function prediction, a 2-order graphic neighbor information feature extraction method based on undirected simple graph is proposed in this paper, which extends the 1-order graphic neighbor featureextraction method. And the chi-square test statistical method is also involved in feature combination. To demonstrate the effectiveness of our 2-order graphic neighbor feature, four logistic regression models (logistic regression (abbrev. LR), diffusion kernel logistic regression (abbrev. DKLR), polynomial kernel logistic regression (abbrev. PKLR), and radial basis function (RBF) based kernel logistic regression (abbrev. RBF KLR)) are investigated on the two feature sets. The experimental results of protein function prediction of Yeast Proteome Database (YPD) using the the protein-protein interaction data of Munich Information Center for Protein Sequences (MIPS) show that 2-order graphic neighbor information of proteins can significantly improve the average overall percentage of protein function prediction especially with RBF KLR. And, with a new 5-top chi-square feature combination method, RBF KLR can achieve 99.05% average overall percentage on 2-order neighbor feature combination set. version:1
arxiv-1207-4455 | First-improvement vs. Best-improvement Local Optima Networks of NK Landscapes | http://arxiv.org/abs/1207.4455 | id:1207.4455 author:Gabriela Ochoa, Sébastien Verel, Marco Tomassini category:cs.NE cs.AI  published:2012-07-18 summary:This paper extends a recently proposed model for combinatorial landscapes: Local Optima Networks (LON), to incorporate a first-improvement (greedy-ascent) hill-climbing algorithm, instead of a best-improvement (steepest-ascent) one, for the definition and extraction of the basins of attraction of the landscape optima. A statistical analysis comparing best and first improvement network models for a set of NK landscapes, is presented and discussed. Our results suggest structural differences between the two models with respect to both the network connectivity, and the nature of the basins of attraction. The impact of these differences in the behavior of search heuristics based on first and best improvement local search is thoroughly discussed. version:1
arxiv-1207-4452 | Pareto Local Optima of Multiobjective NK-Landscapes with Correlated Objectives | http://arxiv.org/abs/1207.4452 | id:1207.4452 author:Sébastien Verel, Arnaud Liefooghe, Laetitia Jourdan, Clarisse Dhaenens category:cs.NE cs.AI  published:2012-07-18 summary:In this paper, we conduct a fitness landscape analysis for multiobjective combinatorial optimization, based on the local optima of multiobjective NK-landscapes with objective correlation. In single-objective optimization, it has become clear that local optima have a strong impact on the performance of metaheuristics. Here, we propose an extension to the multiobjective case, based on the Pareto dominance. We study the co-influence of the problem dimension, the degree of non-linearity, the number of objectives and the correlation degree between objective functions on the number of Pareto local optima. version:1
arxiv-1207-4451 | Set-based Multiobjective Fitness Landscapes: A Preliminary Study | http://arxiv.org/abs/1207.4451 | id:1207.4451 author:Sébastien Verel, Arnaud Liefooghe, Clarisse Dhaenens category:cs.NE cs.AI  published:2012-07-18 summary:Fitness landscape analysis aims to understand the geometry of a given optimization problem in order to design more efficient search algorithms. However, there is a very little knowledge on the landscape of multiobjective problems. In this work, following a recent proposal by Zitzler et al. (2010), we consider multiobjective optimization as a set problem. Then, we give a general definition of set-based multiobjective fitness landscapes. An experimental set-based fitness landscape analysis is conducted on the multiobjective NK-landscapes with objective correlation. The aim is to adapt and to enhance the comprehensive design of set-based multiobjective search approaches, motivated by an a priori analysis of the corresponding set problem properties. version:1
arxiv-1207-4450 | NILS: a Neutrality-based Iterated Local Search and its application to Flowshop Scheduling | http://arxiv.org/abs/1207.4450 | id:1207.4450 author:Marie-Eleonore Marmion, Clarisse Dhaenens, Laetitia Jourdan, Arnaud Liefooghe, Sébastien Verel category:cs.NE cs.AI  published:2012-07-18 summary:This paper presents a new methodology that exploits specific characteristics from the fitness landscape. In particular, we are interested in the property of neutrality, that deals with the fact that the same fitness value is assigned to numerous solutions from the search space. Many combinatorial optimization problems share this property, that is generally very inhibiting for local search algorithms. A neutrality-based iterated local search, that allows neutral walks to move on the plateaus, is proposed and experimented on a permutation flowshop scheduling problem with the aim of minimizing the makespan. Our experiments show that the proposed approach is able to find improving solutions compared with a classical iterated local search. Moreover, the tradeoff between the exploitation of neutrality and the exploration of new parts of the search space is deeply analyzed. version:1
arxiv-1207-4448 | DAMS: Distributed Adaptive Metaheuristic Selection | http://arxiv.org/abs/1207.4448 | id:1207.4448 author:Bilel Derbel, Sébastien Verel category:cs.NE cs.AI  published:2012-07-18 summary:We present a distributed generic algorithm called DAMS dedicated to adaptive optimization in distributed environments. Given a set of metaheuristic, the goal of DAMS is to coordinate their local execution on distributed nodes in order to optimize the global performance of the distributed system. DAMS is based on three-layer architecture allowing node to decide distributively what local information to communicate, and what metaheuristic to apply while the optimization process is in progress. The adaptive features of DAMS are first addressed in a very general setting. A specific DAMS called SBM is then described and analyzed from both a parallel and an adaptive point of view. SBM is a simple, yet efficient, adaptive distributed algorithm using an exploitation component allowing nodes to select the metaheuristic with the best locally observed performance, and an exploration component allowing nodes to detect the metaheuristic with the actual best performance. The efficiency of BSM-DAMS is demonstrated through experimentations and comparisons with other adaptive strategies (sequential and distributed). version:1
arxiv-1207-4445 | Communities of Minima in Local Optima Networks of Combinatorial Spaces | http://arxiv.org/abs/1207.4445 | id:1207.4445 author:Fabio Daolio, Marco Tomassini, Sébastien Verel, Gabriela Ochoa category:cs.NE cs.AI  published:2012-07-18 summary:In this work we present a new methodology to study the structure of the configuration spaces of hard combinatorial problems. It consists in building the network that has as nodes the locally optimal configurations and as edges the weighted oriented transitions between their basins of attraction. We apply the approach to the detection of communities in the optima networks produced by two different classes of instances of a hard combinatorial optimization problem: the quadratic assignment problem (QAP). We provide evidence indicating that the two problem instance classes give rise to very different configuration spaces. For the so-called real-like class, the networks possess a clear modular structure, while the optima networks belonging to the class of random uniform instances are less well partitionable into clusters. This is convincingly supported by using several statistical tests. Finally, we shortly discuss the consequences of the findings for heuristically searching the corresponding problem spaces. version:1
arxiv-1207-4442 | Complex-network analysis of combinatorial spaces: The NK landscape case | http://arxiv.org/abs/1207.4442 | id:1207.4442 author:Marco Tomassini, Sébastien Verel, Gabriela Ochoa category:cond-mat.stat-mech cs.NE nlin.AO  published:2012-07-18 summary:We propose a network characterization of combinatorial fitness landscapes by adapting the notion of inherent networks proposed for energy surfaces. We use the well-known family of NK landscapes as an example. In our case the inherent network is the graph whose vertices represent the local maxima in the landscape, and the edges account for the transition probabilities between their corresponding basins of attraction. We exhaustively extracted such networks on representative NK landscape instances, and performed a statistical characterization of their properties. We found that most of these network properties are related to the search difficulty on the underlying NK landscapes with varying values of K. version:1
arxiv-1206-4667 | Unachievable Region in Precision-Recall Space and Its Effect on Empirical Evaluation | http://arxiv.org/abs/1206.4667 | id:1206.4667 author:Kendrick Boyd, Vitor Santos Costa, Jesse Davis, David Page category:cs.LG cs.AI cs.IR  published:2012-06-18 summary:Precision-recall (PR) curves and the areas under them are widely used to summarize machine learning results, especially for data sets exhibiting class skew. They are often used analogously to ROC curves and the area under ROC curves. It is known that PR curves vary as class skew changes. What was not recognized before this paper is that there is a region of PR space that is completely unachievable, and the size of this region depends only on the skew. This paper precisely characterizes the size of that region and discusses its implications for empirical evaluation methodology in machine learning. version:2
arxiv-1207-4421 | Stochastic optimization and sparse statistical recovery: An optimal algorithm for high dimensions | http://arxiv.org/abs/1207.4421 | id:1207.4421 author:Alekh Agarwal, Sahand Negahban, Martin J. Wainwright category:stat.ML cs.LG math.OC  published:2012-07-18 summary:We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures, yielding an $\order(\pdim/T)$ convergence rate for strongly convex objectives in $\pdim$ dimensions, and an $\order(\sqrt{(\spindex \log \pdim)/T})$ convergence rate when the optimum is $\spindex$-sparse. Our algorithm is based on successively solving a series of $\ell_1$-regularized optimization problems using Nesterov's dual averaging algorithm. We establish that the error of our solution after $T$ iterations is at most $\order((\spindex \log\pdim)/T)$, with natural extensions to approximate sparsity. Our results apply to locally Lipschitz losses including the logistic, exponential, hinge and least-squares losses. By recourse to statistical minimax results, we show that our convergence rates are optimal up to multiplicative constant factors. The effectiveness of our approach is also confirmed in numerical simulations, in which we compare to several baselines on a least-squares regression problem. version:1
arxiv-1207-4404 | Better Mixing via Deep Representations | http://arxiv.org/abs/1207.4404 | id:1207.4404 author:Yoshua Bengio, Grégoire Mesnil, Yann Dauphin, Salah Rifai category:cs.LG  published:2012-07-18 summary:It has previously been hypothesized, and supported with some experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce faster-mixing Markov chains. Consequently, mixing would be more efficient at higher levels of representation. To better understand why and how this is happening, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels. The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing and interpolating between samples. version:1
arxiv-1205-4324 | Universal Properties of Mythological Networks | http://arxiv.org/abs/1205.4324 | id:1205.4324 author:Pádraig Mac Carron, Ralph Kenna category:physics.soc-ph cs.CL cs.SI  published:2012-05-19 summary:As in statistical physics, the concept of universality plays an important, albeit qualitative, role in the field of comparative mythology. Here we apply statistical mechanical tools to analyse the networks underlying three iconic mythological narratives with a view to identifying common and distinguishing quantitative features. Of the three narratives, an Anglo-Saxon and a Greek text are mostly believed by antiquarians to be partly historically based while the third, an Irish epic, is often considered to be fictional. Here we show that network analysis is able to discriminate real from imaginary social networks and place mythological narratives on the spectrum between them. Moreover, the perceived artificiality of the Irish narrative can be traced back to anomalous features associated with six characters. Considering these as amalgams of several entities or proxies, renders the plausibility of the Irish text comparable to the others from a network-theoretic point of view. version:2
arxiv-1207-4318 | Empirical review of standard benchmark functions using evolutionary global optimization | http://arxiv.org/abs/1207.4318 | id:1207.4318 author:Johannes M. Dieterich, Bernd Hartke category:cs.NE  published:2012-07-18 summary:We have employed a recent implementation of genetic algorithms to study a range of standard benchmark functions for global optimization. It turns out that some of them are not very useful as challenging test functions, since they neither allow for a discrimination between different variants of genetic operators nor exhibit a dimensionality scaling resembling that of real-world problems, for example that of global structure optimization of atomic and molecular clusters. The latter properties seem to be simulated better by two other types of benchmark functions. One type is designed to be deceptive, exemplified here by Lunacek's function. The other type offers additional advantages of markedly increased complexity and of broad tunability in search space characteristics. For the latter type, we use an implementation based on randomly distributed Gaussians. We advocate the use of the latter types of test functions for algorithm development and benchmarking. version:1
arxiv-1207-4308 | Assessment of SAR Image Filtering using Adaptive Stack Filters | http://arxiv.org/abs/1207.4308 | id:1207.4308 author:Maria E. Buemi, Marta Mejail, Julio Jacobo, Alejandro C. Frery, Heitor S. Ramos category:cs.CV  published:2012-07-18 summary:Stack filters are a special case of non-linear filters. They have a good performance for filtering images with different types of noise while preserving edges and details. A stack filter decomposes an input image into several binary images according to a set of thresholds. Each binary image is then filtered by a Boolean function, which characterizes the filter. Adaptive stack filters can be designed to be optimal; they are computed from a pair of images consisting of an ideal noiseless image and its noisy version. In this work we study the performance of adaptive stack filters when they are applied to Synthetic Aperture Radar (SAR) images. This is done by evaluating the quality of the filtered images through the use of suitable image quality indexes and by measuring the classification accuracy of the resulting images. version:1
arxiv-1207-4307 | Frame Interpretation and Validation in a Open Domain Dialogue System | http://arxiv.org/abs/1207.4307 | id:1207.4307 author:Artur Ventura, Nuno Diegues, David Martins de Matos category:cs.CL cs.RO I.2.7; I.2.9  published:2012-07-18 summary:Our goal in this paper is to establish a means for a dialogue platform to be able to cope with open domains considering the possible interaction between the embodied agent and humans. To this end we present an algorithm capable of processing natural language utterances and validate them against knowledge structures of an intelligent agent's mind. Our algorithm leverages dialogue techniques in order to solve ambiguities and acquire knowledge about unknown entities. version:1
arxiv-1107-5959 | Expectation-Propagation for Likelihood-Free Inference | http://arxiv.org/abs/1107.5959 | id:1107.5959 author:Simon Barthelmé, Nicolas Chopin category:stat.CO stat.ML  published:2011-07-29 summary:Many models of interest in the natural and social sciences have no closed-form likelihood function, which means that they cannot be treated using the usual techniques of statistical inference. In the case where such models can be efficiently simulated, Bayesian inference is still possible thanks to the Approximate Bayesian Computation (ABC) algorithm. Although many refinements have been suggested, ABC inference is still far from routine. ABC is often excruciatingly slow due to very low acceptance rates. In addition, ABC requires introducing a vector of "summary statistics", the choice of which is relatively arbitrary, and often require some trial and error, making the whole process quite laborious for the user. We introduce in this work the EP-ABC algorithm, which is an adaptation to the likelihood-free context of the variational approximation algorithm known as Expectation Propagation (Minka, 2001). The main advantage of EP-ABC is that it is faster by a few orders of magnitude than standard algorithms, while producing an overall approximation error which is typically negligible. A second advantage of EP-ABC is that it replaces the usual global ABC constraint on the vector of summary statistics computed on the whole dataset, by n local constraints of the form that apply separately to each data-point. As a consequence, it is often possible to do away with summary statistics entirely. In that case, EP-ABC approximates directly the evidence (marginal likelihood) of the model. Comparisons are performed in three real-world applications which are typical of likelihood-free inference, including one application in neuroscience which is novel, and possibly too challenging for standard ABC techniques. version:2
arxiv-1207-3389 | Incremental Learning of 3D-DCT Compact Representations for Robust Visual Tracking | http://arxiv.org/abs/1207.3389 | id:1207.3389 author:Xi Li, Anthony Dick, Chunhua Shen, Anton van den Hengel, Hanzi Wang category:cs.CV cs.LG  published:2012-07-14 summary:Visual tracking usually requires an object appearance model that is robust to changing illumination, pose and other factors encountered in video. In this paper, we construct an appearance model using the 3D discrete cosine transform (3D-DCT). The 3D-DCT is based on a set of cosine basis functions, which are determined by the dimensions of the 3D signal and thus independent of the input video data. In addition, the 3D-DCT can generate a compact energy spectrum whose high-frequency coefficients are sparse if the appearance samples are similar. By discarding these high-frequency coefficients, we simultaneously obtain a compact 3D-DCT based object representation and a signal reconstruction-based similarity measure (reflecting the information loss from signal reconstruction). To efficiently update the object representation, we propose an incremental 3D-DCT algorithm, which decomposes the 3D-DCT into successive operations of the 2D discrete cosine transform (2D-DCT) and 1D discrete cosine transform (1D-DCT) on the input video data. version:2
arxiv-1207-4259 | Content Based Multimedia Information Retrieval to Support Digital Libraries | http://arxiv.org/abs/1207.4259 | id:1207.4259 author:Mohammad Nabil Almunawar category:cs.IR cs.CV  published:2012-07-18 summary:Content-based multimedia information retrieval is an interesting research area since it allows retrieval based on inherent characteristic of multimedia objects. For example retrieval based on visual characteristics such as colour, shapes or textures of objects in images or retrieval based on spatial relationships among objects in the media (images or video clips). This paper reviews some work done in image and video retrieval and then proposes an integrated model that can handle images and video clips uniformly. Using this model retrieval on images or video clips can be done based on the same framework. version:1
arxiv-1207-4089 | A Two-Stage Combined Classifier in Scale Space Texture Classification | http://arxiv.org/abs/1207.4089 | id:1207.4089 author:Mehrdad J. Gangeh, Robert P. W. Duin, Bart M. ter Haar Romeny, Mohamed S. Kamel category:cs.CV cs.LG  published:2012-07-17 summary:Textures often show multiscale properties and hence multiscale techniques are considered useful for texture analysis. Scale-space theory as a biologically motivated approach may be used to construct multiscale textures. In this paper various ways are studied to combine features on different scales for texture classification of small image patches. We use the N-jet of derivatives up to the second order at different scales to generate distinct pattern representations (DPR) of feature subsets. Each feature subset in the DPR is given to a base classifier (BC) of a two-stage combined classifier. The decisions made by these BCs are combined in two stages over scales and derivatives. Various combining systems and their significances and differences are discussed. The learning curves are used to evaluate the performances. We found for small sample sizes combining classifiers performs significantly better than combining feature spaces (CFS). It is also shown that combining classifiers performs better than the support vector machine on CFS in multiscale texture classification. version:1
arxiv-1202-6103 | Nonlinear Laplacian spectral analysis: Capturing intermittent and low-frequency spatiotemporal patterns in high-dimensional data | http://arxiv.org/abs/1202.6103 | id:1202.6103 author:Dimitrios Giannakis, Andrew J. Majda category:physics.data-an cs.LG  published:2012-02-28 summary:We present a technique for spatiotemporal data analysis called nonlinear Laplacian spectral analysis (NLSA), which generalizes singular spectrum analysis (SSA) to take into account the nonlinear manifold structure of complex data sets. The key principle underlying NLSA is that the functions used to represent temporal patterns should exhibit a degree of smoothness on the nonlinear data manifold M; a constraint absent from classical SSA. NLSA enforces such a notion of smoothness by requiring that temporal patterns belong in low-dimensional Hilbert spaces V_l spanned by the leading l Laplace-Beltrami eigenfunctions on M. These eigenfunctions can be evaluated efficiently in high ambient-space dimensions using sparse graph-theoretic algorithms. Moreover, they provide orthonormal bases to expand a family of linear maps, whose singular value decomposition leads to sets of spatiotemporal patterns at progressively finer resolution on the data manifold. The Riemannian measure of M and an adaptive graph kernel width enhances the capability of NLSA to detect important nonlinear processes, including intermittency and rare events. The minimum dimension of V_l required to capture these features while avoiding overfitting is estimated here using spectral entropy criteria. version:2
arxiv-1207-3962 | Computation of the Hausdorff distance between sets of line segments in parallel | http://arxiv.org/abs/1207.3962 | id:1207.3962 author:Helmut Alt, Ludmila Scharf category:cs.CG cs.CV cs.DC  published:2012-07-17 summary:We show that the Hausdorff distance for two sets of non-intersecting line segments can be computed in parallel in $O(\log^2 n)$ time using O(n) processors in a CREW-PRAM computation model. We discuss how some parts of the sequential algorithm can be performed in parallel using previously known parallel algorithms; and identify the so-far unsolved part of the problem for the parallel computation, which is the following: Given two sets of $x$-monotone curve segments, red and blue, for each red segment find its extremal intersection points with the blue set, i.e. points with the minimal and maximal $x$-coordinate. Each segment set is assumed to be intersection free. For this intersection problem we describe a parallel algorithm which completes the Hausdorff distance computation within the stated time and processor bounds. version:1
arxiv-1207-3944 | Polarimetric SAR Image Segmentation with B-Splines and a New Statistical Model | http://arxiv.org/abs/1207.3944 | id:1207.3944 author:Alejandro C. Frery, Julio Jacobo-Berlles, Juliana Gambini, Marta Mejail category:cs.CV stat.ML  published:2012-07-17 summary:We present an approach for polarimetric Synthetic Aperture Radar (SAR) image region boundary detection based on the use of B-Spline active contours and a new model for polarimetric SAR data: the GHP distribution. In order to detect the boundary of a region, initial B-Spline curves are specified, either automatically or manually, and the proposed algorithm uses a deformable contours technique to find the boundary. In doing this, the parameters of the polarimetric GHP model for the data are estimated, in order to find the transition points between the region being segmented and the surrounding area. This is a local algorithm since it works only on the region to be segmented. Results of its performance are presented. version:1
arxiv-1205-1928 | The representer theorem for Hilbert spaces: a necessary and sufficient condition | http://arxiv.org/abs/1205.1928 | id:1205.1928 author:Francesco Dinuzzo, Bernhard Schölkopf category:math.FA cs.LG  published:2012-05-09 summary:A family of regularization functionals is said to admit a linear representer theorem if every member of the family admits minimizers that lie in a fixed finite dimensional subspace. A recent characterization states that a general class of regularization functionals with differentiable regularizer admits a linear representer theorem if and only if the regularization term is a non-decreasing function of the norm. In this report, we improve over such result by replacing the differentiability assumption with lower semi-continuity and deriving a proof that is independent of the dimensionality of the space. version:3
arxiv-1207-3932 | Automatic Segmentation of Manipuri (Meiteilon) Word into Syllabic Units | http://arxiv.org/abs/1207.3932 | id:1207.3932 author:Kishorjit Nongmeikapam, Vidya Raj RK, Oinam Imocha Singh, Sivaji Bandyopadhyay category:cs.CL I.2.7  published:2012-07-17 summary:The work of automatic segmentation of a Manipuri language (or Meiteilon) word into syllabic units is demonstrated in this paper. This language is a scheduled Indian language of Tibeto-Burman origin, which is also a very highly agglutinative language. This language usages two script: a Bengali script and Meitei Mayek (Script). The present work is based on the second script. An algorithm is designed so as to identify mainly the syllables of Manipuri origin word. The result of the algorithm shows a Recall of 74.77, Precision of 91.21 and F-Score of 82.18 which is a reasonable score with the first attempt of such kind for this language. version:1
arxiv-1205-5819 | Measurability Aspects of the Compactness Theorem for Sample Compression Schemes | http://arxiv.org/abs/1205.5819 | id:1205.5819 author:Damjan Kalajdzievski category:stat.ML cs.LG  published:2012-05-25 summary:It was proved in 1998 by Ben-David and Litman that a concept space has a sample compression scheme of size d if and only if every finite subspace has a sample compression scheme of size d. In the compactness theorem, measurability of the hypotheses of the created sample compression scheme is not guaranteed; at the same time measurability of the hypotheses is a necessary condition for learnability. In this thesis we discuss when a sample compression scheme, created from com- pression schemes on finite subspaces via the compactness theorem, have measurable hypotheses. We show that if X is a standard Borel space with a d-maximum and universally separable concept class C, then (X,C) has a sample compression scheme of size d with universally Borel measurable hypotheses. Additionally we introduce a new variant of compression scheme called a copy sample compression scheme. version:2
arxiv-1207-3809 | Image Labeling on a Network: Using Social-Network Metadata for Image Classification | http://arxiv.org/abs/1207.3809 | id:1207.3809 author:Julian McAuley, Jure Leskovec category:cs.CV cs.SI physics.soc-ph  published:2012-07-16 summary:Large-scale image retrieval benchmarks invariably consist of images from the Web. Many of these benchmarks are derived from online photo sharing networks, like Flickr, which in addition to hosting images also provide a highly interactive social community. Such communities generate rich metadata that can naturally be harnessed for image classification and retrieval. Here we study four popular benchmark datasets, extending them with social-network metadata, such as the groups to which each image belongs, the comment thread associated with the image, who uploaded it, their location, and their network of friends. Since these types of data are inherently relational, we propose a model that explicitly accounts for the interdependencies between images sharing common properties. We model the task as a binary labeling problem on a network, and use structured learning techniques to learn model parameters. We find that social-network metadata are useful in a variety of classification tasks, in many cases outperforming methods based on image content. version:1
arxiv-1207-3760 | Towards a Self-Organized Agent-Based Simulation Model for Exploration of Human Synaptic Connections | http://arxiv.org/abs/1207.3760 | id:1207.3760 author:Önder Gürcan, Carole Bernon, Kemal S. Türker category:cs.NE cs.AI cs.LG nlin.AO  published:2012-07-16 summary:In this paper, the early design of our self-organized agent-based simulation model for exploration of synaptic connections that faithfully generates what is observed in natural situation is given. While we take inspiration from neuroscience, our intent is not to create a veridical model of processes in neurodevelopmental biology, nor to represent a real biological system. Instead, our goal is to design a simulation model that learns acting in the same way of human nervous system by using findings on human subjects using reflex methodologies in order to estimate unknown connections. version:1
arxiv-1204-4758 | Morphological Filtering in Shape Spaces: Applications using Tree-Based Image Representations | http://arxiv.org/abs/1204.4758 | id:1204.4758 author:Yongchao Xu, Thierry Géraud, Laurent Najman category:cs.CV math.OA  published:2012-04-20 summary:Connected operators are filtering tools that act by merging elementary regions of an image. A popular strategy is based on tree-based image representations: for example, one can compute an attribute on each node of the tree and keep only the nodes for which the attribute is sufficiently strong. This operation can be seen as a thresholding of the tree, seen as a graph whose nodes are weighted by the attribute. Rather than being satisfied with a mere thresholding, we propose to expand on this idea, and to apply connected filters on this latest graph. Consequently, the filtering is done not in the space of the image, but on the space of shapes build from the image. Such a processing is a generalization of the existing tree-based connected operators. Indeed, the framework includes classical existing connected operators by attributes. It also allows us to propose a class of novel connected operators from the leveling family, based on shape attributes. Finally, we also propose a novel class of self-dual connected operators that we call morphological shapings. version:2
arxiv-1201-2719 | Ultrametric Model of Mind, II: Application to Text Content Analysis | http://arxiv.org/abs/1201.2719 | id:1201.2719 author:Fionn Murtagh category:cs.AI cs.CL 68T01 I.2.0; I.2.3; J.4  published:2012-01-13 summary:In a companion paper, Murtagh (2012), we discussed how Matte Blanco's work linked the unrepressed unconscious (in the human) to symmetric logic and thought processes. We showed how ultrametric topology provides a most useful representational and computational framework for this. Now we look at the extent to which we can find ultrametricity in text. We use coherent and meaningful collections of nearly 1000 texts to show how we can measure inherent ultrametricity. On the basis of our findings we hypothesize that inherent ultrametricty is a basis for further exploring unconscious thought processes. version:3
arxiv-1207-3649 | Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood | http://arxiv.org/abs/1207.3649 | id:1207.3649 author:Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari category:stat.ML  published:2012-07-16 summary:We consider probabilistic multinomial probit classification using Gaussian process (GP) priors. The challenges with the multiclass GP classification are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classification rely on numerical quadratures or independence assumptions between the latent values from different classes to facilitate the computations. In this paper, we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all between-class posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method with respect to MCMC sampling, but the differences between the compared methods were small if only the classification accuracy is concerned. version:1
arxiv-1207-3607 | Fusing image representations for classification using support vector machines | http://arxiv.org/abs/1207.3607 | id:1207.3607 author:Can Demirkesen, Hocine Cherifi category:cs.CV cs.LG  published:2012-07-16 summary:In order to improve classification accuracy different image representations are usually combined. This can be done by using two different fusing schemes. In feature level fusion schemes, image representations are combined before the classification process. In classifier fusion, the decisions taken separately based on individual representations are fused to make a decision. In this paper the main methods derived for both strategies are evaluated. Our experimental results show that classifier fusion performs better. Specifically Bayes belief integration is the best performing strategy for image classification task. version:1
arxiv-1207-3790 | Accuracy Measures for the Comparison of Classifiers | http://arxiv.org/abs/1207.3790 | id:1207.3790 author:Vincent Labatut, Hocine Cherifi category:cs.LG  published:2012-07-16 summary:The selection of the best classification algorithm for a given dataset is a very widespread problem. It is also a complex one, in the sense it requires to make several important methodological choices. Among them, in this work we focus on the measure used to assess the classification performance and rank the algorithms. We present the most popular measures and discuss their properties. Despite the numerous measures proposed over the years, many of them turn out to be equivalent in this specific case, to have interpretation problems, or to be unsuitable for our purpose. Consequently, classic overall success rate or marginal rates should be preferred for this specific task. version:1
arxiv-1207-3603 | Qualitative Comparison of Community Detection Algorithms | http://arxiv.org/abs/1207.3603 | id:1207.3603 author:Günce Orman, Vincent Labatut, Hocine Cherifi category:cs.SI cs.CV physics.soc-ph  published:2012-07-16 summary:Community detection is a very active field in complex networks analysis, consisting in identifying groups of nodes more densely interconnected relatively to the rest of the network. The existing algorithms are usually tested and compared on real-world and artificial networks, their performance being assessed through some partition similarity measure. However, artificial networks realism can be questioned, and the appropriateness of those measures is not obvious. In this study, we take advantage of recent advances concerning the characterization of community structures to tackle these questions. We first generate networks thanks to the most realistic model available to date. Their analysis reveals they display only some of the properties observed in real-world community structures. We then apply five community detection algorithms on these networks and find out the performance assessed quantitatively does not necessarily agree with a qualitative analysis of the identified communities. It therefore seems both approaches should be applied to perform a relevant comparison of the algorithms. version:1
arxiv-1207-7245 | Autofocus Correction of Azimuth Phase Error and Residual Range Cell Migration in Spotlight SAR Polar Format Imagery | http://arxiv.org/abs/1207.7245 | id:1207.7245 author:Xinhua Mao, Daiyin Zhu, Zhaoda Zhu category:astro-ph.IM cs.CV  published:2012-07-16 summary:Synthetic aperture radar (SAR) images are often blurred by phase perturbations induced by uncompensated sensor motion and /or unknown propagation effects caused by turbulent media. To get refocused images, autofocus proves to be useful post-processing technique applied to estimate and compensate the unknown phase errors. However, a severe drawback of the conventional autofocus algorithms is that they are only capable of removing one-dimensional azimuth phase errors (APE). As the resolution becomes finer, residual range cell migration (RCM), which makes the defocus inherently two-dimensional, becomes a new challenge. In this paper, correction of APE and residual RCM are presented in the framework of polar format algorithm (PFA). First, an insight into the underlying mathematical mechanism of polar reformatting is presented. Then based on this new formulation, the effect of polar reformatting on the uncompensated APE and residual RCM is investigated in detail. By using the derived analytical relationship between APE and residual RCM, an efficient two-dimensional (2-D) autofocus method is proposed. Experimental results indicate the effectiveness of the proposed method. version:1
arxiv-1207-3560 | Diagnosing client faults using SVM-based intelligent inference from TCP packet traces | http://arxiv.org/abs/1207.3560 | id:1207.3560 author:Chathuranga Widanapathirana, Y. Ahmet Sekercioglu, Paul G. Fitzpatrick, Milosh V. Ivanovich, Jonathan C. Li category:cs.NI cs.AI cs.LG  published:2012-07-16 summary:We present the Intelligent Automated Client Diagnostic (IACD) system, which only relies on inference from Transmission Control Protocol (TCP) packet traces for rapid diagnosis of client device problems that cause network performance issues. Using soft-margin Support Vector Machine (SVM) classifiers, the system (i) distinguishes link problems from client problems, and (ii) identifies characteristics unique to client faults to report the root cause of the client device problem. Experimental evaluation demonstrated the capability of the IACD system to distinguish between faulty and healthy links and to diagnose the client faults with 98% accuracy in healthy links. The system can perform fault diagnosis independent of the client's specific TCP implementation, enabling diagnosis capability on diverse range of client computers. version:1
arxiv-1207-3520 | Improved brain pattern recovery through ranking approaches | http://arxiv.org/abs/1207.3520 | id:1207.3520 author:Fabian Pedregosa, Alexandre Gramfort, Gaël Varoquaux, Bertrand Thirion, Christophe Pallier, Elodie Cauvet category:cs.LG stat.ML  published:2012-07-15 summary:Inferring the functional specificity of brain regions from functional Magnetic Resonance Images (fMRI) data is a challenging statistical problem. While the General Linear Model (GLM) remains the standard approach for brain mapping, supervised learning techniques (a.k.a.} decoding) have proven to be useful to capture multivariate statistical effects distributed across voxels and brain regions. Up to now, much effort has been made to improve decoding by incorporating prior knowledge in the form of a particular regularization term. In this paper we demonstrate that further improvement can be made by accounting for non-linearities using a ranking approach rather than the commonly used least-square regression. Through simulation, we compare the recovery properties of our approach to linear models commonly used in fMRI based decoding. We demonstrate the superiority of ranking with a real fMRI dataset. version:1
arxiv-1207-0245 | Adversarial Evaluation for Models of Natural Language | http://arxiv.org/abs/1207.0245 | id:1207.0245 author:Noah A. Smith category:cs.CL  published:2012-07-01 summary:We now have a rich and growing set of modeling tools and algorithms for inducing linguistic structure from text that is less than fully annotated. In this paper, we discuss some of the weaknesses of our current methodology. We present a new abstract framework for evaluating natural language processing (NLP) models in general and unsupervised NLP models in particular. The central idea is to make explicit certain adversarial roles among researchers, so that the different roles in an evaluation are more clearly defined and performers of all roles are offered ways to make measurable contributions to the larger goal. Adopting this approach may help to characterize model successes and failures by encouraging earlier consideration of error analysis. The framework can be instantiated in a variety of ways, simulating some familiar intrinsic and extrinsic evaluations as well as some new evaluations. version:2
arxiv-1207-2334 | Distinct word length frequencies: distributions and symbol entropies | http://arxiv.org/abs/1207.2334 | id:1207.2334 author:Reginald D. Smith category:cs.CL physics.data-an  published:2012-07-10 summary:The distribution of frequency counts of distinct words by length in a language's vocabulary will be analyzed using two methods. The first, will look at the empirical distributions of several languages and derive a distribution that reasonably explains the number of distinct words as a function of length. We will be able to derive the frequency count, mean word length, and variance of word length based on the marginal probability of letters and spaces. The second, based on information theory, will demonstrate that the conditional entropies can also be used to estimate the frequency of distinct words of a given length in a language. In addition, it will be shown how these techniques can also be applied to estimate higher order entropies using vocabulary word length. version:2
arxiv-1206-5651 | Optimization of Real, Hermitian Quadratic Forms: Real, Complex Hopfield-Amari Neural Network | http://arxiv.org/abs/1206.5651 | id:1206.5651 author:Garimella Ramamurthy, Bondalapati Nischal category:cs.NE  published:2012-06-25 summary:In this research paper, the problem of optimization of quadratic forms associated with the dynamics of Hopfield-Amari neural network is considered. An elegant (and short) proof of the states at which local/global minima of quadratic form are attained is provided. A theorem associated with local/global minimization of quadratic energy function using the Hopfield-Amari neural network is discussed. The results are generalized to a "Complex Hopfield neural network" dynamics over the complex hypercube (using a "complex signum function"). It is also reasoned through two theorems that there is no loss of generality in assuming the threshold vector to be a zero vector in the case of real as well as a "Complex Hopfield neural network". Some structured quadratic forms like Toeplitz form and Complex Toeplitz form are discussed. version:2
arxiv-1207-3442 | Approximated Computation of Belief Functions for Robust Design Optimization | http://arxiv.org/abs/1207.3442 | id:1207.3442 author:Massimiliano Vasile, Edmondo Minisci, Quirien Wijnands category:cs.CE cs.NE cs.SY math.OC math.PR  published:2012-07-14 summary:This paper presents some ideas to reduce the computational cost of evidence-based robust design optimization. Evidence Theory crystallizes both the aleatory and epistemic uncertainties in the design parameters, providing two quantitative measures, Belief and Plausibility, of the credibility of the computed value of the design budgets. The paper proposes some techniques to compute an approximation of Belief and Plausibility at a cost that is a fraction of the one required for an accurate calculation of the two values. Some simple test cases will show how the proposed techniques scale with the dimension of the problem. Finally a simple example of spacecraft system design is presented. version:1
arxiv-1207-3438 | MahNMF: Manhattan Non-negative Matrix Factorization | http://arxiv.org/abs/1207.3438 | id:1207.3438 author:Naiyang Guan, Dacheng Tao, Zhigang Luo, John Shawe-Taylor category:stat.ML cs.LG cs.NA 65K10  published:2012-07-14 summary:Non-negative matrix factorization (NMF) approximates a non-negative matrix $X$ by a product of two non-negative low-rank factor matrices $W$ and $H$. NMF and its extensions minimize either the Kullback-Leibler divergence or the Euclidean distance between $X$ and $W^T H$ to model the Poisson noise or the Gaussian noise. In practice, when the noise distribution is heavy tailed, they cannot perform well. This paper presents Manhattan NMF (MahNMF) which minimizes the Manhattan distance between $X$ and $W^T H$ for modeling the heavy tailed Laplacian noise. Similar to sparse and low-rank matrix decompositions, MahNMF robustly estimates the low-rank part and the sparse part of a non-negative matrix and thus performs effectively when data are contaminated by outliers. We extend MahNMF for various practical applications by developing box-constrained MahNMF, manifold regularized MahNMF, group sparse MahNMF, elastic net inducing MahNMF, and symmetric MahNMF. The major contribution of this paper lies in two fast optimization algorithms for MahNMF and its extensions: the rank-one residual iteration (RRI) method and Nesterov's smoothing method. In particular, by approximating the residual matrix by the outer product of one row of W and one row of $H$ in MahNMF, we develop an RRI method to iteratively update each variable of $W$ and $H$ in a closed form solution. Although RRI is efficient for small scale MahNMF and some of its extensions, it is neither scalable to large scale matrices nor flexible enough to optimize all MahNMF extensions. Since the objective functions of MahNMF and its extensions are neither convex nor smooth, we apply Nesterov's smoothing method to recursively optimize one factor matrix with another matrix fixed. By setting the smoothing parameter inversely proportional to the iteration number, we improve the approximation accuracy iteratively for both MahNMF and its extensions. version:1
arxiv-1207-3437 | Robust Mission Design Through Evidence Theory and Multi-Agent Collaborative Search | http://arxiv.org/abs/1207.3437 | id:1207.3437 author:Massimiliano Vasile category:cs.CE cs.NE cs.SY math.OC math.PR  published:2012-07-14 summary:In this paper, the preliminary design of a space mission is approached introducing uncertainties on the design parameters and formulating the resulting reliable design problem as a multiobjective optimization problem. Uncertainties are modelled through evidence theory and the belief, or credibility, in the successful achievement of mission goals is maximised along with the reliability of constraint satisfaction. The multiobjective optimisation problem is solved through a novel algorithm based on the collaboration of a population of agents in search for the set of highly reliable solutions. Two typical problems in mission analysis are used to illustrate the proposed methodology. version:1
arxiv-1201-3973 | Split HMC for Gaussian Process Models | http://arxiv.org/abs/1201.3973 | id:1201.3973 author:Shiwei Lan, Babak Shahbaba category:stat.CO stat.ML  published:2012-01-19 summary:In this paper, we discuss an extension of the Split Hamiltonian Monte Carlo (Split HMC) method for Gaussian process model (GPM). This method is based on splitting the Hamiltonian in a way that allows much of the movement around the state space to be done at low computational cost. To this end, we approximate the negative log density (i.e., the energy function) of the distribution of interest by a quadratic function U0 for which Hamiltonian dynamics can be solved analytically. The overall energy function U is then written as U0 + U1, where U1 is the approximation error. The Hamiltonian is then split into two parts; one part is based on U0 is handled analytically, the other part is based on U1 for which we approximate Hamiltonian's equations by discretizing time. We use simulated and real data to compare the performance of our method to the standard HMC. We find that splitting the Hamiltonian for GP models could lead to substantial improvement (up to 10 folds) of sampling efficiency, which is measured in terms of the amount of time required for producing an independent sample with high acceptance probability from posterior distributions. version:2
arxiv-1207-3394 | Dimension Reduction by Mutual Information Feature Extraction | http://arxiv.org/abs/1207.3394 | id:1207.3394 author:Ali Shadvar category:cs.LG cs.CV  published:2012-07-14 summary:During the past decades, to study high-dimensional data in a large variety of problems, researchers have proposed many Feature Extraction algorithms. One of the most effective approaches for optimal feature extraction is based on mutual information (MI). However it is not always easy to get an accurate estimation for high dimensional MI. In terms of MI, the optimal feature extraction is creating a feature set from the data which jointly have the largest dependency on the target class and minimum redundancy. In this paper, a component-by-component gradient ascent method is proposed for feature extraction which is based on one-dimensional MI estimates. We will refer to this algorithm as Mutual Information Feature Extraction (MIFX). The performance of this proposed method is evaluated using UCI databases. The results indicate that MIFX provides a robust performance over different data sets which are almost always the best or comparable to the best ones. version:1
arxiv-1207-3370 | Deconvolution of vibroacoustic images using a simulation model based on a three dimensional point spread function | http://arxiv.org/abs/1207.3370 | id:1207.3370 author:Talita Perciano, Matthew Urban, Nelson D. A. Mascarenhas, Mostafa Fatemi, Alejandro C. Frery, Glauber T. Silva category:cs.CV  published:2012-07-13 summary:Vibro-acoustography (VA) is a medical imaging method based on the difference-frequency generation produced by the mixture of two focused ultrasound beams. VA has been applied to different problems in medical imaging such as imaging bones, microcalcifications in the breast, mass lesions, and calcified arteries. The obtained images may have a resolution of 0.7--0.8 mm. Current VA systems based on confocal or linear array transducers generate C-scan images at the beam focal plane. Images on the axial plane are also possible, however the system resolution along depth worsens when compared to the lateral one. Typical axial resolution is about 1.0 cm. Furthermore, the elevation resolution of linear array systems is larger than that in lateral direction. This asymmetry degrades C-scan images obtained using linear arrays. The purpose of this article is to study VA image restoration based on a 3D point spread function (PSF) using classical deconvolution algorithms: Wiener, constrained least-squares (CLSs), and geometric mean filters. To assess the filters' performance, we use an image quality index that accounts for correlation loss, luminance and contrast distortion. Results for simulated VA images show that the quality index achieved with the Wiener filter is 0.9 (1 indicates perfect restoration). This filter yielded the best result in comparison with the other ones. Moreover, the deconvolution algorithms were applied to an experimental VA image of a phantom composed of three stretched 0.5 mm wires. Experiments were performed using transducer driven at two frequencies, 3075 kHz and 3125 kHz, which resulted in the difference-frequency of 50 kHz. Restorations with the theoretical line spread function (LSF) did not recover sufficient information to identify the wires in the images. However, using an estimated LSF the obtained results displayed enough information to spot the wires in the images. version:1
arxiv-1202-2160 | Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers | http://arxiv.org/abs/1202.2160 | id:1202.2160 author:Clément Farabet, Camille Couprie, Laurent Najman, Yann LeCun category:cs.CV cs.LG  published:2012-02-10 summary:Scene parsing, or semantic segmentation, consists in labeling each pixel in an image with the category of the object it belongs to. It is a challenging task that involves the simultaneous detection, segmentation and recognition of all the objects in the image. The scene parsing method proposed here starts by computing a tree of segments from a graph of pixel dissimilarities. Simultaneously, a set of dense feature vectors is computed which encodes regions of multiple sizes centered on each pixel. The feature extractor is a multiscale convolutional network trained from raw pixels. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average "purity" of the class distributions, hence maximizing the overall likelihood that each segment will contain a single object. The convolutional network feature extractor is trained end-to-end from raw pixels, alleviating the need for engineered features. After training, the system is parameter free. The system yields record accuracies on the Stanford Background Dataset (8 classes), the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) while being an order of magnitude faster than competing approaches, producing a 320 \times 240 image labeling in less than 1 second. version:2
arxiv-1207-3368 | Learning the Pseudoinverse Solution to Network Weights | http://arxiv.org/abs/1207.3368 | id:1207.3368 author:Jonathan Tapson, Andre van Schaik category:cs.NE  published:2012-07-13 summary:The last decade has seen the parallel emergence in computational neuroscience and machine learning of neural network structures which spread the input signal randomly to a higher dimensional space; perform a nonlinear activation; and then solve for a regression or classification output by means of a mathematical pseudoinverse operation. In the field of neuromorphic engineering, these methods are increasingly popular for synthesizing biologically plausible neural networks, but the "learning method" - computation of the pseudoinverse by singular value decomposition - is problematic both for biological plausibility and because it is not an online or an adaptive method. We present an online or incremental method of computing the pseudoinverse, which we argue is biologically plausible as a learning method, and which can be made adaptable for non-stationary data streams. The method is significantly more memory-efficient than the conventional computation of pseudoinverses by singular value decomposition. version:1
arxiv-1207-2328 | Comparative Study for Inference of Hidden Classes in Stochastic Block Models | http://arxiv.org/abs/1207.2328 | id:1207.2328 author:Pan Zhang, Florent Krzakala, Jörg Reichardt, Lenka Zdeborová category:cs.LG cond-mat.stat-mech physics.data-an stat.ML  published:2012-07-10 summary:Inference of hidden classes in stochastic block model is a classical problem with important applications. Most commonly used methods for this problem involve na\"{\i}ve mean field approaches or heuristic spectral methods. Recently, belief propagation was proposed for this problem. In this contribution we perform a comparative study between the three methods on synthetically created networks. We show that belief propagation shows much better performance when compared to na\"{\i}ve mean field and spectral approaches. This applies to accuracy, computational efficiency and the tendency to overfit the data. version:2
arxiv-1207-3127 | Tracking Tetrahymena Pyriformis Cells using Decision Trees | http://arxiv.org/abs/1207.3127 | id:1207.3127 author:Quan Wang, Yan Ou, A. Agung Julius, Kim L. Boyer, Min Jun Kim category:cs.CV  published:2012-07-13 summary:Matching cells over time has long been the most difficult step in cell tracking. In this paper, we approach this problem by recasting it as a classification problem. We construct a feature set for each cell, and compute a feature difference vector between a cell in the current frame and a cell in a previous frame. Then we determine whether the two cells represent the same cell over time by training decision trees as our binary classifiers. With the output of decision trees, we are able to formulate an assignment problem for our cell association task and solve it using a modified version of the Hungarian algorithm. version:1
arxiv-1207-4180 | A Hierarchical Graphical Model for Record Linkage | http://arxiv.org/abs/1207.4180 | id:1207.4180 author:Pradeep Ravikumar, William Cohen category:cs.LG cs.IR stat.ML  published:2012-07-12 summary:The task of matching co-referent records is known among other names as rocord linkage. For large record-linkage problems, often there is little or no labeled data available, but unlabeled data shows a reasonable clear structure. For such problems, unsupervised or semi-supervised methods are preferable to supervised methods. In this paper, we describe a hierarchical graphical model framework for the linakge-problem in an unsupervised setting. In addition to proposing new methods, we also cast existing unsupervised probabilistic record-linkage methods in this framework. Some of the techniques we propose to minimize overfitting in the above model are of interest in the general graphical model setting. We describe a method for incorporating monotinicity constraints in a graphical model. We also outline a bootstrapping approach of using "single-field" classifiers to noisily label latent variables in a hierarchical model. Experimental results show that our proposed unsupervised methods perform quite competitively even with fully supervised record-linkage methods. version:1
arxiv-1207-4179 | Probabilistic index maps for modeling natural signals | http://arxiv.org/abs/1207.4179 | id:1207.4179 author:Nebojsa Jojic, Yaron Caspi, Manuel Reyes-Gomez category:cs.CV  published:2012-07-12 summary:One of the major problems in modeling natural signals is that signals with very similar structure may locally have completely different measurements, e.g., images taken under different illumination conditions, or the speech signal captured in different environments. While there have been many successful attempts to address these problems in application-specific settings, we believe that underlying a large set of problems in signal representation is a representational deficiency of intensity-derived local measurements that are the basis of most efficient models. We argue that interesting structure in signals is better captured when the signal is de- fined as a matrix whose entries are discrete indices to a separate palette of possible measurements. In order to model the variability in signal structure, we define a signal class not by a single index map, but by a probability distribution over the index maps, which can be estimated from the data, and which we call probabilistic index maps. The existing algorithm can be adapted to work with this representation. Furthermore, the probabilistic index map representation leads to algorithms with computational costs proportional to either the size of the palette or the log of the size of the palette, making the cost of significantly increased invariance to non-structural changes quite bearable. We illustrate the benefits of the probabilistic index map representation in several applications in computer vision and speech processing. version:1
arxiv-1207-3285 | Biogeography-Based Informative Gene Selection and Cancer Classification Using SVM and Random Forests | http://arxiv.org/abs/1207.3285 | id:1207.3285 author:Sarvesh Nikumbh, Shameek Ghosh, Valadi Jayaraman category:cs.NE stat.ML  published:2012-07-12 summary:Microarray cancer gene expression data comprise of very high dimensions. Reducing the dimensions helps in improving the overall analysis and classification performance. We propose two hybrid techniques, Biogeography - based Optimization - Random Forests (BBO - RF) and BBO - SVM (Support Vector Machines) with gene ranking as a heuristic, for microarray gene expression analysis. This heuristic is obtained from information gain filter ranking procedure. The BBO algorithm generates a population of candidate subset of genes, as part of an ecosystem of habitats, and employs the migration and mutation processes across multiple generations of the population to improve the classification accuracy. The fitness of each gene subset is assessed by the classifiers - SVM and Random Forests. The performances of these hybrid techniques are evaluated on three cancer gene expression datasets retrieved from the Kent Ridge Biomedical datasets collection and the libSVM data repository. Our results demonstrate that genes selected by the proposed techniques yield classification accuracies comparable to previously reported algorithms. version:1
arxiv-1206-0937 | Detecting Activations over Graphs using Spanning Tree Wavelet Bases | http://arxiv.org/abs/1206.0937 | id:1206.0937 author:James Sharpnack, Akshay Krishnamurthy, Aarti Singh category:stat.ML cs.IT math.IT math.ST stat.TH  published:2012-06-05 summary:We consider the detection of activations over graphs under Gaussian noise, where signals are piece-wise constant over the graph. Despite the wide applicability of such a detection algorithm, there has been little success in the development of computationally feasible methods with proveable theoretical guarantees for general graph topologies. We cast this as a hypothesis testing problem, and first provide a universal necessary condition for asymptotic distinguishability of the null and alternative hypotheses. We then introduce the spanning tree wavelet basis over graphs, a localized basis that reflects the topology of the graph, and prove that for any spanning tree, this approach can distinguish null from alternative in a low signal-to-noise regime. Lastly, we improve on this result and show that using the uniform spanning tree in the basis construction yields a randomized test with stronger theoretical guarantees that in many cases matches our necessary conditions. Specifically, we obtain near-optimal performance in edge transitive graphs, $k$-nearest neighbor graphs, and $\epsilon$-graphs. version:3
arxiv-1207-2959 | Hypothesis Testing in Speckled Data with Stochastic Distances | http://arxiv.org/abs/1207.2959 | id:1207.2959 author:Abraão D. C. Nascimento, Renato J. Cintra, Alejandro C. Frery category:stat.ML cs.GR  published:2012-07-12 summary:Images obtained with coherent illumination, as is the case of sonar, ultrasound-B, laser and Synthetic Aperture Radar -- SAR, are affected by speckle noise which reduces the ability to extract information from the data. Specialized techniques are required to deal with such imagery, which has been modeled by the G0 distribution and under which regions with different degrees of roughness and mean brightness can be characterized by two parameters; a third parameter, the number of looks, is related to the overall signal-to-noise ratio. Assessing distances between samples is an important step in image analysis; they provide grounds of the separability and, therefore, of the performance of classification procedures. This work derives and compares eight stochastic distances and assesses the performance of hypothesis tests that employ them and maximum likelihood estimation. We conclude that tests based on the triangular distance have the closest empirical size to the theoretical one, while those based on the arithmetic-geometric distances have the best power. Since the power of tests based on the triangular distance is close to optimum, we conclude that the safest choice is using this distance for hypothesis testing, even when compared with classical distances as Kullback-Leibler and Bhattacharyya. version:1
arxiv-1207-2922 | ROI Segmentation for Feature Extraction from Human Facial Images | http://arxiv.org/abs/1207.2922 | id:1207.2922 author:Surbhi, Vishal Arora category:cs.CV cs.HC  published:2012-07-12 summary:Human Computer Interaction (HCI) is the biggest goal of computer vision researchers. Features form the different facial images are able to provide a very deep knowledge about the activities performed by the different facial movements. In this paper we presented a technique for feature extraction from various regions of interest with the help of Skin color segmentation technique, Thresholding, knowledge based technique for face recognition. version:1
arxiv-1207-2641 | Camera identification by grouping images from database, based on shared noise patterns | http://arxiv.org/abs/1207.2641 | id:1207.2641 author:Teun Baar, Wiger van Houten, Zeno Geradts category:cs.CV  published:2012-07-11 summary:Previous research showed that camera specific noise patterns, so-called PRNU-patterns, are extracted from images and related images could be found. In this particular research the focus is on grouping images from a database, based on a shared noise pattern as an identification method for cameras. Using the method as described in this article, groups of images, created using the same camera, could be linked from a large database of images. Using MATLAB programming, relevant image noise patterns are extracted from images much quicker than common methods by the use of faster noise extraction filters and improvements to reduce the calculation costs. Relating noise patterns, with a correlation above a certain threshold value, can quickly be matched. Hereby, from a database of images, groups of relating images could be linked and the method could be used to scan a large number of images for suspect noise patterns. version:2
arxiv-1112-6209 | Building high-level features using large scale unsupervised learning | http://arxiv.org/abs/1112.6209 | id:1112.6209 author:Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, Andrew Y. Ng category:cs.LG  published:2011-12-29 summary:We consider the problem of building high- level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 bil- lion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a clus- ter with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental re- sults reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bod- ies. Starting with these learned features, we trained our network to obtain 15.8% accu- racy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative im- provement over the previous state-of-the-art. version:5
