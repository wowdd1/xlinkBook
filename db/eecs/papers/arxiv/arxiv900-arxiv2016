arxiv-1206-4651 | Is margin preserved after random projection? |  http://arxiv.org/abs/1206.4651  | author:Qinfeng Shi, Chunhua Shen, Rhys Hill, Anton van den Hengel category:cs.LG cs.CV stat.ML published:2012-06-18 summary:Random projections have been applied in many machine learning algorithms.However, whether margin is preserved after random projection is non-trivial andnot well studied. In this paper we analyse margin distortion after randomprojection, and give the conditions of margin preservation for binaryclassification problems. We also extend our analysis to margin for multiclassproblems, and provide theoretical bounds on multiclass margin on the projecteddata.
arxiv-1206-4650 | Analysis of Kernel Mean Matching under Covariate Shift |  http://arxiv.org/abs/1206.4650  | author:Yaoliang Yu, Csaba Szepesvari category:cs.LG stat.ML published:2012-06-18 summary:In real supervised learning scenarios, it is not uncommon that the trainingand test sample follow different probability distributions, thus rendering thenecessity to correct the sampling bias. Focusing on a particular covariateshift problem, we derive high probability confidence bounds for the kernel meanmatching (KMM) estimator, whose convergence rate turns out to depend on someregularity measure of the regression function and also on some capacity measureof the kernel. By comparing KMM with the natural plug-in estimator, weestablish the superiority of the former hence provide concreteevidence/understanding to the effectiveness of KMM under covariate shift.
arxiv-1206-4649 | Learning Efficient Structured Sparse Models |  http://arxiv.org/abs/1206.4649  | author:Alex Bronstein, Pablo Sprechmann, Guillermo Sapiro category:cs.LG cs.CV stat.ML published:2012-06-18 summary:We present a comprehensive framework for structured sparse coding andmodeling extending the recent ideas of using learnable fast regressors toapproximate exact sparse codes. For this purpose, we develop a novelblock-coordinate proximal splitting method for the iterative solution ofhierarchical sparse coding problems, and show an efficient feed forwardarchitecture derived from its iteration. This architecture faithfullyapproximates the exact structured sparse codes with a fraction of thecomplexity of the standard optimization methods. We also show that by usingdifferent training objective functions, learnable sparse encoders are no longerrestricted to be mere approximants of the exact sparse code for a pre-givendictionary, as in earlier formulations, but can be rather used as full-featuredsparse encoders or even modelers. A simple implementation shows several ordersof magnitude speedup compared to the state-of-the-art at minimal performancedegradation, making the proposed framework suitable for real time andlarge-scale applications.
arxiv-1206-4648 | Two-Manifold Problems with Applications to Nonlinear System Identification |  http://arxiv.org/abs/1206.4648  | author:Byron Boots, Geoff Gordon category:cs.LG published:2012-06-18 summary:Recently, there has been much interest in spectral approaches to learningmanifolds---so-called kernel eigenmap methods. These methods have had somesuccesses, but their applicability is limited because they are not robust tonoise. To address this limitation, we look at two-manifold problems, in whichwe simultaneously reconstruct two related manifolds, each representing adifferent view of the same data. By solving these interconnected learningproblems together, two-manifold algorithms are able to succeed where anon-integrated approach would fail: each view allows us to suppress noise inthe other, reducing bias. We propose a class of algorithms for two-manifoldproblems, based on spectral decomposition of cross-covariance operators inHilbert space, and discuss when two-manifold problems are useful. Finally, wedemonstrate that solving a two-manifold problem can aid in learning a nonlineardynamical system from limited data.
arxiv-1206-4647 | Active Learning for Matching Problems |  http://arxiv.org/abs/1206.4647  | author:Laurent Charlin, Rich Zemel, Craig Boutilier category:cs.LG cs.AI cs.IR published:2012-06-18 summary:Effective learning of user preferences is critical to easing user burden invarious types of matching problems. Equally important is active query selectionto further reduce the amount of preference information users must provide. Weaddress the problem of active learning of user preferences for matchingproblems, introducing a novel method for determining probabilistic matchings,and developing several new active learning strategies that are sensitive to thespecific matching objective. Experiments with real-world data sets spanningdiverse domains demonstrate that matching-sensitive active learning
arxiv-1206-4646 | Partial-Hessian Strategies for Fast Learning of Nonlinear Embeddings |  http://arxiv.org/abs/1206.4646  | author:Max Vladymyrov, Miguel Carreira-Perpinan category:cs.LG stat.ML published:2012-06-18 summary:Stochastic neighbor embedding (SNE) and related nonlinear manifold learningalgorithms achieve high-quality low-dimensional representations of similaritydata, but are notoriously slow to train. We propose a generic formulation ofembedding algorithms that includes SNE and other existing algorithms, and studytheir relation with spectral methods and graph Laplacians. This allows us todefine several partial-Hessian optimization strategies, characterize theirglobal and local convergence, and evaluate them empirically. We achieve up totwo orders of magnitude speedup over existing training methods with a strategy(which we call the spectral direction) that adds nearly no overhead to thegradient and yet is simple, scalable and applicable to several existing andfuture embedding algorithms.
arxiv-1206-4645 | Ensemble Methods for Convex Regression with Applications to Geometric Programming Based Circuit Design |  http://arxiv.org/abs/1206.4645  | author:Lauren Hannah, David Dunson category:cs.LG cs.NA stat.ME stat.ML published:2012-06-18 summary:Convex regression is a promising area for bridging statistical estimation anddeterministic convex optimization. New piecewise linear convex regressionmethods are fast and scalable, but can have instability when used toapproximate constraints or objective functions for optimization. Ensemblemethods, like bagging, smearing and random partitioning, can alleviate thisproblem and maintain the theoretical properties of the underlying estimator. Weempirically examine the performance of ensemble methods for prediction andoptimization, and then apply them to device modeling and constraintapproximation for geometric programming based circuit design.
arxiv-1206-4644 | Groupwise Constrained Reconstruction for Subspace Clustering |  http://arxiv.org/abs/1206.4644  | author:Ruijiang Li, Bin Li, Ke Zhang, Cheng Jin, Xiangyang Xue category:cs.LG stat.ML published:2012-06-18 summary:Reconstruction based subspace clustering methods compute a selfreconstruction matrix over the samples and use it for spectral clustering toobtain the final clustering result. Their success largely relies on theassumption that the underlying subspaces are independent, which, however, doesnot always hold in the applications with increasing number of subspaces. Inthis paper, we propose a novel reconstruction based subspace clustering modelwithout making the subspace independence assumption. In our model, certainproperties of the reconstruction matrix are explicitly characterized using thelatent cluster indicators, and the affinity matrix used for spectral clusteringcan be directly built from the posterior of the latent cluster indicatorsinstead of the reconstruction matrix. Experimental results on both syntheticand real-world datasets show that the proposed model can outperform thestate-of-the-art methods.
arxiv-1206-4643 | Lightning Does Not Strike Twice: Robust MDPs with Coupled Uncertainty |  http://arxiv.org/abs/1206.4643  | author:Shie Mannor, Ofir Mebel, Huan Xu category:cs.LG cs.GT cs.SY published:2012-06-18 summary:We consider Markov decision processes under parameter uncertainty. Previousstudies all restrict to the case that uncertainties among different states areuncoupled, which leads to conservative solutions. In contrast, we introduce anintuitive concept, termed "Lightning Does not Strike Twice," to model coupleduncertain parameters. Specifically, we require that the system can deviate fromits nominal parameters only a bounded number of times. We give probabilisticguarantees indicating that this model represents real life situations anddevise tractable algorithms for computing optimal control policies using thisconcept.
arxiv-1206-4642 | Fast Computation of Subpath Kernel for Trees |  http://arxiv.org/abs/1206.4642  | author:Daisuke Kimura, Hisashi Kashima category:cs.DS cs.LG stat.ML published:2012-06-18 summary:The kernel method is a potential approach to analyzing structured data suchas sequences, trees, and graphs; however, unordered trees have not beeninvestigated extensively. Kimura et al. (2011) proposed a kernel function forunordered trees on the basis of their subpaths, which are verticalsubstructures of trees responsible for hierarchical information in them. Theirkernel exhibits practically good performance in terms of accuracy and speed;however, linear-time computation is not guaranteed theoretically, unlike thecase of the other unordered tree kernel proposed by Vishwanathan and Smola(2003). In this paper, we propose a theoretically guaranteed linear-time kernelcomputation algorithm that is practically fast, and we present an efficientprediction algorithm whose running time depends only on the size of the inputtree. Experimental results show that the proposed algorithms are quiteefficient in practice.
arxiv-1206-4641 | Total Variation and Euler's Elastica for Supervised Learning |  http://arxiv.org/abs/1206.4641  | author:Tong Lin, Hanlin Xue, Ling Wang, Hongbin Zha category:cs.LG cs.CV stat.ML published:2012-06-18 summary:In recent years, total variation (TV) and Euler's elastica (EE) have beensuccessfully applied to image processing tasks such as denoising andinpainting. This paper investigates how to extend TV and EE to the supervisedlearning settings on high dimensional data. The supervised learning problem canbe formulated as an energy functional minimization under Tikhonovregularization scheme, where the energy is composed of a squared loss and atotal variation smoothing (or Euler's elastica smoothing). Its solution viavariational principles leads to an Euler-Lagrange PDE. However, the PDE isalways high-dimensional and cannot be directly solved by common methods.Instead, radial basis functions are utilized to approximate the targetfunction, reducing the problem to finding the linear coefficients of basisfunctions. We apply the proposed methods to supervised learning tasks(including binary classification, multi-class classification, and regression)on benchmark data sets. Extensive experiments have demonstrated promisingresults of the proposed methods.
arxiv-1206-4640 | Stability of matrix factorization for collaborative filtering |  http://arxiv.org/abs/1206.4640  | author:Yu-Xiang Wang, Huan Xu category:cs.NA cs.LG stat.ML published:2012-06-18 summary:We study the stability vis a vis adversarial noise of matrix factorizationalgorithm for matrix completion. In particular, our results include: (I) webound the gap between the solution matrix of the factorization method and theground truth in terms of root mean square error; (II) we treat the matrixfactorization as a subspace fitting problem and analyze the difference betweenthe solution subspace and the ground truth; (III) we analyze the predictionerror of individual users based on the subspace stability. We apply theseresults to the problem of collaborative filtering under manipulator attack,which leads to useful insights and guidelines for collaborative filteringsystem design.
arxiv-1206-4639 | Adaptive Regularization for Weight Matrices |  http://arxiv.org/abs/1206.4639  | author:Koby Crammer, Gal Chechik category:cs.LG cs.AI published:2012-06-18 summary:Algorithms for learning distributions over weight-vectors, such as AROW wererecently shown empirically to achieve state-of-the-art performance at variousproblems, with strong theoretical guaranties. Extending these algorithms tomatrix models pose challenges since the number of free parameters in thecovariance of the distribution scales as $n^4$ with the dimension $n$ of thematrix, and $n$ tends to be large in real applications. We describe, analyzeand experiment with two new algorithms for learning distribution of matrixmodels. Our first algorithm maintains a diagonal covariance over the parametersand can handle large covariance matrices. The second algorithm factors thecovariance to capture inter-features correlation while keeping the number ofparameters linear in the size of the original matrix. We analyze bothalgorithms in the mistake bound model and show a superior precision performanceof our approach over other algorithms in two tasks: retrieving similar images,and ranking similar documents. The factored algorithm is shown to attain fasterconvergence rate.
arxiv-1206-4638 | Efficient Euclidean Projections onto the Intersection of Norm Balls |  http://arxiv.org/abs/1206.4638  | author:Adams Wei Yu, Hao Su, Li Fei-Fei category:cs.LG stat.ML published:2012-06-18 summary:Using sparse-inducing norms to learn robust models has received increasingattention from many fields for its attractive properties. Projection-basedmethods have been widely applied to learning tasks constrained by such norms.As a key building block of these methods, an efficient operator for Euclideanprojection onto the intersection of $\ell_1$ and $\ell_{1,q}$ norm balls$(q=2\text{or}\infty)$ is proposed in this paper. We prove that the projectioncan be reduced to finding the root of an auxiliary function which is piecewisesmooth and monotonic. Hence, a bisection algorithm is sufficient to solve theproblem. We show that the time complexity of our solution is $O(n+g\log g)$ for$q=2$ and $O(n\log n)$ for $q=\infty$, where $n$ is the dimensionality of thevector to be projected and $g$ is the number of disjoint groups; we confirmthis complexity by experimentation. Empirical study reveals that our methodachieves significantly better performance than classical methods in terms ofrunning time and memory usage. We further show that embedded with our efficientprojection operator, projection-based algorithms can solve regression problemswith composite norm constraints more efficiently than other methods and givesuperior accuracy.
arxiv-1206-4637 | Learning to Identify Regular Expressions that Describe Email Campaigns |  http://arxiv.org/abs/1206.4637  | author:Paul Prasse, Christoph Sawade, Niels Landwehr, Tobias Scheffer category:cs.LG cs.CL stat.ML published:2012-06-18 summary:This paper addresses the problem of inferring a regular expression from agiven set of strings that resembles, as closely as possible, the regularexpression that a human expert would have written to identify the language.This is motivated by our goal of automating the task of postmasters of an emailservice who use regular expressions to describe and blacklist email spamcampaigns. Training data contains batches of messages and corresponding regularexpressions that an expert postmaster feels confident to blacklist. We modelthis task as a learning problem with structured output spaces and anappropriate loss function, derive a decoder and the resulting optimizationproblem, and a report on a case study conducted with an email service.
arxiv-1206-4636 | Modeling Latent Variable Uncertainty for Loss-based Learning |  http://arxiv.org/abs/1206.4636  | author:M. Pawan Kumar, Ben Packer, Daphne Koller category:cs.LG cs.AI cs.CV published:2012-06-18 summary:We consider the problem of parameter estimation using weakly superviseddatasets, where a training sample consists of the input and a partiallyspecified annotation, which we refer to as the output. The missing informationin the annotation is modeled using latent variables. Previous methodsoverburden a single distribution with two separate tasks: (i) modeling theuncertainty in the latent variables during training; and (ii) making accuratepredictions for the output and the latent variables during testing. We proposea novel framework that separates the demands of the two tasks using twodistributions: (i) a conditional distribution to model the uncertainty of thelatent variables for a given input-output pair; and (ii) a delta distributionto predict the output and the latent variables for a given input. Duringlearning, we encourage agreement between the two distributions by minimizing aloss-based dissimilarity coefficient. Our approach generalizes latent SVM intwo important ways: (i) it models the uncertainty over latent variables insteadof relying on a pointwise estimate; and (ii) it allows the use of lossfunctions that depend on latent variables, which greatly increases itsapplicability. We demonstrate the efficacy of our approach on two challengingproblems---object detection and action detection---using publicly availabledatasets.
arxiv-1206-4635 | Deep Mixtures of Factor Analysers |  http://arxiv.org/abs/1206.4635  | author:Yichuan Tang, Ruslan Salakhutdinov, Geoffrey Hinton category:cs.LG stat.ML published:2012-06-18 summary:An efficient way to learn deep density models that have many layers of latentvariables is to learn one layer at a time using a model that has only one layerof latent variables. After learning each layer, samples from the posteriordistributions for that layer are used as training data for learning the nextlayer. This approach is commonly used with Restricted Boltzmann Machines, whichare undirected graphical models with a single hidden layer, but it can also beused with Mixtures of Factor Analysers (MFAs) which are directed graphicalmodels. In this paper, we present a greedy layer-wise learning algorithm forDeep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be convertedto an equivalent shallow MFA by multiplying together the factor loadingmatrices at different levels, learning and inference are much more efficient ina DMFA and the sharing of each lower-level factor loading matrix by manydifferent higher level MFAs prevents overfitting. We demonstrate empiricallythat DMFAs learn better density models than both MFAs and two types ofRestricted Boltzmann Machine on a wide variety of datasets.
arxiv-1206-4634 | Artist Agent: A Reinforcement Learning Approach to Automatic Stroke Generation in Oriental Ink Painting |  http://arxiv.org/abs/1206.4634  | author:Ning Xie, Hirotaka Hachiya, Masashi Sugiyama category:cs.LG cs.GR stat.ML published:2012-06-18 summary:Oriental ink painting, called Sumi-e, is one of the most appealing paintingstyles that has attracted artists around the world. Major challenges incomputer-based Sumi-e simulation are to abstract complex scene information anddraw smooth and natural brush strokes. To automatically find such strokes, wepropose to model the brush as a reinforcement learning agent, and learn desiredbrush-trajectories by maximizing the sum of rewards in the policy searchframework. We also provide elaborate design of actions, states, and rewardstailored for a Sumi-e agent. The effectiveness of our proposed approach isdemonstrated through simulated Sumi-e experiments.
arxiv-1206-4633 | Fast Bounded Online Gradient Descent Algorithms for Scalable Kernel-Based Online Learning |  http://arxiv.org/abs/1206.4633  | author:Peilin Zhao, Jialei Wang, Pengcheng Wu, Rong Jin, Steven C. H. Hoi category:cs.LG stat.ML published:2012-06-18 summary:Kernel-based online learning has often shown state-of-the-art performance formany online learning tasks. It, however, suffers from a major shortcoming, thatis, the unbounded number of support vectors, making it non-scalable andunsuitable for applications with large-scale datasets. In this work, we studythe problem of bounded kernel-based online learning that aims to constrain thenumber of support vectors by a predefined budget. Although several algorithmshave been proposed in literature, they are neither computationally efficientdue to their intensive budget maintenance strategy nor effective due to the useof simple Perceptron algorithm. To overcome these limitations, we propose aframework for bounded kernel-based online learning based on an online gradientdescent approach. We propose two efficient algorithms of bounded onlinegradient descent (BOGD) for scalable kernel-based online learning: (i) BOGD bymaintaining support vectors using uniform sampling, and (ii) BOGD++ bymaintaining support vectors using non-uniform sampling. We present theoreticalanalysis of regret bound for both algorithms, and found promising empiricalperformance in terms of both efficacy and efficiency by comparing them toseveral well-known algorithms for bounded kernel-based online learning onlarge-scale datasets.
arxiv-1206-4632 | A Complete Analysis of the l_1,p Group-Lasso |  http://arxiv.org/abs/1206.4632  | author:Julia Vogt, Volker Roth category:cs.LG math.OC stat.ML published:2012-06-18 summary:The Group-Lasso is a well-known tool for joint regularization in machinelearning methods. While the l_{1,2} and the l_{1,\infty} version have beenstudied in detail and efficient algorithms exist, there are still openquestions regarding other l_{1,p} variants. We characterize conditions forsolutions of the l_{1,p} Group-Lasso for all p-norms with 1 <= p <= \infty, andwe present a unified active set algorithm. For all p-norms, a highly efficientprojected gradient algorithm is presented. This new algorithm enables us tocompare the prediction performance of many variants of the Group-Lasso in amulti-task learning setting, where the aim is to solve many learning problemsin parallel which are coupled via the Group-Lasso constraint. We conductlarge-scale experiments on synthetic data and on two real-world data sets. Inaccordance with theoretical characterizations of the different norms we observethat the weak-coupling norms with p between 1.5 and 2 consistently outperformthe strong-coupling norms with p >> 2.
arxiv-1206-4560 | Residual Component Analysis: Generalising PCA for more flexible inference in linear-Gaussian models |  http://arxiv.org/abs/1206.4560  | author:Alfredo Kalaitzis, Neil Lawrence category:cs.LG stat.ML published:2012-06-18 summary:Probabilistic principal component analysis (PPCA) seeks a low dimensionalrepresentation of a data set in the presence of independent spherical Gaussiannoise. The maximum likelihood solution for the model is an eigenvalue problemon the sample covariance matrix. In this paper we consider the situation wherethe data variance is already partially explained by other actors, for examplesparse conditional dependencies between the covariates, or temporalcorrelations leaving some residual variance. We decompose the residual varianceinto its components through a generalised eigenvalue problem, which we callresidual component analysis (RCA). We explore a range of new algorithms thatarise from the framework, including one that factorises the covariance of aGaussian density into a low-rank and a sparse-inverse component. We illustratethe ideas on the recovery of a protein-signaling network, a gene expressiontime-series data set and the recovery of the human skeleton from motion capture3-D cloud data.
arxiv-1206-4630 | Efficient Decomposed Learning for Structured Prediction |  http://arxiv.org/abs/1206.4630  | author:Rajhans Samdani, Dan Roth category:cs.LG published:2012-06-18 summary:Structured prediction is the cornerstone of several machine learningapplications. Unfortunately, in structured prediction settings with expressiveinter-variable interactions, exact inference-based learning algorithms, e.g.Structural SVM, are often intractable. We present a new way, DecomposedLearning (DecL), which performs efficient learning by restricting the inferencestep to a limited part of the structured spaces. We provide characterizationsbased on the structure, target parameters, and gold labels, under which DecL isequivalent to exact learning. We then show that in real world settings, whereour theoretical assumptions may not completely hold, DecL-based algorithms aresignificantly more efficient and as accurate as exact learning.
arxiv-1206-4629 | Multiple Kernel Learning from Noisy Labels by Stochastic Programming |  http://arxiv.org/abs/1206.4629  | author:Tianbao Yang, Mehrdad Mahdavi, Rong Jin, Lijun Zhang, Yang Zhou category:cs.LG published:2012-06-18 summary:We study the problem of multiple kernel learning from noisy labels. This isin contrast to most of the previous studies on multiple kernel learning thatmainly focus on developing efficient algorithms and assume perfectly labeledtraining examples. Directly applying the existing multiple kernel learningalgorithms to noisily labeled examples often leads to suboptimal performancedue to the incorrect class assignments. We address this challenge by castingmultiple kernel learning from noisy labels into a stochastic programmingproblem, and presenting a minimax formulation. We develop an efficientalgorithm for solving the related convex-concave optimization problem with afast convergence rate of $O(1/T)$ where $T$ is the number of iterations.Empirical studies on UCI data sets verify both the effectiveness of theproposed framework and the efficiency of the proposed optimization algorithm.
arxiv-1206-4628 | Robust PCA in High-dimension: A Deterministic Approach |  http://arxiv.org/abs/1206.4628  | author:Jiashi Feng, Huan Xu, Shuicheng Yan category:cs.LG stat.ML published:2012-06-18 summary:We consider principal component analysis for contaminated data-set in thehigh dimensional regime, where the dimensionality of each observation iscomparable or even more than the number of observations. We propose adeterministic high-dimensional robust PCA algorithm which inherits alltheoretical properties of its randomized counterpart, i.e., it is tractable,robust to contaminated points, easily kernelizable, asymptotic consistent andachieves maximal robustness -- a breakdown point of 50%. More importantly, theproposed method exhibits significantly better computational efficiency, whichmakes it suitable for large-scale real applications.
arxiv-1206-4627 | Convergence Rates of Biased Stochastic Optimization for Learning Sparse Ising Models |  http://arxiv.org/abs/1206.4627  | author:Jean Honorio category:cs.LG stat.ML published:2012-06-18 summary:We study the convergence rate of stochastic optimization of exact (NP-hard)objectives, for which only biased estimates of the gradient are available. Wemotivate this problem in the context of learning the structure and parametersof Ising models. We first provide a convergence-rate analysis of deterministicerrors for forward-backward splitting (FBS). We then extend our analysis tobiased stochastic errors, by first characterizing a family of samplers andproviding a high probability bound that allows understanding not only FBS, butalso proximal gradient (PG) methods. We derive some interesting conclusions:FBS requires only a logarithmically increasing number of random samples inorder to converge (although at a very low rate); the required number of randomsamples is the same for the deterministic and the biased stochastic setting forFBS and basic PG; accelerated PG is not guaranteed to converge in the biasedstochastic setting.
arxiv-1206-4626 | On-Line Portfolio Selection with Moving Average Reversion |  http://arxiv.org/abs/1206.4626  | author:Bin Li, Steven C. H. Hoi category:cs.CE cs.LG q-fin.PM published:2012-06-18 summary:On-line portfolio selection has attracted increasing interests in machinelearning and AI communities recently. Empirical evidences show that stock'shigh and low prices are temporary and stock price relatives are likely tofollow the mean reversion phenomenon. While the existing mean reversionstrategies are shown to achieve good empirical performance on many realdatasets, they often make the single-period mean reversion assumption, which isnot always satisfied in some real datasets, leading to poor performance whenthe assumption does not hold. To overcome the limitation, this article proposesa multiple-period mean reversion, or so-called Moving Average Reversion (MAR),and a new on-line portfolio selection strategy named "On-Line Moving AverageReversion" (OLMAR), which exploits MAR by applying powerful online learningtechniques. From our empirical results, we found that OLMAR can overcome thedrawback of existing mean reversion algorithms and achieve significantly betterresults, especially on the datasets where the existing mean reversionalgorithms failed. In addition to superior trading performance, OLMAR also runsextremely fast, further supporting its practical applicability to a wide rangeof applications.
arxiv-1206-4625 | Optimizing F-measure: A Tale of Two Approaches |  http://arxiv.org/abs/1206.4625  | author:Ye Nan, Kian Ming Chai, Wee Sun Lee, Hai Leong Chieu category:cs.LG published:2012-06-18 summary:F-measures are popular performance metrics, particularly for tasks withimbalanced data sets. Algorithms for learning to maximize F-measures follow twoapproaches: the empirical utility maximization (EUM) approach learns aclassifier having optimal performance on training data, while thedecision-theoretic approach learns a probabilistic model and then predictslabels with maximum expected F-measure. In this paper, we investigate thetheoretical justifications and connections for these two approaches, and westudy the conditions under which one approach is preferable to the other usingsynthetic and real datasets. Given accurate models, our results suggest thatthe two approaches are asymptotically equivalent given large training and testsets. Nevertheless, empirically, the EUM approach appears to be more robustagainst model misspecification, and given a good model, the decision-theoreticapproach appears to be better for handling rare classes and a common domainadaptation scenario.
arxiv-1206-4624 | Robust Multiple Manifolds Structure Learning |  http://arxiv.org/abs/1206.4624  | author:Dian Gong, Xuemei Zhao, Gerard Medioni category:cs.LG stat.ML published:2012-06-18 summary:We present a robust multiple manifolds structure learning (RMMSL) scheme torobustly estimate data structures under the multiple low intrinsic dimensionalmanifolds assumption. In the local learning stage, RMMSL efficiently estimateslocal tangent space by weighted low-rank matrix factorization. In the globallearning stage, we propose a robust manifold clustering method based on localstructure learning results. The proposed clustering method is designed to getthe flattest manifolds clusters by introducing a novel curved-level similarityfunction. Our approach is evaluated and compared to state-of-the-art methods onsynthetic data, handwritten digit images, human motion capture data andmotorbike videos. We demonstrate the effectiveness of the proposed approach,which yields higher clustering accuracy, and produces promising results forchallenging tasks of human motion segmentation and motion flow learning fromvideos.
arxiv-1206-4623 | On the Size of the Online Kernel Sparsification Dictionary |  http://arxiv.org/abs/1206.4623  | author:Yi Sun, Faustino Gomez, Juergen Schmidhuber category:cs.LG stat.ML published:2012-06-18 summary:We analyze the size of the dictionary constructed from online kernelsparsification, using a novel formula that expresses the expected determinantof the kernel Gram matrix in terms of the eigenvalues of the covarianceoperator. Using this formula, we are able to connect the cardinality of thedictionary with the eigen-decay of the covariance operator. In particular, weshow that under certain technical conditions, the size of the dictionary willalways grow sub-linearly in the number of data points, and, as a consequence,the kernel linear regressor constructed from the resulting dictionary isconsistent.
arxiv-1206-4622 | A Graphical Model Formulation of Collaborative Filtering Neighbourhood Methods with Fast Maximum Entropy Training |  http://arxiv.org/abs/1206.4622  | author:Aaron Defazio, Tiberio Caetano category:cs.LG cs.IR stat.ML published:2012-06-18 summary:Item neighbourhood methods for collaborative filtering learn a weighted graphover the set of items, where each item is connected to those it is most similarto. The prediction of a user's rating on an item is then given by that ratingof neighbouring items, weighted by their similarity. This paper presents a newneighbourhood approach which we call item fields, whereby an undirectedgraphical model is formed over the item graph. The resulting prediction rule isa simple generalization of the classical approaches, which takes into accountnon-local information in the graph, allowing its best results to be obtainedwhen using drastically fewer edges than other neighbourhood approaches. A fastapproximate maximum entropy training method based on the Bethe approximation ispresented, which uses a simple gradient ascent procedure. When usingprecomputed sufficient statistics on the Movielens datasets, our method isfaster than maximum likelihood approaches by two orders of magnitude.
arxiv-1206-4621 | Path Integral Policy Improvement with Covariance Matrix Adaptation |  http://arxiv.org/abs/1206.4621  | author:Freek Stulp, Olivier Sigaud category:cs.LG published:2012-06-18 summary:There has been a recent focus in reinforcement learning on addressingcontinuous state and action problems by optimizing parameterized policies. PI2is a recent example of this approach. It combines a derivation from firstprinciples of stochastic optimal control with tools from statistical estimationtheory. In this paper, we consider PI2 as a member of the wider family ofmethods which share the concept of probability-weighted averaging toiteratively update parameters to optimize a cost function. We compare PI2 toother members of the same family - Cross-Entropy Methods and CMAES - at theconceptual level and in terms of performance. The comparison suggests thederivation of a novel algorithm which we call PI2-CMA for "Path Integral PolicyImprovement with Covariance Matrix Adaptation". PI2-CMA's main advantage isthat it determines the magnitude of the exploration noise automatically.
arxiv-1206-4620 | Improved Information Gain Estimates for Decision Tree Induction |  http://arxiv.org/abs/1206.4620  | author:Sebastian Nowozin category:cs.LG stat.ML published:2012-06-18 summary:Ensembles of classification and regression trees remain popular machinelearning methods because they define flexible non-parametric models thatpredict well and are computationally efficient both during training andtesting. During induction of decision trees one aims to find predicates thatare maximally informative about the prediction target. To select goodpredicates most approaches estimate an information-theoretic scoring function,the information gain, both for classification and regression problems. We pointout that the common estimation procedures are biased and show that by replacingthem with improved estimators of the discrete and the differential entropy wecan obtain better decision trees. In effect our modifications yield improvedpredictive performance and are simple to implement in any decision tree code.
arxiv-1206-4619 | Inductive Kernel Low-rank Decomposition with Priors: A Generalized Nystrom Method |  http://arxiv.org/abs/1206.4619  | author:Kai Zhang, Liang Lan, Jun Liu, andreas Rauber, Fabian Moerchen category:cs.LG published:2012-06-18 summary:Low-rank matrix decomposition has gained great popularity recently in scalingup kernel methods to large amounts of data. However, some limitations couldprevent them from working effectively in certain domains. For example, manyexisting approaches are intrinsically unsupervised, which does not incorporateside information (e.g., class labels) to produce task specific decompositions;also, they typically work "transductively", i.e., the factorization does notgeneralize to new samples, so the complete factorization needs to be recomputedwhen new samples become available. To solve these problems, in this paper wepropose an"inductive"-flavored method for low-rank kernel decomposition withpriors. We achieve this by generalizing the Nystr\"om method in a novel way. Onthe one hand, our approach employs a highly flexible, nonparametric structurethat allows us to generalize the low-rank factors to arbitrarily new samples;on the other hand, it has linear time and space complexities, which can beorders of magnitudes faster than existing approaches and renders greatefficiency in learning a low-rank kernel decomposition. Empirical resultsdemonstrate the efficacy and efficiency of the proposed method.
arxiv-1206-4618 | Compact Hyperplane Hashing with Bilinear Functions |  http://arxiv.org/abs/1206.4618  | author:Wei Liu, Jun Wang, Yadong Mu, Sanjiv Kumar, Shih-Fu Chang category:cs.LG stat.ML published:2012-06-18 summary:Hyperplane hashing aims at rapidly searching nearest points to a hyperplane,and has shown practical impact in scaling up active learning with SVMs.Unfortunately, the existing randomized methods need long hash codes to achievereasonable search accuracy and thus suffer from reduced search speed and largememory overhead. To this end, this paper proposes a novel hyperplane hashingtechnique which yields compact hash codes. The key idea is the bilinear form ofthe proposed hash functions, which leads to higher collision probability thanthe existing hyperplane hash functions when using random projections. Tofurther increase the performance, we propose a learning based framework inwhich the bilinear functions are directly learned from the data. This resultsin short yet discriminative codes, and also boosts the search performance overthe random projection based solutions. Large-scale active learning experimentscarried out on two datasets with up to one million samples demonstrate theoverall superiority of the proposed approach.
arxiv-1206-4617 | Continuous Inverse Optimal Control with Locally Optimal Examples |  http://arxiv.org/abs/1206.4617  | author:Sergey Levine, Vladlen Koltun category:cs.LG cs.AI stat.ML published:2012-06-18 summary:Inverse optimal control, also known as inverse reinforcement learning, is theproblem of recovering an unknown reward function in a Markov decision processfrom expert demonstrations of the optimal policy. We introduce a probabilisticinverse optimal control algorithm that scales gracefully with taskdimensionality, and is suitable for large, continuous domains where evencomputing a full policy is impractical. By using a local approximation of thereward function, our method can also drop the assumption that thedemonstrations are globally optimal, requiring only local optimality. Thisallows it to learn from examples that are unsuitable for prior methods.
arxiv-1206-4616 | A Hierarchical Dirichlet Process Model with Multiple Levels of Clustering for Human EEG Seizure Modeling |  http://arxiv.org/abs/1206.4616  | author:Drausin Wulsin, Shane Jensen, Brian Litt category:stat.AP cs.LG stat.ML published:2012-06-18 summary:Driven by the multi-level structure of human intracranialelectroencephalogram (iEEG) recordings of epileptic seizures, we introduce anew variant of a hierarchical Dirichlet Process---the multi-level clusteringhierarchical Dirichlet Process (MLC-HDP)---that simultaneously clustersdatasets on multiple levels. Our seizure dataset contains brain activityrecorded in typically more than a hundred individual channels for each seizureof each patient. The MLC-HDP model clusters over channels-types, seizure-types,and patient-types simultaneously. We describe this model and its implementationin detail. We also present the results of a simulation study comparing theMLC-HDP to a similar model, the Nested Dirichlet Process and finallydemonstrate the MLC-HDP's use in modeling seizures across multiple patients. Wefind the MLC-HDP's clustering to be comparable to independent human physicianclusterings. To our knowledge, the MLC-HDP model is the first in the epilepsyliterature capable of clustering seizures within and between patients.
arxiv-1206-3777 | An Analysis of the Methods Employed for Breast Cancer Diagnosis |  http://arxiv.org/abs/1206.3777  | author:Mahjabeen Mirza Beg, Monika Jain category:cs.NE q-bio.TO published:2012-06-17 summary:Breast cancer research over the last decade has been tremendous. The groundbreaking innovations and novel methods help in the early detection, in settingthe stages of the therapy and in assessing the response of the patient to thetreatment. The prediction of the recurrent cancer is also crucial for thesurvival of the patient. This paper studies various techniques used for thediagnosis of breast cancer. Different methods are explored for their merits andde-merits for the diagnosis of breast lesion. Some of the methods are yetunproven but the studies look very encouraging. It was found that the recentuse of the combination of Artificial Neural Networks in most of the instancesgives accurate results for the diagnosis of breast cancer and their use canalso be extended to other diseases.
arxiv-1206-4042 | The Stability of Convergence of Curve Evolutions in Vector Fields |  http://arxiv.org/abs/1206.4042  | author:Junyan Wang, Kap Luk Chan category:cs.CV math.AP published:2012-06-17 summary:Curve evolution is often used to solve computer vision problems. If the curveevolution fails to converge, we would not be able to solve the targeted problemin a lifetime. This paper studies the theoretical aspect of the convergence ofa type of general curve evolutions. We establish a theory for analyzing andimproving the stability of the convergence of the general curve evolutions.Based on this theory, we ascertain that the convergence of a known curveevolution is marginal stable. We propose a way of modifying the original curveevolution equation to improve the stability of the convergence according to ourtheory. Numerical experiments show that the modification improves theconvergence of the curve evolution, which validates our theory.
arxiv-1206-3721 | Constraint-free Graphical Model with Fast Learning Algorithm |  http://arxiv.org/abs/1206.3721  | author:Kazuya Takabatake, Shotaro Akaho category:cs.LG stat.ML published:2012-06-17 summary:In this paper, we propose a simple, versatile model for learning thestructure and parameters of multivariate distributions from a data set.Learning a Markov network from a given data set is not a simple problem,because Markov networks rigorously represent Markov properties, and this rigorimposes complex constraints on the design of the networks. Our proposed modelremoves these constraints, acquiring important aspects from the informationgeometry. The proposed parameter- and structure-learning algorithms are simpleto execute as they are based solely on local computation at each node.Experiments demonstrate that our algorithms work appropriately.
arxiv-1206-3713 | Learning the Structure and Parameters of Large-Population Graphical Games from Behavioral Data |  http://arxiv.org/abs/1206.3713  | author:Jean Honorio, Luis Ortiz category:cs.LG cs.GT stat.ML published:2012-06-16 summary:We consider learning, from strictly behavioral data, the structure andparameters of linear influence games (LIGs), a class of parametric graphicalgames introduced by Irfan and Ortiz (2014). LIGs facilitate causal strategicinference (CSI): Making inferences from causal interventions on stable behaviorin strategic settings. Applications include the identification of the mostinfluential individuals in large (social) networks. Such tasks can also supportpolicy-making analysis. Motivated by the computational work on LIGs, we castthe learning problem as maximum-likelihood estimation (MLE) of a generativemodel defined by pure-strategy Nash equilibria (PSNE). Our simple formulationuncovers the fundamental interplay between goodness-of-fit and modelcomplexity: good models capture equilibrium behavior within the data whilecontrolling the true number of equilibria, including those unobserved. Weprovide a generalization bound establishing the sample complexity for MLE inour framework. We propose several algorithms including convex loss minimization(CLM) and sigmoidal approximations. We prove that the number of exact PSNE inLIGs is small, with high probability; thus, CLM is sound. We illustrate ourapproach on synthetic data and real-world U.S. congressional voting records. Webriefly discuss our learning framework's generality and potential applicabilityto general graphical games.
arxiv-1206-3633 | Feature Based Fuzzy Rule Base Design for Image Extraction |  http://arxiv.org/abs/1206.3633  | author:Koushik Mondal, Paramartha Dutta, Siddhartha Bhattacharyya category:cs.CV cs.AI published:2012-06-16 summary:In the recent advancement of multimedia technologies, it becomes a majorconcern of detecting visual attention regions in the field of image processing.The popularity of the terminal devices in a heterogeneous environment of themultimedia technology gives us enough scope for the betterment of imagevisualization. Although there exist numerous methods, feature based imageextraction becomes a popular one in the field of image processing. Theobjective of image segmentation is the domain-independent partition of theimage into a set of regions, which are visually distinct and uniform withrespect to some property, such as grey level, texture or colour. Segmentationand subsequent extraction can be considered the first step and key issue inobject recognition, scene understanding and image analysis. Its applicationarea encompasses mobile devices, industrial quality control, medicalappliances, robot navigation, geophysical exploration, military applications,etc. In all these areas, the quality of the final results depends largely onthe quality of the preprocessing work. Most of the times, acquiringspurious-free preprocessing data requires a lot of application cum mathematicalintensive background works. We propose a feature based fuzzy rule guided noveltechnique that is functionally devoid of any external intervention duringexecution. Experimental results suggest that this approach is an efficient onein comparison to different other techniques extensively addressed inliterature. In order to justify the supremacy of performance of our proposedtechnique in respect of its competitors, we take recourse to effective metricslike Mean Squared Error (MSE), Mean Absolute Error (MAE) and Peak Signal toNoise Ratio (PSNR).
arxiv-1206-3666 | Unsupervised adaptation of brain machine interface decoders |  http://arxiv.org/abs/1206.3666  | author:Tayfun GÃ¼rel, Carsten Mehring category:cs.LG q-bio.NC published:2012-06-16 summary:The performance of neural decoders can degrade over time due tononstationarities in the relationship between neuronal activity and behavior.In this case, brain-machine interfaces (BMI) require adaptation of theirdecoders to maintain high performance across time. One way to achieve this isby use of periodical calibration phases, during which the BMI system (or anexternal human demonstrator) instructs the user to perform certain movements orbehaviors. This approach has two disadvantages: (i) calibration phasesinterrupt the autonomous operation of the BMI and (ii) between two calibrationphases the BMI performance might not be stable but continuously decrease. Abetter alternative would be that the BMI decoder is able to continuously adaptin an unsupervised manner during autonomous BMI operation, i.e. without knowingthe movement intentions of the user. In the present article, we present an efficient method for such unsupervisedtraining of BMI systems for continuous movement control. The proposed methodutilizes a cost function derived from neuronal recordings, which guides alearning algorithm to evaluate the decoding parameters. We verify theperformance of our adaptive method by simulating a BMI user with an optimalfeedback control model and its interaction with our adaptive BMI decoder. Thesimulation results show that the cost function and the algorithm yield fast andprecise trajectories towards targets at random orientations on a 2-dimensionalcomputer screen. For initially unknown and non-stationary tuning parameters,our unsupervised method is still able to generate precise trajectories and tokeep its performance stable in the long term. The algorithm can optionally workalso with neuronal error signals instead or in conjunction with the proposedunsupervised adaptation.
arxiv-1206-3714 | How important are Deformable Parts in the Deformable Parts Model? |  http://arxiv.org/abs/1206.3714  | author:Santosh K. Divvala, Alexei A. Efros, Martial Hebert category:cs.CV cs.AI cs.LG published:2012-06-16 summary:The main stated contribution of the Deformable Parts Model (DPM) detector ofFelzenszwalb et al. (over the Histogram-of-Oriented-Gradients approach of Dalaland Triggs) is the use of deformable parts. A secondary contribution is thelatent discriminative learning. Tertiary is the use of multiple components. Acommon belief in the vision community (including ours, before this study) isthat their ordering of contributions reflects the performance of detector inpractice. However, what we have experimentally found is that the ordering ofimportance might actually be the reverse. First, we show that by increasing thenumber of components, and switching the initialization step from theiraspect-ratio, left-right flipping heuristics to appearance-based clustering,considerable improvement in performance is obtained. But more intriguingly, weshow that with these new components, the part deformations can now becompletely switched off, yet obtaining results that are almost on par with theoriginal DPM detector. Finally, we also show initial results for using multiplecomponents on a different problem -- scene classification, suggesting that thisidea might have wider applications in addition to object detection.
arxiv-1206-3594 | Blind PSF estimation and methods of deconvolution optimization |  http://arxiv.org/abs/1206.3594  | author:Yu. A. Bunyak, O. Yu. Sofina, R. N. Kvetnyy category:cs.CV published:2012-06-15 summary:We have shown that the left side null space of the autoregression (AR) matrixoperator is the lexicographical presentation of the point spread function (PSF)on condition the AR parameters are common for original and blurred images. Themethod of inverse PSF evaluation with regularization functional as the functionof surface area is offered. The inverse PSF was used for primary imageestimation. Two methods of original image estimate optimization were designedbasing on maximum entropy generalization of sought and blurred imagesconditional probability density and regularization. The first method usesbalanced variations of convolution and deconvolution transforms to obtainingiterative schema of image optimization. The variations balance was defined bydynamic regularization basing on condition of iteration process convergence.The regularization has dynamic character because depends on current andprevious image estimate variations. The second method implements theregularization of deconvolution optimization in curved space with metricdefined on image estimate surface. It is basing on target functional invarianceto fluctuations of optimal argument value. The given iterative schemas havefaster convergence in comparison with known ones, so they can be used forreconstruction of high resolution images series in real time.
arxiv-1206-3522 | General Upper Bounds on the Running Time of Parallel Evolutionary Algorithms |  http://arxiv.org/abs/1206.3522  | author:JÃ¶rg LÃ¤ssig, Dirk Sudholt category:cs.NE F.2.2 published:2012-06-15 summary:We present a new method for analyzing the running time of parallelevolutionary algorithms with spatially structured populations. Based on thefitness-level method, it yields upper bounds on the expected parallel runningtime. This allows to rigorously estimate the speedup gained by parallelization.Tailored results are given for common migration topologies: ring graphs, torusgraphs, hypercubes, and the complete graph. Example applications forpseudo-Boolean optimization show that our method is easy to apply and that itgives powerful results. In our examples the possible speedup increases with thedensity of the topology. Surprisingly, even sparse topologies like ring graphslead to a significant speedup for many functions while not increasing the totalnumber of function evaluations by more than a constant factor. We also identifywhich number of processors yield asymptotically optimal speedups, thus givinghints on how to parametrize parallel evolutionary algorithms.
arxiv-1206-3509 | A Novel Approach for Protein Structure Prediction |  http://arxiv.org/abs/1206.3509  | author:Saurabh Sarkar, Prateek Malhotra, Virender Guman category:cs.LG q-bio.BM published:2012-06-15 summary:The idea of this project is to study the protein structure and sequencerelationship using the hidden markov model and artificial neural network. Inthis context we have assumed two hidden markov models. In first model we havetaken protein secondary structures as hidden and protein sequences as observed.In second model we have taken protein sequences as hidden and proteinstructures as observed. The efficiencies for both the hidden markov models havebeen calculated. The results show that the efficiencies of first model isgreater that the second one .These efficiencies are cross validated usingartificial neural network. This signifies the importance of protein secondarystructures as the main hidden controlling factors due to which we observe aparticular amino acid sequence. This also signifies that protein secondarystructure is more conserved in comparison to amino acid sequence.
arxiv-1206-3382 | Simple Regret Optimization in Online Planning for Markov Decision Processes |  http://arxiv.org/abs/1206.3382  | author:Zohar Feldman, Carmel Domshlak category:cs.AI cs.LG published:2012-06-15 summary:We consider online planning in Markov decision processes (MDPs). In onlineplanning, the agent focuses on its current state only, deliberates about theset of possible policies from that state onwards and, when interrupted, usesthe outcome of that exploratory deliberation to choose what action to performnext. The performance of algorithms for online planning is assessed in terms ofsimple regret, which is the agent's expected performance loss when the chosenaction, rather than an optimal one, is followed. To date, state-of-the-art algorithms for online planning in general MDPs areeither best effort, or guarantee only polynomial-rate reduction of simpleregret over time. Here we introduce a new Monte-Carlo tree search algorithm,BRUE, that guarantees exponential-rate reduction of simple regret and errorprobability. This algorithm is based on a simple yet non-standard state-spacesampling scheme, MCTS2e, in which different parts of each sample are dedicatedto different exploratory objectives. Our empirical evaluation shows that BRUEnot only provides superior performance guarantees, but is also very effectivein practice and favorably compares to state-of-the-art. We then extend BRUEwith a variant of "learning by forgetting." The resulting set of algorithms,BRUE(alpha), generalizes BRUE, improves the exponential factor in the upperbound on its reduction rate, and exhibits even more attractive empiricalperformance.
arxiv-1206-3381 | On the Cover-Hart Inequality: What's a Sample of Size One Worth? |  http://arxiv.org/abs/1206.3381  | author:Tilmann Gneiting category:math.ST cs.IT math.IT stat.ML stat.TH published:2012-06-15 summary:Bob predicts a future observation based on a sample of size one. Alice candraw a sample of any size before issuing her prediction. How much better canshe do than Bob? Perhaps surprisingly, under a large class of loss functions,which we refer to as the Cover-Hart family, the best Alice can do is to halveBob's risk. In this sense, half the information in an infinite sample iscontained in a sample of size one. The Cover-Hart family is a convex cone thatincludes metrics and negative definite functions, subject to slight regularityconditions. These results may help explain the small relative differences inempirical performance measures in applied classification and forecastingproblems, as well as the success of reasoning and learning by analogy ingeneral, and nearest neighbor techniques in particular.
arxiv-1206-3564 | Functional Currents : a new mathematical tool to model and analyse functional shapes |  http://arxiv.org/abs/1206.3564  | author:Nicolas Charon, Alain TrouvÃ© category:cs.CG cs.CV math.DG published:2012-06-15 summary:This paper introduces the concept of functional current as a mathematicalframework to represent and treat functional shapes, i.e. sub-manifold supportedsignals. It is motivated by the growing occurrence, in medical imaging andcomputational anatomy, of what can be described as geometrico-functional data,that is a data structure that involves a deformable shape (roughly a finitedimensional sub manifold) together with a function defined on this shape takingvalue in another manifold. Indeed, if mathematical currents have already proved to be very efficienttheoretically and numerically to model and process shapes as curves orsurfaces, they are limited to the manipulation of purely geometrical objects.We show that the introduction of the concept of functional currents offers agenuine solution to the simultaneous processing of the geometric and signalinformation of any functional shape. We explain how functional currents can beequipped with a Hilbertian norm mixing geometrical and functional content offunctional shapes nicely behaving under geometrical and functionalperturbations and paving the way to various processing algorithms. Weillustrate this potential on two problems: the redundancy reduction offunctional shapes representations through matching pursuit schemes onfunctional currents and the simultaneous geometric and functional registrationof functional shapes under diffeomorphic transport.
arxiv-1206-3582 | Decentralized Learning for Multi-player Multi-armed Bandits |  http://arxiv.org/abs/1206.3582  | author:Dileep Kalathil, Naumaan Nayyar, Rahul Jain category:math.OC cs.LG cs.SY published:2012-06-14 summary:We consider the problem of distributed online learning with multiple playersin multi-armed bandits (MAB) models. Each player can pick among multiple arms.When a player picks an arm, it gets a reward. We consider both i.i.d. rewardmodel and Markovian reward model. In the i.i.d. model each arm is modelled asan i.i.d. process with an unknown distribution with an unknown mean. In theMarkovian model, each arm is modelled as a finite, irreducible, aperiodic andreversible Markov chain with an unknown probability transition matrix andstationary distribution. The arms give different rewards to different players.If two players pick the same arm, there is a "collision", and neither of themget any reward. There is no dedicated control channel for coordination orcommunication among the players. Any other communication between the users iscostly and will add to the regret. We propose an online index-based distributedlearning policy called ${\tt dUCB_4}$ algorithm that trades off\textit{exploration v. exploitation} in the right way, and achieves expectedregret that grows at most as near-$O(\log^2 T)$. The motivation comes fromopportunistic spectrum access by multiple secondary users in cognitive radionetworks wherein they must pick among various wireless channels that lookdifferent to different users. This is the first distributed learning algorithmfor multi-player MABs to the best of our knowledge.
arxiv-1206-3137 | Identifiability and Unmixing of Latent Parse Trees |  http://arxiv.org/abs/1206.3137  | author:Daniel Hsu, Sham M. Kakade, Percy Liang category:stat.ML cs.LG published:2012-06-14 summary:This paper explores unsupervised learning of parsing models along twodirections. First, which models are identifiable from infinite data? We use ageneral technique for numerically checking identifiability based on the rank ofa Jacobian matrix, and apply it to several standard constituency and dependencyparsing models. Second, for identifiable models, how do we estimate theparameters efficiently? EM suffers from local optima, while recent work usingspectral methods cannot be directly applied since the topology of the parsetree varies across sentences. We develop a strategy, unmixing, which deals withthis additional complexity for restricted classes of parsing models.
arxiv-1206-3099 | Sparse Distributed Learning Based on Diffusion Adaptation |  http://arxiv.org/abs/1206.3099  | author:Paolo Di Lorenzo, Ali H. Sayed category:cs.LG cs.DC published:2012-06-14 summary:This article proposes diffusion LMS strategies for distributed estimationover adaptive networks that are able to exploit sparsity in the underlyingsystem model. The approach relies on convex regularization, common incompressive sensing, to enhance the detection of sparsity via a diffusiveprocess over the network. The resulting algorithms endow networks with learningabilities and allow them to learn the sparse structure from the incoming datain real-time, and also to track variations in the sparsity of the model. Weprovide convergence and mean-square performance analysis of the proposed methodand show under what conditions it outperforms the unregularized diffusionversion. We also show how to adaptively select the regularization parameter.Simulation results illustrate the advantage of the proposed filters for sparsedata recovery.
arxiv-1206-3204 | Improved Spectral-Norm Bounds for Clustering |  http://arxiv.org/abs/1206.3204  | author:Pranjal Awasthi, Or Sheffet category:cs.LG cs.DS published:2012-06-14 summary:Aiming to unify known results about clustering mixtures of distributionsunder separation conditions, Kumar and Kannan[2010] introduced a deterministiccondition for clustering datasets. They showed that this single deterministiccondition encompasses many previously studied clustering assumptions. Morespecifically, their proximity condition requires that in the target$k$-clustering, the projection of a point $x$ onto the line joining its clustercenter $\mu$ and some other center $\mu'$, is a large additive factor closer to$\mu$ than to $\mu'$. This additive factor can be roughly described as $k$times the spectral norm of the matrix representing the differences between thegiven (known) dataset and the means of the (unknown) target clustering.Clearly, the proximity condition implies center separation -- the distancebetween any two centers must be as large as the above mentioned bound. In this paper we improve upon the work of Kumar and Kannan along severalaxes. First, we weaken the center separation bound by a factor of $\sqrt{k}$,and secondly we weaken the proximity condition by a factor of $k$. Using theseweaker bounds we still achieve the same guarantees when all points satisfy theproximity condition. We also achieve better guarantees when only$(1-\epsilon)$-fraction of the points satisfy the weaker proximity condition.The bulk of our analysis relies only on center separation under which one canproduce a clustering which (i) has low error, (ii) has low $k$-means cost, and(iii) has centers very close to the target centers. Our improved separation condition allows us to match the results of thePlanted Partition Model of McSherry[2001], improve upon the results ofOstrovsky et al[2006], and improve separation results for mixture of Gaussianmodels in a particular setting.
arxiv-1206-3072 | Statistical Consistency of Finite-dimensional Unregularized Linear Classification |  http://arxiv.org/abs/1206.3072  | author:Matus Telgarsky category:cs.LG stat.ML published:2012-06-14 summary:This manuscript studies statistical properties of linear classifiers obtainedthrough minimization of an unregularized convex risk over a finite sample.Although the results are explicitly finite-dimensional, inputs may be passedthrough feature maps; in this way, in addition to treating the consistency oflogistic regression, this analysis also handles boosting over a finite weaklearning class with, for instance, the exponential, logistic, and hinge losses.In this finite-dimensional setting, it is still possible to fit arbitrarydecision boundaries: scaling the complexity of the weak learning class with thesample size leads to the optimal classification risk almost surely.
arxiv-1206-2691 | IDS: An Incremental Learning Algorithm for Finite Automata |  http://arxiv.org/abs/1206.2691  | author:Muddassar A. Sindhu, Karl Meinke category:cs.LG cs.DS cs.FL published:2012-06-13 summary:We present a new algorithm IDS for incremental learning of deterministicfinite automata (DFA). This algorithm is based on the concept of distinguishingsequences introduced in (Angluin81). We give a rigorous proof that two versionsof this learning algorithm correctly learn in the limit. Finally we present anempirical performance analysis that compares these two algorithms, focussing onlearning times and different types of learning queries. We conclude that IDS isan efficient algorithm for software engineering applications of automatalearning, such as testing and model inference.
arxiv-1206-3231 | CORL: A Continuous-state Offset-dynamics Reinforcement Learner |  http://arxiv.org/abs/1206.3231  | author:Emma Brunskill, Bethany Leffler, Lihong Li, Michael L. Littman, Nicholas Roy category:cs.LG stat.ML published:2012-06-13 summary:Continuous state spaces and stochastic, switching dynamics characterize anumber of rich, realworld domains, such as robot navigation across varyingterrain. We describe a reinforcementlearning algorithm for learning in thesedomains and prove for certain environments the algorithm is probablyapproximately correct with a sample complexity that scales polynomially withthe state-space dimension. Unfortunately, no optimal planning techniques existin general for such problems; instead we use fitted value iteration to solvethe learned MDP, and include the error due to approximate planning in ourbounds. Finally, we report an experiment using a robotic car driving overvarying terrain to demonstrate that these dynamics representations adequatelycapture real-world dynamics and that our algorithm can be used to efficientlysolve such problems.
arxiv-1206-3275 | Learning Hidden Markov Models for Regression using Path Aggregation |  http://arxiv.org/abs/1206.3275  | author:Keith Noto, Mark Craven category:cs.LG cs.CE q-bio.QM published:2012-06-13 summary:We consider the task of learning mappings from sequential data to real-valuedresponses. We present and evaluate an approach to learning a type of hiddenMarkov model (HMM) for regression. The learning process involves inferring thestructure and parameters of a conventional HMM, while simultaneously learning aregression model that maps features that characterize paths through the modelto continuous responses. Our results, in both synthetic and biological domains,demonstrate the value of jointly learning the two components of our approach.
arxiv-1206-3260 | Causal discovery of linear acyclic models with arbitrary distributions |  http://arxiv.org/abs/1206.3260  | author:Patrik O. Hoyer, Aapo Hyvarinen, Richard Scheines, Peter L. Spirtes, Joseph Ramsey, Gustavo Lacerda, Shohei Shimizu category:stat.ML cs.AI cs.LG published:2012-06-13 summary:An important task in data analysis is the discovery of causal relationshipsbetween observed variables. For continuous-valued data, linear acyclic causalmodels are commonly used to model the data-generating process, and theinference of such models is a well-studied problem. However, existing methodshave significant limitations. Methods based on conditional independencies(Spirtes et al. 1993; Pearl 2000) cannot distinguish betweenindependence-equivalent models, whereas approaches purely based on IndependentComponent Analysis (Shimizu et al. 2006) are inapplicable to data which ispartially Gaussian. In this paper, we generalize and combine the twoapproaches, to yield a method able to learn the model structure in many casesfor which the previous methods provide answers that are either incorrect or arenot as informative as possible. We give exact graphical conditions for when twodistinct models represent the same family of distributions, and empiricallydemonstrate the power of our method through thorough simulations.
arxiv-1206-3269 | Bayesian Out-Trees |  http://arxiv.org/abs/1206.3269  | author:Tony S. Jebara category:cs.LG stat.ML published:2012-06-13 summary:A Bayesian treatment of latent directed graph structure for non-iid data isprovided where each child datum is sampled with a directed conditionaldependence on a single unknown parent datum. The latent graph structure isassumed to lie in the family of directed out-tree graphs which leads toefficient Bayesian inference. The latent likelihood of the data and itsgradients are computable in closed form via Tutte's directed matrix treetheorem using determinants and inverses of the out-Laplacian. This novellikelihood subsumes iid likelihood, is exchangeable and yields efficientunsupervised and semi-supervised learning algorithms. In addition to handlingtaxonomy and phylogenetic datasets the out-tree assumption performssurprisingly well as a semi-parametric density estimator on standard iiddatasets. Experiments with unsupervised and semisupervised learning are shownon various UCI and taxonomy datasets.
arxiv-1206-3259 | Cumulative distribution networks and the derivative-sum-product algorithm |  http://arxiv.org/abs/1206.3259  | author:Jim Huang, Brendan J. Frey category:cs.LG stat.ML published:2012-06-13 summary:We introduce a new type of graphical model called a "cumulative distributionnetwork" (CDN), which expresses a joint cumulative distribution as a product oflocal functions. Each local function can be viewed as providing evidence aboutpossible orderings, or rankings, of variables. Interestingly, we find that theconditional independence properties of CDNs are quite different from othergraphical models. We also describe a messagepassing algorithm that efficientlycomputes conditional cumulative distributions. Due to the unique independenceproperties of the CDN, these messages do not in general have a one-to-onecorrespondence with messages exchanged in standard algorithms, such as beliefpropagation. We demonstrate the application of CDNs for structured rankinglearning using a previously-studied multi-player gaming dataset.
arxiv-1206-3274 | Small Sample Inference for Generalization Error in Classification Using the CUD Bound |  http://arxiv.org/abs/1206.3274  | author:Eric B. Laber, Susan A. Murphy category:cs.LG stat.ML published:2012-06-13 summary:Confidence measures for the generalization error are crucial when smalltraining samples are used to construct classifiers. A common approach is toestimate the generalization error by resampling and then assume the resampledestimator follows a known distribution to form a confidence set [Kohavi 1995,Martin 1996,Yang 2006]. Alternatively, one might bootstrap the resampledestimator of the generalization error to form a confidence set. Unfortunately,these methods do not reliably provide sets of the desired confidence. The poorperformance appears to be due to the lack of smoothness of the generalizationerror as a function of the learned classifier. This results in a non-normaldistribution of the estimated generalization error. We construct a confidenceset for the generalization error by use of a smooth upper bound on thedeviation between the resampled estimate and generalization error. Theconfidence set is formed by bootstrapping this upper bound. In cases in whichthe approximation class for the classifier can be represented as a parametricadditive model, we provide a computationally efficient algorithm. This methodexhibits superior performance across a series of test and simulated data sets.
arxiv-1206-3270 | Estimation and Clustering with Infinite Rankings |  http://arxiv.org/abs/1206.3270  | author:Marina Meila, Le Bao category:cs.LG stat.ML published:2012-06-13 summary:This paper presents a natural extension of stagewise ranking to the the caseof infinitely many items. We introduce the infinite generalized Mallows model(IGM), describe its properties and give procedures to estimate it from data.For estimation of multimodal distributions we introduce theExponential-Blurring-Mean-Shift nonparametric clustering algorithm. Theexperiments highlight the properties of the new model and demonstrate thatinfinite models can be simple, elegant and practical.
arxiv-1206-3285 | Dyna-Style Planning with Linear Function Approximation and Prioritized Sweeping |  http://arxiv.org/abs/1206.3285  | author:Richard S. Sutton, Csaba Szepesvari, Alborz Geramifard, Michael P. Bowling category:cs.AI cs.LG cs.SY published:2012-06-13 summary:We consider the problem of efficiently learning optimal control policies andvalue functions over large state spaces in an online setting in which estimatesmust be available after each interaction with the world. This paper develops anexplicitly model-based approach extending the Dyna architecture to linearfunction approximation. Dynastyle planning proceeds by generating imaginaryexperience from the world model and then applying model-free reinforcementlearning algorithms to the imagined state transitions. Our main results are toprove that linear Dyna-style planning converges to a unique solutionindependent of the generating distribution, under natural conditions. In thepolicy evaluation setting, we prove that the limit point is the least-squares(LSTD) solution. An implication of our results is that prioritized-sweeping canbe soundly extended to the linear approximation case, backing up to precedingfeatures rather than to preceding states. We introduce two versions ofprioritized sweeping with linear Dyna and briefly illustrate their performanceempirically on the Mountain Car and Boyan Chain problems.
arxiv-1206-3297 | Hybrid Variational/Gibbs Collapsed Inference in Topic Models |  http://arxiv.org/abs/1206.3297  | author:Max Welling, Yee Whye Teh, Hilbert Kappen category:cs.LG stat.ML published:2012-06-13 summary:Variational Bayesian inference and (collapsed) Gibbs sampling are the twoimportant classes of inference algorithms for Bayesian networks. Both havetheir advantages and disadvantages: collapsed Gibbs sampling is unbiased but isalso inefficient for large count values and requires averaging over manysamples to reduce variance. On the other hand, variational Bayesian inferenceis efficient and accurate for large count values but suffers from bias forsmall counts. We propose a hybrid algorithm that combines the best of bothworlds: it samples very small counts and applies variational updates to largecounts. This hybridization is shown to significantly improve testset perplexityrelative to variational inference at no computational cost.
arxiv-1206-2807 | An efficient hierarchical graph based image segmentation |  http://arxiv.org/abs/1206.2807  | author:Silvio Jamil F. GuimarÃ£es, Jean Cousty, Yukiko Kenmochi, Laurent Najman category:cs.CV published:2012-06-13 summary:Hierarchical image segmentation provides region-oriented scalespace, i.e., aset of image segmentations at different detail levels in which thesegmentations at finer levels are nested with respect to those at coarserlevels. Most image segmentation algorithms, such as region merging algorithms,rely on a criterion for merging that does not lead to a hierarchy, and forwhich the tuning of the parameters can be difficult. In this work, we propose ahierarchical graph based image segmentation relying on a criterion popularizedby Felzenzwalb and Huttenlocher. We illustrate with both real and syntheticimages, showing efficiency, ease of use, and robustness of our method.
arxiv-1206-2944 | Practical Bayesian Optimization of Machine Learning Algorithms |  http://arxiv.org/abs/1206.2944  | author:Jasper Snoek, Hugo Larochelle, Ryan P. Adams category:stat.ML cs.LG published:2012-06-13 summary:Machine learning algorithms frequently require careful tuning of modelhyperparameters, regularization terms, and optimization parameters.Unfortunately, this tuning is often a "black art" that requires expertexperience, unwritten rules of thumb, or sometimes brute-force search. Muchmore appealing is the idea of developing automatic approaches which canoptimize the performance of a given learning algorithm to the task at hand. Inthis work, we consider the automatic tuning problem within the framework ofBayesian optimization, in which a learning algorithm's generalizationperformance is modeled as a sample from a Gaussian process (GP). The tractableposterior distribution induced by the GP leads to efficient use of theinformation gathered by previous experiments, enabling optimal choices aboutwhat parameters to try next. Here we show how the effects of the Gaussianprocess prior and the associated inference procedure can have a large impact onthe success or failure of Bayesian optimization. We show that thoughtfulchoices can lead to results that exceed expert-level performance in tuningmachine learning algorithms. We also describe new algorithms that take intoaccount the variable cost (duration) of learning experiments and that canleverage the presence of multiple cores for parallel experimentation. We showthat these proposed algorithms improve on previous automatic procedures and canreach or surpass human expert-level optimization on a diverse set ofcontemporary algorithms including latent Dirichlet allocation, structured SVMsand convolutional neural networks.
arxiv-1206-3294 | Flexible Priors for Exemplar-based Clustering |  http://arxiv.org/abs/1206.3294  | author:Daniel Tarlow, Richard S. Zemel, Brendan J. Frey category:cs.LG stat.ML published:2012-06-13 summary:Exemplar-based clustering methods have been shown to produce state-of-the-artresults on a number of synthetic and real-world clustering problems. They areappealing because they offer computational benefits over latent-mean models andcan handle arbitrary pairwise similarity measures between data points. However,when trying to recover underlying structure in clustering problems, tailoredsimilarity measures are often not enough; we also desire control over thedistribution of cluster sizes. Priors such as Dirichlet process priors allowthe number of clusters to be unspecified while expressing priors over datapartitions. To our knowledge, they have not been applied to exemplar-basedmodels. We show how to incorporate priors, including Dirichlet process priors,into the recently introduced affinity propagation algorithm. We develop anefficient maxproduct belief propagation algorithm for our new model anddemonstrate experimentally how the expanded range of clustering priors allowsus to better recover true clusterings in situations where we have someinformation about the generating process.
arxiv-1206-3293 | Propagation using Chain Event Graphs |  http://arxiv.org/abs/1206.3293  | author:Peter Thwaites, Jim Q. Smith, Robert G. Cowell category:cs.AI cs.CL published:2012-06-13 summary:A Chain Event Graph (CEG) is a graphial model which designed to embodyconditional independencies in problems whose state spaces are highly asymmetricand do not admit a natural product structure. In this paer we present aprobability propagation algorithm which uses the topology of the CEG to build atransporter CEG. Intriungly,the transporter CEG is directly analogous to thetriangulated Bayesian Network (BN) in the more conventional junction treepropagation algorithms used with BNs. The propagation method uses factorizationformulae also analogous to (but different from) the ones using potentials oncliques and separators of the BN. It appears that the methods will be typicallymore efficient than the BN algorithms when applied to contexts where there issignificant asymmetry present.
arxiv-1206-3493 | Compressed Sensing of EEG for Wireless Telemonitoring with Low Energy Consumption and Inexpensive Hardware |  http://arxiv.org/abs/1206.3493  | author:Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Bhaskar D. Rao category:stat.AP cs.IT math.IT stat.ML published:2012-06-13 summary:Telemonitoring of electroencephalogram (EEG) through wireless body-areanetworks is an evolving direction in personalized medicine. Among variousconstraints in designing such a system, three important constraints are energyconsumption, data compression, and device cost. Conventional data compressionmethodologies, although effective in data compression, consumes significantenergy and cannot reduce device cost. Compressed sensing (CS), as an emergingdata compression methodology, is promising in catering to these constraints.However, EEG is non-sparse in the time domain and also non-sparse intransformed domains (such as the wavelet domain). Therefore, it is extremelydifficult for current CS algorithms to recover EEG with the quality thatsatisfies the requirements of clinical diagnosis and engineering applications.Recently, Block Sparse Bayesian Learning (BSBL) was proposed as a new method tothe CS problem. This study introduces the technique to the telemonitoring ofEEG. Experimental results show that its recovery quality is better thanstate-of-the-art CS algorithms, and sufficient for practical use. These resultssuggest that BSBL is very promising for telemonitoring of EEG and othernon-sparse physiological signals.
arxiv-1206-3236 | Learning Inclusion-Optimal Chordal Graphs |  http://arxiv.org/abs/1206.3236  | author:Vincent Auvray, Louis Wehenkel category:cs.LG cs.DS stat.ML published:2012-06-13 summary:Chordal graphs can be used to encode dependency models that are representableby both directed acyclic and undirected graphs. This paper discusses a verysimple and efficient algorithm to learn the chordal structure of aprobabilistic model from data. The algorithm is a greedy hill-climbing searchalgorithm that uses the inclusion boundary neighborhood over chordal graphs. Inthe limit of a large sample size and under appropriate hypotheses on thescoring criterion, we prove that the algorithm will find a structure that isinclusion-optimal when the dependency model of the data-generating distributioncan be represented exactly by an undirected graph. The algorithm is evaluatedon simulated datasets.
arxiv-1206-3237 | Clique Matrices for Statistical Graph Decomposition and Parameterising Restricted Positive Definite Matrices |  http://arxiv.org/abs/1206.3237  | author:David Barber category:cs.DM cs.LG stat.ML published:2012-06-13 summary:We introduce Clique Matrices as an alternative representation of undirectedgraphs, being a generalisation of the incidence matrix representation. Here weuse clique matrices to decompose a graph into a set of possibly overlappingclusters, de ned as well-connected subsets of vertices. The decomposition isbased on a statistical description which encourages clusters to be wellconnected and few in number. Inference is carried out using a variationalapproximation. Clique matrices also play a natural role in parameterisingpositive de nite matrices under zero constraints on elements of the matrix. Weshow that clique matrices can parameterise all positive de nite matricesrestricted according to a decomposable graph and form a structured FactorAnalysis approximation in the non-decomposable case.
arxiv-1206-3262 | Convergent Message-Passing Algorithms for Inference over General Graphs with Convex Free Energies |  http://arxiv.org/abs/1206.3262  | author:Tamir Hazan, Amnon Shashua category:cs.LG stat.ML published:2012-06-13 summary:Inference problems in graphical models can be represented as a constrainedoptimization of a free energy function. It is known that when the Bethe freeenergy is used, the fixedpoints of the belief propagation (BP) algorithmcorrespond to the local minima of the free energy. However BP fails to convergein many cases of interest. Moreover, the Bethe free energy is non-convex forgraphical models with cycles thus introducing great difficulty in derivingefficient algorithms for finding local minima of the free energy for generalgraphs. In this paper we introduce two efficient BP-like algorithms, onesequential and the other parallel, that are guaranteed to converge to theglobal minimum, for any graph, over the class of energies known as "convex freeenergies". In addition, we propose an efficient heuristic for setting theparameters of the convex free energy based on the structure of the graph.
arxiv-1206-3257 | Constrained Approximate Maximum Entropy Learning of Markov Random Fields |  http://arxiv.org/abs/1206.3257  | author:Varun Ganapathi, David Vickrey, John Duchi, Daphne Koller category:cs.LG stat.ML published:2012-06-13 summary:Parameter estimation in Markov random fields (MRFs) is a difficult task, inwhich inference over the network is run in the inner loop of a gradient descentprocedure. Replacing exact inference with approximate methods such as loopybelief propagation (LBP) can suffer from poor convergence. In this paper, weprovide a different approach for combining MRF learning and Betheapproximation. We consider the dual of maximum likelihood Markov networklearning - maximizing entropy with moment matching constraints - and thenapproximate both the objective and the constraints in the resultingoptimization problem. Unlike previous work along these lines (Teh & Welling,2003), our formulation allows parameter sharing between features in a generallog-linear model, parameter regularization and conditional training. We showthat piecewise training (Sutton & McCallum, 2005) is a very restricted specialcase of this formulation. We study two optimization strategies: one based on asingle convex approximation and one that uses repeated convex approximations.We show results on several real-world networks that demonstrate that thesealgorithms can significantly outperform learning with loopy and piecewise. Ourresults also provide a framework for analyzing the trade-offs of differentrelaxations of the entropy objective and of the constraints.
arxiv-1206-3238 | Greedy Block Coordinate Descent for Large Scale Gaussian Process Regression |  http://arxiv.org/abs/1206.3238  | author:Liefeng Bo, Cristian Sminchisescu category:cs.LG stat.ML published:2012-06-13 summary:We propose a variable decomposition algorithm -greedy block coordinatedescent (GBCD)- in order to make dense Gaussian process regression practicalfor large scale problems. GBCD breaks a large scale optimization into a seriesof small sub-problems. The challenge in variable decomposition algorithms isthe identification of a subproblem (the active set of variables) that yieldsthe largest improvement. We analyze the limitations of existing methods andcast the active set selection into a zero-norm constrained optimization problemthat we solve using greedy methods. By directly estimating the decrease in theobjective function, we obtain not only efficient approximate solutions forGBCD, but we are also able to demonstrate that the method is globallyconvergent. Empirical comparisons against competing dense methods likeConjugate Gradient or SMO show that GBCD is an order of magnitude faster.Comparisons against sparse GP methods show that GBCD is both accurate andcapable of handling datasets of 100,000 samples or more.
arxiv-1206-3290 | Modelling local and global phenomena with sparse Gaussian processes |  http://arxiv.org/abs/1206.3290  | author:Jarno Vanhatalo, Aki Vehtari category:cs.LG stat.ML published:2012-06-13 summary:Much recent work has concerned sparse approximations to speed up the Gaussianprocess regression from the unfavorable O(n3) scaling in computational time toO(nm2). Thus far, work has concentrated on models with one covariance function.However, in many practical situations additive models with multiple covariancefunctions may perform better, since the data may contain both long and shortlength-scale phenomena. The long length-scales can be captured with globalsparse approximations, such as fully independent conditional (FIC), and theshort length-scales can be modeled naturally by covariance functions withcompact support (CS). CS covariance functions lead to naturally sparsecovariance matrices, which are computationally cheaper to handle than fullcovariance matrices. In this paper, we propose a new sparse Gaussian processmodel with two additive components: FIC for the long length-scales and CScovariance function for the short length-scales. We give theoretical andexperimental results and show that under certain conditions the proposed modelhas the same computational complexity as FIC. We also compare the modelperformance of the proposed model to additive models approximated by fully andpartially independent conditional (PIC). We use real data sets and show thatour model outperforms FIC and PIC approximations for data sets with twoadditive phenomena.
arxiv-1206-3279 | The Phylogenetic Indian Buffet Process: A Non-Exchangeable Nonparametric Prior for Latent Features |  http://arxiv.org/abs/1206.3279  | author:Kurt T. Miller, Thomas Griffiths, Michael I. Jordan category:cs.LG stat.ML published:2012-06-13 summary:Nonparametric Bayesian models are often based on the assumption that theobjects being modeled are exchangeable. While appropriate in some applications(e.g., bag-of-words models for documents), exchangeability is sometimes assumedsimply for computational reasons; non-exchangeable models might be a betterchoice for applications based on subject matter. Drawing on ideas fromgraphical models and phylogenetics, we describe a non-exchangeable prior for aclass of nonparametric latent feature models that is nearly as efficientcomputationally as its exchangeable counterpart. Our model is applicable to thegeneral setting in which the dependencies between objects can be expressedusing a tree, where edge lengths indicate the strength of relationships. Wedemonstrate an application to modeling probabilistic choice.
arxiv-1206-3241 | Approximating the Partition Function by Deleting and then Correcting for Model Edges |  http://arxiv.org/abs/1206.3241  | author:Arthur Choi, Adnan Darwiche category:cs.LG stat.ML published:2012-06-13 summary:We propose an approach for approximating the partition function which isbased on two steps: (1) computing the partition function of a simplified modelwhich is obtained by deleting model edges, and (2) rectifying the result byapplying an edge-by-edge correction. The approach leads to an intuitiveframework in which one can trade-off the quality of an approximation with thecomplexity of computing it. It also includes the Bethe free energyapproximation as a degenerate case. We develop the approach theoretically inthis paper and provide a number of empirical results that reveal its practicalutility.
arxiv-1206-3256 | Multi-View Learning over Structured and Non-Identical Outputs |  http://arxiv.org/abs/1206.3256  | author:Kuzman Ganchev, Joao Graca, John Blitzer, Ben Taskar category:cs.LG stat.ML published:2012-06-13 summary:In many machine learning problems, labeled training data is limited butunlabeled data is ample. Some of these problems have instances that can befactored into multiple views, each of which is nearly sufficent in determiningthe correct labels. In this paper we present a new algorithm for probabilisticmulti-view learning which uses the idea of stochastic agreement between viewsas regularization. Our algorithm works on structured and unstructured problemsand easily generalizes to partial agreement scenarios. For the full agreementcase, our algorithm minimizes the Bhattacharyya distance between the models ofeach view, and performs better than CoBoosting and two-view Perceptron onseveral flat and structured classification problems.
arxiv-1206-3242 | Multi-View Learning in the Presence of View Disagreement |  http://arxiv.org/abs/1206.3242  | author:C. Christoudias, Raquel Urtasun, Trevor Darrell category:cs.LG stat.ML published:2012-06-13 summary:Traditional multi-view learning approaches suffer in the presence of viewdisagreement,i.e., when samples in each view do not belong to the same classdue to view corruption, occlusion or other noise processes. In this paper wepresent a multi-view learning approach that uses a conditional entropycriterion to detect view disagreement. Once detected, samples with viewdisagreement are filtered and standard multi-view learning methods can besuccessfully applied to the remaining samples. Experimental evaluation onsynthetic and audio-visual databases demonstrates that the detection andfiltering of view disagreement considerably increases the performance oftraditional multi-view learning approaches.
arxiv-1206-3243 | Bounds on the Bethe Free Energy for Gaussian Networks |  http://arxiv.org/abs/1206.3243  | author:Botond Cseke, Tom Heskes category:cs.LG stat.ML published:2012-06-13 summary:We address the problem of computing approximate marginals in Gaussianprobabilistic models by using mean field and fractional Bethe approximations.As an extension of Welling and Teh (2001), we define the Gaussian fractionalBethe free energy in terms of the moment parameters of the approximatemarginals and derive an upper and lower bound for it. We give necessaryconditions for the Gaussian fractional Bethe free energies to be bounded frombelow. It turns out that the bounding condition is the same as the pairwisenormalizability condition derived by Malioutov et al. (2006) as a sufficientcondition for the convergence of the message passing algorithm. By giving acounterexample, we disprove the conjecture in Welling and Teh (2001): even whenthe Bethe free energy is not bounded from below, it can possess a local minimumto which the minimization algorithms can converge.
arxiv-1206-3287 | Learning the Bayesian Network Structure: Dirichlet Prior versus Data |  http://arxiv.org/abs/1206.3287  | author:Harald Steck category:cs.LG stat.ME stat.ML published:2012-06-13 summary:In the Bayesian approach to structure learning of graphical models, theequivalent sample size (ESS) in the Dirichlet prior over the model parameterswas recently shown to have an important effect on the maximum-a-posterioriestimate of the Bayesian network structure. In our first contribution, wetheoretically analyze the case of large ESS-values, which complements previouswork: among other results, we find that the presence of an edge in a Bayesiannetwork is favoured over its absence even if both the Dirichlet prior and thedata imply independence, as long as the conditional empirical distribution isnotably different from uniform. In our second contribution, we focus onrealistic ESS-values, and provide an analytical approximation to the "optimal"ESS-value in a predictive sense (its accuracy is also validatedexperimentally): this approximation provides an understanding as to whichproperties of the data have the main effect determining the "optimal"ESS-value.
arxiv-1206-3298 | Continuous Time Dynamic Topic Models |  http://arxiv.org/abs/1206.3298  | author:Chong Wang, David Blei, David Heckerman category:cs.IR cs.LG stat.ML published:2012-06-13 summary:In this paper, we develop the continuous time dynamic topic model (cDTM). ThecDTM is a dynamic topic model that uses Brownian motion to model the latenttopics through a sequential collection of documents, where a "topic" is apattern of word use that we expect to evolve over the course of the collection.We derive an efficient variational approximate inference algorithm that takesadvantage of the sparsity of observations in text, a property that lets useasily handle many time points. In contrast to the cDTM, the originaldiscrete-time dynamic topic model (dDTM) requires that time be discretized.Moreover, the complexity of variational inference for the dDTM grows quickly astime granularity increases, a drawback which limits fine-graineddiscretization. We demonstrate the cDTM on two news corpora, reporting bothpredictive perplexity and the novel task of time stamp prediction.
arxiv-1206-3247 | Learning Convex Inference of Marginals |  http://arxiv.org/abs/1206.3247  | author:Justin Domke category:cs.LG stat.ML published:2012-06-13 summary:Graphical models trained using maximum likelihood are a common tool forprobabilistic inference of marginal distributions. However, this approachsuffers difficulties when either the inference process or the model isapproximate. In this paper, the inference process is first defined to be theminimization of a convex function, inspired by free energy approximations.Learning is then done directly in terms of the performance of the inferenceprocess at univariate marginal prediction. The main novelty is that this is adirect minimization of emperical risk, where the risk measures the accuracy ofpredicted marginals.
arxiv-1206-3249 | Projected Subgradient Methods for Learning Sparse Gaussians |  http://arxiv.org/abs/1206.3249  | author:John Duchi, Stephen Gould, Daphne Koller category:cs.LG stat.ML published:2012-06-13 summary:Gaussian Markov random fields (GMRFs) are useful in a broad range ofapplications. In this paper we tackle the problem of learning a sparse GMRF ina high-dimensional space. Our approach uses the l1-norm as a regularization onthe inverse covariance matrix. We utilize a novel projected gradient method,which is faster than previous methods in practice and equal to the bestperforming of these in asymptotic complexity. We also extend the l1-regularizedobjective to the problem of sparsifying entire blocks within the inversecovariance matrix. Our methods generalize fairly easily to this case, whileother methods do not. We demonstrate that our extensions give bettergeneralization performance on two real domains--biological network analysis anda 2D-shape modeling image task.
arxiv-1206-3252 | Convex Point Estimation using Undirected Bayesian Transfer Hierarchies |  http://arxiv.org/abs/1206.3252  | author:Gal Elidan, Ben Packer, Geremy Heitz, Daphne Koller category:cs.LG stat.ML published:2012-06-13 summary:When related learning tasks are naturally arranged in a hierarchy, anappealing approach for coping with scarcity of instances is that of transferlearning using a hierarchical Bayes framework. As fully Bayesian computationscan be difficult and computationally demanding, it is often desirable to useposterior point estimates that facilitate (relatively) efficient prediction.However, the hierarchical Bayes framework does not always lend itself naturallyto this maximum aposteriori goal. In this work we propose an undirectedreformulation of hierarchical Bayes that relies on priors in the form ofsimilarity measures. We introduce the notion of "degree of transfer" weights oncomponents of these similarity measures, and show how they can be automaticallylearned within a joint probabilistic framework. Importantly, our reformulationresults in a convex objective for many learning problems, thus facilitatingoptimal posterior point estimation using standard optimization techniques. Inaddition, we no longer require proper priors, allowing for flexible andstraightforward specification of joint distributions over transfer hierarchies.We show that our framework is effective for learning models that are part oftransfer hierarchies for two real-life tasks: object shape modeling usingGaussian density estimation and document classification.
arxiv-1206-3254 | Latent Topic Models for Hypertext |  http://arxiv.org/abs/1206.3254  | author:Amit Gruber, Michal Rosen-Zvi, Yair Weiss category:cs.IR cs.CL cs.LG stat.ML published:2012-06-13 summary:Latent topic models have been successfully applied as an unsupervised topicdiscovery technique in large document collections. With the proliferation ofhypertext document collection such as the Internet, there has also been greatinterest in extending these approaches to hypertext [6, 9]. These approachestypically model links in an analogous fashion to how they model words - thedocument-link co-occurrence matrix is modeled in the same way that thedocument-word co-occurrence matrix is modeled in standard topic models. In thispaper we present a probabilistic generative model for hypertext documentcollections that explicitly models the generation of links. Specifically, linksfrom a word w to a document d depend directly on how frequent the topic of w isin d, in addition to the in-degree of d. We show how to perform EM learning onthis model efficiently. By not modeling links as analogous to words, we end upusing far fewer free parameters and obtain better link prediction results.
arxiv-1206-2627 | Image Similarity Using Sparse Representation and Compression Distance |  http://arxiv.org/abs/1206.2627  | author:Tanaya Guha, Rabab K. Ward category:cs.CV published:2012-06-12 summary:A new line of research uses compression methods to measure the similaritybetween signals. Two signals are considered similar if one can be compressedsignificantly when the information of the other is known. The existingcompression-based similarity methods, although successful in the discrete onedimensional domain, do not work well in the context of images. This paperproposes a sparse representation-based approach to encode the informationcontent of an image using information from the other image, and uses thecompactness (sparsity) of the representation as a measure of itscompressibility (how much can the image be compressed) with respect to theother image. The more sparse the representation of an image, the better it canbe compressed and the more it is similar to the other image. The efficacy ofthe proposed measure is demonstrated through the high accuracies achieved inimage clustering, retrieval and classification.
arxiv-1206-2437 | A Novel Windowing Technique for Efficient Computation of MFCC for Speaker Recognition |  http://arxiv.org/abs/1206.2437  | author:Md. Sahidullah, Goutam Saha category:cs.CV published:2012-06-12 summary:In this paper, we propose a novel family of windowing technique to computeMel Frequency Cepstral Coefficient (MFCC) for automatic speaker recognitionfrom speech. The proposed method is based on fundamental property of discretetime Fourier transform (DTFT) related to differentiation in frequency domain.Classical windowing scheme such as Hamming window is modified to obtainderivatives of discrete time Fourier transform coefficients. It has beenmathematically shown that the slope and phase of power spectrum are inherentlyincorporated in newly computed cepstrum. Speaker recognition systems based onour proposed family of window functions are shown to attain substantial andconsistent performance improvement over baseline single tapered Hamming windowas well as recently proposed multitaper windowing technique.
arxiv-1206-2459 | RÃ©nyi Divergence and Kullback-Leibler Divergence |  http://arxiv.org/abs/1206.2459  | author:Tim van Erven, Peter HarremoÃ«s category:cs.IT math.IT math.ST stat.ML stat.TH published:2012-06-12 summary:R\'enyi divergence is related to R\'enyi entropy much like Kullback-Leiblerdivergence is related to Shannon's entropy, and comes up in many settings. Itwas introduced by R\'enyi as a measure of information that satisfies almost thesame axioms as Kullback-Leibler divergence, and depends on a parameter that iscalled its order. In particular, the R\'enyi divergence of order 1 equals theKullback-Leibler divergence. We review and extend the most important properties of R\'enyi divergence andKullback-Leibler divergence, including convexity, continuity, limits of$\sigma$-algebras and the relation of the special order 0 to the Gaussiandichotomy and contiguity. We also show how to generalize the Pythagoreaninequality to orders different from 1, and we extend the known equivalencebetween channel capacity and minimax redundancy to continuous channel inputs(for all orders) and present several other minimax results.
arxiv-1206-2190 | Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation |  http://arxiv.org/abs/1206.2190  | author:Jian-feng Yan, Zhi-Qiang Liu, Yang Gao, Jia Zeng category:cs.LG published:2012-06-11 summary:This paper presents a novel communication-efficient parallel beliefpropagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA).Based on the synchronous belief propagation (BP) algorithm, we first develop aparallel belief propagation (PBP) algorithm on the parallel architecture.Because the extensive communication delay often causes a low efficiency ofparallel topic modeling, we further use Zipf's law to reduce the totalcommunication cost in PBP. Extensive experiments on different data setsdemonstrate that CE-PBP achieves a higher topic modeling accuracy and reducesmore than 80% communication cost than the state-of-the-art parallel Gibbssampling (PGS) algorithm.
arxiv-1206-2197 | Complex Orthogonal Matching Pursuit and Its Exact Recovery Conditions |  http://arxiv.org/abs/1206.2197  | author:Rong Fan, Qun Wan, Yipeng Liu, Hui Chen, Xiao Zhang category:cs.IT math.IT math.NA stat.ML published:2012-06-11 summary:In this paper, we present new results on using orthogonal matching pursuit(OMP), to solve the sparse approximation problem over redundant dictionariesfor complex cases (i.e., complex measurement vector, complex dictionary andcomplex additive white Gaussian noise (CAWGN)). A sufficient condition that OMPcan recover the optimal representation of an exactly sparse signal in thecomplex cases is proposed both in noiseless and bound Gaussian noise settings.Similar to exact recovery condition (ERC) results in real cases, we extend themto complex case and derivate the corresponding ERC in the paper. It leveragesthis theory to show that OMP succeed for k-sparse signal from a class ofcomplex dictionary. Besides, an application with geometrical theory ofdiffraction (GTD) model is presented for complex cases. Finally, simulationexperiments illustrate the validity of the theoretical analysis.
arxiv-1206-2248 | Fast Cross-Validation via Sequential Testing |  http://arxiv.org/abs/1206.2248  | author:Tammo Krueger, Danny Panknin, Mikio Braun category:cs.LG stat.ML I.2.6; I.2.8 published:2012-06-11 summary:With the increasing size of today's data sets, finding the right parameterconfiguration in model selection via cross-validation can be an extremelytime-consuming task. In this paper we propose an improved cross-validationprocedure which uses nonparametric testing coupled with sequential analysis todetermine the best parameter set on linearly increasing subsets of the data. Byeliminating underperforming candidates quickly and keeping promising candidatesas long as possible, the method speeds up the computation while preserving thecapability of the full cross-validation. Theoretical considerations underlinethe statistical power of our procedure. The experimental evaluation shows thatour method reduces the computation time by a factor of up to 120 compared to afull cross-validation with a negligible impact on the accuracy.
arxiv-1206-2372 | PRISMA: PRoximal Iterative SMoothing Algorithm |  http://arxiv.org/abs/1206.2372  | author:Francesco Orabona, Andreas Argyriou, Nathan Srebro category:math.OC cs.LG published:2012-06-11 summary:Motivated by learning problems including max-norm regularized matrixcompletion and clustering, robust PCA and sparse inverse covariance selection,we propose a novel optimization algorithm for minimizing a convex objectivewhich decomposes into three parts: a smooth part, a simple non-smooth Lipschitzpart, and a simple non-smooth non-Lipschitz part. We use a time variantsmoothing strategy that allows us to obtain a guarantee that does not depend onknowing in advance the total number of iterations nor a bound on the domain.
arxiv-1206-2058 | Dimension Reduction by Mutual Information Discriminant Analysis |  http://arxiv.org/abs/1206.2058  | author:Ali Shadvar category:cs.CV cs.IT cs.LG math.IT published:2012-06-10 summary:In the past few decades, researchers have proposed many discriminant analysis(DA) algorithms for the study of high-dimensional data in a variety ofproblems. Most DA algorithms for feature extraction are based ontransformations that simultaneously maximize the between-class scatter andminimize the withinclass scatter matrices. This paper presents a novel DAalgorithm for feature extraction using mutual information (MI). However, it isnot always easy to obtain an accurate estimation for high-dimensional MI. Inthis paper, we propose an efficient method for feature extraction that is basedon one-dimensional MI estimations. We will refer to this algorithm as mutualinformation discriminant analysis (MIDA). The performance of this proposedmethod was evaluated using UCI databases. The results indicate that MIDAprovides robust performance over different data sets with differentcharacteristics and that MIDA always performs better than, or at leastcomparable to, the best performing algorithms.
arxiv-1206-2010 | Temporal expression normalisation in natural language texts |  http://arxiv.org/abs/1206.2010  | author:Michele Filannino category:cs.CL cs.IR 68U02 D.3.2 published:2012-06-10 summary:Automatic annotation of temporal expressions is a research challenge of greatinterest in the field of information extraction. In this report, I describe anovel rule-based architecture, built on top of a pre-existing system, which isable to normalise temporal expressions detected in English texts. Gold standardtemporally-annotated resources are limited in size and this makes researchdifficult. The proposed system outperforms the state-of-the-art systems withrespect to TempEval-2 Shared Task (value attribute) and achieves substantiallybetter results with respect to the pre-existing system on top of which it hasbeen developed. I will also introduce a new free corpus consisting of 2822unique annotated temporal expressions. Both the corpus and the system arefreely available on-line.
arxiv-1206-2009 | Developing a model for a text database indexed pedagogically for teaching the Arabic language |  http://arxiv.org/abs/1206.2009  | author:Asma Boudhief, Mohsen Maraoui, Mounir Zrigui category:cs.CL published:2012-06-10 summary:In this memory we made the design of an indexing model for Arabic languageand adapting standards for describing learning resources used (the LOM andtheir application profiles) with learning conditions such as levels educationof students, their levels of understanding...the pedagogical context withtaking into account the repre-sentative elements of the text, text'slength,...in particular, we highlight the specificity of the Arabic languagewhich is a complex language, characterized by its flexion, its voyellation andits agglutination.
arxiv-1206-2061 | Comments on "On Approximating Euclidean Metrics by Weighted t-Cost Distances in Arbitrary Dimension" |  http://arxiv.org/abs/1206.2061  | author:M. Emre Celebi, Hassan A. Kingravi, Fatih Celiker category:cs.NA cs.CV G.1.2; I.5 published:2012-06-10 summary:Mukherjee (Pattern Recognition Letters, vol. 32, pp. 824-831, 2011) recentlyintroduced a class of distance functions called weighted t-cost distances thatgeneralize m-neighbor, octagonal, and t-cost distances. He proved that weightedt-cost distances form a family of metrics and derived an approximation for theEuclidean norm in $\mathbb{Z}^n$. In this note we compare this approximation totwo previously proposed Euclidean norm approximations and demonstrate that theempirical average errors given by Mukherjee are significantly optimistic in$\mathbb{R}^n$. We also propose a simple normalization scheme that improves theaccuracy of his approximation substantially with respect to both average andmaximum relative errors.
arxiv-1206-2068 | Revolvable Indoor Panoramas Using a Rectified Azimuthal Projection |  http://arxiv.org/abs/1206.2068  | author:Chamberlain Fong category:cs.CV math.DG published:2012-06-10 summary:We present an algorithm for converting an indoor spherical panorama into aphotograph with a simulated overhead view. The resulting image will have anextremely wide field of view covering up to 4{\pi} steradians of the sphericalpanorama. We argue that our method complements the stereographic projectioncommonly used in the "little planet" effect. The stereographic projection workswell in creating little planets of outdoor scenes; whereas our method is awell-suited counterpart for indoor scenes. The main innovation of our method isthe introduction of a novel azimuthal map projection that can smoothly blendbetween the stereographic projection and the Lambert azimuthal equal-areaprojection. Our projection has an adjustable parameter that allows one tocontrol and compromise between distortions in shape and distortions in sizewithin the projected panorama. This extra control parameter gives ourprojection the ability to produce superior results over the stereographicprojection.
arxiv-1206-1898 | A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function |  http://arxiv.org/abs/1206.1898  | author:Pedro A. Ortega, Jordi Grau-Moya, Tim Genewein, David Balduzzi, Daniel A. Braun category:stat.ML cs.AI math.ST stat.TH published:2012-06-09 summary:We propose a novel Bayesian approach to solve stochastic optimizationproblems that involve finding extrema of noisy, nonlinear functions. Previouswork has focused on representing possible functions explicitly, which leads toa two-step procedure of first, doing inference over the function space andsecond, finding the extrema of these functions. Here we skip the representationstep and directly model the distribution over extrema. To this end, we devise anon-parametric conjugate prior based on a kernel regressor. The resultingposterior distribution directly captures the uncertainty over the maximum ofthe unknown function. We illustrate the effectiveness of our model byoptimizing a noisy, high-dimensional, non-convex objective function.
arxiv-1206-1953 | Improvement of Loadability in Distribution System Using Genetic Algorithm |  http://arxiv.org/abs/1206.1953  | author:Mojtaba Nouri, Mahdi Bayat Mokhtari, Sohrab Mirsaeidi, Mohammad Reza Miveh category:cs.SY cs.NE published:2012-06-09 summary:Generally during recent decades due to development of power systems, themethods for delivering electrical energy to consumers, and because of voltagevariations is a very important problem, the power plants follow this criteria.The good solution for improving transfer and distribution of electrical powerthe majority of consumers prefer to use energy near the loads .So small unitsthat are connected to distribution system named "Decentralized Generation" or"Dispersed Generation". Deregulated in power industry and development ofrenewable energies are the most important factors in developing this type ofelectricity generation. Today DG has a key role in electrical distributionsystems. For example we can refer to improving reliability indices, improvementof stability and reduction of losses in power system. One of the key problemsin using DG's, is allocation of these sources in distribution networks. Loadability in distribution systems and its improvement has an effective role inthe operation of power systems. However, placement of distributed generationsources in order to improve the distribution system load ability index was notconsidered, we show DG placement and allocation with genetic algorithmoptimization method maximize load ability of power systems .This methodimplemented on the IEEE Standard bench marks. The results show theeffectiveness of the proposed algorithm .Another benefits of DG in selectedpositions are also studied and compared.
arxiv-1206-1971 | A Connectionist Network Approach to Find Numerical Solutions of Diophantine Equations |  http://arxiv.org/abs/1206.1971  | author:Siby Abraham, Sugata Sanyal, Mukund Sanglikar category:cs.NE published:2012-06-09 summary:The paper introduces a connectionist network approach to find numericalsolutions of Diophantine equations as an attempt to address the famousHilbert's tenth problem. The proposed methodology uses a three layer feedforward neural network with back propagation as sequential learning procedureto find numerical solutions of a class of Diophantine equations. It uses adynamically constructed network architecture where number of nodes in the inputlayer is chosen based on the number of variables in the equation. The powers ofthe given Diophantine equation are taken as input to the input layer. Thetraining of the network starts with initial random integral weights. Theweights are updated based on the back propagation of the error values at theoutput layer. The optimization of weights is augmented by adding a momentumfactor into the network. The optimized weights of the connection between theinput layer and the hidden layer are taken as numerical solution of the givenDiophantine equation. The procedure is validated using different DiophantineEquations of different number of variables and different powers.
arxiv-1206-1846 | Warped Mixtures for Nonparametric Cluster Shapes |  http://arxiv.org/abs/1206.1846  | author:Tomoharu Iwata, David Duvenaud, Zoubin Ghahramani category:stat.ML I.5.3 published:2012-06-08 summary:A mixture of Gaussians fit to a single curved or heavy-tailed cluster willreport that the data contains many clusters. To produce more appropriateclusterings, we introduce a model which warps a latent mixture of Gaussians toproduce nonparametric cluster shapes. The possibly low-dimensional latentmixture model allows us to summarize the properties of the high-dimensionalclusters (or density manifolds) describing the data. The number of manifolds,as well as the shape and dimension of each manifold is automatically inferred.We derive a simple inference scheme for this model which analyticallyintegrates out both the mixture parameters and the warping function. We showthat our model is effective for density estimation, performs better thaninfinite Gaussian mixture models at recovering the true number of clusters, andproduces interpretable summaries of high-dimensional datasets.
arxiv-1206-1874 | Multivariate Bernoulli distribution |  http://arxiv.org/abs/1206.1874  | author:Bin Dai, Shilin Ding, Grace Wahba category:stat.AP math.ST stat.ML stat.TH published:2012-06-08 summary:In this paper, we consider the multivariate Bernoulli distribution as a modelto estimate the structure of graphs with binary nodes. This distribution isdiscussed in the framework of the exponential family, and its statisticalproperties regarding independence of the nodes are demonstrated. Importantlythe model can estimate not only the main effects and pairwise interactionsamong the nodes but also is capable of modeling higher order interactions,allowing for the existence of complex clique effects. We compare themultivariate Bernoulli model with existing graphical inference models - theIsing model and the multivariate Gaussian model, where only the pairwiseinteractions are considered. On the other hand, the multivariate Bernoullidistribution has an interesting property in that independence anduncorrelatedness of the component random variables are equivalent. Both themarginal and conditional distributions of a subset of variables in themultivariate Bernoulli distribution still follow the multivariate Bernoullidistribution. Furthermore, the multivariate Bernoulli logistic model isdeveloped under generalized linear model theory by utilizing the canonical linkfunction in order to include covariate information on the nodes, edges andcliques. We also consider variable selection techniques such as LASSO in thelogistic model to impose sparsity structure on the graph. Finally, we discussextending the smoothing spline ANOVA approach to the multivariate Bernoullilogistic model to enable estimation of non-linear effects of the predictorvariables.
arxiv-1206-1552 | Performance Analysis of Unsymmetrical trimmed median as detector on image noises and its Fpga implementation |  http://arxiv.org/abs/1206.1552  | author:K. Vasanth, V. Jawahar Senthil Kumar category:cs.CV published:2012-06-07 summary:This Paper Analyze the performance of Unsymmetrical trimmed median, which isused as detector for the detection of impulse noise, Gaussian noise and mixednoise is proposed. The proposed algorithm uses a fixed 3x3 window for theincreasing noise densities. The pixels in the current window are arranged insorting order using a improved snake like sorting algorithm with reducedcomparator. The processed pixel is checked for the occurrence of outliers, ifthe absolute difference between processed pixels is greater than fixedthreshold. Under high noise densities the processed pixel is also noisy hencethe median is checked using the above procedure. if found true then the pixelis considered as noisy hence the corrupted pixel is replaced by the median ofthe current processing window. If median is also noisy then replace thecorrupted pixel with unsymmetrical trimmed median else if the pixel is termeduncorrupted and left unaltered. The proposed algorithm (PA) is tested onvarying detail images for various noises. The proposed algorithm effectivelyremoves the high density fixed value impulse noise, low density random valuedimpulse noise, low density Gaussian noise and lower proportion of mixed noise.The proposed algorithm is targeted on Xc3e5000-5fg900 FPGA using Xilinx 7.1compiler version which requires less number of slices, optimum speed and lowpower when compared to the other median finding architectures.
arxiv-1206-1518 | Off-Line Arabic Handwriting Character Recognition Using Word Segmentation |  http://arxiv.org/abs/1206.1518  | author:Manal A. Abdullah, Lulwah M. Al-Harigy, Hanadi H. Al-Fraidi category:cs.CV published:2012-06-07 summary:The ultimate aim of handwriting recognition is to make computers able to readand/or authenticate human written texts, with a performance comparable to oreven better than that of humans. Reading means that the computer is given apiece of handwriting and it provides the electronic transcription of that (e.g.in ASCII format). Two types of handwriting: on-line and offline. The mostimportant purpose of off-line handwriting recognition is in protection systemsand authentication. Arabic Handwriting scripts are much more complicated incomparison to Latin scripts. This paper introduces a simple and novelmethodology to authenticate Arabic handwriting characters. Reaching our aim, webuilt our own character database. The research methodology depends on twostages: The first is character extraction where preprocessing the word and thenapply segmentation process to obtain the character. The second is the characterrecognition by matching the characters comprising the word with the letters inthe database. Our results ensure character recognition with 81%. We eliminateFAR by using similarity percent between 45-55%. Our research is coded usingMATLAB.
arxiv-1206-1623 | Proximal Newton-type methods for minimizing composite functions |  http://arxiv.org/abs/1206.1623 | author:Jason D. Lee, Yuekai Sun, Michael A. Saunders category:stat.ML cs.DS cs.LG cs.NA math.OC published:2012-06-07 summary:We generalize Newton-type methods for minimizing smooth functions to handle asum of two convex functions: a smooth function and a nonsmooth function with asimple proximal mapping. We show that the resulting proximal Newton-typemethods inherit the desirable convergence behavior of Newton-type methods forminimizing smooth functions, even when search directions are computedinexactly. Many popular methods tailored to problems arising in bioinformatics,signal processing, and statistical learning are special cases of proximalNewton-type methods, and our analysis yields new convergence results for someof these methods.
arxiv-1206-1515 | Optimizing Face Recognition Using PCA |  http://arxiv.org/abs/1206.1515  | author:Manal Abdullah, Majda Wazzan, Sahar Bo-saeed category:cs.CV published:2012-06-07 summary:Principle Component Analysis PCA is a classical feature extraction and datarepresentation technique widely used in pattern recognition. It is one of themost successful techniques in face recognition. But it has drawback of highcomputational especially for big size database. This paper conducts a study tooptimize the time complexity of PCA (eigenfaces) that does not affects therecognition performance. The authors minimize the participated eigenvectorswhich consequently decreases the computational time. A comparison is done tocompare the differences between the recognition time in the original algorithmand in the enhanced algorithm. The performance of the original and the enhancedproposed algorithm is tested on face94 face database. Experimental results showthat the recognition time is reduced by 35% by applying our proposed enhancedalgorithm. DET Curves are used to illustrate the experimental results.
arxiv-1206-1529 | Sparse projections onto the simplex |  http://arxiv.org/abs/1206.1529  | author:Anastasios Kyrillidis, Stephen Becker, Volkan Cevher and, Christoph Koch category:cs.LG stat.ML published:2012-06-07 summary:Most learning methods with rank or sparsity constraints use convexrelaxations, which lead to optimization with the nuclear norm or the$\ell_1$-norm. However, several important learning applications cannot benefitfrom this approach as they feature these convex norms as constraints inaddition to the non-convex rank and sparsity constraints. In this setting, wederive efficient sparse projections onto the simplex and its extension, andillustrate how to use them to solve high-dimensional learning problems inquantum tomography, sparse density estimation and portfolio selection withnon-convex constraints.
arxiv-1206-1443 | On applying Neuro - Computing in E-com Domain |  http://arxiv.org/abs/1206.1443  | author:Asif Perwej category:cs.NE published:2012-06-07 summary:Prior studies have generally suggested that Artificial Neural Networks (ANNs)are superior to conventional statistical models in predicting consumer buyingbehavior. There are, however, contradicting findings which raise question overusefulness of ANNs. This paper discusses development of three neural networksfor modeling consumer e-commerce behavior and compares the findings toequivalent logistic regression models. The results showed that ANNs predicte-commerce adoption slightly more accurately than logistic models but this ishardly justifiable given the added complexity. Further, ANNs seem to be highlyadaptive, particularly when a small sample is coupled with a large number ofnodes in hidden layers which, in turn, limits the neural networks'generalisability.
arxiv-1206-1557 | Soil Data Analysis Using Classification Techniques and Soil Attribute Prediction |  http://arxiv.org/abs/1206.1557  | author:Jay Gholap, Anurag Ingole, Jayesh Gohil, Shailesh Gargade, Vahida Attar category:cs.AI stat.AP stat.ML published:2012-06-07 summary:Agricultural research has been profited by technical advances such asautomation, data mining. Today, data mining is used in a vast areas and manyoff-the-shelf data mining system products and domain specific data miningapplication soft wares are available, but data mining in agricultural soildatasets is a relatively a young research field. The large amounts of data thatare nowadays virtually harvested along with the crops have to be analyzed andshould be used to their full extent. This research aims at analysis of soildataset using data mining techniques. It focuses on classification of soilusing various algorithms available. Another important purpose is to predictuntested attributes using regression technique, and implementation of automatedsoil sample classification.
arxiv-1206-1386 | Robust subspace recovery by Tyler's M-estimator |  http://arxiv.org/abs/1206.1386  | author:Teng Zhang category:stat.ML published:2012-06-07 summary:This paper considers the problem of robust subspace recovery: given a set of$N$ points in $\mathbb{R}^D$, if many lie in a $d$-dimensional subspace, thencan we recover the underlying subspace? We show that Tyler's M-estimator can beused to recover the underlying subspace, if the percentage of the inliers islarger than $d/D$ and the data points lie in general position. Empirically,Tyler's M-estimator compares favorably with other convex subspace recoveryalgorithms in both simulations and experiments on real data sets.
arxiv-1206-1402 | A New Greedy Algorithm for Multiple Sparse Regression |  http://arxiv.org/abs/1206.1402  | author:Ali Jalali, Sujay Sanghavi category:stat.ML cs.LG published:2012-06-07 summary:This paper proposes a new algorithm for multiple sparse regression in highdimensions, where the task is to estimate the support and values of several(typically related) sparse vectors from a few noisy linear measurements. Ouralgorithm is a "forward-backward" greedy procedure that -- uniquely -- operateson two distinct classes of objects. In particular, we organize our targetsparse vectors as a matrix; our algorithm involves iterative addition andremoval of both (a) individual elements, and (b) entire rows (corresponding toshared features), of the matrix. Analytically, we establish that our algorithm manages to recover the supports(exactly) and values (approximately) of the sparse vectors, under assumptionssimilar to existing approaches based on convex optimization. However, ouralgorithm has a much smaller computational complexity. Perhaps mostinterestingly, it is seen empirically to require visibly fewer samples. Oursrepresents the first attempt to extend greedy algorithms to the class of modelsthat can only/best be represented by a combination of component structuralassumptions (sparse and group-sparse, in our case).
arxiv-1206-1106 | No More Pesky Learning Rates |  http://arxiv.org/abs/1206.1106  | author:Tom Schaul, Sixin Zhang, Yann LeCun category:stat.ML cs.LG published:2012-06-06 summary:The performance of stochastic gradient descent (SGD) depends critically onhow learning rates are tuned and decreased over time. We propose a method toautomatically adjust multiple learning rates so as to minimize the expectederror at any one time. The method relies on local gradient variations acrosssamples. In our approach, learning rates can increase as well as decrease,making it suitable for non-stationary problems. Using a number of convex andnon-convex learning tasks, we show that the resulting algorithm matches theperformance of SGD or other adaptive approaches with their best settingsobtained through systematic search, and effectively removes the need forlearning rate tuning.
arxiv-1206-1309 | Evidence-Based Robust Design of Deflection Actions for Near Earth Objects |  http://arxiv.org/abs/1206.1309  | author:Federico Zuiani, Massimiliano Vasile, Alison Gibbings category:cs.CE cs.NE math.OC stat.AP published:2012-06-06 summary:This paper presents a novel approach to the robust design of deflectionactions for Near Earth Objects (NEO). In particular, the case of deflection bymeans of Solar-pumped Laser ablation is studied here in detail. The basic ideabehind Laser ablation is that of inducing a sublimation of the NEO surface,which produces a low thrust thereby slowly deviating the asteroid from itsinitial Earth threatening trajectory. This work investigates the integrateddesign of the Space-based Laser system and the deflection action generated bylaser ablation under uncertainty. The integrated design is formulated as amulti-objective optimisation problem in which the deviation is maximised andthe total system mass is minimised. Both the model for the estimation of thethrust produced by surface laser ablation and the spacecraft system model areassumed to be affected by epistemic uncertainties (partial or complete lack ofknowledge). Evidence Theory is used to quantify these uncertainties andintroduce them in the optimisation process. The propagation of the trajectoryof the NEO under the laser-ablation action is performed with a novel approachbased on an approximated analytical solution of Gauss' Variational Equations.An example of design of the deflection of asteroid Apophis with a swarm ofspacecraft is presented.
arxiv-1206-1275 | Alternating Direction Methods for Latent Variable Gaussian Graphical Model Selection |  http://arxiv.org/abs/1206.1275  | author:Shiqian Ma, Lingzhou Xue, Hui Zou category:math.OC stat.ML published:2012-06-06 summary:Chandrasekaran, Parrilo and Willsky (2010) proposed a convex optimizationproblem to characterize graphical model selection in the presence of unobservedvariables. This convex optimization problem aims to estimate an inversecovariance matrix that can be decomposed into a sparse matrix minus a low-rankmatrix from sample data. Solving this convex optimization problem is verychallenging, especially for large problems. In this paper, we propose twoalternating direction methods for solving this problem. The first method is toapply the classical alternating direction method of multipliers to solve theproblem as a consensus problem. The second method is a proximal gradient basedalternating direction method of multipliers. Our methods exploit and takeadvantage of the special structure of the problem and thus can solve largeproblems very efficiently. Global convergence result is established for theproposed methods. Numerical results on both synthetic data and gene expressiondata show that our methods usually solve problems with one million variables inone to two minutes, and are usually five to thirty five times faster than astate-of-the-art Newton-CG proximal point algorithm.
arxiv-1206-1305 | MACS: An Agent-Based Memetic Multiobjective Optimization Algorithm Applied to Space Trajectory Design |  http://arxiv.org/abs/1206.1305  | author:Massimiliano Vasile, Federico Zuiani category:cs.CE cs.NE math.OC published:2012-06-06 summary:This paper presents an algorithm for multiobjective optimization that blendstogether a number of heuristics. A population of agents combines heuristicsthat aim at exploring the search space both globally and in a neighborhood ofeach agent. These heuristics are complemented with a combination of a local andglobal archive. The novel agent- based algorithm is tested at first on a set ofstandard problems and then on three specific problems in space trajectorydesign. Its performance is compared against a number of state-of-the-artmultiobjective optimisation algorithms that use the Pareto dominance asselection criterion: NSGA-II, PAES, MOPSO, MTS. The results demonstrate thatthe agent-based search can identify parts of the Pareto set that the otheralgorithms were not able to capture. Furthermore, convergence is statisticallybetter although the variance of the results is in some cases higher.
arxiv-1206-1270 | Factoring nonnegative matrices with linear programs |  http://arxiv.org/abs/1206.1270  | author:Victor Bittorf, Benjamin Recht, Christopher Re, Joel A. Tropp category:math.OC cs.LG stat.ML published:2012-06-06 summary:This paper describes a new approach, based on linear programming, forcomputing nonnegative matrix factorizations (NMFs). The key idea is adata-driven model for the factorization where the most salient features in thedata are used to express the remaining features. More precisely, given a datamatrix X, the algorithm identifies a matrix C such that X approximately equalsCX and some linear constraints. The constraints are chosen to ensure that thematrix C selects features; these features can then be used to find a low-rankNMF of X. A theoretical analysis demonstrates that this approach has guaranteessimilar to those of the recent NMF algorithm of Arora et al. (2012). Incontrast with this earlier work, the proposed method extends to more generalnoise models and leads to efficient, scalable algorithms. Experiments withsynthetic and real datasets provide evidence that the new approach is alsosuperior in practice. An optimized C++ implementation can factor amultigigabyte matrix in a matter of minutes.
arxiv-1206-1147 | Memory-Efficient Topic Modeling |  http://arxiv.org/abs/1206.1147  | author:Jia Zeng, Zhi-Qiang Liu, Xiao-Qin Cao category:cs.LG cs.IR published:2012-06-06 summary:As one of the simplest probabilistic topic modeling techniques, latentDirichlet allocation (LDA) has found many important applications in textmining, computer vision and computational biology. Recent training algorithmsfor LDA can be interpreted within a unified message passing framework. However,message passing requires storing previous messages with a large amount ofmemory space, increasing linearly with the number of documents or the number oftopics. Therefore, the high memory usage is often a major problem for topicmodeling of massive corpora containing a large number of topics. To reduce thespace complexity, we propose a novel algorithm without storing previousmessages for training LDA: tiny belief propagation (TBP). The basic idea of TBPrelates the message passing algorithms with the non-negative matrixfactorization (NMF) algorithms, which absorb the message updating into themessage passing process, and thus avoid storing previous messages. Experimentalresults on four large data sets confirm that TBP performs comparably well oreven better than current state-of-the-art training algorithms for LDA but witha much less memory consumption. TBP can do topic modeling when massive corporacannot fit in the computer memory, for example, extracting thematic topics from7 GB PUBMED corpora on a common desktop computer with 2GB memory.
arxiv-1206-1121 | Comparison of the C4.5 and a Naive Bayes Classifier for the Prediction of Lung Cancer Survivability |  http://arxiv.org/abs/1206.1121  | author:George Dimitoglou, James A. Adams, Carol M. Jim category:cs.LG published:2012-06-06 summary:Numerous data mining techniques have been developed to extract informationand identify patterns and predict trends from large data sets. In this study,two classification techniques, the J48 implementation of the C4.5 algorithm anda Naive Bayes classifier are applied to predict lung cancer survivability froman extensive data set with fifteen years of patient records. The purpose of theproject is to verify the predictive effectiveness of the two techniques onreal, historical data. Besides the performance outcome that renders J48marginally better than the Naive Bayes technique, there is a detaileddescription of the data and the required pre-processing activities. Theperformance results confirm expectations while some of the issues that appearedduring experimentation, underscore the value of having domain-specificunderstanding to leverage any domain-specific characteristics inherent in thedata.
arxiv-1206-1208 | Cumulative Step-size Adaptation on Linear Functions: Technical Report |  http://arxiv.org/abs/1206.1208  | author:Alexandre Adrien Chotard, Anne Auger, Nikolaus Hansen category:cs.LG published:2012-06-06 summary:The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation,where the step size is adapted measuring the length of a so-called cumulativepath. The cumulative path is a combination of the previous steps realized bythe algorithm, where the importance of each step decreases with time. Thisarticle studies the CSA-ES on composites of strictly increasing with affinelinear functions through the investigation of its underlying Markov chains.Rigorous results on the change and the variation of the step size are derivedwith and without cumulation. The step-size diverges geometrically fast in mostcases. Furthermore, the influence of the cumulation parameter is studied.
arxiv-1206-1088 | Bayesian Structure Learning for Markov Random Fields with a Spike and Slab Prior |  http://arxiv.org/abs/1206.1088  | author:Yutian Chen, Max Welling category:stat.ML cs.LG published:2012-06-05 summary:In recent years a number of methods have been developed for automaticallylearning the (sparse) connectivity structure of Markov Random Fields. Thesemethods are mostly based on L1-regularized optimization which has a number ofdisadvantages such as the inability to assess model uncertainty and expensivecross-validation to find the optimal regularization parameter. Moreover, themodel's predictive performance may degrade dramatically with a suboptimal valueof the regularization parameter (which is sometimes desirable to inducesparseness). We propose a fully Bayesian approach based on a "spike and slab"prior (similar to L0 regularization) that does not suffer from theseshortcomings. We develop an approximate MCMC method combining Langevin dynamicsand reversible jump MCMC to conduct inference in this model. Experiments showthat the proposed model learns a good combination of the structure andparameter values without the need for separate hyper-parameter tuning.Moreover, the model's predictive performance is much more robust than L1-basedmethods with hyper-parameter settings that induce highly sparse modelstructures.
arxiv-1206-0823 | Orthogonal Matching Pursuit with Noisy and Missing Data: Low and High Dimensional Results |  http://arxiv.org/abs/1206.0823  | author:Yudong Chen, Constantine Caramanis category:math.ST cs.IT math.IT stat.ML stat.TH published:2012-06-05 summary:Many models for sparse regression typically assume that the covariates areknown completely, and without noise. Particularly in high-dimensionalapplications, this is often not the case. This paper develops efficientOMP-like algorithms to deal with precisely this setting. Our algorithms are asefficient as OMP, and improve on the best-known results for missing and noisydata in regression, both in the high-dimensional setting where we seek torecover a sparse vector from only a few measurements, and in the classicallow-dimensional setting where we recover an unstructured regressor. In thehigh-dimensional setting, our support-recovery algorithm requires no knowledgeof even the statistics of the noise. Along the way, we also obtain improvedperformance guarantees for OMP for the standard sparse regression problem withGaussian noise.
arxiv-1206-1074 | Memetic Artificial Bee Colony Algorithm for Large-Scale Global Optimization |  http://arxiv.org/abs/1206.1074  | author:Iztok Fister, Iztok Fister Jr., Janez Brest, Viljem Å½umer category:cs.NE cs.AI published:2012-06-05 summary:Memetic computation (MC) has emerged recently as a new paradigm of efficientalgorithms for solving the hardest optimization problems. On the other hand,artificial bees colony (ABC) algorithms demonstrate good performances whensolving continuous and combinatorial optimization problems. This study tries touse these technologies under the same roof. As a result, a memetic ABC (MABC)algorithm has been developed that is hybridized with two local searchheuristics: the Nelder-Mead algorithm (NMA) and the random walk with directionexploitation (RWDE). The former is attended more towards exploration, while thelatter more towards exploitation of the search space. The stochastic adaptationrule was employed in order to control the balancing between exploration andexploitation. This MABC algorithm was applied to a Special suite on Large ScaleContinuous Global Optimization at the 2012 IEEE Congress on EvolutionaryComputation. The obtained results the MABC are comparable with the results ofDECC-G, DECC-G*, and MLCC.
arxiv-1206-0937 | Detecting Activations over Graphs using Spanning Tree Wavelet Bases |  http://arxiv.org/abs/1206.0937  | author:James Sharpnack, Akshay Krishnamurthy, Aarti Singh category:stat.ML cs.IT math.IT math.ST stat.TH published:2012-06-05 summary:We consider the detection of activations over graphs under Gaussian noise,where signals are piece-wise constant over the graph. Despite the wideapplicability of such a detection algorithm, there has been little success inthe development of computationally feasible methods with proveable theoreticalguarantees for general graph topologies. We cast this as a hypothesis testingproblem, and first provide a universal necessary condition for asymptoticdistinguishability of the null and alternative hypotheses. We then introducethe spanning tree wavelet basis over graphs, a localized basis that reflectsthe topology of the graph, and prove that for any spanning tree, this approachcan distinguish null from alternative in a low signal-to-noise regime. Lastly,we improve on this result and show that using the uniform spanning tree in thebasis construction yields a randomized test with stronger theoreticalguarantees that in many cases matches our necessary conditions. Specifically,we obtain near-optimal performance in edge transitive graphs, $k$-nearestneighbor graphs, and $\epsilon$-graphs.
arxiv-1206-0855 | A Mixed Observability Markov Decision Process Model for Musical Pitch |  http://arxiv.org/abs/1206.0855  | author:Pouyan Rafiei Fard, Keyvan Yahya category:cs.AI cs.LG published:2012-06-05 summary:Partially observable Markov decision processes have been widely used toprovide models for real-world decision making problems. In this paper, we willprovide a method in which a slightly different version of them called Mixedobservability Markov decision process, MOMDP, is going to join with ourproblem. Basically, we aim at offering a behavioural model for interaction ofintelligent agents with musical pitch environment and we will show that howMOMDP can shed some light on building up a decision making model for musicalpitch conveniently.
arxiv-1206-1069 | Concepts and Their Dynamics: A Quantum-Theoretic Modeling of Human Thought |  http://arxiv.org/abs/1206.1069  | author:Diederik Aerts, Liane Gabora, Sandro Sozzo category:cs.AI cs.CL quant-ph published:2012-06-05 summary:We analyze different aspects of our quantum modeling approach of humanconcepts, and more specifically focus on the quantum effects of contextuality,interference, entanglement and emergence, illustrating how each of them makesits appearance in specific situations of the dynamics of human concepts andtheir combinations. We point out the relation of our approach, which is basedon an ontology of a concept as an entity in a state changing under influence ofa context, with the main traditional concept theories, i.e. prototype theory,exemplar theory and theory theory. We ponder about the question why quantumtheory performs so well in its modeling of human concepts, and shed light onthis question by analyzing the role of complex amplitudes, showing how theyallow to describe interference in the statistics of measurement outcomes, whilein the traditional theories statistics of outcomes originates in classicalprobability weights, without the possibility of interference. The relevance ofcomplex numbers, the appearance of entanglement, and the role of Fock space inexplaining contextual emergence, all as unique features of the quantummodeling, are explicitly revealed in this paper by analyzing human concepts andtheir dynamics.
arxiv-1206-0985 | Nearly optimal solutions for the Chow Parameters Problem and low-weight approximation of halfspaces |  http://arxiv.org/abs/1206.0985  | author:Anindya De, Ilias Diakonikolas, Vitaly Feldman, Rocco A. Servedio category:cs.CC cs.DS cs.LG published:2012-06-05 summary:The \emph{Chow parameters} of a Boolean function $f: \{-1,1\}^n \to \{-1,1\}$are its $n+1$ degree-0 and degree-1 Fourier coefficients. It has been knownsince 1961 (Chow, Tannenbaum) that the (exact values of the) Chow parameters ofany linear threshold function $f$ uniquely specify $f$ within the space of allBoolean functions, but until recently (O'Donnell and Servedio) nothing wasknown about efficient algorithms for \emph{reconstructing} $f$ (exactly orapproximately) from exact or approximate values of its Chow parameters. Werefer to this reconstruction problem as the \emph{Chow Parameters Problem.} Our main result is a new algorithm for the Chow Parameters Problem which,given (sufficiently accurate approximations to) the Chow parameters of anylinear threshold function $f$, runs in time $\tilde{O}(n^2)\cdot(1/\eps)^{O(\log^2(1/\eps))}$ and with high probability outputs arepresentation of an LTF $f'$ that is $\eps$-close to $f$. The only previousalgorithm (O'Donnell and Servedio) had running time $\poly(n) \cdot2^{2^{\tilde{O}(1/\eps^2)}}.$ As a byproduct of our approach, we show that for any linear thresholdfunction $f$ over $\{-1,1\}^n$, there is a linear threshold function $f'$ whichis $\eps$-close to $f$ and has all weights that are integers at most $\sqrt{n}\cdot (1/\eps)^{O(\log^2(1/\eps))}$. This significantly improves the bestprevious result of Diakonikolas and Servedio which gave a $\poly(n) \cdot2^{\tilde{O}(1/\eps^{2/3})}$ weight bound, and is close to the known lowerbound of $\max\{\sqrt{n},$ $(1/\eps)^{\Omega(\log \log (1/\eps))}\}$ (Goldberg,Servedio). Our techniques also yield improved algorithms for related problemsin learning theory.
arxiv-1206-1066 | Hedge detection as a lens on framing in the GMO debates: A position paper |  http://arxiv.org/abs/1206.1066  | author:Eunsol Choi, Chenhao Tan, Lillian Lee, Cristian Danescu-Niculescu-Mizil, Jennifer Spindel category:cs.CL published:2012-06-05 summary:Understanding the ways in which participants in public discussions frametheir arguments is important in understanding how public opinion is formed. Inthis paper, we adopt the position that it is time for morecomputationally-oriented research on problems involving framing. In theinterests of furthering that goal, we propose the following specific,interesting and, we believe, relatively accessible question: In the controversyregarding the use of genetically-modified organisms (GMOs) in agriculture, dopro- and anti-GMO articles differ in whether they choose to adopt a"scientific" tone? Prior work on the rhetoric and sociology of science suggests that hedging maydistinguish popular-science text from text written by professional scientistsfor their colleagues. We propose a detailed approach to studying whether hedgedetection can be used to understanding scientific framing in the GMO debates,and provide corpora to facilitate this study. Some of our preliminary analysessuggest that hedges occur less frequently in scientific discourse than inpopular text, a finding that contradicts prior assertions in the literature. Wehope that our initial work and data will encourage others to pursue thispromising line of inquiry.
arxiv-1206-0663 | Multi-Sparse Signal Recovery for Compressive Sensing |  http://arxiv.org/abs/1206.0663  | author:Yipeng Liu, Ivan Gligorijevic, Vladimir Matic, Maarten De Vos, Sabine Van Huffel category:cs.IT cs.SY math.IT math.OC stat.ML published:2012-06-04 summary:Signal recovery is one of the key techniques of Compressive sensing (CS). Itreconstructs the original signal from the linear sub-Nyquist measurements.Classical methods exploit the sparsity in one domain to formulate the L0 normoptimization. Recent investigation shows that some signals are sparse inmultiple domains. To further improve the signal reconstruction performance, wecan exploit this multi-sparsity to generate a new convex programming model. Thelatter is formulated with multiple sparsity constraints in multiple domains andthe linear measurement fitting constraint. It improves signal recoveryperformance by additional a priori information. Since some EMG signals exhibitsparsity both in time and frequency domains, we take them as example innumerical experiments. Results show that the newly proposed method achievesbetter performance for multi-sparse signals.
arxiv-1206-0730 | Theoretical foundation for CMA-ES from information geometric perspective |  http://arxiv.org/abs/1206.0730  | author:Youhei Akimoto, Yuichi Nagata, Isao Ono, Shigenobu Kobayashi category:cs.NE published:2012-06-04 summary:This paper explores the theoretical basis of the covariance matrix adaptationevolution strategy (CMA-ES) from the information geometry viewpoint. To establish a theoretical foundation for the CMA-ES, we focus on a geometricstructure of a Riemannian manifold of probability distributions equipped withthe Fisher metric. We define a function on the manifold which is theexpectation of fitness over the sampling distribution, and regard the goal ofupdate of the parameters of sampling distribution in the CMA-ES as maximizationof the expected fitness. We investigate the steepest ascent learning for theexpected fitness maximization, where the steepest ascent direction is given bythe natural gradient, which is the product of the inverse of the Fisherinformation matrix and the conventional gradient of the function. Our first result is that we can obtain under some types of parameterizationof multivariate normal distribution the natural gradient of the expectedfitness without the need for inversion of the Fisher information matrix. Wefind that the update of the distribution parameters in the CMA-ES is the sameas natural gradient learning for expected fitness maximization. Our secondresult is that we derive the range of learning rates such that a step in thedirection of the exact natural gradient improves the parameters in the expectedfitness. We see from the close relation between the CMA-ES and natural gradientlearning that the default setting of learning rates in the CMA-ES seemssuitable in terms of monotone improvement in expected fitness. Then, we discussthe relation to the expectation-maximization framework and provide aninformation geometric interpretation of the CMA-ES.
arxiv-1206-0773 | Changepoint Detection over Graphs with the Spectral Scan Statistic |  http://arxiv.org/abs/1206.0773  | author:James Sharpnack, Alessandro Rinaldo, Aarti Singh category:math.ST cs.IT math.IT stat.ML stat.TH published:2012-06-04 summary:We consider the change-point detection problem of deciding, based on noisymeasurements, whether an unknown signal over a given graph is constant or isinstead piecewise constant over two connected induced subgraphs of relativelylow cut size. We analyze the corresponding generalized likelihood ratio (GLR)statistics and relate it to the problem of finding a sparsest cut in a graph.We develop a tractable relaxation of the GLR statistic based on thecombinatorial Laplacian of the graph, which we call the spectral scanstatistic, and analyze its properties. We show how its performance as a testingprocedure depends directly on the spectrum of the graph, and use this result toexplicitly derive its asymptotic properties on few significant graphtopologies. Finally, we demonstrate both theoretically and by simulations thatthe spectral scan statistic can outperform naive testing procedures based onedge thresholding and $\chi^2$ testing.
arxiv-1206-0771 | Topological graph clustering with thin position |  http://arxiv.org/abs/1206.0771  | author:Jesse Johnson category:math.GT cs.LG stat.ML published:2012-06-04 summary:A clustering algorithm partitions a set of data points into smaller sets(clusters) such that each subset is more tightly packed than the whole. Manyapproaches to clustering translate the vector data into a graph with edgesreflecting a distance or similarity metric on the points, then look for highlyconnected subgraphs. We introduce such an algorithm based on ideas borrowedfrom the topological notion of thin position for knots and 3-dimensionalmanifolds.
arxiv-1206-0500 | Binary hidden Markov models and varieties |  http://arxiv.org/abs/1206.0500  | author:Andrew J. Critch category:math.AG stat.ML 14Q15 published:2012-06-03 summary:The technological applications of hidden Markov models have been extremelydiverse and successful, including natural language processing, gesturerecognition, gene sequencing, and Kalman filtering of physical measurements.HMMs are highly non-linear statistical models, and just as linear models areamenable to linear algebraic techniques, non-linear models are amenable tocommutative algebra and algebraic geometry. This paper closely examines HMMs in which all the hidden random variables arebinary. Its main contributions are (1) a birational parametrization for everysuch HMM, with an explicit inverse for recovering the hidden parameters interms of observables, (2) a semialgebraic model membership test for every suchHMM, and (3) minimal defining equations for the 4-node fully binary model,comprising 21 quadrics and 29 cubics, which were computed using Grobner basesin the cumulant coordinates of Sturmfels and Zwiernik. The new model parametersin (1) are rationally identifiable in the sense of Sullivant, Garcia-Puente,and Spielvogel, and each model's Zariski closure is therefore a rationalprojective variety of dimension 5. Grobner basis computations for the model andits graph are found to be considerably faster using these parameters. In thecase of two hidden states, item (2) supersedes a previous algorithm ofSchonhuth which is only generically defined, and the defining equations (3)yield new invariants for HMMs of all lengths $\geq 4$. Such invariants havebeen used successfully in model selection problems in phylogenetics, and onecan hope for similar applications in the case of HMMs.
arxiv-1206-0335 | A Route Confidence Evaluation Method for Reliable Hierarchical Text Categorization |  http://arxiv.org/abs/1206.0335  | author:Nima Hatami, Camelia Chira, Giuliano Armano category:cs.IR cs.LG published:2012-06-02 summary:Hierarchical Text Categorization (HTC) is becoming increasingly importantwith the rapidly growing amount of text data available in the World Wide Web.Among the different strategies proposed to cope with HTC, the Local Classifierper Node (LCN) approach attains good performance by mirroring the underlyingclass hierarchy while enforcing a top-down strategy in the testing step.However, the problem of embedding hierarchical information (parent-childrelationship) to improve the performance of HTC systems still remains open. Aconfidence evaluation method for a selected route in the hierarchy is proposedto evaluate the reliability of the final candidate labels in an HTC system. Inorder to take into account the information embedded in the hierarchy, weightfactors are used to take into account the importance of each level. Anacceptance/rejection strategy in the top-down decision making process isproposed, which improves the overall categorization accuracy by rejecting a fewpercentage of samples, i.e., those with low reliability score. Experimentalresults on the Reuters benchmark dataset (RCV1- v2) confirm the effectivenessof the proposed method, compared to other state-of-the art HTC methods.
arxiv-1206-0387 | When Does a Mixture of Products Contain a Product of Mixtures? |  http://arxiv.org/abs/1206.0387  | author:Guido F. Montufar, Jason Morton category:stat.ML math.CO G.3 published:2012-06-02 summary:We derive relations between theoretical properties of restricted Boltzmannmachines (RBMs), popular machine learning models which form the building blocksof deep learning models, and several natural notions from discrete mathematicsand convex geometry. We give implications and equivalences relatingRBM-representable probability distributions, perfectly reconstructible inputs,Hamming modes, zonotopes and zonosets, point configurations in hyperplanearrangements, linear threshold codes, and multi-covering numbers of hypercubes.As a motivating application, we prove results on the relative representationalpower of mixtures of product distributions and products of mixtures of pairs ofproduct distributions (RBMs) that formally justify widely held intuitions aboutdistributed representations. In particular, we show that a mixture of productsrequiring an exponentially larger number of parameters is needed to representthe probability distributions which can be obtained as products of mixtures.
arxiv-1206-0338 | Poisson noise reduction with non-local PCA |  http://arxiv.org/abs/1206.0338  | author:Joseph Salmon, Zachary Harmany, Charles-Alban Deledalle, Rebecca Willett category:cs.CV cs.LG stat.CO published:2012-06-02 summary:Photon-limited imaging arises when the number of photons collected by asensor array is small relative to the number of detector elements. Photonlimitations are an important concern for many applications such as spectralimaging, night vision, nuclear medicine, and astronomy. Typically a Poissondistribution is used to model these observations, and the inherentheteroscedasticity of the data combined with standard noise removal methodsyields significant artifacts. This paper introduces a novel denoising algorithmfor photon-limited images which combines elements of dictionary learning andsparse patch-based representations of images. The method employs both anadaptation of Principal Component Analysis (PCA) for Poisson noise and recentlydeveloped sparsity-regularized convex optimization algorithms forphoton-limited images. A comprehensive empirical evaluation of the proposedmethod helps characterize the performance of this approach relative to otherstate-of-the-art denoising methods. The results reveal that, despite itsconceptual simplicity, Poisson PCA-based denoising appears to be highlycompetitive in very low light regimes.
arxiv-1206-0392 | Greedy approximation in convex optimization |  http://arxiv.org/abs/1206.0392  | author:V. N. Temlyakov category:stat.ML math.OC 41A65 published:2012-06-02 summary:We study sparse approximate solutions to convex optimization problems. It isknown that in many engineering applications researchers are interested in anapproximate solution of an optimization problem as a linear combination ofelements from a given system of elements. There is an increasing interest inbuilding such sparse approximate solutions using different greedy-typealgorithms. The problem of approximation of a given element of a Banach spaceby linear combinations of elements from a given system (dictionary) is wellstudied in nonlinear approximation theory. At a first glance the settings ofapproximation and optimization problems are very different. In theapproximation problem an element is given and our task is to find a sparseapproximation of it. In optimization theory an energy function is given and weshould find an approximate sparse solution to the minimization problem. Itturns out that the same technique can be used for solving both problems. Weshow how the technique developed in nonlinear approximation theory, inparticular, the greedy approximation technique can be adjusted for finding asparse solution of an optimization problem.
arxiv-1206-0377 | Automated Word Puzzle Generation via Topic Dictionaries |  http://arxiv.org/abs/1206.0377  | author:Balazs Pinter, Gyula Voros, Zoltan Szabo, Andras Lorincz category:cs.CL math.CO 68T50, 15A23 published:2012-06-02 summary:We propose a general method for automated word puzzle generation. Contrary toprevious approaches in this novel field, the presented method does not rely onhighly structured datasets obtained with serious human annotation effort: itonly needs an unstructured and unannotated corpus (i.e., document collection)as input. The method builds upon two additional pillars: (i) a topic model,which induces a topic dictionary from the input corpus (examples include e.g.,latent semantic analysis, group-structured dictionaries or latent Dirichletallocation), and (ii) a semantic similarity measure of word pairs. Our methodcan (i) generate automatically a large number of proper word puzzles ofdifferent types, including the odd one out, choose the related word andseparate the topics puzzle. (ii) It can easily create domain-specific puzzlesby replacing the corpus component. (iii) It is also capable of automaticallygenerating puzzles with parameterizable levels of difficulty suitable for,e.g., beginners or intermediate learners.
arxiv-1206-0393 | Greedy expansions in convex optimization |  http://arxiv.org/abs/1206.0393  | author:V. N. Temlyakov category:stat.ML math.OC 41A65 published:2012-06-02 summary:This paper is a follow up to the previous author's paper on convexoptimization. In that paper we began the process of adjusting greedy-typealgorithms from nonlinear approximation for finding sparse solutions of convexoptimization problems. We modified there three the most popular in nonlinearapproximation in Banach spaces greedy algorithms -- Weak Chebyshev GreedyAlgorithm, Weak Greedy Algorithm with Free Relaxation and Weak Relaxed GreedyAlgorithm -- for solving convex optimization problems. We continue to studysparse approximate solutions to convex optimization problems. It is known thatin many engineering applications researchers are interested in an approximatesolution of an optimization problem as a linear combination of elements from agiven system of elements. There is an increasing interest in building suchsparse approximate solutions using different greedy-type algorithms. In thispaper we concentrate on greedy algorithms that provide expansions, which meansthat the approximant at the $m$th iteration is equal to the sum of theapproximant from the previous iteration ($(m-1)$th iteration) and one elementfrom the dictionary with an appropriate coefficient. The problem of greedyexpansions of elements of a Banach space is well studied in nonlinearapproximation theory. At a first glance the setting of a problem of expansionof a given element and the setting of the problem of expansion in anoptimization problem are very different. However, it turns out that the sametechnique can be used for solving both problems. We show how the techniquedeveloped in nonlinear approximation theory, in particular, the greedyexpansions technique can be adjusted for finding a sparse solution of anoptimization problem given by an expansion with respect to a given dictionary.
arxiv-1206-0333 | Sparse Trace Norm Regularization |  http://arxiv.org/abs/1206.0333  | author:Jianhui Chen, Jieping Ye category:cs.LG stat.ML published:2012-06-02 summary:We study the problem of estimating multiple predictive functions from adictionary of basis functions in the nonparametric regression setting. Ourestimation scheme assumes that each predictive function can be estimated in theform of a linear combination of the basis functions. By assuming that thecoefficient matrix admits a sparse low-rank structure, we formulate thefunction estimation problem as a convex program regularized by the trace normand the $\ell_1$-norm simultaneously. We propose to solve the convex programusing the accelerated gradient (AG) method and the alternating direction methodof multipliers (ADMM) respectively; we also develop efficient algorithms tosolve the key components in both AG and ADMM. In addition, we conducttheoretical analysis on the proposed function estimation scheme: we derive akey property of the optimal solution to the convex program; based on anassumption on the basis functions, we establish a performance bound of theproposed function estimation scheme (via the composite regularization).Simulation studies demonstrate the effectiveness and efficiency of the proposedalgorithms.
arxiv-1206-0381 | UNL Based Bangla Natural Text Conversion - Predicate Preserving Parser Approach |  http://arxiv.org/abs/1206.0381  | author:Md. Nawab Yousuf Ali, Shamim Ripon, Shaikh Muhammad Allayear category:cs.CL published:2012-06-02 summary:Universal Networking Language (UNL) is a declarative formal language that isused to represent semantic data extracted from natural language texts. Thispaper presents a novel approach to converting Bangla natural language text intoUNL using a method known as Predicate Preserving Parser (PPP) technique. PPPperforms morphological, syntactic and semantic, and lexical analysis of textsynchronously. This analysis produces a semantic-net like structure representedusing UNL. We demonstrate how Bangla texts are analyzed following the PPPtechnique to produce UNL documents which can then be translated into any othersuitable natural language facilitating the opportunity to develop a universallanguage translation method via UNL.
arxiv-1206-0068 | Posterior contraction of the population polytope in finite admixture models |  http://arxiv.org/abs/1206.0068  | author:XuanLong Nguyen category:math.ST cs.LG stat.TH published:2012-06-01 summary:We study the posterior contraction behavior of the latent populationstructure that arises in admixture models as the amount of data increases. Weadopt the geometric view of admixture models - alternatively known as topicmodels - as a data generating mechanism for points randomly sampled from theinterior of a (convex) population polytope, whose extreme points correspond tothe population structure variables of interest. Rates of posterior contractionare established with respect to Hausdorff metric and a minimum matchingEuclidean metric defined on polytopes. Tools developed include posteriorasymptotics of hierarchical models and arguments from convex geometry.
arxiv-1206-0304 | Predictive Information Rate in Discrete-time Gaussian Processes |  http://arxiv.org/abs/1206.0304  | author:Samer A. Abdallah, Mark D. Plumbley category:stat.ML math.ST stat.TH published:2012-06-01 summary:We derive expressions for the predicitive information rate (PIR) for theclass of autoregressive Gaussian processes AR(N), both in terms of theprediction coefficients and in terms of the power spectral density. The latterresult suggests a duality between the PIR and the multi-information rate forprocesses with mutually inverse power spectra (i.e. with poles and zeros of thetransfer function exchanged). We investigate the behaviour of the PIR inrelation to the multi-information rate for some simple examples, which suggest,somewhat counter-intuitively, that the PIR is maximised for very `smooth' ARprocesses whose power spectra have multiple poles at zero frequency. We alsoobtain results for moving average Gaussian processes which are consistent withthe duality conjectured earlier. One consequence of this is that the PIR isunbounded for MA(N) processes.
arxiv-1206-0238 | Rapid Feature Extraction for Optical Character Recognition |  http://arxiv.org/abs/1206.0238  | author:M. Zahid Hossain, M. Ashraful Amin, Hong Yan category:cs.CV I.5.2; I.7.5 published:2012-06-01 summary:Feature extraction is one of the fundamental problems of characterrecognition. The performance of character recognition system is depends onproper feature extraction and correct classifier selection. In this article, arapid feature extraction method is proposed and named as Celled Projection (CP)that compute the projection of each section formed through partitioning animage. The recognition performance of the proposed method is compared withother widely used feature extraction methods that are intensively studied formany different scripts in literature. The experiments have been conducted usingBangla handwritten numerals along with three different well known classifierswhich demonstrate comparable results including 94.12% recognition accuracyusing celled projection.
arxiv-1206-0111 | OpenGM: A C++ Library for Discrete Graphical Models |  http://arxiv.org/abs/1206.0111  | author:Bjoern Andres, Thorsten Beier, Joerg H. Kappes category:cs.AI cs.MS stat.ML published:2012-06-01 summary:OpenGM is a C++ template library for defining discrete graphical models andperforming inference on these models, using a wide range of state-of-the-artalgorithms. No restrictions are imposed on the factor graph to allow forhigher-order factors and arbitrary neighborhood structures. Large models withrepetitive structure are handled efficiently because (i) functions that occurrepeatedly need to be stored only once, and (ii) distinct functions can beimplemented differently, using different encodings alongside each other in thesame model. Several parametric functions (e.g. metrics), sparse and dense valuetables are provided and so is an interface for custom C++ code. Algorithms areseparated by design from the representation of graphical models and are easilyexchangeable. OpenGM, its algorithms, HDF5 file format and command line toolsare modular and extendible.
arxiv-1206-0042 | Language Acquisition in Computers |  http://arxiv.org/abs/1206.0042  | author:Megan Belzner, Sean Colin-Ellerin, Jorge H. Roman category:cs.CL I.2.6; I.2.7 published:2012-05-31 summary:This project explores the nature of language acquisition in computers, guidedby techniques similar to those used in children. While existing naturallanguage processing methods are limited in scope and understanding, our systemaims to gain an understanding of language from first principles and henceminimal initial input. The first portion of our system was implemented in Javaand is focused on understanding the morphology of language using bigrams. Weuse frequency distributions and differences between them to define anddistinguish languages. English and French texts were analyzed to determine adifference threshold of 55 before the texts are considered to be in differentlanguages, and this threshold was verified using Spanish texts. The secondportion of our system focuses on gaining an understanding of the syntax of alanguage using a recursive method. The program uses one of two possible methodsto analyze given sentences based on either sentence patterns or surroundingwords. Both methods have been implemented in C++. The program is able tounderstand the structure of simple sentences and learn new words. In addition,we have provided some suggestions regarding future work and potentialextensions of the existing program.
arxiv-1205-7009 | Oriented and Degree-generated Block Models: Generating and Inferring Communities with Inhomogeneous Degree Distributions |  http://arxiv.org/abs/1205.7009  | author:Yaojia Zhu, Xiaoran Yan, Cristopher Moore category:cs.SI physics.soc-ph stat.ML published:2012-05-31 summary:The stochastic block model is a powerful tool for inferring communitystructure from network topology. However, it predicts a Poisson degreedistribution within each community, while most real-world networks have aheavy-tailed degree distribution. The degree-corrected block model canaccommodate arbitrary degree distributions within communities. But since ittakes the vertex degrees as parameters rather than generating them, it cannotuse them to help it classify the vertices, and its natural generalization todirected graphs cannot even use the orientations of the edges. In this paper,we present variants of the block model with the best of both worlds: they canuse vertex degrees and edge orientations in the classification process, whiletolerating heavy-tailed degree distributions within communities. We show thatfor some networks, including synthetic networks and networks of wordadjacencies in English text, these new block models achieve a higher accuracythan either standard or degree-corrected block models.
arxiv-1205-6849 | Beyond $\ell_1$-norm minimization for sparse signal recovery |  http://arxiv.org/abs/1205.6849  | author:Hassan Mansour category:cs.IT cs.LG math.IT published:2012-05-30 summary:Sparse signal recovery has been dominated by the basis pursuit denoise (BPDN)problem formulation for over a decade. In this paper, we propose an algorithmthat outperforms BPDN in finding sparse solutions to underdetermined linearsystems of equations at no additional computational cost. Our algorithm, calledWSPGL1, is a modification of the spectral projected gradient for $\ell_1$minimization (SPGL1) algorithm in which the sequence of LASSO subproblems arereplaced by a sequence of weighted LASSO subproblems with constant weightsapplied to a support estimate. The support estimate is derived from the dataand is updated at every iteration. The algorithm also modifies the Pareto curveat every iteration to reflect the new weighted $\ell_1$ minimization problemthat is being solved. We demonstrate through extensive simulations that thesparse recovery performance of our algorithm is superior to that of $\ell_1$minimization and approaches the recovery performance of iterative re-weighted$\ell_1$ (IRWL1) minimization of Cand{\`e}s, Wakin, and Boyd, although it doesnot match it in general. Moreover, our algorithm has the computational cost ofa single BPDN problem.
arxiv-1206-0652 | Learning in Hierarchical Social Networks |  http://arxiv.org/abs/1206.0652  | author:Zhenliang Zhang, Edwin K. P. Chong, Ali Pezeshki, William Moran, Stephen D. Howard category:cs.SI cs.IT cs.LG math.IT published:2012-05-30 summary:We study a social network consisting of agents organized as a hierarchicalM-ary rooted tree, common in enterprise and military organizational structures.The goal is to aggregate information to solve a binary hypothesis testingproblem. Each agent at a leaf of the tree, and only such an agent, makes adirect measurement of the underlying true hypothesis. The leaf agent then makesa decision and sends it to its supervising agent, at the next level of thetree. Each supervising agent aggregates the decisions from the M members of itsgroup, produces a summary message, and sends it to its supervisor at the nextlevel, and so on. Ultimately, the agent at the root of the tree makes anoverall decision. We derive upper and lower bounds for the Type I and II errorprobabilities associated with this decision with respect to the number of leafagents, which in turn characterize the converge rates of the Type I, Type II,and total error probabilities. We also provide a message-passing schemeinvolving non-binary message alphabets and characterize the exponent of theerror probability with respect to the message alphabet size.
arxiv-1205-6548 | State Transition Algorithm |  http://arxiv.org/abs/1205.6548  | author:Xiaojun Zhou, Chunhua Yang, Weihua Gui category:math.OC cs.NE published:2012-05-30 summary:In terms of the concepts of state and state transition, a new heuristicrandom search algorithm named state transition algorithm is proposed. Forcontinuous function optimization problems, four special transformationoperators called rotation, translation, expansion and axesion are designed.Adjusting measures of the transformations are mainly studied to keep thebalance of exploration and exploitation. Convergence analysis is also discussedabout the algorithm based on random search theory. In the meanwhile, tostrengthen the search ability in high dimensional space, communication strategyis introduced into the basic algorithm and intermittent exchange is presentedto prevent premature convergence. Finally, experiments are carried out for thealgorithms. With 10 common benchmark unconstrained continuous functions used totest the performance, the results show that state transition algorithms arepromising algorithms due to their good global search capability and convergenceproperty when compared with some popular algorithms.
arxiv-1205-6745 | Fingerprint Gender Classification using Wavelet Transform and Singular Value Decomposition |  http://arxiv.org/abs/1205.6745  | author:P Gnanasivam, Dr. S Muttan category:cs.CV published:2012-05-30 summary:A novel method of gender Classification from fingerprint is proposed based ondiscrete wavelet transform (DWT) and singular value decomposition (SVD). Theclassification is achieved by extracting the energy computed from all thesub-bands of DWT combined with the spatial features of non-zero singular valuesobtained from the SVD of fingerprint images. K nearest neighbor (KNN) used as aclassifier. This method is experimented with the internal database of 3570fingerprints finger prints in which 1980 were male fingerprints and 1590 werefemale fingerprints. Finger-wise gender classification is achieved which is94.32% for the left hand little fingers of female persons and 95.46% for theleft hand index finger of male persons. Gender classification for any finger ofmale persons tested is attained as 91.67% and 84.69% for female personsrespectively. Overall classification rate is 88.28% has been achieved.
arxiv-1205-6605 | Template-Cut: A Pattern-Based Segmentation Paradigm |  http://arxiv.org/abs/1205.6605  | author:Jan Egger, Bernd Freisleben, Christopher Nimsky, Tina Kapur category:cs.CV published:2012-05-30 summary:We present a scale-invariant, template-based segmentation paradigm that setsup a graph and performs a graph cut to separate an object from the background.Typically graph-based schemes distribute the nodes of the graph uniformly andequidistantly on the image, and use a regularizer to bias the cut towards aparticular shape. The strategy of uniform and equidistant nodes does not allowthe cut to prefer more complex structures, especially when areas of the objectare indistinguishable from the background. We propose a solution by introducingthe concept of a "template shape" of the target object in which the nodes aresampled non-uniformly and non-equidistantly on the image. We evaluate it on2D-images where the object's textures and backgrounds are similar, and largeareas of the object have the same gray level appearance as the background. Wealso evaluate it in 3D on 60 brain tumor datasets for neurosurgical planningpurposes.
arxiv-1205-6544 | A Brief Summary of Dictionary Learning Based Approach for Classification (revised) |  http://arxiv.org/abs/1205.6544  | author:Shu Kong, Donghui Wang category:cs.CV cs.LG published:2012-05-30 summary:This note presents some representative methods which are based on dictionarylearning (DL) for classification. We do not review the sophisticated methods orframeworks that involve DL for classification, such as online DL and spatialpyramid matching (SPM), but rather, we concentrate on the direct DL-basedclassification methods. Here, the "so-called direct DL-based method" is theapproach directly deals with DL framework by adding some meaningful penaltyterms. By listing some representative methods, we can roughly divide them intotwo categories, i.e. (1) directly making the dictionary discriminative and (2)forcing the sparse coefficients discriminative to push the discrimination powerof the dictionary. From this taxonomy, we can expect some extensions of them asfuture researches.
arxiv-1205-6572 | An Unsupervised Dynamic Image Segmentation using Fuzzy Hopfield Neural Network based Genetic Algorithm |  http://arxiv.org/abs/1205.6572  | author:Amiya Halder, Soumajit Pramanik category:cs.CV published:2012-05-30 summary:This paper proposes a Genetic Algorithm based segmentation method that canautomatically segment gray-scale images. The proposed method mainly consists ofspatial unsupervised grayscale image segmentation that divides an image intoregions. The aim of this algorithm is to produce precise segmentation of imagesusing intensity information along with neighborhood relationships. In thispaper, Fuzzy Hopfield Neural Network (FHNN) clustering helps in generating thepopulation of Genetic algorithm which there by automatically segments theimage. This technique is a powerful method for image segmentation and works forboth single and multiple-feature data with spatial information. Validity indexhas been utilized for introducing a robust technique for finding the optimumnumber of components in an image. Experimental results shown that the algorithmgenerates good quality segmented image.
arxiv-1205-6523 | Finding Important Genes from High-Dimensional Data: An Appraisal of Statistical Tests and Machine-Learning Approaches |  http://arxiv.org/abs/1205.6523  | author:Chamont Wang, Jana Gevertz, Chaur-Chin Chen, Leonardo Auslender category:stat.ML cs.LG q-bio.QM published:2012-05-30 summary:Over the past decades, statisticians and machine-learning researchers havedeveloped literally thousands of new tools for the reduction ofhigh-dimensional data in order to identify the variables most responsible for aparticular trait. These tools have applications in a plethora of settings,including data analysis in the fields of business, education, forensics, andbiology (such as microarray, proteomics, brain imaging), to name a few. In the present work, we focus our investigation on the limitations andpotential misuses of certain tools in the analysis of the benchmark coloncancer data (2,000 variables; Alon et al., 1999) and the prostate cancer data(6,033 variables; Efron, 2010, 2008). Our analysis demonstrates that modelsthat produce 100% accuracy measures often select different sets of genes andcannot stand the scrutiny of parameter estimates and model stability. Furthermore, we created a host of simulation datasets and "artificialdiseases" to evaluate the reliability of commonly used statistical and datamining tools. We found that certain widely used models can classify the datawith 100% accuracy without using any of the variables responsible for thedisease. With moderate sample size and suitable pre-screening, stochasticgradient boosting will be shown to be a superior model for gene selection andvariable screening from high-dimensional datasets.
arxiv-1205-6391 | A Brief Summary of Dictionary Learning Based Approach for Classification |  http://arxiv.org/abs/1205.6391  | author:Kong Shu, Wang Donghui category:cs.CV published:2012-05-29 summary:This note presents some representative methods which are based on dictionarylearning (DL) for classification. We do not review the sophisticated methods orframeworks that involve DL for classification, such as online DL and spatialpyramid matching (SPM), but rather, we concentrate on the direct DL-basedclassification methods. Here, the "so-called direct DL-based method" is theapproach directly deals with DL framework by adding some meaningful penaltyterms. By listing some representative methods, we can roughly divide them intotwo categories, i.e. (1) directly making the dictionary discriminative and (2)forcing the sparse coefficients discriminative to push the discrimination powerof the dictionary. From this taxonomy, we can expect some extensions of them asfuture researches.
arxiv-1205-6326 | A Framework for Evaluating Approximation Methods for Gaussian Process Regression |  http://arxiv.org/abs/1205.6326  | author:Krzysztof Chalupka, Christopher K. I. Williams, Iain Murray category:stat.ML cs.LG stat.CO published:2012-05-29 summary:Gaussian process (GP) predictors are an important component of many Bayesianapproaches to machine learning. However, even a straightforward implementationof Gaussian process regression (GPR) requires O(n^2) space and O(n^3) time fora dataset of n examples. Several approximation methods have been proposed, butthere is a lack of understanding of the relative merits of the differentapproximations, and in what situations they are most useful. We recommendassessing the quality of the predictions obtained as a function of the computetime taken, and comparing to standard baselines (e.g., Subset of Data andFITC). We empirically investigate four different approximation algorithms onfour different prediction problems, and make our code available to encouragefuture comparisons.
arxiv-1205-6432 | Multiclass Learning Approaches: A Theoretical Comparison with Implications |  http://arxiv.org/abs/1205.6432  | author:Amit Daniely, Sivan Sabato, Shai Shalev Shwartz category:cs.LG published:2012-05-29 summary:We theoretically analyze and compare the following five popular multiclassclassification methods: One vs. All, All Pairs, Tree-based classifiers, ErrorCorrecting Output Codes (ECOC) with randomly generated code matrices, andMulticlass SVM. In the first four methods, the classification is based on areduction to binary classification. We consider the case where the binaryclassifier comes from a class of VC dimension $d$, and in particular from theclass of halfspaces over $\reals^d$. We analyze both the estimation error andthe approximation error of these methods. Our analysis reveals interestingconclusions of practical relevance, regarding the success of the differentapproaches under various conditions. Our proof technique employs tools from VCtheory to analyze the \emph{approximation error} of hypothesis classes. This isin sharp contrast to most, if not all, previous uses of VC theory, which onlydeal with estimation error.
arxiv-1205-6396 | Effective Listings of Function Stop words for Twitter |  http://arxiv.org/abs/1205.6396  | author:Murphy Choy category:cs.IR cs.CL published:2012-05-29 summary:Many words in documents recur very frequently but are essentially meaninglessas they are used to join words together in a sentence. It is commonlyunderstood that stop words do not contribute to the context or content oftextual documents. Due to their high frequency of occurrence, their presence intext mining presents an obstacle to the understanding of the content in thedocuments. To eliminate the bias effects, most text mining software orapproaches make use of stop words list to identify and remove those words.However, the development of such top words list is difficult and inconsistentbetween textual sources. This problem is further aggravated by sources such asTwitter which are highly repetitive or similar in nature. In this paper, wewill be examining the original work using term frequency, inverse documentfrequency and term adjacency for developing a stop words list for the Twitterdata source. We propose a new technique using combinatorial values as analternative measure to effectively list out stop words.
arxiv-1205-6352 | Generalized sequential tree-reweighted message passing |  http://arxiv.org/abs/1205.6352  | author:Vladimir Kolmogorov, Thomas Schoenemann category:cs.CV published:2012-05-29 summary:This paper addresses the problem of approximate MAP-MRF inference in generalgraphical models. Following [36], we consider a family of linear programmingrelaxations of the problem where each relaxation is specified by a set ofnested pairs of factors for which the marginalization constraint needs to beenforced. We develop a generalization of the TRW-S algorithm [9] for thisproblem, where we use a decomposition into junction chains, monotonic w.r.t.some ordering on the nodes. This generalizes the monotonic chains in [9] in anatural way. We also show how to deal with nested factors in an efficient way.Experiments show an improvement over min-sum diffusion, MPLP and subgradientascent algorithms on a number of computer vision and natural languageprocessing problems.
arxiv-1205-6154 | Potentials and Limits of Super-Resolution Algorithms and Signal Reconstruction from Sparse Data |  http://arxiv.org/abs/1205.6154  | author:Gil Shabat category:physics.optics cs.CV math-ph math.MP published:2012-05-28 summary:A common distortion in videos is image instability in the form of chaotic(global and local displacements). Those instabilities can be used to enhanceimage resolution by using subpixel elastic registration. In this work, weinvestigate the performance of such methods over the ability to improve theresolution by accumulating several frames. The second part of this work dealswith reconstruction of discrete signals from a subset of samples underdifferent basis functions such as DFT, Haar, Walsh, Daubechies wavelets and CT(Radon) projections.
arxiv-1205-6031 | Towards a Mathematical Foundation of Immunology and Amino Acid Chains |  http://arxiv.org/abs/1205.6031  | author:Wen-Jun Shen, Hau-San Wong, Quan-Wu Xiao, Xin Guo, Stephen Smale category:stat.ML cs.LG q-bio.GN published:2012-05-28 summary:We attempt to set a mathematical foundation of immunology and amino acidchains. To measure the similarities of these chains, a kernel on strings isdefined using only the sequence of the chains and a good amino acidsubstitution matrix (e.g. BLOSUM62). The kernel is used in learning machines topredict binding affinities of peptides to human leukocyte antigens DR (HLA-DR)molecules. On both fixed allele (Nielsen and Lund 2009) and pan-allele (Nielsenet.al. 2010) benchmark databases, our algorithm achieves the state-of-the-artperformance. The kernel is also used to define a distance on an HLA-DR alleleset based on which a clustering analysis precisely recovers the serotypeclassifications assigned by WHO (Nielsen and Lund 2009, and Marsh et.al. 2010).These results suggest that our kernel relates well the chain structure of bothpeptides and HLA-DR molecules to their biological functions, and that it offersa simple, powerful and promising methodology to immunology and amino acid chainstudies.
arxiv-1205-6210 | Learning Dictionaries with Bounded Self-Coherence |  http://arxiv.org/abs/1205.6210  | author:Christian D. Sigg, Tomas Dikk, Joachim M. Buhmann category:stat.ML cs.LG published:2012-05-28 summary:Sparse coding in learned dictionaries has been established as a successfulapproach for signal denoising, source separation and solving inverse problemsin general. A dictionary learning method adapts an initial dictionary to aparticular signal class by iteratively computing an approximate factorizationof a training data matrix into a dictionary and a sparse coding matrix. Thelearned dictionary is characterized by two properties: the coherence of thedictionary to observations of the signal class, and the self-coherence of thedictionary atoms. A high coherence to the signal class enables the sparsecoding of signal observations with a small approximation error, while a lowself-coherence of the atoms guarantees atom recovery and a more rapid residualerror decay rate for the sparse coding algorithm. The two goals of high signalcoherence and low self-coherence are typically in conflict, therefore one seeksa trade-off between them, depending on the application. We present a dictionarylearning method with an effective control over the self-coherence of thetrained dictionary, enabling a trade-off between maximizing the sparsity ofcodings and approximating an equiangular tight frame.
arxiv-1205-5868 | Sparse estimation via nonconcave penalized likelihood in a factor analysis model |  http://arxiv.org/abs/1205.5868  | author:Kei Hirose, Michio Yamamoto category:stat.ME stat.CO stat.ML published:2012-05-26 summary:We consider the problem of sparse estimation in a factor analysis model. Atraditional estimation procedure in use is the following two-step approach: themodel is estimated by maximum likelihood method and then a rotation techniqueis utilized to find sparse factor loadings. However, the maximum likelihoodestimates cannot be obtained when the number of variables is much larger thanthe number of observations. Furthermore, even if the maximum likelihoodestimates are available, the rotation technique does not often produce asufficiently sparse solution. In order to handle these problems, this paperintroduces a penalized likelihood procedure that imposes a nonconvex penalty onthe factor loadings. We show that the penalized likelihood procedure can beviewed as a generalization of the traditional two-step approach, and theproposed methodology can produce sparser solutions than the rotation technique.A new algorithm via the EM algorithm along with coordinate descent isintroduced to compute the entire solution path, which permits the applicationto a wide variety of convex and nonconvex penalties. Monte Carlo simulationsare conducted to investigate the performance of our modeling strategy. A realdata example is also given to illustrate our procedure.
arxiv-1205-5920 | On latent position inference from doubly stochastic messaging activities |  http://arxiv.org/abs/1205.5920  | author:Nam H. Lee, Jordan Yoder, Minh Tang, Carey E Priebe category:stat.ML math.ST stat.ME stat.TH published:2012-05-26 summary:We model messaging activities as a hierarchical doubly stochastic pointprocess with three main levels, and develop an iterative algorithm forinferring actors' relative latent positions from a stream of messaging activitydata. Each of the message-exchanging actors is modeled as a process in a latentspace. The actors' latent positions are assumed to be influenced by thedistribution of a much larger population over the latent space. Each actor'smovement in the latent space is modeled as being governed by two parametersthat we call confidence and visibility, in addition to dependence on thepopulation distribution. The messaging frequency between a pair of actors isassumed to be inversely proportional to the distance between their latentpositions. Our inference algorithm is based on a projection approach to anonline filtering problem. The algorithm associates each actor with aprobability density-valued process, and each probability density is assumed tobe a mixture of basis functions. For efficient numerical experiments, wefurther develop our algorithm for the case where the basis functions areobtained by translating and scaling a standard Gaussian density.
arxiv-1205-5819 | Measurability Aspects of the Compactness Theorem for Sample Compression Schemes |  http://arxiv.org/abs/1205.5819  | author:Damjan Kalajdzievski category:stat.ML cs.LG published:2012-05-25 summary:It was proved in 1998 by Ben-David and Litman that a concept space has asample compression scheme of size d if and only if every finite subspace has asample compression scheme of size d. In the compactness theorem, measurabilityof the hypotheses of the created sample compression scheme is not guaranteed;at the same time measurability of the hypotheses is a necessary condition forlearnability. In this thesis we discuss when a sample compression scheme,created from com- pression schemes on finite subspaces via the compactnesstheorem, have measurable hypotheses. We show that if X is a standard Borelspace with a d-maximum and universally separable concept class C, then (X,C)has a sample compression scheme of size d with universally Borel measurablehypotheses. Additionally we introduce a new variant of compression schemecalled a copy sample compression scheme.
arxiv-1205-5425 | Locally Orderless Registration |  http://arxiv.org/abs/1205.5425  | author:Sune Darkner, Jon Sporring category:cs.CV published:2012-05-24 summary:Image registration is an important tool for medical image analysis and isused to bring images into the same reference frame by warping the coordinatefield of one image, such that some similarity measure is minimized. We studysimilarity in image registration in the context of Locally Orderless Images(LOI), which is the natural way to study density estimates and reveals the 3fundamental scales: the measurement scale, the intensity scale, and theintegration scale. This paper has three main contributions: Firstly, we rephrase a large set ofpopular similarity measures into a common framework, which we refer to asLocally Orderless Registration, and which makes full use of the features oflocal histograms. Secondly, we extend the theoretical understanding of thelocal histograms. Thirdly, we use our framework to compare two state-of-the-artintensity density estimators for image registration: The Parzen Window (PW) andthe Generalized Partial Volume (GPV), and we demonstrate their differences on apopular similarity measure, Normalized Mutual Information (NMI). We conclude, that complicated similarity measures such as NMI may beevaluated almost as fast as simple measures such as Sum of Squared Distances(SSD) regardless of the choice of PW and GPV. Also, GPV is an asymmetricmeasure, and PW is our preferred choice.
arxiv-1205-5407 | FASTSUBS: An Efficient and Exact Procedure for Finding the Most Likely Lexical Substitutes Based on an N-gram Language Model |  http://arxiv.org/abs/1205.5407  | author:Deniz Yuret category:cs.CL published:2012-05-24 summary:Lexical substitutes have found use in areas such as paraphrasing, textsimplification, machine translation, word sense disambiguation, and part ofspeech induction. However the computational complexity of accuratelyidentifying the most likely substitutes for a word has made large scaleexperiments difficult. In this paper I introduce a new search algorithm,FASTSUBS, that is guaranteed to find the K most likely lexical substitutes fora given word in a sentence based on an n-gram language model. The computationis sub-linear in both K and the vocabulary size V. An implementation of thealgorithm and a dataset with the top 100 substitutes of each token in the WSJsection of the Penn Treebank are available at http://goo.gl/jzKH0.
arxiv-1205-5367 | Language-Constraint Reachability Learning in Probabilistic Graphs |  http://arxiv.org/abs/1205.5367  | author:Claudio Taranto, Nicola Di Mauro, Floriana Esposito category:cs.AI cs.LG published:2012-05-24 summary:The probabilistic graphs framework models the uncertainty inherent inreal-world domains by means of probabilistic edges whose value quantifies thelikelihood of the edge existence or the strength of the link it represents. Thegoal of this paper is to provide a learning method to compute the most likelyrelationship between two nodes in a framework based on probabilistic graphs. Inparticular, given a probabilistic graph we adopted the language-constraintreachability method to compute the probability of possible interconnectionsthat may exists between two nodes. Each of these connections may be viewed asfeature, or a factor, between the two nodes and the corresponding probabilityas its weight. Each observed link is considered as a positive instance for itscorresponding link label. Given the training set of observed links aL2-regularized Logistic Regression has been adopted to learn a model able topredict unobserved link labels. The experiments on a real world collaborativefiltering problem proved that the proposed approach achieves better resultsthan that obtained adopting classical methods.
arxiv-1205-5353 | A hybrid clustering algorithm for data mining |  http://arxiv.org/abs/1205.5353  | author:Ravindra Jain category:cs.DB cs.LG published:2012-05-24 summary:Data clustering is a process of arranging similar data into groups. Aclustering algorithm partitions a data set into several groups such that thesimilarity within a group is better than among groups. In this paper a hybridclustering algorithm based on K-mean and K-harmonic mean (KHM) is described.The proposed algorithm is tested on five different datasets. The research isfocused on fast and accurate clustering. Its performance is compared with thetraditional K-means & KHM algorithm. The result obtained from proposed hybridalgorithm is much better than the traditional K-mean & KHM algorithm.
arxiv-1205-5351 | Linearized Alternating Direction Method with Adaptive Penalty and Warm Starts for Fast Solving Transform Invariant Low-Rank Textures |  http://arxiv.org/abs/1205.5351  | author:Xiang Ren, Zhouchen Lin category:cs.CV published:2012-05-24 summary:Transform Invariant Low-rank Textures (TILT) is a novel and powerful toolthat can effectively rectify a rich class of low-rank textures in 3D scenesfrom 2D images despite significant deformation and corruption. The existingalgorithm for solving TILT is based on the alternating direction method (ADM).It suffers from high computational cost and is not theoretically guaranteed toconverge to a correct solution. In this paper, we propose a novel algorithm tospeed up solving TILT, with guaranteed convergence. Our method is based on therecently proposed linearized alternating direction method with adaptive penalty(LADMAP). To further reduce computation, warm starts are also introduced toinitialize the variables better and cut the cost on singular valuedecomposition. Extensive experimental results on both synthetic and real datademonstrate that this new algorithm works much more efficiently and robustlythan the existing algorithm. It could be at least five times faster than theprevious method.
arxiv-1205-5097 | Neural Network Approach for Eye Detection |  http://arxiv.org/abs/1205.5097  | author:Vijayalaxmi, P. Sudhakara Rao, S. Sreehari category:cs.CV published:2012-05-23 summary:Driving support systems, such as car navigation systems are becoming commonand they support driver in several aspects. Non-intrusive method of detectingFatigue and drowsiness based on eye-blink count and eye directed instructioncontrolhelps the driver to prevent from collision caused by drowsy driving. Eyedetection and tracking under various conditions such as illumination,background, face alignment and facial expression makes the problemcomplex.Neural Network based algorithm is proposed in this paper to detect theeyes efficiently. In the proposed algorithm, first the neural Network istrained to reject the non-eye regionbased on images with features of eyes andthe images with features of non-eye using Gabor filter and Support VectorMachines to reduce the dimension and classify efficiently. In the algorithm,first the face is segmented using L*a*btransform color space, then eyes aredetected using HSV and Neural Network approach. The algorithm is tested onnearly 100 images of different persons under different conditions and theresults are satisfactory with success rate of 98%.The Neural Network is trainedwith 50 non-eye images and 50 eye images with different angles using Gaborfilter. This paper is a part of research work on "Development of Non-Intrusivesystem for real-time Monitoring and Prediction of Driver Fatigue anddrowsiness" project sponsored by Department of Science & Technology, Govt. ofIndia, New Delhi at Vignan Institute of Technology and Sciences, Vignan Hills,Hyderabad.
arxiv-1205-5075 | Efficient Sparse Group Feature Selection via Nonconvex Optimization |  http://arxiv.org/abs/1205.5075  | author:Shuo Xiang, Xiaotong Shen, Jieping Ye category:cs.LG stat.ML published:2012-05-23 summary:Sparse feature selection has been demonstrated to be effective in handlinghigh-dimensional data. While promising, most of the existing works use convexmethods, which may be suboptimal in terms of the accuracy of feature selectionand parameter estimation. In this paper, we expand a nonconvex paradigm tosparse group feature selection, which is motivated by applications that requireidentifying the underlying group structure and performing feature selectionsimultaneously. The main contributions of this article are twofold: (1)statistically, we introduce a nonconvex sparse group feature selection modelwhich can reconstruct the oracle estimator. Therefore, consistent featureselection and parameter estimation can be achieved; (2) computationally, wepropose an efficient algorithm that is applicable to large-scale problems.Numerical results suggest that the proposed nonconvex method compares favorablyagainst its competitors on synthetic data and real-world applications, thusachieving desired goal of delivering high performance.
arxiv-1205-4891 | Clustering is difficult only when it does not matter |  http://arxiv.org/abs/1205.4891  | author:Amit Daniely, Nati Linial, Michael Saks category:cs.LG cs.DS published:2012-05-22 summary:Numerous papers ask how difficult it is to cluster data. We suggest that themore relevant and interesting question is how difficult it is to cluster datasets {\em that can be clustered well}. More generally, despite the ubiquity andthe great importance of clustering, we still do not have a satisfactorymathematical theory of clustering. In order to properly understand clustering,it is clearly necessary to develop a solid theoretical basis for the area. Forexample, from the perspective of computational complexity theory the clusteringproblem seems very hard. Numerous papers introduce various criteria andnumerical measures to quantify the quality of a given clustering. The resultingconclusions are pessimistic, since it is computationally difficult to find anoptimal clustering of a given data set, if we go by any of these popularcriteria. In contrast, the practitioners' perspective is much more optimistic.Our explanation for this disparity of opinions is that complexity theoryconcentrates on the worst case, whereas in reality we only care for data setsthat can be clustered well. We introduce a theoretical framework of clustering in metric spaces thatrevolves around a notion of "good clustering". We show that if a goodclustering exists, then in many cases it can be efficiently found. Ourconclusion is that contrary to popular belief, clustering should not beconsidered a hard task.
arxiv-1205-4776 | Visual and semantic interpretability of projections of high dimensional data for classification tasks |  http://arxiv.org/abs/1205.4776  | author:Ilknur Icke, Andrew Rosenberg category:cs.HC cs.LG published:2012-05-22 summary:A number of visual quality measures have been introduced in visual analyticsliterature in order to automatically select the best views of high dimensionaldata from a large number of candidate data projections. These methods generallyconcentrate on the interpretability of the visualization and pay littleattention to the interpretability of the projection axes. In this paper, weargue that interpretability of the visualizations and the featuretransformation functions are both crucial for visual exploration of highdimensional labeled data. We present a two-part user study to examine these tworelated but orthogonal aspects of interpretability. We first study how humansjudge the quality of 2D scatterplots of various datasets with varying number ofclasses and provide comparisons with ten automated measures, including a numberof visual quality measures and related measures from various machine learningfields. We then investigate how the user perception on interpretability ofmathematical expressions relate to various automated measures of complexitythat can be used to characterize data projection functions. We conclude with adiscussion of how automated measures of visual and semantic interpretability ofdata projections can be used together for exploratory analysis inclassification tasks.
arxiv-1205-4810 | Safe Exploration in Markov Decision Processes |  http://arxiv.org/abs/1205.4810  | author:Teodor Mihai Moldovan, Pieter Abbeel category:cs.LG published:2012-05-22 summary:In environments with uncertain dynamics exploration is necessary to learn howto perform well. Existing reinforcement learning algorithms provide strongexploration guarantees, but they tend to rely on an ergodicity assumption. Theessence of ergodicity is that any state is eventually reachable from any otherstate by following a suitable policy. This assumption allows for explorationalgorithms that operate by simply favoring states that have rarely been visitedbefore. For most physical systems this assumption is impractical as the systemswould break before any reasonable exploration has taken place, i.e., mostphysical systems don't satisfy the ergodicity assumption. In this paper weaddress the need for safe exploration methods in Markov decision processes. Wefirst propose a general formulation of safety through ergodicity. We show thatimposing safety by restricting attention to the resulting set of guaranteedsafe policies is NP-hard. We then present an efficient algorithm for guaranteedsafe, but potentially suboptimal, exploration. At the core is an optimizationformulation in which the constraints restrict attention to a subset of theguaranteed safe policies and the objective favors exploration policies. Ourframework is compatible with the majority of previously proposed explorationmethods, which rely on an exploration bonus. Our experiments, which include aMartian terrain exploration problem, show that our method is able to explorebetter than classical exploration methods.
arxiv-1205-5050 | A lasso for hierarchical interactions |  http://arxiv.org/abs/1205.5050  | author:Jacob Bien, Jonathan Taylor, Robert Tibshirani category:stat.ME math.ST stat.ML stat.TH published:2012-05-22 summary:We add a set of convex constraints to the lasso to produce sparse interactionmodels that honor the hierarchy restriction that an interaction only beincluded in a model if one or both variables are marginally important. We givea precise characterization of the effect of this hierarchy constraint, provethat hierarchy holds with probability one and derive an unbiased estimate forthe degrees of freedom of our estimator. A bound on this estimate reveals theamount of fitting "saved" by the hierarchy constraint. We distinguish betweenparameter sparsity - the number of nonzero coefficients - and practicalsparsity - the number of raw variables one must measure to make a newprediction. Hierarchy focuses on the latter, which is more closely tied toimportant data collection concerns such as cost, time and effort. We develop analgorithm, available in the R package hierNet, and perform an empirical studyof our method.
arxiv-1205-5012 | Learning Mixed Graphical Models |  http://arxiv.org/abs/1205.5012  | author:Jason D. Lee, Trevor J. Hastie category:stat.ML cs.CV cs.LG math.OC published:2012-05-22 summary:We consider the problem of learning the structure of a pairwise graphicalmodel over continuous and discrete variables. We present a new pairwise modelfor graphical models with both continuous and discrete variables that isamenable to structure learning. In previous work, authors have consideredstructure learning of Gaussian graphical models and structure learning ofdiscrete models. Our approach is a natural generalization of these two lines ofwork to the mixed case. The penalization scheme involves a novel symmetric useof the group-lasso norm and follows naturally from a particular parametrizationof the model.
arxiv-1205-4893 | On the practically interesting instances of MAXCUT |  http://arxiv.org/abs/1205.4893  | author:Yonatan Bilu, Amit Daniely, Nati Linial, Michael Saks category:cs.CC cs.LG published:2012-05-22 summary:The complexity of a computational problem is traditionally quantified basedon the hardness of its worst case. This approach has many advantages and hasled to a deep and beautiful theory. However, from the practical perspective,this leaves much to be desired. In application areas, practically interestinginstances very often occupy just a tiny part of an algorithm's space ofinstances, and the vast majority of instances are simply irrelevant. Addressingthese issues is a major challenge for theoretical computer science which maymake theory more relevant to the practice of computer science. Following Bilu and Linial, we apply this perspective to MAXCUT, viewed as aclustering problem. Using a variety of techniques, we investigate practicallyinteresting instances of this problem. Specifically, we show how to solve inpolynomial time distinguished, metric, expanding and dense instances of MAXCUTunder mild stability assumptions. In particular, $(1+\epsilon)$-stability(which is optimal) suffices for metric and dense MAXCUT. We also show how tosolve in polynomial time $\Omega(\sqrt{n})$-stable instances of MAXCUT,substantially improving the best previously known result.
arxiv-1205-4831 | Gray Level Co-Occurrence Matrices: Generalisation and Some New Features |  http://arxiv.org/abs/1205.4831  | author:Bino Sebastian V, A. Unnikrishnan, Kannan Balakrishnan category:cs.CV published:2012-05-22 summary:Gray Level Co-occurrence Matrices (GLCM) are one of the earliest techniquesused for image texture analysis. In this paper we defined a new feature calledtrace extracted from the GLCM and its implications in texture analysis arediscussed in the context of Content Based Image Retrieval (CBIR). Thetheoretical extension of GLCM to n-dimensional gray scale images are alsodiscussed. The results indicate that trace features outperform Haralickfeatures when applied to CBIR.
arxiv-1205-4839 | Off-Policy Actor-Critic |  http://arxiv.org/abs/1205.4839  | author:Thomas Degris, Martha White, Richard S. Sutton category:cs.LG published:2012-05-22 summary:This paper presents the first actor-critic algorithm for off-policyreinforcement learning. Our algorithm is online and incremental, and itsper-time-step complexity scales linearly with the number of learned weights.Previous work on actor-critic algorithms is limited to the on-policy settingand does not take advantage of the recent advances in off-policy gradienttemporal-difference learning. Off-policy techniques, such as Greedy-GQ, enablea target policy to be learned while following and obtaining data from another(behavior) policy. For many problems, however, actor-critic methods are morepractical than action value methods (like Greedy-GQ) because they explicitlyrepresent the policy; consequently, the policy can be stochastic and utilize alarge action space. In this paper, we illustrate how to practically combine thegenerality and learning potential of off-policy learning with the flexibilityin action selection given by actor-critic methods. We derive an incremental,linear time and space complexity algorithm that includes eligibility traces,prove convergence under assumptions similar to previous off-policy algorithms,and empirically show better or comparable performance to existing algorithms onstandard reinforcement-learning benchmark problems.
arxiv-1205-4698 | The Role of Weight Shrinking in Large Margin Perceptron Learning |  http://arxiv.org/abs/1205.4698  | author:Constantinos Panagiotakopoulos, Petroula Tsampouka category:cs.LG published:2012-05-21 summary:We introduce into the classical perceptron algorithm with margin a mechanismthat shrinks the current weight vector as a first step of the update. If theshrinking factor is constant the resulting algorithm may be regarded as amargin-error-driven version of NORMA with constant learning rate. In this casewe show that the allowed strength of shrinking depends on the value of themaximum margin. We also consider variable shrinking factors for which there isno such dependence. In both cases we obtain new generalizations of theperceptron with margin able to provably attain in a finite number of steps anydesirable approximation of the maximal margin hyperplane. The new approximatemaximum margin classifiers appear experimentally to be very competitive in2-norm soft margin tasks involving linear kernels.
arxiv-1205-4591 | Forecastable Component Analysis (ForeCA) |  http://arxiv.org/abs/1205.4591  | author:Georg M. Goerg category:stat.ME stat.ML published:2012-05-21 summary:I introduce Forecastable Component Analysis (ForeCA), a novel dimensionreduction technique for temporally dependent signals. Based on a newforecastability measure, ForeCA finds an optimal transformation to separate amultivariate time series into a forecastable and an orthogonal white noisespace. I present a converging algorithm with a fast eigenvector solution.Applications to financial and macro-economic time series show that ForeCA cansuccessfully discover informative structure, which can be used for forecastingas well as classification. The R package ForeCA(http://cran.r-project.org/web/packages/ForeCA/index.html) accompanies thiswork and is publicly available on CRAN.
arxiv-1205-4476 | Soft Rule Ensembles for Statistical Learning |  http://arxiv.org/abs/1205.4476  | author:Deniz Akdemir, Nicolas Heslot category:stat.ML cs.LG stat.AP published:2012-05-21 summary:In this article supervised learning problems are solved using soft ruleensembles. We first review the importance sampling learning ensembles (ISLE)approach that is useful for generating hard rules. The soft rules are thenobtained with logistic regression from the corresponding hard rules. In orderto deal with the perfect separation problem related to the logistic regression,Firth's bias corrected likelihood is used. Various examples and simulationresults show that soft rule ensembles can improve predictive performance overhard rule ensembles.
arxiv-1205-4477 | Streaming Algorithms for Pattern Discovery over Dynamically Changing Event Sequences |  http://arxiv.org/abs/1205.4477  | author:Debprakash Patnaik, Naren Ramakrishnan, Srivatsan Laxman, Badrish Chandramouli category:cs.LG cs.DB published:2012-05-21 summary:Discovering frequent episodes over event sequences is an important datamining task. In many applications, events constituting the data sequence arriveas a stream, at furious rates, and recent trends (or frequent episodes) canchange and drift due to the dynamical nature of the underlying event generationprocess. The ability to detect and track such the changing sets of frequentepisodes can be valuable in many application scenarios. Current methods forfrequent episode discovery are typically multipass algorithms, making themunsuitable in the streaming context. In this paper, we propose a new streamingalgorithm for discovering frequent episodes over a window of recent events inthe stream. Our algorithm processes events as they arrive, one batch at a time,while discovering the top frequent episodes over a window consisting of severalbatches in the immediate past. We derive approximation guarantees for ouralgorithm under the condition that frequent episodes are approximatelywell-separated from infrequent ones in every batch of the window. We presentextensive experimental evaluations of our algorithm on both real and syntheticdata. We also present comparisons with baselines and adaptations of streamingalgorithms from itemset mining literature.
arxiv-1205-4656 | Conditional mean embeddings as regressors - supplementary |  http://arxiv.org/abs/1205.4656  | author:Steffen GrÃ¼newÃ¤lder, Guy Lever, Luca Baldassarre, Sam Patterson, Arthur Gretton, Massimilano Pontil category:cs.LG stat.ML published:2012-05-21 summary:We demonstrate an equivalence between reproducing kernel Hilbert space (RKHS)embeddings of conditional distributions and vector-valued regressors. Thisconnection introduces a natural regularized loss function which the RKHSembeddings minimise, providing an intuitive understanding of the embeddings anda justification for their use. Furthermore, the equivalence allows theapplication of vector-valued regression methods and results to the problem oflearning conditional distributions. Using this link we derive a sparse versionof the embedding by considering alternative formulations. Further, by applyingconvergence results for vector-valued regression to the embedding problem wederive minimax convergence rates which are O(\log(n)/n) -- compared to currentstate of the art rates of O(n^{-1/4}) -- and are valid under milder and moreintuitive assumptions. These minimax upper rates coincide with lower rates upto a logarithmic factor, showing that the embedding method achieves nearlyoptimal rates. We study our sparse embedding algorithm in a reinforcementlearning task where the algorithm shows significant improvement in sparsityover an incomplete Cholesky decomposition.
arxiv-1205-4546 | Latent Multi-group Membership Graph Model |  http://arxiv.org/abs/1205.4546  | author:Myunghwan Kim, Jure Leskovec category:cs.SI physics.soc-ph stat.ML published:2012-05-21 summary:We develop the Latent Multi-group Membership Graph (LMMG) model, a model ofnetworks with rich node feature structure. In the LMMG model, each node belongsto multiple groups and each latent group models the occurrence of links as wellas the node feature structure. The LMMG can be used to summarize the networkstructure, to predict links between the nodes, and to predict missing featuresof a node. We derive efficient inference and learning algorithms and evaluatethe predictive performance of the LMMG on several social and document networkdatasets.
arxiv-1205-4481 | Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by Exploiting Structure |  http://arxiv.org/abs/1205.4481  | author:Hua Ouyang, Alexander Gray category:cs.LG stat.CO stat.ML published:2012-05-21 summary:In this work we consider the stochastic minimization of nonsmooth convex lossfunctions, a central problem in machine learning. We propose a novel algorithmcalled Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), whichexploits the structure of common nonsmooth loss functions to achieve optimalconvergence rates for a class of problems including SVMs. It is the firststochastic algorithm that can achieve the optimal O(1/t) rate for minimizingnonsmooth loss functions (with strong convexity). The fast rates are confirmedby empirical comparisons, in which ANSGD significantly outperforms previoussubgradient descent algorithms including SGD.
arxiv-1205-4770 | Variance function estimation in high-dimensions |  http://arxiv.org/abs/1205.4770  | author:Mladen Kolar, James Sharpnack category:stat.ML stat.ME published:2012-05-21 summary:We consider the high-dimensional heteroscedastic regression model, where themean and the log variance are modeled as a linear combination of inputvariables. Existing literature on high-dimensional linear regres- sion modelshas largely ignored non-constant error variances, even though they commonlyoccur in a variety of applications ranging from biostatis- tics to finance. Inthis paper we study a class of non-convex penalized pseudolikelihood estimatorsfor both the mean and variance parameters. We show that the HeteroscedasticIterative Penalized Pseudolikelihood Optimizer (HIPPO) achieves the oracleproperty, that is, we prove that the rates of convergence are the same as ifthe true model was known. We demonstrate numerical properties of the procedureon a simulation study and real world data.
arxiv-1205-4471 | Sparse Signal Recovery in the Presence of Intra-Vector and Inter-Vector Correlation |  http://arxiv.org/abs/1205.4471  | author:Bhaskar D. Rao, Zhilin Zhang, Yuzhe Jin category:cs.IT cs.LG math.IT stat.ME stat.ML published:2012-05-20 summary:This work discusses the problem of sparse signal recovery when there iscorrelation among the values of non-zero entries. We examine intra-vectorcorrelation in the context of the block sparse model and inter-vectorcorrelation in the context of the multiple measurement vector model, as well astheir combination. Algorithms based on the sparse Bayesian learning arepresented and the benefits of incorporating correlation at the algorithm levelare discussed. The impact of correlation on the limits of support recovery isalso discussed highlighting the different impact intra-vector and inter-vectorcorrelations have on such limits.
arxiv-1205-4450 | Spectral Graph Cut from a Filtering Point of View |  http://arxiv.org/abs/1205.4450  | author:Chengxi Ye, Yuxu Lin, Mingli Song, Chun Chen, David W. Jacobs category:cs.CV published:2012-05-20 summary:We analyze spectral graph theory based image segmentation algorithms and showthere is a natural connection with edge preserving filtering. Based on thisconnection we show that the normalized cut algorithm is equivalent to repeatedapplication of bilateral filtering. Then, using this interpretation we presentand implement a fast normalized cut algorithm. Experiments show that ourimplementation can solve the original optimization problem with a 10x-100xspeedup. Furthermore, we show this connection makes possible a new model forsegmentation called conditioned normalized cut that easily incorporates imagepatches in color and demonstrate how this problem can be solved with edgepreserving filtering.
arxiv-1205-4463 | Pilgrims Face Recognition Dataset -- HUFRD |  http://arxiv.org/abs/1205.4463  | author:Salah A. Aly category:cs.CV cs.CY published:2012-05-20 summary:In this work, we define a new pilgrims face recognition dataset, called HUFRDdataset. The new developed dataset presents various pilgrims' images taken fromoutside the Holy Masjid El-Harram in Makkah during the 2011-2012 Hajj and Umrahseasons. Such dataset will be used to test our developed facial recognition anddetection algorithms, as well as assess in the missing and found recognitionsystem \cite{crowdsensing}.
arxiv-1206-4880 | Dynamic Domain Classification for Fractal Image Compression |  http://arxiv.org/abs/1206.4880  | author:K. Revathy, M. Jayamohan category:cs.CV cs.GR published:2012-05-20 summary:Fractal image compression is attractive except for its high encoding timerequirements. The image is encoded as a set of contractive affinetransformations. The image is partitioned into non-overlapping range blocks,and a best matching domain block larger than the range block is identified.There are many attempts on improving the encoding time by reducing the size ofsearch pool for range-domain matching. But these methods are attempting toprepare a static domain pool that remains unchanged throughout the encodingprocess. This paper proposes dynamic preparation of separate domain pool foreach range block. This will result in significant reduction in the encodingtime. The domain pool for a particular range block can be selected based upon aparametric value. Here we use classification based on local fractal dimension.
arxiv-1205-4377 | Multi-Stage Classifier Design |  http://arxiv.org/abs/1205.4377  | author:Kirill Trapeznikov, Venkatesh Saligrama, David Castanon category:cs.CV stat.ML published:2012-05-20 summary:In many classification systems, sensing modalities have different acquisitioncosts. It is often {\it unnecessary} to use every modality to classify amajority of examples. We study a multi-stage system in a prediction time costreduction setting, where the full data is available for training, but for atest example, measurements in a new modality can be acquired at each stage foran additional cost. We seek decision rules to reduce the average measurementacquisition cost. We formulate an empirical risk minimization problem (ERM) fora multi-stage reject classifier, wherein the stage $k$ classifier eitherclassifies a sample using only the measurements acquired so far or rejects itto the next stage where more attributes can be acquired for a cost. To solvethe ERM problem, we show that the optimal reject classifier at each stage is acombination of two binary classifiers, one biased towards positive examples andthe other biased towards negative examples. We use this parameterization toconstruct stage-by-stage global surrogate risk, develop an iterative algorithmin the boosting framework and present convergence and generalization results.We test our work on synthetic, medical and explosives detection datasets. Ourresults demonstrate that substantial cost reduction without a significantsacrifice in accuracy is achievable.
arxiv-1205-4387 | Precision-biased Parsing and High-Quality Parse Selection |  http://arxiv.org/abs/1205.4387  | author:Yoav Goldberg, Michael Elhadad category:cs.CL published:2012-05-20 summary:We introduce precision-biased parsing: a parsing task which favors precisionover recall by allowing the parser to abstain from decisions deemed uncertain.We focus on dependency-parsing and present an ensemble method which is capableof assigning parents to 84% of the text tokens while being over 96% accurate onthese tokens. We use the precision-biased parsing task to solve the relatedhigh-quality parse-selection task: finding a subset of high-quality (accurate)trees in a large collection of parsed text. We present a method for choosingover a third of the input trees while keeping unlabeled dependency parsingaccuracy of 97% on these trees. We also present a method which is not based onan ensemble but rather on directly predicting the risk associated withindividual parser decisions. In addition to its efficiency, this methoddemonstrates that a parsing system can provide reasonable estimates ofconfidence in its predictions without relying on ensembles or aggregate corpuscounts.
arxiv-1205-4234 | Visualization of features of a series of measurements with one-dimensional cellular structure |  http://arxiv.org/abs/1205.4234  | author:D. V. Lande category:cs.LG 68R published:2012-05-19 summary:This paper describes the method of visualization of periodic constituents andinstability areas in series of measurements, being based on the algorithm ofsmoothing out and concept of one-dimensional cellular automata. A method can beused at the analysis of temporal series, related to the volumes of thematicpublications in web-space.
arxiv-1205-4343 | New Analysis and Algorithm for Learning with Drifting Distributions |  http://arxiv.org/abs/1205.4343  | author:Mehryar Mohri, Andres Munoz Medina category:cs.LG stat.ML published:2012-05-19 summary:We present a new analysis of the problem of learning with driftingdistributions in the batch setting using the notion of discrepancy. We provelearning bounds based on the Rademacher complexity of the hypothesis set andthe discrepancy of distributions both for a drifting PAC scenario and atracking scenario. Our bounds are always tighter and in some casessubstantially improve upon previous ones based on the $L_1$ distance. We alsopresent a generalization of the standard on-line to batch conversion to thedrifting scenario in terms of the discrepancy and arbitrary convex combinationsof hypotheses. We introduce a new algorithm exploiting these learningguarantees, which we show can be formulated as a simple QP. Finally, we reportthe results of preliminary experiments demonstrating the benefits of thisalgorithm.
arxiv-1205-4298 | Task-specific Word-Clustering for Part-of-Speech Tagging |  http://arxiv.org/abs/1205.4298  | author:Yoav Goldberg category:cs.CL published:2012-05-19 summary:While the use of cluster features became ubiquitous in core NLP tasks, mostcluster features in NLP are based on distributional similarity. We propose anew type of clustering criteria, specific to the task of part-of-speechtagging. Instead of distributional similarity, these clusters are based on thebeha vior of a baseline tagger when applied to a large corpus. These clusterfeatures provide similar gains in accuracy to those achieved bydistributional-similarity derived clusters. Using both types of clusterfeatures together further improve tagging accuracies. We show that the methodis effective for both the in-domain and out-of-domain scenarios for English,and for French, German and Italian. The effect is larger for out-of-domaintext.
arxiv-1205-4349 | From Exact Learning to Computing Boolean Functions and Back Again |  http://arxiv.org/abs/1205.4349  | author:Sergiu Goschin category:cs.LG cs.DM published:2012-05-19 summary:The goal of the paper is to relate complexity measures associated with theevaluation of Boolean functions (certificate complexity, decision treecomplexity) and learning dimensions used to characterize exact learning(teaching dimension, extended teaching dimension). The high level motivation isto discover non-trivial relations between exact learning of an unknown conceptand testing whether an unknown concept is part of a concept class or not.Concretely, the goal is to provide lower and upper bounds of complexitymeasures for one problem type in terms of the other.
arxiv-1205-4336 | Fuzzy - Rough Feature Selection With Î - Membership Function For Mammogram Classification |  http://arxiv.org/abs/1205.4336  | author:K. Thangavel, R. Roselin category:cs.CV published:2012-05-19 summary:Breast cancer is the second leading cause for death among women and it isdiagnosed with the help of mammograms. Oncologists are miserably failed inidentifying the micro calcification at the early stage with the help of themammogram visually. In order to improve the performance of the breast cancerscreening, most of the researchers have proposed Computer Aided Diagnosis usingimage processing. In this study mammograms are preprocessed and features areextracted, then the abnormality is identified through the classification. Ifall the extracted features are used, most of the cases are misidentified. Hencefeature selection procedure is sought. In this paper, Fuzzy-Rough featureselection with {\pi} membership function is proposed. The selected features areused to classify the abnormalities with help of Ant-Miner and Weka tools. Theexperimental analysis shows that the proposed method improves the mammogramsclassification accuracy.
arxiv-1205-4324 | Universal Properties of Mythological Networks |  http://arxiv.org/abs/1205.4324  | author:PÃ¡draig Mac Carron, Ralph Kenna category:physics.soc-ph cs.CL cs.SI published:2012-05-19 summary:As in statistical physics, the concept of universality plays an important,albeit qualitative, role in the field of comparative mythology. Here we applystatistical mechanical tools to analyse the networks underlying three iconicmythological narratives with a view to identifying common and distinguishingquantitative features. Of the three narratives, an Anglo-Saxon and a Greek textare mostly believed by antiquarians to be partly historically based while thethird, an Irish epic, is often considered to be fictional. Here we show thatnetwork analysis is able to discriminate real from imaginary social networksand place mythological narratives on the spectrum between them. Moreover, theperceived artificiality of the Irish narrative can be traced back to anomalousfeatures associated with six characters. Considering these as amalgams ofseveral entities or proxies, renders the plausibility of the Irish textcomparable to the others from a network-theoretic point of view.
arxiv-1205-4295 | Efficient Methods for Unsupervised Learning of Probabilistic Models |  http://arxiv.org/abs/1205.4295  | author:Jascha Sohl-Dickstein category:cs.LG cs.AI cs.IT cs.NE math.IT published:2012-05-19 summary:In this thesis I develop a variety of techniques to train, evaluate, andsample from intractable and high dimensional probabilistic models. Abstractexceeds arXiv space limitations -- see PDF.
arxiv-1205-4213 | Online Structured Prediction via Coactive Learning |  http://arxiv.org/abs/1205.4213  | author:Pannaga Shivaswamy, Thorsten Joachims category:cs.LG cs.AI cs.IR published:2012-05-18 summary:We propose Coactive Learning as a model of interaction between a learningsystem and a human user, where both have the common goal of providing resultsof maximum utility to the user. At each step, the system (e.g. search engine)receives a context (e.g. query) and predicts an object (e.g. ranking). The userresponds by correcting the system if necessary, providing a slightly improved-- but not necessarily optimal -- object as feedback. We argue that suchfeedback can often be inferred from observable user behavior, for example, fromclicks in web-search. Evaluating predictions by their cardinal utility to theuser, we propose efficient learning algorithms that have ${\calO}(\frac{1}{\sqrt{T}})$ average regret, even though the learning algorithmnever observes cardinal utility values as in conventional online learning. Wedemonstrate the applicability of our model and learning algorithms on a movierecommendation task, as well as ranking for web-search.
arxiv-1205-4220 | Diffusion Adaptation over Networks |  http://arxiv.org/abs/1205.4220  | author:Ali H. Sayed category:cs.MA cs.LG published:2012-05-18 summary:Adaptive networks are well-suited to perform decentralized informationprocessing and optimization tasks and to model various types of self-organizedand complex behavior encountered in nature. Adaptive networks consist of acollection of agents with processing and learning abilities. The agents arelinked together through a connection topology, and they cooperate with eachother through local interactions to solve distributed optimization, estimation,and inference problems in real-time. The continuous diffusion of informationacross the network enables agents to adapt their performance in relation tostreaming data and network conditions; it also results in improved adaptationand learning performance relative to non-cooperative agents. This articleprovides an overview of diffusion strategies for adaptation and learning overnetworks. The article is divided into several sections: 1. Motivation; 2.Mean-Square-Error Estimation; 3. Distributed Optimization via DiffusionStrategies; 4. Adaptive Diffusion Strategies; 5. Performance ofSteepest-Descent Diffusion Strategies; 6. Performance of Adaptive DiffusionStrategies; 7. Comparing the Performance of Cooperative Strategies; 8.Selecting the Combination Weights; 9. Diffusion with Noisy InformationExchanges; 10. Extensions and Further Considerations; Appendix A: Properties ofKronecker Products; Appendix B: Graph Laplacian and Network Connectivity;Appendix C: Stochastic Matrices; Appendix D: Block Maximum Norm; Appendix E:Comparison with Consensus Strategies; References.
arxiv-1205-4120 | Two New Algorithms for Solving Covariance Graphical Lasso Based on Coordinate Descent and ECM |  http://arxiv.org/abs/1205.4120  | author:Hao Wang category:stat.CO stat.ML published:2012-05-18 summary:Covariance graphical lasso applies a lasso penalty on the elements of thecovariance matrix. This method is useful because it not only produces sparseestimation of covariance matrix but also discovers marginal independencestructures by generating zeros in the covariance matrix. We propose and exploretwo new algorithms for solving the covariance graphical lasso problem. Our newalgorithms are based on coordinate descent and ECM. We show that these twoalgorithms are more attractive than the only existing competing algorithm ofBien and Tibshirani (2011) in terms of simplicity, speed and stability. We alsodiscuss convergence properties of our algorithms.
arxiv-1205-4159 | Theory of Dependent Hierarchical Normalized Random Measures |  http://arxiv.org/abs/1205.4159  | author:Changyou Chen, Wray Buntine, Nan Ding category:cs.LG math.ST stat.ML stat.TH published:2012-05-18 summary:This paper presents theory for Normalized Random Measures (NRMs), NormalizedGeneralized Gammas (NGGs), a particular kind of NRM, and Dependent HierarchicalNRMs which allow networks of dependent NRMs to be analysed. These have beenused, for instance, for time-dependent topic modelling. In this paper, we firstintroduce some mathematical background of completely random measures (CRMs) andtheir construction from Poisson processes, and then introduce NRMs and NGGs.Slice sampling is also introduced for posterior inference. The dependencyoperators in Poisson processes and for the corresponding CRMs and NRMs is thenintroduced and Posterior inference for the NGG presented. Finally, we givedependency and composition results when applying these operators to NRMs sothey can be used in a network with hierarchical and dependent relations.
arxiv-1205-4133 | Constrained Overcomplete Analysis Operator Learning for Cosparse Signal Modelling |  http://arxiv.org/abs/1205.4133  | author:Mehrdad Yaghoobi, Sangnam Nam, Remi Gribonval, Mike E. Davies category:math.NA cs.LG published:2012-05-18 summary:We consider the problem of learning a low-dimensional signal model from acollection of training samples. The mainstream approach would be to learn anovercomplete dictionary to provide good approximations of the training samplesusing sparse synthesis coefficients. This famous sparse model has a less wellknown counterpart, in analysis form, called the cosparse analysis model. Inthis new model, signals are characterised by their parsimony in a transformeddomain using an overcomplete (linear) analysis operator. We propose to learn ananalysis operator from a training corpus using a constrained optimisationframework based on L1 optimisation. The reason for introducing a constraint inthe optimisation framework is to exclude trivial solutions. Although there isno final answer here for which constraint is the most relevant constraint, weinvestigate some conventional constraints in the model adaptation field and usethe uniformly normalised tight frame (UNTF) for this purpose. We then derive apractical learning algorithm, based on projected subgradients andDouglas-Rachford splitting technique, and demonstrate its ability to robustlyrecover a ground truth analysis operator, when provided with a clean trainingset, of sufficient size. We also find an analysis operator for images, usingsome noisy cosparse signals, which is indeed a more realistic experiment. Asthe derived optimisation problem is not a convex program, we often find a localminimum using such variational methods. Some local optimality conditions arederived for two different settings, providing preliminary theoretical supportfor the well-posedness of the learning problem under appropriate conditions.
arxiv-1205-4217 | Thompson Sampling: An Asymptotically Optimal Finite Time Analysis |  http://arxiv.org/abs/1205.4217  | author:Emilie Kaufmann, Nathaniel Korda, RÃ©mi Munos category:stat.ML cs.LG published:2012-05-18 summary:The question of the optimality of Thompson Sampling for solving thestochastic multi-armed bandit problem had been open since 1933. In this paperwe answer it positively for the case of Bernoulli rewards by providing thefirst finite-time analysis that matches the asymptotic rate given in the Laiand Robbins lower bound for the cumulative regret. The proof is accompanied bya numerical comparison with other optimal policies, experiments that have beenlacking in the literature until now for the Bernoulli case.
arxiv-1205-3966 | Neural Networks for Handwritten English Alphabet Recognition |  http://arxiv.org/abs/1205.3966  | author:Yusuf Perwej, Ashish Chaturvedi category:cs.AI cs.CV published:2012-05-17 summary:This paper demonstrates the use of neural networks for developing a systemthat can recognize hand-written English alphabets. In this system, each Englishalphabet is represented by binary values that are used as input to a simplefeature extraction system, whose output is fed to our neural network system.
arxiv-1205-3999 | Optimal Weights Mixed Filter for Removing Mixture of Gaussian and Impulse Noises |  http://arxiv.org/abs/1205.3999  | author:Qiyu Jin, Ion Grama, Quansheng Liu category:cs.CV published:2012-05-17 summary:According to the character of Gaussian, we modify the Rank-Ordered AbsoluteDifferences (ROAD) to Rank-Ordered Absolute Differences of mixture of Gaussianand impulse noises (ROADG). It will be more effective to detect impulse noisewhen the impulse is mixed with Gaussian noise. Combining rightly the ROADG withOptimal Weights Filter (OWF), we obtain a new method to deal with the mixednoise, called Optimal Weights Mixed Filter (OWMF). The simulation results showthat the method is effective to remove the mixed noise.
arxiv-1205-3997 | Free Energy and the Generalized Optimality Equations for Sequential Decision Making |  http://arxiv.org/abs/1205.3997  | author:Pedro A. Ortega, Daniel A. Braun category:stat.ML cs.AI cs.GT cs.SY published:2012-05-17 summary:The free energy functional has recently been proposed as a variationalprinciple for bounded rational decision-making, since it instantiates a naturaltrade-off between utility gains and information processing costs that can beaxiomatically derived. Here we apply the free energy principle to generaldecision trees that include both adversarial and stochastic environments. Wederive generalized sequential optimality equations that not only include theBellman optimality equations as a limit case, but also lead to well-knowndecision-rules such as Expectimax, Minimax and Expectiminimax. We show howthese decision-rules can be derived from a single free energy principle thatassigns a resource parameter to each node in the decision tree. These resourceparameters express a concrete computational cost that can be measured as theamount of samples that are needed from the distribution that belongs to eachnode. The free energy principle therefore provides the normative basis forgeneralized optimality equations that account for both adversarial andstochastic environments.
arxiv-1205-3981 | kLog: A Language for Logical and Relational Learning with Kernels |  http://arxiv.org/abs/1205.3981  | author:Paolo Frasconi, Fabrizio Costa, Luc De Raedt, Kurt De Grave category:cs.AI cs.LG cs.PL published:2012-05-17 summary:We introduce kLog, a novel approach to statistical relational learning.Unlike standard approaches, kLog does not represent a probability distributiondirectly. It is rather a language to perform kernel-based learning onexpressive logical and relational representations. kLog allows users to specifylearning problems declaratively. It builds on simple but powerful concepts:learning from interpretations, entity/relationship data modeling, logicprogramming, and deductive databases. Access by the kernel to the richrepresentation is mediated by a technique we call graphicalization: therelational representation is first transformed into a graph --- in particular,a grounded entity/relationship diagram. Subsequently, a choice of graph kerneldefines the feature space. kLog supports mixed numerical and symbolic data, aswell as background knowledge in the form of Prolog or Datalog programs as ininductive logic programming systems. The kLog framework can be applied totackle the same range of tasks that has made statistical relational learning sopopular, including classification, regression, multitask learning, andcollective classification. We also report about empirical comparisons, showingthat kLog can be either more accurate, or much faster at the same level ofaccuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available athttp://klog.dinfo.unifi.it along with tutorials.
arxiv-1205-3767 | Universal Algorithm for Online Trading Based on the Method of Calibration |  http://arxiv.org/abs/1205.3767  | author:Vladimir V'yugin, Vladimir Trunov category:cs.LG q-fin.PM published:2012-05-16 summary:We present a universal algorithm for online trading in Stock Market whichperforms asymptotically at least as good as any stationary trading strategythat computes the investment at each step using a fixed function of the sideinformation that belongs to a given RKHS (Reproducing Kernel Hilbert Space).Using a universal kernel, we extend this result for any continuous stationarystrategy. In this learning process, a trader rationally chooses his gamblesusing predictions made by a randomized well-calibrated algorithm. Our strategyis based on Dawid's notion of calibration with more general checking rules andon some modification of Kakade and Foster's randomized rounding algorithm forcomputing the well-calibrated forecasts. We combine the method of randomizedcalibration with Vovk's method of defensive forecasting in RKHS. Unlike thestatistical theory, no stochastic assumptions are made about the stock prices.Our empirical results on historical markets provide strong evidence that thistype of technical trading can "beat the market" if transaction costs areignored.
arxiv-1205-3766 | Efficient Topology-Controlled Sampling of Implicit Shapes |  http://arxiv.org/abs/1205.3766  | author:Jason Chang, John W. Fisher III category:cs.CV published:2012-05-16 summary:Sampling from distributions of implicitly defined shapes enables analysis ofvarious energy functionals used for image segmentation. Recent work describes acomputationally efficient Metropolis-Hastings method for accomplishing thistask. Here, we extend that framework so that samples are accepted at everyiteration of the sampler, achieving an order of magnitude speed up inconvergence. Additionally, we show how to incorporate topological constraints.
arxiv-1205-3776 | The ideal of the trifocal variety |  http://arxiv.org/abs/1205.3776  | author:Chris Aholt, Luke Oeding category:math.AG cs.CV published:2012-05-16 summary:Techniques from representation theory, symbolic computational algebra, andnumerical algebraic geometry are used to find the minimal generators of theideal of the trifocal variety. An effective test for determining whether agiven tensor is a trifocal tensor is also given.
arxiv-1205-3549 | Normalized Maximum Likelihood Coding for Exponential Family with Its Applications to Optimal Clustering |  http://arxiv.org/abs/1205.3549  | author:So Hirai, Kenji Yamanishi category:cs.LG published:2012-05-16 summary:We are concerned with the issue of how to calculate the normalized maximumlikelihood (NML) code-length. There is a problem that the normalization term ofthe NML code-length may diverge when it is continuous and unbounded and astraightforward computation of it is highly expensive when the data domain isfinite . In previous works it has been investigated how to calculate the NMLcode-length for specific types of distributions. We first propose a generalmethod for computing the NML code-length for the exponential family. Then wespecifically focus on Gaussian mixture model (GMM), and propose a new efficientmethod for computing the NML to them. We develop it by generalizing Rissanen'sre-normalizing technique. Then we apply this method to the clustering issue, inwhich a clustering structure is modeled using a GMM, and the main task is toestimate the optimal number of clusters on the basis of the NML code-length. Wedemonstrate using artificial data sets the superiority of the NML-basedclustering over other criteria such as AIC, BIC in terms of the data sizerequired for high accuracy rate to be achieved.
arxiv-1205-3316 | Arabic Language Learning Assisted by Computer, based on Automatic Speech Recognition |  http://arxiv.org/abs/1205.3316  | author:Naim Terbeh, Mounir Zrigui category:cs.CL published:2012-05-15 summary:This work consists of creating a system of the Computer Assisted LanguageLearning (CALL) based on a system of Automatic Speech Recognition (ASR) for theArabic language using the tool CMU Sphinx3 [1], based on the approach of HMM.To this work, we have constructed a corpus of six hours of speech recordingswith a number of nine speakers. we find in the robustness to noise a groundsfor the choice of the HMM approach [2]. the results achieved are encouragingsince our corpus is made by only nine speakers, but they are always reasonsthat open the door for other improvement works.
arxiv-1205-3336 | Distribution of the search of evolutionary product unit neural networks for classification |  http://arxiv.org/abs/1205.3336  | author:A. J. TallÃ³n-Ballesteros, P. A. GutiÃ©rrez-PeÃ±a, C. HervÃ¡s-MartÃ­nez category:cs.NE cs.AI cs.CV published:2012-05-15 summary:This paper deals with the distributed processing in the search for an optimumclassification model using evolutionary product unit neural networks. For thisdistributed search we used a cluster of computers. Our objective is to obtain amore efficient design than those net architectures which do not use adistributed process and which thus result in simpler designs. In order to getthe best classification models we use evolutionary algorithms to train anddesign neural networks, which require a very time consuming computation. Thereasons behind the need for this distribution are various. It is complicated totrain this type of nets because of the difficulty entailed in determining theirarchitecture due to the complex error surface. On the other hand, the use ofevolutionary algorithms involves running a great number of tests with differentseeds and parameters, thus resulting in a high computational cost
arxiv-1205-3234 | Asymptotic Accuracy of Bayes Estimation for Latent Variables with Redundancy |  http://arxiv.org/abs/1205.3234  | author:Keisuke Yamazaki category:stat.ML published:2012-05-15 summary:Hierarchical parametric models consisting of observable and latent variablesare widely used for unsupervised learning tasks. For example, a mixture modelis a representative hierarchical model for clustering. From the statisticalpoint of view, the models can be regular or singular due to the distribution ofdata. In the regular case, the models have the identifiability; there isone-to-one relation between a probability density function for the modelexpression and the parameter. The Fisher information matrix is positivedefinite, and the estimation accuracy of both observable and latent variableshas been studied. In the singular case, on the other hand, the models are notidentifiable and the Fisher matrix is not positive definite. Conventionalstatistical analysis based on the inverse Fisher matrix is not applicable.Recently, an algebraic geometrical analysis has been developed and is used toelucidate the Bayes estimation of observable variables. The present paperapplies this analysis to latent-variable estimation and determines itstheoretical performance. Our results clarify behavior of the convergence of theposterior distribution. It is found that the posterior of theobservable-variable estimation can be different from the one in thelatent-variable estimation. Because of the difference, the Markov chain MonteCarlo method based on the parameter and the latent variable cannot constructthe desired posterior distribution.
arxiv-1205-3137 | Unsupervised Discovery of Mid-Level Discriminative Patches |  http://arxiv.org/abs/1205.3137  | author:Saurabh Singh, Abhinav Gupta, Alexei A. Efros category:cs.CV cs.AI cs.LG published:2012-05-14 summary:The goal of this paper is to discover a set of discriminative patches whichcan serve as a fully unsupervised mid-level visual representation. The desiredpatches need to satisfy two requirements: 1) to be representative, they need tooccur frequently enough in the visual world; 2) to be discriminative, they needto be different enough from the rest of the visual world. The patches couldcorrespond to parts, objects, "visual phrases", etc. but are not restricted tobe any one of them. We pose this as an unsupervised discriminative clusteringproblem on a huge dataset of image patches. We use an iterative procedure whichalternates between clustering and training discriminative classifiers, whileapplying careful cross-validation at each step to prevent overfitting. Thepaper experimentally demonstrates the effectiveness of discriminative patchesas an unsupervised mid-level visual representation, suggesting that it could beused in place of visual words for many tasks. Furthermore, discriminativepatches can also be used in a supervised regime, such as scene classification,where they demonstrate state-of-the-art performance on the MIT Indoor-67dataset.
arxiv-1205-3217 | A Generalized Fellegi-Sunter Framework for Multiple Record Linkage With Application to Homicide Record Systems |  http://arxiv.org/abs/1205.3217  | author:Mauricio Sadinle, Stephen E. Fienberg category:stat.AP stat.ME stat.ML stat.OT published:2012-05-14 summary:We present a probabilistic method for linking multiple datafiles. This taskis not trivial in the absence of unique identifiers for the individualsrecorded. This is a common scenario when linking census data to coveragemeasurement surveys for census coverage evaluation, and in general whenmultiple record-systems need to be integrated for posterior analysis. Ourmethod generalizes the Fellegi-Sunter theory for linking records from twodatafiles and its modern implementations. The multiple record linkage goal isto classify the record K-tuples coming from K datafiles according to thedifferent matching patterns. Our method incorporates the transitivity ofagreement in the computation of the data used to model matching probabilities.We use a mixture model to fit matching probabilities via maximum likelihoodusing the EM algorithm. We present a method to decide the record K-tuplesmembership to the subsets of matching patterns and we prove its optimality. Weapply our method to the integration of three Colombian homicide record systemsand we perform a simulation study in order to explore the performance of themethod under measurement error and different scenarios. The proposed methodworks well and opens some directions for future research.
arxiv-1205-3109 | Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search |  http://arxiv.org/abs/1205.3109  | author:Arthur Guez, David Silver, Peter Dayan category:cs.LG cs.AI stat.ML published:2012-05-14 summary:Bayesian model-based reinforcement learning is a formally elegant approach tolearning optimal behaviour under model uncertainty, trading off exploration andexploitation in an ideal way. Unfortunately, finding the resultingBayes-optimal policies is notoriously taxing, since the search space becomesenormous. In this paper we introduce a tractable, sample-based method forapproximate Bayes-optimal planning which exploits Monte-Carlo tree search. Ourapproach outperformed prior Bayesian model-based RL algorithms by a significantmargin on several well-known benchmark problems -- because it avoids expensiveapplications of Bayes rule within the search tree by lazily sampling modelsfrom the current beliefs. We illustrate the advantages of our approach byshowing it working in an infinite state space domain which is qualitatively outof reach of almost all previous work in Bayesian exploration.
arxiv-1205-2930 | Density Sensitive Hashing |  http://arxiv.org/abs/1205.2930  | author:Yue Lin, Deng Cai, Cheng Li category:cs.IR cs.LG published:2012-05-14 summary:Nearest neighbors search is a fundamental problem in various research fieldslike machine learning, data mining and pattern recognition. Recently,hashing-based approaches, e.g., Locality Sensitive Hashing (LSH), are proved tobe effective for scalable high dimensional nearest neighbors search. Manyhashing algorithms found their theoretic root in random projection. Since thesealgorithms generate the hash tables (projections) randomly, a large number ofhash tables (i.e., long codewords) are required in order to achieve both highprecision and recall. To address this limitation, we propose a novel hashingalgorithm called {\em Density Sensitive Hashing} (DSH) in this paper. DSH canbe regarded as an extension of LSH. By exploring the geometric structure of thedata, DSH avoids the purely random projections selection and uses thoseprojective functions which best agree with the distribution of the data.Extensive experimental results on real-world data sets have shown that theproposed method achieves better performance compared to the state-of-the-arthashing approaches.
arxiv-1205-3193 | A Comparative Study of Collaborative Filtering Algorithms |  http://arxiv.org/abs/1205.3193  | author:Joonseok Lee, Mingxuan Sun, Guy Lebanon category:cs.IR stat.ML I.2.6; H.2.8 published:2012-05-14 summary:Collaborative filtering is a rapidly advancing research area. Every yearseveral new techniques are proposed and yet it is not clear which of thetechniques work best and under what conditions. In this paper we conduct astudy comparing several collaborative filtering techniques -- both classic andrecent state-of-the-art -- in a variety of experimental contexts. Specifically,we report conclusions controlling for number of items, number of users,sparsity level, performance criteria, and computational complexity. Ourconclusions identify what algorithms work well and in what conditions, andcontribute to both industrial deployment collaborative filtering algorithms andto the research community.
arxiv-1205-2958 | b-Bit Minwise Hashing in Practice: Large-Scale Batch and Online Learning and Using GPUs for Fast Preprocessing with Simple Hash Functions |  http://arxiv.org/abs/1205.2958  | author:Ping Li, Anshumali Shrivastava, Arnd Christian Konig category:cs.IR cs.DB cs.LG published:2012-05-14 summary:In this paper, we study several critical issues which must be tackled beforeone can apply b-bit minwise hashing to the volumes of data often usedindustrial applications, especially in the context of search. 1. (b-bit) Minwise hashing requires an expensive preprocessing step thatcomputes k (e.g., 500) minimal values after applying the correspondingpermutations for each data vector. We developed a parallelization scheme usingGPUs and observed that the preprocessing time can be reduced by a factor of20-80 and becomes substantially smaller than the data loading time. 2. One major advantage of b-bit minwise hashing is that it can substantiallyreduce the amount of memory required for batch learning. However, as onlinealgorithms become increasingly popular for large-scale learning in the contextof search, it is not clear if b-bit minwise yields significant improvements forthem. This paper demonstrates that $b$-bit minwise hashing provides aneffective data size/dimension reduction scheme and hence it can dramaticallyreduce the data loading time for each epoch of the online training process.This is significant because online learning often requires many (e.g., 10 to100) epochs to reach a sufficient accuracy. 3. Another critical issue is that for very large data sets it becomesimpossible to store a (fully) random permutation matrix, due to its spacerequirements. Our paper is the first study to demonstrate that $b$-bit minwisehashing implemented using simple hash functions, e.g., the 2-universal (2U) and4-universal (4U) hash families, can produce very similar learning results asusing fully random permutations. Experiments on datasets of up to 200GB arepresented.
arxiv-1205-3181 | Multiple Identifications in Multi-Armed Bandits |  http://arxiv.org/abs/1205.3181  | author:SÃ©bastien Bubeck, Tengyao Wang, Nitin Viswanathan category:cs.LG stat.ML published:2012-05-14 summary:We study the problem of identifying the top $m$ arms in a multi-armed banditgame. Our proposed solution relies on a new algorithm based on successiverejects of the seemingly bad arms, and successive accepts of the good ones.This algorithmic contribution allows to tackle other multiple identificationssettings that were previously out of reach. In particular we show that thisidea of successive accepts and rejects applies to the multi-bandit best armidentification problem.
arxiv-1205-3183 | A Model-Driven Probabilistic Parser Generator |  http://arxiv.org/abs/1205.3183  | author:Luis Quesada, Fernando Berzal, Francisco J. Cortijo category:cs.CL published:2012-05-14 summary:Existing probabilistic scanners and parsers impose hard constraints on theway lexical and syntactic ambiguities can be resolved. Furthermore, traditionalgrammar-based parsing tools are limited in the mechanisms they allow for takingcontext into account. In this paper, we propose a model-driven tool that allowsfor statistical language models with arbitrary probability estimators. Our workon model-driven probabilistic parsing is built on top of ModelCC, a model-basedparser generator, and enables the probabilistic interpretation and resolutionof anaphoric, cataphoric, and recursive references in the disambiguation ofabstract syntax graphs. In order to prove the expression power of ModelCC, wedescribe the design of a general-purpose natural language parser.
arxiv-1205-2874 | Decoupling Exploration and Exploitation in Multi-Armed Bandits |  http://arxiv.org/abs/1205.2874  | author:Orly Avner, Shie Mannor, Ohad Shamir category:cs.LG published:2012-05-13 summary:We consider a multi-armed bandit problem where the decision maker can exploreand exploit different arms at every round. The exploited arm adds to thedecision maker's cumulative reward (without necessarily observing the reward)while the explored arm reveals its value. We devise algorithms for this setupand show that the dependence on the number of arms, k, can be much better thanthe standard square root of k dependence, depending on the behavior of thearms' reward sequences. For the important case of piecewise stationarystochastic bandits, we show a significant improvement over existing algorithms.Our algorithms are based on a non-uniform sampling policy, which we show isessential to the success of any algorithm in the adversarial setup. Finally, weshow some simulation results on an ultra-wide band channel selection inspiredsetting indicating the applicability of our algorithms.
arxiv-1205-2821 | Texture Analysis And Characterization Using Probability Fractal Descriptors |  http://arxiv.org/abs/1205.2821  | author:J. B. Florindo, O. M. Bruno category:cs.CV published:2012-05-13 summary:A gray-level image texture descriptors based on fractal dimension estimationis proposed in this work. The proposed method estimates the fractal dimensionusing probability (Voss) method. The descriptors are computed applying amultiscale transform to the fractal dimension curves of the texture image. Theproposed texture descriptor method is evaluated in a classification task ofwell known benchmark texture datasets. The results show the great performanceof the proposed method as a tool for texture images analysis andcharacterization.
arxiv-1205-2797 | Forecasting of Indian Rupee (INR) / US Dollar (USD) Currency Exchange Rate Using Artificial Neural Network |  http://arxiv.org/abs/1205.2797  | author:Yusuf Perwej, Asif Perwej category:cs.NE published:2012-05-12 summary:A large part of the workforce, and growing every day, is originally fromIndia. India one of the second largest populations in the world, they have alot to offer in terms of jobs. The sheer number of IT workers makes them aformidable travelling force as well, easily picking up employment in Englishspeaking countries. The beginning of the economic crises since 2008 September,many Indians have return homeland, and this has had a substantial impression onthe Indian Rupee (INR) as liken to the US Dollar (USD). We are usingnumerational knowledge based techniques for forecasting has been proved highlysuccessful in present time. The purpose of this paper is to examine the effectsof several important neural network factors on model fitting and forecastingthe behaviours. In this paper, Artificial Neural Network has successfully beenused for exchange rate forecasting. This paper examines the effects of thenumber of inputs and hidden nodes and the size of the training sample on thein-sample and out-of-sample performance. The Indian Rupee (INR) / US Dollar(USD) is used for detailed examinations. The number of input nodes has agreater impact on performance than the number of hidden nodes, while a largenumber of observations do reduce forecast errors.
arxiv-1205-2663 | Are visual dictionaries generalizable? |  http://arxiv.org/abs/1205.2663  | author:Otavio A. B. Penatti, Eduardo Valle, Ricardo da S. Torres category:cs.CV published:2012-05-11 summary:Mid-level features based on visual dictionaries are today a cornerstone ofsystems for classification and retrieval of images. Those state-of-the-artrepresentations depend crucially on the choice of a codebook (visualdictionary), which is usually derived from the dataset. In general-purpose,dynamic image collections (e.g., the Web), one cannot have the entirecollection in order to extract a representative dictionary. However, based onthe hypothesis that the dictionary reflects only the diversity of low-levelappearances and does not capture semantics, we argue that a dictionary based ona small subset of the data, or even on an entirely different dataset, is ableto produce a good representation, provided that the chosen images span adiverse enough portion of the low-level feature space. Our experiments confirmthat hypothesis, opening the opportunity to greatly alleviate the burden ingenerating the codebook, and confirming the feasibility of employing visualdictionaries in large-scale dynamic environments.
arxiv-1205-2536 | Identifiability of Gaussian structural equation models with equal error variances |  http://arxiv.org/abs/1205.2536  | author:Jonas Peters, Peter BÃ¼hlmann category:stat.ML math.ST stat.TH published:2012-05-11 summary:We consider structural equation models in which variables can be written as afunction of their parents and noise terms, which are assumed to be jointlyindependent. Corresponding to each structural equation model, there is adirected acyclic graph describing the relationships between the variables. InGaussian structural equation models with linear functions, the graph can beidentified from the joint distribution only up to Markov equivalence classes,assuming faithfulness. In this work, we prove full identifiability if all noisevariables have the same variances: the directed acyclic graph can be recoveredfrom the joint Gaussian distribution. Our result has direct implications forcausal inference: if the data follow a Gaussian structural equation model withequal error variances and assuming that all variables are observed, the causalstructure can be inferred from observational data only. We propose astatistical method and an algorithm that exploit our theoretical findings.
arxiv-1205-2584 | Low Complexity Damped Gauss-Newton Algorithms for CANDECOMP/PARAFAC |  http://arxiv.org/abs/1205.2584  | author:Anh Huy Phan, Petr TichavskÃ½, Andrzej Cichocki category:cs.NA cs.LG math.OC published:2012-05-11 summary:The damped Gauss-Newton (dGN) algorithm for CANDECOMP/PARAFAC (CP)decomposition can handle the challenges of collinearity of factors anddifferent magnitudes of factors; nevertheless, for factorization of an $N$-Dtensor of size $I_1\times I_N$ with rank $R$, the algorithm is computationallydemanding due to construction of large approximate Hessian of size $(RT \timesRT)$ and its inversion where $T = \sum_n I_n$. In this paper, we propose a fastimplementation of the dGN algorithm which is based on novel expressions of theinverse approximate Hessian in block form. The new implementation has lowercomputational complexity, besides computation of the gradient (this part iscommon to both methods), requiring the inversion of a matrix of size$NR^2\times NR^2$, which is much smaller than the whole approximate Hessian, if$T \gg NR$. In addition, the implementation has lower memory requirements,because neither the Hessian nor its inverse never need to be stored in theirentirety. A variant of the algorithm working with complex valued data isproposed as well. Complexity and performance of the proposed algorithm iscompared with those of dGN and ALS with line search on examples of difficultbenchmark tensors.
arxiv-1205-2282 | A Discussion on Parallelization Schemes for Stochastic Vector Quantization Algorithms |  http://arxiv.org/abs/1205.2282  | author:Matthieu Durut, BenoÃ®t Patra, Fabrice Rossi category:stat.ML cs.DC cs.LG published:2012-05-10 summary:This paper studies parallelization schemes for stochastic Vector Quantizationalgorithms in order to obtain time speed-ups using distributed resources. Weshow that the most intuitive parallelization scheme does not lead to betterperformances than the sequential algorithm. Another distributed scheme istherefore introduced which obtains the expected speed-ups. Then, it is improvedto fit implementation on distributed architectures where communications areslow and inter-machines synchronization too costly. The schemes are tested withsimulated distributed architectures and, for the last one, with MicrosoftWindows Azure platform obtaining speed-ups up to 32 Virtual Machines.
arxiv-1205-2345 | Hajj and Umrah Event Recognition Datasets |  http://arxiv.org/abs/1205.2345  | author:Hossam Zawbaa, Salah A. Aly category:cs.CV cs.CY published:2012-05-10 summary:In this note, new Hajj and Umrah Event Recognition datasets (HUER) arepresented. The demonstrated datasets are based on videos and images takenduring 2011-2012 Hajj and Umrah seasons. HUER is the first collection ofdatasets covering the six types of Hajj and Umrah ritual events (rotating inTawaf around Kabaa, performing Sa'y between Safa and Marwa, standing on themount of Arafat, staying overnight in Muzdalifah, staying two or three days inMina, and throwing Jamarat). The HUER datasets also contain video and imagedatabases for nine types of human actions during Hajj and Umrah (walking,drinking from Zamzam water, sleeping, smiling, eating, praying, sitting,shaving hairs and ablutions, reading the holy Quran and making duaa). Thespatial resolutions are 1280 x 720 pixels for images and 640 x 480 pixels forvideos and have lengths of 20 seconds in average with 30 frame per secondrates.
arxiv-1205-2382 | Mesh Learning for Classifying Cognitive Processes |  http://arxiv.org/abs/1205.2382  | author:Mete Ozay, Ilke Ãztekin, Uygar Ãztekin, Fatos T. Yarman Vural category:cs.NE cs.AI cs.CV stat.ML published:2012-05-10 summary:A relatively recent advance in cognitive neuroscience has been multi-voxelpattern analysis (MVPA), which enables researchers to decode brain statesand/or the type of information represented in the brain during a cognitiveoperation. MVPA methods utilize machine learning algorithms to distinguishamong types of information or cognitive states represented in the brain, basedon distributed patterns of neural activity. In the current investigation, wepropose a new approach for representation of neural data for pattern analysis,namely a Mesh Learning Model. In this approach, at each time instant, a starmesh is formed around each voxel, such that the voxel corresponding to thecenter node is surrounded by its p-nearest neighbors. The arc weights of eachmesh are estimated from the voxel intensity values by least squares method. Theestimated arc weights of all the meshes, called Mesh Arc Descriptors (MADs),are then used to train a classifier, such as Neural Networks, k-NearestNeighbor, Na\"ive Bayes and Support Vector Machines. The proposed Mesh Modelwas tested on neuroimaging data acquired via functional magnetic resonanceimaging (fMRI) during a recognition memory experiment using categorized wordlists, employing a previously established experimental paradigm (\"Oztekin &Badre, 2011). Results suggest that the proposed Mesh Learning approach canprovide an effective algorithm for pattern analysis of brain activity duringcognitive processing.
arxiv-1205-2151 | A Converged Algorithm for Tikhonov Regularized Nonnegative Matrix Factorization with Automatic Regularization Parameters Determination |  http://arxiv.org/abs/1205.2151  | author:Andri Mirzal category:cs.LG published:2012-05-10 summary:We present a converged algorithm for Tikhonov regularized nonnegative matrixfactorization (NMF). We specially choose this regularization because it isknown that Tikhonov regularized least square (LS) is the more preferable formin solving linear inverse problems than the conventional LS. Because an NMFproblem can be decomposed into LS subproblems, it can be expected that Tikhonovregularized NMF will be the more appropriate approach in solving NMF problems.The algorithm is derived using additive update rules which have been shown tohave convergence guarantee. We equip the algorithm with a mechanism toautomatically determine the regularization parameters based on the L-curve, awell-known concept in the inverse problems community, but is rather unknown inthe NMF research. The introduction of this algorithm thus solves two inherentproblems in Tikhonov regularized NMF algorithm research, i.e., convergenceguarantee and regularization parameters determination.
arxiv-1205-2171 | A Generalized Kernel Approach to Structured Output Learning |  http://arxiv.org/abs/1205.2171  | author:Hachem Kadri, Mohammad Ghavamzadeh, Philippe Preux category:stat.ML cs.LG published:2012-05-10 summary:We study the problem of structured output learning from a regressionperspective. We first provide a general formulation of the kernel dependencyestimation (KDE) problem using operator-valued kernels. We show that some ofthe existing formulations of this problem are special cases of our framework.We then propose a covariance-based operator-valued kernel that allows us totake into account the structure of the kernel feature space. This kerneloperates on the output space and encodes the interactions between the outputswithout any reference to the input space. To address this issue, we introduce avariant of our KDE method based on the conditional covariance operator that inaddition to the correlation between the outputs takes into account the effectsof the input variables. Finally, we evaluate the performance of our KDEapproach using both covariance and conditional covariance kernels on twostructured output problems, and compare it to the state-of-the-art kernel-basedstructured output regression methods.
arxiv-1205-2164 | Discrimination of English to other Indian languages (Kannada and Hindi) for OCR system |  http://arxiv.org/abs/1205.2164  | author:Ankit Kumar, Tushar Patnaik, Vivek Kr Verma category:cs.CV published:2012-05-10 summary:India is a multilingual multi-script country. In every state of India thereare two languages one is state local language and the other is English. Forexample in Andhra Pradesh, a state in India, the document may contain textwords in English and Telugu script. For Optical Character Recognition (OCR) ofsuch a bilingual document, it is necessary to identify the script beforefeeding the text words to the OCRs of individual scripts. In this paper, we areintroducing a simple and efficient technique of script identification forKannada, English and Hindi text words of a printed document. The proposedapproach is based on the horizontal and vertical projection profile for thediscrimination of the three scripts. The feature extraction is done based onthe horizontal projection profile of each text words. We analysed 700 differentwords of Kannada, English and Hindi in order to extract the discriminationfeatures and for the development of knowledge base. We use the horizontalprojection profile of each text word and based on the horizontal projectionprofile we extract the appropriate features. The proposed system is tested on100 different document images containing more than 1000 text words of eachscript and a classification rate of 98.25%, 99.25% and 98.87% is achieved forKannada, English and Hindi respectively.
arxiv-1205-2334 | Sparse Approximation via Penalty Decomposition Methods |  http://arxiv.org/abs/1205.2334  | author:Zhaosong Lu, Yong Zhang category:cs.LG math.OC stat.CO stat.ML published:2012-05-10 summary:In this paper we consider sparse approximation problems, that is, general$l_0$ minimization problems with the $l_0$-"norm" of a vector being a part ofconstraints or objective function. In particular, we first study thefirst-order optimality conditions for these problems. We then propose penaltydecomposition (PD) methods for solving them in which a sequence of penaltysubproblems are solved by a block coordinate descent (BCD) method. Under somesuitable assumptions, we establish that any accumulation point of the sequencegenerated by the PD methods satisfies the first-order optimality conditions ofthe problems. Furthermore, for the problems in which the $l_0$ part is the onlynonconvex part, we show that such an accumulation point is a local minimizer ofthe problems. In addition, we show that any accumulation point of the sequencegenerated by the BCD method is a saddle point of the penalty subproblem.Moreover, for the problems in which the $l_0$ part is the only nonconvex part,we establish that such an accumulation point is a local minimizer of thepenalty subproblem. Finally, we test the performance of our PD methods byapplying them to sparse logistic regression, sparse inverse covarianceselection, and compressed sensing problems. The computational resultsdemonstrate that our methods generally outperform the existing methods in termsof solution quality and/or speed.
arxiv-1205-2172 | Modularity-Based Clustering for Network-Constrained Trajectories |  http://arxiv.org/abs/1205.2172  | author:Mohamed Khalil El Mahrsi, Fabrice Rossi category:stat.ML cs.LG published:2012-05-10 summary:We present a novel clustering approach for moving object trajectories thatare constrained by an underlying road network. The approach builds a similaritygraph based on these trajectories then uses modularity-optimization hiearchicalgraph clustering to regroup trajectories with similar profiles. Ourexperimental study shows the superiority of the proposed approach over classichierarchical clustering and gives a brief insight to visualization of theclustering results.
arxiv-1205-2606 | Exploring compact reinforcement-learning representations with linear regression |  http://arxiv.org/abs/1205.2606  | author:Thomas J. Walsh, Istvan Szita, Carlos Diuk, Michael L. Littman category:cs.LG cs.AI published:2012-05-09 summary:This paper presents a new algorithm for online linear regression whoseefficiency guarantees satisfy the requirements of the KWIK (Knows What ItKnows) framework. The algorithm improves on the complexity bounds of thecurrent state-of-the-art procedure in this setting. We explore severalapplications of this algorithm for learning compact reinforcement-learningrepresentations. We show that KWIK linear regression can be used to learn thereward function of a factored MDP and the probabilities of action outcomes inStochastic STRIPS and Object Oriented MDPs, none of which have been proven tobe efficiently learnable in the RL setting before. We also combine KWIK linearregression with other KWIK learners to learn larger portions of these models,including experiments on learning factored MDP transition and reward functionstogether.
arxiv-1205-2605 | Herding Dynamic Weights for Partially Observed Random Field Models |  http://arxiv.org/abs/1205.2605  | author:Max Welling category:cs.LG stat.ML published:2012-05-09 summary:Learning the parameters of a (potentially partially observable) random fieldmodel is intractable in general. Instead of focussing on a single optimalparameter value we propose to treat parameters as dynamical quantities. Weintroduce an algorithm to generate complex dynamics for parameters and (bothvisible and hidden) state vectors. We show that under certain conditionsaverages computed over trajectories of the proposed dynamical system convergeto averages computed over the data. Our "herding dynamics" does not requireexpensive operations such as exponentiation and is fully deterministic.
arxiv-1205-2056 | Dynamic Behavioral Mixed-Membership Model for Large Evolving Networks |  http://arxiv.org/abs/1205.2056  | author:Ryan Rossi, Brian Gallagher, Jennifer Neville, Keith Henderson category:cs.SI cs.LG physics.soc-ph stat.ML published:2012-05-09 summary:The majority of real-world networks are dynamic and extremely large (e.g.,Internet Traffic, Twitter, Facebook, ...). To understand the structuralbehavior of nodes in these large dynamic networks, it may be necessary to modelthe dynamics of behavioral roles representing the main connectivity patternsover time. In this paper, we propose a dynamic behavioral mixed-membershipmodel (DBMM) that captures the roles of nodes in the graph and how they evolveover time. Unlike other node-centric models, our model is scalable foranalyzing large dynamic networks. In addition, DBMM is flexible,parameter-free, has no functional form or parameterization, and isinterpretable (identifies explainable patterns). The performance resultsindicate our approach can be applied to very large networks while theexperimental results show that our model uncovers interesting patternsunderlying the dynamics of these networks.
arxiv-1205-2614 | Products of Hidden Markov Models: It Takes N>1 to Tango |  http://arxiv.org/abs/1205.2614  | author:Graham W Taylor, Geoffrey E. Hinton category:cs.LG stat.ML published:2012-05-09 summary:Products of Hidden Markov Models(PoHMMs) are an interesting class ofgenerative models which have received little attention since theirintroduction. This maybe in part due to their more computationally expensivegradient-based learning algorithm,and the intractability of computing the loglikelihood of sequences under the model. In this paper, we demonstrate how thepartition function can be estimated reliably via Annealed Importance Sampling.We perform experiments using contrastive divergence learning on rainfall dataand data captured from pairs of people dancing. Our results suggest thatadvances in learning and evaluation for undirected graphical models and recentincreases in available computing power make PoHMMs worth considering forcomplex time-series modeling tasks.
arxiv-1205-1928 | The representer theorem for Hilbert spaces: a necessary and sufficient condition |  http://arxiv.org/abs/1205.1928  | author:Francesco Dinuzzo, Bernhard SchÃ¶lkopf category:math.FA cs.LG published:2012-05-09 summary:A family of regularization functionals is said to admit a linear representertheorem if every member of the family admits minimizers that lie in a fixedfinite dimensional subspace. A recent characterization states that a generalclass of regularization functionals with differentiable regularizer admits alinear representer theorem if and only if the regularization term is anon-decreasing function of the norm. In this report, we improve over suchresult by replacing the differentiability assumption with lower semi-continuityand deriving a proof that is independent of the dimensionality of the space.
arxiv-1205-2612 | Computing Posterior Probabilities of Structural Features in Bayesian Networks |  http://arxiv.org/abs/1205.2612  | author:Jin Tian, Ru He category:cs.LG stat.ML published:2012-05-09 summary:We study the problem of learning Bayesian network structures from data.Koivisto and Sood (2004) and Koivisto (2006) presented algorithms that cancompute the exact marginal posterior probability of a subnetwork, e.g., asingle edge, in O(n2n) time and the posterior probabilities for all n(n-1)potential edges in O(n2n) total time, assuming that the number of parents pernode or the indegree is bounded by a constant. One main drawback of theiralgorithms is the requirement of a special structure prior that is non uniformand does not respect Markov equivalence. In this paper, we develop an algorithmthat can compute the exact posterior probability of a subnetwork in O(3n) timeand the posterior probabilities for all n(n-1) potential edges in O(n3n) totaltime. Our algorithm also assumes a bounded indegree but allows generalstructure priors. We demonstrate the applicability of the algorithm on severaldata sets with up to 20 variables.
arxiv-1205-2618 | BPR: Bayesian Personalized Ranking from Implicit Feedback |  http://arxiv.org/abs/1205.2618  | author:Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme category:cs.IR cs.LG stat.ML published:2012-05-09 summary:Item recommendation is the task of predicting a personalized ranking on a setof items (e.g. websites, movies, products). In this paper, we investigate themost common scenario with implicit feedback (e.g. clicks, purchases). There aremany methods for item recommendation from implicit feedback like matrixfactorization (MF) or adaptive knearest-neighbor (kNN). Even though thesemethods are designed for the item prediction task of personalized ranking, noneof them is directly optimized for ranking. In this paper we present a genericoptimization criterion BPR-Opt for personalized ranking that is the maximumposterior estimator derived from a Bayesian analysis of the problem. We alsoprovide a generic learning algorithm for optimizing models with respect toBPR-Opt. The learning method is based on stochastic gradient descent withbootstrap sampling. We show how to apply our method to two state-of-the-artrecommender models: matrix factorization and adaptive kNN. Our experimentsindicate that for the task of personalized ranking our optimization methodoutperforms the standard learning techniques for MF and kNN. The results showthe importance of optimizing models for the right criterion.
arxiv-1205-2106 | Spatial Multiresolution Cluster Detection Method |  http://arxiv.org/abs/1205.2106  | author:Lingsong Zhang, Zhengyuan Zhu category:stat.ME stat.CO stat.ML 62H11 published:2012-05-09 summary:A novel multi-resolution cluster detection (MCD) method is proposed toidentify irregularly shaped clusters in space. Multi-scale test statistic on asingle cell is derived based on likelihood ratio statistic for Bernoullisequence, Poisson sequence and Normal sequence. A neighborhood variabilitymeasure is defined to select the optimal test threshold. The MCD method iscompared with single scale testing methods controlling for false discovery rateand the spatial scan statistics using simulation and f-MRI data. The MCD methodis shown to be more effective for discovering irregularly shaped clusters, andthe implementation of this method does not require heavy computation, making itsuitable for cluster detection for large spatial data.
arxiv-1205-2622 | Using the Gene Ontology Hierarchy when Predicting Gene Function |  http://arxiv.org/abs/1205.2622  | author:Sara Mostafavi, Quaid Morris category:cs.LG cs.CE stat.ML published:2012-05-09 summary:The problem of multilabel classification when the labels are related througha hierarchical categorization scheme occurs in many application domains such ascomputational biology. For example, this problem arises naturally when tryingto automatically assign gene function using a controlled vocabularies like GeneOntology. However, most existing approaches for predicting gene functions solveindependent classification problems to predict genes that are involved in agiven function category, independently of the rest. Here, we propose two simplemethods for incorporating information about the hierarchical nature of thecategorization scheme. In the first method, we use information about a gene'sprevious annotation to set an initial prior on its label. In a second approach,we extend a graph-based semi-supervised learning algorithm for predicting genefunction in a hierarchy. We show that we can efficiently solve this problem bysolving a linear system of equations. We compare these approaches with aprevious label reconciliation-based approach. Results show that using thehierarchy information directly, compared to using reconciliation methods,improves gene function prediction.
arxiv-1205-2617 | Modeling Discrete Interventional Data using Directed Cyclic Graphical Models |  http://arxiv.org/abs/1205.2617  | author:Mark Schmidt, Kevin Murphy category:stat.ML cs.LG stat.ME published:2012-05-09 summary:We outline a representation for discrete multivariate distributions in termsof interventional potential functions that are globally normalized. Thisrepresentation can be used to model the effects of interventions, and theindependence properties encoded in this model can be represented as a directedgraph that allows cycles. In addition to discussing inference and sampling withthis representation, we give an exponential family parametrization that allowsparameter estimation to be stated as a convex optimization problem; we alsogive a convex relaxation of the task of simultaneous parameter and structurelearning using group l1-regularization. The model is evaluated on simulateddata and intracellular flow cytometry data.
arxiv-1205-2611 | Ordinal Boltzmann Machines for Collaborative Filtering |  http://arxiv.org/abs/1205.2611  | author:Tran The Truyen, Dinh Q. Phung, Svetha Venkatesh category:cs.IR cs.LG published:2012-05-09 summary:Collaborative filtering is an effective recommendation technique wherein thepreference of an individual can potentially be predicted based on preferencesof other members. Early algorithms often relied on the strong locality in thepreference data, that is, it is enough to predict preference of a user on aparticular item based on a small subset of other users with similar tastes orof other items with similar properties. More recently, dimensionality reductiontechniques have proved to be equally competitive, and these are based on theco-occurrence patterns rather than locality. This paper explores and extends aprobabilistic model known as Boltzmann Machine for collaborative filteringtasks. It seamlessly integrates both the similarity and co-occurrence in aprincipled manner. In particular, we study parameterisation options to dealwith the ordinal nature of the preferences, and propose a joint modelling ofboth the user-based and item-based processes. Experiments on moderate andlarge-scale movie recommendation show that our framework rivals existingwell-known methods.
arxiv-1205-2623 | Virtual Vector Machine for Bayesian Online Classification |  http://arxiv.org/abs/1205.2623  | author:Thomas P. Minka, Rongjing Xiang, Yuan, Qi category:cs.LG stat.ML published:2012-05-09 summary:In a typical online learning scenario, a learner is required to process alarge data stream using a small memory buffer. Such a requirement is usually inconflict with a learner's primary pursuit of prediction accuracy. To addressthis dilemma, we introduce a novel Bayesian online classi cation algorithm,called the Virtual Vector Machine. The virtual vector machine allows you tosmoothly trade-off prediction accuracy with memory size. The virtual vectormachine summarizes the information contained in the preceding data stream by aGaussian distribution over the classi cation weights plus a constant number ofvirtual data points. The virtual data points are designed to add extranon-Gaussian information about the classi cation weights. To maintain theconstant number of virtual points, the virtual vector machine adds the currentreal data point into the virtual point set, merges two most similar virtualpoints into a new virtual point or deletes a virtual point that is far from thedecision boundary. The information lost in this process is absorbed into theGaussian distribution. The extra information provided by the virtual pointsleads to improved predictive accuracy over previous online classificationalgorithms.
arxiv-1205-2609 | Which Spatial Partition Trees are Adaptive to Intrinsic Dimension? |  http://arxiv.org/abs/1205.2609  | author:Nakul Verma, Samory Kpotufe, Sanjoy Dasgupta category:stat.ML cs.LG published:2012-05-09 summary:Recent theory work has found that a special type of spatial partition tree -called a random projection tree - is adaptive to the intrinsic dimension of thedata from which it is built. Here we examine this same question, with acombination of theory and experiments, for a broader class of trees thatincludes k-d trees, dyadic trees, and PCA trees. Our motivation is to get afeel for (i) the kind of intrinsic low dimensional structure that can beempirically verified, (ii) the extent to which a spatial partition can exploitsuch structure, and (iii) the implications for standard statistical tasks suchas regression, vector quantization, and nearest neighbor search.
arxiv-1205-2610 | Probabilistic Structured Predictors |  http://arxiv.org/abs/1205.2610  | author:Shankar Vembu, Thomas Gartner, Mario Boley category:cs.LG published:2012-05-09 summary:We consider MAP estimators for structured prediction with exponential familymodels. In particular, we concentrate on the case that efficient algorithms foruniform sampling from the output space exist. We show that under thisassumption (i) exact computation of the partition function remains a hardproblem, and (ii) the partition function and the gradient of the log partitionfunction can be approximated efficiently. Our main result is an approximationscheme for the partition function based on Markov Chain Monte Carlo theory. Wealso show that the efficient uniform sampling assumption holds in severalapplication settings that are of importance in machine learning.
arxiv-1205-2604 | The Infinite Latent Events Model |  http://arxiv.org/abs/1205.2604  | author:David Wingate, Noah Goodman, Daniel Roy, Joshua Tenenbaum category:stat.ML cs.LG published:2012-05-09 summary:We present the Infinite Latent Events Model, a nonparametric hierarchicalBayesian distribution over infinite dimensional Dynamic Bayesian Networks withbinary state representations and noisy-OR-like transitions. The distributioncan be used to learn structure in discrete timeseries data by simultaneouslyinferring a set of latent events, which events fired at each timestep, and howthose events are causally linked. We illustrate the model on a soundfactorization task, a network topology identification task, and a video gametask.
arxiv-1205-2608 | Temporal-Difference Networks for Dynamical Systems with Continuous Observations and Actions |  http://arxiv.org/abs/1205.2608  | author:Christopher M. Vigorito category:cs.LG stat.ML published:2012-05-09 summary:Temporal-difference (TD) networks are a class of predictive staterepresentations that use well-established TD methods to learn models ofpartially observable dynamical systems. Previous research with TD networks hasdealt only with dynamical systems with finite sets of observations and actions.We present an algorithm for learning TD network representations of dynamicalsystems with continuous observations and actions. Our results show that thealgorithm is capable of learning accurate and robust models of several noisycontinuous dynamical systems. The algorithm presented here is the first fullyincremental method for learning a predictive representation of a continuousdynamical system.
arxiv-1205-2600 | A Uniqueness Theorem for Clustering |  http://arxiv.org/abs/1205.2600  | author:Reza Bosagh Zadeh, Shai Ben-David category:cs.LG published:2012-05-09 summary:Despite the widespread use of Clustering, there is distressingly littlegeneral theory of clustering available. Questions like "What distinguishes aclustering of data from other data partitioning?", "Are there any principlesgoverning all clustering paradigms?", "How should a user choose an appropriateclustering algorithm for a particular task?", etc. are almost completelyunanswered by the existing body of clustering literature. We consider anaxiomatic approach to the theory of Clustering. We adopt the framework ofKleinberg, [Kle03]. By relaxing one of Kleinberg's clustering axioms, wesidestep his impossibility result and arrive at a consistent set of axioms. Wesuggest to extend these axioms, aiming to provide an axiomatic taxonomy ofclustering paradigms. Such a taxonomy should provide users some guidanceconcerning the choice of the appropriate clustering paradigm for a given task.The main result of this paper is a set of abstract properties that characterizethe Single-Linkage clustering function. This characterization result providesnew insight into the properties of desired data groupings that makeSingle-Linkage the appropriate choice. We conclude by considering a taxonomy ofclustering functions based on abstract properties that each satisfies.
arxiv-1205-2599 | On the Identifiability of the Post-Nonlinear Causal Model |  http://arxiv.org/abs/1205.2599  | author:Kun Zhang, Aapo Hyvarinen category:stat.ML cs.LG published:2012-05-09 summary:By taking into account the nonlinear effect of the cause, the inner noiseeffect, and the measurement distortion effect in the observed variables, thepost-nonlinear (PNL) causal model has demonstrated its excellent performance indistinguishing the cause from effect. However, its identifiability has not beenproperly addressed, and how to apply it in the case of more than two variablesis also a problem. In this paper, we conduct a systematic investigation on itsidentifiability in the two-variable case. We show that this model isidentifiable in most cases; by enumerating all possible situations in which themodel is not identifiable, we provide sufficient conditions for itsidentifiability. Simulations are given to support the theoretical results.Moreover, in the case of more than two variables, we show that the whole causalstructure can be found by applying the PNL causal model to each structure inthe Markov equivalent class and testing if the disturbance is independent ofthe direct causes for each variable. In this way the exhaustive search over allpossible causal structures is avoided.
arxiv-1205-1925 | Hamiltonian Annealed Importance Sampling for partition function estimation |  http://arxiv.org/abs/1205.1925  | author:Jascha Sohl-Dickstein, Benjamin J. Culpepper category:cs.LG published:2012-05-09 summary:We introduce an extension to annealed importance sampling that usesHamiltonian dynamics to rapidly estimate normalization constants. Wedemonstrate this method by computing log likelihoods in directed and undirectedprobabilistic image models. We compare the performance of linear generativemodels with both Gaussian and Laplace priors, product of experts models withLaplace and Student's t experts, the mc-RBM, and a bilinear generative model.We provide code to compare additional models.
arxiv-1205-2624 | Convexifying the Bethe Free Energy |  http://arxiv.org/abs/1205.2624  | author:Ofer Meshi, Ariel Jaimovich, Amir Globerson, Nir Friedman category:cs.AI cs.LG published:2012-05-09 summary:The introduction of loopy belief propagation (LBP) revitalized theapplication of graphical models in many domains. Many recent works presentimprovements on the basic LBP algorithm in an attempt to overcome convergenceand local optima problems. Notable among these are convexified free energyapproximations that lead to inference procedures with provable convergence andquality properties. However, empirically LBP still outperforms most of itsconvex variants in a variety of settings, as we also demonstrate here.Motivated by this fact we seek convexified free energies that directlyapproximate the Bethe free energy. We show that the proposed approximationscompare favorably with state-of-the art convex free energy approximations.
arxiv-1205-1989 | Structured Input-Output Lasso, with Application to eQTL Mapping, and a Thresholding Algorithm for Fast Estimation |  http://arxiv.org/abs/1205.1989  | author:Seunghak Lee, Eric P. Xing category:stat.ML q-bio.GN q-bio.QM stat.AP published:2012-05-09 summary:We consider the problem of learning a high-dimensional multi-task regressionmodel, under sparsity constraints induced by presence of grouping structures onthe input covariates and on the output predictors. This problem is primarilymotivated by expression quantitative trait locus (eQTL) mapping, of which thegoal is to discover genetic variations in the genome (inputs) that influencethe expression levels of multiple co-expressed genes (outputs), eitherepistatically, or pleiotropically, or both. A structured input-output lasso(SIOL) model based on an intricate l1/l2-norm penalty over the regressioncoefficient matrix is employed to enable discovery of complex sparseinput/output relationships; and a highly efficient new optimization algorithmcalled hierarchical group thresholding (HiGT) is developed to solve theresultant non-differentiable, non-separable, and ultra high-dimensionaloptimization problem. We show on both simulation and on a yeast eQTL datasetthat our model leads to significantly better recovery of the structured sparserelationships between the inputs and the outputs, and our algorithmsignificantly outperforms other optimization techniques under the same model.Additionally, we propose a novel approach for efficiently and effectivelydetecting input interactions by exploiting the prior knowledge available frombiological experiments.
arxiv-1205-1939 | Hamiltonian Monte Carlo with Reduced Momentum Flips |  http://arxiv.org/abs/1205.1939  | author:Jascha Sohl-Dickstein category:cs.LG published:2012-05-09 summary:Hamiltonian Monte Carlo (or hybrid Monte Carlo) with partial momentumrefreshment explores the state space more slowly than it otherwise would due tothe momentum reversals which occur on proposal rejection. These causetrajectories to double back on themselves, leading to random walk behavior ontimescales longer than the typical rejection time, and leading to slowermixing. I present a technique by which the number of momentum reversals can bereduced. This is accomplished by maintaining the net exchange of probabilitybetween states with opposite momenta, but reducing the rate of exchange in bothdirections such that it is 0 in one direction. An experiment illustrates thesereduced momentum flips accelerating mixing for a particular distribution.
arxiv-1205-2625 | Convergent message passing algorithms - a unifying view |  http://arxiv.org/abs/1205.2625  | author:Talya Meltzer, Amir Globerson, Yair Weiss category:cs.AI cs.LG published:2012-05-09 summary:Message-passing algorithms have emerged as powerful techniques forapproximate inference in graphical models. When these algorithms converge, theycan be shown to find local (or sometimes even global) optima of variationalformulations to the inference problem. But many of the most popular algorithmsare not guaranteed to converge. This has lead to recent interest in convergentmessage-passing algorithms. In this paper, we present a unified view ofconvergent message-passing algorithms. We present a simple derivation of anabstract algorithm, tree-consistency bound optimization (TCBO) that is provablyconvergent in both its sum and max product forms. We then show that many of theexisting convergent algorithms are instances of our TCBO algorithm, and obtainnovel convergent algorithms "for free" by exchanging maximizations andsummations in existing algorithms. In particular, we show that Wainwright'snon-convergent sum-product algorithm for tree based variational bounds, isactually convergent with the right update order for the case where trees aremonotonic chains.
arxiv-1205-2626 | Group Sparse Priors for Covariance Estimation |  http://arxiv.org/abs/1205.2626  | author:Benjamin Marlin, Mark Schmidt, Kevin Murphy category:stat.ML cs.LG published:2012-05-09 summary:Recently it has become popular to learn sparse Gaussian graphical models(GGMs) by imposing l1 or group l1,2 penalties on the elements of the precisionmatrix. Thispenalized likelihood approach results in a tractable convexoptimization problem. In this paper, we reinterpret these results as performingMAP estimation under a novel prior which we call the group l1 and l1,2positivedefinite matrix distributions. This enables us to build a hierarchicalmodel in which the l1 regularization terms vary depending on which group theentries are assigned to, which in turn allows us to learn block structuredsparse GGMs with unknown group assignments. Exact inference in thishierarchical model is intractable, due to the need to compute the normalizationconstant of these matrix distributions. However, we derive upper bounds on thepartition functions, which lets us use fast variational inference (optimizing alower bound on the joint posterior). We show that on two real world data sets(motion capture and financial data), our method which infers the blockstructure outperforms a method that uses a fixed block structure, which in turnoutperforms baseline methods that ignore block structure.
arxiv-1205-2627 | Domain Knowledge Uncertainty and Probabilistic Parameter Constraints |  http://arxiv.org/abs/1205.2627  | author:Yi Mao, Guy Lebanon category:cs.LG stat.ML published:2012-05-09 summary:Incorporating domain knowledge into the modeling process is an effective wayto improve learning accuracy. However, as it is provided by humans, domainknowledge can only be specified with some degree of uncertainty. We propose toexplicitly model such uncertainty through probabilistic constraints over theparameter space. In contrast to hard parameter constraints, our approach iseffective also when the domain knowledge is inaccurate and generally results insuperior modeling accuracy. We focus on generative and conditional modelingwhere the parameters are assigned a Dirichlet or Gaussian prior and demonstratethe framework with experiments on both synthetic and real-world data.
arxiv-1205-2628 | Multiple Source Adaptation and the Renyi Divergence |  http://arxiv.org/abs/1205.2628  | author:Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh category:cs.LG stat.ML published:2012-05-09 summary:This paper presents a novel theoretical study of the general problem ofmultiple source adaptation using the notion of Renyi divergence. Our resultsbuild on our previous work [12], but significantly broaden the scope of thatwork in several directions. We extend previous multiple source loss guaranteesbased on distribution weighted combinations to arbitrary target distributionsP, not necessarily mixtures of the source distributions, analyze both known andunknown target distribution cases, and prove a lower bound. We further extendour bounds to deal with the case where the learner receives an approximatedistribution for each source instead of the exact one, and show that similarloss guarantees can be achieved depending on the divergence between theapproximate and true distributions. We also analyze the case where the labelingfunctions of the source domains are somewhat different. Finally, we report theresults of experiments with both an artificial data set and a sentimentanalysis task, showing the performance benefits of the distribution weightedcombinations and the quality of our bounds based on the Renyi divergence.
arxiv-1205-2629 | Interpretation and Generalization of Score Matching |  http://arxiv.org/abs/1205.2629  | author:Siwei Lyu category:cs.LG stat.ML published:2012-05-09 summary:Score matching is a recently developed parameter learning method that isparticularly effective to complicated high dimensional density models withintractable partition functions. In this paper, we study two issues that havenot been completely resolved for score matching. First, we provide a formallink between maximum likelihood and score matching. Our analysis shows thatscore matching finds model parameters that are more robust with noisy trainingdata. Second, we develop a generalization of score matching. Based on thisgeneralization, we further demonstrate an extension of score matching to modelsof discrete data.
arxiv-1205-2631 | Multi-Task Feature Learning Via Efficient l2,1-Norm Minimization |  http://arxiv.org/abs/1205.2631  | author:Jun Liu, Shuiwang Ji, Jieping Ye category:cs.LG cs.CV stat.ML published:2012-05-09 summary:The problem of joint feature selection across a group of related tasks hasapplications in many areas including biomedical informatics and computervision. We consider the l2,1-norm regularized regression model for jointfeature selection from multiple tasks, which can be derived in theprobabilistic framework by assuming a suitable prior from the exponentialfamily. One appealing feature of the l2,1-norm regularization is that itencourages multiple predictors to share similar sparsity patterns. However, theresulting optimization problem is challenging to solve due to thenon-smoothness of the l2,1-norm regularization. In this paper, we propose toaccelerate the computation by reformulating it as two equivalent smooth convexoptimization problems which are then solved via the Nesterov's method-anoptimal first-order black-box method for smooth convex optimization. A keybuilding block in solving the reformulations is the Euclidean projection. Weshow that the Euclidean projection for the first reformulation can beanalytically computed, while the Euclidean projection for the second one can becomputed in linear time. Empirical evaluations on several data sets verify theefficiency of the proposed algorithms.
arxiv-1205-2031 | M-FISH Karyotyping - A New Approach Based on Watershed Transform |  http://arxiv.org/abs/1205.2031  | author:K. S. Sreejini, A. Lijiya, V. K. Govindan category:cs.CV published:2012-05-09 summary:Karyotyping is a process in which chromosomes in a dividing cell are properlystained, identified and displayed in a standard format, which helps geneticistto study and diagnose genetic factors behind various genetic diseases and forstudying cancer. M-FISH (Multiplex Fluorescent In-Situ Hybridization) providescolor karyotyping. In this paper, an automated method for M-FISH chromosomesegmentation based on watershed transform followed by naive Bayesclassification of each region using the features, mean and standard deviation,is presented. Also, a post processing step is added to re-classify the smallchromosome segments to the neighboring larger segment for reducing the chancesof misclassification. The approach provided improved accuracy when compared tothe pixel-by-pixel approach. The approach was tested on 40 images from thedataset and achieved an accuracy of 84.21 %.
arxiv-1205-2632 | Improving Compressed Counting |  http://arxiv.org/abs/1205.2632  | author:Ping Li category:cs.DS cs.LG stat.ML published:2012-05-09 summary:Compressed Counting (CC) [22] was recently proposed for estimating the athfrequency moments of data streams, where 0 < a <= 2. CC can be used forestimating Shannon entropy, which can be approximated by certain functions ofthe ath frequency moments as a -> 1. Monitoring Shannon entropy for anomalydetection (e.g., DDoS attacks) in large networks is an important task. Thispaper presents a new algorithm for improving CC. The improvement is mostsubstantial when a -> 1--. For example, when a = 0:99, the new algorithmreduces the estimation variance roughly by 100-fold. This new algorithm wouldmake CC considerably more practical for estimating Shannon entropy.Furthermore, the new algorithm is statistically optimal when a = 0.5.
arxiv-1205-2640 | Identifying confounders using additive noise models |  http://arxiv.org/abs/1205.2640  | author:Dominik Janzing, Jonas Peters, Joris Mooij, Bernhard Schoelkopf category:stat.ML cs.LG published:2012-05-09 summary:We propose a method for inferring the existence of a latent common cause('confounder') of two observed random variables. The method assumes that thetwo effects of the confounder are (possibly nonlinear) functions of theconfounder plus independent, additive noise. We discuss under which conditionsthe model is identifiable (up to an arbitrary reparameterization of theconfounder) from the joint distribution of the effects. We state and prove atheoretical result that provides evidence for the conjecture that the model isgenerically identifiable under suitable technical conditions. In addition, wepropose a practical method to estimate the confounder from a finite i.i.d.sample of the effects and illustrate that the method works well on bothsimulated and real-world data.
arxiv-1205-2641 | Bayesian Discovery of Linear Acyclic Causal Models |  http://arxiv.org/abs/1205.2641  | author:Patrik O. Hoyer, Antti Hyttinen category:stat.ML cs.LG stat.ME published:2012-05-09 summary:Methods for automated discovery of causal relationships fromnon-interventional data have received much attention recently. A widely usedand well understood model family is given by linear acyclic causal models(recursive structural equation models). For Gaussian data both constraint-basedmethods (Spirtes et al., 1993; Pearl, 2000) (which output a single equivalenceclass) and Bayesian score-based methods (Geiger and Heckerman, 1994) (whichassign relative scores to the equivalence classes) are available. On thecontrary, all current methods able to utilize non-Gaussianity in the data(Shimizu et al., 2006; Hoyer et al., 2008) always return only a single graph ora single equivalence class, and so are fundamentally unable to express thedegree of certainty attached to that output. In this paper we develop aBayesian score-based approach able to take advantage of non-Gaussianity whenestimating linear acyclic causal models, and we empirically demonstrate that,at least on very modest size networks, its accuracy is as good as or betterthan existing methods. We provide a complete code package (in R) whichimplements all algorithms and performs all of the analysis provided in thepaper, and hope that this will further the application of these methods tosolving causal inference problems.
arxiv-1205-2643 | New inference strategies for solving Markov Decision Processes using reversible jump MCMC |  http://arxiv.org/abs/1205.2643  | author:Matthias Hoffman, Hendrik Kueck, Nando de Freitas, Arnaud Doucet category:cs.LG cs.SY math.OC stat.CO stat.ML published:2012-05-09 summary:In this paper we build on previous work which uses inferences techniques, inparticular Markov Chain Monte Carlo (MCMC) methods, to solve parameterizedcontrol problems. We propose a number of modifications in order to make thisapproach more practical in general, higher-dimensional spaces. We firstintroduce a new target distribution which is able to incorporate more rewardinformation from sampled trajectories. We also show how to break strongcorrelations between the policy parameters and sampled trajectories in order tosample more freely. Finally, we show how to incorporate these techniques in aprincipled manner to obtain estimates of the optimal policy.
arxiv-1205-2646 | Censored Exploration and the Dark Pool Problem |  http://arxiv.org/abs/1205.2646  | author:Kuzman Ganchev, Michael Kearns, Yuriy Nevmyvaka, Jennifer Wortman Vaughan category:cs.LG cs.GT published:2012-05-09 summary:We introduce and analyze a natural algorithm for multi-venue exploration fromcensored data, which is motivated by the Dark Pool Problem of modernquantitative finance. We prove that our algorithm converges in polynomial timeto a near-optimal allocation policy; prior results for similar problems instochastic inventory control guaranteed only asymptotic convergence andexamined variants in which each venue could be treated independently. Ouranalysis bears a strong resemblance to that of efficient exploration/exploitation schemes in the reinforcement learning literature. We describe anextensive experimental evaluation of our algorithm on the Dark Pool Problemusing real trading data.
arxiv-1205-2648 | Learning Continuous-Time Social Network Dynamics |  http://arxiv.org/abs/1205.2648  | author:Yu Fan, Christian R. Shelton category:cs.SI cs.LG physics.soc-ph stat.ML published:2012-05-09 summary:We demonstrate that a number of sociology models for social network dynamicscan be viewed as continuous time Bayesian networks (CTBNs). A sampling-basedapproximate inference method for CTBNs can be used as the basis of anexpectation-maximization procedure that achieves better accuracy in estimatingthe parameters of the model than the standard method of momentsalgorithmfromthe sociology literature. We extend the existing social networkmodels to allow for indirect and asynchronous observations of the links. AMarkov chain Monte Carlo sampling algorithm for this new model permitsestimation and inference. We provide results on both a synthetic network (forverification) and real social network data.
arxiv-1205-2650 | Correlated Non-Parametric Latent Feature Models |  http://arxiv.org/abs/1205.2650  | author:Finale Doshi-Velez, Zoubin Ghahramani category:cs.LG stat.ML published:2012-05-09 summary:We are often interested in explaining data through a set of hidden factors orfeatures. When the number of hidden features is unknown, the Indian BuffetProcess (IBP) is a nonparametric latent feature model that does not bound thenumber of active features in dataset. However, the IBP assumes that all latentfeatures are uncorrelated, making it inadequate for many realworld problems. Weintroduce a framework for correlated nonparametric feature models, generalisingthe IBP. We use this framework to generate several specific models anddemonstrate applications on realworld datasets.
arxiv-1205-2653 | L2 Regularization for Learning Kernels |  http://arxiv.org/abs/1205.2653  | author:Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh category:cs.LG stat.ML published:2012-05-09 summary:The choice of the kernel is critical to the success of many learningalgorithms but it is typically left to the user. Instead, the training data canbe used to learn the kernel by selecting it out of a given family, such as thatof non-negative linear combinations of p base kernels, constrained by a traceor L1 regularization. This paper studies the problem of learning kernels withthe same family of kernels but with an L2 regularization instead, and forregression problems. We analyze the problem of learning kernels with ridgeregression. We derive the form of the solution of the optimization problem andgive an efficient iterative algorithm for computing that solution. We present anovel theoretical analysis of the problem based on stability and give learningbounds for orthogonal kernels that contain only an additive term O(pp/m) whencompared to the standard kernel ridge regression stability bound. We alsoreport the results of experiments indicating that L1 regularization can lead tomodest improvements for a small number of kernels, but to performancedegradations in larger-scale cases. In contrast, L2 regularization neverdegrades performance and in fact achieves significant improvements with a largenumber of kernels.
arxiv-1205-2656 | Convex Coding |  http://arxiv.org/abs/1205.2656  | author:David M. Bradley, J Andrew Bagnell category:cs.LG cs.IT math.IT stat.ML published:2012-05-09 summary:Inspired by recent work on convex formulations of clustering (Lashkari &Golland, 2008; Nowozin & Bakir, 2008) we investigate a new formulation of theSparse Coding Problem (Olshausen & Field, 1997). In sparse coding we attempt tosimultaneously represent a sequence of data-vectors sparsely (i.e. sparseapproximation (Tropp et al., 2006)) in terms of a 'code' defined by a set ofbasis elements, while also finding a code that enables such an approximation.As existing alternating optimization procedures for sparse coding aretheoretically prone to severe local minima problems, we propose a convexrelaxation of the sparse coding problem and derive a boosting-style algorithm,that (Nowozin & Bakir, 2008) serves as a convex 'master problem' which calls a(potentially non-convex) sub-problem to identify the next code element to add.Finally, we demonstrate the properties of our boosted coding algorithm on animage denoising task.
arxiv-1205-2657 | Multilingual Topic Models for Unaligned Text |  http://arxiv.org/abs/1205.2657  | author:Jordan Boyd-Graber, David Blei category:cs.CL cs.IR cs.LG stat.ML published:2012-05-09 summary:We develop the multilingual topic model for unaligned text (MuTo), aprobabilistic model of text that is designed to analyze corpora composed ofdocuments in two languages. From these documents, MuTo uses stochastic EM tosimultaneously discover both a matching between the languages and multilinguallatent topics. We demonstrate that MuTo is able to find shared topics onreal-world multilingual corpora, successfully pairing related documents acrosslanguages. MuTo provides a new framework for creating multilingual topic modelswithout needing carefully curated parallel corpora and allows applicationsbuilt using the topic model formalism to be applied to a much wider class ofcorpora.
arxiv-1205-1975 | Expressivity of Time-Varying Graphs and the Power of Waiting in Dynamic Networks |  http://arxiv.org/abs/1205.1975  | author:Arnaud Casteigts, Paola Flocchini, Emmanuel Godard, Nicola Santoro, Masafumi Yamashita category:cs.DC cs.CL published:2012-05-09 summary:In infrastructure-less highly dynamic networks, computing and performing evenbasic tasks (such as routing and broadcasting) is a very challenging activitydue to the fact that connectivity does not necessarily hold, and the networkmay actually be disconnected at every time instant. Clearly the task ofdesigning protocols for these networks is less difficult if the environmentallows waiting (i.e., it provides the nodes with store-carry-forward-likemechanisms such as local buffering) than if waiting is not feasible. Noquantitative corroborations of this fact exist (e.g., no answer to thequestion: how much easier?). In this paper, we consider these qualitativequestions about dynamic networks, modeled as time-varying (or evolving) graphs,where edges exist only at some times. We examine the difficulty of the environment in terms of the expressivity ofthe corresponding time-varying graph; that is in terms of the languagegenerated by the feasible journeys in the graph. We prove that the set oflanguages $L_{nowait}$ when no waiting is allowed contains all computablelanguages. On the other end, using algebraic properties of quasi-orders, weprove that $L_{wait}$ is just the family of regular languages. In other words,we prove that, when waiting is no longer forbidden, the power of the acceptingautomaton (difficulty of the environment) drops drastically from being aspowerful as a Turing machine, to becoming that of a Finite-State machine. This(perhaps surprisingly large) gap is a measure of the computational power ofwaiting. We also study bounded waiting; that is when waiting is allowed at a node onlyfor at most $d$ time units. We prove the negative result that $L_{wait[d]} =L_{nowait}$; that is, the expressivity decreases only if the waiting is finitebut unpredictable (i.e., under the control of the protocol designer and not ofthe environment).
arxiv-1205-2658 | Optimization of Structured Mean Field Objectives |  http://arxiv.org/abs/1205.2658  | author:Alexandre Bouchard-Cote, Michael I. Jordan category:stat.ML cs.LG published:2012-05-09 summary:In intractable, undirected graphical models, an intuitive way of creatingstructured mean field approximations is to select an acyclic tractablesubgraph. We show that the hardness of computing the objective function andgradient of the mean field objective qualitatively depends on a simple graphproperty. If the tractable subgraph has this property- we call such subgraphsv-acyclic-a very fast block coordinate ascent algorithm is possible. If not,optimization is harder, but we show a new algorithm based on the constructionof an auxiliary exponential family that can be used to make inference possiblein this case as well. We discuss the advantages and disadvantages of eachregime and compare the algorithms empirically.
arxiv-1205-2602 | The Entire Quantile Path of a Risk-Agnostic SVM Classifier |  http://arxiv.org/abs/1205.2602  | author:Jin Yu, S. V. N. Vishwanatan, Jian Zhang category:cs.LG published:2012-05-09 summary:A quantile binary classifier uses the rule: Classify x as +1 if P(Y = 1X =x) >= t, and as -1 otherwise, for a fixed quantile parameter t {[0, 1]. It hasbeen shown that Support Vector Machines (SVMs) in the limit are quantileclassifiers with t = 1/2 . In this paper, we show that by using asymmetric costof misclassification SVMs can be appropriately extended to recover, in thelimit, the quantile binary classifier for any t. We then present a principledalgorithm to solve the extended SVM classifier for all values of tsimultaneously. This has two implications: First, one can recover the entireconditional distribution P(Y = 1X = x) = t for t {[0, 1]. Second, we can builda risk-agnostic SVM classifier where the cost of misclassification need not beknown apriori. Preliminary numerical experiments show the effectiveness of theproposed algorithm.
arxiv-1205-2660 | Alternating Projections for Learning with Expectation Constraints |  http://arxiv.org/abs/1205.2660  | author:Kedar Bellare, Gregory Druck, Andrew McCallum category:cs.LG stat.ML published:2012-05-09 summary:We present an objective function for learning with unlabeled data thatutilizes auxiliary expectation constraints. We optimize this objective functionusing a procedure that alternates between information and moment projections.Our method provides an alternate interpretation of the posterior regularizationframework (Graca et al., 2008), maintains uncertainty during optimizationunlike constraint-driven learning (Chang et al., 2007), and is more efficientthan generalized expectation criteria (Mann & McCallum, 2008). Applications ofthis framework include minimally supervised learning, semisupervised learning,and learning with constraints that are more expressive than the underlyingmodel. In experiments, we demonstrate comparable accuracy to generalizedexpectation criteria for minimally supervised learning, and use expressivestructural constraints to guide semi-supervised learning, providing a 3%-6%improvement over stateof-the-art constraint-driven learning.
arxiv-1205-2664 | A Bayesian Sampling Approach to Exploration in Reinforcement Learning |  http://arxiv.org/abs/1205.2664  | author:John Asmuth, Lihong Li, Michael L. Littman, Ali Nouri, David Wingate category:cs.LG published:2012-05-09 summary:We present a modular approach to reinforcement learning that uses a Bayesianrepresentation of the uncertainty over models. The approach, BOSS (Best ofSampled Set), drives exploration by sampling multiple models from the posteriorand selecting actions optimistically. It extends previous work by providing arule for deciding when to resample and how to combine the models. We show thatour algorithm achieves nearoptimal reward with high probability with a samplecomplexity that is low relative to the speed at which the posteriordistribution converges during learning. We demonstrate that BOSS performs quitefavorably compared to state-of-the-art reinforcement-learning approaches andillustrate its flexibility by pairing it with a non-parametric model thatgeneralizes across states.
arxiv-1205-2661 | REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs |  http://arxiv.org/abs/1205.2661  | author:Peter L. Bartlett, Ambuj Tewari category:cs.LG published:2012-05-09 summary:We provide an algorithm that achieves the optimal regret rate in an unknownweakly communicating Markov Decision Process (MDP). The algorithm proceeds inepisodes where, in each episode, it picks a policy using regularization basedon the span of the optimal bias vector. For an MDP with S states and A actionswhose optimal bias vector has span bounded by H, we show a regret bound of~O(HSpAT). We also relate the span to various diameter-like quantitiesassociated with the MDP, demonstrating how our results improve on previousregret bounds.
arxiv-1205-2662 | On Smoothing and Inference for Topic Models |  http://arxiv.org/abs/1205.2662  | author:Arthur Asuncion, Max Welling, Padhraic Smyth, Yee Whye Teh category:cs.LG stat.ML published:2012-05-09 summary:Latent Dirichlet analysis, or topic modeling, is a flexible latent variableframework for modeling high-dimensional sparse count data. Various learningalgorithms have been developed in recent years, including collapsed Gibbssampling, variational inference, and maximum a posteriori estimation, and thisvariety motivates the need for careful empirical comparisons. In this paper, wehighlight the close connections between these approaches. We find that the maindifferences are attributable to the amount of smoothing applied to the counts.When the hyperparameters are optimized, the differences in performance amongthe algorithms diminish significantly. The ability of these algorithms toachieve solutions of comparable accuracy gives us the freedom to selectcomputationally efficient approaches. Using the insights gained from thiscomparative study, we show how accurate topic models can be learned in severalseconds on text corpora with thousands of documents.
arxiv-1205-2265 | Efficient Constrained Regret Minimization |  http://arxiv.org/abs/1205.2265  | author:Mehrdad Mahdavi, Tianbao Yang, Rong Jin category:cs.LG published:2012-05-08 summary:Online learning constitutes a mathematical and compelling framework toanalyze sequential decision making problems in adversarial environments. Thelearner repeatedly chooses an action, the environment responds with an outcome,and then the learner receives a reward for the played action. The goal of thelearner is to maximize his total reward. However, there are situations inwhich, in addition to maximizing the cumulative reward, there are someadditional constraints on the sequence of decisions that must be satisfied onaverage by the learner. In this paper we study an extension to the onlinelearning where the learner aims to maximize the total reward given that someadditional constraints need to be satisfied. By leveraging on the theory ofLagrangian method in constrained optimization, we propose Lagrangianexponentially weighted average (LEWA) algorithm, which is a primal-dual variantof the well known exponentially weighted average algorithm, to efficientlysolve constrained online decision making problems. Using novel theoreticalanalysis, we establish the regret and the violation of the constraint bounds infull information and bandit feedback models.
arxiv-1205-1782 | Approximate Dynamic Programming By Minimizing Distributionally Robust Bounds |  http://arxiv.org/abs/1205.1782  | author:Marek Petrik category:stat.ML cs.LG published:2012-05-08 summary:Approximate dynamic programming is a popular method for solving large Markovdecision processes. This paper describes a new class of approximate dynamicprogramming (ADP) methods- distributionally robust ADP-that address the curseof dimensionality by minimizing a pessimistic bound on the policy loss. Thisapproach turns ADP into an optimization problem, for which we derive newmathematical program formulations and analyze its properties. DRADP improves onthe theoretical guarantees of existing ADP methods-it guarantees convergenceand L1 norm based error bounds. The empirical evaluation of DRADP shows thatthe theoretical guarantees translate well into good performance on benchmarkproblems.
arxiv-1205-1765 | Chaotic multi-objective optimization based design of fractional order PIÎ»DÎ¼ controller in AVR system |  http://arxiv.org/abs/1205.1765  | author:Indranil Pan, Saptarshi Das category:cs.SY cs.NE published:2012-05-08 summary:In this paper, a fractional order (FO) PI{\lambda}D\mu controller is designedto take care of various contradictory objective functions for an AutomaticVoltage Regulator (AVR) system. An improved evolutionary Non-dominated SortingGenetic Algorithm II (NSGA II), which is augmented with a chaotic map forgreater effectiveness, is used for the multi-objective optimization problem.The Pareto fronts showing the trade-off between different design criteria areobtained for the PI{\lambda}D\mu and PID controller. A comparative analysis isdone with respect to the standard PID controller to demonstrate the merits anddemerits of the fractional order PI{\lambda}D\mu controller.
arxiv-1206-3559 | Real time facial expression recognition using a novel method |  http://arxiv.org/abs/1206.3559  | author:Saumil Srivastava category:cs.CV published:2012-05-08 summary:This paper discusses a novel method for Facial Expression Recognition Systemwhich performs facial expression analysis in a near real time from a live webcam feed. Primary objectives were to get results in a near real time with lightinvariant, person independent and pose invariant way. The system is composed oftwo different entities trainer and evaluator. Each frame of video feed ispassed through a series of steps including haar classifiers, skin detection,feature extraction, feature points tracking, creating a learned Support VectorMachine model to classify emotions to achieve a tradeoff between accuracy andresult rate. A processing time of 100-120 ms per 10 frames was achieved withaccuracy of around 60%. We measure our accuracy in terms of variety ofinteraction and classification scenarios. We conclude by discussing relevanceof our work to human computer interaction and exploring further measures thatcan be taken.
arxiv-1205-1564 | Characterizing Ranked Chinese Syllable-to-Character Mapping Spectrum: A Bridge Between the Spoken and Written Chinese Language |  http://arxiv.org/abs/1205.1564  | author:Wentian Li category:cs.CL stat.AP published:2012-05-08 summary:One important aspect of the relationship between spoken and written Chineseis the ranked syllable-to-character mapping spectrum, which is the ranked listof syllables by the number of characters that map to the syllable. Previously,this spectrum is analyzed for more than 400 syllables without distinguishingthe four intonations. In the current study, the spectrum with 1280 tonedsyllables is analyzed by logarithmic function, Beta rank function, andpiecewise logarithmic function. Out of the three fitting functions, thetwo-piece logarithmic function fits the data the best, both by the smallest sumof squared errors (SSE) and by the lowest Akaike information criterion (AIC)value. The Beta rank function is the close second. By sampling from a Poissondistribution whose parameter value is chosen from the observed data, weempirically estimate the $p$-value for testing thetwo-piece-logarithmic-function being better than the Beta rank functionhypothesis, to be 0.16. For practical purposes, the piecewise logarithmicfunction and the Beta rank function can be considered a tie.
arxiv-1205-1603 | Parsing of Myanmar sentences with function tagging |  http://arxiv.org/abs/1205.1603  | author:Win Win Thant, Tin Myat Htwe, Ni Lar Thein category:cs.CL published:2012-05-08 summary:This paper describes the use of Naive Bayes to address the task of assigningfunction tags and context free grammar (CFG) to parse Myanmar sentences. Partof the challenge of statistical function tagging for Myanmar sentences comesfrom the fact that Myanmar has free-phrase-order and a complex morphologicalsystem. Function tagging is a pre-processing step for parsing. In the task offunction tagging, we use the functional annotated corpus and tag Myanmarsentences with correct segmentation, POS (part-of-speech) tagging and chunkinginformation. We propose Myanmar grammar rules and apply context free grammar(CFG) to find out the parse tree of function tagged Myanmar sentences.Experiments show that our analysis achieves a good result with parsing ofsimple sentences and three types of complex sentences.
arxiv-1205-1828 | The Natural Gradient by Analogy to Signal Whitening, and Recipes and Tricks for its Use |  http://arxiv.org/abs/1205.1828  | author:Jascha Sohl-Dickstein category:cs.LG stat.ML published:2012-05-08 summary:The natural gradient allows for more efficient gradient descent by removingdependencies and biases inherent in a function's parameterization. Severalpapers present the topic thoroughly and precisely. It remains a very difficultidea to get your head around however. The intent of this note is to providesimple intuition for the natural gradient and its use. We review how an illconditioned parameter space can undermine learning, introduce the naturalgradient by analogy to the more widely understood concept of signal whitening,and present tricks and specific prescriptions for applying the natural gradientto learning problems.
arxiv-1205-1794 | A Novel Method For Speech Segmentation Based On Speakers' Characteristics |  http://arxiv.org/abs/1205.1794  | author:Behrouz Abdolali, Hossein Sameti category:cs.AI cs.CL 92C55 C.3.4 published:2012-05-08 summary:Speech Segmentation is the process change point detection for partitioning aninput audio stream into regions each of which corresponds to only one audiosource or one speaker. One application of this system is in Speaker Diarizationsystems. There are several methods for speaker segmentation; however, most ofthe Speaker Diarization Systems use BIC-based Segmentation methods. The maingoal of this paper is to propose a new method for speaker segmentation withhigher speed than the current methods - e.g. BIC - and acceptable accuracy. Ourproposed method is based on the pitch frequency of the speech. The accuracy ofthis method is similar to the accuracy of common speaker segmentation methods.However, its computation cost is much less than theirs. We show that our methodis about 2.4 times faster than the BIC-based method, while the average accuracyof pitch-based method is slightly higher than that of the BIC-based method.
arxiv-1205-1639 | Spectral Analysis of Projection Histogram for Enhancing Close matching character Recognition in Malayalam |  http://arxiv.org/abs/1205.1639  | author:Sajilal Divakaran category:cs.CL cs.CV cs.IR published:2012-05-08 summary:The success rates of Optical Character Recognition (OCR) systems for printedMalayalam documents is quite impressive with the state of the art accuracylevels in the range of 85-95% for various. However for real applications,further enhancement of this accuracy levels are required. One of the bottlenecks in further enhancement of the accuracy is identified as close-matchingcharacters. In this paper, we delineate the close matching characters inMalayalam and report the development of a specialised classifier for theseclose-matching characters. The output of a state of the art of OCR is taken andcharacters falling into the close-matching character set is further fed intothis specialised classifier for enhancing the accuracy. The classifier is basedon support vector machine algorithm and uses feature vectors derived out ofspectral coefficients of projection histogram signals of close-matchingcharacters.
arxiv-1205-1648 | A novel statistical fusion rule for image fusion and its comparison in non subsampled contourlet transform domain and wavelet domain |  http://arxiv.org/abs/1205.1648  | author:Manu V T, Philomina Simon category:cs.CV math.ST stat.TH published:2012-05-08 summary:Image fusion produces a single fused image from a set of input images. A newmethod for image fusion is proposed based on Weighted Average Merging Method(WAMM) in the NonSubsampled Contourlet Transform (NSCT) domain. A performanceanalysis on various statistical fusion rules are also analysed both in NSCT andWavelet domain. Analysis has been made on medical images, remote sensing imagesand multi focus images. Experimental results shows that the proposed method,WAMM obtained better results in NSCT domain than the wavelet domain as itpreserves more edges and keeps the visual quality intact in the fused image.
arxiv-1205-1644 | DBC based Face Recognition using DWT |  http://arxiv.org/abs/1205.1644  | author:H S Jagadeesh, K Suresh Babu, K B Raja category:cs.CV published:2012-05-08 summary:The applications using face biometric has proved its reliability in lastdecade. In this paper, we propose DBC based Face Recognition using DWT (DBC-FR) model. The Poly-U Near Infra Red (NIR) database images are scanned andcropped to get only the face part in pre-processing. The face part is resizedto 100*100 and DWT is applied to derive LL, LH, HL and HH subbands. The LLsubband of size 50*50 is converted into 100 cells with 5*5 dimention of eachcell. The Directional Binary Code (DBC) is applied on each 5*5 cell to derive100 features. The Euclidian distance measure is used to compare the features oftest image and database images. The proposed algorithm render better percentagerecognition rate compared to the existing algorithm.
arxiv-1205-1496 | Graph-based Learning with Unbalanced Clusters |  http://arxiv.org/abs/1205.1496  | author:Jing Qian, Venkatesh Saligrama, Manqi Zhao category:stat.ML cs.LG published:2012-05-07 summary:Graph construction is a crucial step in spectral clustering (SC) andgraph-based semi-supervised learning (SSL). Spectral methods applied onstandard graphs such as full-RBF, $\epsilon$-graphs and $k$-NN graphs can leadto poor performance in the presence of proximal and unbalanced data. This isbecause spectral methods based on minimizing RatioCut or normalized cut onthese graphs tend to put more importance on balancing cluster sizes overreducing cut values. We propose a novel graph construction technique and showthat the RatioCut solution on this new graph is able to handle proximal andunbalanced data. Our method is based on adaptively modulating the neighborhooddegrees in a $k$-NN graph, which tends to sparsify neighborhoods in low densityregions. Our method adapts to data with varying levels of unbalancedness andcan be naturally used for small cluster detection. We justify our ideas throughlimit cut analysis. Unsupervised and semi-supervised experiments on syntheticand real data sets demonstrate the superiority of our method.
arxiv-1205-1287 | Compressed Sensing for Energy-Efficient Wireless Telemonitoring of Noninvasive Fetal ECG via Block Sparse Bayesian Learning |  http://arxiv.org/abs/1205.1287  | author:Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Bhaskar D. Rao category:stat.ML cs.LG stat.AP published:2012-05-07 summary:Fetal ECG (FECG) telemonitoring is an important branch in telemedicine. Thedesign of a telemonitoring system via a wireless body-area network with lowenergy consumption for ambulatory use is highly desirable. As an emergingtechnique, compressed sensing (CS) shows great promise incompressing/reconstructing data with low energy consumption. However, due tosome specific characteristics of raw FECG recordings such as non-sparsity andstrong noise contamination, current CS algorithms generally fail in thisapplication. This work proposes to use the block sparse Bayesian learning (BSBL) frameworkto compress/reconstruct non-sparse raw FECG recordings. Experimental resultsshow that the framework can reconstruct the raw recordings with high quality.Especially, the reconstruction does not destroy the interdependence relationamong the multichannel recordings. This ensures that the independent componentanalysis decomposition of the reconstructed recordings has high fidelity.Furthermore, the framework allows the use of a sparse binary sensing matrixwith much fewer nonzero entries to compress recordings. Particularly, eachcolumn of the matrix can contain only two nonzero entries. This shows theframework, compared to other algorithms such as current CS algorithms andwavelet algorithms, can greatly reduce code execution in CPU in the datacompression stage.
