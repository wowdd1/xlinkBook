arxiv-1605-06474 | X-ray image separation via coupled dictionary learning |  http://arxiv.org/abs/1605.06474  | author:Nikos Deligiannis, João F. C. Mota, Bruno Cornelis, Miguel R. D. Rodrigues, Ingrid Daubechies category:cs.CV published:2016-05-20 summary:In support of art investigation, we propose a new source sepa- ration methodthat unmixes a single X-ray scan acquired from double-sided paintings. Unlikeprior source separation meth- ods, which are based on statistical or structuralincoherence of the sources, we use visual images taken from the front- andback-side of the panel to drive the separation process. The coupling of the twoimaging modalities is achieved via a new multi-scale dictionary learningmethod. Experimental results demonstrate that our method succeeds in thediscrimination of the sources, while state-of-the-art methods fail to do so.
arxiv-1605-06492 | Linear-memory and Decomposition-invariant Linearly Convergent Conditional Gradient Algorithm for Structured Polytopes |  http://arxiv.org/abs/1605.06492  | author:Dan Garber, Ofer Meshi category:math.OC cs.LG published:2016-05-20 summary:Recently, several works have shown that natural modifications of theclassical conditional gradient method (aka Frank-Wolfe algorithm) forconstrained convex optimization, provably converge with a linear rate when: i)the feasible set is a polytope, and ii) the objective is smooth andstrongly-convex. However, all of these results suffer from two significantshortcomings: large memory requirement due to the need to store an explicitconvex decomposition of the current iterate, and as a consequence, largerunning-time overhead per iteration, and worst case convergence rate thatdepends unfavorably on the dimension. In this work we present a new conditional gradient variant and acorresponding analysis that improves on both of the above shortcomings. Inparticular: both memory and computation overheads are only linear in thedimension. Moreover, in case the optimal solution is sparse, the newconvergence rate replaces a factor which is at least linear in the dimension inprevious works, with a linear dependence on the number of non-zeros in theoptimal solution. At the heart of our method, and corresponding analysis, is a novel way tocompute decomposition-invariant away-steps. While our theoretical guarantees donot apply to any polytope, they apply to several important structured polytopesthat capture central concepts such as paths in graphs, perfect matchings inbipartite graphs, marginal distributions that arise in structured predictiontasks, and more. Our theoretical findings are complemented by empiricalevidence which shows that our method delivers state-of-the-art performance.
arxiv-1605-06489 | Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups |  http://arxiv.org/abs/1605.06489  | author:Yani Ioannou, Duncan Robertson, Roberto Cipolla, Antonio Criminisi category:cs.NE cs.CV cs.LG published:2016-05-20 summary:We propose a new method for training computationally efficient and compactconvolutional neural networks (CNNs) using a novel sparse connection structurethat resembles a tree root. Our sparse connection structure facilitates asignificant reduction in computational cost and number of parameters ofstate-of-the-art deep CNNs without compromising accuracy. We validate ourapproach by using it to train more efficient variants of state-of-the-art CNNarchitectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results showsimilar or higher accuracy than the baseline architectures with much lesscompute, as measured by CPU and GPU timings. For example, for ResNet 50, ourmodel has 40% fewer parameters, 45% fewer floating point operations, and is 31%(12%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 25% fewerfloating point operations and 44% fewer parameters, while maintainingstate-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters andis 21% (16%) faster on a CPU (GPU).
arxiv-1605-06177 | Fine-Grained Classification of Pedestrians in Video: Benchmark and State of the Art |  http://arxiv.org/abs/1605.06177  | author:David Hall, Pietro Perona category:cs.CV published:2016-05-20 summary:A video dataset that is designed to study fine-grained categorisation ofpedestrians is introduced. Pedestrians were recorded "in-the-wild" from amoving vehicle. Annotations include bounding boxes, tracks, 14 keypoints withocclusion information and the fine-grained categories of age (5 classes), sex(2 classes), weight (3 classes) and clothing style (4 classes). There are atotal of 27,454 bounding box and pose labels across 4222 tracks. This datasetis designed to train and test algorithms for fine-grained categorisation ofpeople, it is also useful for benchmarking tracking, detection and poseestimation of pedestrians. State-of-the-art algorithms for fine-grainedclassification and pose estimation were tested using the dataset and theresults are reported as a useful performance baseline.
arxiv-1605-06181 | Variational hybridization and transformation for large inaccurate noisy-or networks |  http://arxiv.org/abs/1605.06181  | author:Yusheng Xie, Nan Du, Wei Fan, Jing Zhai, Weicheng Zhu category:cs.LG cs.AI stat.ML published:2016-05-20 summary:Variational inference provides approximations to the computationallyintractable posterior distribution in Bayesian networks. A prominent medicalapplication of noisy-or Bayesian network is to infer potential diseases givenobserved symptoms. Previous studies focus on approximating a handful ofcomplicated pathological cases using variational transformation. Our goal is touse variational transformation as part of a novel hybridized inference forserving reliable and real time diagnosis at web scale. We propose a hybridizedinference that allows variational parameters to be estimated without diseaseposteriors or priors, making the inference faster and much of its computationrecyclable. In addition, we propose a transformation ranking algorithm that isvery stable to large variances in network prior probabilities, a common issuethat arises in medical applications of Bayesian networks. In experiments, weperform comparative study on a large real life medical network and scalabilitystudy on a much larger (36,000x) synthesized network.
arxiv-1605-06477 | Regression with n$\to$1 by Expert Knowledge Elicitation |  http://arxiv.org/abs/1605.06477  | author:Marta Soare, Muhammad Ammad-ud-din, Samuel Kaski category:cs.LG published:2016-05-20 summary:We consider regression under the "extremely small $n$ large $p$" condition.In particular, we focus on problems with so small sample sizes $n$ compared tothe dimensionality $p$, even $n\to 1$, that predictors cannot be estimatedwithout prior knowledge. Furthermore, we assume all prior knowledge that can beautomatically extracted from databases has already been taken into account.This setup occurs in personalized medicine, for instance, when predictingtreatment outcomes for an individual patient based on noisy high-dimensionalgenomics data. A remaining source of information is expert knowledge which hasreceived relatively little attention in recent years. We formulate theinference problem of asking expert feedback on features on a budget, presentexperimental results for two setups: "small $n$" and "n=1 with similar dataavailable", and derive conditions under which the elicitation strategy isoptimal. Experiments on simulated experts, both on simulated and genomics data,demonstrate that the proposed strategy can drastically improve predictionaccuracy.
arxiv-1605-06182 | Dimensionality Reduction on SPD Manifolds: The Emergence of Geometry-Aware Methods |  http://arxiv.org/abs/1605.06182  | author:Mehrtash Harandi, Mathieu Salzmann, Richard Hartley category:cs.CV published:2016-05-20 summary:Representing images and videos with Symmetric Positive Definite (SPD)matrices, and considering the Riemannian geometry of the resulting space, hasbeen shown to yield high discriminative power in many visual recognition tasks.Unfortunately, computation on the Riemannian manifold of SPD matrices-especially of high-dimensional ones- comes at a high cost that limits theapplicability of existing techniques. In this paper, we introduce algorithmsable to handle high-dimensional SPD matrices by constructing alower-dimensional SPD manifold. To this end, we propose to model the mappingfrom the high-dimensional SPD manifold to the low-dimensional one with anorthonormal projection. This lets us formulate dimensionality reduction as theproblem of finding a projection that yields a low-dimensional manifold eitherwith maximum discriminative power in the supervised scenario, or with maximumvariance of the data in the unsupervised one. We show that learning can beexpressed as an optimization problem on a Grassmann manifold and discuss fastsolutions for special cases. Our evaluation on several classification tasksevidences that our approach leads to a significant accuracy gain overstate-of-the-art methods.
arxiv-1605-06197 | Deep Generative Models with Stick-Breaking Priors |  http://arxiv.org/abs/1605.06197  | author:Eric Nalisnick, Padhraic Smyth category:stat.ML published:2016-05-20 summary:Bayesian nonparametric models are attractive for their data-dependentcapacity, but their implementation can be problematic due to computational oranalytical obstacles. We make progress on this problem by extending StochasticGradient Variational Bayes (Kingma & Welling, 2013), a 'black box' method forapproximate posterior inference, to stick-breaking priors (Ishwaran & James,2001). This innovation allows us to define deep generative models (DGMs) withinfinite dimensional latent variables. We experimentally demonstrate that DGMswith Dirichlet process priors learn highly discriminative latentrepresentations that are well suited for semi-supervised settings and oftenoutperform the popular Gaussian alternative.
arxiv-1605-06201 | Adversarial Delays in Online Strongly-Convex Optimization |  http://arxiv.org/abs/1605.06201  | author:Daniel Khashabi, Kent Quanrud, Amirhossein Taghvaei category:cs.LG cs.AI stat.ML published:2016-05-20 summary:We consider the problem of strongly-convex online optimization in presence ofadversarial delays; in a T-iteration online game, the feedback of the player'squery at time t is arbitrarily delayed by an adversary for d_t rounds anddelivered before the game ends, at iteration t+d_t-1. Specifically for\algo{online-gradient-descent} algorithm we show it has a simple regret boundof \Oh{\sum_{t=1}^T \log (1+ \frac{d_t}{t})}. This gives a clear and simplebound without resorting any distributional and limiting assumptions on thedelays. We further show how this result encompasses and generalizes several ofthe existing known results in the literature. Specifically it matches thecelebrated logarithmic regret \Oh{\log T} when there are no delays (i.e. d_t =1) and regret bound of \Oh{\tau \log T} for constant delays d_t = \tau.
arxiv-1605-06203 | Faster Projection-free Convex Optimization over the Spectrahedron |  http://arxiv.org/abs/1605.06203  | author:Dan Garber category:math.OC cs.LG published:2016-05-20 summary:Minimizing a convex function over the spectrahedron, i.e., the set of allpositive semidefinite matrices with unit trace, is an important optimizationtask with many applications in optimization, machine learning, and signalprocessing. It is also notoriously difficult to solve in large-scale sincestandard techniques require expensive matrix decompositions. An alternative, isthe conditional gradient method (aka Frank-Wolfe algorithm) that regained muchinterest in recent years, mostly due to its application to this specificsetting. The key benefit of the CG method is that it avoids expensive matrixdecompositions all together, and simply requires a single eigenvectorcomputation per iteration, which is much more efficient. On the downside, theCG method, in general, converges with an inferior rate. The error forminimizing a $\beta$-smooth function after $t$ iterations scales like$\beta/t$. This convergence rate does not improve even if the function is alsostrongly convex. In this work we present a modification of the CG method tailored for convexoptimization over the spectrahedron. The per-iteration complexity of the methodis essentially identical to that of the standard CG method: only a singleeigenvecor computation is required. For minimizing an $\alpha$-strongly convexand $\beta$-smooth function, the expected approximation error of the methodafter $t$ iterations is: $$O\left({\min\{\frac{\beta{}}{t},\left({\frac{\beta\sqrt{\textrm{rank}(\textbf{X}^*)}}{\alpha^{1/4}t}}\right)^{4/3},\left({\frac{\beta}{\sqrt{\alpha}\lambda_{\min}(\textbf{X}^*)t}}\right)^{2}\}}\right),$$ where $\textbf{X}^*$ is the optimal solution. To the best of our knowledge,this is the first result that attains provably faster convergence rates for aCG variant for optimization over the spectrahedron. We also present encouragingpreliminary empirical results.
arxiv-1605-06211 | Fully Convolutional Networks for Semantic Segmentation |  http://arxiv.org/abs/1605.06211  | author:Evan Shelhamer, Jonathan Long, Trevor Darrell category:cs.CV published:2016-05-20 summary:Convolutional networks are powerful visual models that yield hierarchies offeatures. We show that convolutional networks by themselves, trainedend-to-end, pixels-to-pixels, improve on the previous best result in semanticsegmentation. Our key insight is to build "fully convolutional" networks thattake input of arbitrary size and produce correspondingly-sized output withefficient inference and learning. We define and detail the space of fullyconvolutional networks, explain their application to spatially dense predictiontasks, and draw connections to prior models. We adapt contemporaryclassification networks (AlexNet, the VGG net, and GoogLeNet) into fullyconvolutional networks and transfer their learned representations byfine-tuning to the segmentation task. We then define a skip architecture thatcombines semantic information from a deep, coarse layer with appearanceinformation from a shallow, fine layer to produce accurate and detailedsegmentations. Our fully convolutional network achieves improved segmentationof PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFTFlow, and PASCAL-Context, while inference takes one tenth of a second for atypical image.
arxiv-1605-06215 | TRIM: Triangulating Images for Efficient Registration |  http://arxiv.org/abs/1605.06215  | author:Chun Pang Yung, Gary Pui-Tung Choi, Ke Chen, Lok Ming Lui category:cs.GR cs.CG cs.CV published:2016-05-20 summary:With the advancement in the digital camera technology, the use of highresolution images and videos has been widespread in the modern society. Inparticular, image and video frame registration is frequently applied incomputer graphics and film production. However, the conventional registrationapproaches usually require long computational time for high quality images andvideo frames. This hinders the applications of the registration approaches inthe modern industries. In this work, we propose a novel approach called {\emTRIM} to accelerate the computations of the registration by triangulating theimages. More specifically, given a high resolution image or video frame, wecompute an optimal coarse triangulation which captures the important featuresof the image. Then, the computation of the registration can be simplified withthe aid of the coarse triangulation. Experimental results suggest that thecomputational time of the registration is significantly reduced using ourtriangulation-based approach, meanwhile the accuracy of the registration iswell retained when compared with the conventional grid-based approach.
arxiv-1605-06217 | Localizing by Describing: Attribute-Guided Attention Localization for Fine-Grained Recognition |  http://arxiv.org/abs/1605.06217  | author:Xiao Liu, Jiang Wang, Shilei Wen, Errui Ding, Yuanqing Lin category:cs.CV published:2016-05-20 summary:A key challenge in fine-grained recognition is how to find and representdiscriminative local regions. Recent attention models are capable of learningdiscriminative region localizers only from category labels with reinforcementlearning. However, not utilizing any explicit part information, they are notable to accurately find multiple distinctive regions. In this work, weintroduce an attribute-guided attention localization scheme where the localregion localizers are learned under the guidance of part attributedescriptions. By designing a novel reward strategy, we are able to learn tolocate regions that are spatially and semantically distinctive withreinforcement learning algorithm. The attribute labeling requirement of thescheme is more amenable than the accurate part location annotation required bytraditional part-based fine-grained recognition methods. Experimental resultson the CUB-200-2011 dataset demonstrate the superiority of the proposed schemeon both fine-grained recognition and attribute recognition.
arxiv-1605-06220 | Convergence of Contrastive Divergence with Annealed Learning Rate in Exponential Family |  http://arxiv.org/abs/1605.06220  | author:Bai Jiang, Tung-yu Wu, Wing H. Wong category:stat.ML cs.LG published:2016-05-20 summary:In our recent paper, we showed that in exponential family, contrastivedivergence (CD) with fixed learning rate will give asymptotically consistentestimates \cite{wu2016convergence}. In this paper, we establish consistency andconvergence rate of CD with annealed learning rate $\eta_t$. Specifically,suppose CD-$m$ generates the sequence of parameters $\{\theta_t\}_{t \ge 0}$using an i.i.d. data sample $\mathbf{X}_1^n \sim p_{\theta^*}$ of size $n$,then $\delta_n(\mathbf{X}_1^n) = \limsup_{t \to \infty} \Vert \sum_{s=t_0}^t\eta_s \theta_s / \sum_{s=t_0}^t \eta_s - \theta^* \Vert$ converges inprobability to 0 at a rate of $1/\sqrt[3]{n}$. The number ($m$) of MCMCtransitions in CD only affects the coefficient factor of convergence rate. Ourproof is not a simple extension of the one in \cite{wu2016convergence}. whichdepends critically on the fact that $\{\theta_t\}_{t \ge 0}$ is a homogeneousMarkov chain conditional on the observed sample $\mathbf{X}_1^n$. Underannealed learning rate, the homogeneous Markov property is not available and wehave to develop an alternative approach based on super-martingales. Experimentresults of CD on a fully-visible $2\times 2$ Boltzmann Machine are provided todemonstrate our theoretical results.
arxiv-1605-06465 | Swapout: Learning an ensemble of deep architectures |  http://arxiv.org/abs/1605.06465  | author:Saurabh Singh, Derek Hoiem, David Forsyth category:cs.CV cs.LG cs.NE published:2016-05-20 summary:We describe Swapout, a new stochastic training method, that outperformsResNets of identical network structure yielding impressive results on CIFAR-10and CIFAR-100. Swapout samples from a rich set of architectures includingdropout, stochastic depth and residual architectures as special cases. Whenviewed as a regularization method swapout not only inhibits co-adaptation ofunits in a layer, similar to dropout, but also across network layers. Weconjecture that swapout achieves strong regularization by implicitly tying theparameters across layers. When viewed as an ensemble training method, itsamples a much richer set of architectures than existing methods such asdropout or stochastic depth. We propose a parameterization that revealsconnections to exiting architectures and suggests a much richer set ofarchitectures to be explored. We show that our formulation suggests anefficient training method and validate our conclusions on CIFAR-10 andCIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer widermodel performs similar to a 1001 layer ResNet model.
arxiv-1605-06240 | FPNN: Field Probing Neural Networks for 3D Data |  http://arxiv.org/abs/1605.06240  | author:Yangyan Li, Soeren Pirk, Hao Su, Charles R. Qi, Leonidas J. Guibas category:cs.CV I.5.1, I.2.10 published:2016-05-20 summary:Building discriminative representations for 3D data has been an importanttask in computer graphics and computer vision research. Convolutional NeuralNetworks (CNNs) have shown to operate on 2D images with great success for avariety of tasks. Lifting convolution operators to 3D (3DCNNs) seems like aplausible and promising next step. Unfortunately, the computational complexityof 3D CNNs grows cubically with respect to voxel resolution. Moreover, sincemost 3D geometry representations are boundary based, occupied regions do notincrease proportionately with the size of the discretization, resulting inwasted computation. In this work, we represent 3D spaces as volumetric fields,and propose a novel design that employs field probing filters to efficientlyextract features from them. Each field probing filter is a set of probingpoints --- sensors that perceive the space. Our learning algorithm optimizesnot only the weights associated with the probing points, but also theirlocations, which deforms the shape of the probing filters and adaptivelydistributes them in 3D space. The optimized probing points sense the 3D space"intelligently", rather than operating blindly over the entire domain. We showthat field probing is significantly more efficient than 3DCNNs, while providingstate-of-the-art performance, on classification tasks for 3D object recognitionbenchmark datasets.
arxiv-1605-06265 | End-to-End Kernel Learning with Supervised Convolutional Kernel Networks |  http://arxiv.org/abs/1605.06265  | author:Julien Mairal category:stat.ML cs.CV cs.LG published:2016-05-20 summary:In this paper, we propose a new image representation based on a multilayerkernel machine that performs end-to-end learning. Unlike traditional kernelmethods, where the kernel is handcrafted or adapted to data in an unsupervisedmanner, we learn how to shape the kernel for a supervised prediction problem.We proceed by generalizing convolutional kernel networks, which originallyprovide unsupervised image representations, and we derive backpropagation rulesto optimize model parameters. As a result, we obtain a new type ofconvolutional neural network with the following properties: (i) at each layer,learning filters is equivalent to optimizing a linear subspace in a reproducingkernel Hilbert space (RKHS), where we project data, (ii) the network may belearned with supervision or without, (iii) the model comes with a naturalregularization function (the norm in the RKHS). We show that our methodachieves reasonably competitive performance on some standard "deep learning"image classification datasets such as CIFAR-10 and SVHN, and alsostate-of-the-art results for image super-resolution, demonstrating theapplicability of our approach to a large variety of image-related tasks.
arxiv-1605-06276 | Piece-wise quadratic lego set for constructing arbitrary error potentials and their fast optimization |  http://arxiv.org/abs/1605.06276  | author:A. N. Gorban, E. M. Mirkes, A. Zinovyev category:cs.LG stat.ML published:2016-05-20 summary:Most of machine learning approaches have stemmed from the application ofminimizing the mean squared distance principle, based on the computationallyefficient quadratic optimization methods. However, when faced withhigh-dimensional and noisy data, the quadratic error functionals demonstratemany weaknesses including high sensitivity to contaminating factors anddimensionality curse. Therefore, a lot of recent applications in machinelearning exploited the properties of non-quadratic error functionals based onL1 norm or even sub-linear potentials corresponding to fractional norms. Theback side of these approaches is tremendous increase in computational cost foroptimization. Till so far, no approaches have been suggested to deal with {\itarbitrary} error functionals, in a flexible and computationally efficientframework. In this paper, we develop the theory and basic universal dataapproximation algorithms ($k$-means, principal components, principal manifoldsand graphs), based on piece-wise quadratic error potentials of subquadraticgrowth (PQSQ potentials). We develop a new and universal framework to minimize{\it arbitrary sub-quadratic error potentials} using an algorithm withguaranteed fast convergence to the local or global error minimum. The approachcan be applied in most of existing machine learning methods, including methodsof data approximation and regularized regression, leading to the improvement inthe computational cost/accuracy trade-off.
arxiv-1605-06457 | Virtual Worlds as Proxy for Multi-Object Tracking Analysis |  http://arxiv.org/abs/1605.06457  | author:Adrien Gaidon, Qiao Wang, Yohann Cabon, Eleonora Vig category:cs.CV cs.LG cs.NE stat.ML published:2016-05-20 summary:Modern computer vision algorithms typically require expensive dataacquisition and accurate manual labeling. In this work, we instead leverage therecent progress in computer graphics to generate fully labeled, dynamic, andphoto-realistic proxy virtual worlds. We propose an efficient real-to-virtualworld cloning method, and validate our approach by building and publiclyreleasing a new video dataset, called Virtual KITTI (seehttp://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds),automatically labeled with accurate ground truth for object detection,tracking, scene and instance segmentation, depth, and optical flow. We providequantitative experimental evidence suggesting that (i) modern deep learningalgorithms pre-trained on real data behave similarly in real and virtualworlds, and (ii) pre-training on virtual data improves performance. As the gapbetween real and virtual worlds is small, virtual worlds enable measuring theimpact of various weather and imaging conditions on recognition performance,all other things being equal. We show these factors may affect drasticallyotherwise high-performing deep models for tracking.
arxiv-1605-06451 | Fixed Points of Belief Propagation -- An Analysis via Polynomial Homotopy Continuation |  http://arxiv.org/abs/1605.06451  | author:Christian Knoll, Franz Pernkopf, Dhagash Mehta, Tianran Chen category:stat.ML published:2016-05-20 summary:Belief propagation (BP) is an iterative method to perform approximateinference on arbitrary graphical models. Whether BP converges and if thesolution is a unique fixed point depends on both, the structure and theparametrization of the model. To understand this dependence we are interestedin finding \emph{all} fixed points. In this work, we formulate BP as a set ofpolynomial equations, the solutions of which correspond to the BP fixed points.We apply the numerical polynomial-homotopy-continuation (NPHC) method to solvesuch systems. It is commonly believed that uniqueness of BP fixed pointsimplies convergence to this fixed point. Contrary to this conjecture, we findgraphs for which BP fails to converge, even though a unique fixed point exists.Moreover, we show that this fixed point gives a good approximation of the exactmarginal distribution.
arxiv-1605-06450 | Query-Efficient Imitation Learning for End-to-End Autonomous Driving |  http://arxiv.org/abs/1605.06450  | author:Jiakai Zhang, Kyunghyun Cho category:cs.LG cs.AI cs.RO published:2016-05-20 summary:One way to approach end-to-end autonomous driving is to learn a policyfunction that maps from a sensory input, such as an image frame from afront-facing camera, to a driving action, by imitating an expert driver, or areference policy. This can be done by supervised learning, where a policyfunction is tuned to minimize the difference between the predicted andground-truth actions. A policy function trained in this way however is known tosuffer from unexpected behaviours due to the mismatch between the statesreachable by the reference policy and trained policy functions. More advancedalgorithms for imitation learning, such as DAgger, addresses this issue byiteratively collecting training examples from both reference and trainedpolicies. These algorithms often requires a large number of queries to areference policy, which is undesirable as the reference policy is oftenexpensive. In this paper, we propose an extension of the DAgger, calledSafeDAgger, that is query-efficient and more suitable for end-to-end autonomousdriving. We evaluate the proposed SafeDAgger in a car racing simulator and showthat it indeed requires less queries to a reference policy. We observe asignificant speed up in convergence, which we conjecture to be due to theeffect of automated curriculum learning.
arxiv-1605-06444 | Unreasonable Effectiveness of Learning Neural Nets: Accessible States and Robust Ensembles |  http://arxiv.org/abs/1605.06444  | author:Carlo Baldassi, Christian Borgs, Jennifer Chayes, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, Riccardo Zecchina category:stat.ML published:2016-05-20 summary:In artificial neural networks, learning from data is a computationallydemanding task in which a large number of connection weights are iterativelytuned through stochastic-gradient-based heuristic processes over acost-function. It is not well understood how learning occurs in these systems,in particular how they avoid getting trapped in configurations with poorcomputational performance. Here we study the difficult case of networks withdiscrete weights, where the optimization landscape is very rough even forsimple architectures, and provide theoretical and numerical evidence of theexistence of rare---but extremely dense and accessible---regions ofconfigurations in the network weight space. We define a novel measure, which wecall the \emph{robust ensemble} (RE), which suppresses trapping by isolatedconfigurations and amplifies the role of these dense regions. We analyticallycompute the RE in some exactly solvable models, and also provide a generalalgorithmic scheme which is straightforward to implement: define acost-function given by a sum of a finite number of replicas of the originalcost-function, with a constraint centering the replicas around a drivingassignment. To illustrate this, we derive several powerful new algorithms,ranging from Markov Chains to message passing to gradient descent processes,where the algorithms target the robust dense states, resulting in substantialimprovements in performance. The weak dependence on the number of precisionbits of the weights leads us to conjecture that very similar reasoning appliesto more conventional neural networks. Analogous algorithmic schemes can also beapplied to other optimization problems.
arxiv-1605-06443 | Structured Prediction Theory and Voted Risk Minimization |  http://arxiv.org/abs/1605.06443  | author:Corinna Cortes, Mehryar Mohri, Vitaly Kuznetsov, Scott Yang category:stat.ML cs.LG published:2016-05-20 summary:We present a general theoretical analysis of structured prediction. Byintroducing a new complexity measure that explicitly factors in the structureof the output space and the loss function, we are able to derive newdata-dependent learning guarantees for a broad family of losses and forhypothesis sets with an arbitrary factor graph decomposition. We extend thistheory by leveraging the principle of Voted Risk Minimization (VRM) and showingthat learning is possible with complex factor graphs. We both present newlearning bounds in this advanced setting as well as derive two new families ofalgorithms, \emph{Voted Conditional Random Fields} and \emph{Voted StructuredBoosting}, which can make use of very complex features and factor graphswithout overfitting. Finally, we also validate our theory through experimentson several datasets.
arxiv-1605-06439 | Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning |  http://arxiv.org/abs/1605.06439  | author:Wouter M. Koolen, Peter Grünwald, Tim van Erven category:cs.LG published:2016-05-20 summary:We consider online learning algorithms that guarantee worst-case regret ratesin adversarial environments (so they can be deployed safely and will performrobustly), yet adapt optimally to favorable stochastic environments (so theywill perform well in a variety of settings of practical importance). Wequantify the friendliness of stochastic environments by means of the well-knownBernstein (a.k.a. generalized Tsybakov margin) condition. For two recentalgorithms (Squint for the Hedge setting and MetaGrad for online convexoptimization) we show that the particular form of their data-dependentindividual-sequence regret guarantees implies that they adapt automatically tothe Bernstein parameters of the stochastic environment. We prove that thesealgorithms attain fast rates in their respective settings both in expectationand with high probability.
arxiv-1605-06296 | On the Robustness of Decision Tree Learning under Label Noise |  http://arxiv.org/abs/1605.06296  | author:Aritra Ghosh, Naresh Manwani, P. S. Sastry category:cs.LG published:2016-05-20 summary:In most practical problems of classifier learning, the training data suffersfrom the label noise. Hence, it is important to understand how robust is alearning algorithm to such label noise. Experimentally, Decision trees havebeen found to be more robust against label noise than SVM and logisticregression. This paper presents some theoretical results to show that decisiontree algorithms are robust to symmetric label noise under the assumption oflarge sample size. We also present some sample complexity results for thisrobustness. Through extensive simulations we illustrate this robustness.
arxiv-1605-06304 | Local communities obstruct global consensus: Naming game on multi-local-world networks |  http://arxiv.org/abs/1605.06304  | author:Yang Lou, Guanrong Chen, Zhengping Fan, Luna Xiang category:cs.SI cs.CL physics.soc-ph published:2016-05-20 summary:Community structure is essential for social communications, where individualsbelonging to the same community are much more actively interacting andcommunicating with each other than those in different communities within thehuman society. Naming game, on the other hand, is a social communication modelthat simulates the process of learning a name of an object within a communityof humans, where the individuals can reach global consensus on naming an objectasymptotically through iterative pair-wise conversations. The underlyingcommunication network indicates the relationships among the individuals. Inthis paper, three typical topologies of human communication networks, namelyrandom-graph, small-world and scale-free networks, are employed, which areembedded with the multi-local-world community structure, to study the naminggame. Simulations show that 1) when the intra-community connections increasewhile the inter-community connections remain to be unchanged, the convergenceto global consensus is slow and eventually might fail; 2) when theinter-community connections are sufficiently dense, both the number and thesize of the communities do not affect the convergence process; and 3) fordifferent topologies with the same average node-degree, local clustering ofindividuals obstruct or prohibit global consensus to take place. The resultsreveal the role of local communities in a global naming game in social networkstudies.
arxiv-1605-06437 | Learning shape correspondence with anisotropic convolutional neural networks |  http://arxiv.org/abs/1605.06437  | author:Davide Boscaini, Jonathan Masci, Emanuele Rodolà, Michael M. Bronstein category:cs.CV published:2016-05-20 summary:Establishing correspondence between shapes is a fundamental problem ingeometry processing, arising in a wide variety of applications. The problem isespecially difficult in the setting of non-isometric deformations, as well asin the presence of topological noise and missing parts, mainly due to thelimited capability to model such deformations axiomatically. Several recentworks showed that invariance to complex shape transformations can be learnedfrom examples. In this paper, we introduce an intrinsic convolutional neuralnetwork architecture based on anisotropic diffusion kernels, which we termAnisotropic Convolutional Neural Network (ACNN). In our construction, wegeneralize convolutions to non-Euclidean domains by constructing a set oforiented anisotropic diffusion kernels, creating in this way a local intrinsicpolar representation of the data (`patch'), which is then correlated with afilter. Several cascades of such filters, linear, and non-linear operators arestacked to form a deep neural network whose parameters are learned byminimizing a task-specific cost. We use ACNNs to effectively learn intrinsicdense correspondences between deformable shapes in very challenging settings,achieving state-of-the-art results on some of the most difficult recentcorrespondence benchmarks.
arxiv-1605-06432 | Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data |  http://arxiv.org/abs/1605.06432  | author:Maximilian Karl, Maximilian Soelch, Justin Bayer, Patrick van der Smagt category:stat.ML cs.LG cs.SY published:2016-05-20 summary:We introduce Deep Variational Bayes Filters (DVBF), a new method forunsupervised learning of latent Markovian state space models. Leveraging recentadvances in Stochastic Gradient Variational Bayes, DVBF can overcomeintractable inference distributions by means of variational inference. Thus, itcan handle highly nonlinear input data with temporal and spatial dependenciessuch as image sequences without domain knowledge. Our experiments show thatenabling backpropagation through transitions enforces state space assumptionsand significantly improves information content of the latent embedding. Thisalso enables realistic long-term prediction.
arxiv-1605-06311 | Poisson multi-Bernoulli conjugate prior for multiple extended object estimation |  http://arxiv.org/abs/1605.06311  | author:Karl Granstrom, Maryam Fatemi, Lennart Svensson category:stat.CO cs.CV cs.SY published:2016-05-20 summary:This paper presents a Poisson multi-Bernoulli mixture (PMBM) conjugate priorfor multiple extended object estimation. A Poisson point process is used todescribe the existence of yet undetected targets, while a multi-Bernoullimixture describes the distribution of the targets that have been detected. Theconjugacy property allows the posterior PMBM density to be computed exactly,meaning that given enough computational power the PMBM filter is correct.However, in practice, the data association problem requires approximations. Theupdate and the prediction of the PMBM density parameters are presented and aregiven interpretations, and a simple linear Gaussian implementation is presentedalong with methods to handle the data association problem. A simulation studyshows that the extended target PMBM filter outperforms the extended targetcardinalized probability hypothesis density (CPHD) filter in scenarios wherethe expected number of detections per target per time step is low.
arxiv-1605-06319 | As Cool as a Cucumber: Towards a Corpus of Contemporary Similes in Serbian |  http://arxiv.org/abs/1605.06319  | author:Nikola Milosevic, Goran Nenadic category:cs.CL cs.AI published:2016-05-20 summary:Similes are natural language expressions used to compare unlikely things,where the comparison is not taken literally. They are often used in everydaycommunication and are an important part of cultural heritage. Having anup-to-date corpus of similes is challenging, as they are constantly coinedand/or adapted to the contemporary times. In this paper we present amethodology for semi-automated collection of similes from the world wide webusing text mining techniques. We expanded an existing corpus of traditionalsimiles (containing 333 similes) by collecting 446 additional expressions. We,also, explore how crowdsourcing can be used to extract and curate new similes.
arxiv-1605-06325 | Superpixel Hierarchy |  http://arxiv.org/abs/1605.06325  | author:Xing Wei, Qingxiong Yang, Yihong Gong, Ming-Hsuan Yang, Narendra Ahuja category:cs.CV published:2016-05-20 summary:Superpixel segmentation is becoming ubiquitous in computer vision. Inpractice, an object can either be represented by a number of segments in finerlevels of detail or included in a surrounding region at coarser levels ofdetail, and thus a superpixel segmentation hierarchy is useful for applicationsthat require different levels of image segmentation detail depending on theparticular image objects segmented. Unfortunately, there is no method that cangenerate all scales of superpixels accurately in real-time. As a result, asimple yet effective algorithm named Super Hierarchy (SH) is proposed in thispaper. It is as accurate as the state-of-the-art but 1-2 orders of magnitudefaster. The proposed method can be directly integrated with recent efficientedge detectors like the structured forest edges to significantly outperformsthe state-of-the-art in terms of segmentation accuracy. Quantitative andqualitative evaluation on a number of computer vision applications wasconducted, demonstrating that the proposed method is the top performer.
arxiv-1605-06336 | Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA |  http://arxiv.org/abs/1605.06336  | author:Aapo Hyvarinen, Hiroshi Morioka category:stat.ML cs.LG published:2016-05-20 summary:Nonlinear independent component analysis (ICA) provides an appealingframework for unsupervised feature learning, but the models proposed so far arenot identifiable. Here, we first propose a new intuitive principle ofunsupervised deep learning from time series which uses the nonstationarystructure of the data. Our learning principle, time-contrastive learning (TCL),finds a representation which allows optimal discrimination of time segments(windows). Surprisingly, we show how TCL can be related to a nonlinear ICAmodel, when ICA is redefined to include temporal nonstationarities. Inparticular, we show that TCL combined with linear ICA estimates the nonlinearICA model up to point-wise transformations of the sources, and this solution isunique --- thus providing the first identifiability result for nonlinear ICAwhich is rigorous, constructive, as well as very general.
arxiv-1605-06431 | Residual Networks are Exponential Ensembles of Relatively Shallow Networks |  http://arxiv.org/abs/1605.06431  | author:Andreas Veit, Michael Wilber, Serge Belongie category:cs.CV cs.AI cs.LG cs.NE published:2016-05-20 summary:In this work, we introduce a novel interpretation of residual networksshowing they are exponential ensembles. This observation is supported by alarge-scale lesion study that demonstrates they behave just like ensembles attest time. Subsequently, we perform an analysis showing these ensembles mostlyconsist of networks that are each relatively shallow. For example, contrary toour expectations, most of the gradient in a residual network with 110 layerscomes from an ensemble of very short networks, i.e., only 10-34 layers deep.This suggests that in addition to describing neural networks in terms of widthand depth, there is a third dimension: multiplicity, the size of the implicitensemble. Ultimately, residual networks do not resolve the vanishing gradientproblem by preserving gradient flow throughout the entire depth of the network- rather, they avoid the problem simply by ensembling many short networkstogether. This insight reveals that depth is still an open research questionand invites the exploration of the related notion of multiplicity.
arxiv-1605-06423 | Coresets for Scalable Bayesian Logistic Regression |  http://arxiv.org/abs/1605.06423  | author:Jonathan H. Huggins, Trevor Campbell, Tamara Broderick category:stat.CO cs.DS stat.ML published:2016-05-20 summary:The use of Bayesian models in large-scale data settings is attractive becauseof the rich hierarchical models, uncertainty quantification, and priorspecification they provide. Standard Bayesian inference algorithms arecomputationally expensive, however, making their direct application to largedatasets difficult or infeasible. Recent work on scaling Bayesian inference hasfocused on modifying the underlying algorithms to, for example, use only arandom data subsample at each iteration. We leverage the insight that data isoften redundant to instead obtain a weighted subset of the data (called acoreset) that is much smaller than the original dataset. We can then use thissmall coreset in any number of existing posterior inference algorithms withoutmodification. In this paper, we develop an efficient coreset constructionalgorithm for Bayesian logistic regression models. We provide theoreticalguarantees on the size and approximation quality of the coreset -- both forfixed, known datasets, and in expectation for a wide class of data generativemodels. The proposed approach also permits efficient construction of thecoreset in both streaming and parallel settings, with minimal additionaleffort. We demonstrate the efficacy of our approach on a number of syntheticand real-world datasets, and find that, in practice, the size of the coreset isindependent of the original dataset size.
arxiv-1605-06353 | Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction |  http://arxiv.org/abs/1605.06353  | author:Marcin Junczys-Dowmunt, Roman Grundkiewicz category:cs.CL published:2016-05-20 summary:In this work, we study parameter tuning towards the M$^2$ metric, thestandard metric for automatic grammar error correction (GEC) tasks. Afterimplementing M$^2$ as a scorer in the Moses tuning framework, we investigateinteractions of dense and sparse features, different optimizers, and tuningstrategies for the CoNLL-2014 shared task. We notice erratic behavior whenoptimizing sparse feature weights with M$^2$ and offer partial solutions. Toour surprise, we find that a bare-bones phrase-based SMT setup withtask-specific parameter-tuning outperforms all previously published results forthe CoNLL-2014 test set by a large margin (46.37% M$^2$ over previously 40.56%,by a neural encoder-decoder model) while being trained on the same data. Ournewly introduced dense and sparse features widen that gap, and we improve thestate-of-the-art to 49.49% M$^2$.
arxiv-1605-06359 | Learning to Discover Probabilistic Graphical Model Structures |  http://arxiv.org/abs/1605.06359  | author:Eugene Belilovsky, Kyle Kastner, Gaël Varoquaux, Matthew Blaschko category:stat.ML published:2016-05-20 summary:In this work we consider structure discovery of undirected graphical modelsfrom observational data. Inferring likely structures from few examples is acomplex task often requiring formulating priors and sophisticated inferenceprocedures. In the setting of Gaussian Graphical Models (GGMs) a popularapproach to formulating an estimator is with a penalized maximum likelihoodobjective on the precision matrix. This objective is often difficult to designto specifically fit ones priors and the graph structure recovery is often notexplicitly possible to embed in the objective, moreover incorporating anyadditional assumptions often requires a great deal of research effort. Bycontrast, it may be easier to generate samples of data that are arise fromgraphs with the desired properties. We propose here to leverage this lattersource of information in order to learn a function that maps from empiricalcovariance matrices to estimated graph structures. This learned function bringstwo benefits: it implicitly models the desired structure or sparsity propertiesto form suitable priors, and it can more directly be tailored to the specificproblem of edge structure discovery. We apply this framework to severalcritical real world problems in structure discovery and show that it can becompetitive to standard approaches such as graphical lasso, at a fraction ofthe execution speed. We use deep neural networks to parametrize our estimators.Experimentally, our learn able graph discovery method trained on synthetic datageneralizes well to different data: identifying relevant edges in real data,completely unknown at training time. We find that on genetics, brain imaging,and simulation data we obtain competitive (and often superior) performance,compared with analytical methods.
arxiv-1605-06422 | Fast Randomized Semi-Supervised Clustering |  http://arxiv.org/abs/1605.06422  | author:Alaa Saade, Florent Krzakala, Marc Lelarge, Lenka Zdeborová category:cs.LG stat.ML published:2016-05-20 summary:We consider the problem of clustering partially labeled data from a minimalnumber of randomly chosen pairwise comparisons between the items. We introducean efficient local algorithm based on a power iteration of the non-backtrackingoperator and study its performance on a simple model. For the case of twoclusters, we give bounds on the classification error and show that a smallerror can be achieved from $O(n)$ randomly chosen measurements, where $n$ isthe number of items in the dataset. Our algorithm is therefore efficient bothin terms of time and space complexities. We also investigate numerically theperformance of the algorithm on synthetic and real world data.
arxiv-1605-06376 | Fast $ε$-free Inference of Simulation Models with Bayesian Conditional Density Estimation |  http://arxiv.org/abs/1605.06376  | author:George Papamakarios, Iain Murray category:stat.ML cs.LG stat.CO published:2016-05-20 summary:Many statistical models can be simulated forwards but have intractablelikelihoods. Approximate Bayesian Computation (ABC) methods are used to inferproperties of these models from data. Traditionally these methods approximatethe posterior over parameters by conditioning on data being inside an$\epsilon$-ball around the observed data, which is only correct in the limit$\epsilon\!\rightarrow\!0$. Monte Carlo methods can then draw samples from theapproximate posterior to approximate predictions or error bars on parameters.These algorithms critically slow down as $\epsilon\!\rightarrow\!0$, and inpractice draw samples from a broader distribution than the posterior. Wepropose a new approach to likelihood-free inference based on Bayesianconditional density estimation. Preliminary inferences based on limitedsimulation data are used to guide later simulations. In some cases, learning anaccurate parametric representation of the entire true posterior distributionrequires fewer model simulations than Monte Carlo ABC methods need to produce asingle sample from an approximate posterior.
arxiv-1605-06421 | Root-cause analysis for time-series anomalies via spatiotemporal causal graphical modeling |  http://arxiv.org/abs/1605.06421  | author:Chao Liu, Kin Gwn Lore, Soumik Sarkar category:cs.LG published:2016-05-20 summary:Modern distributed cyber-physical systems encounter a large variety ofanomalies and in many cases, they are vulnerable to catastrophic faultpropagation scenarios due to strong connectivity among the sub-systems. In thisregard, root-cause analysis becomes highly intractable due to complex faultpropagation mechanisms in combination with diverse operating modes. This paperpresents a new data-driven framework for root-cause analysis for addressingsuch issues. The framework is based on a spatiotemporal feature extractionscheme for multivariate time series built on the concept of symbolic dynamicsfor discovering and representing causal interactions among subsystems of acomplex system. We propose sequential state switching ($S^3$) and artificialanomaly association ($A^3$) methods to implement root-cause analysis in anunsupervised and semi-supervised manner respectively. Synthetic data from caseswith failed pattern(s) and anomalous node are simulated to validate theproposed approaches, then compared with the performance of vectorautoregressive (VAR) model-based root-cause analysis. The results show that:(1) $S^3$ and $A^3$ approaches can obtain high accuracy in root-cause analysisand successfully handle multiple nominal operation modes, and (2) the proposedtool-chain is shown to be scalable while maintaining high accuracy.
arxiv-1605-06377 | Towards Automation of Knowledge Understanding: An Approach for Probabilistic Generative Classifiers |  http://arxiv.org/abs/1605.06377  | author:Dominik Fisch, Christian Gruhl, Edgar Kalkowski, Bernhard Sick, Seppo J. Ovaska category:cs.LG cs.AI published:2016-05-20 summary:After data selection, pre-processing, transformation, and feature extraction,knowledge extraction is not the final step in a data mining process. It is thennecessary to understand this knowledge in order to apply it efficiently andeffectively. Up to now, there is a lack of appropriate techniques that supportthis significant step. This is partly due to the fact that the assessment ofknowledge is often highly subjective, e.g., regarding aspects such as noveltyor usefulness. These aspects depend on the specific knowledge and requirementsof the data miner. There are, however, a number of aspects that are objectiveand for which it is possible to provide appropriate measures. In this articlewe focus on classification problems and use probabilistic generativeclassifiers based on mixture density models that are quite common in datamining applications. We define objective measures to assess theinformativeness, uniqueness, importance, discrimination, representativity,uncertainty, and distinguishability of rules contained in these classifiersnumerically. These measures not only support a data miner in evaluating resultsof a data mining process based on such classifiers. As we will see inillustrative case studies, they may also be used to improve the data miningprocess itself or to support the later application of the extracted knowledge.
arxiv-1605-06391 | Deep Multi-task Representation Learning: A Tensor Factorisation Approach |  http://arxiv.org/abs/1605.06391  | author:Yongxin Yang, Timothy Hospedales category:cs.LG published:2016-05-20 summary:Most contemporary multi-task learning methods assume linear models. Thissetting is considered shallow in the era of deep learning. In this paper, wepresent a new deep multi-task representation learning framework that learnscross-task sharing structure at every layer in a deep network. Our approach isbased on generalising the matrix factorisation techniques explicitly orimplicitly used by many conventional MTL algorithms to tensor factorisation, torealise automatic learning of end-to-end knowledge sharing in deep networks.This is in contrast to existing deep learning approaches that need auser-defined multi-task sharing strategy. Our approach applies to bothhomogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of ourdeep multi-task representation learning in terms of both higher accuracy andfewer design choices.
arxiv-1605-06394 | Bayesian Hyperparameter Optimization for Ensemble Learning |  http://arxiv.org/abs/1605.06394  | author:Julien-Charles Lévesque, Christian Gagné, Robert Sabourin category:cs.LG published:2016-05-20 summary:In this paper, we bridge the gap between hyperparameter optimization andensemble learning by performing Bayesian optimization of an ensemble withregards to its hyperparameters. Our method consists in building a fixed-sizeensemble, optimizing the configuration of one classifier of the ensemble ateach iteration of the hyperparameter optimization algorithm, taking intoconsideration the interaction with the other models when evaluating potentialperformances. We also consider the case where the ensemble is to bereconstructed at the end of the hyperparameter optimization phase, through agreedy selection over the pool of models generated during the optimization. Westudy the performance of our proposed method on three different hyperparameterspaces, showing that our approach is better than both the best single model anda greedy ensemble construction over the models produced by a standard Bayesianoptimization.
arxiv-1605-06398 | Stochastic Variance Reduction Methods for Saddle-Point Problems |  http://arxiv.org/abs/1605.06398  | author:P Balamurugan, Francis Bach category:cs.LG math.OC published:2016-05-20 summary:We consider convex-concave saddle-point problems where the objectivefunctions may be split in many components, and extend recent stochasticvariance reduction methods (such as SVRG or SAGA) to provide the firstlarge-scale linearly convergent algorithms for this class of problems which iscommon in machine learning. While the algorithmic extension is straightforward,it comes with challenges and opportunities: (a) the convex minimizationanalysis does not apply and we use the notion of monotone operators to proveconvergence, showing in particular that the same algorithm applies to a largerclass of problems, such as variational inequalities, (b) there are two notionsof splits, in terms of functions, or in terms of partial derivatives, (c) thesplit does need to be done with convex-concave terms, (d) non-uniform samplingis key to an efficient algorithm, both in theory and practice, and (e) theseincremental algorithms can be easily accelerated using a simple extension ofthe "catalyst" framework, leading to an algorithm which is always superior toaccelerated batch algorithms.
arxiv-1605-06402 | Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks |  http://arxiv.org/abs/1605.06402  | author:Philipp Gysel category:cs.CV cs.LG cs.NE published:2016-05-20 summary:Convolutional neural networks (CNN) have achieved major breakthroughs inrecent years. Their performance in computer vision have matched and in someareas even surpassed human capabilities. Deep neural networks can capturecomplex non-linear features; however this ability comes at the cost of highcomputational and memory requirements. State-of-art networks require billionsof arithmetic operations and millions of parameters. To enable embedded devicessuch as smartphones, Google glasses and monitoring cameras with the astonishingpower of deep learning, dedicated hardware accelerators can be used to decreaseboth execution time and power consumption. In applications where fastconnection to the cloud is not guaranteed or where privacy is important,computation needs to be done locally. Many hardware accelerators for deepneural networks have been proposed recently. A first important step ofaccelerator design is hardware-oriented approximation of deep networks, whichenables energy-efficient inference. We present Ristretto, a fast and automatedframework for CNN approximation. Ristretto simulates the hardware arithmetic ofa custom hardware accelerator. The framework reduces the bit-width of networkparameters and outputs of resource-intense layers, which reduces the chip areafor multiplication units significantly. Alternatively, Ristretto can remove theneed for multipliers altogether, resulting in an adder-only arithmetic. Thetool fine-tunes trimmed networks to achieve high classification accuracy. Sincetraining of deep neural networks can be time-consuming, Ristretto uses highlyoptimized routines which run on the GPU. This enables fast compression of anygiven network. Given a maximum tolerance of 1%, Ristretto can successfullycondense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available.
arxiv-1605-06409 | R-FCN: Object Detection via Region-based Fully Convolutional Networks |  http://arxiv.org/abs/1605.06409  | author:Jifeng Dai, Yi Li, Kaiming He, Jian Sun category:cs.CV published:2016-05-20 summary:We present region-based, fully convolutional networks for accurate andefficient object detection. In contrast to previous region-based detectors suchas Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds oftimes, our region-based detector is fully convolutional with almost allcomputation shared on the entire image. To achieve this goal, we proposeposition-sensitive score maps to address a dilemma betweentranslation-invariance in image classification and translation-variance inobject detection. Our method can thus naturally adopt fully convolutional imageclassifier backbones, such as the latest Residual Networks (ResNets), forobject detection. We show competitive results on the PASCAL VOC datasets (e.g.,83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result isachieved at a test-time speed of 170ms per image, 2.5-20x faster than theFaster R-CNN counterpart. Code will be made publicly available.
arxiv-1605-06416 | Statistical Inference for Cluster Trees |  http://arxiv.org/abs/1605.06416  | author:Yen-Chi Chen, Jisu Kim, Sivaraman Balakrishnan, Alessandro Rinaldo, Larry Wasserman category:math.ST stat.ME stat.ML stat.TH published:2016-05-20 summary:A cluster tree provides a highly-interpretable summary of a density functionby representing the hierarchy of its high-density clusters. It is estimatedusing the empirical tree, which is the cluster tree constructed from a densityestimator. This paper addresses the basic question of quantifying ouruncertainty by assessing the statistical significance of features of anempirical cluster tree. We first study a variety of metrics that can be used tocompare different trees, analyze their properties and assess their suitabilityfor inference. We then propose methods to construct and summarize confidencesets for the unknown true cluster tree. We introduce a partial ordering oncluster trees which we use to prune some of the statistically insignificantfeatures of the empirical tree, yielding interpretable and parsimonious clustertrees. Finally, we illustrate the proposed methods on a variety of syntheticexamples and furthermore demonstrate their utility in the analysis of aGraft-versus-Host Disease (GvHD) data set.
arxiv-1605-06417 | Shape Recognition by Bag of Skeleton-associated Contour Parts |  http://arxiv.org/abs/1605.06417  | author:Wei Shen, Yuan Jiang, Wenjing Gao, Dan Zeng, Xinggang Wang category:cs.CV published:2016-05-20 summary:Contour and skeleton are two complementary representations for shaperecognition. However combining them in a principal way is nontrivial, as theyare generally abstracted by different structures (closed string vs graph),respectively. This paper aims at addressing the shape recognition problem bycombining contour and skeleton according to the correspondence between them.The correspondence provides a straightforward way to associate skeletalinformation with a shape contour. More specifically, we propose a new shapedescriptor. named Skeleton-associated Shape Context (SSC), which captures thefeatures of a contour fragment associated with skeletal information. Benefitedfrom the association, the proposed shape descriptor provides the complementarygeometric information from both contour and skeleton parts, including thespatial distribution and the thickness change along the shape part. To form ameaningful shape feature vector for an overall shape, the Bag of Featuresframework is applied to the SSC descriptors extracted from it. Finally, theshape feature vector is fed into a linear SVM classifier to recognize theshape. The encouraging experimental results demonstrate that the proposed wayto combine contour and skeleton is effective for shape recognition, whichachieves the state-of-the-art performances on several standard shapebenchmarks.
arxiv-1605-06420 | Quantifying the accuracy of approximate diffusions and Markov chains |  http://arxiv.org/abs/1605.06420  | author:Jonathan H. Huggins, James Zou category:math.ST math.PR stat.CO stat.ML stat.TH published:2016-05-20 summary:Diffusions and their discretizations as Markov chains are a workhorse forinference, sampling and modeling. With the growth of large-scale datasets, thecomputational cost associated with simulating these stochastic processes can beconsiderable, and many algorithms have been proposed to approximate theunderlying Markov chain or diffusion. A fundamental question is how thecomputational savings trade off against the statistical error incurred due toapproximations. This paper develops general results to investigate thisquestion. We bound the Wasserstein distance between the equilibriumdistributions of two diffusions as a function of their mixing rates and thedeviation in their drifts. We show that this error bound is exact in simpleGaussian settings. This general result on continuous diffusions can bediscretized to provide insights on the computational--statistical trade-off ofMarkov chains. As an illustration, we apply our framework to derivefinite-sample error bounds of approximate unadjusted Langevin dynamics. Wecharacterize computation-constrained settings where, by using fast-to-computeapproximate gradients in the Langevin dynamics, we obtain more accurate samplescompared to using the exact gradients. Our theoretical analyses are supportedby simulation experiments.
arxiv-1605-06047 | AMSOM: Adaptive Moving Self-organizing Map for Clustering and Visualization |  http://arxiv.org/abs/1605.06047  | author:Gerasimos Spanakis, Gerhard Weiss category:cs.AI cs.NE published:2016-05-19 summary:Self-Organizing Map (SOM) is a neural network model which is used to obtain atopology-preserving mapping from the (usually high dimensional) input/featurespace to an output/map space of fewer dimensions (usually two or three in orderto facilitate visualization). Neurons in the output space are connected witheach other but this structure remains fixed throughout training and learning isachieved through the updating of neuron reference vectors in feature space.Despite the fact that growing variants of SOM overcome the fixed structurelimitation they increase computational cost and also do not allow the removalof a neuron after its introduction. In this paper, a variant of SOM is proposedcalled AMSOM (Adaptive Moving Self-Organizing Map) that on the one hand createsa more flexible structure where neuron positions are dynamically altered duringtraining and on the other hand tackles the drawback of having a predefined gridby allowing neuron addition and/or removal during training. Experiments usingmultiple literature datasets show that the proposed method improves trainingperformance of SOM, leads to a better visualization of the input dataset andprovides a framework for determining the optimal number and structure ofneurons.
arxiv-1605-05904 | Re-ranking Object Proposals for Object Detection in Automatic Driving |  http://arxiv.org/abs/1605.05904  | author:Zhun Zhong, Mingyi Lei, Shaozi Li, Jianping Fan category:cs.CV published:2016-05-19 summary:Object detection often suffers from a plenty of bootless proposals, selectinghigh quality proposals remains a great challenge. In this paper, we propose asemantic, class-specific approach to re-rank object proposals, which canconsistently improve the recall performance even with less proposals. We firstextract features for each proposal including semantic segmentation, stereoinformation, contextual information, CNN-based objectness and low-level cue,and then score them using class-specific weights learnt by Structured SVM. Theadvantages of the proposed model are twofold: 1) it can be easily merged toexisting generators with few computational costs, and 2) it can achieve highrecall rate uner strict critical even using less proposals. Experimentalevaluation on the KITTI benchmark demonstrates that our approach significantlyimproves existing popular generators on recall performance. Moreover, in theexperiment conducted for object detection, even with 1,500 proposals, ourapproach can still have higher average precision (AP) than baselines with 5,000proposals.
arxiv-1605-06049 | A Multi-Batch L-BFGS Method for Machine Learning |  http://arxiv.org/abs/1605.06049  | author:Albert S. Berahas, Jorge Nocedal, Martin Takáč category:math.OC cs.LG stat.ML published:2016-05-19 summary:The question of how to parallelize the stochastic gradient descent (SGD)method has received much attention in the literature. In this paper, we focusinstead on batch methods that use a sizeable fraction of the training set ateach iteration to facilitate parallelism, and that employ second-orderinformation. In order to improve the learning process, we follow a multi-batchapproach in which the batch changes at each iteration. This inherently givesthe algorithm a stochastic flavor that can cause instability in L-BFGS, apopular batch method in machine learning. These difficulties arise becauseL-BFGS employs gradient differences to update the Hessian approximations; whenthese gradients are computed using different data points the process can beunstable. This paper shows how to perform stable quasi-Newton updating in themulti-batch setting, illustrates the behavior of the algorithm in a distributedcomputing platform, and studies its convergence properties for both the convexand nonconvex cases.
arxiv-1605-06170 | Evaluation System for a Bayesian Optimization Service |  http://arxiv.org/abs/1605.06170  | author:Ian Dewancker, Michael McCourt, Scott Clark, Patrick Hayes, Alexandra Johnson, George Ke category:cs.LG published:2016-05-19 summary:Bayesian optimization is an elegant solution to the hyperparameteroptimization problem in machine learning. Building a reliable and robustBayesian optimization service requires careful testing methodology and soundstatistical analysis. In this talk we will outline our development of anevaluation framework to rigorously test and measure the impact of changes tothe SigOpt optimization service. We present an overview of our evaluationsystem and discuss how this framework empowers our research engineers toconfidently and quickly make changes to our core optimization engine
arxiv-1605-06069 | A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues |  http://arxiv.org/abs/1605.06069  | author:Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, Yoshua Bengio category:cs.CL cs.AI cs.LG cs.NE I.5.1; I.2.7 published:2016-05-19 summary:Sequential data often possesses a hierarchical structure with complexdependencies between subsequences, such as found between the utterances in adialogue. In an effort to model this kind of generative process, we propose aneural network-based generative architecture, with latent stochastic variablesthat span a variable number of time steps. We apply the proposed model to thetask of dialogue response generation and compare it with recent neural networkarchitectures. We evaluate the model performance through automatic evaluationmetrics and by carrying out a human evaluation. The experiments demonstratethat our model improves upon recently proposed models and that the latentvariables facilitate the generation of long outputs and maintain the context.
arxiv-1605-06052 | Hierarchical Clustering in Face Similarity Score Space |  http://arxiv.org/abs/1605.06052  | author:Jason Grant, Patrick Flynn category:cs.CV published:2016-05-19 summary:Similarity scores in face recognition represent the proximity between pairsof images as computed by a matching algorithm. Given a large set of images andthe proximities between all pairs, a similarity score space is defined. Clusteranalysis was applied to the similarity score space to develop varioustaxonomies. Given the number of subjects in the dataset, we used hierarchicalmethods to aggregate images of the same subject. We also explored the hierarchyabove and below the subject level, including clusters that reflect gender andethnicity. Evidence supports the existence of clustering by race, gender,subject, and illumination condition.
arxiv-1605-05977 | A Geometric Approach to Color Image Regularization |  http://arxiv.org/abs/1605.05977  | author:Freddie Åström, Christoph Schnörr category:cs.CV published:2016-05-19 summary:We present a new vectorial total variation method that addresses the problemof color consistent image filtering. Our approach is inspired from thedouble-opponent cell representation in the human visual cortex. Existingmethods of vectorial total variation regularizers have insufficient (or no)coupling between the color channels and thus may introduce color artifacts. Weaddress this problem by introducing a novel coupling between the color channelsrelated to a pullback-metric from the opponent space to the data (RGB color)space. Our energy is a non-convex, non-smooth higher-order vectorial totalvariation approach and promotes color consistent image filtering via a couplingterm. For a convex variant, we show well-posedness and existence of a solutionin the space of vectorial bounded variation. For the higher-order scheme weemploy a half-quadratic strategy, which model the non-convex energy terms asthe infimum of a sequence of quadratic functions. In experiments, we elaborateon traditional image restoration applications of inpainting, deblurring anddenoising. Regarding the latter, we demonstrate state of the art restorationquality with respect to structure coherence and color consistency.
arxiv-1605-05785 | Efficient Nonparametric Smoothness Estimation |  http://arxiv.org/abs/1605.05785  | author:Shashank Singh, Simon S. Du, Barnabás Póczos category:math.ST cs.IT math.IT stat.ML stat.TH published:2016-05-19 summary:Sobolev quantities (norms, inner products, and distances) of probabilitydensity functions are important in the theory of nonparametric statistics, buthave rarely been used in practice, partly due to a lack of practicalestimators. They also include, as special cases, $L^2$ quantities which areused in many applications. We propose and analyze a family of estimators forSobolev quantities of unknown probability density functions. We bound the biasand variance of our estimators over finite samples, finding that they aregenerally minimax rate-optimal. Our estimators are significantly morecomputationally tractable than previous estimators, and exhibit astatistical/computational trade-off allowing them to adapt to computationalconstraints. We also draw theoretical connections to recent work on fasttwo-sample testing. Finally, we empirically validate our estimators onsynthetic data.
arxiv-1605-05791 | A Generic Framework for Assessing the Performance Bounds of Image Feature Detectors |  http://arxiv.org/abs/1605.05791  | author:Shoaib Ehsan, Adrian F. Clark, Ales Leonardis, Naveed ur Rehman, Klaus D. McDonald-Maier category:cs.CV published:2016-05-19 summary:Since local feature detection has been one of the most active research areasin computer vision during the last decade, a large number of detectors havebeen proposed. The interest in feature-based applications continues to grow andhas thus rendered the task of characterizing the performance of various featuredetection methods an important issue in vision research. Inspired by the goodpractices of electronic system design, a generic framework based on therepeatability measure is presented in this paper that allows assessment of theupper and lower bounds of detector performance and finds statisticallysignificant performance differences between detectors as a function of imagetransformation amount by introducing a new variant of McNemars test in aneffort to design more reliable and effective vision systems. The proposedframework is then employed to establish operating and guarantee regions forseveral state-of-the-art detectors and to identify their statisticalperformance differences for three specific image transformations: JPEGcompression, uniform light changes and blurring. The results are obtained usinga newly acquired, large image database (20482) images with 539 differentscenes. These results provide new insights into the behaviour of detectors andare also useful from the vision systems design perspective.
arxiv-1605-05799 | Recurrent Exponential-Family Harmoniums without Backprop-Through-Time |  http://arxiv.org/abs/1605.05799  | author:Joseph G. Makin, Benjamin K. Dichter, Philip N. Sabes category:cs.LG stat.ML published:2016-05-19 summary:Exponential-family harmoniums (EFHs), which extend restricted Boltzmannmachines (RBMs) from Bernoulli random variables to other exponential families(Welling et al., 2005), are generative models that can be trained withunsupervised-learning techniques, like contrastive divergence (Hinton et al.2006; Hinton, 2002), as density estimators for static data. Methods forextending RBMs--and likewise EFHs--to data with temporal dependencies have beenproposed previously (Sutskever and Hinton, 2007; Sutskever et al., 2009), thelearning procedure being validated by qualitative assessment of the generativemodel. Here we propose and justify, from a very different perspective, analternative training procedure, proving sufficient conditions for optimalinference under that procedure. The resulting algorithm can be learned withonly forward passes through the data--backprop-through-time is not required, asin previous approaches. The proof exploits a recent result about informationretention in density estimators (Makin and Sabes, 2015), and applies it to a"recurrent EFH" (rEFH) by induction. Finally, we demonstrate optimality bysimulation, testing the rEFH: (1) as a filter on training data generated with alinear dynamical system, the position of which is noisily reported by apopulation of "neurons" with Poisson-distributed spike counts; and (2) with thequalitative experiments proposed by Sutskever et al. (2009).
arxiv-1605-05969 | Randomized Primal-Dual Proximal Block Coordinate Updates |  http://arxiv.org/abs/1605.05969  | author:Xiang Gao, Yangyang Xu, Shuzhong Zhang category:math.OC math.NA stat.ML published:2016-05-19 summary:In this paper we propose a randomized primal-dual proximal block coordinateupdating framework for a general multi-block convex optimization model withcoupled objective function and linear constraints. Assuming mere convexity, weestablish its $O(1/t)$ convergence rate in terms of the objective value andfeasibility measure. The framework includes several existing algorithms asspecial cases such as a primal-dual method for bilinear saddle-point problems(PD-S), the proximal Jacobian ADMM (Prox-JADMM) and a randomized variant of theADMM method for multi-block convex optimization. Our analysis recovers and/orstrengthens the convergence properties of several existing algorithms. Forexample, for PD-S our result leads to the same order of convergence ratewithout the previously assumed boundedness condition on the constraint sets,and for Prox-JADMM the new result provides convergence rate in terms of theobjective value and the feasibility violation. It is well known that theoriginal ADMM may fail to converge when the number of blocks exceeds two. Ourresult shows that if an appropriate randomization procedure is invoked toselect the updating blocks, then a sublinear rate of convergence in expectationcan be guaranteed for multi-block ADMM, without assuming any strong convexity.The new approach is also extended to solve problems where only a stochasticapproximation of the (sub-)gradient of the objective is available, and weestablish an $O(1/\sqrt{t})$ convergence rate of the extended approach forsolving stochastic programming.
arxiv-1605-06065 | One-shot Learning with Memory-Augmented Neural Networks |  http://arxiv.org/abs/1605.06065  | author:Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy Lillicrap category:cs.LG published:2016-05-19 summary:Despite recent breakthroughs in the applications of deep neural networks, onesetting that presents a persistent challenge is that of "one-shot learning."Traditional gradient-based networks require a lot of data to learn, oftenthrough extensive iterative training. When new data is encountered, the modelsmust inefficiently relearn their parameters to adequately incorporate the newinformation without catastrophic interference. Architectures with augmentedmemory capacities, such as Neural Turing Machines (NTMs), offer the ability toquickly encode and retrieve new information, and hence can potentially obviatethe downsides of conventional models. Here, we demonstrate the ability of amemory-augmented neural network to rapidly assimilate new data, and leveragethis data to make accurate predictions after only a few samples. We alsointroduce a new method for accessing an external memory that focuses on memorycontent, unlike previous methods that additionally use memory location-basedfocusing mechanisms.
arxiv-1605-05815 | Bacterial foraging optimization based brain magnetic resonance image segmentation |  http://arxiv.org/abs/1605.05815  | author:Abdul kayom Md Khairuzzaman category:cs.CV cs.NE published:2016-05-19 summary:Segmentation partitions an image into its constituent parts. It isessentially the pre-processing stage of image analysis and computer vision. Inthis work, T1 and T2 weighted brain magnetic resonance images are segmentedusing multilevel thresholding and bacterial foraging optimization (BFO)algorithm. The thresholds are obtained by maximizing the between class variance(multilevel Otsu method) of the image. The BFO algorithm is used to optimizethe threshold searching process. The edges are then obtained from thethresholded image by comparing the intensity of each pixel with its eightconnected neighbourhood. Post processing is performed to remove spuriousresponses in the segmented image. The proposed segmentation technique isevaluated using edge detector evaluation parameters such as figure of merit,Rand Index and variation of information. The proposed brain MR imagesegmentation technique outperforms the traditional edge detectors such as cannyand sobel.
arxiv-1605-06155 | Inter-Battery Topic Representation Learning |  http://arxiv.org/abs/1605.06155  | author:Cheng Zhang, Hedvig Kjellstrom, Carl Henrik Ek category:cs.LG cs.CV published:2016-05-19 summary:In this paper, we present the Inter-Battery Topic Model (IBTM). Our approachextends traditional topic models by learning a factorized latent variablerepresentation. The structured representation leads to a model that marriesbenefits traditionally associated with a discriminative approach, such asfeature selection, with those of a generative model, such as principledregularization and ability to handle missing data. The factorization isprovided by representing data in terms of aligned pairs of observations asdifferent views. This provides means for selecting a representation thatseparately models topics that exist in both views from the topics that areunique to a single view. This structured consolidation allows for efficient androbust inference and provides a compact and efficient representation. Learningis performed in a Bayesian fashion by maximizing a rigorous bound on thelog-likelihood. Firstly, we illustrate the benefits of the model on a syntheticdataset,. The model is then evaluated in both uni- and multi-modality settingson two different classification tasks with off-the-shelf convolutional neuralnetwork (CNN) features which generate state-of-the-art results with extremelycompact representations.
arxiv-1605-05826 | Declarative Machine Learning - A Classification of Basic Properties and Types |  http://arxiv.org/abs/1605.05826  | author:Matthias Boehm, Alexandre V. Evfimievski, Niketan Pansare, Berthold Reinwald category:cs.DB cs.DC cs.LG cs.PL published:2016-05-19 summary:Declarative machine learning (ML) aims at the high-level specification of MLtasks or algorithms, and automatic generation of optimized execution plans fromthese specifications. The fundamental goal is to simplify the usage and/ordevelopment of ML algorithms, which is especially important in the context oflarge-scale computations. However, ML systems at different abstraction levelshave emerged over time and accordingly there has been a controversy about themeaning of this general definition of declarative ML. Specificationalternatives range from ML algorithms expressed in domain-specific languages(DSLs) with optimization for performance, to ML task (learning problem)specifications with optimization for performance and accuracy. We argue thatthese different types of declarative ML complement each other as they addressdifferent users (data scientists and end users). This paper makes an attempt tocreate a taxonomy for declarative ML, including a definition of essential basicproperties and types of declarative ML. Along the way, we provide insights intoimplications of these properties. We also use this taxonomy to classifyexisting systems. Finally, we draw conclusions on defining appropriatebenchmarks and specification languages for declarative ML.
arxiv-1605-05829 | On the Sampling Strategy for Evaluation of Spectral-spatial Methods in Hyperspectral Image Classification |  http://arxiv.org/abs/1605.05829  | author:Jie Liang, Jun Zhou, Yuntao Qian, Lian Wen, Xiao Bai, Yongsheng Gao category:cs.CV published:2016-05-19 summary:Spectral-spatial processing has been increasingly explored in remote sensinghyperspectral image classification. While extensive studies have focused ondeveloping methods to improve the classification accuracy, experimental settingand design for method evaluation have drawn little attention. In the scope ofsupervised classification, we find that traditional experimental designs forspectral processing are often improperly used in the spectral-spatialprocessing context, leading to unfair or biased performance evaluation. This isespecially the case when training and testing samples are randomly drawn fromthe same image - a practice that has been commonly adopted in the experiments.Under such setting, the dependence caused by overlap between the training andtesting samples may be artificially enhanced by some spatial informationprocessing methods such as spatial filtering and morphological operation. Suchinteraction between training and testing sets has violated data independenceassumption that is abided by supervised learning theory and performanceevaluation mechanism. Therefore, the widely adopted pixel-based random samplingstrategy is not always suitable to evaluate spectral-spatial classificationalgorithms because it is difficult to determine whether the improvement ofclassification accuracy is caused by incorporating spatial information intoclassifier or by increasing the overlap between training and testing samples.To partially solve this problem, we propose a novel controlled random samplingstrategy for spectral-spatial methods. It can greatly reduce the overlapbetween training and testing samples and provides more objective and accurateevaluation.
arxiv-1605-05967 | Contour-based 3d tongue motion visualization using ultrasound image sequences |  http://arxiv.org/abs/1605.05967  | author:Kele Xu, Yin Yang, Clémence Leboullenger, Pierre Roussel, Bruce Denby category:cs.CV published:2016-05-19 summary:This article describes a contour-based 3D tongue deformation visualizationframework using B-mode ultrasound image sequences. A robust, automatic trackingalgorithm characterizes tongue motion via a contour, which is then used todrive a generic 3D Finite Element Model (FEM). A novel contour-based 3D dynamicmodeling method is presented. Modal reduction and modal warping techniques areapplied to model the deformation of the tongue physically and efficiently. Thiswork can be helpful in a variety of fields, such as speech production, silentspeech recognition, articulation training, speech disorder study, etc.
arxiv-1605-06106 | Development of a 3D tongue motion visualization platform based on ultrasound image sequences |  http://arxiv.org/abs/1605.06106  | author:Kele Xu, Yin Yang, Aurore Jaumard-Hakoun, Clemence Leboullenger, Gerard Dreyfus, Pierre Roussel, Maureen Stone, Bruce Denby category:cs.CV published:2016-05-19 summary:This article describes the development of a platform designed to visualizethe 3D motion of the tongue using ultrasound image sequences. An overview ofthe system design is given and promising results are presented. Compared to theanalysis of motion in 2D image sequences, such a system can provide additionalvisual information and a quantitative description of the tongue 3D motion. Theplatform can be useful in a variety of fields, such as speech production,articulation training, etc.
arxiv-1605-05860 | False Discovery Rate Control and Statistical Quality Assessment of Annotators in Crowdsourced Ranking |  http://arxiv.org/abs/1605.05860  | author:Qianqian Xu, Jiechao Xiong, Xiaochun Cao, Yuan Yao category:stat.ML published:2016-05-19 summary:With the rapid growth of crowdsourcing platforms it has become easy andrelatively inexpensive to collect a dataset labeled by multiple annotators in ashort time. However due to the lack of control over the quality of theannotators, some abnormal annotators may be affected by position bias which canpotentially degrade the quality of the final consensus labels. In this paper weintroduce a statistical framework to model and detect annotator's position biasin order to control the false discovery rate (FDR) without a prior knowledge onthe amount of biased annotators - the expected fraction of false discoveriesamong all discoveries being not too high, in order to assure that most of thediscoveries are indeed true and replicable. The key technical developmentrelies on some new knockoff filters adapted to our problem and new algorithmsbased on the Inverse Scale Space dynamics whose discretization is potentiallysuitable for large scale crowdsourcing data analysis. Our studies are supportedby experiments with both simulated examples and real-world data. The proposedframework provides us a useful tool for quantitatively studying annotator'sabnormal behavior in crowdsourcing data arising from machine learning,sociology, computer vision, multimedia, etc.
arxiv-1605-05863 | Siamese Instance Search for Tracking |  http://arxiv.org/abs/1605.05863  | author:Ran Tao, Efstratios Gavves, Arnold W. M. Smeulders category:cs.CV published:2016-05-19 summary:In this paper we present a tracker, which is radically different fromstate-of-the-art trackers: we apply no model updating, no occlusion detection,no combination of trackers, no geometric matching, and still deliverstate-of-the-art tracking performance, as demonstrated on the popular onlinetracking benchmark (OTB) and six very challenging YouTube videos. The presentedtracker simply matches the initial patch of the target in the first frame withcandidates in a new frame and returns the most similar patch by a learnedmatching function. The strength of the matching function comes from beingextensively trained generically, i.e., without any data of the target, using aSiamese deep neural network, which we design for tracking. Once learned, thematching function is used as is, without any adapting, to track previouslyunseen targets. It turns out that the learned matching function is so powerfulthat a simple tracker built upon it, coined Siamese INstance search Tracker,SINT, which only uses the original observation of the target from the firstframe, suffices to reach state-of-the-art performance. Further, we show theproposed tracker even allows for target re-identification after the target wasabsent for a complete video shot.
arxiv-1605-06076 | On a convergent off -policy temporal difference learning algorithm in on-line learning environment |  http://arxiv.org/abs/1605.06076  | author:Prasenjit Karmakar, Rajkumar Maity, Shalabh Bhatnagar category:cs.LG published:2016-05-19 summary:In this paper we provide a rigorous convergence analysis of a "off"-policytemporal difference learning algorithm with linear function approximation andper time-step linear computational complexity in "online" learning environment.The algorithm considered here is TDC with importance weighting introduced byMaei et al. We support our theoretical results by providing suitable empiricalresults for standard off-policy counterexamples.
arxiv-1605-05894 | Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages |  http://arxiv.org/abs/1605.05894  | author:Muhammad Imran, Prasenjit Mitra, Carlos Castillo category:cs.CL cs.CY cs.SI published:2016-05-19 summary:Microblogging platforms such as Twitter provide active communication channelsduring mass convergence and emergency events such as earthquakes, typhoons.During the sudden onset of a crisis situation, affected people post usefulinformation on Twitter that can be used for situational awareness and otherhumanitarian disaster response efforts, if processed timely and effectively.Processing social media information pose multiple challenges such as parsingnoisy, brief and informal messages, learning information categories from theincoming stream of messages and classifying them into different classes amongothers. One of the basic necessities of many of these tasks is the availabilityof data, in particular human-annotated data. In this paper, we presenthuman-annotated Twitter corpora collected during 19 different crises that tookplace between 2013 and 2015. To demonstrate the utility of the annotations, wetrain machine learning classifiers. Moreover, we publish first largest word2vecword embeddings trained on 52 million crisis-related tweets. To deal withtweets language issues, we present human-annotated normalized lexical resourcesfor different lexical variations.
arxiv-1605-06083 | Stereotyping and Bias in the Flickr30K Dataset |  http://arxiv.org/abs/1605.06083  | author:Emiel van Miltenburg category:cs.CL cs.CV published:2016-05-19 summary:An untested assumption behind the crowdsourced descriptions of the images inthe Flickr30K dataset (Young et al., 2014) is that they "focus only on theinformation that can be obtained from the image alone" (Hodosh et al., 2013, p.859). This paper presents some evidence against this assumption, and provides alist of biases and unwarranted inferences that can be found in the Flickr30Kdataset. Finally, it considers methods to find examples of these, and discusseshow we should deal with stereotype-driven descriptions in future applications.
arxiv-1605-06094 | Automatic Selection of the Optimal Local Feature Detector |  http://arxiv.org/abs/1605.06094  | author:Bruno Ferrarini, Shoaib Ehsan, Naveed Ur Rehman, Ales Leonardis, Klaus D. McDonald-Maier category:cs.CV published:2016-05-19 summary:A large number of different feature detectors has been proposed so far. Anyexisting approach presents strengths and weaknesses, which make a detectoroptimal only for a limited range of applications. A tool capable of selectingthe optimal feature detector in relation to the operating conditions ispresented in this paper. The input images are quickly analyzed to determinewhat type of image transformation is applied to them and at which amount.Finally, the detector that is expected to obtain the highest repeatabilityunder such conditions, is chosen to extract features from the input images. Theefficiency and the good accuracy in determining the optimal feature detectorfor any operating condition, make the proposed tool suitable to be utilized inreal visual applications. %A large number of different feature detectors hasbeen proposed so far. Any existing approach presents strengths and weaknesses,which make a detector optimal only for a limited range of applications. A largenumber of different local feature detectors have been proposed in the last fewyears. However, each feature detector has its own strengths ad weaknesses thatlimit its use to a specific range of applications. In this paper is presented atool capable of quickly analysing input images to determine which type andamount of transformation is applied to them and then selecting the optimalfeature detector, which is expected to perform the best. The results show thatthe performance and the fast execution time render the proposed tool suitablefor real-world vision applications.
arxiv-1605-05937 | Hierarchical Piecewise-Constant Super-regions |  http://arxiv.org/abs/1605.05937  | author:Imanol Luengo, Mark Basham, Andrew P. French category:cs.CV published:2016-05-19 summary:Recent applications in computer vision have come to heavily rely onsuperpixel over-segmentation as a pre-processing step for higher level visiontasks, such as object recognition, image labelling or image segmentation. Herewe present a new superpixel algorithm called Hierarchical Piecewise-ConstantSuper-regions (HPCS), which not only obtains superpixels comparable to thestate-of-the-art, but can also be applied hierarchically to form what we calln-th order super-regions. In essence, a Markov Random Field (MRF)-basedanisotropic denoising formulation over the quantized feature space is adoptedto form piecewise-constant image regions, which are then combined with agraph-based split & merge post-processing step to form superpixels. The graphand quantized feature based formulation of the problem allows us to generalizeit hierarchically to preserve boundary adherence with fewer superpixels.Experimental results show that, despite the simplicity of our framework, it isable to provide high quality superpixels, and to hierarchically apply them toform layers of over-segmentation, each with a decreasing number of superpixels,while maintaining the same desired properties (such as adherence to strongimage edges). The algorithm is also memory efficient and has a lowcomputational cost.
arxiv-1605-05906 | Automatic TM Cleaning through MT and POS Tagging: Autodesk's Submission to the NLP4TM 2016 Shared Task |  http://arxiv.org/abs/1605.05906  | author:Alena Zwahlen, Olivier Carnal, Samuel Läubli category:cs.CL published:2016-05-19 summary:We describe a machine learning based method to identify incorrect entries intranslation memories. It extends previous work by Barbu (2015) throughincorporating recall-based machine translation and part-of-speech-taggingfeatures. Our system ranked first in the Binary Classification (II) task fortwo out of three language pairs: English-Italian and English-Spanish.
arxiv-1605-05923 | Matching Handwritten Document Images |  http://arxiv.org/abs/1605.05923  | author:Praveen Krishnan, C. V. Jawahar category:cs.CV published:2016-05-19 summary:We address the problem of predicting similarity between a pair of handwrittendocument images written by different individuals. This has applications relatedto matching and mining in image collections containing handwritten content. Asimilarity score is computed by detecting patterns of text re-usages betweendocument images irrespective of the minor variations in word morphology, wordordering, layout and paraphrasing of the content. Our method does not depend onan accurate segmentation of words and lines. We formulate the document matchingproblem as a structured comparison of the word distributions across twodocument images. To match two word images, we propose a convolutional neuralnetwork (CNN) based feature descriptor. Performance of this representationsurpasses the state-of-the-art on handwritten word spotting. Finally, wedemonstrate the applicability of our method on a practical problem of matchinghandwritten assignments.
arxiv-1605-05912 | Tongue contour extraction from ultrasound images based on deep neural network |  http://arxiv.org/abs/1605.05912  | author:Aurore Jaumard-Hakoun, Kele Xu, Pierre Roussel-Ragot, Gérard Dreyfus, Bruce Denby category:cs.CV published:2016-05-19 summary:Studying tongue motion during speech using ultrasound is a standardprocedure, but automatic ultrasound image labelling remains a challenge, asstandard tongue shape extraction methods typically require human intervention.This article presents a method based on deep neural networks to automaticallyextract tongue contour from ultrasound images on a speech dataset. We use adeep autoencoder trained to learn the relationship between an image and itsrelated contour, so that the model is able to automatically reconstructcontours from the ultrasound image alone. In this paper, we use an automaticlabelling algorithm instead of time-consuming hand-labelling during thetraining process, and estimate the performances of both automatic labelling andcontour extraction as compared to hand-labelling. Observed results show qualityscores comparable to the state of the art.
arxiv-1605-05918 | Bayesian Variable Selection for Globally Sparse Probabilistic PCA |  http://arxiv.org/abs/1605.05918  | author:Charles Bouveyron, Pierre Latouche, Pierre-Alexandre Mattei category:stat.ML published:2016-05-19 summary:With the flourishing development of high-dimensional data, sparse versions ofprincipal component analysis (PCA) have imposed themselves as simple, yetpowerful ways of selecting relevant features in an unsupervised manner.However, when several sparse principal components are computed, theinterpretation of the selected variables may be difficult since each axis hasits own sparsity pattern and has to be interpreted separately. To overcome thisdrawback, we propose a Bayesian procedure that allows to obtain several sparsecomponents with the same sparsity pattern. This allows the practitioner toidentify the original variables which are relevant to describe the data. Tothis end, using Roweis' probabilistic interpretation of PCA and an isotropicGaussian prior on the loading matrix, we provide the first exact computation ofthe marginal likelihood of a Bayesian PCA model. In order to avoid thedrawbacks of discrete model selection, we propose a simple relaxation of ourframework which allows to find a path of models using a variationalexpectation-maximization algorithm. The exact marginal likelihood caneventually be maximized over this path, relying on Occam's razor to select therelevant variables. Since the sparsity pattern is common to all components, wecall this approach globally sparse probabilistic PCA (GSPPCA). Its usefulnessis illustrated on synthetic data sets and on several real unsupervised featureselection problems.
arxiv-1605-05721 | Generalized Min-Max Kernel and Generalized Consistent Weighted Sampling |  http://arxiv.org/abs/1605.05721  | author:Ping Li category:cs.LG cs.IR stat.ML published:2016-05-18 summary:We propose the "generalized min-max" (GMM) kernel as a measure of datasimilarity, where data vectors can have both positive and negative entries. GMMis positive definite as there is an associate hashing method named "generalizedconsistent weighted sampling" (GCWS) which linearizes this (nonlinear) kernel.A natural competitor of the GMM kernel is the radial basis function (RBF)kernel, whose corresponding hashing method is known as the "random Fourierfeatures" (RFF). Our classification experiments on public datasets illustratethat both the GMM and RBF kernels can substantially improve linear classifiers.Furthermore, we show that GCWS typically requires substantially fewer samplesthan RFF. We expect that GMM and GCWS will be adopted in practice forlarge-scale machine learning applications and near neighbor search.
arxiv-1605-05622 | Gaussian variational approximation with sparse precision matrix |  http://arxiv.org/abs/1605.05622  | author:Linda S. L. Tan, David J. Nott category:stat.CO stat.ML published:2016-05-18 summary:We consider the problem of learning a Gaussian variational approximation tothe posterior distribution for a high-dimensional parameter, where we imposesparsity in the precision matrix to reflect appropriate conditionalindependence structure in the model. Incorporating sparsity in the precisionmatrix allows the Gaussian variational distribution to be both flexible andparsimonious, and the sparsity is achieved through parameterization in terms ofthe Cholesky factor. Efficient stochastic gradient methods which makeappropriate use of gradient information for the target distribution aredeveloped for the optimization. We consider alternative estimators of thestochastic gradients which have lower variation and are more stable. Ourapproach is illustrated using generalized linear mixed models and state spacemodels for time series.
arxiv-1605-05757 | Robust Image Descriptors for Real-Time Inter-Examination Retargeting in Gastrointestinal Endoscopy |  http://arxiv.org/abs/1605.05757  | author:Menglong Ye, Edward Johns, Benjamin Walter, Alexander Meining, Guang-Zhong Yang category:cs.CV published:2016-05-18 summary:For early diagnosis of malignancies in the gastrointestinal tract,surveillance endoscopy is increasingly used to monitor abnormal tissue changesin serial examinations of the same patient. Despite successes with opticalbiopsy for in vivo and in situ tissue characterisation, biopsy retargeting forserial examinations is challenging because tissue may change in appearancebetween examinations. In this paper, we propose an inter-examinationretargeting framework for optical biopsy, based on an image descriptor designedfor matching between endoscopic scenes over significant time intervals. Eachscene is described by a hierarchy of regional intensity comparisons at variousscales, offering tolerance to long-term change in tissue appearance whilstremaining discriminative. Binary coding is then used to compress the descriptorvia a novel random forests approach, providing fast comparisons in Hammingspace and real-time retargeting. Extensive validation conducted on 13 in vivogastrointestinal videos, collected from six patients, show that our approachoutperforms state-of-the-art methods.
arxiv-1605-05775 | Supervised Learning with Quantum-Inspired Tensor Networks |  http://arxiv.org/abs/1605.05775  | author:E. Miles Stoudenmire, David J. Schwab category:stat.ML cs.LG published:2016-05-18 summary:Tensor networks are efficient representations of high-dimensional tensorswhich have been very successful for physics and mathematics applications. Wedemonstrate how algorithms for optimizing such networks can be adapted tosupervised learning tasks by using matrix product states (tensor trains) toparameterize models for classifying images. For the MNIST data set we obtainless than 1% test set classification error. We discuss how the tensor networkform imparts additional structure to the learned model and suggest a possiblegenerative interpretation.
arxiv-1605-05543 | A deep learning approach to single-particle recognition in cryo-electron microscopy |  http://arxiv.org/abs/1605.05543  | author:Yanan Zhu, Qi Ouyang, Youdong Mao category:cs.CV published:2016-05-18 summary:Particle extraction represents a major practical bottleneck in the structuredetermination of biological macromolecular complexes by single-particlecryo-electron microscopy (cryo-EM). We developed a deep learning-basedalgorithmic framework, DeepEM, for single-particle recognition from noisycryo-EM micrographs, enabling automated particle picking, selection andverification in an integrated fashion. Our approach exhibits improvedperformance and high accuracy when tested on the standard KLH dataset as wellas several challenging experimental cryo-EM datasets.
arxiv-1605-05538 | Improving Weakly-Supervised Object Localization By Micro-Annotation |  http://arxiv.org/abs/1605.05538  | author:Alexander Kolesnikov, Christoph H. Lampert category:cs.CV published:2016-05-18 summary:Weakly-supervised object localization methods tend to fail for object classesthat consistently co-occur with the same background elements, e.g. trains ontracks. We propose a method to overcome these failures by adding a very smallamount of model-specific additional annotation. The main idea is to cluster adeep network's mid-level representations and assign object or distractor labelsto each cluster. Experiments show substantially improved localization resultson the challenging ILSVC2014 dataset for bounding box detection and the PASCALVOC2012 dataset for semantic segmentation.
arxiv-1605-05537 | ABC random forests for Bayesian parameter inference |  http://arxiv.org/abs/1605.05537  | author:Jean-Michel Marin, Louis Raynal, Pierre Pudlo, Mathieu Ribatet, Christian P. Robert category:stat.ME stat.CO stat.ML published:2016-05-18 summary:Approximate Bayesian Computation (ABC) has grown into a standard methodologyto handle Bayesian inference in models associated with intractable likelihoodfunctions. Most ABC implementations require the selection of a summarystatistic as the data itself is too large or too complex to be compared tosimulated realisations from the assumed model. The dimension of this statisticis generally constrained to be close to the dimension of the model parameterfor efficiency reasons. Furthermore, the tolerance level that governs theacceptance or rejection of parameter values needs to be calibrated and therange of calibration techniques available so far is mostly based on asymptoticarguments. We propose here to conduct Bayesian inference based on anarbitrarily large vector of summary statistics without imposing a selection ofthe relevant components and bypassing the derivation of a tolerance. Theapproach relies on the random forest methodology of Breiman (2001) when appliedto regression. We advocate the derivation of a new random forest for eachcomponent of the parameter vector, a tool from which an approximation to themarginal posterior distribution can be derived. Correlations between parametercomponents are handled by separate random forests. This technology offerssignificant gains in terms of robustness to the choice of the summarystatistics and of computing time, when compared with more standard ABCsolutions.
arxiv-1605-05509 | Learning activation functions from data using cubic spline interpolation |  http://arxiv.org/abs/1605.05509  | author:Simone Scardapane, Michele Scarpiniti, Danilo Comminiello, Aurelio Uncini category:stat.ML cs.LG cs.NE published:2016-05-18 summary:Neural networks require a careful design in order to perform properly on agiven task. In particular, selecting a good activation function (possibly in adata-dependent fashion) is a crucial step, which remains an open problem in theresearch community. Despite a large amount of investigations, most currentimplementations simply select one fixed function from a small set ofcandidates, which is not adapted during training, and is shared among allneurons throughout the different layers. However, neither two of theseassumptions can be supposed optimal in practice. In this paper, we present aprincipled way to have data-dependent adaptation of the activation functions,which is performed independently for each neuron. This is achieved byleveraging over past and present advances on cubic spline interpolation,allowing for local adaptation of the functions around their regions of use. Theresulting algorithm is relatively cheap to implement, and overfitting iscounterbalanced by the inclusion of a novel damping criterion, which penalizesunwanted oscillations from a predefined shape. Experimental results validatethe proposal over two well-known benchmarks.
arxiv-1605-05573 | Modelling Interaction of Sentence Pair with coupled-LSTMs |  http://arxiv.org/abs/1605.05573  | author:Pengfei Liu, Xipeng Qiu, Xuanjing Huang category:cs.CL published:2016-05-18 summary:Recently, there is rising interest in modelling the interactions of twosentences with deep neural networks. However, most of the existing methodsencode two sequences with separate encoders, in which a sentence is encodedwith little or no information from the other sentence. In this paper, wepropose a deep architecture to model the strong interaction of sentence pairwith two coupled-LSTMs. Specifically, we introduce two coupled ways to modelthe interdependences of two LSTMs, coupling the local contextualizedinteractions of two sentences. We then aggregate these interactions and use adynamic pooling to select the most informative features. Experiments on twovery large datasets demonstrate the efficacy of our proposed architecture andits superiority to state-of-the-art methods.
arxiv-1605-05466 | Image segmentation with superpixel-based covariance descriptors in low-rank representation |  http://arxiv.org/abs/1605.05466  | author:Xianbin Gu, Jeremiah D. Deng, Martin K. Purvis category:cs.CV published:2016-05-18 summary:This paper investigates the problem of image segmentation using superpixels.We propose two approaches to enhance the discriminative ability of thesuperpixel's covariance descriptors. In the first one, we employ theLog-Euclidean distance as the metric on the covariance manifolds, and then usethe RBF kernel to measure the similarities between covariance descriptors. Thesecond method is focused on extracting the subspace structure of the set ofcovariance descriptors by extending a low rank representation algorithm on tothe covariance manifolds. Experiments are carried out with the BerklySegmentation Dataset, and compared with the state-of-the-art segmentationalgorithms, both methods are competitive.
arxiv-1605-05462 | Dual Local-Global Contextual Pathways for Recognition in Aerial Imagery |  http://arxiv.org/abs/1605.05462  | author:Alina Marcu, Marius Leordeanu category:cs.CV published:2016-05-18 summary:Visual context is important in object recognition and it is still an openproblem in computer vision. Along with the advent of deep convolutional neuralnetworks (CNN), using contextual information with such systems starts toreceive attention in the literature. At the same time, aerial imagery isgaining momentum. While advances in deep learning make good progress in aerialimage analysis, this problem still poses many great challenges. Aerial imagesare often taken under poor lighting conditions and contain low resolutionobjects, many times occluded by trees or taller buildings. In this domain, inparticular, visual context could be of great help, but there are still very fewpapers that consider context in aerial image understanding. Here we introducecontext as a complementary way of recognizing objects. We propose a dual-streamdeep neural network model that processes information along two independentpathways, one for local and another for global visual reasoning. The two arelater combined in the final layers of processing. Our model learns to combinelocal object appearance as well as information from the larger scene at thesame time and in a complementary way, such that together they form a powerfulclassifier. We test our dual-stream network on the task of segmentation ofbuildings and roads in aerial images and obtain state-of-the-art results on theMassachusetts Buildings Dataset. We also introduce two new datasets, forbuildings and road segmentation, respectively, and study the relativeimportance of local appearance vs. the larger scene, as well as theirperformance in combination. While our local-global model could also be usefulin general recognition tasks, we clearly demonstrate the effectiveness ofvisual context in conjunction with deep nets for aerial image understanding.
arxiv-1605-05448 | The Bees Algorithm for the Vehicle Routing Problem |  http://arxiv.org/abs/1605.05448  | author:Aish Fenton category:cs.NE cs.AI published:2016-05-18 summary:In this thesis we present a new algorithm for the Vehicle Routing Problemcalled the Enhanced Bees Algorithm. It is adapted from a fairly recentalgorithm, the Bees Algorithm, which was developed for continuous optimisationproblems. We show that the results obtained by the Enhanced Bees Algorithm arecompetitive with the best meta-heuristics available for the Vehicle RoutingProblem (within 0.5% of the optimal solution for common benchmark problems). Weshow that the algorithm has good runtime performance, producing results within2% of the optimal solution within 60 seconds, making it suitable for use withinreal world dispatch scenarios.
arxiv-1605-05440 | Beyond Caption To Narrative: Video Captioning With Multiple Sentences |  http://arxiv.org/abs/1605.05440  | author:Andrew Shin, Katsunori Ohnishi, Tatsuya Harada category:cs.CV published:2016-05-18 summary:Recent advances in image captioning task have led to increasing interests invideo captioning task. However, most works on video captioning are focused ongenerating single input of aggregated features, which hardly deviates fromimage captioning process and does not fully take advantage of dynamic contentspresent in videos. We attempt to generate video captions that convey richercontents by temporally segmenting the video with action localization,generating multiple captions from multiple frames, and connecting them withnatural language processing techniques, in order to generate a story-likecaption. We show that our proposed method can generate captions that are richerin contents and can compete with state-of-the-art method without explicitlyusing video-level features as input.
arxiv-1605-05433 | Relations such as Hypernymy: Identifying and Exploiting Hearst Patterns in Distributional Vectors for Lexical Entailment |  http://arxiv.org/abs/1605.05433  | author:Stephen Roller, Katrin Erk category:cs.CL cs.AI published:2016-05-18 summary:We consider the task of predicting lexical entailment using distributionalvectors. We focus experiments on one previous classifier which was shown toonly learn to detect prototypicality of a word pair. Analysis shows that themodel single-mindedly learns to detect Hearst Patterns, which are well known tobe predictive of lexical relations. We present a new model which exploits thisHearst Detector functionality, matching or outperforming prior work on multipledata sets.
arxiv-1605-05422 | Optimization Beyond Prediction: Prescriptive Price Optimization |  http://arxiv.org/abs/1605.05422  | author:Shinji Ito, Ryohei Fujimaki category:math.OC cs.LG stat.ML published:2016-05-18 summary:This paper addresses a novel data science problem, prescriptive priceoptimization, which derives the optimal price strategy to maximize futureprofit/revenue on the basis of massive predictive formulas produced by machinelearning. The prescriptive price optimization first builds sales forecastformulas of multiple products, on the basis of historical data, which revealcomplex relationships between sales and prices, such as price elasticity ofdemand and cannibalization. Then, it constructs a mathematical optimizationproblem on the basis of those predictive formulas. We present that theoptimization problem can be formulated as an instance of binary quadraticprogramming (BQP). Although BQP problems are NP-hard in general andcomputationally intractable, we propose a fast approximation algorithm using asemi-definite programming (SDP) relaxation, which is closely related to theGoemans-Williamson's Max-Cut approximation. Our experiments on simulation andreal retail datasets show that our prescriptive price optimizationsimultaneously derives the optimal prices of tens/hundreds products withpractical computational time, that potentially improve 8.2% of gross profit ofthose products.
arxiv-1605-05416 | Leveraging Lexical Resources for Learning Entity Embeddings in Multi-Relational Data |  http://arxiv.org/abs/1605.05416  | author:Teng Long, Ryan Lowe, Jackie Chi Kit Cheung, Doina Precup category:cs.CL published:2016-05-18 summary:Recent work in learning vector-space embeddings for multi-relational data hasfocused on combining relational information derived from knowledge bases withdistributional information derived from large text corpora. We propose a simpleapproach that leverages the descriptions of entities or phrases available inlexical resources, in conjunction with distributional semantics, in order toderive a better initialization for training relational models. Applying thisinitialization to the TransE model results in significant new state-of-the-artperformances on the WordNet dataset, decreasing the mean rank from the previousbest of 212 to 51. It also results in faster convergence of the entityrepresentations. We find that there is a trade-off between improving the meanrank and the hits@10 with this approach. This illustrates that much remains tobe understood regarding performance improvements in relational models.
arxiv-1605-05415 | Relative distance features for gait recognition with Kinect |  http://arxiv.org/abs/1605.05415  | author:Ke Yang, Yong Dou, Shaohe Lv, Fei Zhang, Qi Lv category:cs.CV published:2016-05-18 summary:Gait and static body measurement are important biometric technologies forpassive human recognition. Many previous works argue that recognitionperformance based completely on the gait feature is limited. The reason forthis limited performance remains unclear. This study focuses on humanrecognition with gait feature obtained by Kinect and shows that gait featurecan effectively distinguish from different human beings through a novelrepresentation -- relative distance-based gait features. Experimental resultsshow that the recognition accuracy with relative distance features reaches upto 85%, which is comparable with that of anthropometric features. Thecombination of relative distance features and anthropometric features canprovide an accuracy of more than 95%. Results indicate that the relativedistance feature is quite effective and worthy of further study in more generalscenarios (e.g., without Kinect).
arxiv-1605-05414 | On the Evaluation of Dialogue Systems with Next Utterance Classification |  http://arxiv.org/abs/1605.05414  | author:Ryan Lowe, Iulian V. Serban, Mike Noseworthy, Laurent Charlin, Joelle Pineau category:cs.CL cs.LG published:2016-05-18 summary:An open challenge in constructing dialogue systems is developing methods forautomatically learning dialogue strategies from large amounts of unlabelleddata. Recent work has proposed Next-Utterance-Classification (NUC) as asurrogate task for building dialogue systems from text data. In this paper weinvestigate the performance of humans on this task to validate the relevance ofNUC as a method of evaluation. Our results show three main findings: (1) humansare able to correctly classify responses at a rate much better than chance,thus confirming that the task is feasible, (2) human performance levels varyacross task domains (we consider 3 datasets) and expertise levels (novice vsexperts), thus showing that a range of performance is possible on this type oftask, (3) automated dialogue systems built using state-of-the-art machinelearning methods have similar performance to the human novices, but worse thanthe experts, thus confirming the utility of this class of tasks for drivingfurther research in automated dialogue systems.
arxiv-1605-05411 | Are Facial Attributes Adversarially Robust? |  http://arxiv.org/abs/1605.05411  | author:Andras Rozsa, Manuel Günther, Ethan M. Rudd, Terrance E. Boult category:cs.CV published:2016-05-18 summary:Facial attributes are emerging soft biometrics that have the potential toreject non-matches, for example, based on on mismatching gender. To be usablein stand-alone systems, facial attributes must be extracted from imagesautomatically and reliably. In this paper we propose a simple yet effectivesolution for automatic facial attribute extraction by training a deepconvolutional neural network (DCNN) for each facial attribute separately,without using any pre-training or dataset augmentation, and we obtain newstate-of-the-art facial attribute classification results on the CelebAbenchmark. To test the stability of the networks, we generated adversarialimages via a novel fast flipping attribute (FFA) technique. We show that FFAgenerates more adversarials than other related algorithms, and that the DCNNsfor certain attributes are generally robust to adversarial inputs, while DCNNsfor other attributes are not. This result is surprising because no DCNNs testedto date have exhibited robustness to adversarial images without explicitaugmentation in the training procedure to account for adversarial examples.Finally, we introduce the concept of natural adversarial images, i.e., imagesthat are misclassified but can be easily turned into correctly classifiedimages by applying small perturbations. We demonstrate that naturaladversarials commonly occur, even within the training set, and show that mostof these images remain misclassified even with additional training epochs. Thisphenomenon is surprising because correcting the misclassification, particularlywhen guided by training data, should require only a small adjustment to theDCNN parameters.
arxiv-1605-05697 | Online Algorithms For Parameter Mean And Variance Estimation In Dynamic Regression Models |  http://arxiv.org/abs/1605.05697  | author:Carlos Alberto Gomez-Uribe category:stat.ML published:2016-05-18 summary:We study the problem of estimating the parameters of a regression model froma set of observations, each consisting of a response and a predictor. Theresponse is assumed to be related to the predictor via a regression model ofunknown parameters. Often, in such models the parameters to be estimated areassumed to be constant. Here we consider the more general scenario where theparameters are allowed to evolve over time, a more natural assumption for manyapplications. We model these dynamics via a linear update equation withadditive noise that is often used in a wide range of engineering applications,particularly in the well-known and widely used Kalman filter (where the systemstate it seeks to estimate maps to the parameter values here). We derive anapproximate algorithm to estimate both the mean and the variance of theparameter estimates in an online fashion for a generic regression model. Thisalgorithm turns out to be equivalent to the extended Kalman filter. Wespecialize our algorithm to the multivariate exponential family distribution toobtain a generalization of the generalized linear model (GLM). Because thecommon regression models encountered in practice such as logistic, exponentialand multinomial all have observations modeled through an exponential familydistribution, our results are used to easily obtain algorithms for online meanand variance parameter estimation for all these regression models in thecontext of time-dependent parameters. Lastly, we propose to use thesealgorithms in the contextual multi-armed bandit scenario, where so far modelparameters are assumed static and observations univariate and Gaussian orBernoulli. Both of these restrictions can be relaxed using the algorithmsdescribed here, which we combine with Thompson sampling to show the resultingperformance on a simulation.
arxiv-1605-05579 | Low-Rank Matrices on Graphs: Generalized Recovery & Applications |  http://arxiv.org/abs/1605.05579  | author:Nauman Shahid, Nathanael Perraudin, Gilles Puy, Pierre Vandergheynst category:cs.CV published:2016-05-18 summary:Many real world datasets subsume a linear or non-linear low-rank structure ina very low-dimensional space. Unfortunately, one often has very little or noinformation about the geometry of the space, resulting in a highlyunder-determined recovery problem. Under certain circumstances,state-of-the-art algorithms provide an exact recovery for linear low-rankstructures but at the expense of highly inscalable algorithms which use nuclearnorm. However, the case of non-linear structures remains unresolved. We revisitthe problem of low-rank recovery from a totally different perspective,involving graphs which encode pairwise similarity between the data samples andfeatures. Surprisingly, our analysis confirms that it is possible to recovermany approximate linear and non-linear low-rank structures with recoveryguarantees with a set of highly scalable and efficient algorithms. We call suchdata matrices as \textit{Low-Rank matrices on graphs} and show that many realworld datasets satisfy this assumption approximately due to underlyingstationarity. Our detailed theoretical and experimental analysis unveils thepower of the simple, yet very novel recovery framework \textit{Fast Robust PCAon Graphs}
arxiv-1605-05710 | Active Learning On Weighted Graphs Using Adaptive And Non-adaptive Approaches |  http://arxiv.org/abs/1605.05710  | author:Eyal En Gad, Akshay Gadde, A. Salman Avestimehr, Antonio Ortega category:cs.LG published:2016-05-18 summary:This paper studies graph-based active learning, where the goal is toreconstruct a binary signal defined on the nodes of a weighted graph, bysampling it on a small subset of the nodes. A new sampling algorithm isproposed, which sequentially selects the graph nodes to be sampled, based on anaggressive search for the boundary of the signal over the graph. The algorithmgeneralizes a recent method for sampling nodes in unweighted graphs. Thegeneralization improves the sampling performance using the information gainedfrom the available graph weights. An analysis of the number of samples requiredby the proposed algorithm is provided, and the gain over the unweighted methodis further demonstrated in simulations. Additionally, the proposed method iscompared with an alternative state of-the-art method, which is based on thegraph's spectral properties. It is shown that the proposed method significantlyoutperforms the spectral sampling method, if the signal needs to be predictedwith high accuracy. On the other hand, if a higher level of inaccuracy istolerable, then the spectral method outperforms the proposed aggressive searchmethod. Consequently, we propose a hybrid method, which is shown to combine theadvantages of both approaches.
arxiv-1605-05776 | The Quality of the Covariance Selection Through Detection Problem and AUC Bounds |  http://arxiv.org/abs/1605.05776  | author:Navid Tafaghodi Khajavi, Anthony Kuh category:cs.IT math.IT stat.ML published:2016-05-18 summary:We consider the problem of quantifying the quality of a model selectionproblem for a graphical model. We discuss this by formulating the problem as adetection problem. Model selection problems usually minimize the distance withthe model distribution. For the special case of Gaussian distributions, thisproblem simplifies to the covariance selection problem which is widelydiscussed in literature by Dempster [1] where the Kullback-Leibler (KL)divergence is minimized or equivalently the likelihood criterion maximized tocompute the model covariance matrix. While this solution is optimal forGaussian distributions in the sense of the KL divergence, it is not optimalwhen compared with other information divergences and criteria such as AreaUnder the Curve (AUC). In this paper, we discuss the quality of model approximation using the AUCand its bounds as an average measure of accuracy in detection problem. Wecompute upper and lower bounds for the AUC. We define the correlationapproximation matrix (CAM) and show that the KL divergence and AUC and itsupper and lower bounds depend on the eigenvalues of the CAM. We also show therelationship between the AUC, the KL divergence and the ROC curve by optimizingwith respect to the ROC curve. In the examples provided, we pick treestructures as the simplest graphical models. We perform simulations onfully-connected graphs and compute the tree structured models by applying thewidely used Chow-Liu algorithm. Examples show that the quality of treeapproximation models are not good in general based on information divergences,AUC and its bounds when the number of nodes in the graphical model is large.Specially for 1-AUC, it is shown both in theory and using simulations that the1-AUC for the tree approximation model decays exponentially as the dimension ofthe graphical model increases.
arxiv-1605-05628 | Detecting Novel Processes with CANDIES -- An Holistic Novelty Detection Technique based on Probabilistic Models |  http://arxiv.org/abs/1605.05628  | author:Christian Gruhl, Bernhard Sick category:cs.LG published:2016-05-18 summary:In this article, we propose CANDIES (Combined Approach for Novelty Detectionin Intelligent Embedded Systems), a new approach to novelty detection intechnical systems. We assume that in a technical system several processesinteract. If we observe these processes with sensors, we are able to model theobservations (samples) with a probabilistic model, where, in an ideal case, thecomponents of the parametric mixture density model we use, correspond to theprocesses in the real world. Eventually, at run-time, novel processes emerge inthe technical systems such as in the case of an unpredictable failure. As aconsequence, new kinds of samples are observed that require an adaptation ofthe model. CANDIES relies on mixtures of Gaussians which can be used forclassification purposes, too. New processes may emerge in regions of themodels' input spaces where few samples were observed before (low-densityregions) or in regions where already many samples were available (high-densityregions). The latter case is more difficult, but most existing solutions focuson the former. Novelty detection in low- and high-density regions requiresdifferent detection strategies. With CANDIES, we introduce a new technique todetect novel processes in high-density regions by means of a fast onlinegoodness-of-fit test. For detection in low-density regions we combine thisapproach with a 2SND (Two-Stage-Novelty-Detector) which we presented inpreliminary work. The properties of CANDIES are evaluated using artificial dataand benchmark data from the field of intrusion detection in computer networks,where the task is to detect new kinds of attacks.
arxiv-1605-05652 | Low dimensional manifold model in hyperspectral image reconstruction |  http://arxiv.org/abs/1605.05652  | author:Zuoqiang Shi, Wei Zhu, Stanley Osher category:cs.CV cs.IT math.IT published:2016-05-18 summary:We present the application of a low dimensional manifold model (LDMM) onhyperspectral image (HSI) reconstruction. An important property ofhyperspectral images is that the patch manifold, which is sampled by thethree-dimensional blocks in the data cube, is generally of a low dimensionalnature. This is a generalization of low-rank models in that hyperspectralimages with nonlinear mixing terms can also fit in this framework. The pointintegral method (PIM) is used to solve a Laplace-Beltrami equation over a pointcloud sampling the patch manifold in LDMM. Both numerical simulations andtheoretical analysis show that the sample points constraint is correctlyenforced by PIM. The framework is demonstrated by experiments on thereconstruction of both linear and nonlinear mixed hyperspectral images with asignificant number of missing voxels and several entirely missing spectralbands.
arxiv-1605-05782 | A comparison of semi-deterministic and stochastic search techniques |  http://arxiv.org/abs/1605.05782  | author:Andy M. Connor, Kristina Shea category:cs.NE math.OC published:2016-05-18 summary:This paper presents an investigation of two search techniques, tabu search(TS) and simulated annealing (SA), to assess their relative merits when appliedto engineering design optimisation. Design optimisation problems are generallycharacterised as having multi-modal search spaces and discontinuities makingglobal optimisation techniques beneficial. Both techniques claim to be capableof locating globally optimum solutions on a range of problems but thiscapability is derived from different underlying philosophies. While tabu searchuses a semi-deterministic approach to escape local optima, simulated annealinguses a complete stochastic approach. The performance of each technique isinvestigated using a structural optimisation problem. These performances arethen compared to each other as and to a steepest descent (SD) method.
arxiv-1605-05045 | Incremental Object Recognition in Robotics with Extension to New Classes in Constant Time |  http://arxiv.org/abs/1605.05045  | author:Raffaello Camoriano, Giulia Pasquale, Carlo Ciliberto, Lorenzo Natale, Lorenzo Rosasco, Giorgio Metta category:stat.ML cs.CV cs.LG cs.RO published:2016-05-17 summary:We consider object recognition in the context of lifelong learning, where arobotic agent learns to discriminate between a growing number of object classesas it accumulates experience about the environment. We propose an incrementalvariant of the Regularized Least Squares for Classification (RLSC) algorithm,and exploit its structure to seamlessly add new classes to the learned model.The presented algorithm addresses the problem of having unbalanced proportionof training examples per class, which occurs when new objects are presented tothe system for the first time. We evaluate our algorithm on both a machinelearning benchmark dataset and a challenging object recognition task in arobotic setting. Empirical evidence on both problems shows that our approach issignificantly faster than its batch counterparts while achieving comparable orbetter classification performance when classes are unbalanced.
arxiv-1605-05034 | LIME: A Method for Low-light IMage Enhancement |  http://arxiv.org/abs/1605.05034  | author:Xiaojie Guo category:cs.CV published:2016-05-17 summary:When one captures images in low-light conditions, the images often sufferfrom low visibility. This poor quality may significantly degrade theperformance of many computer vision and multimedia algorithms that areprimarily designed for high-quality inputs. In this paper, we propose a verysimple and effective method, named as LIME, to enhance low-light images. Moreconcretely, the illumination of each pixel is first estimated individually byfinding the maximum value in R, G and B channels (Max-RGB). Further, we refinethe initial illumination map by imposing a structure prior on it, as the finalillumination map. Having the well-constructed illumination map, the enhancementcan be achieved accordingly. Experiments on a number of challenging real-worldlow-light images are present to reveal the efficacy of our LIME and show itssuperiority over several state-of-the-arts.
arxiv-1605-05054 | HARRISON: A Benchmark on HAshtag Recommendation for Real-world Images in Social Networks |  http://arxiv.org/abs/1605.05054  | author:Minseok Park, Hanxiang Li, Junmo Kim category:cs.CV cs.IR cs.SI published:2016-05-17 summary:Simple, short, and compact hashtags cover a wide range of information onsocial networks. Although many works in the field of natural languageprocessing (NLP) have demonstrated the importance of hashtag recommendation,hashtag recommendation for images has barely been studied. In this paper, weintroduce the HARRISON dataset, a benchmark on hashtag recommendation for realworld images in social networks. The HARRISON dataset is a realistic dataset,composed of 57,383 photos from Instagram and an average of 4.5 associatedhashtags for each photo. To evaluate our dataset, we design a baselineframework consisting of visual feature extractor based on convolutional neuralnetwork (CNN) and multi-label classifier based on neural network. Based on thisframework, two single feature-based models, object-based and scene-based model,and an integrated model of them are evaluated on the HARRISON dataset. Ourdataset shows that hashtag recommendation task requires a wide and contextualunderstanding of the situation conveyed in the image. As far as we know, thiswork is the first vision-only attempt at hashtag recommendation for real worldimages in social networks. We expect this benchmark to accelerate theadvancement of hashtag recommendation.
arxiv-1605-05087 | Word2Vec is only a special case of Kernel Correspondence Analysis and Kernels for Natural Language Processing |  http://arxiv.org/abs/1605.05087  | author:Hirotaka Niitsuma category:cs.LG cs.CL published:2016-05-17 summary:We show Correspondence Analysis (CA) is equivalent to defining Gini-indexwith appropriate scaled one-hot encoding. Using this relation, we introducenon-linear kernel extension of CA. The extended CA gives well-known analysisfor categorical data (CD) and natural language processing by specializingkernels. For example, our formulation can give G-test, skip-gram withnegative-sampling (SGNS), and GloVe as a special case. We introduce two kernelsfor natural language processing based on our formulation. First is a stopword(SW) kernel. Second is word similarity(WS) kernel. The SW kernel is thesystem introducing appropriate weights for SW. The WS kernel enables to use WStest data as training data for vector space representations of words. We showthese kernels enhances accuracy when training data is not sufficiently large.
arxiv-1605-05101 | Recurrent Neural Network for Text Classification with Multi-Task Learning |  http://arxiv.org/abs/1605.05101  | author:Pengfei Liu, Xipeng Qiu, Xuanjing Huang category:cs.CL published:2016-05-17 summary:Neural network based methods have obtained great progress on a variety ofnatural language processing tasks. However, in most previous works, the modelsare learned based on single-task supervised objectives, which often suffer frominsufficient training data. In this paper, we use the multi-task learningframework to jointly learn across multiple related tasks. Based on recurrentneural network, we propose three different mechanisms of sharing information tomodel text with task-specific and shared layers. The entire network is trainedjointly on all these tasks. Experiments on four benchmark text classificationtasks show that our proposed models can improve the performance of a task withthe help of other related tasks.
arxiv-1605-05106 | Detecting Violent Crowds using Temporal Analysis of GLCM Texture |  http://arxiv.org/abs/1605.05106  | author:Kaelon Lloyd, David Marshall, Simon C. Moore, Paul L. Rosin category:cs.CV published:2016-05-17 summary:The severity of sustained injury resulting from assault-related violence canbe minimized by reducing detection time. However, it has been shown that humanoperators perform poorly at detecting events found in video footage whenpresented with simultaneous feeds. We utilize computer vision techniques todevelop an automated method of violence detection that can aid a humanoperator. We observed that violence in city centre environments often occur incrowded areas, resulting in individual actions being occluded by other crowdmembers. Measures of visual texture have shown to be effective at encodingcrowd appearance. Therefore, we propose modelling crowd dynamics using changesin crowd texture. We refer to this approach as Violent Crowd Texture (VCT).Real-world surveillance footage of night time environments and the violentflows dataset were tested using a random forest classifier to evaluate theability of the VCT method at discriminating between violent and non-violentbehaviour. Our method achieves ROC values of 0.98 and 0.91 on our own realworld CCTV dataset and the violent flows dataset respectively.
arxiv-1605-05110 | Incorporating Loose-Structured Knowledge into LSTM with Recall Gate for Conversation Modeling |  http://arxiv.org/abs/1605.05110  | author:Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun, Xiaolong Wang category:cs.CL published:2016-05-17 summary:Modeling human conversations is the essence for building satisfying chat-botswith multi-turn dialog ability. Conversation modeling will notably benefit fromdomain knowledge since the relationships between sentences can be clarified dueto semantic hints introduced by knowledge. In this paper, a deep neural networkis proposed to incorporate background knowledge for conversation modeling.Through a specially designed Recall gate, domain knowledge can be transformedinto the extra global memory of Long Short-Term Memory (LSTM), so as to enhanceLSTM by cooperating with its local memory to capture the implicit semanticrelevance between sentences within conversations. In addition, this paperintroduces the loose structured domain knowledge base, which can be built withslight amount of manual work and easily adopted by the Recall gate. Our modelis evaluated on the context-oriented response selecting task, and experimentalresults on both two datasets have shown that our approach is promising formodeling human conversations and building key components of automatic chattingsystems.
arxiv-1605-05134 | A Semi-automatic Method for Efficient Detection of Stories on Social Media |  http://arxiv.org/abs/1605.05134  | author:Soroush Vosoughi, Deb Roy category:cs.SI cs.CL cs.IR published:2016-05-17 summary:Twitter has become one of the main sources of news for many people. Asreal-world events and emergencies unfold, Twitter is abuzz with hundreds ofthousands of stories about the events. Some of these stories are harmless,while others could potentially be life-saving or sources of malicious rumors.Thus, it is critically important to be able to efficiently track stories thatspread on Twitter during these events. In this paper, we present a novelsemi-automatic tool that enables users to efficiently identify and trackstories about real-world events on Twitter. We ran a user study with 25participants, demonstrating that compared to more conventional methods, ourtool can increase the speed and the accuracy with which users can track storiesabout real-world events.
arxiv-1605-05142 | Automatic Classification of Irregularly Sampled Time Series with Unequal Lengths: A Case Study on Estimated Glomerular Filtration Rate |  http://arxiv.org/abs/1605.05142  | author:Santosh Tirunagari, Simon Bull, Norman Poh category:cs.LG cs.CE published:2016-05-17 summary:A patient's estimated glomerular filtration rate (eGFR) can provide importantinformation about disease progression and kidney function. Traditionally, aneGFR time series is interpreted by a human expert labelling it as stable orunstable. While this approach works for individual patients, the time consumingnature of it precludes the quick evaluation of risk in large numbers ofpatients. However, automating this process poses significant challenges as eGFRmeasurements are usually recorded at irregular intervals and the series ofmeasurements differs in length between patients. Here we present a two-tiersystem to automatically classify an eGFR trend. First, we model the time seriesusing Gaussian process regression (GPR) to fill in `gaps' by resampling a fixedsize vector of fifty time-dependent observations. Second, we classify theresampled eGFR time series using a K-NN/SVM classifier, and evaluate itsperformance via 5-fold cross validation. Using this approach we achieved anF-score of 0.90, compared to 0.96 for 5 human experts when scored amongstthemselves.
arxiv-1605-05150 | Automatic Detection and Categorization of Election-Related Tweets |  http://arxiv.org/abs/1605.05150  | author:Prashanth Vijayaraghavan, Soroush Vosoughi, Deb Roy category:cs.CL cs.IT cs.SI math.IT published:2016-05-17 summary:With the rise in popularity of public social media and micro-bloggingservices, most notably Twitter, the people have found a venue to hear and beheard by their peers without an intermediary. As a consequence, and aided bythe public nature of Twitter, political scientists now potentially have themeans to analyse and understand the narratives that organically form, spreadand decline among the public in a political campaign. However, the volume anddiversity of the conversation on Twitter, combined with its noisy andidiosyncratic nature, make this a hard task. Thus, advanced data mining andlanguage processing techniques are required to process and analyse the data. Inthis paper, we present and evaluate a technical framework, based on recentadvances in deep neural networks, for identifying and analysingelection-related conversation on Twitter on a continuous, longitudinal basis.Our models can detect election-related tweets with an F-score of 0.92 and cancategorize these tweets into 22 topics with an F-score of 0.90.
arxiv-1605-05156 | Tweet Acts: A Speech Act Classifier for Twitter |  http://arxiv.org/abs/1605.05156  | author:Soroush Vosoughi, Deb Roy category:cs.CL cs.SI published:2016-05-17 summary:Speech acts are a way to conceptualize speech as action. This holds true forcommunication on any platform, including social media platforms such asTwitter. In this paper, we explored speech act recognition on Twitter bytreating it as a multi-class classification problem. We created a taxonomy ofsix speech acts for Twitter and proposed a set of semantic and syntacticfeatures. We trained and tested a logistic regression classifier using a dataset of manually labelled tweets. Our method achieved a state-of-the-artperformance with an average F1 score of more than $0.70$. We also exploredclassifiers with three different granularities (Twitter-wide, type-specific andtopic-specific) in order to find the right balance between generalization andoverfitting for our task.
arxiv-1605-05157 | Monocular Urban Localization using Street View |  http://arxiv.org/abs/1605.05157  | author:Li Yu, Cyril Joly, Guillaume Bresson, Fabien Moutarde category:cs.RO cs.CV published:2016-05-17 summary:This paper presents a metric global localization in the urban environmentonly with a monocular camera and the Google Street View database. We fullyleverage the abundant sources from the Street View and benefits from itstopo-metric structure to build a coarse-to-fine positioning, namely atopological place recognition process and then a metric pose estimation bylocal bundle adjustment. Our method is tested on a 3 km urban environment anddemonstrates both sub-meter accuracy and robustness to viewpoint changes,illumination and occlusion. To our knowledge, this is the first work thatstudies the global urban localization simply with a single camera and StreetView.
arxiv-1605-05166 | Digital Stylometry: Linking Profiles Across Social Networks |  http://arxiv.org/abs/1605.05166  | author:Soroush Vosoughi, Helen Zhou, Deb Roy category:cs.SI cs.AI cs.CL cs.IR published:2016-05-17 summary:There is an ever growing number of users with accounts on multiple socialmedia and networking sites. Consequently, there is increasing interest inmatching user accounts and profiles across different social networks in orderto create aggregate profiles of users. In this paper, we present models forDigital Stylometry, which is a method for matching users through stylometryinspired techniques. We experimented with linguistic, temporal, and combinedtemporal-linguistic models for matching user accounts, using standard and noveltechniques. Using publicly available data, our best model, a combinedtemporal-linguistic one, was able to correctly match the accounts of 31% of5,612 distinct users across Twitter and Facebook.
arxiv-1605-05172 | Siamese convolutional networks based on phonetic features for cognate identification |  http://arxiv.org/abs/1605.05172  | author:Taraka Rama category:cs.CL published:2016-05-17 summary:In this paper, we explore the use of convolutional networks (ConvNets) forthe purpose of cognate identification. We compare our architecture with binaryclassifiers based on string similarity measures on different language families.Our experiments show that convolutional networks achieve competitive resultsacross concepts and across language families at the task of cognateidentification.
arxiv-1605-05019 | Image stitching with perspective-preserving warping |  http://arxiv.org/abs/1605.05019  | author:Tianzhu Xiang, Gui-Song Xia, Liangpei Zhang category:cs.CV published:2016-05-17 summary:Image stitching algorithms often adopt the global transformation, such ashomography, and work well for planar scenes or parallax free camera motions.However, these conditions are easily violated in practice. With casual cameramotions, variable taken views, large depth change, or complex structures, it isa challenging task for stitching these images. The global transformation modeloften provides dreadful stitching results, such as misalignments or projectivedistortions, especially perspective distortion. To this end, we suggest aperspective-preserving warping for image stitching, which spatially combineslocal projective transformations and similarity transformation. By weightedcombination scheme, our approach gradually extrapolates the local projectivetransformations of the overlapping regions into the non-overlapping regions,and thus the final warping can smoothly change from projective to similarity.The proposed method can provide satisfactory alignment accuracy as well asreduce the projective distortions and maintain the multi-perspective view.Experiments on a variety of challenging images confirm the efficiency of theapproach.
arxiv-1605-05180 | Structured Prediction of 3D Human Pose with Deep Neural Networks |  http://arxiv.org/abs/1605.05180  | author:Bugra Tekin, Isinsu Katircioglu, Mathieu Salzmann, Vincent Lepetit, Pascal Fua category:cs.CV published:2016-05-17 summary:Most recent approaches to monocular 3D pose estimation rely on Deep Learning.They either train a Convolutional Neural Network to directly regress from imageto 3D pose, which ignores the dependencies between human joints, or model thesedependencies via a max-margin structured learning framework, which involves ahigh computational cost at inference time. In this paper, we introduce a Deep Learning regression architecture forstructured prediction of 3D human pose from monocular images that relies on anovercomplete auto-encoder to learn a high-dimensional latent poserepresentation and account for joint dependencies. We demonstrate that ourapproach outperforms state-of-the-art ones both in terms of structurepreservation and prediction accuracy.
arxiv-1605-05011 | Locally Weighted Ensemble Clustering |  http://arxiv.org/abs/1605.05011  | author:Dong Huang, Chang-Dong Wang, Jian-Huang Lai category:cs.LG published:2016-05-17 summary:Due to its ability to combine multiple base clusterings into a probablybetter and more robust clustering, the ensemble clustering technique has beenattracting increasing attention in recent years. Despite the significantsuccess, one limitation to most of the existing ensemble clustering methods isthat they generally treat all base clusterings equally regardless of theirreliability, which makes them vulnerable to low-quality base clusterings.Although some efforts have been made to (globally) evaluate and weight the baseclusterings, yet these methods tend to view each base clustering as anindividual and neglect the local diversity of clusters inside the same baseclustering. It remains an open problem how to evaluate the reliability ofclusters and exploit the local diversity in the ensemble to enhance theconsensus performance, without access to data features or specific assumptionson data distribution. To address this, in this paper, we propose a novelensemble clustering approach based on ensemble-driven cluster uncertaintyestimation and local weighting strategy. In particular, the uncertainty of eachcluster is estimated by considering the cluster labels in the entire ensemblevia an entropic criterion. A novel ensemble-driven cluster validity measure isintroduced, and a locally weighted co-association matrix is presented to serveas a summary for the ensemble of diverse clusters. With the local diversity inensembles exploited, two novel consensus functions are further proposed.Extensive experiments on a variety of real-world datasets demonstrate thesuperiority of the proposed approach over the state-of-the-art.
arxiv-1605-05195 | Enhanced Twitter Sentiment Classification Using Contextual Information |  http://arxiv.org/abs/1605.05195  | author:Soroush Vosoughi, Helen Zhou, Deb Roy category:cs.SI cs.AI cs.CL cs.IR published:2016-05-17 summary:The rise in popularity and ubiquity of Twitter has made sentiment analysis oftweets an important and well-covered area of research. However, the 140character limit imposed on tweets makes it hard to use standard linguisticmethods for sentiment classification. On the other hand, what tweets lack instructure they make up with sheer volume and rich metadata. This metadataincludes geolocation, temporal and author information. We hypothesize thatsentiment is dependent on all these contextual factors. Different locations,times and authors have different emotional valences. In this paper, we exploredthis hypothesis by utilizing distant supervision to collect millions oflabelled tweets from different locations, times and authors. We used this datato analyse the variation of tweet sentiments across different authors, timesand locations. Once we explored and understood the relationship between thesevariables and sentiment, we used a Bayesian approach to combine these variableswith more standard linguistic features such as n-grams to create a Twittersentiment classifier. This combined classifier outperforms the purelylinguistic classifier, showing that integrating the rich contextual informationavailable on Twitter into sentiment classification is a promising direction ofresearch.
arxiv-1605-05197 | Towards Weakly-Supervised Action Localization |  http://arxiv.org/abs/1605.05197  | author:Philippe Weinzaepfel, Xavier Martin, Cordelia Schmid category:cs.CV published:2016-05-17 summary:This paper presents a novel approach for weakly-supervised actionlocalization, i.e., that does not require per-frame spatial annotations fortraining. We first introduce an effective method for extracting human tubes bycombining a state-of-the-art human detector with a tracking-by-detectionapproach. Our tube extraction leverages the large amount of annotated humansavailable today and outperforms the state of the art by an order of magnitude:with less than 5 tubes per video, we obtain a recall of 95% on the UCF-Sportsand J-HMDB datasets. Given these human tubes, we perform weakly-supervisedselection based on multi-fold Multiple Instance Learning (MIL) with improveddense trajectories and achieve excellent results. We obtain a mAP of 84% onUCF-Sports, 54% on J-HMDB and 45% on UCF-101, which outperforms the state ofthe art for weakly-supervised action localization and is close to theperformance of the best fully-supervised approaches. The second contribution of this paper is a new realistic dataset for actionlocalization, named DALY (Daily Action Localization in YouTube). It containshigh quality temporal and spatial annotations for 10 actions in 31 hours ofvideos (3.3M frames), which is an order of magnitude larger than standardaction localization datasets. On the DALY dataset, our tubes have a spatialrecall of 82%, but the detection task is extremely challenging, we obtain 10.8%mAP.
arxiv-1605-04996 | SemiContour: A Semi-supervised Learning Approach for Contour Detection |  http://arxiv.org/abs/1605.04996  | author:Zizhao Zhang, Fuyong Xing, Xiaoshuang Shi, Lin Yang category:cs.CV published:2016-05-17 summary:Supervised contour detection methods usually require many labeled trainingimages to obtain satisfactory performance. However, a large set of annotateddata might be unavailable or extremely labor intensive. In this paper, weinvestigate the usage of semi-supervised learning (SSL) to obtain competitivedetection accuracy with very limited training data (three labeled images).Specifically, we propose a semi-supervised structured ensemble learningapproach for contour detection built on structured random forests (SRF). Toallow SRF to be applicable to unlabeled data, we present an effective sparserepresentation approach to capture inherent structure in image patches byfinding a compact and discriminative low-dimensional subspace representation inan unsupervised manner, enabling the incorporation of abundant unlabeledpatches with their estimated structured labels to help SRF perform better nodesplitting. We re-examine the role of sparsity and propose a novel and fastsparse coding algorithm to boost the overall learning efficiency. To the bestof our knowledge, this is the first attempt to apply SSL for contour detection.Extensive experiments on the BSDS500 segmentation dataset and the NYU Depthdataset demonstrate the superiority of the proposed method.
arxiv-1605-05212 | Multimodal Sparse Coding for Event Detection |  http://arxiv.org/abs/1605.05212  | author:Youngjune Gwon, William Campbell, Kevin Brady, Douglas Sturim, Miriam Cha, H. T. Kung category:cs.LG cs.CV published:2016-05-17 summary:Unsupervised feature learning methods have proven effective forclassification tasks based on a single modality. We present multimodal sparsecoding for learning feature representations shared across multiple modalities.The shared representations are applied to multimedia event detection (MED) andevaluated in comparison to unimodal counterparts, as well as other featurelearning methods such as GMM supervectors and sparse RBM. We report thecross-validated classification accuracy and mean average precision of the MEDsystem trained on features learned from our unimodal and multimodal settingsfor a subset of the TRECVID MED 2014 dataset.
arxiv-1605-05216 | Combinatorially Generated Piecewise Activation Functions |  http://arxiv.org/abs/1605.05216  | author:Justin Chen category:cs.NE published:2016-05-17 summary:In the neuroevolution literature, research has primarily focused on evolvingthe number of nodes, connections, and weights in artificial neural networks.Few attempts have been made to evolve activation functions. Research inevolving activation functions has mainly focused on evolving functionparameters, and developing heterogeneous networks by selecting from a fixedpool of activation functions. This paper introduces a novel technique forevolving heterogeneous artificial neural networks through combinatoriallygenerating piecewise activation functions to enhance expressive power. Idemonstrate this technique on NeuroEvolution of Augmenting Topologies usingArcTan and Sigmoid, and show that it outperforms the original algorithm onnon-Markovian double pole balancing. This technique expands the landscape ofunconventional activation functions by demonstrating that they are competitivewith canonical choices, and introduces a purview for further exploration ofautomatic model selection for artificial neural networks.
arxiv-1605-05273 | Learning Convolutional Neural Networks for Graphs |  http://arxiv.org/abs/1605.05273  | author:Mathias Niepert, Mohamed Ahmed, Konstantin Kutzkov category:cs.LG cs.AI stat.ML published:2016-05-17 summary:Numerous important problems can be framed as learning from graph data. Wepropose a framework for learning convolutional neural networks for arbitrarygraphs. These graphs may be undirected, directed, and with both discrete andcontinuous node and edge attributes. Analogous to image-based convolutionalnetworks that operate on locally connected regions of the input, we present ageneral approach to extracting locally connected regions from graphs. Usingestablished benchmark data sets, we demonstrate that the learned featurerepresentations are competitive with state of the art graph kernels and thattheir computation is highly efficient.
arxiv-1605-05223 | On the boosting ability of top-down decision tree learning algorithm for multiclass classification |  http://arxiv.org/abs/1605.05223  | author:Anna Choromanska, Krzysztof Choromanski, Mariusz Bojarski category:cs.LG published:2016-05-17 summary:We analyze the performance of the top-down multiclass classificationalgorithm for decision tree learning called LOMtree, recently proposed in theliterature Choromanska and Langford (2014) for solving efficientlyclassification problems with very large number of classes. The algorithm onlineoptimizes the objective function which simultaneously controls the depth of thetree and its statistical accuracy. We prove important properties of thisobjective and explore its connection to three well-known entropy-based decisiontree objectives, i.e. Shannon entropy, Gini-entropy and its modified version,for which instead online optimization schemes were not yet developed. We show,via boosting-type guarantees, that maximizing the considered objective leadsalso to the reduction of all of these entropy-based objectives. The bounds weobtain critically depend on the strong-concavity properties of theentropy-based criteria, where the mildest dependence on the number of classes(only logarithmic) corresponds to the Shannon entropy.
arxiv-1605-05239 | Biologically Inspired Radio Signal Feature Extraction with Sparse Denoising Autoencoders |  http://arxiv.org/abs/1605.05239  | author:Benjamin Migliori, Riley Zeller-Townson, Daniel Grady, Daniel Gebhardt category:stat.ML cs.LG cs.NE published:2016-05-17 summary:Automatic modulation classification (AMC) is an important task for moderncommunication systems; however, it is a challenging problem when signalfeatures and precise models for generating each modulation may be unknown. Wepresent a new biologically-inspired AMC method without the need for models ormanually specified features --- thus removing the requirement for expert priorknowledge. We accomplish this task using regularized stacked sparse denoisingautoencoders (SSDAs). Our method selects efficient classification featuresdirectly from raw in-phase/quadrature (I/Q) radio signals in an unsupervisedmanner. These features are then used to construct higher-complexity abstractfeatures which can be used for automatic modulation classification. Wedemonstrate this process using a dataset generated with a software definedradio, consisting of random input bits encoded in 100-sample segments ofvarious common digital radio modulations. Our results show correctclassification rates of > 99% at 7.5 dB signal-to-noise ratio (SNR) and > 92%at 0 dB SNR in a 6-way classification test. Our experiments demonstrate adramatically new and broadly applicable mechanism for performing AMC andrelated tasks without the need for expert-defined or modulation-specific signalinformation.
arxiv-1605-05258 | Real-time Eye Gaze Direction Classification Using Convolutional Neural Network |  http://arxiv.org/abs/1605.05258  | author:Anjith George, Aurobinda Routray category:cs.CV published:2016-05-17 summary:Estimation eye gaze direction is useful in various human-computer interactiontasks. Knowledge of gaze direction can give valuable information regardingusers point of attention. Certain patterns of eye movements known as eyeaccessing cues are reported to be related to the cognitive processes in thehuman brain. We propose a real-time framework for the classification of eyegaze direction and estimation of eye accessing cues. In the first stage, thealgorithm detects faces using a modified version of the Viola-Jones algorithm.A rough eye region is obtained using geometric relations and facial landmarks.The eye region obtained is used in the subsequent stage to classify the eyegaze direction. A convolutional neural network is employed in this work for theclassification of eye gaze direction. The proposed algorithm was tested on EyeChimera database and found to outperform state of the art methods. Thecomputational complexity of the algorithm is very less in the testing phase.The algorithm achieved an average frame rate of 24 fps in the desktopenvironment.
arxiv-1605-05272 | Fast and Accurate Algorithm for Eye Localization for Gaze Tracking in Low Resolution Images |  http://arxiv.org/abs/1605.05272  | author:Anjith George, Aurobinda Routray category:cs.CV published:2016-05-17 summary:Iris centre localization in low-resolution visible images is a challengingproblem in computer vision community due to noise, shadows, occlusions, posevariations, eye blinks, etc. This paper proposes an efficient method fordetermining iris centre in low-resolution images in the visible spectrum. Evenlow-cost consumer-grade webcams can be used for gaze tracking without anyadditional hardware. A two-stage algorithm is proposed for iris centrelocalization. The proposed method uses geometrical characteristics of the eye.In the first stage, a fast convolution based approach is used for obtaining thecoarse location of iris centre (IC). The IC location is further refined in thesecond stage using boundary tracing and ellipse fitting. The algorithm has beenevaluated in public databases like BioID, Gi4E and is found to outperform thestate of the art methods.
arxiv-1605-05278 | Exact Simulation of Noncircular or Improper Complex-Valued Stationary Gaussian Processes using Circulant Embedding |  http://arxiv.org/abs/1605.05278  | author:Adam M. Sykulski, Donald B. Percival category:stat.ME stat.CO stat.ML published:2016-05-17 summary:This paper provides an algorithm for simulating improper (or noncircular)complex-valued stationary Gaussian processes. The technique utilizes recentlydeveloped methods for multivariate Gaussian processes from the circulantembedding literature. The method can be performed in $\mathcal{O}(nlog_2n)$operations, where n is the length of the desired sequence. The method is exact,except when eigenvalues of prescribed circulant matrices are negative. Weevaluate the performance of the algorithm empirically, and provide a practicalexample where the method is guaranteed to be exact for all $n$, with animproper fractional Gaussian noise process.
arxiv-1605-05284 | Minimax Lower Bounds for Kronecker-Structured Dictionary Learning |  http://arxiv.org/abs/1605.05284  | author:Zahra Shakeri, Waheed U. Bajwa, Anand D. Sarwate category:cs.IT cs.LG math.IT stat.ML published:2016-05-17 summary:Dictionary learning is the problem of estimating the collection of atomicelements that provide a sparse representation of measured/collected signals ordata. This paper finds fundamental limits on the sample complexity ofestimating dictionaries for tensor data by proving a lower bound on the minimaxrisk. This lower bound depends on the dimensions of the tensor and parametersof the generative model. The focus of this paper is on second-order tensordata, with the underlying dictionaries constructed by taking the Kroneckerproduct of two smaller dictionaries and the observed data generated by sparselinear combinations of dictionary atoms observed through white Gaussian noise.In this regard, the paper provides a general lower bound on the minimax riskand also adapts the proof techniques for equivalent results using sparse andGaussian coefficient models. The reported results suggest that the samplecomplexity of dictionary learning for tensor data can be significantly lowerthan that for unstructured data.
arxiv-1605-05296 | Dataflow matrix machines as programmable, dynamically expandable, self-referential generalized recurrent neural networks |  http://arxiv.org/abs/1605.05296  | author:Michael Bukatin, Steve Matthews, Andrey Radul category:cs.NE cs.PL published:2016-05-17 summary:Dataflow matrix machines are a powerful generalization of recurrent neuralnetworks. They work with multiple types of linear streams and multiple types ofneurons, including higher-order neurons which dynamically update the matrixdescribing weights and topology of the network in question while the network isrunning. It seems that the power of dataflow matrix machines is sufficient forthem to be a convenient general purpose programming platform. This paperexplores a number of useful programming idioms and constructions arising inthis context.
arxiv-1605-05303 | Fuzzy Sets Across the Natural Language Generation Pipeline |  http://arxiv.org/abs/1605.05303  | author:A. Ramos-Soto, A. Bugarín, S. Barro category:cs.AI cs.CL published:2016-05-17 summary:We explore the implications of using fuzzy techniques (mainly those commonlyused in the linguistic description/summarization of data discipline) from anatural language generation perspective. For this, we provide an extensivediscussion of some general convergence points and an exploration of therelationship between the different tasks involved in the standard NLG systempipeline architecture and the most common fuzzy approaches used in linguisticsummarization/description of data, such as fuzzy quantified statements,evaluation criteria or aggregation operators. Each individual discussion isillustrated with a related use case. Recent work made in the context ofcross-fertilization of both research fields is also referenced. This paperencompasses general ideas that emerged as part of the PhD thesis "Applicationof fuzzy sets in data-to-text systems". It does not present a specificapplication or a formal approach, but rather discusses current high-levelissues and potential usages of fuzzy sets (focused on linguistic summarizationof data) in natural language generation.
arxiv-1605-05368 | Deep Action Sequence Learning for Causal Shape Transformation |  http://arxiv.org/abs/1605.05368  | author:Kin Gwn Lore, Daniel Stoecklein, Michael Davies, Baskar Ganapathysubramanian, Soumik Sarkar category:cs.LG cs.CV cs.NE published:2016-05-17 summary:Deep learning (DL) became the method of choice in recent years for solvingproblems ranging from object recognition and speech recognition to roboticperception and human disease prediction. In this paper, we present a hybridarchitecture of convolutional neural networks (CNN) and stacked autoencoders(SAE) to learn a sequence of actions that nonlinearly transforms an input shapeor distribution into a target shape or distribution with the same support.While such a framework can be useful in a variety of problems such as roboticpath planning, sequential decision-making in games and identifying materialprocessing pathways to achieve desired microstructures, this paper focuses oncontrolling fluid deformations in a microfluidic channel by deliberatelyplacing a sequence of pillars, which has a significant impact on manufacturingfor biomedical and textile applications where highly targeted shapes aredesired. We propose an architecture which simultaneously predicts theintermediate shape lying in the nonlinear transformation pathway between theundeformed and desired flow shape, then learns the causal action--the singlepillar which results in the deformation of the flow--one at a time. Thelearning of stage-wise transformations provides deep insights into the physicalflow deformation. Results show that under the current framework, our model isable to predict a sequence of pillars that reconstructs the flow shape whichhighly resembles the desired shape.
arxiv-1605-05349 | Orthogonal symmetric non-negative matrix factorization under the stochastic block model |  http://arxiv.org/abs/1605.05349  | author:Subhadeep Paul, Yuguo Chen category:stat.ML published:2016-05-17 summary:We present a method based on the orthogonal symmetric non-negative matrixtri-factorization of the normalized Laplacian matrix for community detection incomplex networks. While the exact factorization of a given order may not existand is NP hard to compute, we obtain an approximate factorization by solving anoptimization problem. We establish the connection of the factors obtainedthrough the factorization to a non-negative basis of an invariant subspace ofthe estimated matrix, drawing parallel with the spectral clustering. Using suchfactorization for clustering in networks is motivated by analyzing ablock-diagonal Laplacian matrix with the blocks representing the connectedcomponents of a graph. The method is shown to be consistent for communitydetection in graphs generated from the stochastic block model and the degreecorrected stochastic block model. Simulation results and real data analysisshow the effectiveness of these methods under a wide variety of situations,including sparse and highly heterogeneous graphs where the usual spectralclustering is known to fail. Our method also performs better than the state ofthe art in popular benchmark network datasets, e.g., the political web blogsand the karate club data.
arxiv-1605-05359 | Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions and Deep Neural Networks |  http://arxiv.org/abs/1605.05359  | author:Ramnandan Krishnamurthy, Aravind S. Lakshminarayanan, Peeyush Kumar, Balaraman Ravindran category:cs.LG cs.AI cs.CV cs.NE published:2016-05-17 summary:This paper introduces an automated skill acquisition framework inreinforcement learning which involves identifying a hierarchical description ofthe given task in terms of abstract states and extended actions betweenabstract states. Identifying such structures present in the task provides waysto simplify and speed up reinforcement learning learning algorithms. Thesestructures also help to generalize such algorithms over multiple tasks withoutrelearning policies from scratch. We use ideas from dynamical systems to findmetastable regions in the state space and associate them with abstract states.The spectral clustering algorithm PCCA+ is used to identify suitableabstractions aligned to the underlying structure. Skills are defined in termsof the transitions between such abstract states. The connectivity informationfrom PCCA+ is used to generate these skills or options. The skills areindependent of the learning task and can be efficiently reused across a varietyof tasks defined over a common state space. Another major advantage of theapproach is that it does not need a prior model of the MDP and can work welleven when the MDPs are constructed from sampled trajectories. Finally, wepresent our attempts to extend the automated skills acquisition framework tocomplex tasks such as learning to play video games where we use deep learningtechniques for representation learning to aid our spatio-temporal abstractionframework.
arxiv-1605-05396 | Generative Adversarial Text to Image Synthesis |  http://arxiv.org/abs/1605.05396  | author:Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee category:cs.NE cs.CV published:2016-05-17 summary:Automatic synthesis of realistic images from text would be interesting anduseful, but current AI systems are still far from this goal. However, in recentyears generic and powerful recurrent neural network architectures have beendeveloped to learn discriminative text feature representations. Meanwhile, deepconvolutional generative adversarial networks (GANs) have begun to generatehighly compelling images of specific categories such as faces, album covers,room interiors etc. In this work, we develop a novel deep architecture and GANformulation to effectively bridge these advances in text and image modeling,translating visual concepts from characters to pixels. We demonstrate thecapability of our model to generate plausible images of birds and flowers fromdetailed text descriptions.
arxiv-1605-05362 | Yelp Dataset Challenge: Review Rating Prediction |  http://arxiv.org/abs/1605.05362  | author:Nabiha Asghar category:cs.CL cs.IR cs.LG published:2016-05-17 summary:Review websites, such as TripAdvisor and Yelp, allow users to post onlinereviews for various businesses, products and services, and have been recentlyshown to have a significant influence on consumer shopping behaviour. An onlinereview typically consists of free-form text and a star rating out of 5. Theproblem of predicting a user's star rating for a product, given the user's textreview for that product, is called Review Rating Prediction and has latelybecome a popular, albeit hard, problem in machine learning. In this paper, wetreat Review Rating Prediction as a multi-class classification problem, andbuild sixteen different prediction models by combining four feature extractionmethods, (i) unigrams, (ii) bigrams, (iii) trigrams and (iv) Latent SemanticIndexing, with four machine learning algorithms, (i) logistic regression, (ii)Naive Bayes classification, (iii) perceptrons, and (iv) linear Support VectorClassification. We analyse the performance of each of these sixteen models tocome up with the best model for predicting the ratings from reviews. We use thedataset provided by Yelp for training and testing the models.
arxiv-1605-05365 | Dynamic Frame skip Deep Q Network |  http://arxiv.org/abs/1605.05365  | author:Aravind S. Lakshminarayanan, Sahil Sharma, Balaraman Ravindran category:cs.LG cs.AI cs.NE published:2016-05-17 summary:Deep Reinforcement Learning methods have achieved state of the artperformance in learning control policies for the games in the Atari 2600domain. One of the important parameters in the Arcade Learning Environment(ALE) is the frame skip rate. It decides the granularity at which agents cancontrol game play. A frame skip value of $k$ allows the agent to repeat aselected action $k$ number of times. The current state of the art architectureslike Deep Q-Network (DQN) and Dueling Network Architectures (DuDQN) consist ofa framework with a static frame skip rate, where the action output from thenetwork is repeated for a fixed number of frames regardless of the currentstate. In this paper, we propose a new architecture, Dynamic Frame skip DeepQ-Network (DFDQN) which makes the frame skip rate a dynamic learnableparameter. This allows us to choose the number of times an action is to berepeated based on the current state. We show empirically that such a settingimproves the performance on relatively harder games like Seaquest.
arxiv-1605-05395 | Learning Deep Representations of Fine-grained Visual Descriptions |  http://arxiv.org/abs/1605.05395  | author:Scott Reed, Zeynep Akata, Bernt Schiele, Honglak Lee category:cs.CV published:2016-05-17 summary:State-of-the-art methods for zero-shot visual recognition formulate learningas a joint embedding problem of images and side information. In theseformulations the current best complement to visual features are attributes:manually encoded vectors describing shared characteristics among categories.Despite good performance, attributes have limitations: (1) finer-grainedrecognition requires commensurately more attributes, and (2) attributes do notprovide a natural language interface. We propose to overcome these limitationsby training neural language models from scratch; i.e. without pre-training andonly consuming words and characters. Our proposed models train end-to-end toalign with the fine-grained and category-specific content of images. Naturallanguage provides a flexible and compact way of encoding only the salientvisual aspects for distinguishing categories. By training on raw text, ourmodel can do inference on raw text as well, providing humans a familiar modeboth for annotation and retrieval. Our model achieves strong performance onzero-shot text-based image retrieval and significantly outperforms theattribute-based state-of-the-art for zero-shot classification on the CaltechUCSD Birds 200-2011 dataset.
arxiv-1605-04634 | Heart Beat Characterization from Ballistocardiogram Signals using Extended Functions of Multiple Instances |  http://arxiv.org/abs/1605.04634  | author:Changzhe Jiao, Princess Lyons, Alina Zare, Licet Rosales, Marjorie Skubic category:cs.CV published:2016-05-16 summary:A multiple instance learning (MIL) method, extended Function of MultipleInstances ($e$FUMI), is applied to ballistocardiogram (BCG) signals produced bya hydraulic bed sensor. The goal of this approach is to learn a personalizedheartbeat "concept" for an individual. This heartbeat concept is a prototype(or "signature") that characterizes the heartbeat pattern for an individual inballistocardiogram data. The $e$FUMI method models the problem of learning aheartbeat concept from a BCG signal as a MIL problem. This approach elegantlyaddresses the uncertainty inherent in a BCG signal e. g., misalignment betweentraining data and ground truth, mis-collection of heartbeat by sometransducers, etc. Given a BCG training signal coupled with a ground truthsignal (e.g., a pulse finger sensor), training "bags" labeled with only binarylabels denoting if a training bag contains a heartbeat signal or not can begenerated. Then, using these bags, $e$FUMI learns a personalized concept ofheartbeat for a subject as well as several non-heartbeat background concepts.After learning the heartbeat concept, heartbeat detection and heart rateestimation can be applied to test data. Experimental results show that theestimated heartbeat concept found by $e$FUMI is more representative and a morediscriminative prototype of the heartbeat signals than those found bycomparison MIL methods in the literature.
arxiv-1605-04770 | Automatic Image Annotation via Label Transfer in the Semantic Space |  http://arxiv.org/abs/1605.04770  | author:Tiberio Uricchio, Lamberto Ballan, Lorenzo Seidenari, Alberto Del Bimbo category:cs.CV cs.IR cs.MM published:2016-05-16 summary:While most automatic image annotation methods rely solely on visual features,we consider integrating additional information into an unified embeddingcomprised of visual and textual information. We propose an approach based onKernel Canonical Correlation Analysis, which builds a latent semantic spacewhere correlation of visual and textual features are well preserved into asemantic embedding. Images in the semantic space have reduced semantic gap andthus they are likely to give better annotation performance. The proposedapproach is robust and can work either when the training set is well annotatedby experts, as well as when it is noisy such as in the case of user-generatedtags in social media. We evaluate our framework on four popular datasets. Ourresults show that our KCCA-based approach can be applied to severalstate-of-the-art label transfer methods to obtain significant improvements. Inparticular, nearest neighbor methods for label transfer get the most benefitand enable our approach to scale on never seen labels at training time. Ourapproach works even with the noisy tags of social users, provided thatappropriate denoising is performed. Experiments on a large scale setting showthat our method can provide some benefits even when the semantic space isestimated on a subset of training images.
arxiv-1605-04764 | Geometry Aware Mappings for High Dimensional Sparse Factors |  http://arxiv.org/abs/1605.04764  | author:Avradeep Bhowmik, Nathan Liu, Erheng Zhong, Badri Narayan Bhaskar, Suju Rajan category:cs.LG cs.IR stat.ML published:2016-05-16 summary:While matrix factorisation models are ubiquitous in large scalerecommendation and search, real time application of such models requires innerproduct computations over an intractably large set of item factors. In thismanuscript we present a novel framework that uses the inverted indexrepresentation to exploit structural properties of sparse vectors tosignificantly reduce the run time computational cost of factorisation models.We develop techniques that use geometry aware permutation maps on a tessellatedunit sphere to obtain high dimensional sparse embeddings for latent factorswith sparsity patterns related to angular closeness of the original latentfactors. We also design several efficient and deterministic realisations withinthis framework and demonstrate with experiments that our techniques lead tofaster run time operation with minimal loss of accuracy.
arxiv-1605-04785 | An Alternative Matting Laplacian |  http://arxiv.org/abs/1605.04785  | author:François Pitié category:cs.CV published:2016-05-16 summary:Cutting out and object and estimate its transparency mask is a key task inmany applications. We take on the work on closed-form matting by Levin et al.,that is used at the core of many matting techniques, and propose an alternativeformulation that offers more flexible controls over the matting priors. We alsoshow that this new approach is efficient at upscaling transparency maps fromcoarse estimates.
arxiv-1605-04800 | Log-linear Combinations of Monolingual and Bilingual Neural Machine Translation Models for Automatic Post-Editing |  http://arxiv.org/abs/1605.04800  | author:Marcin Junczys-Dowmunt, Roman Grundkiewicz category:cs.CL published:2016-05-16 summary:This paper describes the submission of the AMU (Adam Mickiewicz University)team to the Automatic Post-Editing (APE) task of WMT 2016. We explore theapplication of neural translation models to the APE problem and achieve goodresults by treating different models as components in a log-linear model,allowing for multiple inputs (the MT-output and the source) that are decoded tothe same target language (post-edited translations). A simple string-matchingpenalty integrated within the log-linear model can be used to control forhigher faithfulness with regard to the to-be-corrected machine translationinput. Our submission outperforms the uncorrected baseline on the unseen testset by -3.2% TER and +5.5% BLEU.
arxiv-1605-04809 | The AMU-UEDIN Submission to the WMT16 News Translation Task: Attention-based NMT Models as Feature Functions in Phrase-based SMT |  http://arxiv.org/abs/1605.04809  | author:Marcin Junczys-Dowmunt, Tomasz Dwojak, Rico Sennrich category:cs.CL published:2016-05-16 summary:This paper describes the AMU-UEDIN submissions to the WMT 2016 shared task onnews translation. We explore methods of decode-time integration ofattention-based neural translation models with phrase-based statistical machinetranslation. Efficient batch-algorithms for GPU-querying are proposed andimplemented. For English-Russian, the phrase-based system cannot surpassstate-of-the-art pure neural models. For the Russian-English task, oursubmission achieves the top BLEU result, outperforming the best pure neuralsystem by 1.1 BLEU points and our own phrase-based baseline by 1.6 BLEU. Infollow-up experiments we improve these results by additional 0.7 BLEU.
arxiv-1605-04731 | CNN based texture synthesize with Semantic segment |  http://arxiv.org/abs/1605.04731  | author:Xianye Liang, Bocheng Zhuo, Peijie Li, Liangju He category:cs.CV cs.GR cs.LG published:2016-05-16 summary:Deep learning algorithm display powerful ability in Computer Vision area, inrecent year, the CNN has been applied to solve problems in the subarea ofImage-generating, which has been widely applied in areas such as photo editing,image design, computer animation, real-time rendering for large scale of scenesand for visual effects in movies. However in the texture synthesize procedure.The state-of-art CNN can not capture the spatial location of texture in image,lead to significant distortion after texture synthesize, we propose a new wayto generating-image by adding the semantic segment step with deep learningalgorithm as Pre-Processing and analyze the outcome.
arxiv-1605-04806 | Multilevel Thresholding Segmentation of T2 weighted Brain MRI images using Convergent Heterogeneous Particle Swarm Optimization |  http://arxiv.org/abs/1605.04806  | author:Mohammad Hamed Mozaffari, Won-Sook Lee category:cs.CV published:2016-05-16 summary:This paper proposes a new image thresholding segmentation approach using theheuristic method, Convergent Heterogeneous Particle Swarm Optimizationalgorithm. The proposed algorithm incorporates a new strategy of searching theproblem space by dividing the swarm into subswarms. Each subswarm particlessearch for better solution separately lead to better exploitation while theycooperate with each other to find the best global position. The consequence ofthe aforementioned cooperation is better exploration, convergence and it ablethe algorithm to jump from local optimal solution to the better spots. Apractical application of this method is demonstrated for the problem of medicalimage thresholding segmentation. We considered two classical thresholdingtechniques of Otsu and Kapur separately as the objective function for theoptimization method and applied on a set of brain MR images. Comparativeexperimental results reveal that the proposed method outperforms another stateof the art method from the literature in terms of accuracy, computation timeand stable results.
arxiv-1605-04812 | Off-policy evaluation for slate recommendation |  http://arxiv.org/abs/1605.04812  | author:Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dudík, John Langford, Damien Jose, Imed Zitouni category:cs.LG cs.AI stat.ML published:2016-05-16 summary:This paper studies the evaluation of policies which recommend an ordered setof items based on some context---a common scenario in web search, ads, andrecommender systems. We develop a novel technique to evaluate such policiesoffline using logged past data with negligible bias. Our method builds on theassumption that the observed quality of the entire recommended set additivelydecomposes across items, but per-item quality is not directly observable, andwe might not be able to model it from the item's features. Empirical evidencereveals that this assumption fits many realistic scenarios and theoreticalanalysis shows that we can achieve exponential savings in the amount ofrequired data compared with na\"ive unbiased approaches.
arxiv-1605-04711 | Ternary Weight Networks |  http://arxiv.org/abs/1605.04711  | author:Fengfu Li, Bin Liu category:cs.CV published:2016-05-16 summary:We introduce Ternary Weight Networks (TWNs) - neural networks with weightsconstrained to +1, 0 and -1. The L2 distance between the full (float or double)precision weights and the ternary weights along with a scaling factor isminimized. With the optimization, the TWNs own high capacity of modelexpression that is good enough to approximate the Full Precision WeightNetworks (FPWNs) counterpart. Besides, the TWNs achieve up to 16x or 32x modelcompression rate and own much fewer multiplications compared with the FPWNs.Compared with recently proposed Binary Precision Weight Networks (BPWNs), theTWNs own nearly 38x more power of expression in a 3$\times$3 size filter, whichis commonly used in most of the state-of-the-art CNN models like residualnetworks or VGG. Besides, the TWNs eliminate the singularity at zero andconverge faster and more stablely at training time. Benchmarks on MNIST,CIFAR-10, and the large scale ImageNet dataset show that TWNs achievestate-of-the-art performance which is only slightly worse than the FPWNscounterpart but outperforms the analogous BPWNs.
arxiv-1605-04850 | Video2GIF: Automatic Generation of Animated GIFs from Video |  http://arxiv.org/abs/1605.04850  | author:Michael Gygli, Yale Song, Liangliang Cao category:cs.CV cs.MM published:2016-05-16 summary:We introduce the novel problem of automatically generating animated GIFs fromvideo. GIFs are short looping video with no sound, and a perfect combinationbetween image and video that really capture our attention. GIFs tell a story,express emotion, turn events into humorous moments, and are the new wave ofphotojournalism. We pose the question: Can we automate the entirely manual andelaborate process of GIF creation by leveraging the plethora of user generatedGIF content? We propose a Robust Deep RankNet that, given a video, generates aranked list of its segments according to their suitability as GIF. We train ourmodel to learn what visual content is often selected for GIFs by using over100K user generated GIFs and their corresponding video sources. We effectivelydeal with the noisy web data by proposing a novel adaptive Huber loss in theranking formulation. We show that our approach is robust to outliers and picksup several patterns that are frequently present in popular animated GIFs. Onour new large-scale benchmark dataset, we show the advantage of our approachover several state-of-the-art methods.
arxiv-1605-04859 | Reducing the Model Order of Deep Neural Networks Using Information Theory |  http://arxiv.org/abs/1605.04859  | author:Ming Tu, Visar Berisha, Yu Cao, Jae-sun Seo category:cs.LG cs.NE published:2016-05-16 summary:Deep neural networks are typically represented by a much larger number ofparameters than shallow models, making them prohibitive for small footprintdevices. Recent research shows that there is considerable redundancy in theparameter space of deep neural networks. In this paper, we propose a method tocompress deep neural networks by using the Fisher Information metric, which weestimate through a stochastic optimization method that keeps track ofsecond-order information in the network. We first remove unimportant parametersand then use non-uniform fixed point quantization to assign more bits toparameters with higher Fisher Information estimates. We evaluate our method ona classification task with a convolutional neural network trained on the MNISTdata set. Experimental results show that our method outperforms existingmethods for both network pruning and quantization.
arxiv-1605-04672 | A Critical Examination of RESCAL for Completion of Knowledge Bases with Transitive Relations |  http://arxiv.org/abs/1605.04672  | author:Pushpendre Rastogi, Benjamin Van Durme category:stat.ML cs.AI cs.DB cs.LG published:2016-05-16 summary:Link prediction in large knowledge graphs has received a lot of attentionrecently because of its importance for inferring missing relations and forcompleting and improving noisily extracted knowledge graphs. Over the years anumber of machine learning researchers have presented various models forpredicting the presence of missing relations in a knowledge base. Although allthe previous methods are presented with empirical results that show highperformance on select datasets, there is almost no previous work onunderstanding the connection between properties of a knowledge base and theperformance of a model. In this paper we analyze the RESCAL method and provethat it can not encode asymmetric transitive relations in knowledge bases.
arxiv-1605-04657 | Solve-Select-Scale: A Three Step Process For Sparse Signal Estimation |  http://arxiv.org/abs/1605.04657  | author:Mithun Das Gupta category:cs.IT cs.LG math.IT stat.ML published:2016-05-16 summary:In the theory of compressed sensing (CS), the sparsity $\x\_0$ of theunknown signal $\mathbf{x} \in \mathcal{R}^n$ is of prime importance and thefocus of reconstruction algorithms has mainly been either $\x\_0$ or itsconvex relaxation (via $\x\_1$). However, it is typically unknown in practiceand has remained a challenge when nothing about the size of the support isknown. As pointed recently, $\x\_0$ might not be the best metric to minimizedirectly, both due to its inherent complexity as well as its noise performance.Recently a novel stable measure of sparsity $s(\mathbf{x}) :=\\mathbf{x}\_1^2/\\mathbf{x}\_2^2$ has been investigated by Lopes\cite{Lopes2012}, which is a sharp lower bound on $\\mathbf{x}\_0$. Theestimation procedure for this measure uses only a small number of linearmeasurements, does not rely on any sparsity assumptions, and requires verylittle computation. The usage of the quantity $s(\mathbf{x})$ in sparse signalestimation problems has not received much importance yet. We develop the ideaof incorporating $s(\mathbf{x})$ into the signal estimation framework. We alsoprovide a three step algorithm to solve problems of the form $\mathbf{Ax=b}$with no additional assumptions on the original signal $\mathbf{x}$.
arxiv-1605-04624 | Learning to Rank Personalized Search Results in Professional Networks |  http://arxiv.org/abs/1605.04624  | author:Viet Ha-Thuc, Shakti Sinha category:cs.IR cs.LG published:2016-05-16 summary:LinkedIn search is deeply personalized - for the same queries, differentsearchers expect completely different results. This paper presents our approachto achieving this by mining various data sources available in LinkedIn to infersearchers' intents (such as hiring, job seeking, etc.), as well as extendingthe concept of homophily to capture the searcher-result similarities on manyaspects. Then, learning-to-rank (LTR) is applied to combine these signals withstandard search features.
arxiv-1605-04655 | Joint Learning of Sentence Embeddings for Relevance and Entailment |  http://arxiv.org/abs/1605.04655  | author:Petr Baudis, Silvestr Stanko, Jan Sedivy category:cs.CL cs.LG cs.NE published:2016-05-16 summary:We consider the problem of Recognizing Textual Entailment within anInformation Retrieval context, where we must simultaneously determine therelevancy as well as degree of entailment for individual pieces of evidence todetermine a yes/no answer to a binary natural language question. We compare several variants of neural networks for sentence embeddings in asetting of decision-making based on evidence of varying relevance. We propose abasic model to integrate evidence for entailment, show that joint training ofthe sentence embeddings to model relevance and entailment is feasible even withno explicit per-evidence supervision, and show the importance of evaluatingstrong baselines. We also demonstrate the benefit of carrying over textcomprehension model trained on an unrelated task for our small datasets. Our research is motivated primarily by a new open dataset we introduce,consisting of binary questions and news-based evidence snippets. We also applythe proposed relevance-entailment model on a similar task of rankingmultiple-choice test answers, evaluating it on a preliminary dataset of schooltest questions as well as the standard MCTest dataset, where we improve theneural model state-of-art.
arxiv-1605-04932 | Classification of Big Data with Application to Imaging Genetics |  http://arxiv.org/abs/1605.04932  | author:Magnus O. Ulfarsson, Frosti Palsson, Jakob Sigurdsson, Johannes R. Sveinsson category:cs.CV stat.ML published:2016-05-16 summary:Big data applications, such as medical imaging and genetics, typicallygenerate datasets that consist of few observations n on many more variables p,a scenario that we denote as p>>n. Traditional data processing methods areoften insufficient for extracting information out of big data. This calls forthe development of new algorithms that can deal with the size, complexity, andthe special structure of such datasets. In this paper, we consider the problemof classifying p>>n data and propose a classification method based on lineardiscriminant analysis (LDA). Traditional LDA depends on the covariance estimateof the data, but when p>>n the sample covariance estimate is singular. Theproposed method estimates the covariance by using a sparse version of noisyprincipal component analysis (nPCA). The use of sparsity in this setting aimsat automatically selecting variables that are relevant for classification. Inexperiments, the new method is compared to state-of-the art methods for bigdata problems using both simulated datasets and imaging genetics datasets.
arxiv-1605-04951 | Viziometrics: Analyzing Visual Information in the Scientific Literature |  http://arxiv.org/abs/1605.04951  | author:Po-shen Lee, Jevin D. West, Bill Howe category:cs.SI cs.CV cs.DL cs.IR published:2016-05-16 summary:Scientific results are communicated visually in the literature throughdiagrams, visualizations, and photographs. These information-dense objects havebeen largely ignored in bibliometrics and scientometrics studies when comparedto citations and text. In this paper, we use techniques from computer visionand machine learning to classify more than 8 million figures from PubMed into 5figure types and study the resulting patterns of visual information as theyrelate to impact. We find that the distribution of figures and figure types inthe literature has remained relatively constant over time, but can vary widelyacross field and topic. Remarkably, we find a significant correlation betweenscientific impact and the use of visual information, where higher impact paperstend to include more diagrams, and to a lesser extent more plots andphotographs. To explore these results and other ways of extracting this visualinformation, we have built a visual browser to illustrate the concept andexplore design alternatives for supporting viziometric analysis and organizingvisual information. We use these results to articulate a new research agenda --viziometrics -- to study the organization and presentation of visualinformation in the scientific literature.
arxiv-1605-04955 | Probing the Geometry of Data with Diffusion Fréchet Functions |  http://arxiv.org/abs/1605.04955  | author:Diego Hernán Díaz Martínez, Christine H. Lee, Peter T. Kim, Washington Mio category:stat.ML 62-07, 92C50 published:2016-05-16 summary:Many complex ecosystems, such as those formed by multiple microbial taxa,involve intricate interactions amongst various sub-communities. The most basicrelationships are frequently modeled as co-occurrence networks in which thenodes represent the various players in the community and the weighted edgesencode levels of interaction. In this setting, the composition of a communitymay be viewed as a probability distribution on the nodes of the network. Thispaper develops methods for modeling the organization of such data, as well astheir Euclidean counterparts, across spatial scales. Using the notion ofdiffusion distance, we introduce diffusion Fr\'echet functions and diffusionFr\'echet vectors associated with probability distributions on Euclidean spacesand the vertex set of a weighted network, respectively. We prove that thesefunctional statistics are stable with respect to the Wasserstein distancebetween probability measures, thus yielding robust descriptors of their shapes.We apply the methodology to investigate bacterial communities in the human gut,seeking to characterize divergence from intestinal homeostasis in patients withClostridium difficile infection (CDI) and the effects of fecal microbiotatransplantation, a treatment used in CDI patients that has proven to besignificantly more effective than traditional treatment with antibiotics. Theproposed method proves useful in deriving a biomarker that might help elucidatethe mechanisms that drive these processes.
arxiv-1605-04654 | Wavelet Scattering Regression of Quantum Chemical Energies |  http://arxiv.org/abs/1605.04654  | author:Matthew Hirn, Stéphane Mallat, Nicolas Poilvert category:math.CA quant-ph stat.ML published:2016-05-16 summary:We introduce multiscale invariant dictionaries to estimate quantum chemicalenergies of organic molecules, from training databases. Molecular energies areinvariant to isometric atomic displacements, and are Lipschitz continuous tomolecular deformations. Similarly to density functional theory (DFT), themolecule is represented by an electronic density function. A multiscaleinvariant dictionary is calculated with wavelet scattering invariants. Itcascades a first wavelet transform which separates scales, with a secondwavelet transform which computes interactions across scales. Sparse scatteringregressions give state of the art results over two databases of organic planarmolecules. On these databases, the regression error is of the order of theerror produced by DFT codes, but at a fraction of the computational cost.
arxiv-1605-04986 | A Constant-Factor Bi-Criteria Approximation Guarantee for $k$-means++ |  http://arxiv.org/abs/1605.04986  | author:Dennis Wei category:cs.LG cs.CG I.5.3; G.1.6 published:2016-05-16 summary:This paper studies the $k$-means++ algorithm for clustering as well as theclass of $D^\ell$ sampling algorithms to which $k$-means++ belongs. It is shownthat for any constant factor $\beta > 1$, selecting $\beta k$ cluster centersby $D^\ell$ sampling yields a constant-factor approximation to the optimalclustering with $k$ centers, in expectation and without conditions on thedataset. This result extends the previously known $O(\log k)$ guarantee for thecase $\beta = 1$ to the constant-factor bi-criteria regime. It also improvesupon an existing constant-factor bi-criteria result that holds only withconstant probability.
arxiv-1605-04988 | Going Deeper into Action Recognition: A Survey |  http://arxiv.org/abs/1605.04988  | author:Samitha Herath, Mehrtash Harandi, Fatih Porikli category:cs.CV published:2016-05-16 summary:We provide a detailed review of the work on human action recognition over thepast decade. We refer to "actions" as meaningful human motions. Starting withmethods that are based on handcrafted representations, we review the impact ofrevamped deep neural networks on action recognition. We follow a systematictaxonomy of action recognition approaches to present a coherent discussion overtheir improvements and fall-backs.
arxiv-1605-04652 | Fast and Accurate Performance Analysis of LTE Radio Access Networks |  http://arxiv.org/abs/1605.04652  | author:Anand Padmanabha Iyer, Ion Stoica, Mosharaf Chowdhury, Li Erran Li category:cs.DC cs.LG cs.NI published:2016-05-16 summary:An increasing amount of analytics is performed on data that is procured in areal-time fashion to make real-time decisions. Such tasks include simplereporting on streams to sophisticated model building. However, the practicalityof such analyses are impeded in several domains because they are faced with afundamental trade-off between data collection latency and analysis accuracy. In this paper, we study this trade-off in the context of a specific domain,Cellular Radio Access Networks (RAN). Our choice of this domain is influencedby its commonalities with several other domains that produce real-time data,our access to a large live dataset, and their real-time nature anddimensionality which makes it a natural fit for a popular analysis technique,machine learning (ML). We find that the latency accuracy trade-off can beresolved using two broad, general techniques: intelligent data grouping andtask formulations that leverage domain characteristics. Based on this, wepresent CellScope, a system that addresses this challenge by applying a domainspecific formulation and application of Multi-task Learning (MTL) to RANperformance analysis. It achieves this goal using three techniques: featureengineering to transform raw data into effective features, a PCA inspiredsimilarity metric to group data from geographically nearby base stationssharing performance commonalities, and a hybrid online-offline model forefficient model updates. Our evaluation of CellScope shows that its accuracyimprovements over direct application of ML range from 2.5x to 4.4x whilereducing the model update overhead by up to 4.8x. We have also used CellScopeto analyze a live LTE consisting of over 2 million subscribers for a period ofover 10 months, where it uncovered several problems and insights, some of thempreviously unknown.
arxiv-1605-04639 | Alternating optimization method based on nonnegative matrix factorizations for deep neural networks |  http://arxiv.org/abs/1605.04639  | author:Tetsuya Sakurai, Akira Imakura, Yuto Inoue, Yasunori Futamura category:cs.LG cs.NE stat.ML published:2016-05-16 summary:The backpropagation algorithm for calculating gradients has been widely usedin computation of weights for deep neural networks (DNNs). This method requiresderivatives of objective functions and has some difficulties findingappropriate parameters such as learning rate. In this paper, we propose a novelapproach for computing weight matrices of fully-connected DNNs by using twotypes of semi-nonnegative matrix factorizations (semi-NMFs). In this method,optimization processes are performed by calculating weight matricesalternately, and backpropagation (BP) is not used. We also present a method tocalculate stacked autoencoder using a NMF. The output results of theautoencoder are used as pre-training data for DNNs. The experimental resultsshow that our method using three types of NMFs attains similar error rates tothe conventional DNNs with BP.
arxiv-1605-04638 | Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient |  http://arxiv.org/abs/1605.04638  | author:Tianbao Yang, Lijun Zhang, Rong Jin, Jinfeng Yi category:cs.LG math.OC stat.ML published:2016-05-16 summary:This work focuses on dynamic regret of online convex optimization thatcompares the performance of online learning to a clairvoyant who knows thesequence of loss functions in advance and hence selects the minimizer of theloss function at each step. By assuming that the clairvoyant moves slowly(i.e., the minimizers change slowly), we present several improvedvariation-based upper bounds of the dynamic regret under the true and noisygradient feedback, which are {\it optimal} in light of the presented lowerbounds. The key to our analysis is to explore a regularity metric that measuresthe temporal changes in the clairvoyant's minimizers, to which we refer as {\itpath variation}. Firstly, we present a general lower bound in terms of the pathvariation, and then show that under full information or gradient feedback weare able to achieve an optimal dynamic regret. Secondly, we present a lowerbound with noisy gradient feedback and then show that we can achieve optimaldynamic regrets under a stochastic gradient feedback and two-point banditfeedback. Moreover, for a sequence of smooth loss functions that admit a smallvariation in the gradients, our dynamic regret under the two-point banditfeedback matches what is achieved with full information.
arxiv-1605-04553 | Treating Similarity with Respect: How to Evaluate Models of Meaning? |  http://arxiv.org/abs/1605.04553  | author:Dmitrijs Milajevs, Sascha Griffiths category:cs.CL published:2016-05-15 summary:Similarity is a core notion that is used in psychology, theoretical andcomputational linguistics. The similarity datasets that come from the twofields differ in design: psychological datasets are focused around a certaintopic such as fruit names; linguistic datasets contain words from variouscategories. The later makes humans assign low similarity scores to the wordsthat have nothing in common and to the words that have contrast in meaning,making similarity scores ambiguous. In this work we discuss the similaritycollection procedure for a multi-category dataset that avoids score ambiguityand suggest changes to the evaluation procedure to reflect the insights ofpsychological literature for word, phrase and sentence similarity. We suggestto ask humans to provide a list of commonalities and differences instead ofnumerical similarity scores and employ the structure of human judgements beyondpairwise similarity. We believe that the proposed approach will give rise todatasets that test meaning representation models more thoroughly with respectto the human treatment of similarity.
arxiv-1605-05588 | A Distributed Quaternion Kalman Filter With Applications to Fly-by-Wire Systems |  http://arxiv.org/abs/1605.05588  | author:Sayed Pouria Talebi category:cs.SY stat.AP stat.ML published:2016-05-15 summary:The introduction of automated flight control and management systems have madepossible aircraft designs that sacrifice arodynamic stability in order toincorporate stealth technology intro their shape, operate more efficiently, andare highly maneuverable. Therefore, modern flight management systems arereliant on multiple redundant sensors to monitor and control the rotations ofthe aircraft. To this end, a novel distributed quaternion Kalman filteringalgorithm is developed for tracking the rotation and orientation of an aircraftin the three-dimensional space. The algorithm is developed to distributecomputation among the sensors in a manner that forces them to consent to aunique solution while being robust to sensor and link failure, a desirablecharacteristic for flight management systems. In addition, the underlyingquaternion-valued state space model allows to avoid problems associated withgimbal lock. The performance of the developed algorithm is verified throughsimulations.
arxiv-1605-04603 | Improving the Neural Algorithm of Artistic Style |  http://arxiv.org/abs/1605.04603  | author:Roman Novak, Yaroslav Nikulin category:cs.CV published:2016-05-15 summary:In this work we investigate different avenues of improving the NeuralAlgorithm of Artistic Style (by Leon A. Gatys, Alexander S. Ecker and MatthiasBethge, arXiv:1508.06576). While showing great results when transferring homogeneous and repetitivepatterns, the original style representation often fails to capture more complexproperties, like having separate styles of foreground and background. Thisleads to visual artifacts and undesirable textures appearing in unexpectedregions when performing style transfer. We tackle this issue with a variety of approaches, mostly by modifying thestyle representation in order for it to capture more information and impose atighter constraint on the style transfer result. In our experiments, we subjectively evaluate our best method as producingfrom barely noticeable to significant improvements in the quality of styletransfer.
arxiv-1605-04614 | DeepLearningKit - an GPU Optimized Deep Learning Framework for Apple's iOS, OS X and tvOS developed in Metal and Swift |  http://arxiv.org/abs/1605.04614  | author:Amund Tveit, Torbjørn Morland, Thomas Brox Røst category:cs.LG cs.DC cs.NE published:2016-05-15 summary:In this paper we present DeepLearningKit - an open source framework thatsupports using pretrained deep learning models (convolutional neural networks)for iOS, OS X and tvOS. DeepLearningKit is developed in Metal in order toutilize the GPU efficiently and Swift for integration with applications, e.g.iOS-based mobile apps on iPhone/iPad, tvOS-based apps for the big screen, or OSX desktop applications. The goal is to support using deep learning modelstrained with popular frameworks such as Caffe, Torch, TensorFlow, Theano,Pylearn, Deeplearning4J and Mocha. Given the massive GPU resources and timerequired to train Deep Learning models we suggest an App Store like model todistribute and download pretrained and reusable Deep Learning models.
arxiv-1605-04481 | Bias and Agreement in Syntactic Annotations |  http://arxiv.org/abs/1605.04481  | author:Yevgeni Berzak, Yan Huang, Andrei Barbu, Anna Korhonen, Boris Katz category:cs.CL published:2016-05-15 summary:We present a study on two key characteristics of human syntactic annotations:anchoring and agreement. Anchoring is a well known cognitive bias in humandecision making, where judgments are drawn towards pre-existing values. Westudy the influence of anchoring on a standard approach to creation ofsyntactic resources where syntactic annotations are obtained via human editingof tagger and parser output. Our experiments demonstrate a clear anchoringeffect and reveal unwanted consequences, including overestimation of parsingperformance and lower quality of annotations in comparison with human-basedannotations. Using sentences from the Penn Treebank WSJ, we also report thefirst systematically obtained inter-annotator agreement estimates for Englishsyntactic parsing. Our agreement results control for anchoring bias, and areconsequential in that they are \emph{on par} with state of the art parsingperformance for English. We discuss the impact of our findings on strategiesfor future annotation efforts and parser evaluations.
arxiv-1605-04502 | Joint Learning of Convolutional Neural Networks and Temporally Constrained Metrics for Tracklet Association Based on Large-Scale Datasets |  http://arxiv.org/abs/1605.04502  | author:Bing Wang, Kap Luk Chan, Li Wang, Bing Shuai, Zhen Zuo, Ting Liu, Gang Wang category:cs.CV published:2016-05-15 summary:In this paper, we study the challenging problem of multi-object tracking in acomplex scene captured by a single camera. Different from the existing trackletassociation-based tracking methods, we propose a novel and efficient way toobtain discriminative appearance-based tracklet affinity models. Our proposedmethod jointly learns the convolutional neural networks (CNNs) and temporallyconstrained metrics. In our method, a siamese convolutional neural network(CNN) is first pre-trained on the auxiliary data. Then the siamese CNN andtemporally constrained metrics are jointly learned online to construct theappearance-based tracklet affinity models. The proposed method can jointlylearn the hierarchical deep features and temporally constrained segment-wisemetrics under a unified framework. For reliable association between tracklets,a novel loss function incorporating temporally constrained multi-task learningmechanism is proposed. By employing the proposed method, tracklet associationcan be accomplished even in challenging situations. Moreover, a large-scaledataset with 40 fully annotated sequences is created to facilitate the trackingevaluation. Experimental results on five public datasets and the newlarge-scale dataset show that our method outperforms several state-of-the-artapproaches in multi-object tracking.
arxiv-1605-04515 | Machine Translation Evaluation: A Survey |  http://arxiv.org/abs/1605.04515  | author:Aaron Li-Feng Han, Khalil Sima'an, Derek Fai Wong category:cs.CL I.2.7; I.2.1 published:2016-05-15 summary:This paper introduces the state-of-the-art MT evaluation survey that containsboth manual and automatic evaluation methods. The traditional human evaluationcriteria mainly include the intelligibility, fidelity, fluency, adequacy,comprehension, and informativeness. We classify the automatic evaluationmethods into two categories, including lexical similarity and linguisticfeatures application. The lexical similarity methods contain edit distance,precision, recall, and word order, etc. The linguistic features can be dividedinto syntactic features and semantic features. Subsequently, we also introducethe evaluation methods for MT evaluation and the recent quality estimationtasks for MT.
arxiv-1605-04569 | Syntactically Guided Neural Machine Translation |  http://arxiv.org/abs/1605.04569  | author:Felix Stahlberg, Eva Hasler, Aurelien Waite, Bill Byrne category:cs.CL published:2016-05-15 summary:We investigate the use of hierarchical phrase-based SMT lattices inend-to-end neural machine translation (NMT). Weight pushing transforms theHiero scores for complete translation hypotheses, with the full translationgrammar score and full n-gram language model score, into posteriors compatiblewith NMT predictive probabilities. With a slightly modified NMT beam-searchdecoder we find gains over both Hiero and NMT decoding alone, with practicaladvantages in extending NMT to very large input and output vocabularies.
arxiv-1605-04469 | Rationale-Augmented Convolutional Neural Networks for Text Classification |  http://arxiv.org/abs/1605.04469  | author:Ye Zhang, Iain Marshall, Byron C. Wallace category:cs.CL published:2016-05-14 summary:We present a new Convolutional Neural Network (CNN) model for textclassification that jointly exploits labels on documents and their componentsentences. Specifically, we consider scenarios in which annotators explicitlymark sentences (or snippets) that support their overall documentcategorization, i.e., they provide rationales. Our model uses such supervisionvia a hierarchical approach in which each document is represented by a linearcombination of the vector representations of its constituent sentences. Wepropose a sentence-level convolutional model that estimates the probabilitythat a given sentence is a rationale, and we then scale the contribution ofeach sentence to the aggregate document representation in proportion to theseestimates. Experiments on five classification datasets that have documentlabels and associated rationales demonstrate that our approach consistentlyoutperforms strong baselines. Moreover, our model naturally providesexplanations for its predictions.
arxiv-1605-04466 | Generalized Linear Models for Aggregated Data |  http://arxiv.org/abs/1605.04466  | author:Avradeep Bhowmik, Joydeep Ghosh, Oluwasanmi Koyejo category:stat.ML cs.AI cs.LG published:2016-05-14 summary:Databases in domains such as healthcare are routinely released to the publicin aggregated form. Unfortunately, naive modeling with aggregated data maysignificantly diminish the accuracy of inferences at the individual level. Thispaper addresses the scenario where features are provided at the individuallevel, but the target variables are only available as histogram aggregates ororder statistics. We consider a limiting case of generalized linear modelingwhen the target variables are only known up to permutation, and explore howthis relates to permutation testing; a standard technique for assessingstatistical dependency. Based on this relationship, we propose a simplealgorithm to estimate the model parameters and individual level inferences viaalternating imputation and standard generalized linear model fitting. Ourresults suggest the effectiveness of the proposed approach when, in theoriginal data, permutation testing accurately ascertains the veracity of thelinear relationship. The framework is extended to general histogram data withlarger bins - with order statistics such as the median as a limiting case. Ourexperimental results on simulated data and aggregated healthcare data suggest adiminishing returns property with respect to the granularity of the histogram -when a linear relationship holds in the original data, the targets can bepredicted accurately given relatively coarse histograms.
arxiv-1605-04465 | Monotone Retargeting for Unsupervised Rank Aggregation with Object Features |  http://arxiv.org/abs/1605.04465  | author:Avradeep Bhowmik, Joydeep Ghosh category:stat.ML cs.AI cs.LG published:2016-05-14 summary:Learning the true ordering between objects by aggregating a set of expertopinion rank order lists is an important and ubiquitous problem in manyapplications ranging from social choice theory to natural language processingand search aggregation. We study the problem of unsupervised rank aggregationwhere no ground truth ordering information in available, neither about the truepreference ordering between any set of objects nor about the quality ofindividual rank lists. Aggregating the often inconsistent and poor quality ranklists in such an unsupervised manner is a highly challenging problem, andstandard consensus-based methods are often ill-defined, and difficult to solve.In this manuscript we propose a novel framework to bypass these issues by usingobject attributes to augment the standard rank aggregation framework. We designalgorithms that learn joint models on both rank lists and object features toobtain an aggregated rank ordering that is more accurate and robust, and alsohelps weed out rank lists of dubious validity. We validate our techniques onsynthetic datasets where our algorithm is able to estimate the true rankordering even when the rank lists are corrupted. Experiments on three realdatasets, MQ2008, MQ2008 and OHSUMED, show that using object features canresult in significant improvement in performance over existing rank aggregationmethods that do not use object information. Furthermore, when at least some ofthe rank lists are of high quality, our methods are able to effectively exploittheir high expertise to output an aggregated rank ordering of great accuracy.
arxiv-1605-04462 | Natural Language Processing for Mental Health: Large Scale Discourse Analysis of Counseling Conversations |  http://arxiv.org/abs/1605.04462  | author:Tim Althoff, Kevin Clark, Jure Leskovec category:cs.CL cs.CY cs.SI published:2016-05-14 summary:Mental illness is one of the most pressing public health issues of our time.While counseling and psychotherapy can be effective treatments, our knowledgeabout how to conduct successful counseling conversations has been limited dueto lack of large-scale data with labeled outcomes of the conversations. In thispaper, we present a large-scale, quantitative study on the discourse oftext-message-based counseling conversations. We develop a set of novelcomputational discourse analysis methods to measure how various linguisticaspects of conversations are correlated with conversation outcomes. Applyingtechniques such as sequence-based conversation models, language modelcomparisons, message clustering, and psycholinguistics-inspired word frequencyanalyses, we discover actionable conversation strategies that are associatedwith better conversation outcomes.
arxiv-1605-04435 | Proceedings of the 5th Workshop on Machine Learning and Interpretation in Neuroimaging (MLINI) at NIPS 2015 |  http://arxiv.org/abs/1605.04435  | author:I. Rish, L. Wehbe, G. Langs, M. Grosse-Wentrup, B. Murphy, G. Cecchi category:stat.ML published:2016-05-14 summary:This volume is a collection of contributions from the 5th Workshop on MachineLearning and Interpretation in Neuroimaging (MLINI) at the Neural InformationProcessing Systems (NIPS 2015) conference. Modern multivariate statisticalmethods developed in the rapidly growing field of machine learning are beingincreasingly applied to various problems in neuroimaging, from cognitive statedetection to clinical diagnosis and prognosis. Multivariate pattern analysismethods are designed to examine complex relationships between high-dimensionalsignals, such as brain images, and outcomes of interest, such as the categoryof a stimulus, a type of a mental state of a subject, or a specific mentaldisorder. Such techniques are in contrast with the traditional mass-univariateapproaches that dominated neuroimaging in the past and treated each individualimaging measurement in isolation. We believe that machine learning has a prominent role in shaping howquestions in neuroscience are framed, and that the machine-learning mind set isnow entering modern psychology and behavioral studies. It is also equallyimportant that practical applications in these fields motivate a rapidlyevolving line or research in the machine learning community. In parallel, thereis an intense interest in learning more about brain function in the context ofrich naturalistic environments and scenes. Efforts to go beyond highly specificparadigms that pinpoint a single function, towards schemes for measuring theinteraction with natural and more varied scene are made. The goal of theworkshop is to pinpoint the most pressing issues and common challenges acrossthe neuroscience, neuroimaging, psychology and machine learning fields, and tosketch future directions and open questions in the light of novel methodology.
arxiv-1605-04369 | Neural Dataset Generality |  http://arxiv.org/abs/1605.04369  | author:Ragav Venkatesan, Vijetha Gattupalli, Baoxin Li category:cs.CV published:2016-05-14 summary:Often the filters learned by Convolutional Neural Networks (CNNs) fromdifferent datasets appear similar. This is prominent in the first few layers.This similarity of filters is being exploited for the purposes of transferlearning and some studies have been made to analyse such transferability offeatures. This is also being used as an initialization technique for differenttasks in the same dataset or for the same task in similar datasets.Off-the-shelf CNN features have capitalized on this idea to promote theirnetworks as best transferable and most general and are used in a cavaliermanner in day-to-day computer vision tasks. It is curious that while the filters learned by these CNNs are related to theatomic structures of the images from which they are learnt, all datasets learnsimilar looking low-level filters. With the understanding that a dataset thatcontains many such atomic structures learn general filters and are thereforeuseful to initialize other networks with, we propose a way to analyse andquantify generality among datasets from their accuracies on transferredfilters. We applied this metric on several popular character recognition,natural image and a medical image dataset, and arrived at some interestingconclusions. On further experimentation we also discovered that particularclasses in a dataset themselves are more general than others.
arxiv-1605-04359 | Occurrence Statistics of Entities, Relations and Types on the Web |  http://arxiv.org/abs/1605.04359  | author:Aman Madaan, Sunita Sarawagi category:cs.CL published:2016-05-14 summary:The problem of collecting reliable estimates of occurrence of entities on theopen web forms the premise for this report. The models learned for taggingentities cannot be expected to perform well when deployed on the web. This isowing to the severe mismatch in the distributions of such entities on the weband in the relatively diminutive training data. In this report, we build up thecase for maximum mean discrepancy for estimation of occurrence statistics ofentities on the web, taking a review of named entity disambiguation techniquesand related concepts along the way.
arxiv-1605-04475 | Capturing divergence in dependency trees to improve syntactic projection |  http://arxiv.org/abs/1605.04475  | author:Ryan Georgi, Fei Xia, William D. Lewis category:cs.CL published:2016-05-14 summary:Obtaining syntactic parses is a crucial part of many NLP pipelines. However,most of the world's languages do not have large amounts of syntacticallyannotated corpora available for building parsers. Syntactic projectiontechniques attempt to address this issue by using parallel corpora consistingof resource-poor and resource-rich language pairs, taking advantage of a parserfor the resource-rich language and word alignment between the languages toproject the parses onto the data for the resource-poor language. Theseprojection methods can suffer, however, when the two languages are divergent.In this paper, we investigate the possibility of using small, parallel,annotated corpora to automatically detect divergent structural patterns betweentwo languages. These patterns can then be used to improve structural projectionalgorithms, allowing for better performing NLP tools for resource-poorlanguages, in particular those that may not have large amounts of annotateddata necessary for traditional, fully-supervised methods. While this detectionprocess is not exhaustive, we demonstrate that common patterns of divergencecan be identified automatically without prior knowledge of a given languagepair, and the patterns can be used to improve performance of projectionalgorithms.
arxiv-1605-04478 | Gabor Barcodes for Medical Image Retrieval |  http://arxiv.org/abs/1605.04478  | author:Mina Nouredanesh, Hamid R. Tizhoosh, Ershad Banijamali category:cs.CV published:2016-05-14 summary:In recent years, advances in medical imaging have led to the emergence ofmassive databases, containing images from a diverse range of modalities. Thishas significantly heightened the need for automated annotation of the images onone side, and fast and memory-efficient content-based image retrieval systemson the other side. Binary descriptors have recently gained more attention as apotential vehicle to achieve these goals. One of the recently introduced binarydescriptors for tagging of medical images are Radon barcodes (RBCs) that aredriven from Radon transform via local thresholding. Gabor transform is also apowerful transform to extract texture-based information. Gabor features haveexhibited robustness against rotation, scale, and also photometricdisturbances, such as illumination changes and image noise in manyapplications. This paper introduces Gabor Barcodes (GBCs), as a novel frameworkfor the image annotation. To find the most discriminative GBC for a given queryimage, the effects of employing Gabor filters with different parameters, i.e.,different sets of scales and orientations, are investigated, resulting indifferent barcode lengths and retrieval performances. The proposed method hasbeen evaluated on the IRMA dataset with 193 classes comprising of 12,677 x-rayimages for indexing, and 1,733 x-rays images for testing. A total error scoreas low as $351$ ($\approx 80\%$ accuracy for the first hit) was achieved.
arxiv-1605-04129 | With Whom Do I Interact? Detecting Social Interactions in Egocentric Photo-streams |  http://arxiv.org/abs/1605.04129  | author:Maedeh Aghaei, Mariella Dimiccoli, Petia Radeva category:cs.CV published:2016-05-13 summary:Given a user wearing a low frame rate wearable camera during a day, this workaims to automatically detect the moments when the user gets engaged into asocial interaction solely by reviewing the automatically captured photos by theworn camera. The proposed method, inspired by the sociological concept ofF-formation, exploits distance and orientation of the appearing individuals-with respect to the user- in the scene from a bird-view perspective. As aresult, the interaction pattern over the sequence can be understood as atwo-dimensional time series that corresponds to the temporal evolution of thedistance and orientation features over time. A Long-Short Term Memory-basedRecurrent Neural Network is then trained to classify each time series.Experimental evaluation over a dataset of 30.000 images has shown promisingresults on the proposed method for social interaction detection in egocentricphoto-streams.
arxiv-1605-04131 | Barzilai-Borwein Step Size for Stochastic Gradient Descent |  http://arxiv.org/abs/1605.04131  | author:Conghui Tan, Shiqian Ma, Yu-Hong Dai, Yuqiu Qian category:math.OC cs.LG stat.ML published:2016-05-13 summary:One of the major issues in stochastic gradient descent (SGD) methods is howto choose an appropriate step size while running the algorithm. Since thetraditional line search technique does not apply for stochastic optimizationalgorithms, the common practice in SGD is either to use a diminishing stepsize, or to tune a fixed step size by hand. Apparently, these two approachescan be time consuming in practice. In this paper, we propose to use theBarzilai-Borwein (BB) method to automatically compute step sizes for SGD andits variant: stochastic variance reduced gradient (SVRG) method, which leads totwo algorithms: SGD-BB and SVRG-BB. We prove that SVRG-BB converges linearlyfor strongly convex objective function. As a by-product, we prove the linearconvergence result of SVRG with Option I proposed in [10], whose convergenceresult has been missing in the literature. Numerical experiments on standarddata sets show that the performance of SGD-BB and SVRG-BB is comparable to andsometimes even better than SGD and SVRG with best-tuned step sizes, and issuperior to some advanced SGD variants.
arxiv-1605-04238 | Semantic Spaces |  http://arxiv.org/abs/1605.04238  | author:Yuri Manin, Matilde Marcolli category:cs.CL 68Q55, 14M15 published:2016-05-13 summary:Any natural language can be considered as a tool for producing largedatabases (consisting of texts, written, or discursive). This tool for itsdescription in turn requires other large databases (dictionaries, grammarsetc.). Nowadays, the notion of database is associated with computer processingand computer memory. However, a natural language resides also in human brainsand functions in human communication, from interpersonal to intergenerationalone. We discuss in this survey/research paper mathematical, in particulargeometric, constructions, which help to bridge these two worlds. In particular,in this paper we consider the Vector Space Model of semantics based onfrequency matrices, as used in Natural Language Processing. We investigateunderlying geometries, formulated in terms of Grassmannians, projective spaces,and flag varieties. We formulate the relation between vector space models andsemantic spaces based on semic axes in terms of projectability of subvarietiesin Grassmannians and projective spaces. We interpret Latent Semantics as ageometric flow on Grassmannians. We also discuss how to formulate G\"ardenfors'notion of "meeting of minds" in our geometric setting.
arxiv-1605-04034 | Transfer Hashing with Privileged Information |  http://arxiv.org/abs/1605.04034  | author:Joey Tianyi Zhou, Xinxing Xu, Sinno Jialin Pan, Ivor W. Tsang, Zheng Qin, Rick Siow Mong Goh category:cs.LG stat.ML published:2016-05-13 summary:Most existing learning to hash methods assume that there are sufficient data,either labeled or unlabeled, on the domain of interest (i.e., the targetdomain) for training. However, this assumption cannot be satisfied in somereal-world applications. To address this data sparsity issue in hashing,inspired by transfer learning, we propose a new framework named TransferHashing with Privileged Information (THPI). Specifically, we extend thestandard learning to hash method, Iterative Quantization (ITQ), in a transferlearning manner, namely ITQ+. In ITQ+, a new slack function is learned fromauxiliary data to approximate the quantization error in ITQ. We developed analternating optimization approach to solve the resultant optimization problemfor ITQ+. We further extend ITQ+ to LapITQ+ by utilizing the geometry structureamong the auxiliary data for learning more precise binary codes in the targetdomain. Extensive experiments on several benchmark datasets verify theeffectiveness of our proposed approaches through comparisons with severalstate-of-the-art baselines.
arxiv-1605-04243 | Simultaneous Surface Reflectance and Fluorescence Spectra Estimation |  http://arxiv.org/abs/1605.04243  | author:Henryk Blasinski, Joyce Farrell, Brian Wandell category:cs.CV published:2016-05-13 summary:There is widespread interest in estimating the fluorescence properties ofnatural materials in an image. However, the separation between reflected andfluoresced components is difficult, because it is impossible to distinguishreflected and fluoresced photons without controlling the illuminant spectrum.We show how to jointly estimate the reflectance and fluorescence from a singleset of images acquired under multiple illuminants. We present a framework basedon a linear approximation to the physical equations describing image formationin terms of surface spectral reflectance and fluorescence due to multiplefluorophores. We relax the non-convex, inverse estimation problem in order tojointly estimate the reflectance and fluorescence properties in a singleoptimization step and we use the Alternating Direction Method of Multipliers(ADMM) approach to efficiently find a solution. We provide a softwareimplementation of the solver for our method and prior methods. We evaluate theaccuracy and reliability of the method using both simulations and experimentaldata. To acquire data to test the methods, we built a custom imaging systemusing a monochrome camera, a filter wheel with bandpass transmissive filtersand a small number of light emitting diodes. We compared the system andalgorithm performance with the ground truth as well as with prior methods. Ourapproach produces lower errors compared to earlier algorithms.
arxiv-1605-04250 | Color Homography |  http://arxiv.org/abs/1605.04250  | author:Graham Finlayson, Han Gong, Robert Fisher category:cs.CV published:2016-05-13 summary:We show the surprising result that colors across a change in viewingcondition (changing light color, shading and camera) are related by ahomography. Our homography color correction application delivers improved colorfidelity compared with the linear least-square.
arxiv-1605-04253 | An Empirical Study and Analysis of Generalized Zero-Shot Learning for Object Recognition in the Wild |  http://arxiv.org/abs/1605.04253  | author:Wei-Lun Chao, Soravit Changpinyo, Boqing Gong, Fei Sha category:cs.CV published:2016-05-13 summary:We investigate the problem of generalized zero-shot learning (GZSL). GZSLrelaxes the unrealistic assumption in conventional ZSL that test data belongonly to unseen novel classes. In GZSL, test data might also come from seenclasses and the labeling space is the union of both types of classes. We showempirically that a straightforward application of the classifiers provided byexisting ZSL approaches does not perform well in the setting of GZSL. Motivatedby this, we propose a surprisingly simple but effective method to adapt ZSLapproaches for GZSL. The main idea is to introduce a calibration factor tocalibrate the classifiers for both seen and unseen classes so as to balance twoconflicting forces: recognizing data from seen classes and those from unseenones. We develop a new performance metric called the Area Under Seen-Unseenaccuracy Curve to characterize this tradeoff. We demonstrate the utility ofthis metric by analyzing existing ZSL approaches applied to the generalizedsetting. Extensive empirical studies reveal strengths and weaknesses of thoseapproaches on three well-studied benchmark datasets, including the large-scaleImageNet Full 2011 with 21,000 unseen categories. We complement our comparativestudies in learning methods by further establishing an upper-bound on theperformance limit of GZSL. There, our idea is to use class-representativevisual features as the idealized semantic embeddings. We show that there is alarge gap between the performance of existing approaches and the performancelimit, suggesting that improving the quality of class semantic embeddings isvital to improving zero-shot learning.
arxiv-1605-04262 | ABtree: An Algorithm for Subgroup-Based Treatment Assignment |  http://arxiv.org/abs/1605.04262  | author:Derek Feng, Xiaofei Wang category:stat.ML published:2016-05-13 summary:Given two possible treatments, there may exist subgroups who benefit greaterfrom one treatment than the other. This problem is relevant to the field ofmarketing, where treatments may correspond to different ways of selling aproduct. It is similarly relevant to the field of public policy, wheretreatments may correspond to specific government programs. And finally,personalized medicine is a field wholly devoted to understanding whichsubgroups of individuals will benefit from particular medical treatments. Wepresent a computationally fast tree-based method, ABtree, for treatment effectdifferentiation. Unlike other methods, ABtree specifically produces decisionrules for optimal treatment assignment on a per-individual basis. The treatmentchoices are selected for maximizing the overall occurrence of a desired binaryoutcome, conditional on a set of covariates. In this poster, we present themethodology on tree growth and pruning, and show performance results whenapplied to simulated data as well as real data.
arxiv-1605-04122 | Natural Language Semantics and Computability |  http://arxiv.org/abs/1605.04122  | author:Richard Moot, Christian Retoré category:cs.CL cs.AI cs.CC published:2016-05-13 summary:This paper is a reflexion on the computability of natural language semantics.It does not contain a new model or new results in the formal semantics ofnatural language: it is rather a computational analysis of the logical modelsand algorithms currently used in natural language semantics, defined as themapping of a statement to logical formulas - formulas, because a statement canbe ambiguous. We argue that as long as possible world semantics is left out,one can compute the semantic representation(s) of a given statement, includingaspects of lexical meaning. We also discuss the algorithmic complexity of thisprocess.
arxiv-1605-04278 | Universal Dependencies for Learner English |  http://arxiv.org/abs/1605.04278  | author:Yevgeni Berzak, Jessica Kenney, Carolyn Spadine, Jing Xian Wang, Lucia Lam, Keiko Sophie Mori, Sebastian Garza, Boris Katz category:cs.CL published:2016-05-13 summary:We introduce the Treebank of Learner English (TLE), the first publiclyavailable syntactic treebank for English as a Second Language (ESL). The TLEprovides manually annotated POS tags and Universal Dependency (UD) trees for5,124 sentences from the Cambridge First Certificate in English (FCE) corpus.The UD annotations are tied to a pre-existing error annotation of the FCE,whereby full syntactic analyses are provided for both the original and errorcorrected versions of each sentence. Further on, we delineate ESL annotationguidelines that allow for consistent syntactic treatment of ungrammaticalEnglish. Finally, we benchmark POS tagging and dependency parsing performanceon the TLE dataset and measure the effect of grammatical errors on parsingaccuracy. We envision the treebank to support a wide range of linguistic andcomputational research on second language acquisition as well as automaticprocessing of ungrammatical language.
arxiv-1605-04337 | Support Vector Algorithms for Optimizing the Partial Area Under the ROC Curve |  http://arxiv.org/abs/1605.04337  | author:Harikrishna Narasimhan, Shivani Agarwal category:cs.LG published:2016-05-13 summary:The area under the ROC curve (AUC) is a widely used performance measure inmachine learning. Increasingly, however, in several applications, ranging fromranking to biometric screening to medicine, performance is measured not interms of the full area under the ROC curve, but in terms of the \emph{partial}area under the ROC curve between two false positive rates. In this paper, wedevelop support vector algorithms for directly optimizing the partial AUCbetween any two false positive rates. Our methods are based on minimizing asuitable proxy or surrogate objective for the partial AUC error. In the case ofthe full AUC, one can readily construct and optimize convex surrogates byexpressing the performance measure as a summation of pairwise terms. Thepartial AUC, on the other hand, does not admit such a simple decomposablestructure, making it more challenging to design and optimize (tight) convexsurrogates for this measure. Our approach builds on the structural SVM framework of Joachims (2005) todesign convex surrogates for partial AUC, and solves the resulting optimizationproblem using a cutting plane solver. Unlike the full AUC, where thecombinatorial optimization needed in each iteration of the cutting plane solvercan be decomposed and solved efficiently, the corresponding problem for thepartial AUC is harder to decompose. One of our main contributions is apolynomial time algorithm for solving the combinatorial optimization problemassociated with partial AUC. We also develop an approach for optimizing atighter non-convex hinge loss based surrogate for the partial AUC usingdifference-of-convex programming. Our experiments on a variety of real-worldand benchmark tasks confirm the efficacy of the proposed methods.
arxiv-1605-04074 | Wisdom of Crowds cluster ensemble |  http://arxiv.org/abs/1605.04074  | author:Hosein Alizadeh, Muhammad Yousefnezhad, Behrouz Minaei Bidgoli category:stat.ML cs.AI cs.SI published:2016-05-13 summary:The Wisdom of Crowds is a phenomenon described in social science thatsuggests four criteria applicable to groups of people. It is claimed that, ifthese criteria are satisfied, then the aggregate decisions made by a group willoften be better than those of its individual members. Inspired by this concept,we present a novel feedback framework for the cluster ensemble problem, whichwe call Wisdom of Crowds Cluster Ensemble (WOCCE). Although many conventionalcluster ensemble methods focusing on diversity have recently been proposed,WOCCE analyzes the conditions necessary for a crowd to exhibit this collectivewisdom. These include decentralization criteria for generating primary results,independence criteria for the base algorithms, and diversity criteria for theensemble members. We suggest appropriate procedures for evaluating thesemeasures, and propose a new measure to assess the diversity. We evaluate theperformance of WOCCE against some other traditional base algorithms as well asstate-of-the-art ensemble methods. The results demonstrate the efficiency ofWOCCE's aggregate decision-making compared to other algorithms.
arxiv-1605-04072 | Towards Empathetic Human-Robot Interactions |  http://arxiv.org/abs/1605.04072  | author:Pascale Fung, Dario Bertero, Yan Wan, Anik Dey, Ricky Ho Yin Chan, Farhad Bin Siddique, Yang Yang, Chien-Sheng Wu, Ruixi Lin category:cs.CL cs.AI cs.HC cs.RO published:2016-05-13 summary:Since the late 1990s when speech companies began providing theircustomer-service software in the market, people have gotten used to speaking tomachines. As people interact more often with voice and gesture controlledmachines, they expect the machines to recognize different emotions, andunderstand other high level communication features such as humor, sarcasm andintention. In order to make such communication possible, the machines need anempathy module in them which can extract emotions from human speech andbehavior and can decide the correct response of the robot. Although research onempathetic robots is still in the early stage, we described our approach usingsignal processing techniques, sentiment analysis and machine learningalgorithms to make robots that can "understand" human emotion. We propose Zarathe Supergirl as a prototype system of empathetic robots. It is a softwarebased virtual android, with an animated cartoon character to present itself onthe screen. She will get "smarter" and more empathetic through its deeplearning algorithms, and by gathering more data and learning from it. In thispaper, we present our work so far in the areas of deep learning of emotion andsentiment recognition, as well as humor recognition. We hope to explore thefuture direction of android development and how it can help improve people'slives.
arxiv-1605-04070 | A Reinforcement Learning System to Encourage Physical Activity in Diabetes Patients |  http://arxiv.org/abs/1605.04070  | author:Irit Hochberg, Guy Feraru, Mark Kozdoba, Shie Mannor, Moshe Tennenholtz, Elad Yom-Tov category:cs.CY cs.LG published:2016-05-13 summary:Regular physical activity is known to be beneficial to people suffering fromdiabetes type 2. Nevertheless, most such people are sedentary. Smartphonescreate new possibilities for helping people to adhere to their physicalactivity goals, through continuous monitoring and communication, coupled withpersonalized feedback. We provided 27 sedentary diabetes type 2 patients with a smartphone-basedpedometer and a personal plan for physical activity. Patients were sent SMSmessages to encourage physical activity between once a day and once per week.Messages were personalized through a Reinforcement Learning (RL) algorithmwhich optimized messages to improve each participant's compliance with theactivity regimen. The RL algorithm was compared to a static policy for sendingmessages and to weekly reminders. Our results show that participants who received messages generated by the RLalgorithm increased the amount of activity and pace of walking, while thecontrol group patients did not. Patients assigned to the RL algorithm groupexperienced a superior reduction in blood glucose levels (HbA1c) compared tocontrol policies, and longer participation caused greater reductions in bloodglucose levels. The learning algorithm improved gradually in predicting whichmessages would lead participants to exercise. Our results suggest that a mobile phone application coupled with a learningalgorithm can improve adherence to exercise in diabetic patients. As a learningalgorithm is automated, and delivers personalized messages, it could be used inlarge populations of diabetic patients to improve health and glycemic control.Our results can be expanded to other areas where computer-led health coachingof humans may have a positive impact.
arxiv-1605-04068 | Fast Semantic Image Segmentation with High Order Context and Guided Filtering |  http://arxiv.org/abs/1605.04068  | author:Falong Shen, Gang Zeng category:cs.CV published:2016-05-13 summary:This paper describes a fast and accurate semantic image segmentation approachthat encodes not only the discriminative features from deep neural networks,but also the high-order context compatibility among adjacent objects as well aslow level image features. We formulate the underlying problem as theconditional random field that embeds local feature extraction, clique potentialconstruction, and guided filtering within the same framework, and provide anefficient coarse-to-fine solver. At the coarse level, we combine local featurerepresentation and context interaction using a deep convolutional network, anddirectly learn the interaction from high order cliques with a message passingroutine, avoiding time-consuming explicit graph inference for joint probabilitydistribution. At the fine level, we introduce a guided filtering interpretationfor the mean field algorithm, and achieve accurate object boundaries with 100+faster than classic learning methods. The two parts are connected and jointlytrained in an end-to-end fashion. Experimental results on Pascal VOC 2012dataset have shown that the proposed algorithm outperforms thestate-of-the-art, and that it achieves the rank 1 performance at the time ofsubmission, both of which prove the effectiveness of this unified framework forsemantic image segmentation.
arxiv-1605-04135 | Stochastic Optimization Techniques for Quantification Performance Measures |  http://arxiv.org/abs/1605.04135  | author:Shuai Li, Harikrishna Narasimhan, Purushottam Kar, Sanjay Chawla, Fabrizio Sebastiani category:stat.ML cs.AI cs.IR cs.LG published:2016-05-13 summary:The estimation of class prevalence, i.e., the fraction of the population thatbelongs to a certain class, is a very useful tool in data analytics andlearning, and finds applications in many domains, such as sentiment analysis,epidemiology, etc. For example, in sentiment analysis, the objective is oftennot to estimate whether a specific text conveys positive or negative sentiment,but rather estimate the overall distribution of positive and negative sentimentduring an event window. A popular way of performing the above task, oftendubbed quantification, is to use supervised learning to train a prevalenceestimator from labelled data. In this paper we propose the first onlinestochastic algorithms for directly optimizing (i) performance measures forquantification, and (ii) hybrid performance measures that seek to balancequantification and classification performance. We prove rigorous bounds for ouralgorithms which demonstrate that they exhibit optimal convergence. Ouralgorithms present a significant advancement in the theory of multivariateoptimization. We also report extensive experiments on benchmark and real datasets which demonstrate that our methods significantly outperform existingoptimization techniques used for the quantification problem.
arxiv-1605-04056 | Causal Discovery for Manufacturing Domains |  http://arxiv.org/abs/1605.04056  | author:Katerina Marazopoulou, Rumi Ghosh, Prasanth Lade, David Jensen category:cs.LG cs.AI published:2016-05-13 summary:Yield and quality improvement is of paramount importance to any manufacturingcompany. One of the ways of improving yield is through discovery of the rootcausal factors affecting yield. We propose the use of data-driven interpretablecausal models to identify key factors affecting yield. We focus on factors thatare measured in different stages of production and testing in the manufacturingcycle of a product. We apply causal structure learning techniques on real datacollected from this line. Specifically, the goal of this work is to learninterpretable causal models from observational data produced by manufacturinglines. Emphasis has been given to the interpretability of the models to make themactionable in the field of manufacturing. We highlight the challenges presentedby assembly line data and propose ways to alleviate them.We also identifyunique characteristics of data originating from assembly lines and how toleverage them in order to improve causal discovery. Standard evaluationtechniques for causal structure learning shows that the learned causal modelsseem to closely represent the underlying latent causal relationship betweendifferent factors in the production process. These results were also validatedby manufacturing domain experts who found them promising. This workdemonstrates how data mining and knowledge discovery can be used for root causeanalysis in the domain of manufacturing and connected industry.
arxiv-1605-04046 | Track Extraction with Hidden Reciprocal Chain Models |  http://arxiv.org/abs/1605.04046  | author:George Stamatescu, Langford B White, Riley Bruce-Doust category:cs.CV published:2016-05-13 summary:This paper develops Bayesian track extraction algorithms for targets modelledas hidden reciprocal chains (HRC). HRC are a class of finite-state randomprocess models that generalise the familiar hidden Markov chains (HMC). HRC areable to model the "intention" of a target to proceed from a given origin to adestination, behaviour which cannot be properly captured by a HMC. WhileBayesian estimation problems for HRC have previously been studied, this paperfocusses principally on the problem of track extraction, of which the primarytask is confirming target existence in a set of detections obtained fromthresholding sensor measurements. Simulation examples are presented which showthat the additional model information contained in a HRC improves detectionperformance when compared to HMC models.
arxiv-1605-04039 | Cross-Domain Visual Matching via Generalized Similarity Measure and Feature Learning |  http://arxiv.org/abs/1605.04039  | author:Liang Lin, Guangrun Wang, Wangmeng Zuo, Xiangchu Feng, Lei Zhang category:cs.CV cs.AI stat.ML published:2016-05-13 summary:Cross-domain visual data matching is one of the fundamental problems in manyreal-world vision tasks, e.g., matching persons across ID photos andsurveillance videos. Conventional approaches to this problem usually involvestwo steps: i) projecting samples from different domains into a common space,and ii) computing (dis-)similarity in this space based on a certain distance.In this paper, we present a novel pairwise similarity measure that advancesexisting models by i) expanding traditional linear projections into affinetransformations and ii) fusing affine Mahalanobis distance and Cosinesimilarity by a data-driven combination. Moreover, we unify our similaritymeasure with feature representation learning via deep convolutional neuralnetworks. Specifically, we incorporate the similarity measure matrix into thedeep architecture, enabling an end-to-end way of model optimization. Weextensively evaluate our generalized similarity model in several challengingcross-domain matching tasks: person re-identification under different views andface verification over different modalities (i.e., faces from still images andvideos, older and younger faces, and sketch and photo portraits). Theexperimental results demonstrate superior performance of our model over otherstate-of-the-art methods.
arxiv-1605-04013 | A corpus-based toy model for DisCoCat |  http://arxiv.org/abs/1605.04013  | author:Stefano Gogioso category:cs.CL cs.LO math.CT published:2016-05-13 summary:We construct an abstract categorical model for DisCoCat starting from ageneric corpus annotated with constituent structure trees. Concretely, we willwork with context-free grammars \`{a} la Chomsky, but Combinatory CategorialGrammar (CCG) and dependency grammars could also be used. We begin by dividingwords in the corpus according to three semantic functions: (i) object words,directly modelled in the semantic space; (ii) modifier words, acting onindividual object words; (iii) interaction words, connecting the meaning ofdistinct object words. We then consider the compact closed symmetric monoidalcategory of $R$-semimodules over an involutive commutative semiring $R$, and wemodel object words as vectors in a free $R$-semimodule $\mathcal{H}$,constructed from the corpus. Based on the grammatical structure annotating thecorpus, we use Frobenius algebras to model modifier words as unary operators on$\mathcal{H}$, and interaction words as binary operators on $\mathcal{H}$. Wediscuss some possible future extensions and improvements of this model.
arxiv-1605-04227 | Relation Schema Induction using Tensor Factorization with Side Information |  http://arxiv.org/abs/1605.04227  | author:Madhav Nimishakavi, Uday Singh Saini, Partha Talukdar category:cs.IR cs.CL cs.DB published:2016-05-12 summary:Given a set of documents from a specific domain (e.g., medical researchjournals), how do we automatically identify the schema of relations i.e., typesignature of arguments of relations (e.g., undergo (Patient, Surgery)) - anecessary first step towards building a Knowledge Graph (KG) out of the givenset of documents? We refer to this problem as Relation Schema Induction (RSI).While Open Information Extraction (OIE) techniques aim at extracting surfacelevel triples of the form (John, underwent, Angioplasty), they don't induce theyet unknown schema of relations themselves. Tensors provide a naturalrepresentation for such triples, and factorization of such tensors provide aplausible solution for the RSI problem. To the best of our knowledge, tensorfactorization methods have not been used for the RSI problem. We fill this gapand propose Coupled Non-negative Tensor Factorization (CNTF), a tensorfactorization method which is able to incorporate additional side informationin a principled way for more effective Relation Schema Induction. We report ourfindings on multiple real-world datasets and demonstrate CNTF's effectivenessover state-of-the-art baselines both in terms of accuracy and speed.
arxiv-1605-03924 | Joint Embeddings of Hierarchical Categories and Entities |  http://arxiv.org/abs/1605.03924  | author:Yuezhang Li, Ronghuo Zheng, Tian Tian, Zhiting Hu, Rahul Iyer, Katia Sycara category:cs.CL published:2016-05-12 summary:Due to the lack of structured knowledge applied in learning distributedrepresentation of categories, existing work cannot incorporate categoryhierarchies into entity information.~We propose a framework that embedsentities and categories into a semantic space by integrating structuredknowledge and taxonomy hierarchy from large knowledge bases. The frameworkallows to compute meaningful semantic relatedness between entities andcategories.~Compared with the previous state of the art, our framework canhandle both single-word concepts and multiple-word concepts with superiorperformance in concept categorization and semantic relatedness.
arxiv-1605-03805 | Detecting Relative Anomaly |  http://arxiv.org/abs/1605.03805  | author:Richard Neuberg, Yixin Shi category:stat.ML cs.LG published:2016-05-12 summary:System states that are anomalous from the perspective of a domain expertoccur frequently in some anomaly detection problems. The performance ofcommonly used unsupervised anomaly detection methods may suffer in thatsetting, because they use frequency as a proxy for anomaly. We propose a novelconcept for anomaly detection, called relative anomaly detection. It istailored to be robust towards anomalies that occur frequently, by taking intoaccount their location relative to the most typical observations. Theapproaches we develop are computationally feasible even for large data sets,and they allow real-time detection. We illustrate using data sets of potentialscraping attempts and Wi-Fi channel utilization, both from Google, Inc.
arxiv-1605-04874 | Gearbox Fault Detection through PSO Exact Wavelet Analysis and SVM Classifier |  http://arxiv.org/abs/1605.04874  | author:Amir Hosein Zamanian, Abdolreza Ohadi category:cs.LG published:2016-05-12 summary:Time-frequency methods for vibration-based gearbox faults detection have beenconsidered the most efficient method. Among these methods, continuous wavelettransform (CWT) as one of the best time-frequency method has been used for bothstationary and transitory signals. Some deficiencies of CWT are problem ofoverlapping and distortion ofsignals. In this condition, a large amount ofredundant information exists so that it may cause false alarm ormisinterpretation of the operator. In this paper a modified method called ExactWavelet Analysis is used to minimize the effects of overlapping and distortionin case of gearbox faults. To implement exact wavelet analysis, Particle SwarmOptimization (PSO) algorithm has been used for this purpose. This method havebeen implemented for the acceleration signals from 2D acceleration sensoracquired by Advantech PCI-1710 card from a gearbox test setup in AmirkabirUniversity of Technology. Gearbox has been considered in both healthy andchipped tooth gears conditions. Kernelized Support Vector Machine (SVM) withradial basis functions has used the extracted features from exact waveletanalysis for classification. The efficiency of this classifier is thenevaluated with the other signals acquired from the setup test. The results showthat in comparison of CWT, PSO Exact Wavelet Transform has better ability infeature extraction in price of more computational effort. In addition, PSOexact wavelet has better speed comparing to Genetic Algorithm (GA) exactwavelet in condition of equal population because of factoring mutation andcrossover in PSO algorithm. SVM classifier with the extracted features ingearbox shows very good results and its ability has been proved.
arxiv-1605-03661 | Learning Representations for Counterfactual Inference |  http://arxiv.org/abs/1605.03661  | author:Fredrik D. Johansson, Uri Shalit, David Sontag category:stat.ML cs.AI cs.LG published:2016-05-12 summary:Observational studies are rising in importance due to the widespreadaccumulation of data in fields such as healthcare, education, employment andecology. We consider the task of answering counterfactual questions such as,"Would this patient have lower blood sugar had she received a differentmedication?". We propose a new algorithmic framework for counterfactualinference which brings together ideas from domain adaptation and representationlearning. In addition to a theoretical justification, we perform an empiricalcomparison with previous approaches to causal inference from observationaldata. Our deep learning algorithm significantly outperforms the previousstate-of-the-art.
arxiv-1605-04006 | A Gaussian Mixture MRF for Model-Based Iterative Reconstruction with Applications to Low-Dose X-ray CT |  http://arxiv.org/abs/1605.04006  | author:Ruoqiao Zhang, Dong Hye Ye, Debashish Pal, Jean-Baptiste Thibault, Ken D. Sauer, Charles A. Bouman category:cs.CV math.OC physics.med-ph published:2016-05-12 summary:Markov random fields (MRFs) have been widely used as prior models in variousinverse problems such as tomographic reconstruction. While MRFs provide asimple and often effective way to model the spatial dependencies in images,they suffer from the fact that parameter estimation is difficult. In practice,this means that MRFs typically have very simple structure that cannotcompletely capture the subtle characteristics of complex images. In this paper, we present a novel Gaussian mixture Markov random field model(GM-MRF) that can be used as a very expressive prior model for inverse problemssuch as denoising and reconstruction. The GM-MRF forms a global image model bymerging together individual Gaussian-mixture models (GMMs) for image patches.In addition, we present a novel analytical framework for computing MAPestimates using the GM-MRF prior model through the construction of surrogatefunctions that result in a sequence of quadratic optimizations. We alsointroduce a simple but effective method to adjust the GM-MRF so as to controlthe sharpness in low- and high-contrast regions of the reconstructionseparately. We demonstrate the value of the model with experiments includingimage denoising and low-dose CT reconstruction.
arxiv-1605-03662 | Subspace Perspective on Canonical Correlation Analysis: Dimension Reduction and Minimax Rates |  http://arxiv.org/abs/1605.03662  | author:Zhuang Ma, Xiaodong Li category:math.ST stat.ML stat.TH published:2016-05-12 summary:Canonical correlation analysis (CCA) is a fundamental statistical tool forexploring the correlation structure between two sets of random variables. Inthis paper, motivated by recent success of applying CCA to learn lowdimensional representations of high dimensional objects, we propose to quantifythe estimation loss of CCA by the excess prediction loss defined through aprediction-after-dimension-reduction framework. Such framework suggests viewingCCA estimation as estimating the subspaces spanned by the canonical variates.Interestedly, the proposed error metrics derived from the excess predictionloss turn out to be closely related to the principal angles between thesubspaces spanned by the population and sample canonical variates respectively. We characterize the non-asymptotic minimax rates under the proposed metrics,especially the dependency of the minimax rates on the key quantities includingthe dimensions, the condition number of the covariance matrices, the canonicalcorrelations and the eigen-gap, with minimal assumptions on the jointcovariance matrix. To the best of our knowledge, this is the first finitesample result that captures the effect of the canonical correlations on theminimax rates.
arxiv-1605-03663 | Item Popularity Prediction in E-commerce Using Image Quality Feature Vectors |  http://arxiv.org/abs/1605.03663  | author:Stephen Zakrewsky, Kamelia Aryafar, Ali Shokoufandeh category:cs.CV published:2016-05-12 summary:Online retail is a visual experience- Shoppers often use images as firstorder information to decide if an item matches their personal style. Imagecharacteristics such as color, simplicity, scene composition, texture, style,aesthetics and overall quality play a crucial role in making a purchasedecision, clicking on or liking a product listing. In this paper we use a setof image features that indicate quality to predict product listing popularityon a major e-commerce website, Etsy. We first define listing popularity throughsearch clicks, favoriting and purchase activity. Next, we infer listing qualityfrom the pixel-level information of listed images as quality features. We thencompare our findings to text-only models for popularity prediction. Our initialresults indicate that a combined image and text modeling of product listingsoutperforms text-only models in popularity prediction.
arxiv-1605-04002 | Which Learning Algorithms Can Generalize Identity-Based Rules to Novel Inputs? |  http://arxiv.org/abs/1605.04002  | author:Paul Tupper, Bobak Shahriari category:cs.CL published:2016-05-12 summary:We propose a novel framework for the analysis of learning algorithms thatallows us to say when such algorithms can and cannot generalize certainpatterns from training data to test data. In particular we focus on situationswhere the rule that must be learned concerns two components of a stimulus beingidentical. We call such a basis for discrimination an identity-based rule.Identity-based rules have proven to be difficult or impossible for certaintypes of learning algorithms to acquire from limited datasets. This is incontrast to human behaviour on similar tasks. Here we provide a framework forrigorously establishing which learning algorithms will fail at generalizingidentity-based rules to novel stimuli. We use this framework to show that suchalgorithms are unable to generalize identity-based rules to novel inputs unlesstrained on virtually all possible inputs. We demonstrate these resultscomputationally with a multilayer feedforward neural network.
arxiv-1605-03956 | On the Convergent Properties of Word Embedding Methods |  http://arxiv.org/abs/1605.03956  | author:Yingtao Tian, Vivek Kulkarni, Bryan Perozzi, Steven Skiena category:cs.CL published:2016-05-12 summary:Do word embeddings converge to learn similar things over differentinitializations? How repeatable are experiments with word embeddings? Are allword embedding techniques equally reliable? In this paper we propose evaluatingmethods for learning word representations by their consistency acrossinitializations. We propose a measure to quantify the similarity of the learnedword representations under this setting (where they are subject to differentrandom initializations). Our preliminary results illustrate that our metric notonly measures a intrinsic property of word embedding methods but alsocorrelates well with other evaluation metrics on downstream tasks. We believeour methods are is useful in characterizing robustness -- an important propertyto consider when developing new word embedding methods.
arxiv-1605-03664 | Real-Time Web Scale Event Summarization Using Sequential Decision Making |  http://arxiv.org/abs/1605.03664  | author:Chris Kedzie, Fernando Diaz, Kathleen McKeown category:cs.CL published:2016-05-12 summary:We present a system based on sequential decision making for the onlinesummarization of massive document streams, such as those found on the web.Given an event of interest (e.g. "Boston marathon bombing"), our system is ableto filter the stream for relevance and produce a series of short text updatesdescribing the event as it unfolds over time. Unlike previous work, ourapproach is able to jointly model the relevance, comprehensiveness, novelty,and timeliness required by time-sensitive queries. We demonstrate a 28.3%improvement in summary F1 and a 43.8% improvement in time-sensitive F1 metrics.
arxiv-1605-03933 | Competitive analysis of the top-K ranking problem |  http://arxiv.org/abs/1605.03933  | author:Xi Chen, Sivakanth Gopi, Jieming Mao, Jon Schneider category:cs.DS cs.IT cs.LG math.IT stat.ML published:2016-05-12 summary:Motivated by applications in recommender systems, web search, social choiceand crowdsourcing, we consider the problem of identifying the set of top $K$items from noisy pairwise comparisons. In our setting, we are non-activelygiven $r$ pairwise comparisons between each pair of $n$ items, where eachcomparison has noise constrained by a very general noise model called thestrong stochastic transitivity (SST) model. We analyze the competitive ratio ofalgorithms for the top-$K$ problem. In particular, we present a linear timealgorithm for the top-$K$ problem which has a competitive ratio of$\tilde{O}(\sqrt{n})$; i.e. to solve any instance of top-$K$, our algorithmneeds at most $\tilde{O}(\sqrt{n})$ times as many samples needed as the bestpossible algorithm for that instance (in contrast, all previous knownalgorithms for the top-$K$ problem have competitive ratios of$\tilde{\Omega}(n)$ or worse). We further show that this is tight: anyalgorithm for the top-$K$ problem has competitive ratio at least$\tilde{\Omega}(\sqrt{n})$.
arxiv-1605-03884 | An Empirical-Bayes Score for Discrete Bayesian Networks |  http://arxiv.org/abs/1605.03884  | author:Marco Scutari category:stat.ML stat.ME published:2016-05-12 summary:Bayesian network structure learning is often performed in a Bayesian setting,by evaluating candidate structures using their posterior probabilities for agiven data set. Score-based algorithms then use those posterior probabilitiesas an objective function and return the maximum a posteriori network as thelearned model. For discrete Bayesian networks, the canonical choice for aposterior score is the Bayesian Dirichlet equivalent uniform (BDeu) marginallikelihood with a uniform (U) graph prior (Heckerman et al., 1995). Itsfavourable theoretical properties descend from assuming a uniform prior both onthe space of the network structures and on the space of the parameters of thenetwork. In this paper, we revisit the limitations of these assumptions; and weintroduce an alternative set of assumptions and the resulting score: theBayesian Dirichlet sparse (BDs) empirical Bayes marginal likelihood with amarginal uniform (MU) graph prior. We evaluate its performance in an extensivesimulation study, showing that MU+BDs is more accurate than U+BDeu both inlearning the structure of the network and in predicting new observations, whilenot being computationally more complex to estimate.
arxiv-1605-03688 | Going Deeper into First-Person Activity Recognition |  http://arxiv.org/abs/1605.03688  | author:Minghuang Ma, Haoqi Fan, Kris M. Kitani category:cs.CV published:2016-05-12 summary:We bring together ideas from recent work on feature design for egocentricaction recognition under one framework by exploring the use of deepconvolutional neural networks (CNN). Recent work has shown that features suchas hand appearance, object attributes, local hand motion and camera ego-motionare important for characterizing first-person actions. To integrate these ideasunder one framework, we propose a twin stream network architecture, where onestream analyzes appearance information and the other stream analyzes motioninformation. Our appearance stream encodes prior knowledge of the egocentricparadigm by explicitly training the network to segment hands and localizeobjects. By visualizing certain neuron activation of our network, we show thatour proposed architecture naturally learns features that capture objectattributes and hand-object configurations. Our extensive experiments onbenchmark egocentric action datasets show that our deep architecture enablesrecognition rates that significantly outperform state-of-the-art techniques --an average $6.6\%$ increase in accuracy over all datasets. Furthermore, bylearning to recognize objects, actions and activities jointly, the performanceof individual recognition tasks also increase by $30\%$ (actions) and $14\%$(objects). We also include the results of extensive ablative analysis tohighlight the importance of network design decisions..
arxiv-1605-03689 | Robust and Efficient Relative Pose with a Multi-camera System for Autonomous Vehicle in Highly Dynamic Environments |  http://arxiv.org/abs/1605.03689  | author:Liu Liu, Hongdong Li, Yuchao Dai category:cs.RO cs.CV published:2016-05-12 summary:This paper studies the relative pose problem for autonomous vehicle drivingin highly dynamic and possibly cluttered environments. This is a challengingscenario due to the existence of multiple, large, and independently movingobjects in the environment, which often leads to excessive portion of outliersand results in erroneous motion estimation. Existing algorithms cannot copewith such situations well. This paper proposes a new algorithm for relativepose using a multi-camera system with multiple non-overlapping individualcameras. The method works robustly even when the numbers of outliers areoverwhelming. By exploiting specific prior knowledge of driving scene we havedeveloped an efficient 4-point algorithm for multi-camera relative pose, whichadmits analytic solutions by solving a polynomial root-finding equation, andruns extremely fast (at about 0.5$u$s per root). When the solver is used incombination with RANSAC, we are able to quickly prune unpromising hypotheses,significantly improve the chance of finding inliers. Experiments on syntheticdata have validated the performance of the proposed algorithm. Tests on realdata further confirm the method's practical relevance.
arxiv-1605-03865 | A New Manifold Distance Measure for Visual Object Categorization |  http://arxiv.org/abs/1605.03865  | author:Fengfu Li, Xiayuan Huang, Hong Qiao, Bo Zhang category:cs.CV published:2016-05-12 summary:Manifold distances are very effective tools for visual object recognition.However, most of the traditional manifold distances between images are based onthe pixel-level comparison and thus easily affected by image rotations andtranslations. In this paper, we propose a new manifold distance to model thedissimilarities between visual objects based on the Complex Wavelet StructuralSimilarity (CW-SSIM) index. The proposed distance is more robust to rotationsand translations of images than the traditional manifold distance and theCW-SSIM index based distance. In addition, the proposed distance is combinedwith the $k$-medoids clustering method to derive a new clustering method forvisual object categorization. Experiments on Coil-20, Coil-100 and OlivettiFace Databases show that the proposed distance measure is better for visualobject categorization than both the traditional manifold distances and theCW-SSIM index based distances.
arxiv-1605-03852 | Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning |  http://arxiv.org/abs/1605.03852  | author:Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Chris Dyer category:cs.CL published:2016-05-12 summary:We use Bayesian optimization to learn curricula for word representationlearning, optimizing performance on downstream tasks that depend on the learnedrepresentations as features. The curricula are modeled by a linear rankingfunction which is the scalar product of a learned weight vector and anengineered feature vector that characterizes the different aspects of thecomplexity of each instance in the training corpus. We show that learning thecurriculum improves performance on a variety of downstream tasks over randomorders and in comparison to the natural corpus order.
arxiv-1605-03848 | Context-dependent feature analysis with random forests |  http://arxiv.org/abs/1605.03848  | author:Antonio Sutera, Gilles Louppe, Vân Anh Huynh-Thu, Louis Wehenkel, Pierre Geurts category:stat.ML cs.LG published:2016-05-12 summary:In many cases, feature selection is often more complicated than identifying asingle subset of input variables that would together explain the output. Theremay be interactions that depend on contextual information, i.e., variables thatreveal to be relevant only in some specific circumstances. In this setting, thecontribution of this paper is to extend the random forest variable importancesframework in order (i) to identify variables whose relevance iscontext-dependent and (ii) to characterize as precisely as possible the effectof contextual information on these variables. The usage and the relevance ofour framework for highlighting context-dependent variables is illustrated onboth artificial and real datasets.
arxiv-1605-03835 | Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model |  http://arxiv.org/abs/1605.03835  | author:Kyunghyun Cho category:cs.CL cs.LG stat.ML published:2016-05-12 summary:Recent advances in conditional recurrent language modelling have mainlyfocused on network architectures (e.g., attention mechanism), learningalgorithms (e.g., scheduled sampling and sequence-level training) and novelapplications (e.g., image/video description generation, speech recognition,etc.) On the other hand, we notice that decoding algorithms/strategies have notbeen investigated as much, and it has become standard to use greedy or beamsearch. In this paper, we propose a novel decoding strategy motivated by anearlier observation that nonlinear hidden layers of a deep neural networkstretch the data manifold. The proposed strategy is embarrassinglyparallelizable without any communication overhead, while improving an existingdecoding algorithm. We extensively evaluate it with attention-based neuralmachine translation on the task of En->Cz translation.
arxiv-1605-03832 | Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning |  http://arxiv.org/abs/1605.03832  | author:Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick Littell, David Mortensen, Alan W Black, Lori Levin, Chris Dyer category:cs.CL published:2016-05-12 summary:We introduce polyglot language models, recurrent neural network modelstrained to predict symbol sequences in many different languages using sharedrepresentations of symbols and conditioning on typological information aboutthe language to be predicted. We apply these to the problem of modeling phonesequences---a domain in which universal symbol inventories andcross-linguistically shared feature representations are a natural fit.Intrinsic evaluation on held-out perplexity, qualitative analysis of thelearned representations, and extrinsic evaluation in two downstreamapplications that make use of phonetic features show (i) that polyglot modelsbetter generalize to held-out data than comparable monolingual models and (ii)that polyglot phonetic feature representations are of higher quality than thoselearned monolingually.
arxiv-1605-03821 | Crowd Pedestrian Counting Considering Network Flow Constraints in Videos |  http://arxiv.org/abs/1605.03821  | author:Liqing Gao, Yanzhang Wang, Xin Ye, Jian Wang category:cs.CV published:2016-05-12 summary:A quadratic programming method with network flow constraints is proposed toimprove crowd pedestrian counting in video surveillance. Most of the existingapproaches estimate the number of pedestrians within one frame, which result ininconsistent predictions in temporal domain. In this paper, firstly, we segmentthe foreground of each frame into different groups, each of which containsseveral pedestrians. Then we train a regression-based map from low levelfeatures of each group to its person number. Secondly, we construct a directedgraph to simulate people flow, whose vertices represent groups of each frameand edges represent people moving from one group to another. Then, the peopleflow can be viewed as an integer flow in the constructed directed graph.Finally, by solving a quadratic programming problem with network flowconstraints in the directed graph, we obtain a consistent pedestrian counting.The experimental results show that our method can improve the crowd countingaccuracy significantly.
arxiv-1605-03804 | A Mid-level Video Representation based on Binary Descriptors: A Case Study for Pornography Detection |  http://arxiv.org/abs/1605.03804  | author:Carlos Caetano, Sandra Avila, William Robson Schwartz, Silvio Jamil F. Guimarães, Arnaldo de A. Araújo category:cs.CV published:2016-05-12 summary:With the growing amount of inappropriate content on the Internet, such aspornography, arises the need to detect and filter such material. The reason forthis is given by the fact that such content is often prohibited in certainenvironments (e.g., schools and workplaces) or for certain publics (e.g.,children). In recent years, many works have been mainly focused on detectingpornographic images and videos based on visual content, particularly on thedetection of skin color. Although these approaches provide good results, theygenerally have the disadvantage of a high false positive rate since not allimages with large areas of skin exposure are necessarily pornographic images,such as people wearing swimsuits or images related to sports. Local featurebased approaches with Bag-of-Words models (BoW) have been successfully appliedto visual recognition tasks in the context of pornography detection. Eventhough existing methods provide promising results, they use local featuredescriptors that require a high computational processing time yieldinghigh-dimensional vectors. In this work, we propose an approach for pornographydetection based on local binary feature extraction and BossaNova imagerepresentation, a BoW model extension that preserves more richly the visualinformation. Moreover, we propose two approaches for video description based onthe combination of mid-level representations namely BossaNova Video Descriptor(BNVD) and BoW Video Descriptor (BoW-VD). The proposed techniques arepromising, achieving an accuracy of 92.40%, thus reducing the classificationerror by 16% over the current state-of-the-art local features approach on thePornography dataset.
arxiv-1605-03795 | Tensor Train polynomial models via Riemannian optimization |  http://arxiv.org/abs/1605.03795  | author:Alexander Novikov, Mikhail Trofimov, Ivan Oseledets category:stat.ML cs.LG published:2016-05-12 summary:Modeling interactions between features improves the performance of machinelearning solutions in many domains (e.g. recommender systems or sentimentanalysis). In this paper, we introduce Exponential Machines (ExM), a predictorthat models all interactions of every order. The key idea is to represent anexponentially large tensor of parameters in a factorized format called TensorTrain (TT). The Tensor Train format regularizes the model and lets you controlthe number of underlying parameters. To train the model, we develop astochastic version of Riemannian optimization, which allows us to fit tensorswith $2^{30}$ entries. We show that the model achieves state-of-the-artperformance on synthetic data with high-order interactions.
arxiv-1605-03764 | Direct Method for Training Feed-forward Neural Networks using Batch Extended Kalman Filter for Multi-Step-Ahead Predictions |  http://arxiv.org/abs/1605.03764  | author:Artem Chernodub category:cs.NE published:2016-05-12 summary:This paper is dedicated to the long-term, or multi-step-ahead, time seriesprediction problem. We propose a novel method for training feed-forward neuralnetworks, such as multilayer perceptrons, with tapped delay lines. Specialbatch calculation of derivatives called Forecasted Propagation Through Time andbatch modification of the Extended Kalman Filter are introduced. Experimentswere carried out on well-known time series benchmarks, the Mackey-Glass chaoticprocess and the Santa Fe Laser Data Series. Recurrent and feed-forward neuralnetworks were evaluated.
arxiv-1605-03705 | Movie Description |  http://arxiv.org/abs/1605.03705  | author:Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo Larochelle, Aaron Courville, Bernt Schiele category:cs.CV cs.CL published:2016-05-12 summary:Audio Description (AD) provides linguistic descriptions of movies and allowsvisually impaired people to follow a movie along with their peers. Suchdescriptions are by design mainly visual and thus naturally form an interestingdata source for computer vision and computational linguistics. In this work wepropose a novel dataset which contains transcribed ADs, which are temporallyaligned to full length movies. In addition we also collected and aligned moviescripts used in prior work and compare the two sources of descriptions. Intotal the Large Scale Movie Description Challenge (LSMDC) contains a parallelcorpus of 118,114 sentences and video clips from 202 movies. First wecharacterize the dataset by benchmarking different approaches for generatingvideo descriptions. Comparing ADs to scripts, we find that ADs are indeed morevisual and describe precisely what is shown rather than what should happenaccording to the scripts created prior to movie production. Furthermore, wepresent and compare the results of several teams who participated in achallenge organized in the context of the workshop "Describing andUnderstanding Video & The Large Scale Movie Description Challenge (LSMDC)", atICCV 2015.
arxiv-1605-03746 | Fast Graph-Based Object Segmentation for RGB-D Images |  http://arxiv.org/abs/1605.03746  | author:Giorgio Toscana, Stefano Rosa category:cs.CV cs.RO published:2016-05-12 summary:Object segmentation is an important capability for robotic systems, inparticular for grasping. We present a graph- based approach for thesegmentation of simple objects from RGB-D images. We are interested insegmenting objects with large variety in appearance, from lack of texture tostrong textures, for the task of robotic grasping. The algorithm does not relyon image features or machine learning. We propose a modified Canny edgedetector for extracting robust edges by using depth information and two simplecost functions for combining color and depth cues. The cost functions are usedto build an undirected graph, which is partitioned using the concept ofinternal and external differences between graph regions. The partitioning isfast with O(NlogN) complexity. We also discuss ways to deal with missing depthinformation. We test the approach on different publicly available RGB-D objectdatasets, such as the Rutgers APC RGB-D dataset and the RGB-D Object Dataset,and compare the results with other existing methods.
arxiv-1605-03720 | Deformable Parts Correlation Filters for Robust Visual Tracking |  http://arxiv.org/abs/1605.03720  | author:Alan Lukežič, Luka Čehovin, Matej Kristan category:cs.CV published:2016-05-12 summary:Deformable parts models show a great potential in tracking by principallyaddressing non-rigid object deformations and self occlusions, but according torecent benchmarks, they often lag behind the holistic approaches. The reason isthat potentially large number of degrees of freedom have to be estimated forobject localization and simplifications of the constellation topology are oftenassumed to make the inference tractable. We present a new formulation of theconstellation model with correlation filters that treats the geometric andvisual constraints within a single convex cost function and derive a highlyefficient optimization for MAP inference of a fully-connected constellation. Wepropose a tracker that models the object at two levels of detail. The coarselevel corresponds a root correlation filter and a novel color model forapproximate object localization, while the mid-level representation is composedof the new deformable constellation of correlation filters that refine theobject location. The resulting tracker is rigorously analyzed on a highlychallenging OTB, VOT2014 and VOT2015 benchmarks, exhibits a state-of-the-artperformance and runs in real-time.
arxiv-1605-03718 | Improved Image Boundaries for Better Video Segmentation |  http://arxiv.org/abs/1605.03718  | author:Anna Khoreva, Rodrigo Benenson, Fabio Galasso, Matthias Hein, Bernt Schiele category:cs.CV published:2016-05-12 summary:Graph-based video segmentation methods rely on superpixels as starting point.While most previous work has focused on the construction of the graph edges andweights as well as solving the graph partitioning problem, this paper focuseson better superpixels for video segmentation. We demonstrate by a comparativeanalysis that superpixels extracted from boundaries perform best, and show thatboundary estimation can be significantly improved via image and time domaincues. With superpixels generated from our better boundaries we observeconsistent improvement for two video segmentation methods in two differentdatasets.
arxiv-1605-03730 | Real-time Robust Manhattan Frame Estimation: Global Optimality and Applications |  http://arxiv.org/abs/1605.03730  | author:Kyungdon Joo, Tae-Hyun Oh, Junsik Kim, In So Kweon category:cs.CV cs.RO published:2016-05-12 summary:Most man-made environments, such as urban and indoor scenes, consist of a setof parallel and orthogonal planar structures. These structures are approximatedby Manhattan world assumption and be referred to Manhattan Frame (MF). Given aset of inputs such as surface normals or vanishing points, we pose an MFestimation problem as a consensus set maximization that maximizes the number ofinliers over the rotation search space. Conventionally this problem can besolved by a branch-and-bound framework which mathematically guarantees globaloptimality. However, the computational time of the conventionalbranch-and-bound algorithms is rather far from real-time performance. In thispaper, we propose a novel bound computation method on an efficient measurementdomain for MF estimation, i.e., the extended Gaussian image (EGI). By relaxingthe original problem, we can compute the bounds in real-time performance, whilepreserving global optimality. Furthermore, we quantitatively and qualitativelydemonstrate the performance of the proposed method for various synthetic andreal-world data. We also show the versatility of our approach through threedifferent applications: extension to multiple MF estimation, videostabilization and line clustering.
arxiv-1605-03639 | Facial Expression Recognition from World Wild Web |  http://arxiv.org/abs/1605.03639  | author:Ali Mollahosseini, Behzad Hassani, Michelle J. Salvador, Hojjat Abdollahi, David Chan, Mohammad H. Mahoor category:cs.CV cs.NE published:2016-05-11 summary:Recognizing facial expression in a wild setting has remained a challengingtask in computer vision. The World Wide Web is a good source of facial imageswhich most of them are captured in uncontrolled conditions. In fact, theInternet is a Word Wild Web of facial images with expressions. This paperpresents the results of a new study on collecting, annotating, and analyzingwild facial expressions from the web. Three search engines were queried using1250 emotion related keywords in six different languages and the retrievedimages were mapped by two annotators to six basic expressions and neutral. Deepneural networks and noise modeling were used in three different trainingscenarios to find how accurately facial expressions can be recognized whentrained on noisy images collected from the web using query terms (e.g. happyface, laughing man, etc)? The results of our experiments show that deep neuralnetworks can recognize wild facial expressions with an accuracy of 82.12%.
arxiv-1605-03481 | Tweet2Vec: Character-Based Distributed Representations for Social Media |  http://arxiv.org/abs/1605.03481  | author:Bhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick, Michael Muehl, William W. Cohen category:cs.LG cs.CL published:2016-05-11 summary:Text from social media provides a set of challenges that can causetraditional NLP approaches to fail. Informal language, spelling errors,abbreviations, and special characters are all commonplace in these posts,leading to a prohibitively large vocabulary size for word-level approaches. Wepropose a character composition model, tweet2vec, which finds vector-spacerepresentations of whole tweets by learning complex, non-local dependencies incharacter sequences. The proposed model outperforms a word-level baseline atpredicting user-annotated hashtags associated with the posts, doingsignificantly better when the input contains many out-of-vocabulary words orunusual character sequences. Our tweet2vec encoder is publicly available.
arxiv-1605-03483 | Real-time 3D Tracking of Articulated Tools for Robotic Surgery |  http://arxiv.org/abs/1605.03483  | author:Menglong Ye, Lin Zhang, Stamatia Giannarou, Guang-Zhong Yang category:cs.CV cs.RO published:2016-05-11 summary:In robotic surgery, tool tracking is important for providing safe tool-tissueinteraction and facilitating surgical skills assessment. Despite recentadvances in tool tracking, existing approaches are faced with majordifficulties in real-time tracking of articulated tools. Most algorithms aretailored for offline processing with pre-recorded videos. In this paper, wepropose a real-time 3D tracking method for articulated tools in roboticsurgery. The proposed method is based on the CAD model of the tools as well asrobot kinematics to generate online part-based templates for efficient 2Dmatching and 3D pose estimation. A robust verification approach is incorporatedto reject outliers in 2D detections, which is then followed by fusing inlierswith robot kinematic readings for 3D pose estimation of the tool. The proposedmethod has been validated with phantom data, as well as ex vivo and in vivoexperiments. The results derived clearly demonstrate the performance advantageof the proposed method when compared to the state-of-the-art.
arxiv-1605-03284 | Machine Comprehension Based on Learning to Rank |  http://arxiv.org/abs/1605.03284  | author:Tian Tian, Yuezhang Li category:cs.CL published:2016-05-11 summary:Machine comprehension plays an essential role in NLP and has been widelyexplored with dataset like MCTest. However, this dataset is too simple and toosmall for learning true reasoning abilities. \cite{hermann2015teaching}therefore release a large scale news article dataset and propose a deep LSTMreader system for machine comprehension. However, the training process isexpensive. We therefore try feature-engineered approach with semantics on thenew dataset to see how traditional machine learning technique and semantics canhelp with machine comprehension. Meanwhile, our proposed L2R reader systemachieves good performance with efficiency and less training data.
arxiv-1605-03259 | Deep Attributes Driven Multi-Camera Person Re-identification |  http://arxiv.org/abs/1605.03259  | author:Chi Su, Shiliang Zhang, Junliang Xing, Wen Gao, Qi Tian category:cs.CV published:2016-05-11 summary:The visual appearance of a person is easily affected by many factors likepose variations, viewpoint changes and camera parameter differences. This makesperson Re-Identification (ReID) among multiple cameras a very challenging task.This work is motivated to learn mid-level human attributes which are robust tosuch visual appearance variations. And we propose a semi-supervised attributelearning framework which progressively boosts the accuracy of attributes onlyusing a limited number of labeled data. Specifically, this framework involves athree-stage training. A deep Convolutional Neural Network (dCNN) is firsttrained on an independent dataset labeled with attributes. Then it isfine-tuned on another dataset only labeled with person IDs using our definedtriplet loss. Finally, the updated dCNN predicts attribute labels for thetarget dataset, which is combined with the independent dataset for the finalround of fine-tuning. The predicted attributes, namely \emph{deep attributes}exhibit superior generalization ability across different datasets. By directlyusing the deep attributes with simple Cosine distance, we have obtainedsurprisingly good accuracy on four person ReID datasets. Experiments also showthat a simple metric learning modular further boosts our method, making itsignificantly outperform many recent works.
arxiv-1605-03261 | Sensorimotor Input as a Language Generalisation Tool: A Neurorobotics Model for Generation and Generalisation of Noun-Verb Combinations with Sensorimotor Inputs |  http://arxiv.org/abs/1605.03261  | author:Junpei Zhong, Martin Peniak, Jun Tani, Tetsuya Ogata, Angelo Cangelosi category:cs.RO cs.CL published:2016-05-11 summary:The paper presents a neurorobotics cognitive model to explain theunderstanding and generalisation of nouns and verbs combinations when a vocalcommand consisting of a verb-noun sentence is provided to a humanoid robot.This generalisation process is done via the grounding process: differentobjects are being interacted, and associated, with different motor behaviours,following a learning approach inspired by developmental language acquisition ininfants. This cognitive model is based on Multiple Time-scale Recurrent NeuralNetworks (MTRNN).With the data obtained from object manipulation tasks with ahumanoid robot platform, the robotic agent implemented with this model canground the primitive embodied structure of verbs through training withverb-noun combination samples. Moreover, we show that a functional hierarchicalarchitecture, based on MTRNN, is able to generalise and produce novelcombinations of noun-verb sentences. Further analyses of the learned networkdynamics and representations also demonstrate how the generalisation ispossible via the exploitation of this functional hierarchical recurrentnetwork.
arxiv-1605-03267 | Generalized Sparse Precision Matrix Selection for Fitting Multivariate Gaussian Random Fields to Large Data Sets |  http://arxiv.org/abs/1605.03267  | author:Sam Davanloo Tajbakhsh, Necdet Serhat Aybat, Enrique del Castillo category:stat.ML published:2016-05-11 summary:This paper generalizes the Sparse Precision matrix Selection (SPS) algorithm,proposed by Davanloo et al. (2015) for estimating scalar Gaussian Random Field(GRF) models, to the multivariate, second-order stationary case under aseparable covariance function. Theoretical convergence rates for the estimatedcovariance matrix and for the estimated parameters of the correlation functionare established. Numerical simulation results validate our theoreticalfindings. Data segmentation is used to handle large data sets.
arxiv-1605-03306 | High dimensional thresholded regression and shrinkage effect |  http://arxiv.org/abs/1605.03306  | author:Zemin Zheng, Yingying Fan, Jinchi Lv category:stat.ME stat.ML published:2016-05-11 summary:High-dimensional sparse modeling via regularization provides a powerful toolfor analyzing large-scale data sets and obtaining meaningful, interpretablemodels. The use of nonconvex penalty functions shows advantage in selectingimportant features in high dimensions, but the global optimality of suchmethods still demands more understanding. In this paper, we consider sparseregression with hard-thresholding penalty, which we show to give rise tothresholded regression. This approach is motivated by its close connection withthe $L_0$-regularization, which can be unrealistic to implement in practice butof appealing sampling properties, and its computational advantage. Under somemild regularity conditions allowing possibly exponentially growingdimensionality, we establish the oracle inequalities of the resultingregularized estimator, as the global minimizer, under various prediction andvariable selection losses, as well as the oracle risk inequalities of thehard-thresholded estimator followed by a further $L_2$-regularization. The riskproperties exhibit interesting shrinkage effects under both estimation andprediction losses. We identify the optimal choice of the ridge parameter, whichis shown to have simultaneous advantages to both the $L_2$-loss and predictionloss. These new results and phenomena are evidenced by simulation and real dataexamples.
arxiv-1605-03310 | Asymptotic equivalence of regularization methods in thresholded parameter space |  http://arxiv.org/abs/1605.03310  | author:Yingying Fan, Jinchi Lv category:stat.ME math.ST stat.ML stat.TH published:2016-05-11 summary:High-dimensional data analysis has motivated a spectrum of regularizationmethods for variable selection and sparse modeling, with two popular classes ofconvex ones and concave ones. A long debate has been on whether one classdominates the other, an important question both in theory and to practitioners.In this paper, we characterize the asymptotic equivalence of regularizationmethods, with general penalty functions, in a thresholded parameter space underthe generalized linear model setting, where the dimensionality can grow up toexponentially with the sample size. To assess their performance, we establishthe oracle inequalities, as in Bickel, Ritov and Tsybakov (2009), of the globalminimizer for these methods under various prediction and variable selectionlosses. These results reveal an interesting phase transition phenomenon. Forpolynomially growing dimensionality, the $L_1$-regularization method of Lassoand concave methods are asymptotically equivalent, having the same convergencerates in the oracle inequalities. For exponentially growing dimensionality,concave methods are asymptotically equivalent but have faster convergence ratesthan the Lasso. We also establish a stronger property of the oracle riskinequalities of the regularization methods, as well as the sampling propertiesof computable solutions. Our new theoretical results are illustrated andjustified by simulation and real data examples.
arxiv-1605-03311 | The constrained Dantzig selector with enhanced consistency |  http://arxiv.org/abs/1605.03311  | author:Yinfei Kong, Zemin Zheng, Jinchi Lv category:stat.ME stat.ML published:2016-05-11 summary:The Dantzig selector has received popularity for many applications such ascompressed sensing and sparse modeling, thanks to its computational efficiencyas a linear programming problem and its nice sampling properties. Existingresults show that it can recover sparse signals mimicking the accuracy of theideal procedure, up to a logarithmic factor of the dimensionality. Such afactor has been shown to hold for many regularization methods. An importantquestion is whether this factor can be reduced to a logarithmic factor of thesample size in ultra-high dimensions under mild regularity conditions. Toprovide an affirmative answer, in this paper we suggest the constrained Dantzigselector, which has more flexible constraints and parameter space. We provethat the suggested method can achieve convergence rates within a logarithmicfactor of the sample size of the oracle rates and improved sparsity, under afairly weak assumption on the signal strength. Such improvement is significantin ultra-high dimensions. This method can be implemented efficiently throughsequential linear programming. Numerical studies confirm that the sample sizeneeded for a certain level of accuracy in these problems can be much reduced.
arxiv-1605-03313 | Innovated scalable efficient estimation in ultra-large Gaussian graphical models |  http://arxiv.org/abs/1605.03313  | author:Yingying Fan, Jinchi Lv category:stat.ME stat.ML published:2016-05-11 summary:Large-scale precision matrix estimation is of fundamental importance yetchallenging in many contemporary applications for recovering Gaussian graphicalmodels. In this paper, we suggest a new approach of innovated scalableefficient estimation (ISEE) for estimating large precision matrix. Motivated bythe innovated transformation, we convert the original problem into that oflarge covariance matrix estimation. The suggested method combines the strengthsof recent advances in high-dimensional sparse modeling and large covariancematrix estimation. Compared to existing approaches, our method is scalable andcan deal with much larger precision matrices with simple tuning. Under mildregularity conditions, we establish that this procedure can recover theunderlying graphical structure with significant probability and provideefficient estimation of link strengths. Both computational and theoreticaladvantages of the procedure are evidenced through simulation and real dataexamples.
arxiv-1605-03315 | Interaction pursuit in high-dimensional multi-response regression via distance correlation |  http://arxiv.org/abs/1605.03315  | author:Yinfei Kong, Daoji Li, Yingying Fan, Jinchi Lv category:stat.ME stat.ML published:2016-05-11 summary:Feature interactions can contribute to a large proportion of variation inmany prediction models. In the era of big data, the coexistence of highdimensionality in both responses and covariates poses unprecedented challengesin identifying important interactions. In this paper, we suggest a two-stageinteraction identification method, called the interaction pursuit via distancecorrelation (IPDC), in the setting of high-dimensional multi-responseinteraction models that exploits feature screening applied to transformedvariables with distance correlation followed by feature selection. Such aprocedure is computationally efficient, generally applicable beyond theheredity assumption, and effective even when the number of responses divergeswith the sample size. Under mild regularity conditions, we show that thismethod enjoys nice theoretical properties including the sure screeningproperty, support union recovery, and oracle inequalities in prediction andestimation for both interactions and main effects. The advantages of our methodare supported by several simulation studies and real data analysis.
arxiv-1605-03321 | Tuning parameter selection in high dimensional penalized likelihood |  http://arxiv.org/abs/1605.03321  | author:Yingying Fan, Cheng Yong Tang category:stat.ME stat.ML published:2016-05-11 summary:Determining how to appropriately select the tuning parameter is essential inpenalized likelihood methods for high-dimensional data analysis. We examinethis problem in the setting of penalized likelihood methods for generalizedlinear models, where the dimensionality of covariates p is allowed to increaseexponentially with the sample size n. We propose to select the tuning parameterby optimizing the generalized information criterion (GIC) with an appropriatemodel complexity penalty. To ensure that we consistently identify the truemodel, a range for the model complexity penalty is identified in GIC. We findthat this model complexity penalty should diverge at the rate of some power of$\log p$ depending on the tail probability behavior of the response variables.This reveals that using the AIC or BIC to select the tuning parameter may notbe adequate for consistently identifying the true model. Based on ourtheoretical study, we propose a uniform choice of the model complexity penaltyand show that the proposed approach consistently identifies the true modelamong candidate models with asymptotic probability one. We justify theperformance of the proposed procedure by numerical simulations and a geneexpression data analysis.
arxiv-1605-03324 | Unsupervised Semantic Action Discovery from Video Collections |  http://arxiv.org/abs/1605.03324  | author:Ozan Sener, Amir Roshan Zamir, Chenxia Wu, Silvio Savarese, Ashutosh Saxena category:cs.CV cs.RO stat.ML published:2016-05-11 summary:Human communication takes many forms, including speech, text andinstructional videos. It typically has an underlying structure, with a startingpoint, ending, and certain objective steps between them. In this paper, weconsider instructional videos where there are tens of millions of them on theInternet. We propose a method for parsing a video into such semantic steps in anunsupervised way. Our method is capable of providing a semantic "storyline" ofthe video composed of its objective steps. We accomplish this using both visualand language cues in a joint generative model. Our method can also provide atextual description for each of the identified semantic steps and videosegments. We evaluate our method on a large number of complex YouTube videosand show that our method discovers semantically correct instructions for avariety of tasks.
arxiv-1605-03328 | A robust particle detection algorithm based on symmetry |  http://arxiv.org/abs/1605.03328  | author:Alvaro Rodriguez, Hanqing Zhang, Krister Wiklund, Tomas Brodin, Jonatan Klaminder, Patrik Andersson, Magnus Andersson category:cs.CV published:2016-05-11 summary:Particle tracking is common in many biophysical, ecological, andmicro-fluidic applications. Reliable tracking information is heavily dependenton of the system under study and algorithms that correctly determines particleposition between images. However, in a real environmental context with thepresence of noise including particular or dissolved matter in water, and lowand fluctuating light conditions, many algorithms fail to obtain reliableinformation. We propose a new algorithm, the Circular Symmetry algorithm(C-Sym), for detecting the position of a circular particle with high accuracyand precision in noisy conditions. The algorithm takes advantage of the spatialsymmetry of the particle allowing for subpixel accuracy. We compare theproposed algorithm with four different methods using both synthetic andexperimental datasets. The results show that C-Sym is the most accurate andprecise algorithm when tracking micro-particles in all tested conditions and ithas the potential for use in applications including tracking biota in theirenvironment.
arxiv-1605-03335 | Asymptotic properties for combined $L_1$ and concave regularization |  http://arxiv.org/abs/1605.03335  | author:Yingying Fan, Jinchi Lv category:stat.ME stat.ML published:2016-05-11 summary:Two important goals of high-dimensional modeling are prediction and variableselection. In this article, we consider regularization with combined $L_1$ andconcave penalties, and study the sampling properties of the global optimum ofthe suggested method in ultra-high dimensional settings. The $L_1$-penaltyprovides the minimum regularization needed for removing noise variables inorder to achieve oracle prediction risk, while concave penalty imposesadditional regularization to control model sparsity. In the linear modelsetting, we prove that the global optimum of our method enjoys the same oracleinequalities as the lasso estimator and admits an explicit bound on the falsesign rate, which can be asymptotically vanishing. Moreover, we establish oraclerisk inequalities for the method and the sampling properties of computablesolutions. Numerical studies suggest that our method yields more stableestimates than using a concave penalty alone.
arxiv-1605-03344 | Go-ICP: A Globally Optimal Solution to 3D ICP Point-Set Registration |  http://arxiv.org/abs/1605.03344  | author:Jiaolong Yang, Hongdong Li, Dylan Campbell, Yunde Jia category:cs.CV published:2016-05-11 summary:The Iterative Closest Point (ICP) algorithm is one of the most widely usedmethods for point-set registration. However, being based on local iterativeoptimization, ICP is known to be susceptible to local minima. Its performancecritically relies on the quality of the initialization and only localoptimality is guaranteed. This paper presents the first globally optimalalgorithm, named Go-ICP, for Euclidean (rigid) registration of two 3Dpoint-sets under the L2 error metric defined in ICP. The Go-ICP method is basedon a branch-and-bound (BnB) scheme that searches the entire 3D motion spaceSE(3). By exploiting the special structure of SE(3) geometry, we derive novelupper and lower bounds for the registration error function. Local ICP isintegrated into the BnB scheme, which speeds up the new method whileguaranteeing global optimality. We also discuss extensions, addressing theissue of outlier robustness. The evaluation demonstrates that the proposedmethod is able to produce reliable registration results regardless of theinitialization. Go-ICP can be applied in scenarios where an optimal solution isdesirable or where a good initialization is not always available.
arxiv-1605-03364 | Active Uncertainty Calibration in Bayesian ODE Solvers |  http://arxiv.org/abs/1605.03364  | author:Hans Kersting, Philipp Hennig category:cs.NA cs.LG math.NA stat.ML published:2016-05-11 summary:There is resurging interest, in statistics and machine learning, in solversfor ordinary differential equations (ODEs) that return probability measuresinstead of point estimates. Recently, Conrad et al. introduced a sampling-basedclass of methods that are 'well-calibrated' in a specific sense. But thecomputational cost of these methods is significantly above that of classicmethods. On the other hand, Schober et al. pointed out a precise connectionbetween classic Runge-Kutta ODE solvers and Gaussian filters, which gives onlya rough probabilistic calibration, but at negligible cost overhead. Byformulating the solution of ODEs as approximate inference in linear GaussianSDEs, we investigate a range of probabilistic ODE solvers, that bridge thetrade-off between computational cost and probabilistic calibration, andidentify the inaccurate gradient measurement as the crucial source ofuncertainty. We propose the novel filtering-based method Bayesian Quadraturefiltering (BQF) which uses Bayesian quadrature to actively learn theimprecision in the gradient measurement by collecting multiple gradientevaluations.
arxiv-1605-03631 | EEF: Exponentially Embedded Families with Class-Specific Features for Classification |  http://arxiv.org/abs/1605.03631  | author:Bo Tang, Steven Kay, Haibo He, Paul M. Baggenstoss category:stat.ML cs.LG published:2016-05-11 summary:In this letter, we present a novel exponentially embedded families (EEF)based classification method, in which the probability density function (PDF) onraw data is estimated from the PDF on features. With the PDF construction, weshow that class-specific features can be used in the proposed classificationmethod, instead of a common feature subset for all classes as used inconventional approaches. We apply the proposed EEF classifier for textcategorization as a case study and derive an optimal Bayesian classificationrule with class-specific feature selection based on the Information Gain (IG)score. The promising performance on real-life data sets demonstrates theeffectiveness of the proposed approach and indicates its wide potentialapplications.
arxiv-1605-03624 | Blind image separation based on exponentiated transmuted Weibull distribution |  http://arxiv.org/abs/1605.03624  | author:A. M. Adam, R. M. Farouk, M. E. Abd El-aziz category:cs.CV published:2016-05-11 summary:In recent years the processing of blind image separation has beeninvestigated. As a result, a number of feature extraction algorithms for directapplication of such image structures have been developed. For example,separation of mixed fingerprints found in any crime scene, in which a mixtureof two or more fingerprints may be obtained, for identification, we have toseparate them. In this paper, we have proposed a new technique for separating amultiple mixed images based on exponentiated transmuted Weibull distribution.To adaptively estimate the parameters of such score functions, an efficientmethod based on maximum likelihood and genetic algorithm will be used. We alsocalculate the accuracy of this proposed distribution and compare thealgorithmic performance using the efficient approach with other previousgeneralized distributions. We find from the numerical results that the proposeddistribution has flexibility and an efficient result
arxiv-1605-03389 | Efficiently Creating 3D Training Data for Fine Hand Pose Estimation |  http://arxiv.org/abs/1605.03389  | author:Markus Oberweger, Gernot Riegler, Paul Wohlhart, Vincent Lepetit category:cs.CV cs.HC published:2016-05-11 summary:While many recent hand pose estimation methods critically rely on a trainingset of labelled frames, the creation of such a dataset is a challenging taskthat has been overlooked so far. As a result, existing datasets are limited toa few sequences and individuals, with limited accuracy, and this prevents thesemethods from delivering their full potential. We propose a semi-automatedmethod for efficiently and accurately labeling each frame of a hand depth videowith the corresponding 3D locations of the joints: The user is asked to provideonly an estimate of the 2D reprojections of the visible joints in somereference frames, which are automatically selected to minimize the labelingwork by efficiently optimizing a sub-modular loss function. We then exploitspatial, temporal, and appearance constraints to retrieve the full 3D poses ofthe hand over the complete sequence. We show that this data can be used totrain a recent state-of-the-art hand pose estimation method, leading toincreased accuracy. The code and dataset can be found on our websitehttps://cvarlab.icg.tugraz.at/projects/hand_detection/
arxiv-1605-03391 | Random forests for survival analysis using maximally selected rank statistics |  http://arxiv.org/abs/1605.03391  | author:Marvin N. Wright, Theresa Dankowski, Andreas Ziegler category:stat.ML cs.LG published:2016-05-11 summary:The most popular approach for analyzing survival data is the Cox regressionmodel. The Cox model may, however, be misspecified, and its proportionalityassumption is not always fulfilled. An alternative approach is random forestsfor survival outcomes. The standard split criterion for random survival forestsis the log-rank test statistics, which favors splitting variables with manypossible split points. Conditional inference forests avoid this split pointselection bias. However, linear rank statistics are utilized in currentsoftware for conditional inference forests to select the optimal splittingvariable, which cannot detect non-linear effects in the independent variables.We therefore use maximally selected rank statistics for split point selectionin random forests for survival analysis. As in conditional inference forests,p-values for association between split points and survival time are minimized.We describe several p-value approximations and the implementation of theproposed random forest approach. A simulation study demonstrates that unbiasedsplit point selection is possible. However, there is a trade-off betweenunbiased split point selection and runtime. In benchmark studies of predictionperformance on simulated and real datasets the new method performs better thanrandom survival forests if informative dichotomous variables are combined withuninformative variables with more categories and better than conditionalinference forests if non-linear covariate effects are included. In a runtimecomparison the method proves to be computationally faster than bothalternatives, if a simple p-value approximation is used.
arxiv-1605-03621 | ASP Vision: Optically Computing the First Layer of Convolutional Neural Networks using Angle Sensitive Pixels |  http://arxiv.org/abs/1605.03621  | author:Huaijin Chen, Suren Jayasuriya, Jiyue Yang, Judy Stephen, Sriram Sivaramakrishnan, Ashok Veeraraghavan, Alyosha Molnar category:cs.CV published:2016-05-11 summary:Deep learning using convolutional neural networks (CNNs) is quickly becomingthe state-of-the-art for challenging computer vision applications. However,deep learning's power consumption and bandwidth requirements currently limitits application in embedded and mobile systems with tight energy budgets. Inthis paper, we explore the energy savings of optically computing the firstlayer of CNNs. To do so, we utilize bio-inspired Angle Sensitive Pixels (ASPs),custom CMOS diffractive image sensors which act similar to Gabor filter banksin the V1 layer of the human visual cortex. ASPs replace both image sensing andthe first layer of a conventional CNN by directly performing optical edgefiltering, saving sensing energy, data bandwidth, and CNN FLOPS to compute. Ourexperimental results (both on synthetic data and a hardware prototype) for avariety of vision tasks such as digit recognition, object recognition, and faceidentification demonstrate 97% reduction in image sensor power consumption and90% reduction in data bandwidth from sensor to CPU, while achieving similarperformance compared to traditional deep learning pipelines.
arxiv-1605-03843 | Asymptotic sequential Rademacher complexity of a finite function class |  http://arxiv.org/abs/1605.03843  | author:Dmitry B. Rokhlin category:cs.LG stat.ML 68Q32, 60F05 published:2016-05-11 summary:For a finite function class we describe the large sample limit of thesequential Rademacher complexity in terms of the viscosity solution of a$G$-heat equation. In the language of Peng's sublinear expectation theory, thesame quantity equals to the expected value of the largest order statistics of amultidimensional $G$-normal random variable. We illustrate this result byderiving upper and lower bounds for the asymptotic sequential Rademachercomplexity.
arxiv-1605-03428 | Image-level Classification in Hyperspectral Images using Feature Descriptors, with Application to Face Recognition |  http://arxiv.org/abs/1605.03428  | author:Vivek Sharma, Luc Van Gool category:cs.CV published:2016-05-11 summary:In this paper, we proposed a novel pipeline for image-level classification inthe hyperspectral images. By doing this, we show that the discriminativespectral information at image-level features lead to significantly improvedperformance in a face recognition task. We also explored the potential oftraditional feature descriptors in the hyperspectral images. From ourevaluations, we observe that SIFT features outperform the state-of-the-arthyperspectral face recognition methods, and also the other descriptors. Withthe increasing deployment of hyperspectral sensors in a multitude ofapplications, we believe that our approach can effectively exploit the spectralinformation in hyperspectral images, thus beneficial to more accurateclassification.
arxiv-1605-03560 | COCO: Performance Assessment |  http://arxiv.org/abs/1605.03560  | author:Nikolaus Hansen, Anne Auger, Dimo Brockhoff, Dejan Tušar, Tea Tušar category:cs.NE published:2016-05-11 summary:We present an any-time performance assessment for benchmarking numericaloptimization algorithms in a black-box scenario, applied within the COCObenchmarking platform. The performance assessment is based on runtimes measuredin number of objective function evaluations to reach one or several qualityindicator target values. We argue that runtime is the only available measurewith a generic, meaningful, and quantitative interpretation. We discuss thechoice of the target values, runlength-based targets, and the aggregation ofresults by using simulated restarts, averages, and empirical distributionfunctions.
arxiv-1605-03557 | View Synthesis by Appearance Flow |  http://arxiv.org/abs/1605.03557  | author:Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, Alexei A. Efros category:cs.CV published:2016-05-11 summary:Given one or more images of an object (or a scene), is it possible tosynthesize a new image of the same instance observed from an arbitraryviewpoint? In this paper, we attempt to tackle this problem, known as novelview synthesis, by re-formulating it as a pixel copying task that avoids thenotorious difficulties of generating pixels from scratch. Our approach is builton the observation that the visual appearance of different views of the sameinstance is highly correlated. Such correlation could be explicitly learned bytraining a convolutional neural network (CNN) to predict appearance flows --2-D coordinate vectors specifying which pixels in the input view could be usedto reconstruct the target view. We show that for both objects and scenes, ourapproach is able to generate higher-quality synthesized views with crisptexture and boundaries than previous CNN-based techniques.
arxiv-1605-03529 | On the Iteration Complexity of Oblivious First-Order Optimization Algorithms |  http://arxiv.org/abs/1605.03529  | author:Yossi Arjevani, Ohad Shamir category:math.OC cs.LG published:2016-05-11 summary:We consider a broad class of first-order optimization algorithms which are\emph{oblivious}, in the sense that their step sizes are scheduled regardlessof the function under consideration, except for limited side-information suchas smoothness or strong convexity parameters. With the knowledge of these twoparameters, we show that any such algorithm attains an iteration complexitylower bound of $\Omega(\sqrt{L/\epsilon})$ for $L$-smooth convex functions, and$\tilde{\Omega}(\sqrt{L/\mu}\ln(1/\epsilon))$ for $L$-smooth $\mu$-stronglyconvex functions. These lower bounds are stronger than those in the traditionaloracle model, as they hold independently of the dimension. To attain these, weabandon the oracle model in favor of a structure-based approach which buildsupon a framework recently proposed in (Arjevani et al., 2015). We further showthat without knowing the strong convexity parameter, it is impossible to attainan iteration complexity better than$\tilde{\Omega}\left((L/\mu)\ln(1/\epsilon)\right)$. This result is then usedto formalize an observation regarding $L$-smooth convex functions, namely, thatthe iteration complexity of algorithms employing time-invariant step sizes mustbe at least $\Omega(L/\epsilon)$.
arxiv-1605-03498 | Deep Neural Networks Under Stress |  http://arxiv.org/abs/1605.03498  | author:Micael Carvalho, Matthieu Cord, Sandra Avila, Nicolas Thome, Eduardo Valle category:cs.CV cs.AI published:2016-05-11 summary:In recent years, deep architectures have been used for transfer learning withstate-of-the-art performance in many datasets. The properties of their featuresremain, however, largely unstudied under the transfer perspective. In thiswork, we present an extensive analysis of the resiliency of feature vectorsextracted from deep models, with special focus on the trade-off betweenperformance and compression rate. By introducing perturbations to imagedescriptions extracted from a deep convolutional neural network, we changetheir precision and number of dimensions, measuring how it affects the finalscore. We show that deep features are more robust to these disturbances whencompared to classical approaches, achieving a compression rate of 98.4%, whilelosing only 0.88% of their original score for Pascal VOC 2007.
arxiv-1605-03477 | On-the-fly Network Pruning for Object Detection |  http://arxiv.org/abs/1605.03477  | author:Marc Masana, Joost van de Weijer, Andrew D. Bagdanov category:cs.CV published:2016-05-11 summary:Object detection with deep neural networks is often performed by passing afew thousand candidate bounding boxes through a deep neural network for eachimage. These bounding boxes are highly correlated since they originate from thesame image. In this paper we investigate how to exploit feature occurrence atthe image scale to prune the neural network which is subsequently applied toall bounding boxes. We show that removing units which have near-zero activationin the image allows us to significantly reduce the number of parameters in thenetwork. Results on the PASCAL 2007 Object Detection Challenge demonstrate thatup to 40% of units in some fully-connected layers can be entirely eliminatedwith little change in the detection result.
arxiv-1605-03468 | A constrained L1 minimization approach for estimating multiple Sparse Gaussian or Nonparanormal Graphical Models |  http://arxiv.org/abs/1605.03468  | author:Beilun Wang, Ritambhara Singh, Yanjun Qi category:cs.LG stat.ML published:2016-05-11 summary:The flood of multi-context measurement data from many scientific domains havecreated an urgent need to reconstruct context-specific variable networks, thatcan significantly simplify network-driven studies. Computationally, thisproblem can be formulated as jointly estimating multiple different, butrelated, sparse Undirected Graphical Models (UGM) from samples aggregatedacross several tasks. Previous joint-UGM studies could not fully address thechallenge since they mostly focus on Gaussian Graphical Models (GGM) and haveused likelihood-based formulations to push multiple estimated networks toward acommon pattern. Differently, we propose a novel approach, SIMULE (learningShared and Individual parts of MULtiple graphs Explicitly) to solve multi-taskUGM using a l1 constrained optimization. SIMULE can handle both multivariateGaussian and multivariate Nonparanormal data (greatly relaxing the normalityassumption most real data do not follow). SIMULE is cast as independentsubproblems of linear programming that can be solved efficiently. Itautomatically infers specific dependencies that are unique to each context aswell as shared substructures preserved among all the contexts. Theoretically weprove that SIMULE achieves a consistent estimation at rate O(log(Kp)/ntot) (notbeen proved before). On four synthetic datasets and two real datasets, SIMULEshows significant improvements over state-of-the-art multi-sGGM and single-UGMbaselines.
arxiv-1605-03124 | PARAPH: Presentation Attack Rejection by Analyzing Polarization Hypotheses |  http://arxiv.org/abs/1605.03124  | author:Ethan M. Rudd, Manuel Gunther, Terrance E. Boult category:cs.CV published:2016-05-10 summary:For applications such as airport border control, biometric technologies thatcan process many capture subjects quickly, efficiently, with weak supervision,and with minimal discomfort are desirable. Facial recognition is particularlyappealing because it is minimally invasive yet offers relatively goodrecognition performance. Unfortunately, the combination of weak supervision andminimal invasiveness makes even highly accurate facial recognition systemssusceptible to spoofing via presentation attacks. Thus, there is great demandfor an effective and low cost system capable of rejecting such attacks.To thisend we introduce PARAPH -- a novel hardware extension that exploits differentmeasurements of light polarization to yield an image space in whichpresentation media are readily discernible from Bona Fide facialcharacteristics. The PARAPH system is inexpensive with an added cost of lessthan 10 US dollars. The system makes two polarization measurements in rapidsuccession, allowing them to be approximately pixel-aligned, with a frame ratelimited by the camera, not the system. There are no moving parts above themolecular level, due to the efficient use of twisted nematic liquid crystals.We present evaluation images using three presentation attack media next to anactual face -- high quality photos on glossy and matte paper and a video of theface on an LCD. In each case, the actual face in the image generated by PARAPHis structurally discernible from the presentations, which appear either asnoise (print attacks) or saturated images (replay attacks).
arxiv-1605-03122 | Kernel-Based Structural Equation Models for Topology Identification of Directed Networks |  http://arxiv.org/abs/1605.03122  | author:Yanning Shen, Brian Baingana, Georgios B. Giannakis category:stat.ML published:2016-05-10 summary:Structural equation models (SEMs) have been widely adopted for inference ofcausal interactions in complex networks. Recent examples include unveilingtopologies of hidden causal networks over which processes such as spreadingdiseases, or rumors propagate. The appeal of SEMs in these settings stems fromtheir simplicity and tractability, since they typically assume lineardependencies among observable variables. Acknowledging the limitations inherentto adopting linear models, the present paper advocates nonlinear SEMs, whichaccount for (possible) nonlinear dependencies among network nodes. Theadvocated approach leverages kernels as a powerful encompassing framework fornonlinear modeling, and an efficient estimator with affordable tradeoffs is putforth. Interestingly, pursuit of the novel kernel-based approach yields aconvex regularized estimator that promotes edge sparsity, and is amenable toproximal-splitting optimization methods. To this end, solvers withcomplementary merits are developed by leveraging the alternating directionmethod of multipliers, and proximal gradient iterations. Experiments conductedon simulated data demonstrate that the novel approach outperforms linear SEMswith respect to edge detection errors. Furthermore, tests on a real geneexpression dataset unveil interesting new edges that were not revealed bylinear SEMs, which could shed more light on regulatory behavior of human genes.
arxiv-1605-03072 | Semi-Supervised Representation Learning based on Probabilistic Labeling |  http://arxiv.org/abs/1605.03072  | author:Ershad Banijamali, Ali Ghodsi category:cs.LG published:2016-05-10 summary:In this paper we present a new way of semi-supervised representationlearning. The algorithm is based on assigning class probabilities to unlabeleddata. The approach will use Hilber-Schmidt Independence Criterion (HSIC) tofind a mapping which takes the data to a lower dimensional space. We call thisalgorithm SSRL-PL. Use of unlabeled data for learning is not always beneficialand there is no algorithm which deterministically guarantee the improvement ofthe performance by using unlabeled data. Therefore, we also propose a bound onthe performance of the algorithm which can be used to determine theeffectiveness of using the structure of unlabeled data in the algorithm.
arxiv-1605-03040 | A note on the statistical view of matrix completion |  http://arxiv.org/abs/1605.03040  | author:Tianxi Li category:stat.ML published:2016-05-10 summary:A very simple interpretation of matrix completion problem is introduced basedon statistical models. Combined with the well-known results from missing dataanalysis, such interpretation indicates that matrix completion is still a validand principled estimation procedure even without the missing completely atrandom (MCAR) assumption, which almost all of the current theoretical studiesof matrix completion assume.
arxiv-1605-03027 | Destination Prediction by Trajectory Distribution Based Model |  http://arxiv.org/abs/1605.03027  | author:Philippe C. Besse, Brendan Guillouet, Jean-Michel Loubes, Francois Royer category:stat.ML published:2016-05-10 summary:In this paper we propose a new method to predict the final destination ofvehicle trips based on their initial partial trajectories. We first review howwe obtained clustering of trajectories that describes user behaviour. Then, weexplain how we model main traffic flow patterns by a mixture of 2d Gaussiandistributions. This yielded a density based clustering of locations, whichproduces a data driven grid of similar points within each pattern. We presenthow this model can be used to predict the final destination of a new trajectorybased on their first locations using a two step procedure: We first assign thenew trajectory to the clusters it mot likely belongs. Secondly, we usecharacteristics from trajectories inside these clusters to predict the finaldestination. Finally, we present experimental results of our methods forclassification of trajectories and final destination prediction on datasets oftimestamped GPS-Location of taxi trips. We test our methods on two differentdatasets, to assess the capacity of our method to adapt automatically todifferent subsets.
arxiv-1605-02877 | Performance Analysis of the Gradient Comparator LMS Algorithm |  http://arxiv.org/abs/1605.02877  | author:Bijit Kumar Das, Mrityunjoy Chakraborty category:cs.IT cs.LG math.IT published:2016-05-10 summary:The sparsity-aware zero attractor least mean square (ZA-LMS) algorithmmanifests much lower misadjustment in strongly sparse environment than itssparsity-agnostic counterpart, the least mean square (LMS), but is shown toperform worse than the LMS when sparsity of the impulse response decreases. Thereweighted variant of the ZA-LMS, namely RZA-LMS shows robustness against thisvariation in sparsity, but at the price of increased computational complexity.The other variants such as the l 0 -LMS and the improved proportionatenormalized LMS (IPNLMS), though perform satisfactorily, are alsocomputationally intensive. The gradient comparator LMS (GC-LMS) is a practicalsolution of this trade-off when hardware constraint is to be considered. Inthis paper, we analyse the mean and the mean square convergence performance ofthe GC-LMS algorithm in detail. The analyses satisfactorily match with thesimulation results.
arxiv-1605-02832 | Decoding Stacked Denoising Autoencoders |  http://arxiv.org/abs/1605.02832  | author:Sho Sonoda, Noboru Murata category:cs.LG stat.ML published:2016-05-10 summary:Data representation in a stacked denoising autoencoder is investigated.Decoding is a simple technique for translating a stacked denoising autoencoderinto a composition of denoising autoencoders in the ground space. In theinfinitesimal limit, a composition of denoising autoencoders is reduced to acontinuous denoising autoencoder, which is rich in analytic properties andgeometric interpretation. For example, the continuous denoising autoencodersolves the backward heat equation and transports each data point so as todecrease entropy of the data distribution. Together with ridgelet analysis, anintegral representation of a stacked denoising autoencoder is derived.
arxiv-1605-03012 | Automatic 3D liver location and segmentation via convolutional neural networks and graph cut |  http://arxiv.org/abs/1605.03012  | author:Fang Lu, Fa Wu, Peijun Hu, Zhiyi Peng, Dexing Kong category:cs.CV published:2016-05-10 summary:Purpose Segmentation of the liver from abdominal computed tomography (CT)image is an essential step in some computer assisted clinical interventions,such as surgery planning for living donor liver transplant (LDLT), radiotherapyand volume measurement. In this work, we develop a deep learning algorithm withgraph cut refinement to automatically segment liver in CT scans. Methods Theproposed method consists of two main steps: (i) simultaneously liver detectionand probabilistic segmentation using 3D convolutional neural networks (CNNs);(ii) accuracy refinement of initial segmentation with graph cut and thepreviously learned probability map. Results The proposed approach was validatedon forty CT volumes taken from two public databases MICCAI-Sliver07 and3Dircadb. For the MICCAI-Sliver07 test set, the calculated mean ratios ofvolumetric overlap error (VOE), relative volume difference (RVD), averagesymmetric surface distance (ASD), root mean square symmetric surface distance(RMSD) and maximum symmetric surface distance (MSD) are 5.9%, 2.7%, 0.91%, 1.88mm, and 18.94 mm, respectively. In the case of 20 3Dircadb data, the calculatedmean ratios of VOE, RVD, ASD, RMSD and MSD are 9.36%, 0.97%, 1.89%, 4.15 mm and33.14 mm, respectively. Conclusion The proposed method is fully automaticwithout any user interaction. Quantitative results reveal that the proposedapproach is efficient and accurate for hepatic volume estimation in a clinicalsetup. The high correlation between the automatic and manual references showsthat the proposed method can be good enough to replace the time-consuming andnon-reproducible manual segmentation method.
arxiv-1605-02878 | Adaptive Combination of l0 LMS Adaptive Filters for Sparse System Identification in Fluctuating Noise Power |  http://arxiv.org/abs/1605.02878  | author:Bijit Kumar Das, Mrityunjoy Chakraborty category:cs.IT cs.LG math.IT published:2016-05-10 summary:Recently, the l0-least mean square (l0-LMS) algorithm has been proposed toidentify sparse linear systems by employing a sparsity-promoting continuousfunction as an approximation of l0 pseudonorm penalty. However, the performanceof this algorithm is sensitive to the appropriate choice of the some parameterresponsible for the zero-attracting intensity. The optimum choice for thisparameter depends on the signal-to-noise ratio (SNR) prevailing in the system.Thus, it becomes difficult to fix a suitable value for this parameter,particularly in a situation where SNR fluctuates over time. In this work, wepropose several adaptive combinations of differently parameterized l0-LMS toget an overall satisfactory performance independent of the SNR, and discusssome issues relevant to these combination structures. We also demonstrate anefficient partial update scheme which not only reduces the number ofcomputations per iteration, but also achieves some interesting performance gaincompared with the full update case. Then, we propose a new recursive leastsquares (RLS)-type rule to update the combining parameter more efficiently.Finally, we extend the combination of two filters to a combination of M numberadaptive filters, which manifests further improvement for M > 2.
arxiv-1605-03148 | A Coverage Embedding Model for Neural Machine Translation |  http://arxiv.org/abs/1605.03148  | author:Haitao Mi, Baskaran Sankaran, Zhiguo Wang, Abe Ittycheriah category:cs.CL published:2016-05-10 summary:In this paper, we enhance the attention-based neural machine translation byadding an explicit coverage embedding model to alleviate issues of repeatingand dropping translations in NMT. For each source word, our model starts with afull coverage embedding vector, and then keeps updating it with a gatedrecurrent unit as the translation goes. All the initialized coverage embeddingsand updating matrix are learned in the training procedure. Experiments on thelarge-scale Chinese-to-English task show that our enhanced model improves thetranslation quality significantly on various test sets over the strong largevocabulary NMT system.
arxiv-1605-02887 | Learning theory estimates with observations from general stationary stochastic processes |  http://arxiv.org/abs/1605.02887  | author:Hanyuan Hang, Yunlong Feng, Ingo Steinwart, Johan A. K. Suykens category:stat.ML cs.LG published:2016-05-10 summary:This paper investigates the supervised learning problem with observationsdrawn from certain general stationary stochastic processes. Here by\emph{general}, we mean that many stationary stochastic processes can beincluded. We show that when the stochastic processes satisfy a generalizedBernstein-type inequality, a unified treatment on analyzing the learningschemes with various mixing processes can be conducted and a sharp oracleinequality for generic regularized empirical risk minimization schemes can beestablished. The obtained oracle inequality is then applied to deriveconvergence rates for several learning schemes such as empirical riskminimization (ERM), least squares support vector machines (LS-SVMs) using givengeneric kernels, and SVMs using Gaussian kernels for both least squares andquantile regression. It turns out that for i.i.d.~processes, our learning ratesfor ERM recover the optimal rates. On the other hand, for non-i.i.d.~processesincluding geometrically $\alpha$-mixing Markov processes, geometrically$\alpha$-mixing processes with restricted decay, $\phi$-mixing processes, and(time-reversed) geometrically $\mathcal{C}$-mixing processes, our learningrates for SVMs with Gaussian kernels match, up to some arbitrarily small extraterm in the exponent, the optimal rates. For the remaining cases, our rates areat least close to the optimal rates. As a by-product, the assumed generalizedBernstein-type inequality also provides an interpretation of the so-called"effective number of observations" for various mixing processes.
arxiv-1605-03004 | MUST-CNN: A Multilayer Shift-and-Stitch Deep Convolutional Architecture for Sequence-based Protein Structure Prediction |  http://arxiv.org/abs/1605.03004  | author:Zeming Lin, Jack Lanchantin, Yanjun Qi category:cs.LG published:2016-05-10 summary:Predicting protein properties such as solvent accessibility and secondarystructure from its primary amino acid sequence is an important task inbioinformatics. Recently, a few deep learning models have surpassed thetraditional window based multilayer perceptron. Taking inspiration from theimage classification domain we propose a deep convolutional neural networkarchitecture, MUST-CNN, to predict protein properties. This architecture uses anovel multilayer shift-and-stitch (MUST) technique to generate fully denseper-position predictions on protein sequences. Our model is significantlysimpler than the state-of-the-art, yet achieves better results. By combiningMUST and the efficient convolution operation, we can consider far moreparameters while retaining very fast prediction speeds. We beat thestate-of-the-art performance on two large protein property prediction datasets.
arxiv-1605-03150 | Road Detection through Supervised Classification |  http://arxiv.org/abs/1605.03150  | author:Yasamin Alkhorshid, Kamelia Aryafar, Sven Bauer, Gerd Wanielik category:cs.CV published:2016-05-10 summary:Autonomous driving is a rapidly evolving technology. Autonomous vehicles arecapable of sensing their environment and navigating without human input throughsensory information such as radar, lidar, GNSS, vehicle odometry, and computervision. This sensory input provides a rich dataset that can be used incombination with machine learning models to tackle multiple problems insupervised settings. In this paper we focus on road detection throughgray-scale images as the sole sensory input. Our contributions are twofold:first, we introduce an annotated dataset of urban roads for machine learningtasks; second, we introduce a road detection framework on this dataset throughsupervised classification and hand-crafted feature vectors.
arxiv-1605-02892 | Compact Hash Codes for Efficient Visual Descriptors Retrieval in Large Scale Databases |  http://arxiv.org/abs/1605.02892  | author:Simone Ercoli, Marco Bertini, Alberto Del Bimbo category:cs.CV published:2016-05-10 summary:In this paper we present an efficient method for visual descriptors retrievalbased on compact hash codes computed using a multiple k-means assignment. Themethod has been applied to the problem of approximate nearest neighbor (ANN)search of local and global visual content descriptors, and it has been testedon different datasets: three large scale public datasets of up to one billiondescriptors (BIGANN) and, supported by recent progress in convolutional neuralnetworks (CNNs), also on the CIFAR-10 and MNIST datasets. Experimental resultsshow that, despite its simplicity, the proposed method obtains a very highperformance that makes it superior to more complex state-of-the-art methods.
arxiv-1605-02914 | Recurrent Human Pose Estimation |  http://arxiv.org/abs/1605.02914  | author:Vasileios Belagiannis, Andrew Zisserman category:cs.CV cs.NE published:2016-05-10 summary:We propose a novel ConvNet model for predicting 2D human body poses in animage. The model regresses a heatmap representation for each body keypoint, andis able to learn and represent both the part appearances and the context of thepart configuration. We make the following three contributions: (i) anarchitecture combining a feed forward module with a recurrent module, where therecurrent module can be run iteratively to improve the performance; (ii) themodel can be trained end-to-end and from scratch, with auxiliary lossesincorporated to improve performance; (iii) we investigate whether keypointvisibility can also be predicted. The model is evaluated on two benchmarkdatasets. The result is a simple architecture that achieves performance on parwith the state of the art, but without the complexity of a graphical modelstage (or layers).
arxiv-1605-02989 | An efficient K-means algorithm for Massive Data |  http://arxiv.org/abs/1605.02989  | author:Marco Capó, Aritz Pérez, José Antonio Lozano category:stat.ML cs.LG published:2016-05-10 summary:Due to the progressive growth of the amount of data available in a widevariety of scientific fields, it has become more difficult to ma- nipulate andanalyze such information. Even though datasets have grown in size, the K-meansalgorithm remains as one of the most popular clustering methods, in spite ofits dependency on the initial settings and high computational cost, especiallyin terms of distance computations. In this work, we propose an efficientapproximation to the K-means problem intended for massive data. Our approachrecursively partitions the entire dataset into a small number of sub- sets,each of which is characterized by its representative (center of mass) andweight (cardinality), afterwards a weighted version of the K-means algorithm isapplied over such local representation, which can drastically reduce the numberof distances computed. In addition to some theoretical properties, experimentalresults indicate that our method outperforms well-known approaches, such as theK-means++ and the minibatch K-means, in terms of the relation between number ofdistance computations and the quality of the approximation.
arxiv-1605-03170 | DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model |  http://arxiv.org/abs/1605.03170  | author:Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres, Mykhaylo Andriluka, Bernt Schiele category:cs.CV published:2016-05-10 summary:The goal of this paper is to advance the state-of-the-art of articulated poseestimation in scenes with multiple people. To that end we contribute on threefronts. We propose (1) improved body part detectors that generate effectivebottom-up proposals for body parts; (2) novel image-conditioned pairwise termsthat allow to assemble the proposals into a variable number of consistent bodypart configurations; and (3) an incremental optimization strategy that exploresthe search space more efficiently thus leading both to better performance andsignificant speed-up factors. We evaluate our approach on two single-person andtwo multi-person pose estimation benchmarks. The proposed approachsignificantly outperforms best known multi-person pose estimation results whiledemonstrating competitive performance on the task of single person poseestimation. Models and code available at http://pose.mpi-inf.mpg.de
arxiv-1605-02964 | Weakly Supervised Learning of Affordances |  http://arxiv.org/abs/1605.02964  | author:Abhilash Srikantha, Juergen Gall category:cs.CV published:2016-05-10 summary:Localizing functional regions of objects or affordances is an importantaspect of scene understanding. In this work, we cast the problem of affordancesegmentation as that of semantic image segmentation. In order to explorevarious levels of supervision, we introduce a pixel-annotated affordancedataset of 3090 images containing 9916 object instances with rich contextualinformation in terms of human-object interactions. We use a deep convolutionalneural network within an expectation maximization framework to take advantageof weakly labeled data like image level annotations or keypoint annotations. Weshow that a further reduction in supervision is possible with a minimal loss inperformance when human pose is used as context.
arxiv-1605-02916 | Grammatical Case Based IS-A Relation Extraction with Boosting for Polish |  http://arxiv.org/abs/1605.02916  | author:Paweł Łoziński, Dariusz Czerski, Mieczysław A. Kłopotek category:cs.CL cs.IR H.3.1 published:2016-05-10 summary:Pattern-based methods of IS-A relation extraction rely heavily on so calledHearst patterns. These are ways of expressing instance enumerations of a classin natural language. While these lexico-syntactic patterns prove quite useful,they may not capture all taxonomical relations expressed in text. Therefore inthis paper we describe a novel method of IS-A relation extraction frompatterns, which uses morpho-syntactical annotations along with grammatical caseof noun phrases that constitute entities participating in IS-A relation. Wealso describe a method for increasing the number of extracted relations that wecall pseudo-subclass boosting which has potential application in anypattern-based relation extraction method. Experiments were conducted on acorpus of about 0.5 billion web documents in Polish language.
arxiv-1605-02971 | Structured Receptive Fields in CNNs |  http://arxiv.org/abs/1605.02971  | author:Jörn-Henrik Jacobsen, Jan van Gemert, Zhongyu Lou, Arnold W. M. Smeulders category:cs.CV published:2016-05-10 summary:Learning powerful feature representations with CNNs is hard when trainingdata are limited. Pre-training is one way to overcome this, but it requireslarge datasets sufficiently similar to the target domain. Another option is todesign priors into the model, which can range from tuned hyperparameters tofully engineered representations like Scattering Networks. We combine theseideas into structured receptive field networks, a model which has a fixedfilter basis and yet retains the flexibility of CNNs. This flexibility isachieved by expressing receptive fields in CNNs as a weighted sum over a fixedbasis which is similar in spirit to Scattering Networks. The key difference isthat we learn arbitrary effective filter sets from the basis rather thanmodeling the filters. This approach explicitly connects classical multiscaleimage analysis with general CNNs. With structured receptive field networks, weimprove considerably over unstructured CNNs for small and medium datasetscenarios as well as over Scattering for large datasets. We validate ourfindings on ILSVRC2012, Cifar-10, Cifar-100 and MNIST. As a realistic smalldataset example, we show state-of-the-art classification results on popular 3DMRI brain-disease datasets where pre-training is difficult due to a lack oflarge public datasets in a similar domain.
arxiv-1605-03209 | Vocabulary Manipulation for Neural Machine Translation |  http://arxiv.org/abs/1605.03209  | author:Haitao Mi, Zhiguo Wang, Abe Ittycheriah category:cs.CL published:2016-05-10 summary:In order to capture rich language phenomena, neural machine translationmodels have to use a large vocabulary size, which requires high computing timeand large memory usage. In this paper, we alleviate this issue by introducing asentence-level or batch-level vocabulary, which is only a very small sub-set ofthe full output vocabulary. For each sentence or batch, we only predict thetarget words in its sentence-level or batch-level vocabulary. Thus, we reduceboth the computing time and the memory usage. Our method simply takes intoaccount the translation options of each word or phrase in the source sentence,and picks a very small target vocabulary for each sentence based on aword-to-word translation model or a bilingual phrase library learned from atraditional machine translation model. Experimental results on the large-scaleEnglish-to-French task show that our method achieves better translationperformance by 1 BLEU point over the large vocabulary neural machinetranslation system of Jean et al. (2015).
arxiv-1605-02948 | A Bayesian Approach to Biomedical Text Summarization |  http://arxiv.org/abs/1605.02948  | author:Milad Moradi, Nasser Ghadiri category:cs.CL cs.IR I.2.7; J.3 published:2016-05-10 summary:Many biomedical researchers and clinicians are faced with the informationoverload problem. Attaining desirable information from the ever-increasing bodyof knowledge is a difficult task without using automatic text summarizationtools that help them to acquire the intended information in shorter time andwith less effort. Although many text summarization methods have been proposed,developing domain-specific methods for the biomedical texts is a challengingtask. In this paper, we propose a biomedical text summarization method, basedon concept extraction technique and a novel sentence classification approach.We incorporate domain knowledge by utilizing the UMLS knowledge source and thena\"ive Bayes classifier to build our text summarizer. Unlike many existingmethods, the system learns to classify the sentences without the need fortraining data, and selects them for the summary according to the distributionof essential concepts within the original text. We show that the use ofcritical concepts to represent the sentences as vectors of features, andclassifying the sentences based on the distribution of those concepts, willimprove the performance of automatic summarization. An extensive evaluation isperformed on a collection of scientific articles in biomedical domain. Theresults show that our proposed method outperforms several well-knownresearch-based, commercial and baseline summarizers according to the mostcommonly used ROUGE evaluation metrics.
arxiv-1605-02917 | Web Spam Detection Using Multiple Kernels in Twin Support Vector Machine |  http://arxiv.org/abs/1605.02917  | author:Seyed Hamid Reza Mohammadi, Mohammad Ali Zare Chahooki category:cs.IR cs.LG published:2016-05-10 summary:Search engines are the most important tools for web data acquisition. Webpages are crawled and indexed by search Engines. Users typically locate usefulweb pages by querying a search engine. One of the challenges in search enginesadministration is spam pages which waste search engine resources. These pagesby deception of search engine ranking algorithms try to be showed in the firstpage of results. There are many approaches to web spam pages detection such asmeasurement of HTML code style similarity, pages linguistic pattern analysisand machine learning algorithm on page content features. One of the famousalgorithms has been used in machine learning approach is Support Vector Machine(SVM) classifier. Recently basic structure of SVM has been changed by newextensions to increase robustness and classification accuracy. In this paper weimproved accuracy of web spam detection by using two nonlinear kernels intoTwin SVM (TSVM) as an improved extension of SVM. The classifier ability to dataseparation has been increased by using two separated kernels for each class ofdata. Effectiveness of new proposed method has been experimented with twopublicly used spam datasets called UK-2007 and UK-2006. Results show theeffectiveness of proposed kernelized version of TSVM in web spam pagedetection.
arxiv-1605-02869 | Modeling Short Over-Dispersed Spike-Train Data: A Hierarchical Parametric Empirical Bayes Framework |  http://arxiv.org/abs/1605.02869  | author:Qi She, Beth Jelfs, Rosa H. M. Chan category:q-bio.QM q-bio.NC stat.ML published:2016-05-10 summary:In this letter, a Hierarchical Parametric Empirical Bayes (HPEB) model isproposed to fit spike count data. We have integrated Generalized Linear Modelsand empirical Bayes theory to simultaneously solve three problems: (1)over-dispersion of spike count values; (2) biased estimation of the maximumlikelihood method and (3) difficulty in sampling from high-dimensional datawith fully Bayes estimators. We apply the model to study both simulated dataand experimental neural data from the retina. The simulation results indicatethat the new model can estimate both the weights of connections among neuralpopulations and the output firing rates efficiently and accurately. The resultsfrom the retinal datasets show that the proposed model outperforms bothstandard Poisson and Negative Binomial Generalized Linear Models in terms ofthe prediction log-likelihood of held-out datasets.
arxiv-1605-02827 | When Do Luxury Cars Hit the Road? Findings by A Big Data Approach |  http://arxiv.org/abs/1605.02827  | author:Yang Feng, Jiebo Luo category:cs.CY cs.CV published:2016-05-10 summary:In this paper, we focus on studying the appearing time of different kinds ofcars on the road. This information will enable us to infer the life style ofthe car owners. The results can further be used to guide marketing towards carowners. Conventionally, this kind of study is carried out by sending outquestionnaires, which is limited in scale and diversity. To solve this problem,we propose a fully automatic method to carry out this study. Our study is basedon publicly available surveillance camera data. To make the results reliable,we only use the high resolution cameras (i.e. resolution greater than $1280\times 720$). Images from the public cameras are downloaded every minute. Afterobtaining 50,000 images, we apply faster R-CNN (region-based convoluntionalneural network) to detect the cars in the downloaded images and a fine-tunedVGG16 model is used to recognize the car makes. Based on the recognitionresults, we present a data-driven analysis on the relationship between carmakes and their appearing times, with implications on lifestyles.
arxiv-1605-02945 | The Yahoo Query Treebank, V. 1.0 |  http://arxiv.org/abs/1605.02945  | author:Yuval Pinter, Roi Reichart, Idan Szpektor category:cs.CL cs.IR published:2016-05-10 summary:A description and annotation guidelines for the Yahoo Webscope release ofQuery Treebank, Version 1.0, May 2016.
arxiv-1605-03222 | Action Recognition in Video Using Sparse Coding and Relative Features |  http://arxiv.org/abs/1605.03222  | author:Anali Alfaro, Domingo Mery, Alvaro Soto category:cs.CV published:2016-05-10 summary:This work presents an approach to category-based action recognition in videousing sparse coding techniques. The proposed approach includes two maincontributions: i) A new method to handle intra-class variations by decomposingeach video into a reduced set of representative atomic action acts orkey-sequences, and ii) A new video descriptor, ITRA: Inter-Temporal RelationalAct Descriptor, that exploits the power of comparative reasoning to capturerelative similarity relations among key-sequences. In terms of the method toobtain key-sequences, we introduce a loss function that, for each video, leadsto the identification of a sparse set of representative key-frames capturingboth, relevant particularities arising in the input video, as well as relevantgeneralities arising in the complete class collection. In terms of the methodto obtain the ITRA descriptor, we introduce a novel scheme to quantify relativeintra and inter-class similarities among local temporal patterns arising in thevideos. The resulting ITRA descriptor demonstrates to be highly effective todiscriminate among action categories. As a result, the proposed approachreaches remarkable action recognition performance on several popular benchmarkdatasets, outperforming alternative state-of-the-art techniques by a largemargin.
arxiv-1605-03234 | Performance Bounds for Sparse Signal Reconstruction with Multiple Side Information |  http://arxiv.org/abs/1605.03234  | author:Huynh Van Luong, Jurgen Seiler, Andre Kaup, Soren Forchhammer, Nikos Deligiannis category:cs.IT cs.CV math.IT math.OC published:2016-05-10 summary:In the context of compressive sensing (CS), this paper considers the problemof reconstructing sparse signals with the aid of other given correlated sourcesas multiple side information (SI). To address this problem, we propose areconstruction algorithm with multiple SI (RAMSI) that solves a generalweighted $n$-$\ell_{1}$ norm minimization. The proposed RAMSI algorithm takesadvantage of both CS and the $n$-$\ell_{1}$ minimization by adaptivelycomputing optimal weights among SI signals at every reconstructed iteration. Inaddition, we establish theoretical performance bounds on the number ofmeasurements that are required to successfully reconstruct the original sparsesource using RAMSI under arbitrary support SI conditions. The analyses of theestablished bounds reveal that RAMSI can achieve sharper bounds and significantperformance improvements compared to classical CS. We evaluate experimentallythe proposed algorithm and the established bounds using synthetic sparsesignals as well as correlated feature histograms, extracted from a multiviewimage database for object recognition. The obtained results show clearly thatthe proposed RAMSI algorithm outperforms classical CS and CS with single SI interms of both the theoretical bounds and the practical performance.
arxiv-1605-02766 | LightNet: A Versatile, Standalone Matlab-based Environment for Deep Learning |  http://arxiv.org/abs/1605.02766  | author:Chengxi Ye, Chen Zhao, Yezhou Yang, Cornelia Fermuller, Yiannis Aloimonos category:cs.LG cs.CV cs.NE published:2016-05-09 summary:LightNet is a lightweight, versatile and purely Matlab-based deep learningframework. The aim of the design is to provide an easy-to-understand,easy-to-use and efficient computational platform for deep learning research.The implemented framework supports major deep learning architectures such asMultilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) andRecurrent Neural Networks (RNN). The framework also supports both CPU and GPUfor computation and the switch between them is straightforward. Differentapplications in computer vision, natural language processing and robotics aredemonstrated as experiments.
arxiv-1605-02783 | Arm muscular effort estimation from images using Computer Vision and Machine Learning |  http://arxiv.org/abs/1605.02783  | author:Leandro Abraham, Facundo Bromberg, Raymundo Forradellas category:cs.CV published:2016-05-09 summary:A problem of great interest in biomechanics is the estimation of internalmuscle forces and joint torques. Direct measurement of these variables is notpossible, so it is addressed in practice using mechanical models that takes asinput kinematics and muscle activity. Input data for these models is capturedusing impractical, intrusive and expensive devices. In this work we presentfirsts steps towards capturing muscle activity of the arm biceps through a lessintrusive and more economic process. First, we consider an isometriccontraction setup for which muscle activity directly correlates with externalweight supported by the muscle. Then, using Computer Vision feature extractionalgorithms (Local Binary Patterns and Color Histograms) and Machine Learningalgorithms we learn a classifier (e.g., Support Vector Machines and RandomForests) for inferring the weight held by the muscle. We consider two similarproblems: discriminating between two weights and discriminating between fourtarget weights. Also, we consider the use of calibration information to bettergeneralize over unseen subjects. We obtained promising results showingaccuracies of $79.78\%$ and $42.34\%$ for the cases with calibration of two andfour levels respectively. The main difficulty of this approach is to generalizethe learned model over unseen subjects, due to the great differences existingamong human subjects arms. Considering the difficulty in the requirement ofgeneralization, and the simplicity of our approach, it is enlightening toachieve over random results, suggesting that it is possible to extractmeaningful information for the predictive task.
arxiv-1605-02775 | Image Classification of Grapevine Buds using Scale-Invariant Features Transform, Bag of Features and Support Vector Machines |  http://arxiv.org/abs/1605.02775  | author:Diego Sebastián Pérez, Facundo Bromberg, Carlos Ariel Diaz category:cs.CV published:2016-05-09 summary:In viticulture, there are several applications where bud detection invineyard images is a necessary task, susceptible of being automated through theuse of computer vision methods. A common and effective family of visualdetection algorithms are the scanning-window type, that slide a (usually) fixedsize window along the original image, classifying each resulting windowed-patchas containing or not containing the target object. The simplicity of thesealgorithms finds its most challenging aspect in the classification stage.Interested in grapevine buds detection in natural field conditions, this paperpresents a classification method for images of grapevine buds ranging 100 to1600 pixels in diameter, captured in outdoor, under natural field conditions,in winter (i.e., no grape bunches, very few leaves, and dormant buds), withoutartificial background, and with minimum equipment requirements. The proposedmethod uses well-known computer vision technologies: Scale-Invariant FeatureTransform for calculating low-level features, Bag of Features for building animage descriptor, and Support Vector Machines for training a classifier. Whenevaluated over images containing buds of at least 100 pixels in diameter, theapproach achieves a recall higher than 0.9 and a precision of 0.86 over allwindowed-patches covering the whole bud and down to 60% of it, and scaled up towindow patches containing a proportion of 20%-80% of bud versus backgroundpixels. This robustness on the position and size of the window demonstrates itsviability for use as the classification stage in a scanning-window detectionalgorithms.
arxiv-1605-02720 | Anytime Bi-Objective Optimization with a Hybrid Multi-Objective CMA-ES (HMO-CMA-ES) |  http://arxiv.org/abs/1605.02720  | author:Ilya Loshchilov, Tobias Glasmachers category:cs.NE published:2016-05-09 summary:We propose a multi-objective optimization algorithm aimed at achieving goodanytime performance over a wide range of problems. Performance is assessed interms of the hypervolume metric. The algorithm called HMO-CMA-ES represents ahybrid of several old and new variants of CMA-ES, complemented by BOBYQA as awarm start. We benchmark HMO-CMA-ES on the recently introduced bi-objectiveproblem suite of the COCO framework (COmparing Continuous Optimizers),consisting of 55 scalable continuous optimization problems, which is used bythe Black-Box Optimization Benchmarking (BBOB) Workshop 2016.
arxiv-1605-02711 | Stochastic Variance Reduced Optimization for Nonconvex Sparse Learning |  http://arxiv.org/abs/1605.02711  | author:Xingguo Li, Tuo Zhao, Raman Arora, Han Liu, Jarvis Haupt category:cs.LG math.OC stat.ML published:2016-05-09 summary:We propose a stochastic variance reduced optimization algorithm for solving aclass of large-scale nonconvex optimization problems with cardinalityconstraints, and provide sufficient conditions under which the proposedalgorithm enjoys strong linear convergence guarantees and optimal estimationaccuracy in high dimensions. We further extend our analysis to an asynchronousvariant of the approach, and demonstrate a near linear speedup in sparsesettings. Numerical experiments demonstrate the efficiency of our method interms of both parameter estimation and computational performance.
arxiv-1605-02686 | An End-to-End System for Unconstrained Face Verification with Deep Convolutional Neural Networks |  http://arxiv.org/abs/1605.02686  | author:Jun-Cheng Chen, Rajeev Ranjan, Swami Sankaranarayanan, Amit Kumar, Ching-Hui Chen, Vishal M. Patel, Carlos D. Castillo, Rama Chellappa category:cs.CV published:2016-05-09 summary:Over the last four years, methods based on Deep Convolutional Neural Networks(DCNNs) have shown impressive performance improvements for object detection andrecognition problems. This has been made possible due to the availability oflarge annotated datasets, a better understanding of the non-linear mappingbetween input images and class labels as well as the affordability of GPUs. Inthis paper, we present the design details of a deep learning system forend-to-end unconstrained face verification/recognition. The quantitativeperformance evaluation is conducted using the newly released IARPA JanusBenchmark A (IJB-A), the JANUS Challenge Set 2 (JANUS CS2), and the LFWdataset. The IJB-A dataset includes real-world unconstrained faces of 500subjects with significant pose and illumination variations which are muchharder than the Labeled Faces in the Wild (LFW) and Youtube Face (YTF)datasets. JANUS CS2 is the extended version of IJB-A which contains not onlyall the images/frames of IJB-A but also includes the original videos forevaluating video-based face verification system. Some open issues regardingDCNNs for object recognition problems are then discussed.
arxiv-1605-02699 | A Theoretical Analysis of Deep Neural Networks for Texture Classification |  http://arxiv.org/abs/1605.02699  | author:Saikat Basu, Manohar Karki, Robert DiBiano, Supratik Mukhopadhyay, Sangram Ganguly, Ramakrishna Nemani, Shreekant Gayaka category:cs.CV cs.LG stat.ML published:2016-05-09 summary:We investigate the use of Deep Neural Networks for the classification ofimage datasets where texture features are important for generatingclass-conditional discriminative representations. To this end, we first derivethe size of the feature space for some standard textural features extractedfrom the input dataset and then use the theory of Vapnik-Chervonenkis dimensionto show that hand-crafted feature extraction creates low-dimensionalrepresentations which help in reducing the overall excess error rate. As acorollary to this analysis, we derive for the first time upper bounds on the VCdimension of Convolutional Neural Network as well as Dropout and Dropconnectnetworks and the relation between excess error rate of Dropout and Dropconnectnetworks. The concept of intrinsic dimension is used to validate the intuitionthat texture-based datasets are inherently higher dimensional as compared tohandwritten digits or other object recognition datasets and hence moredifficult to be shattered by neural networks. We then derive the mean distancefrom the centroid to the nearest and farthest sampling points in ann-dimensional manifold and show that the Relative Contrast of the sample datavanishes as dimensionality of the underlying vector space tends to infinity.
arxiv-1605-02784 | Identification of refugee influx patterns in Greece via model-theoretic analysis of daily arrivals |  http://arxiv.org/abs/1605.02784  | author:Harris V. Georgiou category:stat.ML cs.CY published:2016-05-09 summary:The refugee crisis is perhaps the single most challenging problem for Europetoday. Hundreds of thousands of people have already traveled across dangeroussea passages from Turkish shores to Greek islands, resulting in thousands ofdead and missing, despite the best rescue efforts from both sides. One of themain reasons is the total lack of any early warning-alerting system, whichcould provide some preparation time for the prompt and effective deployment ofresources at the hot zones. This work is such an attempt for a systemicanalysis of the refugee influx in Greece, aiming at (a) the statistical andsignal-level characterization of the smuggling networks and (b) the formulationand preliminary assessment of such models for predictive purposes, i.e., as thebasis of such an early warning-alerting protocol. To our knowledge, this is thefirst-ever attempt to design such a system, since this refugee crisis itselfand its geographical properties are unique (intense event handling, little orno warning). The analysis employs a wide range of statistical, signal-based andmatrix factorization (decomposition) techniques, including linear &linear-cosine regression, spectral analysis, ARMA, SVD, Probabilistic PCA, ICA,K-SVD for Dictionary Learning, as well as fractal dimension analysis. It isestablished that the behavioral patterns of the smuggling networks closelymatch (as expected) the regular burst and pause periods of store-and-forwardnetworks in digital communications. There are also major periodic trends in therange of 6.2-6.5 days and strong correlations in lags of four or more days,with distinct preference in the Sunday-Monday 48-hour time frame. These resultsshow that such models can be used successfully for short-term forecasting ofthe influx intensity, producing an invaluable operational asset for planners,decision-makers and first-responders.
arxiv-1605-02541 | Mean Absolute Percentage Error for regression models |  http://arxiv.org/abs/1605.02541  | author:Arnaud De Myttenaere, Boris Golden, Bénédicte Le Grand, Fabrice Rossi category:stat.ML published:2016-05-09 summary:We study in this paper the consequences of using the Mean Absolute PercentageError (MAPE) as a measure of quality for regression models. We prove theexistence of an optimal MAPE model and we show the universal consistency ofEmpirical Risk Minimization based on the MAPE. We also show that finding thebest model under the MAPE is equivalent to doing weighted Mean Absolute Error(MAE) regression, and we apply this weighting strategy to kernel regression.The behavior of the MAPE kernel regression is illustrated on simulated data.
