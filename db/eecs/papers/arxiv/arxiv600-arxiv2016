arxiv-600-1 | Graph Prediction in a Low-Rank and Autoregressive Setting | http://arxiv.org/pdf/1205.1406v2.pdf | author:Emile Richard, Pierre-Andre Savalle, Nicolas Vayatis category:stat.ML published:2012-05-07 summary:We study the problem of prediction for evolving graph data. We formulate theproblem as the minimization of a convex objective encouraging sparsity andlow-rank of the solution, that reflect natural graph properties. The convexformulation allows to obtain oracle inequalities and efficient solvers. Weprovide empirical results for our algorithm and comparison with competingmethods, and point out two open questions related to compressed sensing andalgebra of low-rank and sparse matrices.
arxiv-600-2 | Risk estimation for matrix recovery with spectral regularization | http://arxiv.org/pdf/1205.1482v3.pdf | author:Charles-Alban Deledalle, Samuel Vaiter, Gabriel Peyré, Jalal Fadili, Charles Dossal category:math.OC cs.IT cs.LG math.IT math.ST stat.ML stat.TH published:2012-05-07 summary:In this paper, we develop an approach to recursively estimate the quadraticrisk for matrix recovery problems regularized with spectral functions. Towardthis end, in the spirit of the SURE theory, a key step is to compute the (weak)derivative and divergence of a solution with respect to the observations. Assuch a solution is not available in closed form, but rather through a proximalsplitting algorithm, we propose to recursively compute the divergence from thesequence of iterates. A second challenge that we unlocked is the computation ofthe (weak) derivative of the proximity operator of a spectral function. To showthe potential applicability of our approach, we exemplify it on a matrixcompletion problem to objectively and automatically select the regularizationparameter.
arxiv-600-3 | Dynamic Multi-Relational Chinese Restaurant Process for Analyzing Influences on Users in Social Media | http://arxiv.org/pdf/1205.1456v1.pdf | author:Himabindu Lakkaraju, Indrajit Bhattacharya, Chiranjib Bhattacharyya category:cs.SI cs.LG physics.soc-ph published:2012-05-07 summary:We study the problem of analyzing influence of various factors affectingindividual messages posted in social media. The problem is challenging becauseof various types of influences propagating through the social media networkthat act simultaneously on any user. Additionally, the topic composition of theinfluencing factors and the susceptibility of users to these influences evolveover time. This problem has not studied before, and off-the-shelf models areunsuitable for this purpose. To capture the complex interplay of these variousfactors, we propose a new non-parametric model called the DynamicMulti-Relational Chinese Restaurant Process. This accounts for the user networkfor data generation and also allows the parameters to evolve over time.Designing inference algorithms for this model suited for large scalesocial-media data is another challenge. To this end, we propose a scalable andmulti-threaded inference algorithm based on online Gibbs Sampling. Extensiveevaluations on large-scale Twitter and Facebook data show that the extractedtopics when applied to authorship and commenting prediction outperformstate-of-the-art baselines. More importantly, our model produces valuableinsights on topic trends and user personality trends, beyond the capability ofexisting approaches.
arxiv-600-4 | Image Enhancement with Statistical Estimation | http://arxiv.org/pdf/1205.1365v1.pdf | author:Aroop Mukherjee, Soumen Kanrar category:cs.MM cs.CV published:2012-05-07 summary:Contrast enhancement is an important area of research for the image analysis.Over the decade, the researcher worked on this domain to develop an efficientand adequate algorithm. The proposed method will enhance the contrast of imageusing Binarization method with the help of Maximum Likelihood Estimation (MLE).The paper aims to enhance the image contrast of bimodal and multi-modal images.The proposed methodology use to collect mathematical information retrieves fromthe image. In this paper, we are using binarization method that generates thedesired histogram by separating image nodes. It generates the enhanced imageusing histogram specification with binarization method. The proposed method hasshowed an improvement in the image contrast enhancement compare with the otherimage.
arxiv-600-5 | Detecting Spammers via Aggregated Historical Data Set | http://arxiv.org/pdf/1205.1357v1.pdf | author:Eitan Menahem, Rami Puzis category:cs.CR cs.LG C.2.0; H.4.3 published:2012-05-07 summary:The battle between email service providers and senders of mass unsolicitedemails (Spam) continues to gain traction. Vast numbers of Spam emails are sentmainly from automatic botnets distributed over the world. One method formitigating Spam in a computationally efficient manner is fast and accurateblacklisting of the senders. In this work we propose a new sender reputationmechanism that is based on an aggregated historical data-set which encodes thebehavior of mail transfer agents over time. A historical data-set is createdfrom labeled logs of received emails. We use machine learning algorithms tobuild a model that predicts the \emph{spammingness} of mail transfer agents inthe near future. The proposed mechanism is targeted mainly at large enterprisesand email service providers and can be used for updating both the black and thewhite lists. We evaluate the proposed mechanism using 9.5M anonymized logentries obtained from the biggest Internet service provider in Europe.Experiments show that proposed method detects more than 94% of the Spam emailsthat escaped the blacklist (i.e., TPR), while having less than 0.5%false-alarms. Therefore, the effectiveness of the proposed method is muchhigher than of previously reported reputation mechanisms, which rely on emailslogs. In addition, the proposed method, when used for updating both the blackand white lists, eliminated the need in automatic content inspection of 4 outof 5 incoming emails, which resulted in dramatic reduction in the filteringcomputational load.
arxiv-600-6 | Convex Relaxation for Combinatorial Penalties | http://arxiv.org/pdf/1205.1240v1.pdf | author:Guillaume Obozinski, Francis Bach category:stat.ML cs.LG published:2012-05-06 summary:In this paper, we propose an unifying view of several recently proposedstructured sparsity-inducing norms. We consider the situation of a modelsimultaneously (a) penalized by a set- function de ned on the support of theunknown parameter vector which represents prior knowledge on supports, and (b)regularized in Lp-norm. We show that the natural combinatorial optimizationproblems obtained may be relaxed into convex optimization problems andintroduce a notion, the lower combinatorial envelope of a set-function, thatcharacterizes the tightness of our relaxations. We moreover establish linkswith norms based on latent representations including the latent group Lasso andblock-coding, and with norms obtained from submodular functions.
arxiv-600-7 | A Hybrid Artificial Bee Colony Algorithm for Graph 3-Coloring | http://arxiv.org/pdf/1206.1012v1.pdf | author:Iztok Fister Jr., Iztok Fister, Janez Brest category:cs.NE published:2012-05-06 summary:The Artificial Bee Colony (ABC) is the name of an optimization algorithm thatwas inspired by the intelligent behavior of a honey bee swarm. It is widelyrecognized as a quick, reliable, and efficient methods for solving optimizationproblems. This paper proposes a hybrid ABC (HABC) algorithm for graph3-coloring, which is a well-known discrete optimization problem. The results ofHABC are compared with results of the well-known graph coloring algorithms oftoday, i.e. the Tabucol and Hybrid Evolutionary algorithm (HEA) and results ofthe traditional evolutionary algorithm with SAW method (EA-SAW). Extensiveexperimentations has shown that the HABC matched the competitive results of thebest graph coloring algorithms, and did better than the traditional heuristicsEA-SAW when solving equi-partite, flat, and random generated medium-sizedgraphs.
arxiv-600-8 | Volumetric Mapping of Genus Zero Objects via Mass Preservation | http://arxiv.org/pdf/1205.1225v1.pdf | author:Romeil Sandhu, Ayelet Dominitz, Yi Gao, Allen Tannenbaum category:cs.CG cs.CV published:2012-05-06 summary:In this work, we present a technique to map any genus zero solid object ontoa hexahedral decomposition of a solid cube. This problem appears in manyapplications ranging from finite element methods to visual tracking. From this,one can then hopefully utilize the proposed technique for shape analysis,registration, as well as other related computer graphics tasks. Moreimportantly, given that we seek to establish a one-to-one correspondence of aninput volume to that of a solid cube, our algorithm can naturally generate aquality hexahedral mesh as an output. In addition, we constrain the mappingitself to be volume preserving allowing for the possibility of further meshsimplification. We demonstrate our method both qualitatively and quantitativelyon various 3D solid models
arxiv-600-9 | On the Complexity of Trial and Error | http://arxiv.org/pdf/1205.1183v2.pdf | author:Xiaohui Bei, Ning Chen, Shengyu Zhang category:cs.CC cs.DS cs.LG published:2012-05-06 summary:Motivated by certain applications from physics, biochemistry, economics, andcomputer science, in which the objects under investigation are not accessiblebecause of various limitations, we propose a trial-and-error model to examinealgorithmic issues in such situations. Given a search problem with a hiddeninput, we are asked to find a valid solution, to find which we can proposecandidate solutions (trials), and use observed violations (errors), to preparefuture proposals. In accordance with our motivating applications, we considerthe fairly broad class of constraint satisfaction problems, and assume thaterrors are signaled by a verification oracle in the format of the index of aviolated constraint (with the content of the constraint still hidden). Our discoveries are summarized as follows. On one hand, despite the seeminglyvery little information provided by the verification oracle, efficientalgorithms do exist for a number of important problems. For the Nash, Core,Stable Matching, and SAT problems, the unknown-input versions are as hard asthe corresponding known-input versions, up to a factor of polynomial. Wefurther give almost tight bounds on the latter two problems' trialcomplexities. On the other hand, there are problems whose complexities aresubstantially increased in the unknown-input model. In particular, notime-efficient algorithms exist (under standard hardness assumptions) for GraphIsomorphism and Group Isomorphism problems. The tools used to achieve theseresults include order theory, strong ellipsoid method, and some non-standardreductions. Our model investigates the value of information, and our results demonstratethat the lack of input information can introduce various levels of extradifficulty. The model exhibits intimate connections with (and we hope can alsoserve as a useful supplement to) certain existing learning and complexitytheories.
arxiv-600-10 | TIGRESS: Trustful Inference of Gene REgulation using Stability Selection | http://arxiv.org/pdf/1205.1181v1.pdf | author:Anne-Claire Haury, Fantine Mordelet, Paola Vera-Licona, Jean-Philippe Vert category:stat.ML q-bio.QM published:2012-05-06 summary:Inferring the structure of gene regulatory networks (GRN) from geneexpression data has many applications, from the elucidation of complexbiological processes to the identification of potential drug targets. It ishowever a notoriously difficult problem, for which the many existing methodsreach limited accuracy. In this paper, we formulate GRN inference as a sparseregression problem and investigate the performance of a popular featureselection method, least angle regression (LARS) combined with stabilityselection. We introduce a novel, robust and accurate scoring technique forstability selection, which improves the performance of feature selection withLARS. The resulting method, which we call TIGRESS (Trustful Inference of GeneREgulation using Stability Selection), was ranked among the top methods in theDREAM5 gene network reconstruction challenge. We investigate in depth theinfluence of the various parameters of the method and show that a fineparameter tuning can lead to significant improvements and state-of-the-artperformance for GRN inference. TIGRESS reaches state-of-the-art performance onbenchmark data. This study confirms the potential of feature selectiontechniques for GRN inference. Code and data are available onhttp://cbio.ensmp.fr/~ahaury. Running TIGRESS online is possible onGenePattern: http://www.broadinstitute.org/cancer/software/genepattern/.
arxiv-600-11 | Sparse group lasso and high dimensional multinomial classification | http://arxiv.org/pdf/1205.1245v2.pdf | author:Martin Vincent, Niels Richard Hansen category:stat.ML cs.LG stat.CO published:2012-05-06 summary:The sparse group lasso optimization problem is solved using a coordinategradient descent algorithm. The algorithm is applicable to a broad class ofconvex loss functions. Convergence of the algorithm is established, and thealgorithm is used to investigate the performance of the multinomial sparsegroup lasso classifier. On three different real data examples the multinomialgroup lasso clearly outperforms multinomial lasso in terms of achievedclassification error rate and in terms of including fewer features for theclassification. The run-time of our sparse group lasso implementation is of thesame order of magnitude as the multinomial lasso algorithm implemented in the Rpackage glmnet. Our implementation scales well with the problem size. One ofthe high dimensional examples considered is a 50 class classification problemwith 10k features, which amounts to estimating 500k parameters. Theimplementation is available as the R package msgl.
arxiv-600-12 | Rakeness in the design of Analog-to-Information Conversion of Sparse and Localized Signals | http://arxiv.org/pdf/1205.1144v1.pdf | author:Mauro Mangia, Riccardo Rovatti, Gianluca Setti category:cs.IT cs.CV math.IT published:2012-05-05 summary:Design of Random Modulation Pre-Integration systems based on therestricted-isometry property may be suboptimal when the energy of the signalsto be acquired is not evenly distributed, i.e. when they are both sparse andlocalized. To counter this, we introduce an additional design criterion, thatwe call rakeness, accounting for the amount of energy that the measurementscapture from the signal to be acquired. Hence, for localized signals a propersystem tuning increases the rakeness as well as the average SNR of the samplesused in its reconstruction. Yet, maximizing average SNR may go against the needof capturing all the components that are potentially non-zero in a sparsesignal, i.e., against the restricted isometry requirement ensuringreconstructability. What we propose is to administer the trade-off betweenrakeness and restricted isometry in a statistical way by laying down anoptimization problem. The solution of such an optimization problem is thestatistic of the process generating the random waveforms onto which the signalis projected to obtain the measurements. The formal definition of such aproblems is given as well as its solution for signals that are either localizedin frequency or in more generic domain. Sample applications, to ECG signals andsmall images of printed letters and numbers, show that rakeness-based designleads to non-negligible improvements in both cases.
arxiv-600-13 | Robot Navigation using Reinforcement Learning and Slow Feature Analysis | http://arxiv.org/pdf/1205.0986v1.pdf | author:Wendelin Böhmer category:cs.AI cs.NE published:2012-05-04 summary:The application of reinforcement learning algorithms onto real life problemsalways bears the challenge of filtering the environmental state out of rawsensor readings. While most approaches use heuristics, biology suggests thatthere must exist an unsupervised method to construct such filtersautomatically. Besides the extraction of environmental states, the filters haveto represent them in a fashion that support modern reinforcement algorithms.Many popular algorithms use a linear architecture, so one should aim at filtersthat have good approximation properties in combination with linear functions.This thesis wants to propose the unsupervised method slow feature analysis(SFA) for this task. Presented with a random sequence of sensor readings, SFAlearns a set of filters. With growing model complexity and training examples,the filters converge against trigonometric polynomial functions. These areknown to possess excellent approximation capabilities and should therforesupport the reinforcement algorithms well. We evaluate this claim on a robot.The task is to learn a navigational control in a simple environment using theleast square policy iteration (LSPI) algorithm. The only accessible sensor is ahead mounted video camera, but without meaningful filtering, video images arenot suited as LSPI input. We will show that filters learned by SFA, based on arandom walk video of the robot, allow the learned control to navigatesuccessfully in ca. 80% of the test trials.
arxiv-600-14 | Non-negative least squares for high-dimensional linear models: consistency and sparse recovery without regularization | http://arxiv.org/pdf/1205.0953v2.pdf | author:Martin Slawski, Matthias Hein category:math.ST stat.ML stat.TH published:2012-05-04 summary:Least squares fitting is in general not useful for high-dimensional linearmodels, in which the number of predictors is of the same or even larger orderof magnitude than the number of samples. Theory developed in recent years hascoined a paradigm according to which sparsity-promoting regularization isregarded as a necessity in such setting. Deviating from this paradigm, we showthat non-negativity constraints on the regression coefficients may be similarlyeffective as explicit regularization if the design matrix has additionalproperties, which are met in several applications of non-negative least squares(NNLS). We show that for these designs, the performance of NNLS with regard toprediction and estimation is comparable to that of the lasso. We argue furtherthat in specific cases, NNLS may have a better $\ell_{\infty}$-rate inestimation and hence also advantages with respect to support recovery whencombined with thresholding. From a practical point of view, NNLS does notdepend on a regularization parameter and is hence easier to use.
arxiv-600-15 | Variable Selection for Latent Dirichlet Allocation | http://arxiv.org/pdf/1205.1053v1.pdf | author:Dongwoo Kim, Yeonseung Chung, Alice Oh category:cs.LG stat.ML published:2012-05-04 summary:In latent Dirichlet allocation (LDA), topics are multinomial distributionsover the entire vocabulary. However, the vocabulary usually contains many wordsthat are not relevant in forming the topics. We adopt a variable selectionmethod widely used in statistical modeling as a dimension reduction tool andcombine it with LDA. In this variable selection model for LDA (vsLDA), topicsare multinomial distributions over a subset of the vocabulary, and by excludingwords that are not informative for finding the latent topic structure of thecorpus, vsLDA finds topics that are more robust and discriminative. We comparethree models, vsLDA, LDA with symmetric priors, and LDA with asymmetric priors,on heldout likelihood, MCMC chain consistency, and document classification. Theperformance of vsLDA is better than symmetric LDA for likelihood andclassification, better than asymmetric LDA for consistency and classification,and about the same in the other comparisons.
arxiv-600-16 | Weighted Patterns as a Tool for Improving the Hopfield Model | http://arxiv.org/pdf/1205.0908v1.pdf | author:Iakov Karandashev, Boris Kryzhanovsky, Leonid Litinskii category:cs.LG cs.NE published:2012-05-04 summary:We generalize the standard Hopfield model to the case when a weight isassigned to each input pattern. The weight can be interpreted as the frequencyof the pattern occurrence at the input of the network. In the framework of thestatistical physics approach we obtain the saddle-point equation allowing us toexamine the memory of the network. In the case of unequal weights our modeldoes not lead to the catastrophic destruction of the memory due to itsoverfilling (that is typical for the standard Hopfield model). The real memoryconsists only of the patterns with weights exceeding a critical value that isdetermined by the weights distribution. We obtain the algorithm allowing us tofind this critical value for an arbitrary distribution of the weights, andanalyze in detail some particular weights distributions. It is shown that thememory decreases as compared to the case of the standard Hopfield model.However, in our model the network can learn online without the catastrophicdestruction of the memory.
arxiv-600-17 | A powerful and efficient set test for genetic markers that handles confounders | http://arxiv.org/pdf/1205.0793v3.pdf | author:Jennifer Listgarten, Christoph Lippert, Eun Yong Kang, Jing Xiang, Carl M. Kadie, David Heckerman category:q-bio.GN stat.AP stat.ML published:2012-05-03 summary:Approaches for testing sets of variants, such as a set of rare or commonvariants within a gene or pathway, for association with complex traits areimportant. In particular, set tests allow for aggregation of weak signal withina set, can capture interplay among variants, and reduce the burden of multiplehypothesis testing. Until now, these approaches did not address confounding byfamily relatedness and population structure, a problem that is becoming moreimportant as larger data sets are used to increase power. Results: We introduce a new approach for set tests that handles confounders.Our model is based on the linear mixed model and uses two random effects-one tocapture the set association signal and one to capture confounders. We alsointroduce a computational speedup for two-random-effects models that makes thisapproach feasible even for extremely large cohorts. Using this model with boththe likelihood ratio test and score test, we find that the former yields morepower while controlling type I error. Application of our approach to richlystructured GAW14 data demonstrates that our method successfully corrects forpopulation structure and family relatedness, while application of our method toa 15,000 individual Crohn's disease case-control cohort demonstrates that itadditionally recovers genes not recoverable by univariate analysis. Availability: A Python-based library implementing our approach is availableat http://mscompbio.codeplex.com
arxiv-600-18 | Discretization of a matrix in the problem of quadratic functional binary minimization | http://arxiv.org/pdf/1205.0732v1.pdf | author:Boris Kryzhanovsky, Mikhail Kryzhanovsky, Magomed Malsagov category:cs.NE published:2012-05-03 summary:The capability of discretization of matrix elements in the problem ofquadratic functional minimization with linear member built on matrix inN-dimensional configuration space with discrete coordinates is researched. Itis shown, that optimal procedure of replacement matrix elements by the integerquantities with the limited number of gradations exist, and the efficient ofminimization does not reduce. Parameter depends on matrix properties, whichallows estimate the capability of using described procedure for given type ofmatrix, is found. Computational complexities of algorithm and RAM requirementsare reduced by 16 times, correct using of integer elements allows increaseminimization algorithm speed by the orders.
arxiv-600-19 | Rule-weighted and terminal-weighted context-free grammars have identical expressivity | http://arxiv.org/pdf/1205.0627v1.pdf | author:Yann Ponty category:cs.CL published:2012-05-03 summary:Two formalisms, both based on context-free grammars, have recently beenproposed as a basis for a non-uniform random generation of combinatorialobjects. The former, introduced by Denise et al, associates weights withletters, while the latter, recently explored by Weinberg et al in the contextof random generation, associates weights to transitions. In this short note, weuse a simple modification of the Greibach Normal Form transformation algorithm,due to Blum and Koch, to show the equivalent expressivities, in term of theirinduced distributions, of these two formalisms.
arxiv-600-20 | Generative Maximum Entropy Learning for Multiclass Classification | http://arxiv.org/pdf/1205.0651v3.pdf | author:Ambedkar Dukkipati, Gaurav Pandey, Debarghya Ghoshdastidar, Paramita Koley, D. M. V. Satya Sriram category:cs.IT cs.LG math.IT published:2012-05-03 summary:Maximum entropy approach to classification is very well studied in appliedstatistics and machine learning and almost all the methods that exists inliterature are discriminative in nature. In this paper, we introduce a maximumentropy classification method with feature selection for large dimensional datasuch as text datasets that is generative in nature. To tackle the curse ofdimensionality of large data sets, we employ conditional independenceassumption (Naive Bayes) and we perform feature selection simultaneously, byenforcing a `maximum discrimination' between estimated class conditionaldensities. For two class problems, in the proposed method, we use Jeffreys($J$) divergence to discriminate the class conditional densities. To extend ourmethod to the multi-class case, we propose a completely new approach byconsidering a multi-distribution divergence: we replace Jeffreys divergence byJensen-Shannon ($JS$) divergence to discriminate conditional densities ofmultiple classes. In order to reduce computational complexity, we employ amodified Jensen-Shannon divergence ($JS_{GM}$), based on AM-GM inequality. Weshow that the resulting divergence is a natural generalization of Jeffreysdivergence to a multiple distributions case. As far as the theoreticaljustifications are concerned we show that when one intends to select the bestfeatures in a generative maximum entropy approach, maximum discrimination using$J-$divergence emerges naturally in binary classification. Performance andcomparative study of the proposed algorithms have been demonstrated on largedimensional text and gene expression datasets that show our methods scale upvery well with large dimensional datasets.
arxiv-600-21 | Greedy Multiple Instance Learning via Codebook Learning and Nearest Neighbor Voting | http://arxiv.org/pdf/1205.0610v1.pdf | author:Gang Chen, Jason Corso category:cs.LG I.5.1 published:2012-05-03 summary:Multiple instance learning (MIL) has attracted great attention recently inmachine learning community. However, most MIL algorithms are very slow andcannot be applied to large datasets. In this paper, we propose a greedystrategy to speed up the multiple instance learning process. Our contributionis two fold. First, we propose a density ratio model, and show that maximizinga density ratio function is the low bound of the DD model under certainconditions. Secondly, we make use of a histogram ratio between positive bagsand negative bags to represent the density ratio function and find codebooksseparately for positive bags and negative bags by a greedy strategy. Fortesting, we make use of a nearest neighbor strategy to classify new bags. Wetest our method on both small benchmark datasets and the large TRECVID MED11dataset. The experimental results show that our method yields comparableaccuracy to the current state of the art, while being up to at least one orderof magnitude faster.
arxiv-600-22 | An Evolutionary Approach to Drug-Design Using a Novel Neighbourhood Based Genetic Algorithm | http://arxiv.org/pdf/1205.6412v1.pdf | author:Arnab Ghosh, Avishek Ghosh, Arkabandhu Chowdhury, Amit Konar category:cs.NE cs.CE published:2012-05-03 summary:The present work provides a new approach to evolve ligand structures whichrepresent possible drug to be docked to the active site of the target protein.The structure is represented as a tree where each non-empty node represents afunctional group. It is assumed that the active site configuration of thetarget protein is known with position of the essential residues. In this paperthe interaction energy of the ligands with the protein target is minimized.Moreover, the size of the tree is difficult to obtain and it will be differentfor different active sites. To overcome the difficulty, a variable tree sizeconfiguration is used for designing ligands. The optimization is done using anovel Neighbourhood Based Genetic Algorithm (NBGA) which uses dynamicneighbourhood topology. To get variable tree size, a variable-length version ofthe above algorithm is devised. To judge the merit of the algorithm, it isinitially applied on the well known Travelling Salesman Problem (TSP).
arxiv-600-23 | Multi-robot Cooperative Box-pushing problem using Multi-objective Particle Swarm Optimization Technique | http://arxiv.org/pdf/1206.5170v1.pdf | author:Arnab Ghosh, Avishek Ghosh, Arkabandhu Chowdhury, Amit Konar, R. Janarthanan category:cs.RO cs.NE published:2012-05-03 summary:The present work provides a new approach to solve the well-known multi-robotco-operative box pushing problem as a multi objective optimization problemusing modified Multi-objective Particle Swarm Optimization. The method proposedhere allows both turning and translation of the box, during shift to a desiredgoal position. We have employed local planning scheme to determine themagnitude of the forces applied by the two mobile robots perpendicularly atspecific locations on the box to align and translate it in each distinct stepof motion of the box, for minimization of both time and energy. Finally theresults are compared with the results obtained by solving the same problemusing Non-dominated Sorting Genetic Algorithm-II (NSGA-II). The proposed schemeis found to give better results compared to NSGA-II.
arxiv-600-24 | An Evolutionary Approach to Drug-Design Using Quantam Binary Particle Swarm Optimization Algorithm | http://arxiv.org/pdf/1206.4588v1.pdf | author:Avishek Ghosh, Arnab Ghosh, Arkabandhu Chowdhury, Jubin Hazra category:cs.NE cs.CE published:2012-05-03 summary:The present work provides a new approach to evolve ligand structures whichrepresent possible drug to be docked to the active site of the target protein.The structure is represented as a tree where each non-empty node represents afunctional group. It is assumed that the active site configuration of thetarget protein is known with position of the essential residues. In this paperthe interaction energy of the ligands with the protein target is minimized.Moreover, the size of the tree is difficult to obtain and it will be differentfor different active sites. To overcome the difficulty, a variable tree sizeconfiguration is used for designing ligands. The optimization is done using aquantum discrete PSO. The result using fixed length and variable lengthconfiguration are compared.
arxiv-600-25 | Hypothesis testing using pairwise distances and associated kernels (with Appendix) | http://arxiv.org/pdf/1205.0411v2.pdf | author:Dino Sejdinovic, Arthur Gretton, Bharath Sriperumbudur, Kenji Fukumizu category:cs.LG stat.ME stat.ML published:2012-05-02 summary:We provide a unifying framework linking two classes of statistics used intwo-sample and independence testing: on the one hand, the energy distances anddistance covariances from the statistics literature; on the other, distancesbetween embeddings of distributions to reproducing kernel Hilbert spaces(RKHS), as established in machine learning. The equivalence holds when energydistances are computed with semimetrics of negative type, in which case akernel may be defined such that the RKHS distance between distributionscorresponds exactly to the energy distance. We determine the class ofprobability distributions for which kernels induced by semimetrics arecharacteristic (that is, for which embeddings of the distributions to an RKHSare injective). Finally, we investigate the performance of this family ofkernels in two-sample and independence tests: we show in particular that theenergy distance most commonly employed in statistics is just one member of aparametric family of kernels, and that other choices from this family can yieldmore powerful tests.
arxiv-600-26 | Minimax Classifier for Uncertain Costs | http://arxiv.org/pdf/1205.0406v1.pdf | author:Rui Wang, Ke Tang category:cs.LG published:2012-05-02 summary:Many studies on the cost-sensitive learning assumed that a unique cost matrixis known for a problem. However, this assumption may not hold for manyreal-world problems. For example, a classifier might need to be applied inseveral circumstances, each of which associates with a different cost matrix.Or, different human experts have different opinions about the costs for a givenproblem. Motivated by these facts, this study aims to seek the minimaxclassifier over multiple cost matrices. In summary, we theoretically provedthat, no matter how many cost matrices are involved, the minimax problem can betackled by solving a number of standard cost-sensitive problems andsub-problems that involve only two cost matrices. As a result, a generalframework for achieving minimax classifier over multiple cost matrices issuggested and justified by preliminary empirical studies.
arxiv-600-27 | Bayesian inference for logistic models using Polya-Gamma latent variables | http://arxiv.org/pdf/1205.0310v3.pdf | author:Nicholas G. Polson, James G. Scott, Jesse Windle category:stat.ME stat.CO stat.ML published:2012-05-02 summary:We propose a new data-augmentation strategy for fully Bayesian inference inmodels with binomial likelihoods. The approach appeals to a new class ofPolya-Gamma distributions, which are constructed in detail. A variety ofexamples are presented to show the versatility of the method, includinglogistic regression, negative binomial regression, nonlinear mixed-effectsmodels, and spatial models for count data. In each case, our data-augmentationstrategy leads to simple, effective methods for posterior inference that: (1)circumvent the need for analytic approximations, numerical integration, orMetropolis-Hastings; and (2) outperform other known data-augmentationstrategies, both in ease of use and in computational efficiency. All methods,including an efficient sampler for the Polya-Gamma distribution, areimplemented in the R package BayesLogit. In the technical supplement appended to the end of the paper, we providefurther details regarding the generation of Polya-Gamma random variables; theempirical benchmarks reported in the main manuscript; and the extension of thebasic data-augmentation framework to contingency tables and multinomialoutcomes.
arxiv-600-28 | A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning | http://arxiv.org/pdf/1205.0288v2.pdf | author:Arash Afkanpour, András György, Csaba Szepesvári, Michael Bowling category:cs.LG stat.ML published:2012-05-01 summary:We consider the problem of simultaneously learning to linearly combine a verylarge number of kernels and learn a good predictor based on the learnt kernel.When the number of kernels $d$ to be combined is very large, multiple kernellearning methods whose computational cost scales linearly in $d$ areintractable. We propose a randomized version of the mirror descent algorithm toovercome this issue, under the objective of minimizing the group $p$-normpenalized empirical risk. The key to achieve the required exponential speed-upis the computationally efficient construction of low-variance estimates of thegradient. We propose importance sampling based estimates, and find that theideal distribution samples a coordinate with a probability proportional to themagnitude of the corresponding gradient. We show the surprising result that inthe case of learning the coefficients of a polynomial kernel, the combinatorialstructure of the base kernels to be combined allows the implementation ofsampling from this distribution to run in $O(\log(d))$ time, making the totalcomputational cost of the method to achieve an $\epsilon$-optimal solution tobe $O(\log(d)/\epsilon^2)$, thereby allowing our method to operate for verylarge values of $d$. Experiments with simulated and real data confirm that thenew algorithm is computationally more efficient than its state-of-the-artalternatives.
arxiv-600-29 | ProPPA: A Fast Algorithm for $\ell_1$ Minimization and Low-Rank Matrix Completion | http://arxiv.org/pdf/1205.0088v2.pdf | author:Ranch Y. Q. Lai, Pong C. Yuen category:cs.LG math.OC published:2012-05-01 summary:We propose a Projected Proximal Point Algorithm (ProPPA) for solving a classof optimization problems. The algorithm iteratively computes the proximal pointof the last estimated solution projected into an affine space which itself isparallel and approaching to the feasible set. We provide convergence analysistheoretically supporting the general algorithm, and then apply it for solving$\ell_1$-minimization problems and the matrix completion problem. Theseproblems arise in many applications including machine learning, image andsignal processing. We compare our algorithm with the existing state-of-the-artalgorithms. Experimental results on solving these problems show that ouralgorithm is very efficient and competitive.
arxiv-600-30 | Complexity Analysis of the Lasso Regularization Path | http://arxiv.org/pdf/1205.0079v2.pdf | author:Julien Mairal, Bin Yu category:stat.ML cs.LG math.OC published:2012-05-01 summary:The regularization path of the Lasso can be shown to be piecewise linear,making it possible to "follow" and explicitly compute the entire path. Weanalyze in this paper this popular strategy, and prove that its worst casecomplexity is exponential in the number of variables. We then oppose thispessimistic result to an (optimistic) approximate analysis: We show that anapproximate path with at most O(1/sqrt(epsilon)) linear segments can always beobtained, where every point on the path is guaranteed to be optimal up to arelative epsilon-duality gap. We complete our theoretical analysis with apractical algorithm to compute these approximate paths.
arxiv-600-31 | Elimination of Glass Artifacts and Object Segmentation | http://arxiv.org/pdf/1204.6653v1.pdf | author:Vini Katyal, Aviral, Deepesh Srivastava category:cs.CV published:2012-04-30 summary:Many images nowadays are captured from behind the glasses and may havecertain stains discrepancy because of glass and must be processed to makedifferentiation between the glass and objects behind it. This research paperproposes an algorithm to remove the damaged or corrupted part of the image andmake it consistent with other part of the image and to segment objects behindthe glass. The damaged part is removed using total variation inpainting methodand segmentation is done using kmeans clustering, anisotropic diffusion andwatershed transformation. The final output is obtained by interpolation. Thisalgorithm can be useful to applications in which some part of the images arecorrupted due to data transmission or needs to segment objects from an imagefor further processing.
arxiv-600-32 | Residual Belief Propagation for Topic Modeling | http://arxiv.org/pdf/1204.6610v1.pdf | author:Jia Zeng, Xiao-Qin Cao, Zhi-Qiang Liu category:cs.LG cs.IR published:2012-04-30 summary:Fast convergence speed is a desired property for training latent Dirichletallocation (LDA), especially in online and parallel topic modeling for massivedata sets. This paper presents a novel residual belief propagation (RBP)algorithm to accelerate the convergence speed for training LDA. The proposedRBP uses an informed scheduling scheme for asynchronous message passing, whichpasses fast-convergent messages with a higher priority to influence thoseslow-convergent messages at each learning iteration. Extensive empiricalstudies confirm that RBP significantly reduces the training time untilconvergence while achieves a much lower predictive perplexity than otherstate-of-the-art training algorithms for LDA, including variational Bayes (VB),collapsed Gibbs sampling (GS), loopy belief propagation (BP), and residual VB(RVB).
arxiv-600-33 | A Conjugate Property between Loss Functions and Uncertainty Sets in Classification Problems | http://arxiv.org/pdf/1204.6583v1.pdf | author:Takafumi Kanamori, Akiko Takeda, Taiji Suzuki category:stat.ML cs.LG published:2012-04-30 summary:In binary classification problems, mainly two approaches have been proposed;one is loss function approach and the other is uncertainty set approach. Theloss function approach is applied to major learning algorithms such as supportvector machine (SVM) and boosting methods. The loss function represents thepenalty of the decision function on the training samples. In the learningalgorithm, the empirical mean of the loss function is minimized to obtain theclassifier. Against a backdrop of the development of mathematical programming,nowadays learning algorithms based on loss functions are widely applied toreal-world data analysis. In addition, statistical properties of such learningalgorithms are well-understood based on a lots of theoretical works. On theother hand, the learning method using the so-called uncertainty set is used inhard-margin SVM, mini-max probability machine (MPM) and maximum margin MPM. Inthe learning algorithm, firstly, the uncertainty set is defined for each binarylabel based on the training samples. Then, the best separating hyperplanebetween the two uncertainty sets is employed as the decision function. This isregarded as an extension of the maximum-margin approach. The uncertainty setapproach has been studied as an application of robust optimization in the fieldof mathematical programming. The statistical properties of learning algorithmswith uncertainty sets have not been intensively studied. In this paper, weconsider the relation between the above two approaches. We point out that theuncertainty set is described by using the level set of the conjugate of theloss function. Based on such relation, we study statistical properties oflearning algorithms using uncertainty sets.
arxiv-600-34 | Parametric annealing: a stochastic search method for human pose tracking | http://arxiv.org/pdf/1204.6563v2.pdf | author:Prabhu Kaliamoorthi, Ramakrishna Kakarala category:cs.CV published:2012-04-30 summary:Model based methods to marker-free motion capture have a very highcomputational overhead that make them unattractive. In this paper we describe amethod that improves on existing global optimization techniques to trackingarticulated objects. Our method improves on the state-of-the-art AnnealedParticle Filter (APF) by reusing samples across annealing layers and by usingan adaptive parametric density for diffusion. We compare the proposed methodwith APF on a scalable problem and study how the two methods scale with thedimensionality, multi-modality and the range of search. Then we performsensitivity analysis on the parameters of our algorithm and show that ittolerates a wide range of parameter settings. We also show results on trackinghuman pose from the widely-used Human Eva I dataset. Our results show that theproposed method reduces the tracking error despite using less than 50% of thecomputational resources as APF. The tracked output also shows a significantqualitative improvement over APF as demonstrated through image and videoresults.
arxiv-600-35 | Recovery of Low-Rank Plus Compressed Sparse Matrices with Application to Unveiling Traffic Anomalies | http://arxiv.org/pdf/1204.6537v1.pdf | author:Morteza Mardani, Gonzalo Mateos, Georgios B. Giannakis category:cs.IT cs.NI math.IT stat.ML published:2012-04-30 summary:Given the superposition of a low-rank matrix plus the product of a known fatcompression matrix times a sparse matrix, the goal of this paper is toestablish deterministic conditions under which exact recovery of the low-rankand sparse components becomes possible. This fundamental identifiability issuearises with traffic anomaly detection in backbone networks, and subsumescompressed sensing as well as the timely low-rank plus sparse matrix recoverytasks encountered in matrix decomposition problems. Leveraging the ability of$\ell_1$- and nuclear norms to recover sparse and low-rank matrices, a convexprogram is formulated to estimate the unknowns. Analysis and simulationsconfirm that the said convex program can recover the unknowns for sufficientlylow-rank and sparse enough components, along with a compression matrixpossessing an isometry property when restricted to operate on sparse vectors.When the low-rank, sparse, and compression matrices are drawn from certainrandom ensembles, it is established that exact recovery is possible with highprobability. First-order algorithms are developed to solve the nonsmooth convexoptimization problem with provable iteration complexity guarantees. Insightfultests with synthetic and real network data corroborate the effectiveness of thenovel approach in unveiling traffic anomalies across flows and time, and itsability to outperform existing alternatives.
arxiv-600-36 | OCT Segmentation Survey and Summary Reviews and a Novel 3D Segmentation Algorithm and a Proof of Concept Implementation | http://arxiv.org/pdf/1204.6725v2.pdf | author:Serguei A. Mokhov, Yankui Sun category:cs.CV physics.optics published:2012-04-30 summary:We overview the existing OCT work, especially the practical aspects of it. Wecreate a novel algorithm for 3D OCT segmentation with the goals of speed and/oraccuracy while remaining flexible in the design and implementation for futureextensions and improvements. The document at this point is a running draftbeing iteratively "developed" as a progress report as the work and surveyadvance. It contains the review and summarization of select OCT works, thedesign and implementation of the OCTMARF experimentation application and someresults.
arxiv-600-37 | $QD$-Learning: A Collaborative Distributed Strategy for Multi-Agent Reinforcement Learning Through Consensus + Innovations | http://arxiv.org/pdf/1205.0047v2.pdf | author:Soummya Kar, Jose' M. F. Moura, H. Vincent Poor category:stat.ML cs.LG cs.MA math.OC math.PR published:2012-04-30 summary:The paper considers a class of multi-agent Markov decision processes (MDPs),in which the network agents respond differently (as manifested by theinstantaneous one-stage random costs) to a global controlled state and thecontrol actions of a remote controller. The paper investigates a distributedreinforcement learning setup with no prior information on the global statetransition and local agent cost statistics. Specifically, with the agents'objective consisting of minimizing a network-averaged infinite horizondiscounted cost, the paper proposes a distributed version of $Q$-learning,$\mathcal{QD}$-learning, in which the network agents collaborate by means oflocal processing and mutual information exchange over a sparse (possiblystochastic) communication network to achieve the network goal. Under theassumption that each agent is only aware of its local online cost data and theinter-agent communication network is \emph{weakly} connected, the proposeddistributed scheme is almost surely (a.s.) shown to yield asymptotically thedesired value function and the optimal stationary control policy at eachnetwork agent. The analytical techniques developed in the paper to address themixed time-scale stochastic dynamics of the \emph{consensus + innovations}form, which arise as a result of the proposed interactive distributed scheme,are of independent interest.
arxiv-600-38 | A Singly-Exponential Time Algorithm for Computing Nonnegative Rank | http://arxiv.org/pdf/1205.0044v1.pdf | author:Ankur Moitra category:cs.DS cs.IR cs.LG published:2012-04-30 summary:Here, we give an algorithm for deciding if the nonnegative rank of a matrix$M$ of dimension $m \times n$ is at most $r$ which runs in time$(nm)^{O(r^2)}$. This is the first exact algorithm that runs in timesingly-exponential in $r$. This algorithm (and earlier algorithms) are built onmethods for finding a solution to a system of polynomial inequalities (if oneexists). Notably, the best algorithms for this task run in time exponential inthe number of variables but polynomial in all of the other parameters (thenumber of inequalities and the maximum degree). Hence these algorithms motivate natural algebraic questions whose solutionhave immediate {\em algorithmic} implications: How many variables do we need torepresent the decision problem, does $M$ have nonnegative rank at most $r$? Anaive formulation uses $nr + mr$ variables and yields an algorithm that isexponential in $n$ and $m$ even for constant $r$. (Arora, Ge, Kannan, Moitra,STOC 2012) recently reduced the number of variables to $2r^2 2^r$, and here weexponentially reduce the number of variables to $2r^2$ and this yields our mainalgorithm. In fact, the algorithm that we obtain is nearly-optimal (under theExponential Time Hypothesis) since an algorithm that runs in time $(nm)^{o(r)}$would yield a subexponential algorithm for 3-SAT . Our main result is based on establishing a normal form for nonnegative matrixfactorization - which in turn allows us to exploit algebraic dependence among alarge collection of linear transformations with variable entries. Additionally,we also demonstrate that nonnegative rank cannot be certified by even a verylarge submatrix of $M$, and this property also follows from the intuitiongained from viewing nonnegative rank through the lens of systems of polynomialinequalities.
arxiv-600-39 | A Spectral Algorithm for Latent Dirichlet Allocation | http://arxiv.org/pdf/1204.6703v4.pdf | author:Animashree Anandkumar, Dean P. Foster, Daniel Hsu, Sham M. Kakade, Yi-Kai Liu category:cs.LG stat.ML published:2012-04-30 summary:The problem of topic modeling can be seen as a generalization of theclustering problem, in that it posits that observations are generated due tomultiple latent factors (e.g., the words in each document are generated as amixture of several active topics, as opposed to just one). This increasedrepresentational power comes at the cost of a more challenging unsupervisedlearning problem of estimating the topic probability vectors (the distributionsover words for each topic), when only the words are observed and thecorresponding topics are hidden. We provide a simple and efficient learning procedure that is guaranteed torecover the parameters for a wide class of mixture models, including thepopular latent Dirichlet allocation (LDA) model. For LDA, the procedurecorrectly recovers both the topic probability vectors and the prior over thetopics, using only trigram statistics (i.e., third order moments, which may beestimated with documents containing just three words). The method, termedExcess Correlation Analysis (ECA), is based on a spectral decomposition of loworder moments (third and fourth order) via two singular value decompositions(SVDs). Moreover, the algorithm is scalable since the SVD operations arecarried out on $k\times k$ matrices, where $k$ is the number of latent factors(e.g. the number of topics), rather than in the $d$-dimensional observed space(typically $d \gg k$).
arxiv-600-40 | Dissimilarity Clustering by Hierarchical Multi-Level Refinement | http://arxiv.org/pdf/1204.6509v1.pdf | author:Brieuc Conan-Guez, Fabrice Rossi category:stat.ML cs.LG published:2012-04-29 summary:We introduce in this paper a new way of optimizing the natural extension ofthe quantization error using in k-means clustering to dissimilarity data. Theproposed method is based on hierarchical clustering analysis combined withmulti-level heuristic refinement. The method is computationally efficient andachieves better quantization errors than the
arxiv-600-41 | Optimality of Graphlet Screening in High Dimensional Variable Selection | http://arxiv.org/pdf/1204.6452v2.pdf | author:Jiashun Jin, Cun-Hui Zhang, Qi Zhang category:math.ST stat.ME stat.ML stat.TH published:2012-04-29 summary:Consider a linear regression model where the design matrix X has n rows and pcolumns. We assume (a) p is much large than n, (b) the coefficient vector betais sparse in the sense that only a small fraction of its coordinates isnonzero, and (c) the Gram matrix G = X'X is sparse in the sense that each rowhas relatively few large coordinates (diagonals of G are normalized to 1). The sparsity in G naturally induces the sparsity of the so-called graph ofstrong dependence (GOSD). We find an interesting interplay between the signalsparsity and the graph sparsity, which ensures that in a broad context, the setof true signals decompose into many different small-size components of GOSD,where different components are disconnected. We propose Graphlet Screening (GS) as a new approach to variable selection,which is a two-stage Screen and Clean method. The key methodological innovationof GS is to use GOSD to guide both the screening and cleaning. Compared tom-variate brute-forth screening that has a computational cost of p^m, the GSonly has a computational cost of p (up to some multi-log(p) factors) inscreening. We measure the performance of any variable selection procedure by the minimaxHamming distance. We show that in a very broad class of situations, GS achievesthe optimal rate of convergence in terms of the Hamming distance. Somewhatsurprisingly, the well-known procedures subset selection and the lasso are ratenon-optimal, even in very simple settings and even when their tuning parametersare ideally set.
arxiv-600-42 | Active Contour with A Tangential Component | http://arxiv.org/pdf/1204.6458v1.pdf | author:Junyan Wang, Kap Luk Chan category:cs.CV published:2012-04-29 summary:Conventional edge-based active contours often require the normal component ofan edge indicator function on the optimal contours to approximate zero, whilethe tangential component can still be significant. In real images, the fullgradients of the edge indicator function along the object boundaries are oftensmall. Hence, the curve evolution of edge-based active contours can terminateearly before converging to the object boundaries with a careless contourinitialization. We propose a novel Geodesic Snakes (GeoSnakes) active contourthat requires the full gradients of the edge indicator to vanish at the optimalsolution. Besides, the conventional curve evolution approach for minimizingactive contour energy cannot fully solve the Euler-Lagrange (EL) equation ofour GeoSnakes active contour, causing a Pseudo Stationary Phenomenon (PSP). Toaddress the PSP problem, we propose an auxiliary curve evolution equation,named the equilibrium flow (EF) equation. Based on the EF and the conventionalcurve evolution, we obtain a solution to the full EL equation of GeoSnakesactive contour. Experimental results validate the proposed geometricalinterpretation of the early termination problem, and they also show that theproposed method overcomes the problem.
arxiv-600-43 | A Corpus-based Evaluation of Lexical Components of a Domainspecific Text to Knowledge Mapping Prototype | http://arxiv.org/pdf/1204.6362v1.pdf | author:Rushdi Shams, Adel Elsayed category:cs.IR cs.CL published:2012-04-28 summary:The aim of this paper is to evaluate the lexical components of a Text toKnowledge Mapping (TKM) prototype. The prototype is domain-specific, thepurpose of which is to map instructional text onto a knowledge domain. Thecontext of the knowledge domain of the prototype is physics, specifically DCelectrical circuits. During development, the prototype has been tested with alimited data set from the domain. The prototype now reached a stage where itneeds to be evaluated with a representative linguistic data set called corpus.A corpus is a collection of text drawn from typical sources which can be usedas a test data set to evaluate NLP systems. As there is no available corpus forthe domain, we developed a representative corpus and annotated it withlinguistic information. The evaluation of the prototype considers one of itstwo main components- lexical knowledge base. With the corpus, the evaluationenriches the lexical knowledge resources like vocabulary and grammar structure.This leads the prototype to parse a reasonable amount of sentences in thecorpus.
arxiv-600-44 | A Corpus-based Evaluation of a Domain-specific Text to Knowledge Mapping Prototype | http://arxiv.org/pdf/1204.6364v1.pdf | author:Rushdi Shams, Adel Elsayed, Quazi Mah-Zereen Akter category:cs.CL published:2012-04-28 summary:The aim of this paper is to evaluate a Text to Knowledge Mapping (TKM)Prototype. The prototype is domain-specific, the purpose of which is to mapinstructional text onto a knowledge domain. The context of the knowledge domainis DC electrical circuit. During development, the prototype has been testedwith a limited data set from the domain. The prototype reached a stage where itneeds to be evaluated with a representative linguistic data set called corpus.A corpus is a collection of text drawn from typical sources which can be usedas a test data set to evaluate NLP systems. As there is no available corpus forthe domain, we developed and annotated a representative corpus. The evaluationof the prototype considers two of its major components- lexical components andknowledge model. Evaluation on lexical components enriches the lexicalresources of the prototype like vocabulary and grammar structures. This leadsthe prototype to parse a reasonable amount of sentences in the corpus. Whiledealing with the lexicon was straight forward, the identification andextraction of appropriate semantic relations was much more involved. It wasnecessary, therefore, to manually develop a conceptual structure for the domainto formulate a domain-specific framework of semantic relations. The frameworkof semantic relationsthat has resulted from this study consisted of 55relations, out of which 42 have inverse relations. We also conducted rhetoricalanalysis on the corpus to prove its representativeness in conveying semantic.Finally, we conducted a topical and discourse analysis on the corpus to analyzethe coverage of discourse by the prototype.
arxiv-600-45 | A 3D Segmentation Method for Retinal Optical Coherence Tomography Volume Data | http://arxiv.org/pdf/1204.6385v1.pdf | author:Yankui Sun, Tian Zhang category:cs.CV physics.optics published:2012-04-28 summary:With the introduction of spectral-domain optical coherence tomography (OCT),much larger image datasets are routinely acquired compared to what was possibleusing the previous generation of time-domain OCT. Thus, the need for 3-Dsegmentation methods for processing such data is becoming increasinglyimportant. We present a new 3D segmentation method for retinal OCT volume data,which generates an enhanced volume data by using pixel intensity, boundaryposition information, intensity changes on both sides of the bordersimultaneously, and preliminary discrete boundary points are found from allA-Scans and then the smoothed boundary surface can be obtained after removing asmall quantity of error points. Our experiments show that this method isefficient, accurate and robust.
arxiv-600-46 | "I Wanted to Predict Elections with Twitter and all I got was this Lousy Paper" -- A Balanced Survey on Election Prediction using Twitter Data | http://arxiv.org/pdf/1204.6441v1.pdf | author:Daniel Gayo-Avello category:cs.CY cs.CL cs.SI physics.soc-ph published:2012-04-28 summary:Predicting X from Twitter is a popular fad within the Twitter researchsubculture. It seems both appealing and relatively easy. Among such kind ofstudies, electoral prediction is maybe the most attractive, and at this momentthere is a growing body of literature on such a topic. This is not only aninteresting research problem but, above all, it is extremely difficult.However, most of the authors seem to be more interested in claiming positiveresults than in providing sound and reproducible methods. It is also especiallyworrisome that many recent papers seem to only acknowledge those studiessupporting the idea of Twitter predicting elections, instead of conducting abalanced literature review showing both sides of the matter. After reading manyof such papers I have decided to write such a survey myself. Hence, in thispaper, every study relevant to the matter of electoral prediction using socialmedia is commented. From this review it can be concluded that the predictivepower of Twitter regarding elections has been greatly exaggerated, and thathard research problems still lie ahead.
arxiv-600-47 | CELL: Connecting Everyday Life in an archipeLago | http://arxiv.org/pdf/1204.6325v2.pdf | author:Konstantinos Chorianopoulos, Vassiliki Tsaknaki category:cs.HC cs.LG published:2012-04-27 summary:We explore the design of a seamless broadcast communication system thatbrings together the distributed community of remote secondary educationschools. In contrast to higher education, primary and secondary educationestablishments should remain distributed, in order to maintain a balance ofurban and rural life in the developing and the developed world. We plan todeploy an ambient and social interactive TV platform (physical installation,authoring tools, interactive content) that supports social communication in apositive way. In particular, we present the physical design and the conceptualmodel of the system.
arxiv-600-48 | The conduciveness of CA-rule graphs | http://arxiv.org/pdf/1204.6181v1.pdf | author:Valmir C. Barbosa category:nlin.CG cs.NE published:2012-04-27 summary:Given two subsets A and B of nodes in a directed graph, the conduciveness ofthe graph from A to B is the ratio representing how many of the edges outgoingfrom nodes in A are incoming to nodes in B. When the graph's nodes stand forthe possible solutions to certain problems of combinatorial optimization,choosing its edges appropriately has been shown to lead to conducivenessproperties that provide useful insight into the performance of algorithms tosolve those problems. Here we study the conduciveness of CA-rule graphs, thatis, graphs whose node set is the set of all CA rules given a cell's number ofpossible states and neighborhood size. We consider several different edge setsinterconnecting these nodes, both deterministic and random ones, and deriveanalytical expressions for the resulting graph's conduciveness toward ruleshaving a fixed number of non-quiescent entries. We demonstrate that one of therandom edge sets, characterized by allowing nodes to be sparsely interconnectedacross any Hamming distance between the corresponding rules, has the potentialof providing reasonable conduciveness toward the desired rules. We conjecturethat this may lie at the bottom of the best strategies known to date fordiscovering complex rules to solve specific problems, all of an evolutionarynature.
arxiv-600-49 | Background subtraction based on Local Shape | http://arxiv.org/pdf/1204.6326v2.pdf | author:Jean-Philippe Jodoin, Guillaume-Alexandre Bilodeau, Nicolas Saunier category:cs.CV published:2012-04-27 summary:We present a novel approach to background subtraction that is based on thelocal shape of small image regions. In our approach, an image region centeredon a pixel is mod-eled using the local self-similarity descriptor. We aim atobtaining a reliable change detection based on local shape change in an imagewhen foreground objects are moving. The method first builds a background modeland compares the local self-similarities between the background model and thesubsequent frames to distinguish background and foreground objects.Post-processing is then used to refine the boundaries of moving objects.Results show that this approach is promising as the foregrounds obtained arecom-plete, although they often include shadows.
arxiv-600-50 | Data-driven density derivative estimation, with applications to nonparametric clustering and bump hunting | http://arxiv.org/pdf/1204.6160v3.pdf | author:José E. Chacón, Tarn Duong category:math.ST stat.ME stat.ML stat.TH 62G05, 62H30 published:2012-04-27 summary:Important information concerning a multivariate data set, such as clustersand modal regions, is contained in the derivatives of the probability densityfunction. Despite this importance, nonparametric estimation of higher orderderivatives of the density functions have received only relatively scantattention. Kernel estimators of density functions are widely used as theyexhibit excellent theoretical and practical properties, though theirgeneralization to density derivatives has progressed more slowly due to themathematical intractabilities encountered in the crucial problem of bandwidth(or smoothing parameter) selection. This paper presents the first fullyautomatic, data-based bandwidth selectors for multivariate kernel densityderivative estimators. This is achieved by synthesizing recent advances inmatrix analytic theory which allow mathematically and computationally tractablerepresentations of higher order derivatives of multivariate vector valuedfunctions. The theoretical asymptotic properties as well as the finite samplebehaviour of the proposed selectors are studied. {In addition, we explore indetail the applications of the new data-driven methods for two otherstatistical problems: clustering and bump hunting. The introduced techniquesare combined with the mean shift algorithm to develop novel automatic,nonparametric clustering procedures which are shown to outperform mixture-modelcluster analysis and other recent nonparametric approaches in practice.Furthermore, the advantage of the use of smoothing parameters designed fordensity derivative estimation for feature significance analysis for bumphunting is illustrated with a real data example.
arxiv-600-51 | Geometry of Online Packing Linear Programs | http://arxiv.org/pdf/1204.5810v1.pdf | author:Marco Molinaro, R. Ravi category:cs.DS cs.LG published:2012-04-26 summary:We consider packing LP's with $m$ rows where all constraint coefficients arenormalized to be in the unit interval. The n columns arrive in random order andthe goal is to set the corresponding decision variables irrevocably when theyarrive so as to obtain a feasible solution maximizing the expected reward.Previous (1 - \epsilon)-competitive algorithms require the right-hand side ofthe LP to be Omega((m/\epsilon^2) log (n/\epsilon)), a bound that worsens withthe number of columns and rows. However, the dependence on the number ofcolumns is not required in the single-row case and known lower bounds for thegeneral case are also independent of n. Our goal is to understand whether the dependence on n is required in themulti-row case, making it fundamentally harder than the single-row version. Werefute this by exhibiting an algorithm which is (1 - \epsilon)-competitive aslong as the right-hand sides are Omega((m^2/\epsilon^2) log (m/\epsilon)). Ourtechniques refine previous PAC-learning based approaches which interpret theonline decisions as linear classifications of the columns based on sampled dualprices. The key ingredient of our improvement comes from a non-standardcovering argument together with the realization that only when the columns ofthe LP belong to few 1-d subspaces we can obtain small such covers; boundingthe size of the cover constructed also relies on the geometry of linearclassifiers. General packing LP's are handled by perturbing the input columns,which can be seen as making the learning problem more robust.
arxiv-600-52 | Quantitative Concept Analysis | http://arxiv.org/pdf/1204.5802v1.pdf | author:Dusko Pavlovic category:cs.LG math.CT 18D20, 06B23 I.2.6 published:2012-04-26 summary:Formal Concept Analysis (FCA) begins from a context, given as a binaryrelation between some objects and some attributes, and derives a lattice ofconcepts, where each concept is given as a set of objects and a set ofattributes, such that the first set consists of all objects that satisfy allattributes in the second, and vice versa. Many applications, though, providecontexts with quantitative information, telling not just whether an objectsatisfies an attribute, but also quantifying this satisfaction. Contexts inthis form arise as rating matrices in recommender systems, as occurrencematrices in text analysis, as pixel intensity matrices in digital imageprocessing, etc. Such applications have attracted a lot of attention, andseveral numeric extensions of FCA have been proposed. We propose the frameworkof proximity sets (proxets), which subsume partially ordered sets (posets) aswell as metric spaces. One feature of this approach is that it extracts fromquantified contexts quantified concepts, and thus allows full use of theavailable information. Another feature is that the categorical approach allowsanalyzing any universal properties that the classical FCA and the new versionsmay have, and thus provides structural guidance for aligning and combining theapproaches.
arxiv-600-53 | Distributed GraphLab: A Framework for Machine Learning in the Cloud | http://arxiv.org/pdf/1204.6078v1.pdf | author:Yucheng Low, Joseph Gonzalez, Aapo Kyrola, Danny Bickson, Carlos Guestrin, Joseph M. Hellerstein category:cs.DB cs.LG published:2012-04-26 summary:While high-level data parallel frameworks, like MapReduce, simplify thedesign and implementation of large-scale data processing systems, they do notnaturally or efficiently support many important data mining and machinelearning algorithms and can lead to inefficient learning systems. To help fillthis critical void, we introduced the GraphLab abstraction which naturallyexpresses asynchronous, dynamic, graph-parallel computation while ensuring dataconsistency and achieving a high degree of parallel performance in theshared-memory setting. In this paper, we extend the GraphLab framework to thesubstantially more challenging distributed setting while preserving strong dataconsistency guarantees. We develop graph based extensions to pipelined lockingand data versioning to reduce network congestion and mitigate the effect ofnetwork latency. We also introduce fault tolerance to the GraphLab abstractionusing the classic Chandy-Lamport snapshot algorithm and demonstrate how it canbe easily implemented by exploiting the GraphLab abstraction itself. Finally,we evaluate our distributed implementation of the GraphLab abstraction on alarge Amazon EC2 deployment and show 1-2 orders of magnitude performance gainsover Hadoop-based implementations.
arxiv-600-54 | Context-sensitive Spelling Correction Using Google Web 1T 5-Gram Information | http://arxiv.org/pdf/1204.5852v1.pdf | author:Youssef Bassil, Mohammad Alwani category:cs.CL published:2012-04-26 summary:In computing, spell checking is the process of detecting and sometimesproviding spelling suggestions for incorrectly spelled words in a text.Basically, a spell checker is a computer program that uses a dictionary ofwords to perform spell checking. The bigger the dictionary is, the higher isthe error detection rate. The fact that spell checkers are based on regulardictionaries, they suffer from data sparseness problem as they cannot capturelarge vocabulary of words including proper names, domain-specific terms,technical jargons, special acronyms, and terminologies. As a result, theyexhibit low error detection rate and often fail to catch major errors in thetext. This paper proposes a new context-sensitive spelling correction methodfor detecting and correcting non-word and real-word errors in digital textdocuments. The approach hinges around data statistics from Google Web 1T 5-gramdata set which consists of a big volume of n-gram word sequences, extractedfrom the World Wide Web. Fundamentally, the proposed method comprises an errordetector that detects misspellings, a candidate spellings generator based on acharacter 2-gram model that generates correction suggestions, and an errorcorrector that performs contextual error correction. Experiments conducted on aset of text documents from different domains and containing misspellings,showed an outstanding spelling error correction rate and a drastic reduction ofboth non-word and real-word errors. In a further study, the proposed algorithmis to be parallelized so as to lower the computational cost of the errordetection and correction processes.
arxiv-600-55 | Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems | http://arxiv.org/pdf/1204.5721v2.pdf | author:Sébastien Bubeck, Nicolò Cesa-Bianchi category:cs.LG stat.ML published:2012-04-25 summary:Multi-armed bandit problems are the most basic examples of sequentialdecision problems with an exploration-exploitation trade-off. This is thebalance between staying with the option that gave highest payoffs in the pastand exploring new options that might give higher payoffs in the future.Although the study of bandit problems dates back to the Thirties,exploration-exploitation trade-offs arise in several modern applications, suchas ad placement, website optimization, and packet routing. Mathematically, amulti-armed bandit is defined by the payoff process associated with eachoption. In this survey, we focus on two extreme cases in which the analysis ofregret is particularly simple and elegant: i.i.d. payoffs and adversarialpayoffs. Besides the basic setting of finitely many actions, we also analyzesome of the most important variants and extensions, such as the contextualbandit model.
arxiv-600-56 | Learning Loosely Connected Markov Random Fields | http://arxiv.org/pdf/1204.5540v3.pdf | author:Rui Wu, R. Srikant, Jian Ni category:stat.ML published:2012-04-25 summary:We consider the structure learning problem for graphical models that we callloosely connected Markov random fields, in which the number of short pathsbetween any pair of nodes is small, and present a new conditional independencetest based algorithm for learning the underlying graph structure. The novelmaximization step in our algorithm ensures that the true edges are detectedcorrectly even when there are short cycles in the graph. The number of samplesrequired by our algorithm is C*log p, where p is the size of the graph and theconstant C depends on the parameters of the model. We show that severalpreviously studied models are examples of loosely connected Markov randomfields, and our algorithm achieves the same or lower computational complexitythan the previously designed algorithms for individual cases. We also get newresults for more general graphical models, in particular, our algorithm learnsgeneral Ising models on the Erdos-Renyi random graph G(p, c/p) correctly withrunning time O(np^5).
arxiv-600-57 | Learning AMP Chain Graphs under Faithfulness | http://arxiv.org/pdf/1204.5357v1.pdf | author:Jose M. Peña category:stat.ML cs.AI math.ST stat.TH published:2012-04-24 summary:This paper deals with chain graphs under the alternativeAndersson-Madigan-Perlman (AMP) interpretation. In particular, we present aconstraint based algorithm for learning an AMP chain graph a given probabilitydistribution is faithful to. We also show that the extension of Meek'sconjecture to AMP chain graphs does not hold, which compromises the developmentof efficient and correct score+search learning algorithms under assumptionsweaker than faithfulness.
arxiv-600-58 | Time-dependent wave selection for information processing in excitable media | http://arxiv.org/pdf/1204.5345v1.pdf | author:William M. Stevens, Andrew Adamatzky, Ishrat Jahan, Ben de Lacy Costello category:nlin.PS cs.CL published:2012-04-24 summary:We demonstrate an improved technique for implementing logic circuits inlight-sensitive chemical excitable media. The technique makes use of theconstant-speed propagation of waves along defined channels in an excitablemedium based on the Belousov-Zhabotinsky reaction, along with the mutualannihilation of colliding waves. What distinguishes this work from previouswork in this area is that regions where channels meet at a junction canperiodically alternate between permitting the propagation of waves and blockingthem. These valve-like areas are used to select waves based on the length oftime that it takes waves to propagate from one valve to another. In anexperimental implementation, the channels which make up the circuit layout areprojected by a digital projector connected to a computer. Excitable channelsare projected as dark areas, unexcitable regions as light areas. Valvesalternate between dark and light: every valve has the same period and phase,with a 50% duty cycle. This scheme can be used to make logic gates based oncombinations of OR and AND-NOT operations, with few geometrical constraints.Because there are few geometrical constraints, compact circuits can beimplemented. Experimental results from an implementation of a 4-bit input,2-bit output integer square root circuit are given. This is the most complexlogic circuit that has been implemented in BZ excitable media to date.
arxiv-600-59 | ILexicOn: toward an ECD-compliant interlingual lexical ontology described with semantic web formalisms | http://arxiv.org/pdf/1204.5316v1.pdf | author:Maxime Lefrançois, Fabien Gandon category:cs.CL cs.AI published:2012-04-24 summary:We are interested in bridging the world of natural language and the world ofthe semantic web in particular to support natural multilingual access to theweb of data. In this paper we introduce a new type of lexical ontology calledinterlingual lexical ontology (ILexicOn), which uses semantic web formalisms tomake each interlingual lexical unit class (ILUc) support the projection of itssemantic decomposition on itself. After a short overview of existing lexicalontologies, we briefly introduce the semantic web formalisms we use. We thenpresent the three layered architecture of our approach: i) the interlinguallexical meta-ontology (ILexiMOn); ii) the ILexicOn where ILUcs are formallydefined; iii) the data layer. We illustrate our approach with a standaloneILexicOn, and introduce and explain a concise human-readable notation torepresent ILexicOns. Finally, we show how semantic web formalisms enable theprojection of a semantic decomposition on the decomposed ILUc.
arxiv-600-60 | Black-box optimization benchmarking of IPOP-saACM-ES and BIPOP-saACM-ES on the BBOB-2012 noiseless testbed | http://arxiv.org/pdf/1206.5780v1.pdf | author:Ilya Loshchilov, Marc Schoenauer, Michèle Sebag category:cs.NE published:2012-04-24 summary:In this paper, we study the performance of IPOP-saACM-ES and BIPOP-saACM-ES,recently proposed self-adaptive surrogate-assisted Covariance Matrix AdaptationEvolution Strategies. Both algorithms were tested using restarts till a totalnumber of function evaluations of $10^6D$ was reached, where $D$ is thedimension of the function search space. We compared surrogate-assistedalgorithms with their surrogate-less versions IPOP-saACM-ES and BIPOP-saACM-ES,two algorithms with one of the best overall performance observed during theBBOB-2009 and BBOB-2010. The comparison shows that the surrogate-assistedversions outperform the original CMA-ES algorithms by a factor from 2 to 4 on 8out of 24 noiseless benchmark problems, showing the best results among allalgorithms of the BBOB-2009 and BBOB-2010 on Ellipsoid, Discus, Bent Cigar,Sharp Ridge and Sum of different powers functions.
arxiv-600-61 | Black-box optimization benchmarking of IPOP-saACM-ES on the BBOB-2012 noisy testbed | http://arxiv.org/pdf/1206.0974v1.pdf | author:Ilya Loshchilov, Marc Schoenauer, Michèle Sebag category:cs.NE published:2012-04-24 summary:In this paper, we study the performance of IPOP-saACM-ES, recently proposedself-adaptive surrogate-assisted Covariance Matrix Adaptation EvolutionStrategy. The algorithm was tested using restarts till a total number offunction evaluations of $10^6D$ was reached, where $D$ is the dimension of thefunction search space. The experiments show that the surrogate model controlallows IPOP-saACM-ES to be as robust as the original IPOP-aCMA-ES andoutperforms the latter by a factor from 2 to 3 on 6 benchmark problems withmoderate noise. On 15 out of 30 benchmark problems in dimension 20,IPOP-saACM-ES exceeds the records observed during BBOB-2009 and BBOB-2010.
arxiv-600-62 | Ecological Evaluation of Persuasive Messages Using Google AdWords | http://arxiv.org/pdf/1204.5369v1.pdf | author:Marco Guerini, Carlo Strapparava, Oliviero Stock category:cs.CL cs.SI I.2.7 published:2012-04-24 summary:In recent years there has been a growing interest in crowdsourcingmethodologies to be used in experimental research for NLP tasks. In particular,evaluation of systems and theories about persuasion is difficult to accommodatewithin existing frameworks. In this paper we present a new cheap and fastmethodology that allows fast experiment building and evaluation withfully-automated analysis at a low cost. The central idea is exploiting existingcommercial tools for advertising on the web, such as Google AdWords, to measuremessage impact in an ecological setting. The paper includes a description ofthe approach, tips for how to use AdWords for scientific research, and resultsof pilot experiments on the impact of affective text variations which confirmthe effectiveness of the approach.
arxiv-600-63 | A New Approach of Improving CFA Image for Digital Camera's | http://arxiv.org/pdf/1204.5416v1.pdf | author:Manoj Kumar, Vikas Kaushik, Pradeep Singla category:cs.CV published:2012-04-24 summary:This paper work directly towards the improving the quality of the image forthe digital cameras and other visual capturing products. In this Paper, theauthors clearly defines the problems occurs in the CFA image. A differentmethodology for removing the noise is discuses in the paper for colorcorrection and color balancing of the image. At the same time, the authors alsoproposed a new methodology of providing denoisiing process before thedemosaickingfor the improving the image quality of CFA which is much efficientthen the other previous defined. The demosaicking process for producing thecolors in the image in a best way is also discuss.
arxiv-600-64 | Robust Head Pose Estimation Using Contourlet Transform | http://arxiv.org/pdf/1204.5431v2.pdf | author:Mohammad Tofighi, Hashem Kalbkhani, Mahrokh G. Shayesteh, Mehdi Ghasemzadeh category:cs.CV published:2012-04-24 summary:Estimating pose of the head is an important preprocessing step in manypattern recognition and computer vision systems such as face recognition. Sincethe performance of the face recognition systems is greatly affected by theposes of the face, how to estimate the accurate pose of the face in human faceimage is still a challenging problem. In this paper, we represent a novelmethod for head pose estimation. To enhance the efficiency of the estimation weuse contourlet transform for feature extraction. Contourlet transform ismulti-resolution, multi-direction transform. In order to reduce the featurespace dimension and obtain appropriate features we use LDA (Linear DiscriminantAnalysis) and PCA (Principal Component Analysis) to remove ineffcient features.Then, we apply different classifiers such as k-nearest neighborhood (knn) andminimum distance. We use the public available FERET database to evaluate theperformance of proposed method. Simulation results indicate the superiorrobustness of the proposed method.
arxiv-600-65 | Analysis Operator Learning and Its Application to Image Reconstruction | http://arxiv.org/pdf/1204.5309v3.pdf | author:Simon Hawe, Martin Kleinsteuber, Klaus Diepold category:cs.LG cs.CV I.4.5 published:2012-04-24 summary:Exploiting a priori known structural information lies at the core of manyimage reconstruction methods that can be stated as inverse problems. Thesynthesis model, which assumes that images can be decomposed into a linearcombination of very few atoms of some dictionary, is now a well establishedtool for the design of image reconstruction algorithms. An interestingalternative is the analysis model, where the signal is multiplied by ananalysis operator and the outcome is assumed to be the sparse. This approachhas only recently gained increasing interest. The quality of reconstructionmethods based on an analysis model severely depends on the right choice of thesuitable operator. In this work, we present an algorithm for learning an analysis operator fromtraining images. Our method is based on an $\ell_p$-norm minimization on theset of full rank matrices with normalized columns. We carefully introduce theemployed conjugate gradient method on manifolds, and explain the underlyinggeometry of the constraints. Moreover, we compare our approach tostate-of-the-art methods for image denoising, inpainting, and single imagesuper-resolution. Our numerical results show competitive performance of ourgeneral approach in all presented applications compared to the specializedstate-of-the-art techniques.
arxiv-600-66 | Objective Function Designing Led by User Preferences Acquisition | http://arxiv.org/pdf/1204.4990v1.pdf | author:Patrick Taillandier, Julien Gaffuri category:cs.LG cs.AI cs.HC published:2012-04-23 summary:Many real world problems can be defined as optimisation problems in which theaim is to maximise an objective function. The quality of obtained solution isdirectly linked to the pertinence of the used objective function. However,designing such function, which has to translate the user needs, is usuallyfastidious. In this paper, a method to help user objective functions designingis proposed. Our approach, which is highly interactive, is based on man machinedialogue and more particularly on the comparison of problem instance solutionsby the user. We propose an experiment in the domain of cartographicgeneralisation that shows promising results.
arxiv-600-67 | Knowledge revision in systems based on an informed tree search strategy : application to cartographic generalisation | http://arxiv.org/pdf/1204.4991v1.pdf | author:Patrick Taillandier, Cécile Duchêne, Alexis Drogoul category:cs.AI cs.LG published:2012-04-23 summary:Many real world problems can be expressed as optimisation problems. Solvingthis kind of problems means to find, among all possible solutions, the one thatmaximises an evaluation function. One approach to solve this kind of problem isto use an informed search strategy. The principle of this kind of strategy isto use problem-specific knowledge beyond the definition of the problem itselfto find solutions more efficiently than with an uninformed strategy. This kindof strategy demands to define problem-specific knowledge (heuristics). Theefficiency and the effectiveness of systems based on it directly depend on theused knowledge quality. Unfortunately, acquiring and maintaining such knowledgecan be fastidious. The objective of the work presented in this paper is topropose an automatic knowledge revision approach for systems based on aninformed tree search strategy. Our approach consists in analysing the systemexecution logs and revising knowledge based on these logs by modelling therevision problem as a knowledge space exploration problem. We present anexperiment we carried out in an application domain where informed searchstrategies are often used: cartographic generalisation.
arxiv-600-68 | Sparse Prediction with the $k$-Support Norm | http://arxiv.org/pdf/1204.5043v2.pdf | author:Andreas Argyriou, Rina Foygel, Nathan Srebro category:stat.ML cs.LG published:2012-04-23 summary:We derive a novel norm that corresponds to the tightest convex relaxation ofsparsity combined with an $\ell_2$ penalty. We show that this new {\em$k$-support norm} provides a tighter relaxation than the elastic net and isthus a good replacement for the Lasso or the elastic net in sparse predictionproblems. Through the study of the $k$-support norm, we also bound thelooseness of the elastic net, thus shedding new light on it and providingjustification for its use.
arxiv-600-69 | A Unified Multiscale Framework for Discrete Energy Minimization | http://arxiv.org/pdf/1204.4867v1.pdf | author:Shai Bagon, Meirav Galun category:cs.CV cs.DM published:2012-04-22 summary:Discrete energy minimization is a ubiquitous task in computer vision, yet isNP-hard in most cases. In this work we propose a multiscale framework forcoping with the NP-hardness of discrete optimization. Our approach utilizesalgebraic multiscale principles to efficiently explore the discrete solutionspace, yielding improved results on challenging, non-submodular energies forwhich current methods provide unsatisfactory approximations. In contrast topopular multiscale methods in computer vision, that builds an image pyramid,our framework acts directly on the energy to construct an energy pyramid.Deriving a multiscale scheme from the energy itself makes our frameworkapplication independent and widely applicable. Our framework gives rise to twocomplementary energy coarsening strategies: one in which coarser scales involvefewer variables, and a more revolutionary one in which the coarser scalesinvolve fewer discrete labels. We empirically evaluated our unified frameworkon a variety of both non-submodular and submodular energies, including energiesfrom Middlebury benchmark.
arxiv-600-70 | EHRs Connect Research and Practice: Where Predictive Modeling, Artificial Intelligence, and Clinical Decision Support Intersect | http://arxiv.org/pdf/1204.4927v1.pdf | author:Casey Bennett, Tom Doub, Rebecca Selove category:cs.AI cs.DB stat.ML published:2012-04-22 summary:Objectives: Electronic health records (EHRs) are only a first step incapturing and utilizing health-related data - the challenge is turning thatdata into useful information. Furthermore, EHRs are increasingly likely toinclude data relating to patient outcomes, functionality such as clinicaldecision support, and genetic information as well, and, as such, can be seen asrepositories of increasingly valuable information about patients' healthconditions and responses to treatment over time. Methods: We describe a casestudy of 423 patients treated by Centerstone within Tennessee and Indiana inwhich we utilized electronic health record data to generate predictivealgorithms of individual patient treatment response. Multiple models wereconstructed using predictor variables derived from clinical, financial andgeographic data. Results: For the 423 patients, 101 deteriorated, 223 improvedand in 99 there was no change in clinical condition. Based on modeling ofvarious clinical indicators at baseline, the highest accuracy in predictingindividual patient response ranged from 70-72% within the models tested. Interms of individual predictors, the Centerstone Assessment of Recovery Level -Adult (CARLA) baseline score was most significant in predicting outcome overtime (odds ratio 4.1 + 2.27). Other variables with consistently significantimpact on outcome included payer, diagnostic category, location and provisionof case management services. Conclusions: This approach represents a promisingavenue toward reducing the current gap between research and practice acrosshealthcare, developing data-driven clinical decision support based onreal-world populations, and serving as a component of embedded clinicalartificial intelligences that "learn" over time.
arxiv-600-71 | Quantum Interference in Cognition: Structural Aspects of the Brain | http://arxiv.org/pdf/1204.4914v1.pdf | author:Diederik Aerts, Sandro Sozzo category:cs.AI cs.CL quant-ph published:2012-04-22 summary:We identify the presence of typically quantum effects, namely 'superposition'and 'interference', in what happens when human concepts are combined, andprovide a quantum model in complex Hilbert space that represents faithfullyexperimental data measuring the situation of combining concepts. Our modelshows how 'interference of concepts' explains the effects of underextension andoverextension when two concepts combine to the disjunction of these twoconcepts. This result supports our earlier hypothesis that human thought has asuperposed two-layered structure, one layer consisting of 'classical logicalthought' and a superposed layer consisting of 'quantum conceptual thought'.Possible connections with recent findings of a 'grid-structure' for the brainare analyzed, and influences on the mind/brain relation, and consequences onapplied disciplines, such as artificial intelligence and quantum computation,are considered.
arxiv-600-72 | Paraiso : An Automated Tuning Framework for Explicit Solvers of Partial Differential Equations | http://arxiv.org/pdf/1204.4779v2.pdf | author:Takayuki Muranushi category:astro-ph.IM cs.DC cs.NE published:2012-04-21 summary:We propose Paraiso, a domain specific language embedded in functionalprogramming language Haskell, for automated tuning of explicit solvers ofpartial differential equations (PDEs) on GPUs as well as multicore CPUs. InParaiso, one can describe PDE solving algorithms succinctly using tensorequations notation. Hydrodynamic properties, interpolation methods and otherbuilding blocks are described in abstract, modular, re-usable and combinableforms, which lets us generate versatile solvers from little set of Paraisosource codes. We demonstrate Paraiso by implementing a compressive hydrodynamics solver. Asingle source code less than 500 lines can be used to generate solvers ofarbitrary dimensions, for both multicore CPUs and GPUs. We demonstrate bothmanual annotation based tuning and evolutionary computing based automatedtuning of the program.
arxiv-600-73 | An Optimization Framework for Semi-Supervised and Transfer Learning using Multiple Classifiers and Clusterers | http://arxiv.org/pdf/1206.0994v1.pdf | author:Ayan Acharya, Eduardo R. Hruschka, Joydeep Ghosh, Sreangsu Acharyya category:cs.LG published:2012-04-20 summary:Unsupervised models can provide supplementary soft constraints to helpclassify new, "target" data since similar instances in the target set are morelikely to share the same class label. Such models can also help detect possibledifferences between training and target distributions, which is useful inapplications where concept drift may take place, as in transfer learningsettings. This paper describes a general optimization framework that takes asinput class membership estimates from existing classifiers learnt on previouslyencountered "source" data, as well as a similarity matrix from a clusterensemble operating solely on the target data to be classified, and yields aconsensus labeling of the target data. This framework admits a wide range ofloss functions and classification/clustering methods. It exploits properties ofBregman divergences in conjunction with Legendre duality to yield a principledand scalable approach. A variety of experiments show that the proposedframework can yield results substantially superior to those provided by populartransductive learning techniques or by naively applying classifiers learnt onthe original task to the target data.
arxiv-600-74 | Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows | http://arxiv.org/pdf/1204.4539v3.pdf | author:Julien Mairal, Bin Yu category:stat.ML cs.LG math.OC published:2012-04-20 summary:We consider supervised learning problems where the features are embedded in agraph, such as gene expressions in a gene network. In this context, it is ofmuch interest to automatically select a subgraph with few connected components;by exploiting prior knowledge, one can indeed improve the predictionperformance or obtain results that are easier to interpret. Regularization orpenalty functions for selecting features in graphs have recently been proposed,but they raise new algorithmic challenges. For example, they typically requiresolving a combinatorially hard selection problem among all connected subgraphs.In this paper, we propose computationally feasible strategies to select asparse and well-connected subset of features sitting on a directed acyclicgraph (DAG). We introduce structured sparsity penalties over paths on a DAGcalled "path coding" penalties. Unlike existing regularization functions thatmodel long-range interactions between features in a graph, path codingpenalties are tractable. The penalties and their proximal operators involvepath selection problems, which we efficiently solve by leveraging network flowoptimization. We experimentally show on synthetic, image, and genomic data thatour approach is scalable and leads to more connected subgraphs than otherregularization functions for graphs.
arxiv-600-75 | A Fast and Effective Local Search Algorithm for Optimizing the Placement of Wind Turbines | http://arxiv.org/pdf/1204.4560v1.pdf | author:Markus Wagner, Jareth Day, Frank Neumann category:cs.NE published:2012-04-20 summary:The placement of wind turbines on a given area of land such that the windfarm produces a maximum amount of energy is a challenging optimization problem.In this article, we tackle this problem, taking into account wake effects thatare produced by the different turbines on the wind farm. We significantlyimprove upon existing results for the minimization of wake effects bydeveloping a new problem-specific local search algorithm. One key step in thespeed-up of our algorithm is the reduction in computation time needed to assessa given wind farm layout compared to previous approaches. Our new method allowsthe optimization of large real-world scenarios within a single night on astandard computer, whereas weeks on specialized computing servers were requiredfor previous approaches.
arxiv-600-76 | Regret in Online Combinatorial Optimization | http://arxiv.org/pdf/1204.4710v2.pdf | author:Jean-Yves Audibert, Sébastien Bubeck, Gábor Lugosi category:cs.LG stat.ML published:2012-04-20 summary:We address online linear optimization problems when the possible actions ofthe decision maker are represented by binary vectors. The regret of thedecision maker is the difference between her realized loss and the best lossshe would have achieved by picking, in hindsight, the best possible action. Ourgoal is to understand the magnitude of the best possible (minimax) regret. Westudy the problem under three different assumptions for the feedback thedecision maker receives: full information, and the partial information modelsof the so-called "semi-bandit" and "bandit" problems. Combining the MirrorDescent algorithm and the INF (Implicitely Normalized Forecaster) strategy, weare able to prove optimal bounds for the semi-bandit case. We also recover theoptimal bounds for the full information setting. In the bandit case we discussexisting results in light of a new lower bound, and suggest a conjecture on theoptimal regret in that case. Finally we also prove that the standardexponentially weighted average forecaster is provably suboptimal in the settingof online combinatorial optimization.
arxiv-600-77 | Efficient hierarchical clustering for continuous data | http://arxiv.org/pdf/1204.4708v1.pdf | author:Ricardo Henao, Joseph E. Lucas category:stat.ML published:2012-04-20 summary:We present an new sequential Monte Carlo sampler for coalescent basedBayesian hierarchical clustering. Our model is appropriate for modelingnon-i.i.d. data and offers a substantial reduction of computational cost whencompared to the original sampler without resorting to approximations. We alsopropose a quadratic complexity approximation that in practice shows almost noloss in performance compared to its counterpart. We show that as a byproduct ofour formulation, we obtain a greedy algorithm that exhibits performanceimprovement over other greedy algorithms, particularly in small data sets. Inorder to exploit the correlation structure of the data, we describe how toincorporate Gaussian process priors in the model as a flexible way to modelnon-i.i.d. data. Results on artificial and real data show significantimprovements over closely related approaches.
arxiv-600-78 | A Privacy-Aware Bayesian Approach for Combining Classifier and Cluster Ensembles | http://arxiv.org/pdf/1204.4521v1.pdf | author:Ayan Acharya, Eduardo R. Hruschka, Joydeep Ghosh category:cs.LG cs.CV stat.ML I.5.4 published:2012-04-20 summary:This paper introduces a privacy-aware Bayesian approach that combinesensembles of classifiers and clusterers to perform semi-supervised andtransductive learning. We consider scenarios where instances and theirclassification/clustering results are distributed across different data sitesand have sharing restrictions. As a special case, the privacy aware computationof the model when instances of the target data are distributed across differentdata sites, is also discussed. Experimental results show that the proposedapproach can provide good classification accuracies while adhering to thedata/model sharing constraints.
arxiv-600-79 | Modeling, dependence, classification, united statistical science, many cultures | http://arxiv.org/pdf/1204.4699v3.pdf | author:Emanuel Parzen, Subhadeep Mukhopadhyay category:math.ST stat.ME stat.ML stat.TH 62Gxx published:2012-04-20 summary:Breiman (2001) proposed to statisticians awareness of two cultures: 1.Parametric modeling culture, pioneered by R.A.Fisher and Jerzy Neyman; 2.Algorithmic predictive culture, pioneered by machine learning research. Parzen (2001), as a part of discussing Breiman (2001), proposed thatresearchers be aware of many cultures, including the focus of our research: 3.Nonparametric, quantile based, information theoretic modeling. We provide aunification of many statistical methods for traditional small data sets andemerging big data sets in terms of comparison density, copula density, measureof dependence, correlation, information, new measures (called LP scorecomoments) that apply to long tailed distributions with out finite second ordermoments. A very important goal is to unify methods for discrete and continuousrandom variables. Our research extends these methods to modern high dimensionaldata modeling.
arxiv-600-80 | Energy-Efficient Building HVAC Control Using Hybrid System LBMPC | http://arxiv.org/pdf/1204.4717v1.pdf | author:Anil Aswani, Neal Master, Jay Taneja, Andrew Krioukov, David Culler, Claire Tomlin category:math.OC cs.LG cs.SY published:2012-04-20 summary:Improving the energy-efficiency of heating, ventilation, and air-conditioning(HVAC) systems has the potential to realize large economic and societalbenefits. This paper concerns the system identification of a hybrid systemmodel of a building-wide HVAC system and its subsequent control using a hybridsystem formulation of learning-based model predictive control (LBMPC). Here,the learning refers to model updates to the hybrid system model thatincorporate the heating effects due to occupancy, solar effects, outside airtemperature (OAT), and equipment, in addition to integrator dynamics inherentlypresent in low-level control. Though we make significant modelingsimplifications, our corresponding controller that uses this model is able toexperimentally achieve a large reduction in energy usage without anydegradations in occupant comfort. It is in this way that we justify themodeling simplifications that we have made. We conclude by presenting resultsfrom experiments on our building HVAC testbed, which show an average of 1.5MWhof energy savings per day (p = 0.002) with a 95% confidence interval of 1.0MWhto 2.1MWh of energy savings.
arxiv-600-81 | Morphological Filtering in Shape Spaces: Applications using Tree-Based Image Representations | http://arxiv.org/pdf/1204.4758v2.pdf | author:Yongchao Xu, Thierry Géraud, Laurent Najman category:cs.CV math.OA published:2012-04-20 summary:Connected operators are filtering tools that act by merging elementaryregions of an image. A popular strategy is based on tree-based imagerepresentations: for example, one can compute an attribute on each node of thetree and keep only the nodes for which the attribute is sufficiently strong.This operation can be seen as a thresholding of the tree, seen as a graph whosenodes are weighted by the attribute. Rather than being satisfied with a merethresholding, we propose to expand on this idea, and to apply connected filterson this latest graph. Consequently, the filtering is done not in the space ofthe image, but on the space of shapes build from the image. Such a processingis a generalization of the existing tree-based connected operators. Indeed, theframework includes classical existing connected operators by attributes. Italso allows us to propose a class of novel connected operators from theleveling family, based on shape attributes. Finally, we also propose a novelclass of self-dual connected operators that we call morphological shapings.
arxiv-600-82 | EP-GIG Priors and Applications in Bayesian Sparse Learning | http://arxiv.org/pdf/1204.4243v1.pdf | author:Zhihua Zhang, Shusen Wang, Dehua Liu, Michael I. Jordan category:stat.ML published:2012-04-19 summary:In this paper we propose a novel framework for the construction ofsparsity-inducing priors. In particular, we define such priors as a mixture ofexponential power distributions with a generalized inverse Gaussian density(EP-GIG). EP-GIG is a variant of generalized hyperbolic distributions, and thespecial cases include Gaussian scale mixtures and Laplace scale mixtures.Furthermore, Laplace scale mixtures can subserve a Bayesian framework forsparse learning with nonconvex penalization. The densities of EP-GIG can beexplicitly expressed. Moreover, the corresponding posterior distribution alsofollows a generalized inverse Gaussian distribution. These properties lead usto EM algorithms for Bayesian sparse learning. We show that these algorithmsbear an interesting resemblance to iteratively re-weighted $\ell_2$ or $\ell_1$methods. In addition, we present two extensions for grouped variable selectionand logistic regression.
arxiv-600-83 | Speech Recognition: Increasing Efficiency of Support Vector Machines | http://arxiv.org/pdf/1204.4257v1.pdf | author:Aamir Khan, Muhammad Farhan, Asar Ali category:cs.CV published:2012-04-19 summary:With the advancement of communication and security technologies, it hasbecome crucial to have robustness of embedded biometric systems. This paperpresents the realization of such technologies which demands reliable anderror-free biometric identity verification systems. High dimensional patternsare not permitted due to eigen-decomposition in high dimensional feature spaceand degeneration of scattering matrices in small size sample. Generalization,dimensionality reduction and maximizing the margins are controlled byminimizing weight vectors. Results show good pattern by multimodal biometricsystem proposed in this paper. This paper is aimed at investigating a biometricidentity system using Support Vector Machines(SVMs) and Lindear DiscriminantAnalysis(LDA) with MFCCs and implementing such system in real-time usingSignalWAVE.
arxiv-600-84 | Learning in Riemannian Orbifolds | http://arxiv.org/pdf/1204.4294v1.pdf | author:Brijnesh J. Jain, Klaus Obermayer category:cs.LG cs.AI cs.CV published:2012-04-19 summary:Learning in Riemannian orbifolds is motivated by existing machine learningalgorithms that directly operate on finite combinatorial structures such aspoint patterns, trees, and graphs. These methods, however, lack statisticaljustification. This contribution derives consistency results for learningproblems in structured domains and thereby generalizes learning in vectorspaces and manifolds.
arxiv-600-85 | Estimating Unknown Sparsity in Compressed Sensing | http://arxiv.org/pdf/1204.4227v2.pdf | author:Miles E. Lopes category:cs.IT math.IT math.ST stat.ME stat.ML stat.TH published:2012-04-19 summary:In the theory of compressed sensing (CS), the sparsity x_0 of the unknownsignal x\in\R^p is commonly assumed to be a known parameter. However, it istypically unknown in practice. Due to the fact that many aspects of CS dependon knowing x_0, it is important to estimate this parameter in a data-drivenway. A second practical concern is that x_0 is a highly unstable functionof x. In particular, for real signals with entries not exactly equal to 0, thevalue x_0=p is not a useful description of the effective number ofcoordinates. In this paper, we propose to estimate a stable measure of sparsitys(x):=x_1^2/x_2^2, which is a sharp lower bound on x_0. Ourestimation procedure uses only a small number of linear measurements, does notrely on any sparsity assumptions, and requires very little computation. Aconfidence interval for s(x) is provided, and its width is shown to have nodependence on the signal dimension p. Moreover, this result extends naturallyto the matrix recovery setting, where a soft version of matrix rank can beestimated with analogous guarantees. Finally, we show that the use ofrandomized measurements is essential to estimating s(x). This is accomplishedby proving that the minimax risk for estimating s(x) with deterministicmeasurements is large when n<<p.
arxiv-600-86 | Designing generalisation evaluation function through human-machine dialogue | http://arxiv.org/pdf/1204.4332v1.pdf | author:Patrick Taillandier, Julien Gaffuri category:cs.HC cs.LG published:2012-04-19 summary:Automated generalisation has known important improvements these last fewyears. However, an issue that still deserves more study concerns the automaticevaluation of generalised data. Indeed, many automated generalisation systemsrequire the utilisation of an evaluation function to automatically assessgeneralisation outcomes. In this paper, we propose a new approach dedicated tothe design of such a function. This approach allows an imperfectly definedevaluation function to be revised through a man-machine dialogue. The usergives its preferences to the system by comparing generalisation outcomes.Machine Learning techniques are then used to improve the evaluation function.An experiment carried out on buildings shows that our approach significantlyimproves generalisation evaluation functions defined by users.
arxiv-600-87 | Supervised feature evaluation by consistency analysis: application to measure sets used to characterise geographic objects | http://arxiv.org/pdf/1204.4329v1.pdf | author:Patrick Taillandier, Alexis Drogoul category:cs.LG published:2012-04-19 summary:Nowadays, supervised learning is commonly used in many domains. Indeed, manyworks propose to learn new knowledge from examples that translate the expectedbehaviour of the considered system. A key issue of supervised learning concernsthe description language used to represent the examples. In this paper, wepropose a method to evaluate the feature set used to describe them. Our methodis based on the computation of the consistency of the example base. We carriedout a case study in the domain of geomatic in order to evaluate the sets ofmeasures used to characterise geographic objects. The case study shows that ourmethod allows to give relevant evaluations of measure sets.
arxiv-600-88 | Your Two Weeks of Fame and Your Grandmother's | http://arxiv.org/pdf/1204.4346v1.pdf | author:James Cook, Atish Das Sarma, Alex Fabrikant, Andrew Tomkins category:cs.DL cs.CL cs.SI physics.soc-ph J.4 published:2012-04-19 summary:Did celebrity last longer in 1929, 1992 or 2009? We investigate thephenomenon of fame by mining a collection of news articles that spans thetwentieth century, and also perform a side study on a collection of blog postsfrom the last 10 years. By analyzing mentions of personal names, we measureeach person's time in the spotlight, using two simple metrics that evaluate,roughly, the duration of a single news story about a person, and the overallduration of public interest in a person. We watched the distribution evolvefrom 1895 to 2010, expecting to find significantly shortening fame durations,per the much popularly bemoaned shortening of society's attention spans andquickening of media's news cycles. Instead, we conclusively demonstrate that,through many decades of rapid technological and societal change, through theappearance of Twitter, communication satellites, and the Internet, famedurations did not decrease, neither for the typical case nor for the extremelyfamous, with the last statistically significant fame duration decreases comingin the early 20th century, perhaps from the spread of telegraphy and telephony.Furthermore, while median fame durations stayed persistently constant, for themost famous of the famous, as measured by either volume or duration of mediaattention, fame durations have actually trended gently upward since the 1940s,with statistically significant increases on 40-year timescales. Similar studieshave been done with much shorter timescales specifically in the context ofinformation spreading on Twitter and similar social networking sites. To thebest of our knowledge, this is the first massive scale study of this naturethat spans over a century of archived data, thereby allowing us to trackchanges across decades.
arxiv-600-89 | Dynamic Template Tracking and Recognition | http://arxiv.org/pdf/1204.4476v1.pdf | author:Rizwan Chaudhry, Gregory Hager, Rene Vidal category:cs.CV cs.SY published:2012-04-19 summary:In this paper we address the problem of tracking non-rigid objects whoselocal appearance and motion changes as a function of time. This class ofobjects includes dynamic textures such as steam, fire, smoke, water, etc., aswell as articulated objects such as humans performing various actions. We modelthe temporal evolution of the object's appearance/motion using a LinearDynamical System (LDS). We learn such models from sample videos and use them asdynamic templates for tracking objects in novel videos. We pose the problem oftracking a dynamic non-rigid object in the current frame as a maximuma-posteriori estimate of the location of the object and the latent state of thedynamical system, given the current image features and the best estimate of thestate in the previous frame. The advantage of our approach is that we canspecify a-priori the type of texture to be tracked in the scene by usingpreviously trained models for the dynamics of these textures. Our frameworknaturally generalizes common tracking methods such as SSD and kernel-basedtracking from static templates to dynamic templates. We test our algorithm onsynthetic as well as real examples of dynamic textures and show that our simpledynamics-based trackers perform at par if not better than the state-of-the-art.Since our approach is general and applicable to any image feature, we alsoapply it to the problem of human action tracking and build action-specificoptical flow trackers that perform better than the state-of-the-art whentracking a human performing a particular action. Finally, since our approach isgenerative, we can use a-priori trained trackers for different texture oraction classes to simultaneously track and recognize the texture or action inthe video.
arxiv-600-90 | Semi-Supervised learning with Density-Ratio Estimation | http://arxiv.org/pdf/1204.3965v1.pdf | author:Masanori Kawakita, Takafumi Kanamori category:stat.ML published:2012-04-18 summary:In this paper, we study statistical properties of semi-supervised learning,which is considered as an important problem in the community of machinelearning. In the standard supervised learning, only the labeled data isobserved. The classification and regression problems are formalized as thesupervised learning. In semi-supervised learning, unlabeled data is alsoobtained in addition to labeled data. Hence, exploiting unlabeled data isimportant to improve the prediction accuracy in semi-supervised learning. Thisproblems is regarded as a semiparametric estimation problem with missing data.Under the the discriminative probabilistic models, it had been considered thatthe unlabeled data is useless to improve the estimation accuracy. Recently, itwas revealed that the weighted estimator using the unlabeled data achievesbetter prediction accuracy in comparison to the learning method using onlylabeled data, especially when the discriminative probabilistic model ismisspecified. That is, the improvement under the semiparametric model withmissing data is possible, when the semiparametric model is misspecified. Inthis paper, we apply the density-ratio estimator to obtain the weight functionin the semi-supervised learning. The benefit of our approach is that theproposed estimator does not require well-specified probabilistic models for theprobability of the unlabeled data. Based on the statistical asymptotic theory,we prove that the estimation accuracy of our method outperforms the supervisedlearning using only labeled data. Some numerical experiments present theusefulness of our methods.
arxiv-600-91 | Convolutional Neural Networks Applied to House Numbers Digit Classification | http://arxiv.org/pdf/1204.3968v1.pdf | author:Pierre Sermanet, Soumith Chintala, Yann LeCun category:cs.CV cs.LG cs.NE published:2012-04-18 summary:We classify digits of real-world house numbers using convolutional neuralnetworks (ConvNets). ConvNets are hierarchical feature learning neural networkswhose structure is biologically inspired. Unlike many popular vision approachesthat are hand-designed, ConvNets can automatically learn a unique set offeatures optimized for a given task. We augmented the traditional ConvNetarchitecture by learning multi-stage features and by using Lp pooling andestablish a new state-of-the-art of 94.85% accuracy on the SVHN dataset (45.2%error improvement). Furthermore, we analyze the benefits of different poolingmethods and multi-stage features in ConvNets. The source code and a tutorialare available at eblearn.sf.net.
arxiv-600-92 | Learning From An Optimization Viewpoint | http://arxiv.org/pdf/1204.4145v1.pdf | author:Karthik Sridharan category:cs.LG cs.GT published:2012-04-18 summary:In this dissertation we study statistical and online learning problems froman optimization viewpoint.The dissertation is divided into two parts : I. We first consider the question of learnability for statistical learningproblems in the general learning setting. The question of learnability is wellstudied and fully characterized for binary classification and for real valuedsupervised learning problems using the theory of uniform convergence. Howeverwe show that for the general learning setting uniform convergence theory failsto characterize learnability. To fill this void we use stability of learningalgorithms to fully characterize statistical learnability in the generalsetting. Next we consider the problem of online learning. Unlike thestatistical learning framework there is a dearth of generic tools that can beused to establish learnability and rates for online learning problems ingeneral. We provide online analogs to classical tools from statistical learningtheory like Rademacher complexity, covering numbers, etc. We further use thesetools to fully characterize learnability for online supervised learningproblems. II. In the second part, for general classes of convex learning problems, weprovide appropriate mirror descent (MD) updates for online and statisticallearning of these problems. Further, we show that the the MD is near optimalfor online convex learning and for most cases, is also near optimal forstatistical convex learning. We next consider the problem of convexoptimization and show that oracle complexity can be lower bounded by the socalled fat-shattering dimension of the associated linear class. Thus weestablish a strong connection between offline convex optimization problems andstatistical learning problems. We also show that for a large class of highdimensional optimization problems, MD is in fact near optimal even for convexoptimization.
arxiv-600-93 | The Artificial Regression Market | http://arxiv.org/pdf/1204.4154v1.pdf | author:Nathan Lay, Adrian Barbu category:stat.ML math.ST stat.TH published:2012-04-18 summary:The Artificial Prediction Market is a recent machine learning technique formulti-class classification, inspired from the financial markets. It involves anumber of trained market participants that bet on the possible outcomes and arerewarded if they predict correctly. This paper generalizes the scope of theArtificial Prediction Markets to regression, where there are uncountably manypossible outcomes and the error is usually the MSE. For that, we introduce thereward kernel that rewards each participant based on its prediction error andwe derive the price equations. Using two reward kernels we obtain two differentlearning rules, one of which is approximated using Hermite-Gauss quadrature.The market setting makes it easy to aggregate specialized regressors that onlypredict when an observation falls into their specialization domain. Experimentsshow that regression markets based on the two learning rules outperform RandomForest Regression on many UCI datasets and are rarely outperformed.
arxiv-600-94 | Fuzzy Dynamical Genetic Programming in XCSF | http://arxiv.org/pdf/1204.4202v1.pdf | author:Richard J. Preen, Larry Bull category:cs.AI cs.LG cs.NE cs.SY I.2.6 published:2012-04-18 summary:A number of representation schemes have been presented for use withinLearning Classifier Systems, ranging from binary encodings to Neural Networks,and more recently Dynamical Genetic Programming (DGP). This paper presentsresults from an investigation into using a fuzzy DGP representation within theXCSF Learning Classifier System. In particular, asynchronous Fuzzy LogicNetworks are used to represent the traditional condition-action productionsystem rules. It is shown possible to use self-adaptive, open-ended evolutionto design an ensemble of such fuzzy dynamical systems within XCSF to solveseveral well-known continuous-valued test problems.
arxiv-600-95 | Message passing with relaxed moment matching | http://arxiv.org/pdf/1204.4166v2.pdf | author:Yuan Qi, Yandong Guo category:cs.LG stat.CO stat.ML published:2012-04-18 summary:Bayesian learning is often hampered by large computational expense. As apowerful generalization of popular belief propagation, expectation propagation(EP) efficiently approximates the exact Bayesian computation. Nevertheless, EPcan be sensitive to outliers and suffer from divergence for difficult cases. Toaddress this issue, we propose a new approximate inference approach, relaxedexpectation propagation (REP). It relaxes the moment matching requirement ofexpectation propagation by adding a relaxation factor into the KL minimization.We penalize this relaxation with a $l_1$ penalty. As a result, when twodistributions in the relaxed KL divergence are similar, the relaxation factorwill be penalized to zero and, therefore, we obtain the original momentmatching; In the presence of outliers, these two distributions aresignificantly different and the relaxation factor will be used to reduce thecontribution of the outlier. Based on this penalized KL minimization, REP isrobust to outliers and can greatly improve the posterior approximation qualityover EP. To examine the effectiveness of REP, we apply it to Gaussian processclassification, a task known to be suitable to EP. Our classification resultson synthetic and UCI benchmark datasets demonstrate significant improvement ofREP over EP and Power EP--in terms of algorithmic stability, estimationaccuracy and predictive performance.
arxiv-600-96 | Towards the Evolution of Vertical-Axis Wind Turbines using Supershapes | http://arxiv.org/pdf/1204.4107v4.pdf | author:Richard J. Preen, Larry Bull category:cs.NE cs.CG published:2012-04-18 summary:We have recently presented an initial study of evolutionary algorithms usedto design vertical-axis wind turbines (VAWTs) wherein candidate prototypes areevaluated under approximated wind tunnel conditions after being physicallyinstantiated by a 3D printer. That is, unlike other approaches such ascomputational fluid dynamics simulations, no mathematical formulations are usedand no model assumptions are made. However, the representation usedsignificantly restricted the range of morphologies explored. In this paper, wepresent initial explorations into the use of a simple generative encoding,known as Gielis superformula, that produces a highly flexible 3D shaperepresentation to design VAWT. First, the target-based evolution of 3Dartefacts is investigated and subsequently initial design experiments areperformed wherein each VAWT candidate is physically instantiated and evaluatedunder approximated wind tunnel conditions. It is shown possible to produce veryclosely matching designs of a number of 3D objects through the evolution ofsupershapes produced by Gielis superformula. Moreover, it is shown possible touse artificial physical evolution to identify novel and increasingly efficientsupershape VAWT designs.
arxiv-600-97 | EigenGP: Sparse Gaussian process models with data-dependent eigenfunctions | http://arxiv.org/pdf/1204.3972v3.pdf | author:Yuan Qi, Bo Dai, Yao Zhu category:cs.LG stat.CO stat.ML published:2012-04-18 summary:Gaussian processes (GPs) provide a nonparametric representation of functions.However, classical GP inference suffers from high computational cost and it isdifficult to design nonstationary GP priors in practice. In this paper, wepropose a sparse Gaussian process model, EigenGP, based on the Karhunen-Loeve(KL) expansion of a GP prior. We use the Nystrom approximation to obtain datadependent eigenfunctions and select these eigenfunctions by evidencemaximization. This selection reduces the number of eigenfunctions in our modeland provides a nonstationary covariance function. To handle nonlinearlikelihoods, we develop an efficient expectation propagation (EP) inferencealgorithm, and couple it with expectation maximization for eigenfunctionselection. Because the eigenfunctions of a Gaussian kernel are associated withclusters of samples - including both the labeled and unlabeled - selectingrelevant eigenfunctions enables EigenGP to conduct semi-supervised learning.Our experimental results demonstrate improved predictive performance of EigenGPover alternative state-of-the-art sparse GP and semisupervised learning methodsfor regression, classification, and semisupervised classification.
arxiv-600-98 | Discrete Dynamical Genetic Programming in XCS | http://arxiv.org/pdf/1204.4200v2.pdf | author:Richard J. Preen, Larry Bull category:cs.AI cs.LG cs.NE cs.SY I.2.6 published:2012-04-18 summary:A number of representation schemes have been presented for use withinLearning Classifier Systems, ranging from binary encodings to neural networks.This paper presents results from an investigation into using a discretedynamical system representation within the XCS Learning Classifier System. Inparticular, asynchronous random Boolean networks are used to represent thetraditional condition-action production system rules. It is shown possible touse self-adaptive, open-ended evolution to design an ensemble of such discretedynamical systems within XCS to solve a number of well-known test problems.
arxiv-600-99 | Towards Real-Time Summarization of Scheduled Events from Twitter Streams | http://arxiv.org/pdf/1204.3731v1.pdf | author:Arkaitz Zubiaga, Damiano Spina, Enrique Amigó, Julio Gonzalo category:cs.IR cs.CL cs.SI published:2012-04-17 summary:This paper explores the real-time summarization of scheduled events such assoccer games from torrential flows of Twitter streams. We propose and evaluatean approach that substantially shrinks the stream of tweets in real-time, andconsists of two steps: (i) sub-event detection, which determines if somethingnew has occurred, and (ii) tweet selection, which picks a representative tweetto describe each sub-event. We compare the summaries generated in threelanguages for all the soccer games in "Copa America 2011" to reference livereports offered by Yahoo! Sports journalists. We show that simple text analysismethods which do not involve external knowledge lead to summaries that cover84% of the sub-events on average, and 100% of key types of sub-events (such asgoals in soccer). Our approach should be straightforwardly applicable to otherkinds of scheduled events such as other sports, award ceremonies, keynotetalks, TV shows, etc.
arxiv-600-100 | Distributed Iterative Processing for Interference Channels with Receiver Cooperation | http://arxiv.org/pdf/1204.3742v1.pdf | author:Mihai-Alin Badiu, Carles Navarro Manchón, Vasile Bota, Bernard Henri Fleury category:cs.IT math.IT stat.ML published:2012-04-17 summary:We propose a framework for the derivation and evaluation of distributediterative algorithms for receiver cooperation in interference-limited wirelesssystems. Our approach views the processing within and collaboration betweenreceivers as the solution to an inference problem in the probabilistic model ofthe whole system. The probabilistic model is formulated to explicitlyincorporate the receivers' ability to share information of a predefined type.We employ a recently proposed unified message-passing tool to infer thevariables of interest in the factor graph representation of the probabilisticmodel. The exchange of information between receivers arises in the form ofpassing messages along some specific edges of the factor graph; the rate ofupdating and passing these messages determines the communication overheadassociated with cooperation. Simulation results illustrate the high performanceof the proposed algorithm even with a low number of message exchanges betweenreceivers.
arxiv-600-101 | Statistical Multiresolution Estimation for Variational Imaging: With an Application in Poisson-Biophotonics | http://arxiv.org/pdf/1204.3748v1.pdf | author:Klaus Frick, Philipp Marnitz, Axel Munk category:stat.AP cs.CV published:2012-04-17 summary:In this paper we present a spatially-adaptive method for image reconstructionthat is based on the concept of statistical multiresolution estimation asintroduced in [Frick K, Marnitz P, and Munk A. "Statistical multiresolutionDantzig estimation in imaging: Fundamental concepts and algorithmic framework".Electron. J. Stat., 6:231-268, 2012]. It constitutes a variationalregularization technique that uses an supremum-type distance measure asdata-fidelity combined with a convex cost functional. The resulting convexoptimization problem is approached by a combination of an inexact alternatingdirection method of multipliers and Dykstra's projection algorithm. We describea novel method for balancing data-fit and regularity that is fully automaticand allows for a sound statistical interpretation. The performance of ourestimation approach is studied for various problems in imaging. Among others,this includes deconvolution problems that arise in Poisson nanoscalefluorescence microscopy.
arxiv-600-102 | Indus script corpora, archaeo-metallurgy and Meluhha (Mleccha) | http://arxiv.org/pdf/1204.3800v1.pdf | author:Srinivasan Kalyanaraman category:cs.CL published:2012-04-17 summary:Jules Bloch's work on formation of the Marathi language has to be expandedfurther to provide for a study of evolution and formation of Indian languagesin the Indian language union (sprachbund). The paper analyses the stages in theevolution of early writing systems which began with the evolution of countingin the ancient Near East. A stage anterior to the stage of syllabicrepresentation of sounds of a language, is identified. Unique geometric shapesrequired for tokens to categorize objects became too large to handle toabstract hundreds of categories of goods and metallurgical processes during theproduction of bronze-age goods. About 3500 BCE, Indus script as a writingsystem was developed to use hieroglyphs to represent the 'spoken words'identifying each of the goods and processes. A rebus method of representingsimilar sounding words of the lingua franca of the artisans was used in Indusscript. This method is recognized and consistently applied for the linguafranca of the Indian sprachbund. That the ancient languages of India,constituted a sprachbund (or language union) is now recognized by manylinguists. The sprachbund area is proximate to the area where most of the Indusscript inscriptions were discovered, as documented in the corpora. Thathundreds of Indian hieroglyphs continued to be used in metallurgy is evidencedby their use on early punch-marked coins. This explains the combined use ofsyllabic scripts such as Brahmi and Kharoshti together with the hieroglyphs onRampurva copper bolt, and Sohgaura copper plate from about 6th centuryBCE.Indian hieroglyphs constitute a writing system for meluhha language and arerebus representations of archaeo-metallurgy lexemes. The rebus principle wasemployed by the early scripts and can legitimately be used to decipher theIndus script, after secure pictorial identification.
arxiv-600-103 | Regularized Partial Least Squares with an Application to NMR Spectroscopy | http://arxiv.org/pdf/1204.3942v1.pdf | author:Genevera I. Allen, Christine Peterson, Marina Vannucci, Mirjana Maletic-Savatic category:stat.ML published:2012-04-17 summary:High-dimensional data common in genomics, proteomics, and chemometrics oftencontains complicated correlation structures. Recently, partial least squares(PLS) and Sparse PLS methods have gained attention in these areas as dimensionreduction techniques in the context of supervised data analysis. We introduce aframework for Regularized PLS by solving a relaxation of the SIMPLSoptimization problem with penalties on the PLS loadings vectors. Our approachenjoys many advantages including flexibility, general penalties, easyinterpretation of results, and fast computation in high-dimensional settings.We also outline extensions of our methods leading to novel methods forNon-negative PLS and Generalized PLS, an adaption of PLS for structured data.We demonstrate the utility of our methods through simulations and a case studyon proton Nuclear Magnetic Resonance (NMR) spectroscopy data.
arxiv-600-104 | The logic of quantum mechanics - Take II | http://arxiv.org/pdf/1204.3458v1.pdf | author:Bob Coecke category:quant-ph cs.CL cs.LO math.CT math.LO published:2012-04-16 summary:We put forward a new take on the logic of quantum mechanics, followingSchroedinger's point of view that it is composition which makes quantum theorywhat it is, rather than its particular propositional structure due to theexistence of superpositions, as proposed by Birkhoff and von Neumann. Thisgives rise to an intrinsically quantitative kind of logic, which truly deservesthe name `logic' in that it also models meaning in natural language, the latterbeing the origin of logic, that it supports automation, the most prominentpractical use of logic, and that it supports probabilistic inference.
arxiv-600-105 | Explaining Adaptation in Genetic Algorithms With Uniform Crossover: The Hyperclimbing Hypothesis | http://arxiv.org/pdf/1204.3436v1.pdf | author:Keki M. Burjorjee category:cs.NE cs.AI I.2.8; F.2 published:2012-04-16 summary:The hyperclimbing hypothesis is a hypothetical explanation for adaptation ingenetic algorithms with uniform crossover (UGAs). Hyperclimbing is anintuitive, general-purpose, non-local search heuristic applicable to discreteproduct spaces with rugged or stochastic cost functions. The strength of thisheuristic lie in its insusceptibility to local optima when the cost function isdeterministic, and its tolerance for noise when the cost function isstochastic. Hyperclimbing works by decimating a search space, i.e. byiteratively fixing the values of small numbers of variables. The hyperclimbinghypothesis holds that UGAs work by implementing efficient hyperclimbing. Proofof concept for this hypothesis comes from the use of a novel analytic techniqueinvolving the exploitation of algorithmic symmetry. We have also obtainedexperimental results that show that a simple tweak inspired by thehyperclimbing hypothesis dramatically improves the performance of a UGA onlarge, random instances of MAX-3SAT and the Sherrington Kirkpatrick SpinGlasses problem.
arxiv-600-106 | Learning to Predict the Wisdom of Crowds | http://arxiv.org/pdf/1204.3611v1.pdf | author:Seyda Ertekin, Haym Hirsh, Cynthia Rudin category:cs.SI cs.LG published:2012-04-16 summary:The problem of "approximating the crowd" is that of estimating the crowd'smajority opinion by querying only a subset of it. Algorithms that approximatethe crowd can intelligently stretch a limited budget for a crowdsourcing task.We present an algorithm, "CrowdSense," that works in an online fashion todynamically sample subsets of labelers based on an exploration/exploitationcriterion. The algorithm produces a weighted combination of a subset of thelabelers' votes that approximates the crowd's opinion.
arxiv-600-107 | A Computational Analysis of Collective Discourse | http://arxiv.org/pdf/1204.3498v2.pdf | author:Vahed Qazvinian, Dragomir R. Radev category:cs.SI cs.CL physics.soc-ph published:2012-04-16 summary:This paper is focused on the computational analysis of collective discourse,a collective behavior seen in non-expert content contributions in online socialmedia. We collect and analyze a wide range of real-world collective discoursedatasets from movie user reviews to microblogs and news headlines to scientificcitations. We show that all these datasets exhibit diversity of perspective, aproperty seen in other collective systems and a criterion in wise crowds. Ourexperiments also confirm that the network of different perspectiveco-occurrences exhibits the small-world property with high clustering ofdifferent perspectives. Finally, we show that non-expert contributions incollective discourse can be used to answer simple questions that are otherwisehard to answer.
arxiv-600-108 | Large-Scale Automatic Labeling of Video Events with Verbs Based on Event-Participant Interaction | http://arxiv.org/pdf/1204.3616v1.pdf | author:Andrei Barbu, Alexander Bridge, Dan Coroian, Sven Dickinson, Sam Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark Siskind, Jarrell Waggoner, Song Wang, Jinlian Wei, Yifan Yin, Zhiqi Zhang category:cs.CV cs.AI published:2012-04-16 summary:We present an approach to labeling short video clips with English verbs asevent descriptions. A key distinguishing aspect of this work is that it labelsvideos with verbs that describe the spatiotemporal interaction between eventparticipants, humans and objects interacting with each other, abstracting awayall object-class information and fine-grained image characteristics, andrelying solely on the coarse-grained motion of the event participants. We applyour approach to a large set of 22 distinct verb classes and a corpus of 2,584videos, yielding two surprising outcomes. First, a classification accuracy ofgreater than 70% on a 1-out-of-22 labeling task and greater than 85% on avariety of 1-out-of-10 subsets of this labeling task is independent of thechoice of which of two different time-series classifiers we employ. Second, weachieve this level of accuracy using a highly impoverished intermediaterepresentation consisting solely of the bounding boxes of one or two eventparticipants as a function of time. This indicates that successful eventrecognition depends more on the choice of appropriate features thatcharacterize the linguistic invariants of the event classes than on theparticular classifier algorithms.
arxiv-600-109 | Efficient Protocols for Distributed Classification and Optimization | http://arxiv.org/pdf/1204.3523v1.pdf | author:Hal Daume III, Jeff M. Phillips, Avishek Saha, Suresh Venkatasubramanian category:cs.LG stat.ML published:2012-04-16 summary:In distributed learning, the goal is to perform a learning task over datadistributed across multiple nodes with minimal (expensive) communication. Priorwork (Daume III et al., 2012) proposes a general model that bounds thecommunication required for learning classifiers while allowing for $\eps$training error on linearly separable data adversarially distributed acrossnodes. In this work, we develop key improvements and extensions to this basic model.Our first result is a two-party multiplicative-weight-update based protocolthat uses $O(d^2 \log{1/\eps})$ words of communication to classify distributeddata in arbitrary dimension $d$, $\eps$-optimally. This readily extends toclassification over $k$ nodes with $O(kd^2 \log{1/\eps})$ words ofcommunication. Our proposed protocol is simple to implement and is considerablymore efficient than baselines compared, as demonstrated by our empiricalresults. In addition, we illustrate general algorithm design paradigms for doingefficient learning over distributed data. We show how to solvefixed-dimensional and high dimensional linear programming efficiently in adistributed setting where constraints may be distributed across nodes. Sincemany learning problems can be viewed as convex optimization problems whereconstraints are generated by individual points, this models many typicaldistributed learning scenarios. Our techniques make use of a novel connectionfrom multipass streaming, as well as adapting the multiplicative-weight-updateframework more generally to a distributed setting. As a consequence, ourmethods extend to the wide range of problems solvable using these techniques.
arxiv-600-110 | Learning Sets with Separating Kernels | http://arxiv.org/pdf/1204.3573v2.pdf | author:Ernesto De Vito, Lorenzo Rosasco, Alessandro Toigo category:stat.ML published:2012-04-16 summary:We consider the problem of learning a set from random samples. We show howrelevant geometric and topological properties of a set can be studiedanalytically using concepts from the theory of reproducing kernel Hilbertspaces. A new kind of reproducing kernel, that we call separating kernel, playsa crucial role in our study and is analyzed in detail. We prove a new analyticcharacterization of the support of a distribution, that naturally leads to afamily of provably consistent regularized learning algorithms and we discussthe stability of these methods with respect to random sampling. Numericalexperiments show that the approach is competitive, and often better, than otherstate of the art techniques.
arxiv-600-111 | Distributed Learning, Communication Complexity and Privacy | http://arxiv.org/pdf/1204.3514v3.pdf | author:Maria-Florina Balcan, Avrim Blum, Shai Fine, Yishay Mansour category:cs.LG cs.DS F.2.2; I.2.6 published:2012-04-16 summary:We consider the problem of PAC-learning from distributed data and analyzefundamental communication complexity questions involved. We provide generalupper and lower bounds on the amount of communication needed to learn well,showing that in addition to VC-dimension and covering number, quantities suchas the teaching-dimension and mistake-bound of a class play an important role.We also present tight results for a number of common concept classes includingconjunctions, parity functions, and decision lists. For linear separators, weshow that for non-concentrated distributions, we can use a version of thePerceptron algorithm to learn with much less communication than the number ofupdates given by the usual margin bound. We also show how boosting can beperformed in a generic manner in the distributed setting to achievecommunication with only logarithmic dependence on 1/epsilon for any conceptclass, and demonstrate how recent work on agnostic learning fromclass-conditional queries can be used to achieve low communication in agnosticsettings as well. We additionally present an analysis of privacy, consideringboth differential privacy and a notion of distributional privacy that isespecially appealing in this context.
arxiv-600-112 | Plug-in martingales for testing exchangeability on-line | http://arxiv.org/pdf/1204.3251v2.pdf | author:Valentina Fedorova, Alex Gammerman, Ilia Nouretdinov, Vladimir Vovk category:cs.LG stat.ME 62G10 I.2.6 published:2012-04-15 summary:A standard assumption in machine learning is the exchangeability of data,which is equivalent to assuming that the examples are generated from the sameprobability distribution independently. This paper is devoted to testing theassumption of exchangeability on-line: the examples arrive one by one, andafter receiving each example we would like to have a valid measure of thedegree to which the assumption of exchangeability has been falsified. Suchmeasures are provided by exchangeability martingales. We extend knowntechniques for constructing exchangeability martingales and show that our newmethod is competitive with the martingales introduced before. Finally weinvestigate the performance of our testing method on two benchmark datasets,USPS and Statlog Satellite data; for the former, the known techniques givesatisfactory results, but for the latter our new more flexible method becomesnecessary.
arxiv-600-113 | Neuroevolution Results in Emergence of Short-Term Memory for Goal-Directed Behavior | http://arxiv.org/pdf/1204.3221v1.pdf | author:Konstantin Lakhman, Mikhail Burtsev category:cs.NE cs.AI nlin.AO published:2012-04-14 summary:Animals behave adaptively in the environment with multiply competing goals.Understanding of the mechanisms underlying such goal-directed behavior remainsa challenge for neuroscience as well for adaptive system research. To addressthis problem we developed an evolutionary model of adaptive behavior in themultigoal stochastic environment. Proposed neuroevolutionary algorithm is basedon neuron's duplication as a basic mechanism of agent's recurrent neuralnetwork development. Results of simulation demonstrate that in the course ofevolution agents acquire the ability to store the short-term memory and,therefore, use it in behavioral strategies with alternative actions. We foundthat evolution discovered two mechanisms for short-term memory. The firstmechanism is integration of sensory signals and ongoing internal neuralactivity, resulting in emergence of cell groups specialized on alternativeactions. And the second mechanism is slow neurodynamical processes that makespossible to code the previous behavioral choice.
arxiv-600-114 | The failure of the law of brevity in two New World primates. Statistical caveats | http://arxiv.org/pdf/1204.3198v2.pdf | author:Ramon Ferrer-i-Cancho, Antoni Hernández-Fernández category:q-bio.NC cs.CL published:2012-04-14 summary:Parallels of Zipf's law of brevity, the tendency of more frequent words to beshorter, have been found in bottlenose dolphins and Formosan macaques. Althoughthese findings suggest that behavioral repertoires are shaped by a generalprinciple of compression, common marmosets and golden-backed uakaris do notexhibit the law. However, we argue that the law may be impossible or difficultto detect statistically in a given species if the repertoire is too small, aproblem that could be affecting golden backed uakaris, and show that the law ispresent in a subset of the repertoire of common marmosets. We suggest that thevisibility of the law will depend on the subset of the repertoire underconsideration or the repertoire size.
arxiv-600-115 | Compensating Interpolation Distortion by Using New Optimized Modular Method | http://arxiv.org/pdf/1204.3618v1.pdf | author:Mohammad Tofighi, Ali Ayremlou, Farokh Marvasti category:cs.CV cs.MM published:2012-04-13 summary:A modular method was suggested before to recover a band limited signal fromthe sample and hold and linearly interpolated (or, in general, annth-order-hold) version of the regular samples. In this paper a novel approachfor compensating the distortion of any interpolation based on modular methodhas been proposed. In this method the performance of the modular method isoptimized by adding only some simply calculated coefficients. This approachcauses drastic improvement in terms of signal-to-noise ratios with fewermodules compared to the classical modular method. Simulation results clearlyconfirm the improvement of the proposed method and also its superior robustnessagainst additive noise.
arxiv-600-116 | Non-sparse Linear Representations for Visual Tracking with Online Reservoir Metric Learning | http://arxiv.org/pdf/1204.2912v1.pdf | author:Xi Li, Chunhua Shen, Qinfeng Shi, Anthony Dick, Anton van den Hengel category:cs.CV published:2012-04-13 summary:Most sparse linear representation-based trackers need to solve acomputationally expensive L1-regularized optimization problem. To address thisproblem, we propose a visual tracker based on non-sparse linearrepresentations, which admit an efficient closed-form solution withoutsacrificing accuracy. Moreover, in order to capture the correlation informationbetween different feature dimensions, we learn a Mahalanobis distance metric inan online fashion and incorporate the learned metric into the optimizationproblem for obtaining the linear representation. We show that online metriclearning using proximity comparison significantly improves the robustness ofthe tracking, especially on those sequences exhibiting drastic appearancechanges. Furthermore, in order to prevent the unbounded growth in the number oftraining samples for the metric learning, we design a time-weighted reservoirsampling method to maintain and update limited-sized foreground and backgroundsample buffers for balancing sample diversity and adaptability. Experimentalresults on challenging videos demonstrate the effectiveness and robustness ofthe proposed tracker.
arxiv-600-117 | Collaboration and Coordination in Secondary Networks for Opportunistic Spectrum Access | http://arxiv.org/pdf/1204.3005v1.pdf | author:Wassim Jouini, Marco Di Felice, Luciano Bononi, Christophe Moy category:stat.AP cs.NI stat.ML published:2012-04-13 summary:In this paper, we address the general case of a coordinated secondary networkwilling to exploit communication opportunities left vacant by a licensedprimary network. Since secondary users (SU) usually have no prior knowledge onthe environment, they need to learn the availability of each channel throughsensing techniques, which however can be prone to detection errors. We arguethat cooperation among secondary users can enable efficient learning andcoordination mechanisms in order to maximize the spectrum exploitation by SUs,while minimizing the impact on the primary network. To this goal, we providethree novel contributions in this paper. First, we formulate the spectrumselection in secondary networks as an instance of the Multi-Armed Bandit (MAB)problem, and we extend the analysis to the collaboration learning case, inwhich each SU learns the spectrum occupation, and shares this information withother SUs. We show that collaboration among SUs can mitigate the impact ofsensing errors on system performance, and improve the convergence of thelearning process to the optimal solution. Second, we integrate the learningalgorithms with two collaboration techniques based on modified versions of theHungarian algorithm and of the Round Robin algorithm that allows reducing theinterference among SUs. Third, we derive fundamental limits to the performanceof cooperative learning algorithms based on Upper Confidence Bound (UCB)policies in a symmetric scenario where all SU have the same perception of thequality of the resources. Extensive simulation results confirm theeffectiveness of our joint learning-collaboration algorithm in protecting theoperations of Primary Users (PUs), while maximizing the performance of SUs.
arxiv-600-118 | Image Restoration with Signal-dependent Camera Noise | http://arxiv.org/pdf/1204.2994v1.pdf | author:Ayan Chakrabarti, Todd Zickler category:cs.CV stat.AP published:2012-04-13 summary:This article describes a fast iterative algorithm for image denoising anddeconvolution with signal-dependent observation noise. We use an optimizationstrategy based on variable splitting that adapts traditional Gaussiannoise-based restoration algorithms to account for the observed image beingcorrupted by mixed Poisson-Gaussian noise and quantization errors.
arxiv-600-119 | Detecting lateral genetic material transfer | http://arxiv.org/pdf/1204.2601v1.pdf | author:C. Calderón, L. Delaye, V. Mireles, P. Miramontes category:cs.NE cs.AI q-bio.GN published:2012-04-12 summary:The bioinformatical methods to detect lateral gene transfer events are mainlybased on functional coding DNA characteristics. In this paper, we propose theuse of DNA traits not depending on protein coding requirements. We introduceseveral semilocal variables that depend on DNA primary sequence and thatreflect thermodynamic as well as physico-chemical magnitudes that are able totell apart the genome of different organisms. After combining these variablesin a neural classificator, we obtain results whose power of resolution go asfar as to detect the exchange of genomic material between bacteria that arephylogenetically close.
arxiv-600-120 | A practical approach to language complexity: a Wikipedia case study | http://arxiv.org/pdf/1204.2765v2.pdf | author:Taha Yasseri, András Kornai, János Kertész category:cs.CL physics.soc-ph published:2012-04-12 summary:In this paper we present statistical analysis of English texts fromWikipedia. We try to address the issue of language complexity empirically bycomparing the simple English Wikipedia (Simple) to comparable samples of themain English Wikipedia (Main). Simple is supposed to use a more simplifiedlanguage with a limited vocabulary, and editors are explicitly requested tofollow this guideline, yet in practice the vocabulary richness of both samplesare at the same level. Detailed analysis of longer units (n-grams of words andpart of speech tags) shows that the language of Simple is less complex thanthat of Main primarily due to the use of shorter sentences, as opposed todrastically simplified syntax or vocabulary. Comparing the two languagevarieties by the Gunning readability index supports this conclusion. We alsoreport on the topical dependence of language complexity, e.g. that the languageis more advanced in conceptual articles compared to person-based (biographical)and object-based articles. Finally, we investigate the relation betweenconflict and language complexity by analyzing the content of the talk pagesassociated to controversial and peacefully developing articles, concluding thatcontroversy has the effect of reducing language complexity.
arxiv-600-121 | Seeing Unseeability to See the Unseeable | http://arxiv.org/pdf/1204.2801v1.pdf | author:Siddharth Narayanaswamy, Andrei Barbu, Jeffrey Mark Siskind category:cs.CV cs.AI cs.RO published:2012-04-12 summary:We present a framework that allows an observer to determine occluded portionsof a structure by finding the maximum-likelihood estimate of those occludedportions consistent with visible image evidence and a consistency model. Doingthis requires determining which portions of the structure are occluded in thefirst place. Since each process relies on the other, we determine a solution toboth problems in tandem. We extend our framework to determine confidence ofone's assessment of which portions of an observed structure are occluded, andthe estimate of that occluded structure, by determining the sensitivity ofone's assessment to potential new observations. We further extend our frameworkto determine a robotic action whose execution would allow a new observationthat would maximally increase one's confidence.
arxiv-600-122 | Simultaneous Object Detection, Tracking, and Event Recognition | http://arxiv.org/pdf/1204.2741v1.pdf | author:Andrei Barbu, Aaron Michaux, Siddharth Narayanaswamy, Jeffrey Mark Siskind category:cs.CV cs.AI published:2012-04-12 summary:The common internal structure and algorithmic organization of objectdetection, detection-based tracking, and event recognition facilitates ageneral approach to integrating these three components. This supportsmultidirectional information flow between these components allowing objectdetection to influence tracking and event recognition and event recognition toinfluence tracking and object detection. The performance of the combination canexceed the performance of the components in isolation. This can be done withlinear asymptotic complexity.
arxiv-600-123 | Video In Sentences Out | http://arxiv.org/pdf/1204.2742v1.pdf | author:Andrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven Dickinson, Sanja Fidler, Aaron Michaux, Sam Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark Siskind, Jarrell Waggoner, Song Wang, Jinlian Wei, Yifan Yin, Zhiqi Zhang category:cs.CV cs.AI published:2012-04-12 summary:We present a system that produces sentential descriptions of video: who didwhat to whom, and where and how they did it. Action class is rendered as averb, participant objects as noun phrases, properties of those objects asadjectival modifiers in those noun phrases,spatial relations between thoseparticipants as prepositional phrases, and characteristics of the event asprepositional-phrase adjuncts and adverbial modifiers. Extracting theinformation needed to render these linguistic entities requires an approach toevent recognition that recovers object tracks, the track-to-role assignments,and changing body posture.
arxiv-600-124 | Segmentation Similarity and Agreement | http://arxiv.org/pdf/1204.2847v2.pdf | author:Chris Fournier, Diana Inkpen category:cs.CL published:2012-04-12 summary:We propose a new segmentation evaluation metric, called segmentationsimilarity (S), that quantifies the similarity between two segmentations as theproportion of boundaries that are not transformed when comparing them usingedit distance, essentially using edit distance as a penalty function andscaling penalties by segmentation size. We propose several adaptedinter-annotator agreement coefficients which use S that are suitable forsegmentation. We show that S is configurable enough to suit a wide variety ofsegmentation evaluations, and is an improvement upon the state of the art. Wealso propose using inter-annotator agreement coefficients to evaluate automaticsegmenters in terms of human performance.
arxiv-600-125 | Watersheds, waterfalls, on edge or node weighted graphs | http://arxiv.org/pdf/1204.2837v1.pdf | author:Fernand Meyer category:cs.CV cs.DM 68U10, 05C85 published:2012-04-12 summary:We present an algebraic approach to the watershed adapted to edge or nodeweighted graphs. Starting with the flooding adjunction, we introduce theflooding graphs, for which node and edge weights may be deduced one from theother. Each node weighted or edge weighted graph may be transformed in aflooding graph, showing that there is no superiority in using one or the other,both being equivalent. We then introduce pruning operators extract subgraphs ofincreasing steepness. For an increasing steepness, the number of neverascending paths becomes smaller and smaller. This reduces the watershed zone,where catchment basins overlap. A last pruning operator called scissorassociates to each node outside the regional minima one and only one edge. Thecatchment basins of this new graph do not overlap and form a watershedpartition. Again, with an increasing steepness, the number of distinctwatershed partitions contained in a graph becomes smaller and smaller.Ultimately, for natural image, an infinite steepness leads to a uniquesolution, as it is not likely that two absolutely identical non ascending pathsof infinite steepness connect a node with two distinct minima. It happens thatnon ascending paths of a given steepness are the geodesics of lexicographicdistance functions of a given depth. This permits to extract the watershedpartitions as skeletons by zone of influence of the minima for suchlexicographic distances. The waterfall hierarchy is obtained by a sequence ofoperations. The first constructs the minimum spanning forest which spans aninitial watershed partition. The contraction of the trees into one nodeproduces a reduced graph which may be submitted to the same treatment. Theprocess is iterated until only one region remains. The union of the edges ofall forests produced constitutes a minimum spanning tree of the initial graph.
arxiv-600-126 | Stochastic Feature Mapping for PAC-Bayes Classification | http://arxiv.org/pdf/1204.2609v2.pdf | author:Xiong Li, Tai Sing Lee, Yuncai Liu category:cs.LG published:2012-04-12 summary:Probabilistic generative modeling of data distributions can potentiallyexploit hidden information which is useful for discriminative classification.This observation has motivated the development of approaches that couplegenerative and discriminative models for classification. In this paper, wepropose a new approach to couple generative and discriminative models in anunified framework based on PAC-Bayes risk theory. We first derive themodel-parameter-independent stochastic feature mapping from a practical MAPclassifier operating on generative models. Then we construct a linearstochastic classifier equipped with the feature mapping, and derive theexplicit PAC-Bayes risk bounds for such classifier for both supervised andsemi-supervised learning. Minimizing the risk bound, using an EM-like iterativeprocedure, results in a new posterior over hidden variables (E-step) and theupdate rules of model parameters (M-step). The derivation of the posterior isalways feasible due to the way of equipping feature mapping and the explicitform of bounding risk. The derived posterior allows the tuning of generativemodels and subsequently the feature mappings for better classification. Thederived update rules of the model parameters are same to those of the uncoupledmodels as the feature mapping is model-parameter-independent. Our experimentsshow that the coupling between data modeling generative model and thediscriminative classifier via a stochastic feature mapping in this frameworkleads to a general classification tool with state-of-the-art performance.
arxiv-600-127 | Estimating the Prevalence of Deception in Online Review Communities | http://arxiv.org/pdf/1204.2804v1.pdf | author:Myle Ott, Claire Cardie, Jeff Hancock category:cs.SI cs.CL cs.CY published:2012-04-12 summary:Consumers' purchase decisions are increasingly influenced by user-generatedonline reviews. Accordingly, there has been growing concern about the potentialfor posting "deceptive opinion spam" -- fictitious reviews that have beendeliberately written to sound authentic, to deceive the reader. But while thispractice has received considerable public attention and concern, relativelylittle is known about the actual prevalence, or rate, of deception in onlinereview communities, and less still about the factors that influence it. We propose a generative model of deception which, in conjunction with adeception classifier, we use to explore the prevalence of deception in sixpopular online review communities: Expedia, Hotels.com, Orbitz, Priceline,TripAdvisor, and Yelp. We additionally propose a theoretical model of onlinereviews based on economic signaling theory, in which consumer reviews diminishthe inherent information asymmetry between consumers and producers, by actingas a signal to a product's true, unknown quality. We find that deceptiveopinion spam is a growing problem overall, but with different growth ratesacross communities. These rates, we argue, are driven by the differentsignaling costs associated with deception for each review community, e.g.,posting requirements. When measures are taken to increase signaling cost, e.g.,filtering reviews written by first-time reviewers, deception prevalence iseffectively reduced.
arxiv-600-128 | Derivation of Upper Bounds on Optimization Time of Population-Based Evolutionary Algorithm on a Function with Fitness Plateaus Using Elitism Levels Traverse Mechanism | http://arxiv.org/pdf/1204.2321v6.pdf | author:Aram Ter-Sarkisov, Stephen Marsland category:cs.NE cs.AI published:2012-04-11 summary:In this article a tool for the analysis of population-based EAs is used toderive asymptotic upper bounds on the optimization time of the algorithmsolving Royal Roads problem, a test function with plateaus of fitness. Inaddition to this, limiting distribution of a certain subset of the populationis approximated.
arxiv-600-129 | Collaborative Representation based Classification for Face Recognition | http://arxiv.org/pdf/1204.2358v2.pdf | author:Lei Zhang, Meng Yang, Xiangchu Feng, Yi Ma, David Zhang category:cs.CV published:2012-04-11 summary:By coding a query sample as a sparse linear combination of all trainingsamples and then classifying it by evaluating which class leads to the minimalcoding residual, sparse representation based classification (SRC) leads tointeresting results for robust face recognition. It is widely believed that thel1- norm sparsity constraint on coding coefficients plays a key role in thesuccess of SRC, while its use of all training samples to collaborativelyrepresent the query sample is rather ignored. In this paper we discuss how SRCworks, and show that the collaborative representation mechanism used in SRC ismuch more crucial to its success of face classification. The SRC is a specialcase of collaborative representation based classification (CRC), which hasvarious instantiations by applying different norms to the coding residual andcoding coefficient. More specifically, the l1 or l2 norm characterization ofcoding residual is related to the robustness of CRC to outlier facial pixels,while the l1 or l2 norm characterization of coding coefficient is related tothe degree of discrimination of facial features. Extensive experiments wereconducted to verify the face recognition accuracy and efficiency of CRC withdifferent instantiations.
arxiv-600-130 | Least Absolute Gradient Selector: Statistical Regression via Pseudo-Hard Thresholding | http://arxiv.org/pdf/1204.2353v4.pdf | author:Kun Yang category:stat.ML stat.AP stat.ME published:2012-04-11 summary:Variable selection in linear models plays a pivotal role in modernstatistics. Hard-thresholding methods such as $l_0$ regularization aretheoretically ideal but computationally infeasible. In this paper, we propose anew approach, called the LAGS, short for "least absulute gradient selector", tothis challenging yet interesting problem by mimicking the discrete selectionprocess of $l_0$ regularization. To estimate $\beta$ under the influence ofnoise, we consider, nevertheless, the following convex program [\hat{\beta} =\textrm{arg min}\frac{1}{n}\X^{T}(y - X\beta)\_1 + \lambda_n\sum_{i =1}^pw_i(y;X;n)\beta_i] $\lambda_n > 0$ controls the sparsity and $w_i > 0$ dependent on $y, X$ and$n$ is the weights on different $\beta_i$; $n$ is the sample size.Surprisingly, we shall show in the paper, both geometrically and analytically,that LAGS enjoys two attractive properties: (1) LAGS demonstrates discreteselection behavior and hard thresholding property as $l_0$ regularization bystrategically chosen $w_i$, we call this property "pseudo-hard thresholding";(2) Asymptotically, LAGS is consistent and capable of discovering the truemodel; nonasymptotically, LAGS is capable of identifying the sparsity in themodel and the prediction error of the coefficients is bounded at the noiselevel up to a logarithmic factor---$\log p$, where $p$ is the number ofpredictors. Computationally, LAGS can be solved efficiently by convex program routinesfor its convexity or by simplex algorithm after recasting it into a linearprogram. The numeric simulation shows that LAGS is superior compared tosoft-thresholding methods in terms of mean squared error and parsimony of themodel.
arxiv-600-131 | Robust Nonnegative Matrix Factorization via $L_1$ Norm Regularization | http://arxiv.org/pdf/1204.2311v1.pdf | author:Bin Shen, Luo Si, Rongrong Ji, Baodi Liu category:cs.LG cs.CV stat.ML published:2012-04-11 summary:Nonnegative Matrix Factorization (NMF) is a widely used technique in manyapplications such as face recognition, motion segmentation, etc. Itapproximates the nonnegative data in an original high dimensional space with alinear representation in a low dimensional space by using the product of twononnegative matrices. In many applications data are often partially corruptedwith large additive noise. When the positions of noise are known, some existingvariants of NMF can be applied by treating these corrupted entries as missingvalues. However, the positions are often unknown in many real worldapplications, which prevents the usage of traditional NMF or other existingvariants of NMF. This paper proposes a Robust Nonnegative Matrix Factorization(RobustNMF) algorithm that explicitly models the partial corruption as largeadditive noise without requiring the information of positions of noise. Inpractice, large additive noise can be used to model outliers. In particular,the proposed method jointly approximates the clean data matrix with the productof two nonnegative matrices and estimates the positions and values ofoutliers/noise. An efficient iterative optimization algorithm with a solidtheoretical justification has been proposed to learn the desired matrixfactorization. Experimental results demonstrate the advantages of the proposedalgorithm.
arxiv-600-132 | Automated Generation of Cross-Domain Analogies via Evolutionary Computation | http://arxiv.org/pdf/1204.2335v1.pdf | author:Atilim Gunes Baydin, Ramon Lopez de Mantaras, Santiago Ontanon category:cs.NE nlin.AO published:2012-04-11 summary:Analogy plays an important role in creativity, and is extensively used inscience as well as art. In this paper we introduce a technique for theautomated generation of cross-domain analogies based on a novel evolutionaryalgorithm (EA). Unlike existing work in computational analogy-making restrictedto creating analogies between two given cases, our approach, for a given case,is capable of creating an analogy along with the novel analogous case itself.Our algorithm is based on the concept of "memes", which are units of culture,or knowledge, undergoing variation and selection under a fitness measure, andrepresents evolving pieces of knowledge as semantic networks. Using a fitnessfunction based on Gentner's structure mapping theory of analogies, wedemonstrate the feasibility of spontaneously generating semantic networks thatare analogous to a given base network.
arxiv-600-133 | Probabilistic Latent Tensor Factorization Model for Link Pattern Prediction in Multi-relational Networks | http://arxiv.org/pdf/1204.2588v1.pdf | author:Sheng Gao, Ludovic Denoyer, Patrick Gallinari category:cs.SI cs.LG stat.ML 15A69 H.2.8; J.4 published:2012-04-11 summary:This paper aims at the problem of link pattern prediction in collections ofobjects connected by multiple relation types, where each type may play adistinct role. While common link analysis models are limited to single-typelink prediction, we attempt here to capture the correlations among differentrelation types and reveal the impact of various relation types on performancequality. For that, we define the overall relations between object pairs as a\textit{link pattern} which consists in interaction pattern and connectionstructure in the network, and then use tensor formalization to jointly modeland predict the link patterns, which we refer to as \textit{Link PatternPrediction} (LPP) problem. To address the issue, we propose a ProbabilisticLatent Tensor Factorization (PLTF) model by introducing another latent factorfor multiple relation types and furnish the Hierarchical Bayesian treatment ofthe proposed probabilistic model to avoid overfitting for solving the LPPproblem. To learn the proposed model we develop an efficient Markov Chain MonteCarlo sampling method. Extensive experiments are conducted on several realworld datasets and demonstrate significant improvements over several existingstate-of-the-art methods.
arxiv-600-134 | Modeling Relational Data via Latent Factor Blockmodel | http://arxiv.org/pdf/1204.2581v1.pdf | author:Sheng Gao, Ludovic Denoyer, Patrick Gallinari category:cs.DS cs.LG stat.ML 15A83 H.2.8; J.4 published:2012-04-11 summary:In this paper we address the problem of modeling relational data, whichappear in many applications such as social network analysis, recommendersystems and bioinformatics. Previous studies either consider latent featurebased models but disregarding local structure in the network, or focusexclusively on capturing local structure of objects based on latent blockmodelswithout coupling with latent characteristics of objects. To combine thebenefits of the previous work, we propose a novel model that can simultaneouslyincorporate the effect of latent features and covariates if any, as well as theeffect of latent structure that may exist in the data. To achieve this, wemodel the relation graph as a function of both latent feature factors andlatent cluster memberships of objects to collectively discover globallypredictive intrinsic properties of objects and capture latent block structurein the network to improve prediction performance. We also develop anoptimization transfer algorithm based on the generalized EM-style strategy tolearn the latent factors. We prove the efficacy of our proposed model throughthe link prediction task and cluster analysis task, and extensive experimentson the synthetic data and several real world datasets suggest that our proposedLFBM model outperforms the other state of the art approaches in the evaluatedtasks.
arxiv-600-135 | Concept Modeling with Superwords | http://arxiv.org/pdf/1204.2523v1.pdf | author:Khalid El-Arini, Emily B. Fox, Carlos Guestrin category:stat.ML cs.CL cs.IR cs.LG published:2012-04-11 summary:In information retrieval, a fundamental goal is to transform a document intoconcepts that are representative of its content. The term "representative" isin itself challenging to define, and various tasks require differentgranularities of concepts. In this paper, we aim to model concepts that aresparse over the vocabulary, and that flexibly adapt their content based onother relevant semantic information such as textual structure or associatedimage features. We explore a Bayesian nonparametric model based on nested betaprocesses that allows for inferring an unknown number of strictly sparseconcepts. The resulting model provides an inherently different representationof concepts than a standard LDA (or HDP) based topic model, and allows fordirect incorporation of semantic features. We demonstrate the utility of thisrepresentation on multilingual blog data and the Congressional Record.
arxiv-600-136 | Feature Extraction Methods for Color Image Similarity | http://arxiv.org/pdf/1204.2336v1.pdf | author:R. Venkata Ramana Chary, D. Rajya Lakshmi, K. V. N. Sunitha category:cs.CV published:2012-04-11 summary:Many User interactive systems are proposed all methods are trying toimplement as a user friendly and various approaches proposed but most of thesystems not reached to the use specifications like user friendly systems withuser interest, all proposed method implemented basic techniques some areimproved methods also propose but not reaching to the user specifications. Inthis proposed paper we concentrated on image retrieval system with in earlydays many user interactive systems performed with basic concepts but suchsystems are not reaching to the user specifications and not attracted to theuser so a lot of research interest in recent years with new specifications,recent approaches have user is interested in friendly interacted methods areexpecting, many are concentrated for improvement in all methods. In thisproposed system we focus on the retrieval of images within a large imagecollection based on color projections and different mathematical approaches areintroduced and applied for retrieval of images. before Appling proposed methodsimages are sub grouping using threshold values, in this paper R G B colorcombinations considered for retrieval of images, in proposed methods areimplemented and results are included, through results it is observed that weobtaining efficient results comparatively previous and existing.
arxiv-600-137 | A Simple Explanation of A Spectral Algorithm for Learning Hidden Markov Models | http://arxiv.org/pdf/1204.2477v1.pdf | author:Matthew James Johnson category:stat.ME cs.LG stat.ML published:2012-04-11 summary:A simple linear algebraic explanation of the algorithm in "A SpectralAlgorithm for Learning Hidden Markov Models" (COLT 2009). Most of the contentis in Figure 2; the text just makes everything precise in four nearly-trivialclaims.
arxiv-600-138 | Self-Adaptive Surrogate-Assisted Covariance Matrix Adaptation Evolution Strategy | http://arxiv.org/pdf/1204.2356v1.pdf | author:Ilya Loshchilov, Marc Schoenauer, Michèle Sebag category:cs.NE published:2012-04-11 summary:This paper presents a novel mechanism to adapt surrogate-assistedpopulation-based algorithms. This mechanism is applied to ACM-ES, a recentlyproposed surrogate-assisted variant of CMA-ES. The resulting algorithm,saACM-ES, adjusts online the lifelength of the current surrogate model (thenumber of CMA-ES generations before learning a new surrogate) and the surrogatehyper-parameters. Both heuristics significantly improve the quality of thesurrogate model, yielding a significant speed-up of saACM-ES compared to theACM-ES and CMA-ES baselines. The empirical validation of saACM-ES on theBBOB-2012 noiseless testbed demonstrates the efficiency and the scalabilityw.r.t the problem dimension and the population size of the proposed approach,that reaches new best results on some of the benchmark problems.
arxiv-600-139 | Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing | http://arxiv.org/pdf/1204.2436v1.pdf | author:Nicolas Gillis category:stat.ML math.NA math.OC published:2012-04-11 summary:Nonnegative matrix factorization (NMF) has become a very popular technique inmachine learning because it automatically extracts meaningful features througha sparse and part-based representation. However, NMF has the drawback of beinghighly ill-posed, that is, there typically exist many different but equivalentfactorizations. In this paper, we introduce a completely new way to obtainingmore well-posed NMF problems whose solutions are sparser. Our technique isbased on the preprocessing of the nonnegative input data matrix, and relies onthe theory of M-matrices and the geometric interpretation of NMF. This approachprovably leads to optimal and sparse solutions under the separabilityassumption of Donoho and Stodden (NIPS, 2003), and, for rank-three matrices,makes the number of exact factorizations finite. We illustrate theeffectiveness of our technique on several image datasets.
arxiv-600-140 | Coherence Functions with Applications in Large-Margin Classification Methods | http://arxiv.org/pdf/1204.2049v1.pdf | author:Zhihua Zhang, Guang Dai, Michael I. Jordan category:stat.ML published:2012-04-10 summary:Support vector machines (SVMs) naturally embody sparseness due to their useof hinge loss functions. However, SVMs can not directly estimate conditionalclass probabilities. In this paper we propose and study a family of coherencefunctions, which are convex and differentiable, as surrogates of the hingefunction. The coherence function is derived by using the maximum-entropyprinciple and is characterized by a temperature parameter. It bridges the hingefunction and the logit function in logistic regression. The limit of thecoherence function at zero temperature corresponds to the hinge function, andthe limit of the minimizer of its expected error is the minimizer of theexpected error of the hinge loss. We refer to the use of the coherence functionin large-margin classification as C-learning, and we present efficientcoordinate descent algorithms for the training of regularized ${\calC}$-learning models.
arxiv-600-141 | A Fuzzy Similarity Based Concept Mining Model for Text Classification | http://arxiv.org/pdf/1204.2061v1.pdf | author:Shalini Puri category:cs.IR cs.LG published:2012-04-10 summary:Text Classification is a challenging and a red hot field in the currentscenario and has great importance in text categorization applications. A lot ofresearch work has been done in this field but there is a need to categorize acollection of text documents into mutually exclusive categories by extractingthe concepts or features using supervised learning paradigm and differentclassification algorithms. In this paper, a new Fuzzy Similarity Based ConceptMining Model (FSCMM) is proposed to classify a set of text documents into pre -defined Category Groups (CG) by providing them training and preparing on thesentence, document and integrated corpora levels along with feature reduction,ambiguity removal on each level to achieve high system performance. FuzzyFeature Category Similarity Analyzer (FFCSA) is used to analyze each extractedfeature of Integrated Corpora Feature Vector (ICFV) with the correspondingcategories or classes. This model uses Support Vector Machine Classifier (SVMC)to classify correctly the training data patterns into two groups; i. e., + 1and - 1, thereby producing accurate and correct results. The proposed modelworks efficiently and effectively with great performance and high - accuracyresults.
arxiv-600-142 | Co-clustering for directed graphs: the Stochastic co-Blockmodel and spectral algorithm Di-Sim | http://arxiv.org/pdf/1204.2296v2.pdf | author:Karl Rohe, Tai Qin, Bin Yu category:stat.ML math.ST stat.TH published:2012-04-10 summary:Directed graphs have asymmetric connections, yet the current graph clusteringmethodologies cannot identify the potentially global structure of theseasymmetries. We give a spectral algorithm called di-sim that builds on a dualmeasure of similarity that correspond to how a node (i) sends and (ii) receivesedges. Using di-sim, we analyze the global asymmetries in the networks of Enronemails, political blogs, and the c elegans neural connectome. In each example,a small subset of nodes have persistent asymmetries; these nodes send edgeswith one cluster, but receive edges with another cluster. Previous approacheswould have assigned these asymmetric nodes to only one cluster, failing toidentify their sending/receiving asymmetries. Regularization and "projection"are two steps of di-sim that are essential for spectral clustering algorithmsto work in practice. The theoretical results show that these steps make thealgorithm weakly consistent under the degree corrected Stochasticco-Blockmodel, a model that generalizes the Stochastic Blockmodel to allow forboth (i) degree heterogeneity and (ii) the global asymmetries that we intend todetect. The theoretical results make no assumptions on the smallest degreenodes. Instead, the theorem requires that the average degree grows sufficientlyfast and that the weak consistency only applies to the subset of the nodes withsufficiently large leverage scores. The results results also apply to bipartitegraphs.
arxiv-600-143 | Automatic facial feature extraction and expression recognition based on neural network | http://arxiv.org/pdf/1204.2073v1.pdf | author:S. P. Khandait, R. C. Thool, P. D. Khandait category:cs.CV published:2012-04-10 summary:In this paper, an approach to the problem of automatic facial featureextraction from a still frontal posed image and classification and recognitionof facial expression and hence emotion and mood of a person is presented. Feedforward back propagation neural network is used as a classifier for classifyingthe expressions of supplied face into seven basic categories like surprise,neutral, sad, disgust, fear, happy and angry. For face portion segmentation andlocalization, morphological image processing operations are used. Permanentfacial features like eyebrows, eyes, mouth and nose are extracted using SUSANedge detection operator, facial geometry, edge projection analysis. Experimentsare carried out on JAFFE facial expression database and gives betterperformance in terms of 100% accuracy for training set and 95.26% accuracy fortest set.
arxiv-600-144 | Ubiquitous WLAN/Camera Positioning using Inverse Intensity Chromaticity Space-based Feature Detection and Matching: A Preliminary Result | http://arxiv.org/pdf/1204.2294v1.pdf | author:Wan Mohd Yaakob Wan Bejuri, Mohd Murtadha Mohamad, Maimunah Sapri, Mohd Adly Rosly category:cs.CV published:2012-04-10 summary:This paper present our new intensity chromaticity space-based featuredetection and matching algorithm. This approach utilizes hybridization ofwireless local area network and camera internal sensor which to receive signalstrength from a access point and the same time retrieve interest pointinformation from hallways. This information is combined by model fittingapproach in order to find the absolute of user target position. No conventionalsearching algorithm is required, thus it is expected reducing the computationalcomplexity. Finally we present pre-experimental results to illustrate theperformance of the localization system for an indoor environment set-up.
arxiv-600-145 | Asymptotic Accuracy of Distribution-Based Estimation for Latent Variables | http://arxiv.org/pdf/1204.2069v4.pdf | author:Keisuke Yamazaki category:stat.ML cs.LG published:2012-04-10 summary:Hierarchical statistical models are widely employed in information scienceand data engineering. The models consist of two types of variables: observablevariables that represent the given data and latent variables for theunobservable labels. An asymptotic analysis of the models plays an importantrole in evaluating the learning process; the result of the analysis is appliednot only to theoretical but also to practical situations, such as optimal modelselection and active learning. There are many studies of generalization errors,which measure the prediction accuracy of the observable variables. However, theaccuracy of estimating the latent variables has not yet been elucidated. For aquantitative evaluation of this, the present paper formulatesdistribution-based functions for the errors in the estimation of the latentvariables. The asymptotic behavior is analyzed for both the maximum likelihoodand the Bayes methods.
arxiv-600-146 | SVD-EBP Algorithm for Iris Pattern Recognition | http://arxiv.org/pdf/1204.2062v1.pdf | author:Babasaheb G. Patil, Shaila Subbaraman category:cs.CV published:2012-04-10 summary:This paper proposes a neural network approach based on Error Back Propagation(EBP) for classification of different eye images. To reduce the complexity oflayered neural network the dimensions of input vectors are optimized usingSingular Value Decomposition (SVD). The main of this work is to provide forbest method for feature extraction and classification. The details of thiscombined system named as SVD-EBP system, and results thereof are presented inthis paper. Keywords- Singular value decomposition(SVD), Error back Propagation(EBP).
arxiv-600-147 | Affine Image Registration Transformation Estimation Using a Real Coded Genetic Algorithm with SBX | http://arxiv.org/pdf/1204.2139v1.pdf | author:Mosab Bazargani, António dos Anjos, Fernando G. Lobo, Ali Mollahosseini, Hamid Reza Shahbazkia category:cs.NE published:2012-04-10 summary:This paper describes the application of a real coded genetic algorithm (GA)to align two or more 2-D images by means of image registration. The proposedsearch strategy is a transformation parameters-based approach involving theaffine transform. The real coded GA uses Simulated Binary Crossover (SBX), aparent-centric recombination operator that has shown to deliver a goodperformance in many optimization problems in the continuous domain. Inaddition, we propose a new technique for matching points between a warped andstatic images by using a randomized ordering when visiting the points duringthe matching procedure. This new technique makes the evaluation of theobjective function somewhat noisy, but GAs and other population-based searchalgorithms have been shown to cope well with noisy fitness evaluations. Theresults obtained are competitive to those obtained by state-of-the-artclassical methods in image registration, confirming the usefulness of theproposed noisy objective function and the suitability of SBX as a recombinationoperator for this type of problem.
arxiv-600-148 | The steepest watershed: from graphs to images | http://arxiv.org/pdf/1204.2134v1.pdf | author:Fernand Meyer category:cs.CV 68U10, 05C85 published:2012-04-10 summary:The watershed is a powerful tool for segmenting objects whose contours appearas crest lines on a gradient image. The watershed transform associates to atopographic surface a partition into catchment basins, defined as attractionzones of a drop of water falling on the relief and following a line of steepestdescent. Unfortunately, catchment basins may overlap and do not form apartition. Moreover, current watershed algorithms, being shortsighted, do notcorrectly estimate the steepness of the downwards trajectories and overestimatethe overlapping zones of catchment basins. An arbitrary division of these zonesbetween adjacent catchment basin results in a poor localization of thecontours. We propose an algorithm without myopia, which considers the totallength of a trajectory for estimating its steepness. We first considertopographic surfaces defined on node weighted graphs. The graphs are pruned inorder to eliminate all downwards trajectories which are not the steepest. Aniterative algorithm with simple neighborhood operations performs the pruningand constructs the catchment basins. The algorithm is then adapted to gray toneimages. The graph structure itself is encoded as an image thanks to the fixedneighborhood structure of grids. A pair of adaptative erosions and dilationsprune the graph and extend the catchment basins. As a result one obtains aprecise detection of the catchment basins and a graph of the steepesttrajectories. A last iterative algorithm allows to follow selected downwardstrajectories in order to detect particular structures such as rivers or thalweglines of the topographic surface.
arxiv-600-149 | A technical study and analysis on fuzzy similarity based models for text classification | http://arxiv.org/pdf/1204.2058v1.pdf | author:Shalini Puri, Sona Kaushik category:cs.IR cs.LG published:2012-04-10 summary:In this new and current era of technology, advancements and techniques,efficient and effective text document classification is becoming a challengingand highly required area to capably categorize text documents into mutuallyexclusive categories. Fuzzy similarity provides a way to find the similarity offeatures among various documents. In this paper, a technical review on variousfuzzy similarity based models is given. These models are discussed and comparedto frame out their use and necessity. A tour of different methodologies isprovided which is based upon fuzzy similarity related concerns. It shows thathow text and web documents are categorized efficiently into differentcategories. Various experimental results of these models are also discussed.The technical comparisons among each model's parameters are shown in the formof a 3-D chart. Such study and technical review provide a strong base ofresearch work done on fuzzy similarity based text document categorization.
arxiv-600-150 | Image-based Vehicle Classification System | http://arxiv.org/pdf/1204.2114v1.pdf | author:Jun Yee Ng, Yong Haur Tay category:cs.CV published:2012-04-10 summary:Electronic toll collection (ETC) system has been a common trend used for tollcollection on toll road nowadays. The implementation of electronic tollcollection allows vehicles to travel at low or full speed during the tollpayment, which help to avoid the traffic delay at toll road. One of the majorcomponents of an electronic toll collection is the automatic vehicle detectionand classification (AVDC) system which is important to classify the vehicle sothat the toll is charged according to the vehicle classes. Vision-based vehicleclassification system is one type of vehicle classification system which adoptcamera as the input sensing device for the system. This type of system hasadvantage over the rest for it is cost efficient as low cost camera is used.The implementation of vision-based vehicle classification system requires lowerinitial investment cost and very suitable for the toll collection trendmigration in Malaysia from single ETC system to full-scale multi-lane free flow(MLFF). This project includes the development of an image-based vehicleclassification system as an effort to seek for a robust vision-based vehicleclassification system. The techniques used in the system includescale-invariant feature transform (SIFT) technique, Canny's edge detector,K-means clustering as well as Euclidean distance matching. In this project, aunique way to image description as matching medium is proposed. Thisdistinctiveness of method is analogous to the human DNA concept which is highlyunique. The system is evaluated on open datasets and return promising results.
arxiv-600-151 | Learning Topic Models - Going beyond SVD | http://arxiv.org/pdf/1204.1956v2.pdf | author:Sanjeev Arora, Rong Ge, Ankur Moitra category:cs.LG cs.DS cs.IR published:2012-04-09 summary:Topic Modeling is an approach used for automatic comprehension andclassification of data in a variety of settings, and perhaps the canonicalapplication is in uncovering thematic structure in a corpus of documents. Anumber of foundational works both in machine learning and in theory havesuggested a probabilistic model for documents, whereby documents arise as aconvex combination of (i.e. distribution on) a small number of topic vectors,each topic vector being a distribution on words (i.e. a vector ofword-frequencies). Similar models have since been used in a variety ofapplication areas; the Latent Dirichlet Allocation or LDA model of Blei et al.is especially popular. Theoretical studies of topic modeling focus on learning the model'sparameters assuming the data is actually generated from it. Existing approachesfor the most part rely on Singular Value Decomposition(SVD), and consequentlyhave one of two limitations: these works need to either assume that eachdocument contains only one topic, or else can only recover the span of thetopic vectors instead of the topic vectors themselves. This paper formally justifies Nonnegative Matrix Factorization(NMF) as a maintool in this context, which is an analog of SVD where all vectors arenonnegative. Using this tool we give the first polynomial-time algorithm forlearning topic models without the above two limitations. The algorithm uses afairly mild assumption about the underlying topic matrix called separability,which is usually found to hold in real-life data. A compelling feature of ouralgorithm is that it generalizes to models that incorporate topic-topiccorrelations, such as the Correlated Topic Model and the Pachinko AllocationModel. We hope that this paper will motivate further theoretical results that useNMF as a replacement for SVD - just as NMF has come to replace SVD in manyapplications.
arxiv-600-152 | Estimation of causal orders in a linear non-Gaussian acyclic model: a method robust against latent confounders | http://arxiv.org/pdf/1204.1795v1.pdf | author:Tatsuya Tashiro, Shohei Shimizu, Aapo Hyvarinen, Takashi Washio category:stat.ML published:2012-04-09 summary:We consider to learn a causal ordering of variables in a linear non-Gaussianacyclic model called LiNGAM. Several existing methods have been shown toconsistently estimate a causal ordering assuming that all the model assumptionsare correct. But, the estimation results could be distorted if some assumptionsactually are violated. In this paper, we propose a new algorithm for learningcausal orders that is robust against one typical violation of the modelassumptions: latent confounders. We demonstrate the effectiveness of our methodusing artificial data.
arxiv-600-153 | On Power-law Kernels, corresponding Reproducing Kernel Hilbert Space and Applications | http://arxiv.org/pdf/1204.1800v2.pdf | author:Debarghya Ghoshdastidar, Ambedkar Dukkipati category:cs.LG cs.IT math.IT stat.ML published:2012-04-09 summary:The role of kernels is central to machine learning. Motivated by theimportance of power-law distributions in statistical modeling, in this paper,we propose the notion of power-law kernels to investigate power-laws inlearning problem. We propose two power-law kernels by generalizing Gaussian andLaplacian kernels. This generalization is based on distributions, arising outof maximization of a generalized information measure known as nonextensiveentropy that is very well studied in statistical mechanics. We prove that theproposed kernels are positive definite, and provide some insights regarding thecorresponding Reproducing Kernel Hilbert Space (RKHS). We also study practicalsignificance of both kernels in classification and regression, and present somesimulation results.
arxiv-600-154 | Skin-color based videos categorization | http://arxiv.org/pdf/1204.1811v1.pdf | author:Rehanullah Khan, Asad Maqsood, Zeeshan Khan, Muhammad Ishaq, Arsalan Arif category:cs.CV cs.AI published:2012-04-09 summary:On dedicated websites, people can upload videos and share it with the rest ofthe world. Currently these videos are cat- egorized manually by the help of theuser community. In this paper, we propose a combination of color spaces withthe Bayesian network approach for robust detection of skin color followed by anautomated video categorization. Exper- imental results show that our method canachieve satisfactory performance for categorizing videos based on skin color.
arxiv-600-155 | Knapsack based Optimal Policies for Budget-Limited Multi-Armed Bandits | http://arxiv.org/pdf/1204.1909v1.pdf | author:Long Tran-Thanh, Archie Chapman, Alex Rogers, Nicholas R. Jennings category:cs.AI cs.LG published:2012-04-09 summary:In budget-limited multi-armed bandit (MAB) problems, the learner's actionsare costly and constrained by a fixed budget. Consequently, an optimalexploitation policy may not be to pull the optimal arm repeatedly, as is thecase in other variants of MAB, but rather to pull the sequence of differentarms that maximises the agent's total reward within the budget. This differencefrom existing MABs means that new approaches to maximising the total reward arerequired. Given this, we develop two pulling policies, namely: (i) KUBE; and(ii) fractional KUBE. Whereas the former provides better performance up to 40%in our experimental settings, the latter is computationally less expensive. Wealso prove logarithmic upper bounds for the regret of both policies, and showthat these bounds are asymptotically optimal (i.e. they only differ from thebest possible regret by a constant factor).
arxiv-600-156 | Non-asymptotic Oracle Inequalities for the High-Dimensional Cox Regression via Lasso | http://arxiv.org/pdf/1204.1992v1.pdf | author:Shengchun Kong, Bin Nan category:math.ST stat.ML stat.TH published:2012-04-09 summary:We consider the finite sample properties of the regularized high-dimensionalCox regression via lasso. Existing literature focuses on linear models orgeneralized linear models with Lipschitz loss functions, where the empiricalrisk functions are the summations of independent and identically distributed(iid) losses. The summands in the negative log partial likelihood function forcensored survival data, however, are neither iid nor Lipschitz. We firstapproximate the negative log partial likelihood function by a sum of iidnon-Lipschitz terms, then derive the non-asymptotic oracle inequalities for thelasso penalized Cox regression using pointwise arguments to tackle thedifficulty caused by the lack of iid and Lipschitz property.
arxiv-600-157 | Directed Information Graphs | http://arxiv.org/pdf/1204.2003v2.pdf | author:Christopher J. Quinn, Negar Kiyavash, Todd P. Coleman category:cs.IT cs.AI math.IT stat.ML published:2012-04-09 summary:We propose a graphical model for representing networks of stochasticprocesses, the minimal generative model graph. It is based on reducedfactorizations of the joint distribution over time. We show that underappropriate conditions, it is unique and consistent with another type ofgraphical model, the directed information graph, which is based on ageneralization of Granger causality. We demonstrate how directed informationquantifies Granger causality in a particular sequential prediction setting. Wealso develop efficient methods to estimate the topological structure from datathat obviate estimating the joint statistics. One algorithm assumesupper-bounds on the degrees and uses the minimal dimension statisticsnecessary. In the event that the upper-bounds are not valid, the resultinggraph is nonetheless an optimal approximation. Another algorithm usesnear-minimal dimension statistics when no bounds are known but the distributionsatisfies a certain criterion. Analogous to how structure learning algorithmsfor undirected graphical models use mutual information estimates, thesealgorithms use directed information estimates. We characterize thesample-complexity of two plug-in directed information estimators and obtainconfidence intervals. For the setting when point estimates are unreliable, wepropose an algorithm that uses confidence intervals to identify the bestapproximation that is robust to estimation error. Lastly, we demonstrate theeffectiveness of the proposed algorithms through analysis of both syntheticdata and real data from the Twitter network. In the latter case, we identifywhich news sources influence users in the network by merely analyzing tweettimes.
arxiv-600-158 | Efficient Design of Triplet Based Spike-Timing Dependent Plasticity | http://arxiv.org/pdf/1204.1706v1.pdf | author:Mostafa Rahimi Azghadi, Said Al-Sarawi, Nicolangelo Iannella, Derek Abbott category:cs.NE published:2012-04-08 summary:Spike-Timing Dependent Plasticity (STDP) is believed to play an importantrole in learning and the formation of computational function in the brain. Theclassical model of STDP which considers the timing between pairs ofpre-synaptic and post-synaptic spikes (p-STDP) is incapable of reproducingsynaptic weight changes similar to those seen in biological experiments whichinvestigate the effect of either higher order spike trains (e.g. triplet andquadruplet of spikes), or, simultaneous effect of the rate and timing of spikepairs on synaptic plasticity. In this paper, we firstly investigate synapticweight changes using a p-STDP circuit and show how it fails to reproduce thementioned complex biological experiments. We then present a new STDP VLSIcircuit which acts based on the timing among triplets of spikes (t-STDP) thatis able to reproduce all the mentioned experimental results. We believe thatour new STDP VLSI circuit improves upon previous circuits, whose learningcapacity exceeds current designs due to its capability of mimicking theoutcomes of biological experiments more closely; thus plays a significant rolein future VLSI implementation of neuromorphic systems.
arxiv-600-159 | Multi-Level Coding Efficiency with Improved Quality for Image Compression based on AMBTC | http://arxiv.org/pdf/1204.1704v1.pdf | author:K. Somasundaram, S. Vimala category:cs.CV published:2012-04-08 summary:In this paper, we have proposed an extended version of Absolute Moment BlockTruncation Coding (AMBTC) to compress images. Generally the elements of abitplane used in the variants of Block Truncation Coding (BTC) are of size 1bit. But it has been extended to two bits in the proposed method. Number ofstatistical moments preserved to reconstruct the compressed has also beenraised from 2 to 4. Hence, the quality of the reconstructed images has beenimproved significantly from 33.62 to 38.12 with the increase in bpp by 1. Theincreased bpp (3) is further reduced to 1.75in multiple levels: in one level,by dropping 4 elements of the bitplane in such a away that the pixel values ofthe dropped elements can easily be interpolated with out much of loss in thequality, in level two, eight elements are dropped and reconstructed later andin level three, the size of the statistical moments is reduced. The experimentswere carried over standard images of varying intensities. In all the cases, theproposed method outperforms the existing AMBTC technique in terms of both PSNRand bpp.
arxiv-600-160 | Optimally-Weighted Herding is Bayesian Quadrature | http://arxiv.org/pdf/1204.1664v2.pdf | author:Ferenc Huszár, David Duvenaud category:stat.ML math.NA G.1.4 published:2012-04-07 summary:Herding and kernel herding are deterministic methods of choosing sampleswhich summarise a probability distribution. A related task is choosing samplesfor estimating integrals using Bayesian quadrature. We show that the criterionminimised when selecting samples in kernel herding is equivalent to theposterior variance in Bayesian quadrature. We then show that sequentialBayesian quadrature can be viewed as a weighted version of kernel herding whichachieves performance superior to any other weighted herding method. Wedemonstrate empirically a rate of convergence faster than O(1/N). Our resultsalso imply an upper bound on the empirical error of the Bayesian quadratureestimate.
arxiv-600-161 | The threshold EM algorithm for parameter learning in bayesian network with incomplete data | http://arxiv.org/pdf/1204.1681v1.pdf | author:Fradj Ben Lamine, Karim Kalti, Mohamed Ali Mahjoub category:cs.AI cs.LG stat.ML published:2012-04-07 summary:Bayesian networks (BN) are used in a big range of applications but they haveone issue concerning parameter learning. In real application, training data arealways incomplete or some nodes are hidden. To deal with this problem manylearning parameter algorithms are suggested foreground EM, Gibbs sampling andRBE algorithms. In order to limit the search space and escape from local maximaproduced by executing EM algorithm, this paper presents a learning parameteralgorithm that is a fusion of EM and RBE algorithms. This algorithmincorporates the range of a parameter into the EM algorithm. This range iscalculated by the first step of RBE algorithm allowing a regularization of eachparameter in bayesian network after the maximization step of the EM algorithm.The threshold EM algorithm is applied in brain tumor diagnosis and show someadvantages and disadvantages over the EM algorithm.
arxiv-600-162 | Image segmentation by adaptive distance based on EM algorithm | http://arxiv.org/pdf/1204.1629v1.pdf | author:Mohamed Ali Mahjoub, karim kalti category:cs.CV published:2012-04-07 summary:This paper introduces a Bayesian image segmentation algorithm based on finitemixtures. An EM algorithm is developed to estimate parameters of the Gaussianmixtures. The finite mixture is a flexible and powerful probabilistic modelingtool. It can be used to provide a model-based clustering in the field ofpattern recognition. However, the application of finite mixtures to imagesegmentation presents some difficulties; especially it's sensible to noise. Inthis paper we propose a variant of this method which aims to resolve thisproblem. Our approach proceeds by the characterization of pixels by twofeatures: the first one describes the intrinsic properties of the pixel and thesecond characterizes the neighborhood of pixel. Then the classification is madeon the base on adaptive distance which privileges the one or the other featuresaccording to the spatial position of the pixel in the image. The obtainedresults have shown a significant improvement of our approach compared to thestandard version of EM algorithm.
arxiv-600-163 | Vision-based Human Gender Recognition: A Survey | http://arxiv.org/pdf/1204.1611v1.pdf | author:Choon Boon Ng, Yong Haur Tay, Bok Min Goi category:cs.CV published:2012-04-07 summary:Gender is an important demographic attribute of people. This paper provides asurvey of human gender recognition in computer vision. A review of approachesexploiting information from face and whole body (either from a still image orgait sequence) is presented. We highlight the challenges faced and survey therepresentative methods of these approaches. Based on the results, goodperformance have been achieved for datasets captured under controlledenvironments, but there is still much work that can be done to improve therobustness of gender recognition under real-life environments.
arxiv-600-164 | Automatic liver segmentation method in CT images | http://arxiv.org/pdf/1204.1634v1.pdf | author:Oussema zayane, besma jouini, Mohamed Ali Mahjoub category:cs.CV published:2012-04-07 summary:The aim of this work is to develop a method for automatic segmentation of theliver based on a priori knowledge of the image, such as location and shape ofthe liver.
arxiv-600-165 | A New Approach for Arabic Handwritten Postal Addresses Recognition | http://arxiv.org/pdf/1204.1678v1.pdf | author:Moncef Charfi, Monji Kherallah, Abdelkarim El Baati, Adel M. Alimi category:cs.CV published:2012-04-07 summary:In this paper, we propose an automatic analysis system for the Arabichandwriting postal addresses recognition, by using the beta elliptical model.Our system is divided into different steps: analysis, pre-processing andclassification. The first operation is the filtering of image. In the second,we remove the border print, stamps and graphics. After locating the address onthe envelope, the address segmentation allows the extraction of postal code andcity name separately. The pre-processing system and the modeling approach arebased on two basic steps. The first step is the extraction of the temporalorder in the image of the handwritten trajectory. The second step is based onthe use of Beta-Elliptical model for the representation of handwritten script.The recognition system is based on Graph-matching algorithm. Our modeling andrecognition approaches were validated by using the postal code and city namesextracted from the Tunisian postal envelopes data. The recognition rateobtained is about 98%.
arxiv-600-166 | New approach using Bayesian Network to improve content based image classification systems | http://arxiv.org/pdf/1204.1631v1.pdf | author:Khlifia jayech, mohamed ali mahjoub category:cs.CV cs.IR published:2012-04-07 summary:This paper proposes a new approach based on augmented naive Bayes for imageclassification. Initially, each image is cutting in a whole of blocks. For eachblock, we compute a vector of descriptors. Then, we propose to carry out aclassification of the vectors of descriptors to build a vector of labels foreach image. Finally, we propose three variants of Bayesian Networks such asNaive Bayesian Network (NB), Tree Augmented Naive Bayes (TAN) and ForestAugmented Naive Bayes (FAN) to classify the image using the vector of labels.The results showed a marked improvement over the FAN, NB and TAN.
arxiv-600-167 | UCB Algorithm for Exponential Distributions | http://arxiv.org/pdf/1204.1624v1.pdf | author:Wassim Jouini, Christophe Moy category:stat.ML cs.LG published:2012-04-07 summary:We introduce in this paper a new algorithm for Multi-Armed Bandit (MAB)problems. A machine learning paradigm popular within Cognitive Network relatedtopics (e.g., Spectrum Sensing and Allocation). We focus on the case where therewards are exponentially distributed, which is common when dealing withRayleigh fading channels. This strategy, named Multiplicative Upper ConfidenceBound (MUCB), associates a utility index to every available arm, and thenselects the arm with the highest index. For every arm, the associated index isequal to the product of a multiplicative factor by the sample mean of therewards collected by this arm. We show that the MUCB policy has a lowcomplexity and is order optimal.
arxiv-600-168 | Clustering and Bayesian network for image of faces classification | http://arxiv.org/pdf/1204.1679v1.pdf | author:Khlifia Jayech, Mohamed Ali Mahjoub category:cs.CV cs.AI published:2012-04-07 summary:In a content based image classification system, target images are sorted byfeature similarities with respect to the query (CBIR). In this paper, wepropose to use new approach combining distance tangent, k-means algorithm andBayesian network for image classification. First, we use the technique oftangent distance to calculate several tangent spaces representing the sameimage. The objective is to reduce the error in the classification phase.Second, we cut the image in a whole of blocks. For each block, we compute avector of descriptors. Then, we use K-means to cluster the low-level featuresincluding color and texture information to build a vector of labels for eachimage. Finally, we apply five variants of Bayesian networks classifiers(Na\"ive Bayes, Global Tree Augmented Na\"ive Bayes (GTAN), Global ForestAugmented Na\"ive Bayes (GFAN), Tree Augmented Na\"ive Bayes for each class(TAN), and Forest Augmented Na\"ive Bayes for each class (FAN) to classify theimage of faces using the vector of labels. In order to validate the feasibilityand effectively, we compare the results of GFAN to FAN and to the othersclassifiers (NB, GTAN, TAN). The results demonstrate FAN outperforms than GFAN,NB, GTAN and TAN in the overall classification accuracy.
arxiv-600-169 | Density-sensitive semisupervised inference | http://arxiv.org/pdf/1204.1685v2.pdf | author:Martin Azizyan, Aarti Singh, Larry Wasserman category:math.ST cs.LG stat.ML stat.TH published:2012-04-07 summary:Semisupervised methods are techniques for using labeled data$(X_1,Y_1),\ldots,(X_n,Y_n)$ together with unlabeled data $X_{n+1},\ldots,X_N$to make predictions. These methods invoke some assumptions that link themarginal distribution $P_X$ of X to the regression function f(x). For example,it is common to assume that f is very smooth over high density regions of$P_X$. Many of the methods are ad-hoc and have been shown to work in specificexamples but are lacking a theoretical foundation. We provide a minimaxframework for analyzing semisupervised methods. In particular, we study methodsbased on metrics that are sensitive to the distribution $P_X$. Our modelincludes a parameter $\alpha$ that controls the strength of the semisupervisedassumption. We then use the data to adapt to $\alpha$.
arxiv-600-170 | The asymptotics of ranking algorithms | http://arxiv.org/pdf/1204.1688v3.pdf | author:John C. Duchi, Lester Mackey, Michael I. Jordan category:math.ST cs.LG stat.ML stat.TH published:2012-04-07 summary:We consider the predictive problem of supervised ranking, where the task isto rank sets of candidate items returned in response to queries. Although thereexist statistical procedures that come with guarantees of consistency in thissetting, these procedures require that individuals provide a complete rankingof all items, which is rarely feasible in practice. Instead, individualsroutinely provide partial preference information, such as pairwise comparisonsof items, and more practical approaches to ranking have aimed at modeling thispartial preference data directly. As we show, however, such an approach raisesserious theoretical challenges. Indeed, we demonstrate that many commonly usedsurrogate losses for pairwise comparison data do not yield consistency;surprisingly, we show inconsistency even in low-noise settings. With thesenegative results as motivation, we present a new approach to supervised rankingbased on aggregation of partial preferences, and we develop $U$-statistic-basedempirical risk minimization procedures. We present an asymptotic analysis ofthese new procedures, showing that they yield consistency results that parallelthose available for classification. We complement our theoretical results withan experiment studying the new procedures in a large-scale web-ranking task.
arxiv-600-171 | Discrimination between Arabic and Latin from bilingual documents | http://arxiv.org/pdf/1204.1615v1.pdf | author:Sofiene Haboubi, Samia Maddouri, Hamid Amiri category:cs.CV cs.CL cs.IR published:2012-04-07 summary:2011 International Conference on Communications, Computing and ControlApplications (CCCA)
arxiv-600-172 | Fast projections onto mixed-norm balls with applications | http://arxiv.org/pdf/1204.1437v1.pdf | author:Suvrit Sra category:stat.ML cs.LG math.OC published:2012-04-06 summary:Joint sparsity offers powerful structural cues for feature selection,especially for variables that are expected to demonstrate a "grouped" behavior.Such behavior is commonly modeled via group-lasso, multitask lasso, and relatedmethods where feature selection is effected via mixed-norms. Several mixed-normbased sparse models have received substantial attention, and for some casesefficient algorithms are also available. Surprisingly, several constrainedsparse models seem to be lacking scalable algorithms. We address thisdeficiency by presenting batch and online (stochastic-gradient) optimizationmethods, both of which rely on efficient projections onto mixed-norm balls. Weillustrate our methods by applying them to the multitask lasso. We conclude bymentioning some open problems.
arxiv-600-173 | Learning Fuzzy β-Certain and β-Possible rules from incomplete quantitative data by rough sets | http://arxiv.org/pdf/1204.1467v1.pdf | author:Ali Soltan Mohammadi, L. Asadzadeh, D. D. Rezaee category:cs.DS cs.LG published:2012-04-06 summary:The rough-set theory proposed by Pawlak, has been widely used in dealing withdata classification problems. The original rough-set model is, however, quitesensitive to noisy data. Tzung thus proposed deals with the problem ofproducing a set of fuzzy certain and fuzzy possible rules from quantitativedata with a predefined tolerance degree of uncertainty and misclassification.This model allowed, which combines the variable precision rough-set model andthe fuzzy set theory, is thus proposed to solve this problem. This paper thusdeals with the problem of producing a set of fuzzy certain and fuzzy possiblerules from incomplete quantitative data with a predefined tolerance degree ofuncertainty and misclassification. A new method, incomplete quantitative datafor rough-set model and the fuzzy set theory, is thus proposed to solve thisproblem. It first transforms each quantitative value into a fuzzy set oflinguistic terms using membership functions and then finding incompletequantitative data with lower and the fuzzy upper approximations. It secondcalculates the fuzzy {\beta}-lower and the fuzzy {\beta}-upper approximations.The certain and possible rules are then generated based on these fuzzyapproximations. These rules can then be used to classify unknown objects.
arxiv-600-174 | Minimal model of associative learning for cross-situational lexicon acquisition | http://arxiv.org/pdf/1204.1564v4.pdf | author:Paulo F. C. Tilles, Jose F. Fontanari category:q-bio.NC cs.LG published:2012-04-06 summary:An explanation for the acquisition of word-object mappings is the associativelearning in a cross-situational scenario. Here we present analytical results ofthe performance of a simple associative learning algorithm for acquiring aone-to-one mapping between $N$ objects and $N$ words based solely on theco-occurrence between objects and words. In particular, a learning trial in ourlearning scenario consists of the presentation of $C + 1 < N$ objects togetherwith a target word, which refers to one of the objects in the context. We findthat the learning times are distributed exponentially and the learning ratesare given by $\ln{[\frac{N(N-1)}{C + (N-1)^{2}}]}$ in the case the $N$ targetwords are sampled randomly and by $\frac{1}{N} \ln [\frac{N-1}{C}] $ in thecase they follow a deterministic presentation sequence. This learningperformance is much superior to those exhibited by humans and more realisticlearning algorithms in cross-situational experiments. We show that introductionof discrimination limitations using Weber's law and forgetting reduce theperformance of the associative algorithm to the human level.
arxiv-600-175 | Continuous Markov Random Fields for Robust Stereo Estimation | http://arxiv.org/pdf/1204.1393v1.pdf | author:Koichiro Yamaguchi, Tamir Hazan, David McAllester, Raquel Urtasun category:cs.CV I.2.10; I.4.8 published:2012-04-06 summary:In this paper we present a novel slanted-plane MRF model which reasonsjointly about occlusion boundaries as well as depth. We formulate the problemas the one of inference in a hybrid MRF composed of both continuous (i.e.,slanted 3D planes) and discrete (i.e., occlusion boundaries) random variables.This allows us to define potentials encoding the ownership of the pixels thatcompose the boundary between segments, as well as potentials encoding whichjunctions are physically possible. Our approach outperforms thestate-of-the-art on Middlebury high resolution imagery as well as in the morechallenging KITTI dataset, while being more efficient than existing slantedplane MRF-based methods, taking on average 2 minutes to perform inference onhigh resolution imagery.
arxiv-600-176 | A Machine Learning Approach For Opinion Holder Extraction In Arabic Language | http://arxiv.org/pdf/1206.1011v1.pdf | author:Mohamed Elarnaoty, Samir AbdelRahman, Aly Fahmy category:cs.IR cs.LG published:2012-04-06 summary:Opinion mining aims at extracting useful subjective information from reliableamounts of text. Opinion mining holder recognition is a task that has not beenconsidered yet in Arabic Language. This task essentially requires deepunderstanding of clauses structures. Unfortunately, the lack of a robust,publicly available, Arabic parser further complicates the research. This paperpresents a leading research for the opinion holder extraction in Arabic newsindependent from any lexical parsers. We investigate constructing acomprehensive feature set to compensate the lack of parsing structuraloutcomes. The proposed feature set is tuned from English previous works coupledwith our proposed semantic field and named entities features. Our featureanalysis is based on Conditional Random Fields (CRF) and semi-supervisedpattern recognition techniques. Different research models are evaluated viacross-validation experiments achieving 54.03 F-measure. We publicly release ourown research outcome corpus and lexicon for opinion mining community toencourage further research.
arxiv-600-177 | An Implementation of Intrusion Detection System Using Genetic Algorithm | http://arxiv.org/pdf/1204.1336v1.pdf | author:Mohammad Sazzadul Hoque, Md. Abdul Mukit, Md. Abu Naser Bikas category:cs.CR cs.NE cs.NI published:2012-04-05 summary:Nowadays it is very important to maintain a high level security to ensuresafe and trusted communication of information between various organizations.But secured data communication over internet and any other network is alwaysunder threat of intrusions and misuses. So Intrusion Detection Systems havebecome a needful component in terms of computer and network security. There arevarious approaches being utilized in intrusion detections, but unfortunatelyany of the systems so far is not completely flawless. So, the quest ofbetterment continues. In this progression, here we present an IntrusionDetection System (IDS), by applying genetic algorithm (GA) to efficientlydetect various types of network intrusions. Parameters and evolution processesfor GA are discussed in details and implemented. This approach uses evolutiontheory to information evolution in order to filter the traffic data and thusreduce the complexity. To implement and measure the performance of our systemwe used the KDD99 benchmark dataset and obtained reasonable detection rate.
arxiv-600-178 | Mouse Simulation Using Two Coloured Tapes | http://arxiv.org/pdf/1204.1277v1.pdf | author:Vikram Kumar, Kamran Niyazi, Swapnil Mahe, Swapnil Vyawahare category:cs.AI cs.CV published:2012-04-05 summary:In this paper, we present a novel approach for Human Computer Interaction(HCI) where, we control cursor movement using a real-time camera. Currentmethods involve changing mouse parts such as adding more buttons or changingthe position of the tracking ball. Instead, our method is to use a camera andcomputer vision technology, such as image segmentation and gesture recognition,to control mouse tasks (left and right clicking, double-clicking, andscrolling) and we show how it can perform everything as current mouse devicescan. The software will be developed in JAVA language. Recognition and poseestimation in this system are user independent and robust as we will be usingcolour tapes on our finger to perform actions. The software can be used as anintuitive input interface to applications that require multi-dimensionalcontrol e.g. computer games etc.
arxiv-600-179 | A Complete Workflow for Development of Bangla OCR | http://arxiv.org/pdf/1204.1198v1.pdf | author:Farjana Yeasmin Omee, Shiam Shabbir Himel, Md. Abu Naser Bikas category:cs.CV published:2012-04-05 summary:Developing a Bangla OCR requires bunch of algorithm and methods. There weremany effort went on for developing a Bangla OCR. But all of them failed toprovide an error free Bangla OCR. Each of them has some lacking. We discussedabout the problem scope of currently existing Bangla OCR's. In this paper, wepresent the basic steps required for developing a Bangla OCR and a completeworkflow for development of a Bangla OCR with mentioning all the possiblealgorithms required.
arxiv-600-180 | Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback | http://arxiv.org/pdf/1204.1259v2.pdf | author:Balázs Hidasi, Domonkos Tikk category:cs.LG cs.IR cs.NA published:2012-04-05 summary:Albeit, the implicit feedback based recommendation problem - when only theuser history is available but there are no ratings - is the most typicalsetting in real-world applications, it is much less researched than theexplicit feedback case. State-of-the-art algorithms that are efficient on theexplicit case cannot be straightforwardly transformed to the implicit case ifscalability should be maintained. There are few if any implicit feedbackbenchmark datasets, therefore new ideas are usually experimented on explicitbenchmarks. In this paper, we propose a generic context-aware implicit feedbackrecommender algorithm, coined iTALS. iTALS apply a fast, ALS-based tensorfactorization learning method that scales linearly with the number of non-zeroelements in the tensor. The method also allows us to incorporate diversecontext information into the model while maintaining its computationalefficiency. In particular, we present two such context-aware implementationvariants of iTALS. The first incorporates seasonality and enables todistinguish user behavior in different time intervals. The other views the userhistory as sequential information and has the ability to recognize usagepattern typical to certain group of items, e.g. to automatically tell apartproduct types or categories that are typically purchased repetitively(collectibles, grocery goods) or once (household appliances). Experimentsperformed on three implicit datasets (two proprietary ones and an implicitvariant of the Netflix dataset) show that by integrating context-awareinformation with our factorization framework into the state-of-the-art implicitrecommender algorithm the recommendation quality improves significantly.
arxiv-600-181 | Distribution-Dependent Sample Complexity of Large Margin Learning | http://arxiv.org/pdf/1204.1276v4.pdf | author:Sivan Sabato, Nathan Srebro, Naftali Tishby category:stat.ML cs.LG published:2012-04-05 summary:We obtain a tight distribution-specific characterization of the samplecomplexity of large-margin classification with L2 regularization: We introducethe margin-adapted dimension, which is a simple function of the second orderstatistics of the data distribution, and show distribution-specific upper andlower bounds on the sample complexity, both governed by the margin-adapteddimension of the data distribution. The upper bounds are universal, and thelower bounds hold for the rich family of sub-Gaussian distributions withindependent features. We conclude that this new quantity tightly characterizesthe true sample complexity of large-margin classification. To prove the lowerbound, we develop several new tools of independent interest. These include newconnections between shattering and hardness of learning, new properties ofshattering with linear classifiers, and a new lower bound on the smallesteigenvalue of a random Gram matrix generated by sub-Gaussian variables. Ourresults can be used to quantitatively compare large margin learning to otherlearning rules, and to improve the effectiveness of methods that use samplecomplexity bounds, such as active learning.
arxiv-600-182 | Principal Component Analysis-Linear Discriminant Analysis Feature Extractor for Pattern Recognition | http://arxiv.org/pdf/1204.1177v1.pdf | author:Aamir Khan, Hasan Farooq category:cs.CV published:2012-04-05 summary:Robustness of embedded biometric systems is of prime importance with theemergence of fourth generation communication devices and advancement insecurity systems This paper presents the realization of such technologies whichdemands reliable and error-free biometric identity verification systems. Highdimensional patterns are not permitted due to eigen-decomposition in highdimensional image space and degeneration of scattering matrices in small sizesample. Generalization, dimensionality reduction and maximizing the margins arecontrolled by minimizing weight vectors. Results show good pattern bymultimodal biometric system proposed in this paper. This paper is aimed atinvestigating a biometric identity system using Principal Component Analysisand Lindear Discriminant Analysis with K-Nearest Neighbor and implementing suchsystem in real-time using SignalWAVE.
arxiv-600-183 | Distributed Robust Power System State Estimation | http://arxiv.org/pdf/1204.0991v2.pdf | author:Vassilis Kekatos, Georgios B. Giannakis category:stat.ML math.OC published:2012-04-04 summary:Deregulation of energy markets, penetration of renewables, advanced meteringcapabilities, and the urge for situational awareness, all call for system-widepower system state estimation (PSSE). Implementing a centralized estimatorthough is practically infeasible due to the complexity scale of aninterconnection, the communication bottleneck in real-time monitoring, regionaldisclosure policies, and reliability issues. In this context, distributed PSSEmethods are treated here under a unified and systematic framework. A novelalgorithm is developed based on the alternating direction method ofmultipliers. It leverages existing PSSE solvers, respects privacy policies,exhibits low communication load, and its convergence to the centralizedestimates is guaranteed even in the absence of local observability. Beyond theconventional least-squares based PSSE, the decentralized framework accommodatesa robust state estimator. By exploiting interesting links to the compressivesampling advances, the latter jointly estimates the state and identifiescorrupted measurements. The novel algorithms are numerically evaluated usingthe IEEE 14-, 118-bus, and a 4,200-bus benchmarks. Simulations demonstrate thatthe attainable accuracy can be reached within a few inter-area exchanges, whilelargest residual tests are outperformed.
arxiv-600-184 | Kernel Methods for the Approximation of Some Key Quantities of Nonlinear Systems | http://arxiv.org/pdf/1204.0563v2.pdf | author:Jake Bouvrie, Boumediene Hamzi category:math.OC math.DS stat.ML published:2012-04-03 summary:We introduce a data-based approach to estimating key quantities which arisein the study of nonlinear control systems and random nonlinear dynamicalsystems. Our approach hinges on the observation that much of the existinglinear theory may be readily extended to nonlinear systems - with a reasonableexpectation of success - once the nonlinear system has been mapped into a highor infinite dimensional feature space. In particular, we develop computable,non-parametric estimators approximating controllability and observabilityenergy functions for nonlinear systems, and study the ellipsoids they induce.In all cases the relevant quantities are estimated from simulated or observeddata. It is then shown that the controllability energy estimator provides a keymeans for approximating the invariant measure of an ergodic, stochasticallyforced nonlinear system.
arxiv-600-185 | Convergence Properties of Kronecker Graphical Lasso Algorithms | http://arxiv.org/pdf/1204.0585v4.pdf | author:Theodoros Tsiligkaridis, Alfred O. Hero III, Shuheng Zhou category:stat.ME stat.ML published:2012-04-03 summary:This paper studies iteration convergence of Kronecker graphical lasso(KGLasso) algorithms for estimating the covariance of an i.i.d. Gaussian randomsample under a sparse Kronecker-product covariance model and MSE convergencerates. The KGlasso model, originally called the transposable regularizedcovariance model by Allen ["Transposable regularized covariance models with anapplication to missing data imputation," Ann. Appl. Statist., vol. 4, no. 2,pp. 764-790, 2010], implements a pair of $\ell_1$ penalties on each Kroneckerfactor to enforce sparsity in the covariance estimator. The KGlasso algorithmgeneralizes Glasso, introduced by Yuan and Lin ["Model selection and estimationin the Gaussian graphical model," Biometrika, vol. 94, pp. 19-35, 2007] andBanerjee ["Model selection through sparse maximum likelihood estimation formultivariate Gaussian or binary data," J. Mach. Learn. Res., vol. 9, pp.485-516, Mar. 2008], to estimate covariances having Kronecker product form. Italso generalizes the unpenalized ML flip-flop (FF) algorithm of Dutilleul ["TheMLE algorithm for the matrix normal distribution," J. Statist. Comput. Simul.,vol. 64, pp. 105-123, 1999] and Werner ["On estimation of covariance matriceswith Kronecker product structure," IEEE Trans. Signal Process., vol. 56, no. 2,pp. 478-491, Feb. 2008] to estimation of sparse Kronecker factors. We establishthat the KGlasso iterates converge pointwise to a local maximum of thepenalized likelihood function. We derive high dimensional rates of convergenceto the true covariance as both the number of samples and the number ofvariables go to infinity. Our results establish that KGlasso has significantlyfaster asymptotic convergence than Glasso and FF. Simulations are presentedthat validate the results of our analysis.
arxiv-600-186 | The Kernelized Stochastic Batch Perceptron | http://arxiv.org/pdf/1204.0566v2.pdf | author:Andrew Cotter, Shai Shalev-Shwartz, Nathan Srebro category:cs.LG published:2012-04-03 summary:We present a novel approach for training kernel Support Vector Machines,establish learning runtime guarantees for our method that are better then thoseof any other known kernelized SVM optimization approach, and show that ourmethod works well in practice compared to existing alternatives.
arxiv-600-187 | Efficient Fruit Defect Detection and Glare removal Algorithm by anisotropic diffusion and 2D Gabor filter | http://arxiv.org/pdf/1204.0767v2.pdf | author:Vini Katyal, Deepesh Srivastava category:cs.CV published:2012-04-03 summary:This paper focuses on fruit defect detection and glare removal usingmorphological operations, Glare removal can be considered as an importantpreprocessing step as uneven lighting may introduce it in images, which hamperthe results produced through segmentation by Gabor filters .The problem ofglare in images is very pronounced sometimes due to the unusual reflectancefrom the camera sensor or stray light entering, this method counteracts thisproblem and makes the defect detection much more pronounced. Anisotropicdiffusion is used for further smoothening of the images and removing the highenergy regions in an image for better defect detection and makes the defectsmore retrievable. Our algorithm is robust and scalable the employability of aparticular mask for glare removal has been checked and proved useful forcounteracting.this problem, anisotropic diffusion further enhances the defectswith its use further Optimal Gabor filter at various orientations is used fordefect detection.
arxiv-600-188 | A New Fuzzy Stacked Generalization Technique and Analysis of its Performance | http://arxiv.org/pdf/1204.0171v5.pdf | author:Mete Ozay, Fatos T. Yarman Vural category:cs.LG cs.CV published:2012-04-01 summary:In this study, a new Stacked Generalization technique called Fuzzy StackedGeneralization (FSG) is proposed to minimize the difference between N -sampleand large-sample classification error of the Nearest Neighbor classifier. Theproposed FSG employs a new hierarchical distance learning strategy to minimizethe error difference. For this purpose, we first construct an ensemble ofbase-layer fuzzy k- Nearest Neighbor (k-NN) classifiers, each of which receivesa different feature set extracted from the same sample set. The fuzzymembership values computed at the decision space of each fuzzy k-NN classifierare concatenated to form the feature vectors of a fusion space. Finally, thefeature vectors are fed to a meta-layer classifier to learn the degree ofaccuracy of the decisions of the base-layer classifiers for meta-layerclassification. Rather than the power of the individual base layer-classifiers,diversity and cooperation of the classifiers become an important issue toimprove the overall performance of the proposed FSG. A weak base-layerclassifier may boost the overall performance more than a strong classifier, ifit is capable of recognizing the samples, which are not recognized by the restof the classifiers, in its own feature space. The experiments explore the typeof the collaboration among the individual classifiers required for an improvedperformance of the suggested architecture. Experiments on multiple featurereal-world datasets show that the proposed FSG performs better than the stateof the art ensemble learning algorithms such as Adaboost, Random Subspace andRotation Forest. On the other hand, compatible performances are observed in theexperiments on single feature multi-attribute datasets.
arxiv-600-189 | Managing contextual artificial neural networks with a service-based mediator | http://arxiv.org/pdf/1204.0262v2.pdf | author:Greg Fish category:cs.NE 97R40 B.6.0 published:2012-04-01 summary:Today, a wide variety of probabilistic and expert AI systems used to analyzereal world inputs such as unstructured text, sounds, images, and statisticaldata. However, all these systems exist on different platforms, with differentimplementations, and with very different, often very specific goals in mind.This paper introduces a concept for a mediator framework for such systems andseeks to show several architectures which would support it, potential benefitsin combining the signals of disparate networks for formalized, high level logicand signal processing, and its possible academic and industrial uses.
arxiv-600-190 | A New Approach to Speeding Up Topic Modeling | http://arxiv.org/pdf/1204.0170v2.pdf | author:Jia Zeng, Zhi-Qiang Liu, Xiao-Qin Cao category:cs.LG cs.IR published:2012-04-01 summary:Latent Dirichlet allocation (LDA) is a widely-used probabilistic topicmodeling paradigm, and recently finds many applications in computer vision andcomputational biology. In this paper, we propose a fast and accurate batchalgorithm, active belief propagation (ABP), for training LDA. Usually batch LDAalgorithms require repeated scanning of the entire corpus and searching thecomplete topic space. To process massive corpora having a large number oftopics, the training iteration of batch LDA algorithms is often inefficient andtime-consuming. To accelerate the training speed, ABP actively scans the subsetof corpus and searches the subset of topic space for topic modeling, thereforesaves enormous training time in each iteration. To ensure accuracy, ABP selectsonly those documents and topics that contribute to the largest residuals withinthe residual belief propagation (RBP) framework. On four real-world corpora,ABP performs around $10$ to $100$ times faster than state-of-the-art batch LDAalgorithms with a comparable topic modeling accuracy.
arxiv-600-191 | A Lipschitz Exploration-Exploitation Scheme for Bayesian Optimization | http://arxiv.org/pdf/1204.0047v2.pdf | author:Ali Jalali, Javad Azimi, Xiaoli Fern, Ruofei Zhang category:cs.LG stat.ML published:2012-03-30 summary:The problem of optimizing unknown costly-to-evaluate functions has beenstudied for a long time in the context of Bayesian Optimization. Algorithms inthis field aim to find the optimizer of the function by asking only a fewfunction evaluations at locations carefully selected based on a posteriormodel. In this paper, we assume the unknown function is Lipschitz continuous.Leveraging the Lipschitz property, we propose an algorithm with a distinctexploration phase followed by an exploitation phase. The exploration phase aimsto select samples that shrink the search space as much as possible. Theexploitation phase then focuses on the reduced search space and selects samplesclosest to the optimizer. Considering the Expected Improvement (EI) as abaseline, we empirically show that the proposed algorithm significantlyoutperforms EI.
arxiv-600-192 | Analysis of Magnification in Depth from Defocus | http://arxiv.org/pdf/1203.6329v2.pdf | author:Arnav Bhavsar category:cs.CV published:2012-03-28 summary:In depth from defocus (DFD), when images are captured with different cameraparameters, a relative magnification is induced between them. Image warping isa simpler solution to account for magnification than seemingly more accurateoptical approaches. This work is an investigation into the effects ofmagnification on the accuracy of DFD. We comment on issues regarding scalingeffect on relative blur computation. We statistically analyze accountability ofscale factor, commenting on the bias and efficiency of the estimator that doesnot consider scale. We also discuss the effect of interpolation errors on blurestimation in a warping based solution to handle magnification and carry outexperimental analysis to comment on the blur estimation accuracy.
arxiv-600-193 | A Multi-objective Exploratory Procedure for Regression Model Selection | http://arxiv.org/pdf/1203.6276v3.pdf | author:Ankur Sinha, Pekka Malo, Timo Kuosmanen category:stat.CO cs.NE stat.AP G.3; G.1.6 published:2012-03-28 summary:Variable selection is recognized as one of the most critical steps instatistical modeling. The problems encountered in engineering and socialsciences are commonly characterized by over-abundance of explanatory variables,non-linearities and unknown interdependencies between the regressors. An addeddifficulty is that the analysts may have little or no prior knowledge on therelative importance of the variables. To provide a robust method for modelselection, this paper introduces the Multi-objective Genetic Algorithm forVariable Selection (MOGA-VS) that provides the user with an optimal set ofregression models for a given data-set. The algorithm considers the regressionproblem as a two objective task, and explores the Pareto-optimal (best subset)models by preferring those models over the other which have less number ofregression coefficients and better goodness of fit. The model exploration canbe performed based on in-sample or generalization error minimization. The modelselection is proposed to be performed in two steps. First, we generate thefrontier of Pareto-optimal regression models by eliminating the dominatedmodels without any user intervention. Second, a decision making process isexecuted which allows the user to choose the most preferred model usingvisualizations and simple metrics. The method has been evaluated on a recentlypublished real dataset on Communities and Crime within United States.
arxiv-600-194 | On the Easiest and Hardest Fitness Functions | http://arxiv.org/pdf/1203.6286v5.pdf | author:Jun He, Tianshi Chen, Xin Yao category:cs.NE published:2012-03-28 summary:The hardness of fitness functions is an important research topic in the fieldof evolutionary computation. In theory, the study can help understanding theability of evolutionary algorithms. In practice, the study may provide aguideline to the design of benchmarks. The aim of this paper is to answer thefollowing research questions: Given a fitness function class, which functionsare the easiest with respect to an evolutionary algorithm? Which are thehardest? How are these functions constructed? The paper provides theoreticalanswers to these questions. The easiest and hardest fitness functions areconstructed for an elitist (1+1) evolutionary algorithm to maximise a class offitness functions with the same optima. It is demonstrated that the unimodalfunctions are the easiest and deceptive functions are the hardest in terms ofthe time-fitness landscape. The paper also reveals that the easiest fitnessfunction to one algorithm may become the hardest to another algorithm, and viceversa.
arxiv-600-195 | Statistical Mechanics of Dictionary Learning | http://arxiv.org/pdf/1203.6178v3.pdf | author:Ayaka Sakata, Yoshiyuki Kabashima category:cs.IT cs.LG math.IT published:2012-03-28 summary:Finding a basis matrix (dictionary) by which objective signals arerepresented sparsely is of major relevance in various scientific andtechnological fields. We consider a problem to learn a dictionary from a set oftraining signals. We employ techniques of statistical mechanics of disorderedsystems to evaluate the size of the training set necessary to typically succeedin the dictionary learning. The results indicate that the necessary size ismuch smaller than previously estimated, which theoretically supports and/orencourages the use of dictionary learning in practical situations.
arxiv-600-196 | Empirical Normalization for Quadratic Discriminant Analysis and Classifying Cancer Subtypes | http://arxiv.org/pdf/1203.6345v2.pdf | author:Mark A. Kon, Nikolay Nikolaev category:stat.ML published:2012-03-28 summary:We introduce a new discriminant analysis method (Empirical DiscriminantAnalysis or EDA) for binary classification in machine learning. Given a datasetof feature vectors, this method defines an empirical feature map transformingthe training and test data into new data with components having Gaussianempirical distributions. This map is an empirical version of the Gaussiancopula used in probability and mathematical finance. The purpose is to form afeature mapped dataset as close as possible to Gaussian, after which standardquadratic discriminants can be used for classification. We discuss this methodin general, and apply it to some datasets in computational biology.
arxiv-600-197 | You had me at hello: How phrasing affects memorability | http://arxiv.org/pdf/1203.6360v2.pdf | author:Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon Kleinberg, Lillian Lee category:cs.CL cs.SI physics.soc-ph I.2.7; J.4 published:2012-03-28 summary:Understanding the ways in which information achieves widespread publicawareness is a research question of significant interest. We consider whether,and how, the way in which the information is phrased --- the choice of wordsand sentence structure --- can affect this process. To this end, we develop ananalysis framework and build a corpus of movie quotes, annotated withmemorability information, in which we are able to control for both the speakerand the setting of the quotes. We find that there are significant differencesbetween memorable and non-memorable quotes in several key dimensions, evenafter controlling for situational and contextual factors. One is lexicaldistinctiveness: in aggregate, memorable quotes use less common word choices,but at the same time are built upon a scaffolding of common syntactic patterns.Another is that memorable quotes tend to be more general in ways that make themeasy to apply in new contexts --- that is, more portable. We also show how theconcept of "memorable language" can be extended across domains.
arxiv-600-198 | Greedy Sparsity-Constrained Optimization | http://arxiv.org/pdf/1203.5483v3.pdf | author:Sohail Bahmani, Bhiksha Raj, Petros Boufounos category:stat.ML math.NA math.OC stat.CO 62FXX, 65KXX published:2012-03-25 summary:Sparsity-constrained optimization has wide applicability in machine learning,statistics, and signal processing problems such as feature selection andcompressive Sensing. A vast body of work has studied the sparsity-constrainedoptimization from theoretical, algorithmic, and application aspects in thecontext of sparse estimation in linear models where the fidelity of theestimate is measured by the squared error. In contrast, relatively less efforthas been made in the study of sparsity-constrained optimization in cases wherenonlinear models are involved or the cost function is not quadratic. In thispaper we propose a greedy algorithm, Gradient Support Pursuit (GraSP), toapproximate sparse minima of cost functions of arbitrary form. Should a costfunction have a Stable Restricted Hessian (SRH) or a Stable RestrictedLinearization (SRL), both of which are introduced in this paper, our algorithmis guaranteed to produce a sparse vector within a bounded distance from thetrue sparse optimum. Our approach generalizes known results for quadratic costfunctions that arise in sparse linear regression and Compressive Sensing. Wealso evaluate the performance of GraSP through numerical simulations onsynthetic data, where the algorithm is employed for sparse logistic regressionwith and without $\ell_2$-regularization.
arxiv-600-199 | Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA | http://arxiv.org/pdf/1203.5443v2.pdf | author:Martin Pelikan, Mark W. Hauschild, Pier Luca Lanzi category:cs.NE cs.AI cs.LG published:2012-03-24 summary:An automated technique has recently been proposed to transfer learning in thehierarchical Bayesian optimization algorithm (hBOA) based on distance-basedstatistics. The technique enables practitioners to improve hBOA efficiency bycollecting statistics from probabilistic models obtained in previous hBOA runsand using the obtained statistics to bias future hBOA runs on similar problems.The purpose of this paper is threefold: (1) test the technique on severalclasses of NP-complete problems, including MAXSAT, spin glasses and minimumvertex cover; (2) demonstrate that the technique is effective even whenprevious runs were done on problems of different size; (3) provide empiricalevidence that combining transfer learning with other efficiency enhancementtechniques can often yield nearly multiplicative speedups.
arxiv-600-200 | Acceleration of the shiftable O(1) algorithm for bilateral filtering and non-local means | http://arxiv.org/pdf/1203.5128v2.pdf | author:Kunal N. Chaudhury category:cs.CV cs.DC published:2012-03-22 summary:A direct implementation of the bilateral filter [1] requires O(\sigma_s^2)operations per pixel, where \sigma_s is the (effective) width of the spatialkernel. A fast implementation of the bilateral filter was recently proposed in[2] that required O(1) operations per pixel with respect to \sigma_s. This wasdone by using trigonometric functions for the range kernel of the bilateralfilter, and by exploiting their so-called shiftability property. In particular,a fast implementation of the Gaussian bilateral filter was realized byapproximating the Gaussian range kernel using raised cosines. Later, it wasdemonstrated in [3] that this idea could be extended to a larger class offilters, including the popular non-local means filter [4]. As already observedin [2], a flip side of this approach was that the run time depended on thewidth \sigma_r of the range kernel. For an image with (local) intensityvariations in the range [0,T], the run time scaled as O(T^2/\sigma^2_r) with\sigma_r. This made it difficult to implement narrow range kernels,particularly for images with large dynamic range. We discuss this problem inthis note, and propose some simple steps to accelerate the implementation ingeneral, and for small \sigma_r in particular. [1] C. Tomasi and R. Manduchi, "Bilateral filtering for gray and colorimages", Proc. IEEE International Conference on Computer Vision, 1998. [2] K.N. Chaudhury, Daniel Sage, and M. Unser, "Fast O(1) bilateral filteringusing trigonometric range kernels", IEEE Transactions on Image Processing,2011. [3] K.N. Chaudhury, "Constant-time filtering using shiftable kernels", IEEESignal Processing Letters, 2011. [4] A. Buades, B. Coll, and J.M. Morel, "A review of image denoisingalgorithms, with a new one", Multiscale Modeling and Simulation, 2005.
arxiv-600-201 | On the Equivalence between Herding and Conditional Gradient Algorithms | http://arxiv.org/pdf/1203.4523v2.pdf | author:Francis Bach, Simon Lacoste-Julien, Guillaume Obozinski category:cs.LG math.OC stat.ML published:2012-03-20 summary:We show that the herding procedure of Welling (2009) takes exactly the formof a standard convex optimization algorithm--namely a conditional gradientalgorithm minimizing a quadratic moment discrepancy. This link enables us toinvoke convergence results from convex optimization and to consider fasteralternatives for the task of approximating integrals in a reproducing kernelHilbert space. We study the behavior of the different variants throughnumerical simulations. The experiments indicate that while we can improve overherding on the task of approximating integrals, the original herding algorithmtends to approach more often the maximum entropy distribution, shedding morelight on the learning bias behind herding.
arxiv-600-202 | Selection of tuning parameters in bridge regression models via Bayesian information criterion | http://arxiv.org/pdf/1203.4326v3.pdf | author:Shuichi Kawano category:stat.ME stat.ML published:2012-03-20 summary:We consider the bridge linear regression modeling, which can produce a sparseor non-sparse model. A crucial point in the model building process is theselection of adjusted parameters including a regularization parameter and atuning parameter in bridge regression models. The choice of the adjustedparameters can be viewed as a model selection and evaluation problem. Wepropose a model selection criterion for evaluating bridge regression models interms of Bayesian approach. This selection criterion enables us to select theadjusted parameters objectively. We investigate the effectiveness of ourproposed modeling strategy through some numerical examples.
arxiv-600-203 | Learning loopy graphical models with latent variables: Efficient methods and guarantees | http://arxiv.org/pdf/1203.3887v4.pdf | author:Animashree Anandkumar, Ragupathyraj Valluvan category:stat.ML cs.AI cs.LG math.ST stat.TH published:2012-03-17 summary:The problem of structure estimation in graphical models with latent variablesis considered. We characterize conditions for tractable graph estimation anddevelop efficient methods with provable guarantees. We consider models wherethe underlying Markov graph is locally tree-like, and the model is in theregime of correlation decay. For the special case of the Ising model, thenumber of samples $n$ required for structural consistency of our method scalesas $n=\Omega(\theta_{\min}^{-\delta\eta(\eta+1)-2}\log p)$, where p is thenumber of variables, $\theta_{\min}$ is the minimum edge potential, $\delta$ isthe depth (i.e., distance from a hidden node to the nearest observed nodes),and $\eta$ is a parameter which depends on the bounds on node and edgepotentials in the Ising model. Necessary conditions for structural consistencyunder any algorithm are derived and our method nearly matches the lower boundon sample requirements. Further, the proposed method is practical to implementand provides flexibility to control the number of latent variables and thecycle lengths in the output graph.
arxiv-600-204 | Adaptive experimental design for one-qubit state estimation with finite data based on a statistical update criterion | http://arxiv.org/pdf/1203.3391v2.pdf | author:Takanori Sugiyama, Peter S. Turner, Mio Murao category:quant-ph math.ST stat.ML stat.TH published:2012-03-15 summary:We consider 1-qubit mixed quantum state estimation by adaptively updatingmeasurements according to previously obtained outcomes and measurementsettings. Updates are determined by the average-variance-optimality(A-optimality) criterion, known in the classical theory of experimental designand applied here to quantum state estimation. In general, A-optimization is anonlinear minimization problem; however, we find an analytic solution for1-qubit state estimation using projective measurements, reducing computationaleffort. We compare numerically two adaptive and two nonadaptive schemes forfinite data sets and show that the A-optimality criterion gives more preciseestimates than standard quantum tomography.
arxiv-600-205 | Evolving Culture vs Local Minima | http://arxiv.org/pdf/1203.2990v2.pdf | author:Yoshua Bengio category:cs.LG cs.AI I.2.6 published:2012-03-14 summary:We propose a theory that relates difficulty of learning in deep architecturesto culture and language. It is articulated around the following hypotheses: (1)learning in an individual human brain is hampered by the presence of effectivelocal minima; (2) this optimization difficulty is particularly important whenit comes to learning higher-level abstractions, i.e., concepts that cover avast and highly-nonlinear span of sensory configurations; (3) such high-levelabstractions are best represented in brains by the composition of many levelsof representation, i.e., by deep architectures; (4) a human brain can learnsuch high-level abstractions if guided by the signals produced by other humans,which act as hints or indirect supervision for these high-level abstractions;and (5), language and the recombination and optimization of mental conceptsprovide an efficient evolutionary recombination operator, and this gives riseto rapid search in the space of communicable ideas that help humans build upbetter high-level internal representations of their world. These hypotheses puttogether imply that human culture and the evolution of ideas have been crucialto counter an optimization difficulty: this optimization difficulty wouldotherwise make it very difficult for human brains to capture high-levelknowledge of the world. The theory is grounded in experimental observations ofthe difficulties of training deep artificial neural networks. Plausibleconsequences of this theory for the efficiency of cultural evolutions aresketched.
arxiv-600-206 | Marginal multi-Bernoulli filters: RFS derivation of MHT, JIPDA and association-based MeMBer | http://arxiv.org/pdf/1203.2995v5.pdf | author:Jason L. Williams category:cs.SY cs.CV published:2012-03-14 summary:Recent developments in random finite sets (RFSs) have yielded a variety oftracking methods that avoid data association. This paper derives a form of thefull Bayes RFS filter and observes that data association is implicitly present,in a data structure similar to MHT. Subsequently, algorithms are obtained byapproximating the distribution of associations. Two algorithms result: onenearly identical to JIPDA, and another related to the MeMBer filter. Bothimprove performance in challenging environments.
arxiv-600-207 | Deviation optimal learning using greedy Q-aggregation | http://arxiv.org/pdf/1203.2507v2.pdf | author:Dong Dai, Philippe Rigollet, Tong Zhang category:math.ST cs.LG stat.ML stat.TH published:2012-03-12 summary:Given a finite family of functions, the goal of model selection aggregationis to construct a procedure that mimics the function from this family that isthe closest to an unknown regression function. More precisely, we consider ageneral regression model with fixed design and measure the distance betweenfunctions by the mean squared error at the design points. While proceduresbased on exponential weights are known to solve the problem of model selectionaggregation in expectation, they are, surprisingly, sub-optimal in deviation.We propose a new formulation called Q-aggregation that addresses thislimitation; namely, its solution leads to sharp oracle inequalities that areoptimal in a minimax sense. Moreover, based on the new formulation, we designgreedy Q-aggregation procedures that produce sparse aggregation modelsachieving the optimal rate. The convergence and performance of these greedyprocedures are illustrated and compared with other standard methods onsimulated examples.
arxiv-600-208 | On the Necessity of Irrelevant Variables | http://arxiv.org/pdf/1203.2557v3.pdf | author:David P. Helmbold, Philip M. Long category:cs.LG published:2012-03-12 summary:This work explores the effects of relevant and irrelevant boolean variableson the accuracy of classifiers. The analysis uses the assumption that thevariables are conditionally independent given the class, and focuses on anatural family of learning algorithms for such sources when the relevantvariables have a small advantage over random guessing. The main result is thatalgorithms relying predominately on irrelevant variables have errorprobabilities that quickly go to 0 in situations where algorithms that limitthe use of irrelevant variables have errors bounded below by a positiveconstant. We also show that accurate learning is possible even when there areso few examples that one cannot determine with high confidence whether or notany individual variable is relevant.
arxiv-600-209 | Fixed-Rank Representation for Unsupervised Visual Learning | http://arxiv.org/pdf/1203.2210v2.pdf | author:Risheng Liu, Zhouchen Lin, Fernando De la Torre, Zhixun Su category:cs.CV cs.NA published:2012-03-09 summary:Subspace clustering and feature extraction are two of the most commonly usedunsupervised learning techniques in computer vision and pattern recognition.State-of-the-art techniques for subspace clustering make use of recent advancesin sparsity and rank minimization. However, existing techniques arecomputationally expensive and may result in degenerate solutions that degradeclustering performance in the case of insufficient data sampling. To partiallysolve these problems, and inspired by existing work on matrix factorization,this paper proposes fixed-rank representation (FRR) as a unified framework forunsupervised visual learning. FRR is able to reveal the structure of multiplesubspaces in closed-form when the data is noiseless. Furthermore, we prove thatunder some suitable conditions, even with insufficient observations, FRR canstill reveal the true subspace memberships. To achieve robustness to outliersand noise, a sparse regularizer is introduced into the FRR framework. Beyondsubspace clustering, FRR can be used for unsupervised feature extraction. As anon-trivial byproduct, a fast numerical solver is developed for FRR.Experimental results on both synthetic data and real applications validate ourtheoretical analysis and demonstrate the benefits of FRR for unsupervisedvisual learning.
arxiv-600-210 | Fault detection system for Arabic language | http://arxiv.org/pdf/1203.2498v2.pdf | author:Riadh Bouslimi, Houda Amraoui category:cs.CL published:2012-03-08 summary:The study of natural language, especially Arabic, and mechanisms for theimplementation of automatic processing is a fascinating field of study, withvarious potential applications. The importance of tools for natural languageprocessing is materialized by the need to have applications that caneffectively treat the vast mass of information available nowadays on electronicforms. Among these tools, mainly driven by the necessity of a fast writing inalignment to the actual daily life speed, our interest is on the writingauditors. The morphological and syntactic properties of Arabic make it adifficult language to master, and explain the lack in the processing tools forthat language. Among these properties, we can mention: the complex structure ofthe Arabic word, the agglutinative nature, lack of vocalization, thesegmentation of the text, the linguistic richness, etc.
arxiv-600-211 | Multiple Change Point Estimation in Stationary Ergodic Time Series | http://arxiv.org/pdf/1203.1515v10.pdf | author:Azadeh Khaleghi, Daniil Ryabko category:stat.ML cs.IT math.IT math.ST stat.TH published:2012-03-07 summary:Given a heterogeneous time-series sample, the objective is to find points intime (called change points) where the probability distribution generating thedata has changed. The data are assumed to have been generated by arbitraryunknown stationary ergodic distributions. No modelling, independence or mixingassumptions are made. A novel, computationally efficient, nonparametric methodis proposed, and is shown to be asymptotically consistent in this generalframework. The theoretical results are complemented with experimentalevaluations.
arxiv-600-212 | Bayesian Nonparametric Hidden Semi-Markov Models | http://arxiv.org/pdf/1203.1365v2.pdf | author:Matthew J. Johnson, Alan S. Willsky category:stat.ME stat.AP stat.ML published:2012-03-07 summary:There is much interest in the Hierarchical Dirichlet Process Hidden MarkovModel (HDP-HMM) as a natural Bayesian nonparametric extension of the ubiquitousHidden Markov Model for learning from sequential and time-series data. However,in many settings the HDP-HMM's strict Markovian constraints are undesirable,particularly if we wish to learn or encode non-geometric state durations. Wecan extend the HDP-HMM to capture such structure by drawing uponexplicit-duration semi-Markovianity, which has been developed mainly in theparametric frequentist setting, to allow construction of highly interpretablemodels that admit natural prior information on state durations. In this paper we introduce the explicit-duration Hierarchical DirichletProcess Hidden semi-Markov Model (HDP-HSMM) and develop sampling algorithms forefficient posterior inference. The methods we introduce also provide newmethods for sampling inference in the finite Bayesian HSMM. Our modular Gibbssampling methods can be embedded in samplers for larger hierarchical Bayesianmodels, adding semi-Markov chain modeling as another tool in the Bayesianinference toolbox. We demonstrate the utility of the HDP-HSMM and our inferencemethods on both synthetic and real experiments.
arxiv-600-213 | Multiple Operator-valued Kernel Learning | http://arxiv.org/pdf/1203.1596v2.pdf | author:Hachem Kadri, Alain Rakotomamonjy, Francis Bach, Philippe Preux category:stat.ML cs.LG published:2012-03-07 summary:Positive definite operator-valued kernels generalize the well-known notion ofreproducing kernels, and are naturally adapted to multi-output learningsituations. This paper addresses the problem of learning a finite linearcombination of infinite-dimensional operator-valued kernels which are suitablefor extending functional data analysis methods to nonlinear contexts. We studythis problem in the case of kernel ridge regression for functional responseswith an lr-norm constraint on the combination coefficients. The resultingoptimization problem is more involved than those of multiple scalar-valuedkernel learning since operator-valued kernels pose more technical andtheoretical issues. We propose a multiple operator-valued kernel learningalgorithm based on solving a system of linear operator equations by using ablock coordinatedescent procedure. We experimentally validate our approach on afunctional regression task in the context of finger movement prediction inbrain-computer interfaces.
arxiv-600-214 | A Short Note on Gaussian Process Modeling for Large Datasets using Graphics Processing Units | http://arxiv.org/pdf/1203.1269v2.pdf | author:Mark Franey, Pritam Ranjan, Hugh Chipman category:stat.CO stat.ML published:2012-03-06 summary:The graphics processing unit (GPU) has emerged as a powerful and costeffective processor for general performance computing. GPUs are capable of anorder of magnitude more floating-point operations per second as compared tomodern central processing units (CPUs), and thus provide a great deal ofpromise for computationally intensive statistical applications. Fitting complexstatistical models with a large number of parameters and/or for large datasetsis often very computationally expensive. In this study, we focus on Gaussianprocess (GP) models -- statistical models commonly used for emulating expensivecomputer simulators. We demonstrate that the computational cost of implementingGP models can be significantly reduced by using a CPU+GPU heterogeneouscomputing system over an analogous implementation on a traditional computingsystem with no GPU acceleration. Our small study suggests that GP models arefertile ground for further implementation on CPU+GPU systems.
arxiv-600-215 | Sequential Design for Computer Experiments with a Flexible Bayesian Additive Model | http://arxiv.org/pdf/1203.1078v2.pdf | author:Hugh Chipman, Pritam Ranjan, Weiwei Wang category:stat.ME stat.ML published:2012-03-06 summary:In computer experiments, a mathematical model implemented on a computer isused to represent complex physical phenomena. These models, known as computersimulators, enable experimental study of a virtual representation of thecomplex phenomena. Simulators can be thought of as complex functions that takemany inputs and provide an output. Often these simulators are themselvesexpensive to compute, and may be approximated by "surrogate models" such asstatistical regression models. In this paper we consider a new kind ofsurrogate model, a Bayesian ensemble of trees (Chipman et al. 2010), with thespecific goal of learning enough about the simulator that a particular featureof the simulator can be estimated. We focus on identifying the simulator'sglobal minimum. Utilizing the Bayesian version of the Expected Improvementcriterion (Jones et al. 1998), we show that this ensemble is particularlyeffective when the simulator is ill-behaved, exhibiting nonstationarity orabrupt changes in the response. A number of illustrations of the approach aregiven, including a tidal power application.
arxiv-600-216 | Autocalibration with the Minimum Number of Cameras with Known Pixel Shape | http://arxiv.org/pdf/1203.0905v2.pdf | author:José I. Ronda, Antonio Valdés, Guillermo Gallego category:cs.CV published:2012-03-05 summary:In 3D reconstruction, the recovery of the calibration parameters of thecameras is paramount since it provides metric information about the observedscene, e.g., measures of angles and ratios of distances. Autocalibrationenables the estimation of the camera parameters without using a calibrationdevice, but by enforcing simple constraints on the camera parameters. In theabsence of information about the internal camera parameters such as the focallength and the principal point, the knowledge of the camera pixel shape isusually the only available constraint. Given a projective reconstruction of arigid scene, we address the problem of the autocalibration of a minimal set ofcameras with known pixel shape and otherwise arbitrarily varying intrinsic andextrinsic parameters. We propose an algorithm that only requires 5 cameras (thetheoretical minimum), thus halving the number of cameras required by previousalgorithms based on the same constraint. To this purpose, we introduce as ourbasic geometric tool the six-line conic variety (SLCV), consisting in the setof planes intersecting six given lines of 3D space in points of a conic. Weshow that the set of solutions of the Euclidean upgrading problem for threecameras with known pixel shape can be parameterized in a computationallyefficient way. This parameterization is then used to solve autocalibration fromfive or more cameras, reducing the three-dimensional search space to atwo-dimensional one. We provide experiments with real images showing the goodperformance of the technique.
arxiv-600-217 | Infinite Shift-invariant Grouped Multi-task Learning for Gaussian Processes | http://arxiv.org/pdf/1203.0970v2.pdf | author:Yuyang Wang, Roni Khardon, Pavlos Protopapas category:cs.LG astro-ph.IM stat.ML published:2012-03-05 summary:Multi-task learning leverages shared information among data sets to improvethe learning performance of individual tasks. The paper applies this frameworkfor data where each task is a phase-shifted periodic time series. Inparticular, we develop a novel Bayesian nonparametric model capturing a mixtureof Gaussian processes where each task is a sum of a group-specific function anda component capturing individual variation, in addition to each task beingphase shifted. We develop an efficient \textsc{em} algorithm to learn theparameters of the model. As a special case we obtain the Gaussian mixture modeland \textsc{em} algorithm for phased-shifted periodic time series. Furthermore,we extend the proposed model by using a Dirichlet Process prior and therebyleading to an infinite mixture model that is capable of doing automatic modelselection. A Variational Bayesian approach is developed for inference in thismodel. Experiments in regression, classification and class discoverydemonstrate the performance of the proposed models using both synthetic dataand real-world time series data from astrophysics. Our methods are particularlyuseful when the time series are sparsely and non-synchronously sampled.
arxiv-600-218 | Sparse Subspace Clustering: Algorithm, Theory, and Applications | http://arxiv.org/pdf/1203.1005v3.pdf | author:Ehsan Elhamifar, Rene Vidal category:cs.CV cs.IR cs.IT cs.LG math.IT math.OC stat.ML published:2012-03-05 summary:In many real-world problems, we are dealing with collections ofhigh-dimensional data, such as images, videos, text and web documents, DNAmicroarray data, and more. Often, high-dimensional data lie close tolow-dimensional structures corresponding to several classes or categories thedata belongs to. In this paper, we propose and study an algorithm, calledSparse Subspace Clustering (SSC), to cluster data points that lie in a union oflow-dimensional subspaces. The key idea is that, among infinitely many possiblerepresentations of a data point in terms of other points, a sparserepresentation corresponds to selecting a few points from the same subspace.This motivates solving a sparse optimization program whose solution is used ina spectral clustering framework to infer the clustering of data into subspaces.Since solving the sparse optimization program is in general NP-hard, weconsider a convex relaxation and show that, under appropriate conditions on thearrangement of subspaces and the distribution of data, the proposedminimization program succeeds in recovering the desired sparse representations.The proposed algorithm can be solved efficiently and can handle data pointsnear the intersections of subspaces. Another key advantage of the proposedalgorithm with respect to the state of the art is that it can deal with datanuisances, such as noise, sparse outlying entries, and missing entries,directly by incorporating the model of the data into the sparse optimizationprogram. We demonstrate the effectiveness of the proposed algorithm throughexperiments on synthetic data as well as the two real-world problems of motionsegmentation and face clustering.
arxiv-600-219 | Agnostic System Identification for Model-Based Reinforcement Learning | http://arxiv.org/pdf/1203.1007v2.pdf | author:Stephane Ross, J. Andrew Bagnell category:cs.LG cs.AI cs.SY stat.ML published:2012-03-05 summary:A fundamental problem in control is to learn a model of a system fromobservations that is useful for controller synthesis. To provide goodperformance guarantees, existing methods must assume that the real system is inthe class of models considered during learning. We present an iterative methodwith strong guarantees even in the agnostic case where the system is not in theclass. In particular, we show that any no-regret online learning algorithm canbe used to obtain a near-optimal policy, provided some model achieves lowtraining error and access to a good exploration distribution. Our approachapplies to both discrete and continuous domains. We demonstrate its efficacyand scalability on a challenging helicopter domain from the literature.
arxiv-600-220 | Learning High-Dimensional Mixtures of Graphical Models | http://arxiv.org/pdf/1203.0697v2.pdf | author:A. Anandkumar, D. Hsu, F. Huang, S. M. Kakade category:stat.ML cs.AI cs.LG published:2012-03-04 summary:We consider unsupervised estimation of mixtures of discrete graphical models,where the class variable corresponding to the mixture components is hidden andeach mixture component over the observed variables can have a potentiallydifferent Markov graph structure and parameters. We propose a novel approachfor estimating the mixture components, and our output is a tree-mixture modelwhich serves as a good approximation to the underlying graphical model mixture.Our method is efficient when the union graph, which is the union of the Markovgraphs of the mixture components, has sparse vertex separators between any pairof observed variables. This includes tree mixtures and mixtures of boundeddegree graphs. For such models, we prove that our method correctly recovers theunion graph structure and the tree structures corresponding tomaximum-likelihood tree approximations of the mixture components. The sampleand computational complexities of our method scale as $\poly(p, r)$, for an$r$-component mixture of $p$-variate graphical models. We further extend ourresults to the case when the union graph has sparse local separators betweenany pair of observed variables, such as mixtures of locally tree-like graphs,and the mixture components are in the regime of correlation decay.
arxiv-600-221 | Learning DNF Expressions from Fourier Spectrum | http://arxiv.org/pdf/1203.0594v3.pdf | author:Vitaly Feldman category:cs.LG cs.CC cs.DS published:2012-03-03 summary:Since its introduction by Valiant in 1984, PAC learning of DNF expressionsremains one of the central problems in learning theory. We consider thisproblem in the setting where the underlying distribution is uniform, or moregenerally, a product distribution. Kalai, Samorodnitsky and Teng (2009) showedthat in this setting a DNF expression can be efficiently approximated from its"heavy" low-degree Fourier coefficients alone. This is in contrast to previousapproaches where boosting was used and thus Fourier coefficients of the targetfunction modified by various distributions were needed. This property iscrucial for learning of DNF expressions over smoothed product distributions, alearning model introduced by Kalai et al. (2009) and inspired by the seminalsmoothed analysis model of Spielman and Teng (2001). We introduce a new approach to learning (or approximating) a polynomialthreshold functions which is based on creating a function with range [-1,1]that approximately agrees with the unknown function on low-degree Fouriercoefficients. We then describe conditions under which this is sufficient forlearning polynomial threshold functions. Our approach yields a new, simplealgorithm for approximating any polynomial-size DNF expression from its "heavy"low-degree Fourier coefficients alone. Our algorithm greatly simplifies theproof of learnability of DNF expressions over smoothed product distributions.We also describe an application of our algorithm to learning monotone DNFexpressions over product distributions. Building on the work of Servedio(2001), we give an algorithm that runs in time $\poly((s \cdot\log{(s/\eps)})^{\log{(s/\eps)}}, n)$, where $s$ is the size of the target DNFexpression and $\eps$ is the accuracy. This improves on $\poly((s \cdot\log{(ns/\eps)})^{\log{(s/\eps)} \cdot \log{(1/\eps)}}, n)$ bound of Servedio(2001).
arxiv-600-222 | Checking Tests for Read-Once Functions over Arbitrary Bases | http://arxiv.org/pdf/1203.0631v3.pdf | author:Dmitry V. Chistikov category:cs.DM cs.CC cs.LG published:2012-03-03 summary:A Boolean function is called read-once over a basis B if it can be expressedby a formula over B where no variable appears more than once. A checking testfor a read-once function f over B depending on all its variables is a set ofinput vectors distinguishing f from all other read-once functions of the samevariables. We show that every read-once function f over B has a checking testcontaining O(n^l) vectors, where n is the number of relevant variables of f andl is the largest arity of functions in B. For some functions, this bound cannotbe improved by more than a constant factor. The employed technique involvesreconstructing f from its l-variable projections and provides a stronger formof Kuznetsov's classic theorem on read-once representations.
arxiv-600-223 | A Method of Moments for Mixture Models and Hidden Markov Models | http://arxiv.org/pdf/1203.0683v3.pdf | author:Animashree Anandkumar, Daniel Hsu, Sham M. Kakade category:cs.LG stat.ML published:2012-03-03 summary:Mixture models are a fundamental tool in applied statistics and machinelearning for treating data taken from multiple subpopulations. The currentpractice for estimating the parameters of such models relies on local searchheuristics (e.g., the EM algorithm) which are prone to failure, and existingconsistent methods are unfavorable due to their high computational and samplecomplexity which typically scale exponentially with the number of mixturecomponents. This work develops an efficient method of moments approach toparameter estimation for a broad class of high-dimensional mixture models withmany components, including multi-view mixtures of Gaussians (such as mixturesof axis-aligned Gaussians) and hidden Markov models. The new method leads torigorous unsupervised learning results for mixture models that were notachieved by previous works; and, because of its simplicity, it offers a viablealternative to EM for practical deployment.
arxiv-600-224 | Algorithms for Learning Kernels Based on Centered Alignment | http://arxiv.org/pdf/1203.0550v2.pdf | author:Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh category:cs.LG cs.AI published:2012-03-02 summary:This paper presents new and effective algorithms for learning kernels. Inparticular, as shown by our empirical results, these algorithms consistentlyoutperform the so-called uniform combination solution that has proven to bedifficult to improve upon in the past, as well as other algorithms for learningkernels based on convex combinations of base kernels in both classification andregression. Our algorithms are based on the notion of centered alignment whichis used as a similarity measure between kernels or kernel matrices. We presenta number of novel algorithmic, theoretical, and empirical results for learningkernels based on our notion of centered alignment. In particular, we describeefficient algorithms for learning a maximum alignment kernel by showing thatthe problem can be reduced to a simple QP and discuss a one-stage algorithm forlearning both a kernel and a hypothesis based on that kernel using analignment-based regularization. Our theoretical results include a novelconcentration bound for centered alignment between kernel matrices, the proofof the existence of effective predictors for kernels with high alignment, bothfor classification and for regression, and the proof of stability-basedgeneralization bounds for a broad family of algorithms for learning kernelsbased on centered alignment. We also report the results of experiments with ourcentered alignment-based algorithms in both classification and regression.
arxiv-600-225 | Change-Point Detection in Time-Series Data by Relative Density-Ratio Estimation | http://arxiv.org/pdf/1203.0453v2.pdf | author:Song Liu, Makoto Yamada, Nigel Collier, Masashi Sugiyama category:stat.ML cs.LG stat.ME published:2012-03-02 summary:The objective of change-point detection is to discover abrupt propertychanges lying behind time-series data. In this paper, we present a novelstatistical change-point detection algorithm based on non-parametric divergenceestimation between time-series samples from two retrospective segments. Ourmethod uses the relative Pearson divergence as a divergence measure, and it isaccurately and efficiently estimated by a method of direct density-ratioestimation. Through experiments on artificial and real-world datasets includinghuman-activity sensing, speech, and Twitter messages, we demonstrate theusefulness of the proposed method.
arxiv-600-226 | Fast learning rate of multiple kernel learning: Trade-off between sparsity and smoothness | http://arxiv.org/pdf/1203.0565v2.pdf | author:Taiji Suzuki, Masashi Sugiyama category:stat.ML math.ST stat.TH published:2012-03-02 summary:We investigate the learning rate of multiple kernel learning (MKL) with$\ell_1$ and elastic-net regularizations. The elastic-net regularization is acomposition of an $\ell_1$-regularizer for inducing the sparsity and an$\ell_2$-regularizer for controlling the smoothness. We focus on a sparsesetting where the total number of kernels is large, but the number of nonzerocomponents of the ground truth is relatively small, and show sharperconvergence rates than the learning rates have ever shown for both $\ell_1$ andelastic-net regularizations. Our analysis reveals some relations between thechoice of a regularization function and the performance. If the ground truth issmooth, we show a faster convergence rate for the elastic-net regularizationwith less conditions than $\ell_1$-regularization; otherwise, a fasterconvergence rate for the $\ell_1$-regularization is shown.
arxiv-600-227 | Learning a Common Substructure of Multiple Graphical Gaussian Models | http://arxiv.org/pdf/1203.0117v3.pdf | author:Satoshi Hara, Takashi Washio category:stat.ML published:2012-03-01 summary:Properties of data are frequently seen to vary depending on the sampledsituations, which usually changes along a time evolution or owing toenvironmental effects. One way to analyze such data is to find invariances, orrepresentative features kept constant over changes. The aim of this paper is toidentify one such feature, namely interactions or dependencies among variablesthat are common across multiple datasets collected under different conditions.To that end, we propose a common substructure learning (CSSL) framework basedon a graphical Gaussian model. We further present a simple learning algorithmbased on the Dual Augmented Lagrangian and the Alternating Direction Method ofMultipliers. We confirm the performance of CSSL over other existing techniquesin finding unchanging dependency structures in multiple datasets throughnumerical simulations on synthetic data and through a real world application toanomaly detection in automobile sensors.
arxiv-600-228 | The Horse Raced Past: Gardenpath Processing in Dynamical Systems | http://arxiv.org/pdf/1203.0145v2.pdf | author:Peter beim Graben category:cs.CL published:2012-03-01 summary:I pinpoint an interesting similarity between a recent account to rationalparsing and the treatment of sequential decisions problems in a dynamicalsystems approach. I argue that expectation-driven search heuristics aiming atfast computation resembles a high-risk decision strategy in favor of largetransition velocities. Hale's rational parser, combining generalizedleft-corner parsing with informed $\mathrm{A}^*$ search to resolve processingconflicts, explains gardenpath effects in natural sentence processing bymisleading estimates of future processing costs that are to be minimized. Onthe other hand, minimizing the duration of cognitive computations intime-continuous dynamical systems can be described by combining vector spacerepresentations of cognitive states by means of filler/role decompositions andsubsequent tensor product representations with the paradigm of stableheteroclinic sequences. Maximizing transition velocities according to ahigh-risk decision strategy could account for a fast race even between statesthat are apparently remote in representation space.
arxiv-600-229 | Uniform random generation of large acyclic digraphs | http://arxiv.org/pdf/1202.6590v4.pdf | author:Jack Kuipers, Giusi Moffa category:stat.CO math.ST stat.ML stat.TH published:2012-02-29 summary:Directed acyclic graphs are the basic representation of the structureunderlying Bayesian networks, which represent multivariate probabilitydistributions. In many practical applications, such as the reverse engineeringof gene regulatory networks, not only the estimation of model parameters butthe reconstruction of the structure itself is of great interest. As well as forthe assessment of different structure learning algorithms in simulationstudies, a uniform sample from the space of directed acyclic graphs is requiredto evaluate the prevalence of certain structural features. Here we analyse howto sample acyclic digraphs uniformly at random through recursive enumeration,an approach previously thought too computationally involved. Based oncomplexity considerations, we discuss in particular how the enumerationdirectly provides an exact method, which avoids the convergence issues of thealternative Markov chain methods and is actually computationally much faster.The limiting behaviour of the distribution of acyclic digraphs then allows usto sample arbitrarily large graphs. Building on the ideas of recursiveenumeration based sampling we also introduce a novel hybrid Markov chain withmuch faster convergence than current alternatives while still being easy toadapt to various restrictions. Finally we discuss how to include suchrestrictions in the combinatorial enumeration and the new hybrid Markov chainmethod for efficient uniform sampling of the corresponding graphs.
arxiv-600-230 | Learning from Distributions via Support Measure Machines | http://arxiv.org/pdf/1202.6504v2.pdf | author:Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, Bernhard Schölkopf category:stat.ML cs.LG published:2012-02-29 summary:This paper presents a kernel-based discriminative learning framework onprobability measures. Rather than relying on large collections of vectorialtraining examples, our framework learns using a collection of probabilitydistributions that have been constructed to meaningfully represent trainingdata. By representing these probability distributions as mean embeddings in thereproducing kernel Hilbert space (RKHS), we are able to apply many standardkernel-based learning techniques in straightforward fashion. To accomplishthis, we construct a generalization of the support vector machine (SVM) calleda support measure machine (SMM). Our analyses of SMMs provides several insightsinto their relationship to traditional SVMs. Based on such insights, we proposea flexible SVM (Flex-SVM) that places different kernel functions on eachtraining example. Experimental results on both synthetic and real-world datademonstrate the effectiveness of our proposed framework.
arxiv-600-231 | Stable image reconstruction using total variation minimization | http://arxiv.org/pdf/1202.6429v9.pdf | author:Deanna Needell, Rachel Ward category:cs.CV cs.IT math.IT math.NA published:2012-02-29 summary:This article presents near-optimal guarantees for accurate and robust imagerecovery from under-sampled noisy measurements using total variationminimization. In particular, we show that from O(slog(N)) nonadaptive linearmeasurements, an image can be reconstructed to within the best s-termapproximation of its gradient up to a logarithmic factor, and this factor canbe removed by taking slightly more measurements. Along the way, we prove astrengthened Sobolev inequality for functions lying in the null space ofsuitably incoherent matrices.
arxiv-600-232 | Hyperspectral Unmixing Overview: Geometrical, Statistical, and Sparse Regression-Based Approaches | http://arxiv.org/pdf/1202.6294v2.pdf | author:José M. Bioucas-Dias, Antonio Plaza, Nicolas Dobigeon, Mario Parente, Qian Du, Paul Gader, Jocelyn Chanussot category:math.OC stat.AP stat.ML published:2012-02-28 summary:Imaging spectrometers measure electromagnetic energy scattered in theirinstantaneous field view in hundreds or thousands of spectral channels withhigher spectral resolution than multispectral cameras. Imaging spectrometersare therefore often referred to as hyperspectral cameras (HSCs). Higherspectral resolution enables material identification via spectroscopic analysis,which facilitates countless applications that require identifying materials inscenarios unsuitable for classical spectroscopic analysis. Due to low spatialresolution of HSCs, microscopic material mixing, and multiple scattering,spectra measured by HSCs are mixtures of spectra of materials in a scene. Thus,accurate estimation requires unmixing. Pixels are assumed to be mixtures of afew materials, called endmembers. Unmixing involves estimating all or some of:the number of endmembers, their spectral signatures, and their abundances ateach pixel. Unmixing is a challenging, ill-posed inverse problem because ofmodel inaccuracies, observation noise, environmental conditions, endmembervariability, and data set size. Researchers have devised and investigated manymodels searching for robust, stable, tractable, and accurate unmixingalgorithms. This paper presents an overview of unmixing methods from the timeof Keshava and Mustard's unmixing tutorial [1] to the present. Mixing modelsare first discussed. Signal-subspace, geometrical, statistical, sparsity-based,and spatial-contextual unmixing algorithms are described. Mathematical problemsand potential solutions are described. Algorithm characteristics areillustrated experimentally.
arxiv-600-233 | Nonlinear Laplacian spectral analysis: Capturing intermittent and low-frequency spatiotemporal patterns in high-dimensional data | http://arxiv.org/pdf/1202.6103v2.pdf | author:Dimitrios Giannakis, Andrew J. Majda category:cs.LG published:2012-02-28 summary:We present a technique for spatiotemporal data analysis called nonlinearLaplacian spectral analysis (NLSA), which generalizes singular spectrumanalysis (SSA) to take into account the nonlinear manifold structure of complexdata sets. The key principle underlying NLSA is that the functions used torepresent temporal patterns should exhibit a degree of smoothness on thenonlinear data manifold M; a constraint absent from classical SSA. NLSAenforces such a notion of smoothness by requiring that temporal patterns belongin low-dimensional Hilbert spaces V_l spanned by the leading l Laplace-Beltramieigenfunctions on M. These eigenfunctions can be evaluated efficiently in highambient-space dimensions using sparse graph-theoretic algorithms. Moreover,they provide orthonormal bases to expand a family of linear maps, whosesingular value decomposition leads to sets of spatiotemporal patterns atprogressively finer resolution on the data manifold. The Riemannian measure ofM and an adaptive graph kernel width enhances the capability of NLSA to detectimportant nonlinear processes, including intermittency and rare events. Theminimum dimension of V_l required to capture these features while avoidingoverfitting is estimated here using spectral entropy criteria.
arxiv-600-234 | Confusion Matrix Stability Bounds for Multiclass Classification | http://arxiv.org/pdf/1202.6221v2.pdf | author:Pierre Machart, Liva Ralaivola category:cs.LG published:2012-02-28 summary:In this paper, we provide new theoretical results on the generalizationproperties of learning algorithms for multiclass classification problems. Theoriginality of our work is that we propose to use the confusion matrix of aclassifier as a measure of its quality; our contribution is in the line of workwhich attempts to set up and study the statistical properties of new evaluationmeasures such as, e.g. ROC curves. In the confusion-based learning framework wepropose, we claim that a targetted objective is to minimize the size of theconfusion matrix C, measured through its operator norm C. We derivegeneralization bounds on the (size of the) confusion matrix in an extendedframework of uniform stability, adapted to the case of matrix valued loss.Pivotal to our study is a very recent matrix concentration inequality thatgeneralizes McDiarmid's inequality. As an illustration of the relevance of ourtheoretical results, we show how two SVM learning procedures can be proved tobe confusion-friendly. To the best of our knowledge, the present paper is thefirst that focuses on the confusion matrix from a theoretical point of view.
arxiv-600-235 | A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets | http://arxiv.org/pdf/1202.6258v4.pdf | author:Nicolas Le Roux, Mark Schmidt, Francis Bach category:math.OC cs.LG published:2012-02-28 summary:We propose a new stochastic gradient method for optimizing the sum of afinite set of smooth functions, where the sum is strongly convex. Whilestandard stochastic gradient methods converge at sublinear rates for thisproblem, the proposed method incorporates a memory of previous gradient valuesin order to achieve a linear convergence rate. In a machine learning context,numerical experiments indicate that the new algorithm can dramaticallyoutperform standard algorithms, both in terms of optimizing the training errorand reducing the test error quickly.
arxiv-600-236 | PAC-Bayesian Generalization Bound on Confusion Matrix for Multi-Class Classification | http://arxiv.org/pdf/1202.6228v6.pdf | author:Emilie Morvant, Sokol Koço, Liva Ralaivola category:stat.ML cs.LG published:2012-02-28 summary:In this work, we propose a PAC-Bayes bound for the generalization risk of theGibbs classifier in the multi-class classification framework. The novelty ofour work is the critical use of the confusion matrix of a classifier as anerror measure; this puts our contribution in the line of work aiming at dealingwith performance measure that are richer than mere scalar criterion such as themisclassification rate. Thanks to very recent and beautiful results on matrixconcentration inequalities, we derive two bounds showing that the trueconfusion risk of the Gibbs classifier is upper-bounded by its empirical riskplus a term depending on the number of training examples in each class. To thebest of our knowledge, this is the first PAC-Bayes bounds based on confusionmatrices.
arxiv-600-237 | Replica theory for learning curves for Gaussian processes on random graphs | http://arxiv.org/pdf/1202.5918v3.pdf | author:Matthew J. Urry, Peter Sollich category:stat.ML published:2012-02-27 summary:Statistical physics approaches can be used to derive accurate predictions forthe performance of inference methods learning from potentially noisy data, asquantified by the learning curve defined as the average error versus number oftraining examples. We analyse a challenging problem in the area ofnon-parametric inference where an effectively infinite number of parameters hasto be learned, specifically Gaussian process regression. When the inputs arevertices on a random graph and the outputs noisy function values, we show thatreplica techniques can be used to obtain exact performance predictions in thelimit of large graphs. The covariance of the Gaussian process prior is definedby a random walk kernel, the discrete analogue of squared exponential kernelson continuous spaces. Conventionally this kernel is normalised only globally,so that the prior variance can differ between vertices; as a more principledalternative we consider local normalisation, where the prior variance isuniform.
arxiv-600-238 | Divide-and-Conquer Method for L1 Norm Matrix Factorization in the Presence of Outliers and Missing Data | http://arxiv.org/pdf/1202.5844v3.pdf | author:Deyu Meng, Zongben Xu category:cs.NA cs.CV published:2012-02-27 summary:The low-rank matrix factorization as a L1 norm minimization problem hasrecently attracted much attention due to its intrinsic robustness to thepresence of outliers and missing data. In this paper, we propose a new method,called the divide-and-conquer method, for solving this problem. The main ideais to break the original problem into a series of smallest possiblesub-problems, each involving only unique scalar parameter. Each of thesesubproblems is proved to be convex and has closed-form solution. By recursivelyoptimizing these small problems in an analytical way, efficient algorithm,entirely avoiding the time-consuming numerical optimization as an inner loop,for solving the original problem can naturally be constructed. Thecomputational complexity of the proposed algorithm is approximately linear inboth data size and dimensionality, making it possible to handle large-scale L1norm matrix factorization problems. The algorithm is also theoretically provedto be convergent. Based on a series of experiment results, it is substantiatedthat our method always achieves better results than the currentstate-of-the-art methods on $L1$ matrix factorization calculation in bothcomputational time and accuracy, especially on large-scale applications such asface recognition and structure from motion.
arxiv-600-239 | Hybrid Batch Bayesian Optimization | http://arxiv.org/pdf/1202.5597v3.pdf | author:Javad Azimi, Ali Jalali, Xiaoli Fern category:cs.AI cs.LG published:2012-02-25 summary:Bayesian Optimization aims at optimizing an unknown non-convex/concavefunction that is costly to evaluate. We are interested in application scenarioswhere concurrent function evaluations are possible. Under such a setting, BOcould choose to either sequentially evaluate the function, one input at a timeand wait for the output of the function before making the next selection, orevaluate the function at a batch of multiple inputs at once. These twodifferent settings are commonly referred to as the sequential and batchsettings of Bayesian Optimization. In general, the sequential setting leads tobetter optimization performance as each function evaluation is selected withmore information, whereas the batch setting has an advantage in terms of thetotal experimental time (the number of iterations). In this work, our goal isto combine the strength of both settings. Specifically, we systematicallyanalyze Bayesian optimization using Gaussian process as the posterior estimatorand provide a hybrid algorithm that, based on the current state, dynamicallyswitches between a sequential policy and a batch policy with variable batchsizes. We provide theoretical justification for our algorithm and presentexperimental results on eight benchmark BO problems. The results show that ourmethod achieves substantial speedup (up to %78) compared to a pure sequentialpolicy, without suffering any significant performance loss.
arxiv-600-240 | Training Restricted Boltzmann Machines on Word Observations | http://arxiv.org/pdf/1202.5695v2.pdf | author:George E. Dahl, Ryan P. Adams, Hugo Larochelle category:cs.LG stat.ML published:2012-02-25 summary:The restricted Boltzmann machine (RBM) is a flexible tool for modelingcomplex data, however there have been significant computational difficulties inusing RBMs to model high-dimensional multinomial observations. In naturallanguage processing applications, words are naturally modeled by K-ary discretedistributions, where K is determined by the vocabulary size and can easily bein the hundreds of thousands. The conventional approach to training RBMs onword observations is limited because it requires sampling the states of K-waysoftmax visible units during block Gibbs updates, an operation that takes timelinear in K. In this work, we address this issue by employing a more generalclass of Markov chain Monte Carlo operators on the visible units, yieldingupdates with computational complexity independent of K. We demonstrate thesuccess of our approach by training RBMs on hundreds of millions of wordn-grams using larger vocabularies than previously feasible and using thelearned features to improve performance on chunking and sentimentclassification tasks, achieving state-of-the-art results on the latter.
arxiv-600-241 | Clustering using Max-norm Constrained Optimization | http://arxiv.org/pdf/1202.5598v4.pdf | author:Ali Jalali, Nathan Srebro category:cs.LG stat.ML published:2012-02-25 summary:We suggest using the max-norm as a convex surrogate constraint forclustering. We show how this yields a better exact cluster recovery guaranteethan previously suggested nuclear-norm relaxation, and study the effectivenessof our method, and other related convex relaxations, compared to otherclustering approaches.
arxiv-600-242 | Classification approach based on association rules mining for unbalanced data | http://arxiv.org/pdf/1202.5514v2.pdf | author:Cheikh Ndour, Aliou Diop, Simplice Dossou-Gbété category:stat.ML cs.LG published:2012-02-24 summary:This paper deals with the binary classification task when the target classhas the lower probability of occurrence. In such situation, it is not possibleto build a powerful classifier by using standard methods such as logisticregression, classification tree, discriminant analysis, etc. To overcome thisshort-coming of these methods which yield classifiers with low sensibility, wetackled the classification problem here through an approach based on theassociation rules learning. This approach has the advantage of allowing theidentification of the patterns that are well correlated with the target class.Association rules learning is a well known method in the area of data-mining.It is used when dealing with large database for unsupervised discovery of localpatterns that expresses hidden relationships between input variables. Inconsidering association rules from a supervised learning point of view, arelevant set of weak classifiers is obtained from which one derives aclassifier that performs well.
arxiv-600-243 | Min Max Generalization for Two-stage Deterministic Batch Mode Reinforcement Learning: Relaxation Schemes | http://arxiv.org/pdf/1202.5298v2.pdf | author:Raphael Fonteneau, Damien Ernst, Bernard Boigelot, Quentin Louveaux category:cs.SY cs.LG published:2012-02-23 summary:We study the minmax optimization problem introduced in [22] for computingpolicies for batch mode reinforcement learning in a deterministic setting.First, we show that this problem is NP-hard. In the two-stage case, we providetwo relaxation schemes. The first relaxation scheme works by dropping someconstraints in order to obtain a problem that is solvable in polynomial time.The second relaxation scheme, based on a Lagrangian relaxation where allconstraints are dualized, leads to a conic quadratic programming problem. Wealso theoretically prove and empirically illustrate that both relaxationschemes provide better results than those given in [22].
arxiv-600-244 | Support Vector Regression for Right Censored Data | http://arxiv.org/pdf/1202.5130v2.pdf | author:Yair Goldberg, Michael R. Kosorok category:stat.ML math.ST stat.TH published:2012-02-23 summary:We develop a unified approach for classification and regression supportvector machines for data subject to right censoring. We provide finite samplebounds on the generalization error of the algorithm, prove risk consistency fora wide class of probability measures, and study the associated learning rates.We apply the general methodology to estimation of the (truncated) mean, median,quantiles, and for classification problems. We present a simulation study thatdemonstrates the performance of the proposed approach.
arxiv-600-245 | Optimal detection of sparse principal components in high dimension | http://arxiv.org/pdf/1202.5070v3.pdf | author:Quentin Berthet, Philippe Rigollet category:math.ST stat.ML stat.TH published:2012-02-23 summary:We perform a finite sample analysis of the detection levels for sparseprincipal components of a high-dimensional covariance matrix. Our minimaxoptimal test is based on a sparse eigenvalue statistic. Alas, computing thistest is known to be NP-complete in general, and we describe a computationallyefficient alternative test using convex relaxations. Our relaxation is alsoproved to detect sparse principal components at near optimal detection levels,and it performs well on simulated datasets. Moreover, using polynomial timereductions from theoretical computer science, we bring significant evidencethat our results cannot be improved, thus revealing an inherent trade offbetween statistical and computational performance.
arxiv-600-246 | Metabolic cost as an organizing principle for cooperative learning | http://arxiv.org/pdf/1202.4482v2.pdf | author:David Balduzzi, Pedro A Ortega, Michel Besserve category:q-bio.NC cs.LG nlin.AO published:2012-02-20 summary:This paper investigates how neurons can use metabolic cost to facilitatelearning at a population level. Although decision-making by individual neuronshas been extensively studied, questions regarding how neurons should behave tocooperate effectively remain largely unaddressed. Under assumptions thatcapture a few basic features of cortical neurons, we show that constrainingreward maximization by metabolic cost aligns the information content of actionswith their expected reward. Thus, metabolic cost provides a mechanism wherebyneurons encode expected reward into their outputs. Further, aside from reducingenergy expenditures, imposing a tight metabolic constraint also increases theaccuracy of empirical estimates of rewards, increasing the robustness ofdistributed learning. Finally, we present two implementations of metabolicallyconstrained learning that confirm our theoretical finding. These resultssuggest that metabolic cost may be an organizing principle underlying theneural code, and may also provide a useful guide to the design and analysis ofother cooperating populations.
arxiv-600-247 | Robust computation of linear models by convex relaxation | http://arxiv.org/pdf/1202.4044v2.pdf | author:Gilad Lerman, Michael McCoy, Joel A. Tropp, Teng Zhang category:cs.IT math.IT stat.CO stat.ML published:2012-02-18 summary:Consider a dataset of vector-valued observations that consists of noisyinliers, which are explained well by a low-dimensional subspace, along withsome number of outliers. This work describes a convex optimization problem,called REAPER, that can reliably fit a low-dimensional model to this type ofdata. This approach parameterizes linear subspaces using orthogonal projectors,and it uses a relaxation of the set of orthogonal projectors to reach theconvex formulation. The paper provides an efficient algorithm for solving theREAPER problem, and it documents numerical experiments which confirm thatREAPER can dependably find linear structure in synthetic and natural data. Inaddition, when the inliers lie near a low-dimensional subspace, there is arigorous theory that describes when REAPER can approximate this subspace.
arxiv-600-248 | On the Sample Complexity of Predictive Sparse Coding | http://arxiv.org/pdf/1202.4050v2.pdf | author:Nishant A. Mehta, Alexander G. Gray category:cs.LG stat.ML published:2012-02-18 summary:The goal of predictive sparse coding is to learn a representation of examplesas sparse linear combinations of elements from a dictionary, such that alearned hypothesis linear in the new representation performs well on apredictive task. Predictive sparse coding algorithms recently have demonstratedimpressive performance on a variety of supervised tasks, but theirgeneralization properties have not been studied. We establish the firstgeneralization error bounds for predictive sparse coding, covering twosettings: 1) the overcomplete setting, where the number of features k exceedsthe original dimensionality d; and 2) the high or infinite-dimensional setting,where only dimension-free bounds are useful. Both learning bounds intimatelydepend on stability properties of the learned sparse encoder, as measured onthe training sample. Consequently, we first present a fundamental stabilityresult for the LASSO, a result characterizing the stability of the sparse codeswith respect to perturbations to the dictionary. In the overcomplete setting,we present an estimation error bound that decays as \tilde{O}(sqrt(d k/m)) withrespect to d and k. In the high or infinite-dimensional setting, we show adimension-free bound that is \tilde{O}(sqrt(k^2 s / m)) with respect to k ands, where s is an upper bound on the number of non-zeros in the sparse code forany training data point.
arxiv-600-249 | Near-optimal Coresets For Least-Squares Regression | http://arxiv.org/pdf/1202.3505v2.pdf | author:Christos Boutsidis, Petros Drineas, Malik Magdon-Ismail category:cs.DS cs.LG published:2012-02-16 summary:We study (constrained) least-squares regression as well as multiple responseleast-squares regression and ask the question of whether a subset of the data,a coreset, suffices to compute a good approximate solution to the regression.We give deterministic, low order polynomial-time algorithms to construct suchcoresets with approximation guarantees, together with lower bounds indicatingthat there is not much room for improvement upon our results.
arxiv-600-250 | Guaranteed clustering and biclustering via semidefinite programming | http://arxiv.org/pdf/1202.3663v6.pdf | author:Brendan P. W. Ames category:math.OC cs.LG published:2012-02-16 summary:Identifying clusters of similar objects in data plays a significant role in awide range of applications. As a model problem for clustering, we consider thedensest k-disjoint-clique problem, whose goal is to identify the collection ofk disjoint cliques of a given weighted complete graph maximizing the sum of thedensities of the complete subgraphs induced by these cliques. In this paper, weestablish conditions ensuring exact recovery of the densest k cliques of agiven graph from the optimal solution of a particular semidefinite program. Inparticular, the semidefinite relaxation is exact for input graphs correspondingto data consisting of k large, distinct clusters and a smaller number ofoutliers. This approach also yields a semidefinite relaxation for thebiclustering problem with similar recovery guarantees. Given a set of objectsand a set of features exhibited by these objects, biclustering seeks tosimultaneously group the objects and features according to their expressionlevels. This problem may be posed as partitioning the nodes of a weightedbipartite complete graph such that the sum of the densities of the resultingbipartite complete subgraphs is maximized. As in our analysis of the densestk-disjoint-clique problem, we show that the correct partition of the objectsand features can be recovered from the optimal solution of a semidefiniteprogram in the case that the given data consists of several disjoint sets ofobjects exhibiting similar features. Empirical evidence from numericalexperiments supporting these theoretical guarantees is also provided.
arxiv-600-251 | Finding a most biased coin with fewest flips | http://arxiv.org/pdf/1202.3639v3.pdf | author:Karthekeyan Chandrasekaran, Richard Karp category:cs.DS cs.LG published:2012-02-16 summary:We study the problem of learning a most biased coin among a set of coins bytossing the coins adaptively. The goal is to minimize the number of tossesuntil we identify a coin i* whose posterior probability of being most biased isat least 1-delta for a given delta. Under a particular probabilistic model, wegive an optimal algorithm, i.e., an algorithm that minimizes the expectednumber of future tosses. The problem is closely related to finding the best armin the multi-armed bandit problem using adaptive strategies. Our algorithmemploys an optimal adaptive strategy -- a strategy that performs the bestpossible action at each step after observing the outcomes of all previous cointosses. Consequently, our algorithm is also optimal for any starting history ofoutcomes. To our knowledge, this is the first algorithm that employs an optimaladaptive strategy under a Bayesian setting for this problem. Our proof ofoptimality employs tools from the field of Markov games.
arxiv-600-252 | Mirror Descent Meets Fixed Share (and feels no regret) | http://arxiv.org/pdf/1202.3323v2.pdf | author:Nicolò Cesa-Bianchi, Pierre Gaillard, Gabor Lugosi, Gilles Stoltz category:cs.LG stat.ML published:2012-02-15 summary:Mirror descent with an entropic regularizer is known to achieve shiftingregret bounds that are logarithmic in the dimension. This is done using eithera carefully designed projection or by a weight sharing technique. Via a novelunified analysis, we show that these two approaches deliver essentiallyequivalent bounds on a notion of regret generalizing shifting, adaptive,discounted, and other related regrets. Our analysis also captures and extendsthe generalized weight sharing technique of Bousquet and Warmuth, and can berefined in several ways, including improvements for small losses and adaptivetuning of parameters.
arxiv-600-253 | Rank/Norm Regularization with Closed-Form Solutions: Application to Subspace Clustering | http://arxiv.org/pdf/1202.3772v2.pdf | author:Yao-Liang Yu, Dale Schuurmans category:cs.LG cs.NA stat.ML published:2012-02-14 summary:When data is sampled from an unknown subspace, principal component analysis(PCA) provides an effective way to estimate the subspace and hence reduce thedimension of the data. At the heart of PCA is the Eckart-Young-Mirsky theorem,which characterizes the best rank k approximation of a matrix. In this paper,we prove a generalization of the Eckart-Young-Mirsky theorem under allunitarily invariant norms. Using this result, we obtain closed-form solutionsfor a set of rank/norm regularized problems, and derive closed-form solutionsfor a general class of subspace clustering problems (where data is modelled byunions of unknown subspaces). From these results we obtain new theoreticalinsights and promising experimental results.
arxiv-600-254 | Sparse Matrix Inversion with Scaled Lasso | http://arxiv.org/pdf/1202.2723v2.pdf | author:Tingni Sun, Cun-Hui Zhang category:math.ST stat.ML stat.TH published:2012-02-13 summary:We propose a new method of learning a sparse nonnegative-definite targetmatrix. Our primary example of the target matrix is the inverse of a populationcovariance or correlation matrix. The algorithm first estimates each column ofthe target matrix by the scaled Lasso and then adjusts the matrix estimator tobe symmetric. The penalty level of the scaled Lasso for each column iscompletely determined by data via convex minimization, without usingcross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate ofconvergence in the spectrum norm under conditions of weaker form than those inthe existing analyses of other $\ell_1$ regularized algorithms, and has fasterguaranteed rate of convergence when the ratio of the $\ell_1$ and spectrumnorms of the target inverse matrix diverges to infinity. A simulation studydemonstrates the computational feasibility and superb performance of theproposed method. Our analysis also provides new performance bounds for the Lasso and scaledLasso to guarantee higher concentration of the error at a smaller thresholdlevel than previous analyses, and to allow the use of the union bound incolumn-by-column applications of the scaled Lasso without an adjustment of thepenalty level. In addition, the least squares estimation after the scaled Lassoselection is considered and proven to guarantee performance bounds similar tothat of the scaled Lasso.
arxiv-600-255 | Multi-Level Error-Resilient Neural Networks with Learning | http://arxiv.org/pdf/1202.2770v4.pdf | author:Amir Hesam Salavati, Amin Karbasi category:cs.NE cs.AI cs.IT math.IT published:2012-02-13 summary:The problem of neural network association is to retrieve a previouslymemorized pattern from its noisy version using a network of neurons. An idealneural network should include three components simultaneously: a learningalgorithm, a large pattern retrieval capacity and resilience against noise.Prior works in this area usually improve one or two aspects at the cost of thethird. Our work takes a step forward in closing this gap. More specifically, we showthat by forcing natural constraints on the set of learning patterns, we candrastically improve the retrieval capacity of our neural network. Moreover, wedevise a learning algorithm whose role is to learn those patterns satisfyingthe above mentioned constraints. Finally we show that our neural network cancope with a fair amount of noise.
arxiv-600-256 | Evolutionary Computation in Astronomy and Astrophysics: A Review | http://arxiv.org/pdf/1202.2523v2.pdf | author:José A. García Gutiérrez, Carlos Cotta, Antonio J. Fernández-Leiva category:cs.AI astro-ph.IM cs.NE published:2012-02-12 summary:In general Evolutionary Computation (EC) includes a number of optimizationmethods inspired by biological mechanisms of evolution. The methods cataloguedin this area use the Darwinian principles of life evolution to producealgorithms that returns high quality solutions to hard-to-solve optimizationproblems. The main strength of EC is precisely that they provide good solutionseven if the computational resources (e.g., running time) are limited. Astronomyand Astrophysics are two fields that often require optimizing problems of highcomplexity or analyzing a huge amount of data and the so-called completeoptimization methods are inherently limited by the size of the problem/data.For instance, reliable analysis of large amounts of data is central to modernastrophysics and astronomical sciences in general. EC techniques perform wellwhere other optimization methods are inherently limited (as complete methodsapplied to NP-hard problems), and in the last ten years, numerous proposalshave come up that apply with greater or lesser success methodologies ofevolutional computation to common engineering problems. Some of these problems,such as the estimation of non-lineal parameters, the development of automaticlearning techniques, the implementation of control systems, or the resolutionof multi-objective optimization problems, have had (and have) a specialrepercussion in the fields. For these reasons EC emerges as a feasiblealternative for traditional methods. In this paper, we discuss some promisingapplications in this direction and a number of recent works in this area; thepaper also includes a general description of EC to provide a global perspectiveto the reader and gives some guidelines of application of EC techniques forfuture research
arxiv-600-257 | Segmenting DNA sequence into `words' | http://arxiv.org/pdf/1202.2518v4.pdf | author:Wang Liang category:q-bio.GN cs.CL published:2012-02-12 summary:This paper presents a novel method to segment/decode DNA sequences based onn-grams statistical language model. Firstly, we find the length of most DNA'words' is 12 to 15 bps by analyzing the genomes of 12 model species. Then wedesign an unsupervised probability based approach to segment the DNA sequences.The benchmark of segmenting method is also proposed.
arxiv-600-258 | A better Beta for the H measure of classification performance | http://arxiv.org/pdf/1202.2564v2.pdf | author:David J. Hand, Christoforos Anagnostopoulos category:stat.ME cs.CV stat.ML published:2012-02-12 summary:The area under the ROC curve is widely used as a measure of performance ofclassification rules. However, it has recently been shown that the measure isfundamentally incoherent, in the sense that it treats the relative severitiesof misclassifications differently when different classifiers are used. Toovercome this, Hand (2009) proposed the $H$ measure, which allows a givenresearcher to fix the distribution of relative severities to aclassifier-independent setting on a given problem. This note extends thediscussion, and proposes a modified standard distribution for the $H$ measure,which better matches the requirements of researchers, in particular those facedwith heavily unbalanced datasets, the $Beta(\pi_1+1,\pi_0+1)$ distribution.[Preprint submitted at Pattern Recognition Letters]
arxiv-600-259 | High Dimensional Semiparametric Gaussian Copula Graphical Models | http://arxiv.org/pdf/1202.2169v3.pdf | author:Han Liu, Fang Han, Ming Yuan, John Lafferty, Larry Wasserman category:stat.ML published:2012-02-10 summary:In this paper, we propose a semiparametric approach, named nonparanormalskeptic, for efficiently and robustly estimating high dimensional undirectedgraphical models. To achieve modeling flexibility, we consider Gaussian Copulagraphical models (or the nonparanormal) as proposed by Liu et al. (2009). Toachieve estimation robustness, we exploit nonparametric rank-based correlationcoefficient estimators, including Spearman's rho and Kendall's tau. In highdimensional settings, we prove that the nonparanormal skeptic achieves theoptimal parametric rate of convergence in both graph and parameter estimation.This celebrating result suggests that the Gaussian copula graphical models canbe used as a safe replacement of the popular Gaussian graphical models, evenwhen the data are truly Gaussian. Besides theoretical analysis, we also conductthorough numerical simulations to compare different estimators for their graphrecovery performance under both ideal and noisy settings. The proposed methodsare then applied on a large-scale genomic dataset to illustrate their empiricalusefulness. The R language software package huge implementing the proposedmethods is available on the Comprehensive R Archive Network: http://cran.r-project.org/.
arxiv-600-260 | Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers | http://arxiv.org/pdf/1202.2160v2.pdf | author:Clément Farabet, Camille Couprie, Laurent Najman, Yann LeCun category:cs.CV cs.LG published:2012-02-10 summary:Scene parsing, or semantic segmentation, consists in labeling each pixel inan image with the category of the object it belongs to. It is a challengingtask that involves the simultaneous detection, segmentation and recognition ofall the objects in the image. The scene parsing method proposed here starts by computing a tree of segmentsfrom a graph of pixel dissimilarities. Simultaneously, a set of dense featurevectors is computed which encodes regions of multiple sizes centered on eachpixel. The feature extractor is a multiscale convolutional network trained fromraw pixels. The feature vectors associated with the segments covered by eachnode in the tree are aggregated and fed to a classifier which produces anestimate of the distribution of object categories contained in the segment. Asubset of tree nodes that cover the image are then selected so as to maximizethe average "purity" of the class distributions, hence maximizing the overalllikelihood that each segment will contain a single object. The convolutionalnetwork feature extractor is trained end-to-end from raw pixels, alleviatingthe need for engineered features. After training, the system is parameter free. The system yields record accuracies on the Stanford Background Dataset (8classes), the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170classes) while being an order of magnitude faster than competing approaches,producing a 320 \times 240 image labeling in less than 1 second.
arxiv-600-261 | Efficient statistical classification of satellite measurements | http://arxiv.org/pdf/1202.2194v4.pdf | author:Peter Mills category:physics.ao-ph stat.ML published:2012-02-10 summary:Supervised statistical classification is a vital tool for satellite imageprocessing. It is useful not only when a discrete result, such as featureextraction or surface type, is required, but also for continuum retrievals bydividing the quantity of interest into discrete ranges. Because of the highresolution of modern satellite instruments and because of the requirement forreal-time processing, any algorithm has to be fast to be useful. Here wedescribe an algorithm based on kernel estimation called Adaptive GaussianFiltering that incorporates several innovations to produce superior efficiencyas compared to three other popular methods: k-nearest-neighbour (KNN), LearningVector Quantization (LVQ) and Support Vector Machines (SVM). This efficiency isgained with no compromises: accuracy is maintained, while estimates of theconditional probabilities are returned. These are useful not only to gauge theaccuracy of an estimate in the absence of its true value, but also tore-calibrate a retrieved image and as a proxy for a discretized continuumvariable. The algorithm is demonstrated and compared with the other three on apair of synthetic test classes and to map the waterways of the Netherlands.Software may be found at: http://libagf.sourceforge.net.
arxiv-600-262 | Compressed Beamforming in Ultrasound Imaging | http://arxiv.org/pdf/1202.6037v2.pdf | author:Noam Wagner, Yonina C. Eldar, Zvi Friedman category:cs.IT cs.CV math.IT published:2012-02-09 summary:Emerging sonography techniques often require increasing the number oftransducer elements involved in the imaging process. Consequently, largeramounts of data must be acquired and processed. The significant growth in theamounts of data affects both machinery size and power consumption. Within theclassical sampling framework, state of the art systems reduce processing ratesby exploiting the bandpass bandwidth of the detected signals. It has beenrecently shown, that a much more significant sample-rate reduction may beobtained, by treating ultrasound signals within the Finite Rate of Innovationframework. These ideas follow the spirit of Xampling, which combines classicmethods from sampling theory with recent developments in Compressed Sensing.Applying such low-rate sampling schemes to individual transducer elements,which detect energy reflected from biological tissues, is limited by the noisynature of the signals. This often results in erroneous parameter extraction,bringing forward the need to enhance the SNR of the low-rate samples. In ourwork, we achieve SNR enhancement, by beamforming the sub-Nyquist samplesobtained from multiple elements. We refer to this process as "compressedbeamforming". Applying it to cardiac ultrasound data, we successfully imagemacroscopic perturbations, while achieving a nearly eight-fold reduction insample-rate, compared to standard techniques.
arxiv-600-263 | A quantum genetic algorithm with quantum crossover and mutation operations | http://arxiv.org/pdf/1202.2026v5.pdf | author:Akira SaiToh, Robabeh Rahimi, Mikio Nakahara category:cs.NE quant-ph 68Q12 published:2012-02-09 summary:In the context of evolutionary quantum computing in the literal meaning, aquantum crossover operation has not been introduced so far. Here, we introducea novel quantum genetic algorithm which has a quantum crossover procedureperforming crossovers among all chromosomes in parallel for each generation. Acomplexity analysis shows that a quadratic speedup is achieved over itsclassical counterpart in the dominant factor of the run time to handle eachgeneration.
arxiv-600-264 | Signal Recovery on Incoherent Manifolds | http://arxiv.org/pdf/1202.1595v2.pdf | author:Chinmay Hegde, Richard G. Baraniuk category:cs.IT math.IT stat.ML published:2012-02-08 summary:Suppose that we observe noisy linear measurements of an unknown signal thatcan be modeled as the sum of two component signals, each of which arises from anonlinear sub-manifold of a high dimensional ambient space. We introduce SPIN,a first order projected gradient method to recover the signal components.Despite the nonconvex nature of the recovery problem and the possibility ofunderdetermined measurements, SPIN provably recovers the signal components,provided that the signal manifolds are incoherent and that the measurementoperator satisfies a certain restricted isometry property. SPIN significantlyextends the scope of current recovery models and algorithms for low dimensionallinear inverse problems and matches (or exceeds) the current state of the artin terms of performance.
arxiv-600-265 | A Polynomial Time Approximation Scheme for a Single Machine Scheduling Problem Using a Hybrid Evolutionary Algorithm | http://arxiv.org/pdf/1202.1708v2.pdf | author:Boris Mitavskiy, Jun He category:cs.NE published:2012-02-08 summary:Nowadays hybrid evolutionary algorithms, i.e, heuristic search algorithmscombining several mutation operators some of which are meant to implementstochastically a well known technique designed for the specific problem inquestion while some others playing the role of random search, have becomerather popular for tackling various NP-hard optimization problems. Whileempirical studies demonstrate that hybrid evolutionary algorithms arefrequently successful at finding solutions having fitness sufficiently close tothe optimal, many fewer articles address the computational complexity in amathematically rigorous fashion. This paper is devoted to a mathematicallymotivated design and analysis of a parameterized family of evolutionaryalgorithms which provides a polynomial time approximation scheme for one of thewell-known NP-hard combinatorial optimization problems, namely the "singlemachine scheduling problem without precedence constraints". The authors hopethat the techniques and ideas developed in this article may be applied in manyother situations.
arxiv-600-266 | Beyond Sentiment: The Manifold of Human Emotions | http://arxiv.org/pdf/1202.1568v2.pdf | author:Seungyeon Kim, Fuxin Li, Guy Lebanon, Irfan Essa category:cs.CL published:2012-02-08 summary:Sentiment analysis predicts the presence of positive or negative emotions ina text document. In this paper we consider higher dimensional extensions of thesentiment concept, which represent a richer set of human emotions. Our approachgoes beyond previous work in that our model contains a continuous manifoldrather than a finite set of human emotions. We investigate the resulting model,compare it to psychological observations, and explore its predictivecapabilities. Besides obtaining significant improvements over a baselinewithout manifold, we are also able to visualize different notions of positivesentiment in different domains.
arxiv-600-267 | Message-Passing Algorithms for Channel Estimation and Decoding Using Approximate Inference | http://arxiv.org/pdf/1202.1467v2.pdf | author:Mihai-Alin Badiu, Gunvor Elisabeth Kirkelund, Carles Navarro Manchón, Erwin Riegler, Bernard Henri Fleury category:cs.IT math.IT stat.ML published:2012-02-07 summary:We design iterative receiver schemes for a generic wireless communicationsystem by treating channel estimation and information decoding as an inferenceproblem in graphical models. We introduce a recently proposed inferenceframework that combines belief propagation (BP) and the mean field (MF)approximation and includes these algorithms as special cases. We also show thatthe expectation propagation and expectation maximization algorithms can beembedded in the BP-MF framework with slight modifications. By applying theconsidered inference algorithms to our probabilistic model, we derive fourdifferent message-passing receiver schemes. Our numerical evaluationdemonstrates that the receiver based on the BP-MF framework and its variantbased on BP-EM yield the best compromise between performance, computationalcomplexity and numerical stability among all candidate algorithms.
arxiv-600-268 | Fully Automatic Expression-Invariant Face Correspondence | http://arxiv.org/pdf/1202.1444v2.pdf | author:Augusto Salazar, Stefanie Wuhrer, Chang Shu, Flavio Prieto category:cs.CV cs.GR published:2012-02-07 summary:We consider the problem of computing accurate point-to-point correspondencesamong a set of human face scans with varying expressions. Our fully automaticapproach does not require any manually placed markers on the scan. Instead, theapproach learns the locations of a set of landmarks present in a database anduses this knowledge to automatically predict the locations of these landmarkson a newly available scan. The predicted landmarks are then used to computepoint-to-point correspondences between a template model and the newly availablescan. To accurately fit the expression of the template to the expression of thescan, we use as template a blendshape model. Our algorithm was tested on adatabase of human faces of different ethnic groups with strongly varyingexpressions. Experimental results show that the obtained point-to-pointcorrespondence is both highly accurate and consistent for most of the tested 3Dface models.
arxiv-600-269 | rFerns: An Implementation of the Random Ferns Method for General-Purpose Machine Learning | http://arxiv.org/pdf/1202.1121v2.pdf | author:Miron B. Kursa category:cs.LG stat.ML published:2012-02-06 summary:In this paper I present an extended implementation of the Random fernsalgorithm contained in the R package rFerns. It differs from the original bythe ability of consuming categorical and numerical attributes instead of onlybinary ones. Also, instead of using simple attribute subspace ensemble itemploys bagging and thus produce error approximation and variable importancemeasure modelled after Random forest algorithm. I also present benchmarks'results which show that although Random ferns' accuracy is mostly smaller thanachieved by Random forest, its speed and good quality of importance measure itprovides make rFerns a reasonable choice for a specific applications.
arxiv-600-270 | Cramer Rao-Type Bounds for Sparse Bayesian Learning | http://arxiv.org/pdf/1202.1119v2.pdf | author:Ranjitha Prasad, Chandra R. Murthy category:cs.LG stat.ML published:2012-02-06 summary:In this paper, we derive Hybrid, Bayesian and Marginalized Cram\'{e}r-Raolower bounds (HCRB, BCRB and MCRB) for the single and multiple measurementvector Sparse Bayesian Learning (SBL) problem of estimating compressiblevectors and their prior distribution parameters. We assume the unknown vectorto be drawn from a compressible Student-t prior distribution. We derive CRBsthat encompass the deterministic or random nature of the unknown parameters ofthe prior distribution and the regression noise variance. We extend the MCRB tothe case where the compressible vector is distributed according to a generalcompressible prior distribution, of which the generalized Pareto distributionis a special case. We use the derived bounds to uncover the relationshipbetween the compressibility and Mean Square Error (MSE) in the estimates.Further, we illustrate the tightness and utility of the bounds throughsimulations, by comparing them with the MSE performance of two popularSBL-based estimators. It is found that the MCRB is generally the tightest amongthe bounds derived and that the MSE performance of the Expectation-Maximization(EM) algorithm coincides with the MCRB for the compressible vector. Throughsimulations, we demonstrate the dependence of the MSE performance of SBL basedestimators on the compressibility of the vector for several values of thenumber of observations and at different signal powers.
arxiv-600-271 | Lossy Compression via Sparse Linear Regression: Performance under Minimum-distance Encoding | http://arxiv.org/pdf/1202.0840v4.pdf | author:Ramji Venkataramanan, Antony Joseph, Sekhar Tatikonda category:cs.IT math.IT stat.ML published:2012-02-03 summary:We study a new class of codes for lossy compression with the squared-errordistortion criterion, designed using the statistical framework ofhigh-dimensional linear regression. Codewords are linear combinations ofsubsets of columns of a design matrix. Called a Sparse Superposition or SparseRegression codebook, this structure is motivated by an analogous constructionproposed recently by Barron and Joseph for communication over an AWGN channel.For i.i.d Gaussian sources and minimum-distance encoding, we show that such acode can attain the Shannon rate-distortion function with the optimal errorexponent, for all distortions below a specified value. It is also shown thatsparse regression codes are robust in the following sense: a codebook designedto compress an i.i.d Gaussian source of variance $\sigma^2$ with(squared-error) distortion $D$ can compress any ergodic source of variance lessthan $\sigma^2$ to within distortion $D$. Thus the sparse regression ensembleretains many of the good covering properties of the i.i.d random Gaussianensemble, while having having a compact representation in terms of a matrixwhose size is a low-order polynomial in the block-length.
arxiv-600-272 | High-Dimensional Feature Selection by Feature-Wise Non-Linear Lasso | http://arxiv.org/pdf/1202.0515v3.pdf | author:Makoto Yamada, Wittawat Jitkrittum, Leonid Sigal, Eric P. Xing, Masashi Sugiyama category:stat.ML cs.AI stat.ME published:2012-02-02 summary:The goal of supervised feature selection is to find a subset of inputfeatures that are responsible for predicting output values. The least absoluteshrinkage and selection operator (Lasso) allows computationally efficientfeature selection based on linear dependency between input features and outputvalues. In this paper, we consider a feature-wise kernelized Lasso forcapturing non-linear input-output dependency. We first show that, withparticular choices of kernel functions, non-redundant features with strongstatistical dependence on output values can be found in terms of kernel-basedindependence measures. We then show that the globally optimal solution can beefficiently computed; this makes the approach scalable to high-dimensionalproblems. The effectiveness of the proposed method is demonstrated throughfeature selection experiments with thousands of features.
arxiv-600-273 | Kernels on Sample Sets via Nonparametric Divergence Estimates | http://arxiv.org/pdf/1202.0302v2.pdf | author:Dougal J. Sutherland, Liang Xiong, Barnabás Póczos, Jeff Schneider category:cs.LG stat.ML published:2012-02-01 summary:Most machine learning algorithms, such as classification or regression, treatthe individual data point as the object of interest. Here we consider extendingmachine learning algorithms to operate on groups of data points. We suggesttreating a group of data points as an i.i.d. sample set from an underlyingfeature distribution for that group. Our approach employs kernel machines witha kernel on i.i.d. sample sets of vectors. We define certain kernel functionson pairs of distributions, and then use a nonparametric estimator toconsistently estimate those functions based on sample sets. The projection ofthe estimated Gram matrix to the cone of symmetric positive semi-definitematrices enables us to use kernel machines for classification, regression,anomaly detection, and low-dimensional embedding in the space of distributions.We present several numerical experiments both on real and simulated datasets todemonstrate the advantages of our new approach.
arxiv-600-274 | Statistical analysis of emotions and opinions at Digg website | http://arxiv.org/pdf/1201.5484v2.pdf | author:Piotr Pohorecki, Julian Sienkiewicz, Marija Mitrovic, Georgios Paltoglou, Janusz A. Holyst category:physics.soc-ph cs.CL cs.SI published:2012-01-26 summary:We performed statistical analysis on data from the Digg.com website, whichenables its users to express their opinion on news stories by taking part inforum-like discussions as well as directly evaluate previous posts and storiesby assigning so called "diggs". Owing to fact that the content of each post hasbeen annotated with its emotional value, apart from the strictly structuralproperties, the study also includes an analysis of the average emotionalresponse of the posts commenting the main story. While analysing correlationsat the story level, an interesting relationship between the number of diggs andthe number of comments received by a story was found. The correlation betweenthe two quantities is high for data where small threads dominate andconsistently decreases for longer threads. However, while the correlation ofthe number of diggs and the average emotional response tends to grow for longerthreads, correlations between numbers of comments and the average emotionalresponse are almost zero. We also show that the initial set of comments givento a story has a substantial impact on the further "life" of the discussion:high negative average emotions in the first 10 comments lead to longer threadswhile the opposite situation results in shorter discussions. We also suggestpresence of two different mechanisms governing the evolution of the discussionand, consequently, its length.
arxiv-600-275 | Discrete and fuzzy dynamical genetic programming in the XCSF learning classifier system | http://arxiv.org/pdf/1201.5604v2.pdf | author:Richard J. Preen, Larry Bull category:cs.AI cs.LG cs.NE cs.SY math.OC published:2012-01-26 summary:A number of representation schemes have been presented for use withinlearning classifier systems, ranging from binary encodings to neural networks.This paper presents results from an investigation into using discrete and fuzzydynamical system representations within the XCSF learning classifier system. Inparticular, asynchronous random Boolean networks are used to represent thetraditional condition-action production system rules in the discrete case andasynchronous fuzzy logic networks in the continuous-valued case. It is shownpossible to use self-adaptive, open-ended evolution to design an ensemble ofsuch dynamical systems within XCSF to solve a number of well-known testproblems.
arxiv-600-276 | On Constrained Spectral Clustering and Its Applications | http://arxiv.org/pdf/1201.5338v2.pdf | author:Xiang Wang, Buyue Qian, Ian Davidson category:cs.LG stat.ML H.2.8 published:2012-01-25 summary:Constrained clustering has been well-studied for algorithms such as $K$-meansand hierarchical clustering. However, how to satisfy many constraints in thesealgorithmic settings has been shown to be intractable. One alternative toencode many constraints is to use spectral clustering, which remains adeveloping area. In this paper, we propose a flexible framework for constrainedspectral clustering. In contrast to some previous efforts that implicitlyencode Must-Link and Cannot-Link constraints by modifying the graph Laplacianor constraining the underlying eigenspace, we present a more natural andprincipled formulation, which explicitly encodes the constraints as part of aconstrained optimization problem. Our method offers several practicaladvantages: it can encode the degree of belief in Must-Link and Cannot-Linkconstraints; it guarantees to lower-bound how well the given constraints aresatisfied using a user-specified threshold; it can be solved deterministicallyin polynomial time through generalized eigendecomposition. Furthermore, byinheriting the objective function from spectral clustering and encoding theconstraints explicitly, much of the existing analysis of unconstrained spectralclustering techniques remains valid for our formulation. We validate theeffectiveness of our approach by empirical results on both artificial and realdatasets. We also demonstrate an innovative use of encoding large number ofconstraints: transfer learning via constraints.
arxiv-600-277 | An Efficient Primal-Dual Prox Method for Non-Smooth Optimization | http://arxiv.org/pdf/1201.5283v5.pdf | author:Tianbao Yang, Mehrdad Mahdavi, Rong Jin, Shenghuo Zhu category:cs.LG published:2012-01-24 summary:We study the non-smooth optimization problems in machine learning, where boththe loss function and the regularizer are non-smooth functions. Previousstudies on efficient empirical loss minimization assume either a smooth lossfunction or a strongly convex regularizer, making them unsuitable fornon-smooth optimization. We develop a simple yet efficient method for a familyof non-smooth optimization problems where the dual form of the loss function isbilinear in primal and dual variables. We cast a non-smooth optimizationproblem into a minimax optimization problem, and develop a primal dual proxmethod that solves the minimax optimization problem at a rate of $O(1/T)${assuming that the proximal step can be efficiently solved}, significantlyfaster than a standard subgradient descent method that has an $O(1/\sqrt{T})$convergence rate. Our empirical study verifies the efficiency of the proposedmethod for various non-smooth optimization problems that arise ubiquitously inmachine learning by comparing it to the state-of-the-art first order methods.
arxiv-600-278 | A probabilistic methodology for multilabel classification | http://arxiv.org/pdf/1201.4777v2.pdf | author:Alfonso E. Romero, Luis M. de Campos category:cs.AI cs.LG 68T37, 68T10 published:2012-01-23 summary:Multilabel classification is a relatively recent subfield of machinelearning. Unlike to the classical approach, where instances are labeled withonly one category, in multilabel classification, an arbitrary number ofcategories is chosen to label an instance. Due to the problem complexity (thesolution is one among an exponential number of alternatives), a very commonsolution (the binary method) is frequently used, learning a binary classifierfor every category, and combining them all afterwards. The assumption taken inthis solution is not realistic, and in this work we give examples where thedecisions for all the labels are not taken independently, and thus, asupervised approach should learn those existing relationships among categoriesto make a better classification. Therefore, we show here a generic methodologythat can improve the results obtained by a set of independent probabilisticbinary classifiers, by using a combination procedure with a classifier trainedon the co-occurrences of the labels. We show an exhaustive experimentation inthree different standard corpora of labeled documents (Reuters-21578,Ohsumed-23 and RCV1), which present noticeable improvements in all of them,when using our methodology, in three probabilistic base classifiers.
arxiv-600-279 | Compressive Acquisition of Dynamic Scenes | http://arxiv.org/pdf/1201.4895v2.pdf | author:Aswin C Sankaranarayanan, Pavan K Turaga, Rama Chellappa, Richard G Baraniuk category:cs.CV published:2012-01-23 summary:Compressive sensing (CS) is a new approach for the acquisition and recoveryof sparse signals and images that enables sampling rates significantly belowthe classical Nyquist rate. Despite significant progress in the theory andmethods of CS, little headway has been made in compressive video acquisitionand recovery. Video CS is complicated by the ephemeral nature of dynamicevents, which makes direct extensions of standard CS imaging architectures andsignal models difficult. In this paper, we develop a new framework for video CSfor dynamic textured scenes that models the evolution of the scene as a lineardynamical system (LDS). This reduces the video recovery problem to firstestimating the model parameters of the LDS from compressive measurements, andthen reconstructing the image frames. We exploit the low-dimensional dynamicparameters (the state sequence) and high-dimensional static parameters (theobservation matrix) of the LDS to devise a novel compressive measurementstrategy that measures only the dynamic part of the scene at each instant andaccumulates measurements over time to estimate the static parameters. Thisenables us to lower the compressive measurement rate considerably. We validateour approach with a range of experiments involving both video recovery, sensinghyper-spectral data, and classification of dynamic scenes from compressivedata. Together, these applications demonstrate the effectiveness of theapproach.
arxiv-600-280 | On the Prior and Posterior Distributions Used in Graphical Modelling | http://arxiv.org/pdf/1201.4058v2.pdf | author:Marco Scutari category:math.ST stat.ML stat.TH published:2012-01-19 summary:Graphical model learning and inference are often performed using Bayesiantechniques. In particular, learning is usually performed in two separate steps.First, the graph structure is learned from the data; then the parameters of themodel are estimated conditional on that graph structure. While the probabilitydistributions involved in this second step have been studied in depth, the onesused in the first step have not been explored in as much detail. In this paper, we will study the prior and posterior distributions definedover the space of the graph structures for the purpose of learning thestructure of a graphical model. In particular, we will provide acharacterisation of the behaviour of those distributions as a function of thepossible edges of the graph. We will then use the properties resulting fromthis characterisation to define measures of structural variability for bothBayesian and Markov networks, and we will point out some of their possibleapplications.
arxiv-600-281 | Split HMC for Gaussian Process Models | http://arxiv.org/pdf/1201.3973v2.pdf | author:Shiwei Lan, Babak Shahbaba category:stat.CO stat.ML published:2012-01-19 summary:In this paper, we discuss an extension of the Split Hamiltonian Monte Carlo(Split HMC) method for Gaussian process model (GPM). This method is based onsplitting the Hamiltonian in a way that allows much of the movement around thestate space to be done at low computational cost. To this end, we approximatethe negative log density (i.e., the energy function) of the distribution ofinterest by a quadratic function U0 for which Hamiltonian dynamics can besolved analytically. The overall energy function U is then written as U0 + U1,where U1 is the approximation error. The Hamiltonian is then split into twoparts; one part is based on U0 is handled analytically, the other part is basedon U1 for which we approximate Hamiltonian's equations by discretizing time. Weuse simulated and real data to compare the performance of our method to thestandard HMC. We find that splitting the Hamiltonian for GP models could leadto substantial improvement (up to 10 folds) of sampling efficiency, which ismeasured in terms of the amount of time required for producing an independentsample with high acceptance probability from posterior distributions.
arxiv-600-282 | Ultrametric Model of Mind, II: Application to Text Content Analysis | http://arxiv.org/pdf/1201.2719v3.pdf | author:Fionn Murtagh category:cs.AI cs.CL 68T01 published:2012-01-13 summary:In a companion paper, Murtagh (2012), we discussed how Matte Blanco's worklinked the unrepressed unconscious (in the human) to symmetric logic andthought processes. We showed how ultrametric topology provides a most usefulrepresentational and computational framework for this. Now we look at theextent to which we can find ultrametricity in text. We use coherent andmeaningful collections of nearly 1000 texts to show how we can measure inherentultrametricity. On the basis of our findings we hypothesize that inherentultrametricty is a basis for further exploring unconscious thought processes.
arxiv-600-283 | Autonomous Cleaning of Corrupted Scanned Documents - A Generative Modeling Approach | http://arxiv.org/pdf/1201.2605v2.pdf | author:Zhenwen Dai, Jörg Lücke category:cs.CV cs.LG published:2012-01-12 summary:We study the task of cleaning scanned text documents that are stronglycorrupted by dirt such as manual line strokes, spilled ink etc. We aim atautonomously removing dirt from a single letter-size page based only on theinformation the page contains. Our approach, therefore, has to learn characterrepresentations without supervision and requires a mechanism to distinguishlearned representations from irregular patterns. To learn characterrepresentations, we use a probabilistic generative model parameterizing patternfeatures, feature variances, the features' planar arrangements, and patternfrequencies. The latent variables of the model describe pattern class, patternposition, and the presence or absence of individual pattern features. The modelparameters are optimized using a novel variational EM approximation. Afterlearning, the parameters represent, independently of their absolute position,planar feature arrangements and their variances. A quality measure definedbased on the learned representation then allows for an autonomousdiscrimination between regular character patterns and the irregular patternsmaking up the dirt. The irregular patterns can thus be removed to clean thedocument. For a full Latin alphabet we found that a single page does notcontain sufficiently many character examples. However, even if heavilycorrupted by dirt, we show that a page containing a lower number of charactertypes can efficiently and autonomously be cleaned solely based on thestructural regularity of the characters it contains. In different examplesusing characters from different alphabets, we demonstrate generality of theapproach and discuss its implications for future developments.
arxiv-600-284 | Sparse Reward Processes | http://arxiv.org/pdf/1201.2555v2.pdf | author:Christos Dimitrakakis category:cs.LG stat.ML published:2012-01-12 summary:We introduce a class of learning problems where the agent is presented with aseries of tasks. Intuitively, if there is relation among those tasks, then theinformation gained during execution of one task has value for the execution ofanother task. Consequently, the agent is intrinsically motivated to explore itsenvironment beyond the degree necessary to solve the current task it has athand. We develop a decision theoretic setting that generalises standardreinforcement learning tasks and captures this intuition. More precisely, weconsider a multi-stage stochastic game between a learning agent and anopponent. We posit that the setting is a good model for the problem oflife-long learning in uncertain environments, where while resources must bespent learning about currently important tasks, there is also the need toallocate effort towards learning about aspects of the world which are notrelevant at the moment. This is due to the fact that unpredictable futureevents may lead to a change of priorities for the decision maker. Thus, in somesense, the model "explains" the necessity of curiosity. Apart from introducingthe general formalism, the paper provides algorithms. These are evaluatedexperimentally in some exemplary domains. In addition, performance bounds areproven for some cases of this problem.
arxiv-600-285 | A United Image Force for Deformable Models and Direct Transforming Geometric Active Contorus to Snakes by Level Sets | http://arxiv.org/pdf/1201.1571v3.pdf | author:Hongyu Lu, Yutian Wang, Shanglian Bao category:cs.CV published:2012-01-07 summary:A uniform distribution of the image force field around the object fasts theconvergence speed of the segmentation process. However, to achieve this aim, itcauses the force constructed from the heat diffusion model unable to indicatethe object boundaries accurately. The image force based on electrostatic fieldmodel can perform an exact shape recovery. First, this study introduces afusion scheme of these two image forces, which is capable of extracting theobject boundary with high precision and fast speed. Until now, there is nosatisfied analysis about the relationship between Snakes and Geometric ActiveContours (GAC). The second contribution of this study addresses that the GACmodel can be deduced directly from Snakes model. It proves that each term inGAC and Snakes is correspondent and has similar function. However, the twomodels are expressed using different mathematics. Further, since losing theability of rotating the contour, adoption of level sets can limits the usage ofGAC in some circumstances.
arxiv-600-286 | A Thermodynamical Approach for Probability Estimation | http://arxiv.org/pdf/1201.1384v2.pdf | author:Takashi Isozaki category:cs.LG stat.ME published:2012-01-06 summary:The issue of discrete probability estimation for samples of small size isaddressed in this study. The maximum likelihood method often suffersover-fitting when insufficient data is available. Although the Bayesianapproach can avoid over-fitting by using prior distributions, it still hasproblems with objective analysis. In response to these drawbacks, a newtheoretical framework based on thermodynamics, where energy and temperature areintroduced, was developed. Entropy and likelihood are placed at the center ofthis method. The key principle of inference for probability mass functions isthe minimum free energy, which is shown to unify the two principles of maximumlikelihood and maximum entropy. Our method can robustly estimate probabilityfunctions from small size data.
arxiv-600-287 | Extension of SBL Algorithms for the Recovery of Block Sparse Signals with Intra-Block Correlation | http://arxiv.org/pdf/1201.0862v5.pdf | author:Zhilin Zhang, Bhaskar D. Rao category:stat.ML stat.ME published:2012-01-04 summary:We examine the recovery of block sparse signals and extend the framework intwo important directions; one by exploiting signals' intra-block correlationand the other by generalizing signals' block structure. We propose two familiesof algorithms based on the framework of block sparse Bayesian learning (BSBL).One family, directly derived from the BSBL framework, requires knowledge of theblock structure. Another family, derived from an expanded BSBL framework, isbased on a weaker assumption on the block structure, and can be used when theblock structure is completely unknown. Using these algorithms we show thatexploiting intra-block correlation is very helpful in improving recoveryperformance. These algorithms also shed light on how to modify existingalgorithms or design new ones to exploit such correlation and improveperformance.
arxiv-600-288 | A Topic Modeling Toolbox Using Belief Propagation | http://arxiv.org/pdf/1201.0838v2.pdf | author:Jia Zeng category:cs.LG published:2012-01-04 summary:Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian modelfor probabilistic topic modeling, which attracts worldwide interests andtouches on many important applications in text mining, computer vision andcomputational biology. This paper introduces a topic modeling toolbox (TMBP)based on the belief propagation (BP) algorithms. TMBP toolbox is implemented byMEX C++/Matlab/Octave for either Windows 7 or Linux. Compared with existingtopic modeling packages, the novelty of this toolbox lies in the BP algorithmsfor learning LDA-based topic models. The current version includes BP algorithmsfor latent Dirichlet allocation (LDA), author-topic models (ATM), relationaltopic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing projectand more BP-based algorithms for various topic models will be added in the nearfuture. Interested users may also extend BP algorithms for learning morecomplicated topic models. The source codes are freely available under the GNUGeneral Public Licence, Version 1.0 at https://mloss.org/software/view/399/.
arxiv-600-289 | Sparse Nonparametric Graphical Models | http://arxiv.org/pdf/1201.0794v2.pdf | author:John Lafferty, Han Liu, Larry Wasserman category:stat.ML cs.LG stat.ME published:2012-01-04 summary:We present some nonparametric methods for graphical modeling. In the discretecase, where the data are binary or drawn from a finite alphabet, Markov randomfields are already essentially nonparametric, since the cliques can take only afinite number of values. Continuous data are different. The Gaussian graphicalmodel is the standard parametric model for continuous data, but it makesdistributional assumptions that are often unrealistic. We discuss twoapproaches to building more flexible graphical models. One allows arbitrarygraphs and a nonparametric extension of the Gaussian; the other uses kerneldensity estimation and restricts the graphs to trees and forests. Examples ofboth methods are presented. We also discuss possible future research directionsfor nonparametric graphical modeling.
arxiv-600-290 | Learning joint intensity-depth sparse representations | http://arxiv.org/pdf/1201.0566v2.pdf | author:Ivana Tosic, Sarah Drewes category:cs.CV published:2012-01-03 summary:This paper presents a method for learning overcomplete dictionaries composedof two modalities that describe a 3D scene: image intensity and scene depth. Wepropose a novel Joint Basis Pursuit (JBP) algorithm that finds related sparsefeatures in two modalities using conic programming and integrate it into atwo-step dictionary learning algorithm. JBP differs from related convexalgorithms because it finds joint sparsity models with different atoms anddifferent coefficient values for intensity and depth. This is crucial forrecovering generative models where the same sparse underlying causes (3Dfeatures) give rise to different signals (intensity and depth). We give atheoretical bound for the sparse coefficient recovery error obtained by JBP,and show experimentally that JBP is far superior to the state of the art GroupLasso algorithm. When applied to the Middlebury depth-intensity database, ourlearning algorithm converges to a set of related features, such as pairs ofdepth and intensity edges or image textures and depth slants. Finally, we showthat the learned dictionary and JBP achieve the state of the art depthinpainting performance on time-of-flight 3D data.
arxiv-600-291 | Scikit-learn: Machine Learning in Python | http://arxiv.org/pdf/1201.0490v2.pdf | author:Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, Édouard Duchesnay category:cs.LG cs.MS published:2012-01-02 summary:Scikit-learn is a Python module integrating a wide range of state-of-the-artmachine learning algorithms for medium-scale supervised and unsupervisedproblems. This package focuses on bringing machine learning to non-specialistsusing a general-purpose high-level language. Emphasis is put on ease of use,performance, documentation, and API consistency. It has minimal dependenciesand is distributed under the simplified BSD license, encouraging its use inboth academic and commercial settings. Source code, binaries, and documentationcan be downloaded from http://scikit-learn.sourceforge.net.
arxiv-600-292 | Building high-level features using large scale unsupervised learning | http://arxiv.org/pdf/1112.6209v5.pdf | author:Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, Andrew Y. Ng category:cs.LG published:2011-12-29 summary:We consider the problem of building high- level, class-specific featuredetectors from only unlabeled data. For example, is it possible to learn a facedetector using only unlabeled images? To answer this, we train a 9-layeredlocally connected sparse autoencoder with pooling and local contrastnormalization on a large dataset of images (the model has 1 bil- lionconnections, the dataset has 10 million 200x200 pixel images downloaded fromthe Internet). We train this network using model parallelism and asynchronousSGD on a clus- ter with 1,000 machines (16,000 cores) for three days. Contraryto what appears to be a widely-held intuition, our experimental re- sultsreveal that it is possible to train a face detector without having to labelimages as containing a face or not. Control experiments show that this featuredetector is robust not only to translation but also to scaling and out-of-planerotation. We also find that the same network is sensitive to other high-levelconcepts such as cat faces and human bod- ies. Starting with these learnedfeatures, we trained our network to obtain 15.8% accu- racy in recognizing20,000 object categories from ImageNet, a leap of 70% relative im- provementover the previous state-of-the-art.
arxiv-600-293 | Sparse Recovery from Nonlinear Measurements with Applications in Bad Data Detection for Power Networks | http://arxiv.org/pdf/1112.6234v2.pdf | author:Weiyu Xu, Meng Wang, Jianfeng Cai, Ao Tang category:cs.IT cs.LG cs.SY math.IT published:2011-12-29 summary:In this paper, we consider the problem of sparse recovery from nonlinearmeasurements, which has applications in state estimation and bad data detectionfor power networks. An iterative mixed $\ell_1$ and $\ell_2$ convex program isused to estimate the true state by locally linearizing the nonlinearmeasurements. When the measurements are linear, through using the almostEuclidean property for a linear subspace, we derive a new performance bound forthe state estimation error under sparse bad data and additive observationnoise. As a byproduct, in this paper we provide sharp bounds on the almostEuclidean property of a linear subspace, using the "escape-through-the-mesh"theorem from geometric functional analysis. When the measurements arenonlinear, we give conditions under which the solution of the iterativealgorithm converges to the true state even though the locally linearizedmeasurements may not be the actual nonlinear measurements. We numericallyevaluate our iterative convex programming approach to perform bad datadetections in nonlinear electrical power networks problems. We are able to usesemidefinite programming to verify the conditions for convergence of theproposed iterative sparse recovery algorithms from nonlinear measurements.
arxiv-600-294 | Multispectral Palmprint Recognition Using a Hybrid Feature | http://arxiv.org/pdf/1112.5997v3.pdf | author:Sina Akbari Mistani, Shervin Minaee, Emad Fatemizadeh category:cs.CV published:2011-12-27 summary:Personal identification problem has been a major field of research in recentyears. Biometrics-based technologies that exploit fingerprints, iris, face,voice and palmprints, have been in the center of attention to solve thisproblem. Palmprints can be used instead of fingerprints that have been of theearliest of these biometrics technologies. A palm is covered with the same skinas the fingertips but has a larger surface, giving us more information than thefingertips. The major features of the palm are palm-lines, including principallines, wrinkles and ridges. Using these lines is one of the most popularapproaches towards solving the palmprint recognition problem. Another robustfeature is the wavelet energy of palms. In this paper we used a hybrid featurewhich combines both of these features. %Moreover, multispectral analysis isapplied to improve the performance of the system. At the end, minimum distanceclassifier is used to match test images with one of the training samples. Theproposed algorithm has been tested on a well-known multispectral palmprintdataset and achieved an average accuracy of 98.8\%.
arxiv-600-295 | Search space analysis with Wang-Landau sampling and slow adaptive walks | http://arxiv.org/pdf/1112.5980v2.pdf | author:Susan Khor category:cs.NE published:2011-12-27 summary:Two complementary techniques for analyzing search spaces are proposed: (i) analgorithm to detect search points with potential to be local optima; and (ii) aslightly adjusted Wang-Landau sampling algorithm to explore larger searchspaces. The detection algorithm assumes that local optima are points which areeasier to reach and harder to leave by a slow adaptive walker. A slow adaptivewalker moves to a nearest fitter point. Thus, points with larger outgoing stepsizes relative to incoming step sizes are marked using the local optima scoreformulae as potential local optima points (PLOPs). Defining local optima inthese more general terms allows their detection within the closure of a subsetof a search space, and the sampling of a search space unshackled by aparticular move set. Tests are done with NK and HIFF problems to confirm thatPLOPs detected in the manner proposed retain characteristics of local optima,and that the adjusted Wang-Landau samples are more representative of the searchspace than samples produced by choosing points uniformly at random. While ourapproach shows promise, more needs to be done to reduce its computation costthat it may pave a way toward analyzing larger search spaces of practicalmeaning.
arxiv-600-296 | Learning Smooth Pattern Transformation Manifolds | http://arxiv.org/pdf/1112.5640v5.pdf | author:Elif Vural, Pascal Frossard category:cs.CV published:2011-12-23 summary:Manifold models provide low-dimensional representations that are useful forprocessing and analyzing data in a transformation-invariant way. In this paper,we study the problem of learning smooth pattern transformation manifolds fromimage sets that represent observations of geometrically transformed signals. Inorder to construct a manifold, we build a representative pattern whosetransformations accurately fit various input images. We examine two objectivesof the manifold building problem, namely, approximation and classification. Forthe approximation problem, we propose a greedy method that constructs arepresentative pattern by selecting analytic atoms from a continuous dictionarymanifold. We present a DC (Difference-of-Convex) optimization scheme that isapplicable to a wide range of transformation and dictionary models, anddemonstrate its application to transformation manifolds generated by rotation,translation and anisotropic scaling of a reference pattern. Then, we generalizethis approach to a setting with multiple transformation manifolds, where eachmanifold represents a different class of signals. We present an iterativemultiple manifold building algorithm such that the classification accuracy ispromoted in the learning of the representative patterns. Experimental resultssuggest that the proposed methods yield high accuracy in the approximation andclassification of data compared to some reference methods, while the invarianceto geometric transformations is achieved due to the transformation manifoldmodel.
arxiv-600-297 | A Study on Using Uncertain Time Series Matching Algorithms in MapReduce Applications | http://arxiv.org/pdf/1112.5505v5.pdf | author:Nikzad Babaii Rizvandi, Javid Taheri, Albert Y. Zomaya, Reza Moraveji category:cs.DC cs.AI cs.LG cs.PF published:2011-12-23 summary:In this paper, we study CPU utilization time patterns of several Map-Reduceapplications. After extracting running patterns of several applications, thepatterns with their statistical information are saved in a reference databaseto be later used to tweak system parameters to efficiently execute unknownapplications in future. To achieve this goal, CPU utilization patterns of newapplications along with its statistical information are compared with thealready known ones in the reference database to find/predict their mostprobable execution patterns. Because of different patterns lengths, the DynamicTime Warping (DTW) is utilized for such comparison; a statistical analysis isthen applied to DTWs' outcomes to select the most suitable candidates.Moreover, under a hypothesis, another algorithm is proposed to classifyapplications under similar CPU utilization patterns. Three widely used textprocessing applications (WordCount, Distributed Grep, and Terasort) and anotherapplication (Exim Mainlog parsing) are used to evaluate our hypothesis intweaking system parameters in executing similar applications. Results were verypromising and showed effectiveness of our approach on 5-node Map-Reduceplatform
arxiv-600-298 | Spatio-temporal wavelet regularization for parallel MRI reconstruction: application to functional MRI | http://arxiv.org/pdf/1201.0022v3.pdf | author:Lotfi Chaari, Sébastien Mériaux, Jean-Christophe Pesquet, Philippe Ciuciu category:stat.AP cs.CV physics.med-ph published:2011-12-23 summary:Parallel MRI is a fast imaging technique that enables the acquisition ofhighly resolved images in space or/and in time. The performance of parallelimaging strongly depends on the reconstruction algorithm, which can proceedeither in the original k-space (GRAPPA, SMASH) or in the image domain(SENSE-like methods). To improve the performance of the widely used SENSEalgorithm, 2D- or slice-specific regularization in the wavelet domain has beendeeply investigated. In this paper, we extend this approach using 3D-waveletrepresentations in order to handle all slices together and addressreconstruction artifacts which propagate across adjacent slices. The gaininduced by such extension (3D-Unconstrained Wavelet Regularized -SENSE:3D-UWR-SENSE) is validated on anatomical image reconstruction where no temporalacquisition is considered. Another important extension accounts for temporalcorrelations that exist between successive scans in functional MRI (fMRI). Inaddition to the case of 2D+t acquisition schemes addressed by some othermethods like kt-FOCUSS, our approach allows us to deal with 3D+t acquisitionschemes which are widely used in neuroimaging. The resulting 3D-UWR-SENSE and4D-UWR-SENSE reconstruction schemes are fully unsupervised in the sense thatall regularization parameters are estimated in the maximum likelihood sense ona reference scan. The gain induced by such extensions is illustrated on bothanatomical and functional image reconstruction, and also measured in terms ofstatistical sensitivity for the 4D-UWR-SENSE approach during a fastevent-related fMRI protocol. Our 4D-UWR-SENSE algorithm outperforms the SENSEreconstruction at the subject and group levels (15 subjects) for differentcontrasts of interest (eg, motor or computation tasks) and using differentparallel acceleration factors (R=2 and R=4) on 2x2x3mm3 EPI images.
arxiv-600-299 | POWERPLAY: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem | http://arxiv.org/pdf/1112.5309v2.pdf | author:Jürgen Schmidhuber category:cs.AI cs.LG published:2011-12-22 summary:Most of computer science focuses on automatically solving given computationalproblems. I focus on automatically inventing or discovering problems in a wayinspired by the playful behavior of animals and humans, to train a more andmore general problem solver from scratch in an unsupervised fashion. Considerthe infinite set of all computable descriptions of tasks with possiblycomputable solutions. The novel algorithmic framework POWERPLAY (2011)continually searches the space of possible pairs of new tasks and modificationsof the current problem solver, until it finds a more powerful problem solverthat provably solves all previously learned tasks plus the new one, while theunmodified predecessor does not. Wow-effects are achieved by continually makingpreviously learned skills more efficient such that they require less time andspace. New skills may (partially) re-use previously learned skills. POWERPLAY'ssearch orders candidate pairs of tasks and solver modifications by theirconditional computational (time & space) complexity, given the storedexperience so far. The new task and its corresponding task-solving skill arethose first found and validated. The computational costs of validating newtasks need not grow with task repertoire size. POWERPLAY's ongoing search fornovelty keeps breaking the generalization abilities of its present solver. Thisis related to Goedel's sequence of increasingly powerful formal theories basedon adding formerly unprovable statements to the axioms without affectingpreviously provable theorems. The continually increasing repertoire of problemsolving procedures can be exploited by a parallel search for solutions toadditional externally posed tasks. POWERPLAY may be viewed as a greedy butpractical implementation of basic principles of creativity. A firstexperimental analysis can be found in separate papers [53,54].
arxiv-600-300 | Combining One-Class Classifiers via Meta-Learning | http://arxiv.org/pdf/1112.5246v3.pdf | author:Eitan Menahem, Lior Rokach, Yuval Elovici category:cs.LG K.3.2 published:2011-12-22 summary:Selecting the best classifier among the available ones is a difficult task,especially when only instances of one class exist. In this work we examine thenotion of combining one-class classifiers as an alternative for selecting thebest classifier. In particular, we propose two new one-class classificationperformance measures to weigh classifiers and show that a simple ensemble thatimplements these measures can outperform the most popular one-class ensembles.Furthermore, we propose a new one-class ensemble scheme, TUPSO, which usesmeta-learning to combine one-class classifiers. Our experiments demonstrate thesuperiority of TUPSO over all other tested ensembles and show that the TUPSOperformance is statistically indistinguishable from that of the hypotheticalbest classifier.
