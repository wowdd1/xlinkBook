arxiv-1308-1507 | Logical analysis of natural language semantics to solve the problem of computer understanding | http://arxiv.org/abs/1308.1507 | id:1308.1507 author:Yuriy Ostapov category:cs.CL  published:2013-08-07 summary:An object--oriented approach to create a natural language understanding system is considered. The understanding program is a formal system built on the base of predicative calculus. Horn's clauses are used as well--formed formulas. An inference is based on the principle of resolution. Sentences of natural language are represented in the view of typical predicate set. These predicates describe physical objects and processes, abstract objects, categories and semantic relations between objects. Predicates for concrete assertions are saved in a database. To describe the semantics of classes for physical objects, abstract concepts and processes, a knowledge base is applied. The proposed representation of natural language sentences is a semantic net. Nodes of such net are typical predicates. This approach is perspective as, firstly, such typification of nodes facilitates essentially forming of processing algorithms and object descriptions, secondly, the effectiveness of algorithms is increased (particularly for the great number of nodes), thirdly, to describe the semantics of words, encyclopedic knowledge is used, and this permits essentially to extend the class of solved problems. version:1
arxiv-1308-1484 | A Multi-Swarm Cellular PSO based on Clonal Selection Algorithm in Dynamic Environments | http://arxiv.org/abs/1308.1484 | id:1308.1484 author:Somayeh Nabizadeh, Alireza Rezvanian, Mohammd Reza Meybodi category:cs.NE cs.AI  published:2013-08-07 summary:Many real-world problems are dynamic optimization problems. In this case, the optima in the environment change dynamically. Therefore, traditional optimization algorithms disable to track and find optima. In this paper, a new multi-swarm cellular particle swarm optimization based on clonal selection algorithm (CPSOC) is proposed for dynamic environments. In the proposed algorithm, the search space is partitioned into cells by a cellular automaton. Clustered particles in each cell, which make a sub-swarm, are evolved by the particle swarm optimization and clonal selection algorithm. Experimental results on Moving Peaks Benchmark demonstrate the superiority of the CPSOC its popular methods. version:1
arxiv-1308-1374 | Bayesian ensemble learning for image denoising | http://arxiv.org/abs/1308.1374 | id:1308.1374 author:Hyuntaek Oh category:cs.CV  published:2013-08-06 summary:Natural images are often affected by random noise and image denoising has long been a central topic in Computer Vision. Many algorithms have been introduced to remove the noise from the natural images, such as Gaussian, Wiener filtering and wavelet thresholding. However, many of these algorithms remove the fine edges and make them blur. Recently, many promising denoising algorithms have been introduced such as Non-local Means, Fields of Experts, and BM3D. In this paper, we explore Bayesian method of ensemble learning for image denoising. Ensemble methods seek to combine multiple different algorithms to retain the strengths of all methods and the weaknesses of none. Bayesian ensemble models are Non-local Means and Fields of Experts, the very successful recent algorithms. The Non-local Means presumes that the image contains an extensive amount of self-similarity. The approach of the Fields of Experts model extends traditional Markov Random Field model by learning potential functions over extended pixel neighborhoods. The two models are implemented and image denoising is performed on natural images. The experimental results obtained are used to compare with the single algorithm and discuss the ensemble learning and their approaches. Comparing to the results of Non-local Means and Fields of Experts, Ensemble learning showed improvement nearly 1dB. version:1
arxiv-1308-1359 | Invariances of random fields paths, with applications in Gaussian Process Regression | http://arxiv.org/abs/1308.1359 | id:1308.1359 author:David Ginsbourger, Olivier Roustant, Nicolas Durrande category:math.ST math.PR stat.ME stat.ML stat.TH  published:2013-08-06 summary:We study pathwise invariances of centred random fields that can be controlled through the covariance. A result involving composition operators is obtained in second-order settings, and we show that various path properties including additivity boil down to invariances of the covariance kernel. These results are extended to a broader class of operators in the Gaussian case, via the Lo\`eve isometry. Several covariance-driven pathwise invariances are illustrated, including fields with symmetric paths, centred paths, harmonic paths, or sparse paths. The proposed approach delivers a number of promising results and perspectives in Gaussian process regression. version:1
arxiv-1206-5771 | The evolution of representation in simple cognitive networks | http://arxiv.org/abs/1206.5771 | id:1206.5771 author:Lars Marstaller, Arend Hintze, Christoph Adami category:q-bio.NC cs.NE q-bio.PE  published:2012-06-25 summary:Representations are internal models of the environment that can provide guidance to a behaving agent, even in the absence of sensory information. It is not clear how representations are developed and whether or not they are necessary or even essential for intelligent behavior. We argue here that the ability to represent relevant features of the environment is the expected consequence of an adaptive process, give a formal definition of representation based on information theory, and quantify it with a measure R. To measure how R changes over time, we evolve two types of networks---an artificial neural network and a network of hidden Markov gates---to solve a categorization task using a genetic algorithm. We find that the capacity to represent increases during evolutionary adaptation, and that agents form representations of their environment during their lifetime. This ability allows the agents to act on sensorial inputs in the context of their acquired representations and enables complex and context-dependent behavior. We examine which concepts (features of the environment) our networks are representing, how the representations are logically encoded in the networks, and how they form as an agent behaves to solve a task. We conclude that R should be able to quantify the representations within any cognitive system, and should be predictive of an agent's long-term adaptive success. version:2
arxiv-1308-1292 | Science Fiction as a Worldwide Phenomenon: A Study of International Creation, Consumption and Dissemination | http://arxiv.org/abs/1308.1292 | id:1308.1292 author:Elysia Wells category:cs.DL cs.CL cs.SI physics.soc-ph  published:2013-08-06 summary:This paper examines the international nature of science fiction. The focus of this research is to determine whether science fiction is primarily English speaking and Western or global; being created and consumed by people in non-Western, non-English speaking countries? Science fiction's international presence was found in three ways, by network analysis, by examining a online retailer and with a survey. Condor, a program developed by GalaxyAdvisors was used to determine if science fiction is being talked about by non-English speakers. An analysis of the international Amazon.com websites was done to discover if it was being consumed worldwide. A survey was also conducted to see if people had experience with science fiction. All three research methods revealed similar results. Science fiction was found to be international, with science fiction creators originating in different countries and writing in a host of different languages. English and non-English science fiction was being created and consumed all over the world, not just in the English speaking West. version:1
arxiv-1308-1196 | The Group Lasso for Design of Experiments | http://arxiv.org/abs/1308.1196 | id:1308.1196 author:Kentaro Tanaka, Masami Miyakawa category:stat.ML 62K05  published:2013-08-06 summary:We introduce an application of the group lasso to design of exper- iments. We show that the problem of constructing an optimal design matrix can be transformed into a problem of the group lasso. We also give a numerical example that we can obtain several orthogonal arrays as the solutions of the group lasso problems. version:1
arxiv-1308-1187 | Spatial-Aware Dictionary Learning for Hyperspectral Image Classification | http://arxiv.org/abs/1308.1187 | id:1308.1187 author:Ali Soltani-Farani, Hamid R. Rabiee, Seyyed Abbas Hosseini category:cs.CV cs.LG  published:2013-08-06 summary:This paper presents a structured dictionary-based model for hyperspectral data that incorporates both spectral and contextual characteristics of a spectral sample, with the goal of hyperspectral image classification. The idea is to partition the pixels of a hyperspectral image into a number of spatial neighborhoods called contextual groups and to model each pixel with a linear combination of a few dictionary elements learned from the data. Since pixels inside a contextual group are often made up of the same materials, their linear combinations are constrained to use common elements from the dictionary. To this end, dictionary learning is carried out with a joint sparse regularizer to induce a common sparsity pattern in the sparse coefficients of each contextual group. The sparse coefficients are then used for classification using a linear SVM. Experimental results on a number of real hyperspectral images confirm the effectiveness of the proposed representation for hyperspectral image classification. Moreover, experiments with simulated multispectral data show that the proposed model is capable of finding representations that may effectively be used for classification of multispectral-resolution samples. version:1
arxiv-1308-1150 | Multimodal Approach for Video Surveillance Indexing and Retrieval | http://arxiv.org/abs/1308.1150 | id:1308.1150 author:Ali Wali, Adel M. Alimi category:cs.MM cs.CV  published:2013-08-06 summary:In this paper, we present an overview of a multimodal system to indexing and searching video sequence by the content that has been developed within the REGIMVid project. A large part of our system has been developed as part of TRECVideo evaluation. The MAVSIR platform provides High-level feature extraction from audio-visual content and concept/event-based video retrieval. We illustrate the architecture of the system as well as provide an overview of the descriptors supported to date. Then we demonstrate the usefulness of the toolbox in the context of feature extraction, concepts/events learning and retrieval in large collections of video surveillance dataset. The results are encouraging as we are able to get good results on several event categories, while for all events we have gained valuable insights and experience. version:1
arxiv-1308-1147 | Empirical Entropy, Minimax Regret and Minimax Risk | http://arxiv.org/abs/1308.1147 | id:1308.1147 author:Alexander Rakhlin, Karthik Sridharan, Alexandre B. Tsybakov category:math.ST cs.LG stat.TH  published:2013-08-06 summary:We consider the random design regression model with square loss. We propose a method that aggregates empirical minimizers (ERM) over appropriately chosen random subsets and reduces to ERM in the extreme case, and we establish sharp oracle inequalities for its risk. We show that, under the $\epsilon^{-p}$ growth of the empirical $\epsilon$-entropy, the excess risk of the proposed method attains the rate $n^{-\frac{2}{2+p}}$ for $p\in(0,2]$ and $n^{-1/p}$ for $p> 2$ where $n$ is the sample size. Furthermore, for $p\in(0,2]$, the excess risk rate matches the behavior of the minimax risk of function estimation in regression problems under the well-specified model. This yields a conclusion that the rates of statistical estimation in well-specified models (minimax risk) and in misspecified models (minimax regret) are equivalent in the regime $p\in(0,2]$. In other words, for $p\in(0,2]$ the problem of statistical learning enjoys the same minimax rate as the problem of statistical estimation. On the contrary, for $p>2$ we show that the rates of the minimax regret are, in general, slower than for the minimax risk. Our oracle inequalities also imply the $v\log(n/v)/n$ rates for Vapnik-Chervonenkis type classes of dimension $v$ without the usual convexity assumption on the class; we show that these rates are optimal. Finally, for a slightly modified method, we derive a bound on the excess risk of $s$-sparse convex aggregation improving that of (Lounici 07) and providing the optimal rate. version:1
arxiv-1308-1126 | Image interpolation using Shearlet based iterative refinement | http://arxiv.org/abs/1308.1126 | id:1308.1126 author:H. Lakshman, W. -Q Lim, H. Schwarz, D. Marpe, G. Kutyniok, T. Wiegand category:cs.CV 94A08 65T60  published:2013-08-05 summary:This paper proposes an image interpolation algorithm exploiting sparse representation for natural images. It involves three main steps: (a) obtaining an initial estimate of the high resolution image using linear methods like FIR filtering, (b) promoting sparsity in a selected dictionary through iterative thresholding, and (c) extracting high frequency information from the approximation to refine the initial estimate. For the sparse modeling, a shearlet dictionary is chosen to yield a multiscale directional representation. The proposed algorithm is compared to several state-of-the-art methods to assess its objective as well as subjective performance. Compared to the cubic spline interpolation method, an average PSNR gain of around 0.8 dB is observed over a dataset of 200 images. version:1
arxiv-1308-1066 | Theoretical Issues for Global Cumulative Treatment Analysis (GCTA) | http://arxiv.org/abs/1308.1066 | id:1308.1066 author:Jeff Shrager category:stat.AP cs.LG  published:2013-08-05 summary:Adaptive trials are now mainstream science. Recently, researchers have taken the adaptive trial concept to its natural conclusion, proposing what we call "Global Cumulative Treatment Analysis" (GCTA). Similar to the adaptive trial, decision making and data collection and analysis in the GCTA are continuous and integrated, and treatments are ranked in accord with the statistics of this information, combined with what offers the most information gain. Where GCTA differs from an adaptive trial, or, for that matter, from any trial design, is that all patients are implicitly participants in the GCTA process, regardless of whether they are formally enrolled in a trial. This paper discusses some of the theoretical and practical issues that arise in the design of a GCTA, along with some preliminary thoughts on how they might be approached. version:1
arxiv-1308-1049 | Coevolutionary networks of reinforcement-learning agents | http://arxiv.org/abs/1308.1049 | id:1308.1049 author:Ardeshir Kianercy, Aram Galstyan category:cs.MA cs.LG nlin.AO  published:2013-08-05 summary:This paper presents a model of network formation in repeated games where the players adapt their strategies and network ties simultaneously using a simple reinforcement-learning scheme. It is demonstrated that the coevolutionary dynamics of such systems can be described via coupled replicator equations. We provide a comprehensive analysis for three-player two-action games, which is the minimum system size with nontrivial structural dynamics. In particular, we characterize the Nash equilibria (NE) in such games and examine the local stability of the rest points corresponding to those equilibria. We also study general n-player networks via both simulations and analytical methods and find that in the absence of exploration, the stable equilibria consist of star motifs as the main building blocks of the network. Furthermore, in all stable equilibria the agents play pure strategies, even when the game allows mixed NE. Finally, we study the impact of exploration on learning outcomes, and observe that there is a critical exploration rate above which the symmetric and uniformly connected network topology becomes stable. version:1
arxiv-1308-1009 | Sign Stable Projections, Sign Cauchy Projections and Chi-Square Kernels | http://arxiv.org/abs/1308.1009 | id:1308.1009 author:Ping Li, Gennady Samorodnitsky, John Hopcroft category:cs.LG cs.DS cs.IR  published:2013-08-05 summary:The method of stable random projections is popular for efficiently computing the Lp distances in high dimension (where 0<p<=2), using small space. Because it adopts nonadaptive linear projections, this method is naturally suitable when the data are collected in a dynamic streaming fashion (i.e., turnstile data streams). In this paper, we propose to use only the signs of the projected data and analyze the probability of collision (i.e., when the two signs differ). We derive a bound of the collision probability which is exact when p=2 and becomes less sharp when p moves away from 2. Interestingly, when p=1 (i.e., Cauchy random projections), we show that the probability of collision can be accurately approximated as functions of the chi-square similarity. For example, when the (un-normalized) data are binary, the maximum approximation error of the collision probability is smaller than 0.0192. In text and vision applications, the chi-square similarity is a popular measure for nonnegative data when the features are generated from histograms. Our experiments confirm that the proposed method is promising for large-scale learning applications. version:1
arxiv-1308-1006 | Fast Semidifferential-based Submodular Function Optimization | http://arxiv.org/abs/1308.1006 | id:1308.1006 author:Rishabh Iyer, Stefanie Jegelka, Jeff Bilmes category:cs.DS cs.DM cs.LG  published:2013-08-05 summary:We present a practical and powerful new framework for both unconstrained and constrained submodular function optimization based on discrete semidifferentials (sub- and super-differentials). The resulting algorithms, which repeatedly compute and then efficiently optimize submodular semigradients, offer new and generalize many old methods for submodular optimization. Our approach, moreover, takes steps towards providing a unifying paradigm applicable to both submodular min- imization and maximization, problems that historically have been treated quite distinctly. The practicality of our algorithms is important since interest in submodularity, owing to its natural and wide applicability, has recently been in ascendance within machine learning. We analyze theoretical properties of our algorithms for minimization and maximization, and show that many state-of-the-art maximization algorithms are special cases. Lastly, we complement our theoretical analyses with supporting empirical experiments. version:1
arxiv-1308-0897 | Context Specific Event Model For News Articles | http://arxiv.org/abs/1308.0897 | id:1308.0897 author:Kowcika A, Uma Maheswari, Geetha T V category:cs.CL cs.IR  published:2013-08-05 summary:We present a new context based event indexing and event ranking model for News Articles. The context event clusters formed from the UNL Graphs uses the modified scoring scheme for segmenting events which is followed by clustering of events. From the context clusters obtained three models are developed- Identification of Main and Sub events; Event Indexing and Event Ranking. Based on the properties considered from the UNL Graphs for the modified scoring main events and sub events associated with main-events are identified. The temporal details obtained from the context cluster are stored using hashmap data structure. The temporal details are place-where the event took; person-who involved in that event; time-when the event took place. Based on the information collected from the context clusters three indices are generated- Time index, Person index, and Place index. This index gives complete details about every event obtained from context clusters. A new scoring scheme is introduced for ranking the events. The scoring scheme for event ranking gives weight-age based on the priority level of the events. The priority level includes the occurrence of the event in the title of the document, event frequency, and inverse document frequency of the events. version:1
arxiv-1308-0890 | Head Gesture Recognition using Optical Flow based Classification with Reinforcement of GMM based Background Subtraction | http://arxiv.org/abs/1308.0890 | id:1308.0890 author:Parimita Saikia, Karen Das category:cs.CV  published:2013-08-05 summary:This paper describes a technique of real time head gesture recognition system. The method includes Gaussian mixture model (GMM) accompanied by optical flow algorithm which provided us the required information regarding head movement. The proposed model can be implemented in various control system. We are also presenting the result and implementation of both mentioned method. version:1
arxiv-1308-0768 | MonoStream: A Minimal-Hardware High Accuracy Device-free WLAN Localization System | http://arxiv.org/abs/1308.0768 | id:1308.0768 author:Ibrahim Sabek, Moustafa Youssef category:cs.NI cs.LG  published:2013-08-04 summary:Device-free (DF) localization is an emerging technology that allows the detection and tracking of entities that do not carry any devices nor participate actively in the localization process. Typically, DF systems require a large number of transmitters and receivers to achieve acceptable accuracy, which is not available in many scenarios such as homes and small businesses. In this paper, we introduce MonoStream as an accurate single-stream DF localization system that leverages the rich Channel State Information (CSI) as well as MIMO information from the physical layer to provide accurate DF localization with only one stream. To boost its accuracy and attain low computational requirements, MonoStream models the DF localization problem as an object recognition problem and uses a novel set of CSI-context features and techniques with proven accuracy and efficiency. Experimental evaluation in two typical testbeds, with a side-by-side comparison with the state-of-the-art, shows that MonoStream can achieve an accuracy of 0.95m with at least 26% enhancement in median distance error using a single stream only. This enhancement in accuracy comes with an efficient execution of less than 23ms per location update on a typical laptop. This highlights the potential of MonoStream usage for real-time DF tracking applications. version:1
arxiv-1308-0701 | Ontology Enrichment by Extracting Hidden Assertional Knowledge from Text | http://arxiv.org/abs/1308.0701 | id:1308.0701 author:Meisam Booshehri, Abbas Malekpour, Peter Luksch, Kamran Zamanifar, Shahdad Shariatmadari category:cs.IR cs.CL 68Txx I.2.6  published:2013-08-03 summary:In this position paper we present a new approach for discovering some special classes of assertional knowledge in the text by using large RDF repositories, resulting in the extraction of new non-taxonomic ontological relations. Also we use inductive reasoning beside our approach to make it outperform. Then, we prepare a case study by applying our approach on sample data and illustrate the soundness of our proposed approach. Moreover in our point of view current LOD cloud is not a suitable base for our proposal in all informational domains. Therefore we figure out some directions based on prior works to enrich datasets of Linked Data by using web mining. The result of such enrichment can be reused for further relation extraction and ontology enrichment from unstructured free text documents. version:1
arxiv-1308-0661 | A Comparison of Named Entity Recognition Tools Applied to Biographical Texts | http://arxiv.org/abs/1308.0661 | id:1308.0661 author:Samet AtdaÄŸ, Vincent Labatut category:cs.IR cs.CL  published:2013-08-03 summary:Named entity recognition (NER) is a popular domain of natural language processing. For this reason, many tools exist to perform this task. Amongst other points, they differ in the processing method they rely upon, the entity types they can detect, the nature of the text they can handle, and their input/output formats. This makes it difficult for a user to select an appropriate NER tool for a specific situation. In this article, we try to answer this question in the context of biographic texts. For this matter, we first constitute a new corpus by annotating Wikipedia articles. We then select publicly available, well known and free for research NER tools for comparison: Stanford NER, Illinois NET, OpenCalais NER WS and Alias-i LingPipe. We apply them to our corpus, assess their performances and compare them. When considering overall performances, a clear hierarchy emerges: Stanford has the best results, followed by LingPipe, Illionois and OpenCalais. However, a more detailed evaluation performed relatively to entity types and article categories highlights the fact their performances are diversely influenced by those factors. This complementarity opens an interesting perspective regarding the combination of these individual tools in order to improve performance. version:1
arxiv-1303-4996 | Compressive Shift Retrieval | http://arxiv.org/abs/1303.4996 | id:1303.4996 author:Henrik Ohlsson, Yonina C. Eldar, Allen Y. Yang, S. Shankar Sastry category:cs.SY cs.IT math.IT stat.ML  published:2013-03-20 summary:The classical shift retrieval problem considers two signals in vector form that are related by a shift. The problem is of great importance in many applications and is typically solved by maximizing the cross-correlation between the two signals. Inspired by compressive sensing, in this paper, we seek to estimate the shift directly from compressed signals. We show that under certain conditions, the shift can be recovered using fewer samples and less computation compared to the classical setup. Of particular interest is shift estimation from Fourier coefficients. We show that under rather mild conditions only one Fourier coefficient suffices to recover the true shift. version:2
arxiv-1308-0658 | Exploring The Contribution of Unlabeled Data in Financial Sentiment Analysis | http://arxiv.org/abs/1308.0658 | id:1308.0658 author:Jimmy SJ. Ren, Wei Wang, Jiawei Wang, Stephen Shaoyi Liao category:cs.CL cs.LG  published:2013-08-03 summary:With the proliferation of its applications in various industries, sentiment analysis by using publicly available web data has become an active research area in text classification during these years. It is argued by researchers that semi-supervised learning is an effective approach to this problem since it is capable to mitigate the manual labeling effort which is usually expensive and time-consuming. However, there was a long-term debate on the effectiveness of unlabeled data in text classification. This was partially caused by the fact that many assumptions in theoretic analysis often do not hold in practice. We argue that this problem may be further understood by adding an additional dimension in the experiment. This allows us to address this problem in the perspective of bias and variance in a broader view. We show that the well-known performance degradation issue caused by unlabeled data can be reproduced as a subset of the whole scenario. We argue that if the bias-variance trade-off is to be better balanced by a more effective feature selection method unlabeled data is very likely to boost the classification performance. We then propose a feature selection framework in which labeled and unlabeled training samples are both considered. We discuss its potential in achieving such a balance. Besides, the application in financial sentiment analysis is chosen because it not only exemplifies an important application, the data possesses better illustrative power as well. The implications of this study in text classification and financial sentiment analysis are both discussed. version:1
arxiv-1301-0127 | A Semi-automated Statistical Algorithm for Object Separation | http://arxiv.org/abs/1301.0127 | id:1301.0127 author:Madhur Srivastava, Satish K. Singh, Prasanta K. Panigrahi category:cs.CV  published:2013-01-01 summary:We explicate a semi-automated statistical algorithm for object identification and segregation in both gray scale and color images. The algorithm makes optimal use of the observation that definite objects in an image are typically represented by pixel values having narrow Gaussian distributions about characteristic mean values. Furthermore, for visually distinct objects, the corresponding Gaussian distributions have negligible overlap with each other and hence the Mahalanobis distance between these distributions are large. These statistical facts enable one to sub-divide images into multiple thresholds of variable sizes, each segregating similar objects. The procedure incorporates the sensitivity of human eye to the gray pixel values into the variable threshold size, while mapping the Gaussian distributions into localized \delta-functions, for object separation. The effectiveness of this recursive statistical algorithm is demonstrated using a wide variety of images. version:3
arxiv-1308-0641 | United Statistical Algorithm, Small and Big Data: Future OF Statistician | http://arxiv.org/abs/1308.0641 | id:1308.0641 author:Emanuel Parzen, Subhadeep Mukhopadhyay category:math.ST stat.ME stat.ML stat.TH  published:2013-08-02 summary:This article provides the role of big idea statisticians in future of Big Data Science. We describe the `United Statistical Algorithms' framework for comprehensive uni?cation of traditional and novel statistical methods for modeling Small Data and Big Data, especially mixed data (discrete, continuous). version:1
arxiv-1307-6995 | Finite State Machine Synthesis for Evolutionary Hardware | http://arxiv.org/abs/1307.6995 | id:1307.6995 author:Andrey Bereza, Maksim Lyashov, Luis Blanco category:cs.NE cs.FL  published:2013-07-26 summary:This article considers application of genetic algorithms for finite machine synthesis. The resulting genetic finite state machines synthesis algorithm allows for creation of machines with less number of states and within shorter time. This makes it possible to use hardware-oriented genetic finite machines synthesis algorithm in autonomous systems on reconfigurable platforms. version:2
arxiv-1302-3785 | Analysis of Descent-Based Image Registration | http://arxiv.org/abs/1302.3785 | id:1302.3785 author:Elif Vural, Pascal Frossard category:cs.CV  published:2013-02-15 summary:We present a performance analysis for image registration with gradient descent methods. We consider a typical multiscale registration setting where the global 2-D translation between a pair of images is estimated by smoothing the images and minimizing the distance between them with gradient descent. Our study particularly concentrates on the effect of noise and low-pass filtering on the alignment accuracy. We adopt an analytic representation for images and analyze the well-behavedness of the image distance function by estimating the neighborhood of translations for which it is free of undesired local minima. This corresponds to the neighborhood of translation vectors that are correctly computable with a simple gradient descent minimization. We show that the area of this neighborhood increases at least quadratically with the smoothing filter size, which justifies the use of a smoothing step in image registration with local optimizers such as gradient descent. We then examine the effect of noise on the alignment accuracy and derive an upper bound for the alignment error in terms of the noise properties and filter size. Our main finding is that the error increases at a rate that is at least linear with respect to the filter size. Therefore, smoothing improves the well-behavedness of the distance function; however, this comes at the cost of amplifying the alignment error in noisy settings. Our results provide a mathematical insight about why hierarchical techniques are effective in image registration, suggesting that the multiscale coarse-to-fine alignment strategy of these techniques is very suitable from the perspective of the trade-off between the well-behavedness of the objective function and the registration accuracy. To the best of our knowledge, this is the first such study for descent-based image registration. version:2
arxiv-1112-0076 | Bandit Market Makers | http://arxiv.org/abs/1112.0076 | id:1112.0076 author:Nicolas Della Penna, Mark D. Reid category:q-fin.TR cs.GT stat.ML  published:2011-12-01 summary:We introduce a modular framework for market making. It combines cost-function based automated market makers with bandit algorithms. We obtain worst-case profits guarantee's relative to the best in hindsight within a class of natural "overround" cost functions . This combination allow us to have distribution-free guarantees on the regret of profits while preserving the bounded worst-case losses and computational tractability over combinatorial spaces of the cost function based approach. We present simulation results to better understand the practical behaviour of market makers from the framework. version:4
arxiv-1301-6659 | Clustering-Based Matrix Factorization | http://arxiv.org/abs/1301.6659 | id:1301.6659 author:Nima Mirbakhsh, Charles X. Ling category:cs.LG  published:2013-01-28 summary:Recommender systems are emerging technologies that nowadays can be found in many applications such as Amazon, Netflix, and so on. These systems help users to find relevant information, recommendations, and their preferred items. Slightly improvement of the accuracy of these recommenders can highly affect the quality of recommendations. Matrix Factorization is a popular method in Recommendation Systems showing promising results in accuracy and complexity. In this paper we propose an extension of matrix factorization which adds general neighborhood information on the recommendation model. Users and items are clustered into different categories to see how these categories share preferences. We then employ these shared interests of categories in a fusion by Biased Matrix Factorization to achieve more accurate recommendations. This is a complement for the current neighborhood aware matrix factorization models which rely on using direct neighborhood information of users and items. The proposed model is tested on two well-known recommendation system datasets: Movielens100k and Netflix. Our experiment shows applying the general latent features of categories into factorized recommender models improves the accuracy of recommendations. The current neighborhood-aware models need a great number of neighbors to acheive good accuracies. To the best of our knowledge, the proposed model is better than or comparable with the current neighborhood-aware models when they consider fewer number of neighbors. version:4
arxiv-1308-0365 | Hybrid Focal Stereo Networks for Pattern Analysis in Homogeneous Scenes | http://arxiv.org/abs/1308.0365 | id:1308.0365 author:Emanuel Aldea, Khurom H. Kiyani category:cs.CV  published:2013-08-01 summary:In this paper we address the problem of multiple camera calibration in the presence of a homogeneous scene, and without the possibility of employing calibration object based methods. The proposed solution exploits salient features present in a larger field of view, but instead of employing active vision we replace the cameras with stereo rigs featuring a long focal analysis camera, as well as a short focal registration camera. Thus, we are able to propose an accurate solution which does not require intrinsic variation models as in the case of zooming cameras. Moreover, the availability of the two views simultaneously in each rig allows for pose re-estimation between rigs as often as necessary. The algorithm has been successfully validated in an indoor setting, as well as on a difficult scene featuring a highly dense pilgrim crowd in Makkah. version:1
arxiv-1308-0356 | Design and Development of an Expert System to Help Head of University Departments | http://arxiv.org/abs/1308.0356 | id:1308.0356 author:Shervan Fekri-Ershad, Hadi Tajalizadeh, Shahram Jafari category:cs.AI cs.LG  published:2013-08-01 summary:One of the basic tasks which is responded for head of each university department, is employing lecturers based on some default factors such as experience, evidences, qualifies and etc. In this respect, to help the heads, some automatic systems have been proposed until now using machine learning methods, decision support systems (DSS) and etc. According to advantages and disadvantages of the previous methods, a full automatic system is designed in this paper using expert systems. The proposed system is included two main steps. In the first one, the human expert's knowledge is designed as decision trees. The second step is included an expert system which is evaluated using extracted rules of these decision trees. Also, to improve the quality of the proposed system, a majority voting algorithm is proposed as post processing step to choose the best lecturer which satisfied more expert's decision trees for each course. The results are shown that the designed system average accuracy is 78.88. Low computational complexity, simplicity to program and are some of other advantages of the proposed system. version:1
arxiv-1308-0315 | MAS for video objects segmentation and tracking based on active contours and SURF descriptor | http://arxiv.org/abs/1308.0315 | id:1308.0315 author:Mohamed Chakroun, Ali Wali, Adel M. Alimi category:cs.MM cs.CV  published:2013-08-01 summary:In computer vision, video segmentation and tracking is an important challenging issue. In this paper, we describe a new video sequences segmentation and tracking algorithm based on MAS "multi-agent systems" and SURF "Speeded Up Robust Features". Our approach consists in modelling a multi-agent system for segmenting the first image from a video sequence and tracking objects in the video sequences. The used agents are supervisor and explorator agents, they are communicating between them and they inspire in their behavior from active contours approaches. The tracking of objects is based on SURF descriptors "Speed Up Robust Features". We used the DIMA platform and "API Ateji PX" (an extension of the Java language to facilitate parallel programming on heterogeneous architectures) to implement this algorithm. The experimental results indicate that the proposed algorithm is more robust and faster than previous approaches. version:1
arxiv-1308-0290 | Sparse Dictionary-based Attributes for Action Recognition and Summarization | http://arxiv.org/abs/1308.0290 | id:1308.0290 author:Qiang Qiu, Zhuolin Jiang, Rama Chellappa category:cs.CV  published:2013-08-01 summary:We present an approach for dictionary learning of action attributes via information maximization. We unify the class distribution and appearance information into an objective function for learning a sparse dictionary of action attributes. The objective function maximizes the mutual information between what has been learned and what remains to be learned in terms of appearance information and class distribution for each dictionary atom. We propose a Gaussian Process (GP) model for sparse representation to optimize the dictionary objective function. The sparse coding property allows a kernel with compact support in GP to realize a very efficient dictionary learning process. Hence we can describe an action video by a set of compact and discriminative action attributes. More importantly, we can recognize modeled action categories in a sparse feature space, which can be generalized to unseen and unmodeled action categories. Experimental results demonstrate the effectiveness of our approach in action recognition and summarization. version:1
arxiv-1209-4951 | An efficient model-free estimation of multiclass conditional probability | http://arxiv.org/abs/1209.4951 | id:1209.4951 author:Tu Xu, Junhui Wang category:stat.ML cs.LG stat.ME  published:2012-09-22 summary:Conventional multiclass conditional probability estimation methods, such as Fisher's discriminate analysis and logistic regression, often require restrictive distributional model assumption. In this paper, a model-free estimation method is proposed to estimate multiclass conditional probability through a series of conditional quantile regression functions. Specifically, the conditional class probability is formulated as difference of corresponding cumulative distribution functions, where the cumulative distribution functions can be converted from the estimated conditional quantile regression functions. The proposed estimation method is also efficient as its computation cost does not increase exponentially with the number of classes. The theoretical and numerical studies demonstrate that the proposed estimation method is highly competitive against the existing competitors, especially when the number of classes is relatively large. version:3
arxiv-1308-0275 | Domain-invariant Face Recognition using Learned Low-rank Transformation | http://arxiv.org/abs/1308.0275 | id:1308.0275 author:Qiang Qiu, Guillermo Sapiro, Ching-Hui Chen category:cs.CV  published:2013-08-01 summary:We present a low-rank transformation approach to compensate for face variations due to changes in visual domains, such as pose and illumination. The key idea is to learn discriminative linear transformations for face images using matrix rank as the optimization criteria. The learned linear transformations restore a shared low-rank structure for faces from the same subject, and, at the same time, force a high-rank structure for faces from different subjects. In this way, among the transformed faces, we reduce variations caused by domain changes within the classes, and increase separations between the classes for better face recognition across domains. Extensive experiments using public datasets are presented to demonstrate the effectiveness of our approach for face recognition across domains. The potential of the approach for feature extraction in generic object recognition and coded aperture design are discussed as well. version:1
arxiv-1308-0273 | Learning Robust Subspace Clustering | http://arxiv.org/abs/1308.0273 | id:1308.0273 author:Qiang Qiu, Guillermo Sapiro category:cs.CV  published:2013-08-01 summary:We propose a low-rank transformation-learning framework to robustify subspace clustering. Many high-dimensional data, such as face images and motion sequences, lie in a union of low-dimensional subspaces. The subspace clustering problem has been extensively studied in the literature to partition such high-dimensional data into clusters corresponding to their underlying low-dimensional subspaces. However, low-dimensional intrinsic structures are often violated for real-world observations, as they can be corrupted by errors or deviate from ideal models. We propose to address this by learning a linear transformation on subspaces using matrix rank, via its convex surrogate nuclear norm, as the optimization criteria. The learned linear transformation restores a low-rank structure for data from the same subspace, and, at the same time, forces a high-rank structure for data from different subspaces. In this way, we reduce variations within the subspaces, and increase separations between the subspaces for more accurate subspace clustering. This proposed learned robust subspace clustering framework significantly enhances the performance of existing subspace clustering methods. To exploit the low-rank structures of the transformed subspaces, we further introduce a subspace clustering technique, called Robust Sparse Subspace Clustering, which efficiently combines robust PCA with sparse modeling. We also discuss the online learning of the transformation, and learning of the transformation while simultaneously reducing the data dimensionality. Extensive experiments using public datasets are presented, showing that the proposed approach significantly outperforms state-of-the-art subspace clustering methods. version:1
arxiv-1307-2302 | The blessing of transitivity in sparse and stochastic networks | http://arxiv.org/abs/1307.2302 | id:1307.2302 author:Karl Rohe, Tai Qin category:stat.ML  published:2013-07-08 summary:The interaction between transitivity and sparsity, two common features in empirical networks, implies that there are local regions of large sparse networks that are dense. We call this the blessing of transitivity and it has consequences for both modeling and inference. Extant research suggests that statistical inference for the Stochastic Blockmodel is more difficult when the edges are sparse. However, this conclusion is confounded by the fact that the asymptotic limit in all of the previous studies is not merely sparse, but also non-transitive. To retain transitivity, the blocks cannot grow faster than the expected degree. Thus, in sparse models, the blocks must remain asymptotically small. \n Previous algorithmic research demonstrates that small "local" clusters are more amenable to computation, visualization, and interpretation when compared to "global" graph partitions. This paper provides the first statistical results that demonstrate how these small transitive clusters are also more amenable to statistical estimation. Theorem 2 shows that a "local" clustering algorithm can, with high probability, detect a transitive stochastic block of a fixed size (e.g. 30 nodes) embedded in a large graph. The only constraint on the ambient graph is that it is large and sparse--it could be generated at random or by an adversary--suggesting a theoretical explanation for the robust empirical performance of local clustering algorithms. version:2
arxiv-1202-2564 | A better Beta for the H measure of classification performance | http://arxiv.org/abs/1202.2564 | id:1202.2564 author:David J. Hand, Christoforos Anagnostopoulos category:stat.ME cs.CV stat.ML  published:2012-02-12 summary:The area under the ROC curve is widely used as a measure of performance of classification rules. However, it has recently been shown that the measure is fundamentally incoherent, in the sense that it treats the relative severities of misclassifications differently when different classifiers are used. To overcome this, Hand (2009) proposed the $H$ measure, which allows a given researcher to fix the distribution of relative severities to a classifier-independent setting on a given problem. This note extends the discussion, and proposes a modified standard distribution for the $H$ measure, which better matches the requirements of researchers, in particular those faced with heavily unbalanced datasets, the $Beta(\pi_1+1,\pi_0+1)$ distribution. [Preprint submitted at Pattern Recognition Letters] version:2
arxiv-1209-6561 | Scoring and Searching over Bayesian Networks with Causal and Associative Priors | http://arxiv.org/abs/1209.6561 | id:1209.6561 author:Giorgos Borboudakis, Ioannis Tsamardinos category:cs.AI cs.LG stat.ML  published:2012-09-28 summary:A significant theoretical advantage of search-and-score methods for learning Bayesian Networks is that they can accept informative prior beliefs for each possible network, thus complementing the data. In this paper, a method is presented for assigning priors based on beliefs on the presence or absence of certain paths in the true network. Such beliefs correspond to knowledge about the possible causal and associative relations between pairs of variables. This type of knowledge naturally arises from prior experimental and observational data, among others. In addition, a novel search-operator is proposed to take advantage of such prior knowledge. Experiments show that, using path beliefs improves the learning of the skeleton, as well as the edge directions in the network. version:2
arxiv-1307-8430 | Fast Simultaneous Training of Generalized Linear Models (FaSTGLZ) | http://arxiv.org/abs/1307.8430 | id:1307.8430 author:Bryan R. Conroy, Jennifer M. Walz, Brian Cheung, Paul Sajda category:cs.LG stat.ML  published:2013-07-31 summary:We present an efficient algorithm for simultaneously training sparse generalized linear models across many related problems, which may arise from bootstrapping, cross-validation and nonparametric permutation testing. Our approach leverages the redundancies across problems to obtain significant computational improvements relative to solving the problems sequentially by a conventional algorithm. We demonstrate our fast simultaneous training of generalized linear models (FaSTGLZ) algorithm on a number of real-world datasets, and we run otherwise computationally intensive bootstrapping and permutation test analyses that are typically necessary for obtaining statistically rigorous classification results and meaningful interpretation. Code is freely available at http://liinc.bme.columbia.edu/fastglz. version:1
arxiv-1307-8405 | Who and Where: People and Location Co-Clustering | http://arxiv.org/abs/1307.8405 | id:1307.8405 author:Zixuan Wang, Jinyun Yan category:cs.CV  published:2013-07-31 summary:In this paper, we consider the clustering problem on images where each image contains patches in people and location domains. We exploit the correlation between people and location domains, and proposed a semi-supervised co-clustering algorithm to cluster images. Our algorithm updates the correlation links at the runtime, and produces clustering in both domains simultaneously. We conduct experiments in a manually collected dataset and a Flickr dataset. The result shows that the such correlation improves the clustering performance. version:1
arxiv-1307-8305 | The Planning-ahead SMO Algorithm | http://arxiv.org/abs/1307.8305 | id:1307.8305 author:Tobias Glasmachers category:cs.LG  published:2013-07-31 summary:The sequential minimal optimization (SMO) algorithm and variants thereof are the de facto standard method for solving large quadratic programs for support vector machine (SVM) training. In this paper we propose a simple yet powerful modification. The main emphasis is on an algorithm improving the SMO step size by planning-ahead. The theoretical analysis ensures its convergence to the optimum. Experiments involving a large number of datasets were carried out to demonstrate the superiority of the new algorithm. version:1
arxiv-1307-8279 | Tracking Extrema in Dynamic Environment using Multi-Swarm Cellular PSO with Local Search | http://arxiv.org/abs/1307.8279 | id:1307.8279 author:Somayeh Nabizadeh, Alireza Rezvanian, Mohammad Reza Meybodi category:cs.AI cs.NE  published:2013-07-31 summary:Many real-world phenomena can be modelled as dynamic optimization problems. In such cases, the environment problem changes dynamically and therefore, conventional methods are not capable of dealing with such problems. In this paper, a novel multi-swarm cellular particle swarm optimization algorithm is proposed by clustering and local search. In the proposed algorithm, the search space is partitioned into cells, while the particles identify changes in the search space and form clusters to create sub-swarms. Then a local search is applied to improve the solutions in the each cell. Simulation results for static standard benchmarks and dynamic environments show superiority of the proposed method over other alternative approaches. version:1
arxiv-1307-8233 | A Prototyping Environment for Integrated Artificial Attention Systems | http://arxiv.org/abs/1307.8233 | id:1307.8233 author:Jan TÃ¼nnermann, Markus Hennig, Michael Silbernagel, BÃ¤rbel Mertsching category:cs.CV  published:2013-07-31 summary:Artificial visual attention systems aim to support technical systems in visual tasks by applying the concepts of selective attention observed in humans and other animals. Such systems are typically evaluated against ground truth obtained from human gaze-data or manually annotated test images. When applied to robotics, the systems are required to be adaptable to the target system. Here, we describe a flexible environment based on a robotic middleware layer allowing the development and testing of attention-guided vision systems. In such a framework, the systems can be tested with input from various sources, different attention algorithms at the core, and diverse subsequent tasks. version:1
arxiv-1307-8225 | A Novel Architecture for Relevant Blog Page Identifcation | http://arxiv.org/abs/1307.8225 | id:1307.8225 author:Deepti Kapri, Rosy Madaan, A. K Sharma, Ashutosh Dixit category:cs.IR cs.CL  published:2013-07-31 summary:Blogs are undoubtedly the richest source of information available in cyberspace. Blogs can be of various natures i.e. personal blogs which contain posts on mixed issues or blogs can be domain specific which contains posts on particular topics, this is the reason, they offer wide variety of relevant information which is often focused. A general search engine gives back a huge collection of web pages which may or may not give correct answers, as web is the repository of information of all kinds and a user has to go through various documents before he gets what he was originally looking for, which is a very time consuming process. So, the search can be made more focused and accurate if it is limited to blogosphere instead of web pages. The reason being that the blogs are more focused in terms of information. So, User will only get related blogs in response to his query. These results will be then ranked according to our proposed method and are finally presented in front of user in descending order version:1
arxiv-1307-8136 | DeBaCl: A Python Package for Interactive DEnsity-BAsed CLustering | http://arxiv.org/abs/1307.8136 | id:1307.8136 author:Brian P. Kent, Alessandro Rinaldo, Timothy Verstynen category:stat.ME cs.LG stat.ML  published:2013-07-30 summary:The level set tree approach of Hartigan (1975) provides a probabilistically based and highly interpretable encoding of the clustering behavior of a dataset. By representing the hierarchy of data modes as a dendrogram of the level sets of a density estimator, this approach offers many advantages for exploratory analysis and clustering, especially for complex and high-dimensional data. Several R packages exist for level set tree estimation, but their practical usefulness is limited by computational inefficiency, absence of interactive graphical capabilities and, from a theoretical perspective, reliance on asymptotic approximations. To make it easier for practitioners to capture the advantages of level set trees, we have written the Python package DeBaCl for DEnsity-BAsed CLustering. In this article we illustrate how DeBaCl's level set tree estimates can be used for difficult clustering tasks and interactive graphical data analysis. The package is intended to promote the practical use of level set trees through improvements in computational efficiency and a high degree of user customization. In addition, the flexible algorithms implemented in DeBaCl enjoy finite sample accuracy, as demonstrated in recent literature on density clustering. Finally, we show the level set tree framework can be easily extended to deal with functional data. version:1
arxiv-1307-8104 | Neural Network Capacity for Multilevel Inputs | http://arxiv.org/abs/1307.8104 | id:1307.8104 author:Matt Stowe, Subhash Kak category:cs.NE  published:2013-07-30 summary:This paper examines the memory capacity of generalized neural networks. Hopfield networks trained with a variety of learning techniques are investigated for their capacity both for binary and non-binary alphabets. It is shown that the capacity can be much increased when multilevel inputs are used. New learning strategies are proposed to increase Hopfield network capacity, and the scalability of these methods is also examined in respect to size of the network. The ability to recall entire patterns from stimulation of a single neuron is examined for the increased capacity networks. version:1
arxiv-1307-8060 | Extracting Information-rich Part of Texts using Text Denoising | http://arxiv.org/abs/1307.8060 | id:1307.8060 author:Rushdi Shams category:cs.IR cs.CL  published:2013-07-30 summary:The aim of this paper is to report on a novel text reduction technique, called Text Denoising, that highlights information-rich content when processing a large volume of text data, especially from the biomedical domain. The core feature of the technique, the text readability index, embodies the hypothesis that complex text is more information-rich than the rest. When applied on tasks like biomedical relation bearing text extraction, keyphrase indexing and extracting sentences describing protein interactions, it is evident that the reduced set of text produced by text denoising is more information-rich than the rest. version:1
arxiv-1307-8057 | Extracting Connected Concepts from Biomedical Texts using Fog Index | http://arxiv.org/abs/1307.8057 | id:1307.8057 author:Rushdi Shams, Robert E. Mercer category:cs.CL cs.IR  published:2013-07-30 summary:In this paper, we establish Fog Index (FI) as a text filter to locate the sentences in texts that contain connected biomedical concepts of interest. To do so, we have used 24 random papers each containing four pairs of connected concepts. For each pair, we categorize sentences based on whether they contain both, any or none of the concepts. We then use FI to measure difficulty of the sentences of each category and find that sentences containing both of the concepts have low readability. We rank sentences of a text according to their FI and select 30 percent of the most difficult sentences. We use an association matrix to track the most frequent pairs of concepts in them. This matrix reports that the first filter produces some pairs that hold almost no connections. To remove these unwanted pairs, we use the Equally Weighted Harmonic Mean of their Positive Predictive Value (PPV) and Sensitivity as a second filter. Experimental results demonstrate the effectiveness of our method. version:1
arxiv-1307-8049 | Optimistic Concurrency Control for Distributed Unsupervised Learning | http://arxiv.org/abs/1307.8049 | id:1307.8049 author:Xinghao Pan, Joseph E. Gonzalez, Stefanie Jegelka, Tamara Broderick, Michael I. Jordan category:cs.LG cs.AI cs.DC  published:2013-07-30 summary:Research on distributed machine learning algorithms has focused primarily on one of two extremes - algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints. We consider an intermediate alternative in which algorithms optimistically assume that conflicts are unlikely and if conflicts do arise a conflict-resolution protocol is invoked. We view this "optimistic concurrency control" paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting. We demonstrate our approach in three problem areas: clustering, feature learning and online facility location. We evaluate our methods via large-scale experiments in a cluster computing environment. version:1
arxiv-1307-8012 | A Study on Classification in Imbalanced and Partially-Labelled Data Streams | http://arxiv.org/abs/1307.8012 | id:1307.8012 author:R. J. Lyon, J. M. Brooke, J. D. Knowles, B. W. Stappers category:astro-ph.IM cs.LG  published:2013-07-30 summary:The domain of radio astronomy is currently facing significant computational challenges, foremost amongst which are those posed by the development of the world's largest radio telescope, the Square Kilometre Array (SKA). Preliminary specifications for this instrument suggest that the final design will incorporate between 2000 and 3000 individual 15 metre receiving dishes, which together can be expected to produce a data rate of many TB/s. Given such a high data rate, it becomes crucial to consider how this information will be processed and stored to maximise its scientific utility. In this paper, we consider one possible data processing scenario for the SKA, for the purposes of an all-sky pulsar survey. In particular we treat the selection of promising signals from the SKA processing pipeline as a data stream classification problem. We consider the feasibility of classifying signals that arrive via an unlabelled and heavily class imbalanced data stream, using currently available algorithms and frameworks. Our results indicate that existing stream learners exhibit unacceptably low recall on real astronomical data when used in standard configuration; however, good false positive performance and comparable accuracy to static learners, suggests they have definite potential as an on-line solution to this particular big data challenge. version:1
arxiv-1307-7993 | Sharp Threshold for Multivariate Multi-Response Linear Regression via Block Regularized Lasso | http://arxiv.org/abs/1307.7993 | id:1307.7993 author:Weiguang Wang, Yingbin Liang, Eric P. Xing category:cs.LG stat.ML  published:2013-07-30 summary:In this paper, we investigate a multivariate multi-response (MVMR) linear regression problem, which contains multiple linear regression models with differently distributed design matrices, and different regression and output vectors. The goal is to recover the support union of all regression vectors using $l_1/l_2$-regularized Lasso. We characterize sufficient and necessary conditions on sample complexity \emph{as a sharp threshold} to guarantee successful recovery of the support union. Namely, if the sample size is above the threshold, then $l_1/l_2$-regularized Lasso correctly recovers the support union; and if the sample size is below the threshold, $l_1/l_2$-regularized Lasso fails to recover the support union. In particular, the threshold precisely captures the impact of the sparsity of regression vectors and the statistical properties of the design matrices on sample complexity. Therefore, the threshold function also captures the advantages of joint support union recovery using multi-task Lasso over individual support recovery using single-task Lasso. version:1
arxiv-1307-7981 | Likelihood-ratio calibration using prior-weighted proper scoring rules | http://arxiv.org/abs/1307.7981 | id:1307.7981 author:Niko BrÃ¼mmer, George Doddington category:stat.ML cs.LG  published:2013-07-30 summary:Prior-weighted logistic regression has become a standard tool for calibration in speaker recognition. Logistic regression is the optimization of the expected value of the logarithmic scoring rule. We generalize this via a parametric family of proper scoring rules. Our theoretical analysis shows how different members of this family induce different relative weightings over a spectrum of applications of which the decision thresholds range from low to high. Special attention is given to the interaction between prior weighting and proper scoring rule parameters. Experiments on NIST SRE'12 suggest that for applications with low false-alarm rate requirements, scoring rules tailored to emphasize higher score thresholds may give better accuracy than logistic regression. version:1
arxiv-1307-7973 | Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction | http://arxiv.org/abs/1307.7973 | id:1307.7973 author:Jason Weston, Antoine Bordes, Oksana Yakhnenko, Nicolas Usunier category:cs.CL cs.IR cs.LG  published:2013-07-30 summary:This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on two scoring functions that operate by learning low-dimensional embeddings of words and of entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over existing methods that rely on text features alone. version:1
arxiv-1307-7948 | On the accuracy of the Viterbi alignment | http://arxiv.org/abs/1307.7948 | id:1307.7948 author:Kristi Kuljus, JÃ¼ri Lember category:stat.ME cs.LG stat.CO  published:2013-07-30 summary:In a hidden Markov model, the underlying Markov chain is usually hidden. Often, the maximum likelihood alignment (Viterbi alignment) is used as its estimate. Although having the biggest likelihood, the Viterbi alignment can behave very untypically by passing states that are at most unexpected. To avoid such situations, the Viterbi alignment can be modified by forcing it not to pass these states. In this article, an iterative procedure for improving the Viterbi alignment is proposed and studied. The iterative approach is compared with a simple bunch approach where a number of states with low probability are all replaced at the same time. It can be seen that the iterative way of adjusting the Viterbi alignment is more efficient and it has several advantages over the bunch approach. The same iterative algorithm for improving the Viterbi alignment can be used in the case of peeping, that is when it is possible to reveal hidden states. In addition, lower bounds for classification probabilities of the Viterbi alignment under different conditions on the model parameters are studied. version:1
arxiv-1307-7897 | Energy Distribution of EEG Signals: EEG Signal Wavelet-Neural Network Classifier | http://arxiv.org/abs/1307.7897 | id:1307.7897 author:Ibrahim Omerhodzic, Samir Avdakovic, Amir Nuhanovic, Kemal Dizdarevic category:cs.NE q-bio.NC  published:2013-07-30 summary:In this paper, a wavelet-based neural network (WNN) classifier for recognizing EEG signals is implemented and tested under three sets EEG signals (healthy subjects, patients with epilepsy and patients with epileptic syndrome during the seizure). First, the Discrete Wavelet Transform (DWT) with the Multi-Resolution Analysis (MRA) is applied to decompose EEG signal at resolution levels of the components of the EEG signal (delta, theta, alpha, beta and gamma) and the Parsevals theorem are employed to extract the percentage distribution of energy features of the EEG signal at different resolution levels. Second, the neural network (NN) classifies these extracted features to identify the EEGs type according to the percentage distribution of energy features. The performance of the proposed algorithm has been evaluated using in total 300 EEG signals. The results showed that the proposed classifier has the ability of recognizing and classifying EEG signals efficiently. version:1
arxiv-1307-6170 | 6th International Symposium on Attention in Cognitive Systems 2013 | http://arxiv.org/abs/1307.6170 | id:1307.6170 author:Lucas Paletta, Laurent Itti, BjÃ¶rn Schuller, Fang Fang category:cs.CV  published:2013-07-22 summary:This volume contains the papers accepted at the 6th International Symposium on Attention in Cognitive Systems (ISACS 2013), held in Beijing, August 5, 2013. The aim of this symposium is to highlight the central role of attention on various kinds of performance in cognitive systems processing. It brings together researchers and developers from both academia and industry, from computer vision, robotics, perception psychology, psychophysics and neuroscience, in order to provide an interdisciplinary forum to present and communicate on computational models of attention, with the focus on interdependencies with visual cognition. Furthermore, it intends to investigate relevant objectives for performance comparison, to document and to investigate promising application domains, and to discuss visual attention with reference to other aspects of AI enabled systems. version:2
arxiv-1307-7852 | Scalable $k$-NN graph construction | http://arxiv.org/abs/1307.7852 | id:1307.7852 author:Jingdong Wang, Jing Wang, Gang Zeng, Zhuowen Tu, Rui Gan, Shipeng Li category:cs.CV cs.LG stat.ML  published:2013-07-30 summary:The $k$-NN graph has played a central role in increasingly popular data-driven techniques for various learning and vision tasks; yet, finding an efficient and effective way to construct $k$-NN graphs remains a challenge, especially for large-scale high-dimensional data. In this paper, we propose a new approach to construct approximate $k$-NN graphs with emphasis in: efficiency and accuracy. We hierarchically and randomly divide the data points into subsets and build an exact neighborhood graph over each subset, achieving a base approximate neighborhood graph; we then repeat this process for several times to generate multiple neighborhood graphs, which are combined to yield a more accurate approximate neighborhood graph. Furthermore, we propose a neighborhood propagation scheme to further enhance the accuracy. We show both theoretical and empirical accuracy and efficiency of our approach to $k$-NN graph construction and demonstrate significant speed-up in dealing with large scale visual data. version:1
arxiv-1307-7851 | Hybrid Affinity Propagation | http://arxiv.org/abs/1307.7851 | id:1307.7851 author:Jingdong Wang, Hao Xu, Xian-Sheng Hua, Shipeng Li category:cs.CV  published:2013-07-30 summary:In this paper, we address a problem of managing tagged images with hybrid summarization. We formulate this problem as finding a few image exemplars to represent the image set semantically and visually, and solve it in a hybrid way by exploiting both visual and textual information associated with images. We propose a novel approach, called homogeneous and heterogeneous message propagation ($\text{H}^\text{2}\text{MP}$). Similar to the affinity propagation (AP) approach, $\text{H}^\text{2}\text{MP}$ reduce the conventional \emph{vector} message propagation to \emph{scalar} message propagation to make the algorithm more efficient. Beyond AP that can only handle homogeneous data, $\text{H}^\text{2}\text{MP}$ generalizes it to exploit extra heterogeneous relations and the generalization is non-trivial as the reduction to scalar messages from vector messages is more challenging. The main advantages of our approach lie in 1) that $\text{H}^\text{2}\text{MP}$ exploits visual similarity and in addition the useful information from the associated tags, including the associations relation between images and tags and the relations within tags, and 2) that the summary is both visually and semantically satisfactory. In addition, our approach can also present a textual summary to a tagged image collection, which can be used to automatically generate a textual description. The experimental results demonstrate the effectiveness and efficiency of the roposed approach. version:1
arxiv-1307-7848 | An Integrated System for 3D Gaze Recovery and Semantic Analysis of Human Attention | http://arxiv.org/abs/1307.7848 | id:1307.7848 author:Lucas Paletta, Katrin Santner, Gerald Fritz category:cs.CV  published:2013-07-30 summary:This work describes a computer vision system that enables pervasive mapping and monitoring of human attention. The key contribution is that our methodology enables full 3D recovery of the gaze pointer, human view frustum and associated human centered measurements directly into an automatically computed 3D model in real-time. We apply RGB-D SLAM and descriptor matching methodologies for the 3D modeling, localization and fully automated annotation of ROIs (regions of interest) within the acquired 3D model. This innovative methodology will open new avenues for attention studies in real world environments, bringing new potential into automated processing for human factors technologies. version:1
arxiv-1307-7800 | Efficient Energy Minimization for Enforcing Statistics | http://arxiv.org/abs/1307.7800 | id:1307.7800 author:Yongsub Lim, Kyomin Jung, Pushmeet Kohli category:cs.CV  published:2013-07-30 summary:Energy minimization algorithms, such as graph cuts, enable the computation of the MAP solution under certain probabilistic models such as Markov random fields. However, for many computer vision problems, the MAP solution under the model is not the ground truth solution. In many problem scenarios, the system has access to certain statistics of the ground truth. For instance, in image segmentation, the area and boundary length of the object may be known. In these cases, we want to estimate the most probable solution that is consistent with such statistics, i.e., satisfies certain equality or inequality constraints. The above constrained energy minimization problem is NP-hard in general, and is usually solved using Linear Programming formulations, which relax the integrality constraints. This paper proposes a novel method that finds the discrete optimal solution of such problems by maximizing the corresponding Lagrangian dual. This method can be applied to any constrained energy minimization problem whose unconstrained version is polynomial time solvable, and can handle multiple, equality or inequality, and linear or non-linear constraints. We demonstrate the efficacy of our method on the foreground/background image segmentation problem, and show that it produces impressive segmentation results with less error, and runs more than 20 times faster than the state-of-the-art LP relaxation based approaches. version:1
arxiv-1307-7795 | Protein (Multi-)Location Prediction: Using Location Inter-Dependencies in a Probabilistic Framework | http://arxiv.org/abs/1307.7795 | id:1307.7795 author:Ramanuja Simha, Hagit Shatkay category:q-bio.QM cs.CE cs.LG q-bio.GN  published:2013-07-30 summary:Knowing the location of a protein within the cell is important for understanding its function, role in biological processes, and potential use as a drug target. Much progress has been made in developing computational methods that predict single locations for proteins, assuming that proteins localize to a single location. However, it has been shown that proteins localize to multiple locations. While a few recent systems have attempted to predict multiple locations of proteins, they typically treat locations as independent or capture inter-dependencies by treating each locations-combination present in the training set as an individual location-class. We present a new method and a preliminary system we have developed that directly incorporates inter-dependencies among locations into the multiple-location-prediction process, using a collection of Bayesian network classifiers. We evaluate our system on a dataset of single- and multi-localized proteins. Our results, obtained by incorporating inter-dependencies are significantly higher than those obtained by classifiers that do not use inter-dependencies. The performance of our system on multi-localized proteins is comparable to a top performing system (YLoc+), without restricting predictions to be based only on location-combinations present in the training set. version:1
arxiv-1307-7793 | Multi-dimensional Parametric Mincuts for Constrained MAP Inference | http://arxiv.org/abs/1307.7793 | id:1307.7793 author:Yongsub Lim, Kyomin Jung, Pushmeet Kohli category:cs.LG cs.AI  published:2013-07-30 summary:In this paper, we propose novel algorithms for inferring the Maximum a Posteriori (MAP) solution of discrete pairwise random field models under multiple constraints. We show how this constrained discrete optimization problem can be formulated as a multi-dimensional parametric mincut problem via its Lagrangian dual, and prove that our algorithm isolates all constraint instances for which the problem can be solved exactly. These multiple solutions enable us to even deal with `soft constraints' (higher order penalty functions). Moreover, we propose two practical variants of our algorithm to solve problems with hard constraints. We also show how our method can be applied to solve various constrained discrete optimization problems such as submodular minimization and shortest path computation. Experimental evaluation using the foreground-background image segmentation problem with statistic constraints reveals that our method is faster and its results are closer to the ground truth labellings compared with the popular continuous relaxation based methods. version:1
arxiv-1207-6684 | Group Iterative Spectrum Thresholding for Super-Resolution Sparse Spectral Selection | http://arxiv.org/abs/1207.6684 | id:1207.6684 author:Yiyuan She, Huanghuang Li, Jiangping Wang, Dapeng Wu category:stat.ML  published:2012-07-28 summary:Recently, sparsity-based algorithms are proposed for super-resolution spectrum estimation. However, to achieve adequately high resolution in real-world signal analysis, the dictionary atoms have to be close to each other in frequency, thereby resulting in a coherent design. The popular convex compressed sensing methods break down in presence of high coherence and large noise. We propose a new regularization approach to handle model collinearity and obtain parsimonious frequency selection simultaneously. It takes advantage of the pairing structure of sine and cosine atoms in the frequency dictionary. A probabilistic spectrum screening is also developed for fast computation in high dimensions. A data-resampling version of high-dimensional Bayesian Information Criterion is used to determine the regularization parameters. Experiments show the efficacy and efficiency of the proposed algorithms in challenging situations with small sample size, high frequency resolution, and low signal-to-noise ratio. version:2
arxiv-1303-5492 | Sample Distortion for Compressed Imaging | http://arxiv.org/abs/1303.5492 | id:1303.5492 author:Chunli Guo, Mike E. Davies category:cs.CV cs.IT math.IT  published:2013-03-22 summary:We propose the notion of a sample distortion (SD) function for independent and identically distributed (i.i.d) compressive distributions to fundamentally quantify the achievable reconstruction performance of compressed sensing for certain encoder-decoder pairs at a given sampling ratio. Two lower bounds on the achievable performance and the intrinsic convexity property is derived. A zeroing procedure is then introduced to improve non convex SD functions. The SD framework is then applied to analyse compressed imaging with a multi-resolution statistical image model using both the generalized Gaussian distribution and the two-state Gaussian mixture distribution. We subsequently focus on the Gaussian encoder-Bayesian optimal approximate message passing (AMP) decoder pair, whose theoretical SD function is provided by the rigorous analysis of the AMP algorithm. Given the image statistics, analytic bandwise sample allocation for bandwise independent model is derived as a reverse water-filling scheme. Som and Schniter's turbo message passing approach is further deployed to integrate the bandwise sampling with the exploitation of the hidden Markov tree structure of wavelet coefficients. Natural image simulations confirm that with oracle image statistics, the SD function associated with the optimized sample allocation can accurately predict the possible compressed sensing gains. Finally, a general sample allocation profile based on average image statistics not only illustrates preferable performance but also makes the scheme practical. version:2
arxiv-1307-7666 | Tight Lower Bounds for Homology Inference | http://arxiv.org/abs/1307.7666 | id:1307.7666 author:Sivaraman Balakrishnan, Alessandro Rinaldo, Aarti Singh, Larry Wasserman category:stat.ML cs.CG math.ST stat.TH  published:2013-07-29 summary:The homology groups of a manifold are important topological invariants that provide an algebraic summary of the manifold. These groups contain rich topological information, for instance, about the connected components, holes, tunnels and sometimes the dimension of the manifold. In earlier work, we have considered the statistical problem of estimating the homology of a manifold from noiseless samples and from noisy samples under several different noise models. We derived upper and lower bounds on the minimax risk for this problem. In this note we revisit the noiseless case. In previous work we used Le Cam's lemma to establish a lower bound that differed from the upper bound of Niyogi, Smale and Weinberger by a polynomial factor in the condition number. In this note we use a different construction based on the direct analysis of the likelihood ratio test to show that the upper bound of Niyogi, Smale and Weinberger is in fact tight, thus establishing rate optimal asymptotic minimax bounds for the problem. The techniques we use here extend in a straightforward way to the noisy settings considered in our earlier work. version:1
arxiv-1307-7474 | Automatic Mammogram image Breast Region Extraction and Removal of Pectoral Muscle | http://arxiv.org/abs/1307.7474 | id:1307.7474 author:R. Subash Chandra Boss, K. Thangavel, D. Arul Pon Daniel category:cs.CV  published:2013-07-29 summary:Currently Mammography is a most effective imaging modality used by radiologists for the screening of breast cancer. Finding an accurate, robust and efficient breast region segmentation technique still remains a challenging problem in digital mammography. Extraction of the breast profile region and the removal of pectoral muscle are essential pre-processing steps in Computer Aided Diagnosis (CAD) system for the diagnosis of breast cancer. Primarily it allows the search for abnormalities to be limited to the region of the breast tissue without undue influence from the background of the mammogram. The presence of pectoral muscle in mammograms biases detection procedures, which recommends removing the pectoral muscle during mammogram image pre-processing. The presence of pectoral muscle in mammograms may disturb or influence the detection of breast cancer as the pectoral muscle and mammographic parenchymas appear similar. The goal of breast region extraction is reducing the image size without losing anatomic information, it improve the accuracy of the overall CAD system. The main objective of this study is to propose an automated method to identify the pectoral muscle in Medio-Lateral Oblique (MLO) view mammograms. In this paper, we proposed histogram based 8-neighborhood connected component labelling method for breast region extraction and removal of pectoral muscle. The proposed method is evaluated by using the mean values of accuracy and error. The comparative analysis shows that the proposed method identifies the breast region more accurately. version:1
arxiv-1307-7466 | Integration of 3D Object Recognition and Planning for Robotic Manipulation: A Preliminary Report | http://arxiv.org/abs/1307.7466 | id:1307.7466 author:Damien Jade Duff, Esra Erdem, Volkan Patoglu category:cs.AI cs.CV cs.RO  published:2013-07-29 summary:We investigate different approaches to integrating object recognition and planning in a tabletop manipulation domain with the set of objects used in the 2012 RoboCup@Work competition. Results of our preliminary experiments show that, with some approaches, close integration of perception and planning improves the quality of plans, as well as the computation times of feasible plans. version:1
arxiv-1303-7410 | ParceLiNGAM: A causal ordering method robust against latent confounders | http://arxiv.org/abs/1303.7410 | id:1303.7410 author:Tatsuya Tashiro, Shohei Shimizu, Aapo Hyvarinen, Takashi Washio category:stat.ML  published:2013-03-29 summary:We consider learning a causal ordering of variables in a linear non-Gaussian acyclic model called LiNGAM. Several existing methods have been shown to consistently estimate a causal ordering assuming that all the model assumptions are correct. But, the estimation results could be distorted if some assumptions actually are violated. In this paper, we propose a new algorithm for learning causal orders that is robust against one typical violation of the model assumptions: latent confounders. The key idea is to detect latent confounders by testing independence between estimated external influences and find subsets (parcels) that include variables that are not affected by latent confounders. We demonstrate the effectiveness of our method using artificial data and simulated brain imaging data. version:2
arxiv-1307-8333 | Borel Isomorphic Dimensionality Reduction of Data and Supervised Learning | http://arxiv.org/abs/1307.8333 | id:1307.8333 author:Stan Hatko category:stat.ML  published:2013-07-29 summary:In this project we further investigate the idea of reducing the dimensionality of datasets using a Borel isomorphism with the purpose of subsequently applying supervised learning algorithms, as originally suggested by my supervisor V. Pestov (in 2011 Dagstuhl preprint). Any consistent learning algorithm, for example kNN, retains universal consistency after a Borel isomorphism is applied. A series of concrete examples of Borel isomorphisms that reduce the number of dimensions in a dataset is provided, based on multiplying the data by orthogonal matrices before the dimensionality reducing Borel isomorphism is applied. We test the accuracy of the resulting classifier in a lower dimensional space with various data sets. Working with a phoneme voice recognition dataset, of dimension 256 with 5 classes (phonemes), we show that a Borel isomorphic reduction to dimension 16 leads to a minimal drop in accuracy. In conclusion, we discuss further prospects of the method. version:1
arxiv-1307-7435 | A new approach in dynamic traveling salesman problem: a hybrid of ant colony optimization and descending gradient | http://arxiv.org/abs/1307.7435 | id:1307.7435 author:Farhad Soleimanian Gharehchopogh, Isa Maleki, Seyyed Reza Khaze category:cs.NE  published:2013-07-29 summary:Nowadays swarm intelligence-based algorithms are being used widely to optimize the dynamic traveling salesman problem (DTSP). In this paper, we have used mixed method of Ant Colony Optimization (AOC)and gradient descent to optimize DTSP which differs with ACO algorithm in evaporation rate and innovative data. This approach prevents premature convergence and scape from local optimum spots and also makes it possible to find better solutions for algorithm. In this paper, we are going to offer gradient descent and ACO algorithm which in comparison to some former methods it shows that algorithm has significantly improved routes optimization. version:1
arxiv-1307-7432 | Data mining application for cyber space users tendency in blog writing: a case study | http://arxiv.org/abs/1307.7432 | id:1307.7432 author:Farhad Soleimanian Gharehchopogh, Seyyed Reza Khaze category:cs.CY cs.LG  published:2013-07-29 summary:Blogs are the recent emerging media which relies on information technology and technological advance. Since the mass media in some less-developed and developing countries are in government service and their policies are developed based on governmental interests, so blogs are provided for ideas and exchanging opinions. In this paper, we highlighted performed simulations from obtained information from 100 users and bloggers in Kohkiloye and Boyer Ahmad Province and using Weka 3.6 tool and c4.5 algorithm by applying decision tree with more than %82 precision for getting future tendency anticipation of users to blogging and using in strategically areas. version:1
arxiv-1307-7429 | Participation anticipating in elections using data mining methods | http://arxiv.org/abs/1307.7429 | id:1307.7429 author:Amin Babazadeh Sangar, Seyyed Reza Khaze, Laya Ebrahimi category:cs.CY cs.LG  published:2013-07-29 summary:Anticipating the political behavior of people will be considerable help for election candidates to assess the possibility of their success and to be acknowledged about the public motivations to select them. In this paper, we provide a general schematic of the architecture of participation anticipating system in presidential election by using KNN, Classification Tree and Na\"ive Bayes and tools orange based on crisp which had hopeful output. To test and assess the proposed model, we begin to use the case study by selecting 100 qualified persons who attend in 11th presidential election of Islamic republic of Iran and anticipate their participation in Kohkiloye & Boyerahmad. We indicate that KNN can perform anticipation and classification processes with high accuracy in compared with two other algorithms to anticipate participation. version:1
arxiv-1212-4269 | Accelerated Time-of-Flight Mass Spectrometry | http://arxiv.org/abs/1212.4269 | id:1212.4269 author:Morteza Ibrahimi, Andrea Montanari, George S Moore category:math.OC cs.CE stat.ML  published:2012-12-18 summary:We study a simple modification to the conventional time of flight mass spectrometry (TOFMS) where a \emph{variable} and (pseudo)-\emph{random} pulsing rate is used which allows for traces from different pulses to overlap. This modification requires little alteration to the currently employed hardware. However, it requires a reconstruction method to recover the spectrum from highly aliased traces. We propose and demonstrate an efficient algorithm that can process massive TOFMS data using computational resources that can be considered modest with today's standards. This approach can be used to improve duty cycle, speed, and mass resolving power of TOFMS at the same time. We expect this to extend the applicability of TOFMS to new domains. version:2
arxiv-1307-7382 | Learning Frames from Text with an Unsupervised Latent Variable Model | http://arxiv.org/abs/1307.7382 | id:1307.7382 author:Brendan O'Connor category:cs.CL  published:2013-07-28 summary:We develop a probabilistic latent-variable model to discover semantic frames---types of events and their participants---from corpora. We present a Dirichlet-multinomial model in which frames are latent categories that explain the linking of verb-subject-object triples, given document-level sparsity. We analyze what the model learns, and compare it to FrameNet, noting it learns some novel and interesting frames. This document also contains a discussion of inference issues, including concentration parameter learning; and a small-scale error analysis of syntactic parsing accuracy. version:1
arxiv-1307-7303 | Learning to Understand by Evolving Theories | http://arxiv.org/abs/1307.7303 | id:1307.7303 author:Martin E. Mueller, Madhura D. Thosar category:cs.LG cs.AI  published:2013-07-27 summary:In this paper, we describe an approach that enables an autonomous system to infer the semantics of a command (i.e. a symbol sequence representing an action) in terms of the relations between changes in the observations and the action instances. We present a method of how to induce a theory (i.e. a semantic description) of the meaning of a command in terms of a minimal set of background knowledge. The only thing we have is a sequence of observations from which we extract what kinds of effects were caused by performing the command. This way, we yield a description of the semantics of the action and, hence, a definition. version:1
arxiv-1209-2355 | Counterfactual Reasoning and Learning Systems | http://arxiv.org/abs/1209.2355 | id:1209.2355 author:LÃ©on Bottou, Jonas Peters, Joaquin QuiÃ±onero-Candela, Denis X. Charles, D. Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed Snelson category:cs.LG cs.AI cs.IR math.ST stat.TH  published:2012-09-11 summary:This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select changes that improve both the short-term and long-term performance of such systems. This work is illustrated by experiments carried out on the ad placement system associated with the Bing search engine. version:5
arxiv-1307-7286 | A Review of Machine Learning based Anomaly Detection Techniques | http://arxiv.org/abs/1307.7286 | id:1307.7286 author:Harjinder Kaur, Gurpreet Singh, Jaspreet Minhas category:cs.LG cs.CR  published:2013-07-27 summary:Intrusion detection is so much popular since the last two decades where intrusion is attempted to break into or misuse the system. It is mainly of two types based on the intrusions, first is Misuse or signature based detection and the other is Anomaly detection. In this paper Machine learning based methods which are one of the types of Anomaly detection techniques is discussed. version:1
arxiv-1307-7198 | Self-Learning for Player Localization in Sports Video | http://arxiv.org/abs/1307.7198 | id:1307.7198 author:Kenji Okuma, David G. Lowe, James J. Little category:cs.CV cs.AI  published:2013-07-27 summary:This paper introduces a novel self-learning framework that automates the label acquisition process for improving models for detecting players in broadcast footage of sports games. Unlike most previous self-learning approaches for improving appearance-based object detectors from videos, we allow an unknown, unconstrained number of target objects in a more generalized video sequence with non-static camera views. Our self-learning approach uses a latent SVM learning algorithm and deformable part models to represent the shape and colour information of players, constraining their motions, and learns the colour of the playing field by a gentle Adaboost algorithm. We combine those image cues and discover additional labels automatically from unlabelled data. In our experiments, our approach exploits both labelled and unlabelled data in sparsely labelled videos of sports games, providing a mean performance improvement of over 20% in the average precision for detecting sports players and improved tracking, when videos contain very few labelled images. version:1
arxiv-1307-7192 | MixedGrad: An O(1/T) Convergence Rate Algorithm for Stochastic Smooth Optimization | http://arxiv.org/abs/1307.7192 | id:1307.7192 author:Mehrdad Mahdavi, Rong Jin category:cs.LG math.OC  published:2013-07-26 summary:It is well known that the optimal convergence rate for stochastic optimization of smooth functions is $O(1/\sqrt{T})$, which is same as stochastic optimization of Lipschitz continuous convex functions. This is in contrast to optimizing smooth functions using full gradients, which yields a convergence rate of $O(1/T^2)$. In this work, we consider a new setup for optimizing smooth functions, termed as {\bf Mixed Optimization}, which allows to access both a stochastic oracle and a full gradient oracle. Our goal is to significantly improve the convergence rate of stochastic optimization of smooth functions by having an additional small number of accesses to the full gradient oracle. We show that, with an $O(\ln T)$ calls to the full gradient oracle and an $O(T)$ calls to the stochastic oracle, the proposed mixed optimization algorithm is able to achieve an optimization error of $O(1/T)$. version:1
arxiv-1307-7050 | A Comprehensive Evaluation of Machine Learning Techniques for Cancer Class Prediction Based on Microarray Data | http://arxiv.org/abs/1307.7050 | id:1307.7050 author:Khalid Raza, Atif N Hasan category:cs.LG cs.CE  published:2013-07-26 summary:Prostate cancer is among the most common cancer in males and its heterogeneity is well known. Its early detection helps making therapeutic decision. There is no standard technique or procedure yet which is full-proof in predicting cancer class. The genomic level changes can be detected in gene expression data and those changes may serve as standard model for any random cancer data for class prediction. Various techniques were implied on prostate cancer data set in order to accurately predict cancer class including machine learning techniques. Huge number of attributes and few number of sample in microarray data leads to poor machine learning, therefore the most challenging part is attribute reduction or non significant gene reduction. In this work we have compared several machine learning techniques for their accuracy in predicting the cancer class. Machine learning is effective when number of attributes (genes) are larger than the number of samples which is rarely possible with gene expression data. Attribute reduction or gene filtering is absolutely required in order to make the data more meaningful as most of the genes do not participate in tumor development and are irrelevant for cancer prediction. Here we have applied combination of statistical techniques such as inter-quartile range and t-test, which has been effective in filtering significant genes and minimizing noise from data. Further we have done a comprehensive evaluation of ten state-of-the-art machine learning techniques for their accuracy in class prediction of prostate cancer. Out of these techniques, Bayes Network out performed with an accuracy of 94.11% followed by Navie Bayes with an accuracy of 91.17%. To cross validate our results, we modified our training dataset in six different way and found that average sensitivity, specificity, precision and accuracy of Bayes Network is highest among all other techniques used. version:1
arxiv-1307-7028 | Infinite Mixtures of Multivariate Gaussian Processes | http://arxiv.org/abs/1307.7028 | id:1307.7028 author:Shiliang Sun category:cs.LG stat.ML  published:2013-07-26 summary:This paper presents a new model called infinite mixtures of multivariate Gaussian processes, which can be used to learn vector-valued functions and applied to multitask learning. As an extension of the single multivariate Gaussian process, the mixture model has the advantages of modeling multimodal data and alleviating the computationally cubic complexity of the multivariate Gaussian process. A Dirichlet process prior is adopted to allow the (possibly infinite) number of mixture components to be automatically inferred from training data, and Markov chain Monte Carlo sampling techniques are used for parameter and latent variable inference. Preliminary experimental results on multivariate regression show the feasibility of the proposed model. version:1
arxiv-1307-7024 | Multi-view Laplacian Support Vector Machines | http://arxiv.org/abs/1307.7024 | id:1307.7024 author:Shiliang Sun category:cs.LG stat.ML  published:2013-07-26 summary:We propose a new approach, multi-view Laplacian support vector machines (SVMs), for semi-supervised learning under the multi-view scenario. It integrates manifold regularization and multi-view regularization into the usual formulation of SVMs and is a natural extension of SVMs from supervised learning to multi-view semi-supervised learning. The function optimization problem in a reproducing kernel Hilbert space is converted to an optimization in a finite-dimensional Euclidean space. After providing a theoretical bound for the generalization performance of the proposed method, we further give a formulation of the empirical Rademacher complexity which affects the bound significantly. From this bound and the empirical Rademacher complexity, we can gain insights into the roles played by different regularization terms to the generalization performance. Experimental results on synthetic and real-world data sets are presented, which validate the effectiveness of the proposed multi-view Laplacian SVMs approach. version:1
arxiv-1307-6937 | A Novel Architecture For Question Classification Based Indexing Scheme For Efficient Question Answering | http://arxiv.org/abs/1307.6937 | id:1307.6937 author:Renu Mudgal, Rosy Madaan, A. K. Sharma, Ashutosh Dixit category:cs.IR cs.CL  published:2013-07-26 summary:Question answering system can be seen as the next step in information retrieval, allowing users to pose question in natural language and receive compact answers. For the Question answering system to be successful, research has shown that the correct classification of question with respect to the expected answer type is requisite. We propose a novel architecture for question classification and searching in the index, maintained on the basis of expected answer types, for efficient question answering. The system uses the criteria for Answer Relevance Score for finding the relevance of each answer returned by the system. On analysis of the proposed system, it has been found that the system has shown promising results than the existing systems based on question classification. version:1
arxiv-1201-5283 | An Efficient Primal-Dual Prox Method for Non-Smooth Optimization | http://arxiv.org/abs/1201.5283 | id:1201.5283 author:Tianbao Yang, Mehrdad Mahdavi, Rong Jin, Shenghuo Zhu category:cs.LG  published:2012-01-24 summary:We study the non-smooth optimization problems in machine learning, where both the loss function and the regularizer are non-smooth functions. Previous studies on efficient empirical loss minimization assume either a smooth loss function or a strongly convex regularizer, making them unsuitable for non-smooth optimization. We develop a simple yet efficient method for a family of non-smooth optimization problems where the dual form of the loss function is bilinear in primal and dual variables. We cast a non-smooth optimization problem into a minimax optimization problem, and develop a primal dual prox method that solves the minimax optimization problem at a rate of $O(1/T)$ {assuming that the proximal step can be efficiently solved}, significantly faster than a standard subgradient descent method that has an $O(1/\sqrt{T})$ convergence rate. Our empirical study verifies the efficiency of the proposed method for various non-smooth optimization problems that arise ubiquitously in machine learning by comparing it to the state-of-the-art first order methods. version:5
arxiv-1307-6921 | Memcapacitive neural networks | http://arxiv.org/abs/1307.6921 | id:1307.6921 author:Y. V. Pershin, M. Di Ventra category:cond-mat.dis-nn cs.ET cs.NE q-bio.NC  published:2013-07-26 summary:We show that memcapacitive (memory capacitive) systems can be used as synapses in artificial neural networks. As an example of our approach, we discuss the architecture of an integrate-and-fire neural network based on memcapacitive synapses. Moreover, we demonstrate that the spike-timing-dependent plasticity can be simply realized with some of these devices. Memcapacitive synapses are a low-energy alternative to memristive synapses for neuromorphic computation. version:1
arxiv-1307-6887 | Sequential Transfer in Multi-armed Bandit with Finite Set of Models | http://arxiv.org/abs/1307.6887 | id:1307.6887 author:Mohammad Gheshlaghi Azar, Alessandro Lazaric, Emma Brunskill category:stat.ML cs.LG  published:2013-07-25 summary:Learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents. Although results in supervised and reinforcement learning show that transfer may significantly improve the learning performance, most of the literature on transfer is focused on batch learning tasks. In this paper we study the problem of \textit{sequential transfer in online learning}, notably in the multi-armed bandit framework, where the objective is to minimize the cumulative regret over a sequence of tasks by incrementally transferring knowledge from prior tasks. We introduce a novel bandit algorithm based on a method-of-moments approach for the estimation of the possible tasks and derive regret bounds for it. version:1
arxiv-1307-6814 | A Propound Method for the Improvement of Cluster Quality | http://arxiv.org/abs/1307.6814 | id:1307.6814 author:Shveta Kundra Bhatia, V. S. Dixit category:cs.LG  published:2013-07-25 summary:In this paper Knockout Refinement Algorithm (KRA) is proposed to refine original clusters obtained by applying SOM and K-Means clustering algorithms. KRA Algorithm is based on Contingency Table concepts. Metrics are computed for the Original and Refined Clusters. Quality of Original and Refined Clusters are compared in terms of metrics. The proposed algorithm (KRA) is tested in the educational domain and results show that it generates better quality clusters in terms of improved metric values. version:1
arxiv-1307-6726 | Information content versus word length in natural language: A reply to Ferrer-i-Cancho and Moscoso del Prado Martin [arXiv:1209.1751] | http://arxiv.org/abs/1307.6726 | id:1307.6726 author:Steven T. Piantadosi, Harry Tily, Edward Gibson category:cs.CL math.PR physics.data-an  published:2013-07-25 summary:Recently, Ferrer i Cancho and Moscoso del Prado Martin [arXiv:1209.1751] argued that an observed linear relationship between word length and average surprisal (Piantadosi, Tily, & Gibson, 2011) is not evidence for communicative efficiency in human language. We discuss several shortcomings of their approach and critique: their model critically rests on inaccurate assumptions, is incapable of explaining key surprisal patterns in language, and is incompatible with recent behavioral results. More generally, we argue that statistical models must not critically rely on assumptions that are incompatible with the real system under study. version:1
arxiv-1307-6616 | Does generalization performance of $l^q$ regularization learning depend on $q$? A negative example | http://arxiv.org/abs/1307.6616 | id:1307.6616 author:Shaobo Lin, Chen Xu, Jingshan Zeng, Jian Fang category:cs.LG stat.ML  published:2013-07-25 summary:$l^q$-regularization has been demonstrated to be an attractive technique in machine learning and statistical modeling. It attempts to improve the generalization (prediction) capability of a machine (model) through appropriately shrinking its coefficients. The shape of a $l^q$ estimator differs in varying choices of the regularization order $q$. In particular, $l^1$ leads to the LASSO estimate, while $l^{2}$ corresponds to the smooth ridge regression. This makes the order $q$ a potential tuning parameter in applications. To facilitate the use of $l^{q}$-regularization, we intend to seek for a modeling strategy where an elaborative selection on $q$ is avoidable. In this spirit, we place our investigation within a general framework of $l^{q}$-regularized kernel learning under a sample dependent hypothesis space (SDHS). For a designated class of kernel functions, we show that all $l^{q}$ estimators for $0< q < \infty$ attain similar generalization error bounds. These estimated bounds are almost optimal in the sense that up to a logarithmic factor, the upper and lower bounds are asymptotically identical. This finding tentatively reveals that, in some modeling contexts, the choice of $q$ might not have a strong impact in terms of the generalization capability. From this perspective, $q$ can be arbitrarily specified, or specified merely by other no generalization criteria like smoothness, computational complexity, sparsity, etc.. version:1
arxiv-1304-3285 | Scaling the Indian Buffet Process via Submodular Maximization | http://arxiv.org/abs/1304.3285 | id:1304.3285 author:Colorado Reed, Zoubin Ghahramani category:stat.ML cs.LG  published:2013-04-11 summary:Inference for latent feature models is inherently difficult as the inference space grows exponentially with the size of the input data and number of latent features. In this work, we use Kurihara & Welling (2008)'s maximization-expectation framework to perform approximate MAP inference for linear-Gaussian latent feature models with an Indian Buffet Process (IBP) prior. This formulation yields a submodular function of the features that corresponds to a lower bound on the model evidence. By adding a constant to this function, we obtain a nonnegative submodular function that can be maximized via a greedy algorithm that obtains at least a one-third approximation to the optimal solution. Our inference method scales linearly with the size of the input data, and we show the efficacy of our method on the largest datasets currently analyzed using an IBP model. version:4
arxiv-1307-6522 | When is the majority-vote classifier beneficial? | http://arxiv.org/abs/1307.6522 | id:1307.6522 author:Mu Zhu category:math.ST stat.ML stat.TH  published:2013-07-24 summary:In his seminal work, Schapire (1990) proved that weak classifiers could be improved to achieve arbitrarily high accuracy, but he never implied that a simple majority-vote mechanism could always do the trick. By comparing the asymptotic misclassification error of the majority-vote classifier with the average individual error, we discover an interesting phase-transition phenomenon. For binary classification with equal prior probabilities, our result implies that, for the majority-vote mechanism to work, the collection of weak classifiers must meet the minimum requirement of having an average true positive rate of at least 50% and an average false positive rate of at most 50%. version:1
arxiv-1307-6515 | Cluster Trees on Manifolds | http://arxiv.org/abs/1307.6515 | id:1307.6515 author:Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti Singh, Larry Wasserman category:stat.ML cs.LG  published:2013-07-24 summary:In this paper we investigate the problem of estimating the cluster tree for a density $f$ supported on or near a smooth $d$-dimensional manifold $M$ isometrically embedded in $\mathbb{R}^D$. We analyze a modified version of a $k$-nearest neighbor based algorithm recently proposed by Chaudhuri and Dasgupta. The main results of this paper show that under mild assumptions on $f$ and $M$, we obtain rates of convergence that depend on $d$ only but not on the ambient dimension $D$. We also show that similar (albeit non-algorithmic) results can be obtained for kernel density estimators. We sketch a construction of a sample complexity lower bound instance for a natural class of manifold oblivious clustering algorithms. We further briefly consider the known manifold case and show that in this case a spatially adaptive algorithm achieves better rates. version:1
arxiv-1307-6163 | Human and Automatic Evaluation of English-Hindi Machine Translation | http://arxiv.org/abs/1307.6163 | id:1307.6163 author:Nisheeth Joshi, Hemant Darbari, Iti Mathur category:cs.CL  published:2013-07-23 summary:For the past 60 years, Research in machine translation is going on. For the development in this field, a lot of new techniques are being developed each day. As a result, we have witnessed development of many automatic machine translators. A manager of machine translation development project needs to know the performance increase/decrease, after changes have been done in his system. Due to this reason, a need for evaluation of machine translation systems was felt. In this article, we shall present the evaluation of some machine translators. This evaluation will be done by a human evaluator and by some automatic evaluation metrics, which will be done at sentence, document and system level. In the end we shall also discuss the comparison between the evaluations. version:2
arxiv-1307-6143 | Generative, Fully Bayesian, Gaussian, Openset Pattern Classifier | http://arxiv.org/abs/1307.6143 | id:1307.6143 author:Niko Brummer category:stat.ML cs.LG  published:2013-07-23 summary:This report works out the details of a closed-form, fully Bayesian, multiclass, openset, generative pattern classifier using multivariate Gaussian likelihoods, with conjugate priors. The generative model has a common within-class covariance, which is proportional to the between-class covariance in the conjugate prior. The scalar proportionality constant is the only plugin parameter. All other model parameters are intergated out in closed form. An expression is given for the model evidence, which can be used to make plugin estimates for the proportionality constant. Pattern recognition is done via the predictive likeihoods of classes for which training data is available, as well as a predicitve likelihood for any as yet unseen class. version:2
arxiv-1307-6410 | Storing non-uniformly distributed messages in networks of neural cliques | http://arxiv.org/abs/1307.6410 | id:1307.6410 author:Bartosz Boguslawski, Vincent Gripon, Fabrice Seguin, FrÃ©dÃ©ric Heitzmann category:cs.NE cs.SY  published:2013-07-24 summary:Associative memories are data structures that allow retrieval of stored messages from part of their content. They thus behave similarly to human brain that is capable for instance of retrieving the end of a song given its beginning. Among different families of associative memories, sparse ones are known to provide the best efficiency (ratio of the number of bits stored to that of bits used). Nevertheless, it is well known that non-uniformity of the stored messages can lead to dramatic decrease in performance. We introduce several strategies to allow efficient storage of non-uniform messages in recently introduced sparse associative memories. We analyse and discuss the methods introduced. We also present a practical application example. version:1
arxiv-1307-6303 | Matching-Constrained Active Contours | http://arxiv.org/abs/1307.6303 | id:1307.6303 author:Junyan Wang, Kap Luk Chan category:cs.CV  published:2013-07-24 summary:In object segmentation by active contours, the initial contour is often required. Conventionally, the initial contour is provided by the user. This paper extends the conventional active contour model by incorporating feature matching in the formulation, which gives rise to a novel matching-constrained active contour. The numerical solution to the new optimization model provides an automated framework of object segmentation without user intervention. The main idea is to incorporate feature point matching as a constraint in active contour models. To this effect, we obtain a mathematical model of interior points to boundary contour such that matching of interior feature points gives contour alignment, and we formulate the matching score as a constraint to active contour model such that the feature matching of maximum score that gives the contour alignment provides the initial feasible solution to the constrained optimization model of segmentation. The constraint also ensures that the optimal contour does not deviate too much from the initial contour. Projected-gradient descent equations are derived to solve the constrained optimization. In the experiments, we show that our method is capable of achieving the automatic object segmentation, and it outperforms the related methods. version:1
arxiv-1305-3814 | Multi-View Learning for Web Spam Detection | http://arxiv.org/abs/1305.3814 | id:1305.3814 author:Ali Hadian, Behrouz Minaei-Bidgoli category:cs.IR cs.LG  published:2013-05-16 summary:Spam pages are designed to maliciously appear among the top search results by excessive usage of popular terms. Therefore, spam pages should be removed using an effective and efficient spam detection system. Previous methods for web spam classification used several features from various information sources (page contents, web graph, access logs, etc.) to detect web spam. In this paper, we follow page-level classification approach to build fast and scalable spam filters. We show that each web page can be classified with satisfiable accuracy using only its own HTML content. In order to design a multi-view classification system, we used state-of-the-art spam classification methods with distinct feature sets (views) as the base classifiers. Then, a fusion model is learned to combine the output of the base classifiers and make final prediction. Results show that multi-view learning significantly improves the classification performance, namely AUC by 22%, while providing linear speedup for parallel execution. version:2
arxiv-1203-6276 | A Multi-objective Exploratory Procedure for Regression Model Selection | http://arxiv.org/abs/1203.6276 | id:1203.6276 author:Ankur Sinha, Pekka Malo, Timo Kuosmanen category:stat.CO cs.NE stat.AP G.3; G.1.6  published:2012-03-28 summary:Variable selection is recognized as one of the most critical steps in statistical modeling. The problems encountered in engineering and social sciences are commonly characterized by over-abundance of explanatory variables, non-linearities and unknown interdependencies between the regressors. An added difficulty is that the analysts may have little or no prior knowledge on the relative importance of the variables. To provide a robust method for model selection, this paper introduces the Multi-objective Genetic Algorithm for Variable Selection (MOGA-VS) that provides the user with an optimal set of regression models for a given data-set. The algorithm considers the regression problem as a two objective task, and explores the Pareto-optimal (best subset) models by preferring those models over the other which have less number of regression coefficients and better goodness of fit. The model exploration can be performed based on in-sample or generalization error minimization. The model selection is proposed to be performed in two steps. First, we generate the frontier of Pareto-optimal regression models by eliminating the dominated models without any user intervention. Second, a decision making process is executed which allows the user to choose the most preferred model using visualizations and simple metrics. The method has been evaluated on a recently published real dataset on Communities and Crime within United States. version:3
arxiv-1307-5336 | Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts | http://arxiv.org/abs/1307.5336 | id:1307.5336 author:Pekka Malo, Ankur Sinha, Pyry Takala, Pekka Korhonen, Jyrki Wallenius category:cs.CL cs.IR q-fin.CP I.2.7  published:2013-07-19 summary:The use of robo-readers to analyze news texts is an emerging technology trend in computational finance. In recent research, a substantial effort has been invested to develop sophisticated financial polarity-lexicons that can be used to investigate how financial sentiments relate to future company performance. However, based on experience from other fields, where sentiment analysis is commonly applied, it is well-known that the overall semantic orientation of a sentence may differ from the prior polarity of individual words. The objective of this article is to investigate how semantic orientations can be better detected in financial and economic news by accommodating the overall phrase-structure information and domain-specific use of language. Our three main contributions are: (1) establishment of a human-annotated finance phrase-bank, which can be used as benchmark for training and evaluating alternative models; (2) presentation of a technique to enhance financial lexicons with attributes that help to identify expected direction of events that affect overall sentiment; (3) development of a linearized phrase-structure model for detecting contextual semantic orientations in financial and economic news texts. The relevance of the newly added lexicon features and the benefit of using the proposed learning-algorithm are demonstrated in a comparative study against previously used general sentiment models as well as the popular word frequency models used in recent financial studies. The proposed framework is parsimonious and avoids the explosion in feature-space caused by the use of conventional n-gram features. version:2
arxiv-1307-4514 | Supervised Metric Learning with Generalization Guarantees | http://arxiv.org/abs/1307.4514 | id:1307.4514 author:AurÃ©lien Bellet category:cs.LG stat.ML  published:2013-07-17 summary:The crucial importance of metrics in machine learning algorithms has led to an increasing interest in optimizing distance and similarity functions, an area of research known as metric learning. When data consist of feature vectors, a large body of work has focused on learning a Mahalanobis distance. Less work has been devoted to metric learning from structured objects (such as strings or trees), most of it focusing on optimizing a notion of edit distance. We identify two important limitations of current metric learning approaches. First, they allow to improve the performance of local algorithms such as k-nearest neighbors, but metric learning for global algorithms (such as linear classifiers) has not been studied so far. Second, the question of the generalization ability of metric learning methods has been largely ignored. In this thesis, we propose theoretical and algorithmic contributions that address these limitations. Our first contribution is the derivation of a new kernel function built from learned edit probabilities. Our second contribution is a novel framework for learning string and tree edit similarities inspired by the recent theory of (e,g,t)-good similarity functions. Using uniform stability arguments, we establish theoretical guarantees for the learned similarity that give a bound on the generalization error of a linear classifier built from that similarity. In our third contribution, we extend these ideas to metric learning from feature vectors by proposing a bilinear similarity learning method that efficiently optimizes the (e,g,t)-goodness. Generalization guarantees are derived for our approach, highlighting that our method minimizes a tighter bound on the generalization error of the classifier. Our last contribution is a framework for establishing generalization bounds for a large class of existing metric learning algorithms based on a notion of algorithmic robustness. version:2
arxiv-1303-6163 | Machine learning of hierarchical clustering to segment 2D and 3D images | http://arxiv.org/abs/1303.6163 | id:1303.6163 author:Juan Nunez-Iglesias, Ryan Kennedy, Toufiq Parag, Jianbo Shi, Dmitri B. Chklovskii category:cs.CV cs.LG  published:2013-03-25 summary:We aim to improve segmentation through the use of machine learning tools during region agglomeration. We propose an active learning approach for performing hierarchical agglomerative segmentation from superpixels. Our method combines multiple features at all scales of the agglomerative process, works for data with an arbitrary number of dimensions, and scales to very large datasets. We advocate the use of variation of information to measure segmentation accuracy, particularly in 3D electron microscopy (EM) images of neural tissue, and using this metric demonstrate an improvement over competing algorithms in EM and natural images. version:3
arxiv-1307-6008 | Numerical Methods for Coupled Reconstruction and Registration in Digital Breast Tomosynthesis | http://arxiv.org/abs/1307.6008 | id:1307.6008 author:Guang Yang, John H. Hipwell, David J. Hawkes, Simon R. Arridge category:cs.CV physics.med-ph  published:2013-07-23 summary:Digital Breast Tomosynthesis (DBT) provides an insight into the fine details of normal fibroglandular tissues and abnormal lesions by reconstructing a pseudo-3D image of the breast. In this respect, DBT overcomes a major limitation of conventional X-ray mammography by reducing the confounding effects caused by the superposition of breast tissue. In a breast cancer screening or diagnostic context, a radiologist is interested in detecting change, which might be indicative of malignant disease. To help automate this task image registration is required to establish spatial correspondence between time points. Typically, images, such as MRI or CT, are first reconstructed and then registered. This approach can be effective if reconstructing using a complete set of data. However, for ill-posed, limited-angle problems such as DBT, estimating the deformation is complicated by the significant artefacts associated with the reconstruction, leading to severe inaccuracies in the registration. This paper presents a mathematical framework, which couples the two tasks and jointly estimates both image intensities and the parameters of a transformation. We evaluate our methods using various computational digital phantoms, uncompressed breast MR images, and in-vivo DBT simulations. Firstly, we compare both iterative and simultaneous methods to the conventional, sequential method using an affine transformation model. We show that jointly estimating image intensities and parametric transformations gives superior results with respect to reconstruction fidelity and registration accuracy. Also, we incorporate a non-rigid B-spline transformation model into our simultaneous method. The results demonstrate a visually plausible recovery of the deformation with preservation of the reconstruction fidelity. version:1
arxiv-1305-0208 | Perceptron Mistake Bounds | http://arxiv.org/abs/1305.0208 | id:1305.0208 author:Mehryar Mohri, Afshin Rostamizadeh category:cs.LG  published:2013-05-01 summary:We present a brief survey of existing mistake bounds and introduce novel bounds for the Perceptron or the kernel Perceptron algorithm. Our novel bounds generalize beyond standard margin-loss type bounds, allow for any convex and Lipschitz loss function, and admit a very simple proof. version:2
arxiv-1307-5800 | An Adaptive GMM Approach to Background Subtraction for Application in Real Time Surveillance | http://arxiv.org/abs/1307.5800 | id:1307.5800 author:Subra Mukherjee, Karen Das category:cs.CV  published:2013-07-22 summary:Efficient security management has become an important parameter in todays world. As the problem is growing, there is an urgent need for the introduction of advanced technology and equipment to improve the state-of art of surveillance. In this paper we propose a model for real time background subtraction using AGMM. The proposed model is robust and adaptable to dynamic background, fast illumination changes, repetitive motion. Also we have incorporated a method for detecting shadows using the Horpresert color model. The proposed model can be employed for monitoring areas where movement or entry is highly restricted. So on detection of any unexpected events in the scene an alarm can be triggered and hence we can achieve real time surveillance even in the absence of constant human monitoring. version:1
arxiv-1205-0310 | Bayesian inference for logistic models using Polya-Gamma latent variables | http://arxiv.org/abs/1205.0310 | id:1205.0310 author:Nicholas G. Polson, James G. Scott, Jesse Windle category:stat.ME stat.CO stat.ML  published:2012-05-02 summary:We propose a new data-augmentation strategy for fully Bayesian inference in models with binomial likelihoods. The approach appeals to a new class of Polya-Gamma distributions, which are constructed in detail. A variety of examples are presented to show the versatility of the method, including logistic regression, negative binomial regression, nonlinear mixed-effects models, and spatial models for count data. In each case, our data-augmentation strategy leads to simple, effective methods for posterior inference that: (1) circumvent the need for analytic approximations, numerical integration, or Metropolis-Hastings; and (2) outperform other known data-augmentation strategies, both in ease of use and in computational efficiency. All methods, including an efficient sampler for the Polya-Gamma distribution, are implemented in the R package BayesLogit. In the technical supplement appended to the end of the paper, we provide further details regarding the generation of Polya-Gamma random variables; the empirical benchmarks reported in the main manuscript; and the extension of the basic data-augmentation framework to contingency tables and multinomial outcomes. version:3
arxiv-1307-5748 | Appearance Descriptors for Person Re-identification: a Comprehensive Review | http://arxiv.org/abs/1307.5748 | id:1307.5748 author:Riccardo Satta category:cs.CV  published:2013-07-22 summary:In video-surveillance, person re-identification is the task of recognising whether an individual has already been observed over a network of cameras. Typically, this is achieved by exploiting the clothing appearance, as classical biometric traits like the face are impractical in real-world video surveillance scenarios. Clothing appearance is represented by means of low-level \textit{local} and/or \textit{global} features of the image, usually extracted according to some part-based body model to treat different body parts (e.g. torso and legs) independently. This paper provides a comprehensive review of current approaches to build appearance descriptors for person re-identification. The most relevant techniques are described in detail, and categorised according to the body models and features used. The aim of this work is to provide a structured body of knowledge and a starting point for researchers willing to conduct novel investigations on this challenging topic. version:1
arxiv-1307-5730 | A New Strategy of Cost-Free Learning in the Class Imbalance Problem | http://arxiv.org/abs/1307.5730 | id:1307.5730 author:Xiaowan Zhang, Bao-Gang Hu category:cs.LG  published:2013-07-22 summary:In this work, we define cost-free learning (CFL) formally in comparison with cost-sensitive learning (CSL). The main difference between them is that a CFL approach seeks optimal classification results without requiring any cost information, even in the class imbalance problem. In fact, several CFL approaches exist in the related studies, such as sampling and some criteria-based pproaches. However, to our best knowledge, none of the existing CFL and CSL approaches are able to process the abstaining classifications properly when no information is given about errors and rejects. Based on information theory, we propose a novel CFL which seeks to maximize normalized mutual information of the targets and the decision outputs of classifiers. Using the strategy, we can deal with binary/multi-class classifications with/without abstaining. Significant features are observed from the new strategy. While the degree of class imbalance is changing, the proposed strategy is able to balance the errors and rejects accordingly and automatically. Another advantage of the strategy is its ability of deriving optimal rejection thresholds for abstaining classifications and the "equivalent" costs in binary classifications. The connection between rejection thresholds and ROC curve is explored. Empirical investigation is made on several benchmark data sets in comparison with other existing approaches. The classification results demonstrate a promising perspective of the strategy in machine learning. version:1
arxiv-1307-5720 | Top-down and Bottom-up Feature Combination for Multi-sensor Attentive Robots | http://arxiv.org/abs/1307.5720 | id:1307.5720 author:Esther L. Colombini, Alexandre S. SimÃµes, Carlos H. C. Ribeiro category:cs.RO cs.CV  published:2013-07-22 summary:The information available to robots in real tasks is widely distributed both in time and space, requiring the agent to search for relevant data. In humans, that face the same problem when sounds, images and smells are presented to their sensors in a daily scene, a natural system is applied: Attention. As vision plays an important role in our routine, most research regarding attention has involved this sensorial system and the same has been replicated to the robotics field. However,most of the robotics tasks nowadays do not rely only in visual data, that are still costly. To allow the use of attentive concepts with other robotics sensors that are usually used in tasks such as navigation, self-localization, searching and mapping, a generic attentional model has been previously proposed. In this work, feature mapping functions were designed to build feature maps to this attentive model from data from range scanner and sonar sensors. Experiments were performed in a high fidelity simulated robotics environment and results have demonstrated the capability of the model on dealing with both salient stimuli and goal-driven attention over multiple features extracted from multiple sensors. version:1
arxiv-1307-5713 | Understanding Humans' Strategies in Maze Solving | http://arxiv.org/abs/1307.5713 | id:1307.5713 author:Min Zhao, Andre G. Marquez category:cs.CV cs.AI q-bio.NC  published:2013-07-22 summary:Navigating through a visual maze relies on the strategic use of eye movements to select and identify the route. When navigating the maze, there are trade-offs between exploring to the environment and relying on memory. This study examined strategies used to navigating through novel and familiar mazes that were viewed from above and traversed by a mouse cursor. Eye and mouse movements revealed two modes that almost never occurred concurrently: exploration and guidance. Analyses showed that people learned mazes and were able to devise and carry out complex, multi-faceted strategies that traded-off visual exploration against active motor performance. These strategies took into account available visual information, memory, confidence, the estimated cost in time for exploration, and idiosyncratic tolerance for error. Understanding the strategies humans used for maze solving is valuable for applications in cognitive neuroscience as well as in AI, robotics and human-robot interactions. version:1
arxiv-1307-5710 | Saliency-Guided Perceptual Grouping Using Motion Cues in Region-Based Artificial Visual Attention | http://arxiv.org/abs/1307.5710 | id:1307.5710 author:Jan TÃ¼nnermann, Dieter Enns, BÃ¤rbel Mertsching category:cs.CV  published:2013-07-22 summary:Region-based artificial attention constitutes a framework for bio-inspired attentional processes on an intermediate abstraction level for the use in computer vision and mobile robotics. Segmentation algorithms produce regions of coherently colored pixels. These serve as proto-objects on which the attentional processes determine image portions of relevance. A single region---which not necessarily represents a full object---constitutes the focus of attention. For many post-attentional tasks, however, such as identifying or tracking objects, single segments are not sufficient. Here, we present a saliency-guided approach that groups regions that potentially belong to the same object based on proximity and similarity of motion. We compare our results to object selection by thresholding saliency maps and a further attention-guided strategy. version:1
arxiv-1307-5702 | Is Bottom-Up Attention Useful for Scene Recognition? | http://arxiv.org/abs/1307.5702 | id:1307.5702 author:Samuel F. Dodge, Lina J. Karam category:cs.CV  published:2013-07-22 summary:The human visual system employs a selective attention mechanism to understand the visual world in an eficient manner. In this paper, we show how computational models of this mechanism can be exploited for the computer vision application of scene recognition. First, we consider saliency weighting and saliency pruning, and provide a comparison of the performance of different attention models in these approaches in terms of classification accuracy. Pruning can achieve a high degree of computational savings without significantly sacrificing classification accuracy. In saliency weighting, however, we found that classification performance does not improve. In addition, we present a new method to incorporate salient and non-salient regions for improved classification accuracy. We treat the salient and non-salient regions separately and combine them using Multiple Kernel Learning. We evaluate our approach using the UIUC sports dataset and find that with a small training size, our method improves upon the classification accuracy of the baseline bag of features approach. version:1
arxiv-1307-5693 | Visual saliency estimation by integrating features using multiple kernel learning | http://arxiv.org/abs/1307.5693 | id:1307.5693 author:Yasin Kavak, Erkut Erdem, Aykut Erdem category:cs.CV  published:2013-07-22 summary:In the last few decades, significant achievements have been attained in predicting where humans look at images through different computational models. However, how to determine contributions of different visual features to overall saliency still remains an open problem. To overcome this issue, a recent class of models formulates saliency estimation as a supervised learning problem and accordingly apply machine learning techniques. In this paper, we also address this challenging problem and propose to use multiple kernel learning (MKL) to combine information coming from different feature dimensions and to perform integration at an intermediate level. Besides, we suggest to use responses of a recently proposed filterbank of object detectors, known as Object-Bank, as additional semantic high-level features. Here we show that our MKL-based framework together with the proposed object-specific features provide state-of-the-art performance as compared to SVM or AdaBoost-based saliency models. version:1
arxiv-1307-5691 | A study of parameters affecting visual saliency assessment | http://arxiv.org/abs/1307.5691 | id:1307.5691 author:Nicolas Riche, Matthieu Duvinage, Matei Mancas, Bernard Gosselin, Thierry Dutoit category:cs.CV  published:2013-07-22 summary:Since the early 2000s, computational visual saliency has been a very active research area. Each year, more and more new models are published in the main computer vision conferences. Nowadays, one of the big challenges is to find a way to fairly evaluate all of these models. In this paper, a new framework is proposed to assess models of visual saliency. This evaluation is divided into three experiments leading to the proposition of a new evaluation framework. Each experiment is based on a basic question: 1) there are two ground truths for saliency evaluation: what are the differences between eye fixations and manually segmented salient regions?, 2) the properties of the salient regions: for example, do large, medium and small salient regions present different difficulties for saliency models? and 3) the metrics used to assess saliency models: what advantages would there be to mix them with PCA? Statistical analysis is used here to answer each of these three questions. version:1
arxiv-1307-5684 | Using a Dynamic Neural Field Model to Explore a Direct Collicular Inhibition Account of Inhibition of Return | http://arxiv.org/abs/1307.5684 | id:1307.5684 author:Jason Satel, Ross Story, Matthew D. Hilchey, Zhiguo Wang, Raymond M. Klein category:q-bio.NC cs.CV  published:2013-07-22 summary:When the interval between a transient ash of light (a "cue") and a second visual response signal (a "target") exceeds at least 200ms, responding is slowest in the direction indicated by the first signal. This phenomenon is commonly referred to as inhibition of return (IOR). The dynamic neural field model (DNF) has proven to have broad explanatory power for IOR, effectively capturing many empirical results. Previous work has used a short-term depression (STD) implementation of IOR, but this approach fails to explain many behavioral phenomena observed in the literature. Here, we explore a variant model of IOR involving a combination of STD and delayed direct collicular inhibition. We demonstrate that this hybrid model can better reproduce established behavioural results. We use the results of this model to propose several experiments that would yield particularly valuable insight into the nature of the neurophysiological mechanisms underlying IOR. version:1
arxiv-1307-5840 | Sub- Diving Labeling Method for Optimization Problem by Genetic Algorithm | http://arxiv.org/abs/1307.5840 | id:1307.5840 author:Masoumeh Vali category:cs.NE math.OC  published:2013-07-22 summary:In many global Optimization Problems, it is required to evaluate a global point (min or max) in large space that calculation effort is very high. In this paper is presented new approach for optimization problem with subdivision labeling method (SLM) but in this method for higher dimensional has high computational. SLM Genetic Algorithm (SLMGA) in optimization problems is one of the solutions of this problem. In proposed algorithm the initial population is crossing points and subdividing in each step is according to mutation. RSLMGA is compared with other well known algorithms: DE, PGA, Grefensstette and Eshelman and numerical results show that RSLMGA achieve global optimal point with more decision by smaller generations. version:1
arxiv-1307-5679 | Sub-Dividing Genetic Method for Optimization Problems | http://arxiv.org/abs/1307.5679 | id:1307.5679 author:Masoumeh Vali category:cs.NE math.OC  published:2013-07-22 summary:Nowadays, optimization problem have more application in all major but they have problem in computation. Computation global point in continuous functions have high calculation and this became clearer in large space .In this paper, we proposed Sub- Dividing Genetic Method(SGM) that have less computation than other method for achieving global points . This method userotation mutation and crossover based sub-division method that sub diving method is used for minimize search space and rotation mutation with crossover is used for finding global optimal points. In experimental, SGM algorithm is implemented on De Jong function. The numerical examples show that SGM is performed more optimal than other methods such as Grefensstette, Random Value, and PNG. version:1
arxiv-1307-5839 | A New Approach for Finding the Global Optimal Point Using Subdividing Labeling Method (SLM) | http://arxiv.org/abs/1307.5839 | id:1307.5839 author:Masoumeh Vali category:cs.NE math.OC  published:2013-07-22 summary:In most global optimization problems, finding global optimal point inthe multidimensional and great search space needs high computations. In this paper, we present a new approach to find global optimal point with the low computation and few steps using subdividing labeling method (SLM) which can also be used in the multi-dimensional and great search space. In this approach, in each step, crossing points will be labeled and complete label polytope search space of selected polytope will be subdivided after being selected. SLM algorithm finds the global point until h (subdivision function) turns into zero. SLM will be implemented on five applications and compared with the latest techniques such as random search, random search-walk and simulated annealing method. The results of the proposed method demonstrate that our new approach is faster and more reliable and presents an optimal time complexity O (logn). version:1
arxiv-1307-5674 | Solving Traveling Salesman Problem by Marker Method | http://arxiv.org/abs/1307.5674 | id:1307.5674 author:Masoumeh Vali category:cs.NE cs.DS math.OC  published:2013-07-22 summary:In this paper we use marker method and propose a new mutation operator that selects the nearest neighbor among all near neighbors solving Traveling Salesman Problem. version:1
arxiv-1307-5838 | Rotational Mutation Genetic Algorithm on optimization Problems | http://arxiv.org/abs/1307.5838 | id:1307.5838 author:Masoumeh Vali category:cs.NE math.OC  published:2013-07-22 summary:Optimization problem, nowadays, have more application in all major but they have problem in computation. Calculation of the optimum point in the spaces with the above dimensions is very time consuming. In this paper, there is presented a new approach for the optimization of continuous functions with rotational mutation that is called RM. The proposed algorithm starts from the point which has best fitness value by elitism mechanism. Then, method of rotational mutation is used to reach optimal point. In this paper, RM algorithm is implemented by GA(Briefly RMGA) and is compared with other well- known algorithms: DE, PGA, Grefensstette and Eshelman [15, 16] and numerical and simulation results show that RMGA achieve global optimal point with more decision by smaller generations. version:1
arxiv-1307-5667 | New Optimization Approach Using Clustering-Based Parallel Genetic Algorithm | http://arxiv.org/abs/1307.5667 | id:1307.5667 author:Masoumeh Vali category:cs.NE math.OC  published:2013-07-22 summary:In many global Optimization Problems, it is required to evaluate a global point (min or max) in large space that calculation effort is very high. In this paper is presented new approach for optimization problem with subdivision labeling method (SLM) but in this method for higher dimensional has high calculation effort. Clustering-Based Parallel Genetic Algorithm (CBPGA) in optimization problems is one of the solutions of this problem. That the initial population is crossing points and subdividing in each step is according to mutation. After labeling all of crossing points, selecting is according to polytope that has complete label. In this method we propose an algorithm, based on parallelization scheme using master-slave. SLM algorithm is implemented by CBPGA and compared the experimental results. The numerical examples and numerical results show that SLMCBPGA is improved speed up and efficiency. version:1
arxiv-1307-5653 | Online Tracking Parameter Adaptation based on Evaluation | http://arxiv.org/abs/1307.5653 | id:1307.5653 author:Duc Phu Chau, Julien Badie, FranÃ§ois Bremond, Monique Thonnat category:cs.CV  published:2013-07-22 summary:Parameter tuning is a common issue for many tracking algorithms. In order to solve this problem, this paper proposes an online parameter tuning to adapt a tracking algorithm to various scene contexts. In an offline training phase, this approach learns how to tune the tracker parameters to cope with different contexts. In the online control phase, once the tracking quality is evaluated as not good enough, the proposed approach computes the current context and tunes the tracking parameters using the learned values. The experimental results show that the proposed approach improves the performance of the tracking algorithm and outperforms recent state of the art trackers. This paper brings two contributions: (1) an online tracking evaluation, and (2) a method to adapt online tracking parameters to scene contexts. version:1
arxiv-1304-4806 | Unsupervised model-free representation learning | http://arxiv.org/abs/1304.4806 | id:1304.4806 author:Daniil Ryabko category:cs.LG stat.ML  published:2013-04-17 summary:Numerous control and learning problems face the situation where sequences of high-dimensional highly dependent data are available, but no or little feedback is provided to the learner. To address this issue, we formulate the following problem. Given a series of observations X_0,...,X_n coming from a large (high-dimensional) space X, find a representation function f mapping X to a finite space Y such that the series f(X_0),...,f(X_n) preserve as much information as possible about the original time-series dependence in X_0,...,X_n. We show that, for stationary time series, the function f can be selected as the one maximizing the time-series information h_0(f(X))- h_\infty (f(X)) where h_0(f(X)) is the Shannon entropy of f(X_0) and h_\infty (f(X)) is the entropy rate of the time series f(X_0),...,f(X_n),... Implications for the problem of optimal control are presented. version:3
arxiv-1307-6179 | Multi-horizon solar radiation forecasting for Mediterranean locations using time series models | http://arxiv.org/abs/1307.6179 | id:1307.6179 author:Cyril Voyant, Christophe Paoli, Marc Muselli, Marie Laure Nivet category:physics.ao-ph cs.NE  published:2013-07-22 summary:Considering the grid manager's point of view, needs in terms of prediction of intermittent energy like the photovoltaic resource can be distinguished according to the considered horizon: following days (d+1, d+2 and d+3), next day by hourly step (h+24), next hour (h+1) and next few minutes (m+5 e.g.). Through this work, we have identified methodologies using time series models for the prediction horizon of global radiation and photovoltaic power. What we present here is a comparison of different predictors developed and tested to propose a hierarchy. For horizons d+1 and h+1, without advanced ad hoc time series pre-processing (stationarity) we find it is not easy to differentiate between autoregressive moving average (ARMA) and multilayer perceptron (MLP). However we observed that using exogenous variables improves significantly the results for MLP . We have shown that the MLP were more adapted for horizons h+24 and m+5. In summary, our results are complementary and improve the existing prediction techniques with innovative tools: stationarity, numerical weather prediction combination, MLP and ARMA hybridization, multivariate analysis, time index, etc. version:1
arxiv-1307-5599 | Performance comparison of State-of-the-art Missing Value Imputation Algorithms on Some Bench mark Datasets | http://arxiv.org/abs/1307.5599 | id:1307.5599 author:M. Naresh Kumar category:cs.LG stat.ML  published:2013-07-22 summary:Decision making from data involves identifying a set of attributes that contribute to effective decision making through computational intelligence. The presence of missing values greatly influences the selection of right set of attributes and this renders degradation in classification accuracies of the classifiers. As missing values are quite common in data collection phase during field experiments or clinical trails appropriate handling would improve the classifier performance. In this paper we present a review of recently developed missing value imputation algorithms and compare their performance on some bench mark datasets. version:1
arxiv-1307-5591 | A Novel Equation based Classifier for Detecting Human in Images | http://arxiv.org/abs/1307.5591 | id:1307.5591 author:Subra Mukherjee, Karen Das category:cs.CV  published:2013-07-22 summary:Shape based classification is one of the most challenging tasks in the field of computer vision. Shapes play a vital role in object recognition. The basic shapes in an image can occur in varying scale, position and orientation. And specially when detecting human, the task becomes more challenging owing to the largely varying size, shape, posture and clothing of human. So, in our work we detect human, based on the head-shoulder shape as it is the most unvarying part of human body. Here, firstly a new and a novel equation named as the Omega Equation that describes the shape of human head-shoulder is developed and based on this equation, a classifier is designed particularly for detecting human presence in a scene. The classifier detects human by analyzing some of the discriminative features of the values of the parameters obtained from the Omega equation. The proposed method has been tested on a variety of shape dataset taking into consideration the complexities of human head-shoulder shape. In all the experiments the proposed method demonstrated satisfactory results. version:1
arxiv-1307-5551 | Regularized Discrete Optimal Transport | http://arxiv.org/abs/1307.5551 | id:1307.5551 author:Sira Ferradans, Nicolas Papadakis, Gabriel PeyrÃ©, Jean-FranÃ§ois Aujol category:cs.CV cs.DM math.OC  published:2013-07-21 summary:This article introduces a generalization of the discrete optimal transport, with applications to color image manipulations. This new formulation includes a relaxation of the mass conservation constraint and a regularization term. These two features are crucial for image processing tasks, which necessitate to take into account families of multimodal histograms, with large mass variation across modes. The corresponding relaxed and regularized transportation problem is the solution of a convex optimization problem. Depending on the regularization used, this minimization can be solved using standard linear programming methods or first order proximal splitting schemes. The resulting transportation plan can be used as a color transfer map, which is robust to mass variation across images color palettes. Furthermore, the regularization of the transport plan helps to remove colorization artifacts due to noise amplification. We also extend this framework to the computation of barycenters of distributions. The barycenter is the solution of an optimization problem, which is separately convex with respect to the barycenter and the transportation plans, but not jointly convex. A block coordinate descent scheme converges to a stationary point of the energy. We show that the resulting algorithm can be used for color normalization across several images. The relaxed and regularized barycenter defines a common color palette for those images. Applying color transfer toward this average palette performs a color normalization of the input images. version:1
arxiv-1307-5534 | A New Optimization Approach Based on Rotational Mutation and Crossover Operator | http://arxiv.org/abs/1307.5534 | id:1307.5534 author:Masoumeh Vali category:cs.NE math.OC  published:2013-07-21 summary:Evaluating a global optimal point in many global optimization problems in large space is required to more calculations. In this paper, there is presented a new approach for the continuous functions optimization with rotational mutation and crossover operator. This proposed method (RMC) starts from the point which has best fitness value by elitism mechanism and after that rotational mutation and crossover operator are used to reach optimal point. RMC method is implemented by GA (Briefly RMCGA) and is compared with other wellknown algorithms such as: DE, PGA, Grefensstette and Eshelman[15,16] and numerical and simulating results show that RMCGA achieve global optimal point with more decision by smaller generations. version:1
arxiv-1303-7032 | A Massively Parallel Associative Memory Based on Sparse Neural Networks | http://arxiv.org/abs/1303.7032 | id:1303.7032 author:Zhe Yao, Vincent Gripon, Michael G. Rabbat category:cs.AI cs.DC cs.NE  published:2013-03-28 summary:Associative memories store content in such a way that the content can be later retrieved by presenting the memory with a small portion of the content, rather than presenting the memory with an address as in more traditional memories. Associative memories are used as building blocks for algorithms within database engines, anomaly detection systems, compression algorithms, and face recognition systems. A classical example of an associative memory is the Hopfield neural network. Recently, Gripon and Berrou have introduced an alternative construction which builds on ideas from the theory of error correcting codes and which greatly outperforms the Hopfield network in capacity, diversity, and efficiency. In this paper we implement a variation of the Gripon-Berrou associative memory on a general purpose graphical processing unit (GPU). The work of Gripon and Berrou proposes two retrieval rules, sum-of-sum and sum-of-max. The sum-of-sum rule uses only matrix-vector multiplication and is easily implemented on the GPU. The sum-of-max rule is much less straightforward to implement because it involves non-linear operations. However, the sum-of-max rule gives significantly better retrieval error rates. We propose a hybrid rule tailored for implementation on a GPU which achieves a 880-fold speedup without sacrificing any accuracy. version:2
arxiv-1112-5246 | Combining One-Class Classifiers via Meta-Learning | http://arxiv.org/abs/1112.5246 | id:1112.5246 author:Eitan Menahem, Lior Rokach, Yuval Elovici category:cs.LG K.3.2  published:2011-12-22 summary:Selecting the best classifier among the available ones is a difficult task, especially when only instances of one class exist. In this work we examine the notion of combining one-class classifiers as an alternative for selecting the best classifier. In particular, we propose two new one-class classification performance measures to weigh classifiers and show that a simple ensemble that implements these measures can outperform the most popular one-class ensembles. Furthermore, we propose a new one-class ensemble scheme, TUPSO, which uses meta-learning to combine one-class classifiers. Our experiments demonstrate the superiority of TUPSO over all other tested ensembles and show that the TUPSO performance is statistically indistinguishable from that of the hypothetical best classifier. version:3
arxiv-1307-5519 | Optimal Recombination in Genetic Algorithms | http://arxiv.org/abs/1307.5519 | id:1307.5519 author:Anton V. Eremeev, Julia V. Kovalenko category:cs.NE cs.DS  published:2013-07-21 summary:This paper surveys results on complexity of the optimal recombination problem (ORP), which consists in finding the best possible offspring as a result of a recombination operator in a genetic algorithm, given two parent solutions. We consider efficient reductions of the ORPs, allowing to establish polynomial solvability or NP-hardness of the ORPs, as well as direct proofs of hardness results. version:1
arxiv-1307-5497 | A scalable stage-wise approach to large-margin multi-class loss based boosting | http://arxiv.org/abs/1307.5497 | id:1307.5497 author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.LG  published:2013-07-21 summary:We present a scalable and effective classification model to train multi-class boosting for multi-class classification problems. Shen and Hao introduced a direct formulation of multi- class boosting in the sense that it directly maximizes the multi- class margin [C. Shen and Z. Hao, "A direct formulation for totally-corrective multi- class boosting", in Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2011]. The major problem of their approach is its high computational complexity for training, which hampers its application on real-world problems. In this work, we propose a scalable and simple stage-wise multi-class boosting method, which also directly maximizes the multi-class margin. Our approach of- fers a few advantages: 1) it is simple and computationally efficient to train. The approach can speed up the training time by more than two orders of magnitude without sacrificing the classification accuracy. 2) Like traditional AdaBoost, it is less sensitive to the choice of parameters and empirically demonstrates excellent generalization performance. Experimental results on challenging multi-class machine learning and vision tasks demonstrate that the proposed approach substantially improves the convergence rate and accuracy of the final visual detector at no additional computational cost compared to existing multi-class boosting. version:1
arxiv-1307-5494 | On GROUSE and Incremental SVD | http://arxiv.org/abs/1307.5494 | id:1307.5494 author:Laura Balzano, Stephen J. Wright category:cs.NA cs.LG stat.ML  published:2013-07-21 summary:GROUSE (Grassmannian Rank-One Update Subspace Estimation) is an incremental algorithm for identifying a subspace of Rn from a sequence of vectors in this subspace, where only a subset of components of each vector is revealed at each iteration. Recent analysis has shown that GROUSE converges locally at an expected linear rate, under certain assumptions. GROUSE has a similar flavor to the incremental singular value decomposition algorithm, which updates the SVD of a matrix following addition of a single column. In this paper, we modify the incremental SVD approach to handle missing data, and demonstrate that this modified approach is equivalent to GROUSE, for a certain choice of an algorithmic parameter. version:1
arxiv-1305-0047 | Dictionary LASSO: Guaranteed Sparse Recovery under Linear Transformation | http://arxiv.org/abs/1305.0047 | id:1305.0047 author:Ji Liu, Lei Yuan, Jieping Ye category:stat.ML  published:2013-04-30 summary:We consider the following signal recovery problem: given a measurement matrix $\Phi\in \mathbb{R}^{n\times p}$ and a noisy observation vector $c\in \mathbb{R}^{n}$ constructed from $c = \Phi\theta^* + \epsilon$ where $\epsilon\in \mathbb{R}^{n}$ is the noise vector whose entries follow i.i.d. centered sub-Gaussian distribution, how to recover the signal $\theta^*$ if $D\theta^*$ is sparse {\rca under a linear transformation} $D\in\mathbb{R}^{m\times p}$? One natural method using convex optimization is to solve the following problem: $$\min_{\theta} {1\over 2}\ \Phi\theta - c\ ^2 + \lambda\ D\theta\ _1.$$ This paper provides an upper bound of the estimate error and shows the consistency property of this method by assuming that the design matrix $\Phi$ is a Gaussian random matrix. Specifically, we show 1) in the noiseless case, if the condition number of $D$ is bounded and the measurement number $n\geq \Omega(s\log(p))$ where $s$ is the sparsity number, then the true solution can be recovered with high probability; and 2) in the noisy case, if the condition number of $D$ is bounded and the measurement increases faster than $s\log(p)$, that is, $s\log(p)=o(n)$, the estimate error converges to zero with probability 1 when $p$ and $s$ go to infinity. Our results are consistent with those for the special case $D=\bold{I}_{p\times p}$ (equivalently LASSO) and improve the existing analysis. The condition number of $D$ plays a critical role in our analysis. We consider the condition numbers in two cases including the fused LASSO and the random graph: the condition number in the fused LASSO case is bounded by a constant, while the condition number in the random graph case is bounded with high probability if $m\over p$ (i.e., $#text{edge}\over #text{vertex}$) is larger than a certain constant. Numerical simulations are consistent with our theoretical results. version:2
arxiv-1307-5393 | Clustering Algorithm for Gujarati Language | http://arxiv.org/abs/1307.5393 | id:1307.5393 author:Miral Patel, Prem Balani category:cs.CL  published:2013-07-20 summary:Natural language processing area is still under research. But now a day it is on platform for worldwide researchers. Natural language processing includes analyzing the language based on its structure and then tagging of each word appropriately with its grammar base. Here we have 50,000 tagged words set and we try to cluster those Gujarati words based on proposed algorithm, we have defined our own algorithm for processing. Many clustering techniques are available Ex. Single linkage, complete, linkage,average linkage, Hear no of clusters to be formed are not known, so it is all depends on the type of data set provided . Clustering is preprocess for stemming . Stemming is the process where root is extracted from its word. Ex. cats= cat+S, meaning. Cat: Noun and plural form. version:1
arxiv-1307-5348 | Tensor-based formulation and nuclear norm regularization for multi-energy computed tomography | http://arxiv.org/abs/1307.5348 | id:1307.5348 author:Oguz Semerci, Ning Hao, Misha E. Kilmer, Eric L. Miller category:cs.CV physics.med-ph  published:2013-07-19 summary:The development of energy selective, photon counting X-ray detectors allows for a wide range of new possibilities in the area of computed tomographic image formation. Under the assumption of perfect energy resolution, here we propose a tensor-based iterative algorithm that simultaneously reconstructs the X-ray attenuation distribution for each energy. We use a multi-linear image model rather than a more standard "stacked vector" representation in order to develop novel tensor-based regularizers. Specifically, we model the multi-spectral unknown as a 3-way tensor where the first two dimensions are space and the third dimension is energy. This approach allows for the design of tensor nuclear norm regularizers, which like its two dimensional counterpart, is a convex function of the multi-spectral unknown. The solution to the resulting convex optimization problem is obtained using an alternating direction method of multipliers (ADMM) approach. Simulation results shows that the generalized tensor nuclear norm can be used as a stand alone regularization technique for the energy selective (spectral) computed tomography (CT) problem and when combined with total variation regularization it enhances the regularization capabilities especially at low energy images where the effects of noise are most prominent. version:1
arxiv-1307-5339 | The Cluster Graphical Lasso for improved estimation of Gaussian graphical models | http://arxiv.org/abs/1307.5339 | id:1307.5339 author:Kean Ming Tan, Daniela Witten, Ali Shojaie category:stat.ML stat.ME  published:2013-07-19 summary:We consider the task of estimating a Gaussian graphical model in the high-dimensional setting. The graphical lasso, which involves maximizing the Gaussian log likelihood subject to an l1 penalty, is a well-studied approach for this task. We begin by introducing a surprising connection between the graphical lasso and hierarchical clustering: the graphical lasso in effect performs a two-step procedure, in which (1) single linkage hierarchical clustering is performed on the variables in order to identify connected components, and then (2) an l1-penalized log likelihood is maximized on the subset of variables within each connected component. In other words, the graphical lasso determines the connected components of the estimated network via single linkage clustering. Unfortunately, single linkage clustering is known to perform poorly in certain settings. Therefore, we propose the cluster graphical lasso, which involves clustering the features using an alternative to single linkage clustering, and then performing the graphical lasso on the subset of variables within each cluster. We establish model selection consistency for this technique, and demonstrate its improved performance relative to the graphical lasso in a simulation study, as well as in applications to an equities data set, a university webpage data set, and a gene expression data set. version:1
arxiv-1303-5685 | Sparse Factor Analysis for Learning and Content Analytics | http://arxiv.org/abs/1303.5685 | id:1303.5685 author:Andrew S. Lan, Andrew E. Waters, Christoph Studer, Richard G. Baraniuk category:stat.ML cs.LG math.OC stat.AP  published:2013-03-22 summary:We develop a new model and algorithms for machine learning-based learning analytics, which estimate a learner's knowledge of the concepts underlying a domain, and content analytics, which estimate the relationships among a collection of questions and those concepts. Our model represents the probability that a learner provides the correct response to a question in terms of three factors: their understanding of a set of underlying concepts, the concepts involved in each question, and each question's intrinsic difficulty. We estimate these factors given the graded responses to a collection of questions. The underlying estimation problem is ill-posed in general, especially when only a subset of the questions are answered. The key observation that enables a well-posed solution is the fact that typical educational domains of interest involve only a small number of key concepts. Leveraging this observation, we develop both a bi-convex maximum-likelihood and a Bayesian solution to the resulting SPARse Factor Analysis (SPARFA) problem. We also incorporate user-defined tags on questions to facilitate the interpretability of the estimated factors. Experiments with synthetic and real-world data demonstrate the efficacy of our approach. Finally, we make a connection between SPARFA and noisy, binary-valued (1-bit) dictionary learning that is of independent interest. version:2
arxiv-1307-6549 | Making Laplacians commute | http://arxiv.org/abs/1307.6549 | id:1307.6549 author:Michael M. Bronstein, Klaus Glashoff, Terry A. Loring category:cs.CV cs.GR math.SP  published:2013-07-19 summary:In this paper, we construct multimodal spectral geometry by finding a pair of closest commuting operators (CCO) to a given pair of Laplacians. The CCOs are jointly diagonalizable and hence have the same eigenbasis. Our construction naturally extends classical data analysis tools based on spectral geometry, such as diffusion maps and spectral clustering. We provide several synthetic and real examples of applications in dimensionality reduction, shape analysis, and clustering, demonstrating that our method better captures the inherent structure of multi-modal data. version:1
arxiv-1307-5736 | Speaker Independent Continuous Speech to Text Converter for Mobile Application | http://arxiv.org/abs/1307.5736 | id:1307.5736 author:R. Sandanalakshmi, P. Abinaya Viji, M. Kiruthiga, M. Manjari, M. Sharina category:cs.CL cs.NE cs.SD  published:2013-07-19 summary:An efficient speech to text converter for mobile application is presented in this work. The prime motive is to formulate a system which would give optimum performance in terms of complexity, accuracy, delay and memory requirements for mobile environment. The speech to text converter consists of two stages namely front-end analysis and pattern recognition. The front end analysis involves preprocessing and feature extraction. The traditional voice activity detection algorithms which track only energy cannot successfully identify potential speech from input because the unwanted part of the speech also has some energy and appears to be speech. In the proposed system, VAD that calculates energy of high frequency part separately as zero crossing rate to differentiate noise from speech is used. Mel Frequency Cepstral Coefficient (MFCC) is used as feature extraction method and Generalized Regression Neural Network is used as recognizer. MFCC provides low word error rate and better feature extraction. Neural Network improves the accuracy. Thus a small database containing all possible syllable pronunciation of the user is sufficient to give recognition accuracy closer to 100%. Thus the proposed technique entertains realization of real time speaker independent applications like mobile phones, PDAs etc. version:1
arxiv-1307-5118 | Model-Based Policy Gradients with Parameter-Based Exploration by Least-Squares Conditional Density Estimation | http://arxiv.org/abs/1307.5118 | id:1307.5118 author:Syogo Mori, Voot Tangkaratt, Tingting Zhao, Jun Morimoto, Masashi Sugiyama category:stat.ML cs.LG  published:2013-07-19 summary:The goal of reinforcement learning (RL) is to let an agent learn an optimal control policy in an unknown environment so that future expected rewards are maximized. The model-free RL approach directly learns the policy based on data samples. Although using many samples tends to improve the accuracy of policy learning, collecting a large number of samples is often expensive in practice. On the other hand, the model-based RL approach first estimates the transition model of the environment and then learns the policy based on the estimated transition model. Thus, if the transition model is accurately learned from a small amount of data, the model-based approach can perform better than the model-free approach. In this paper, we propose a novel model-based RL method by combining a recently proposed model-free policy search method called policy gradients with parameter-based exploration and the state-of-the-art transition model estimator called least-squares conditional density estimation. Through experiments, we demonstrate the practical usefulness of the proposed method. version:1
arxiv-1307-5102 | Automated Defect Localization via Low Rank Plus Outlier Modeling of Propagating Wavefield Data | http://arxiv.org/abs/1307.5102 | id:1307.5102 author:Stefano Gonella, Jarvis D. Haupt category:cs.CV  published:2013-07-19 summary:This work proposes an agnostic inference strategy for material diagnostics, conceived within the context of laser-based non-destructive evaluation methods, which extract information about structural anomalies from the analysis of acoustic wavefields measured on the structure's surface by means of a scanning laser interferometer. The proposed approach couples spatiotemporal windowing with low rank plus outlier modeling, to identify a priori unknown deviations in the propagating wavefields caused by material inhomogeneities or defects, using virtually no knowledge of the structural and material properties of the medium. This characteristic makes the approach particularly suitable for diagnostics scenarios where the mechanical and material models are complex, unknown, or unreliable. We demonstrate our approach in a simulated environment using benchmark point and line defect localization problems based on propagating flexural waves in a thin plate. version:1
arxiv-1307-4145 | A Safe Screening Rule for Sparse Logistic Regression | http://arxiv.org/abs/1307.4145 | id:1307.4145 author:Jie Wang, Jiayu Zhou, Jun Liu, Peter Wonka, Jieping Ye category:cs.LG stat.ML  published:2013-07-16 summary:The l1-regularized logistic regression (or sparse logistic regression) is a widely used method for simultaneous classification and feature selection. Although many recent efforts have been devoted to its efficient implementation, its application to high dimensional data still poses significant challenges. In this paper, we present a fast and effective sparse logistic regression screening rule (Slores) to identify the 0 components in the solution vector, which may lead to a substantial reduction in the number of features to be entered to the optimization. An appealing feature of Slores is that the data set needs to be scanned only once to run the screening and its computational cost is negligible compared to that of solving the sparse logistic regression problem. Moreover, Slores is independent of solvers for sparse logistic regression, thus Slores can be integrated with any existing solver to improve the efficiency. We have evaluated Slores using high-dimensional data sets from different applications. Extensive experimental results demonstrate that Slores outperforms the existing state-of-the-art screening rules and the efficiency of solving sparse logistic regression is improved by one magnitude in general. version:2
arxiv-1307-4986 | On the Necessity of Mixed Models: Dynamical Frustrations in the Mind | http://arxiv.org/abs/1307.4986 | id:1307.4986 author:Diego Gabriel Krivochen category:nlin.CD cs.CL math.DS  published:2013-07-18 summary:In the present work we will present and analyze some basic processes at the local and global level in linguistic derivations that seem to go beyond the limits of Markovian or Turing-like computation, and require, in our opinion, a quantum processor. We will first present briefly the working hypothesis and then focus on the empirical domain. At the same time, we will argue that a model appealing to only one kind of computation (be it quantum or not) is necessarily insufficient, and thus both linear and non-linear formal models are to be invoked in order to pursue a fuller understanding of mental computations within a unified framework. version:1
arxiv-1304-1875 | Nonlinear unmixing of hyperspectral images: models and algorithms | http://arxiv.org/abs/1304.1875 | id:1304.1875 author:Nicolas Dobigeon, Jean-Yves Tourneret, CÃ©dric Richard, JosÃ© C. M. Bermudez, Stephen McLaughlin, Alfred O. Hero category:physics.data-an stat.AP stat.ME stat.ML  published:2013-04-06 summary:When considering the problem of unmixing hyperspectral images, most of the literature in the geoscience and image processing areas relies on the widely used linear mixing model (LMM). However, the LMM may be not valid and other nonlinear models need to be considered, for instance, when there are multi-scattering effects or intimate interactions. Consequently, over the last few years, several significant contributions have been proposed to overcome the limitations inherent in the LMM. In this paper, we present an overview of recent advances in nonlinear unmixing modeling. version:2
arxiv-1305-3486 | Noisy Subspace Clustering via Thresholding | http://arxiv.org/abs/1305.3486 | id:1305.3486 author:Reinhard Heckel, Helmut BÃ¶lcskei category:cs.IT cs.LG math.IT math.ST stat.ML stat.TH  published:2013-05-15 summary:We consider the problem of clustering noisy high-dimensional data points into a union of low-dimensional subspaces and a set of outliers. The number of subspaces, their dimensions, and their orientations are unknown. A probabilistic performance analysis of the thresholding-based subspace clustering (TSC) algorithm introduced recently in [1] shows that TSC succeeds in the noisy case, even when the subspaces intersect. Our results reveal an explicit tradeoff between the allowed noise level and the affinity of the subspaces. We furthermore find that the simple outlier detection scheme introduced in [1] provably succeeds in the noisy case. version:2
arxiv-1302-6584 | Variational Algorithms for Marginal MAP | http://arxiv.org/abs/1302.6584 | id:1302.6584 author:Qiang Liu, Alexander Ihler category:stat.ML cs.AI cs.IT cs.LG math.IT  published:2013-02-26 summary:The marginal maximum a posteriori probability (MAP) estimation problem, which calculates the mode of the marginal posterior distribution of a subset of variables with the remaining variables marginalized, is an important inference problem in many models, such as those with hidden variables or uncertain parameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has attracted less attention in the literature compared to the joint MAP (maximization) and marginalization problems. We derive a general dual representation for marginal MAP that naturally integrates the marginalization and maximization operations into a joint variational optimization problem, making it possible to easily extend most or all variational-based algorithms to marginal MAP. In particular, we derive a set of "mixed-product" message passing algorithms for marginal MAP, whose form is a hybrid of max-product, sum-product and a novel "argmax-product" message updates. We also derive a class of convergent algorithms based on proximal point methods, including one that transforms the marginal MAP problem into a sequence of standard marginalization problems. Theoretically, we provide guarantees under which our algorithms give globally or locally optimal solutions, and provide novel upper bounds on the optimal objectives. Empirically, we demonstrate that our algorithms significantly outperform the existing approaches, including a state-of-the-art algorithm based on local search methods. version:3
arxiv-1305-1027 | Regret Bounds for Reinforcement Learning with Policy Advice | http://arxiv.org/abs/1305.1027 | id:1305.1027 author:Mohammad Gheshlaghi Azar, Alessandro Lazaric, Emma Brunskill category:stat.ML cs.LG  published:2013-05-05 summary:In some reinforcement learning problems an agent may be provided with a set of input policies, perhaps learned from prior experience or provided by advisors. We present a reinforcement learning with policy advice (RLPA) algorithm which leverages this input set and learns to use the best policy in the set for the reinforcement learning task at hand. We prove that RLPA has a sub-linear regret of \tilde O(\sqrt{T}) relative to the best input policy, and that both this regret and its computational complexity are independent of the size of the state and action space. Our empirical simulations support our theoretical analysis. This suggests RLPA may offer significant advantages in large domains where some prior good policies are provided. version:2
arxiv-1306-3525 | Approximation Algorithms for Bayesian Multi-Armed Bandit Problems | http://arxiv.org/abs/1306.3525 | id:1306.3525 author:Sudipto Guha, Kamesh Munagala category:cs.DS cs.LG  published:2013-06-14 summary:In this paper, we consider several finite-horizon Bayesian multi-armed bandit problems with side constraints which are computationally intractable (NP-Hard) and for which no optimal (or near optimal) algorithms are known to exist with sub-exponential running time. All of these problems violate the standard exchange property, which assumes that the reward from the play of an arm is not contingent upon when the arm is played. Not only are index policies suboptimal in these contexts, there has been little analysis of such policies in these problem settings. We show that if we consider near-optimal policies, in the sense of approximation algorithms, then there exists (near) index policies. Conceptually, if we can find policies that satisfy an approximate version of the exchange property, namely, that the reward from the play of an arm depends on when the arm is played to within a constant factor, then we have an avenue towards solving these problems. However such an approximate version of the idling bandit property does not hold on a per-play basis and are shown to hold in a global sense. Clearly, such a property is not necessarily true of arbitrary single arm policies and finding such single arm policies is nontrivial. We show that by restricting the state spaces of arms we can find single arm policies and that these single arm policies can be combined into global (near) index policies where the approximate version of the exchange property is true in expectation. The number of different bandit problems that can be addressed by this technique already demonstrate its wide applicability. version:2
arxiv-1307-4717 | Content Based Image Retrieval System using Feature Classification with Modified KNN Algorithm | http://arxiv.org/abs/1307.4717 | id:1307.4717 author:T. Dharani, I. Laurence Aroquiaraj category:cs.CV  published:2013-07-17 summary:Feature means countenance, remote sensing scene objects with similar characteristics, associated to interesting scene elements in the image formation process. They are classified into three types in image processing, that is low, middle and high. Low level features are color, texture and middle level feature is shape and high level feature is semantic gap of objects. An image retrieval system is a computer system for browsing, searching and retrieving images from a large image database. Content Based Image Retrieval is a technique which uses visual features of image such as color, shape, texture to search user required image from large image database according to user requests in the form of a query. MKNN is an enhancing method of KNN. The proposed KNN classification is called MKNN. MKNN contains two parts for processing, they are validity of the train samples and applying weighted KNN. The validity of each point is computed according to its neighbors. In our proposal, Modified K-Nearest Neighbor can be considered a kind of weighted KNN so that the query label is approximated by weighting the neighbors of the query. version:1
arxiv-1301-5288 | The connection between Bayesian estimation of a Gaussian random field and RKHS | http://arxiv.org/abs/1301.5288 | id:1301.5288 author:Aleksandr Y. Aravkin, Bradley M. Bell, James V. Burke, Gianluigi Pillonetto category:stat.ML cs.LG math.ST stat.TH 47N30  65K10  published:2013-01-22 summary:Reconstruction of a function from noisy data is often formulated as a regularized optimization problem over an infinite-dimensional reproducing kernel Hilbert space (RKHS). The solution describes the observed data and has a small RKHS norm. When the data fit is measured using a quadratic loss, this estimator has a known statistical interpretation. Given the noisy measurements, the RKHS estimate represents the posterior mean (minimum variance estimate) of a Gaussian random field with covariance proportional to the kernel associated with the RKHS. In this paper, we provide a statistical interpretation when more general losses are used, such as absolute value, Vapnik or Huber. Specifically, for any finite set of sampling locations (including where the data were collected), the MAP estimate for the signal samples is given by the RKHS estimate evaluated at these locations. version:3
arxiv-1307-4653 | A New Convex Relaxation for Tensor Completion | http://arxiv.org/abs/1307.4653 | id:1307.4653 author:Bernardino Romera-Paredes, Massimiliano Pontil category:cs.LG math.OC stat.ML  published:2013-07-17 summary:We study the problem of learning a tensor from a set of linear measurements. A prominent methodology for this problem is based on a generalization of trace norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting. In this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean ball. We then describe a technique to solve the associated regularization problem, which builds upon the alternating direction method of multipliers. Experiments on one synthetic dataset and two real datasets indicate that the proposed method improves significantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable. version:1
arxiv-1301-3683 | Convex Variational Image Restoration with Histogram Priors | http://arxiv.org/abs/1301.3683 | id:1301.3683 author:Paul Swoboda, Christoph SchnÃ¶rr category:math.OC cs.CV G.1.6; I.4.4  published:2013-01-16 summary:We present a novel variational approach to image restoration (e.g., denoising, inpainting, labeling) that enables to complement established variational approaches with a histogram-based prior enforcing closeness of the solution to some given empirical measure. By minimizing a single objective function, the approach utilizes simultaneously two quite different sources of information for restoration: spatial context in terms of some smoothness prior and non-spatial statistics in terms of the novel prior utilizing the Wasserstein distance between probability measures. We study the combination of the functional lifting technique with two different relaxations of the histogram prior and derive a jointly convex variational approach. Mathematical equivalence of both relaxations is established and cases where optimality holds are discussed. Additionally, we present an efficient algorithmic scheme for the numerical treatment of the presented model. Experiments using the basic total-variation based denoising approach as a case study demonstrate our novel regularization approach. version:2
arxiv-1306-3309 | Symmetries in LDDMM with higher order momentum distributions | http://arxiv.org/abs/1306.3309 | id:1306.3309 author:Henry Jacobs category:math.DS cs.CV  published:2013-06-14 summary:In some implementations of the Large Deformation Diffeomorphic Metric Mapping formulation for image registration we consider the motion of particles which locally translate image data. We then lift the motion of the particles to obtain a motion on the entire image. However, it is certainly possible to consider particles which do more than translate, and this is what will be described in this paper. As the unreduced Lagrangian associated to EPDiff possesses $\Diff(M)$ symmetry, it must also exhibit $G \subset \Diff(M)$ symmetry, for any Lie subgroup. In this paper we will describe a tower of Lie groups $G^{(0)} \subseteq G^{(1)} \subseteq G^{(2)} \subseteq...$ which correspond to preserving $k$-th order jet-data. The reduced configuration spaces $Q^{(k)} := \Diff(M) / G^{(k)}$ will be finite-dimensional (in particular, $Q^{(0)}$ is the configuration manifold for $N$ particles in $M$). We will observe that $G^{(k)}$ is a normal subgroup of $G^{(0)}$ and so the quotient $G^{(0)} / G^{(k)}$ is itself a (finite dimensional) Lie group which acts on $Q^{(k)}$. This makes $Q^{(k)}$ a principle bundle over $Q^{(0)}$ and the reduced geodesic equations on $Q^{(k)}$ will possess $G^{(0)} / G^{(k)}$-symmetry. Noether's theorem implies the existence of conserved momenta for the reduced system on $T^{\ast}Q^{(k)}$. version:2
arxiv-1307-4592 | Processing stationary noise: model and parameter selection in variational methods | http://arxiv.org/abs/1307.4592 | id:1307.4592 author:JÃ©rÃ´me Fehrenbach, Pierre Weiss category:cs.CV math.OC stat.AP  published:2013-07-17 summary:Additive or multiplicative stationary noise recently became an important issue in applied fields such as microscopy or satellite imaging. Relatively few works address the design of dedicated denoising methods compared to the usual white noise setting. We recently proposed a variational algorithm to tackle this issue. In this paper, we analyze this problem from a statistical point of view and provide deterministic properties of the solutions of the associated variational problems. In the first part of this work, we demonstrate that in many practical problems, the noise can be assimilated to a colored Gaussian noise. We provide a quantitative measure of the distance between a stationary process and the corresponding Gaussian process. In the second part, we focus on the Gaussian setting and analyze denoising methods which consist of minimizing the sum of a total variation term and an $l^2$ data fidelity term. While the constrained formulation of this problem allows to easily tune the parameters, the Lagrangian formulation can be solved more efficiently since the problem is strongly convex. Our second contribution consists in providing analytical values of the regularization parameter in order to approximately satisfy Morozov's discrepancy principle. version:1
arxiv-1307-4564 | From Bandits to Experts: A Tale of Domination and Independence | http://arxiv.org/abs/1307.4564 | id:1307.4564 author:Noga Alon, NicolÃ² Cesa-Bianchi, Claudio Gentile, Yishay Mansour category:cs.LG stat.ML  published:2013-07-17 summary:We consider the partial observability model for multi-armed bandits, introduced by Mannor and Shamir. Our main result is a characterization of regret in the directed observability model in terms of the dominating and independence numbers of the observability graph. We also show that in the undirected case, the learner can achieve optimal regret without even accessing the observability graph before selecting an action. Both results are shown using variants of the Exp3 algorithm operating on the observability graph in a time-efficient manner. version:1
arxiv-1307-4516 | Mammogram Edge Detection Using Hybrid Soft Computing Methods | http://arxiv.org/abs/1307.4516 | id:1307.4516 author:I. Laurence Aroquiaraj, K. Thangavel category:cs.CV  published:2013-07-17 summary:Image segmentation is a crucial step in a wide range of method image processing systems. It is useful in visualization of the different objects present in the image. In spite of the several methods available in the literature, image segmentation still a challenging problem in most of image processing applications. The challenge comes from the fuzziness of image objects and the overlapping of the different regions. Detection of edges in an image is a very important step towards understanding image features. There are large numbers of edge detection operators available, each designed to be sensitive to certain types of edges. The Quality of edge detection can be measured from several criteria objectively. Some criteria are proposed in terms of mathematical measurement, some of them are based on application and implementation requirements. Since edges often occur at image locations representing object boundaries, edge detection is extensively used in image segmentation when images are divided into areas corresponding to different objects. This can be used specifically for enhancing the tumor area in mammographic images. Different methods are available for edge detection like Roberts, Sobel, Prewitt, Canny, Log edge operators. In this paper a novel algorithms for edge detection has been proposed for mammographic images. Breast boundary, pectoral region and tumor location can be seen clearly by using this method. For comparison purpose Roberts, Sobel, Prewitt, Canny, Log edge operators are used and their results are displayed. Experimental results demonstrate the effectiveness of the proposed approach. version:1
arxiv-1210-5323 | The performance of orthogonal multi-matching pursuit under RIP | http://arxiv.org/abs/1210.5323 | id:1210.5323 author:Zhiqiang Xu category:cs.IT cs.LG math.IT math.NA  published:2012-10-19 summary:The orthogonal multi-matching pursuit (OMMP) is a natural extension of orthogonal matching pursuit (OMP). We denote the OMMP with the parameter $M$ as OMMP(M) where $M\geq 1$ is an integer. The main difference between OMP and OMMP(M) is that OMMP(M) selects $M$ atoms per iteration, while OMP only adds one atom to the optimal atom set. In this paper, we study the performance of orthogonal multi-matching pursuit (OMMP) under RIP. In particular, we show that, when the measurement matrix A satisfies $(9s, 1/10)$-RIP, there exists an absolutely constant $M_0\leq 8$ so that OMMP(M_0) can recover $s$-sparse signal within $s$ iterations. We furthermore prove that, for slowly-decaying $s$-sparse signal, OMMP(M) can recover s-sparse signal within $O(\frac{s}{M})$ iterations for a large class of $M$. In particular, for $M=s^a$ with $a\in [0,1/2]$, OMMP(M) can recover slowly-decaying $s$-sparse signal within $O(s^{1-a})$ iterations. The result implies that OMMP can reduce the computational complexity heavily. version:3
arxiv-1307-4502 | Universally Elevating the Phase Transition Performance of Compressed Sensing: Non-Isometric Matrices are Not Necessarily Bad Matrices | http://arxiv.org/abs/1307.4502 | id:1307.4502 author:Weiyu Xu, Myung Cho category:cs.IT math.IT math.OC stat.ML  published:2013-07-17 summary:In compressed sensing problems, $\ell_1$ minimization or Basis Pursuit was known to have the best provable phase transition performance of recoverable sparsity among polynomial-time algorithms. It is of great theoretical and practical interest to find alternative polynomial-time algorithms which perform better than $\ell_1$ minimization. \cite{Icassp reweighted l_1}, \cite{Isit reweighted l_1}, \cite{XuScaingLaw} and \cite{iterativereweightedjournal} have shown that a two-stage re-weighted $\ell_1$ minimization algorithm can boost the phase transition performance for signals whose nonzero elements follow an amplitude probability density function (pdf) $f(\cdot)$ whose $t$-th derivative $f^{t}(0) \neq 0$ for some integer $t \geq 0$. However, for signals whose nonzero elements are strictly suspended from zero in distribution (for example, constant-modulus, only taking values `$+d$' or `$-d$' for some nonzero real number $d$), no polynomial-time signal recovery algorithms were known to provide better phase transition performance than plain $\ell_1$ minimization, especially for dense sensing matrices. In this paper, we show that a polynomial-time algorithm can universally elevate the phase-transition performance of compressed sensing, compared with $\ell_1$ minimization, even for signals with constant-modulus nonzero elements. Contrary to conventional wisdoms that compressed sensing matrices are desired to be isometric, we show that non-isometric matrices are not necessarily bad sensing matrices. In this paper, we also provide a framework for recovering sparse signals when sensing matrices are not isometric. version:1
arxiv-1307-6544 | Veni Vidi Vici, A Three-Phase Scenario For Parameter Space Analysis in Image Analysis and Visualization | http://arxiv.org/abs/1307.6544 | id:1307.6544 author:M. A. El-Dosuky category:cs.CV  published:2013-07-17 summary:Automatic analysis of the enormous sets of images is a critical task in life sciences. This faces many challenges such as: algorithms are highly parameterized, significant human input is intertwined, and lacking a standard meta-visualization approach. This paper proposes an alternative iterative approach for optimizing input parameters, saving time by minimizing the user involvement, and allowing for understanding the workflow of algorithms and discovering new ones. The main focus is on developing an interactive visualization technique that enables users to analyze the relationships between sampled input parameters and corresponding output. This technique is implemented as a prototype called Veni Vidi Vici, or "I came, I saw, I conquered." This strategy is inspired by the mathematical formulas of numbering computable functions and is developed atop ImageJ, a scientific image processing program. A case study is presented to investigate the proposed framework. Finally, the paper explores some potential future issues in the application of the proposed approach in parameter space analysis in visualization. version:1
arxiv-1204-0047 | A Lipschitz Exploration-Exploitation Scheme for Bayesian Optimization | http://arxiv.org/abs/1204.0047 | id:1204.0047 author:Ali Jalali, Javad Azimi, Xiaoli Fern, Ruofei Zhang category:cs.LG stat.ML  published:2012-03-30 summary:The problem of optimizing unknown costly-to-evaluate functions has been studied for a long time in the context of Bayesian Optimization. Algorithms in this field aim to find the optimizer of the function by asking only a few function evaluations at locations carefully selected based on a posterior model. In this paper, we assume the unknown function is Lipschitz continuous. Leveraging the Lipschitz property, we propose an algorithm with a distinct exploration phase followed by an exploitation phase. The exploration phase aims to select samples that shrink the search space as much as possible. The exploitation phase then focuses on the reduced search space and selects samples closest to the optimizer. Considering the Expected Improvement (EI) as a baseline, we empirically show that the proposed algorithm significantly outperforms EI. version:2
arxiv-1307-4274 | The Fitness Level Method with Tail Bounds | http://arxiv.org/abs/1307.4274 | id:1307.4274 author:Carsten Witt category:cs.NE  published:2013-07-16 summary:The fitness-level method, also called the method of f-based partitions, is an intuitive and widely used technique for the running time analysis of randomized search heuristics. It was originally defined to prove upper and lower bounds on the expected running time. Recently, upper tail bounds were added to the technique; however, these tail bounds only apply to running times that are at least twice as large as the expectation. We remove this restriction and supplement the fitness-level method with sharp tail bounds, including lower tails. As an exemplary application, we prove that the running time of randomized local search on OneMax is sharply concentrated around n ln n - 0.1159 n. version:1
arxiv-1307-4186 | A Brief Review of Nature-Inspired Algorithms for Optimization | http://arxiv.org/abs/1307.4186 | id:1307.4186 author:Iztok Fister Jr., Xin-She Yang, Iztok Fister, Janez Brest, DuÅ¡an Fister category:cs.NE  published:2013-07-16 summary:Swarm intelligence and bio-inspired algorithms form a hot topic in the developments of new algorithms inspired by nature. These nature-inspired metaheuristic algorithms can be based on swarm intelligence, biological systems, physical and chemical systems. Therefore, these algorithms can be called swarm-intelligence-based, bio-inspired, physics-based and chemistry-based, depending on the sources of inspiration. Though not all of them are efficient, a few algorithms have proved to be very efficient and thus have become popular tools for solving real-world problems. Some algorithms are insufficiently studied. The purpose of this review is to present a relatively comprehensive list of all the algorithms in the literature, so as to inspire further research. version:1
arxiv-1307-4156 | Efficient Mixed-Norm Regularization: Algorithms and Safe Screening Methods | http://arxiv.org/abs/1307.4156 | id:1307.4156 author:Jie Wang, Jun Liu, Jieping Ye category:cs.LG stat.ML  published:2013-07-16 summary:Sparse learning has recently received increasing attention in many areas including machine learning, statistics, and applied mathematics. The mixed-norm regularization based on the l1q norm with q>1 is attractive in many applications of regression and classification in that it facilitates group sparsity in the model. The resulting optimization problem is, however, challenging to solve due to the inherent structure of the mixed-norm regularization. Existing work deals with special cases with q=1, 2, infinity, and they cannot be easily extended to the general case. In this paper, we propose an efficient algorithm based on the accelerated gradient method for solving the general l1q-regularized problem. One key building block of the proposed algorithm is the l1q-regularized Euclidean projection (EP_1q). Our theoretical analysis reveals the key properties of EP_1q and illustrates why EP_1q for the general q is significantly more challenging to solve than the special cases. Based on our theoretical analysis, we develop an efficient algorithm for EP_1q by solving two zero finding problems. To further improve the efficiency of solving large dimensional mixed-norm regularized problems, we propose a screening method which is able to quickly identify the inactive groups, i.e., groups that have 0 components in the solution. This may lead to substantial reduction in the number of groups to be entered to the optimization. An appealing feature of our screening method is that the data set needs to be scanned only once to run the screening. Compared to that of solving the mixed-norm regularized problems, the computational cost of our screening test is negligible. The key of the proposed screening method is an accurate sensitivity analysis of the dual optimal solution when the regularization parameter varies. Experimental results demonstrate the efficiency of the proposed algorithm. version:1
arxiv-1307-4048 | Modified SPLICE and its Extension to Non-Stereo Data for Noise Robust Speech Recognition | http://arxiv.org/abs/1307.4048 | id:1307.4048 author:D. S. Pavan Kumar, N. Vishnu Prasad, Vikas Joshi, S. Umesh category:cs.LG cs.CV stat.ML  published:2013-07-15 summary:In this paper, a modification to the training process of the popular SPLICE algorithm has been proposed for noise robust speech recognition. The modification is based on feature correlations, and enables this stereo-based algorithm to improve the performance in all noise conditions, especially in unseen cases. Further, the modified framework is extended to work for non-stereo datasets where clean and noisy training utterances, but not stereo counterparts, are required. Finally, an MLLR-based computationally efficient run-time noise adaptation method in SPLICE framework has been proposed. The modified SPLICE shows 8.6% absolute improvement over SPLICE in Test C of Aurora-2 database, and 2.93% overall. Non-stereo method shows 10.37% and 6.93% absolute improvements over Aurora-2 and Aurora-4 baseline models respectively. Run-time adaptation shows 9.89% absolute improvement in modified framework as compared to SPLICE for Test C, and 4.96% overall w.r.t. standard MLLR adaptation on HMMs. version:1
arxiv-1307-4038 | An alternative Gospel of structure: order, composition, processes | http://arxiv.org/abs/1307.4038 | id:1307.4038 author:Bob Coecke category:math.CT cs.CL quant-ph  published:2013-07-15 summary:We survey some basic mathematical structures, which arguably are more primitive than the structures taught at school. These structures are orders, with or without composition, and (symmetric) monoidal categories. We list several `real life' incarnations of each of these. This paper also serves as an introduction to these structures and their current and potentially future uses in linguistics, physics and knowledge representation. version:1
arxiv-1305-6211 | Development of a Hindi Lemmatizer | http://arxiv.org/abs/1305.6211 | id:1305.6211 author:Snigdha Paul, Nisheeth Joshi, Iti Mathur category:cs.CL  published:2013-05-24 summary:We live in a translingual society, in order to communicate with people from different parts of the world we need to have an expertise in their respective languages. Learning all these languages is not at all possible; therefore we need a mechanism which can do this task for us. Machine translators have emerged as a tool which can perform this task. In order to develop a machine translator we need to develop several different rules. The very first module that comes in machine translation pipeline is morphological analysis. Stemming and lemmatization comes under morphological analysis. In this paper we have created a lemmatizer which generates rules for removing the affixes along with the addition of rules for creating a proper root word. version:2
arxiv-1307-4299 | Part of Speech Tagging of Marathi Text Using Trigram Method | http://arxiv.org/abs/1307.4299 | id:1307.4299 author:Jyoti Singh, Nisheeth Joshi, Iti Mathur category:cs.CL  published:2013-07-15 summary:In this paper we present a Marathi part of speech tagger. It is a morphologically rich language. It is spoken by the native people of Maharashtra. The general approach used for development of tagger is statistical using trigram Method. The main concept of trigram is to explore the most likely POS for a token based on given information of previous two tags by calculating probabilities to determine which is the best sequence of a tag. In this paper we show the development of the tagger. Moreover we have also shown the evaluation done. version:1
arxiv-1307-4300 | Rule Based Transliteration Scheme for English to Punjabi | http://arxiv.org/abs/1307.4300 | id:1307.4300 author:Deepti Bhalla, Nisheeth Joshi, Iti Mathur category:cs.CL  published:2013-07-15 summary:Machine Transliteration has come out to be an emerging and a very important research area in the field of machine translation. Transliteration basically aims to preserve the phonological structure of words. Proper transliteration of name entities plays a very significant role in improving the quality of machine translation. In this paper we are doing machine transliteration for English-Punjabi language pair using rule based approach. We have constructed some rules for syllabification. Syllabification is the process to extract or separate the syllable from the words. In this we are calculating the probabilities for name entities (Proper names and location). For those words which do not come under the category of name entities, separate probabilities are being calculated by using relative frequency through a statistical machine translation toolkit known as MOSES. Using these probabilities we are transliterating our input text from English to Punjabi. version:1
arxiv-1307-3964 | Learning Markov networks with context-specific independences | http://arxiv.org/abs/1307.3964 | id:1307.3964 author:Alejandro Edera, Federico SchlÃ¼ter, Facundo Bromberg category:cs.AI cs.LG stat.ML  published:2013-07-15 summary:Learning the Markov network structure from data is a problem that has received considerable attention in machine learning, and in many other application fields. This work focuses on a particular approach for this purpose called independence-based learning. Such approach guarantees the learning of the correct structure efficiently, whenever data is sufficient for representing the underlying distribution. However, an important issue of such approach is that the learned structures are encoded in an undirected graph. The problem with graphs is that they cannot encode some types of independence relations, such as the context-specific independences. They are a particular case of conditional independences that is true only for a certain assignment of its conditioning set, in contrast to conditional independences that must hold for all its assignments. In this work we present CSPC, an independence-based algorithm for learning structures that encode context-specific independences, and encoding them in a log-linear model, instead of a graph. The central idea of CSPC is combining the theoretical guarantees provided by the independence-based approach with the benefits of representing complex structures by using features in a log-linear model. We present experiments in a synthetic case, showing that CSPC is more accurate than the state-of-the-art IB algorithms when the underlying distribution contains CSIs. version:1
arxiv-1307-3846 | Bayesian Structured Prediction Using Gaussian Processes | http://arxiv.org/abs/1307.3846 | id:1307.3846 author:Sebastien Bratieres, Novi Quadrianto, Zoubin Ghahramani category:stat.ML cs.LG  published:2013-07-15 summary:We introduce a conceptually novel structured prediction model, GPstruct, which is kernelized, non-parametric and Bayesian, by design. We motivate the model with respect to existing approaches, among others, conditional random fields (CRFs), maximum margin Markov networks (M3N), and structured support vector machines (SVMstruct), which embody only a subset of its properties. We present an inference procedure based on Markov Chain Monte Carlo. The framework can be instantiated for a wide range of structured objects such as linear chains, trees, grids, and other general graphs. As a proof of concept, the model is benchmarked on several natural language processing tasks and a video gesture segmentation task involving a linear chain structure. We show prediction accuracies for GPstruct which are comparable to or exceeding those of CRFs and SVMstruct. version:1
arxiv-1307-3824 | The Fundamental Learning Problem that Genetic Algorithms with Uniform Crossover Solve Efficiently and Repeatedly As Evolution Proceeds | http://arxiv.org/abs/1307.3824 | id:1307.3824 author:Keki M. Burjorjee category:cs.NE cs.AI cs.CC cs.DM cs.LG I.2.8; I.2.6; F.2  published:2013-07-15 summary:This paper establishes theoretical bonafides for implicit concurrent multivariate effect evaluation--implicit concurrency for short---a broad and versatile computational learning efficiency thought to underlie general-purpose, non-local, noise-tolerant optimization in genetic algorithms with uniform crossover (UGAs). We demonstrate that implicit concurrency is indeed a form of efficient learning by showing that it can be used to obtain close-to-optimal bounds on the time and queries required to approximately correctly solve a constrained version (k=7, \eta=1/5) of a recognizable computational learning problem: learning parities with noisy membership queries. We argue that a UGA that treats the noisy membership query oracle as a fitness function can be straightforwardly used to approximately correctly learn the essential attributes in O(log^1.585 n) queries and O(n log^1.585 n) time, where n is the total number of attributes. Our proof relies on an accessible symmetry argument and the use of statistical hypothesis testing to reject a global null hypothesis at the 10^-100 level of significance. It is, to the best of our knowledge, the first relatively rigorous identification of efficient computational learning in an evolutionary algorithm on a non-trivial learning problem. version:1
arxiv-1307-3811 | Multiview Hessian Discriminative Sparse Coding for Image Annotation | http://arxiv.org/abs/1307.3811 | id:1307.3811 author:Weifeng Liu, Dacheng Tao, Jun Cheng, Yuanyan Tang category:cs.MM cs.CV cs.IT math.IT  published:2013-07-15 summary:Sparse coding represents a signal sparsely by using an overcomplete dictionary, and obtains promising performance in practical computer vision applications, especially for signal restoration tasks such as image denoising and image inpainting. In recent years, many discriminative sparse coding algorithms have been developed for classification problems, but they cannot naturally handle visual data represented by multiview features. In addition, existing sparse coding algorithms use graph Laplacian to model the local geometry of the data distribution. It has been identified that Laplacian regularization biases the solution towards a constant function which possibly leads to poor extrapolating power. In this paper, we present multiview Hessian discriminative sparse coding (mHDSC) which seamlessly integrates Hessian regularization with discriminative sparse coding for multiview learning problems. In particular, mHDSC exploits Hessian regularization to steer the solution which varies smoothly along geodesics in the manifold, and treats the label information as an additional view of feature for incorporating the discriminative power for image annotation. We conduct extensive experiments on PASCAL VOC'07 dataset and demonstrate the effectiveness of mHDSC for image annotation. version:1
arxiv-1307-3785 | Probabilistic inverse reinforcement learning in unknown environments | http://arxiv.org/abs/1307.3785 | id:1307.3785 author:Aristide C. Y. Tossou, Christos Dimitrakakis category:stat.ML cs.LG  published:2013-07-14 summary:We consider the problem of learning by demonstration from agents acting in unknown stochastic Markov environments or games. Our aim is to estimate agent preferences in order to construct improved policies for the same task that the agents are trying to solve. To do so, we extend previous probabilistic approaches for inverse reinforcement learning in known MDPs to the case of unknown dynamics or opponents. We do this by deriving two simplified probabilistic models of the demonstrator's policy and utility. For tractability, we use maximum a posteriori estimation rather than full Bayesian inference. Under a flat prior, this results in a convex optimisation problem. We find that the resulting algorithms are highly competitive against a variety of other methods for inverse reinforcement learning that do have knowledge of the dynamics. version:1
arxiv-1307-3759 | A Minimal Six-Point Auto-Calibration Algorithm | http://arxiv.org/abs/1307.3759 | id:1307.3759 author:Evgeniy Martyushev category:cs.CV  published:2013-07-14 summary:A non-iterative auto-calibration algorithm is presented. It deals with a minimal set of six scene points in three views taken by a camera with fixed but unknown intrinsic parameters. Calibration is based on the image correspondences only. The algorithm is implemented and validated on synthetic image data. version:1
arxiv-1307-3755 | Map of Life: Measuring and Visualizing Species' Relatedness with "Molecular Distance Maps" | http://arxiv.org/abs/1307.3755 | id:1307.3755 author:Lila Kari, Kathleen A. Hill, Abu Sadat Sayem, Nathaniel Bryans, Katelyn Davis, Nikesh S. Dattani category:q-bio.GN cs.CV q-bio.PE q-bio.QM 92  68  published:2013-07-14 summary:We propose a novel combination of methods that (i) portrays quantitative characteristics of a DNA sequence as an image, (ii) computes distances between these images, and (iii) uses these distances to output a map wherein each sequence is a point in a common Euclidean space. In the resulting "Molecular Distance Map" each point signifies a DNA sequence, and the geometric distance between any two points reflects the degree of relatedness between the corresponding sequences and species. Molecular Distance Maps present compelling visual representations of relationships between species and could be used for taxonomic clarifications, for species identification, and for studies of evolutionary history. One of the advantages of this method is its general applicability since, as sequence alignment is not required, the DNA sequences chosen for comparison can be completely different regions in different genomes. In fact, this method can be used to compare any two DNA sequences. For example, in our dataset of 3,176 mitochondrial DNA sequences, it correctly finds the mtDNA sequences most closely related to that of the anatomically modern human (the Neanderthal, the Denisovan, and the chimp), and it finds that the sequence most different from it belongs to a cucumber. Furthermore, our method can be used to compare real sequences to artificial, computer-generated, DNA sequences. For example, it is used to determine that the distances between a Homo sapiens sapiens mtDNA and artificial sequences of the same length and same trinucleotide frequencies can be larger than the distance between the same human mtDNA and the mtDNA of a fruit-fly. We demonstrate this method's promising potential for taxonomical clarifications by applying it to a diverse variety of cases that have been historically controversial, such as the genus Polypterus, the family Tarsiidae, and the vast (super)kingdom Protista. version:1
arxiv-1307-3687 | On Analyzing Estimation Errors due to Constrained Connections in Online Review Systems | http://arxiv.org/abs/1307.3687 | id:1307.3687 author:Junzhou Zhao category:cs.SI cs.LG  published:2013-07-14 summary:Constrained connection is the phenomenon that a reviewer can only review a subset of products/services due to narrow range of interests or limited attention capacity. In this work, we study how constrained connections can affect estimation performance in online review systems (ORS). We find that reviewers' constrained connections will cause poor estimation performance, both from the measurements of estimation accuracy and Bayesian Cramer Rao lower bound. version:1
arxiv-1211-6013 | Online Stochastic Optimization with Multiple Objectives | http://arxiv.org/abs/1211.6013 | id:1211.6013 author:Mehrdad Mahdavi, Tianbao Yang, Rong Jin category:cs.LG math.OC  published:2012-11-26 summary:In this paper we propose a general framework to characterize and solve the stochastic optimization problems with multiple objectives underlying many real world learning applications. We first propose a projection based algorithm which attains an $O(T^{-1/3})$ convergence rate. Then, by leveraging on the theory of Lagrangian in constrained optimization, we devise a novel primal-dual stochastic approximation algorithm which attains the optimal convergence rate of $O(T^{-1/2})$ for general Lipschitz continuous objectives. version:2
arxiv-1007-0546 | Computational Model of Music Sight Reading: A Reinforcement Learning Approach | http://arxiv.org/abs/1007.0546 | id:1007.0546 author:Keyvan Yahya, Pouyan Rafiei Fard category:cs.AI cs.LG cs.NE math.OC  published:2010-07-04 summary:Although the Music Sight Reading process has been studied from the cognitive psychology view points, but the computational learning methods like the Reinforcement Learning have not yet been used to modeling of such processes. In this paper, with regards to essential properties of our specific problem, we consider the value function concept and will indicate that the optimum policy can be obtained by the method we offer without to be getting involved with computing of the complex value functions. Also, we will offer a normative behavioral model for the interaction of the agent with the musical pitch environment and by using a slightly different version of Partially observable Markov decision processes we will show that our method helps for faster learning of state-action pairs in our implemented agents. version:4
arxiv-1307-3675 | Minimum Error Rate Training and the Convex Hull Semiring | http://arxiv.org/abs/1307.3675 | id:1307.3675 author:Chris Dyer category:cs.LG I.2.6; I.2.7  published:2013-07-13 summary:We describe the line search used in the minimum error rate training algorithm MERT as the "inside score" of a weighted proof forest under a semiring defined in terms of well-understood operations from computational geometry. This conception leads to a straightforward complexity analysis of the dynamic programming MERT algorithms of Macherey et al. (2008) and Kumar et al. (2009) and practical approaches to implementation. version:1
arxiv-1307-3673 | A Data Management Approach for Dataset Selection Using Human Computation | http://arxiv.org/abs/1307.3673 | id:1307.3673 author:Alexandros Ntoulas, Omar Alonso, Vasilis Kandylas category:cs.LG cs.IR  published:2013-07-13 summary:As the number of applications that use machine learning algorithms increases, the need for labeled data useful for training such algorithms intensifies. Getting labels typically involves employing humans to do the annotation, which directly translates to training and working costs. Crowdsourcing platforms have made labeling cheaper and faster, but they still involve significant costs, especially for the cases where the potential set of candidate data to be labeled is large. In this paper we describe a methodology and a prototype system aiming at addressing this challenge for Web-scale problems in an industrial setting. We discuss ideas on how to efficiently select the data to use for training of machine learning algorithms in an attempt to reduce cost. We show results achieving good performance with reduced cost by carefully selecting which instances to label. Our proposed algorithm is presented as part of a framework for managing and generating training datasets, which includes, among other components, a human computation element. version:1
arxiv-1301-4083 | Knowledge Matters: Importance of Prior Information for Optimization | http://arxiv.org/abs/1301.4083 | id:1301.4083 author:Ã‡aÄŸlar GÃ¼lÃ§ehre, Yoshua Bengio category:cs.LG cs.CV cs.NE stat.ML  published:2013-01-17 summary:We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hypothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset with 64x64 binary inputs images, each image with three sprites. The final task is to decide whether all the sprites are the same or one of them is different. Sprites are pentomino tetris shapes and they are placed in an image with different locations using scaling and rotation transformations. The first part of the two-tiered MLP is pre-trained with intermediate-level targets being the presence of sprites at each location, while the second part takes the output of the first part as input and predicts the final task's target binary event. The two-tiered MLP architecture, with a few tens of thousand examples, was able to learn the task perfectly, whereas all other algorithms (include unsupervised pre-training, but also traditional algorithms like SVMs, decision trees and boosting) all perform no better than chance. We hypothesize that the optimization difficulty involved when the intermediate pre-training is not performed is due to the {\em composition} of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of optimization problems with deep learning, presumably because of effective local minima. version:6
arxiv-1307-3490 | On-line Bayesian parameter estimation in general non-linear state-space models: A tutorial and new results | http://arxiv.org/abs/1307.3490 | id:1307.3490 author:Aditya Tulsyan, Biao Huang, R. Bhushan Gopaluni, J. Fraser Forbes category:stat.CO stat.AP stat.ME stat.ML  published:2013-07-12 summary:On-line estimation plays an important role in process control and monitoring. Obtaining a theoretical solution to the simultaneous state-parameter estimation problem for non-linear stochastic systems involves solving complex multi-dimensional integrals that are not amenable to analytical solution. While basic sequential Monte-Carlo (SMC) or particle filtering (PF) algorithms for simultaneous estimation exist, it is well recognized that there is a need for making these on-line algorithms non-degenerate, fast and applicable to processes with missing measurements. To overcome the deficiencies in traditional algorithms, this work proposes a Bayesian approach to on-line state and parameter estimation. Its extension to handle missing data in real-time is also provided. The simultaneous estimation is performed by filtering an extended vector of states and parameters using an adaptive sequential-importance-resampling (SIR) filter with a kernel density estimation method. The approach uses an on-line optimization algorithm based on Kullback-Leibler (KL) divergence to allow adaptation of the SIR filter for combined state-parameter estimation. An optimal tuning rule to control the width of the kernel and the variance of the artificial noise added to the parameters is also proposed. The approach is illustrated through numerical examples. version:1
arxiv-1307-3457 | Energy-aware adaptive bi-Lipschitz embeddings | http://arxiv.org/abs/1307.3457 | id:1307.3457 author:Bubacarr Bah, Ali Sadeghian, Volkan Cevher category:cs.LG cs.IT math.IT 68Q99  published:2013-07-12 summary:We propose a dimensionality reducing matrix design based on training data with constraints on its Frobenius norm and number of rows. Our design criteria is aimed at preserving the distances between the data points in the dimensionality reduced space as much as possible relative to their distances in original data space. This approach can be considered as a deterministic Bi-Lipschitz embedding of the data points. We introduce a scalable learning algorithm, dubbed AMUSE, and provide a rigorous estimation guarantee by leveraging game theoretic tools. We also provide a generalization characterization of our matrix based on our sample data. We use compressive sensing problems as an example application of our problem, where the Frobenius norm design constraint translates into the sensing energy. version:1
arxiv-1307-3439 | Speedy Object Detection based on Shape | http://arxiv.org/abs/1307.3439 | id:1307.3439 author:Y. Jayanta Singh, Shalu Gupta category:cs.CV  published:2013-07-12 summary:This study is a part of design of an audio system for in-house object detection system for visually impaired, low vision personnel by birth or by an accident or due to old age. The input of the system will be scene and output as audio. Alert facility is provided based on severity levels of the objects (snake, broke glass etc) and also during difficulties. The study proposed techniques to provide speedy detection of objects based on shapes and its scale. Features are extraction to have minimum spaces using dynamic scaling. From a scene, clusters of objects are formed based on the scale and shape. Searching is performed among the clusters initially based on the shape, scale, mean cluster value and index of object(s). The minimum operation to detect the possible shape of the object is performed. In case the object does not have a likely matching shape, scale etc, then the several operations required for an object detection will not perform; instead, it will declared as a new object. In such way, this study finds a speedy way of detecting objects. version:1
arxiv-1307-3400 | Thompson Sampling for 1-Dimensional Exponential Family Bandits | http://arxiv.org/abs/1307.3400 | id:1307.3400 author:Nathaniel Korda, Emilie Kaufmann, Remi Munos category:stat.ML  published:2013-07-12 summary:Thompson Sampling has been demonstrated in many complex bandit models, however the theoretical guarantees available for the parametric multi-armed bandit are still limited to the Bernoulli case. Here we extend them by proving asymptotic optimality of the algorithm using the Jeffreys prior for 1-dimensional exponential family bandits. Our proof builds on previous work, but also makes extensive use of closed forms for Kullback-Leibler divergence and Fisher information (and thus Jeffreys prior) available in an exponential family. This allow us to give a finite time exponential concentration inequality for posterior distributions on exponential families that may be of interest in its own right. Moreover our analysis covers some distributions for which no optimistic algorithm has yet been proposed, including heavy-tailed exponential families. version:1
arxiv-1307-3549 | Performance Analysis of Clustering Algorithms for Gene Expression Data | http://arxiv.org/abs/1307.3549 | id:1307.3549 author:T. Chandrasekhar, K. Thangavel, E. Elayaraja category:cs.CE cs.LG  published:2013-07-12 summary:Microarray technology is a process that allows thousands of genes simultaneously monitor to various experimental conditions. It is used to identify the co-expressed genes in specific cells or tissues that are actively used to make proteins, This method is used to analysis the gene expression, an important task in bioinformatics research. Cluster analysis of gene expression data has proved to be a useful tool for identifying co-expressed genes, biologically relevant groupings of genes and samples. In this paper we analysed K-Means with Automatic Generations of Merge Factor for ISODATA- AGMFI, to group the microarray data sets on the basic of ISODATA. AGMFI is to generate initial values for merge and Spilt factor, maximum merge times instead of selecting efficient values as in ISODATA. The initial seeds for each cluster were normally chosen either sequentially or randomly. The quality of the final clusters was found to be influenced by these initial seeds. For the real life problems, the suitable number of clusters cannot be predicted. To overcome the above drawback the current research focused on developing the clustering algorithms without giving the initial number of clusters. version:1
arxiv-1307-3337 | Unsupervised Gene Expression Data using Enhanced Clustering Method | http://arxiv.org/abs/1307.3337 | id:1307.3337 author:T. Chandrasekhar, K. Thangavel, E. Elayaraja, E. N. Sathishkumar category:cs.CE cs.LG  published:2013-07-12 summary:Microarrays are made it possible to simultaneously monitor the expression profiles of thousands of genes under various experimental conditions. Identification of co-expressed genes and coherent patterns is the central goal in microarray or gene expression data analysis and is an important task in bioinformatics research. Feature selection is a process to select features which are more informative. It is one of the important steps in knowledge discovery. The problem is that not all features are important. Some of the features may be redundant, and others may be irrelevant and noisy. In this work the unsupervised Gene selection method and Enhanced Center Initialization Algorithm (ECIA) with K-Means algorithms have been applied for clustering of Gene Expression Data. This proposed clustering algorithm overcomes the drawbacks in terms of specifying the optimal number of clusters and initialization of good cluster centroids. Gene Expression Data show that could identify compact clusters with performs well in terms of the Silhouette Coefficients cluster measure. version:1
arxiv-1307-3336 | Opinion Mining and Analysis: A survey | http://arxiv.org/abs/1307.3336 | id:1307.3336 author:Arti Buche, Dr. M. B. Chandak, Akshay Zadgaonkar category:cs.CL cs.IR  published:2013-07-12 summary:The current research is focusing on the area of Opinion Mining also called as sentiment analysis due to sheer volume of opinion rich web resources such as discussion forums, review sites and blogs are available in digital form. One important problem in sentiment analysis of product reviews is to produce summary of opinions based on product features. We have surveyed and analyzed in this paper, various techniques that have been developed for the key tasks of opinion mining. We have provided an overall picture of what is involved in developing a software system for opinion mining on the basis of our survey and analysis. version:1
arxiv-1207-3576 | Hierarchical Approach for Total Variation Digital Image Inpainting | http://arxiv.org/abs/1207.3576 | id:1207.3576 author:S. Padmavathi, N. Archana, K. P. Soman category:cs.CV  published:2012-07-16 summary:The art of recovering an image from damage in an undetectable form is known as inpainting. The manual work of inpainting is most often a very time consuming process. Due to digitalization of this technique, it is automatic and faster. In this paper, after the user selects the regions to be reconstructed, the algorithm automatically reconstruct the lost regions with the help of the information surrounding them. The existing methods perform very well when the region to be reconstructed is very small, but fails in proper reconstruction as the area increases. This paper describes a Hierarchical method by which the area to be inpainted is reduced in multiple levels and Total Variation(TV) method is used to inpaint in each level. This algorithm gives better performance when compared with other existing algorithms such as nearest neighbor interpolation, Inpainting through Blurring and Sobolev Inpainting. version:2
arxiv-1110-4198 | A Reliable Effective Terascale Linear Learning System | http://arxiv.org/abs/1110.4198 | id:1110.4198 author:Alekh Agarwal, Olivier Chapelle, Miroslav Dudik, John Langford category:cs.LG stat.ML  published:2011-10-19 summary:We present a system and a set of techniques for learning linear predictors with convex losses on terascale datasets, with trillions of features, {The number of features here refers to the number of non-zero entries in the data matrix.} billions of training examples and millions of parameters in an hour using a cluster of 1000 machines. Individually none of the component techniques are new, but the careful synthesis required to obtain an efficient implementation is. The result is, up to our knowledge, the most scalable and efficient linear learning system reported in the literature (as of 2011 when our experiments were conducted). We describe and thoroughly evaluate the components of the system, showing the importance of the various design choices. version:3
arxiv-1307-3310 | Improving the quality of Gujarati-Hindi Machine Translation through part-of-speech tagging and stemmer-assisted transliteration | http://arxiv.org/abs/1307.3310 | id:1307.3310 author:Juhi Ameta, Nisheeth Joshi, Iti Mathur category:cs.CL  published:2013-07-12 summary:Machine Translation for Indian languages is an emerging research area. Transliteration is one such module that we design while designing a translation system. Transliteration means mapping of source language text into the target language. Simple mapping decreases the efficiency of overall translation system. We propose the use of stemming and part-of-speech tagging for transliteration. The effectiveness of translation can be improved if we use part-of-speech tagging and stemming assisted transliteration.We have shown that much of the content in Gujarati gets transliterated while being processed for translation to Hindi language. version:1
arxiv-1307-3271 | Fuzzy Fibers: Uncertainty in dMRI Tractography | http://arxiv.org/abs/1307.3271 | id:1307.3271 author:Thomas Schultz, Anna Vilanova, Ralph Brecheisen, Gordon Kindlmann category:cs.CV  published:2013-07-11 summary:Fiber tracking based on diffusion weighted Magnetic Resonance Imaging (dMRI) allows for noninvasive reconstruction of fiber bundles in the human brain. In this chapter, we discuss sources of error and uncertainty in this technique, and review strategies that afford a more reliable interpretation of the results. This includes methods for computing and rendering probabilistic tractograms, which estimate precision in the face of measurement noise and artifacts. However, we also address aspects that have received less attention so far, such as model selection, partial voluming, and the impact of parameters, both in preprocessing and in fiber tracking itself. We conclude by giving impulses for future research. version:1
arxiv-1307-3227 | Minimum Distance Estimation for Robust High-Dimensional Regression | http://arxiv.org/abs/1307.3227 | id:1307.3227 author:AurÃ©lie C. Lozano, Nicolai Meinshausen category:stat.ME stat.ML  published:2013-07-11 summary:We propose a minimum distance estimation method for robust regression in sparse high-dimensional settings. The traditional likelihood-based estimators lack resilience against outliers, a critical issue when dealing with high-dimensional noisy data. Our method, Minimum Distance Lasso (MD-Lasso), combines minimum distance functionals, customarily used in nonparametric estimation for their robustness, with l1-regularization for high-dimensional regression. The geometry of MD-Lasso is key to its consistency and robustness. The estimator is governed by a scaling parameter that caps the influence of outliers: the loss per observation is locally convex and close to quadratic for small squared residuals, and flattens for squared residuals larger than the scaling parameter. As the parameter approaches infinity, the estimator becomes equivalent to least-squares Lasso. MD-Lasso enjoys fast convergence rates under mild conditions on the model error distribution, which hold for any of the solutions in a convexity region around the true parameter and in certain cases for every solution. Remarkably, a first-order optimization method is able to produce iterates very close to the consistent solutions, with geometric convergence and regardless of the initialization. A connection is established with re-weighted least-squares that intuitively explains MD-Lasso robustness. The merits of our method are demonstrated through simulation and eQTL data analysis. version:1
arxiv-1307-3054 | Contrast Enhancement And Brightness Preservation Using Multi- Decomposition Histogram Equalization | http://arxiv.org/abs/1307.3054 | id:1307.3054 author:Sayali Nimkar, Sanal Varghese, Sucheta Shrivastava category:cs.CV  published:2013-07-11 summary:Histogram Equalization (HE) has been an essential addition to the Image Enhancement world. Enhancement techniques like Classical Histogram Equalization (CHE), Adaptive Histogram Equalization (ADHE), Bi-Histogram Equalization (BHE) and Recursive Mean Separate Histogram Equalization (RMSHE) methods enhance contrast, however, brightness is not well preserved with these methods, which gives an unpleasant look to the final image obtained. Thus, we introduce a novel technique Multi-Decomposition Histogram Equalization (MDHE) to eliminate the drawbacks of the earlier methods. In MDHE, we have decomposed the input sixty-four parts, applied CHE in each of the sub-images and then finally interpolated them in correct order. The final image after MDHE results in contrast enhanced and brightness preserved image compared to all other techniques mentioned above. We have calculated the various parameters like PSNR, SNR, RMSE, MSE, etc. for every technique. Our results are well supported by bar graphs, histograms and the parameter calculations at the end. version:1
arxiv-1211-4289 | Application of three graph Laplacian based semi-supervised learning methods to protein function prediction problem | http://arxiv.org/abs/1211.4289 | id:1211.4289 author:Loc Tran category:cs.LG cs.CE q-bio.QM stat.ML H.2.8  published:2012-11-19 summary:Protein function prediction is the important problem in modern biology. In this paper, the un-normalized, symmetric normalized, and random walk graph Laplacian based semi-supervised learning methods will be applied to the integrated network combined from multiple networks to predict the functions of all yeast proteins in these multiple networks. These multiple networks are network created from Pfam domain structure, co-participation in a protein complex, protein-protein interaction network, genetic interaction network, and network created from cell cycle gene expression measurements. Multiple networks are combined with fixed weights instead of using convex optimization to determine the combination weights due to high time complexity of convex optimization method. This simple combination method will not affect the accuracy performance measures of the three semi-supervised learning methods. Experiment results show that the un-normalized and symmetric normalized graph Laplacian based methods perform slightly better than random walk graph Laplacian based method for integrated network. Moreover, the accuracy performance measures of these three semi-supervised learning methods for integrated network are much better than the best accuracy performance measures of these three methods for the individual network. version:3
arxiv-1307-3489 | Genetic approach for arabic part of speech tagging | http://arxiv.org/abs/1307.3489 | id:1307.3489 author:Bilel Ben Ali, Fethi Jarray category:cs.CL cs.NE 68T50  published:2013-07-11 summary:With the growing number of textual resources available, the ability to understand them becomes critical. An essential first step in understanding these sources is the ability to identify the part of speech in each sentence. Arabic is a morphologically rich language, wich presents a challenge for part of speech tagging. In this paper, our goal is to propose, improve and implement a part of speech tagger based on a genetic alorithm. The accuracy obtained with this method is comparable to that of other probabilistic approaches. version:1
arxiv-1307-3014 | A New Approach to the Solution of Economic Dispatch Using Particle Swarm Optimization with Simulated Annealing | http://arxiv.org/abs/1307.3014 | id:1307.3014 author:V. Karthikeyan, S. Senthilkumar, V. J. Vijayalakshmi category:cs.CE cs.NE  published:2013-07-11 summary:A new approach to the solution of Economic Dispatch using Particle Swarm Optimization is presented. It is the progression of allocating production amongst the dedicated units such that the restriction forced are fulfilled and the power needs are reduced. More just, the soft computing method has received supplementary concentration and was used in a quantity of successful and sensible applications. Here, an attempt has been made to find out the minimum cost by using Particle Swarm Optimization Algorithm using the data of three generating units. In this work, data has been taken such as the loss coefficients with the max-min power limit and cost function. PSO and Simulated Annealing are functional to put out the least amount for dissimilar energy requirements. When the outputs are compared with the conventional method, PSO seems to give an improved result with enhanced convergence feature. All the methods are executed in MATLAB environment. The effectiveness and feasibility of the proposed method were demonstrated by three generating units case study. Output gives hopeful results, signifying that the projected method of calculation is competent of economically formative advanced eminence solutions addressing economic dispatch problems. version:1
arxiv-1307-2997 | Conversion of Braille to Text in English, Hindi and Tamil Languages | http://arxiv.org/abs/1307.2997 | id:1307.2997 author:S. Padmavathi, Manojna K. S. S, S. Sphoorthy Reddy, D. Meenakshy category:cs.CV  published:2013-07-11 summary:The Braille system has been used by the visually impaired for reading and writing. Due to limited availability of the Braille text books an efficient usage of the books becomes a necessity. This paper proposes a method to convert a scanned Braille document to text which can be read out to many through the computer. The Braille documents are pre processed to enhance the dots and reduce the noise. The Braille cells are segmented and the dots from each cell is extracted and converted in to a number sequence. These are mapped to the appropriate alphabets of the language. The converted text is spoken out through a speech synthesizer. The paper also provides a mechanism to type the Braille characters through the number pad of the keyboard. The typed Braille character is mapped to the alphabet and spoken out. The Braille cell has a standard representation but the mapping differs for each language. In this paper mapping of English, Hindi and Tamil are considered. version:1
arxiv-1307-2971 | Accuracy of MAP segmentation with hidden Potts and Markov mesh prior models via Path Constrained Viterbi Training, Iterated Conditional Modes and Graph Cut based algorithms | http://arxiv.org/abs/1307.2971 | id:1307.2971 author:Ana Georgina Flesia, Josef Baumgartner, Javier Gimenez, Jorge Martinez category:cs.LG cs.CV stat.ML  published:2013-07-11 summary:In this paper, we study statistical classification accuracy of two different Markov field environments for pixelwise image segmentation, considering the labels of the image as hidden states and solving the estimation of such labels as a solution of the MAP equation. The emission distribution is assumed the same in all models, and the difference lays in the Markovian prior hypothesis made over the labeling random field. The a priori labeling knowledge will be modeled with a) a second order anisotropic Markov Mesh and b) a classical isotropic Potts model. Under such models, we will consider three different segmentation procedures, 2D Path Constrained Viterbi training for the Hidden Markov Mesh, a Graph Cut based segmentation for the first order isotropic Potts model, and ICM (Iterated Conditional Modes) for the second order isotropic Potts model. We provide a unified view of all three methods, and investigate goodness of fit for classification, studying the influence of parameter estimation, computational gain, and extent of automation in the statistical measures Overall Accuracy, Relative Improvement and Kappa coefficient, allowing robust and accurate statistical analysis on synthetic and real-life experimental data coming from the field of Dental Diagnostic Radiography. All algorithms, using the learned parameters, generate good segmentations with little interaction when the images have a clear multimodal histogram. Suboptimal learning proves to be frail in the case of non-distinctive modes, which limits the complexity of usable models, and hence the achievable error rate as well. All Matlab code written is provided in a toolbox available for download from our website, following the Reproducible Research Paradigm. version:1
arxiv-1306-3721 | Online Alternating Direction Method (longer version) | http://arxiv.org/abs/1306.3721 | id:1306.3721 author:Huahua Wang, Arindam Banerjee category:cs.LG math.OC  published:2013-06-17 summary:Online optimization has emerged as powerful tool in large scale optimization. In this pa- per, we introduce efficient online optimization algorithms based on the alternating direction method (ADM), which can solve online convex optimization under linear constraints where the objective could be non-smooth. We introduce new proof techniques for ADM in the batch setting, which yields a O(1/T) convergence rate for ADM and forms the basis for regret anal- ysis in the online setting. We consider two scenarios in the online setting, based on whether an additional Bregman divergence is needed or not. In both settings, we establish regret bounds for both the objective function as well as constraints violation for general and strongly convex functions. We also consider inexact ADM updates where certain terms are linearized to yield efficient updates and show the stochastic convergence rates. In addition, we briefly discuss that online ADM can be used as projection- free online learning algorithm in some scenarios. Preliminary results are presented to illustrate the performance of the proposed algorithms. version:2
arxiv-1302-1515 | A Polynomial Time Algorithm for Lossy Population Recovery | http://arxiv.org/abs/1302.1515 | id:1302.1515 author:Ankur Moitra, Michael Saks category:cs.DS cs.LG  published:2013-02-06 summary:We give a polynomial time algorithm for the lossy population recovery problem. In this problem, the goal is to approximately learn an unknown distribution on binary strings of length $n$ from lossy samples: for some parameter $\mu$ each coordinate of the sample is preserved with probability $\mu$ and otherwise is replaced by a `?'. The running time and number of samples needed for our algorithm is polynomial in $n$ and $1/\varepsilon$ for each fixed $\mu>0$. This improves on algorithm of Wigderson and Yehudayoff that runs in quasi-polynomial time for any $\mu > 0$ and the polynomial time algorithm of Dvir et al which was shown to work for $\mu \gtrapprox 0.30$ by Batman et al. In fact, our algorithm also works in the more general framework of Batman et al. in which there is no a priori bound on the size of the support of the distribution. The algorithm we analyze is implicit in previous work; our main contribution is to analyze the algorithm by showing (via linear programming duality and connections to complex analysis) that a certain matrix associated with the problem has a robust local inverse even though its condition number is exponentially small. A corollary of our result is the first polynomial time algorithm for learning DNFs in the restriction access model of Dvir et al. version:2
arxiv-1307-2818 | Anisotropic Diffusion for Details Enhancement in Multi-Exposure Image Fusion | http://arxiv.org/abs/1307.2818 | id:1307.2818 author:Harbinder Singh, Vinay Kumar, Sunil Bhooshan category:cs.MM cs.CV  published:2013-07-10 summary:We develop a multiexposure image fusion method based on texture features, which exploits the edge preserving and intraregion smoothing property of nonlinear diffusion filters based on partial differential equations (PDE). With the captured multiexposure image series, we first decompose images into base layers and detail layers to extract sharp details and fine details, respectively. The magnitude of the gradient of the image intensity is utilized to encourage smoothness at homogeneous regions in preference to inhomogeneous regions. Then, we have considered texture features of the base layer to generate a mask (i.e., decision mask) that guides the fusion of base layers in multiresolution fashion. Finally, well-exposed fused image is obtained that combines fused base layer and the detail layers at each scale across all the input exposures. Proposed algorithm skipping complex High Dynamic Range Image (HDRI) generation and tone mapping steps to produce detail preserving image for display on standard dynamic range display devices. Moreover, our technique is effective for blending flash/no-flash image pair and multifocus images, that is, images focused on different targets. version:1
arxiv-1307-2715 | Optimisation dans la dÃ©tection de communautÃ©s recouvrantes et Ã©quilibre de Nash | http://arxiv.org/abs/1307.2715 | id:1307.2715 author:Michel Crampes, Michel PlantiÃ©, Marie Lopez category:stat.ML  published:2013-07-10 summary:Community detection in graphs has been the subject of many algorithms. Recent methods want to optimize a modularity function which shows a maximum of relationships within communities and found a minimum of inter-community relations. these algorithms are applied to unipartite, multipartite and directed graphs. However, given the NP-completeness of the problem, these algorithms are heuristics that do not guarantee an optimum. In this paper we introduce an algorithm which, based on an approximate solution obtained through a efficient detection algorithm, modifie it to achieve a local optimum based on a function. this reassignment function is a potential function and therefore the computed optimum is a Nash equilibrium. We supplement our method with an overlap function that allows to have simultaneously the two detection modes. Several experiments show the interest of our approach. version:1
arxiv-1307-2674 | Error Rate Bounds in Crowdsourcing Models | http://arxiv.org/abs/1307.2674 | id:1307.2674 author:Hongwei Li, Bin Yu, Dengyong Zhou category:stat.ML cs.LG stat.AP  published:2013-07-10 summary:Crowdsourcing is an effective tool for human-powered computation on many tasks challenging for computers. In this paper, we provide finite-sample exponential bounds on the error rate (in probability and in expectation) of hyperplane binary labeling rules under the Dawid-Skene crowdsourcing model. The bounds can be applied to analyze many common prediction methods, including the majority voting and weighted majority voting. These bound results could be useful for controlling the error rate and designing better algorithms. We show that the oracle Maximum A Posterior (MAP) rule approximately optimizes our upper bound on the mean error rate for any hyperplane binary labeling rule, and propose a simple data-driven weighted majority voting (WMV) rule (called one-step WMV) that attempts to approximate the oracle MAP and has a provable theoretical guarantee on the error rate. Moreover, we use simulated and real data to demonstrate that the data-driven EM-MAP rule is a good approximation to the oracle MAP rule, and to demonstrate that the mean error rate of the data-driven EM-MAP rule is also bounded by the mean error rate bound of the oracle MAP rule with estimated parameters plugging into the bound. version:1
arxiv-1307-6542 | Selection Mammogram Texture Descriptors Based on Statistics Properties Backpropagation Structure | http://arxiv.org/abs/1307.6542 | id:1307.6542 author:Shofwatul 'Uyun, Sri Hartati, Agus Harjoko, Subanar category:cs.CV  published:2013-07-10 summary:Computer Aided Diagnosis (CAD) system has been developed for the early detection of breast cancer, one of the most deadly cancer for women. The benign of mammogram has different texture from malignant. There are fifty mammogram images used in this work which are divided for training and testing. Therefore, the selection of the right texture to determine the level of accuracy of CAD system is important. The first and second order statistics are the texture feature extraction methods which can be used on a mammogram. This work classifies texture descriptor into nine groups where the extraction of features is classified using backpropagation learning with two types of multi-layer perceptron (MLP). The best texture descriptor as selected when the value of regression 1 appears in both the MLP-1 and the MLP-2 with the number of epoches less than 1000. The results of testing show that the best selected texture descriptor is the second order (combination) using all direction (0, 45, 90 and 135) that have twenty four descriptors. version:1
arxiv-1307-2611 | Controlling the Precision-Recall Tradeoff in Differential Dependency Network Analysis | http://arxiv.org/abs/1307.2611 | id:1307.2611 author:Diane Oyen, Alexandru Niculescu-Mizil, Rachel Ostroff, Alex Stewart, Vincent P. Clark category:stat.ML cs.LG  published:2013-07-09 summary:Graphical models have gained a lot of attention recently as a tool for learning and representing dependencies among variables in multivariate data. Often, domain scientists are looking specifically for differences among the dependency networks of different conditions or populations (e.g. differences between regulatory networks of different species, or differences between dependency networks of diseased versus healthy populations). The standard method for finding these differences is to learn the dependency networks for each condition independently and compare them. We show that this approach is prone to high false discovery rates (low precision) that can render the analysis useless. We then show that by imposing a bias towards learning similar dependency networks for each condition the false discovery rates can be reduced to acceptable levels, at the cost of finding a reduced number of differences. Algorithms developed in the transfer learning literature can be used to vary the strength of the imposed similarity bias and provide a natural mechanism to smoothly adjust this differential precision-recall tradeoff to cater to the requirements of the analysis conducted. We present real case studies (oncological and neurological) where domain experts use the proposed technique to extract useful differential networks that shed light on the biological processes involved in cancer and brain function. version:1
arxiv-1207-2346 | Cups Products in Z2-Cohomology of 3D Polyhedral Complexes | http://arxiv.org/abs/1207.2346 | id:1207.2346 author:Rocio Gonalez-Diaz, Javier Lamar, Ronald Umble category:cs.CV 55-XX  published:2012-07-10 summary:Let $I=(\mathbb{Z}^3,26,6,B)$ be a 3D digital image, let $Q(I)$ be the associated cubical complex and let $\partial Q(I)$ be the subcomplex of $Q(I)$ whose maximal cells are the quadrangles of $Q(I)$ shared by a voxel of $B$ in the foreground -- the object under study -- and by a voxel of $\mathbb{Z}^3\smallsetminus B$ in the background -- the ambient space. We show how to simplify the combinatorial structure of $\partial Q(I)$ and obtain a 3D polyhedral complex $P(I)$ homeomorphic to $\partial Q(I)$ but with fewer cells. We introduce an algorithm that computes cup products on $H^*(P(I);\mathbb{Z}_2)$ directly from the combinatorics. The computational method introduced here can be effectively applied to any polyhedral complex embedded in $\mathbb{R}^3$. version:3
arxiv-1307-2579 | Tuned Models of Peer Assessment in MOOCs | http://arxiv.org/abs/1307.2579 | id:1307.2579 author:Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong Do, Andrew Ng, Daphne Koller category:cs.LG cs.AI cs.HC stat.AP stat.ML  published:2013-07-09 summary:In massive open online courses (MOOCs), peer grading serves as a critical tool for scaling the grading of complex, open-ended assignments to courses with tens or hundreds of thousands of students. But despite promising initial trials, it does not always deliver accurate results compared to human experts. In this paper, we develop algorithms for estimating and correcting for grader biases and reliabilities, showing significant improvement in peer grading accuracy on real data with 63,199 peer grades from Coursera's HCI course offerings --- the largest peer grading networks analysed to date. We relate grader biases and reliabilities to other student factors such as student engagement, performance as well as commenting style. We also show that our model can lead to more intelligent assignment of graders to gradees. version:1
arxiv-1307-2559 | General Drift Analysis with Tail Bounds | http://arxiv.org/abs/1307.2559 | id:1307.2559 author:Per Kristian Lehre, Carsten Witt category:cs.NE  published:2013-07-09 summary:Drift analysis is one of the state-of-the-art techniques for the runtime analysis of randomized search heuristics. In recent years, many different drift theorems, including additive, multiplicative and variable drift, have been developed, applied and partly generalized or adapted to particular processes. A comprehensive overview article was missing. We provide not only such an overview but also present a universal drift theorem that generalizes virtually all existing drift theorems found in the literature. On the one hand, the new theorem bounds the expected first hitting time of optimal states in the underlying stochastic process. On the other hand, it also allows for general upper and lower tail bounds on the hitting time, which were not known before except for the special case of upper bounds in multiplicative drift scenarios. As a proof of concept, the new tail bounds are applied to prove very precise sharp-concentration results on the running time of the (1+1) EA on OneMax, general linear functions and LeadingOnes. Moreover, user-friendly specializations of the general drift theorem are given. version:1
arxiv-1307-2440 | Image Fusion Technologies In Commercial Remote Sensing Packages | http://arxiv.org/abs/1307.2440 | id:1307.2440 author:Firouz Abdullah Al-Wassai, N. V. Kalyankar category:cs.CV  published:2013-07-09 summary:Several remote sensing software packages are used to the explicit purpose of analyzing and visualizing remotely sensed data, with the developing of remote sensing sensor technologies from last ten years. Accord-ing to literature, the remote sensing is still the lack of software tools for effective information extraction from remote sensing data. So, this paper provides a state-of-art of multi-sensor image fusion technologies as well as review on the quality evaluation of the single image or fused images in the commercial remote sensing pack-ages. It also introduces program (ALwassaiProcess) developed for image fusion and classification. version:1
arxiv-1307-2434 | Major Limitations of Satellite images | http://arxiv.org/abs/1307.2434 | id:1307.2434 author:Firouz A. Al-Wassai, N. V. Kalyankar category:cs.CV  published:2013-07-09 summary:Remote sensing has proven to be a powerful tool for the monitoring of the Earth surface to improve our perception of our surroundings has led to unprecedented developments in sensor and information technologies. However, technologies for effective use of the data and for extracting useful information from the data of Remote sensing are still very limited since no single sensor combines the optimal spectral, spatial and temporal resolution. This paper briefly reviews the limitations of satellite remote sensing. Also, reviews on the problems of image fusion techniques. The conclusion of this, According to literature, the remote sensing is still the lack of software tools for effective information extraction from remote sensing data. The trade-off in spectral and spatial resolution will remain and new advanced data fusion approaches are needed to make optimal use of remote sensors for extract the most useful information. version:1
arxiv-1302-1043 | The price of bandit information in multiclass online classification | http://arxiv.org/abs/1302.1043 | id:1302.1043 author:Amit Daniely, Tom Helbertal category:cs.LG  published:2013-02-05 summary:We consider two scenarios of multiclass online learning of a hypothesis class $H\subseteq Y^X$. In the {\em full information} scenario, the learner is exposed to instances together with their labels. In the {\em bandit} scenario, the true label is not exposed, but rather an indication whether the learner's prediction is correct or not. We show that the ratio between the error rates in the two scenarios is at most $8\cdot Y \cdot \log( Y )$ in the realizable case, and $\tilde{O}(\sqrt{ Y })$ in the agnostic case. The results are tight up to a logarithmic factor and essentially answer an open question from (Daniely et. al. - Multiclass learnability and the erm principle). We apply these results to the class of $\gamma$-margin multiclass linear classifiers in $\reals^d$. We show that the bandit error rate of this class is $\tilde{\Theta}(\frac{ Y }{\gamma^2})$ in the realizable case and $\tilde{\Theta}(\frac{1}{\gamma}\sqrt{ Y T})$ in the agnostic case. This resolves an open question from (Kakade et. al. - Efficient bandit algorithms for online multiclass prediction). version:2
arxiv-1307-1759 | Approximate dynamic programming using fluid and diffusion approximations with applications to power management | http://arxiv.org/abs/1307.1759 | id:1307.1759 author:Wei Chen, Dayu Huang, Ankur A. Kulkarni, Jayakrishnan Unnikrishnan, Quanyan Zhu, Prashant Mehta, Sean Meyn, Adam Wierman category:cs.LG math.OC  published:2013-07-06 summary:Neuro-dynamic programming is a class of powerful techniques for approximating the solution to dynamic programming equations. In their most computationally attractive formulations, these techniques provide the approximate solution only within a prescribed finite-dimensional function class. Thus, the question that always arises is how should the function class be chosen? The goal of this paper is to propose an approach using the solutions to associated fluid and diffusion approximations. In order to illustrate this approach, the paper focuses on an application to dynamic speed scaling for power management in computer processors. version:2
arxiv-1307-2312 | Bayesian Discovery of Multiple Bayesian Networks via Transfer Learning | http://arxiv.org/abs/1307.2312 | id:1307.2312 author:Diane Oyen, Terran Lane category:stat.ML cs.LG  published:2013-07-09 summary:Bayesian network structure learning algorithms with limited data are being used in domains such as systems biology and neuroscience to gain insight into the underlying processes that produce observed data. Learning reliable networks from limited data is difficult, therefore transfer learning can improve the robustness of learned networks by leveraging data from related tasks. Existing transfer learning algorithms for Bayesian network structure learning give a single maximum a posteriori estimate of network models. Yet, many other models may be equally likely, and so a more informative result is provided by Bayesian structure discovery. Bayesian structure discovery algorithms estimate posterior probabilities of structural features, such as edges. We present transfer learning for Bayesian structure discovery which allows us to explore the shared and unique structural features among related tasks. Efficient computation requires that our transfer learning objective factors into local calculations, which we prove is given by a broad class of transfer biases. Theoretically, we show the efficiency of our approach. Empirically, we show that compared to single task learning, transfer learning is better able to positively identify true edges. We apply the method to whole-brain neuroimaging data. version:1
arxiv-1307-2307 | Bridging Information Criteria and Parameter Shrinkage for Model Selection | http://arxiv.org/abs/1307.2307 | id:1307.2307 author:Kun Zhang, Heng Peng, Laiwan Chan, Aapo Hyvarinen category:stat.ML cs.LG  published:2013-07-08 summary:Model selection based on classical information criteria, such as BIC, is generally computationally demanding, but its properties are well studied. On the other hand, model selection based on parameter shrinkage by $\ell_1$-type penalties is computationally efficient. In this paper we make an attempt to combine their strengths, and propose a simple approach that penalizes the likelihood with data-dependent $\ell_1$ penalties as in adaptive Lasso and exploits a fixed penalization parameter. Even for finite samples, its model selection results approximately coincide with those based on information criteria; in particular, we show that in some special cases, this approach and the corresponding information criterion produce exactly the same model. One can also consider this approach as a way to directly determine the penalization parameter in adaptive Lasso to achieve information criteria-like model selection. As extensions, we apply this idea to complex models including Gaussian mixture model and mixture of factor analyzers, whose model selection is traditionally difficult to do; by adopting suitable penalties, we provide continuous approximators to the corresponding information criteria, which are easy to optimize and enable efficient model selection. version:1
arxiv-1306-4653 | Multiarmed Bandits With Limited Expert Advice | http://arxiv.org/abs/1306.4653 | id:1306.4653 author:Satyen Kale category:cs.LG  published:2013-06-19 summary:We solve the COLT 2013 open problem of \citet{SCB} on minimizing regret in the setting of advice-efficient multiarmed bandits with expert advice. We give an algorithm for the setting of K arms and N experts out of which we are allowed to query and use only M experts' advices in each round, which has a regret bound of \tilde{O}\bigP{\sqrt{\frac{\min\{K, M\} N}{M} T}} after T rounds. We also prove that any algorithm for this problem must have expected regret at least \tilde{\Omega}\bigP{\sqrt{\frac{\min\{K, M\} N}{M}T}}, thus showing that our upper bound is nearly tight. version:4
arxiv-1307-2150 | Transmodal Analysis of Neural Signals | http://arxiv.org/abs/1307.2150 | id:1307.2150 author:Yaroslav O. Halchenko, Michael Hanke, James V. Haxby, Stephen Jose Hanson, Christoph S. Herrmann category:q-bio.NC cs.LG q-bio.QM  published:2013-07-08 summary:Localizing neuronal activity in the brain, both in time and in space, is a central challenge to advance the understanding of brain function. Because of the inability of any single neuroimaging techniques to cover all aspects at once, there is a growing interest to combine signals from multiple modalities in order to benefit from the advantages of each acquisition method. Due to the complexity and unknown parameterization of any suggested complete model of BOLD response in functional magnetic resonance imaging (fMRI), the development of a reliable ultimate fusion approach remains difficult. But besides the primary goal of superior temporal and spatial resolution, conjoint analysis of data from multiple imaging modalities can alternatively be used to segregate neural information from physiological and acquisition noise. In this paper we suggest a novel methodology which relies on constructing a quantifiable mapping of data from one modality (electroencephalography; EEG) into another (fMRI), called transmodal analysis of neural signals (TRANSfusion). TRANSfusion attempts to map neural data embedded within the EEG signal into its reflection in fMRI data. Assessing the mapping performance on unseen data allows to localize brain areas where a significant portion of the signal could be reliably reconstructed, hence the areas neural activity of which is reflected in both EEG and fMRI data. Consecutive analysis of the learnt model allows to localize areas associated with specific frequency bands of EEG, or areas functionally related (connected or coherent) to any given EEG sensor. We demonstrate the performance of TRANSfusion on artificial and real data from an auditory experiment. We further speculate on possible alternative uses: cross-modal data filtering and EEG-driven interpolation of fMRI signals to obtain arbitrarily high temporal sampling of BOLD. version:1
arxiv-0903-2299 | Differential Contrastive Divergence | http://arxiv.org/abs/0903.2299 | id:0903.2299 author:David McAllester category:cs.LG  published:2009-03-13 summary:This paper has been retracted. version:3
arxiv-1307-2118 | A PAC-Bayesian Tutorial with A Dropout Bound | http://arxiv.org/abs/1307.2118 | id:1307.2118 author:David McAllester category:cs.LG  published:2013-07-08 summary:This tutorial gives a concise overview of existing PAC-Bayesian theory focusing on three generalization bounds. The first is an Occam bound which handles rules with finite precision parameters and which states that generalization loss is near training loss when the number of bits needed to write the rule is small compared to the sample size. The second is a PAC-Bayesian bound providing a generalization guarantee for posterior distributions rather than for individual rules. The PAC-Bayesian bound naturally handles infinite precision rule parameters, $L_2$ regularization, {\em provides a bound for dropout training}, and defines a natural notion of a single distinguished PAC-Bayesian posterior distribution. The third bound is a training-variance bound --- a kind of bias-variance analysis but with bias replaced by expected training loss. The training-variance bound dominates the other bounds but is more difficult to interpret. It seems to suggest variance reduction methods such as bagging and may ultimately provide a more meaningful analysis of dropouts. version:1
arxiv-1307-2111 | Finding the creatures of habit; Clustering households based on their flexibility in using electricity | http://arxiv.org/abs/1307.2111 | id:1307.2111 author:Ian Dent, Tony Craig, Uwe Aickelin, Tom Rodden category:cs.LG cs.CE  published:2013-07-08 summary:Changes in the UK electricity market, particularly with the roll out of smart meters, will provide greatly increased opportunities for initiatives intended to change households' electricity usage patterns for the benefit of the overall system. Users show differences in their regular behaviours and clustering households into similar groupings based on this variability provides for efficient targeting of initiatives. Those people who are stuck into a regular pattern of activity may be the least receptive to an initiative to change behaviour. A sample of 180 households from the UK are clustered into four groups as an initial test of the concept and useful, actionable groupings are found. version:1
arxiv-1305-5566 | The most controversial topics in Wikipedia: A multilingual and geographical analysis | http://arxiv.org/abs/1305.5566 | id:1305.5566 author:Taha Yasseri, Anselm Spoerri, Mark Graham, JÃ¡nos KertÃ©sz category:physics.soc-ph cs.CL cs.DL cs.SI physics.data-an  published:2013-05-23 summary:We present, visualize and analyse the similarities and differences between the controversial topics related to "edit wars" identified in 10 different language versions of Wikipedia. After a brief review of the related work we describe the methods developed to locate, measure, and categorize the controversial topics in the different languages. Visualizations of the degree of overlap between the top 100 lists of most controversial articles in different languages and the content related to geographical locations will be presented. We discuss what the presented analysis and visualizations can tell us about the multicultural aspects of Wikipedia and practices of peer-production. Our results indicate that Wikipedia is more than just an encyclopaedia; it is also a window into convergent and divergent social-spatial priorities, interests and preferences. version:2
arxiv-1307-1998 | Using Clustering to extract Personality Information from socio economic data | http://arxiv.org/abs/1307.1998 | id:1307.1998 author:Alexandros Ladas, Uwe Aickelin, Jon Garibaldi, Eamonn Ferguson category:cs.LG cs.CE  published:2013-07-08 summary:It has become apparent that models that have been applied widely in economics, including Machine Learning techniques and Data Mining methods, should take into consideration principles that derive from the theories of Personality Psychology in order to discover more comprehensive knowledge regarding complicated economic behaviours. In this work, we present a method to extract Behavioural Groups by using simple clustering techniques that can potentially reveal aspects of the Personalities for their members. We believe that this is very important because the psychological information regarding the Personalities of individuals is limited in real world applications and because it can become a useful tool in improving the traditional models of Knowledge Economy. version:1
arxiv-1212-3618 | Machine Learning in Proof General: Interfacing Interfaces | http://arxiv.org/abs/1212.3618 | id:1212.3618 author:Ekaterina Komendantskaya, JÃ³nathan Heras, Gudmund Grov category:cs.AI cs.LG cs.LO I.2.6; F.4.1  published:2012-12-14 summary:We present ML4PG - a machine learning extension for Proof General. It allows users to gather proof statistics related to shapes of goals, sequences of applied tactics, and proof tree structures from the libraries of interactive higher-order proofs written in Coq and SSReflect. The gathered data is clustered using the state-of-the-art machine learning algorithms available in MATLAB and Weka. ML4PG provides automated interfacing between Proof General and MATLAB/Weka. The results of clustering are used by ML4PG to provide proof hints in the process of interactive proof development. version:2
arxiv-1307-1872 | Intelligent Hybrid Man-Machine Translation Quality Estimation | http://arxiv.org/abs/1307.1872 | id:1307.1872 author:Ibrahim Sabek, Noha A. Yousri, Nagwa Elmakky, Mona Habib category:cs.CL  published:2013-07-07 summary:Inferring evaluation scores based on human judgments is invaluable compared to using current evaluation metrics which are not suitable for real-time applications e.g. post-editing. However, these judgments are much more expensive to collect especially from expert translators, compared to evaluation based on indicators contrasting source and translation texts. This work introduces a novel approach for quality estimation by combining learnt confidence scores from a probabilistic inference model based on human judgments, with selective linguistic features-based scores, where the proposed inference model infers the credibility of given human ranks to solve the scarcity and inconsistency issues of human judgments. Experimental results, using challenging language-pairs, demonstrate improvement in correlation with human judgments over traditional evaluation metrics. version:1
arxiv-1307-1769 | Ensemble Methods for Multi-label Classification | http://arxiv.org/abs/1307.1769 | id:1307.1769 author:Lior Rokach, Alon Schclar, Ehud Itach category:stat.ML cs.LG 68T05  68Q32 I.5; I.2.6; K.3.2  published:2013-07-06 summary:Ensemble methods have been shown to be an effective tool for solving multi-label classification tasks. In the RAndom k-labELsets (RAKEL) algorithm, each member of the ensemble is associated with a small randomly-selected subset of k labels. Then, a single label classifier is trained according to each combination of elements in the subset. In this paper we adopt a similar approach, however, instead of randomly choosing subsets, we select the minimum required subsets of k labels that cover all labels and meet additional constraints such as coverage of inter-label correlations. Construction of the cover is achieved by formulating the subset selection as a minimum set covering problem (SCP) and solving it by using approximation algorithms. Every cover needs only to be prepared once by offline algorithms. Once prepared, a cover may be applied to the classification of any given multi-label dataset whose properties conform with those of the cover. The contribution of this paper is two-fold. First, we introduce SCP as a general framework for constructing label covers while allowing the user to incorporate cover construction constraints. We demonstrate the effectiveness of this framework by proposing two construction constraints whose enforcement produces covers that improve the prediction performance of random selection. Second, we provide theoretical bounds that quantify the probabilities of random selection to produce covers that meet the proposed construction criteria. The experimental results indicate that the proposed methods improve multi-label classification accuracy and stability compared with the RAKEL algorithm and to other state-of-the-art algorithms. version:1
arxiv-1305-4077 | Indexing Medical Images based on Collaborative Experts Reports | http://arxiv.org/abs/1305.4077 | id:1305.4077 author:Abir Messaoudi, Riadh Bouslimi, Jalel Akaichi category:cs.CV cs.IR  published:2013-05-17 summary:A patient is often willing to quickly get, from his physician, reliable analysis and concise explanation according to provided linked medical images. The fact of making choices individually by the patient's physician may lead to malpractices and consequently generates unforeseeable damages. The Institute of Medicine of the National Sciences Academy(IMNAS) in USA published a study estimating that up to 98,000 hospital deathseach year can be attributed to medical malpractice [1]. Moreover, physician, in charge of medical image analysis, might be unavailable at the right time, which may complicate the patient's state. The goal of this paper is to provide to physicians and patients, a social network that permits to foster cooperation and to overcome the problem of unavailability of doctors on site any time. Therefore, patients can submit their medical images to be diagnosed and commented by several experts instantly. Consequently, the need to process opinions and to extract information automatically from the proposed social network became a necessity due to the huge number of comments expressing specialist's reviews. For this reason, we propose a kind of comments' summary keywords-based method which extracts the major current terms and relevant words existing on physicians' annotations. The extracted keywords will present a new and robust method for image indexation. In fact, significant extracted terms will be used later to index images in order to facilitate their discovery for any appropriate use. To overcome this challenge, we propose our Terminology Extraction of Annotation (TEA) mixed approach which focuses on algorithms mainly based on statistical methods and on external semantic resources. version:2
arxiv-1307-1674 | Stochastic Optimization of PCA with Capped MSG | http://arxiv.org/abs/1307.1674 | id:1307.1674 author:Raman Arora, Andrew Cotter, Nathan Srebro category:stat.ML cs.LG  published:2013-07-05 summary:We study PCA as a stochastic optimization problem and propose a novel stochastic approximation algorithm which we refer to as "Matrix Stochastic Gradient" (MSG), as well as a practical variant, Capped MSG. We study the method both theoretically and empirically. version:1
arxiv-1307-1601 | Biomarker Clustering of Colorectal Cancer Data to Complement Clinical Classification | http://arxiv.org/abs/1307.1601 | id:1307.1601 author:Chris Roadknight, Uwe Aickelin, Alex Ladas, Daniele Soria, John Scholefield, Lindy Durrant category:cs.LG cs.CE  published:2013-07-05 summary:In this paper, we describe a dataset relating to cellular and physical conditions of patients who are operated upon to remove colorectal tumours. This data provides a unique insight into immunological status at the point of tumour removal, tumour classification and post-operative survival. Attempts are made to cluster this dataset and important subsets of it in an effort to characterize the data and validate existing standards for tumour classification. It is apparent from optimal clustering that existing tumour classification is largely unrelated to immunological factors within a patient and that there may be scope for re-evaluating treatment options and survival estimates based on a combination of tumour physiology and patient histochemistry. version:1
arxiv-1307-1599 | Supervised Learning and Anti-learning of Colorectal Cancer Classes and Survival Rates from Cellular Biology Parameters | http://arxiv.org/abs/1307.1599 | id:1307.1599 author:Chris Roadknight, Uwe Aickelin, Guoping Qiu, John Scholefield, Lindy Durrant category:cs.LG cs.CE stat.ML  published:2013-07-05 summary:In this paper, we describe a dataset relating to cellular and physical conditions of patients who are operated upon to remove colorectal tumours. This data provides a unique insight into immunological status at the point of tumour removal, tumour classification and post-operative survival. Attempts are made to learn relationships between attributes (physical and immunological) and the resulting tumour stage and survival. Results for conventional machine learning approaches can be considered poor, especially for predicting tumour stages for the most important types of cancer. This poor performance is further investigated and compared with a synthetic, dataset based on the logical exclusive-OR function and it is shown that there is a significant level of 'anti-learning' present in all supervised methods used and this can be explained by the highly dimensional, complex and sparsely representative dataset. For predicting the stage of cancer from the immunological attributes, anti-learning approaches outperform a range of popular algorithms. version:1
arxiv-1307-1584 | Comparing Data-mining Algorithms Developed for Longitudinal Observational Databases | http://arxiv.org/abs/1307.1584 | id:1307.1584 author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.LG cs.CE cs.DB  published:2013-07-05 summary:Longitudinal observational databases have become a recent interest in the post marketing drug surveillance community due to their ability of presenting a new perspective for detecting negative side effects. Algorithms mining longitudinal observation databases are not restricted by many of the limitations associated with the more conventional methods that have been developed for spontaneous reporting system databases. In this paper we investigate the robustness of four recently developed algorithms that mine longitudinal observational databases by applying them to The Health Improvement Network (THIN) for six drugs with well document known negative side effects. Our results show that none of the existing algorithms was able to consistently identify known adverse drug reactions above events related to the cause of the drug and no algorithm was superior. version:1
arxiv-1307-1561 | A Sub-block Based Image Retrieval Using Modified Integrated Region Matching | http://arxiv.org/abs/1307.1561 | id:1307.1561 author:E. R. Vimina, K. Poulose Jacob category:cs.IR cs.CV  published:2013-07-05 summary:This paper proposes a content based image retrieval (CBIR) system using the local colour and texture features of selected image sub-blocks and global colour and shape features of the image. The image sub-blocks are roughly identified by segmenting the image into partitions of different configuration, finding the edge density in each partition using edge thresholding followed by morphological dilation. The colour and texture features of the identified regions are computed from the histograms of the quantized HSV colour space and Gray Level Co- occurrence Matrix (GLCM) respectively. The colour and texture feature vectors is computed for each region. The shape features are computed from the Edge Histogram Descriptor (EHD). A modified Integrated Region Matching (IRM) algorithm is used for finding the minimum distance between the sub-blocks of the query and target image. Experimental results show that the proposed method provides better retrieving result than retrieval using some of the existing methods. version:1
arxiv-1307-1437 | Toward Guaranteed Illumination Models for Non-Convex Objects | http://arxiv.org/abs/1307.1437 | id:1307.1437 author:Yuqian Zhang, Cun Mu, Han-wen Kuo, John Wright category:cs.CV  published:2013-07-04 summary:Illumination variation remains a central challenge in object detection and recognition. Existing analyses of illumination variation typically pertain to convex, Lambertian objects, and guarantee quality of approximation in an average case sense. We show that it is possible to build V(vertex)-description convex cone models with worst-case performance guarantees, for non-convex Lambertian objects. Namely, a natural verification test based on the angle to the constructed cone guarantees to accept any image which is sufficiently well-approximated by an image of the object under some admissible lighting condition, and guarantees to reject any image that does not have a sufficiently good approximation. The cone models are generated by sampling point illuminations with sufficient density, which follows from a new perturbation bound for point images in the Lambertian model. As the number of point images required for guaranteed verification may be large, we introduce a new formulation for cone preserving dimensionality reduction, which leverages tools from sparse and low-rank decomposition to reduce the complexity, while controlling the approximation error with respect to the original cone. version:1
arxiv-1307-1411 | Discovering Sequential Patterns in a UK General Practice Database | http://arxiv.org/abs/1307.1411 | id:1307.1411 author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.LG cs.CE stat.AP  published:2013-07-04 summary:The wealth of computerised medical information becoming readily available presents the opportunity to examine patterns of illnesses, therapies and responses. These patterns may be able to predict illnesses that a patient is likely to develop, allowing the implementation of preventative actions. In this paper sequential rule mining is applied to a General Practice database to find rules involving a patients age, gender and medical history. By incorporating these rules into current health-care a patient can be highlighted as susceptible to a future illness based on past or current illnesses, gender and year of birth. This knowledge has the ability to greatly improve health-care and reduce health-care costs. version:1
arxiv-1307-1394 | Detect adverse drug reactions for drug Alendronate | http://arxiv.org/abs/1307.1394 | id:1307.1394 author:Yihui Liu, Uwe Aickelin category:cs.CE cs.LG  published:2013-07-04 summary:Adverse drug reaction (ADR) is widely concerned for public health issue. In this study we propose an original approach to detect the ADRs using feature matrix and feature selection. The experiments are carried out on the drug Simvastatin. Major side effects for the drug are detected and better performance is achieved compared to other computerized methods. The detected ADRs are based on the computerized method, further investigation is needed. version:1
arxiv-1307-1391 | Quiet in Class: Classification, Noise and the Dendritic Cell Algorithm | http://arxiv.org/abs/1307.1391 | id:1307.1391 author:Feng Gu, Jan Feyereisl, Robert Oates, Jenna Reps, Julie Greensmith, Uwe Aickelin category:cs.LG cs.CR  published:2013-07-04 summary:Theoretical analyses of the Dendritic Cell Algorithm (DCA) have yielded several criticisms about its underlying structure and operation. As a result, several alterations and fixes have been suggested in the literature to correct for these findings. A contribution of this work is to investigate the effects of replacing the classification stage of the DCA (which is known to be flawed) with a traditional machine learning technique. This work goes on to question the merits of those unique properties of the DCA that are yet to be thoroughly analysed. If none of these properties can be found to have a benefit over traditional approaches, then "fixing" the DCA is arguably less efficient than simply creating a new algorithm. This work examines the dynamic filtering property of the DCA and questions the utility of this unique feature for the anomaly detection problem. It is found that this feature, while advantageous for noisy, time-ordered classification, is not as useful as a traditional static filter for processing a synthetic dataset. It is concluded that there are still unique features of the DCA left to investigate. Areas that may be of benefit to the Artificial Immune Systems community are suggested. version:1
arxiv-1307-1387 | Examining the Classification Accuracy of TSVMs with ?Feature Selection in Comparison with the GLAD Algorithm | http://arxiv.org/abs/1307.1387 | id:1307.1387 author:Hala Helmi, Jon M. Garibaldi, Uwe Aickelin category:cs.LG cs.CE  published:2013-07-04 summary:Gene expression data sets are used to classify and predict patient diagnostic categories. As we know, it is extremely difficult and expensive to obtain gene expression labelled examples. Moreover, conventional supervised approaches cannot function properly when labelled data (training examples) are insufficient using Support Vector Machines (SVM) algorithms. Therefore, in this paper, we suggest Transductive Support Vector Machines (TSVMs) as semi-supervised learning algorithms, learning with both labelled samples data and unlabelled samples to perform the classification of microarray data. To prune the superfluous genes and samples we used a feature selection method called Recursive Feature Elimination (RFE), which is supposed to enhance the output of classification and avoid the local optimization problem. We examined the classification prediction accuracy of the TSVM-RFE algorithm in comparison with the Genetic Learning Across Datasets (GLAD) algorithm, as both are semi-supervised learning methods. Comparing these two methods, we found that the TSVM-RFE surpassed both a SVM using RFE and GLAD. version:1
arxiv-1307-1385 | Creating Personalised Energy Plans. From Groups to Individuals using Fuzzy C Means Clustering | http://arxiv.org/abs/1307.1385 | id:1307.1385 author:Ian Dent, Christian Wagner, Uwe Aickelin, Tom Rodden category:cs.CE cs.LG  published:2013-07-04 summary:Changes in the UK electricity market mean that domestic users will be required to modify their usage behaviour in order that supplies can be maintained. Clustering allows usage profiles collected at the household level to be clustered into groups and assigned a stereotypical profile which can be used to target marketing campaigns. Fuzzy C Means clustering extends this by allowing each household to be a member of many groups and hence provides the opportunity to make personalised offers to the household dependent on their degree of membership of each group. In addition, feedback can be provided on how user's changing behaviour is moving them towards more "green" or cost effective stereotypical usage. version:1
arxiv-1307-1380 | The Application of a Data Mining Framework to Energy Usage Profiling in Domestic Residences using UK data | http://arxiv.org/abs/1307.1380 | id:1307.1380 author:Ian Dent, Uwe Aickelin, Tom Rodden category:cs.CE cs.LG stat.AP  published:2013-07-04 summary:This paper describes a method for defining representative load profiles for domestic electricity users in the UK. It considers bottom up and clustering methods and then details the research plans for implementing and improving existing framework approaches based on the overall usage profile. The work focuses on adapting and applying analysis framework approaches to UK energy data in order to determine the effectiveness of creating a few (single figures) archetypical users with the intention of improving on the current methods of determining usage profiles. The work is currently in progress and the paper details initial results using data collected in Milton Keynes around 1990. Various possible enhancements to the work are considered including a split based on temperature to reflect the varying UK weather conditions. version:1
arxiv-1301-6646 | Image registration with sparse approximations in parametric dictionaries | http://arxiv.org/abs/1301.6646 | id:1301.6646 author:Alhussein Fawzi, Pascal Frossard category:cs.CV  published:2013-01-28 summary:We examine in this paper the problem of image registration from the new perspective where images are given by sparse approximations in parametric dictionaries of geometric functions. We propose a registration algorithm that looks for an estimate of the global transformation between sparse images by examining the set of relative geometrical transformations between the respective features. We propose a theoretical analysis of our registration algorithm and we derive performance guarantees based on two novel important properties of redundant dictionaries, namely the robust linear independence and the transformation inconsistency. We propose several illustrations and insights about the importance of these dictionary properties and show that common properties such as coherence or restricted isometry property fail to provide sufficient information in registration problems. We finally show with illustrative experiments on simple visual objects and handwritten digits images that our algorithm outperforms baseline competitor methods in terms of transformation-invariant distance computation and classification. version:2
arxiv-1307-1289 | Further results on dissimilarity spaces for hyperspectral images RF-CBIR | http://arxiv.org/abs/1307.1289 | id:1307.1289 author:Miguel Angel Veganzones, Mihai Datcu, Manuel GraÃ±a category:cs.IR cs.CV  published:2013-07-04 summary:Content-Based Image Retrieval (CBIR) systems are powerful search tools in image databases that have been little applied to hyperspectral images. Relevance feedback (RF) is an iterative process that uses machine learning techniques and user's feedback to improve the CBIR systems performance. We pursued to expand previous research in hyperspectral CBIR systems built on dissimilarity functions defined either on spectral and spatial features extracted by spectral unmixing techniques, or on dictionaries extracted by dictionary-based compressors. These dissimilarity functions were not suitable for direct application in common machine learning techniques. We propose to use a RF general approach based on dissimilarity spaces which is more appropriate for the application of machine learning algorithms to the hyperspectral RF-CBIR. We validate the proposed RF method for hyperspectral CBIR systems over a real hyperspectral dataset. version:1
arxiv-1307-1275 | Constructing Hierarchical Image-tags Bimodal Representations for Word Tags Alternative Choice | http://arxiv.org/abs/1307.1275 | id:1307.1275 author:Fangxiang Feng, Ruifan Li, Xiaojie Wang category:cs.LG cs.NE  published:2013-07-04 summary:This paper describes our solution to the multi-modal learning challenge of ICML. This solution comprises constructing three-level representations in three consecutive stages and choosing correct tag words with a data-specific strategy. Firstly, we use typical methods to obtain level-1 representations. Each image is represented using MPEG-7 and gist descriptors with additional features released by the contest organizers. And the corresponding word tags are represented by bag-of-words model with a dictionary of 4000 words. Secondly, we learn the level-2 representations using two stacked RBMs for each modality. Thirdly, we propose a bimodal auto-encoder to learn the similarities/dissimilarities between the pairwise image-tags as level-3 representations. Finally, during the test phase, based on one observation of the dataset, we come up with a data-specific strategy to choose the correct tag words leading to a leap of an improved overall performance. Our final average accuracy on the private test set is 100%, which ranks the first place in this challenge. version:1
arxiv-1307-1192 | AdaBoost and Forward Stagewise Regression are First-Order Convex Optimization Methods | http://arxiv.org/abs/1307.1192 | id:1307.1192 author:Robert M. Freund, Paul Grigas, Rahul Mazumder category:stat.ML cs.LG math.OC  published:2013-07-04 summary:Boosting methods are highly popular and effective supervised learning methods which combine weak learners into a single accurate model with good statistical performance. In this paper, we analyze two well-known boosting methods, AdaBoost and Incremental Forward Stagewise Regression (FS$_\varepsilon$), by establishing their precise connections to the Mirror Descent algorithm, which is a first-order method in convex optimization. As a consequence of these connections we obtain novel computational guarantees for these boosting methods. In particular, we characterize convergence bounds of AdaBoost, related to both the margin and log-exponential loss function, for any step-size sequence. Furthermore, this paper presents, for the first time, precise computational complexity results for FS$_\varepsilon$. version:1
arxiv-1210-0115 | Demosaicing and Superresolution for Color Filter Array via Residual Image Reconstruction and Sparse Representation | http://arxiv.org/abs/1210.0115 | id:1210.0115 author:Guangling Sun category:cs.CV  published:2012-09-29 summary:A framework of demosaicing and superresolution for color filter array (CFA) via residual image reconstruction and sparse representation is presented.Given the intermediate image produced by certain demosaicing and interpolation technique, a residual image between the final reconstruction image and the intermediate image is reconstructed using sparse representation.The final reconstruction image has richer edges and details than that of the intermediate image. Specifically, a generic dictionary is learned from a large set of composite training data composed of intermediate data and residual data. The learned dictionary implies a mapping between the two data. A specific dictionary adaptive to the input CFA is learned thereafter. Using the adaptive dictionary, the sparse coefficients of intermediate data are computed and transformed to predict residual image. The residual image is added back into the intermediate image to obtain the final reconstruction image. Experimental results demonstrate the state-of-the-art performance in terms of PSNR and subjective visual perception. version:2
arxiv-1205-5012 | Learning Mixed Graphical Models | http://arxiv.org/abs/1205.5012 | id:1205.5012 author:Jason D. Lee, Trevor J. Hastie category:stat.ML cs.CV cs.LG math.OC  published:2012-05-22 summary:We consider the problem of learning the structure of a pairwise graphical model over continuous and discrete variables. We present a new pairwise model for graphical models with both continuous and discrete variables that is amenable to structure learning. In previous work, authors have considered structure learning of Gaussian graphical models and structure learning of discrete models. Our approach is a natural generalization of these two lines of work to the mixed case. The penalization scheme involves a novel symmetric use of the group-lasso norm and follows naturally from a particular parametrization of the model. version:3
arxiv-1307-1166 | A Novel Robust Method to Add Watermarks to Bitmap Images by Fading Technique | http://arxiv.org/abs/1307.1166 | id:1307.1166 author:Firas A. Jassim category:cs.CV cs.MM  published:2013-07-03 summary:Digital water marking is one of the essential fields in image security and copyright protection. The proposed technique in this paper was based on the principle of protecting images by hide an invisible watermark in the image. The technique starts with merging the cover image and the watermark image with suitable ratios, i.e., 99% from the cover image will be merged with 1% from the watermark image. Technically, the fading process is irreversible but with the proposed technique, the probability to reconstruct the original watermark image is great. There is no perceptible difference between the original and watermarked image by human eye. The experimental results show that the proposed technique proven its ability to hide images that have the same size of the cover image. Three performance measures were implemented to support the proposed techniques which are MSE, PSNR, and SSIM. Fortunately, all the three measures have excellent values. version:1
arxiv-1303-4778 | Greedy Feature Selection for Subspace Clustering | http://arxiv.org/abs/1303.4778 | id:1303.4778 author:Eva L. Dyer, Aswin C. Sankaranarayanan, Richard G. Baraniuk category:cs.LG math.NA stat.ML  published:2013-03-19 summary:Unions of subspaces provide a powerful generalization to linear subspace models for collections of high-dimensional data. To learn a union of subspaces from a collection of data, sets of signals in the collection that belong to the same subspace must be identified in order to obtain accurate estimates of the subspace structures present in the data. Recently, sparse recovery methods have been shown to provide a provable and robust strategy for exact feature selection (EFS)--recovering subsets of points from the ensemble that live in the same subspace. In parallel with recent studies of EFS with L1-minimization, in this paper, we develop sufficient conditions for EFS with a greedy method for sparse signal recovery known as orthogonal matching pursuit (OMP). Following our analysis, we provide an empirical study of feature selection strategies for signals living on unions of subspaces and characterize the gap between sparse recovery methods and nearest neighbor (NN)-based approaches. In particular, we demonstrate that sparse recovery methods provide significant advantages over NN methods and the gap between the two approaches is particularly pronounced when the sampling of subspaces in the dataset is sparse. Our results suggest that OMP may be employed to reliably recover exact feature sets in a number of regimes where NN approaches fail to reveal the subspace membership of points in the ensemble. version:2
arxiv-1307-1079 | Application of a clustering framework to UK domestic electricity data | http://arxiv.org/abs/1307.1079 | id:1307.1079 author:Ian Dent, Uwe Aickelin, Tom Rodden category:cs.CE cs.LG  published:2013-07-03 summary:This paper takes an approach to clustering domestic electricity load profiles that has been successfully used with data from Portugal and applies it to UK data. Clustering techniques are applied and it is found that the preferred technique in the Portuguese work (a two stage process combining Self Organised Maps and Kmeans) is not appropriate for the UK data. The work shows that up to nine clusters of households can be identified with the differences in usage profiles being visually striking. This demonstrates the appropriateness of breaking the electricity usage patterns down to more detail than the two load profiles currently published by the electricity industry. The paper details initial results using data collected in Milton Keynes around 1990. Further work is described and will concentrate on building accurate and meaningful clusters of similar electricity users in order to better direct demand side management initiatives to the most relevant target customers. version:1
arxiv-1307-1078 | Investigating the Detection of Adverse Drug Events in a UK General Practice Electronic Health-Care Database | http://arxiv.org/abs/1307.1078 | id:1307.1078 author:Jenna Reps, Jan Feyereisl, Jonathan M. Garibaldi, Uwe Aickelin, Jack E. Gibson, Richard B. Hubbard category:cs.CE cs.LG  published:2013-07-03 summary:Data-mining techniques have frequently been developed for Spontaneous reporting databases. These techniques aim to find adverse drug events accurately and efficiently. Spontaneous reporting databases are prone to missing information, under reporting and incorrect entries. This often results in a detection lag or prevents the detection of some adverse drug events. These limitations do not occur in electronic health-care databases. In this paper, existing methods developed for spontaneous reporting databases are implemented on both a spontaneous reporting database and a general practice electronic health-care database and compared. The results suggests that the application of existing methods to the general practice database may help find signals that have gone undetected when using the spontaneous reporting system database. In addition the general practice database provides far more supplementary information, that if incorporated in analysis could provide a wealth of information for identifying adverse events more accurately. version:1
arxiv-1307-1070 | A Comparison of Non-stationary, Type-2 and Dual Surface Fuzzy Control | http://arxiv.org/abs/1307.1070 | id:1307.1070 author:Naisan Benatar, Uwe Aickelin, Jonathan M. Garibaldi category:cs.AI cs.NE  published:2013-07-03 summary:Type-1 fuzzy logic has frequently been used in control systems. However this method is sometimes shown to be too restrictive and unable to adapt in the presence of uncertainty. In this paper we compare type-1 fuzzy control with several other fuzzy approaches under a range of uncertain conditions. Interval type-2 and non-stationary fuzzy controllers are compared, along with 'dual surface' type-2 control, named due to utilising both the lower and upper values produced from standard interval type-2 systems. We tune a type-1 controller, then derive the membership functions and footprints of uncertainty from the type-1 system and evaluate them using a simulated autonomous sailing problem with varying amounts of environmental uncertainty. We show that while these more sophisticated controllers can produce better performance than the type-1 controller, this is not guaranteed and that selection of Footprint of Uncertainty (FOU) size has a large effect on this relative performance. version:1
arxiv-1307-0995 | An Efficient Model Selection for Gaussian Mixture Model in a Bayesian Framework | http://arxiv.org/abs/1307.0995 | id:1307.0995 author:Ji Won Yoon category:cs.LG stat.ML  published:2013-07-03 summary:In order to cluster or partition data, we often use Expectation-and-Maximization (EM) or Variational approximation with a Gaussian Mixture Model (GMM), which is a parametric probability density function represented as a weighted sum of $\hat{K}$ Gaussian component densities. However, model selection to find underlying $\hat{K}$ is one of the key concerns in GMM clustering, since we can obtain the desired clusters only when $\hat{K}$ is known. In this paper, we propose a new model selection algorithm to explore $\hat{K}$ in a Bayesian framework. The proposed algorithm builds the density of the model order which any information criterions such as AIC and BIC basically fail to reconstruct. In addition, this algorithm reconstructs the density quickly as compared to the time-consuming Monte Carlo simulation. version:1
arxiv-1307-0937 | Extending UML for Conceptual Modeling of Annotation of Medical Images | http://arxiv.org/abs/1307.0937 | id:1307.0937 author:Mouhamed Gaith Ayadi, Riadh Bouslimi, Jalel Akaichi category:cs.CV  published:2013-07-03 summary:Imaging has occupied a huge role in the management of patients, whether hospitalized or not. Depending on the patients clinical problem, a variety of imaging modalities were available for use. This gave birth of the annotation of medical image process. The annotation is intended to image analysis and solve the problem of semantic gap. The reason for image annotation is due to increase in acquisition of images. Physicians and radiologists feel better while using annotation techniques for faster remedy in surgery and medicine due to the following reasons: giving details to the patients, searching the present and past records from the larger databases, and giving solutions to them in a faster and more accurate way. However, classical conceptual modeling does not incorporate the specificity of medical domain specially the annotation of medical image. The design phase is the most important activity in the successful building of annotation process. For this reason, we focus in this paper on presenting the conceptual modeling of the annotation of medical image by defining a new profile using the StarUML extensibility mechanism. version:1
arxiv-1307-0915 | Separation of cardiac and respiratory components from the electrical bio-impedance signal using PCA and fast ICA | http://arxiv.org/abs/1307.0915 | id:1307.0915 author:Yar M. Mughal, A. Krivoshei, P. Annus category:stat.AP physics.ins-det stat.ML  published:2013-07-03 summary:This paper is an attempt to separate cardiac and respiratory signals from an electrical bio-impedance (EBI) dataset. For this two well-known algorithms, namely Principal Component Analysis (PCA) and Independent Component Analysis (ICA), were used to accomplish the task. The ability of the PCA and the ICA methods first reduces the dimension and attempt to separate the useful components of the EBI, the cardiac and respiratory ones accordingly. It was investigated with an assumption, that no motion artefacts are present. To carry out this procedure the two channel complex EBI measurements were provided using classical Kelvin type four electrode configurations for the each complex channel. Thus four real signals were used as inputs for the PCA and fast ICA. The results showed, that neither PCA nor ICA nor combination of them can not accurately separate the components at least are used only two complex (four real valued) input components. version:1
arxiv-1307-0846 | Semi-supervised Ranking Pursuit | http://arxiv.org/abs/1307.0846 | id:1307.0846 author:Evgeni Tsivtsivadze, Tom Heskes category:stat.ML cs.IR cs.LG  published:2013-07-02 summary:We propose a novel sparse preference learning/ranking algorithm. Our algorithm approximates the true utility function by a weighted sum of basis functions using the squared loss on pairs of data points, and is a generalization of the kernel matching pursuit method. It can operate both in a supervised and a semi-supervised setting and allows efficient search for multiple, near-optimal solutions. Furthermore, we describe the extension of the algorithm suitable for combined ranking and regression tasks. In our experiments we demonstrate that the proposed algorithm outperforms several state-of-the-art learning methods when taking into account unlabeled data and performs comparably in a supervised learning scenario, while providing sparser solutions. version:1
arxiv-1307-0841 | Comparing various regression methods on ensemble strategies in differential evolution | http://arxiv.org/abs/1307.0841 | id:1307.0841 author:Iztok Fister Jr., Iztok Fister, Janez Brest category:cs.NE  published:2013-07-02 summary:Differential evolution possesses a multitude of various strategies for generating new trial solutions. Unfortunately, the best strategy is not known in advance. Moreover, this strategy usually depends on the problem to be solved. This paper suggests using various regression methods (like random forest, extremely randomized trees, gradient boosting, decision trees, and a generalized linear model) on ensemble strategies in differential evolution algorithm by predicting the best differential evolution strategy during the run. Comparing the preliminary results of this algorithm by optimizing a suite of five well-known functions from literature, it was shown that using the random forest regression method substantially outperformed the results of the other regression methods. version:1
arxiv-1301-7189 | Approximate Counting of Graphical Models Via MCMC Revisited | http://arxiv.org/abs/1301.7189 | id:1301.7189 author:Jose M. PeÃ±a category:stat.ML cs.AI  published:2013-01-30 summary:In Pe\~na (2007), MCMC sampling is applied to approximately calculate the ratio of essential graphs (EGs) to directed acyclic graphs (DAGs) for up to 20 nodes. In the present paper, we extend that work from 20 to 31 nodes. We also extend that work by computing the approximate ratio of connected EGs to connected DAGs, of connected EGs to EGs, and of connected DAGs to DAGs. Furthermore, we prove that the latter ratio is asymptotically 1. We also discuss the implications of these results for learning DAGs from data. version:2
arxiv-1307-0781 | Distributed Online Big Data Classification Using Context Information | http://arxiv.org/abs/1307.0781 | id:1307.0781 author:Cem Tekin, Mihaela van der Schaar category:cs.LG stat.ML  published:2013-07-02 summary:Distributed, online data mining systems have emerged as a result of applications requiring analysis of large amounts of correlated and high-dimensional data produced by multiple distributed data sources. We propose a distributed online data classification framework where data is gathered by distributed data sources and processed by a heterogeneous set of distributed learners which learn online, at run-time, how to classify the different data streams either by using their locally available classification functions or by helping each other by classifying each other's data. Importantly, since the data is gathered at different locations, sending the data to another learner to process incurs additional costs such as delays, and hence this will be only beneficial if the benefits obtained from a better classification will exceed the costs. We model the problem of joint classification by the distributed and heterogeneous learners from multiple data sources as a distributed contextual bandit problem where each data is characterized by a specific context. We develop a distributed online learning algorithm for which we can prove sublinear regret. Compared to prior work in distributed online data mining, our work is the first to provide analytic regret results characterizing the performance of the proposed algorithm. version:1
arxiv-1307-0776 | Regularized Spherical Polar Fourier Diffusion MRI with Optimal Dictionary Learning | http://arxiv.org/abs/1307.0776 | id:1307.0776 author:Jian Cheng, Tianzi Jiang, Rachid Deriche, Dinggang Shen, Pew-Thian Yap category:cs.CV  published:2013-07-02 summary:Compressed Sensing (CS) takes advantage of signal sparsity or compressibility and allows superb signal reconstruction from relatively few measurements. Based on CS theory, a suitable dictionary for sparse representation of the signal is required. In diffusion MRI (dMRI), CS methods were proposed to reconstruct diffusion-weighted signal and the Ensemble Average Propagator (EAP), and there are two kinds of Dictionary Learning (DL) methods: 1) Discrete Representation DL (DR-DL), and 2) Continuous Representation DL (CR-DL). DR-DL is susceptible to numerical inaccuracy owing to interpolation and regridding errors in a discretized q-space. In this paper, we propose a novel CR-DL approach, called Dictionary Learning - Spherical Polar Fourier Imaging (DL-SPFI) for effective compressed-sensing reconstruction of the q-space diffusion-weighted signal and the EAP. In DL-SPFI, an dictionary that sparsifies the signal is learned from the space of continuous Gaussian diffusion signals. The learned dictionary is then adaptively applied to different voxels using a weighted LASSO framework for robust signal reconstruction. The adaptive dictionary is proved to be optimal. Compared with the start-of-the-art CR-DL and DR-DL methods proposed by Merlet et al. and Bilgic et al., espectively, our work offers the following advantages. First, the learned dictionary is proved to be optimal for Gaussian diffusion signals. Second, to our knowledge, this is the first work to learn a voxel-adaptive dictionary. The importance of the adaptive dictionary in EAP reconstruction will be demonstrated theoretically and empirically. Third, optimization in DL-SPFI is only performed in a small subspace resided by the SPF coefficients, as opposed to the q-space approach utilized by Merlet et al. The experiment results demonstrate the advantages of DL-SPFI over the original SPF basis and Bilgic et al.'s method. version:1
arxiv-1307-1303 | Submodularity of a Set Label Disagreement Function | http://arxiv.org/abs/1307.1303 | id:1307.1303 author:Toufiq Parag category:cs.CV  published:2013-07-02 summary:A set label disagreement function is defined over the number of variables that deviates from the dominant label. The dominant label is the value assumed by the largest number of variables within a set of binary variables. The submodularity of a certain family of set label disagreement function is discussed in this manuscript. Such disagreement function could be utilized as a cost function in combinatorial optimization approaches for problems defined over hypergraphs. version:1
arxiv-1306-0393 | Learning from networked examples in a k-partite graph | http://arxiv.org/abs/1306.0393 | id:1306.0393 author:Yuyi Wang, Jan Ramon, Zheng-Chu Guo category:cs.LG stat.ML  published:2013-06-03 summary:Many machine learning algorithms are based on the assumption that training examples are drawn independently. However, this assumption does not hold anymore when learning from a networked sample where two or more training examples may share common features. We propose an efficient weighting method for learning from networked examples and show the sample error bound which is better than previous work. version:2
arxiv-1307-0643 | Discovering the Markov network structure | http://arxiv.org/abs/1307.0643 | id:1307.0643 author:Edith KovÃ¡cs, TamÃ¡s SzÃ¡ntai category:cs.IT cs.LG math.IT  published:2013-07-02 summary:In this paper a new proof is given for the supermodularity of information content. Using the decomposability of the information content an algorithm is given for discovering the Markov network graph structure endowed by the pairwise Markov property of a given probability distribution. A discrete probability distribution is given for which the equivalence of Hammersley-Clifford theorem is fulfilled although some of the possible vector realizations are taken on with zero probability. Our algorithm for discovering the pairwise Markov network is illustrated on this example, too. version:1
arxiv-1307-0596 | Improving Pointwise Mutual Information (PMI) by Incorporating Significant Co-occurrence | http://arxiv.org/abs/1307.0596 | id:1307.0596 author:Om P. Damani category:cs.CL  published:2013-07-02 summary:We design a new co-occurrence based word association measure by incorporating the concept of significant cooccurrence in the popular word association measure Pointwise Mutual Information (PMI). By extensive experiments with a large number of publicly available datasets we show that the newly introduced measure performs better than other co-occurrence based measures and despite being resource-light, compares well with the best known resource-heavy distributional similarity and knowledge based word association measures. We investigate the source of this performance improvement and find that of the two types of significant co-occurrence - corpus-level and document-level, the concept of corpus level significance combined with the use of document counts in place of word counts is responsible for all the performance gains observed. The concept of document level significance is not helpful for PMI adaptation. version:1
arxiv-1307-0589 | The Orchive : Data mining a massive bioacoustic archive | http://arxiv.org/abs/1307.0589 | id:1307.0589 author:Steven Ness, Helena Symonds, Paul Spong, George Tzanetakis category:cs.LG cs.DB cs.SD  published:2013-07-02 summary:The Orchive is a large collection of over 20,000 hours of audio recordings from the OrcaLab research facility located off the northern tip of Vancouver Island. It contains recorded orca vocalizations from the 1980 to the present time and is one of the largest resources of bioacoustic data in the world. We have developed a web-based interface that allows researchers to listen to these recordings, view waveform and spectral representations of the audio, label clips with annotations, and view the results of machine learning classifiers based on automatic audio features extraction. In this paper we describe such classifiers that discriminate between background noise, orca calls, and the voice notes that are present in most of the tapes. Furthermore we show classification results for individual calls based on a previously existing orca call catalog. We have also experimentally investigated the scalability of classifiers over the entire Orchive. version:1
arxiv-1307-0578 | A non-parametric conditional factor regression model for high-dimensional input and response | http://arxiv.org/abs/1307.0578 | id:1307.0578 author:Ava Bargi, Richard Yi Da Xu, Massimo Piccardi category:stat.ML cs.LG  published:2013-07-02 summary:In this paper, we propose a non-parametric conditional factor regression (NCFR)model for domains with high-dimensional input and response. NCFR enhances linear regression in two ways: a) introducing low-dimensional latent factors leading to dimensionality reduction and b) integrating an Indian Buffet Process as a prior for the latent factors to derive unlimited sparse dimensions. Experimental results comparing NCRF to several alternatives give evidence to remarkable prediction performance. version:1
arxiv-1306-6802 | Evaluation Measures for Hierarchical Classification: a unified view and novel approaches | http://arxiv.org/abs/1306.6802 | id:1306.6802 author:Aris Kosmopoulos, Ioannis Partalas, Eric Gaussier, Georgios Paliouras, Ion Androutsopoulos category:cs.AI cs.LG  published:2013-06-28 summary:Hierarchical classification addresses the problem of classifying items into a hierarchy of classes. An important issue in hierarchical classification is the evaluation of different classification algorithms, which is complicated by the hierarchical relations among the classes. Several evaluation measures have been proposed for hierarchical classification using the hierarchy in different ways. This paper studies the problem of evaluation in hierarchical classification by analyzing and abstracting the key components of the existing performance measures. It also proposes two alternative generic views of hierarchical evaluation and introduces two corresponding novel measures. The proposed measures, along with the state-of-the art ones, are empirically tested on three large datasets from the domain of text classification. The empirical results illustrate the undesirable behavior of existing approaches and how the proposed methods overcome most of these methods across a range of cases. version:2
arxiv-1307-0414 | Challenges in Representation Learning: A report on three machine learning contests | http://arxiv.org/abs/1307.0414 | id:1307.0414 author:Ian J. Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville, Mehdi Mirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler, Dong-Hyun Lee, Yingbo Zhou, Chetan Ramaiah, Fangxiang Feng, Ruifan Li, Xiaojie Wang, Dimitris Athanasakis, John Shawe-Taylor, Maxim Milakov, John Park, Radu Ionescu, Marius Popescu, Cristian Grozea, James Bergstra, Jingjing Xie, Lukasz Romaszko, Bing Xu, Zhang Chuang, Yoshua Bengio category:stat.ML cs.LG  published:2013-07-01 summary:The ICML 2013 Workshop on Challenges in Representation Learning focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions. version:1
arxiv-1307-0373 | Gaussian Process Conditional Copulas with Applications to Financial Time Series | http://arxiv.org/abs/1307.0373 | id:1307.0373 author:JosÃ© Miguel HernÃ¡ndez-Lobato, James Robert Lloyd, Daniel HernÃ¡ndez-Lobato category:stat.ML  published:2013-07-01 summary:The estimation of dependencies between multiple variables is a central problem in the analysis of financial time series. A common approach is to express these dependencies in terms of a copula function. Typically the copula function is assumed to be constant but this may be inaccurate when there are covariates that could have a large influence on the dependence structure of the data. To account for this, a Bayesian framework for the estimation of conditional copulas is proposed. In this framework the parameters of a copula are non-linearly related to some arbitrary conditioning variables. We evaluate the ability of our method to predict time-varying dependencies on several equities and currencies and observe consistent performance gains compared to static copula models and other time-varying copula methods. version:1
arxiv-1305-0763 | Quantifying the Impact of Parameter Tuning on Nature-Inspired Algorithms | http://arxiv.org/abs/1305.0763 | id:1305.0763 author:Matthew Crossley, Andy Nisbet, Martyn Amos category:cs.NE  published:2013-05-03 summary:The problem of parameterization is often central to the effective deployment of nature-inspired algorithms. However, finding the optimal set of parameter values for a combination of problem instance and solution method is highly challenging, and few concrete guidelines exist on how and when such tuning may be performed. Previous work tends to either focus on a specific algorithm or use benchmark problems, and both of these restrictions limit the applicability of any findings. Here, we examine a number of different algorithms, and study them in a "problem agnostic" fashion (i.e., one that is not tied to specific instances) by considering their performance on fitness landscapes with varying characteristics. Using this approach, we make a number of observations on which algorithms may (or may not) benefit from tuning, and in which specific circumstances. version:2
arxiv-1307-0323 | Dimensionality Detection and Integration of Multiple Data Sources via the GP-LVM | http://arxiv.org/abs/1307.0323 | id:1307.0323 author:James Barrett, Anthony C. C. Coolen category:stat.ML  published:2013-07-01 summary:The Gaussian Process Latent Variable Model (GP-LVM) is a non-linear probabilistic method of embedding a high dimensional dataset in terms low dimensional `latent' variables. In this paper we illustrate that maximum a posteriori (MAP) estimation of the latent variables and hyperparameters can be used for model selection and hence we can determine the optimal number or latent variables and the most appropriate model. This is an alternative to the variational approaches developed recently and may be useful when we want to use a non-Gaussian prior or kernel functions that don't have automatic relevance determination (ARD) parameters. Using a second order expansion of the latent variable posterior we can marginalise the latent variables and obtain an estimate for the hyperparameter posterior. Secondly, we use the GP-LVM to integrate multiple data sources by simultaneously embedding them in terms of common latent variables. We present results from synthetic data to illustrate the successful detection and retrieval of low dimensional structure from high dimensional data. We demonstrate that the integration of multiple data sources leads to more robust performance. Finally, we show that when the data are used for binary classification tasks we can attain a significant gain in prediction accuracy when the low dimensional representation is used. version:1
arxiv-1307-0317 | Algorithms of the LDA model [REPORT] | http://arxiv.org/abs/1307.0317 | id:1307.0317 author:Jaka Å peh, Andrej MuhiÄ, Jan Rupnik category:cs.LG cs.IR stat.ML  published:2013-07-01 summary:We review three algorithms for Latent Dirichlet Allocation (LDA). Two of them are variational inference algorithms: Variational Bayesian inference and Online Variational Bayesian inference and one is Markov Chain Monte Carlo (MCMC) algorithm -- Collapsed Gibbs sampling. We compare their time complexity and performance. We find that online variational Bayesian inference is the fastest algorithm and still returns reasonably good results. version:1
arxiv-1304-6233 | A Counterexample for the Validity of Using Nuclear Norm as a Convex Surrogate of Rank | http://arxiv.org/abs/1304.6233 | id:1304.6233 author:Hongyang Zhang, Zhouchen Lin, Chao Zhang category:stat.ML math.OC  published:2013-04-23 summary:Rank minimization has attracted a lot of attention due to its robustness in data recovery. To overcome the computational difficulty, rank is often replaced with nuclear norm. For several rank minimization problems, such a replacement has been theoretically proven to be valid, i.e., the solution to nuclear norm minimization problem is also the solution to rank minimization problem. Although it is easy to believe that such a replacement may not always be valid, no concrete example has ever been found. We argue that such a validity checking cannot be done by numerical computation and show, by analyzing the noiseless latent low rank representation (LatLRR) model, that even for very simple rank minimization problems the validity may still break down. As a by-product, we find that the solution to the nuclear norm minimization formulation of LatLRR is non-unique. Hence the results of LatLRR reported in the literature may be questionable. version:2
arxiv-1307-0277 | Multilevel Threshold Based Gray Scale Image Segmentation using Cuckoo Search | http://arxiv.org/abs/1307.0277 | id:1307.0277 author:Sourav Samantaa, Nilanjan Dey, Poulami Das, Suvojit Acharjee, Sheli Sinha Chaudhuri category:cs.CV  published:2013-07-01 summary:Image Segmentation is a technique of partitioning the original image into some distinct classes. Many possible solutions may be available for segmenting an image into a certain number of classes, each one having different quality of segmentation. In our proposed method, multilevel thresholding technique has been used for image segmentation. A new approach of Cuckoo Search (CS) is used for selection of optimal threshold value. In other words, the algorithm is used to achieve the best solution from the initial random threshold values or solutions and to evaluate the quality of a solution correlation function is used. Finally, MSE and PSNR are measured to understand the segmentation quality. version:1
arxiv-1307-0261 | WebSets: Extracting Sets of Entities from the Web Using Unsupervised Information Extraction | http://arxiv.org/abs/1307.0261 | id:1307.0261 author:Bhavana Dalvi, William W. Cohen, Jamie Callan category:cs.LG cs.CL cs.IR  published:2013-07-01 summary:We describe a open-domain information extraction method for extracting concept-instance pairs from an HTML corpus. Most earlier approaches to this problem rely on combining clusters of distributionally similar terms and concept-instance pairs obtained with Hearst patterns. In contrast, our method relies on a novel approach for clustering terms found in HTML tables, and then assigning concept names to these clusters using Hearst patterns. The method can be efficiently applied to a large corpus, and experimental results on several datasets show that our method can accurately extract large numbers of concept-instance pairs. version:1
arxiv-1307-0253 | Exploratory Learning | http://arxiv.org/abs/1307.0253 | id:1307.0253 author:Bhavana Dalvi, William W. Cohen, Jamie Callan category:cs.LG  published:2013-07-01 summary:In multiclass semi-supervised learning (SSL), it is sometimes the case that the number of classes present in the data is not known, and hence no labeled examples are provided for some classes. In this paper we present variants of well-known semi-supervised multiclass learning methods that are robust when the data contains an unknown number of classes. In particular, we present an "exploratory" extension of expectation-maximization (EM) that explores different numbers of classes while learning. "Exploratory" SSL greatly improves performance on three datasets in terms of F1 on the classes with seed examples i.e., the classes which are expected to be in the data. Our Exploratory EM algorithm also outperforms a SSL method based non-parametric Bayesian clustering. version:1
arxiv-1307-0252 | Semi-supervised clustering methods | http://arxiv.org/abs/1307.0252 | id:1307.0252 author:Eric Bair category:stat.ME cs.LG stat.ML  published:2013-07-01 summary:Cluster analysis methods seek to partition a data set into homogeneous subgroups. It is useful in a wide variety of applications, including document processing and modern genetics. Conventional clustering methods are unsupervised, meaning that there is no outcome variable nor is anything known about the relationship between the observations in the data set. In many situations, however, information about the clusters is available in addition to the values of the features. For example, the cluster labels of some observations may be known, or certain observations may be known to belong to the same cluster. In other cases, one may wish to identify clusters that are associated with a particular outcome variable. This review describes several clustering algorithms (known as "semi-supervised clustering" methods) that can be applied in these situations. The majority of these methods are modifications of the popular k-means clustering method, and several of them will be described in detail. A brief description of some other semi-supervised clustering algorithms is also provided. version:1
arxiv-1307-0164 | Sparse Principal Component Analysis for High Dimensional Vector Autoregressive Models | http://arxiv.org/abs/1307.0164 | id:1307.0164 author:Zhaoran Wang, Fang Han, Han Liu category:stat.ML  published:2013-06-30 summary:We study sparse principal component analysis for high dimensional vector autoregressive time series under a doubly asymptotic framework, which allows the dimension $d$ to scale with the series length $T$. We treat the transition matrix of time series as a nuisance parameter and directly apply sparse principal component analysis on multivariate time series as if the data are independent. We provide explicit non-asymptotic rates of convergence for leading eigenvector estimation and extend this result to principal subspace estimation. Our analysis illustrates that the spectral norm of the transition matrix plays an essential role in determining the final rates. We also characterize sufficient conditions under which sparse principal component analysis attains the optimal parametric rate. Our theoretical results are backed up by thorough numerical studies. version:1
arxiv-1307-0129 | Hyperspectral Data Unmixing Using GNMF Method and Sparseness Constraint | http://arxiv.org/abs/1307.0129 | id:1307.0129 author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV  published:2013-06-29 summary:Hyperspectral images contain mixed pixels due to low spatial resolution of hyperspectral sensors. Mixed pixels are pixels containing more than one distinct material called endmembers. The presence percentages of endmembers in mixed pixels are called abundance fractions. Spectral unmixing problem refers to decomposing these pixels into a set of endmembers and abundance fractions. Due to nonnegativity constraint on abundance fractions, nonnegative matrix factorization methods (NMF) have been widely used for solving spectral unmixing problem. In this paper we have used graph regularized (GNMF) method with sparseness constraint to unmix hyperspectral data. This method applied on simulated data using AVIRIS Indian Pines dataset and USGS library and results are quantified based on AAD and SAD measures. Results in comparison with other methods show that the proposed method can unmix data more effectively. version:1
arxiv-1307-0127 | Concentration and Confidence for Discrete Bayesian Sequence Predictors | http://arxiv.org/abs/1307.0127 | id:1307.0127 author:Tor Lattimore, Marcus Hutter, Peter Sunehag category:cs.LG stat.ML  published:2013-06-29 summary:Bayesian sequence prediction is a simple technique for predicting future symbols sampled from an unknown measure on infinite sequences over a countable alphabet. While strong bounds on the expected cumulative error are known, there are only limited results on the distribution of this error. We prove tight high-probability bounds on the cumulative error, which is measured in terms of the Kullback-Leibler (KL) divergence. We also consider the problem of constructing upper confidence bounds on the KL and Hellinger errors similar to those constructed from Hoeffding-like bounds in the i.i.d. case. The new results are applied to show that Bayesian sequence prediction can be used in the Knows What It Knows (KWIK) framework with bounds that match the state-of-the-art. version:1
arxiv-1307-0087 | Semantics and pragmatics in actual software applications and in web search engines: exploring innovations | http://arxiv.org/abs/1307.0087 | id:1307.0087 author:Fabrizio M. A. Lolli category:cs.IR cs.CL cs.HC  published:2013-06-29 summary:While new ways to use the Semantic Web are developed every week, which allow the user to find information on web more accurately - for example in search engines - some sophisticated pragmatic tools are becoming more important - for example in web interfaces known as Social Intelligence, or in the most famous Siri by Apple. The work aims to analyze whether and where we can identify the boundary between semantics and pragmatics in the software used by analyzed systems. examining how the linguistic disciplines are fundamental in their progress. Is it possible to assume that the tools of social intelligence have a pragmatic approach to the questions of the user, or it is just a use of a very rich vocabulary, with the use of semantic tools? version:1
arxiv-1307-0060 | Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs | http://arxiv.org/abs/1307.0060 | id:1307.0060 author:Vikash K. Mansinghka, Tejas D. Kulkarni, Yura N. Perov, Joshua B. Tenenbaum category:cs.AI cs.CV stat.ML  published:2013-06-29 summary:The idea of computer vision as the Bayesian inverse problem to computer graphics has a long history and an appealing elegance, but it has proved difficult to directly implement. Instead, most vision tasks are approached via complex bottom-up processing pipelines. Here we show that it is possible to write short, simple probabilistic graphics programs that define flexible generative models and to automatically invert them to interpret real-world images. Generative probabilistic graphics programs consist of a stochastic scene generator, a renderer based on graphics software, a stochastic likelihood model linking the renderer's output and the data, and latent variables that adjust the fidelity of the renderer and the tolerance of the likelihood model. Representations and algorithms from computer graphics, originally designed to produce high-quality images, are instead used as the deterministic backbone for highly approximate and stochastic generative models. This formulation combines probabilistic programming, computer graphics, and approximate Bayesian computation, and depends only on general-purpose, automatic inference techniques. We describe two applications: reading sequences of degraded and adversarially obscured alphanumeric characters, and inferring 3D road models from vehicle-mounted camera images. Each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code, and supports accurate, approximately Bayesian inferences about ambiguous real-world images. version:1
arxiv-1306-5390 | P-HGRMS: A Parallel Hypergraph Based Root Mean Square Algorithm for Image Denoising | http://arxiv.org/abs/1306.5390 | id:1306.5390 author:Tejaswi Agarwal, Saurabh Jha, B. Rajesh Kanna category:cs.DC cs.CV I.3  published:2013-06-23 summary:This paper presents a parallel Salt and Pepper (SP) noise removal algorithm in a grey level digital image based on the Hypergraph Based Root Mean Square (HGRMS) approach. HGRMS is generic algorithm for identifying noisy pixels in any digital image using a two level hierarchical serial approach. However, for SP noise removal, we reduce this algorithm to a parallel model by introducing a cardinality matrix and an iteration factor, k, which helps us reduce the dependencies in the existing approach. We also observe that the performance of the serial implementation is better on smaller images, but once the threshold is achieved in terms of image resolution, its computational complexity increases drastically. We test P-HGRMS using standard images from the Berkeley Segmentation dataset on NVIDIAs Compute Unified Device Architecture (CUDA) for noise identification and attenuation. We also compare the noise removal efficiency of the proposed algorithm using Peak Signal to Noise Ratio (PSNR) to the existing approach. P-HGRMS maintains the noise removal efficiency and outperforms its sequential counterpart by 6 to 18 times (6x - 18x) in computational efficiency. version:2
arxiv-1307-0036 | Increasing Compression Ratio in PNG Images by k-Modulus Method for Image Transformation | http://arxiv.org/abs/1307.0036 | id:1307.0036 author:Firas A. Jassim category:cs.CV cs.MM  published:2013-06-28 summary:Image compression is an important filed in image processing. The science welcomes any tinny contribution that may increase the compression ratio by whichever insignificant percentage. Therefore, the essential contribution in this paper is to increase the compression ratio for the well known Portable Network Graphics (PNG) image file format. The contribution starts with converting the original PNG image into k-Modulus Method (k-MM). Practically, taking k equals to ten, and then the pixels in the constructed image will be integers divisible by ten. Since PNG uses Lempel-Ziv compression algorithm, then the ability to reduce file size will increase according to the repetition in pixels in each k-by-k window according to the transformation done by k-MM. Experimental results show that the proposed technique (k-PNG) produces high compression ratio with smaller file size in comparison to the original PNG file. version:1
arxiv-1307-0032 | Memory Limited, Streaming PCA | http://arxiv.org/abs/1307.0032 | id:1307.0032 author:Ioannis Mitliagkas, Constantine Caramanis, Prateek Jain category:stat.ML cs.IT cs.LG math.IT  published:2013-06-28 summary:We consider streaming, one-pass principal component analysis (PCA), in the high-dimensional regime, with limited memory. Here, $p$-dimensional samples are presented sequentially, and the goal is to produce the $k$-dimensional subspace that best approximates these points. Standard algorithms require $O(p^2)$ memory; meanwhile no algorithm can do better than $O(kp)$ memory, since this is what the output itself requires. Memory (or storage) complexity is most meaningful when understood in the context of computational and sample complexity. Sample complexity for high-dimensional PCA is typically studied in the setting of the {\em spiked covariance model}, where $p$-dimensional points are generated from a population covariance equal to the identity (white noise) plus a low-dimensional perturbation (the spike) which is the signal to be recovered. It is now well-understood that the spike can be recovered when the number of samples, $n$, scales proportionally with the dimension, $p$. Yet, all algorithms that provably achieve this, have memory complexity $O(p^2)$. Meanwhile, algorithms with memory-complexity $O(kp)$ do not have provable bounds on sample complexity comparable to $p$. We present an algorithm that achieves both: it uses $O(kp)$ memory (meaning storage of any kind) and is able to compute the $k$-dimensional spike with $O(p \log p)$ sample-complexity -- the first algorithm of its kind. While our theoretical analysis focuses on the spiked covariance model, our simulations show that our algorithm is successful on much more general models for the data. version:1
arxiv-1305-3250 | Bioacoustical Periodic Pulse Train Signal Detection and Classification using Spectrogram Intensity Binarization and Energy Projection | http://arxiv.org/abs/1305.3250 | id:1305.3250 author:Marian Popescu, Peter J. Dugan, Mohammad Pourhomayoun, Denise Risch, Harold W. Lewis III, Christopher W. Clark category:cs.CV  published:2013-05-14 summary:The following work outlines an approach for automatic detection and recognition of periodic pulse train signals using a multi-stage process based on spectrogram edge detection, energy projection and classification. The method has been implemented to automatically detect and recognize pulse train songs of minke whales. While the long term goal of this work is to properly identify and detect minke songs from large multi-year datasets, this effort was developed using sounds off the coast of Massachusetts, in the Stellwagen Bank National Marine Sanctuary. The detection methodology is presented and evaluated on 232 continuous hours of acoustic recordings and a qualitative analysis of machine learning classifiers and their performance is described. The trained automatic detection and classification system is applied to 120 continuous hours, comprised of various challenges such as broadband and narrowband noises, low SNR, and other pulse train signatures. This automatic system achieves a TPR of 63% for FPR of 0.6% (or 0.87 FP/h), at a Precision (PPV) of 84% and an F1 score of 71%. version:3
arxiv-1306-6842 | New Mathematical and Algorithmic Schemes for Pattern Classification with Application to the Identification of Writers of Important Ancient Documents | http://arxiv.org/abs/1306.6842 | id:1306.6842 author:Dimitris Arabadjis, Fotios Giannopoulos, Constantin Papaodysseus, Solomon Zannos, Panayiotis Rousopoulos, Michail Panagopoulos, Christopher Blackwell category:cs.CV  published:2013-06-28 summary:In this paper, a novel approach is introduced for classifying curves into proper families, according to their similarity. First, a mathematical quantity we call plane curvature is introduced and a number of propositions are stated and proved. Proper similarity measures of two curves are introduced and a subsequent statistical analysis is applied. First, the efficiency of the curve fitting process has been tested on 2 shapes datasets of reference. Next, the methodology has been applied to the very important problem of classifying 23 Byzantine codices and 46 Ancient inscriptions to their writers, thus achieving correct dating of their content. The inscriptions have been attributed to ten individual hands and the Byzantine codices to four writers. version:1
arxiv-1303-6977 | ABC Reinforcement Learning | http://arxiv.org/abs/1303.6977 | id:1303.6977 author:Christos Dimitrakakis, Nikolaos Tziortziotis category:stat.ML cs.LG  published:2013-03-27 summary:This paper introduces a simple, general framework for likelihood-free Bayesian reinforcement learning, through Approximate Bayesian Computation (ABC). The main advantage is that we only require a prior distribution on a class of simulators (generative models). This is useful in domains where an analytical probabilistic model of the underlying process is too complex to formulate, but where detailed simulation models are available. ABC-RL allows the use of any Bayesian reinforcement learning technique, even in this case. In addition, it can be seen as an extension of rollout algorithms to the case where we do not know what the correct model to draw rollouts from is. We experimentally demonstrate the potential of this approach in a comparison with LSPI. Finally, we introduce a theorem showing that ABC is a sound methodology in principle, even when non-sufficient statistics are used. version:4
arxiv-1306-3692 | An open diachronic corpus of historical Spanish: annotation criteria and automatic modernisation of spelling | http://arxiv.org/abs/1306.3692 | id:1306.3692 author:Felipe SÃ¡nchez-MartÃ­nez, Isabel MartÃ­nez-Sempere, Xavier Ivars-Ribes, Rafael C. Carrasco category:cs.CL cs.DL  published:2013-06-16 summary:The IMPACT-es diachronic corpus of historical Spanish compiles over one hundred books --containing approximately 8 million words-- in addition to a complementary lexicon which links more than 10 thousand lemmas with attestations of the different variants found in the documents. This textual corpus and the accompanying lexicon have been released under an open license (Creative Commons by-nc-sa) in order to permit their intensive exploitation in linguistic research. Approximately 7% of the words in the corpus (a selection aimed at enhancing the coverage of the most frequent word forms) have been annotated with their lemma, part of speech, and modern equivalent. This paper describes the annotation criteria followed and the standards, based on the Text Encoding Initiative recommendations, used to the represent the texts in digital form. As an illustration of the possible synergies between diachronic textual resources and linguistic research, we describe the application of statistical machine translation techniques to infer probabilistic context-sensitive rules for the automatic modernisation of spelling. The automatic modernisation with this type of statistical methods leads to very low character error rates when the output is compared with the supervised modern version of the text. version:2
arxiv-1306-6755 | Arabizi Detection and Conversion to Arabic | http://arxiv.org/abs/1306.6755 | id:1306.6755 author:Kareem Darwish category:cs.CL cs.IR I.2.7  published:2013-06-28 summary:Arabizi is Arabic text that is written using Latin characters. Arabizi is used to present both Modern Standard Arabic (MSA) or Arabic dialects. It is commonly used in informal settings such as social networking sites and is often with mixed with English. In this paper we address the problems of: identifying Arabizi in text and converting it to Arabic characters. We used word and sequence-level features to identify Arabizi that is mixed with English. We achieved an identification accuracy of 98.5%. As for conversion, we used transliteration mining with language modeling to generate equivalent Arabic text. We achieved 88.7% conversion accuracy, with roughly a third of errors being spelling and morphological variants of the forms in ground truth. version:1
arxiv-1306-6737 | Digital Image Tamper Detection Techniques - A Comprehensive Study | http://arxiv.org/abs/1306.6737 | id:1306.6737 author:Minati Mishra, Flt. Lt. Dr. M. C. Adhikary category:cs.CR cs.CV  published:2013-06-28 summary:Photographs are considered to be the most powerful and trustworthy media of expression. For a long time, those were accepted as proves of evidences in varied fields such as journalism, forensic investigations, military intelligence, scientific research and publications, crime detection and legal proceedings, investigation of insurance claims, medical imaging etc. Today, digital images have completely replaced the conventional photographs from every sphere of life but unfortunately, they seldom enjoy the credibility of their conventional counterparts, thanks to the rapid advancements in the field of digital image processing. The increasing availability of low cost and sometimes free of cost image editing software such as Photoshop, Corel Paint Shop, Photoscape, PhotoPlus, GIMP and Pixelmator have made the tampering of digital images even more easier and a common practice. Now it has become quite impossible to say whether a photograph is a genuine camera output or a manipulated version of it just by looking at it. As a result, photographs have almost lost their reliability and place as proves of evidences in all fields. This is why digital image tamper detection has emerged as an important research area to establish the authenticity of digital photographs by separating the tampered lots from the original ones. This paper gives a brief history of image tampering and a state-of-the-art review of the tamper detection techniques. version:1
arxiv-1306-6726 | A Novel Active Contour Model for Texture Segmentation | http://arxiv.org/abs/1306.6726 | id:1306.6726 author:Aditya Tatu, Sumukh Bansal category:cs.CV  published:2013-06-28 summary:Texture is intuitively defined as a repeated arrangement of a basic pattern or object in an image. There is no mathematical definition of a texture though. The human visual system is able to identify and segment different textures in a given image. Automating this task for a computer is far from trivial. There are three major components of any texture segmentation algorithm: (a) The features used to represent a texture, (b) the metric induced on this representation space and (c) the clustering algorithm that runs over these features in order to segment a given image into different textures. In this paper, we propose an active contour based novel unsupervised algorithm for texture segmentation. We use intensity covariance matrices of regions as the defining feature of textures and find regions that have the most inter-region dissimilar covariance matrices using active contours. Since covariance matrices are symmetric positive definite, we use geodesic distance defined on the manifold of symmetric positive definite matrices PD(n) as a measure of dissimlarity between such matrices. We demonstrate performance of our algorithm on both artificial and real texture images. version:1
arxiv-1306-6557 | Optimal Feature Selection in High-Dimensional Discriminant Analysis | http://arxiv.org/abs/1306.6557 | id:1306.6557 author:Mladen Kolar, Han Liu category:stat.ML math.ST stat.TH  published:2013-06-27 summary:We consider the high-dimensional discriminant analysis problem. For this problem, different methods have been proposed and justified by establishing exact convergence rates for the classification risk, as well as the l2 convergence results to the discriminative rule. However, sharp theoretical analysis for the variable selection performance of these procedures have not been established, even though model interpretation is of fundamental importance in scientific data analysis. This paper bridges the gap by providing sharp sufficient conditions for consistent variable selection using the sparse discriminant analysis (Mai et al., 2012). Through careful analysis, we establish rates of convergence that are significantly faster than the best known results and admit an optimal scaling of the sample size n, dimensionality p, and sparsity level s in the high-dimensional setting. Sufficient conditions are complemented by the necessary information theoretic limits on the variable selection problem in the context of high-dimensional discriminant analysis. Exploiting a numerical equivalence result, our method also establish the optimal results for the ROAD estimator (Fan et al., 2012) and the sparse optimal scaling estimator (Clemmensen et al., 2011). Furthermore, we analyze an exhaustive search procedure, whose performance serves as a benchmark, and show that it is variable selection consistent under weaker conditions. Extensive simulations demonstrating the sharpness of the bounds are also provided. version:1
arxiv-1306-6302 | Solving Relational MDPs with Exogenous Events and Additive Rewards | http://arxiv.org/abs/1306.6302 | id:1306.6302 author:S. Joshi, R. Khardon, P. Tadepalli, A. Raghavan, A. Fern category:cs.AI cs.LG  published:2013-06-26 summary:We formalize a simple but natural subclass of service domains for relational planning problems with object-centered, independent exogenous events and additive rewards capturing, for example, problems in inventory control. Focusing on this subclass, we present a new symbolic planning algorithm which is the first algorithm that has explicit performance guarantees for relational MDPs with exogenous events. In particular, under some technical conditions, our planning algorithm provides a monotonic lower bound on the optimal value function. To support this algorithm we present novel evaluation and reduction techniques for generalized first order decision diagrams, a knowledge representation for real-valued functions over relational world states. Our planning algorithm uses a set of focus states, which serves as a training set, to simplify and approximate the symbolic solution, and can thus be seen to perform learning for planning. A preliminary experimental evaluation demonstrates the validity of our approach. version:2
arxiv-1306-6482 | Traffic data reconstruction based on Markov random field modeling | http://arxiv.org/abs/1306.6482 | id:1306.6482 author:Shun Kataoka, Muneki Yasuda, Cyril Furtlehner, Kazuyuki Tanaka category:stat.ML cond-mat.dis-nn cs.LG  published:2013-06-27 summary:We consider the traffic data reconstruction problem. Suppose we have the traffic data of an entire city that are incomplete because some road data are unobserved. The problem is to reconstruct the unobserved parts of the data. In this paper, we propose a new method to reconstruct incomplete traffic data collected from various traffic sensors. Our approach is based on Markov random field modeling of road traffic. The reconstruction is achieved by using mean-field method and a machine learning method. We numerically verify the performance of our method using realistic simulated traffic data for the real road network of Sendai, Japan. version:1
arxiv-1101-4681 | Close the Gaps: A Learning-while-Doing Algorithm for a Class of Single-Product Revenue Management Problems | http://arxiv.org/abs/1101.4681 | id:1101.4681 author:Zizhuo Wang, Shiming Deng, Yinyu Ye category:cs.LG 93E35  published:2011-01-24 summary:We consider a retailer selling a single product with limited on-hand inventory over a finite selling season. Customer demand arrives according to a Poisson process, the rate of which is influenced by a single action taken by the retailer (such as price adjustment, sales commission, advertisement intensity, etc.). The relationship between the action and the demand rate is not known in advance. However, the retailer is able to learn the optimal action "on the fly" as she maximizes her total expected revenue based on the observed demand reactions. Using the pricing problem as an example, we propose a dynamic "learning-while-doing" algorithm that only involves function value estimation to achieve a near-optimal performance. Our algorithm employs a series of shrinking price intervals and iteratively tests prices within that interval using a set of carefully chosen parameters. We prove that the convergence rate of our algorithm is among the fastest of all possible algorithms in terms of asymptotic "regret" (the relative loss comparing to the full information optimal solution). Our result closes the performance gaps between parametric and non-parametric learning and between a post-price mechanism and a customer-bidding mechanism. Important managerial insight from this research is that the values of information on both the parametric form of the demand function as well as each customer's exact reservation price are less important than prior literature suggests. Our results also suggest that firms would be better off to perform dynamic learning and action concurrently rather than sequentially. version:6
arxiv-1305-4955 | A Data Mining Approach to Solve the Goal Scoring Problem | http://arxiv.org/abs/1305.4955 | id:1305.4955 author:Renato Oliveira, Paulo Adeodato, Arthur Carvalho, Icamaan Viegas, Christian Diego, Tsang Ing-Ren category:cs.AI cs.LG  published:2013-05-21 summary:In soccer, scoring goals is a fundamental objective which depends on many conditions and constraints. Considering the RoboCup soccer 2D-simulator, this paper presents a data mining-based decision system to identify the best time and direction to kick the ball towards the goal to maximize the overall chances of scoring during a simulated soccer match. Following the CRISP-DM methodology, data for modeling were extracted from matches of major international tournaments (10691 kicks), knowledge about soccer was embedded via transformation of variables and a Multilayer Perceptron was used to estimate the scoring chance. Experimental performance assessment to compare this approach against previous LDA-based approach was conducted from 100 matches. Several statistical metrics were used to analyze the performance of the system and the results showed an increase of 7.7% in the number of kicks, producing an overall increase of 78% in the number of goals scored. version:2
arxiv-1201-4895 | Compressive Acquisition of Dynamic Scenes | http://arxiv.org/abs/1201.4895 | id:1201.4895 author:Aswin C Sankaranarayanan, Pavan K Turaga, Rama Chellappa, Richard G Baraniuk category:cs.CV  published:2012-01-23 summary:Compressive sensing (CS) is a new approach for the acquisition and recovery of sparse signals and images that enables sampling rates significantly below the classical Nyquist rate. Despite significant progress in the theory and methods of CS, little headway has been made in compressive video acquisition and recovery. Video CS is complicated by the ephemeral nature of dynamic events, which makes direct extensions of standard CS imaging architectures and signal models difficult. In this paper, we develop a new framework for video CS for dynamic textured scenes that models the evolution of the scene as a linear dynamical system (LDS). This reduces the video recovery problem to first estimating the model parameters of the LDS from compressive measurements, and then reconstructing the image frames. We exploit the low-dimensional dynamic parameters (the state sequence) and high-dimensional static parameters (the observation matrix) of the LDS to devise a novel compressive measurement strategy that measures only the dynamic part of the scene at each instant and accumulates measurements over time to estimate the static parameters. This enables us to lower the compressive measurement rate considerably. We validate our approach with a range of experiments involving both video recovery, sensing hyper-spectral data, and classification of dynamic scenes from compressive data. Together, these applications demonstrate the effectiveness of the approach. version:2
arxiv-1306-6281 | Compressive Coded Aperture Keyed Exposure Imaging with Optical Flow Reconstruction | http://arxiv.org/abs/1306.6281 | id:1306.6281 author:Zachary T. Harmany, Roummel F. Marcia, Rebecca M. Willett category:cs.IT cs.CV math.IT stat.AP  published:2013-06-26 summary:This paper describes a coded aperture and keyed exposure approach to compressive video measurement which admits a small physical platform, high photon efficiency, high temporal resolution, and fast reconstruction algorithms. The proposed projections satisfy the Restricted Isometry Property (RIP), and hence compressed sensing theory provides theoretical guarantees on the video reconstruction quality. Moreover, the projections can be easily implemented using existing optical elements such as spatial light modulators (SLMs). We extend these coded mask designs to novel dual-scale masks (DSMs) which enable the recovery of a coarse-resolution estimate of the scene with negligible computational cost. We develop fast numerical algorithms which utilize both temporal correlations and optical flow in the video sequence as well as the innovative structure of the projections. Our numerical experiments demonstrate the efficacy of the proposed approach on short-wave infrared data. version:1
arxiv-1305-1199 | How to find real-world applications for compressive sensing | http://arxiv.org/abs/1305.1199 | id:1305.1199 author:Leslie N. Smith category:cs.CV  published:2013-05-06 summary:The potential of compressive sensing (CS) has spurred great interest in the research community and is a fast growing area of research. However, research translating CS theory into practical hardware and demonstrating clear and significant benefits with this hardware over current, conventional imaging techniques has been limited. This article helps researchers to find those niche applications where the CS approach provides substantial gain over conventional approaches by articulating lessons learned in finding one such application; sea skimming missile detection. As a proof of concept, it is demonstrated that a simplified CS missile detection architecture and algorithm provides comparable results to the conventional imaging approach but using a smaller FPA. The primary message is that all of the excitement surrounding CS is necessary and appropriate for encouraging our creativity but we all must also take off our "rose colored glasses" and critically judge our ideas, methods and results relative to conventional imaging approaches. version:4
arxiv-1306-6058 | A maximal-information color to gray conversion method for document images: Toward an optimal grayscale representation for document image binarization | http://arxiv.org/abs/1306.6058 | id:1306.6058 author:Reza Farrahi Moghaddam, Shaohua Chen, Rachid Hedjam, Mohamed Cheriet category:cs.CV  published:2013-06-25 summary:A novel method to convert color/multi-spectral images to gray-level images is introduced to increase the performance of document binarization methods. The method uses the distribution of the pixel data of the input document image in a color space to find a transformation, called the dual transform, which balances the amount of information on all color channels. Furthermore, in order to reduce the intensity variations on the gray output, a color reduction preprocessing step is applied. Then, a channel is selected as the gray value representation of the document image based on the homogeneity criterion on the text regions. In this way, the proposed method can provide a luminance-independent contrast enhancement. The performance of the method is evaluated against various images from two databases, the ICDAR'03 Robust Reading, the KAIST and the DIBCO'09 datasets, subjectively and objectively with promising results. The ground truth images for the images from the ICDAR'03 Robust Reading dataset have been created manually by the authors. version:2
arxiv-1306-6189 | Scaling Up Robust MDPs by Reinforcement Learning | http://arxiv.org/abs/1306.6189 | id:1306.6189 author:Aviv Tamar, Huan Xu, Shie Mannor category:cs.LG stat.ML  published:2013-06-26 summary:We consider large-scale Markov decision processes (MDPs) with parameter uncertainty, under the robust MDP paradigm. Previous studies showed that robust MDPs, based on a minimax approach to handle uncertainty, can be solved using dynamic programming for small to medium sized problems. However, due to the "curse of dimensionality", MDPs that model real-life problems are typically prohibitively large for such approaches. In this work we employ a reinforcement learning approach to tackle this planning problem: we develop a robust approximate dynamic programming method based on a projected fixed point equation to approximately solve large scale robust MDPs. We show that the proposed method provably succeeds under certain technical conditions, and demonstrate its effectiveness through simulation of an option pricing problem. To the best of our knowledge, this is the first attempt to scale up the robust MDPs paradigm. version:1
arxiv-1306-6130 | Competency Tracking for English as a Second or Foreign Language Learners | http://arxiv.org/abs/1306.6130 | id:1306.6130 author:Robert Bishop Jr category:cs.CL  published:2013-06-26 summary:My system utilizes the outcomes feature found in Moodle and other learning content management systems (LCMSs) to keep track of where students are in terms of what language competencies they have mastered and the competencies they need to get where they want to go. These competencies are based on the Common European Framework for (English) Language Learning. This data can be available for everyone involved with a given student's progress (e.g. educators, parents, supervisors and the students themselves). A given student's record of past accomplishments can also be meshed with those of his classmates. Not only are a student's competencies easily seen and tracked, educators can view competencies of a group of students that were achieved prior to enrollment in the class. This should make curriculum decision making easier and more efficient for educators. version:1
