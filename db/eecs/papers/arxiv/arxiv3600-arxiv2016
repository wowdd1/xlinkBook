arxiv-1309-5977 | Efficient Sampling from Time-Varying Log-Concave Distributions | http://arxiv.org/abs/1309.5977 | id:1309.5977 author:Hariharan Narayanan, Alexander Rakhlin category:stat.ML stat.CO  published:2013-09-23 summary:We propose a computationally efficient random walk on a convex body which rapidly mixes and closely tracks a time-varying log-concave distribution. We develop general theoretical guarantees on the required number of steps; this number can be calculated on the fly according to the distance from and the shape of the next distribution. We then illustrate the technique on several examples. Within the context of exponential families, the proposed method produces samples from a posterior distribution which is updated as data arrive in a streaming fashion. The sampling technique can be used to track time-varying truncated distributions, as well as to obtain samples from a changing mixture model, fitted in a streaming fashion to data. In the setting of linear optimization, the proposed method has oracle complexity with best known dependence on the dimension for certain geometries. In the context of online learning and repeated games, the algorithm is an efficient method for implementing no-regret mixture forecasting strategies. Remarkably, in some of these examples, only one step of the random walk is needed to track the next distribution. version:1
arxiv-1309-5931 | Data Mining using Unguided Symbolic Regression on a Blast Furnace Dataset | http://arxiv.org/abs/1309.5931 | id:1309.5931 author:Michael Kommenda, Gabriel Kronberger, Christoph Feilmayr, Michael Affenzeller category:cs.NE  published:2013-09-23 summary:In this paper a data mining approach for variable selection and knowledge extraction from datasets is presented. The approach is based on unguided symbolic regression (every variable present in the dataset is treated as the target variable in multiple regression runs) and a novel variable relevance metric for genetic programming. The relevance of each input variable is calculated and a model approximating the target variable is created. The genetic programming configurations with different target variables are executed multiple times to reduce stochastic effects and the aggregated results are displayed as a variable interaction network. This interaction network highlights important system components and implicit relations between the variables. The whole approach is tested on a blast furnace dataset, because of the complexity of the blast furnace and the many interrelations between the variables. Finally the achieved results are discussed with respect to existing knowledge about the blast furnace process. version:1
arxiv-1309-5909 | From Once Upon a Time to Happily Ever After: Tracking Emotions in Novels and Fairy Tales | http://arxiv.org/abs/1309.5909 | id:1309.5909 author:Saif Mohammad category:cs.CL  published:2013-09-23 summary:Today we have access to unprecedented amounts of literary texts. However, search still relies heavily on key words. In this paper, we show how sentiment analysis can be used in tandem with effective visualizations to quantify and track emotions in both individual books and across very large collections. We introduce the concept of emotion word density, and using the Brothers Grimm fairy tales as example, we show how collections of text can be organized for better search. Using the Google Books Corpus we show how to determine an entity's emotion associations from co-occurring words. Finally, we compare emotion words in fairy tales and novels, to show that fairy tales have a much wider range of emotion word densities than novels. version:1
arxiv-1309-5904 | Fenchel Duals for Drifting Adversaries | http://arxiv.org/abs/1309.5904 | id:1309.5904 author:Suman K Bera, Anamitra R Choudhury, Syamantak Das, Sambuddha Roy, Jayram S. Thatchachar category:cs.LG  published:2013-09-23 summary:We describe a primal-dual framework for the design and analysis of online convex optimization algorithms for {\em drifting regret}. Existing literature shows (nearly) optimal drifting regret bounds only for the $\ell_2$ and the $\ell_1$-norms. Our work provides a connection between these algorithms and the Online Mirror Descent ($\omd$) updates; one key insight that results from our work is that in order for these algorithms to succeed, it suffices to have the gradient of the regularizer to be bounded (in an appropriate norm). For situations (like for the $\ell_1$ norm) where the vanilla regularizer does not have this property, we have to {\em shift} the regularizer to ensure this. Thus, this helps explain the various updates presented in \cite{bansal10, buchbinder12}. We also consider the online variant of the problem with 1-lookahead, and with movement costs in the $\ell_2$-norm. Our primal dual approach yields nearly optimal competitive ratios for this problem. version:1
arxiv-1309-5896 | On the Success Rate of Crossover Operators for Genetic Programming with Offspring Selection | http://arxiv.org/abs/1309.5896 | id:1309.5896 author:Gabriel Kronberger, Stephan Winkler, Michael Affenzeller, Andreas Beham, Stefan Wagner category:cs.NE  published:2013-09-23 summary:Genetic programming is a powerful heuristic search technique that is used for a number of real world applications to solve among others regression, classification, and time-series forecasting problems. A lot of progress towards a theoretic description of genetic programming in form of schema theorems has been made, but the internal dynamics and success factors of genetic programming are still not fully understood. In particular, the effects of different crossover operators in combination with offspring selection are largely unknown. This contribution sheds light on the ability of well-known GP crossover operators to create better offspring when applied to benchmark problems. We conclude that standard (sub-tree swapping) crossover is a good default choice in combination with offspring selection, and that GP with offspring selection and random selection of crossover operators can improve the performance of the algorithm in terms of best solution quality when no solution size constraints are applied. version:1
arxiv-1309-5885 | Smooth minimization of nonsmooth functions with parallel coordinate descent methods | http://arxiv.org/abs/1309.5885 | id:1309.5885 author:Olivier Fercoq, Peter Richtárik category:cs.DC math.OC stat.ML  published:2013-09-23 summary:We study the performance of a family of randomized parallel coordinate descent methods for minimizing the sum of a nonsmooth and separable convex functions. The problem class includes as a special case L1-regularized L1 regression and the minimization of the exponential loss ("AdaBoost problem"). We assume the input data defining the loss function is contained in a sparse $m\times n$ matrix $A$ with at most $\omega$ nonzeros in each row. Our methods need $O(n \beta/\tau)$ iterations to find an approximate solution with high probability, where $\tau$ is the number of processors and $\beta = 1 + (\omega-1)(\tau-1)/(n-1)$ for the fastest variant. The notation hides dependence on quantities such as the required accuracy and confidence levels and the distance of the starting iterate from an optimal point. Since $\beta/\tau$ is a decreasing function of $\tau$, the method needs fewer iterations when more processors are used. Certain variants of our algorithms perform on average only $O(\nnz(A)/n)$ arithmetic operations during a single iteration per processor and, because $\beta$ decreases when $\omega$ does, fewer iterations are needed for sparser problems. version:1
arxiv-1211-6807 | Scalable Spectral Algorithms for Community Detection in Directed Networks | http://arxiv.org/abs/1211.6807 | id:1211.6807 author:Sungmin Kim, Tao Shi category:cs.SI physics.soc-ph stat.ML  published:2012-11-29 summary:Community detection has been one of the central problems in network studies and directed network is particularly challenging due to asymmetry among its links. In this paper, we found that incorporating the direction of links reveals new perspectives on communities regarding to two different roles, source and terminal, that a node plays in each community. Intriguingly, such communities appear to be connected with unique spectral property of the graph Laplacian of the adjacency matrix and we exploit this connection by using regularized SVD methods. We propose harvesting algorithms, coupled with regularized SVDs, that are linearly scalable for efficient identification of communities in huge directed networks. The proposed algorithm shows great performance and scalability on benchmark networks in simulations and successfully recovers communities in real network applications. version:2
arxiv-1212-2044 | Macro-Economic Time Series Modeling and Interaction Networks | http://arxiv.org/abs/1212.2044 | id:1212.2044 author:Gabriel Kronberger, Stefan Fink, Michael Kommenda, Michael Affenzeller category:cs.NE stat.AP  published:2012-12-10 summary:Macro-economic models describe the dynamics of economic quantities. The estimations and forecasts produced by such models play a substantial role for financial and political decisions. In this contribution we describe an approach based on genetic programming and symbolic regression to identify variable interactions in large datasets. In the proposed approach multiple symbolic regression runs are executed for each variable of the dataset to find potentially interesting models. The result is a variable interaction network that describes which variables are most relevant for the approximation of each variable of the dataset. This approach is applied to a macro-economic dataset with monthly observations of important economic indicators in order to identify potentially interesting dependencies of these indicators. The resulting interaction network of macro-economic indicators is briefly discussed and two of the identified models are presented in detail. The two models approximate the help wanted index and the CPI inflation in the US. version:2
arxiv-1309-5843 | Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet | http://arxiv.org/abs/1309.5843 | id:1309.5843 author:Marco Guerini, Lorenzo Gatti, Marco Turchi category:cs.CL  published:2013-09-23 summary:Assigning a positive or negative score to a word out of context (i.e. a word's prior polarity) is a challenging task for sentiment analysis. In the literature, various approaches based on SentiWordNet have been proposed. In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores. Using two different versions of SentiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-of-the-art approach in computing words' prior polarity for sentiment analysis. We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered. version:1
arxiv-1309-5823 | A Kernel Classification Framework for Metric Learning | http://arxiv.org/abs/1309.5823 | id:1309.5823 author:Faqiang Wang, Wangmeng Zuo, Lei Zhang, Deyu Meng, David Zhang category:cs.LG I.5.1  published:2013-09-23 summary:Learning a distance metric from the given training samples plays a crucial role in many machine learning tasks, and various models and optimization algorithms have been proposed in the past decade. In this paper, we generalize several state-of-the-art metric learning methods, such as large margin nearest neighbor (LMNN) and information theoretic metric learning (ITML), into a kernel classification framework. First, doublets and triplets are constructed from the training samples, and a family of degree-2 polynomial kernel functions are proposed for pairs of doublets or triplets. Then, a kernel classification framework is established, which can not only generalize many popular metric learning methods such as LMNN and ITML, but also suggest new metric learning methods, which can be efficiently implemented, interestingly, by using the standard support vector machine (SVM) solvers. Two novel metric learning methods, namely doublet-SVM and triplet-SVM, are then developed under the proposed framework. Experimental results show that doublet-SVM and triplet-SVM achieve competitive classification accuracies with state-of-the-art metric learning methods such as ITML and LMNN but with significantly less training time. version:1
arxiv-1309-6176 | Feature Learning with Gaussian Restricted Boltzmann Machine for Robust Speech Recognition | http://arxiv.org/abs/1309.6176 | id:1309.6176 author:Xin Zheng, Zhiyong Wu, Helen Meng, Weifeng Li, Lianhong Cai category:cs.CL cs.LG cs.SD  published:2013-09-23 summary:In this paper, we first present a new variant of Gaussian restricted Boltzmann machine (GRBM) called multivariate Gaussian restricted Boltzmann machine (MGRBM), with its definition and learning algorithm. Then we propose using a learned GRBM or MGRBM to extract better features for robust speech recognition. Our experiments on Aurora2 show that both GRBM-extracted and MGRBM-extracted feature performs much better than Mel-frequency cepstral coefficient (MFCC) with either HMM-GMM or hybrid HMM-deep neural network (DNN) acoustic model, and MGRBM-extracted feature is slightly better. version:1
arxiv-1309-5660 | Spike Synchronization Dynamics of Small-World Networks | http://arxiv.org/abs/1309.5660 | id:1309.5660 author:Derek Harter category:cs.NE nlin.AO q-bio.NC  published:2013-09-22 summary:In this research report, we examine the effects of small-world network organization on spike synchronization dynamics in networks of Izhikevich spiking units. We interpolate network organizations from regular ring lattices, through the small-world region, to random networks, and measure global spike synchronization dynamics. We examine how average path length and clustering effect the dynamics of global and neighborhood clique spike organization and propagation. We show that the emergence of global synchronization undergoes a phase transition in the small-world region, between the clustering and path length phase transitions that are known to exist. We add additional realistic constraints on the dynamics by introducing propagation delays of spiking signals proportional to wiring length. The addition of delays interferes with the ability of random networks to sustain global synchronization, in relation to the breakdown of clustering in the networks. The addition of delays further enhances the finding that small-world organization is beneficial for balancing neighborhood synchronized waves of organization with global synchronization dynamics. version:1
arxiv-1309-5657 | A Hybrid Algorithm for Matching Arabic Names | http://arxiv.org/abs/1309.5657 | id:1309.5657 author:T. El-Shishtawy category:cs.CL  published:2013-09-22 summary:In this paper, a new hybrid algorithm which combines both of token-based and character-based approaches is presented. The basic Levenshtein approach has been extended to token-based distance metric. The distance metric is enhanced to set the proper granularity level behavior of the algorithm. It smoothly maps a threshold of misspellings differences at the character level, and the importance of token level errors in terms of token's position and frequency. Using a large Arabic dataset, the experimental results show that the proposed algorithm overcomes successfully many types of errors such as: typographical errors, omission or insertion of middle name components, omission of non-significant popular name components, and different writing styles character variations. When compared the results with other classical algorithms, using the same dataset, the proposed algorithm was found to increase the minimum success level of best tested algorithms, while achieving higher upper limits . version:1
arxiv-1309-5652 | LDC Arabic Treebanks and Associated Corpora: Data Divisions Manual | http://arxiv.org/abs/1309.5652 | id:1309.5652 author:Mona Diab, Nizar Habash, Owen Rambow, Ryan Roth category:cs.CL  published:2013-09-22 summary:The Linguistic Data Consortium (LDC) has developed hundreds of data corpora for natural language processing (NLP) research. Among these are a number of annotated treebank corpora for Arabic. Typically, these corpora consist of a single collection of annotated documents. NLP research, however, usually requires multiple data sets for the purposes of training models, developing techniques, and final evaluation. Therefore it becomes necessary to divide the corpora used into the required data sets (divisions). This document details a set of rules that have been defined to enable consistent divisions for old and new Arabic treebanks (ATB) and related corpora. version:1
arxiv-1309-5605 | Stochastic Bound Majorization | http://arxiv.org/abs/1309.5605 | id:1309.5605 author:Anna Choromanska, Tony Jebara category:cs.LG  published:2013-09-22 summary:Recently a majorization method for optimizing partition functions of log-linear models was proposed alongside a novel quadratic variational upper-bound. In the batch setting, it outperformed state-of-the-art first- and second-order optimization methods on various learning tasks. We propose a stochastic version of this bound majorization method as well as a low-rank modification for high-dimensional data-sets. The resulting stochastic second-order method outperforms stochastic gradient descent (across variations and various tunings) both in terms of the number of iterations and computation time till convergence while finding a better quality parameter setting. The proposed method bridges first- and second-order stochastic optimization methods by maintaining a computational complexity that is linear in the data dimension and while exploiting second order information about the pseudo-global curvature of the objective function (as opposed to the local curvature in the Hessian). version:1
arxiv-1309-5549 | Stochastic First- and Zeroth-order Methods for Nonconvex Stochastic Programming | http://arxiv.org/abs/1309.5549 | id:1309.5549 author:Saeed Ghadimi, Guanghui Lan category:math.OC cs.CC stat.ML  published:2013-09-22 summary:In this paper, we introduce a new stochastic approximation (SA) type algorithm, namely the randomized stochastic gradient (RSG) method, for solving an important class of nonlinear (possibly nonconvex) stochastic programming (SP) problems. We establish the complexity of this method for computing an approximate stationary point of a nonlinear programming problem. We also show that this method possesses a nearly optimal rate of convergence if the problem is convex. We discuss a variant of the algorithm which consists of applying a post-optimization phase to evaluate a short list of solutions generated by several independent runs of the RSG method, and show that such modification allows to improve significantly the large-deviation properties of the algorithm. These methods are then specialized for solving a class of simulation-based optimization problems in which only stochastic zeroth-order information is available. version:1
arxiv-1309-5427 | Latent Fisher Discriminant Analysis | http://arxiv.org/abs/1309.5427 | id:1309.5427 author:Gang Chen category:cs.LG cs.CV stat.ML I.2.10  published:2013-09-21 summary:Linear Discriminant Analysis (LDA) is a well-known method for dimensionality reduction and classification. Previous studies have also extended the binary-class case into multi-classes. However, many applications, such as object detection and keyframe extraction cannot provide consistent instance-label pairs, while LDA requires labels on instance level for training. Thus it cannot be directly applied for semi-supervised classification problem. In this paper, we overcome this limitation and propose a latent variable Fisher discriminant analysis model. We relax the instance-level labeling into bag-level, is a kind of semi-supervised (video-level labels of event type are required for semantic frame extraction) and incorporates a data-driven prior over the latent variables. Hence, our method combines the latent variable inference and dimension reduction in an unified bayesian framework. We test our method on MUSK and Corel data sets and yield competitive results compared to the baseline approach. We also demonstrate its capacity on the challenging TRECVID MED11 dataset for semantic keyframe extraction and conduct a human-factors ranking-based experimental evaluation, which clearly demonstrates our proposed method consistently extracts more semantically meaningful keyframes than challenging baselines. version:1
arxiv-1309-6195 | Scan-based Compressed Terahertz Imaging and Real-Time Reconstruction via the Complex-valued Fast Block Sparse Bayesian Learning Algorithm | http://arxiv.org/abs/1309.6195 | id:1309.6195 author:Benyuan Liu, Hongqi Fan, Zaiqi Lu, Qiang Fu category:cs.CV  published:2013-09-20 summary:Compressed Sensing based Terahertz imaging (CS-THz) is a computational imaging technique. It uses only one THz receiver to accumulate the random modulated image measurements where the original THz image is reconstruct from these measurements using compressed sensing solvers. The advantage of the CS-THz is its reduced acquisition time compared with the raster scan mode. However, when it applied to large-scale two-dimensional (2D) imaging, the increased dimension resulted in both high computational complexity and excessive memory usage. In this paper, we introduced a novel CS-based THz imaging system that progressively compressed the THz image column by column. Therefore, the CS-THz system could be simplified with a much smaller sized modulator and reduced dimension. In order to utilize the block structure and the correlation of adjacent columns of the THz image, a complex-valued block sparse Bayesian learning algorithm was proposed. We conducted systematic evaluation of state-of-the-art CS algorithms under the scan based CS-THz architecture. The compression ratios and the choices of the sensing matrices were analyzed in detail using both synthetic and real-life THz images. Simulation results showed that both the scan based architecture and the proposed recovery algorithm were superior and efficient for large scale CS-THz applications. version:1
arxiv-1309-5401 | Nonmyopic View Planning for Active Object Detection | http://arxiv.org/abs/1309.5401 | id:1309.5401 author:Nikolay Atanasov, Bharath Sankaran, Jerome Le Ny, George J. Pappas, Kostas Daniilidis category:cs.RO cs.CV cs.SY  published:2013-09-20 summary:One of the central problems in computer vision is the detection of semantically important objects and the estimation of their pose. Most of the work in object detection has been based on single image processing and its performance is limited by occlusions and ambiguity in appearance and geometry. This paper proposes an active approach to object detection by controlling the point of view of a mobile depth camera. When an initial static detection phase identifies an object of interest, several hypotheses are made about its class and orientation. The sensor then plans a sequence of views, which balances the amount of energy used to move with the chance of identifying the correct hypothesis. We formulate an active hypothesis testing problem, which includes sensor mobility, and solve it using a point-based approximate POMDP algorithm. The validity of our approach is verified through simulation and real-world experiments with the PR2 robot. The results suggest that our approach outperforms the widely-used greedy view point selection and provides a significant improvement over static object detection. version:1
arxiv-1309-5942 | Colourful Language: Measuring Word-Colour Associations | http://arxiv.org/abs/1309.5942 | id:1309.5942 author:Saif Mohammad category:cs.CL  published:2013-09-20 summary:Since many real-world concepts are associated with colour, for example danger with red, linguistic information is often complimented with the use of appropriate colours in information visualization and product marketing. Yet, there is no comprehensive resource that captures concept-colour associations. We present a method to create a large word-colour association lexicon by crowdsourcing. We focus especially on abstract concepts and emotions to show that even though they cannot be physically visualized, they too tend to have strong colour associations. Finally, we show how word-colour associations manifest themselves in language, and quantify usefulness of co-occurrence and polarity cues in automatically detecting colour associations. version:1
arxiv-1309-5391 | Even the Abstract have Colour: Consensus in Word-Colour Associations | http://arxiv.org/abs/1309.5391 | id:1309.5391 author:Saif M. Mohammad category:cs.CL  published:2013-09-20 summary:Colour is a key component in the successful dissemination of information. Since many real-world concepts are associated with colour, for example danger with red, linguistic information is often complemented with the use of appropriate colours in information visualization and product marketing. Yet, there is no comprehensive resource that captures concept-colour associations. We present a method to create a large word-colour association lexicon by crowdsourcing. A word-choice question was used to obtain sense-level annotations and to ensure data quality. We focus especially on abstract concepts and emotions to show that even they tend to have strong colour associations. Thus, using the right colours can not only improve semantic coherence, but also inspire the desired emotional response. version:1
arxiv-1303-4694 | Recovering Non-negative and Combined Sparse Representations | http://arxiv.org/abs/1303.4694 | id:1303.4694 author:Karthikeyan Natesan Ramamurthy, Jayaraman J. Thiagarajan, Andreas Spanias category:math.NA cs.LG stat.ML  published:2013-03-12 summary:The non-negative solution to an underdetermined linear system can be uniquely recovered sometimes, even without imposing any additional sparsity constraints. In this paper, we derive conditions under which a unique non-negative solution for such a system can exist, based on the theory of polytopes. Furthermore, we develop the paradigm of combined sparse representations, where only a part of the coefficient vector is constrained to be non-negative, and the rest is unconstrained (general). We analyze the recovery of the unique, sparsest solution, for combined representations, under three different cases of coefficient support knowledge: (a) the non-zero supports of non-negative and general coefficients are known, (b) the non-zero support of general coefficients alone is known, and (c) both the non-zero supports are unknown. For case (c), we propose the combined orthogonal matching pursuit algorithm for coefficient recovery and derive the deterministic sparsity threshold under which recovery of the unique, sparsest coefficient vector is possible. We quantify the order complexity of the algorithms, and examine their performance in exact and approximate recovery of coefficients under various conditions of noise. Furthermore, we also obtain their empirical phase transition characteristics. We show that the basis pursuit algorithm, with partial non-negative constraints, and the proposed greedy algorithm perform better in recovering the unique sparse representation when compared to their unconstrained counterparts. Finally, we demonstrate the utility of the proposed methods in recovering images corrupted by saturation noise. version:2
arxiv-1309-5319 | Recognizing Speech in a Novel Accent: The Motor Theory of Speech Perception Reframed | http://arxiv.org/abs/1309.5319 | id:1309.5319 author:Clément Moulin-Frier, M. A. Arbib category:cs.CL cs.LG q-bio.NC  published:2013-09-20 summary:The motor theory of speech perception holds that we perceive the speech of another in terms of a motor representation of that speech. However, when we have learned to recognize a foreign accent, it seems plausible that recognition of a word rarely involves reconstruction of the speech gestures of the speaker rather than the listener. To better assess the motor theory and this observation, we proceed in three stages. Part 1 places the motor theory of speech perception in a larger framework based on our earlier models of the adaptive formation of mirror neurons for grasping, and for viewing extensions of that mirror system as part of a larger system for neuro-linguistic processing, augmented by the present consideration of recognizing speech in a novel accent. Part 2 then offers a novel computational model of how a listener comes to understand the speech of someone speaking the listener's native language with a foreign accent. The core tenet of the model is that the listener uses hypotheses about the word the speaker is currently uttering to update probabilities linking the sound produced by the speaker to phonemes in the native language repertoire of the listener. This, on average, improves the recognition of later words. This model is neutral regarding the nature of the representations it uses (motor vs. auditory). It serve as a reference point for the discussion in Part 3, which proposes a dual-stream neuro-linguistic architecture to revisits claims for and against the motor theory of speech perception and the relevance of mirror neurons, and extracts some implications for the reframing of the motor theory. version:1
arxiv-1309-5290 | An introduction to the Europe Media Monitor family of applications | http://arxiv.org/abs/1309.5290 | id:1309.5290 author:Ralf Steinberger, Bruno Pouliquen, Erik van der Goot category:cs.CL H.3.1; H.3.3; I.2.7  published:2013-09-20 summary:Most large organizations have dedicated departments that monitor the media to keep up-to-date with relevant developments and to keep an eye on how they are represented in the news. Part of this media monitoring work can be automated. In the European Union with its 23 official languages, it is particularly important to cover media reports in many languages in order to capture the complementary news content published in the different countries. It is also important to be able to access the news content across languages and to merge the extracted information. We present here the four publicly accessible systems of the Europe Media Monitor (EMM) family of applications, which cover between 19 and 50 languages (see http://press.jrc.it/overview.html). We give an overview of their functionality and discuss some of the implications of the fact that they cover quite so many languages. We discuss design issues necessary to be able to achieve this high multilinguality, as well as the benefits of this multilinguality. version:1
arxiv-1309-5803 | Scalable Anomaly Detection in Large Homogenous Populations | http://arxiv.org/abs/1309.5803 | id:1309.5803 author:Henrik Ohlsson, Tianshi Chen, Sina Khoshfetrat Pakazad, Lennart Ljung, S. Shankar Sastry category:cs.LG cs.DC cs.SY math.OC  published:2013-09-20 summary:Anomaly detection in large populations is a challenging but highly relevant problem. The problem is essentially a multi-hypothesis problem, with a hypothesis for every division of the systems into normal and anomal systems. The number of hypothesis grows rapidly with the number of systems and approximate solutions become a necessity for any problems of practical interests. In the current paper we take an optimization approach to this multi-hypothesis problem. We first observe that the problem is equivalent to a non-convex combinatorial optimization problem. We then relax the problem to a convex problem that can be solved distributively on the systems and that stays computationally tractable as the number of systems increase. An interesting property of the proposed method is that it can under certain conditions be shown to give exactly the same result as the combinatorial multi-hypothesis problem and the relaxation is hence tight. version:1
arxiv-1309-5226 | DGT-TM: A freely Available Translation Memory in 22 Languages | http://arxiv.org/abs/1309.5226 | id:1309.5226 author:Ralf Steinberger, Andreas Eisele, Szymon Klocek, Spyridon Pilos, Patrick Schlüter category:cs.CL I.2.7  published:2013-09-20 summary:The European Commission's (EC) Directorate General for Translation, together with the EC's Joint Research Centre, is making available a large translation memory (TM; i.e. sentences and their professionally produced translations) covering twenty-two official European Union (EU) languages and their 231 language pairs. Such a resource is typically used by translation professionals in combination with TM software to improve speed and consistency of their translations. However, this resource has also many uses for translation studies and for language technology applications, including Statistical Machine Translation (SMT), terminology extraction, Named Entity Recognition (NER), multilingual classification and clustering, and many more. In this reference paper for DGT-TM, we introduce this new resource, provide statistics regarding its size, and explain how it was produced and how to use it. version:1
arxiv-1309-5223 | JRC EuroVoc Indexer JEX - A freely available multi-label categorisation tool | http://arxiv.org/abs/1309.5223 | id:1309.5223 author:Ralf Steinberger, Mohamed Ebrahim, Marco Turchi category:cs.CL H.3.1; H.3.6  published:2013-09-20 summary:EuroVoc (2012) is a highly multilingual thesaurus consisting of over 6,700 hierarchically organised subject domains used by European Institutions and many authorities in Member States of the European Union (EU) for the classification and retrieval of official documents. JEX is JRC-developed multi-label classification software that learns from manually labelled data to automatically assign EuroVoc descriptors to new documents in a profile-based category-ranking task. The JEX release consists of trained classifiers for 22 official EU languages, of parallel training data in the same languages, of an interface that allows viewing and amending the assignment results, and of a module that allows users to re-train the tool on their own document collections. JEX allows advanced users to change the document representation so as to possibly improve the categorisation result through linguistic pre-processing. JEX can be used as a tool for interactive EuroVoc descriptor assignment to increase speed and consistency of the human categorisation process, or it can be used fully automatically. The output of JEX is a language-independent EuroVoc feature vector lending itself also as input to various other Language Technology tasks, including cross-lingual clustering and classification, cross-lingual plagiarism detection, sentence selection and ranking, and more. version:1
arxiv-1302-4389 | Maxout Networks | http://arxiv.org/abs/1302.4389 | id:1302.4389 author:Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG  published:2013-02-18 summary:We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN. version:4
arxiv-1309-5211 | mTim: Rapid and accurate transcript reconstruction from RNA-Seq data | http://arxiv.org/abs/1309.5211 | id:1309.5211 author:Georg Zeller, Nico Goernitz, Andre Kahles, Jonas Behr, Pramod Mudrakarta, Soeren Sonnenburg, Gunnar Raetsch category:q-bio.GN stat.ML  published:2013-09-20 summary:Recent advances in high-throughput cDNA sequencing (RNA-Seq) technology have revolutionized transcriptome studies. A major motivation for RNA-Seq is to map the structure of expressed transcripts at nucleotide resolution. With accurate computational tools for transcript reconstruction, this technology may also become useful for genome (re-)annotation, which has mostly relied on de novo gene finding where gene structures are primarily inferred from the genome sequence. We developed a machine-learning method, called mTim (margin-based transcript inference method) for transcript reconstruction from RNA-Seq read alignments that is based on discriminatively trained hidden Markov support vector machines. In addition to features derived from read alignments, it utilizes characteristic genomic sequences, e.g. around splice sites, to improve transcript predictions. mTim inferred transcripts that were highly accurate and relatively robust to alignment errors in comparison to those from Cufflinks, a widely used transcript assembly method. version:1
arxiv-1309-5174 | Saying What You're Looking For: Linguistics Meets Video Search | http://arxiv.org/abs/1309.5174 | id:1309.5174 author:Andrei Barbu, N. Siddharth, Jeffrey Mark Siskind category:cs.CV cs.CL cs.IR  published:2013-09-20 summary:We present an approach to searching large video corpora for video clips which depict a natural-language query in the form of a sentence. This approach uses compositional semantics to encode subtle meaning that is lost in other systems, such as the difference between two sentences which have identical words but entirely different meaning: "The person rode the horse} vs. \emph{The horse rode the person". Given a video-sentence pair and a natural-language parser, along with a grammar that describes the space of sentential queries, we produce a score which indicates how well the video depicts the sentence. We produce such a score for each video clip in a corpus and return a ranked list of clips. Furthermore, this approach addresses two fundamental problems simultaneously: detecting and tracking objects, and recognizing whether those tracks depict the query. Because both tracking and object detection are unreliable, this uses knowledge about the intended sentential query to focus the tracker on the relevant participants and ensures that the resulting tracks are described by the sentential query. While earlier work was limited to single-word queries which correspond to either verbs or nouns, we show how one can search for complex queries which contain multiple phrases, such as prepositional phrases, and modifiers, such as adverbs. We demonstrate this approach by searching for 141 queries involving people and horses interacting with each other in 10 full-length Hollywood movies. version:1
arxiv-1309-5110 | An ant colony optimization algorithm for job shop scheduling problem | http://arxiv.org/abs/1309.5110 | id:1309.5110 author:Edson Flórez, Wilfredo Gómez, Lola Bautista category:cs.AI cs.NE  published:2013-09-19 summary:The nature has inspired several metaheuristics, outstanding among these is Ant Colony Optimization (ACO), which have proved to be very effective and efficient in problems of high complexity (NP-hard) in combinatorial optimization. This paper describes the implementation of an ACO model algorithm known as Elitist Ant System (EAS), applied to a combinatorial optimization problem called Job Shop Scheduling Problem (JSSP). We propose a method that seeks to reduce delays designating the operation immediately available, but considering the operations that lack little to be available and have a greater amount of pheromone. The performance of the algorithm was evaluated for problems of JSSP reference, comparing the quality of the solutions obtained regarding the best known solution of the most effective methods. The solutions were of good quality and obtained with a remarkable efficiency by having to make a very low number of objective function evaluations. version:1
arxiv-1309-5047 | A Comparative Analysis of Ensemble Classifiers: Case Studies in Genomics | http://arxiv.org/abs/1309.5047 | id:1309.5047 author:Sean Whalen, Gaurav Pandey category:cs.LG q-bio.GN stat.ML  published:2013-09-19 summary:The combination of multiple classifiers using ensemble methods is increasingly important for making progress in a variety of difficult prediction problems. We present a comparative analysis of several ensemble methods through two case studies in genomics, namely the prediction of genetic interactions and protein functions, to demonstrate their efficacy on real-world datasets and draw useful conclusions about their behavior. These methods include simple aggregation, meta-learning, cluster-based meta-learning, and ensemble selection using heterogeneous classifiers trained on resampled data to improve the diversity of their predictions. We present a detailed analysis of these methods across 4 genomics datasets and find the best of these methods offer statistically significant improvements over the state of the art in their respective domains. In addition, we establish a novel connection between ensemble selection and meta-learning, demonstrating how both of these disparate methods establish a balance between ensemble diversity and performance. version:1
arxiv-1106-1194 | Constructing Runge-Kutta Methods with the Use of Artificial Neural Networks | http://arxiv.org/abs/1106.1194 | id:1106.1194 author:Angelos A. Anastassi category:cs.NE math.NA 68T05  65L06  published:2011-06-06 summary:A methodology that can generate the optimal coefficients of a numerical method with the use of an artificial neural network is presented in this work. The network can be designed to produce a finite difference algorithm that solves a specific system of ordinary differential equations numerically. The case we are examining here concerns an explicit two-stage Runge-Kutta method for the numerical solution of the two-body problem. Following the implementation of the network, the latter is trained to obtain the optimal values for the coefficients of the Runge-Kutta method. The comparison of the new method to others that are well known in the literature proves its efficiency and demonstrates the capability of the network to provide efficient algorithms for specific problems. version:2
arxiv-1309-5004 | Blind Deconvolution via Maximum Kurtosis Adaptive Filtering | http://arxiv.org/abs/1309.5004 | id:1309.5004 author:Deborah Pereg, Doron Benzvi category:cs.CV  published:2013-09-19 summary:In this paper, we present an algorithm for identifying a parametrically described destructive unknown system based on a non-gaussianity measure. It is known that under certain conditions the output of a linear system is more gaussian than the input. Hence, an inverse filter is searched, such that its output is minimally gaussian. We use the kurtosis as a measure of the non-gaussianity of the signal. A maximum of the kurtosis as a function of the deconvolving filter coefficients is searched. The search is done iteratively using the gradient ascent algorithm, and the coefficients at the maximum point correspond to the inverse filter coefficients. This filter may be applied to the distorted signal to obtain the original undistorted signal. While a similar approach has been used before, it was always directed at a particular kind of a signal, commonly of impulsive characteristics. In this paper a successful attempt has been made to apply the algorithm to a wider range of signals, such as to process distorted audio signals and destructed images. This innovative implementation required the revelation of a way to preprocess the distorted signal at hand. The experimental results show very good performance in terms of recovering audio signals and blurred images, both for an FIR and IIR distorting filters. version:1
arxiv-1309-4962 | HOL(y)Hammer: Online ATP Service for HOL Light | http://arxiv.org/abs/1309.4962 | id:1309.4962 author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO cs.MS  published:2013-09-19 summary:HOL(y)Hammer is an online AI/ATP service for formal (computer-understandable) mathematics encoded in the HOL Light system. The service allows its users to upload and automatically process an arbitrary formal development (project) based on HOL Light, and to attack arbitrary conjectures that use the concepts defined in some of the uploaded projects. For that, the service uses several automated reasoning systems combined with several premise selection methods trained on all the project proofs. The projects that are readily available on the server for such query answering include the recent versions of the Flyspeck, Multivariate Analysis and Complex Analysis libraries. The service runs on a 48-CPU server, currently employing in parallel for each task 7 AI/ATP combinations and 4 decision procedures that contribute to its overall performance. The system is also available for local installation by interested users, who can customize it for their own proof development. An Emacs interface allowing parallel asynchronous queries to the service is also provided. The overall structure of the service is outlined, problems that arise and their solutions are discussed, and an initial account of using the system is given. version:1
arxiv-1309-7959 | Exploration and Exploitation in Visuomotor Prediction of Autonomous Agents | http://arxiv.org/abs/1309.7959 | id:1309.7959 author:Laurens Bliek category:cs.LG cs.CV math.DS  published:2013-09-19 summary:This paper discusses various techniques to let an agent learn how to predict the effects of its own actions on its sensor data autonomously, and their usefulness to apply them to visual sensors. An Extreme Learning Machine is used for visuomotor prediction, while various autonomous control techniques that can aid the prediction process by balancing exploration and exploitation are discussed and tested in a simple system: a camera moving over a 2D greyscale image. version:1
arxiv-1202-5070 | Optimal detection of sparse principal components in high dimension | http://arxiv.org/abs/1202.5070 | id:1202.5070 author:Quentin Berthet, Philippe Rigollet category:math.ST stat.ML stat.TH  published:2012-02-23 summary:We perform a finite sample analysis of the detection levels for sparse principal components of a high-dimensional covariance matrix. Our minimax optimal test is based on a sparse eigenvalue statistic. Alas, computing this test is known to be NP-complete in general, and we describe a computationally efficient alternative test using convex relaxations. Our relaxation is also proved to detect sparse principal components at near optimal detection levels, and it performs well on simulated datasets. Moreover, using polynomial time reductions from theoretical computer science, we bring significant evidence that our results cannot be improved, thus revealing an inherent trade off between statistical and computational performance. version:3
arxiv-1309-4859 | Predictive PAC Learning and Process Decompositions | http://arxiv.org/abs/1309.4859 | id:1309.4859 author:Cosma Rohilla Shalizi, Aryeh Kontorovich category:stat.ML  published:2013-09-19 summary:We informally call a stochastic process learnable if it admits a generalization error approaching zero in probability for any concept class with finite VC-dimension (IID processes are the simplest example). A mixture of learnable processes need not be learnable itself, and certainly its generalization error need not decay at the same rate. In this paper, we argue that it is natural in predictive PAC to condition not on the past observations but on the mixture component of the sample path. This definition not only matches what a realistic learner might demand, but also allows us to sidestep several otherwise grave problems in learning from dependent data. In particular, we give a novel PAC generalization bound for mixtures of learnable processes with a generalization error that is not worse than that of each mixture component. We also provide a characterization of mixtures of absolutely regular ($\beta$-mixing) processes, of independent probability-theoretic interest. version:1
arxiv-1309-4844 | Network Anomaly Detection: A Survey and Comparative Analysis of Stochastic and Deterministic Methods | http://arxiv.org/abs/1309.4844 | id:1309.4844 author:Jing Wang, Daniel Rossell, Christos G. Cassandras, Ioannis Ch. Paschalidis category:stat.ML cs.LG cs.NI  published:2013-09-19 summary:We present five methods to the problem of network anomaly detection. These methods cover most of the common techniques in the anomaly detection field, including Statistical Hypothesis Tests (SHT), Support Vector Machines (SVM) and clustering analysis. We evaluate all methods in a simulated network that consists of nominal data, three flow-level anomalies and one packet-level attack. Through analyzing the results, we point out the advantages and disadvantages of each method and conclude that combining the results of the individual methods can yield improved anomaly detection results. version:1
arxiv-1201-0566 | Learning joint intensity-depth sparse representations | http://arxiv.org/abs/1201.0566 | id:1201.0566 author:Ivana Tosic, Sarah Drewes category:cs.CV  published:2012-01-03 summary:This paper presents a method for learning overcomplete dictionaries composed of two modalities that describe a 3D scene: image intensity and scene depth. We propose a novel Joint Basis Pursuit (JBP) algorithm that finds related sparse features in two modalities using conic programming and integrate it into a two-step dictionary learning algorithm. JBP differs from related convex algorithms because it finds joint sparsity models with different atoms and different coefficient values for intensity and depth. This is crucial for recovering generative models where the same sparse underlying causes (3D features) give rise to different signals (intensity and depth). We give a theoretical bound for the sparse coefficient recovery error obtained by JBP, and show experimentally that JBP is far superior to the state of the art Group Lasso algorithm. When applied to the Middlebury depth-intensity database, our learning algorithm converges to a set of related features, such as pairs of depth and intensity edges or image textures and depth slants. Finally, we show that the learned dictionary and JBP achieve the state of the art depth inpainting performance on time-of-flight 3D data. version:2
arxiv-1309-3323 | Mapping Mutable Genres in Structurally Complex Volumes | http://arxiv.org/abs/1309.3323 | id:1309.3323 author:Ted Underwood, Michael L. Black, Loretta Auvil, Boris Capitanu category:cs.CL cs.DL  published:2013-09-12 summary:To mine large digital libraries in humanistically meaningful ways, scholars need to divide them by genre. This is a task that classification algorithms are well suited to assist, but they need adjustment to address the specific challenges of this domain. Digital libraries pose two problems of scale not usually found in the article datasets used to test these algorithms. 1) Because libraries span several centuries, the genres being identified may change gradually across the time axis. 2) Because volumes are much longer than articles, they tend to be internally heterogeneous, and the classification task needs to begin with segmentation. We describe a multi-layered solution that trains hidden Markov models to segment volumes, and uses ensembles of overlapping classifiers to address historical change. We test this approach on a collection of 469,200 volumes drawn from HathiTrust Digital Library. To demonstrate the humanistic value of these methods, we extract 32,209 volumes of fiction from the digital library, and trace the changing proportions of first- and third-person narration in the corpus. We note that narrative points of view seem to have strong associations with particular themes and genres. version:2
arxiv-1309-4714 | Temporal-Difference Learning to Assist Human Decision Making during the Control of an Artificial Limb | http://arxiv.org/abs/1309.4714 | id:1309.4714 author:Ann L. Edwards, Alexandra Kearney, Michael Rory Dawson, Richard S. Sutton, Patrick M. Pilarski category:cs.AI cs.LG cs.RO  published:2013-09-18 summary:In this work we explore the use of reinforcement learning (RL) to help with human decision making, combining state-of-the-art RL algorithms with an application to prosthetics. Managing human-machine interaction is a problem of considerable scope, and the simplification of human-robot interfaces is especially important in the domains of biomedical technology and rehabilitation medicine. For example, amputees who control artificial limbs are often required to quickly switch between a number of control actions or modes of operation in order to operate their devices. We suggest that by learning to anticipate (predict) a user's behaviour, artificial limbs could take on an active role in a human's control decisions so as to reduce the burden on their users. Recently, we showed that RL in the form of general value functions (GVFs) could be used to accurately detect a user's control intent prior to their explicit control choices. In the present work, we explore the use of temporal-difference learning and GVFs to predict when users will switch their control influence between the different motor functions of a robot arm. Experiments were performed using a multi-function robot arm that was controlled by muscle signals from a user's body (similar to conventional artificial limb control). Our approach was able to acquire and maintain forecasts about a user's switching decisions in real time. It also provides an intuitive and reward-free way for users to correct or reinforce the decisions made by the machine learning system. We expect that when a system is certain enough about its predictions, it can begin to take over switching decisions from the user to streamline control and potentially decrease the time and effort needed to complete tasks. This preliminary study therefore suggests a way to naturally integrate human- and machine-based decision making systems. version:1
arxiv-1204-1276 | Distribution-Dependent Sample Complexity of Large Margin Learning | http://arxiv.org/abs/1204.1276 | id:1204.1276 author:Sivan Sabato, Nathan Srebro, Naftali Tishby category:stat.ML cs.LG  published:2012-04-05 summary:We obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with L2 regularization: We introduce the margin-adapted dimension, which is a simple function of the second order statistics of the data distribution, and show distribution-specific upper and lower bounds on the sample complexity, both governed by the margin-adapted dimension of the data distribution. The upper bounds are universal, and the lower bounds hold for the rich family of sub-Gaussian distributions with independent features. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification. To prove the lower bound, we develop several new tools of independent interest. These include new connections between shattering and hardness of learning, new properties of shattering with linear classifiers, and a new lower bound on the smallest eigenvalue of a random Gram matrix generated by sub-Gaussian variables. Our results can be used to quantitatively compare large margin learning to other learning rules, and to improve the effectiveness of methods that use sample complexity bounds, such as active learning. version:4
arxiv-1309-4628 | Text segmentation with character-level text embeddings | http://arxiv.org/abs/1309.4628 | id:1309.4628 author:Grzegorz Chrupała category:cs.CL  published:2013-09-18 summary:Learning word representations has recently seen much success in computational linguistics. However, assuming sequences of word tokens as input to linguistic analysis is often unjustified. For many languages word segmentation is a non-trivial task and naturally occurring text is sometimes a mixture of natural language strings and other character data. We propose to learn text representations directly from raw character sequences by training a Simple recurrent Network to predict the next character in text. The network uses its hidden layer to evolve abstract representations of the character sequences it sees. To demonstrate the usefulness of the learned text embeddings, we use them as features in a supervised character level text segmentation and labeling task: recognizing spans of text containing programming language code. By using the embeddings as features we are able to substantially improve over a baseline which uses only surface character n-grams. version:1
arxiv-1309-4582 | A novel approach to nose-tip and eye corners detection using H-K Curvature Analysis in case of 3D images | http://arxiv.org/abs/1309.4582 | id:1309.4582 author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV  published:2013-09-18 summary:In this paper we present a novel method that combines a HK curvature-based approach for three-dimensional (3D) face detection in different poses (X-axis, Y-axis and Z-axis). Salient face features, such as the eyes and nose, are detected through an analysis of the curvature of the entire facial surface. All the experiments have been performed on the FRAV3D Database. After applying the proposed algorithm to the 3D facial surface we have obtained considerably good results i.e. on 752 3D face images our method detected the eye corners for 543 face images, thus giving a 72.20% of eye corners detection and 743 face images for nose-tip detection thus giving a 98.80% of good nose tip localization version:1
arxiv-1309-4577 | Detection of pose orientation across single and multiple axes in case of 3D face images | http://arxiv.org/abs/1309.4577 | id:1309.4577 author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV  published:2013-09-18 summary:In this paper, we propose a new approach that takes as input a 3D face image across X, Y and Z axes as well as both Y and X axes and gives output as its pose i.e. it tells whether the face is oriented with respect the X, Y or Z axes or is it oriented across multiple axes with angles of rotation up to 42 degree. All the experiments have been performed on the FRAV3D, GAVADB and Bosphorus database which has two figures of each individual across multiple axes. After applying the proposed algorithm to the 3D facial surface from FRAV3D on 848 3D faces, 566 3D faces were correctly recognized for pose thus giving 67% of correct identification rate. We had experimented on 420 images from the GAVADB database, and only 336 images were detected for correct pose identification rate i.e. 80% and from Bosphorus database on 560 images only 448 images were detected for correct pose identification i.e. 80%.abstract goes here. version:1
arxiv-1309-4573 | A novel approach for nose tip detection using smoothing by weighted median filtering applied to 3D face images in variant poses | http://arxiv.org/abs/1309.4573 | id:1309.4573 author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV  published:2013-09-18 summary:This paper is based on an application of smoothing of 3D face images followed by feature detection i.e. detecting the nose tip. The present method uses a weighted mesh median filtering technique for smoothing. In this present smoothing technique we have built the neighborhood surrounding a particular point in 3D face and replaced that with the weighted value of the surrounding points in 3D face image. After applying the smoothing technique to the 3D face images our experimental results show that we have obtained considerable improvement as compared to the algorithm without smoothing. We have used here the maximum intensity algorithm for detecting the nose-tip and this method correctly detects the nose-tip in case of any pose i.e. along X, Y, and Z axes. The present technique gave us worked successfully on 535 out of 542 3D face images as compared to the method without smoothing which worked only on 521 3D face images out of 542 face images. Thus we have obtained a 98.70% performance rate over 96.12% performance rate of the algorithm without smoothing. All the experiments have been performed on the FRAV3D database. version:1
arxiv-1309-4999 | Bayesian rules and stochastic models for high accuracy prediction of solar radiation | http://arxiv.org/abs/1309.4999 | id:1309.4999 author:Cyril Voyant, C. Darras, Marc Muselli, Christophe Paoli, Marie Laure Nivet, Philippe Poggi category:cs.LG stat.AP  published:2013-09-18 summary:It is essential to find solar predictive methods to massively insert renewable energies on the electrical distribution grid. The goal of this study is to find the best methodology allowing predicting with high accuracy the hourly global radiation. The knowledge of this quantity is essential for the grid manager or the private PV producer in order to anticipate fluctuations related to clouds occurrences and to stabilize the injected PV power. In this paper, we test both methodologies: single and hybrid predictors. In the first class, we include the multi-layer perceptron (MLP), auto-regressive and moving average (ARMA), and persistence models. In the second class, we mix these predictors with Bayesian rules to obtain ad-hoc models selections, and Bayesian averages of outputs related to single models. If MLP and ARMA are equivalent (nRMSE close to 40.5% for the both), this hybridization allows a nRMSE gain upper than 14 percentage points compared to the persistence estimation (nRMSE=37% versus 51%). version:1
arxiv-1212-1527 | Learning Mixtures of Arbitrary Distributions over Large Discrete Domains | http://arxiv.org/abs/1212.1527 | id:1212.1527 author:Yuval Rabani, Leonard Schulman, Chaitanya Swamy category:cs.LG cs.DS F.2.2; G.2; G.3  published:2012-12-07 summary:We give an algorithm for learning a mixture of {\em unstructured} distributions. This problem arises in various unsupervised learning scenarios, for example in learning {\em topic models} from a corpus of documents spanning several topics. We show how to learn the constituents of a mixture of $k$ arbitrary distributions over a large discrete domain $[n]=\{1,2,\dots,n\}$ and the mixture weights, using $O(n\polylog n)$ samples. (In the topic-model learning setting, the mixture constituents correspond to the topic distributions.) This task is information-theoretically impossible for $k>1$ under the usual sampling process from a mixture distribution. However, there are situations (such as the above-mentioned topic model case) in which each sample point consists of several observations from the same mixture constituent. This number of observations, which we call the {\em "sampling aperture"}, is a crucial parameter of the problem. We obtain the {\em first} bounds for this mixture-learning problem {\em without imposing any assumptions on the mixture constituents.} We show that efficient learning is possible exactly at the information-theoretically least-possible aperture of $2k-1$. Thus, we achieve near-optimal dependence on $n$ and optimal aperture. While the sample-size required by our algorithm depends exponentially on $k$, we prove that such a dependence is {\em unavoidable} when one considers general mixtures. A sequence of tools contribute to the algorithm, such as concentration results for random matrices, dimension reduction, moment estimations, and sensitivity analysis. version:3
arxiv-1301-6847 | Robust Face Recognition via Block Sparse Bayesian Learning | http://arxiv.org/abs/1301.6847 | id:1301.6847 author:Taiyong Li, Zhilin Zhang category:cs.CV  published:2013-01-29 summary:Face recognition (FR) is an important task in pattern recognition and computer vision. Sparse representation (SR) has been demonstrated to be a powerful framework for FR. In general, an SR algorithm treats each face in a training dataset as a basis function, and tries to find a sparse representation of a test face under these basis functions. The sparse representation coefficients then provide a recognition hint. Early SR algorithms are based on a basic sparse model. Recently, it has been found that algorithms based on a block sparse model can achieve better recognition rates. Based on this model, in this study we use block sparse Bayesian learning (BSBL) to find a sparse representation of a test face for recognition. BSBL is a recently proposed framework, which has many advantages over existing block-sparse-model based algorithms. Experimental results on the Extended Yale B, the AR and the CMU PIE face databases show that using BSBL can achieve better recognition rates and higher robustness than state-of-the-art algorithms in most cases. version:2
arxiv-1212-3268 | Robust image reconstruction from multi-view measurements | http://arxiv.org/abs/1212.3268 | id:1212.3268 author:Gilles Puy, Pierre Vandergheynst category:cs.CV  published:2012-12-13 summary:We propose a novel method to accurately reconstruct a set of images representing a single scene from few linear multi-view measurements. Each observed image is modeled as the sum of a background image and a foreground one. The background image is common to all observed images but undergoes geometric transformations, as the scene is observed from different viewpoints. In this paper, we assume that these geometric transformations are represented by a few parameters, e.g., translations, rotations, affine transformations, etc.. The foreground images differ from one observed image to another, and are used to model possible occlusions of the scene. The proposed reconstruction algorithm estimates jointly the images and the transformation parameters from the available multi-view measurements. The ideal solution of this multi-view imaging problem minimizes a non-convex functional, and the reconstruction technique is an alternating descent method built to minimize this functional. The convergence of the proposed algorithm is studied, and conditions under which the sequence of estimated images and parameters converges to a critical point of the non-convex functional are provided. Finally, the efficiency of the algorithm is demonstrated using numerical simulations for applications such as compressed sensing or super-resolution. version:3
arxiv-1309-4426 | GRED: Graph-Regularized 3D Shape Reconstruction from Highly Anisotropic and Noisy Images | http://arxiv.org/abs/1309.4426 | id:1309.4426 author:Christian Widmer, Philipp Drewe, Xinghua Lou, Shefali Umrania, Stephanie Heinrich, Gunnar Rätsch category:cs.CV  published:2013-09-17 summary:Analysis of microscopy images can provide insight into many biological processes. One particularly challenging problem is cell nuclear segmentation in highly anisotropic and noisy 3D image data. Manually localizing and segmenting each and every cell nuclei is very time consuming, which remains a bottleneck in large scale biological experiments. In this work we present a tool for automated segmentation of cell nuclei from 3D fluorescent microscopic data. Our tool is based on state-of-the-art image processing and machine learning techniques and supports a friendly graphical user interface (GUI). We show that our tool is as accurate as manual annotation but greatly reduces the time for the registration. version:1
arxiv-1309-4385 | Photon counting compressive depth mapping | http://arxiv.org/abs/1309.4385 | id:1309.4385 author:Gregory A. Howland, Daniel J. Lum, Matthew R. Ware, John C. Howell category:physics.optics cs.CV  published:2013-09-17 summary:We demonstrate a compressed sensing, photon counting lidar system based on the single-pixel camera. Our technique recovers both depth and intensity maps from a single under-sampled set of incoherent, linear projections of a scene of interest at ultra-low light levels around 0.5 picowatts. Only two-dimensional reconstructions are required to image a three-dimensional scene. We demonstrate intensity imaging and depth mapping at 256 x 256 pixel transverse resolution with acquisition times as short as 3 seconds. We also show novelty filtering, reconstructing only the difference between two instances of a scene. Finally, we acquire 32 x 32 pixel real-time video for three-dimensional object tracking at 14 frames-per-second. version:1
arxiv-1309-7004 | Calculation of Entailed Rank Constraints in Partially Non-Linear and Cyclic Models | http://arxiv.org/abs/1309.7004 | id:1309.7004 author:Peter L. Spirtes category:cs.AI stat.ML  published:2013-09-17 summary:The Trek Separation Theorem (Sullivant et al. 2010) states necessary and sufficient conditions for a linear directed acyclic graphical model to entail for all possible values of its linear coefficients that the rank of various sub-matrices of the covariance matrix is less than or equal to n, for any given n. In this paper, I extend the Trek Separation Theorem in two ways: I prove that the same necessary and sufficient conditions apply even when the generating model is partially non-linear and contains some cycles. This justifies application of constraint-based causal search algorithms such as the BuildPureClusters algorithm (Silva et al. 2006) for discovering the causal structure of latent variable models to data generated by a wider class of causal models that may contain non-linear and cyclic relations among the latent variables. version:1
arxiv-1303-2912 | Integrated Pre-Processing for Bayesian Nonlinear System Identification with Gaussian Processes | http://arxiv.org/abs/1303.2912 | id:1303.2912 author:Roger Frigola, Carl Edward Rasmussen category:cs.AI cs.RO cs.SY stat.ML  published:2013-03-12 summary:We introduce GP-FNARX: a new model for nonlinear system identification based on a nonlinear autoregressive exogenous model (NARX) with filtered regressors (F) where the nonlinear regression problem is tackled using sparse Gaussian processes (GP). We integrate data pre-processing with system identification into a fully automated procedure that goes from raw data to an identified model. Both pre-processing parameters and GP hyper-parameters are tuned by maximizing the marginal likelihood of the probabilistic model. We obtain a Bayesian model of the system's dynamics which is able to report its uncertainty in regions where the data is scarce. The automated approach, the modeling of uncertainty and its relatively low computational cost make of GP-FNARX a good candidate for applications in robotics and adaptive control. version:3
arxiv-1210-2352 | A notion of continuity in discrete spaces and applications | http://arxiv.org/abs/1210.2352 | id:1210.2352 author:Valerio Capraro category:math.MG cs.CV math.CO math.GN  published:2012-10-08 summary:We propose a notion of continuous path for locally finite metric spaces, taking inspiration from the recent development of A-theory for locally finite connected graphs. We use this notion of continuity to derive an analogue in Z^2 of the Jordan curve theorem and to extend to a quite large class of locally finite metric spaces (containing all finite metric spaces) an inequality for the \ell^p-distortion of a metric space that has been recently proved by Pierre-Nicolas Jolissaint and Alain Valette for finite connected graphs. version:2
arxiv-1309-4168 | Exploiting Similarities among Languages for Machine Translation | http://arxiv.org/abs/1309.4168 | id:1309.4168 author:Tomas Mikolov, Quoc V. Le, Ilya Sutskever category:cs.CL  published:2013-09-17 summary:Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs. version:1
arxiv-1309-4151 | A Non-Local Means Filter for Removing the Poisson Noise | http://arxiv.org/abs/1309.4151 | id:1309.4151 author:Qiyu Jin, Ion Grama, Quansheng Liu category:stat.AP cs.CV  published:2013-09-17 summary:A new image denoising algorithm to deal with the Poisson noise model is given, which is based on the idea of Non-Local Mean. By using the "Oracle" concept, we establish a theorem to show that the Non-Local Means Filter can effectively deal with Poisson noise with some modification. Under the theoretical result, we construct our new algorithm called Non-Local Means Poisson Filter and demonstrate in theory that the filter converges at the usual optimal rate. The filter is as simple as the classic Non-Local Means and the simulation results show that our filter is very competitive. version:1
arxiv-1310-7443 | On Convergent Finite Difference Schemes for Variational - PDE Based Image Processing | http://arxiv.org/abs/1310.7443 | id:1310.7443 author:V. B. S. Prasath, Juan C. Moreno category:cs.CV math.NA 65N06  65N22  68U10 I.4.3  published:2013-09-16 summary:We study an adaptive anisotropic Huber functional based image restoration scheme. By using a combination of L2-L1 regularization functions, an adaptive Huber functional based energy minimization model provides denoising with edge preservation in noisy digital images. We study a convergent finite difference scheme based on continuous piecewise linear functions and use a variable splitting scheme, namely the Split Bregman, to obtain the discrete minimizer. Experimental results are given in image denoising and comparison with additive operator splitting, dual fixed point, and projected gradient schemes illustrate that the best convergence rates are obtained for our algorithm. version:1
arxiv-1309-4111 | Regularized Spectral Clustering under the Degree-Corrected Stochastic Blockmodel | http://arxiv.org/abs/1309.4111 | id:1309.4111 author:Tai Qin, Karl Rohe category:stat.ML cs.LG math.ST stat.TH  published:2013-09-16 summary:Spectral clustering is a fast and popular algorithm for finding clusters in networks. Recently, Chaudhuri et al. (2012) and Amini et al.(2012) proposed inspired variations on the algorithm that artificially inflate the node degrees for improved statistical performance. The current paper extends the previous statistical estimation results to the more canonical spectral clustering algorithm in a way that removes any assumption on the minimum degree and provides guidance on the choice of the tuning parameter. Moreover, our results show how the "star shape" in the eigenvectors--a common feature of empirical networks--can be explained by the Degree-Corrected Stochastic Blockmodel and the Extended Planted Partition model, two statistical models that allow for highly heterogeneous degrees. Throughout, the paper characterizes and justifies several of the variations of the spectral clustering algorithm in terms of these models. version:1
arxiv-1309-4061 | Learning a Loopy Model For Semantic Segmentation Exactly | http://arxiv.org/abs/1309.4061 | id:1309.4061 author:Andreas Christian Mueller, Sven Behnke category:cs.LG cs.CV  published:2013-09-16 summary:Learning structured models using maximum margin techniques has become an indispensable tool for com- puter vision researchers, as many computer vision applications can be cast naturally as an image labeling problem. Pixel-based or superpixel-based conditional random fields are particularly popular examples. Typ- ically, neighborhood graphs, which contain a large number of cycles, are used. As exact inference in loopy graphs is NP-hard in general, learning these models without approximations is usually deemed infeasible. In this work we show that, despite the theoretical hardness, it is possible to learn loopy models exactly in practical applications. To this end, we analyze the use of multiple approximate inference techniques together with cutting plane training of structural SVMs. We show that our proposed method yields exact solutions with an optimality guarantees in a computer vision application, for little additional computational cost. We also propose a dynamic caching scheme to accelerate training further, yielding runtimes that are comparable with approximate methods. We hope that this insight can lead to a reconsideration of the tractability of loopy models in computer vision. version:1
arxiv-1309-4058 | Why SOV might be initially preferred and then lost or recovered? A theoretical framework | http://arxiv.org/abs/1309.4058 | id:1309.4058 author:Ramon Ferrer-i-Cancho category:cs.CL nlin.AO physics.soc-ph q-bio.NC  published:2013-09-16 summary:Little is known about why SOV order is initially preferred and then discarded or recovered. Here we present a framework for understanding these and many related word order phenomena: the diversity of dominant orders, the existence of free words orders, the need of alternative word orders and word order reversions and cycles in evolution. Under that framework, word order is regarded as a multiconstraint satisfaction problem in which at least two constraints are in conflict: online memory minimization and maximum predictability. version:1
arxiv-1309-4035 | Domain and Function: A Dual-Space Model of Semantic Relations and Compositions | http://arxiv.org/abs/1309.4035 | id:1309.4035 author:Peter D. Turney category:cs.CL cs.AI cs.LG H.3.1; I.2.6; I.2.7  published:2013-09-16 summary:Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected. However, up to now, the best models for relations are significantly different from the best models for compositions. In this paper, we introduce a dual-space model that unifies these two tasks. This model matches the performance of the best previous models for relations and compositions. The dual-space model consists of a space for measuring domain similarity and a space for measuring function similarity. Carpenter and wood share the same domain, the domain of carpentry. Mason and stone share the same domain, the domain of masonry. Carpenter and mason share the same function, the function of artisans. Wood and stone share the same function, the function of materials. In the composition dog house, kennel has some domain overlap with both dog and house (the domains of pets and buildings). The function of kennel is similar to the function of house (the function of shelters). By combining domain and function similarities in various ways, we can model relations, compositions, and other aspects of semantics. version:1
arxiv-1309-4024 | The Cyborg Astrobiologist: Matching of Prior Textures by Image Compression for Geological Mapping and Novelty Detection | http://arxiv.org/abs/1309.4024 | id:1309.4024 author:P. C. McGuire, A. Bonnici, K. R. Bruner, C. Gross, J. Ormö, R. A. Smosna, S. Walter, L. Wendt category:cs.CV astro-ph.EP astro-ph.IM cs.LG  published:2013-09-16 summary:(abridged) We describe an image-comparison technique of Heidemann and Ritter that uses image compression, and is capable of: (i) detecting novel textures in a series of images, as well as of: (ii) alerting the user to the similarity of a new image to a previously-observed texture. This image-comparison technique has been implemented and tested using our Astrobiology Phone-cam system, which employs Bluetooth communication to send images to a local laptop server in the field for the image-compression analysis. We tested the system in a field site displaying a heterogeneous suite of sandstones, limestones, mudstones and coalbeds. Some of the rocks are partly covered with lichen. The image-matching procedure of this system performed very well with data obtained through our field test, grouping all images of yellow lichens together and grouping all images of a coal bed together, and giving a 91% accuracy for similarity detection. Such similarity detection could be employed to make maps of different geological units. The novelty-detection performance of our system was also rather good (a 64% accuracy). Such novelty detection may become valuable in searching for new geological units, which could be of astrobiological interest. The image-comparison technique is an unsupervised technique that is not capable of directly classifying an image as containing a particular geological feature; labeling of such geological features is done post facto by human geologists associated with this study, for the purpose of analyzing the system's performance. By providing more advanced capabilities for similarity detection and novelty detection, this image-compression technique could be useful in giving more scientific autonomy to robotic planetary rovers, and in assisting human astronauts in their geological exploration and assessment. version:1
arxiv-1309-3949 | Performance Investigation of Feature Selection Methods | http://arxiv.org/abs/1309.3949 | id:1309.3949 author:Anuj sharma, Shubhamoy Dey category:cs.IR cs.CL cs.LG  published:2013-09-16 summary:Sentiment analysis or opinion mining has become an open research domain after proliferation of Internet and Web 2.0 social media. People express their attitudes and opinions on social media including blogs, discussion forums, tweets, etc. and, sentiment analysis concerns about detecting and extracting sentiment or opinion from online text. Sentiment based text classification is different from topical text classification since it involves discrimination based on expressed opinion on a topic. Feature selection is significant for sentiment analysis as the opinionated text may have high dimensions, which can adversely affect the performance of sentiment analysis classifier. This paper explores applicability of feature selection methods for sentiment analysis and investigates their performance for classification in term of recall, precision and accuracy. Five feature selection methods (Document Frequency, Information Gain, Gain Ratio, Chi Squared, and Relief-F) and three popular sentiment feature lexicons (HM, GI and Opinion Lexicon) are investigated on movie reviews corpus with a size of 2000 documents. The experimental results show that Information Gain gave consistent results and Gain Ratio performs overall best for sentimental feature selection while sentiment lexicons gave poor performance. Furthermore, we found that performance of the classifier depends on appropriate number of representative feature selected from text. version:1
arxiv-1309-3946 | Using Self-Organizing Maps for Sentiment Analysis | http://arxiv.org/abs/1309.3946 | id:1309.3946 author:Anuj Sharma, Shubhamoy Dey category:cs.IR cs.CL cs.NE  published:2013-09-16 summary:Web 2.0 services have enabled people to express their opinions, experience and feelings in the form of user-generated content. Sentiment analysis or opinion mining involves identifying, classifying and aggregating opinions as per their positive or negative polarity. This paper investigates the efficacy of different implementations of Self-Organizing Maps (SOM) for sentiment based visualization and classification of online reviews. Specifically, this paper implements the SOM algorithm for both supervised and unsupervised learning from text documents. The unsupervised SOM algorithm is implemented for sentiment based visualization and classification tasks. For supervised sentiment analysis, a competitive learning algorithm known as Learning Vector Quantization is used. Both algorithms are also compared with their respective multi-pass implementations where a quick rough ordering pass is followed by a fine tuning pass. The experimental results on the online movie review data set show that SOMs are well suited for sentiment based classification and sentiment polarity visualization. version:1
arxiv-1309-3945 | A Neural Network based Approach for Predicting Customer Churn in Cellular Network Services | http://arxiv.org/abs/1309.3945 | id:1309.3945 author:Anuj Sharma, Dr. Prabin Kumar Panigrahi category:cs.NE cs.CE  published:2013-09-16 summary:Marketing literature states that it is more costly to engage a new customer than to retain an existing loyal customer. Churn prediction models are developed by academics and practitioners to effectively manage and control customer churn in order to retain existing customers. As churn management is an important activity for companies to retain loyal customers, the ability to correctly predict customer churn is necessary. As the cellular network services market becoming more competitive, customer churn management has become a crucial task for mobile communication operators. This paper proposes a neural network based approach to predict customer churn in subscription of cellular wireless services. The results of experiments indicate that neural network based approach can predict customer churn. version:1
arxiv-1308-2428 | Hidden Structure and Function in the Lexicon | http://arxiv.org/abs/1308.2428 | id:1308.2428 author:Olivier Picard, Mélanie Lord, Alexandre Blondin-Massé, Odile Marcotte, Marcos Lopes, Stevan Harnad category:cs.CL  published:2013-08-11 summary:How many words are needed to define all the words in a dictionary? Graph-theoretic analysis reveals that about 10% of a dictionary is a unique Kernel of words that define one another and all the rest, but this is not the smallest such subset. The Kernel consists of one huge strongly connected component (SCC), about half its size, the Core, surrounded by many small SCCs, the Satellites. Core words can define one another but not the rest of the dictionary. The Kernel also contains many overlapping Minimal Grounding Sets (MGSs), each about the same size as the Core, each part-Core, part-Satellite. MGS words can define all the rest of the dictionary. They are learned earlier, more concrete and more frequent than the rest of the dictionary. Satellite words, not correlated with age or frequency, are less concrete (more abstract) words that are also needed for full lexical power. version:2
arxiv-1309-3877 | A Metric-learning based framework for Support Vector Machines and Multiple Kernel Learning | http://arxiv.org/abs/1309.3877 | id:1309.3877 author:Huyen Do, Alexandros Kalousis category:cs.LG  published:2013-09-16 summary:Most metric learning algorithms, as well as Fisher's Discriminant Analysis (FDA), optimize some cost function of different measures of within-and between-class distances. On the other hand, Support Vector Machines(SVMs) and several Multiple Kernel Learning (MKL) algorithms are based on the SVM large margin theory. Recently, SVMs have been analyzed from SVM and metric learning, and to develop new algorithms that build on the strengths of each. Inspired by the metric learning interpretation of SVM, we develop here a new metric-learning based SVM framework in which we incorporate metric learning concepts within SVM. We extend the optimization problem of SVM to include some measure of the within-class distance and along the way we develop a new within-class distance measure which is appropriate for SVM. In addition, we adopt the same approach for MKL and show that it can be also formulated as a Mahalanobis metric learning problem. Our end result is a number of SVM/MKL algorithms that incorporate metric learning concepts. We experiment with them on a set of benchmark datasets and observe important predictive performance improvements. version:1
arxiv-1310-7961 | Evaluation the efficiency of artificial bee colony and the firefly algorithm in solving the continuous optimization problem | http://arxiv.org/abs/1310.7961 | id:1310.7961 author:Seyyed Reza Khaze, Isa maleki, Sohrab Hojjatkhah, Ali Bagherinia category:cs.NE cs.AI  published:2013-09-16 summary:Now the Meta-Heuristic algorithms have been used vastly in solving the problem of continuous optimization. In this paper the Artificial Bee Colony (ABC) algorithm and the Firefly Algorithm (FA) are valuated. And for presenting the efficiency of the algorithms and also for more analysis of them, the continuous optimization problems which are of the type of the problems of vast limit of answer and the close optimized points are tested. So, in this paper the efficiency of the ABC algorithm and FA are presented for solving the continuous optimization problems and also the said algorithms are studied from the accuracy in reaching the optimized solution and the resulting time and the reliability of the optimized answer points of view. version:1
arxiv-1309-3848 | SEEDS: Superpixels Extracted via Energy-Driven Sampling | http://arxiv.org/abs/1309.3848 | id:1309.3848 author:Michael Van den Bergh, Xavier Boix, Gemma Roig, Luc Van Gool category:cs.CV  published:2013-09-16 summary:Superpixel algorithms aim to over-segment the image by grouping pixels that belong to the same object. Many state-of-the-art superpixel algorithms rely on minimizing objective functions to enforce color ho- mogeneity. The optimization is accomplished by sophis- ticated methods that progressively build the superpix- els, typically by adding cuts or growing superpixels. As a result, they are computationally too expensive for real-time applications. We introduce a new approach based on a simple hill-climbing optimization. Starting from an initial superpixel partitioning, it continuously refines the superpixels by modifying the boundaries. We define a robust and fast to evaluate energy function, based on enforcing color similarity between the bound- aries and the superpixel color histogram. In a series of experiments, we show that we achieve an excellent com- promise between accuracy and efficiency. We are able to achieve a performance comparable to the state-of- the-art, but in real-time on a single Intel i7 CPU at 2.8GHz. version:1
arxiv-1309-3842 | Estimation of intrinsic volumes from digital grey-scale images | http://arxiv.org/abs/1309.3842 | id:1309.3842 author:Anne Marie Svane category:math.ST cs.CV stat.TH 62H35  published:2013-09-16 summary:Local algorithms are common tools for estimating intrinsic volumes from black-and-white digital images. However, these algorithms are typically biased in the design based setting, even when the resolution tends to infinity. Moreover, images recorded in practice are most often blurred grey-scale images rather than black-and-white. In this paper, an extended definition of local algorithms, applying directly to grey-scale images without thresholding, is suggested. We investigate the asymptotics of these new algorithms when the resolution tends to infinity and apply this to construct estimators for surface area and integrated mean curvature that are asymptotically unbiased in certain natural settings. version:1
arxiv-1310-7448 | An iterative algorithm for computed tomography image reconstruction from limited-angle projections | http://arxiv.org/abs/1310.7448 | id:1310.7448 author:Yuli Sun, Jinxu Tao, Conggui Liu category:cs.CV G.1.1  published:2013-09-16 summary:In application of tomography imaging, limited-angle problem is a quite practical and important issue. In this paper, an iterative reprojection-reconstruction (IRR) algorithm using a modified Papoulis-Gerchberg (PG) iterative scheme is developed for reconstruction from limited-angle projections which contain noise. The proposed algorithm has two iterative update processes, one is the extrapolation of unknown data, and the other is the modification of the known noisy observation data. And the algorithm introduces scaling factors to control the two processes, respectively. The convergence of the algorithm is guaranteed, and the method of choosing the scaling factors is given with energy constraints. The simulation result demonstrates our conclusions and indicates that the algorithm proposed in this paper can obviously improve the reconstruction quality. version:1
arxiv-1305-6143 | Fast and accurate sentiment classification using an enhanced Naive Bayes model | http://arxiv.org/abs/1305.6143 | id:1305.6143 author:Vivek Narayanan, Ishan Arora, Arjun Bhatia category:cs.CL cs.IR cs.LG I.2.7  published:2013-05-27 summary:We have explored different methods of improving the accuracy of a Naive Bayes classifier for sentiment analysis. We observed that a combination of methods like negation handling, word n-grams and feature selection by mutual information results in a significant improvement in accuracy. This implies that a highly accurate and fast sentiment classifier can be built using a simple Naive Bayes model that has linear training and testing time complexities. We achieved an accuracy of 88.80% on the popular IMDB movie reviews dataset. version:2
arxiv-1309-3816 | Multiplicative Approximations, Optimal Hypervolume Distributions, and the Choice of the Reference Point | http://arxiv.org/abs/1309.3816 | id:1309.3816 author:Tobias Friedrich, Frank Neumann, Christian Thyssen category:cs.NE  published:2013-09-16 summary:Many optimization problems arising in applications have to consider several objective functions at the same time. Evolutionary algorithms seem to be a very natural choice for dealing with multi-objective problems as the population of such an algorithm can be used to represent the trade-offs with respect to the given objective functions. In this paper, we contribute to the theoretical understanding of evolutionary algorithms for multi-objective problems. We consider indicator-based algorithms whose goal is to maximize the hypervolume for a given problem by distributing {\mu} points on the Pareto front. To gain new theoretical insights into the behavior of hypervolume-based algorithms we compare their optimization goal to the goal of achieving an optimal multiplicative approximation ratio. Our studies are carried out for different Pareto front shapes of bi-objective problems. For the class of linear fronts and a class of convex fronts, we prove that maximizing the hypervolume gives the best possible approximation ratio when assuming that the extreme points have to be included in both distributions of the points on the Pareto front. Furthermore, we investigate the choice of the reference point on the approximation behavior of hypervolume-based approaches and examine Pareto fronts of different shapes by numerical calculations. version:1
arxiv-1309-3809 | Visual-Semantic Scene Understanding by Sharing Labels in a Context Network | http://arxiv.org/abs/1309.3809 | id:1309.3809 author:Ishani Chakraborty, Ahmed Elgammal category:cs.CV cs.LG stat.ML  published:2013-09-16 summary:We consider the problem of naming objects in complex, natural scenes containing widely varying object appearance and subtly different names. Informed by cognitive research, we propose an approach based on sharing context based object hypotheses between visual and lexical spaces. To this end, we present the Visual Semantic Integration Model (VSIM) that represents object labels as entities shared between semantic and visual contexts and infers a new image by updating labels through context switching. At the core of VSIM is a semantic Pachinko Allocation Model and a visual nearest neighbor Latent Dirichlet Allocation Model. For inference, we derive an iterative Data Augmentation algorithm that pools the label probabilities and maximizes the joint label posterior of an image. Our model surpasses the performance of state-of-art methods in several visual tasks on the challenging SUN09 dataset. version:1
arxiv-1309-3699 | Local Support Vector Machines:Formulation and Analysis | http://arxiv.org/abs/1309.3699 | id:1309.3699 author:Ravi Ganti, Alexander Gray category:stat.ML  published:2013-09-14 summary:We provide a formulation for Local Support Vector Machines (LSVMs) that generalizes previous formulations, and brings out the explicit connections to local polynomial learning used in nonparametric estimation literature. We investigate the simplest type of LSVMs called Local Linear Support Vector Machines (LLSVMs). For the first time we establish conditions under which LLSVMs make Bayes consistent predictions at each test point $x_0$. We also establish rates at which the local risk of LLSVMs converges to the minimum value of expected local risk at each point $x_0$. Using stability arguments we establish generalization error bounds for LLSVMs. version:1
arxiv-1309-3697 | Group Learning and Opinion Diffusion in a Broadcast Network | http://arxiv.org/abs/1309.3697 | id:1309.3697 author:Yang Liu, Mingyan Liu category:cs.LG  published:2013-09-14 summary:We analyze the following group learning problem in the context of opinion diffusion: Consider a network with $M$ users, each facing $N$ options. In a discrete time setting, at each time step, each user chooses $K$ out of the $N$ options, and receive randomly generated rewards, whose statistics depend on the options chosen as well as the user itself, and are unknown to the users. Each user aims to maximize their expected total rewards over a certain time horizon through an online learning process, i.e., a sequence of exploration (sampling the return of each option) and exploitation (selecting empirically good options) steps. Within this context we consider two group learning scenarios, (1) users with uniform preferences and (2) users with diverse preferences, and examine how a user should construct its learning process to best extract information from other's decisions and experiences so as to maximize its own reward. Performance is measured in {\em weak regret}, the difference between the user's total reward and the reward from a user-specific best single-action policy (i.e., always selecting the set of options generating the highest mean rewards for this user). Within each scenario we also consider two cases: (i) when users exchange full information, meaning they share the actual rewards they obtained from their choices, and (ii) when users exchange limited information, e.g., only their choices but not rewards obtained from these choices. version:1
arxiv-1309-3676 | Optimized projections for compressed sensing via rank-constrained nearest correlation matrix | http://arxiv.org/abs/1309.3676 | id:1309.3676 author:Nicolae Cleju category:cs.IT cs.LG math.IT stat.ML  published:2013-09-14 summary:Optimizing the acquisition matrix is useful for compressed sensing of signals that are sparse in overcomplete dictionaries, because the acquisition matrix can be adapted to the particular correlations of the dictionary atoms. In this paper a novel formulation of the optimization problem is proposed, in the form of a rank-constrained nearest correlation matrix problem. Furthermore, improvements for three existing optimization algorithms are introduced, which are shown to be particular instances of the proposed formulation. Simulation results show notable improvements and superior robustness in sparse signal recovery. version:1
arxiv-1309-3533 | Mixed Membership Models for Time Series | http://arxiv.org/abs/1309.3533 | id:1309.3533 author:Emily B. Fox, Michael I. Jordan category:stat.ME cs.LG stat.ML  published:2013-09-13 summary:In this article we discuss some of the consequences of the mixed membership perspective on time series analysis. In its most abstract form, a mixed membership model aims to associate an individual entity with some set of attributes based on a collection of observed data. Although much of the literature on mixed membership models considers the setting in which exchangeable collections of data are associated with each member of a set of entities, it is equally natural to consider problems in which an entire time series is viewed as an entity and the goal is to characterize the time series in terms of a set of underlying dynamic attributes or "dynamic regimes". Indeed, this perspective is already present in the classical hidden Markov model, where the dynamic regimes are referred to as "states", and the collection of states realized in a sample path of the underlying process can be viewed as a mixed membership characterization of the observed time series. Our goal here is to review some of the richer modeling possibilities for time series that are provided by recent developments in the mixed membership framework. version:1
arxiv-1307-3043 | A two-layer Conditional Random Field for the classification of partially occluded objects | http://arxiv.org/abs/1307.3043 | id:1307.3043 author:Sergey Kosov, Pushmeet Kohli, Franz Rottensteiner, Christian Heipke category:cs.CV  published:2013-07-11 summary:Conditional Random Fields (CRF) are among the most popular techniques for image labelling because of their flexibility in modelling dependencies between the labels and the image features. This paper proposes a novel CRF-framework for image labeling problems which is capable to classify partially occluded objects. Our approach is evaluated on aerial near-vertical images as well as on urban street-view images and compared with another methods. version:2
arxiv-1309-3425 | A method for nose-tip based 3D face registration using maximum intensity algorithm | http://arxiv.org/abs/1309.3425 | id:1309.3425 author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak kr. Basu category:cs.CV  published:2013-09-13 summary:In this paper we present a novel technique of registering 3D images across pose. In this context, we have taken into account the images which are aligned across X, Y and Z axes. We have first determined the angle across which the image is rotated with respect to X, Y and Z axes and then translation is performed on the images. After testing the proposed method on 472 images from the FRAV3D database, the method correctly registers 358 images thus giving a performance rate of 75.84%. version:1
arxiv-1309-3418 | A Novel Approach in detecting pose orientation of a 3D face required for face | http://arxiv.org/abs/1309.3418 | id:1309.3418 author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV  published:2013-09-13 summary:In this paper we present a novel approach that takes as input a 3D image and gives as output its pose i.e. it tells whether the face is oriented with respect the X, Y or Z axes with angles of rotation up to 40 degree. All the experiments have been performed on the FRAV3D Database. After applying the proposed algorithm to the 3D facial surface we have obtained i.e. on 848 3D face images our method detected the pose correctly for 566 face images,thus giving an approximately 67 % of correct pose detection. version:1
arxiv-1309-3233 | Efficient Orthogonal Tensor Decomposition, with an Application to Latent Variable Model Learning | http://arxiv.org/abs/1309.3233 | id:1309.3233 author:Franz J. Király category:stat.ML cs.LG math.ST stat.TH  published:2013-09-12 summary:Decomposing tensors into orthogonal factors is a well-known task in statistics, machine learning, and signal processing. We study orthogonal outer product decompositions where the factors in the summands in the decomposition are required to be orthogonal across summands, by relating this orthogonal decomposition to the singular value decompositions of the flattenings. We show that it is a non-trivial assumption for a tensor to have such an orthogonal decomposition, and we show that it is unique (up to natural symmetries) in case it exists, in which case we also demonstrate how it can be efficiently and reliably obtained by a sequence of singular value decompositions. We demonstrate how the factoring algorithm can be applied for parameter identification in latent variable and mixture models. version:1
arxiv-1309-3214 | Modeling Based on Elman Wavelet Neural Network for Class-D Power Amplifiers | http://arxiv.org/abs/1309.3214 | id:1309.3214 author:Li Wang, Jie Shao, Yaqin Zhong, Weisong Zhao, Reza Malekian category:cs.NE  published:2013-09-12 summary:In Class-D Power Amplifiers (CDPAs), the power supply noise can intermodulate with the input signal, manifesting into power-supply induced intermodulation distortion (PS-IMD) and due to the memory effects of the system, there exist asymmetries in the PS-IMDs. In this paper, a new behavioral modeling based on the Elman Wavelet Neural Network (EWNN) is proposed to study the nonlinear distortion of the CDPAs. In EWNN model, the Morlet wavelet functions are employed as the activation function and there is a normalized operation in the hidden layer, the modification of the scale factor and translation factor in the wavelet functions are ignored to avoid the fluctuations of the error curves. When there are 30 neurons in the hidden layer, to achieve the same square sum error (SSE) $\epsilon_{min}=10^{-3}$, EWNN needs 31 iteration steps, while the basic Elman neural network (BENN) model needs 86 steps. The Volterra-Laguerre model has 605 parameters to be estimated but still can't achieve the same magnitude accuracy of EWNN. Simulation results show that the proposed approach of EWNN model has fewer parameters and higher accuracy than the Volterra-Laguerre model and its convergence rate is much faster than the BENN model. version:1
arxiv-1305-1396 | A new framework for optimal classifier design | http://arxiv.org/abs/1305.1396 | id:1305.1396 author:Matías Di Martino, Guzman Hernández, Marcelo Fiori, Alicia Fernández category:cs.CV cs.LG stat.ML  published:2013-05-07 summary:The use of alternative measures to evaluate classifier performance is gaining attention, specially for imbalanced problems. However, the use of these measures in the classifier design process is still unsolved. In this work we propose a classifier designed specifically to optimize one of these alternative measures, namely, the so-called F-measure. Nevertheless, the technique is general, and it can be used to optimize other evaluation measures. An algorithm to train the novel classifier is proposed, and the numerical scheme is tested with several databases, showing the optimality and robustness of the presented classifier. version:2
arxiv-1306-3946 | Multi-view in Lensless Compressive Imaging | http://arxiv.org/abs/1306.3946 | id:1306.3946 author:Hong Jiang, Gang Huang, Paul Wilford category:cs.IT cs.CV math.IT  published:2013-06-17 summary:Multi-view images are acquired by a lensless compressive imaging architecture, which consists of an aperture assembly and multiple sensors. The aperture assembly consists of a two dimensional array of aperture elements whose transmittance can be individually controlled to implement a compressive sensing matrix. For each transmittance pattern of the aperture assembly, each of the sensors takes a measurement. The measurement vectors from the multiple sensors represent multi-view images of the same scene. We present theoretical framework for multi-view reconstruction and experimental results for enhancing quality of image using multi-view. version:2
arxiv-1309-3117 | Convex relaxations of structured matrix factorizations | http://arxiv.org/abs/1309.3117 | id:1309.3117 author:Francis Bach category:cs.LG math.OC  published:2013-09-12 summary:We consider the factorization of a rectangular matrix $X $ into a positive linear combination of rank-one factors of the form $u v^\top$, where $u$ and $v$ belongs to certain sets $\mathcal{U}$ and $\mathcal{V}$, that may encode specific structures regarding the factors, such as positivity or sparsity. In this paper, we show that computing the optimal decomposition is equivalent to computing a certain gauge function of $X$ and we provide a detailed analysis of these gauge functions and their polars. Since these gauge functions are typically hard to compute, we present semi-definite relaxations and several algorithms that may recover approximate decompositions with approximation guarantees. We illustrate our results with simulations on finding decompositions with elements in $\{0,1\}$. As side contributions, we present a detailed analysis of variational quadratic representations of norms as well as a new iterative basis pursuit algorithm that can deal with inexact first-order oracles. version:1
arxiv-1309-3103 | Temporal Autoencoding Improves Generative Models of Time Series | http://arxiv.org/abs/1309.3103 | id:1309.3103 author:Chris Häusler, Alex Susemihl, Martin P Nawrot, Manfred Opper category:stat.ML cs.LG  published:2013-09-12 summary:Restricted Boltzmann Machines (RBMs) are generative models which can learn useful representations from samples of a dataset in an unsupervised fashion. They have been widely employed as an unsupervised pre-training method in machine learning. RBMs have been modified to model time series in two main ways: The Temporal RBM stacks a number of RBMs laterally and introduces temporal dependencies between the hidden layer units; The Conditional RBM, on the other hand, considers past samples of the dataset as a conditional bias and learns a representation which takes these into account. Here we propose a new training method for both the TRBM and the CRBM, which enforces the dynamic structure of temporal datasets. We do so by treating the temporal models as denoising autoencoders, considering past frames of the dataset as corrupted versions of the present frame and minimizing the reconstruction error of the present data by the model. We call this approach Temporal Autoencoding. This leads to a significant improvement in the performance of both models in a filling-in-frames task across a number of datasets. The error reduction for motion capture data is 56\% for the CRBM and 80\% for the TRBM. Taking the posterior mean prediction instead of single samples further improves the model's estimates, decreasing the error by as much as 91\% for the CRBM on motion capture data. We also trained the model to perform forecasting on a large number of datasets and have found TA pretraining to consistently improve the performance of the forecasts. Furthermore, by looking at the prediction error across time, we can see that this improvement reflects a better representation of the dynamics of the data as opposed to a bias towards reconstructing the observed data on a short time scale. version:1
arxiv-1212-1245 | Distributed Adaptive Networks: A Graphical Evolutionary Game-Theoretic View | http://arxiv.org/abs/1212.1245 | id:1212.1245 author:Chunxiao Jiang, Yan Chen, K. J. Ray Liu category:cs.GT cs.LG  published:2012-12-06 summary:Distributed adaptive filtering has been considered as an effective approach for data processing and estimation over distributed networks. Most existing distributed adaptive filtering algorithms focus on designing different information diffusion rules, regardless of the nature evolutionary characteristic of a distributed network. In this paper, we study the adaptive network from the game theoretic perspective and formulate the distributed adaptive filtering problem as a graphical evolutionary game. With the proposed formulation, the nodes in the network are regarded as players and the local combiner of estimation information from different neighbors is regarded as different strategies selection. We show that this graphical evolutionary game framework is very general and can unify the existing adaptive network algorithms. Based on this framework, as examples, we further propose two error-aware adaptive filtering algorithms. Moreover, we use graphical evolutionary game theory to analyze the information diffusion process over the adaptive networks and evolutionarily stable strategy of the system. Finally, simulation results are shown to verify the effectiveness of our analysis and proposed methods. version:2
arxiv-1309-2895 | Sparse and Functional Principal Components Analysis | http://arxiv.org/abs/1309.2895 | id:1309.2895 author:Genevera I. Allen category:stat.ML  published:2013-09-11 summary:Regularized principal components analysis, especially Sparse PCA and Functional PCA, has become widely used for dimension reduction in high-dimensional settings. Many examples of massive data, however, may benefit from estimating both sparse AND functional factors. These include neuroimaging data where there are discrete brain regions of activation (sparsity) but these regions tend to be smooth spatially (functional). Here, we introduce an optimization framework that can encourage both sparsity and smoothness of the row and/or column PCA factors. This framework generalizes many of the existing approaches to Sparse PCA, Functional PCA and two-way Sparse PCA and Functional PCA, as these are all special cases of our method. In particular, our method permits flexible combinations of sparsity and smoothness that lead to improvements in feature selection and signal recovery as well as more interpretable PCA factors. We demonstrate our method on simulated data and a neuroimaging example on EEG data. This work provides a unified framework for regularized PCA that can form the foundation for a cohesive approach to regularization in high-dimensional multivariate analysis. version:1
arxiv-1309-1007 | Concentration in unbounded metric spaces and algorithmic stability | http://arxiv.org/abs/1309.1007 | id:1309.1007 author:Aryeh Kontorovich category:math.PR cs.LG math.FA 60D99  published:2013-09-04 summary:We prove an extension of McDiarmid's inequality for metric spaces with unbounded diameter. To this end, we introduce the notion of the {\em subgaussian diameter}, which is a distribution-dependent refinement of the metric diameter. Our technique provides an alternative approach to that of Kutin and Niyogi's method of weakly difference-bounded functions, and yields nontrivial, dimension-free results in some interesting cases where the former does not. As an application, we give apparently the first generalization bound in the algorithmic stability setting that holds for unbounded loss functions. We furthermore extend our concentration inequality to strongly mixing processes. version:2
arxiv-1309-2853 | General Purpose Textual Sentiment Analysis and Emotion Detection Tools | http://arxiv.org/abs/1309.2853 | id:1309.2853 author:Alexandre Denis, Samuel Cruz-Lara, Nadia Bellalem category:cs.CL  published:2013-09-11 summary:Textual sentiment analysis and emotion detection consists in retrieving the sentiment or emotion carried by a text or document. This task can be useful in many domains: opinion mining, prediction, feedbacks, etc. However, building a general purpose tool for doing sentiment analysis and emotion detection raises a number of issues, theoretical issues like the dependence to the domain or to the language but also pratical issues like the emotion representation for interoperability. In this paper we present our sentiment/emotion analysis tools, the way we propose to circumvent the di culties and the applications they are used for. version:1
arxiv-1309-2848 | High-dimensional cluster analysis with the Masked EM Algorithm | http://arxiv.org/abs/1309.2848 | id:1309.2848 author:Shabnam N. Kadir, Dan F. M. Goodman, Kenneth D. Harris category:q-bio.QM cs.LG q-bio.NC stat.AP  published:2013-09-11 summary:Cluster analysis faces two problems in high dimensions: first, the `curse of dimensionality' that can lead to overfitting and poor generalization performance; and second, the sheer time taken for conventional algorithms to process large amounts of high-dimensional data. In many applications, only a small subset of features provide information about the cluster membership of any one data point, however this informative feature subset may not be the same for all data points. Here we introduce a `Masked EM' algorithm for fitting mixture of Gaussians models in such cases. We show that the algorithm performs close to optimally on simulated Gaussian data, and in an application of `spike sorting' of high channel-count neuronal recordings. version:1
arxiv-1106-2429 | Efficient Transductive Online Learning via Randomized Rounding | http://arxiv.org/abs/1106.2429 | id:1106.2429 author:Nicolò Cesa-Bianchi, Ohad Shamir category:cs.LG stat.ML  published:2011-06-13 summary:Most traditional online learning algorithms are based on variants of mirror descent or follow-the-leader. In this paper, we present an online algorithm based on a completely different approach, tailored for transductive settings, which combines "random playout" and randomized rounding of loss subgradients. As an application of our approach, we present the first computationally efficient online algorithm for collaborative filtering with trace-norm constrained matrices. As a second application, we solve an open question linking batch learning and transductive online learning version:4
arxiv-1309-2765 | Enhancements of Multi-class Support Vector Machine Construction from Binary Learners using Generalization Performance | http://arxiv.org/abs/1309.2765 | id:1309.2765 author:Patoomsiri Songsiri, Thimaporn Phetkaew, Boonserm Kijsirikul category:cs.LG stat.ML  published:2013-09-11 summary:We propose several novel methods for enhancing the multi-class SVMs by applying the generalization performance of binary classifiers as the core idea. This concept will be applied on the existing algorithms, i.e., the Decision Directed Acyclic Graph (DDAG), the Adaptive Directed Acyclic Graphs (ADAG), and Max Wins. Although in the previous approaches there have been many attempts to use some information such as the margin size and the number of support vectors as performance estimators for binary SVMs, they may not accurately reflect the actual performance of the binary SVMs. We show that the generalization ability evaluated via a cross-validation mechanism is more suitable to directly extract the actual performance of binary SVMs. Our methods are built around this performance measure, and each of them is crafted to overcome the weakness of the previous algorithm. The proposed methods include the Reordering Adaptive Directed Acyclic Graph (RADAG), Strong Elimination of the classifiers (SE), Weak Elimination of the classifiers (WE), and Voting based Candidate Filtering (VCF). Experimental results demonstrate that our methods give significantly higher accuracy than all of the traditional ones. Especially, WE provides significantly superior results compared to Max Wins which is recognized as the state of the art algorithm in terms of both accuracy and classification speed with two times faster in average. version:1
arxiv-1309-2752 | Robust Periocular Recognition By Fusing Sparse Representations of Color and Geometry Information | http://arxiv.org/abs/1309.2752 | id:1309.2752 author:Juan C. Moreno, V. B. S. Prasath, Gil Santos, Hugo Proenca category:cs.CV 65F22  65F50  94A08  published:2013-09-11 summary:In this paper, we propose a re-weighted elastic net (REN) model for biometric recognition. The new model is applied to data separated into geometric and color spatial components. The geometric information is extracted using a fast cartoon - texture decomposition model based on a dual formulation of the total variation norm allowing us to carry information about the overall geometry of images. Color components are defined using linear and nonlinear color spaces, namely the red-green-blue (RGB), chromaticity-brightness (CB) and hue-saturation-value (HSV). Next, according to a Bayesian fusion-scheme, sparse representations for classification purposes are obtained. The scheme is numerically solved using a gradient projection (GP) algorithm. In the empirical validation of the proposed model, we have chosen the periocular region, which is an emerging trait known for its robustness against low quality data. Our results were obtained in the publicly available UBIRIS.v2 data set and show consistent improvements in recognition effectiveness when compared to related state-of-the-art techniques. version:1
arxiv-1309-2593 | Maximizing submodular functions using probabilistic graphical models | http://arxiv.org/abs/1309.2593 | id:1309.2593 author:K. S. Sesh Kumar, Francis Bach category:cs.LG math.OC  published:2013-09-10 summary:We consider the problem of maximizing submodular functions; while this problem is known to be NP-hard, several numerically efficient local search techniques with approximation guarantees are available. In this paper, we propose a novel convex relaxation which is based on the relationship between submodular functions, entropies and probabilistic graphical models. In a graphical model, the entropy of the joint distribution decomposes as a sum of marginal entropies of subsets of variables; moreover, for any distribution, the entropy of the closest distribution factorizing in the graphical model provides an bound on the entropy. For directed graphical models, this last property turns out to be a direct consequence of the submodularity of the entropy function, and allows the generalization of graphical-model-based upper bounds to any submodular functions. These upper bounds may then be jointly maximized with respect to a set, while minimized with respect to the graph, leading to a convex variational inference scheme for maximizing submodular functions, based on outer approximations of the marginal polytope and maximum likelihood bounded treewidth structures. By considering graphs of increasing treewidths, we may then explore the trade-off between computational complexity and tightness of the relaxation. We also present extensions to constrained problems and maximizing the difference of submodular functions, which include all possible set functions. version:1
arxiv-1309-1649 | Preparing Korean Data for the Shared Task on Parsing Morphologically Rich Languages | http://arxiv.org/abs/1309.1649 | id:1309.1649 author:Jinho D. Choi category:cs.CL  published:2013-09-06 summary:This document gives a brief description of Korean data prepared for the SPMRL 2013 shared task. A total of 27,363 sentences with 350,090 tokens are used for the shared task. All constituent trees are collected from the KAIST Treebank and transformed to the Penn Treebank style. All dependency trees are converted from the transformed constituent trees using heuristics and labeling rules de- signed specifically for the KAIST Treebank. In addition to the gold-standard morphological analysis provided by the KAIST Treebank, two sets of automatic morphological analysis are provided for the shared task, one is generated by the HanNanum morphological analyzer, and the other is generated by the Sejong morphological analyzer. version:2
arxiv-1309-2506 | A multi-stream hmm approach to offline handwritten arabic word recognition | http://arxiv.org/abs/1309.2506 | id:1309.2506 author:Ahlam Maqqor, Akram Halli, Khaled Satori category:cs.CV  published:2013-09-10 summary:In This paper we presented new approach for cursive Arabic text recognition system. The objective is to propose methodology analytical offline recognition of handwritten Arabic for rapid implementation. The first part in the writing recognition system is the preprocessing phase is the preprocessing phase to prepare the data was introduces and extracts a set of simple statistical features by two methods : from a window which is sliding long that text line the right to left and the approach VH2D (consists in projecting every character on the abscissa, on the ordinate and the diagonals 45{\deg} and 135{\deg}) . It then injects the resulting feature vectors to Hidden Markov Model (HMM) and combined the two HMM by multi-stream approach. version:1
arxiv-1309-2505 | Compressed Sensing for Block-Sparse Smooth Signals | http://arxiv.org/abs/1309.2505 | id:1309.2505 author:Shahzad Gishkori, Geert Leus category:stat.ML cs.IT math.IT math.ST stat.TH  published:2013-09-10 summary:We present reconstruction algorithms for smooth signals with block sparsity from their compressed measurements. We tackle the issue of varying group size via group-sparse least absolute shrinkage selection operator (LASSO) as well as via latent group LASSO regularizations. We achieve smoothness in the signal via fusion. We develop low-complexity solvers for our proposed formulations through the alternating direction method of multipliers. version:1
arxiv-1306-4650 | Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization | http://arxiv.org/abs/1306.4650 | id:1306.4650 author:Julien Mairal category:stat.ML cs.LG math.OC  published:2013-06-19 summary:Majorization-minimization algorithms consist of iteratively minimizing a majorizing surrogate of an objective function. Because of its simplicity and its wide applicability, this principle has been very popular in statistics and in signal processing. In this paper, we intend to make this principle scalable. We introduce a stochastic majorization-minimization scheme which is able to deal with large-scale or possibly infinite data sets. When applied to convex optimization problems under suitable assumptions, we show that it achieves an expected convergence rate of $O(1/\sqrt{n})$ after $n$ iterations, and of $O(1/n)$ for strongly convex functions. Equally important, our scheme almost surely converges to stationary points for a large class of non-convex problems. We develop several efficient algorithms based on our framework. First, we propose a new stochastic proximal gradient method, which experimentally matches state-of-the-art solvers for large-scale $\ell_1$-logistic regression. Second, we develop an online DC programming algorithm for non-convex sparse estimation. Finally, we demonstrate the effectiveness of our approach for solving large-scale structured matrix factorization problems. version:2
arxiv-1309-2471 | Implementation of nlization framework for verbs, pronouns and determiners with eugene | http://arxiv.org/abs/1309.2471 | id:1309.2471 author:Harinder Singh, Parteek Kumar category:cs.CL  published:2013-09-10 summary:UNL system is designed and implemented by a nonprofit organization, UNDL Foundation at Geneva in 1999. UNL applications are application softwares that allow end users to accomplish natural language tasks, such as translating, summarizing, retrieving or extracting information, etc. Two major web based application softwares are Interactive ANalyzer (IAN), which is a natural language analysis system. It represents natural language sentences as semantic networks in the UNL format. Other application software is dEep-to-sUrface GENErator (EUGENE), which is an open-source interactive NLizer. It generates natural language sentences out of semantic networks represented in the UNL format. In this paper, NLization framework with EUGENE is focused, while using UNL system for accomplishing the task of machine translation. In whole NLization process, EUGENE takes a UNL input and delivers an output in natural language without any human intervention. It is language-independent and has to be parametrized to the natural language input through a dictionary and a grammar, provided as separate interpretable files. In this paper, it is explained that how UNL input is syntactically and semantically analyzed with the UNL-NL T-Grammar for NLization of UNL sentences involving verbs, pronouns and determiners for Punjabi natural language. version:1
arxiv-1309-2094 | The Linearized Bregman Method via Split Feasibility Problems: Analysis and Generalizations | http://arxiv.org/abs/1309.2094 | id:1309.2094 author:Dirk A. Lorenz, Frank Schöpfer, Stephan Wenger category:math.OC cs.CV cs.NA math.NA 68U10  65K10  90C25  published:2013-09-09 summary:The linearized Bregman method is a method to calculate sparse solutions to systems of linear equations. We formulate this problem as a split feasibility problem, propose an algorithmic framework based on Bregman projections and prove a general convergence result for this framework. Convergence of the linearized Bregman method will be obtained as a special case. Our approach also allows for several generalizations such as other objective functions, incremental iterations, incorporation of non-gaussian noise models or box constraints. version:2
arxiv-1309-2350 | Exponentially Fast Parameter Estimation in Networks Using Distributed Dual Averaging | http://arxiv.org/abs/1309.2350 | id:1309.2350 author:Shahin Shahrampour, Ali Jadbabaie category:cs.LG cs.SI math.OC stat.ML  published:2013-09-10 summary:In this paper we present an optimization-based view of distributed parameter estimation and observational social learning in networks. Agents receive a sequence of random, independent and identically distributed (i.i.d.) signals, each of which individually may not be informative about the underlying true state, but the signals together are globally informative enough to make the true state identifiable. Using an optimization-based characterization of Bayesian learning as proximal stochastic gradient descent (with Kullback-Leibler divergence from a prior as a proximal function), we show how to efficiently use a distributed, online variant of Nesterov's dual averaging method to solve the estimation with purely local information. When the true state is globally identifiable, and the network is connected, we prove that agents eventually learn the true parameter using a randomized gossip scheme. We demonstrate that with high probability the convergence is exponentially fast with a rate dependent on the KL divergence of observations under the true state from observations under the second likeliest state. Furthermore, our work also highlights the possibility of learning under continuous adaptation of network which is a consequence of employing constant, unit stepsize for the algorithm. version:1
arxiv-1309-2303 | Spectral Clustering with Imbalanced Data | http://arxiv.org/abs/1309.2303 | id:1309.2303 author:Jing Qian, Venkatesh Saligrama category:stat.ML  published:2013-09-09 summary:Spectral clustering is sensitive to how graphs are constructed from data particularly when proximal and imbalanced clusters are present. We show that Ratio-Cut (RCut) or normalized cut (NCut) objectives are not tailored to imbalanced data since they tend to emphasize cut sizes over cut values. We propose a graph partitioning problem that seeks minimum cut partitions under minimum size constraints on partitions to deal with imbalanced data. Our approach parameterizes a family of graphs, by adaptively modulating node degrees on a fixed node set, to yield a set of parameter dependent cuts reflecting varying levels of imbalance. The solution to our problem is then obtained by optimizing over these parameters. We present rigorous limit cut analysis results to justify our approach. We demonstrate the superiority of our method through unsupervised and semi-supervised experiments on synthetic and real data sets. version:1
arxiv-1309-2240 | Contour Manifolds and Optimal Transport | http://arxiv.org/abs/1309.2240 | id:1309.2240 author:Bernhard Schmitzer, Christoph Schnörr category:math.DG cs.CV  published:2013-09-09 summary:Describing shapes by suitable measures in object segmentation, as proposed in [24], allows to combine the advantages of the representations as parametrized contours and indicator functions. The pseudo-Riemannian structure of optimal transport can be used to model shapes in ways similar as with contours, while the Kantorovich functional enables the application of convex optimization methods for global optimality of the segmentation functional. In this paper we provide a mathematical study of the shape measure representation and its relation to the contour description. In particular we show that the pseudo-Riemannian structure of optimal transport, when restricted to the set of shape measures, yields a manifold which is diffeomorphic to the manifold of closed contours. A discussion of the metric induced by optimal transport and the corresponding geodesic equation is given. version:1
arxiv-1309-2183 | Application of Artificial Neural Networks in Estimating Participation in Elections | http://arxiv.org/abs/1309.2183 | id:1309.2183 author:Seyyed Reza Khaze, Mohammad Masdari, Sohrab Hojjatkhah category:cs.NE cs.CY  published:2013-09-09 summary:It is approved that artificial neural networks can be considerable effective in anticipating and analyzing flows in which traditional methods and statics are not able to solve. in this article, by using two-layer feedforward network with tan-sigmoid transmission function in input and output layers, we can anticipate participation rate of public in kohgiloye and boyerahmad province in future presidential election of islamic republic of iran with 91% accuracy. the assessment standards of participation such as confusion matrix and roc diagrams have been approved our claims. version:1
arxiv-1009-5358 | Task-Driven Dictionary Learning | http://arxiv.org/abs/1009.5358 | id:1009.5358 author:Julien Mairal, Francis Bach, Jean Ponce category:stat.ML  published:2010-09-27 summary:Modeling data with linear combinations of a few elements from a learned dictionary has been the focus of much recent research in machine learning, neuroscience and signal processing. For signals such as natural images that admit such sparse representations, it is now well established that these models are well suited to restoration tasks. In this context, learning the dictionary amounts to solving a large-scale matrix factorization problem, which can be done efficiently with classical optimization tools. The same approach has also been used for learning features from data for other purposes, e.g., image classification, but tuning the dictionary in a supervised way for these tasks has proven to be more difficult. In this paper, we present a general formulation for supervised dictionary learning adapted to a wide variety of tasks, and present an efficient algorithm for solving the corresponding optimization problem. Experiments on handwritten digit classification, digital art identification, nonlinear inverse image problems, and compressed sensing demonstrate that our approach is effective in large-scale settings, and is well suited to supervised and semi-supervised classification, as well as regression tasks for data that admit sparse representations. version:2
arxiv-1309-2084 | Real-Time and Continuous Hand Gesture Spotting: an Approach Based on Artificial Neural Networks | http://arxiv.org/abs/1309.2084 | id:1309.2084 author:Pedro Neto, Dário Pereira, Norberto Pires, Paulo Moreira category:cs.RO cs.CV  published:2013-09-09 summary:New and more natural human-robot interfaces are of crucial interest to the evolution of robotics. This paper addresses continuous and real-time hand gesture spotting, i.e., gesture segmentation plus gesture recognition. Gesture patterns are recognized by using artificial neural networks (ANNs) specifically adapted to the process of controlling an industrial robot. Since in continuous gesture recognition the communicative gestures appear intermittently with the noncommunicative, we are proposing a new architecture with two ANNs in series to recognize both kinds of gesture. A data glove is used as interface technology. Experimental results demonstrated that the proposed solution presents high recognition rates (over 99% for a library of ten gestures and over 96% for a library of thirty gestures), low training and learning time and a good capacity to generalize from particular situations. version:1
arxiv-1309-2080 | Structure Learning of Probabilistic Logic Programs by Searching the Clause Space | http://arxiv.org/abs/1309.2080 | id:1309.2080 author:Elena Bellodi, Fabrizio Riguzzi category:cs.LG cs.AI  published:2013-09-09 summary:Learning probabilistic logic programming languages is receiving an increasing attention and systems are available for learning the parameters (PRISM, LeProbLog, LFI-ProbLog and EMBLEM) or both the structure and the parameters (SEM-CP-logic and SLIPCASE) of these languages. In this paper we present the algorithm SLIPCOVER for "Structure LearnIng of Probabilistic logic programs by searChing OVER the clause space". It performs a beam search in the space of probabilistic clauses and a greedy search in the space of theories, using the log likelihood of the data as the guiding heuristics. To estimate the log likelihood SLIPCOVER performs Expectation Maximization with EMBLEM. The algorithm has been tested on five real world datasets and compared with SLIPCASE, SEM-CP-logic, Aleph and two algorithms for learning Markov Logic Networks (Learning using Structural Motifs (LSM) and ALEPH++ExactL1). SLIPCOVER achieves higher areas under the precision-recall and ROC curves in most cases. version:1
arxiv-1309-2057 | Single image super resolution in spatial and wavelet domain | http://arxiv.org/abs/1309.2057 | id:1309.2057 author:Sapan Naik, Nikunj Patel category:cs.CV  published:2013-09-09 summary:Recently single image super resolution is very important research area to generate high resolution image from given low resolution image. Algorithms of single image resolution are mainly based on wavelet domain and spatial domain. Filters support to model the regularity of natural images is exploited in wavelet domain while edges of images get sharp during up sampling in spatial domain. Here single image super resolution algorithm is presented which based on both spatial and wavelet domain and take the advantage of both. Algorithm is iterative and use back projection to minimize reconstruction error. Wavelet based denoising method is also introduced to remove noise. version:1
arxiv-1309-1901 | Variational Bayes Approximations for Clustering via Mixtures of Normal Inverse Gaussian Distributions | http://arxiv.org/abs/1309.1901 | id:1309.1901 author:Sanjeena Subedi, Paul D. McNicholas category:stat.ME stat.CO stat.ML  published:2013-09-07 summary:Parameter estimation for model-based clustering using a finite mixture of normal inverse Gaussian (NIG) distributions is achieved through variational Bayes approximations. Univariate NIG mixtures and multivariate NIG mixtures are considered. The use of variational Bayes approximations here is a substantial departure from the traditional EM approach and alleviates some of the associated computational complexities and uncertainties. Our variational algorithm is applied to simulated and real data. The paper concludes with discussion and suggestions for future work. version:1
arxiv-1202-3639 | Finding a most biased coin with fewest flips | http://arxiv.org/abs/1202.3639 | id:1202.3639 author:Karthekeyan Chandrasekaran, Richard Karp category:cs.DS cs.LG  published:2012-02-16 summary:We study the problem of learning a most biased coin among a set of coins by tossing the coins adaptively. The goal is to minimize the number of tosses until we identify a coin i* whose posterior probability of being most biased is at least 1-delta for a given delta. Under a particular probabilistic model, we give an optimal algorithm, i.e., an algorithm that minimizes the expected number of future tosses. The problem is closely related to finding the best arm in the multi-armed bandit problem using adaptive strategies. Our algorithm employs an optimal adaptive strategy -- a strategy that performs the best possible action at each step after observing the outcomes of all previous coin tosses. Consequently, our algorithm is also optimal for any starting history of outcomes. To our knowledge, this is the first algorithm that employs an optimal adaptive strategy under a Bayesian setting for this problem. Our proof of optimality employs tools from the field of Markov games. version:3
arxiv-1211-2190 | Efficient Monte Carlo Methods for Multi-Dimensional Learning with Classifier Chains | http://arxiv.org/abs/1211.2190 | id:1211.2190 author:Jesse Read, Luca Martino, David Luengo category:cs.LG stat.CO stat.ML  published:2012-11-09 summary:Multi-dimensional classification (MDC) is the supervised learning problem where an instance is associated with multiple classes, rather than with a single class, as in traditional classification problems. Since these classes are often strongly correlated, modeling the dependencies between them allows MDC methods to improve their performance - at the expense of an increased computational cost. In this paper we focus on the classifier chains (CC) approach for modeling dependencies, one of the most popular and highest- performing methods for multi-label classification (MLC), a particular case of MDC which involves only binary classes (i.e., labels). The original CC algorithm makes a greedy approximation, and is fast but tends to propagate errors along the chain. Here we present novel Monte Carlo schemes, both for finding a good chain sequence and performing efficient inference. Our algorithms remain tractable for high-dimensional data sets and obtain the best predictive performance across several real data sets. version:4
arxiv-1309-1853 | A General Two-Step Approach to Learning-Based Hashing | http://arxiv.org/abs/1309.1853 | id:1309.1853 author:Guosheng Lin, Chunhua Shen, David Suter, Anton van den Hengel category:cs.LG cs.CV  published:2013-09-07 summary:Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problem-specific hashing methods. Our framework decomposes hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art. version:1
arxiv-1309-1830 | Radar shadow detection in SAR images using DEM and projections | http://arxiv.org/abs/1309.1830 | id:1309.1830 author:V. B. S. Prasath, O. Haddad category:cs.CV 68U10 I.4.8  published:2013-09-07 summary:Synthetic aperture radar (SAR) images are widely used in target recognition tasks nowadays. In this letter, we propose an automatic approach for radar shadow detection and extraction from SAR images utilizing geometric projections along with the digital elevation model (DEM) which corresponds to the given geo-referenced SAR image. First, the DEM is rotated into the radar geometry so that each row would match that of a radar line of sight. Next, we extract the shadow regions by processing row by row until the image is covered fully. We test the proposed shadow detection approach on different DEMs and a simulated 1D signals and 2D hills and volleys modeled by various variance based Gaussian functions. Experimental results indicate the proposed algorithm produces good results in detecting shadows in SAR images with high resolution. version:1
arxiv-1301-3781 | Efficient Estimation of Word Representations in Vector Space | http://arxiv.org/abs/1301.3781 | id:1301.3781 author:Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean category:cs.CL  published:2013-01-16 summary:We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. version:3
arxiv-1309-1761 | Convergence of Nearest Neighbor Pattern Classification with Selective Sampling | http://arxiv.org/abs/1309.1761 | id:1309.1761 author:Shaun N. Joseph, Seif Omar Abu Bakr, Gabriel Lugo category:cs.LG stat.ML 60G25 F.2.2; G.3  published:2013-09-06 summary:In the panoply of pattern classification techniques, few enjoy the intuitive appeal and simplicity of the nearest neighbor rule: given a set of samples in some metric domain space whose value under some function is known, we estimate the function anywhere in the domain by giving the value of the nearest sample per the metric. More generally, one may use the modal value of the m nearest samples, where m is a fixed positive integer (although m=1 is known to be admissible in the sense that no larger value is asymptotically superior in terms of prediction error). The nearest neighbor rule is nonparametric and extremely general, requiring in principle only that the domain be a metric space. The classic paper on the technique, proving convergence under independent, identically-distributed (iid) sampling, is due to Cover and Hart (1967). Because taking samples is costly, there has been much research in recent years on selective sampling, in which each sample is selected from a pool of candidates ranked by a heuristic; the heuristic tries to guess which candidate would be the most "informative" sample. Lindenbaum et al. (2004) apply selective sampling to the nearest neighbor rule, but their approach sacrifices the austere generality of Cover and Hart; furthermore, their heuristic algorithm is complex and computationally expensive. Here we report recent results that enable selective sampling in the original Cover-Hart setting. Our results pose three selection heuristics and prove that their nearest neighbor rule predictions converge to the true pattern. Two of the algorithms are computationally cheap, with complexity growing linearly in the number of samples. We believe that these results constitute an important advance in the art. version:1
arxiv-1309-1543 | A Comparism of the Performance of Supervised and Unsupervised Machine Learning Techniques in evolving Awale/Mancala/Ayo Game Player | http://arxiv.org/abs/1309.1543 | id:1309.1543 author:O. A. Randle, O. O. Ogunduyile, T. Zuva, N. A. Fashola category:cs.LG cs.GT  published:2013-09-06 summary:Awale games have become widely recognized across the world, for their innovative strategies and techniques which were used in evolving the agents (player) and have produced interesting results under various conditions. This paper will compare the results of the two major machine learning techniques by reviewing their performance when using minimax, endgame database, a combination of both techniques or other techniques, and will determine which are the best techniques. version:1
arxiv-1309-1541 | Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application | http://arxiv.org/abs/1309.1541 | id:1309.1541 author:Weiran Wang, Miguel Á. Carreira-Perpiñán category:cs.LG math.OC stat.ML  published:2013-09-06 summary:We provide an elementary proof of a simple, efficient algorithm for computing the Euclidean projection of a point onto the probability simplex. We also show an application in Laplacian K-modes clustering. version:1
arxiv-1309-1524 | Guided Self-Organization of Input-Driven Recurrent Neural Networks | http://arxiv.org/abs/1309.1524 | id:1309.1524 author:Oliver Obst, Joschka Boedecker category:cs.NE cs.AI nlin.AO  published:2013-09-06 summary:We review attempts that have been made towards understanding the computational properties and mechanisms of input-driven dynamical systems like RNNs, and reservoir computing networks in particular. We provide details on methods that have been developed to give quantitative answers to the questions above. Following this, we show how self-organization may be used to improve reservoirs for better performance, in some cases guided by the measures presented before. We also present a possible way to quantify task performance using an information-theoretic approach, and finally discuss promising future directions aimed at a better understanding of how these systems perform their computations and how to best guide self-organized processes for their optimization. version:1
arxiv-1308-4206 | Nested Nonnegative Cone Analysis | http://arxiv.org/abs/1308.4206 | id:1308.4206 author:Lingsong Zhang, J. S. Marron, Shu Lu category:stat.ME cs.LG  published:2013-08-20 summary:Motivated by the analysis of nonnegative data objects, a novel Nested Nonnegative Cone Analysis (NNCA) approach is proposed to overcome some drawbacks of existing methods. The application of traditional PCA/SVD method to nonnegative data often cause the approximation matrix leave the nonnegative cone, which leads to non-interpretable and sometimes nonsensical results. The nonnegative matrix factorization (NMF) approach overcomes this issue, however the NMF approximation matrices suffer several drawbacks: 1) the factorization may not be unique, 2) the resulting approximation matrix at a specific rank may not be unique, and 3) the subspaces spanned by the approximation matrices at different ranks may not be nested. These drawbacks will cause troubles in determining the number of components and in multi-scale (in ranks) interpretability. The NNCA approach proposed in this paper naturally generates a nested structure, and is shown to be unique at each rank. Simulations are used in this paper to illustrate the drawbacks of the traditional methods, and the usefulness of the NNCA method. version:2
arxiv-1309-1521 | Nano-scale reservoir computing | http://arxiv.org/abs/1309.1521 | id:1309.1521 author:Oliver Obst, Adrian Trinchi, Simon G. Hardin, Matthew Chadwick, Ivan Cole, Tim H. Muster, Nigel Hoschke, Diet Ostry, Don Price, Khoa N. Pham, Tim Wark category:cs.ET cs.NE nlin.AO  published:2013-09-06 summary:This work describes preliminary steps towards nano-scale reservoir computing using quantum dots. Our research has focused on the development of an accumulator-based sensing system that reacts to changes in the environment, as well as the development of a software simulation. The investigated systems generate nonlinear responses to inputs that make them suitable for a physical implementation of a neural network. This development will enable miniaturisation of the neurons to the molecular level, leading to a range of applications including monitoring of changes in materials or structures. The system is based around the optical properties of quantum dots. The paper will report on experimental work on systems using Cadmium Selenide (CdSe) quantum dots and on the various methods to render the systems sensitive to pH, redox potential or specific ion concentration. Once the quantum dot-based systems are rendered sensitive to these triggers they can provide a distributed array that can monitor and transmit information on changes within the material. version:1
arxiv-1309-1274 | A Small Universal Petri Net | http://arxiv.org/abs/1309.1274 | id:1309.1274 author:Dmitry A. Zaitsev category:cs.FL cs.CC cs.DC cs.NE  published:2013-09-05 summary:A universal deterministic inhibitor Petri net with 14 places, 29 transitions and 138 arcs was constructed via simulation of Neary and Woods' weakly universal Turing machine with 2 states and 4 symbols; the total time complexity is exponential in the running time of their weak machine. To simulate the blank words of the weakly universal Turing machine, a couple of dedicated transitions insert their codes when reaching edges of the working zone. To complete a chain of a given Petri net encoding to be executed by the universal Petri net, a translation of a bi-tag system into a Turing machine was constructed. The constructed Petri net is universal in the standard sense; a weaker form of universality for Petri nets was not introduced in this work. version:1
arxiv-1210-1928 | Information fusion in multi-task Gaussian processes | http://arxiv.org/abs/1210.1928 | id:1210.1928 author:Shrihari Vasudevan, Arman Melkumyan, Steven Scheding category:stat.ML cs.AI cs.LG  published:2012-10-06 summary:This paper evaluates heterogeneous information fusion using multi-task Gaussian processes in the context of geological resource modeling. Specifically, it empirically demonstrates that information integration across heterogeneous information sources leads to superior estimates of all the quantities being modeled, compared to modeling them individually. Multi-task Gaussian processes provide a powerful approach for simultaneous modeling of multiple quantities of interest while taking correlations between these quantities into consideration. Experiments are performed on large scale real sensor data. version:3
arxiv-1309-1194 | Some Options for L1-Subspace Signal Processing | http://arxiv.org/abs/1309.1194 | id:1309.1194 author:Panos P. Markopoulos, George N. Karystinos, Dimitris A. Pados category:stat.ML  published:2013-09-04 summary:We describe ways to define and calculate $L_1$-norm signal subspaces which are less sensitive to outlying data than $L_2$-calculated subspaces. We focus on the computation of the $L_1$ maximum-projection principal component of a data matrix containing N signal samples of dimension D and conclude that the general problem is formally NP-hard in asymptotically large N, D. We prove, however, that the case of engineering interest of fixed dimension D and asymptotically large sample support N is not and we present an optimal algorithm of complexity $O(N^D)$. We generalize to multiple $L_1$-max-projection components and present an explicit optimal $L_1$ subspace calculation algorithm in the form of matrix nuclear-norm evaluations. We conclude with illustrations of $L_1$-subspace signal processing in the fields of data dimensionality reduction and direction-of-arrival estimation. version:1
arxiv-1309-1129 | Analysing Quality of English-Hindi Machine Translation Engine Outputs Using Bayesian Classification | http://arxiv.org/abs/1309.1129 | id:1309.1129 author:Rashmi Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL  published:2013-09-04 summary:This paper considers the problem for estimating the quality of machine translation outputs which are independent of human intervention and are generally addressed using machine learning techniques.There are various measures through which a machine learns translations quality. Automatic Evaluation metrics produce good co-relation at corpus level but cannot produce the same results at the same segment or sentence level. In this paper 16 features are extracted from the input sentences and their translations and a quality score is obtained based on Bayesian inference produced from training data. version:1
arxiv-1309-1125 | Learning to answer questions | http://arxiv.org/abs/1309.1125 | id:1309.1125 author:Ana Cristina Mendes, Luísa Coheur, Sérgio Curto category:cs.CL  published:2013-09-04 summary:We present an open-domain Question-Answering system that learns to answer questions based on successful past interactions. We follow a pattern-based approach to Answer-Extraction, where (lexico-syntactic) patterns that relate a question to its answer are automatically learned and used to answer future questions. Results show that our approach contributes to the system's best performance when it is conjugated with typical Answer-Extraction strategies. Moreover, it allows the system to learn with the answered questions and to rectify wrong or unsolved past questions. version:1
arxiv-1309-1080 | Boosting in Location Space | http://arxiv.org/abs/1309.1080 | id:1309.1080 author:Damian Eads, David Helmbold, Ed Rosten category:cs.CV  published:2013-09-04 summary:The goal of object detection is to find objects in an image. An object detector accepts an image and produces a list of locations as $(x,y)$ pairs. Here we introduce a new concept: {\bf location-based boosting}. Location-based boosting differs from previous boosting algorithms because it optimizes a new spatial loss function to combine object detectors, each of which may have marginal performance, into a single, more accurate object detector. A structured representation of object locations as a list of $(x,y)$ pairs is a more natural domain for object detection than the spatially unstructured representation produced by classifiers. Furthermore, this formulation allows us to take advantage of the intuition that large areas of the background are uninteresting and it is not worth expending computational effort on them. This results in a more scalable algorithm because it does not need to take measures to prevent the background data from swamping the foreground data such as subsampling or applying an ad-hoc weighting to the pixels. We first present the theory of location-based boosting, and then motivate it with empirical results on a challenging data set. version:1
arxiv-1309-1156 | Thermal Human face recognition based on Haar wavelet transform and series matching technique | http://arxiv.org/abs/1309.1156 | id:1309.1156 author:Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri, Dipak kr. Basu category:cs.CV  published:2013-09-04 summary:Thermal infrared (IR) images represent the heat patterns emitted from hot object and they do not consider the energies reflected from an object. Objects living or non-living emit different amounts of IR energy according to their body temperature and characteristics. Humans are homoeothermic and hence capable of maintaining constant temperature under different surrounding temperature. Face recognition from thermal (IR) images should focus on changes of temperature on facial blood vessels. These temperature changes can be regarded as texture features of images and wavelet transform is a very good tool to analyze multi-scale and multi-directional texture. Wavelet transform is also used for image dimensionality reduction, by removing redundancies and preserving original features of the image. The sizes of the facial images are normally large. So, the wavelet transform is used before image similarity is measured. Therefore this paper describes an efficient approach of human face recognition based on wavelet transform from thermal IR images. The system consists of three steps. At the very first step, human thermal IR face image is preprocessed and the face region is only cropped from the entire image. Secondly, Haar wavelet is used to extract low frequency band from the cropped face region. Lastly, the image classification between the training images and the test images is done, which is based on low-frequency components. The proposed approach is tested on a number of human thermal infrared face images created at our own laboratory and Terravic Facial IR Database. Experimental results indicated that the thermal infra red face images can be recognized by the proposed system effectively. The maximum success of 95% recognition has been achieved. version:1
arxiv-1309-1155 | Minutiae Based Thermal Human Face Recognition using Label Connected Component Algorithm | http://arxiv.org/abs/1309.1155 | id:1309.1155 author:Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV  published:2013-09-04 summary:In this paper, a thermal infra red face recognition system for human identification and verification using blood perfusion data and back propagation feed forward neural network is proposed. The system consists of three steps. At the very first step face region is cropped from the colour 24-bit input images. Secondly face features are extracted from the croped region, which will be taken as the input of the back propagation feed forward neural network in the third step and classification and recognition is carried out. The proposed approaches are tested on a number of human thermal infra red face images created at our own laboratory. Experimental results reveal the higher degree performance version:1
arxiv-1309-1014 | Advances in the Logical Representation of Lexical Semantics | http://arxiv.org/abs/1309.1014 | id:1309.1014 author:Bruno Mery, Christian Retoré category:cs.CL  published:2013-09-04 summary:The integration of lexical semantics and pragmatics in the analysis of the meaning of natural lan- guage has prompted changes to the global framework derived from Montague. In those works, the original lexicon, in which words were assigned an atomic type of a single-sorted logic, has been re- placed by a set of many-facetted lexical items that can compose their meaning with salient contextual properties using a rich typing system as a guide. Having related our proposal for such an expanded framework \LambdaTYn, we present some recent advances in the logical formalisms associated, including constraints on lexical transformations and polymorphic quantifiers, and ongoing discussions and research on the granularity of the type system and the limits of transitivity. version:1
arxiv-1309-1009 | A Comparative Study of Human thermal face recognition based on Haar wavelet transform (HWT) and Local Binary Pattern (LBP) | http://arxiv.org/abs/1309.1009 | id:1309.1009 author:Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CV  published:2013-09-04 summary:Thermal infra-red (IR) images focus on changes of temperature distribution on facial muscles and blood vessels. These temperature changes can be regarded as texture features of images. A comparative study of face recognition methods working in thermal spectrum is carried out in this paper. In these study two local-matching methods based on Haar wavelet transform and Local Binary Pattern (LBP) are analyzed. Wavelet transform is a good tool to analyze multi-scale, multi-direction changes of texture. Local binary patterns (LBP) are a type of feature used for classification in computer vision. Firstly, human thermal IR face image is preprocessed and cropped the face region only from the entire image. Secondly, two different approaches are used to extract the features from the cropped face region. In the first approach, the training images and the test images are processed with Haar wavelet transform and the LL band and the average of LH/HL/HH bands sub-images are created for each face image. Then a total confidence matrix is formed for each face image by taking a weighted sum of the corresponding pixel values of the LL band and average band. For LBP feature extraction, each of the face images in training and test datasets is divided into 161 numbers of sub images, each of size 8X8 pixels. For each such sub images, LBP features are extracted which are concatenated in row wise manner. PCA is performed separately on the individual feature set for dimensionality reeducation. Finally two different classifiers are used to classify face images. One such classifier multi-layer feed forward neural network and another classifier is minimum distance classifier. The Experiments have been performed on the database created at our own laboratory and Terravic Facial IR Database. version:1
arxiv-1309-1000 | Automated Thermal Face recognition based on Minutiae Extraction | http://arxiv.org/abs/1309.1000 | id:1309.1000 author:Ayan Seal, Suranjan Ganguly, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kr. Basu category:cs.CV  published:2013-09-04 summary:In this paper an efficient approach for human face recognition based on the use of minutiae points in thermal face image is proposed. The thermogram of human face is captured by thermal infra-red camera. Image processing methods are used to pre-process the captured thermogram, from which different physiological features based on blood perfusion data are extracted. Blood perfusion data are related to distribution of blood vessels under the face skin. In the present work, three different methods have been used to get the blood perfusion image, namely bit-plane slicing and medial axis transform, morphological erosion and medial axis transform, sobel edge operators. Distribution of blood vessels is unique for each person and a set of extracted minutiae points from a blood perfusion data of a human face should be unique for that face. Two different methods are discussed for extracting minutiae points from blood perfusion data. For extraction of features entire face image is partitioned into equal size blocks and the total number of minutiae points from each block is computed to construct final feature vector. Therefore, the size of the feature vectors is found to be same as total number of blocks considered. A five layer feed-forward back propagation neural network is used as the classification tool. A number of experiments were conducted to evaluate the performance of the proposed face recognition methodologies with varying block size on the database created at our own laboratory. It has been found that the first method supercedes the other two producing an accuracy of 97.62% with block size 16X16 for bit-plane 4. version:1
arxiv-1309-0999 | Minutiae Based Thermal Face Recognition using Blood Perfusion Data | http://arxiv.org/abs/1309.0999 | id:1309.0999 author:Ayan Seal, Mita Nasipuri, Debotosh Bhattacharjee, Dipak Kumar Basu category:cs.CV  published:2013-09-04 summary:This paper describes an efficient approach for human face recognition based on blood perfusion data from infra-red face images. Blood perfusion data are characterized by the regional blood flow in human tissue and therefore do not depend entirely on surrounding temperature. These data bear a great potential for deriving discriminating facial thermogram for better classification and recognition of face images in comparison to optical image data. Blood perfusion data are related to distribution of blood vessels under the face skin. A distribution of blood vessels are unique for each person and as a set of extracted minutiae points from a blood perfusion data of a human face should be unique for that face. There may be several such minutiae point sets for a single face but all of these correspond to that particular face only. Entire face image is partitioned into equal blocks and the total number of minutiae points from each block is computed to construct final vector. Therefore, the size of the feature vectors is found to be same as total number of blocks considered. For classification, a five layer feed-forward backpropagation neural network has been used. A number of experiments were conducted to evaluate the performance of the proposed face recognition system with varying block sizes. Experiments have been performed on the database created at our own laboratory. The maximum success of 91.47% recognition has been achieved with block size 8X8. version:1
arxiv-1309-0985 | Efficient binary tomographic reconstruction | http://arxiv.org/abs/1309.0985 | id:1309.0985 author:Stephane Roux, Hugo Leclerc, François Hild category:physics.class-ph cs.CV  published:2013-09-04 summary:Tomographic reconstruction of a binary image from few projections is considered. A novel {\em heuristic} algorithm is proposed, the central element of which is a nonlinear transformation $\psi(p)=\log(p/(1-p))$ of the probability $p$ that a pixel of the sought image be 1-valued. It consists of backprojections based on $\psi(p)$ and iterative corrections. Application of this algorithm to a series of artificial test cases leads to exact binary reconstructions, (i.e recovery of the binary image for each single pixel) from the knowledge of projection data over a few directions. Images up to $10^6$ pixels are reconstructed in a few seconds. A series of test cases is performed for comparison with previous methods, showing a better efficiency and reduced computation times. version:1
arxiv-1309-0866 | On the Robustness of Temporal Properties for Stochastic Models | http://arxiv.org/abs/1309.0866 | id:1309.0866 author:Ezio Bartocci, Luca Bortolussi, Laura Nenzi, Guido Sanguinetti category:cs.LO cs.AI cs.LG cs.SY  published:2013-09-03 summary:Stochastic models such as Continuous-Time Markov Chains (CTMC) and Stochastic Hybrid Automata (SHA) are powerful formalisms to model and to reason about the dynamics of biological systems, due to their ability to capture the stochasticity inherent in biological processes. A classical question in formal modelling with clear relevance to biological modelling is the model checking problem. i.e. calculate the probability that a behaviour, expressed for instance in terms of a certain temporal logic formula, may occur in a given stochastic process. However, one may not only be interested in the notion of satisfiability, but also in the capacity of a system to mantain a particular emergent behaviour unaffected by the perturbations, caused e.g. from extrinsic noise, or by possible small changes in the model parameters. To address this issue, researchers from the verification community have recently proposed several notions of robustness for temporal logic providing suitable definitions of distance between a trajectory of a (deterministic) dynamical system and the boundaries of the set of trajectories satisfying the property of interest. The contributions of this paper are twofold. First, we extend the notion of robustness to stochastic systems, showing that this naturally leads to a distribution of robustness scores. By discussing two examples, we show how to approximate the distribution of the robustness score and its key indicators: the average robustness and the conditional average robustness. Secondly, we show how to combine these indicators with the satisfaction probability to address the system design problem, where the goal is to optimize some control parameters of a stochastic model in order to best maximize robustness of the desired specifications. version:1
arxiv-1308-0642 | Nonlinear Time Series Modeling by LPTime,Nonparametric Empirical Learning | http://arxiv.org/abs/1308.0642 | id:1308.0642 author:Subhadeep Mukhopadhyay, Emanuel Parzen category:math.ST stat.AP stat.ME stat.ML stat.TH  published:2013-08-03 summary:We describe a new comprehensive approach to nonlinear time series analysis and modeling based on recently developed theory on unified algorithms of data science via LP modeling. We introduce novel data-specific mid-distribution based Legendre Polynomial(LP) like nonlinear transformations of the original time series Y(t) that enables us to adapt all the existing stationary linear Gaussian time series modeling strategy and made it applicable for non-Gaussian and nonlinear processes in a robust fashion. The emphasis of the present paper is on empirical time series modeling via the algorithm LPTime. We describe each stage of the model building process, associated theoretical concepts and illustrate with daily S&P 500 return data between Jan/2/1963 - Dec/31/2009. Our proposed LPTime algorithm systematically discovers all the `stylized facts' of the financial time series automatically all at once, which were previously noted by many researchers one at a time. version:2
arxiv-1309-0671 | BayesOpt: A Library for Bayesian optimization with Robotics Applications | http://arxiv.org/abs/1309.0671 | id:1309.0671 author:Ruben Martinez-Cantin category:cs.RO cs.AI cs.LG cs.MS  published:2013-09-03 summary:The purpose of this paper is twofold. On one side, we present a general framework for Bayesian optimization and we compare it with some related fields in active learning and Bayesian numerical analysis. On the other hand, Bayesian optimization and related problems (bandits, sequential experimental design) are highly dependent on the surrogate model that is selected. However, there is no clear standard in the literature. Thus, we present a fast and flexible toolbox that allows to test and combine different models and criteria with little effort. It includes most of the state-of-the-art contributions, algorithms and models. Its speed also removes part of the stigma that Bayesian optimization methods are only good for "expensive functions". The software is free and it can be used in many operating systems and computer languages. version:1
arxiv-1309-0213 | Learning to Rank for Blind Image Quality Assessment | http://arxiv.org/abs/1309.0213 | id:1309.0213 author:Fei Gao, Dacheng Tao, Xinbo Gao, Xuelong Li category:cs.CV  published:2013-09-01 summary:Blind image quality assessment (BIQA) aims to predict perceptual image quality scores without access to reference images. State-of-the-art BIQA methods typically require subjects to score a large number of images to train a robust model. However, the acquisition of image quality scores has several limitations: 1) scores are not precise, because subjects are usually uncertain about which score most precisely represents the perceptual quality of a given image; 2) subjective judgements of quality may be biased by image content; 3) the quality scales between different distortion categories are inconsistent; and 4) it is challenging to obtain a large scale database, or to extend existing databases, because of the inconvenience of collecting sufficient images, training the subjects, conducting subjective experiments, and realigning human quality evaluations. To combat these limitations, this paper explores and exploits preference image pairs such as "the quality of image Ia is better than that of image Ib" for training a robust BIQA model. The preference label, representing the relative quality of two images, is generally precise and consistent, and is not sensitive to image content, distortion type, or subject identity; such PIPs can be generated at very low cost. The proposed BIQA method is one of learning to rank. We first formulate the problem of learning the mapping from the image features to the preference label as one of classification. In particular, we investigate the utilization of a multiple kernel learning algorithm based on group lasso (MKLGL) to provide a solution. A simple but effective strategy to estimate perceptual image quality scores is then presented. Experiments show that the proposed BIQA method is highly effective and achieves comparable performance to state-of-the-art BIQA algorithms. Moreover, the proposed method can be easily extended to new distortion categories. version:2
arxiv-1212-4522 | A Multi-View Embedding Space for Modeling Internet Images, Tags, and their Semantics | http://arxiv.org/abs/1212.4522 | id:1212.4522 author:Yunchao Gong, Qifa Ke, Michael Isard, Svetlana Lazebnik category:cs.CV cs.IR cs.LG cs.MM  published:2012-12-18 summary:This paper investigates the problem of modeling Internet images and associated text or tags for tasks such as image-to-image search, tag-to-image search, and image-to-tag search (image annotation). We start with canonical correlation analysis (CCA), a popular and successful approach for mapping visual and textual features to the same latent space, and incorporate a third view capturing high-level image semantics, represented either by a single category or multiple non-mutually-exclusive concepts. We present two ways to train the three-view embedding: supervised, with the third view coming from ground-truth labels or search keywords; and unsupervised, with semantic themes automatically obtained by clustering the tags. To ensure high accuracy for retrieval tasks while keeping the learning process scalable, we combine multiple strong visual features and use explicit nonlinear kernel mappings to efficiently approximate kernel CCA. To perform retrieval, we use a specially designed similarity function in the embedded space, which substantially outperforms the Euclidean distance. The resulting system produces compelling qualitative results and outperforms a number of two-view baselines on retrieval tasks on three large-scale Internet image datasets. version:2
arxiv-1309-0337 | Scalable Probabilistic Entity-Topic Modeling | http://arxiv.org/abs/1309.0337 | id:1309.0337 author:Neil Houlsby, Massimiliano Ciaramita category:stat.ML cs.IR cs.LG  published:2013-09-02 summary:We present an LDA approach to entity disambiguation. Each topic is associated with a Wikipedia article and topics generate either content words or entity mentions. Training such models is challenging because of the topic and vocabulary size, both in the millions. We tackle these problems using a novel distributed inference and representation framework based on a parallel Gibbs sampler guided by the Wikipedia link graph, and pipelines of MapReduce allowing fast and memory-frugal processing of large datasets. We report state-of-the-art performance on a public dataset. version:1
arxiv-1304-5350 | Parallel Gaussian Process Optimization with Upper Confidence Bound and Pure Exploration | http://arxiv.org/abs/1304.5350 | id:1304.5350 author:Emile Contal, David Buffoni, Alexandre Robicquet, Nicolas Vayatis category:cs.LG stat.ML  published:2013-04-19 summary:In this paper, we consider the challenge of maximizing an unknown function f for which evaluations are noisy and are acquired with high cost. An iterative procedure uses the previous measures to actively select the next estimation of f which is predicted to be the most useful. We focus on the case where the function can be evaluated in parallel with batches of fixed size and analyze the benefit compared to the purely sequential procedure in terms of cumulative regret. We introduce the Gaussian Process Upper Confidence Bound and Pure Exploration algorithm (GP-UCB-PE) which combines the UCB strategy and Pure Exploration in the same batch of evaluations along the parallel iterations. We prove theoretical upper bounds on the regret with batches of size K for this procedure which show the improvement of the order of sqrt{K} for fixed iteration cost over purely sequential versions. Moreover, the multiplicative constants involved have the property of being dimension-free. We also confirm empirically the efficiency of GP-UCB-PE on real and synthetic problems compared to state-of-the-art competitors. version:3
arxiv-1309-0309 | A Study on Unsupervised Dictionary Learning and Feature Encoding for Action Classification | http://arxiv.org/abs/1309.0309 | id:1309.0309 author:Xiaojiang Peng, Qiang Peng, Yu Qiao, Junzhou Chen, Mehtab Afzal category:cs.CV  published:2013-09-02 summary:Many efforts have been devoted to develop alternative methods to traditional vector quantization in image domain such as sparse coding and soft-assignment. These approaches can be split into a dictionary learning phase and a feature encoding phase which are often closely connected. In this paper, we investigate the effects of these phases by separating them for video-based action classification. We compare several dictionary learning methods and feature encoding schemes through extensive experiments on KTH and HMDB51 datasets. Experimental results indicate that sparse coding performs consistently better than the other encoding methods in large complex dataset (i.e., HMDB51), and it is robust to different dictionaries. For small simple dataset (i.e., KTH) with less variation, however, all the encoding strategies perform competitively. In addition, we note that the strength of sophisticated encoding approaches comes not from their corresponding dictionaries but the encoding mechanisms, and we can just use randomly selected exemplars as dictionaries for video-based action classification. version:1
arxiv-1309-0302 | Unmixing Incoherent Structures of Big Data by Randomized or Greedy Decomposition | http://arxiv.org/abs/1309.0302 | id:1309.0302 author:Tianyi Zhou, Dacheng Tao category:stat.ML cs.DS cs.LG  published:2013-09-02 summary:Learning big data by matrix decomposition always suffers from expensive computation, mixing of complicated structures and noise. In this paper, we study more adaptive models and efficient algorithms that decompose a data matrix as the sum of semantic components with incoherent structures. We firstly introduce "GO decomposition (GoDec)", an alternating projection method estimating the low-rank part $L$ and the sparse part $S$ from data matrix $X=L+S+G$ corrupted by noise $G$. Two acceleration strategies are proposed to obtain scalable unmixing algorithm on big data: 1) Bilateral random projection (BRP) is developed to speed up the update of $L$ in GoDec by a closed-form built from left and right random projections of $X-S$ in lower dimensions; 2) Greedy bilateral (GreB) paradigm updates the left and right factors of $L$ in a mutually adaptive and greedy incremental manner, and achieve significant improvement in both time and sample complexities. Then we proposes three nontrivial variants of GoDec that generalizes GoDec to more general data type and whose fast algorithms can be derived from the two strategies...... version:1
arxiv-1309-1453 | Parallel machine scheduling with step deteriorating jobs and setup times by a hybrid discrete cuckoo search algorithm | http://arxiv.org/abs/1309.1453 | id:1309.1453 author:Peng Guo, Wenming Cheng, Yi Wang category:math.OC cs.DS cs.NE  published:2013-09-02 summary:This article considers the parallel machine scheduling problem with step-deteriorating jobs and sequence-dependent setup times. The objective is to minimize the total tardiness by determining the allocation and sequence of jobs on identical parallel machines. In this problem, the processing time of each job is a step function dependent upon its starting time. An individual extended time is penalized when the starting time of a job is later than a specific deterioration date. The possibility of deterioration of a job makes the parallel machine scheduling problem more challenging than ordinary ones. A mixed integer programming model for the optimal solution is derived. Due to its NP-hard nature, a hybrid discrete cuckoo search algorithm is proposed to solve this problem. In order to generate a good initial swarm, a modified heuristic named the MBHG is incorporated into the initialization of population. Several discrete operators are proposed in the random walk of L\'{e}vy Flights and the crossover search. Moreover, a local search procedure based on variable neighborhood descent is integrated into the algorithm as a hybrid strategy in order to improve the quality of elite solutions. Computational experiments are executed on two sets of randomly generated test instances. The results show that the proposed hybrid algorithm can yield better solutions in comparison with the commercial solver CPLEX with one hour time limit, discrete cuckoo search algorithm and the existing variable neighborhood search algorithm. version:1
arxiv-1309-5854 | Demodulation of Sparse PPM Signals with Low Samples Using Trained RIP Matrix | http://arxiv.org/abs/1309.5854 | id:1309.5854 author:Seyed Hossein Hosseini, Mahrokh G. Shayesteh, Mehdi Chehel Amirani category:cs.OH cs.IT cs.LG math.IT  published:2013-09-01 summary:Compressed sensing (CS) theory considers the restricted isometry property (RIP) as a sufficient condition for measurement matrix which guarantees the recovery of any sparse signal from its compressed measurements. The RIP condition also preserves enough information for classification of sparse symbols, even with fewer measurements. In this work, we utilize RIP bound as the cost function for training a simple neural network in order to exploit the near optimal measurements or equivalently near optimal features for classification of a known set of sparse symbols. As an example, we consider demodulation of pulse position modulation (PPM) signals. The results indicate that the proposed method has much better performance than the random measurements and requires less samples than the optimum matched filter demodulator, at the expense of some performance loss. Further, the proposed approach does not need equalizer for multipath channels in contrast to the conventional receiver. version:1
arxiv-1309-0261 | Multi-Column Deep Neural Networks for Offline Handwritten Chinese Character Classification | http://arxiv.org/abs/1309.0261 | id:1309.0261 author:Dan Cireşan, Jürgen Schmidhuber category:cs.CV  published:2013-09-01 summary:Our Multi-Column Deep Neural Networks achieve best known recognition rates on Chinese characters from the ICDAR 2011 and 2013 offline handwriting competitions, approaching human performance. version:1
arxiv-1309-0242 | Ensemble approaches for improving community detection methods | http://arxiv.org/abs/1309.0242 | id:1309.0242 author:Johan Dahlin, Pontus Svenson category:physics.soc-ph cs.LG cs.SI stat.ML  published:2013-09-01 summary:Statistical estimates can often be improved by fusion of data from several different sources. One example is so-called ensemble methods which have been successfully applied in areas such as machine learning for classification and clustering. In this paper, we present an ensemble method to improve community detection by aggregating the information found in an ensemble of community structures. This ensemble can found by re-sampling methods, multiple runs of a stochastic community detection method, or by several different community detection algorithms applied to the same network. The proposed method is evaluated using random networks with community structures and compared with two commonly used community detection methods. The proposed method when applied on a stochastic community detection algorithm performs well with low computational complexity, thus offering both a new approach to community detection and an additional community detection method. version:1
arxiv-1309-0238 | API design for machine learning software: experiences from the scikit-learn project | http://arxiv.org/abs/1309.0238 | id:1309.0238 author:Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake Vanderplas, Arnaud Joly, Brian Holt, Gaël Varoquaux category:cs.LG cs.MS  published:2013-09-01 summary:Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library. version:1
arxiv-1309-0113 | Non-Asymptotic Convergence Analysis of Inexact Gradient Methods for Machine Learning Without Strong Convexity | http://arxiv.org/abs/1309.0113 | id:1309.0113 author:Anthony Man-Cho So category:math.OC cs.LG  published:2013-08-31 summary:Many recent applications in machine learning and data fitting call for the algorithmic solution of structured smooth convex optimization problems. Although the gradient descent method is a natural choice for this task, it requires exact gradient computations and hence can be inefficient when the problem size is large or the gradient is difficult to evaluate. Therefore, there has been much interest in inexact gradient methods (IGMs), in which an efficiently computable approximate gradient is used to perform the update in each iteration. Currently, non-asymptotic linear convergence results for IGMs are typically established under the assumption that the objective function is strongly convex, which is not satisfied in many applications of interest; while linear convergence results that do not require the strong convexity assumption are usually asymptotic in nature. In this paper, we combine the best of these two types of results and establish---under the standard assumption that the gradient approximation errors decrease linearly to zero---the non-asymptotic linear convergence of IGMs when applied to a class of structured convex optimization problems. Such a class covers settings where the objective function is not necessarily strongly convex and includes the least squares and logistic regression problems. We believe that our techniques will find further applications in the non-asymptotic convergence analysis of other first-order methods. version:1
arxiv-1204-2321 | Derivation of Upper Bounds on Optimization Time of Population-Based Evolutionary Algorithm on a Function with Fitness Plateaus Using Elitism Levels Traverse Mechanism | http://arxiv.org/abs/1204.2321 | id:1204.2321 author:Aram Ter-Sarkisov, Stephen Marsland category:cs.NE cs.AI  published:2012-04-11 summary:In this article a tool for the analysis of population-based EAs is used to derive asymptotic upper bounds on the optimization time of the algorithm solving Royal Roads problem, a test function with plateaus of fitness. In addition to this, limiting distribution of a certain subset of the population is approximated. version:6
arxiv-1307-5558 | Mixtures of Common Skew-t Factor Analyzers | http://arxiv.org/abs/1307.5558 | id:1307.5558 author:Paula M. Murray, Paul D. McNicholas, Ryan P. Browne category:stat.ME stat.AP stat.CO stat.ML  published:2013-07-21 summary:A mixture of common skew-t factor analyzers model is introduced for model-based clustering of high-dimensional data. By assuming common component factor loadings, this model allows clustering to be performed in the presence of a large number of mixture components or when the number of dimensions is too large to be well-modelled by the mixtures of factor analyzers model or a variant thereof. Furthermore, assuming that the component densities follow a skew-t distribution allows robust clustering of skewed data. The alternating expectation-conditional maximization algorithm is employed for parameter estimation. We demonstrate excellent clustering performance when our model is applied to real and simulated data.This paper marks the first time that skewed common factors have been used. version:3
arxiv-1309-0024 | Inconsistency of Pitman-Yor process mixtures for the number of components | http://arxiv.org/abs/1309.0024 | id:1309.0024 author:Jeffrey W. Miller, Matthew T. Harrison category:math.ST stat.ML stat.TH  published:2013-08-30 summary:In many applications, a finite mixture is a natural model, but it can be difficult to choose an appropriate number of components. To circumvent this choice, investigators are increasingly turning to Dirichlet process mixtures (DPMs), and Pitman-Yor process mixtures (PYMs), more generally. While these models may be well-suited for Bayesian density estimation, many investigators are using them for inferences about the number of components, by considering the posterior on the number of components represented in the observed data. We show that this posterior is not consistent --- that is, on data from a finite mixture, it does not concentrate at the true number of components. This result applies to a large class of nonparametric mixtures, including DPMs and PYMs, over a wide variety of families of component distributions, including essentially all discrete families, as well as continuous exponential families satisfying mild regularity conditions (such as multivariate Gaussians). version:1
arxiv-1309-0003 | Concentration Inequalities for Bounded Random Vectors | http://arxiv.org/abs/1309.0003 | id:1309.0003 author:Xinjia Chen category:math.PR cs.LG math.ST stat.TH  published:2013-08-30 summary:We derive simple concentration inequalities for bounded random vectors, which generalize Hoeffding's inequalities for bounded scalar random variables. As applications, we apply the general results to multinomial and Dirichlet distributions to obtain multivariate concentration inequalities. version:1
arxiv-1210-2346 | Blending Learning and Inference in Structured Prediction | http://arxiv.org/abs/1210.2346 | id:1210.2346 author:Tamir Hazan, Alexander Schwing, David McAllester, Raquel Urtasun category:cs.LG  published:2012-10-08 summary:In this paper we derive an efficient algorithm to learn the parameters of structured predictors in general graphical models. This algorithm blends the learning and inference tasks, which results in a significant speedup over traditional approaches, such as conditional random fields and structured support vector machines. For this purpose we utilize the structures of the predictors to describe a low dimensional structured prediction task which encourages local consistencies within the different structures while learning the parameters of the model. Convexity of the learning task provides the means to enforce the consistencies between the different parts. The inference-learning blending algorithm that we propose is guaranteed to converge to the optimum of the low dimensional primal and dual programs. Unlike many of the existing approaches, the inference-learning blending allows us to learn efficiently high-order graphical models, over regions of any size, and very large number of parameters. We demonstrate the effectiveness of our approach, while presenting state-of-the-art results in stereo estimation, semantic segmentation, shape reconstruction, and indoor scene understanding. version:2
arxiv-1308-6774 | Separable Approximations and Decomposition Methods for the Augmented Lagrangian | http://arxiv.org/abs/1308.6774 | id:1308.6774 author:Rachael Tappenden, Peter Richtarik, Burak Buke category:math.OC cs.DC cs.NA stat.ML  published:2013-08-30 summary:In this paper we study decomposition methods based on separable approximations for minimizing the augmented Lagrangian. In particular, we study and compare the Diagonal Quadratic Approximation Method (DQAM) of Mulvey and Ruszczy\'{n}ski and the Parallel Coordinate Descent Method (PCDM) of Richt\'arik and Tak\'a\v{c}. We show that the two methods are equivalent for feasibility problems up to the selection of a single step-size parameter. Furthermore, we prove an improved complexity bound for PCDM under strong convexity, and show that this bound is at least $8(L'/\bar{L})(\omega-1)^2$ times better than the best known bound for DQAM, where $\omega$ is the degree of partial separability and $L'$ and $\bar{L}$ are the maximum and average of the block Lipschitz constants of the gradient of the quadratic penalty appearing in the augmented Lagrangian. version:1
arxiv-1308-6721 | Discriminative Parameter Estimation for Random Walks Segmentation | http://arxiv.org/abs/1308.6721 | id:1308.6721 author:Pierre-Yves Baudin, Danny Goodman, Puneet Kumar, Noura Azzabou, Pierre G. Carlier, Nikos Paragios, M. Pawan Kumar category:cs.CV cs.LG  published:2013-08-30 summary:The Random Walks (RW) algorithm is one of the most e - cient and easy-to-use probabilistic segmentation methods. By combining contrast terms with prior terms, it provides accurate segmentations of medical images in a fully automated manner. However, one of the main drawbacks of using the RW algorithm is that its parameters have to be hand-tuned. we propose a novel discriminative learning framework that estimates the parameters using a training dataset. The main challenge we face is that the training samples are not fully supervised. Speci cally, they provide a hard segmentation of the images, instead of a proba- bilistic segmentation. We overcome this challenge by treating the opti- mal probabilistic segmentation that is compatible with the given hard segmentation as a latent variable. This allows us to employ the latent support vector machine formulation for parameter estimation. We show that our approach signi cantly outperforms the baseline methods on a challenging dataset consisting of real clinical 3D MRI volumes of skeletal muscles. version:1
arxiv-1308-6687 | Image Set based Collaborative Representation for Face Recognition | http://arxiv.org/abs/1308.6687 | id:1308.6687 author:Pengfei Zhu, Wangmeng Zuo, Lei Zhang, Simon C. K. Shiu, David Zhang category:cs.CV  published:2013-08-30 summary:With the rapid development of digital imaging and communication technologies, image set based face recognition (ISFR) is becoming increasingly important. One key issue of ISFR is how to effectively and efficiently represent the query face image set by using the gallery face image sets. The set-to-set distance based methods ignore the relationship between gallery sets, while representing the query set images individually over the gallery sets ignores the correlation between query set images. In this paper, we propose a novel image set based collaborative representation and classification method for ISFR. By modeling the query set as a convex or regularized hull, we represent this hull collaboratively over all the gallery sets. With the resolved representation coefficients, the distance between the query set and each gallery set can then be calculated for classification. The proposed model naturally and effectively extends the image based collaborative representation to an image set based one, and our extensive experiments on benchmark ISFR databases show the superiority of the proposed method to state-of-the-art ISFR methods under different set sizes in terms of both recognition rate and efficiency. version:1
arxiv-1306-3108 | Guaranteed Classification via Regularized Similarity Learning | http://arxiv.org/abs/1306.3108 | id:1306.3108 author:Zheng-Chu Guo, Yiming Ying category:cs.LG  published:2013-06-13 summary:Learning an appropriate (dis)similarity function from the available data is a central problem in machine learning, since the success of many machine learning algorithms critically depends on the choice of a similarity function to compare examples. Despite many approaches for similarity metric learning have been proposed, there is little theoretical study on the links between similarity met- ric learning and the classification performance of the result classifier. In this paper, we propose a regularized similarity learning formulation associated with general matrix-norms, and establish their generalization bounds. We show that the generalization error of the resulting linear separator can be bounded by the derived generalization bound of similarity learning. This shows that a good gen- eralization of the learnt similarity function guarantees a good classification of the resulting linear classifier. Our results extend and improve those obtained by Bellet at al. [3]. Due to the techniques dependent on the notion of uniform stability [6], the bound obtained there holds true only for the Frobenius matrix- norm regularization. Our techniques using the Rademacher complexity [5] and its related Khinchin-type inequality enable us to establish bounds for regularized similarity learning formulations associated with general matrix-norms including sparse L 1 -norm and mixed (2,1)-norm. version:2
arxiv-1102-2490 | The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond | http://arxiv.org/abs/1102.2490 | id:1102.2490 author:Aurélien Garivier, Olivier Cappé category:math.ST cs.LG cs.SY math.OC stat.TH 93E35  published:2011-02-12 summary:This paper presents a finite-time analysis of the KL-UCB algorithm, an online, horizon-free index policy for stochastic bandit problems. We prove two distinct results: first, for arbitrary bounded rewards, the KL-UCB algorithm satisfies a uniformly better regret bound than UCB or UCB2; second, in the special case of Bernoulli rewards, it reaches the lower bound of Lai and Robbins. Furthermore, we show that simple adaptations of the KL-UCB algorithm are also optimal for specific classes of (possibly unbounded) rewards, including those generated from exponential families of distributions. A large-scale numerical study comparing KL-UCB with its main competitors (UCB, UCB2, UCB-Tuned, UCB-V, DMED) shows that KL-UCB is remarkably efficient and stable, including for short time horizons. KL-UCB is also the only method that always performs better than the basic UCB policy. Our regret bounds rely on deviations results of independent interest which are stated and proved in the Appendix. As a by-product, we also obtain an improved regret bound for the standard UCB algorithm. version:5
arxiv-1308-6487 | A New Algorithm of Speckle Filtering using Stochastic Distances | http://arxiv.org/abs/1308.6487 | id:1308.6487 author:Leonardo Torres, Tamer Cavalcante, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML  published:2013-08-29 summary:This paper presents a new approach for filter design based on stochastic distances and tests between distributions. A window is defined around each pixel, overlapping samples are compared and only those which pass a goodness-of-fit test are used to compute the filtered value. The technique is applied to intensity SAR data with homogeneous regions using the Gamma model. The proposal is compared with the Lee's filter using a protocol based on Monte Carlo. Among the criteria used to quantify the quality of filters, we employ the equivalent number of looks, line and edge preservation. Moreover, we also assessed the filters by the Universal Image Quality Index and the Pearson's correlation on edges regions. version:1
arxiv-1204-4539 | Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows | http://arxiv.org/abs/1204.4539 | id:1204.4539 author:Julien Mairal, Bin Yu category:stat.ML cs.LG math.OC  published:2012-04-20 summary:We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called "path coding" penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efficiently solve by leveraging network flow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs. version:3
arxiv-1308-6401 | A Synergistic Approach for Recovering Occlusion-Free Textured 3D Maps of Urban Facades from Heterogeneous Cartographic Data | http://arxiv.org/abs/1308.6401 | id:1308.6401 author:Karim Hammoudi, Fadi Dornaika, Bahman Soheilian, Bruno Vallet, John McDonald, Nicolas Paparoditis category:cs.CV I.4; I.5  published:2013-08-29 summary:In this paper we present a practical approach for generating an occlusion-free textured 3D map of urban facades by the synergistic use of terrestrial images, 3D point clouds and area-based information. Particularly in dense urban environments, the high presence of urban objects in front of the facades causes significant difficulties for several stages in computational building modeling. Major challenges lie on the one hand in extracting complete 3D facade quadrilateral delimitations and on the other hand in generating occlusion-free facade textures. For these reasons, we describe a straightforward approach for completing and recovering facade geometry and textures by exploiting the data complementarity of terrestrial multi-source imagery and area-based information. version:1
arxiv-1307-4879 | Says who? Automatic Text-Based Content Analysis of Television News | http://arxiv.org/abs/1307.4879 | id:1307.4879 author:Carlos Castillo, Gianmarco De Francisci Morales, Marcelo Mendoza, Nasir Khan category:cs.CL cs.IR  published:2013-07-18 summary:We perform an automatic analysis of television news programs, based on the closed captions that accompany them. Specifically, we collect all the news broadcasted in over 140 television channels in the US during a period of six months. We start by segmenting, processing, and annotating the closed captions automatically. Next, we focus on the analysis of their linguistic style and on mentions of people using NLP methods. We present a series of key insights about news providers, people in the news, and we discuss the biases that can be uncovered by automatic means. These insights are contrasted by looking at the data from multiple points of view, including qualitative assessment. version:2
arxiv-1308-6388 | GNCGCP - Graduated NonConvexity and Graduated Concavity Procedure | http://arxiv.org/abs/1308.6388 | id:1308.6388 author:Zhi-Yong Liu, Hong Qiao category:cs.CV  published:2013-08-29 summary:In this paper we propose the Graduated NonConvexity and Graduated Concavity Procedure (GNCGCP) as a general optimization framework to approximately solve the combinatorial optimization problems on the set of partial permutation matrices. GNCGCP comprises two sub-procedures, graduated nonconvexity (GNC) which realizes a convex relaxation and graduated concavity (GC) which realizes a concave relaxation. It is proved that GNCGCP realizes exactly a type of convex-concave relaxation procedure (CCRP), but with a much simpler formulation without needing convex or concave relaxation in an explicit way. Actually, GNCGCP involves only the gradient of the objective function and is therefore very easy to use in practical applications. Two typical NP-hard problems, (sub)graph matching and quadratic assignment problem (QAP), are employed to demonstrate its simplicity and state-of-the-art performance. version:1
arxiv-1308-6384 | Collecting Coupons with Random Initial Stake | http://arxiv.org/abs/1308.6384 | id:1308.6384 author:Benjamin Doerr, Carola Doerr category:cs.DM cs.DS cs.NE F.2.2  published:2013-08-29 summary:Motivated by a problem in the theory of randomized search heuristics, we give a very precise analysis for the coupon collector problem where the collector starts with a random set of coupons (chosen uniformly from all sets). We show that the expected number of rounds until we have a coupon of each type is $nH_{n/2} - 1/2 \pm o(1)$, where $H_{n/2}$ denotes the $(n/2)$th harmonic number when $n$ is even, and $H_{n/2}:= (1/2) H_{\lfloor n/2 \rfloor} + (1/2) H_{\lceil n/2 \rceil}$ when $n$ is odd. Consequently, the coupon collector with random initial stake is by half a round faster than the one starting with exactly $n/2$ coupons (apart from additive $o(1)$ terms). This result implies that classic simple heuristic called \emph{randomized local search} needs an expected number of $nH_{n/2} - 1/2 \pm o(1)$ iterations to find the optimum of any monotonic function defined on bit-strings of length $n$. version:1
arxiv-0912-5193 | Ranking relations using analogies in biological and information networks | http://arxiv.org/abs/0912.5193 | id:0912.5193 author:Ricardo Silva, Katherine Heller, Zoubin Ghahramani, Edoardo M. Airoldi category:stat.ME cs.LG physics.soc-ph q-bio.QM stat.AP  published:2009-12-28 summary:Analogical reasoning depends fundamentally on the ability to learn and generalize about relations between objects. We develop an approach to relational learning which, given a set of pairs of objects $\mathbf{S}=\{A^{(1)}:B^{(1)},A^{(2)}:B^{(2)},\ldots,A^{(N)}:B ^{(N)}\}$, measures how well other pairs A:B fit in with the set $\mathbf{S}$. Our work addresses the following question: is the relation between objects A and B analogous to those relations found in $\mathbf{S}$? Such questions are particularly relevant in information retrieval, where an investigator might want to search for analogous pairs of objects that match the query set of interest. There are many ways in which objects can be related, making the task of measuring analogies very challenging. Our approach combines a similarity measure on function spaces with Bayesian analysis to produce a ranking. It requires data containing features of the objects of interest and a link matrix specifying which relationships exist; no further attributes of such relationships are necessary. We illustrate the potential of our method on text analysis and information networks. An application on discovering functional interactions between pairs of proteins is discussed in detail, where we show that our approach can work in practice even if a small set of protein pairs is provided. version:3
arxiv-1308-6319 | A proposition of a robust system for historical document images indexation | http://arxiv.org/abs/1308.6319 | id:1308.6319 author:Nizar Zaghden, Remy Mullot, Mohamed Adel Alimi category:cs.CV  published:2013-08-28 summary:Characterizing noisy or ancient documents is a challenging problem up to now. Many techniques have been done in order to effectuate feature extraction and image indexation for such documents. Global approaches are in general less robust and exact than local approaches. That's why, we propose in this paper, a hybrid system based on global approach(fractal dimension), and a local one based on SIFT descriptor. The Scale Invariant Feature Transform seems to do well with our application since it's rotation invariant and relatively robust to changing illumination.In the first step the calculation of fractal dimension is applied to images in order to eliminate images which have distant features than image request characteristics. Next, the SIFT is applied to show which images match well the request. However the average matching time using the hybrid approach is better than "fractal dimension" and "SIFT descriptor" if they are used alone. version:1
arxiv-1308-6311 | Categorizing ancient documents | http://arxiv.org/abs/1308.6311 | id:1308.6311 author:Nizar Zaghden, Remy Mullot, Mohamed Adel Alimi category:cs.CV  published:2013-08-28 summary:The analysis of historical documents is still a topical issue given the importance of information that can be extracted and also the importance given by the institutions to preserve their heritage. The main idea in order to characterize the content of the images of ancient documents after attempting to clean the image is segmented blocks texts from the same image and tries to find similar blocks in either the same image or the entire image database. Most approaches of offline handwriting recognition proceed by segmenting words into smaller pieces (usually characters) which are recognized separately. Recognition of a word then requires the recognition of all characters (OCR) that compose it. Our work focuses mainly on the characterization of classes in images of old documents. We use Som toolbox for finding classes in documents. We applied also fractal dimensions and points of interest to categorize and match ancient documents. version:1
arxiv-1308-6309 | Text recognition in both ancient and cartographic documents | http://arxiv.org/abs/1308.6309 | id:1308.6309 author:Nizar Zaghden, Badreddine Khelifi, Adel M. Alimi, Remy Mullot category:cs.CV  published:2013-08-28 summary:This paper deals with the recognition and matching of text in both cartographic maps and ancient documents. The purpose of this work is to find similar text regions based on statistical and global features. A phase of normalization is done first, in object to well categorize the same quantity of information. A phase of wordspotting is done next by combining local and global features. We make different experiments by combining the different techniques of extracting features in order to obtain better results in recognition phase. We applied fontspotting on both ancient documents and cartographic ones. We also applied the wordspotting in which we adopted a new technique which tries to compare the images of character and not the entire images words. We present the precision and recall values obtained with three methods for the new method of wordspotting applied on characters only. version:1
arxiv-1308-6300 | Computing Lexical Contrast | http://arxiv.org/abs/1308.6300 | id:1308.6300 author:Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, Peter D. Turney category:cs.CL  published:2013-08-28 summary:Knowing the degree of semantic contrast between words has widespread application in natural language processing, including machine translation, information retrieval, and dialogue systems. Manually-created lexicons focus on opposites, such as {\rm hot} and {\rm cold}. Opposites are of many kinds such as antipodals, complementaries, and gradable. However, existing lexicons often do not classify opposites into the different kinds. They also do not explicitly list word pairs that are not opposites but yet have some degree of contrast in meaning, such as {\rm warm} and {\rm cold} or {\rm tropical} and {\rm freezing}. We propose an automatic method to identify contrasting word pairs that is based on the hypothesis that if a pair of words, $A$ and $B$, are contrasting, then there is a pair of opposites, $C$ and $D$, such that $A$ and $C$ are strongly related and $B$ and $D$ are strongly related. (For example, there exists the pair of opposites {\rm hot} and {\rm cold} such that {\rm tropical} is related to {\rm hot,} and {\rm freezing} is related to {\rm cold}.) We will call this the contrast hypothesis. We begin with a large crowdsourcing experiment to determine the amount of human agreement on the concept of oppositeness and its different kinds. In the process, we flesh out key features of different kinds of opposites. We then present an automatic and empirical measure of lexical contrast that relies on the contrast hypothesis, corpus statistics, and the structure of a {\it Roget}-like thesaurus. We show that the proposed measure of lexical contrast obtains high precision and large coverage, outperforming existing methods. version:1
arxiv-1308-6297 | Crowdsourcing a Word-Emotion Association Lexicon | http://arxiv.org/abs/1308.6297 | id:1308.6297 author:Saif M. Mohammad, Peter D. Turney category:cs.CL  published:2013-08-28 summary:Even though considerable attention has been given to the polarity of words (positive and negative) and the creation of large polarity lexicons, research in emotion analysis has had to rely on limited and small emotion lexicons. In this paper we show how the combined strength and wisdom of the crowds can be used to generate a large, high-quality, word-emotion and word-polarity association lexicon quickly and inexpensively. We enumerate the challenges in emotion annotation in a crowdsourcing scenario and propose solutions to address them. Most notably, in addition to questions about emotions associated with terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term (allowing us to reject such annotations), and help obtain annotations at sense level (rather than at word level). We conducted experiments on how to formulate the emotion-annotation questions, and show that asking if a term is associated with an emotion leads to markedly higher inter-annotator agreement than that obtained by asking if a term evokes an emotion. version:1
arxiv-1308-6242 | NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets | http://arxiv.org/abs/1308.6242 | id:1308.6242 author:Saif M. Mohammad, Svetlana Kiritchenko, Xiaodan Zhu category:cs.CL  published:2013-08-28 summary:In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a submissions stood first in both tasks on tweets, obtaining an F-score of 69.02 in the message-level task and 88.93 in the term-level task. We implemented a variety of surface-form, semantic, and sentiment features. with sentiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 F-score points over all others. Both of our systems can be replicated us available resources. version:1
arxiv-1308-6181 | Bayesian Conditional Gaussian Network Classifiers with Applications to Mass Spectra Classification | http://arxiv.org/abs/1308.6181 | id:1308.6181 author:Victor Bellon, Jesus Cerquides, Ivo Grosse category:cs.LG stat.ML  published:2013-08-28 summary:Classifiers based on probabilistic graphical models are very effective. In continuous domains, maximum likelihood is usually used to assess the predictions of those classifiers. When data is scarce, this can easily lead to overfitting. In any probabilistic setting, Bayesian averaging (BA) provides theoretically optimal predictions and is known to be robust to overfitting. In this work we introduce Bayesian Conditional Gaussian Network Classifiers, which efficiently perform exact Bayesian averaging over the parameters. We evaluate the proposed classifiers against the maximum likelihood alternatives proposed so far over standard UCI datasets, concluding that performing BA improves the quality of the assessed probabilities (conditional log likelihood) whilst maintaining the error rate. Overfitting is more likely to occur in domains where the number of data items is small and the number of variables is large. These two conditions are met in the realm of bioinformatics, where the early diagnosis of cancer from mass spectra is a relevant task. We provide an application of our classification framework to that problem, comparing it with the standard maximum likelihood alternative, where the improvement of quality in the assessed probabilities is confirmed. version:1
arxiv-1205-2536 | Identifiability of Gaussian structural equation models with equal error variances | http://arxiv.org/abs/1205.2536 | id:1205.2536 author:Jonas Peters, Peter Bühlmann category:stat.ML math.ST stat.TH  published:2012-05-11 summary:We consider structural equation models in which variables can be written as a function of their parents and noise terms, which are assumed to be jointly independent. Corresponding to each structural equation model, there is a directed acyclic graph describing the relationships between the variables. In Gaussian structural equation models with linear functions, the graph can be identified from the joint distribution only up to Markov equivalence classes, assuming faithfulness. In this work, we prove full identifiability if all noise variables have the same variances: the directed acyclic graph can be recovered from the joint Gaussian distribution. Our result has direct implications for causal inference: if the data follow a Gaussian structural equation model with equal error variances and assuming that all variables are observed, the causal structure can be inferred from observational data only. We propose a statistical method and an algorithm that exploit our theoretical findings. version:3
arxiv-1308-6744 | Preventing Disclosure of Sensitive Knowledge by Hiding Inference | http://arxiv.org/abs/1308.6744 | id:1308.6744 author:A. S. Syed Navaz, M. Ravi, T. Prabhu category:cs.CR cs.DB cs.LG  published:2013-08-28 summary:Data Mining is a way of extracting data or uncovering hidden patterns of information from databases. So, there is a need to prevent the inference rules from being disclosed such that the more secure data sets cannot be identified from non sensitive attributes. This can be done through removing or adding certain item sets in the transactions Sanitization. The purpose is to hide the Inference rules, so that the user may not be able to discover any valuable information from other non sensitive data and any organisation can release all samples of their data without the fear of Knowledge Discovery In Databases which can be achieved by investigating frequently occurring item sets, rules that can be mined from them with the objective of hiding them. Another way is to release only limited samples in the new database so that there is no information loss and it also satisfies the legitimate needs of the users. The major problem is uncovering hidden patterns, which causes a threat to the database security. Sensitive data are inferred from non-sensitive data based on the semantics of the application the user has, commonly known as the inference problem. Two fundamental approaches to protect sensitive rules from disclosure are that, preventing rules from being generated by hiding the frequent sets of data items and reducing the importance of the rules by setting their confidence below a user-specified threshold. version:1
arxiv-1308-6056 | Brain MRI Segmentation with Fast and Globally Convex Multiphase Active Contours | http://arxiv.org/abs/1308.6056 | id:1308.6056 author:Juan C. Moreno, V. B. S. Prasath, Hugo Proenca, K. Palaniappan category:cs.CV 68U10 I.4.6  published:2013-08-28 summary:Multiphase active contour based models are useful in identifying multiple regions with different characteristics such as the mean values of regions. This is relevant in brain magnetic resonance images (MRIs), allowing the differentiation of white matter against gray matter. We consider a well defined globally convex formulation of Vese and Chan multiphase active contour model for segmenting brain MRI images. A well-established theory and an efficient dual minimization scheme are thoroughly described which guarantees optimal solutions and provides stable segmentations. Moreover, under the dual minimization implementation our model perfectly describes disjoint regions by avoiding local minima solutions. Experimental results indicate that the proposed approach provides better accuracy than other related multiphase active contour algorithms even under severe noise, intensity inhomogeneities, and partial volume effects. version:1
arxiv-1009-3613 | On the Doubt about Margin Explanation of Boosting | http://arxiv.org/abs/1009.3613 | id:1009.3613 author:Wei Gao, Zhi-Hua Zhou category:cs.LG 68Q01  68Q32 68Txx  published:2010-09-19 summary:Margin theory provides one of the most popular explanations to the success of \texttt{AdaBoost}, where the central point lies in the recognition that \textit{margin} is the key for characterizing the performance of \texttt{AdaBoost}. This theory has been very influential, e.g., it has been used to argue that \texttt{AdaBoost} usually does not overfit since it tends to enlarge the margin even after the training error reaches zero. Previously the \textit{minimum margin bound} was established for \texttt{AdaBoost}, however, \cite{Breiman1999} pointed out that maximizing the minimum margin does not necessarily lead to a better generalization. Later, \cite{Reyzin:Schapire2006} emphasized that the margin distribution rather than minimum margin is crucial to the performance of \texttt{AdaBoost}. In this paper, we first present the \textit{$k$th margin bound} and further study on its relationship to previous work such as the minimum margin bound and Emargin bound. Then, we improve the previous empirical Bernstein bounds \citep{Maurer:Pontil2009,Audibert:Munos:Szepesvari2009}, and based on such findings, we defend the margin-based explanation against Breiman's doubts by proving a new generalization error bound that considers exactly the same factors as \cite{Schapire:Freund:Bartlett:Lee1998} but is sharper than \cite{Breiman1999}'s minimum margin bound. By incorporating factors such as average margin and variance, we present a generalization error bound that is heavily related to the whole margin distribution. We also provide margin distribution bounds for generalization error of voting classifiers in finite VC-dimension space. version:5
arxiv-1308-6003 | Improving Sparse Associative Memories by Escaping from Bogus Fixed Points | http://arxiv.org/abs/1308.6003 | id:1308.6003 author:Zhe Yao, Vincent Gripon, Michael Rabbat category:cs.NE cs.IT math.IT  published:2013-08-27 summary:The Gripon-Berrou neural network (GBNN) is a recently invented recurrent neural network embracing a LDPC-like sparse encoding setup which makes it extremely resilient to noise and errors. A natural use of GBNN is as an associative memory. There are two activation rules for the neuron dynamics, namely sum-of-sum and sum-of-max. The latter outperforms the former in terms of retrieval rate by a huge margin. In prior discussions and experiments, it is believed that although sum-of-sum may lead the network to oscillate, sum-of-max always converges to an ensemble of neuron cliques corresponding to previously stored patterns. However, this is not entirely correct. In fact, sum-of-max often converges to bogus fixed points where the ensemble only comprises a small subset of the converged state. By taking advantage of this overlooked fact, we can greatly improve the retrieval rate. We discuss this particular issue and propose a number of heuristics to push sum-of-max beyond these bogus fixed points. To tackle the problem directly and completely, a novel post-processing algorithm is also developed and customized to the structure of GBNN. Experimental results show that the new algorithm achieves a huge performance boost in terms of both retrieval rate and run-time, compared to the standard sum-of-max and all the other heuristics. version:1
arxiv-1308-5876 | Hierarchized block wise image approximation by greedy pursuit strategies | http://arxiv.org/abs/1308.5876 | id:1308.5876 author:Laura Rebollo-Neira, Ryszard Maciol, Shabnam Bibi category:cs.CV 68U10  94A08 G.1.2  published:2013-08-27 summary:An approach for effective implementation of greedy selection methodologies, to approximate an image partitioned into blocks, is proposed. The method is specially designed for approximating partitions on a transformed image. It evolves by selecting, at each iteration step, i) the elements for approximating each of the blocks partitioning the image and ii) the hierarchized sequence in which the blocks are approximated to reach the required global condition on sparsity. version:1
arxiv-1308-5835 | Backhaul-Aware Interference Management in the Uplink of Wireless Small Cell Networks | http://arxiv.org/abs/1308.5835 | id:1308.5835 author:Sumudu Samarakoon, Mehdi Bennis, Walid Saad, Matti Latva-aho category:cs.NI cs.GT cs.LG  published:2013-08-27 summary:The design of distributed mechanisms for interference management is one of the key challenges in emerging wireless small cell networks whose backhaul is capacity limited and heterogeneous (wired, wireless and a mix thereof). In this paper, a novel, backhaul-aware approach to interference management in wireless small cell networks is proposed. The proposed approach enables macrocell user equipments (MUEs) to optimize their uplink performance, by exploiting the presence of neighboring small cell base stations. The problem is formulated as a noncooperative game among the MUEs that seek to optimize their delay-rate tradeoff, given the conditions of both the radio access network and the -- possibly heterogeneous -- backhaul. To solve this game, a novel, distributed learning algorithm is proposed using which the MUEs autonomously choose their optimal uplink transmission strategies, given a limited amount of available information. The convergence of the proposed algorithm is shown and its properties are studied. Simulation results show that, under various types of backhauls, the proposed approach yields significant performance gains, in terms of both average throughput and delay for the MUEs, when compared to existing benchmark algorithms. version:1
arxiv-1308-5807 | Multi-Objective Particle Swarm Optimization for Facility Location Problem in Wireless Mesh Networks | http://arxiv.org/abs/1308.5807 | id:1308.5807 author:Tarik Mountassir Bou, Abdelkrim Haqiq, Samir Bennani category:cs.NI cs.NE  published:2013-08-27 summary:Wireless mesh networks have seen a real progress due of their implementation at a low cost. They present one of Next Generation Networks technologies and can serve as home, companies and universities networks. In this paper, we propose and discuss a new multi-objective model for nodes deployment optimization in Multi-Radio Multi-Channel Wireless Mesh Networks. We exploit the trade-off between network cost and the overall network performance. This optimization problem is solved simultaneously by using a meta-heuristic method that returns a non-dominated set of near optimal solutions. A comparative study was driven to evaluate the efficiency of the proposed model. version:1
arxiv-1308-5712 | The Generalized Mean Information Coefficient | http://arxiv.org/abs/1308.5712 | id:1308.5712 author:Alexander Luedtke, Linh Tran category:stat.ML  published:2013-08-26 summary:Reshef & Reshef recently published a paper in which they present a method called the Maximal Information Coefficient (MIC) that can detect all forms of statistical dependence between pairs of variables as sample size goes to infinity. While this method has been praised by some, it has also been criticized for its lack of power in finite samples. We seek to modify MIC so that it has higher power in detecting associations for limited sample sizes. Here we present the Generalized Mean Information Coefficient (GMIC), a generalization of MIC which incorporates a tuning parameter that can be used to modify the complexity of the association favored by the measure. We define GMIC and prove it maintains several key asymptotic properties of MIC. Its increased power over MIC is demonstrated using a simulation of eight different functional relationships at sixty different noise levels. The results are compared to the Pearson correlation, distance correlation, and MIC. Simulation results suggest that while generally GMIC has slightly lower power than the distance correlation measure, it achieves higher power than MIC for many forms of underlying association. For some functional relationships, GMIC surpasses all other statistics calculated. Preliminary results suggest choosing a moderate value of the tuning parameter for GMIC will yield a test that is robust across underlying relationships. GMIC is a promising new method that mitigates the power issues suffered by MIC, at the possible expense of equitability. Nonetheless, distance correlation was in our simulations more powerful for many forms of underlying relationships. At a minimum, this work motivates further consideration of maximal information-based nonparametric exploration (MINE) methods as statistical tests of independence. version:1
arxiv-1308-5661 | Detection of copy-move forgery in digital images based on DCT | http://arxiv.org/abs/1308.5661 | id:1308.5661 author:Nathalie Diane Wandji, Sun Xingming, Moise Fah Kue category:cs.CV cs.CR  published:2013-08-26 summary:With rapid advances in digital information processing systems, and more specifically in digital image processing software, there is a widespread development of advanced tools and techniques for digital image forgery. One of the techniques most commonly used is the Copy-move forgery which proceeds by copying a part of an image and pasting it into the same image, in order to maliciously hide an object or a region. In this paper, we propose a method to detect this specific kind of counterfeit. Firstly, the color image is converted from RGB color space to YCbCr color space and then the R, G, B and Y-component are splitted into fixed-size overlapping blocks and, features are extracted from the R, G and B-components image blocks on one hand and on the other, from the DCT representation of the R, G, B and Ycomponent image block. The feature vectors obtained are then lexicographically sorted to make similar image blocks neighbors and duplicated image blocks are identified using Euclidean distance as similarity criterion. Experimental results showed that the proposed method can detect the duplicated regions when there is more than one copy move forged area in the image and even in case of slight rotations, JPEG compression, shift, scale, blur and noise addition. version:1
arxiv-1308-5576 | A Comparison of Algorithms for Learning Hidden Variables in Normal Graphs | http://arxiv.org/abs/1308.5576 | id:1308.5576 author:Francesco A. N. Palmieri category:stat.ML cs.IT cs.SY math.IT  published:2013-08-26 summary:A Bayesian factor graph reduced to normal form consists in the interconnection of diverter units (or equal constraint units) and Single-Input/Single-Output (SISO) blocks. In this framework localized adaptation rules are explicitly derived from a constrained maximum likelihood (ML) formulation and from a minimum KL-divergence criterion using KKT conditions. The learning algorithms are compared with two other updating equations based on a Viterbi-like and on a variational approximation respectively. The performance of the various algorithm is verified on synthetic data sets for various architectures. The objective of this paper is to provide the programmer with explicit algorithms for rapid deployment of Bayesian graphs in the applications. version:1
arxiv-1308-5546 | Sparse and Non-Negative BSS for Noisy Data | http://arxiv.org/abs/1308.5546 | id:1308.5546 author:Jérémy Rapin, Jérôme Bobin, Anthony Larue, Jean-Luc Starck category:stat.ML cs.LG 94A12 I.5.4  published:2013-08-26 summary:Non-negative blind source separation (BSS) has raised interest in various fields of research, as testified by the wide literature on the topic of non-negative matrix factorization (NMF). In this context, it is fundamental that the sources to be estimated present some diversity in order to be efficiently retrieved. Sparsity is known to enhance such contrast between the sources while producing very robust approaches, especially to noise. In this paper we introduce a new algorithm in order to tackle the blind separation of non-negative sparse sources from noisy measurements. We first show that sparsity and non-negativity constraints have to be carefully applied on the sought-after solution. In fact, improperly constrained solutions are unlikely to be stable and are therefore sub-optimal. The proposed algorithm, named nGMCA (non-negative Generalized Morphological Component Analysis), makes use of proximal calculus techniques to provide properly constrained solutions. The performance of nGMCA compared to other state-of-the-art algorithms is demonstrated by numerical experiments encompassing a wide variety of settings, with negligible parameter tuning. In particular, nGMCA is shown to provide robustness to noise and performs well on synthetic mixtures of real NMR spectra. version:1
arxiv-1308-5499 | Linear models and linear mixed effects models in R with linguistic applications | http://arxiv.org/abs/1308.5499 | id:1308.5499 author:Bodo Winter category:cs.CL  published:2013-08-26 summary:This text is a conceptual introduction to mixed effects modeling with linguistic applications, using the R programming environment. The reader is introduced to linear modeling and assumptions, as well as to mixed effects/multilevel modeling, including a discussion of random intercepts, random slopes and likelihood ratio tests. The example used throughout the text focuses on the phonetic analysis of voice pitch data. version:1
arxiv-1203-0565 | Fast learning rate of multiple kernel learning: Trade-off between sparsity and smoothness | http://arxiv.org/abs/1203.0565 | id:1203.0565 author:Taiji Suzuki, Masashi Sugiyama category:stat.ML math.ST stat.TH  published:2012-03-02 summary:We investigate the learning rate of multiple kernel learning (MKL) with $\ell_1$ and elastic-net regularizations. The elastic-net regularization is a composition of an $\ell_1$-regularizer for inducing the sparsity and an $\ell_2$-regularizer for controlling the smoothness. We focus on a sparse setting where the total number of kernels is large, but the number of nonzero components of the ground truth is relatively small, and show sharper convergence rates than the learning rates have ever shown for both $\ell_1$ and elastic-net regularizations. Our analysis reveals some relations between the choice of a regularization function and the performance. If the ground truth is smooth, we show a faster convergence rate for the elastic-net regularization with less conditions than $\ell_1$-regularization; otherwise, a faster convergence rate for the $\ell_1$-regularization is shown. version:2
arxiv-1308-5465 | Stability of Phase Retrievable Frames | http://arxiv.org/abs/1308.5465 | id:1308.5465 author:Radu Balan category:math.FA cs.CV stat.ML 15A29  65H10  90C26  published:2013-08-25 summary:In this paper we study the property of phase retrievability by redundant sysems of vectors under perturbations of the frame set. Specifically we show that if a set $\fc$ of $m$ vectors in the complex Hilbert space of dimension n allows for vector reconstruction from magnitudes of its coefficients, then there is a perturbation bound $\rho$ so that any frame set within $\rho$ from $\fc$ has the same property. In particular this proves the recent construction in \cite{BH13} is stable under perturbations. By the same token we reduce the critical cardinality conjectured in \cite{BCMN13a} to proving a stability result for non phase-retrievable frames. version:1
arxiv-1209-1064 | A Max-Product EM Algorithm for Reconstructing Markov-tree Sparse Signals from Compressive Samples | http://arxiv.org/abs/1209.1064 | id:1209.1064 author:Zhao Song, Aleksandar Dogandzic category:stat.ML cs.IT math.IT  published:2012-09-05 summary:We propose a Bayesian expectation-maximization (EM) algorithm for reconstructing Markov-tree sparse signals via belief propagation. The measurements follow an underdetermined linear model where the regression-coefficient vector is the sum of an unknown approximately sparse signal and a zero-mean white Gaussian noise with an unknown variance. The signal is composed of large- and small-magnitude components identified by binary state variables whose probabilistic dependence structure is described by a Markov tree. Gaussian priors are assigned to the signal coefficients given their state variables and the Jeffreys' noninformative prior is assigned to the noise variance. Our signal reconstruction scheme is based on an EM iteration that aims at maximizing the posterior distribution of the signal and its state variables given the noise variance. We construct the missing data for the EM iteration so that the complete-data posterior distribution corresponds to a hidden Markov tree (HMT) probabilistic graphical model that contains no loops and implement its maximization (M) step via a max-product algorithm. This EM algorithm estimates the vector of state variables as well as solves iteratively a linear system of equations to obtain the corresponding signal estimate. We select the noise variance so that the corresponding estimated signal and state variables obtained upon convergence of the EM iteration have the largest marginal posterior distribution. We compare the proposed and existing state-of-the-art reconstruction methods via signal and image reconstruction experiments. version:4
arxiv-1006-2513 | On the Achievability of Cramér-Rao Bound In Noisy Compressed Sensing | http://arxiv.org/abs/1006.2513 | id:1006.2513 author:Rad Niazadeh, Masoud Babaie-Zadeh, Christian Jutten category:cs.IT cs.LG math.IT  published:2010-06-13 summary:Recently, it has been proved in Babadi et al. that in noisy compressed sensing, a joint typical estimator can asymptotically achieve the Cramer-Rao lower bound of the problem.To prove this result, this paper used a lemma,which is provided in Akcakaya et al,that comprises the main building block of the proof. This lemma is based on the assumption of Gaussianity of the measurement matrix and its randomness in the domain of noise. In this correspondence, we generalize the results obtained in Babadi et al by dropping the Gaussianity assumption on the measurement matrix. In fact, by considering the measurement matrix as a deterministic matrix in our analysis, we find a theorem similar to the main theorem of Babadi et al for a family of randomly generated (but deterministic in the noise domain) measurement matrices that satisfy a generalized condition known as The Concentration of Measures Inequality. By this, we finally show that under our generalized assumptions, the Cramer-Rao bound of the estimation is achievable by using the typical estimator introduced in Babadi et al. version:3
arxiv-1308-5423 | A Literature Review: Stemming Algorithms for Indian Languages | http://arxiv.org/abs/1308.5423 | id:1308.5423 author:M. Thangarasu, R. Manavalan category:cs.CL  published:2013-08-25 summary:Stemming is the process of extracting root word from the given inflection word. It also plays significant role in numerous application of Natural Language Processing (NLP). The stemming problem has addressed in many contexts and by researchers in many disciplines. This expository paper presents survey of some of the latest developments on stemming algorithms in data mining and also presents with some of the solutions for various Indian language stemming algorithms along with the results. version:1
arxiv-1308-4565 | Decentralized Online Big Data Classification - a Bandit Framework | http://arxiv.org/abs/1308.4565 | id:1308.4565 author:Cem Tekin, Mihaela van der Schaar category:cs.LG cs.MA  published:2013-08-21 summary:Distributed, online data mining systems have emerged as a result of applications requiring analysis of large amounts of correlated and high-dimensional data produced by multiple distributed data sources. We propose a distributed online data classification framework where data is gathered by distributed data sources and processed by a heterogeneous set of distributed learners which learn online, at run-time, how to classify the different data streams either by using their locally available classification functions or by helping each other by classifying each other's data. Importantly, since the data is gathered at different locations, sending the data to another learner to process incurs additional costs such as delays, and hence this will be only beneficial if the benefits obtained from a better classification will exceed the costs. We assume that the classification functions available to each processing element are fixed, but their prediction accuracy for various types of incoming data are unknown and can change dynamically over time, and thus they need to be learned online. We model the problem of joint classification by the distributed and heterogeneous learners from multiple data sources as a distributed contextual bandit problem where each data is characterized by a specific context. We develop distributed online learning algorithms for which we can prove that they have sublinear regret. Compared to prior work in distributed online data mining, our work is the first to provide analytic regret results characterizing the performance of the proposed algorithms. version:2
arxiv-1308-5338 | A stochastic hybrid model of a biological filter | http://arxiv.org/abs/1308.5338 | id:1308.5338 author:Andrea Ocone, Guido Sanguinetti category:cs.LG cs.CE q-bio.MN  published:2013-08-24 summary:We present a hybrid model of a biological filter, a genetic circuit which removes fast fluctuations in the cell's internal representation of the extra cellular environment. The model takes the classic feed-forward loop (FFL) motif and represents it as a network of continuous protein concentrations and binary, unobserved gene promoter states. We address the problem of statistical inference and parameter learning for this class of models from partial, discrete time observations. We show that the hybrid representation leads to an efficient algorithm for approximate statistical inference in this circuit, and show its effectiveness on a simulated data set. version:1
arxiv-1308-5329 | Monitoring with uncertainty | http://arxiv.org/abs/1308.5329 | id:1308.5329 author:Ezio Bartocci, Radu Grosu category:cs.LO cs.LG cs.SY  published:2013-08-24 summary:We discuss the problem of runtime verification of an instrumented program that misses to emit and to monitor some events. These gaps can occur when a monitoring overhead control mechanism is introduced to disable the monitor of an application with real-time constraints. We show how to use statistical models to learn the application behavior and to "fill in" the introduced gaps. Finally, we present and discuss some techniques developed in the last three years to estimate the probability that a property of interest is violated in the presence of an incomplete trace. version:1
arxiv-1308-5315 | Edge-detection applied to moving sand dunes on Mars | http://arxiv.org/abs/1308.5315 | id:1308.5315 author:Amelia Carolina Sparavigna category:cs.CV  published:2013-08-24 summary:Here we discuss the application of an edge detection filter, the Sobel filter of GIMP, to the recently discovered motion of some sand dunes on Mars. The filter allows a good comparison of an image HiRISE of 2007 and an image of 1999 recorded by the Mars Global Surveyor of the dunes in the Nili Patera caldera, measuring therefore the motion of the dunes on a longer period of time than that previously investigated. version:1
arxiv-1207-0560 | Algorithms for Approximate Minimization of the Difference Between Submodular Functions, with Applications | http://arxiv.org/abs/1207.0560 | id:1207.0560 author:Rishabh Iyer, Jeff Bilmes category:cs.DS cs.LG  published:2012-07-03 summary:We extend the work of Narasimhan and Bilmes [30] for minimizing set functions representable as a difference between submodular functions. Similar to [30], our new algorithms are guaranteed to monotonically reduce the objective function at every step. We empirically and theoretically show that the per-iteration cost of our algorithms is much less than [30], and our algorithms can be used to efficiently minimize a difference between submodular functions under various combinatorial constraints, a problem not previously addressed. We provide computational bounds and a hardness result on the mul- tiplicative inapproximability of minimizing the difference between submodular functions. We show, however, that it is possible to give worst-case additive bounds by providing a polynomial time computable lower-bound on the minima. Finally we show how a number of machine learning problems can be modeled as minimizing the difference between submodular functions. We experimentally show the validity of our algorithms by testing them on the problem of feature selection with submodular cost features. version:4
arxiv-1308-5281 | Ensemble of Distributed Learners for Online Classification of Dynamic Data Streams | http://arxiv.org/abs/1308.5281 | id:1308.5281 author:Luca Canzian, Yu Zhang, Mihaela van der Schaar category:cs.LG  published:2013-08-24 summary:We present an efficient distributed online learning scheme to classify data captured from distributed, heterogeneous, and dynamic data sources. Our scheme consists of multiple distributed local learners, that analyze different streams of data that are correlated to a common event that needs to be classified. Each learner uses a local classifier to make a local prediction. The local predictions are then collected by each learner and combined using a weighted majority rule to output the final prediction. We propose a novel online ensemble learning algorithm to update the aggregation rule in order to adapt to the underlying data dynamics. We rigorously determine a bound for the worst case misclassification probability of our algorithm which depends on the misclassification probabilities of the best static aggregation rule, and of the best local classifier. Importantly, the worst case misclassification probability of our algorithm tends asymptotically to 0 if the misclassification probability of the best static aggregation rule or the misclassification probability of the best local classifier tend to 0. Then we extend our algorithm to address challenges specific to the distributed implementation and we prove new bounds that apply to these settings. Finally, we test our scheme by performing an evaluation study on several data sets. When applied to data sets widely used by the literature dealing with dynamic data streams and concept drift, our scheme exhibits performance gains ranging from 34% to 71% with respect to state of the art solutions. version:1
arxiv-1308-5275 | The Lovasz-Bregman Divergence and connections to rank aggregation, clustering, and web ranking | http://arxiv.org/abs/1308.5275 | id:1308.5275 author:Rishabh Iyer, Jeff Bilmes category:cs.LG cs.IR stat.ML  published:2013-08-24 summary:We extend the recently introduced theory of Lovasz-Bregman (LB) divergences (Iyer & Bilmes, 2012) in several ways. We show that they represent a distortion between a 'score' and an 'ordering', thus providing a new view of rank aggregation and order based clustering with interesting connections to web ranking. We show how the LB divergences have a number of properties akin to many permutation based metrics, and in fact have as special cases forms very similar to the Kendall-$\tau$ metric. We also show how the LB divergences subsume a number of commonly used ranking measures in information retrieval, like the NDCG and AUC. Unlike the traditional permutation based metrics, however, the LB divergence naturally captures a notion of "confidence" in the orderings, thus providing a new representation to applications involving aggregating scores as opposed to just orderings. We show how a number of recently used web ranking models are forms of Lovasz-Bregman rank aggregation and also observe that a natural form of Mallow's model using the LB divergence has been used as conditional ranking models for the 'Learning to Rank' problem. version:1
arxiv-1306-6111 | Understanding the Predictive Power of Computational Mechanics and Echo State Networks in Social Media | http://arxiv.org/abs/1306.6111 | id:1306.6111 author:David Darmon, Jared Sylvester, Michelle Girvan, William Rand category:cs.SI cs.LG physics.soc-ph stat.AP stat.ML  published:2013-06-26 summary:There is a large amount of interest in understanding users of social media in order to predict their behavior in this space. Despite this interest, user predictability in social media is not well-understood. To examine this question, we consider a network of fifteen thousand users on Twitter over a seven week period. We apply two contrasting modeling paradigms: computational mechanics and echo state networks. Both methods attempt to model the behavior of users on the basis of their past behavior. We demonstrate that the behavior of users on Twitter can be well-modeled as processes with self-feedback. We find that the two modeling approaches perform very similarly for most users, but that they differ in performance on a small subset of the users. By exploring the properties of these performance-differentiated users, we highlight the challenges faced in applying predictive models to dynamic social data. version:2
arxiv-1302-6557 | Geodesic-based Salient Object Detection | http://arxiv.org/abs/1302.6557 | id:1302.6557 author:Richard M Jiang category:cs.CV cs.AI  published:2013-02-26 summary:Saliency detection has been an intuitive way to provide useful cues for object detection and segmentation, as desired for many vision and graphics applications. In this paper, we provided a robust method for salient object detection and segmentation. Other than using various pixel-level contrast definitions, we exploited global image structures and proposed a new geodesic method dedicated for salient object detection. In the proposed approach, a new geodesic scheme, namely geodesic tunneling is proposed to tackle with textures and local chaotic structures. With our new geodesic approach, a geodesic saliency map is estimated in correspondence to spatial structures in an image. Experimental evaluation on a salient object benchmark dataset validated that our algorithm consistently outperformed a number of the state-of-art saliency methods, yielding higher precision and better recall rates. With the robust saliency estimation, we also present an unsupervised hierarchical salient object cut scheme simply using adaptive saliency thresholding, which attained the highest score in our F-measure test. We also applied our geodesic cut scheme to a number of image editing tasks as demonstrated in additional experiments. version:2
arxiv-1308-5200 | Manopt, a Matlab toolbox for optimization on manifolds | http://arxiv.org/abs/1308.5200 | id:1308.5200 author:Nicolas Boumal, Bamdev Mishra, P. -A. Absil, Rodolphe Sepulchre category:cs.MS cs.LG math.OC stat.ML  published:2013-08-23 summary:Optimization on manifolds is a rapidly developing branch of nonlinear optimization. Its focus is on problems where the smooth geometry of the search space can be leveraged to design efficient numerical algorithms. In particular, optimization on manifolds is well-suited to deal with rank and orthogonality constraints. Such structured constraints appear pervasively in machine learning applications, including low-rank matrix completion, sensor network localization, camera network registration, independent component analysis, metric learning, dimensionality reduction and so on. The Manopt toolbox, available at www.manopt.org, is a user-friendly, documented piece of software dedicated to simplify experimenting with state of the art Riemannian optimization algorithms. We aim particularly at reaching practitioners outside our field. version:1
arxiv-1306-5550 | Spectral redemption: clustering sparse networks | http://arxiv.org/abs/1306.5550 | id:1306.5550 author:Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zdeborová, Pan Zhang category:cs.SI cond-mat.stat-mech physics.soc-ph stat.ML  published:2013-06-24 summary:Spectral algorithms are classic approaches to clustering and community detection in networks. However, for sparse networks the standard versions of these algorithms are suboptimal, in some cases completely failing to detect communities even when other algorithms such as belief propagation can do so. Here we introduce a new class of spectral algorithms based on a non-backtracking walk on the directed edges of the graph. The spectrum of this operator is much better-behaved than that of the adjacency matrix or other commonly used matrices, maintaining a strong separation between the bulk eigenvalues and the eigenvalues relevant to community structure even in the sparse case. We show that our algorithm is optimal for graphs generated by the stochastic block model, detecting communities all the way down to the theoretical limit. We also show the spectrum of the non-backtracking operator for some real-world networks, illustrating its advantages over traditional spectral clustering. version:2
arxiv-1209-2160 | Group descent algorithms for nonconvex penalized linear and logistic regression models with grouped predictors | http://arxiv.org/abs/1209.2160 | id:1209.2160 author:Patrick Breheny, Jian Huang category:stat.CO stat.ML  published:2012-09-10 summary:Penalized regression is an attractive framework for variable selection problems. Often, variables possess a grouping structure, and the relevant selection problem is that of selecting groups, not individual variables. The group lasso has been proposed as a way of extending the ideas of the lasso to the problem of group selection. Nonconvex penalties such as SCAD and MCP have been proposed and shown to have several advantages over the lasso; these penalties may also be extended to the group selection problem, giving rise to group SCAD and group MCP methods. Here, we describe algorithms for fitting these models stably and efficiently. In addition, we present simulation results and real data examples comparing and contrasting the statistical properties of these methods. version:2
arxiv-1308-5138 | Artificial Immune Systems (INTROS 2) | http://arxiv.org/abs/1308.5138 | id:1308.5138 author:Uwe Aickelin, Dipankar Dasgupta, Feng Gu category:cs.NE cs.ET  published:2013-08-23 summary:The biological immune system is a robust, complex, adaptive system that defends the body from foreign pathogens. It is able to categorize all cells (or molecules) within the body as self or non-self substances. It does this with the help of a distributed task force that has the intelligence to take action from a local and also a global perspective using its network of chemical messengers for communication. There are two major branches of the immune system. The innate immune system is an unchanging mechanism that detects and destroys certain invading organisms, whilst the adaptive immune system responds to previously unknown foreign cells and builds a response to them that can remain in the body over a long period of time. This remarkable information processing biological system has caught the attention of computer science in recent years. A novel computational intelligence technique, inspired by immunology, has emerged, called Artificial Immune Systems. Several concepts from the immune system have been extracted and applied for solution to real world science and engineering problems. In this tutorial, we briefly describe the immune system metaphors that are relevant to existing Artificial Immune Systems methods. We will then show illustrative real-world problems suitable for Artificial Immune Systems and give a step-by-step algorithm walkthrough for one such problem. A comparison of the Artificial Immune Systems to other well-known algorithms, areas for future work, tips & tricks and a list of resources will round this tutorial off. It should be noted that as Artificial Immune Systems is still a young and evolving field, there is not yet a fixed algorithm template and hence actual implementations might differ somewhat from time to time and from those examples given here. version:1
arxiv-1301-1555 | Coupled Neural Associative Memories | http://arxiv.org/abs/1301.1555 | id:1301.1555 author:Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi category:cs.NE cs.IT cs.LG math.IT  published:2013-01-08 summary:We propose a novel architecture to design a neural associative memory that is capable of learning a large number of patterns and recalling them later in presence of noise. It is based on dividing the neurons into local clusters and parallel plains, very similar to the architecture of the visual cortex of macaque brain. The common features of our proposed architecture with those of spatially-coupled codes enable us to show that the performance of such networks in eliminating noise is drastically better than the previous approaches while maintaining the ability of learning an exponentially large number of patterns. Previous work either failed in providing good performance during the recall phase or in offering large pattern retrieval (storage) capacities. We also present computational experiments that lend additional support to the theoretical analysis. version:5
arxiv-1308-5133 | Performance Measurement Under Increasing Environmental Uncertainty In The Context of Interval Type-2 Fuzzy Logic Based Robotic Sailing | http://arxiv.org/abs/1308.5133 | id:1308.5133 author:Naisan Benatar, Uwe Aickelin, Jonathan M. Garibald category:cs.RO cs.NE cs.SY  published:2013-08-23 summary:Performance measurement of robotic controllers based on fuzzy logic, operating under uncertainty, is a subject area which has been somewhat ignored in the current literature. In this paper standard measures such as RMSE are shown to be inappropriate for use under conditions where the environmental uncertainty changes significantly between experiments. An overview of current methods which have been applied by other authors is presented, followed by a design of a more sophisticated method of comparison. This method is then applied to a robotic control problem to observe its outcome compared with a single measure. Results show that the technique described provides a more robust method of performance comparison than less complex methods allowing better comparisons to be drawn. version:1
arxiv-1308-5094 | Complexity of evolutionary equilibria in static fitness landscapes | http://arxiv.org/abs/1308.5094 | id:1308.5094 author:Artem Kaznatcheev category:q-bio.PE cs.NE F.2.2; J.3  published:2013-08-23 summary:A fitness landscape is a genetic space -- with two genotypes adjacent if they differ in a single locus -- and a fitness function. Evolutionary dynamics produce a flow on this landscape from lower fitness to higher; reaching equilibrium only if a local fitness peak is found. I use computational complexity to question the common assumption that evolution on static fitness landscapes can quickly reach a local fitness peak. I do this by showing that the popular NK model of rugged fitness landscapes is PLS-complete for K >= 2; the reduction from Weighted 2SAT is a bijection on adaptive walks, so there are NK fitness landscapes where every adaptive path from some vertices is of exponential length. Alternatively -- under the standard complexity theoretic assumption that there are problems in PLS not solvable in polynomial time -- this means that there are no evolutionary dynamics (known, or to be discovered, and not necessarily following adaptive paths) that can converge to a local fitness peak on all NK landscapes with K = 2. Applying results from the analysis of simplex algorithms, I show that there exist single-peaked landscapes with no reciprocal sign epistasis where the expected length of an adaptive path following strong selection weak mutation dynamics is $e^{O(n^{1/3})}$ even though an adaptive path to the optimum of length less than n is available from every vertex. The technical results are written to be accessible to mathematical biologists without a computer science background, and the biological literature is summarized for the convenience of non-biologists with the aim to open a constructive dialogue between the two disciplines. version:1
arxiv-1308-5063 | Suspicious Object Recognition Method in Video Stream Based on Visual Attention | http://arxiv.org/abs/1308.5063 | id:1308.5063 author:Panqu Wang, Yan Zhang category:cs.CV  published:2013-08-23 summary:We propose a state of the art method for intelligent object recognition and video surveillance based on human visual attention. Bottom up and top down attention are applied respectively in the process of acquiring interested object(saliency map) and object recognition. The revision of 4 channel PFT method is proposed for bottom up attention and enhances the speed and accuracy. Inhibit of return (IOR) is applied in judging the sequence of saliency object pop out. Euclidean distance of color distribution, object center coordinates and speed are considered in judging whether the target is match and suspicious. The extensive tests on videos and images show that our method in video analysis has high accuracy and fast speed compared with traditional method. The method can be applied into many fields such as video surveillance and security. version:1
arxiv-1308-5036 | Likelihood Adaptively Modified Penalties | http://arxiv.org/abs/1308.5036 | id:1308.5036 author:Yang Feng, Tengfei Li, Zhiliang Ying category:stat.ME math.ST stat.ML stat.TH  published:2013-08-23 summary:A new family of penalty functions, adaptive to likelihood, is introduced for model selection in general regression models. It arises naturally through assuming certain types of prior distribution on the regression parameters. To study stability properties of the penalized maximum likelihood estimator, two types of asymptotic stability are defined. Theoretical properties, including the parameter estimation consistency, model selection consistency, and asymptotic stability, are established under suitable regularity conditions. An efficient coordinate-descent algorithm is proposed. Simulation results and real data analysis show that the proposed method has competitive performance in comparison with existing ones. version:1
arxiv-1308-5033 | A hybrid evolutionary algorithm with importance sampling for multi-dimensional optimization | http://arxiv.org/abs/1308.5033 | id:1308.5033 author:Guanghui Huang, Zhifeng Pan category:cs.NE  published:2013-08-23 summary:A hybrid evolutionary algorithm with importance sampling method is proposed for multi-dimensional optimization problems in this paper. In order to make use of the information provided in the search process, a set of visited solutions is selected to give scores for intervals in each dimension, and they are updated as algorithm proceeds. Those intervals with higher scores are regarded as good intervals, which are used to estimate the joint distribution of optimal solutions through an interaction between the pool of good genetics, which are the individuals with smaller fitness values. And the sampling probabilities for good genetics are determined through an interaction between those estimated good intervals. It is a cross validation mechanism which determines the sampling probabilities for good intervals and genetics, and the resulted probabilities are used to design crossover, mutation and other stochastic operators with importance sampling method. As the selection of genetics and intervals is not directly dependent on the values of fitness, the resulted offsprings may avoid the trap of local optima. And a purely random EA is also combined into the proposed algorithm to maintain the diversity of population. 30 benchmark test functions are used to evaluate the performance of the proposed algorithm, and it is found that the proposed hybrid algorithm is an efficient algorithm for multi-dimensional optimization problems considered in this paper. version:1
arxiv-1308-5032 | How Did Humans Become So Creative? A Computational Approach | http://arxiv.org/abs/1308.5032 | id:1308.5032 author:Liane Gabora, Steve DiPaola category:cs.NE cs.AI cs.MA q-bio.NC  published:2013-08-23 summary:This paper summarizes efforts to computationally model two transitions in the evolution of human creativity: its origins about two million years ago, and the 'big bang' of creativity about 50,000 years ago. Using a computational model of cultural evolution in which neural network based agents evolve ideas for actions through invention and imitation, we tested the hypothesis that human creativity began with onset of the capacity for recursive recall. We compared runs in which agents were limited to single-step actions to runs in which they used recursive recall to chain simple actions into complex ones. Chaining resulted in higher diversity, open-ended novelty, no ceiling on the mean fitness of actions, and greater ability to make use of learning. Using a computational model of portrait painting, we tested the hypothesis that the explosion of creativity in the Middle/Upper Paleolithic was due to onset of con-textual focus: the capacity to shift between associative and analytic thought. This resulted in faster convergence on portraits that resembled the sitter, employed painterly techniques, and were rated as preferable. We conclude that recursive recall and contextual focus provide a computationally plausible explanation of how humans evolved the means to transform this planet. version:1
arxiv-1303-0561 | Top-down particle filtering for Bayesian decision trees | http://arxiv.org/abs/1303.0561 | id:1303.0561 author:Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh category:stat.ML cs.LG  published:2013-03-03 summary:Decision tree learning is a popular approach for classification and regression in machine learning and statistics, and Bayesian formulations---which introduce a prior distribution over decision trees, and formulate learning as posterior inference given data---have been shown to produce competitive performance. Unlike classic decision tree learning algorithms like ID3, C4.5 and CART, which work in a top-down manner, existing Bayesian algorithms produce an approximation to the posterior distribution by evolving a complete tree (or collection thereof) iteratively via local Monte Carlo modifications to the structure of the tree, e.g., using Markov chain Monte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm that instead works in a top-down manner, mimicking the behavior and speed of classic algorithms. We demonstrate empirically that our approach delivers accuracy comparable to the most popular MCMC method, but operates more than an order of magnitude faster, and thus represents a better computation-accuracy tradeoff. version:2
arxiv-1308-5010 | Sentiment in New York City: A High Resolution Spatial and Temporal View | http://arxiv.org/abs/1308.5010 | id:1308.5010 author:Karla Z. Bertrand, Maya Bialik, Kawandeep Virdee, Andreas Gros, Yaneer Bar-Yam category:physics.soc-ph cs.CL cs.CY  published:2013-08-22 summary:Measuring public sentiment is a key task for researchers and policymakers alike. The explosion of available social media data allows for a more time-sensitive and geographically specific analysis than ever before. In this paper we analyze data from the micro-blogging site Twitter and generate a sentiment map of New York City. We develop a classifier specifically tuned for 140-character Twitter messages, or tweets, using key words, phrases and emoticons to determine the mood of each tweet. This method, combined with geotagging provided by users, enables us to gauge public sentiment on extremely fine-grained spatial and temporal scales. We find that public mood is generally highest in public parks and lowest at transportation hubs, and locate other areas of strong sentiment such as cemeteries, medical centers, a jail, and a sewage facility. Sentiment progressively improves with proximity to Times Square. Periodic patterns of sentiment fluctuate on both a daily and a weekly scale: more positive tweets are posted on weekends than on weekdays, with a daily peak in sentiment around midnight and a nadir between 9:00 a.m. and noon. version:1
arxiv-1212-5524 | Reinforcement learning for port-Hamiltonian systems | http://arxiv.org/abs/1212.5524 | id:1212.5524 author:Olivier Sprangers, Gabriel A. D. Lopes, Robert Babuska category:cs.SY cs.LG  published:2012-12-21 summary:Passivity-based control (PBC) for port-Hamiltonian systems provides an intuitive way of achieving stabilization by rendering a system passive with respect to a desired storage function. However, in most instances the control law is obtained without any performance considerations and it has to be calculated by solving a complex partial differential equation (PDE). In order to address these issues we introduce a reinforcement learning approach into the energy-balancing passivity-based control (EB-PBC) method, which is a form of PBC in which the closed-loop energy is equal to the difference between the stored and supplied energies. We propose a technique to parameterize EB-PBC that preserves the systems's PDE matching conditions, does not require the specification of a global desired Hamiltonian, includes performance criteria, and is robust to extra non-linearities such as control input saturation. The parameters of the control law are found using actor-critic reinforcement learning, enabling learning near-optimal control policies satisfying a desired closed-loop energy landscape. The advantages are that near-optimal controllers can be generated using standard energy shaping techniques and that the solutions learned can be interpreted in terms of energy shaping and damping injection, which makes it possible to numerically assess stability using passivity theory. From the reinforcement learning perspective, our proposal allows for the class of port-Hamiltonian systems to be incorporated in the actor-critic framework, speeding up the learning thanks to the resulting parameterization of the policy. The method has been successfully applied to the pendulum swing-up problem in simulations and real-life experiments. version:2
arxiv-1308-4908 | A Unified Framework for Multi-Sensor HDR Video Reconstruction | http://arxiv.org/abs/1308.4908 | id:1308.4908 author:Joel Kronander, Stefan Gustavson, Gerhard Bonnet, Anders Ynnerman, Jonas Unger category:cs.CV cs.GR cs.MM  published:2013-08-22 summary:One of the most successful approaches to modern high quality HDR-video capture is to use camera setups with multiple sensors imaging the scene through a common optical system. However, such systems pose several challenges for HDR reconstruction algorithms. Previous reconstruction techniques have considered debayering, denoising, resampling (align- ment) and exposure fusion as separate problems. In contrast, in this paper we present a unifying approach, performing HDR assembly directly from raw sensor data. Our framework includes a camera noise model adapted to HDR video and an algorithm for spatially adaptive HDR reconstruction based on fitting of local polynomial approximations to observed sensor data. The method is easy to implement and allows reconstruction to an arbitrary resolution and output mapping. We present an implementation in CUDA and show real-time performance for an experimental 4 Mpixel multi-sensor HDR video system. We further show that our algorithm has clear advantages over existing methods, both in terms of flexibility and reconstruction quality. version:1
arxiv-1301-0264 | Validation of Soft Classification Models using Partial Class Memberships: An Extended Concept of Sensitivity & Co. applied to the Grading of Astrocytoma Tissues | http://arxiv.org/abs/1301.0264 | id:1301.0264 author:Claudia Beleites, Reiner Salzer, Valter Sergo category:stat.ML stat.AP  published:2013-01-02 summary:We use partial class memberships in soft classification to model uncertain labelling and mixtures of classes. Partial class memberships are not restricted to predictions, but may also occur in reference labels (ground truth, gold standard diagnosis) for training and validation data. Classifier performance is usually expressed as fractions of the confusion matrix, such as sensitivity, specificity, negative and positive predictive values. We extend this concept to soft classification and discuss the bias and variance properties of the extended performance measures. Ambiguity in reference labels translates to differences between best-case, expected and worst-case performance. We show a second set of measures comparing expected and ideal performance which is closely related to regression performance, namely the root mean squared error RMSE and the mean absolute error MAE. All calculations apply to classical crisp classification as well as to soft classification (partial class memberships and/or one-class classifiers). The proposed performance measures allow to test classifiers with actual borderline cases. In addition, hardening of e.g. posterior probabilities into class labels is not necessary, avoiding the corresponding information loss and increase in variance. We implement the proposed performance measures in the R package "softclassval", which is available from CRAN and at http://softclassval.r-forge.r-project.org. Our reasoning as well as the importance of partial memberships for chemometric classification is illustrated by a real-word application: astrocytoma brain tumor tissue grading (80 patients, 37000 spectra) for finding surgical excision borders. As borderline cases are the actual target of the analytical technique, samples which are diagnosed to be borderline cases must be included in the validation. version:2
arxiv-1308-4902 | A review on handwritten character and numeral recognition for Roman, Arabic, Chinese and Indian scripts | http://arxiv.org/abs/1308.4902 | id:1308.4902 author:Aini Najwa Azmi, Dewi Nasien, Siti Mariyam Shamsuddin category:cs.CV  published:2013-08-22 summary:There are a lot of intensive researches on handwritten character recognition (HCR) for almost past four decades. The research has been done on some of popular scripts such as Roman, Arabic, Chinese and Indian. In this paper we present a review on HCR work on the four popular scripts. We have summarized most of the published paper from 2005 to recent and also analyzed the various methods in creating a robust HCR system. We also added some future direction of research on HCR. version:1
arxiv-1109-2034 | Learning Sequence Neighbourhood Metrics | http://arxiv.org/abs/1109.2034 | id:1109.2034 author:Justin Bayer, Christian Osendorfer, Patrick van der Smagt category:cs.NE cs.LG  published:2011-09-09 summary:Recurrent neural networks (RNNs) in combination with a pooling operator and the neighbourhood components analysis (NCA) objective function are able to detect the characterizing dynamics of sequences and embed them into a fixed-length vector space of arbitrary dimensionality. Subsequently, the resulting features are meaningful and can be used for visualization or nearest neighbour classification in linear time. This kind of metric learning for sequential data enables the use of algorithms tailored towards fixed length vector spaces such as R^n. version:2
arxiv-1308-4828 | The Sample-Complexity of General Reinforcement Learning | http://arxiv.org/abs/1308.4828 | id:1308.4828 author:Tor Lattimore, Marcus Hutter, Peter Sunehag category:cs.LG  published:2013-08-22 summary:We present a new algorithm for general reinforcement learning where the true environment is known to belong to a finite class of N arbitrary models. The algorithm is shown to be near-optimal for all but O(N log^2 N) time-steps with high probability. Infinite classes are also considered where we show that compactness is a key criterion for determining the existence of uniform sample-complexity bounds. A matching lower bound is given for the finite case. version:1
arxiv-1303-1271 | Convex and Scalable Weakly Labeled SVMs | http://arxiv.org/abs/1303.1271 | id:1303.1271 author:Yu-Feng Li, Ivor W. Tsang, James T. Kwok, Zhi-Hua Zhou category:cs.LG  published:2013-03-06 summary:In this paper, we study the problem of learning from weakly labeled data, where labels of the training examples are incomplete. This includes, for example, (i) semi-supervised learning where labels are partially known; (ii) multi-instance learning where labels are implicitly known; and (iii) clustering where labels are completely unknown. Unlike supervised learning, learning with weak labels involves a difficult Mixed-Integer Programming (MIP) problem. Therefore, it can suffer from poor scalability and may also get stuck in local minimum. In this paper, we focus on SVMs and propose the WellSVM via a novel label generation strategy. This leads to a convex relaxation of the original MIP, which is at least as tight as existing convex Semi-Definite Programming (SDP) relaxations. Moreover, the WellSVM can be solved via a sequence of SVM subproblems that are much more scalable than previous convex SDP relaxations. Experiments on three weakly labeled learning tasks, namely, (i) semi-supervised learning; (ii) multi-instance learning for locating regions of interest in content-based information retrieval; and (iii) clustering, clearly demonstrate improved performance, and WellSVM is also readily applicable on large data sets. version:5
arxiv-1308-4718 | Invertibility and Robustness of Phaseless Reconstruction | http://arxiv.org/abs/1308.4718 | id:1308.4718 author:Radu Balan, Yang Wang category:math.FA cs.CV stat.ML 15A29  65H10  90C26  published:2013-08-21 summary:This paper is concerned with the question of reconstructing a vector in a finite-dimensional real Hilbert space when only the magnitudes of the coefficients of the vector under a redundant linear map are known. We analyze various Lipschitz bounds of the nonlinear analysis map and we establish theoretical performance bounds of any reconstruction algorithm. We show that robust and stable reconstruction requires additional redundancy than the critical threshold. version:1
arxiv-1308-4965 | A proposal for a Chinese keyboard for cellphones, smartphones, ipads and tablets | http://arxiv.org/abs/1308.4965 | id:1308.4965 author:Maurice Margenstern, Lan Wu category:cs.HC cs.CL 69U99  94A99 H.1.2; H.5.2  published:2013-08-21 summary:In this paper, we investigate the possibility to use two tilings of the hyperbolic plane as basic frame for devising a way to input texts in Chinese characters into messages of cellphones, smartphones, ipads and tablets. version:1
arxiv-1308-4618 | Can inferred provenance and its visualisation be used to detect erroneous annotation? A case study using UniProtKB | http://arxiv.org/abs/1308.4618 | id:1308.4618 author:Michael J. Bell, Matthew Collison, Phillip Lord category:cs.CL cs.CE cs.DL q-bio.QM  published:2013-08-21 summary:A constant influx of new data poses a challenge in keeping the annotation in biological databases current. Most biological databases contain significant quantities of textual annotation, which often contains the richest source of knowledge. Many databases reuse existing knowledge, during the curation process annotations are often propagated between entries. However, this is often not made explicit. Therefore, it can be hard, potentially impossible, for a reader to identify where an annotation originated from. Within this work we attempt to identify annotation provenance and track its subsequent propagation. Specifically, we exploit annotation reuse within the UniProt Knowledgebase (UniProtKB), at the level of individual sentences. We describe a visualisation approach for the provenance and propagation of sentences in UniProtKB which enables a large-scale statistical analysis. Initially levels of sentence reuse within UniProtKB were analysed, showing that reuse is heavily prevalent, which enables the tracking of provenance and propagation. By analysing sentences throughout UniProtKB, a number of interesting propagation patterns were identified, covering over 100, 000 sentences. Over 8000 sentences remain in the database after they have been removed from the entries where they originally occurred. Analysing a subset of these sentences suggest that approximately 30% are erroneous, whilst 35% appear to be inconsistent. These results suggest that being able to visualise sentence propagation and provenance can aid in the determination of the accuracy and quality of textual annotation. Source code and supplementary data are available from the authors website. version:1
arxiv-1308-4506 | A study of retrieval algorithms of sparse messages in networks of neural cliques | http://arxiv.org/abs/1308.4506 | id:1308.4506 author:Ala Aboudib, Vincent Gripon, Xiaoran Jiang category:cs.NE  published:2013-08-21 summary:Associative memories are data structures addressed using part of the content rather than an index. They offer good fault reliability and biological plausibility. Among different families of associative memories, sparse ones are known to offer the best efficiency (ratio of the amount of bits stored to that of bits used by the network itself). Their retrieval process performance has been shown to benefit from the use of iterations. However classical algorithms require having prior knowledge about the data to retrieve such as the number of nonzero symbols. We introduce several families of algorithms to enhance the retrieval process performance in recently proposed sparse associative memories based on binary neural networks. We show that these algorithms provide better performance, along with better plausibility than existing techniques. We also analyze the required number of iterations and derive corresponding curves. version:1
arxiv-1202-0515 | High-Dimensional Feature Selection by Feature-Wise Non-Linear Lasso | http://arxiv.org/abs/1202.0515 | id:1202.0515 author:Makoto Yamada, Wittawat Jitkrittum, Leonid Sigal, Eric P. Xing, Masashi Sugiyama category:stat.ML cs.AI stat.ME  published:2012-02-02 summary:The goal of supervised feature selection is to find a subset of input features that are responsible for predicting output values. The least absolute shrinkage and selection operator (Lasso) allows computationally efficient feature selection based on linear dependency between input features and output values. In this paper, we consider a feature-wise kernelized Lasso for capturing non-linear input-output dependency. We first show that, with particular choices of kernel functions, non-redundant features with strong statistical dependence on output values can be found in terms of kernel-based independence measures. We then show that the globally optimal solution can be efficiently computed; this makes the approach scalable to high-dimensional problems. The effectiveness of the proposed method is demonstrated through feature selection experiments with thousands of features. version:3
arxiv-1308-4479 | An Investigation of the Sampling-Based Alignment Method and Its Contributions | http://arxiv.org/abs/1308.4479 | id:1308.4479 author:Juan Luo, Yves Lepage category:cs.CL  published:2013-08-21 summary:By investigating the distribution of phrase pairs in phrase translation tables, the work in this paper describes an approach to increase the number of n-gram alignments in phrase translation tables output by a sampling-based alignment method. This approach consists in enforcing the alignment of n-grams in distinct translation subtables so as to increase the number of n-grams. Standard normal distribution is used to allot alignment time among translation subtables, which results in adjustment of the distribution of n- grams. This leads to better evaluation results on statistical machine translation tasks than the original sampling-based alignment approach. Furthermore, the translation quality obtained by merging phrase translation tables computed from the sampling-based alignment method and from MGIZA++ is examined. version:1
arxiv-1308-4440 | Influences Combination of Multi-Sensor Images on Classification Accuracy | http://arxiv.org/abs/1308.4440 | id:1308.4440 author:AL-Wassai Firouz, N. V. Kalyankar category:cs.CV  published:2013-08-20 summary:This paper focuses on two main issues; first one is the impact of combination of multi-sensor images on the supervised learning classification accuracy using segment Fusion (SF). The second issue attempts to undertake the study of supervised machine learning classification technique of remote sensing images by using four classifiers like Parallelepiped (Pp), Mahalanobis Distance (MD), Maximum-Likelihood (ML) and Euclidean Distance(ED) classifiers, and their accuracies have been evaluated on their respected classification to choose the best technique for classification of remote sensing images. QuickBird multispectral data (MS) and panchromatic data (PAN) have been used in this study to demonstrate the enhancement and accuracy assessment of fused image over the original images using ALwassaiProcess software. According to experimental result of this study, is that the test results indicate the supervised classification results of fusion image, which generated better than the MS did. As well as the result with Euclidean classifier is robust and provides better results than the other classifiers do, despite of the popular belief that the maximum-likelihood classifier is the most accurate classifier. version:1
arxiv-1308-4338 | SAR Image Despeckling Algorithms using Stochastic Distances and Nonlocal Means | http://arxiv.org/abs/1308.4338 | id:1308.4338 author:Leonardo Torres, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML  published:2013-08-20 summary:This paper presents two approaches for filter design based on stochastic distances for intensity speckle reduction. A window is defined around each pixel, overlapping samples are compared and only those which pass a goodness-of-fit test are used to compute the filtered value. The tests stem from stochastic divergences within the Information Theory framework. The technique is applied to intensity Synthetic Aperture Radar (SAR) data with homogeneous regions using the Gamma model. The first approach uses a Nagao-Matsuyama-type procedure for setting the overlapping samples, and the second uses the nonlocal method. The proposals are compared with the Improved Sigma filter and with anisotropic diffusion for speckled data (SRAD) using a protocol based on Monte Carlo simulation. Among the criteria used to quantify the quality of filters, we employ the equivalent number of looks, and line and edge preservation. Moreover, we also assessed the filters by the Universal Image Quality Index and by the Pearson correlation between edges. Applications to real images are also discussed. The proposed methods show good results. version:1
arxiv-1308-4077 | Support Recovery for the Drift Coefficient of High-Dimensional Diffusions | http://arxiv.org/abs/1308.4077 | id:1308.4077 author:Jose Bento, Morteza Ibrahimi category:cs.IT cs.LG math.IT math.PR math.ST stat.TH  published:2013-08-19 summary:Consider the problem of learning the drift coefficient of a $p$-dimensional stochastic differential equation from a sample path of length $T$. We assume that the drift is parametrized by a high-dimensional vector, and study the support recovery problem when both $p$ and $T$ can tend to infinity. In particular, we prove a general lower bound on the sample-complexity $T$ by using a characterization of mutual information as a time integral of conditional variance, due to Kadota, Zakai, and Ziv. For linear stochastic differential equations, the drift coefficient is parametrized by a $p\times p$ matrix which describes which degrees of freedom interact under the dynamics. In this case, we analyze a $\ell_1$-regularized least squares estimator and prove an upper bound on $T$ that nearly matches the lower bound on specific classes of sparse matrices. version:2
arxiv-1308-4214 | Pylearn2: a machine learning research library | http://arxiv.org/abs/1308.4214 | id:1308.4214 author:Ian J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Frédéric Bastien, Yoshua Bengio category:stat.ML cs.LG cs.MS  published:2013-08-20 summary:Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially. version:1
arxiv-1308-4211 | Scalable Convex Methods for Flexible Low-Rank Matrix Modeling | http://arxiv.org/abs/1308.4211 | id:1308.4211 author:William Fithian, Rahul Mazumder category:stat.ML  published:2013-08-20 summary:We propose a general framework for reduced-rank modeling of matrix-valued data. By applying a generalized nuclear norm penalty we directly model low-dimensional latent variables associated with rows and columns. Our framework flexibly incorporates row and column features, smoothing kernels, and other sources of side information by penalizing deviations from the row and column models. Under general con- ditions these models can be estimated scalably using convex optimization. The computational bottleneck - one singular value decomposition per iteration of a large but easy-to-apply matrix - can be scaled to large problems by representing the matrix implicitly and using warm starts [Mazumder and Hastie, 2013]. Our framework generalizes traditional convex matrix completion and multi-task learning methods as well as MAP estimation under a large class of popular hierarchical Bayesian models. version:1
arxiv-1308-4200 | Towards Adapting ImageNet to Reality: Scalable Domain Adaptation with Implicit Low-rank Transformations | http://arxiv.org/abs/1308.4200 | id:1308.4200 author:Erik Rodner, Judy Hoffman, Jeff Donahue, Trevor Darrell, Kate Saenko category:cs.CV cs.LG stat.ML  published:2013-08-20 summary:Images seen during test time are often not from the same distribution as images used for learning. This problem, known as domain shift, occurs when training classifiers from object-centric internet image databases and trying to apply them directly to scene understanding tasks. The consequence is often severe performance degradation and is one of the major barriers for the application of classifiers in real-world systems. In this paper, we show how to learn transform-based domain adaptation classifiers in a scalable manner. The key idea is to exploit an implicit rank constraint, originated from a max-margin domain adaptation formulation, to make optimization tractable. Experiments show that the transformation between domains can be very efficiently learned from data and easily applied to new categories. This begins to bridge the gap between large-scale internet image collections and object images captured in everyday life environments. version:1
arxiv-1308-1975 | Predicting protein contact map using evolutionary and physical constraints by integer programming (extended version) | http://arxiv.org/abs/1308.1975 | id:1308.1975 author:Zhiyong Wang, Jinbo Xu category:q-bio.QM cs.CE cs.LG math.OC q-bio.BM stat.ML  published:2013-08-08 summary:Motivation. Protein contact map describes the pairwise spatial and functional relationship of residues in a protein and contains key information for protein 3D structure prediction. Although studied extensively, it remains very challenging to predict contact map using only sequence information. Most existing methods predict the contact map matrix element-by-element, ignoring correlation among contacts and physical feasibility of the whole contact map. A couple of recent methods predict contact map based upon residue co-evolution, taking into consideration contact correlation and enforcing a sparsity restraint, but these methods require a very large number of sequence homologs for the protein under consideration and the resultant contact map may be still physically unfavorable. Results. This paper presents a novel method PhyCMAP for contact map prediction, integrating both evolutionary and physical restraints by machine learning and integer linear programming (ILP). The evolutionary restraints include sequence profile, residue co-evolution and context-specific statistical potential. The physical restraints specify more concrete relationship among contacts than the sparsity restraint. As such, our method greatly reduces the solution space of the contact map matrix and thus, significantly improves prediction accuracy. Experimental results confirm that PhyCMAP outperforms currently popular methods no matter how many sequence homologs are available for the protein under consideration. PhyCMAP can predict contacts within minutes after PSIBLAST search for sequence homologs is done, much faster than the two recent methods PSICOV and EvFold. See http://raptorx.uchicago.edu for the web server. version:2
arxiv-1308-4004 | A balanced k-means algorithm for weighted point sets | http://arxiv.org/abs/1308.4004 | id:1308.4004 author:Steffen Borgwardt, Andreas Brieden, Peter Gritzmann category:math.OC cs.LG stat.ML  published:2013-08-19 summary:The classical k-means algorithm for paritioning n points in R^d into k subsets is one of the most popular and widely spread clustering methods in scientific and business applications. The present paper gives a generalization that is capable of handling weighted point sets and prescribed lower and upper bounds on the cluster sizes. The new algorithm replaces the assignment step of k-means by the computation of a weight-balanced least-squares assignment. This is modelled as a linear program over a weight-balanced partition polytope whose optimal vertices correspond to clusterings that allow strongly feasible power diagrams. We use this correspondence to derive a worst-case upper bound n^O(dk) for the number of operations. This is similar to the known upper bound for k-means, polynomial for fixed k and d, and in view of the known complexity results for k-means, essentially the best one can expect. Further, we show the kernelizability of our approach. version:1
arxiv-1307-1372 | Clustering of Complex Networks and Community Detection Using Group Search Optimization | http://arxiv.org/abs/1307.1372 | id:1307.1372 author:G. Kishore Kumar, V. K. Jayaraman category:cs.NE cs.DS  published:2013-07-04 summary:Group Search Optimizer(GSO) is one of the best algorithms, is very new in the field of Evolutionary Computing. It is very robust and efficient algorithm, which is inspired by animal searching behaviour. The paper describes an application of GSO to clustering of networks. We have tested GSO against five standard benchmark datasets, GSO algorithm is proved very competitive in terms of accuracy and convergence speed. version:2
arxiv-1308-3946 | Optimal Algorithms for Testing Closeness of Discrete Distributions | http://arxiv.org/abs/1308.3946 | id:1308.3946 author:Siu-On Chan, Ilias Diakonikolas, Gregory Valiant, Paul Valiant category:cs.DS cs.IT cs.LG math.IT  published:2013-08-19 summary:We study the question of closeness testing for two discrete distributions. More precisely, given samples from two distributions $p$ and $q$ over an $n$-element set, we wish to distinguish whether $p=q$ versus $p$ is at least $\eps$-far from $q$, in either $\ell_1$ or $\ell_2$ distance. Batu et al. gave the first sub-linear time algorithms for these problems, which matched the lower bounds of Valiant up to a logarithmic factor in $n$, and a polynomial factor of $\eps.$ In this work, we present simple (and new) testers for both the $\ell_1$ and $\ell_2$ settings, with sample complexity that is information-theoretically optimal, to constant factors, both in the dependence on $n$, and the dependence on $\eps$; for the $\ell_1$ testing problem we establish that the sample complexity is $\Theta(\max\{n^{2/3}/\eps^{4/3}, n^{1/2}/\eps^2 \}).$ version:1
arxiv-1308-4123 | A Likelihood Ratio Approach for Probabilistic Inequalities | http://arxiv.org/abs/1308.4123 | id:1308.4123 author:Xinjia Chen category:math.PR cs.LG math.ST stat.TH  published:2013-08-18 summary:We propose a new approach for deriving probabilistic inequalities based on bounding likelihood ratios. We demonstrate that this approach is more general and powerful than the classical method frequently used for deriving concentration inequalities such as Chernoff bounds. We discover that the proposed approach is inherently related to statistical concepts such as monotone likelihood ratio, maximum likelihood, and the method of moments for parameter estimation. A connection between the proposed approach and the large deviation theory is also established. We show that, without using moment generating functions, tightest possible concentration inequalities may be readily derived by the proposed approach. We have derived new concentration inequalities using the proposed approach, which cannot be obtained by the classical approach based on moment generating functions. version:1
arxiv-1306-2801 | Understanding Dropout: Training Multi-Layer Perceptrons with Auxiliary Independent Stochastic Neurons | http://arxiv.org/abs/1306.2801 | id:1306.2801 author:Kyunghyun Cho category:cs.NE cs.LG stat.ML  published:2013-06-12 summary:In this paper, a simple, general method of adding auxiliary stochastic neurons to a multi-layer perceptron is proposed. It is shown that the proposed method is a generalization of recently successful methods of dropout (Hinton et al., 2012), explicit noise injection (Vincent et al., 2010; Bishop, 1995) and semantic hashing (Salakhutdinov & Hinton, 2009). Under the proposed framework, an extension of dropout which allows using separate dropping probabilities for different hidden neurons, or layers, is found to be available. The use of different dropping probabilities for hidden layers separately is empirically investigated. version:4
arxiv-1308-2655 | KL-based Control of the Learning Schedule for Surrogate Black-Box Optimization | http://arxiv.org/abs/1308.2655 | id:1308.2655 author:Ilya Loshchilov, Marc Schoenauer, Michèle Sebag category:cs.LG cs.AI stat.ML  published:2013-08-12 summary:This paper investigates the control of an ML component within the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) devoted to black-box optimization. The known CMA-ES weakness is its sample complexity, the number of evaluations of the objective function needed to approximate the global optimum. This weakness is commonly addressed through surrogate optimization, learning an estimate of the objective function a.k.a. surrogate model, and replacing most evaluations of the true objective function with the (inexpensive) evaluation of the surrogate model. This paper presents a principled control of the learning schedule (when to relearn the surrogate model), based on the Kullback-Leibler divergence of the current search distribution and the training distribution of the former surrogate model. The experimental validation of the proposed approach shows significant performance gains on a comprehensive set of ill-conditioned benchmark problems, compared to the best state of the art including the quasi-Newton high-precision BFGS method. version:2
arxiv-1308-3830 | Natural Language Web Interface for Database (NLWIDB) | http://arxiv.org/abs/1308.3830 | id:1308.3830 author:Rukshan Alexander, Prashanthi Rukshan, Sinnathamby Mahesan category:cs.CL cs.DB cs.HC  published:2013-08-18 summary:It is a long term desire of the computer users to minimize the communication gap between the computer and a human. On the other hand, almost all ICT applications store information in to databases and retrieve from them. Retrieving information from the database requires knowledge of technical languages such as Structured Query Language. However majority of the computer users who interact with the databases do not have a technical background and are intimidated by the idea of using languages such as SQL. For above reasons, a Natural Language Web Interface for Database (NLWIDB) has been developed. The NLWIDB allows the user to query the database in a language more like English, through a convenient interface over the Internet. version:1
arxiv-1308-3818 | Reference Distance Estimator | http://arxiv.org/abs/1308.3818 | id:1308.3818 author:Yanpeng Li category:cs.LG stat.ML  published:2013-08-18 summary:A theoretical study is presented for a simple linear classifier called reference distance estimator (RDE), which assigns the weight of each feature j as P(r j)-P(r), where r is a reference feature relevant to the target class y. The analysis shows that if r performs better than random guess in predicting y and is conditionally independent with each feature j, the RDE will have the same classification performance as that from P(y j)-P(y), a classifier trained with the gold standard y. Since the estimation of P(r j)-P(r) does not require labeled data, under the assumption above, RDE trained with a large number of unlabeled examples would be close to that trained with infinite labeled examples. For the case the assumption does not hold, we theoretically analyze the factors that influence the closeness of the RDE to the perfect one under the assumption, and present an algorithm to select reference features and combine multiple RDEs from different reference features using both labeled and unlabeled data. The experimental results on 10 text classification tasks show that the semi-supervised learning method improves supervised methods using 5,000 labeled examples and 13 million unlabeled ones, and in many tasks, its performance is even close to a classifier trained with 13 million labeled examples. In addition, the bounds in the theorems provide good estimation of the classification performance and can be useful for new algorithm design. version:1
arxiv-1308-3785 | Implementation Of Back-Propagation Neural Network For Isolated Bangla Speech Recognition | http://arxiv.org/abs/1308.3785 | id:1308.3785 author:Md. Ali Hossain, Md. Mijanur Rahman, Uzzal Kumar Prodhan, Md. Farukuzzaman Khan category:cs.CL cs.NE  published:2013-08-17 summary:This paper is concerned with the development of Back-propagation Neural Network for Bangla Speech Recognition. In this paper, ten bangla digits were recorded from ten speakers and have been recognized. The features of these speech digits were extracted by the method of Mel Frequency Cepstral Coefficient (MFCC) analysis. The mfcc features of five speakers were used to train the network with Back propagation algorithm. The mfcc features of ten bangla digit speeches, from 0 to 9, of another five speakers were used to test the system. All the methods and algorithms used in this research were implemented using the features of Turbo C and C++ languages. From our investigation it is seen that the developed system can successfully encode and analyze the mfcc features of the speech signal to recognition. The developed system achieved recognition rate about 96.332% for known speakers (i.e., speaker dependent) and 92% for unknown speakers (i.e., speaker independent). version:1
arxiv-1308-3784 | Graph Colouring Problem Based on Discrete Imperialist Competitive Algorithm | http://arxiv.org/abs/1308.3784 | id:1308.3784 author:Hojjat Emami, Shahriar Lotfi category:cs.AI cs.NE  published:2013-08-17 summary:In graph theory, Graph Colouring Problem (GCP) is an assignment of colours to vertices of any given graph such that the colours on adjacent vertices are different. The GCP is known to be an optimization and NP-hard problem. Imperialist Competitive Algorithm (ICA) is a meta-heuristic optimization and stochastic search strategy which is inspired from socio-political phenomenon of imperialistic competition. The ICA contains two main operators: the assimilation and the imperialistic competition. The ICA has excellent capabilities such as high convergence rate and better global optimum achievement. In this research, a discrete version of ICA is proposed to deal with the solution of GCP. We call this algorithm as the DICA. The performance of the proposed method is compared with Genetic Algorithm (GA) on seven well-known graph colouring benchmarks. Experimental results demonstrate the superiority of the DICA for the benchmarks. This means DICA can produce optimal and valid solutions for different GCP instances. version:1
arxiv-1308-3750 | Comment on "robustness and regularization of support vector machines" by H. Xu, et al., (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009, arXiv:0803.3490) | http://arxiv.org/abs/1308.3750 | id:1308.3750 author:Yahya Forghani, Hadi Sadoghi Yazdi category:cs.LG  published:2013-08-17 summary:This paper comments on the published work dealing with robustness and regularization of support vector machines (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009) [arXiv:0803.3490] by H. Xu, etc. They proposed a theorem to show that it is possible to relate robustness in the feature space and robustness in the sample space directly. In this paper, we propose a counter example that rejects their theorem. version:1
arxiv-1309-5357 | Development of Comprehensive Devnagari Numeral and Character Database for Offline Handwritten Character Recognition | http://arxiv.org/abs/1309.5357 | id:1309.5357 author:Vikas J. Dongre, Vijay H. Mankar category:cs.CV  published:2013-08-17 summary:In handwritten character recognition, benchmark database plays an important role in evaluating the performance of various algorithms and the results obtained by various researchers. In Devnagari script, there is lack of such official benchmark. This paper focuses on the generation of offline benchmark database for Devnagari handwritten numerals and characters. The present work generated 5137 and 20305 isolated samples for numeral and character database, respectively, from 750 writers of all ages, sex, education, and profession. The offline sample images are stored in TIFF image format as it occupies less memory. Also, the data is presented in binary level so that memory requirement is further reduced. It will facilitate research on handwriting recognition of Devnagari script through free access to the researchers. version:1
arxiv-1209-0125 | A History of Cluster Analysis Using the Classification Society's Bibliography Over Four Decades | http://arxiv.org/abs/1209.0125 | id:1209.0125 author:Fionn Murtagh, Michael J. Kurtz category:cs.DL cs.LG stat.ML 62H30 I.5.3; H.3.3  published:2012-09-01 summary:The Classification Literature Automated Search Service, an annual bibliography based on citation of one or more of a set of around 80 book or journal publications, ran from 1972 to 2012. We analyze here the years 1994 to 2011. The Classification Society's Service, as it was termed, has been produced by the Classification Society. In earlier decades it was distributed as a diskette or CD with the Journal of Classification. Among our findings are the following: an enormous increase in scholarly production post approximately 2000; a very major increase in quantity, coupled with work in different disciplines, from approximately 2004; and a major shift also from cluster analysis in earlier times having mathematics and psychology as disciplines of the journals published in, and affiliations of authors, contrasted with, in more recent times, a "centre of gravity" in management and engineering. version:2
arxiv-1308-3740 | Standardizing Interestingness Measures for Association Rules | http://arxiv.org/abs/1308.3740 | id:1308.3740 author:Mateen Shaikh, Paul D. McNicholas, M. Luiza Antonie, T. Brendan Murphy category:stat.AP cs.LG stat.ML  published:2013-08-16 summary:Interestingness measures provide information that can be used to prune or select association rules. A given value of an interestingness measure is often interpreted relative to the overall range of the values that the interestingness measure can take. However, properties of individual association rules restrict the values an interestingness measure can achieve. An interesting measure can be standardized to take this into account, but this has only been done for one interestingness measure to date, i.e., the lift. Standardization provides greater insight than the raw value and may even alter researchers' perception of the data. We derive standardized analogues of three interestingness measures and use real and simulated data to compare them to their raw versions, each other, and the standardized lift. version:1
arxiv-1308-1847 | The Royal Birth of 2013: Analysing and Visualising Public Sentiment in the UK Using Twitter | http://arxiv.org/abs/1308.1847 | id:1308.1847 author:Vu Dung Nguyen, Blesson Varghese, Adam Barker category:cs.CL cs.IR cs.SI physics.soc-ph  published:2013-08-08 summary:Analysis of information retrieved from microblogging services such as Twitter can provide valuable insight into public sentiment in a geographic region. This insight can be enriched by visualising information in its geographic context. Two underlying approaches for sentiment analysis are dictionary-based and machine learning. The former is popular for public sentiment analysis, and the latter has found limited use for aggregating public sentiment from Twitter data. The research presented in this paper aims to extend the machine learning approach for aggregating public sentiment. To this end, a framework for analysing and visualising public sentiment from a Twitter corpus is developed. A dictionary-based approach and a machine learning approach are implemented within the framework and compared using one UK case study, namely the royal birth of 2013. The case study validates the feasibility of the framework for analysis and rapid visualisation. One observation is that there is good correlation between the results produced by the popular dictionary-based approach and the machine learning approach when large volumes of tweets are analysed. However, for rapid analysis to be possible faster methods need to be developed using big data techniques and parallel methods. version:2
arxiv-1308-3558 | Fast Stochastic Alternating Direction Method of Multipliers | http://arxiv.org/abs/1308.3558 | id:1308.3558 author:Leon Wenliang Zhong, James T. Kwok category:cs.LG cs.NA  published:2013-08-16 summary:In this paper, we propose a new stochastic alternating direction method of multipliers (ADMM) algorithm, which incrementally approximates the full gradient in the linearized ADMM formulation. Besides having a low per-iteration complexity as existing stochastic ADMM algorithms, the proposed algorithm improves the convergence rate on convex problems from $O(\frac 1 {\sqrt{T}})$ to $O(\frac 1 T)$, where $T$ is the number of iterations. This matches the convergence rate of the batch ADMM algorithm, but without the need to visit all the samples in each iteration. Experiments on the graph-guided fused lasso demonstrate that the new algorithm is significantly faster than state-of-the-art stochastic and batch ADMM algorithms. version:1
arxiv-1308-4675 | Genetic Algorithm for Solving Simple Mathematical Equality Problem | http://arxiv.org/abs/1308.4675 | id:1308.4675 author:Denny Hermawanto category:cs.NE  published:2013-08-16 summary:This paper explains genetic algorithm for novice in this field. Basic philosophy of genetic algorithm and its flowchart are described. Step by step numerical computation of genetic algorithm for solving simple mathematical equality problem will be briefly explained version:1
arxiv-1308-3524 | Innovative Second-Generation Wavelets Construction With Recurrent Neural Networks for Solar Radiation Forecasting | http://arxiv.org/abs/1308.3524 | id:1308.3524 author:Giacomo Capizzi, Christian Napoli, Francesco Bonanno category:cs.NE  published:2013-08-15 summary:Solar radiation prediction is an important challenge for the electrical engineer because it is used to estimate the power developed by commercial photovoltaic modules. This paper deals with the problem of solar radiation prediction based on observed meteorological data. A 2-day forecast is obtained by using novel wavelet recurrent neural networks (WRNNs). In fact, these WRNNS are used to exploit the correlation between solar radiation and timescale-related variations of wind speed, humidity, and temperature. The input to the selected WRNN is provided by timescale-related bands of wavelet coefficients obtained from meteorological time series. The experimental setup available at the University of Catania, Italy, provided this information. The novelty of this approach is that the proposed WRNN performs the prediction in the wavelet domain and, in addition, also performs the inverse wavelet transform, giving the predicted signal as output. The obtained simulation results show a very low root-mean-square error compared to the results of the solar radiation prediction approaches obtained by hybrid neural networks reported in the recent literature. version:1
arxiv-1308-3513 | Hidden Parameter Markov Decision Processes: A Semiparametric Regression Approach for Discovering Latent Task Parametrizations | http://arxiv.org/abs/1308.3513 | id:1308.3513 author:Finale Doshi-Velez, George Konidaris category:cs.LG cs.AI  published:2013-08-15 summary:Control applications often feature tasks with similar, but not identical, dynamics. We introduce the Hidden Parameter Markov Decision Process (HiP-MDP), a framework that parametrizes a family of related dynamical systems with a low-dimensional set of latent factors, and introduce a semiparametric regression approach for learning its structure from data. In the control setting, we show that a learned HiP-MDP rapidly identifies the dynamics of a new task instance, allowing an agent to flexibly adapt to task variations. version:1
arxiv-1308-3509 | Stochastic Optimization for Machine Learning | http://arxiv.org/abs/1308.3509 | id:1308.3509 author:Andrew Cotter category:cs.LG  published:2013-08-15 summary:It has been found that stochastic algorithms often find good solutions much more rapidly than inherently-batch approaches. Indeed, a very useful rule of thumb is that often, when solving a machine learning problem, an iterative technique which relies on performing a very large number of relatively-inexpensive updates will often outperform one which performs a smaller number of much "smarter" but computationally-expensive updates. In this thesis, we will consider the application of stochastic algorithms to two of the most important machine learning problems. Part i is concerned with the supervised problem of binary classification using kernelized linear classifiers, for which the data have labels belonging to exactly two classes (e.g. "has cancer" or "doesn't have cancer"), and the learning problem is to find a linear classifier which is best at predicting the label. In Part ii, we will consider the unsupervised problem of Principal Component Analysis, for which the learning task is to find the directions which contain most of the variance of the data distribution. Our goal is to present stochastic algorithms for both problems which are, above all, practical--they work well on real-world data, in some cases better than all known competing algorithms. A secondary, but still very important, goal is to derive theoretical bounds on the performance of these algorithms which are at least competitive with, and often better than, those known for other approaches. version:1
arxiv-1308-3506 | Computational Rationalization: The Inverse Equilibrium Problem | http://arxiv.org/abs/1308.3506 | id:1308.3506 author:Kevin Waugh, Brian D. Ziebart, J. Andrew Bagnell category:cs.GT cs.LG stat.ML  published:2013-08-15 summary:Modeling the purposeful behavior of imperfect agents from a small number of observations is a challenging task. When restricted to the single-agent decision-theoretic setting, inverse optimal control techniques assume that observed behavior is an approximately optimal solution to an unknown decision problem. These techniques learn a utility function that explains the example behavior and can then be used to accurately predict or imitate future behavior in similar observed or unobserved situations. In this work, we consider similar tasks in competitive and cooperative multi-agent domains. Here, unlike single-agent settings, a player cannot myopically maximize its reward; it must speculate on how the other agents may act to influence the game's outcome. Employing the game-theoretic notion of regret and the principle of maximum entropy, we introduce a technique for predicting and generalizing behavior. version:1
arxiv-1308-0484 | Using Incomplete Information for Complete Weight Annotation of Road Networks -- Extended Version | http://arxiv.org/abs/1308.0484 | id:1308.0484 author:Bin Yang, Manohar Kaul, Christian S. Jensen category:cs.LG cs.DB  published:2013-08-02 summary:We are witnessing increasing interests in the effective use of road networks. For example, to enable effective vehicle routing, weighted-graph models of transportation networks are used, where the weight of an edge captures some cost associated with traversing the edge, e.g., greenhouse gas (GHG) emissions or travel time. It is a precondition to using a graph model for routing that all edges have weights. Weights that capture travel times and GHG emissions can be extracted from GPS trajectory data collected from the network. However, GPS trajectory data typically lack the coverage needed to assign weights to all edges. This paper formulates and addresses the problem of annotating all edges in a road network with travel cost based weights from a set of trips in the network that cover only a small fraction of the edges, each with an associated ground-truth travel cost. A general framework is proposed to solve the problem. Specifically, the problem is modeled as a regression problem and solved by minimizing a judiciously designed objective function that takes into account the topology of the road network. In particular, the use of weighted PageRank values of edges is explored for assigning appropriate weights to all edges, and the property of directional adjacency of edges is also taken into account to assign weights. Empirical studies with weights capturing travel time and GHG emissions on two road networks (Skagen, Denmark, and North Jutland, Denmark) offer insight into the design properties of the proposed techniques and offer evidence that the techniques are effective. version:2
arxiv-1302-3831 | Quantum Entanglement in Concept Combinations | http://arxiv.org/abs/1302.3831 | id:1302.3831 author:Diederik Aerts, Sandro Sozzo category:cs.AI cs.CL quant-ph  published:2013-02-15 summary:Research in the application of quantum structures to cognitive science confirms that these structures quite systematically appear in the dynamics of concepts and their combinations and quantum-based models faithfully represent experimental data of situations where classical approaches are problematical. In this paper, we analyze the data we collected in an experiment on a specific conceptual combination, showing that Bell's inequalities are violated in the experiment. We present a new refined entanglement scheme to model these data within standard quantum theory rules, where 'entangled measurements and entangled evolutions' occur, in addition to the expected 'entangled states', and present a full quantum representation in complex Hilbert space of the data. This stronger form of entanglement in measurements and evolutions might have relevant applications in the foundations of quantum theory, as well as in the interpretation of nonlocality tests. It could indeed explain some non-negligible 'anomalies' identified in EPR-Bell experiments. version:2
arxiv-1308-3432 | Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation | http://arxiv.org/abs/1308.3432 | id:1308.3432 author:Yoshua Bengio, Nicholas Léonard, Aaron Courville category:cs.LG  published:2013-08-15 summary:Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of {\em conditional computation}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful. version:1
arxiv-1310-1250 | Learning ambiguous functions by neural networks | http://arxiv.org/abs/1310.1250 | id:1310.1250 author:Rui Ligeiro, R. Vilela Mendes category:cs.NE cs.LG physics.data-an 68T37  82C32 I.2.6; I.5.1; I.5.5  published:2013-08-15 summary:It is not, in general, possible to have access to all variables that determine the behavior of a system. Having identified a number of variables whose values can be accessed, there may still be hidden variables which influence the dynamics of the system. The result is model ambiguity in the sense that, for the same (or very similar) input values, different objective outputs should have been obtained. In addition, the degree of ambiguity may vary widely across the whole range of input values. Thus, to evaluate the accuracy of a model it is of utmost importance to create a method to obtain the degree of reliability of each output result. In this paper we present such a scheme composed of two coupled artificial neural networks: the first one being responsible for outputting the predicted value, whereas the other evaluates the reliability of the output, which is learned from the error values of the first one. As an illustration, the scheme is applied to a model for tracking slopes in a straw chamber and to a credit scoring model. version:1
arxiv-1308-3314 | The algorithm of noisy k-means | http://arxiv.org/abs/1308.3314 | id:1308.3314 author:Camille Brunet, Sébastien Loustau category:stat.ML cs.LG  published:2013-08-15 summary:In this note, we introduce a new algorithm to deal with finite dimensional clustering with errors in variables. The design of this algorithm is based on recent theoretical advances (see Loustau (2013a,b)) in statistical learning with errors in variables. As the previous mentioned papers, the algorithm mixes different tools from the inverse problem literature and the machine learning community. Coarsely, it is based on a two-step procedure: (1) a deconvolution step to deal with noisy inputs and (2) Newton's iterations as the popular k-means. version:1
arxiv-1307-5870 | Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery | http://arxiv.org/abs/1307.5870 | id:1307.5870 author:Cun Mu, Bo Huang, John Wright, Donald Goldfarb category:stat.ML cs.LG  published:2013-07-22 summary:Recovering a low-rank tensor from incomplete information is a recurring problem in signal processing and machine learning. The most popular convex relaxation of this problem minimizes the sum of the nuclear norms of the unfoldings of the tensor. We show that this approach can be substantially suboptimal: reliably recovering a $K$-way tensor of length $n$ and Tucker rank $r$ from Gaussian measurements requires $\Omega(r n^{K-1})$ observations. In contrast, a certain (intractable) nonconvex formulation needs only $O(r^K + nrK)$ observations. We introduce a very simple, new convex relaxation, which partially bridges this gap. Our new formulation succeeds with $O(r^{\lfloor K/2 \rfloor}n^{\lceil K/2 \rceil})$ observations. While these results pertain to Gaussian measurements, simulations strongly suggest that the new norm also outperforms the sum of nuclear norms for tensor completion from a random subset of entries. Our lower bound for the sum-of-nuclear-norms model follows from a new result on recovering signals with multiple sparse structures (e.g. sparse, low rank), which perhaps surprisingly demonstrates the significant suboptimality of the commonly used recovery approach via minimizing the sum of individual sparsity inducing norms (e.g. $l_1$, nuclear norm). Our new formulation for low-rank tensor recovery however opens the possibility in reducing the sample complexity by exploiting several structures jointly. version:2
arxiv-1308-3294 | A Secure and Comparable Text Encryption Algorithm | http://arxiv.org/abs/1308.3294 | id:1308.3294 author:Nicholas Kersting category:cs.CR cs.CL cs.CY cs.SI  published:2013-08-15 summary:This paper discloses a simple algorithm for encrypting text messages, based on the NP-completeness of the subset sum problem, such that the similarity between encryptions is roughly proportional to the semantic similarity between their generating messages. This allows parties to compare encrypted messages for semantic overlap without trusting an intermediary and might be applied, for example, as a means of finding scientific collaborators over the Internet. version:1
arxiv-1310-7447 | Impulse Noise Removal In Speech Using Wavelets | http://arxiv.org/abs/1310.7447 | id:1310.7447 author:R. C. Nongpiur category:cs.CV  published:2013-08-14 summary:A new method for removing impulse noise from speech in the wavelet transform domain is proposed. The method utilizes the multiresolution property of the wavelet transform, which provides finer time resolution at the higher frequencies than the short-time Fourier transform (STFT), to effectively identify and remove impulse noise. It uses two features of speech to discriminate speech from impulse noise: one is the slow time-varying nature of speech and the other is the Lipschitz regularity of the speech components. On the basis of these features, an algorithm has been developed to identify and suppress wavelet coefficients that correspond to impulse noise. Experiment results show that the new method is able to significantly reduce impulse noise without degrading the quality of the speech signal or introducing any audible artifacts. version:1
arxiv-1301-6314 | Equitability Analysis of the Maximal Information Coefficient, with Comparisons | http://arxiv.org/abs/1301.6314 | id:1301.6314 author:David Reshef, Yakir Reshef, Michael Mitzenmacher, Pardis Sabeti category:cs.LG q-bio.QM stat.ML  published:2013-01-27 summary:A measure of dependence is said to be equitable if it gives similar scores to equally noisy relationships of different types. Equitability is important in data exploration when the goal is to identify a relatively small set of strongest associations within a dataset as opposed to finding as many non-zero associations as possible, which often are too many to sift through. Thus an equitable statistic, such as the maximal information coefficient (MIC), can be useful for analyzing high-dimensional data sets. Here, we explore both equitability and the properties of MIC, and discuss several aspects of the theory and practice of MIC. We begin by presenting an intuition behind the equitability of MIC through the exploration of the maximization and normalization steps in its definition. We then examine the speed and optimality of the approximation algorithm used to compute MIC, and suggest some directions for improving both. Finally, we demonstrate in a range of noise models and sample sizes that MIC is more equitable than natural alternatives, such as mutual information estimation and distance correlation. version:2
arxiv-1308-3243 | Arabic Text Recognition in Video Sequences | http://arxiv.org/abs/1308.3243 | id:1308.3243 author:M. Ben Halima, H. Karray, A. M. Alimi category:cs.MM cs.CL cs.CV  published:2013-08-14 summary:In this paper, we propose a robust approach for text extraction and recognition from Arabic news video sequence. The text included in video sequences is an important needful for indexing and searching system. However, this text is difficult to detect and recognize because of the variability of its size, their low resolution characters and the complexity of the backgrounds. To solve these problems, we propose a system performing in two main tasks: extraction and recognition of text. Our system is tested on a varied database composed of different Arabic news programs and the obtained results are encouraging and show the merits of our approach. version:1
arxiv-1308-3225 | An interactive engine for multilingual video browsing using semantic content | http://arxiv.org/abs/1308.3225 | id:1308.3225 author:M. Ben Halima, M. Hamroun, S. Ben Moussa, A. M. Alimi category:cs.MM cs.CV cs.IR  published:2013-08-14 summary:The amount of audio-visual information has increased dramatically with the advent of High Speed Internet. Furthermore, technological advances in recent years in the field of information technology, have simplified the use of video data in various fields by the general public. This made it possible to store large collections of video documents into computer systems. To enable efficient use of these collections, it is necessary to develop tools to facilitate access to these documents and handling them. In this paper we propose a method for indexing and retrieval of video sequences in a video database of large dimension, based on a weighting technique to calculate the degree of membership of a concept in a video also a structuring of the data of the audio-visual (context / concept / video) and a relevance feedback mechanism. version:1
arxiv-1308-3400 | Guiding Designs of Self-Organizing Swarms: Interactive and Automated Approaches | http://arxiv.org/abs/1308.3400 | id:1308.3400 author:Hiroki Sayama category:cs.NE nlin.AO  published:2013-08-14 summary:Self-organization of heterogeneous particle swarms is rich in its dynamics but hard to design in a traditional top-down manner, especially when many types of kinetically distinct particles are involved. In this chapter, we discuss how we have been addressing this problem by (1) utilizing and enhancing interactive evolutionary design methods and (2) realizing spontaneous evolution of self organizing swarms within an artificial ecosystem. version:1
arxiv-1308-3177 | Normalized Google Distance of Multisets with Applications | http://arxiv.org/abs/1308.3177 | id:1308.3177 author:Andrew R. Cohen, P. M. B. Vitanyi category:cs.IR cs.LG  published:2013-08-14 summary:Normalized Google distance (NGD) is a relative semantic distance based on the World Wide Web (or any other large electronic database, for instance Wikipedia) and a search engine that returns aggregate page counts. The earlier NGD between pairs of search terms (including phrases) is not sufficient for all applications. We propose an NGD of finite multisets of search terms that is better for many applications. This gives a relative semantics shared by a multiset of search terms. We give applications and compare the results with those obtained using the pairwise NGD. The derivation of NGD method is based on Kolmogorov complexity. version:1
arxiv-1302-3463 | Locally epistatic genomic relationship matrices for genomic association, prediction and selection | http://arxiv.org/abs/1302.3463 | id:1302.3463 author:Deniz Akdemir category:stat.AP stat.ML  published:2013-02-14 summary:As the amount and complexity of genetic information increases it is necessary that we explore some efficient ways of handling these data. This study takes the "divide and conquer" approach for analyzing high dimensional genomic data. Our aims include reducing the dimensionality of the problem that has to be dealt one at a time, improving the performance and interpretability of the models. We propose using the inherent structures in the genome; to divide the bigger problem into manageable parts. In plant and animal breeding studies a distinction is made between the commercial value (additive + epistatic genetic effects) and the breeding value (additive genetic effects) of an individual since it is expected that some of the epistatic genetic effects will be lost due to recombination. In this paper, we argue that the breeder can take advantage of some of the epistatic marker effects in regions of low recombination. The models introduced here aim to estimate local epistatic line heritability by using the genetic map information and combine the local additive and epistatic effects. To this end, we have used semi-parametric mixed models with multiple local genomic relationship matrices with hierarchical testing designs and lasso post-processing for sparsity in the final model and speed. Our models produce good predictive performance along with genetic association information. version:6
arxiv-1308-3106 | System and Methods for Converting Speech to SQL | http://arxiv.org/abs/1308.3106 | id:1308.3106 author:Sachin Kumar, Ashish Kumar, Pinaki Mitra, Girish Sundaram category:cs.CL cs.DB  published:2013-08-14 summary:This paper concerns with the conversion of a Spoken English Language Query into SQL for retrieving data from RDBMS. A User submits a query as speech signal through the user interface and gets the result of the query in the text format. We have developed the acoustic and language models using which a speech utterance can be converted into English text query and thus natural language processing techniques can be applied on this English text query to generate an equivalent SQL query. For conversion of speech into English text HTK and Julius tools have been used and for conversion of English text query into SQL query we have implemented a System which uses rule based translation to translate English Language Query into SQL Query. The translation uses lexical analyzer, parser and syntax directed translation techniques like in compilers. JFLex and BYACC tools have been used to build lexical analyzer and parser respectively. System is domain independent i.e. system can run on different database as it generates lex files from the underlying database. version:1
arxiv-1308-3101 | Compact Relaxations for MAP Inference in Pairwise MRFs with Piecewise Linear Priors | http://arxiv.org/abs/1308.3101 | id:1308.3101 author:Christopher Zach, Christian Haene category:cs.CV cs.LG stat.ML  published:2013-08-14 summary:Label assignment problems with large state spaces are important tasks especially in computer vision. Often the pairwise interaction (or smoothness prior) between labels assigned at adjacent nodes (or pixels) can be described as a function of the label difference. Exact inference in such labeling tasks is still difficult, and therefore approximate inference methods based on a linear programming (LP) relaxation are commonly used in practice. In this work we study how compact linear programs can be constructed for general piecwise linear smoothness priors. The number of unknowns is O(LK) per pairwise clique in terms of the state space size $L$ and the number of linear segments K. This compares to an O(L^2) size complexity of the standard LP relaxation if the piecewise linear structure is ignored. Our compact construction and the standard LP relaxation are equivalent and lead to the same (approximate) label assignment. version:1
arxiv-1308-2853 | When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity | http://arxiv.org/abs/1308.2853 | id:1308.2853 author:Animashree Anandkumar, Daniel Hsu, Majid Janzamin, Sham Kakade category:cs.LG cs.IR math.NA math.ST stat.ML stat.TH  published:2013-08-13 summary:Overcomplete latent representations have been very popular for unsupervised feature learning in recent years. In this paper, we specify which overcomplete models can be identified given observable moments of a certain order. We consider probabilistic admixture or topic models in the overcomplete regime, where the number of latent topics can greatly exceed the size of the observed word vocabulary. While general overcomplete topic models are not identifiable, we establish generic identifiability under a constraint, referred to as topic persistence. Our sufficient conditions for identifiability involve a novel set of "higher order" expansion conditions on the topic-word matrix or the population structure of the model. This set of higher-order expansion conditions allow for overcomplete models, and require the existence of a perfect matching from latent topics to higher order observed words. We establish that random structured topic models are identifiable w.h.p. in the overcomplete regime. Our identifiability results allows for general (non-degenerate) distributions for modeling the topic proportions, and thus, we can handle arbitrarily correlated topics in our framework. Our identifiability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of Tucker decompositions, but is more general than the Candecomp/Parafac (CP) decomposition. version:1
arxiv-1212-1182 | Universally consistent vertex classification for latent positions graphs | http://arxiv.org/abs/1212.1182 | id:1212.1182 author:Minh Tang, Daniel L. Sussman, Carey E. Priebe category:stat.ML math.ST stat.TH  published:2012-12-05 summary:In this work we show that, using the eigen-decomposition of the adjacency matrix, we can consistently estimate feature maps for latent position graphs with positive definite link function $\kappa$, provided that the latent positions are i.i.d. from some distribution F. We then consider the exploitation task of vertex classification where the link function $\kappa$ belongs to the class of universal kernels and class labels are observed for a number of vertices tending to infinity and that the remaining vertices are to be classified. We show that minimization of the empirical $\varphi$-risk for some convex surrogate $\varphi$ of 0-1 loss over a class of linear classifiers with increasing complexities yields a universally consistent classifier, that is, a classification rule with error converging to Bayes optimal for any distribution F. version:3
arxiv-1308-2762 | An efficient ant based qos aware intelligent temporally ordered routing algorithm for manets | http://arxiv.org/abs/1308.2762 | id:1308.2762 author:Debajit Sensarma, Koushik Majumder category:cs.NI cs.NE  published:2013-08-13 summary:A Mobile Ad hoc network (MANET) is a self configurable network connected by wireless links. This type of network is only suitable for temporary communication links as it is infrastructure-less and there is no centralised control. Providing QoS aware routing is a challenging task in this type of network due to dynamic topology and limited resources. The main purpose of QoS aware routing is to find a feasible path from source to destination which will satisfy two or more end to end QoS constrains. Therefore, the task of designing an efficient routing algorithm which will satisfy all the quality of service requirements and be robust and adaptive is considered as a highly challenging problem. In this work we have designed a new efficient and energy aware multipath routing algorithm based on ACO framework, inspired by the behaviours of biological ants. Basically by considering QoS constraints and artificial ants we have designed an intelligent version of classical Temporally Ordered Routing Algorithm (TORA) which will increase network lifetime and decrease packet loss and average end to end delay that makes this algorithm suitable for real time and multimedia applications. version:1
arxiv-1204-0171 | A New Fuzzy Stacked Generalization Technique and Analysis of its Performance | http://arxiv.org/abs/1204.0171 | id:1204.0171 author:Mete Ozay, Fatos T. Yarman Vural category:cs.LG cs.CV  published:2012-04-01 summary:In this study, a new Stacked Generalization technique called Fuzzy Stacked Generalization (FSG) is proposed to minimize the difference between N -sample and large-sample classification error of the Nearest Neighbor classifier. The proposed FSG employs a new hierarchical distance learning strategy to minimize the error difference. For this purpose, we first construct an ensemble of base-layer fuzzy k- Nearest Neighbor (k-NN) classifiers, each of which receives a different feature set extracted from the same sample set. The fuzzy membership values computed at the decision space of each fuzzy k-NN classifier are concatenated to form the feature vectors of a fusion space. Finally, the feature vectors are fed to a meta-layer classifier to learn the degree of accuracy of the decisions of the base-layer classifiers for meta-layer classification. Rather than the power of the individual base layer-classifiers, diversity and cooperation of the classifiers become an important issue to improve the overall performance of the proposed FSG. A weak base-layer classifier may boost the overall performance more than a strong classifier, if it is capable of recognizing the samples, which are not recognized by the rest of the classifiers, in its own feature space. The experiments explore the type of the collaboration among the individual classifiers required for an improved performance of the suggested architecture. Experiments on multiple feature real-world datasets show that the proposed FSG performs better than the state of the art ensemble learning algorithms such as Adaboost, Random Subspace and Rotation Forest. On the other hand, compatible performances are observed in the experiments on single feature multi-attribute datasets. version:5
arxiv-1308-2696 | B(eo)W(u)LF: Facilitating recurrence analysis on multi-level language | http://arxiv.org/abs/1308.2696 | id:1308.2696 author:A. Paxton, R. Dale category:cs.CL  published:2013-08-12 summary:Discourse analysis may seek to characterize not only the overall composition of a given text but also the dynamic patterns within the data. This technical report introduces a data format intended to facilitate multi-level investigations, which we call the by-word long-form or B(eo)W(u)LF. Inspired by the long-form data format required for mixed-effects modeling, B(eo)W(u)LF structures linguistic data into an expanded matrix encoding any number of researchers-specified markers, making it ideal for recurrence-based analyses. While we do not necessarily claim to be the first to use methods along these lines, we have created a series of tools utilizing Python and MATLAB to enable such discourse analyses and demonstrate them using 319 lines of the Old English epic poem, Beowulf, translated into modern English. version:1
arxiv-1308-2654 | Local image registration a comparison for bilateral registration mammography | http://arxiv.org/abs/1308.2654 | id:1308.2654 author:José M. Celaya-Padilla, Juan Rodriguez-Rojas, Victor Trevino, José G. Gerardo Tamez-Pena category:cs.CV  published:2013-08-12 summary:Early tumor detection is key in reducing the number of breast cancer death and screening mammography is one of the most widely available and reliable method for early detection. However, it is difficult for the radiologist to process with the same attention each case, due the large amount of images to be read. Computer aided detection (CADe) systems improve tumor detection rate; but the current efficiency of these systems is not yet adequate and the correct interpretation of CADe outputs requires expert human intervention. Computer aided diagnosis systems (CADx) are being designed to improve cancer diagnosis accuracy, but they have not been efficiently applied in breast cancer. CADx efficiency can be enhanced by considering the natural mirror symmetry between the right and left breast. The objective of this work is to evaluate co-registration algorithms for the accurate alignment of the left to right breast for CADx enhancement. A set of mammograms were artificially altered to create a ground truth set to evaluate the registration efficiency of DEMONs, and SPLINE deformable registration algorithms. The registration accuracy was evaluated using mean square errors, mutual information and correlation. The results on the 132 images proved that the SPLINE deformable registration over-perform the DEMONS on mammography images. version:1
arxiv-1308-2464 | Faster gradient descent and the efficient recovery of images | http://arxiv.org/abs/1308.2464 | id:1308.2464 author:Hui Huang, Uri Ascher category:cs.CV cs.NA math.NA  published:2013-08-12 summary:Much recent attention has been devoted to gradient descent algorithms where the steepest descent step size is replaced by a similar one from a previous iteration or gets updated only once every second step, thus forming a {\em faster gradient descent method}. For unconstrained convex quadratic optimization these methods can converge much faster than steepest descent. But the context of interest here is application to certain ill-posed inverse problems, where the steepest descent method is known to have a smoothing, regularizing effect, and where a strict optimization solution is not necessary. Specifically, in this paper we examine the effect of replacing steepest descent by a faster gradient descent algorithm in the practical context of image deblurring and denoising tasks. We also propose several highly efficient schemes for carrying out these tasks independently of the step size selection, as well as a scheme for the case where both blur and significant noise are present. In the above context there are situations where many steepest descent steps are required, thus building slowness into the solution procedure. Our general conclusion regarding gradient descent methods is that in such cases the faster gradient descent methods offer substantial advantages. In other situations where no such slowness buildup arises the steepest descent method can still be very effective. version:1
arxiv-1308-2410 | Collective Mind: cleaning up the research and experimentation mess in computer engineering using crowdsourcing, big data and machine learning | http://arxiv.org/abs/1308.2410 | id:1308.2410 author:Grigori Fursin category:cs.SE cs.HC stat.ML  published:2013-08-11 summary:Software and hardware co-design and optimization of HPC systems has become intolerably complex, ad-hoc, time consuming and error prone due to enormous number of available design and optimization choices, complex interactions between all software and hardware components, and multiple strict requirements placed on performance, power consumption, size, reliability and cost. We present our novel long-term holistic and practical solution to this problem based on customizable, plugin-based, schema-free, heterogeneous, open-source Collective Mind repository and infrastructure with unified web interfaces and on-line advise system. This collaborative framework distributes analysis and multi-objective off-line and on-line auto-tuning of computer systems among many participants while utilizing any available smart phone, tablet, laptop, cluster or data center, and continuously observing, classifying and modeling their realistic behavior. Any unexpected behavior is analyzed using shared data mining and predictive modeling plugins or exposed to the community at cTuning.org for collaborative explanation, top-down complexity reduction, incremental problem decomposition and detection of correlating program, architecture or run-time properties (features). Gradually increasing optimization knowledge helps to continuously improve optimization heuristics of any compiler, predict optimizations for new programs or suggest efficient run-time (online) tuning and adaptation strategies depending on end-user requirements. We decided to share all our past research artifacts including hundreds of codelets, numerical applications, data sets, models, universal experimental analysis and auto-tuning pipelines, self-tuning machine learning based meta compiler, and unified statistical analysis and machine learning plugins in a public repository to initiate systematic, reproducible and collaborative research, development and experimentation with a new publication model where experiments and techniques are validated, ranked and improved by the community. version:1
arxiv-1308-2403 | CDfdr: A Comparison Density Approach to Local False Discovery Rate Estimation | http://arxiv.org/abs/1308.2403 | id:1308.2403 author:Subhadeep Mukhopadhyay category:stat.ME math.ST stat.AP stat.ML stat.TH  published:2013-08-11 summary:Efron et al. (2001) proposed empirical Bayes formulation of the frequentist Benjamini and Hochbergs False Discovery Rate method (Benjamini and Hochberg,1995). This article attempts to unify the `two cultures' using concepts of comparison density and distribution function. We have also shown how almost all of the existing local fdr methods can be viewed as proposing various model specification for comparison density - unifies the vast literature of false discovery methods under one concept and notation. version:1
arxiv-1308-2375 | A radial basis function neural network based approach for the electrical characteristics estimation of a photovoltaic module | http://arxiv.org/abs/1308.2375 | id:1308.2375 author:Francesco Bonanno, Giacomo Capizzi, Christian Napoli, Giorgio Graditi, Giuseppe Marco Tina category:cs.NE  published:2013-08-11 summary:The design process of photovoltaic (PV) modules can be greatly enhanced by using advanced and accurate models in order to predict accurately their electrical output behavior. The main aim of this paper is to investigate the application of an advanced neural network based model of a module to improve the accuracy of the predicted output I--V and P--V curves and to keep in account the change of all the parameters at different operating conditions. Radial basis function neural networks (RBFNN) are here utilized to predict the output characteristic of a commercial PV module, by reading only the data of solar irradiation and temperature. A lot of available experimental data were used for the training of the RBFNN, and a backpropagation algorithm was employed. Simulation and experimental validation is reported. version:1
arxiv-1308-2359 | Exploratory Analysis of Highly Heterogeneous Document Collections | http://arxiv.org/abs/1308.2359 | id:1308.2359 author:Arun S. Maiya, John P. Thompson, Francisco Loaiza-Lemos, Robert M. Rolfe category:cs.CL cs.HC cs.IR I.2.7; H.3.3; H.5.2  published:2013-08-11 summary:We present an effective multifaceted system for exploratory analysis of highly heterogeneous document collections. Our system is based on intelligently tagging individual documents in a purely automated fashion and exploiting these tags in a powerful faceted browsing framework. Tagging strategies employed include both unsupervised and supervised approaches based on machine learning and natural language processing. As one of our key tagging strategies, we introduce the KERA algorithm (Keyword Extraction for Reports and Articles). KERA extracts topic-representative terms from individual documents in a purely unsupervised fashion and is revealed to be significantly more effective than state-of-the-art methods. Finally, we evaluate our system in its ability to help users locate documents pertaining to military critical technologies buried deep in a large heterogeneous sea of information. version:1
arxiv-1308-2350 | Learning Features and their Transformations by Spatial and Temporal Spherical Clustering | http://arxiv.org/abs/1308.2350 | id:1308.2350 author:Jayanta K. Dutta, Bonny Banerjee category:cs.NE cs.CV q-bio.NC I.2; I.4; I.5  published:2013-08-10 summary:Learning features invariant to arbitrary transformations in the data is a requirement for any recognition system, biological or artificial. It is now widely accepted that simple cells in the primary visual cortex respond to features while the complex cells respond to features invariant to different transformations. We present a novel two-layered feedforward neural model that learns features in the first layer by spatial spherical clustering and invariance to transformations in the second layer by temporal spherical clustering. Learning occurs in an online and unsupervised manner following the Hebbian rule. When exposed to natural videos acquired by a camera mounted on a cat's head, the first and second layer neurons in our model develop simple and complex cell-like receptive field properties. The model can predict by learning lateral connections among the first layer neurons. A topographic map to their spatial features emerges by exponentially decaying the flow of activation with distance from one neuron to another in the first layer that fire in close temporal proximity, thereby minimizing the pooling length in an online manner simultaneously with feature learning. version:1
arxiv-1308-2307 | Finite Element Model Updating Using Fish School Search Optimization Method | http://arxiv.org/abs/1308.2307 | id:1308.2307 author:I. Boulkabeit, L. Mthembu, T. Marwala, F. De Lima Neto category:cs.CE cs.NE  published:2013-08-10 summary:A recent nature inspired optimization algorithm, Fish School Search (FSS) is applied to the finite element model (FEM) updating problem. This method is tested on a GARTEUR SM-AG19 aeroplane structure. The results of this algorithm are compared with two other metaheuristic algorithms; Genetic Algorithm (GA) and Particle Swarm Optimization (PSO). It is observed that on average, the FSS and PSO algorithms give more accurate results than the GA. A minor modification to the FSS is proposed. This modification improves the performance of FSS on the FEM updating problem which has a constrained search space. version:1
arxiv-1310-1257 | Second order scattering descriptors predict fMRI activity due to visual textures | http://arxiv.org/abs/1310.1257 | id:1310.1257 author:Michael Eickenberg, Fabian Pedregosa, Senoussi Mehdi, Alexandre Gramfort, Bertrand Thirion category:cs.CV  published:2013-08-10 summary:Second layer scattering descriptors are known to provide good classification performance on natural quasi-stationary processes such as visual textures due to their sensitivity to higher order moments and continuity with respect to small deformations. In a functional Magnetic Resonance Imaging (fMRI) experiment we present visual textures to subjects and evaluate the predictive power of these descriptors with respect to the predictive power of simple contour energy - the first scattering layer. We are able to conclude not only that invariant second layer scattering coefficients better encode voxel activity, but also that well predicted voxels need not necessarily lie in known retinotopic regions. version:1
arxiv-1308-2292 | Fast image segmentation and restoration using parametric curve evolution with junctions and topology changes | http://arxiv.org/abs/1308.2292 | id:1308.2292 author:Heike Benninghoff, Harald Garcke category:cs.CV math.AP math.NA  published:2013-08-10 summary:Curve evolution schemes for image segmentation based on a region based contour model allowing for junctions, vector-valued images and topology changes are introduced. Together with an a posteriori denoising in the segmented homogeneous regions this leads to a fast and efficient method for image segmentation and restoration. An uneven spread of mesh points is avoided by using the tangential degrees of freedom. Several numerical simulations on artificial test problems and on real images illustrate the performance of the method. version:1
arxiv-1303-0632 | Supplement to "Reversible MCMC on Markov equivalence classes of sparse directed acyclic graphs" | http://arxiv.org/abs/1303.0632 | id:1303.0632 author:Yangbo He, Jinzhu Jia, Bin Yu category:stat.ML math.CO  published:2013-03-04 summary:This supplementary material includes three parts: some preliminary results, four examples, an experiment, three new algorithms, and all proofs of the results in the paper "Reversible MCMC on Markov equivalence classes of sparse directed acyclic graphs". version:2
arxiv-1306-2665 | Precisely Verifying the Null Space Conditions in Compressed Sensing: A Sandwiching Algorithm | http://arxiv.org/abs/1306.2665 | id:1306.2665 author:Myung Cho, Weiyu Xu category:cs.IT cs.LG cs.SY math.IT math.OC stat.ML  published:2013-06-11 summary:In this paper, we propose new efficient algorithms to verify the null space condition in compressed sensing (CS). Given an $(n-m) \times n$ ($m>0$) CS matrix $A$ and a positive $k$, we are interested in computing $\displaystyle \alpha_k = \max_{\{z: Az=0,z\neq 0\}}\max_{\{K: K \leq k\}}$ ${\ z_K \ _{1}}{\ z\ _{1}}$, where $K$ represents subsets of $\{1,2,...,n\}$, and $ K $ is the cardinality of $K$. In particular, we are interested in finding the maximum $k$ such that $\alpha_k < {1}{2}$. However, computing $\alpha_k$ is known to be extremely challenging. In this paper, we first propose a series of new polynomial-time algorithms to compute upper bounds on $\alpha_k$. Based on these new polynomial-time algorithms, we further design a new sandwiching algorithm, to compute the \emph{exact} $\alpha_k$ with greatly reduced complexity. When needed, this new sandwiching algorithm also achieves a smooth tradeoff between computational complexity and result accuracy. Empirical results show the performance improvements of our algorithm over existing known methods; and our algorithm outputs precise values of $\alpha_k$, with much lower complexity than exhaustive search. version:3
arxiv-1308-2218 | Coding for Random Projections | http://arxiv.org/abs/1308.2218 | id:1308.2218 author:Ping Li, Michael Mitzenmacher, Anshumali Shrivastava category:cs.LG cs.DS cs.IT math.IT stat.CO  published:2013-08-09 summary:The method of random projections has become very popular for large-scale applications in statistical learning, information retrieval, bio-informatics and other applications. Using a well-designed coding scheme for the projected data, which determines the number of bits needed for each projected value and how to allocate these bits, can significantly improve the effectiveness of the algorithm, in storage cost as well as computational speed. In this paper, we study a number of simple coding schemes, focusing on the task of similarity estimation and on an application to training linear classifiers. We demonstrate that uniform quantization outperforms the standard existing influential method (Datar et. al. 2004). Indeed, we argue that in many cases coding with just a small number of bits suffices. Furthermore, we also develop a non-uniform 2-bit coding scheme that generally performs well in practice, as confirmed by our experiments on training linear support vector machines (SVM). version:1
arxiv-1304-2850 | Entropy landscape of solutions in the binary perceptron problem | http://arxiv.org/abs/1304.2850 | id:1304.2850 author:Haiping Huang, K. Y. Michael Wong, Yoshiyuki Kabashima category:cond-mat.dis-nn cond-mat.stat-mech cs.LG  published:2013-04-10 summary:The statistical picture of the solution space for a binary perceptron is studied. The binary perceptron learns a random classification of input random patterns by a set of binary synaptic weights. The learning of this network is difficult especially when the pattern (constraint) density is close to the capacity, which is supposed to be intimately related to the structure of the solution space. The geometrical organization is elucidated by the entropy landscape from a reference configuration and of solution-pairs separated by a given Hamming distance in the solution space. We evaluate the entropy at the annealed level as well as replica symmetric level and the mean field result is confirmed by the numerical simulations on single instances using the proposed message passing algorithms. From the first landscape (a random configuration as a reference), we see clearly how the solution space shrinks as more constraints are added. From the second landscape of solution-pairs, we deduce the coexistence of clustering and freezing in the solution space. version:2
arxiv-1308-1940 | Time series modeling with pruned multi-layer perceptron and 2-stage damped least-squares method | http://arxiv.org/abs/1308.1940 | id:1308.1940 author:Cyril Voyant, Wani W. Tamas, Christophe Paoli, Aurélia Balu, Marc Muselli, Marie Laure Nivet, Gilles Notton category:cs.NE  published:2013-08-08 summary:A Multi-Layer Perceptron (MLP) defines a family of artificial neural networks often used in TS modeling and forecasting. Because of its "black box" aspect, many researchers refuse to use it. Moreover, the optimization (often based on the exhaustive approach where "all" configurations are tested) and learning phases of this artificial intelligence tool (often based on the Levenberg-Marquardt algorithm; LMA) are weaknesses of this approach (exhaustively and local minima). These two tasks must be repeated depending on the knowledge of each new problem studied, making the process, long, laborious and not systematically robust. In this paper a pruning process is proposed. This method allows, during the training phase, to carry out an inputs selecting method activating (or not) inter-nodes connections in order to verify if forecasting is improved. We propose to use iteratively the popular damped least-squares method to activate inputs and neurons. A first pass is applied to 10% of the learning sample to determine weights significantly different from 0 and delete other. Then a classical batch process based on LMA is used with the new MLP. The validation is done using 25 measured meteorological TS and cross-comparing the prediction results of the classical LMA and the 2-stage LMA. version:1
arxiv-1003-1410 | Local Space-Time Smoothing for Version Controlled Documents | http://arxiv.org/abs/1003.1410 | id:1003.1410 author:Seungyeon Kim, Guy Lebanon category:cs.GR cs.CL cs.LG  published:2010-03-06 summary:Unlike static documents, version controlled documents are continuously edited by one or more authors. Such collaborative revision process makes traditional modeling and visualization techniques inappropriate. In this paper we propose a new representation based on local space-time smoothing that captures important revision patterns. We demonstrate the applicability of our framework using experiments on synthetic and real-world data. version:2
arxiv-1202-1568 | Beyond Sentiment: The Manifold of Human Emotions | http://arxiv.org/abs/1202.1568 | id:1202.1568 author:Seungyeon Kim, Fuxin Li, Guy Lebanon, Irfan Essa category:cs.CL  published:2012-02-08 summary:Sentiment analysis predicts the presence of positive or negative emotions in a text document. In this paper we consider higher dimensional extensions of the sentiment concept, which represent a richer set of human emotions. Our approach goes beyond previous work in that our model contains a continuous manifold rather than a finite set of human emotions. We investigate the resulting model, compare it to psychological observations, and explore its predictive capabilities. Besides obtaining significant improvements over a baseline without manifold, we are also able to visualize different notions of positive sentiment in different domains. version:2
arxiv-1303-6145 | Particles Prefer Walking Along the Axes: Experimental Insights into the Behavior of a Particle Swarm | http://arxiv.org/abs/1303.6145 | id:1303.6145 author:Manuel Schmitt, Rolf Wanka category:cs.NE cs.AI I.2.8  published:2013-03-25 summary:Particle swarm optimization (PSO) is a widely used nature-inspired meta-heuristic for solving continuous optimization problems. However, when running the PSO algorithm, one encounters the phenomenon of so-called stagnation, that means in our context, the whole swarm starts to converge to a solution that is not (even a local) optimum. The goal of this work is to point out possible reasons why the swarm stagnates at these non-optimal points. To achieve our results, we use the newly defined potential of a swarm. The total potential has a portion for every dimension of the search space, and it drops when the swarm approaches the point of convergence. As it turns out experimentally, the swarm is very likely to come sometimes into "unbalanced" states, i. e., almost all potential belongs to one axis. Therefore, the swarm becomes blind for improvements still possible in any other direction. Finally, we show how in the light of the potential and these observations, a slightly adapted PSO rebalances the potential and therefore increases the quality of the solution. version:2
arxiv-1308-1801 | Satellite image classification methods and Landsat 5TM bands | http://arxiv.org/abs/1308.1801 | id:1308.1801 author:Jamshid Tamouk, Nasser Lotfi, Mina Farmanbar category:cs.CV astro-ph.IM  published:2013-08-08 summary:This paper attempts to find the most accurate classification method among parallelepiped, minimum distance and chain methods. Moreover, this study also challenges to find the suitable combination of bands, which can lead to better results in case combinations of bands occur. After comparing these three methods, the chain method over perform the other methods with 79% overall accuracy. Hence, it is more accurate than minimum distance with 67% and parallelepiped with 65%. On the other hand, based on bands features, and also by combining several researchers' findings, a table was created which includes the main objects on the land and the suitable combination of the bands for accurately detecting of landcover objects. During this process, it was observed that band 4 (out of 7 bands of Landsat 5TM) is the band, which can be used for increasing the accuracy of the combined bands in detecting objects on the land. version:1
arxiv-1308-1792 | OFF-Set: One-pass Factorization of Feature Sets for Online Recommendation in Persistent Cold Start Settings | http://arxiv.org/abs/1308.1792 | id:1308.1792 author:Michal Aharon, Natalie Aizenberg, Edward Bortnikov, Ronny Lempel, Roi Adadi, Tomer Benyamini, Liron Levin, Ran Roth, Ohad Serfaty category:cs.LG cs.IR H.4; D.2.8  published:2013-08-08 summary:One of the most challenging recommendation tasks is recommending to a new, previously unseen user. This is known as the 'user cold start' problem. Assuming certain features or attributes of users are known, one approach for handling new users is to initially model them based on their features. Motivated by an ad targeting application, this paper describes an extreme online recommendation setting where the cold start problem is perpetual. Every user is encountered by the system just once, receives a recommendation, and either consumes or ignores it, registering a binary reward. We introduce One-pass Factorization of Feature Sets, OFF-Set, a novel recommendation algorithm based on Latent Factor analysis, which models users by mapping their features to a latent space. Furthermore, OFF-Set is able to model non-linear interactions between pairs of features. OFF-Set is designed for purely online recommendation, performing lightweight updates of its model per each recommendation-reward observation. We evaluate OFF-Set against several state of the art baselines, and demonstrate its superiority on real ad-targeting data. version:1
arxiv-1308-1603 | A Note on Topology Preservation in Classification, and the Construction of a Universal Neuron Grid | http://arxiv.org/abs/1308.1603 | id:1308.1603 author:Dietmar Volz category:cs.NE cs.AI nlin.AO stat.ML 92F99  published:2013-08-07 summary:It will be shown that according to theorems of K. Menger, every neuron grid if identified with a curve is able to preserve the adopted qualitative structure of a data space. Furthermore, if this identification is made, the neuron grid structure can always be mapped to a subset of a universal neuron grid which is constructable in three space dimensions. Conclusions will be drawn for established neuron grid types as well as neural fields. version:2
arxiv-1207-2812 | Near-Optimal Algorithms for Differentially-Private Principal Components | http://arxiv.org/abs/1207.2812 | id:1207.2812 author:Kamalika Chaudhuri, Anand D. Sarwate, Kaushik Sinha category:stat.ML cs.CR cs.LG  published:2012-07-12 summary:Principal components analysis (PCA) is a standard tool for identifying good low-dimensional approximations to data in high dimension. Many data sets of interest contain private or sensitive information about individuals. Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs. Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs. In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output. We show that the sample complexity of the proposed method differs from the existing procedure in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling. We furthermore illustrate our results, showing that on real data there is a large performance gap between the existing method and our method. version:3
