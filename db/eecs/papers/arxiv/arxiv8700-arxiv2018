arxiv-1502-02761 | Generative Moment Matching Networks | http://arxiv.org/abs/1502.02761 | id:1502.02761 author:Yujia Li, Kevin Swersky, Richard Zemel category:cs.LG cs.AI stat.ML  published:2015-02-10 summary:We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database. version:1
arxiv-1501-06769 | Particle Gibbs with Ancestor Sampling for Probabilistic Programs | http://arxiv.org/abs/1501.06769 | id:1501.06769 author:Jan-Willem van de Meent, Hongseok Yang, Vikash Mansinghka, Frank Wood category:stat.ML cs.AI cs.PL  published:2015-01-27 summary:Particle Markov chain Monte Carlo techniques rank among current state-of-the-art methods for probabilistic program inference. A drawback of these techniques is that they rely on importance resampling, which results in degenerate particle trajectories and a low effective sample size for variables sampled early in a program. We here develop a formalism to adapt ancestor resampling, a technique that mitigates particle degeneracy, to the probabilistic programming setting. We present empirical results that demonstrate nontrivial performance gains. version:5
arxiv-1502-02704 | Learning Reductions that Really Work | http://arxiv.org/abs/1502.02704 | id:1502.02704 author:Alina Beygelzimer, Hal Daumé III, John Langford, Paul Mineiro category:cs.LG  published:2015-02-09 summary:We provide a summary of the mathematical and computational techniques that have enabled learning reductions to effectively address a wide class of problems, and show that this approach to solving machine learning problems can be broadly useful. version:1
arxiv-1502-02651 | Optimal and Adaptive Algorithms for Online Boosting | http://arxiv.org/abs/1502.02651 | id:1502.02651 author:Alina Beygelzimer, Satyen Kale, Haipeng Luo category:cs.LG  published:2015-02-09 summary:We study online boosting, the task of converting any weak online learner into a strong online learner. Based on a novel and natural definition of weak online learnability, we develop two online boosting algorithms. The first algorithm is an online version of boost-by-majority. By proving a matching lower bound, we show that this algorithm is essentially optimal in terms of the number of weak learners and the sample complexity needed to achieve a specified accuracy. This optimal algorithm is not adaptive however. Using tools from online loss minimization, we derive an adaptive online boosting algorithm that is also parameter-free, but not optimal. Both algorithms work with base learners that can handle example importance weights directly, as well as by rejection sampling examples with probability defined by the booster. Results are complemented with an extensive experimental study. version:1
arxiv-1502-02643 | Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions | http://arxiv.org/abs/1502.02643 | id:1502.02643 author:Alina Ene, Huy L. Nguyen category:cs.LG cs.AI  published:2015-02-09 summary:Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of ``simple" functions. In this paper, we use random coordinate descent methods to obtain algorithms with faster linear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly fewer iterations. version:1
arxiv-1402-5481 | From Predictive to Prescriptive Analytics | http://arxiv.org/abs/1402.5481 | id:1402.5481 author:Dimitris Bertsimas, Nathan Kallus category:stat.ML cs.LG math.OC  published:2014-02-22 summary:In this paper, we combine ideas from machine learning (ML) and operations research and management science (OR/MS) in developing a framework, along with specific methods, for using data to prescribe decisions in OR/MS problems. In a departure from other work on data-driven optimization and reflecting our practical experience with the data available in applications of OR/MS, we consider data consisting, not only of observations of quantities with direct effect on costs/revenues, such as demand or returns, but predominantly of observations of associated auxiliary quantities. The main problem of interest is a conditional stochastic optimization problem, given imperfect observations, where the joint probability distributions that specify the problem are unknown. We demonstrate that our proposed solution methods are generally applicable to a wide range of decision problems. We prove that they are computationally tractable and asymptotically optimal under mild conditions even when data is not independent and identically distributed (iid) and even for censored observations. As an analogue to the coefficient of determination $R^2$, we develop a metric $P$ termed the coefficient of prescriptiveness to measure the prescriptive content of data and the efficacy of a policy from an operations perspective. To demonstrate the power of our approach in a real-world setting we study an inventory management problem faced by the distribution arm of an international media conglomerate, which ships an average of 1 billion units per year. We leverage both internal data and public online data harvested from IMDb, Rotten Tomatoes, and Google to prescribe operational decisions that outperform baseline measures. Specifically, the data we collect, leveraged by our methods, accounts for an 88% improvement as measured by our coefficient of prescriptiveness. version:3
arxiv-1502-00558 | Complex-Valued Hough Transforms for Circles | http://arxiv.org/abs/1502.00558 | id:1502.00558 author:Marcelo Cicconet, Davi Geiger, Michael Werman category:cs.CV  published:2015-02-02 summary:This paper advocates the use of complex variables to represent votes in the Hough transform for circle detection. Replacing the positive numbers classically used in the parameter space of the Hough transforms by complex numbers allows cancellation effects when adding up the votes. Cancellation and the computation of shape likelihood via a complex number's magnitude square lead to more robust solutions than the "classic" algorithms, as shown by computational experiments on synthetic and real datasets. version:2
arxiv-1502-00561 | Quantum Pairwise Symmetry: Applications in 2D Shape Analysis | http://arxiv.org/abs/1502.00561 | id:1502.00561 author:Marcelo Cicconet, Davi Geiger, Michael Werman category:cs.CV  published:2015-02-02 summary:A pair of rooted tangents -- defining a quantum triangle -- with an associated quantum wave of spin 1/2 is proposed as the primitive to represent and compute symmetry. Measures of the spin characterize how "isosceles" or how "degenerate" these triangles are -- which corresponds to their mirror or parallel symmetry. We also introduce a complex-valued kernel to model probability errors in the parameter space, which is more robust to noise and clutter than the classical model. version:2
arxiv-1502-02609 | Efficient model-based reinforcement learning for approximate online optimal | http://arxiv.org/abs/1502.02609 | id:1502.02609 author:Rushikesh Kamalapurkar, Joel A. Rosenfeld, Warren E. Dixon category:cs.SY cs.LG math.OC  published:2015-02-09 summary:In this paper the infinite horizon optimal regulation problem is solved online for a deterministic control-affine nonlinear dynamical system using the state following (StaF) kernel method to approximate the value function. Unlike traditional methods that aim to approximate a function over a large compact set, the StaF kernel method aims to approximate a function in a small neighborhood of a state that travels within a compact set. Simulation results demonstrate that stability and approximate optimality of the control system can be achieved with significantly fewer basis functions than may be required for global approximation methods. version:1
arxiv-1502-02599 | Adaptive Random SubSpace Learning (RSSL) Algorithm for Prediction | http://arxiv.org/abs/1502.02599 | id:1502.02599 author:Mohamed Elshrif, Ernest Fokoue category:cs.LG  published:2015-02-09 summary:We present a novel adaptive random subspace learning algorithm (RSSL) for prediction purpose. This new framework is flexible where it can be adapted with any learning technique. In this paper, we tested the algorithm for regression and classification problems. In addition, we provide a variety of weighting schemes to increase the robustness of the developed algorithm. These different wighting flavors were evaluated on simulated as well as on real-world data sets considering the cases where the ratio between features (attributes) and instances (samples) is large and vice versa. The framework of the new algorithm consists of many stages: first, calculate the weights of all features on the data set using the correlation coefficient and F-statistic statistical measurements. Second, randomly draw n samples with replacement from the data set. Third, perform regular bootstrap sampling (bagging). Fourth, draw without replacement the indices of the chosen variables. The decision was taken based on the heuristic subspacing scheme. Fifth, call base learners and build the model. Sixth, use the model for prediction purpose on test set of the data. The results show the advancement of the adaptive RSSL algorithm in most of the cases compared with the synonym (conventional) machine learning algorithms. version:1
arxiv-1412-4446 | Domain-Adversarial Neural Networks | http://arxiv.org/abs/1412.4446 | id:1412.4446 author:Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand category:stat.ML cs.LG cs.NE  published:2014-12-15 summary:We introduce a new representation learning algorithm suited to the context of domain adaptation, in which data at training and test time come from similar but different distributions. Our algorithm is directly inspired by theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on a data representation that cannot discriminate between the training (source) and test (target) domains. We propose a training objective that implements this idea in the context of a neural network, whose hidden layer is trained to be predictive of the classification task, but uninformative as to the domain of the input. Our experiments on a sentiment analysis classification benchmark, where the target domain data available at training time is unlabeled, show that our neural network for domain adaption algorithm has better performance than either a standard neural network or an SVM, even if trained on input features extracted with the state-of-the-art marginalized stacked denoising autoencoders of Chen et al. (2012). version:2
arxiv-1502-02551 | Deep Learning with Limited Numerical Precision | http://arxiv.org/abs/1502.02551 | id:1502.02551 author:Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, Pritish Narayanan category:cs.LG cs.NE stat.ML  published:2015-02-09 summary:Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding. version:1
arxiv-1501-03326 | Unbiased Bayes for Big Data: Paths of Partial Posteriors | http://arxiv.org/abs/1501.03326 | id:1501.03326 author:Heiko Strathmann, Dino Sejdinovic, Mark Girolami category:stat.ML cs.LG stat.ME  published:2015-01-14 summary:A key quantity of interest in Bayesian inference are expectations of functions with respect to a posterior distribution. Markov Chain Monte Carlo is a fundamental tool to consistently compute these expectations via averaging samples drawn from an approximate posterior. However, its feasibility is being challenged in the era of so called Big Data as all data needs to be processed in every iteration. Realising that such simulation is an unnecessarily hard problem if the goal is estimation, we construct a computationally scalable methodology that allows unbiased estimation of the required expectations -- without explicit simulation from the full posterior. The scheme's variance is finite by construction and straightforward to control, leading to algorithms that are provably unbiased and naturally arrive at a desired error tolerance. This is achieved at an average computational complexity that is sub-linear in the size of the dataset and its free parameters are easy to tune. We demonstrate the utility and generality of the methodology on a range of common statistical models applied to large-scale benchmark and real-world datasets. version:2
arxiv-1502-02513 | Evaluation of modelling approaches for predicting the spatial distribution of soil organic carbon stocks at the national scale | http://arxiv.org/abs/1502.02513 | id:1502.02513 author:M. P. Martin, T. G. Orton, E. Lacarce, J. Meersmans, N. P. A. Saby, J. B. Paroissien, C. Jolivet, L. Boulonne, D. Arrouays category:stat.AP stat.ML  published:2015-02-09 summary:Soil organic carbon (SOC) plays a major role in the global carbon budget. It can act as a source or a sink of atmospheric carbon, thereby possibly influencing the course of climate change. Improving the tools that model the spatial distributions of SOC stocks at national scales is a priority, both for monitoring changes in SOC and as an input for global carbon cycles studies. In this paper, we compare and evaluate two recent and promising modelling approaches. First, we considered several increasingly complex boosted regression trees (BRT), a convenient and efficient multiple regression model from the statistical learning field. Further, we considered a robust geostatistical approach coupled to the BRT models. Testing the different approaches was performed on the dataset from the French Soil Monitoring Network, with a consistent cross-validation procedure. We showed that when a limited number of predictors were included in the BRT model, the standalone BRT predictions were significantly improved by robust geostatistical modelling of the residuals. However, when data for several SOC drivers were included, the standalone BRT model predictions were not significantly improved by geostatistical modelling. Therefore, in this latter situation, the BRT predictions might be considered adequate without the need for geostatistical modelling, provided that i) care is exercised in model fitting and validating, and ii) the dataset does not allow for modelling of local spatial autocorrelations, as is the case for many national systematic sampling schemes. version:1
arxiv-1502-02512 | The Adaptive Mean-Linkage Algorithm: A Bottom-Up Hierarchical Cluster Technique | http://arxiv.org/abs/1502.02512 | id:1502.02512 author:H. M. de Oliveira category:stat.ME cs.LG stat.AP  published:2015-02-09 summary:In this paper a variant of the classical hierarchical cluster analysis is reported. This agglomerative (bottom-up) cluster technique is referred to as the Adaptive Mean-Linkage Algorithm. It can be interpreted as a linkage algorithm where the value of the threshold is conveniently up-dated at each interaction. The superiority of the adaptive clustering with respect to the average-linkage algorithm follows because it achieves a good compromise on threshold values: Thresholds based on the cut-off distance are sufficiently small to assure the homogeneity and also large enough to guarantee at least a pair of merging sets. This approach is applied to a set of possible substituents in a chemical series. version:1
arxiv-1502-02506 | Predicting Alzheimer's disease: a neuroimaging study with 3D convolutional neural networks | http://arxiv.org/abs/1502.02506 | id:1502.02506 author:Adrien Payan, Giovanni Montana category:cs.CV cs.LG stat.AP stat.ML  published:2015-02-09 summary:Pattern recognition methods using neuroimaging data for the diagnosis of Alzheimer's disease have been the subject of extensive research in recent years. In this paper, we use deep learning methods, and in particular sparse autoencoders and 3D convolutional neural networks, to build an algorithm that can predict the disease status of a patient, based on an MRI scan of the brain. We report on experiments using the ADNI data set involving 2,265 historical scans. We demonstrate that 3D convolutional neural networks outperform several other classifiers reported in the literature and produce state-of-art results. version:1
arxiv-1502-02478 | Efficient batchwise dropout training using submatrices | http://arxiv.org/abs/1502.02478 | id:1502.02478 author:Ben Graham, Jeremy Reizenstein, Leigh Robinson category:cs.NE cs.CV  published:2015-02-09 summary:Dropout is a popular technique for regularizing artificial neural networks. Dropout networks are generally trained by minibatch gradient descent with a dropout mask turning off some of the units---a different pattern of dropout is applied to every sample in the minibatch. We explore a very simple alternative to the dropout mask. Instead of masking dropped out units by setting them to zero, we perform matrix multiplication using a submatrix of the weight matrix---unneeded hidden units are never calculated. Performing dropout batchwise, so that one pattern of dropout is used for each sample in a minibatch, we can substantially reduce training times. Batchwise dropout can be used with fully-connected and convolutional neural networks. version:1
arxiv-1409-7930 | Cognitive Learning of Statistical Primary Patterns via Bayesian Network | http://arxiv.org/abs/1409.7930 | id:1409.7930 author:Weijia Han, Huiyan Sang, Min Sheng, Jiandong Li, Shuguang Cui category:cs.LG  published:2014-09-28 summary:In cognitive radio (CR) technology, the trend of sensing is no longer to only detect the presence of active primary users. A large number of applications demand for more comprehensive knowledge on primary user behaviors in spatial, temporal, and frequency domains. To satisfy such requirements, we study the statistical relationship among primary users by introducing a Bayesian network (BN) based framework. How to learn such a BN structure is a long standing issue, not fully understood even in the statistical learning community. Besides, another key problem in this learning scenario is that the CR has to identify how many variables are in the BN, which is usually considered as prior knowledge in statistical learning applications. To solve such two issues simultaneously, this paper proposes a BN structure learning scheme consisting of an efficient structure learning algorithm and a blind variable identification scheme. The proposed approach incurs significantly lower computational complexity compared with previous ones, and is capable of determining the structure without assuming much prior knowledge about variables. With this result, cognitive users could efficiently understand the statistical pattern of primary networks, such that more efficient cognitive protocols could be designed across different network layers. version:5
arxiv-1502-02444 | On the Dynamics of a Recurrent Hopfield Network | http://arxiv.org/abs/1502.02444 | id:1502.02444 author:Rama Garimella, Berkay Kicanaoglu, Moncef Gabbouj category:cs.NE  published:2015-02-09 summary:In this research paper novel real/complex valued recurrent Hopfield Neural Network (RHNN) is proposed. The method of synthesizing the energy landscape of such a network and the experimental investigation of dynamics of Recurrent Hopfield Network is discussed. Parallel modes of operation (other than fully parallel mode) in layered RHNN is proposed. Also, certain potential applications are proposed. version:1
arxiv-1502-02410 | Out-of-sample generalizations for supervised manifold learning for classification | http://arxiv.org/abs/1502.02410 | id:1502.02410 author:Elif Vural, Christine Guillemot category:cs.CV cs.LG  published:2015-02-09 summary:Supervised manifold learning methods for data classification map data samples residing in a high-dimensional ambient space to a lower-dimensional domain in a structure-preserving way, while enhancing the separation between different classes in the learned embedding. Most nonlinear supervised manifold learning methods compute the embedding of the manifolds only at the initially available training points, while the generalization of the embedding to novel points, known as the out-of-sample extension problem in manifold learning, becomes especially important in classification applications. In this work, we propose a semi-supervised method for building an interpolation function that provides an out-of-sample extension for general supervised manifold learning algorithms studied in the context of classification. The proposed algorithm computes a radial basis function (RBF) interpolator that minimizes an objective function consisting of the total embedding error of unlabeled test samples, defined as their distance to the embeddings of the manifolds of their own class, as well as a regularization term that controls the smoothness of the interpolation function in a direction-dependent way. The class labels of test data and the interpolation function parameters are estimated jointly with a progressive procedure. Experimental results on face and object images demonstrate the potential of the proposed out-of-sample extension algorithm for the classification of manifold-modeled data sets. version:1
arxiv-1502-02407 | A Social Spider Algorithm for Global Optimization | http://arxiv.org/abs/1502.02407 | id:1502.02407 author:James J. Q. Yu, Victor O. K. Li category:cs.NE  published:2015-02-09 summary:The growing complexity of real-world problems has motivated computer scientists to search for efficient problem-solving methods. Metaheuristics based on evolutionary computation and swarm intelligence are outstanding examples of nature-inspired solution techniques. Inspired by the social spiders, we propose a novel Social Spider Algorithm to solve global optimization problems. This algorithm is mainly based on the foraging strategy of social spiders, utilizing the vibrations on the spider web to determine the positions of preys. Different from the previously proposed swarm intelligence algorithms, we introduce a new social animal foraging strategy model to solve optimization problems. In addition, we perform preliminary parameter sensitivity analysis for our proposed algorithm, developing guidelines for choosing the parameter values. The Social Spider Algorithm is evaluated by a series of widely-used benchmark functions, and our proposed algorithm has superior performance compared with other state-of-the-art metaheuristics. version:1
arxiv-1502-02330 | Tensor Canonical Correlation Analysis for Multi-view Dimension Reduction | http://arxiv.org/abs/1502.02330 | id:1502.02330 author:Yong Luo, Dacheng Tao, Yonggang Wen, Kotagiri Ramamohanarao, Chao Xu category:stat.ML cs.CV cs.LG  published:2015-02-09 summary:Canonical correlation analysis (CCA) has proven an effective tool for two-view dimension reduction due to its profound theoretical foundation and success in practical applications. In respect of multi-view learning, however, it is limited by its capability of only handling data represented by two-view features, while in many real-world applications, the number of views is frequently many more. Although the ad hoc way of simultaneously exploring all possible pairs of features can numerically deal with multi-view data, it ignores the high order statistics (correlation information) which can only be discovered by simultaneously exploring all features. Therefore, in this work, we develop tensor CCA (TCCA) which straightforwardly yet naturally generalizes CCA to handle the data of an arbitrary number of views by analyzing the covariance tensor of the different views. TCCA aims to directly maximize the canonical correlation of multiple (more than two) views. Crucially, we prove that the multi-view canonical correlation maximization problem is equivalent to finding the best rank-1 approximation of the data covariance tensor, which can be solved efficiently using the well-known alternating least squares (ALS) algorithm. As a consequence, the high order correlation information contained in the different views is explored and thus a more reliable common subspace shared by all features can be obtained. In addition, a non-linear extension of TCCA is presented. Experiments on various challenge tasks, including large scale biometric structure prediction, internet advertisement classification and web image annotation, demonstrate the effectiveness of the proposed method. version:1
arxiv-1411-4033 | Sparse And Low Rank Decomposition Based Batch Image Alignment for Speckle Reduction of retinal OCT Images | http://arxiv.org/abs/1411.4033 | id:1411.4033 author:Ahmadreza Baghaie, Roshan M. D'souza, Zeyun Yu category:cs.CV  published:2014-11-14 summary:Optical Coherence Tomography (OCT) is an emerging technique in the field of biomedical imaging, with applications in ophthalmology, dermatology, coronary imaging etc. Due to the underlying physics, OCT images usually suffer from a granular pattern, called speckle noise, which restricts the process of interpretation. Here, a sparse and low rank decomposition based method is used for speckle reduction in retinal OCT images. This technique works on input data that consists of several B-scans of the same location. The next step is the batch alignment of the images using a sparse and low-rank decomposition based technique. Finally the denoised image is created by median filtering of the low-rank component of the processed data. Simultaneous decomposition and alignment of the images result in better performance in comparison to simple registration-based methods that are used in the literature for noise reduction of OCT images. version:3
arxiv-1408-5574 | Supervised Hashing Using Graph Cuts and Boosted Decision Trees | http://arxiv.org/abs/1408.5574 | id:1408.5574 author:Guosheng Lin, Chunhua Shen, Anton van den Hengel category:cs.LG cs.CV  published:2014-08-24 summary:Embedding image features into a binary Hamming space can improve both the speed and accuracy of large-scale query-by-example image retrieval systems. Supervised hashing aims to map the original features to compact binary codes in a manner which preserves the label-based similarities of the original data. Most existing approaches apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of those methods, and can result in complex optimization problems that are difficult to solve. In this work we proffer a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. The proposed framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problem-specific hashing methods. Our framework decomposes the into two steps: binary code (hash bits) learning, and hash function learning. The first step can typically be formulated as a binary quadratic problem, and the second step can be accomplished by training standard binary classifiers. For solving large-scale binary code inference, we show how to ensure that the binary quadratic problems are submodular such that an efficient graph cut approach can be used. To achieve efficiency as well as efficacy on large-scale high-dimensional data, we propose to use boosted decision trees as the hash functions, which are nonlinear, highly descriptive, and very fast to train and evaluate. Experiments demonstrate that our proposed method significantly outperforms most state-of-the-art methods, especially on high-dimensional data. version:2
arxiv-1502-02309 | Measuring the functional connectome "on-the-fly": towards a new control signal for fMRI-based brain-computer interfaces | http://arxiv.org/abs/1502.02309 | id:1502.02309 author:Ricardo Pio Monti, Romy Lorenz, Christoforos Anagnostopoulos, Robert Leech, Giovanni Montana category:stat.ML  published:2015-02-08 summary:There has been an explosion of interest in functional Magnetic Resonance Imaging (MRI) during the past two decades. Naturally, this has been accompanied by many major advances in the understanding of the human connectome. These advances have served to pose novel challenges as well as open new avenues for research. One of the most promising and exciting of such avenues is the study of functional MRI in real-time. Such studies have recently gained momentum and have been applied in a wide variety of settings; ranging from training of healthy subjects to self-regulate neuronal activity to being suggested as potential treatments for clinical populations. To date, the vast majority of these studies have focused on a single region at a time. This is due in part to the many challenges faced when estimating dynamic functional connectivity networks in real-time. In this work we propose a novel methodology with which to accurately track changes in functional connectivity networks in real-time. We adapt the recently proposed SINGLE algorithm for estimating sparse and temporally homo- geneous dynamic networks to be applicable in real-time. The proposed method is applied to motor task data from the Human Connectome Project as well as to real-time data ob- tained while exploring a virtual environment. We show that the algorithm is able to estimate signi?cant task-related changes in network structure quickly enough to be useful in future brain-computer interface applications. version:1
arxiv-1502-02277 | Improving Term Frequency Normalization for Multi-topical Documents, and Application to Language Modeling Approaches | http://arxiv.org/abs/1502.02277 | id:1502.02277 author:Seung-Hoon Na, In-Su Kang, Jong-Hyeok Lee category:cs.IR cs.CL H.3.3  published:2015-02-08 summary:Term frequency normalization is a serious issue since lengths of documents are various. Generally, documents become long due to two different reasons - verbosity and multi-topicality. First, verbosity means that the same topic is repeatedly mentioned by terms related to the topic, so that term frequency is more increased than the well-summarized one. Second, multi-topicality indicates that a document has a broad discussion of multi-topics, rather than single topic. Although these document characteristics should be differently handled, all previous methods of term frequency normalization have ignored these differences and have used a simplified length-driven approach which decreases the term frequency by only the length of a document, causing an unreasonable penalization. To attack this problem, we propose a novel TF normalization method which is a type of partially-axiomatic approach. We first formulate two formal constraints that the retrieval model should satisfy for documents having verbose and multi-topicality characteristic, respectively. Then, we modify language modeling approaches to better satisfy these two constraints, and derive novel smoothing methods. Experimental results show that the proposed method increases significantly the precision for keyword queries, and substantially improves MAP (Mean Average Precision) for verbose queries. version:1
arxiv-1502-02268 | SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization | http://arxiv.org/abs/1502.02268 | id:1502.02268 author:Zheng Qu, Peter Richtárik, Martin Takáč, Olivier Fercoq category:cs.LG  published:2015-02-08 summary:We propose a new algorithm for minimizing regularized empirical loss: Stochastic Dual Newton Ascent (SDNA). Our method is dual in nature: in each iteration we update a random subset of the dual variables. However, unlike existing methods such as stochastic dual coordinate ascent, SDNA is capable of utilizing all curvature information contained in the examples, which leads to striking improvements in both theory and practice - sometimes by orders of magnitude. In the special case when an L2-regularizer is used in the primal, the dual problem is a concave quadratic maximization problem plus a separable term. In this regime, SDNA in each step solves a proximal subproblem involving a random principal submatrix of the Hessian of the quadratic function; whence the name of the method. If, in addition, the loss functions are quadratic, our method can be interpreted as a novel variant of the recently introduced Iterative Hessian Sketch. version:1
arxiv-1502-02259 | Contextual Markov Decision Processes | http://arxiv.org/abs/1502.02259 | id:1502.02259 author:Assaf Hallak, Dotan Di Castro, Shie Mannor category:stat.ML cs.LG  published:2015-02-08 summary:We consider a planning problem where the dynamics and rewards of the environment depend on a hidden static parameter referred to as the context. The objective is to learn a strategy that maximizes the accumulated reward across all contexts. The new model, called Contextual Markov Decision Process (CMDP), can model a customer's behavior when interacting with a website (the learner). The customer's behavior depends on gender, age, location, device, etc. Based on that behavior, the website objective is to determine customer characteristics, and to optimize the interaction between them. Our work focuses on one basic scenario--finite horizon with a small known number of possible contexts. We suggest a family of algorithms with provable guarantees that learn the underlying models and the latent contexts, and optimize the CMDPs. Bounds are obtained for specific naive implementations, and extensions of the framework are discussed, laying the ground for future research. version:1
arxiv-1502-02233 | Hierarchical Dirichlet process for tracking complex topical structure evolution and its application to autism research literature | http://arxiv.org/abs/1502.02233 | id:1502.02233 author:Adham Beykikhoshk, Ognjen Arandjelovic, Dinh Phung, Svetha Venkatesh category:cs.IR cs.CL  published:2015-02-08 summary:In this paper we describe a novel framework for the discovery of the topical content of a data corpus, and the tracking of its complex structural changes across the temporal dimension. In contrast to previous work our model does not impose a prior on the rate at which documents are added to the corpus nor does it adopt the Markovian assumption which overly restricts the type of changes that the model can capture. Our key technical contribution is a framework based on (i) discretization of time into epochs, (ii) epoch-wise topic discovery using a hierarchical Dirichlet process-based model, and (iii) a temporal similarity graph which allows for the modelling of complex topic changes: emergence and disappearance, evolution, and splitting and merging. The power of the proposed framework is demonstrated on the medical literature corpus concerned with the autism spectrum disorder (ASD) - an increasingly important research subject of significant social and healthcare importance. In addition to the collected ASD literature corpus which we will make freely available, our contributions also include two free online tools we built as aids to ASD researchers. These can be used for semantically meaningful navigation and searching, as well as knowledge discovery from this large and rapidly growing corpus of literature. version:1
arxiv-1112-3010 | A new variational principle for the Euclidean distance function: Linear approach to the non-linear eikonal problem | http://arxiv.org/abs/1112.3010 | id:1112.3010 author:Karthik S. Gurumoorthy, Anand Rangarajan category:cs.CV math.NA 65D18  65M80  published:2011-12-13 summary:We present a fast convolution-based technique for computing an approximate, signed Euclidean distance function $S$ on a set of 2D and 3D grid locations. Instead of solving the non-linear, static Hamilton-Jacobi equation ($\ \nabla S\ =1$), our solution stems from first solving for a scalar field $\phi$ in a linear differential equation and then deriving the solution for $S$ by taking the negative logarithm. In other words, when $S$ and $\phi$ are related by $\phi = \exp \left(-\frac{S}{\tau} \right)$ and $\phi$ satisfies a specific linear differential equation corresponding to the extremum of a variational problem, we obtain the approximate Euclidean distance function $S = -\tau \log(\phi)$ which converges to the true solution in the limit as $\tau \rightarrow 0$. This is in sharp contrast to techniques like the fast marching and fast sweeping methods which directly solve the Hamilton-Jacobi equation by the Godunov upwind discretization scheme. Our linear formulation results in a closed-form solution to the approximate Euclidean distance function expressible as a discrete convolution, and hence efficiently computable using the fast Fourier transform (FFT). Our solution also circumvents the need for spatial discretization of the derivative operator. As $\tau\rightarrow0$ we show the convergence of our results to the true solution and also bound the error for a given value of $\tau$. The differentiability of our solution allows us to compute---using a set of convolutions---the first and second derivatives of the approximate distance function. In order to determine the sign of the distance function (defined to be positive inside a closed region and negative outside), we compute the winding number in 2D and the topological degree in 3D, whose computations can also be performed via fast convolutions. We demonstrate the efficacy of our method through a set of experimental results. version:4
arxiv-1403-1937 | A fast eikonal equation solver using the Schrodinger wave equation | http://arxiv.org/abs/1403.1937 | id:1403.1937 author:Karthik S. Gurumoorthy, Adrian M. Peter, Birmingham Hang Guan, Anand Rangarajan category:math.NA cs.CV cs.NA  published:2014-03-08 summary:We use a Schr\"odinger wave equation formalism to solve the eikonal equation. In our framework, a solution to the eikonal equation is obtained in the limit as Planck's constant $\hbar$ (treated as a free parameter) tends to zero of the solution to the corresponding linear Schr\"odinger equation. The Schr\"odinger equation corresponding to the eikonal turns out to be a \emph{generalized, screened Poisson equation}. Despite being linear, it does not have a closed-form solution for arbitrary forcing functions. We present two different techniques to solve the screened Poisson equation. In the first approach we use a standard perturbation analysis approach to derive a new algorithm which is guaranteed to converge provided the forcing function is bounded and positive. The perturbation technique requires a sequence of discrete convolutions which can be performed in $O(N\log N)$ using the Fast Fourier Transform (FFT) where $N$ is the number of grid points. In the second method we discretize the linear Laplacian operator by the finite difference method leading to a sparse linear system of equations which can be solved using the plethora of sparse solvers. The eikonal solution is recovered from the exponent of the resultant scalar field. Our approach eliminates the need to explicitly construct viscosity solutions as customary with direct solutions to the eikonal. Since the linear equation is computed for a small but non-zero $\hbar$, the obtained solution is an approximation. Though our solution framework is applicable to the general class of eikonal problems, we detail specifics for the popular vision applications of shape-from-shading, vessel segmentation, and path planning. version:2
arxiv-1502-02215 | Real World Applications of Machine Learning Techniques over Large Mobile Subscriber Datasets | http://arxiv.org/abs/1502.02215 | id:1502.02215 author:Jobin Wilson, Chitharanj Kachappilly, Rakesh Mohan, Prateek Kapadia, Arun Soman, Santanu Chaudhury category:cs.LG cs.CY cs.SE  published:2015-02-08 summary:Communication Service Providers (CSPs) are in a unique position to utilize their vast transactional data assets generated from interactions of subscribers with network elements as well as with other subscribers. CSPs could leverage its data assets for a gamut of applications such as service personalization, predictive offer management, loyalty management, revenue forecasting, network capacity planning, product bundle optimization and churn management to gain significant competitive advantage. However, due to the sheer data volume, variety, velocity and veracity of mobile subscriber datasets, sophisticated data analytics techniques and frameworks are necessary to derive actionable insights in a useable timeframe. In this paper, we describe our journey from a relational database management system (RDBMS) based campaign management solution which allowed data scientists and marketers to use hand-written rules for service personalization and targeted promotions to a distributed Big Data Analytics platform, capable of performing large scale machine learning and data mining to deliver real time service personalization, predictive modelling and product optimization. Our work involves a careful blend of technology, processes and best practices, which facilitate man-machine collaboration and continuous experimentation to derive measurable economic value from data. Our platform has a reach of more than 500 million mobile subscribers worldwide, delivering over 1 billion personalized recommendations annually, processing a total data volume of 64 Petabytes, corresponding to 8.5 trillion events. version:1
arxiv-1407-6089 | Learning Rank Functionals: An Empirical Study | http://arxiv.org/abs/1407.6089 | id:1407.6089 author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:cs.IR cs.LG stat.ML  published:2014-07-23 summary:Ranking is a key aspect of many applications, such as information retrieval, question answering, ad placement and recommender systems. Learning to rank has the goal of estimating a ranking model automatically from training data. In practical settings, the task often reduces to estimating a rank functional of an object with respect to a query. In this paper, we investigate key issues in designing an effective learning to rank algorithm. These include data representation, the choice of rank functionals, the design of the loss function so that it is correlated with the rank metrics used in evaluation. For the loss function, we study three techniques: approximating the rank metric by a smooth function, decomposition of the loss into a weighted sum of element-wise losses and into a weighted sum of pairwise losses. We then present derivations of piecewise losses using the theory of high-order Markov chains and Markov random fields. In experiments, we evaluate these design aspects on two tasks: answer ranking in a Social Question Answering site, and Web Information Retrieval. version:2
arxiv-1502-02655 | An investigation into language complexity of World-of-Warcraft game-external texts | http://arxiv.org/abs/1502.02655 | id:1502.02655 author:Simon Šuster category:cs.CL  published:2015-02-07 summary:We present a language complexity analysis of World of Warcraft (WoW) community texts, which we compare to texts from a general corpus of web English. Results from several complexity types are presented, including lexical diversity, density, readability and syntactic complexity. The language of WoW texts is found to be comparable to the general corpus on some complexity measures, yet more specialized on other measures. Our findings can be used by educators willing to include game-related activities into school curricula. version:1
arxiv-1502-02171 | Person Re-identification Meets Image Search | http://arxiv.org/abs/1502.02171 | id:1502.02171 author:Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jiahao Bu, Qi Tian category:cs.CV  published:2015-02-07 summary:For long time, person re-identification and image search are two separately studied tasks. However, for person re-identification, the effectiveness of local features and the "query-search" mode make it well posed for image search techniques. In the light of recent advances in image search, this paper proposes to treat person re-identification as an image search problem. Specifically, this paper claims two major contributions. 1) By designing an unsupervised Bag-of-Words representation, we are devoted to bridging the gap between the two tasks by integrating techniques from image search in person re-identification. We show that our system sets up an effective yet efficient baseline that is amenable to further supervised/unsupervised improvements. 2) We contribute a new high quality dataset which uses DPM detector and includes a number of distractor images. Our dataset reaches closer to realistic settings, and new perspectives are provided. Compared with approaches that rely on feature-feature match, our method is faster by over two orders of magnitude. Moreover, on three datasets, we report competitive results compared with the state-of-the-art methods. version:1
arxiv-1502-02160 | A Survey on Hough Transform, Theory, Techniques and Applications | http://arxiv.org/abs/1502.02160 | id:1502.02160 author:Allam Shehata Hassanein, Sherien Mohammad, Mohamed Sameer, Mohammad Ehab Ragab category:cs.CV  published:2015-02-07 summary:For more than half a century, the Hough transform is ever-expanding for new frontiers. Thousands of research papers and numerous applications have evolved over the decades. Carrying out an all-inclusive survey is hardly possible and enormously space-demanding. What we care about here is emphasizing some of the most crucial milestones of the transform. We describe its variations elaborating on the basic ones such as the line and circle Hough transforms. The high demand for storage and computation time is clarified with different solution approaches. Since most uses of the transform take place on binary images, we have been concerned with the work done directly on gray or color images. The myriad applications of the standard transform and its variations have been classified highlighting the up-to-date and the unconventional ones. Due to its merits such as noise-immunity and expandability, the transform has an excellent history, and a bright future as well. version:1
arxiv-1502-02158 | Learning Parametric-Output HMMs with Two Aliased States | http://arxiv.org/abs/1502.02158 | id:1502.02158 author:Roi Weiss, Boaz Nadler category:cs.LG  published:2015-02-07 summary:In various applications involving hidden Markov models (HMMs), some of the hidden states are aliased, having identical output distributions. The minimality, identifiability and learnability of such aliased HMMs have been long standing problems, with only partial solutions provided thus far. In this paper we focus on parametric-output HMMs, whose output distributions come from a parametric family, and that have exactly two aliased states. For this class, we present a complete characterization of their minimality and identifiability. Furthermore, for a large family of parametric output distributions, we derive computationally efficient and statistically consistent algorithms to detect the presence of aliasing and learn the aliased HMM transition and emission parameters. We illustrate our theoretical analysis by several simulations. version:1
arxiv-1409-5937 | Distributed Robust Learning | http://arxiv.org/abs/1409.5937 | id:1409.5937 author:Jiashi Feng, Huan Xu, Shie Mannor category:stat.ML cs.LG  published:2014-09-21 summary:We propose a framework for distributed robust statistical learning on {\em big contaminated data}. The Distributed Robust Learning (DRL) framework can reduce the computational time of traditional robust learning methods by several orders of magnitude. We analyze the robustness property of DRL, showing that DRL not only preserves the robustness of the base robust learning method, but also tolerates contaminations on a constant fraction of results from computing nodes (node failures). More precisely, even in presence of the most adversarial outlier distribution over computing nodes, DRL still achieves a breakdown point of at least $ \lambda^*/2 $, where $ \lambda^* $ is the break down point of corresponding centralized algorithm. This is in stark contrast with naive division-and-averaging implementation, which may reduce the breakdown point by a factor of $ k $ when $ k $ computing nodes are used. We then specialize the DRL framework for two concrete cases: distributed robust principal component analysis and distributed robust regression. We demonstrate the efficiency and the robustness advantages of DRL through comprehensive simulations and predicting image tags on a large-scale image set. version:2
arxiv-1502-01418 | RELEAF: An Algorithm for Learning and Exploiting Relevance | http://arxiv.org/abs/1502.01418 | id:1502.01418 author:Cem Tekin, Mihaela van der Schaar category:cs.LG stat.ML  published:2015-02-05 summary:Recommender systems, medical diagnosis, network security, etc., require on-going learning and decision-making in real time. These -- and many others -- represent perfect examples of the opportunities and difficulties presented by Big Data: the available information often arrives from a variety of sources and has diverse features so that learning from all the sources may be valuable but integrating what is learned is subject to the curse of dimensionality. This paper develops and analyzes algorithms that allow efficient learning and decision-making while avoiding the curse of dimensionality. We formalize the information available to the learner/decision-maker at a particular time as a context vector which the learner should consider when taking actions. In general the context vector is very high dimensional, but in many settings, the most relevant information is embedded into only a few relevant dimensions. If these relevant dimensions were known in advance, the problem would be simple -- but they are not. Moreover, the relevant dimensions may be different for different actions. Our algorithm learns the relevant dimensions for each action, and makes decisions based in what it has learned. Formally, we build on the structure of a contextual multi-armed bandit by adding and exploiting a relevance relation. We prove a general regret bound for our algorithm whose time order depends only on the maximum number of relevant dimensions among all the actions, which in the special case where the relevance relation is single-valued (a function), reduces to $\tilde{O}(T^{2(\sqrt{2}-1)})$; in the absence of a relevance relation, the best known contextual bandit algorithms achieve regret $\tilde{O}(T^{(D+1)/(D+2)})$, where $D$ is the full dimension of the context vector. version:2
arxiv-1502-02092 | Reflectance Hashing for Material Recognition | http://arxiv.org/abs/1502.02092 | id:1502.02092 author:Hang Zhang, Kristin Dana, Ko Nishino category:cs.CV  published:2015-02-07 summary:We introduce a novel method for using reflectance to identify materials. Reflectance offers a unique signature of the material but is challenging to measure and use for recognizing materials due to its high-dimensionality. In this work, one-shot reflectance is captured using a unique optical camera measuring {\it reflectance disks} where the pixel coordinates correspond to surface viewing angles. The reflectance has class-specific stucture and angular gradients computed in this reflectance space reveal the material class. These reflectance disks encode discriminative information for efficient and accurate material recognition. We introduce a framework called reflectance hashing that models the reflectance disks with dictionary learning and binary hashing. We demonstrate the effectiveness of reflectance hashing for material recognition with a number of real-world materials. version:1
arxiv-1502-02089 | Discriminative training for Convolved Multiple-Output Gaussian processes | http://arxiv.org/abs/1502.02089 | id:1502.02089 author:Sebastián Gómez-González, Mauricio A. Álvarez, Hernán Felipe García category:stat.ML  published:2015-02-07 summary:Multi-output Gaussian processes (MOGP) are probability distributions over vector-valued functions, and have been previously used for multi-output regression and for multi-class classification. A less explored facet of the multi-output Gaussian process is that it can be used as a generative model for vector-valued random fields in the context of pattern recognition. As a generative model, the multi-output GP is able to handle vector-valued functions with continuous inputs, as opposed, for example, to hidden Markov models. It also offers the ability to model multivariate random functions with high dimensional inputs. In this report, we use a discriminative training criteria known as Minimum Classification Error to fit the parameters of a multi-output Gaussian process. We compare the performance of generative training and discriminative training of MOGP in emotion recognition, activity recognition, and face recognition. We also compare the proposed methodology against hidden Markov models trained in a generative and in a discriminative way. version:1
arxiv-1502-02072 | Massively Multitask Networks for Drug Discovery | http://arxiv.org/abs/1502.02072 | id:1502.02072 author:Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David Konerding, Vijay Pande category:stat.ML cs.LG cs.NE  published:2015-02-06 summary:Massively multitask neural architectures provide a learning framework for drug discovery that synthesizes information from many distinct biological sources. To train these architectures at scale, we gather large amounts of data from public sources to create a dataset of nearly 40 million measurements across more than 200 biological targets. We investigate several aspects of the multitask framework by performing a series of empirical studies and obtain some interesting results: (1) massively multitask networks obtain predictive accuracies significantly better than single-task methods, (2) the predictive power of multitask networks improves as additional tasks and data are added, (3) the total amount of data and the total number of tasks both contribute significantly to multitask improvement, and (4) multitask networks afford limited transferability to tasks not in the training set. Our results underscore the need for greater data sharing and further algorithmic innovation to accelerate the drug discovery process. version:1
arxiv-1501-03069 | Learning from Multiple Sources for Video Summarisation | http://arxiv.org/abs/1501.03069 | id:1501.03069 author:Xiatian Zhu, Chen Change Loy, Shaogang Gong category:cs.CV  published:2015-01-13 summary:Many visual surveillance tasks, e.g.video summarisation, is conventionally accomplished through analysing imagerybased features. Relying solely on visual cues for public surveillance video understanding is unreliable, since visual observations obtained from public space CCTV video data are often not sufficiently trustworthy and events of interest can be subtle. On the other hand, non-visual data sources such as weather reports and traffic sensory signals are readily accessible but are not explored jointly to complement visual data for video content analysis and summarisation. In this paper, we present a novel unsupervised framework to learn jointly from both visual and independently-drawn non-visual data sources for discovering meaningful latent structure of surveillance video data. In particular, we investigate ways to cope with discrepant dimension and representation whist associating these heterogeneous data sources, and derive effective mechanism to tolerate with missing and incomplete data from different sources. We show that the proposed multi-source learning framework not only achieves better video content clustering than state-of-the-art methods, but also is capable of accurately inferring missing non-visual semantics from previously unseen videos. In addition, a comprehensive user study is conducted to validate the quality of video summarisation generated using the proposed multi-source model. version:2
arxiv-1409-6080 | Temporally Coherent Bayesian Models for Entity Discovery in Videos by Tracklet Clustering | http://arxiv.org/abs/1409.6080 | id:1409.6080 author:Adway Mitra, Soma Biswas, Chiranjib Bhattacharyya category:cs.CV  published:2014-09-22 summary:A video can be represented as a sequence of tracklets, each spanning 10-20 frames, and associated with one entity (eg. a person). The task of \emph{Entity Discovery} in videos can be naturally posed as tracklet clustering. We approach this task by leveraging \emph{Temporal Coherence}(TC): the fundamental property of videos that each tracklet is likely to be associated with the same entity as its temporal neighbors. Our major contributions are the first Bayesian nonparametric models for TC at tracklet-level. We extend Chinese Restaurant Process (CRP) to propose TC-CRP, and further to Temporally Coherent Chinese Restaurant Franchise (TC-CRF) to jointly model short temporal segments. On the task of discovering persons in TV serial videos without meta-data like scripts, these methods show considerable improvement in cluster purity and person coverage compared to state-of-the-art approaches to tracklet clustering. We represent entities with mixture components, and tracklets with vectors of very generic features, which can work for any type of entity (not necessarily person). The proposed methods can perform online tracklet clustering on streaming videos with little performance deterioration unlike existing approaches, and can automatically reject tracklets resulting from false detections. Finally we discuss entity-driven video summarization- where some temporal segments of the video are selected automatically based on the discovered entities. version:2
arxiv-1502-01403 | Distributed Estimation of Generalized Matrix Rank: Efficient Algorithms and Lower Bounds | http://arxiv.org/abs/1502.01403 | id:1502.01403 author:Yuchen Zhang, Martin J. Wainwright, Michael I. Jordan category:cs.DS cs.CC stat.ML  published:2015-02-05 summary:We study the following generalized matrix rank estimation problem: given an $n \times n$ matrix and a constant $c \geq 0$, estimate the number of eigenvalues that are greater than $c$. In the distributed setting, the matrix of interest is the sum of $m$ matrices held by separate machines. We show that any deterministic algorithm solving this problem must communicate $\Omega(n^2)$ bits, which is order-equivalent to transmitting the whole matrix. In contrast, we propose a randomized algorithm that communicates only $\widetilde O(n)$ bits. The upper bound is matched by an $\Omega(n)$ lower bound on the randomized communication complexity. We demonstrate the practical effectiveness of the proposed algorithm with some numerical experiments. version:2
arxiv-1502-01943 | Active Function Cross-Entropy Clustering | http://arxiv.org/abs/1502.01943 | id:1502.01943 author:P. Spurek, J. Tabor, P. Markowicz category:stat.ML  published:2015-02-06 summary:Gaussian Mixture Models (GMM) have found many applications in density estimation and data clustering. However, the model does not adapt well to curved and strongly nonlinear data. Recently there appeared an improvement called AcaGMM (Active curve axis Gaussian Mixture Model), which fits Gaussians along curves using an EM-like (Expectation Maximization) approach. Using the ideas standing behind AcaGMM, we build an alternative active function model of clustering, which has some advantages over AcaGMM. In particular it is naturally defined in arbitrary dimensions and enables an easy adaptation to clustering of complicated datasets along the predefined family of functions. Moreover, it does not need external methods to determine the number of clusters as it automatically reduces the number of groups on-line. version:1
arxiv-1307-0803 | Data Fusion by Matrix Factorization | http://arxiv.org/abs/1307.0803 | id:1307.0803 author:Marinka Žitnik, Blaž Zupan category:cs.LG cs.AI cs.DB stat.ML  published:2013-07-02 summary:For most problems in science and engineering we can obtain data sets that describe the observed system from various perspectives and record the behavior of its individual components. Heterogeneous data sets can be collectively mined by data fusion. Fusion can focus on a specific target relation and exploit directly associated data together with contextual data and data about system's constraints. In the paper we describe a data fusion approach with penalized matrix tri-factorization (DFMF) that simultaneously factorizes data matrices to reveal hidden associations. The approach can directly consider any data that can be expressed in a matrix, including those from feature-based representations, ontologies, associations and networks. We demonstrate the utility of DFMF for gene function prediction task with eleven different data sources and for prediction of pharmacologic actions by fusing six data sources. Our data fusion algorithm compares favorably to alternative data integration approaches and achieves higher accuracy than can be obtained from any single data source alone. version:2
arxiv-1406-0167 | Feature Selection for Linear SVM with Provable Guarantees | http://arxiv.org/abs/1406.0167 | id:1406.0167 author:Saurabh Paul, Malik Magdon-Ismail, Petros Drineas category:stat.ML cs.LG  published:2014-06-01 summary:We give two provably accurate feature-selection techniques for the linear SVM. The algorithms run in deterministic and randomized time respectively. Our algorithms can be used in an unsupervised or supervised setting. The supervised approach is based on sampling features from support vectors. We prove that the margin in the feature space is preserved to within $\epsilon$-relative error of the margin in the full feature space in the worst-case. In the unsupervised setting, we also provide worst-case guarantees of the radius of the minimum enclosing ball, thereby ensuring comparable generalization as in the full feature space and resolving an open problem posed in Dasgupta et al. We present extensive experiments on real-world datasets to support our theory and to demonstrate that our method is competitive and often better than prior state-of-the-art, for which there are no known provable guarantees. version:3
arxiv-1301-1954 | On the Incommensurability Phenomenon | http://arxiv.org/abs/1301.1954 | id:1301.1954 author:Donniell E. Fishkind, Cencheng Shen, Youngser Park, Carey E. Priebe category:stat.ML  published:2013-01-09 summary:Suppose that two large, multi-dimensional data sets are each noisy measurements of the same underlying random process, and principle components analysis is performed separately on the data sets to reduce their dimensionality. In some circumstances it may happen that the two lower-dimensional data sets have an inordinately large Procrustean fitting-error between them. The purpose of this manuscript is to quantify this "incommensurability phenomenon." In particular, under specified conditions, the square Procrustean fitting-error of the two normalized lower-dimensional data sets is (asymptotically) a convex combination (via a correlation parameter) of the Hausdorff distance between the projection subspaces and the maximum possible value of the square Procrustean fitting-error for normalized data. We show how this gives rise to the incommensurability phenomenon, and we employ illustrative simulations as well as a real data experiment to explore how the incommensurability phenomenon may have an appreciable impact. version:5
arxiv-1502-01880 | A Fingerprint-based Access Control using Principal Component Analysis and Edge Detection | http://arxiv.org/abs/1502.01880 | id:1502.01880 author:E. F. Melo, H. M. de Oliveira category:cs.CV cs.CR stat.AP  published:2015-02-06 summary:This paper presents a novel approach for deciding on the appropriateness or not of an acquired fingerprint image into a given database. The process begins with the assembly of a training base in an image space constructed by combining Principal Component Analysis (PCA) and edge detection. Then, the parameter H, a new feature that helps in the decision making about the relevance of a fingerprint image in databases, is derived from a relationship between Euclidean and Mahalanobian distances. This procedure ends with the lifting of the curve of the Receiver Operating Characteristic (ROC), where the thresholds defined on the parameter H are chosen according to the acceptable rates of false positives and false negatives. version:1
arxiv-1502-01853 | Generalized Inpainting Method for Hyperspectral Image Acquisition | http://arxiv.org/abs/1502.01853 | id:1502.01853 author:K. Degraux, V. Cambareri, L. Jacques, B. Geelen, C. Blanch, G. Lafruit category:cs.CV  published:2015-02-06 summary:A recently designed hyperspectral imaging device enables multiplexed acquisition of an entire data volume in a single snapshot thanks to monolithically-integrated spectral filters. Such an agile imaging technique comes at the cost of a reduced spatial resolution and the need for a demosaicing procedure on its interleaved data. In this work, we address both issues and propose an approach inspired by recent developments in compressed sensing and analysis sparse models. We formulate our superresolution and demosaicing task as a 3-D generalized inpainting problem. Interestingly, the target spatial resolution can be adjusted for mitigating the compression level of our sensing. The reconstruction procedure uses a fast greedy method called Pseudo-inverse IHT. We also show on simulations that a random arrangement of the spectral filters on the sensor is preferable to regular mosaic layout as it improves the quality of the reconstruction. The efficiency of our technique is demonstrated through numerical experiments on both synthetic and real data as acquired by the snapshot imager. version:1
arxiv-1502-01852 | Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification | http://arxiv.org/abs/1502.01852 | id:1502.01852 author:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun category:cs.CV cs.AI cs.LG  published:2015-02-06 summary:Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge. version:1
arxiv-1502-01827 | Hierarchical Maximum-Margin Clustering | http://arxiv.org/abs/1502.01827 | id:1502.01827 author:Guang-Tong Zhou, Sung Ju Hwang, Mark Schmidt, Leonid Sigal, Greg Mori category:cs.LG cs.CV  published:2015-02-06 summary:We present a hierarchical maximum-margin clustering method for unsupervised data analysis. Our method extends beyond flat maximum-margin clustering, and performs clustering recursively in a top-down manner. We propose an effective greedy splitting criteria for selecting which cluster to split next, and employ regularizers that enforce feature sharing/competition for capturing data semantics. Experimental results obtained on four standard datasets show that our method outperforms flat and hierarchical clustering baselines, while forming clean and semantically meaningful cluster hierarchies. version:1
arxiv-1502-01823 | Unsupervised Fusion Weight Learning in Multiple Classifier Systems | http://arxiv.org/abs/1502.01823 | id:1502.01823 author:Anurag Kumar, Bhiksha Raj category:cs.LG cs.CV  published:2015-02-06 summary:In this paper we present an unsupervised method to learn the weights with which the scores of multiple classifiers must be combined in classifier fusion settings. We also introduce a novel metric for ranking instances based on an index which depends upon the rank of weighted scores of test points among the weighted scores of training points. We show that the optimized index can be used for computing measures such as average precision. Unlike most classifier fusion methods where a single weight is learned to weigh all examples our method learns instance-specific weights. The problem is formulated as learning the weight which maximizes a clarity index; subsequently the index itself and the learned weights both are used separately to rank all the test points. Our method gives an unsupervised method of optimizing performance on actual test data, unlike the well known stacking-based methods where optimization is done over a labeled training set. Moreover, we show that our method is tolerant to noisy classifiers and can be used for selecting N-best classifiers. version:1
arxiv-1502-01812 | Crowded Scene Analysis: A Survey | http://arxiv.org/abs/1502.01812 | id:1502.01812 author:Teng Li, Huan Chang, Meng Wang, Bingbing Ni, Richang Hong, Shuicheng Yan category:cs.CV 68-02  published:2015-02-06 summary:Automated scene analysis has been a topic of great interest in computer vision and cognitive science. Recently, with the growth of crowd phenomena in the real world, crowded scene analysis has attracted much attention. However, the visual occlusions and ambiguities in crowded scenes, as well as the complex behaviors and scene semantics, make the analysis a challenging task. In the past few years, an increasing number of works on crowded scene analysis have been reported, covering different aspects including crowd motion pattern learning, crowd behavior and activity analysis, and anomaly detection in crowds. This paper surveys the state-of-the-art techniques on this topic. We first provide the background knowledge and the available features related to crowded scenes. Then, existing models, popular algorithms, evaluation protocols, as well as system performance are provided corresponding to different aspects of crowded scene analysis. We also outline the available datasets for performance evaluation. Finally, some research problems and promising future directions are presented with discussions. version:1
arxiv-1502-01783 | Learning Efficient Anomaly Detectors from $K$-NN Graphs | http://arxiv.org/abs/1502.01783 | id:1502.01783 author:Jing Qian, Jonathan Root, Venkatesh Saligrama category:cs.LG stat.ML  published:2015-02-06 summary:We propose a non-parametric anomaly detection algorithm for high dimensional data. We score each datapoint by its average $K$-NN distance, and rank them accordingly. We then train limited complexity models to imitate these scores based on the max-margin learning-to-rank framework. A test-point is declared as an anomaly at $\alpha$-false alarm level if the predicted score is in the $\alpha$-percentile. The resulting anomaly detector is shown to be asymptotically optimal in that for any false alarm rate $\alpha$, its decision region converges to the $\alpha$-percentile minimum volume level set of the unknown underlying density. In addition, we test both the statistical performance and computational efficiency of our algorithm on a number of synthetic and real-data experiments. Our results demonstrate the superiority of our algorithm over existing $K$-NN based anomaly detection algorithms, with significant computational savings. version:1
arxiv-1502-01782 | Multi-Action Recognition via Stochastic Modelling of Optical Flow and Gradients | http://arxiv.org/abs/1502.01782 | id:1502.01782 author:Johanna Carvajal, Conrad Sanderson, Chris McCool, Brian C. Lovell category:cs.CV  published:2015-02-06 summary:In this paper we propose a novel approach to multi-action recognition that performs joint segmentation and classification. This approach models each action using a Gaussian mixture using robust low-dimensional action features. Segmentation is achieved by performing classification on overlapping temporal windows, which are then merged to produce the final result. This approach is considerably less complicated than previous methods which use dynamic programming or computationally expensive hidden Markov models (HMMs). Initial experiments on a stitched version of the KTH dataset show that the proposed approach achieves an accuracy of 78.3%, outperforming a recent HMM-based approach which obtained 71.2%. version:1
arxiv-1404-7048 | Multiscale Event Detection in Social Media | http://arxiv.org/abs/1404.7048 | id:1404.7048 author:Xiaowen Dong, Dimitrios Mavroeidis, Francesco Calabrese, Pascal Frossard category:cs.SI cs.LG physics.soc-ph stat.ML  published:2014-04-25 summary:Event detection has been one of the most important research topics in social media analysis. Most of the traditional approaches detect events based on fixed temporal and spatial resolutions, while in reality events of different scales usually occur simultaneously, namely, they span different intervals in time and space. In this paper, we propose a novel approach towards multiscale event detection using social media data, which takes into account different temporal and spatial scales of events in the data. Specifically, we explore the properties of the wavelet transform, which is a well-developed multiscale transform in signal processing, to enable automatic handling of the interaction between temporal and spatial scales. We then propose a novel algorithm to compute a data similarity graph at appropriate scales and detect events of different scales simultaneously by a single graph-based clustering process. Furthermore, we present spatiotemporal statistical analysis of the noisy information present in the data stream, which allows us to define a novel term-filtering procedure for the proposed event detection algorithm and helps us study its behavior using simulated noisy data. Experimental results on both synthetically generated data and real world data collected from Twitter demonstrate the meaningfulness and effectiveness of the proposed approach. Our framework further extends to numerous application domains that involve multiscale and multiresolution data analysis. version:2
arxiv-1502-01761 | A Framework for Symmetric Part Detection in Cluttered Scenes | http://arxiv.org/abs/1502.01761 | id:1502.01761 author:Tom Lee, Sanja Fidler, Alex Levinshtein, Cristian Sminchisescu, Sven Dickinson category:cs.CV  published:2015-02-05 summary:The role of symmetry in computer vision has waxed and waned in importance during the evolution of the field from its earliest days. At first figuring prominently in support of bottom-up indexing, it fell out of favor as shape gave way to appearance and recognition gave way to detection. With a strong prior in the form of a target object, the role of the weaker priors offered by perceptual grouping was greatly diminished. However, as the field returns to the problem of recognition from a large database, the bottom-up recovery of the parts that make up the objects in a cluttered scene is critical for their recognition. The medial axis community has long exploited the ubiquitous regularity of symmetry as a basis for the decomposition of a closed contour into medial parts. However, today's recognition systems are faced with cluttered scenes, and the assumption that a closed contour exists, i.e. that figure-ground segmentation has been solved, renders much of the medial axis community's work inapplicable. In this article, we review a computational framework, previously reported in Lee et al. (2013), Levinshtein et al. (2009, 2013), that bridges the representation power of the medial axis and the need to recover and group an object's parts in a cluttered scene. Our framework is rooted in the idea that a maximally inscribed disc, the building block of a medial axis, can be modeled as a compact superpixel in the image. We evaluate the method on images of cluttered scenes. version:1
arxiv-1502-01753 | Monitoring Term Drift Based on Semantic Consistency in an Evolving Vector Field | http://arxiv.org/abs/1502.01753 | id:1502.01753 author:Peter Wittek, Sándor Darányi, Efstratios Kontopoulos, Theodoros Moysiadis, Ioannis Kompatsiaris category:cs.CL cs.LG cs.NE stat.ML  published:2015-02-05 summary:Based on the Aristotelian concept of potentiality vs. actuality allowing for the study of energy and dynamics in language, we propose a field approach to lexical analysis. Falling back on the distributional hypothesis to statistically model word meaning, we used evolving fields as a metaphor to express time-dependent changes in a vector space model by a combination of random indexing and evolving self-organizing maps (ESOM). To monitor semantic drifts within the observation period, an experiment was carried out on the term space of a collection of 12.8 million Amazon book reviews. For evaluation, the semantic consistency of ESOM term clusters was compared with their respective neighbourhoods in WordNet, and contrasted with distances among term vectors by random indexing. We found that at 0.05 level of significance, the terms in the clusters showed a high level of semantic consistency. Tracking the drift of distributional patterns in the term space across time periods, we found that consistency decreased, but not at a statistically significant level. Our method is highly scalable, with interpretations in philosophy. version:1
arxiv-1205-2382 | Mesh Learning for Classifying Cognitive Processes | http://arxiv.org/abs/1205.2382 | id:1205.2382 author:Mete Ozay, Ilke Öztekin, Uygar Öztekin, Fatos T. Yarman Vural category:cs.NE cs.AI cs.CV stat.ML  published:2012-05-10 summary:A relatively recent advance in cognitive neuroscience has been multi-voxel pattern analysis (MVPA), which enables researchers to decode brain states and/or the type of information represented in the brain during a cognitive operation. MVPA methods utilize machine learning algorithms to distinguish among types of information or cognitive states represented in the brain, based on distributed patterns of neural activity. In the current investigation, we propose a new approach for representation of neural data for pattern analysis, namely a Mesh Learning Model. In this approach, at each time instant, a star mesh is formed around each voxel, such that the voxel corresponding to the center node is surrounded by its p-nearest neighbors. The arc weights of each mesh are estimated from the voxel intensity values by least squares method. The estimated arc weights of all the meshes, called Mesh Arc Descriptors (MADs), are then used to train a classifier, such as Neural Networks, k-Nearest Neighbor, Na\"ive Bayes and Support Vector Machines. The proposed Mesh Model was tested on neuroimaging data acquired via functional magnetic resonance imaging (fMRI) during a recognition memory experiment using categorized word lists, employing a previously established experimental paradigm (\"Oztekin & Badre, 2011). Results suggest that the proposed Mesh Learning approach can provide an effective algorithm for pattern analysis of brain activity during cognitive processing. version:3
arxiv-1502-01733 | Arrhythmia Detection using Mutual Information-Based Integration Method | http://arxiv.org/abs/1502.01733 | id:1502.01733 author:Othman Soufan, Samer Arafat category:cs.CE cs.LG  published:2015-02-05 summary:The aim of this paper is to propose an application of mutual information-based ensemble methods to the analysis and classification of heart beats associated with different types of Arrhythmia. Models of multilayer perceptrons, support vector machines, and radial basis function neural networks were trained and tested using the MIT-BIH arrhythmia database. This research brings a focus to an ensemble method that, to our knowledge, is a novel application in the area of ECG Arrhythmia detection. The proposed classifier ensemble method showed improved performance, relative to either majority voting classifier integration or to individual classifier performance. The overall ensemble accuracy was 98.25%. version:1
arxiv-1410-6973 | Differentially- and non-differentially-private random decision trees | http://arxiv.org/abs/1410.6973 | id:1410.6973 author:Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Yann LeCun category:cs.LG  published:2014-10-26 summary:We consider supervised learning with random decision trees, where the tree construction is completely random. The method is popularly used and works well in practice despite the simplicity of the setting, but its statistical mechanism is not yet well-understood. In this paper we provide strong theoretical guarantees regarding learning with random decision trees. We analyze and compare three different variants of the algorithm that have minimal memory requirements: majority voting, threshold averaging and probabilistic averaging. The random structure of the tree enables us to adapt these methods to a differentially-private setting thus we also propose differentially-private versions of all three schemes. We give upper-bounds on the generalization error and mathematically explain how the accuracy depends on the number of random decision trees. Furthermore, we prove that only logarithmic (in the size of the dataset) number of independently selected random decision trees suffice to correctly classify most of the data, even when differential-privacy guarantees must be maintained. We empirically show that majority voting and threshold averaging give the best accuracy, also for conservative users requiring high privacy guarantees. Furthermore, we demonstrate that a simple majority voting rule is an especially good candidate for the differentially-private classifier since it is much less sensitive to the choice of forest parameters than other methods. version:2
arxiv-1502-01705 | A Confident Information First Principle for Parametric Reduction and Model Selection of Boltzmann Machines | http://arxiv.org/abs/1502.01705 | id:1502.01705 author:Xiaozhao Zhao, Yuexian Hou, Dawei Song, Wenjie Li category:cs.LG stat.ML  published:2015-02-05 summary:Typical dimensionality reduction (DR) methods are often data-oriented, focusing on directly reducing the number of random variables (features) while retaining the maximal variations in the high-dimensional data. In unsupervised situations, one of the main limitations of these methods lies in their dependency on the scale of data features. This paper aims to address the problem from a new perspective and considers model-oriented dimensionality reduction in parameter spaces of binary multivariate distributions. Specifically, we propose a general parameter reduction criterion, called Confident-Information-First (CIF) principle, to maximally preserve confident parameters and rule out less confident parameters. Formally, the confidence of each parameter can be assessed by its contribution to the expected Fisher information distance within the geometric manifold over the neighbourhood of the underlying real distribution. We then revisit Boltzmann machines (BM) from a model selection perspective and theoretically show that both the fully visible BM (VBM) and the BM with hidden units can be derived from the general binary multivariate distribution using the CIF principle. This can help us uncover and formalize the essential parts of the target density that BM aims to capture and the non-essential parts that BM should discard. Guided by the theoretical analysis, we develop a sample-specific CIF for model selection of BM that is adaptive to the observed samples. The method is studied in a series of density estimation experiments and has been shown effective in terms of the estimate accuracy. version:1
arxiv-1502-01687 | Graph Partitioning for Independent Sets | http://arxiv.org/abs/1502.01687 | id:1502.01687 author:Sebastian Lamm, Peter Sanders, Christian Schulz category:cs.DS cs.NE  published:2015-02-05 summary:Computing maximum independent sets in graphs is an important problem in computer science. In this paper, we develop an evolutionary algorithm to tackle the problem. The core innovations of the algorithm are very natural combine operations based on graph partitioning and local search algorithms. More precisely, we employ a state-of-the-art graph partitioner to derive operations that enable us to quickly exchange whole blocks of given independent sets. To enhance newly computed offsprings we combine our operators with a local search algorithm. Our experimental evaluation indicates that we are able to outperform state-of-the-art algorithms on a variety of instances. version:1
arxiv-1502-01684 | On Anomaly Ranking and Excess-Mass Curves | http://arxiv.org/abs/1502.01684 | id:1502.01684 author:Nicolas Goix, Anne Sabourin, Stéphan Clémençon category:stat.ML math.PR  published:2015-02-05 summary:Learning how to rank multivariate unlabeled observations depending on their degree of abnormality/novelty is a crucial problem in a wide range of applications. In practice, it generally consists in building a real valued "scoring" function on the feature space so as to quantify to which extent observations should be considered as abnormal. In the 1-d situation, measurements are generally considered as "abnormal" when they are remote from central measures such as the mean or the median. Anomaly detection then relies on tail analysis of the variable of interest. Extensions to the multivariate setting are far from straightforward and it is precisely the main purpose of this paper to introduce a novel and convenient (functional) criterion for measuring the performance of a scoring function regarding the anomaly ranking task, referred to as the Excess-Mass curve (EM curve). In addition, an adaptive algorithm for building a scoring function based on unlabeled data X1 , . . . , Xn with a nearly optimal EM is proposed and is analyzed from a statistical perspective. version:1
arxiv-1502-01682 | Use of Modality and Negation in Semantically-Informed Syntactic MT | http://arxiv.org/abs/1502.01682 | id:1502.01682 author:Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Chris Callison-Burch, Nathaniel W. Filardo, Christine Piatko, Lori Levin, Scott Miller category:cs.CL cs.LG stat.ML  published:2015-02-05 summary:This paper describes the resource- and system-building efforts of an eight-week Johns Hopkins University Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically-Informed Machine Translation (SIMT). We describe a new modality/negation (MN) annotation scheme, the creation of a (publicly available) MN lexicon, and two automated MN taggers that we built using the annotation scheme and lexicon. Our annotation scheme isolates three components of modality and negation: a trigger (a word that conveys modality or negation), a target (an action associated with modality or negation) and a holder (an experiencer of modality). We describe how our MN lexicon was semi-automatically produced and we demonstrate that a structure-based MN tagger results in precision around 86% (depending on genre) for tagging of a standard LDC data set. We apply our MN annotation scheme to statistical machine translation using a syntactic framework that supports the inclusion of semantic annotations. Syntactic tags enriched with semantic annotations are assigned to parse trees in the target-language training texts through a process of tree grafting. While the focus of our work is modality and negation, the tree grafting procedure is general and supports other types of semantic information. We exploit this capability by including named entities, produced by a pre-existing tagger, in addition to the MN elements produced by the taggers described in this paper. The resulting system significantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English test set. This finding supports the hypothesis that both syntactic and semantic information can improve translation quality. version:1
arxiv-1502-01664 | Estimating Optimal Active Learning via Model Retraining Improvement | http://arxiv.org/abs/1502.01664 | id:1502.01664 author:Lewis P. G. Evans, Niall M. Adams, Christoforos Anagnostopoulos category:stat.ML cs.LG  published:2015-02-05 summary:A central question for active learning (AL) is: "what is the optimal selection?" Defining optimality by classifier loss produces a new characterisation of optimal AL behaviour, by treating expected loss reduction as a statistical target for estimation. This target forms the basis of model retraining improvement (MRI), a novel approach providing a statistical estimation framework for AL. This framework is constructed to address the central question of AL optimality, and to motivate the design of estimation algorithms. MRI allows the exploration of optimal AL behaviour, and the examination of AL heuristics, showing precisely how they make sub-optimal selections. The abstract formulation of MRI is used to provide a new guarantee for AL, that an unbiased MRI estimator should outperform random selection. This MRI framework reveals intricate estimation issues that in turn motivate the construction of new statistical AL algorithms. One new algorithm in particular performs strongly in a large-scale experimental study, compared to standard AL methods. This competitive performance suggests that practical efforts to minimise estimation bias may be important for AL applications. version:1
arxiv-1502-01659 | Learning Articulated Motions From Visual Demonstration | http://arxiv.org/abs/1502.01659 | id:1502.01659 author:Sudeep Pillai, Matthew R. Walter, Seth Teller category:cs.RO cs.CV  published:2015-02-05 summary:Many functional elements of human homes and workplaces consist of rigid components which are connected through one or more sliding or rotating linkages. Examples include doors and drawers of cabinets and appliances; laptops; and swivel office chairs. A robotic mobile manipulator would benefit from the ability to acquire kinematic models of such objects from observation. This paper describes a method by which a robot can acquire an object model by capturing depth imagery of the object as a human moves it through its range of motion. We envision that in future, a machine newly introduced to an environment could be shown by its human user the articulated objects particular to that environment, inferring from these "visual demonstrations" enough information to actuate each object independently of the user. Our method employs sparse (markerless) feature tracking, motion segmentation, component pose estimation, and articulation learning; it does not require prior object models. Using the method, a robot can observe an object being exercised, infer a kinematic model incorporating rigid, prismatic and revolute joints, then use the model to predict the object's motion from a novel vantage point. We evaluate the method's performance, and compare it to that of a previously published technique, for a variety of household objects. version:1
arxiv-1502-01643 | Performance Analysis of Cone Detection Algorithms | http://arxiv.org/abs/1502.01643 | id:1502.01643 author:Letizia Mariotti, Nicholas Devaney category:physics.med-ph cs.CV  published:2015-02-05 summary:Many algorithms have been proposed to help clinicians evaluate cone density and spacing, as these may be related to the onset of retinal diseases. However, there has been no rigorous comparison of the performance of these algorithms. In addition, the performance of such algorithms is typically determined by comparison with human observers. Here we propose a technique to simulate realistic images of the cone mosaic. We use the simulated images to test the performance of two popular cone detection algorithms and we introduce an algorithm which is used by astronomers to detect stars in astronomical images. We use Free Response Operating Characteristic (FROC) curves to evaluate and compare the performance of the three algorithms. This allows us to optimize the performance of each algorithm. We observe that performance is significantly enhanced by up-sampling the images. We investigate the effect of noise and image quality on cone mosaic parameters estimated using the different algorithms, finding that the estimated regularity is the most sensitive parameter. This paper was published in JOSA A and is made available as an electronic reprint with the permission of OSA. The paper can be found at the following URL on the OSA website: http://www.opticsinfobase.org/abstract.cfm?msid=224577. Systematic or multiple reproduction or distribution to multiple locations via electronic or other means is prohibited and is subject to penalties under law. version:1
arxiv-1502-01632 | A Simple Expression for Mill's Ratio of the Student's $t$-Distribution | http://arxiv.org/abs/1502.01632 | id:1502.01632 author:Francesco Orabona category:cs.LG math.PR  published:2015-02-05 summary:I show a simple expression of the Mill's ratio of the Student's t-Distribution. I use it to prove Conjecture 1 in P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Mach. Learn., 47(2-3):235--256, May 2002. version:1
arxiv-1502-01563 | A PARTAN-Accelerated Frank-Wolfe Algorithm for Large-Scale SVM Classification | http://arxiv.org/abs/1502.01563 | id:1502.01563 author:Emanuele Frandi, Ricardo Nanculef, Johan A. K. Suykens category:stat.ML cs.LG math.OC  published:2015-02-05 summary:Frank-Wolfe algorithms have recently regained the attention of the Machine Learning community. Their solid theoretical properties and sparsity guarantees make them a suitable choice for a wide range of problems in this field. In addition, several variants of the basic procedure exist that improve its theoretical properties and practical performance. In this paper, we investigate the application of some of these techniques to Machine Learning, focusing in particular on a Parallel Tangent (PARTAN) variant of the FW algorithm that has not been previously suggested or studied for this type of problems. We provide experiments both in a standard setting and using a stochastic speed-up technique, showing that the considered algorithms obtain promising results on several medium and large-scale benchmark datasets for SVM classification. version:1
arxiv-1502-01540 | Semantic Embedding Space for Zero-Shot Action Recognition | http://arxiv.org/abs/1502.01540 | id:1502.01540 author:Xun Xu, Timothy Hospedales, Shaogang Gong category:cs.CV  published:2015-02-05 summary:The number of categories for action recognition is growing rapidly. It is thus becoming increasingly hard to collect sufficient training data to learn conventional models for each category. This issue may be ameliorated by the increasingly popular 'zero-shot learning' (ZSL) paradigm. In this framework a mapping is constructed between visual features and a human interpretable semantic description of each category, allowing categories to be recognised in the absence of any training data. Existing ZSL studies focus primarily on image data, and attribute-based semantic representations. In this paper, we address zero-shot recognition in contemporary video action recognition tasks, using semantic word vector space as the common space to embed videos and category labels. This is more challenging because the mapping between the semantic space and space-time features of videos containing complex actions is more complex and harder to learn. We demonstrate that a simple self-training and data augmentation strategy can significantly improve the efficacy of this mapping. Experiments on human action datasets including HMDB51 and UCF101 demonstrate that our approach achieves the state-of-the-art zero-shot action recognition performance. version:1
arxiv-1502-01526 | Object Proposal via Partial Re-ranking | http://arxiv.org/abs/1502.01526 | id:1502.01526 author:Jing Wang, Jie Shen category:cs.CV  published:2015-02-05 summary:Object proposals are an ensemble of bounding boxes with high potential to contain objects. Usually, the ranking models are utilized in order to provide a manageable number of candidate boxes. To obtain the rank for each candidate, prior ranking models generally compare each pair of candidates. However, one may be interested in only the top-$k$ candidates rather than all ones. Thus, in this paper, we propose a new ranking model for object proposals, which aims to produce a reliable estimation for only the top-$k$ candidates. To this end, we compute the IoU for each candidate and split the candidates into two subsets consisting of the top-$k$ candidates and the others respectively. Partial ranking constraints are imposed on the two subsets: any candidate from the first subset is better than that from the second one. In this way, the constraints are reduced dramatically compared to the full ranking model, which further facilitates an efficient learning procedure. Moreover, we show that our partial ranking model can be reduced into the large margin based framework. Extensive experiments demonstrate that after a re-ranking step of our model, the top-$k$ detection rate can be significantly improved. version:1
arxiv-1502-01241 | A specialized face-processing network consistent with the representational geometry of monkey face patches | http://arxiv.org/abs/1502.01241 | id:1502.01241 author:Amirhossein Farzmahdi, Karim Rajaei, Masoud Ghodrati, Reza Ebrahimpour, Seyed-Mahdi Khaligh-Razavi category:q-bio.NC cs.CV  published:2015-02-04 summary:Ample evidence suggests that face processing in human and non-human primates is performed differently compared with other objects. Converging reports, both physiologically and psychophysically, indicate that faces are processed in specialized neural networks in the brain -i.e. face patches in monkeys and the fusiform face area (FFA) in humans. We are all expert face-processing agents, and able to identify very subtle differences within the category of faces, despite substantial visual and featural similarities. Identification is performed rapidly and accurately after viewing a whole face, while significantly drops if some of the face configurations (e.g. inversion, misalignment) are manipulated or if partial views of faces are shown due to occlusion. This refers to a hotly-debated, yet highly-supported concept, known as holistic face processing. We built a hierarchical computational model of face-processing based on evidence from recent neuronal and behavioural studies on faces processing in primates. Representational geometries of the last three layers of the model have characteristics similar to those observed in monkey face patches (posterior, middle and anterior patches). Furthermore, several face-processing-related phenomena reported in the literature automatically emerge as properties of this model. The representations are evolved through several computational layers, using biologically plausible learning rules. The model satisfies face inversion effect, composite face effect, other race effect, view and identity selectivity, and canonical face views. To our knowledge, no models have so far been proposed with this performance and agreement with biological data. version:2
arxiv-1502-01493 | A mixture Cox-Logistic model for feature selection from survival and classification data | http://arxiv.org/abs/1502.01493 | id:1502.01493 author:Samuel Branders, Roberto D'Ambrosio, Pierre Dupont category:stat.ML cs.LG stat.ME  published:2015-02-05 summary:This paper presents an original approach for jointly fitting survival times and classifying samples into subgroups. The Coxlogit model is a generalized linear model with a common set of selected features for both tasks. Survival times and class labels are here assumed to be conditioned by a common risk score which depends on those features. Learning is then naturally expressed as maximizing the joint probability of subgroup labels and the ordering of survival events, conditioned to a common weight vector. The model is estimated by minimizing a regularized log-likelihood through a coordinate descent algorithm. Validation on synthetic and breast cancer data shows that the proposed approach outperforms a standard Cox model or logistic regression when both predicting the survival times and classifying new samples into subgroups. It is also better at selecting informative features for both tasks. version:1
arxiv-1502-01480 | Ring artifacts correction in compressed sensing tomographic reconstruction | http://arxiv.org/abs/1502.01480 | id:1502.01480 author:Pierre Paleo, Alessandro Mirone category:physics.comp-ph cs.CV  published:2015-02-05 summary:We present a novel approach to handle ring artifacts correction in compressed sensing tomographic reconstruction. The correction is part of the reconstruction process, which differs from classical sinogram pre-processing and image post-processing techniques. The principle of compressed sensing tomographic reconstruction is presented. Then, we show that the ring artifacts correction can be integrated in the reconstruction problem formalism. We provide numerical results for both simulated and real data. This technique is included in the PyHST2 code which is used at the European Synchrotron Radiation Facility for tomographic reconstruction. version:1
arxiv-1502-01475 | Fast Constraint Propagation for Image Segmentation | http://arxiv.org/abs/1502.01475 | id:1502.01475 author:Peng Han category:cs.CV  published:2015-02-05 summary:This paper presents a novel selective constraint propagation method for constrained image segmentation. In the literature, many pairwise constraint propagation methods have been developed to exploit pairwise constraints for cluster analysis. However, since most of these methods have a polynomial time complexity, they are not much suitable for segmentation of images even with a moderate size, which is actually equivalent to cluster analysis with a large data size. Considering the local homogeneousness of a natural image, we choose to perform pairwise constraint propagation only over a selected subset of pixels, but not over the whole image. Such a selective constraint propagation problem is then solved by an efficient graph-based learning algorithm. To further speed up our selective constraint propagation, we also discard those less important propagated constraints during graph-based learning. Finally, the selectively propagated constraints are exploited based on $L_1$-minimization for normalized cuts over the whole image. The experimental results demonstrate the promising performance of the proposed method for segmentation with selectively propagated constraints. version:1
arxiv-1410-7171 | Exponentiated Subgradient Algorithm for Online Optimization under the Random Permutation Model | http://arxiv.org/abs/1410.7171 | id:1410.7171 author:Reza Eghbali, Jon Swenson, Maryam Fazel category:math.OC cs.DS cs.LG  published:2014-10-27 summary:Online optimization problems arise in many resource allocation tasks, where the future demands for each resource and the associated utility functions change over time and are not known apriori, yet resources need to be allocated at every point in time despite the future uncertainty. In this paper, we consider online optimization problems with general concave utilities. We modify and extend an online optimization algorithm proposed by Devanur et al. for linear programming to this general setting. The model we use for the arrival of the utilities and demands is known as the random permutation model, where a fixed collection of utilities and demands are presented to the algorithm in random order. We prove that under this model the algorithm achieves a competitive ratio of $1-O(\epsilon)$ under a near-optimal assumption that the bid to budget ratio is $O (\frac{\epsilon^2}{\log({m}/{\epsilon})})$, where $m$ is the number of resources, while enjoying a significantly lower computational cost than the optimal algorithm proposed by Kesselheim et al. We draw a connection between the proposed algorithm and subgradient methods used in convex optimization. In addition, we present numerical experiments that demonstrate the performance and speed of this algorithm in comparison to existing algorithms. version:2
arxiv-1502-01446 | Beyond Word-based Language Model in Statistical Machine Translation | http://arxiv.org/abs/1502.01446 | id:1502.01446 author:Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, Chengqing Zong category:cs.CL  published:2015-02-05 summary:Language model is one of the most important modules in statistical machine translation and currently the word-based language model dominants this community. However, many translation models (e.g. phrase-based models) generate the target language sentences by rendering and compositing the phrases rather than the words. Thus, it is much more reasonable to model dependency between phrases, but few research work succeed in solving this problem. In this paper, we tackle this problem by designing a novel phrase-based language model which attempts to solve three key sub-problems: 1, how to define a phrase in language model; 2, how to determine the phrase boundary in the large-scale monolingual data in order to enlarge the training set; 3, how to alleviate the data sparsity problem due to the huge vocabulary size of phrases. By carefully handling these issues, the extensive experiments on Chinese-to-English translation show that our phrase-based language model can significantly improve the translation quality by up to +1.47 absolute BLEU score. version:1
arxiv-1412-0180 | Empirical Q-Value Iteration | http://arxiv.org/abs/1412.0180 | id:1412.0180 author:Dileep Kalathil, Vivek S. Borkar, Rahul Jain category:math.OC cs.LG  published:2014-11-30 summary:We propose a new simple and natural algorithm for learning the optimal $Q$-value function of a discounted-cost Markov Decision Process (MDP) when the transition kernels are unknown. Unlike the classical learning algorithms for MDPs, such as $Q$-learning and `actor-critic' algorithms, this algorithm doesn't depend on a stochastic approximation-based method. We show that our algorithm, which we call the empirical $Q$-value iteration (EQVI) algorithm, converges almost surely to the optimal $Q$-value function. To the best of our knowledge, this is the first algorithm for learning in MDPs that guarantees an almost sure convergence without using stochastic approximations. We also give a rate of convergence or a non-aymptotic sample complexity bound, and also show that an asynchronous (or online) version of the algorithm will also work. Preliminary experimental results suggest a faster rate of convergence to a ball park estimate for our algorithm compared to stochastic approximation-based algorithms. In fact, the asynchronous setting EQVI vastly outperforms the popular and widely-used Q-learning algorithm. version:2
arxiv-1501-01697 | Super-resolution MRI Using Finite Rate of Innovation Curves | http://arxiv.org/abs/1501.01697 | id:1501.01697 author:Greg Ongie, Mathews Jacob category:cs.CV  published:2015-01-08 summary:We propose a two-stage algorithm for the super-resolution of MR images from their low-frequency k-space samples. In the first stage we estimate a resolution-independent mask whose zeros represent the edges of the image. This builds off recent work extending the theory of sampling signals of finite rate of innovation (FRI) to two-dimensional curves. We enable its application to MRI by proposing extensions of the signal models allowed by FRI theory, and by developing a more robust and efficient means to determine the edge mask. In the second stage of the scheme, we recover the super-resolved MR image using the discretized edge mask as an image prior. We evaluate our scheme on simulated single-coil MR data obtained from analytical phantoms, and compare against total variation reconstructions. Our experiments show improved performance in both noiseless and noisy settings. version:2
arxiv-1502-00831 | Open System Categorical Quantum Semantics in Natural Language Processing | http://arxiv.org/abs/1502.00831 | id:1502.00831 author:Robin Piedeleu, Dimitri Kartsaklis, Bob Coecke, Mehrnoosh Sadrzadeh category:cs.CL cs.LO math.CT math.QA  published:2015-02-03 summary:Originally inspired by categorical quantum mechanics (Abramsky and Coecke, LiCS'04), the categorical compositional distributional model of natural language meaning of Coecke, Sadrzadeh and Clark provides a conceptually motivated procedure to compute the meaning of a sentence, given its grammatical structure within a Lambek pregroup and a vectorial representation of the meaning of its parts. The predictions of this first model have outperformed that of other models in mainstream empirical language processing tasks on large scale data. Moreover, just like CQM allows for varying the model in which we interpret quantum axioms, one can also vary the model in which we interpret word meaning. In this paper we show that further developments in categorical quantum mechanics are relevant to natural language processing too. Firstly, Selinger's CPM-construction allows for explicitly taking into account lexical ambiguity and distinguishing between the two inherently different notions of homonymy and polysemy. In terms of the model in which we interpret word meaning, this means a passage from the vector space model to density matrices. Despite this change of model, standard empirical methods for comparing meanings can be easily adopted, which we demonstrate by a small-scale experiment on real-world data. This experiment moreover provides preliminary evidence of the validity of our proposed new model for word meaning. Secondly, commutative classical structures as well as their non-commutative counterparts that arise in the image of the CPM-construction allow for encoding relative pronouns, verbs and adjectives, and finally, iteration of the CPM-construction, something that has no counterpart in the quantum realm, enables one to accommodate both entailment and ambiguity. version:2
arxiv-1404-1356 | Optimal learning with Bernstein Online Aggregation | http://arxiv.org/abs/1404.1356 | id:1404.1356 author:Olivier Wintenberger category:stat.ML cs.LG math.ST stat.TH  published:2014-04-04 summary:We introduce a new recursive aggregation procedure called Bernstein Online Aggregation (BOA). The exponential weights include an accuracy term and a second order term that is a proxy of the quadratic variation as in Hazan and Kale (2010). This second term stabilizes the procedure that is optimal in different senses. We first obtain optimal regret bounds in the deterministic context. Then, an adaptive version is the first exponential weights algorithm that exhibits a second order bound with excess losses that appears first in Gaillard et al. (2014). The second order bounds in the deterministic context are extended to a general stochastic context using the cumulative predictive risk. Such conversion provides the main result of the paper, an inequality of a novel type comparing the procedure with any deterministic aggregation procedure for an integrated criteria. Then we obtain an observable estimate of the excess of risk of the BOA procedure. To assert the optimality, we consider finally the iid case for strongly convex and Lipschitz continuous losses and we prove that the optimal rate of aggregation of Tsybakov (2003) is achieved. The batch version of the BOA procedure is then the first adaptive explicit algorithm that satisfies an optimal oracle inequality with high probability. version:3
arxiv-1502-01245 | Authorship recognition via fluctuation analysis of network topology and word intermittency | http://arxiv.org/abs/1502.01245 | id:1502.01245 author:Diego R. Amancio category:cs.CL  published:2015-02-04 summary:Statistical methods have been widely employed in many practical natural language processing applications. More specifically, complex networks concepts and methods from dynamical systems theory have been successfully applied to recognize stylistic patterns in written texts. Despite the large amount of studies devoted to represent texts with physical models, only a few studies have assessed the relevance of attributes derived from the analysis of stylistic fluctuations. Because fluctuations represent a pivotal factor for characterizing a myriad of real systems, this study focused on the analysis of the properties of stylistic fluctuations in texts via topological analysis of complex networks and intermittency measurements. The results showed that different authors display distinct fluctuation patterns. In particular, it was found that it is possible to identify the authorship of books using the intermittency of specific words. Taken together, the results described here suggest that the patterns found in stylistic fluctuations could be used to analyze other related complex systems. Furthermore, the discovery of novel patterns related to textual stylistic fluctuations indicates that these patterns could be useful to improve the state of the art of many stylistic-based natural language processing tasks. version:1
arxiv-1502-00254 | Freehand Sketch Recognition Using Deep Features | http://arxiv.org/abs/1502.00254 | id:1502.00254 author:Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu category:cs.CV  published:2015-02-01 summary:Freehand sketches often contain sparse visual detail. In spite of the sparsity, they are easily and consistently recognized by humans across cultures, languages and age groups. Therefore, analyzing such sparse sketches can aid our understanding of the neuro-cognitive processes involved in visual representation and recognition. In the recent past, Convolutional Neural Networks (CNNs) have emerged as a powerful framework for feature representation and recognition for a variety of image domains. However, the domain of sketch images has not been explored. This paper introduces a freehand sketch recognition framework based on "deep" features extracted from CNNs. We use two popular CNNs for our experiments -- Imagenet CNN and a modified version of LeNet CNN. We evaluate our recognition framework on a publicly available benchmark database containing thousands of freehand sketches depicting everyday objects. Our results are an improvement over the existing state-of-the-art accuracies by 3% - 11%. The effectiveness and relative compactness of our deep features also make them an ideal candidate for related problems such as sketch-based image retrieval. In addition, we provide a preliminary glimpse of how such features can help identify crucial attributes (e.g. object-parts) of the sketched objects. version:2
arxiv-1502-01176 | Learning Local Invariant Mahalanobis Distances | http://arxiv.org/abs/1502.01176 | id:1502.01176 author:Ethan Fetaya, Shimon Ullman category:cs.LG stat.ML  published:2015-02-04 summary:For many tasks and data types, there are natural transformations to which the data should be invariant or insensitive. For instance, in visual recognition, natural images should be insensitive to rotation and translation. This requirement and its implications have been important in many machine learning applications, and tolerance for image transformations was primarily achieved by using robust feature vectors. In this paper we propose a novel and computationally efficient way to learn a local Mahalanobis metric per datum, and show how we can learn a local invariant metric to any transformation in order to improve performance. version:1
arxiv-1501-06115 | Constrained Extreme Learning Machines: A Study on Classification Cases | http://arxiv.org/abs/1501.06115 | id:1501.06115 author:Wentao Zhu, Jun Miao, Laiyun Qing category:cs.LG cs.CV cs.NE  published:2015-01-25 summary:Extreme learning machine (ELM) is an extremely fast learning method and has a powerful performance for pattern recognition tasks proven by enormous researches and engineers. However, its good generalization ability is built on large numbers of hidden neurons, which is not beneficial to real time response in the test process. In this paper, we proposed new ways, named "constrained extreme learning machines" (CELMs), to randomly select hidden neurons based on sample distribution. Compared to completely random selection of hidden nodes in ELM, the CELMs randomly select hidden nodes from the constrained vector space containing some basic combinations of original sample vectors. The experimental results show that the CELMs have better generalization ability than traditional ELM, SVM and some other related methods. Additionally, the CELMs have a similar fast learning speed as ELM. version:2
arxiv-1502-01068 | Composite convex minimization involving self-concordant-like cost functions | http://arxiv.org/abs/1502.01068 | id:1502.01068 author:Quoc Tran-Dinh, Yen-Huan Li, Volkan Cevher category:math.OC stat.ML  published:2015-02-04 summary:The self-concordant-like property of a smooth convex function is a new analytical structure that generalizes the self-concordant notion. While a wide variety of important applications feature the self-concordant-like property, this concept has heretofore remained unexploited in convex optimization. To this end, we develop a variable metric framework of minimizing the sum of a "simple" convex function and a self-concordant-like function. We introduce a new analytic step-size selection procedure and prove that the basic gradient algorithm has improved convergence guarantees as compared to "fast" algorithms that rely on the Lipschitz gradient property. Our numerical tests with real-data sets shows that the practice indeed follows the theory. version:1
arxiv-1311-2495 | The Noisy Power Method: A Meta Algorithm with Applications | http://arxiv.org/abs/1311.2495 | id:1311.2495 author:Moritz Hardt, Eric Price category:cs.DS cs.LG  published:2013-11-11 summary:We provide a new robust convergence analysis of the well-known power method for computing the dominant singular vectors of a matrix that we call the noisy power method. Our result characterizes the convergence behavior of the algorithm when a significant amount noise is introduced after each matrix-vector multiplication. The noisy power method can be seen as a meta-algorithm that has recently found a number of important applications in a broad range of machine learning problems including alternating minimization for matrix completion, streaming principal component analysis (PCA), and privacy-preserving spectral analysis. Our general analysis subsumes several existing ad-hoc convergence bounds and resolves a number of open problems in multiple applications including streaming PCA and privacy-preserving singular vector computation. version:4
arxiv-1502-01057 | Personalized Web Search | http://arxiv.org/abs/1502.01057 | id:1502.01057 author:Li Zhou category:cs.IR cs.LG  published:2015-02-03 summary:Personalization is important for search engines to improve user experience. Most of the existing work do pure feature engineering and extract a lot of session-style features and then train a ranking model. Here we proposed a novel way to model both long term and short term user behavior using Multi-armed bandit algorithm. Our algorithm can generalize session information across users well, and as an Explore-Exploit style algorithm, it can generalize to new urls and new users well. Experiments show that our algorithm can improve performance over the default ranking and outperforms several popular Multi-armed bandit algorithms. version:1
arxiv-1502-01032 | DFDL: Discriminative Feature-oriented Dictionary Learning for Histopathological Image Classification | http://arxiv.org/abs/1502.01032 | id:1502.01032 author:Tiep H. Vu, Hojjat S. Mousavi, Vishal Monga, UK Arvind Rao, Ganesh Rao category:cs.CV  published:2015-02-03 summary:In histopathological image analysis, feature extraction for classification is a challenging task due to the diversity of histology features suitable for each problem as well as presence of rich geometrical structure. In this paper, we propose an automatic feature discovery framework for extracting discriminative class-specific features and present a low-complexity method for classification and disease grading in histopathology. Essentially, our Discriminative Feature-oriented Dictionary Learning (DFDL) method learns class-specific features which are suitable for representing samples from the same class while are poorly capable of representing samples from other classes. Experiments on three challenging real-world image databases: 1) histopathological images of intraductal breast lesions, 2) mammalian lung images provided by the Animal Diagnostics Lab (ADL) at Pennsylvania State University, and 3) brain tumor images from The Cancer Genome Atlas (TCGA) database, show the significance of DFDL model in a variety problems over state-of-the-art methods version:1
arxiv-1306-0514 | Riemannian metrics for neural networks II: recurrent networks and learning symbolic data sequences | http://arxiv.org/abs/1306.0514 | id:1306.0514 author:Yann Ollivier category:cs.NE cs.LG 68T05  68T10  published:2013-06-03 summary:Recurrent neural networks are powerful models for sequential data, able to represent complex dependencies in the sequence that simpler models such as hidden Markov models cannot handle. Yet they are notoriously hard to train. Here we introduce a training procedure using a gradient ascent in a Riemannian metric: this produces an algorithm independent from design choices such as the encoding of parameters and unit activities. This metric gradient ascent is designed to have an algorithmic cost close to backpropagation through time for sparsely connected networks. We use this procedure on gated leaky neural networks (GLNNs), a variant of recurrent neural networks with an architecture inspired by finite automata and an evolution equation inspired by continuous-time networks. GLNNs trained with a Riemannian gradient are demonstrated to effectively capture a variety of structures in synthetic problems: basic block nesting as in context-free grammars (an important feature of natural languages, but difficult to learn), intersections of multiple independent Markov-type relations, or long-distance relationships such as the distant-XOR problem. This method does not require adjusting the network structure or initial parameters: the network used is a sparse random graph and the initialization is identical for all problems considered. version:4
arxiv-1303-0818 | Riemannian metrics for neural networks I: feedforward networks | http://arxiv.org/abs/1303.0818 | id:1303.0818 author:Yann Ollivier category:cs.NE cs.IT cs.LG math.DG math.IT 68T05  published:2013-03-04 summary:We describe four algorithms for neural network training, each adapted to different scalability constraints. These algorithms are mathematically principled and invariant under a number of transformations in data and network representation, from which performance is thus independent. These algorithms are obtained from the setting of differential geometry, and are based on either the natural gradient using the Fisher information matrix, or on Hessian methods, scaled down in a specific way to allow for scalability while keeping some of their key mathematical properties. version:5
arxiv-1502-00946 | Classification of Hyperspectral Imagery on Embedded Grassmannians | http://arxiv.org/abs/1502.00946 | id:1502.00946 author:Sofya Chepushtanova, Michael Kirby category:cs.CV  published:2015-02-03 summary:We propose an approach for capturing the signal variability in hyperspectral imagery using the framework of the Grassmann manifold. Labeled points from each class are sampled and used to form abstract points on the Grassmannian. The resulting points on the Grassmannian have representations as orthonormal matrices and as such do not reside in Euclidean space in the usual sense. There are a variety of metrics which allow us to determine a distance matrices that can be used to realize the Grassmannian as an embedding in Euclidean space. We illustrate that we can achieve an approximately isometric embedding of the Grassmann manifold using the chordal metric while this is not the case with geodesic distances. However, non-isometric embeddings generated by using a pseudometric on the Grassmannian lead to the best classification results. We observe that as the dimension of the Grassmannian grows, the accuracy of the classification grows to 100% on two illustrative examples. We also observe a decrease in classification rates if the dimension of the points on the Grassmannian is too large for the dimension of the Euclidean space. We use sparse support vector machines to perform additional model reduction. The resulting classifier selects a subset of dimensions of the embedding without loss in classification performance. version:1
arxiv-1502-00916 | Learning Planar Ising Models | http://arxiv.org/abs/1502.00916 | id:1502.00916 author:Jason K. Johnson, Diane Oyen, Michael Chertkov, Praneeth Netrapalli category:stat.ML  published:2015-02-03 summary:Inference and learning of graphical models are both well-studied problems in statistics and machine learning that have found many applications in science and engineering. However, exact inference is intractable in general graphical models, which suggests the problem of seeking the best approximation to a collection of random variables within some tractable family of graphical models. In this paper, we focus on the class of planar Ising models, for which exact inference is tractable using techniques of statistical physics. Based on these techniques and recent methods for planarity testing and planar embedding, we propose a simple greedy algorithm for learning the best planar Ising model to approximate an arbitrary collection of binary random variables (possibly from sample data). Given the set of all pairwise correlations among variables, we select a planar graph and optimal planar Ising model defined on this graph to best approximate that set of correlations. We demonstrate our method in simulations and for the application of modeling senate voting records. version:1
arxiv-1502-00873 | DeepID3: Face Recognition with Very Deep Neural Networks | http://arxiv.org/abs/1502.00873 | id:1502.00873 author:Yi Sun, Ding Liang, Xiaogang Wang, Xiaoou Tang category:cs.CV  published:2015-02-03 summary:The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net and GoogLeNet to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53% LFW face verification accuracy and 96.0% LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end. version:1
arxiv-1404-2078 | Optimistic Risk Perception in the Temporal Difference error Explains the Relation between Risk-taking, Gambling, Sensation-seeking and Low Fear | http://arxiv.org/abs/1404.2078 | id:1404.2078 author:Joost Broekens, Tim Baarslag category:cs.LG q-bio.NC  published:2014-04-08 summary:Understanding the affective, cognitive and behavioural processes involved in risk taking is essential for treatment and for setting environmental conditions to limit damage. Using Temporal Difference Reinforcement Learning (TDRL) we computationally investigated the effect of optimism in risk perception in a variety of goal-oriented tasks. Optimism in risk perception was studied by varying the calculation of the Temporal Difference error, i.e., delta, in three ways: realistic (stochastically correct), optimistic (assuming action control), and overly optimistic (assuming outcome control). We show that for the gambling task individuals with 'healthy' perception of control, i.e., action optimism, do not develop gambling behaviour while individuals with 'unhealthy' perception of control, i.e., outcome optimism, do. We show that high intensity of sensations and low levels of fear co-occur due to optimistic risk perception. We found that overly optimistic risk perception (outcome optimism) results in risk taking and in persistent gambling behaviour in addition to high intensity of sensations. We discuss how our results replicate risk-taking related phenomena. version:2
arxiv-1502-00852 | Face frontalization for Alignment and Recognition | http://arxiv.org/abs/1502.00852 | id:1502.00852 author:Christos Sagonas, Yannis Panagakis, Stefanos Zafeiriou, Maja Pantic category:cs.CV  published:2015-02-03 summary:Recently, it was shown that excellent results can be achieved in both face landmark localization and pose-invariant face recognition. These breakthroughs are attributed to the efforts of the community to manually annotate facial images in many different poses and to collect 3D faces data. In this paper, we propose a novel method for joint face landmark localization and frontal face reconstruction (pose correction) using a small set of frontal images only. By observing that the frontal facial image is the one with the minimum rank from all different poses we formulate an appropriate model which is able to jointly recover the facial landmarks as well as the frontalized version of the face. To this end, a suitable optimization problem, involving the minimization of the nuclear norm and the matrix $\ell_1$ norm, is solved. The proposed method is assessed in frontal face reconstruction (pose correction), face landmark localization, and pose-invariant face recognition and verification by conducting experiments on $6$ facial images databases. The experimental results demonstrate the effectiveness of the proposed method. version:1
arxiv-1502-00839 | A multiset model of multi-species evolution to solve big deceptive problems | http://arxiv.org/abs/1502.00839 | id:1502.00839 author:Luis Correia, Antonio Manso category:cs.NE q-bio.PE  published:2015-02-03 summary:This chapter presents SMuGA, an integration of symbiogenesis with the Multiset Genetic Algorithm (MuGA). The symbiogenetic approach used here is based on the host-parasite model with the novelty of varying the length of parasites along the evolutionary process. Additionally, it models collaborations between multiple parasites and a single host. To improve efficiency, we introduced proxy evaluation of parasites, which saves fitness function calls and exponentially reduces the symbiotic collaborations produced. Another novel feature consists of breaking the evolutionary cycle into two phases: a symbiotic phase and a phase of independent evolution of both hosts and parasites. SMuGA was tested in optimization of a variety of deceptive functions, with results one order of magnitude better than state of the art symbiotic algorithms. This allowed to optimize deceptive problems with large sizes, and showed a linear scaling in the number of iterations to attain the optimum. version:1
arxiv-1502-00836 | Task-Driven Dictionary Learning for Hyperspectral Image Classification with Structured Sparsity Constraints | http://arxiv.org/abs/1502.00836 | id:1502.00836 author:Xiaoxia Sun, Nasser M. Nasrabadi, Trac D. Tran category:cs.CV  published:2015-02-03 summary:Sparse representation models a signal as a linear combination of a small number of dictionary atoms. As a generative model, it requires the dictionary to be highly redundant in order to ensure both a stable high sparsity level and a low reconstruction error for the signal. However, in practice, this requirement is usually impaired by the lack of labelled training samples. Fortunately, previous research has shown that the requirement for a redundant dictionary can be less rigorous if simultaneous sparse approximation is employed, which can be carried out by enforcing various structured sparsity constraints on the sparse codes of the neighboring pixels. In addition, numerous works have shown that applying a variety of dictionary learning methods for the sparse representation model can also improve the classification performance. In this paper, we highlight the task-driven dictionary learning algorithm, which is a general framework for the supervised dictionary learning method. We propose to enforce structured sparsity priors on the task-driven dictionary learning method in order to improve the performance of the hyperspectral classification. Our approach is able to benefit from both the advantages of the simultaneous sparse representation and those of the supervised dictionary learning. We enforce two different structured sparsity priors, the joint and Laplacian sparsity, on the task-driven dictionary learning method and provide the details of the corresponding optimization algorithms. Experiments on numerous popular hyperspectral images demonstrate that the classification performance of our approach is superior to sparse representation classifier with structured priors or the task-driven dictionary learning method. version:1
arxiv-1502-07666 | Landmark-Guided Elastic Shape Analysis of Human Character Motions | http://arxiv.org/abs/1502.07666 | id:1502.07666 author:Martin Bauer, Markus Eslitzbichler, Markus Grasmair category:cs.CV cs.GR 65D18  58D10  49Q10  published:2015-02-03 summary:Motions of virtual characters in movies or video games are typically generated by recording actors using motion capturing methods. Animations generated this way often need postprocessing, such as improving the periodicity of cyclic animations or generating entirely new motions by interpolation of existing ones. Furthermore, search and classification of recorded motions becomes more and more important as the amount of recorded motion data grows. In this paper, we will apply methods from shape analysis to the processing of animations. More precisely, we will use the by now classical elastic metric model used in shape matching, and extend it by incorporating additional inexact feature point information, which leads to an improved temporal alignment of different animations. version:1
arxiv-1410-2871 | An Ontology for Comprehensive Tutoring of Euphonic Conjunctions of Sanskrit Grammar | http://arxiv.org/abs/1410.2871 | id:1410.2871 author:S. V. Kasmir Raja, V. Rajitha, Meenakshi Lakshmanan category:cs.CL  published:2014-10-10 summary:Euphonic conjunctions (sandhis) form a very important aspect of Sanskrit morphology and phonology. The traditional and modern methods of studying about euphonic conjunctions in Sanskrit follow different methodologies. The former involves a rigorous study of the Paninian system embodied in Panini's Ashtadhyayi, while the latter usually involves the study of a few important sandhi rules with the use of examples. The former is not suitable for beginners, and the latter, not sufficient to gain a comprehensive understanding of the operation of sandhi rules. This is so since there are not only numerous sandhi rules and exceptions, but also complex precedence rules involved. The need for a new ontology for sandhi-tutoring was hence felt. This work presents a comprehensive ontology designed to enable a student-user to learn in stages all about euphonic conjunctions and the relevant aphorisms of Sanskrit grammar and to test and evaluate the progress of the student-user. The ontology forms the basis of a multimedia sandhi tutor that was given to different categories of users including Sanskrit scholars for extensive and rigorous testing. version:2
arxiv-1412-7185 | Parameter Selection In Particle Swarm Optimization For Transportation Network Design Problem | http://arxiv.org/abs/1412.7185 | id:1412.7185 author:Mehran Fasihozaman Langerudi category:math.OC cs.NE  published:2014-12-22 summary:In transportation planning and development, transport network design problem seeks to optimize specific objectives (e.g. total travel time) through choosing among a given set of projects while keeping consumption of resources (e.g. budget) within their limits. Due to the numerous cases of choosing projects, solving such a problem is very difficult and time-consuming. Based on particle swarm optimization (PSO) technique, a heuristic solution algorithm for the bi-level problem is designed. This paper evaluates the algorithm performance in the response of changing certain basic PSO parameters. version:3
arxiv-1502-00750 | Recognizing Focal Liver Lesions in Contrast-Enhanced Ultrasound with Discriminatively Trained Spatio-Temporal Model | http://arxiv.org/abs/1502.00750 | id:1502.00750 author:Xiaodan Liang, Qingxing Cao, Rui Huang, Liang Lin category:cs.CV 68U01  published:2015-02-03 summary:The aim of this study is to provide an automatic computational framework to assist clinicians in diagnosing Focal Liver Lesions (FLLs) in Contrast-Enhancement Ultrasound (CEUS). We represent FLLs in a CEUS video clip as an ensemble of Region-of-Interests (ROIs), whose locations are modeled as latent variables in a discriminative model. Different types of FLLs are characterized by both spatial and temporal enhancement patterns of the ROIs. The model is learned by iteratively inferring the optimal ROI locations and optimizing the model parameters. To efficiently search the optimal spatial and temporal locations of the ROIs, we propose a data-driven inference algorithm by combining effective spatial and temporal pruning. The experiments show that our method achieves promising results on the largest dataset in the literature (to the best of our knowledge), which we have made publicly available. version:1
arxiv-1502-00749 | Data-Driven Scene Understanding with Adaptively Retrieved Exemplars | http://arxiv.org/abs/1502.00749 | id:1502.00749 author:Xionghao Liu, Wei Yang, Liang Lin, Qing Wang, Zhaoquan Cai, Jianhuang Lai category:cs.CV 68U01  published:2015-02-03 summary:This article investigates a data-driven approach for semantically scene understanding, without pixelwise annotation and classifier training. Our framework parses a target image with two steps: (i) retrieving its exemplars (i.e. references) from an image database, where all images are unsegmented but annotated with tags; (ii) recovering its pixel labels by propagating semantics from the references. We present a novel framework making the two steps mutually conditional and bootstrapped under the probabilistic Expectation-Maximization (EM) formulation. In the first step, the references are selected by jointly matching their appearances with the target as well as the semantics (i.e. the assigned labels of the target and the references). We process the second step via a combinatorial graphical representation, in which the vertices are superpixels extracted from the target and its selected references. Then we derive the potentials of assigning labels to one vertex of the target, which depend upon the graph edges that connect the vertex to its spatial neighbors of the target and to its similar vertices of the references. Besides, the proposed framework can be naturally applied to perform image annotation on new test images. In the experiments, we validate our approach on two public databases, and demonstrate superior performances over the state-of-the-art methods in both semantic segmentation and image annotation tasks. version:1
arxiv-1502-00744 | Incorporating Structural Alternatives and Sharing into Hierarchy for Multiclass Object Recognition and Detection | http://arxiv.org/abs/1502.00744 | id:1502.00744 author:Xiaolong Wang, Liang Lin, Lichao Huang, Shuicheng Yan category:cs.CV 68U01  published:2015-02-03 summary:This paper proposes a reconfigurable model to recognize and detect multiclass (or multiview) objects with large variation in appearance. Compared with well acknowledged hierarchical models, we study two advanced capabilities in hierarchy for object modeling: (i) "switch" variables(i.e. or-nodes) for specifying alternative compositions, and (ii) making local classifiers (i.e. leaf-nodes) shared among different classes. These capabilities enable us to account well for structural variabilities while preserving the model compact. Our model, in the form of an And-Or Graph, comprises four layers: a batch of leaf-nodes with collaborative edges in bottom for localizing object parts; the or-nodes over bottom to activate their children leaf-nodes; the and-nodes to classify objects as a whole; one root-node on the top for switching multiclass classification, which is also an or-node. For model training, we present an EM-type algorithm, namely dynamical structural optimization (DSO), to iteratively determine the structural configuration, (e.g., leaf-node generation associated with their parent or-nodes and shared across other classes), along with optimizing multi-layer parameters. The proposed method is valid on challenging databases, e.g., PASCAL VOC 2007 and UIUC-People, and it achieves state-of-the-arts performance. version:1
arxiv-1502-00743 | Deep Joint Task Learning for Generic Object Extraction | http://arxiv.org/abs/1502.00743 | id:1502.00743 author:Xiaolong Wang, Liliang Zhang, Liang Lin, Zhujin Liang, Wangmeng Zuo category:cs.CV 68U01  published:2015-02-03 summary:This paper investigates how to extract objects-of-interest without relying on hand-craft features and sliding windows approaches, that aims to jointly solve two sub-tasks: (i) rapidly localizing salient objects from images, and (ii) accurately segmenting the objects based on the localizations. We present a general joint task learning framework, in which each task (either object localization or object segmentation) is tackled via a multi-layer convolutional neural network, and the two networks work collaboratively to boost performance. In particular, we propose to incorporate latent variables bridging the two networks in a joint optimization manner. The first network directly predicts the positions and scales of salient objects from raw images, and the latent variables adjust the object localizations to feed the second network that produces pixelwise object masks. An EM-type method is presented for the optimization, iterating with two steps: (i) by using the two networks, it estimates the latent variables by employing an MCMC-based sampling method; (ii) it optimizes the parameters of the two networks unitedly via back propagation, with the fixed latent variables. Extensive experiments suggest that our framework significantly outperforms other state-of-the-art approaches in both accuracy and efficiency (e.g. 1000 times faster than competing approaches). version:1
arxiv-1502-00741 | Dynamical And-Or Graph Learning for Object Shape Modeling and Detection | http://arxiv.org/abs/1502.00741 | id:1502.00741 author:Xiaolong Wang, Liang Lin category:cs.CV 68U01  published:2015-02-03 summary:This paper studies a novel discriminative part-based model to represent and recognize object shapes with an "And-Or graph". We define this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global verification. A discriminative learning algorithm, extended from the CCCP [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the configuration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration. The advantages of our method are two-fold. (i) The And-Or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images. (ii) The proposed learning algorithm is able to obtain the And-Or graph representation without requiring elaborate supervision and initialization. We validate the proposed method on several challenging databases (e.g., INRIA-Horse, ETHZ-Shape, and UIUC-People), and it outperforms the state-of-the-arts approaches. version:1
arxiv-1502-00739 | Clothing Co-Parsing by Joint Image Segmentation and Labeling | http://arxiv.org/abs/1502.00739 | id:1502.00739 author:Wei Yang, Ping Luo, Liang Lin category:cs.CV 68U01  published:2015-02-03 summary:This paper aims at developing an integrated system of clothing co-parsing, in order to jointly parse a set of clothing images (unsegmented but annotated with tags) into semantic configurations. We propose a data-driven framework consisting of two phases of inference. The first phase, referred as "image co-segmentation", iterates to extract consistent regions on images and jointly refines the regions over all images by employing the exemplar-SVM (E-SVM) technique [23]. In the second phase (i.e. "region co-labeling"), we construct a multi-image graphical model by taking the segmented regions as vertices, and incorporate several contexts of clothing configuration (e.g., item location and mutual interactions). The joint label assignment can be solved using the efficient Graph Cuts algorithm. In addition to evaluate our framework on the Fashionista dataset [30], we construct a dataset called CCP consisting of 2098 high-resolution street fashion photos to demonstrate the performance of our system. We achieve 90.29% / 88.23% segmentation accuracy and 65.52% / 63.89% recognition rate on the Fashionista and the CCP datasets, respectively, which are superior compared with state-of-the-art methods. version:1
arxiv-1502-00725 | Cheaper and Better: Selecting Good Workers for Crowdsourcing | http://arxiv.org/abs/1502.00725 | id:1502.00725 author:Hongwei Li, Qiang Liu category:stat.ML cs.AI cs.LG stat.AP  published:2015-02-03 summary:Crowdsourcing provides a popular paradigm for data collection at scale. We study the problem of selecting subsets of workers from a given worker pool to maximize the accuracy under a budget constraint. One natural question is whether we should hire as many workers as the budget allows, or restrict on a small number of top-quality workers. By theoretically analyzing the error rate of a typical setting in crowdsourcing, we frame the worker selection problem into a combinatorial optimization problem and propose an algorithm to solve it efficiently. Empirical results on both simulated and real-world datasets show that our algorithm is able to select a small number of high-quality workers, and performs as good as, sometimes even better than, the much larger crowds as the budget allows. version:1
arxiv-1502-00723 | Learning Contour-Fragment-based Shape Model with And-Or Tree Representation | http://arxiv.org/abs/1502.00723 | id:1502.00723 author:Liang Lin, Xiaolong Wang, Wei Yang, Jianhuang Lai category:cs.CV 68U01  published:2015-02-03 summary:This paper proposes a simple yet effective method to learn the hierarchical object shape model consisting of local contour fragments, which represents a category of shapes in the form of an And-Or tree. This model extends the traditional hierarchical tree structures by introducing the "switch" variables (i.e. the or-nodes) that explicitly specify production rules to capture shape variations. We thus define the model with three layers: the leaf-nodes for detecting local contour fragments, the or-nodes specifying selection of leaf-nodes, and the root-node encoding the holistic distortion. In the training stage, for optimization of the And-Or tree learning, we extend the concave-convex procedure (CCCP) by embedding the structural clustering during the iterative learning steps. The inference of shape detection is consistent with the model optimization, which integrates the local testings via the leaf-nodes and or-nodes with the global verification via the root-node. The advantages of our approach are validated on the challenging shape databases (i.e., ETHZ and INRIA Horse) and summarized as follows. (1) The proposed method is able to accurately localize shape contours against unreliable edge detection and edge tracing. (2) The And-Or tree model enables us to well capture the intraclass variance. version:1
arxiv-1502-00717 | Beyond Pixels: A Comprehensive Survey from Bottom-up to Semantic Image Segmentation and Cosegmentation | http://arxiv.org/abs/1502.00717 | id:1502.00717 author:Hongyuan Zhu, Fanman Meng, Jianfei Cai, Shijian Lu category:cs.CV  published:2015-02-03 summary:Image segmentation refers to the process to divide an image into nonoverlapping meaningful regions according to human perception, which has become a classic topic since the early ages of computer vision. A lot of research has been conducted and has resulted in many applications. However, while many segmentation algorithms exist, yet there are only a few sparse and outdated summarizations available, an overview of the recent achievements and issues is lacking. We aim to provide a comprehensive review of the recent progress in this field. Covering 180 publications, we give an overview of broad areas of segmentation topics including not only the classic bottom-up approaches, but also the recent development in superpixel, interactive methods, object proposals, semantic image parsing and image cosegmentation. In addition, we also review the existing influential datasets and evaluation metrics. Finally, we suggest some design flavors and research directions for future research in image segmentation. version:1
arxiv-1502-00712 | Deep Boosting: Layered Feature Mining for General Image Classification | http://arxiv.org/abs/1502.00712 | id:1502.00712 author:Zhanglin Peng, Liang Lin, Ruimao Zhang, Jing Xu category:cs.CV 68U01  published:2015-02-03 summary:Constructing effective representations is a critical but challenging problem in multimedia understanding. The traditional handcraft features often rely on domain knowledge, limiting the performances of exiting methods. This paper discusses a novel computational architecture for general image feature mining, which assembles the primitive filters (i.e. Gabor wavelets) into compositional features in a layer-wise manner. In each layer, we produce a number of base classifiers (i.e. regression stumps) associated with the generated features, and discover informative compositions by using the boosting algorithm. The output compositional features of each layer are treated as the base components to build up the next layer. Our framework is able to generate expressive image representations while inducing very discriminate functions for image classification. The experiments are conducted on several public datasets, and we demonstrate superior performances over state-of-the-art approaches. version:1
arxiv-1502-00705 | Recovery of Piecewise Smooth Images from Few Fourier Samples | http://arxiv.org/abs/1502.00705 | id:1502.00705 author:Greg Ongie, Mathews Jacob category:cs.CV  published:2015-02-03 summary:We introduce a Prony-like method to recover a continuous domain 2-D piecewise smooth image from few of its Fourier samples. Assuming the discontinuity set of the image is localized to the zero level-set of a trigonometric polynomial, we show the Fourier transform coefficients of partial derivatives of the signal satisfy an annihilation relation. We present necessary and sufficient conditions for unique recovery of piecewise constant images using the above annihilation relation. We pose the recovery of the Fourier coefficients of the signal from the measurements as a convex matrix completion algorithm, which relies on the lifting of the Fourier data to a structured low-rank matrix; this approach jointly estimates the signal and the annihilating filter. Finally, we demonstrate our algorithm on the recovery of MRI phantoms from few low-resolution Fourier samples. version:1
arxiv-1401-6686 | Perturbed Message Passing for Constraint Satisfaction Problems | http://arxiv.org/abs/1401.6686 | id:1401.6686 author:Siamak Ravanbakhsh, Russell Greiner category:cs.AI cs.CC stat.ML  published:2014-01-26 summary:We introduce an efficient message passing scheme for solving Constraint Satisfaction Problems (CSPs), which uses stochastic perturbation of Belief Propagation (BP) and Survey Propagation (SP) messages to bypass decimation and directly produce a single satisfying assignment. Our first CSP solver, called Perturbed Blief Propagation, smoothly interpolates two well-known inference procedures; it starts as BP and ends as a Gibbs sampler, which produces a single sample from the set of solutions. Moreover we apply a similar perturbation scheme to SP to produce another CSP solver, Perturbed Survey Propagation. Experimental results on random and real-world CSPs show that Perturbed BP is often more successful and at the same time tens to hundreds of times more efficient than standard BP guided decimation. Perturbed BP also compares favorably with state-of-the-art SP-guided decimation, which has a computational complexity that generally scales exponentially worse than our method (wrt the cardinality of variable domains and constraints). Furthermore, our experiments with random satisfiability and coloring problems demonstrate that Perturbed SP can outperform SP-guided decimation, making it the best incomplete random CSP-solver in difficult regimes. version:3
arxiv-1502-00652 | Learning the Matching Function | http://arxiv.org/abs/1502.00652 | id:1502.00652 author:Ľubor Ladický, Christian Häne, Marc Pollefeys category:cs.CV  published:2015-02-02 summary:The matching function for the problem of stereo reconstruction or optical flow has been traditionally designed as a function of the distance between the features describing matched pixels. This approach works under assumption, that the appearance of pixels in two stereo cameras or in two consecutive video frames does not change dramatically. However, this might not be the case, if we try to match pixels over a large interval of time. In this paper we propose a method, which learns the matching function, that automatically finds the space of allowed changes in visual appearance, such as due to the motion blur, chromatic distortions, different colour calibration or seasonal changes. Furthermore, it automatically learns the importance of matching scores of contextual features at different relative locations and scales. Proposed classifier gives reliable estimations of pixel disparities already without any form of regularization. We evaluated our method on two standard problems - stereo matching on KITTI outdoor dataset, optical flow on Sintel data set, and on newly introduced TimeLapse change detection dataset. Our algorithm obtained very promising results comparable to the state-of-the-art. version:1
arxiv-1310-4661 | Minimax rates in permutation estimation for feature matching | http://arxiv.org/abs/1310.4661 | id:1310.4661 author:Olivier Collier, Arnak S. Dalalyan category:math.ST cs.LG stat.TH  published:2013-10-17 summary:The problem of matching two sets of features appears in various tasks of computer vision and can be often formalized as a problem of permutation estimation. We address this problem from a statistical point of view and provide a theoretical analysis of the accuracy of several natural estimators. To this end, the minimax rate of separation is investigated and its expression is obtained as a function of the sample size, noise level and dimension. We consider the cases of homoscedastic and heteroscedastic noise and establish, in each case, tight upper bounds on the separation distance of several estimators. These upper bounds are shown to be unimprovable both in the homoscedastic and heteroscedastic settings. Interestingly, these bounds demonstrate that a phase transition occurs when the dimension $d$ of the features is of the order of the logarithm of the number of features $n$. For $d=O(\log n)$, the rate is dimension free and equals $\sigma (\log n)^{1/2}$, where $\sigma$ is the noise level. In contrast, when $d$ is larger than $c\log n$ for some constant $c>0$, the minimax rate increases with $d$ and is of the order $\sigma(d\log n)^{1/4}$. We also discuss the computational aspects of the estimators and provide empirical evidence of their consistency on synthetic data. Finally, we show that our results extend to more general matching criteria. version:2
arxiv-1502-00592 | A Class of DCT Approximations Based on the Feig-Winograd Algorithm | http://arxiv.org/abs/1502.00592 | id:1502.00592 author:C. J. Tablada, F. M. Bayer, R. J. Cintra category:stat.ME cs.CV cs.MM cs.NA stat.AP  published:2015-02-02 summary:A new class of matrices based on a parametrization of the Feig-Winograd factorization of 8-point DCT is proposed. Such parametrization induces a matrix subspace, which unifies a number of existing methods for DCT approximation. By solving a comprehensive multicriteria optimization problem, we identified several new DCT approximations. Obtained solutions were sought to possess the following properties: (i) low multiplierless computational complexity, (ii) orthogonality or near orthogonality, (iii) low complexity invertibility, and (iv) close proximity and performance to the exact DCT. Proposed approximations were submitted to assessment in terms of proximity to the DCT, coding performance, and suitability for image compression. Considering Pareto efficiency, particular new proposed approximations could outperform various existing methods archived in literature. version:1
arxiv-1406-5291 | Generalized Dantzig Selector: Application to the k-support norm | http://arxiv.org/abs/1406.5291 | id:1406.5291 author:Soumyadeep Chatterjee, Sheng Chen, Arindam Banerjee category:stat.ML cs.LG  published:2014-06-20 summary:We propose a Generalized Dantzig Selector (GDS) for linear models, in which any norm encoding the parameter structure can be leveraged for estimation. We investigate both computational and statistical aspects of the GDS. Based on conjugate proximal operator, a flexible inexact ADMM framework is designed for solving GDS, and non-asymptotic high-probability bounds are established on the estimation error, which rely on Gaussian width of unit norm ball and suitable set encompassing estimation error. Further, we consider a non-trivial example of the GDS using $k$-support norm. We derive an efficient method to compute the proximal operator for $k$-support norm since existing methods are inapplicable in this setting. For statistical analysis, we provide upper bounds for the Gaussian widths needed in the GDS analysis, yielding the first statistical recovery guarantee for estimation with the $k$-support norm. The experimental results confirm our theoretical analysis. version:3
arxiv-1403-0388 | Cascading Randomized Weighted Majority: A New Online Ensemble Learning Algorithm | http://arxiv.org/abs/1403.0388 | id:1403.0388 author:Mohammadzaman Zamani, Hamid Beigy, Amirreza Shaban category:stat.ML cs.LG  published:2014-03-03 summary:With the increasing volume of data in the world, the best approach for learning from this data is to exploit an online learning algorithm. Online ensemble methods are online algorithms which take advantage of an ensemble of classifiers to predict labels of data. Prediction with expert advice is a well-studied problem in the online ensemble learning literature. The Weighted Majority algorithm and the randomized weighted majority (RWM) are the most well-known solutions to this problem, aiming to converge to the best expert. Since among some expert, the best one does not necessarily have the minimum error in all regions of data space, defining specific regions and converging to the best expert in each of these regions will lead to a better result. In this paper, we aim to resolve this defect of RWM algorithms by proposing a novel online ensemble algorithm to the problem of prediction with expert advice. We propose a cascading version of RWM to achieve not only better experimental results but also a better error bound for sufficiently large datasets. version:4
arxiv-1501-06478 | Compressed Support Vector Machines | http://arxiv.org/abs/1501.06478 | id:1501.06478 author:Zhixiang Xu, Jacob R. Gardner, Stephen Tyree, Kilian Q. Weinberger category:cs.LG  published:2015-01-26 summary:Support vector machines (SVM) can classify data sets along highly non-linear decision boundaries because of the kernel-trick. This expressiveness comes at a price: During test-time, the SVM classifier needs to compute the kernel inner-product between a test sample and all support vectors. With large training data sets, the time required for this computation can be substantial. In this paper, we introduce a post-processing algorithm, which compresses the learned SVM model by reducing and optimizing support vectors. We evaluate our algorithm on several medium-scaled real-world data sets, demonstrating that it maintains high test accuracy while reducing the test-time evaluation cost by several orders of magnitude---in some cases from hours to seconds. It is fair to say that most of the work in this paper was previously been invented by Burges and Sch\"olkopf almost 20 years ago. For most of the time during which we conducted this research, we were unaware of this prior work. However, in the past two decades, computing power has increased drastically, and we can therefore provide empirical insights that were not possible in their original paper. version:2
arxiv-1502-00512 | Scaling Recurrent Neural Network Language Models | http://arxiv.org/abs/1502.00512 | id:1502.00512 author:Will Williams, Niranjani Prasad, David Mrva, Tom Ash, Tony Robinson category:cs.CL cs.LG  published:2015-02-02 summary:This paper investigates the scaling properties of Recurrent Neural Network Language Models (RNNLMs). We discuss how to train very large RNNs on GPUs and address the questions of how RNNLMs scale with respect to model size, training-set size, computational costs and memory. Our analysis shows that despite being more costly to train, RNNLMs obtain much lower perplexities on standard benchmarks than n-gram models. We train the largest known RNNs and present relative word error rates gains of 18% on an ASR task. We also present the new lowest perplexities on the recently released billion word language modelling benchmark, 1 BLEU point gain on machine translation and a 17% relative hit rate gain in word prediction. version:1
arxiv-1502-00501 | An Expressive Deep Model for Human Action Parsing from A Single Image | http://arxiv.org/abs/1502.00501 | id:1502.00501 author:Zhujin Liang, Xiaolong Wang, Rui Huang, Liang Lin category:cs.CV 68U01  published:2015-02-02 summary:This paper aims at one newly raising task in vision and multimedia research: recognizing human actions from still images. Its main challenges lie in the large variations in human poses and appearances, as well as the lack of temporal motion information. Addressing these problems, we propose to develop an expressive deep model to naturally integrate human layout and surrounding contexts for higher level action understanding from still images. In particular, a Deep Belief Net is trained to fuse information from different noisy sources such as body part detection and object detection. To bridge the semantic gap, we used manually labeled data to greatly improve the effectiveness and efficiency of the pre-training and fine-tuning stages of the DBN training. The resulting framework is shown to be robust to sometimes unreliable inputs (e.g., imprecise detections of human parts and objects), and outperforms the state-of-the-art approaches. version:1
arxiv-1502-00500 | Fast and Robust Feature Matching for RGB-D Based Localization | http://arxiv.org/abs/1502.00500 | id:1502.00500 author:Miguel Heredia, Felix Endres, Wolfram Burgard, Rafael Sanz category:cs.CV cs.RO  published:2015-02-02 summary:In this paper we present a novel approach to global localization using an RGB-D camera in maps of visual features. For large maps, the performance of pure image matching techniques decays in terms of robustness and computational cost. Particularly, repeated occurrences of similar features due to repeating structure in the world (e.g., doorways, chairs, etc.) or missing associations between observations pose critical challenges to visual localization. We address these challenges using a two-step approach. We first estimate a candidate pose using few correspondences between features of the current camera frame and the feature map. The initial set of correspondences is established by proximity in feature space. The initial pose estimate is used in the second step to guide spatial matching of features in 3D, i.e., searching for associations where the image features are expected to be found in the map. A RANSAC algorithm is used to compute a fine estimation of the pose from the correspondences. Our approach clearly outperforms localization based on feature matching exclusively in feature space, both in terms of estimation accuracy and robustness to failure and allows for global localization in real time (30Hz). version:1
arxiv-1411-7783 | From neural PCA to deep unsupervised learning | http://arxiv.org/abs/1411.7783 | id:1411.7783 author:Harri Valpola category:stat.ML cs.LG cs.NE  published:2014-11-28 summary:A network supporting deep unsupervised learning is presented. The network is an autoencoder with lateral shortcut connections from the encoder to decoder at each level of the hierarchy. The lateral shortcut connections allow the higher levels of the hierarchy to focus on abstract invariant features. While standard autoencoders are analogous to latent variable models with a single layer of stochastic variables, the proposed network is analogous to hierarchical latent variables models. Learning combines denoising autoencoder and denoising sources separation frameworks. Each layer of the network contributes to the cost function a term which measures the distance of the representations produced by the encoder and the decoder. Since training signals originate from all levels of the network, all layers can learn efficiently even in deep networks. The speedup offered by cost terms from higher levels of the hierarchy and the ability to learn invariant features are demonstrated in experiments. version:2
arxiv-1406-6603 | A scaled gradient projection method for Bayesian learning in dynamical systems | http://arxiv.org/abs/1406.6603 | id:1406.6603 author:Silvia Bonettini, Alessandro Chiuso, Marco Prato category:math.NA cs.LG stat.ML  published:2014-06-25 summary:A crucial task in system identification problems is the selection of the most appropriate model class, and is classically addressed resorting to cross-validation or using asymptotic arguments. As recently suggested in the literature, this can be addressed in a Bayesian framework, where model complexity is regulated by few hyperparameters, which can be estimated via marginal likelihood maximization. It is thus of primary importance to design effective optimization methods to solve the corresponding optimization problem. If the unknown impulse response is modeled as a Gaussian process with a suitable kernel, the maximization of the marginal likelihood leads to a challenging nonconvex optimization problem, which requires a stable and effective solution strategy. In this paper we address this problem by means of a scaled gradient projection algorithm, in which the scaling matrix and the steplength parameter play a crucial role to provide a meaning solution in a computational time comparable with second order methods. In particular, we propose both a generalization of the split gradient approach to design the scaling matrix in the presence of box constraints, and an effective implementation of the gradient and objective function. The extensive numerical experiments carried out on several test problems show that our method is very effective in providing in few tenths of a second solutions of the problems with accuracy comparable with state-of-the-art approaches. Moreover, the flexibility of the proposed strategy makes it easily adaptable to a wider range of problems arising in different areas of machine learning, signal processing and system identification. version:3
arxiv-1502-00416 | Towards a solid solution of real-time fire and flame detection | http://arxiv.org/abs/1502.00416 | id:1502.00416 author:Bo Jiang, Yongyi Lu, Xiying Li, Liang Lin category:cs.CV 68U01  published:2015-02-02 summary:Although the object detection and recognition has received growing attention for decades, a robust fire and flame detection method is rarely explored. This paper presents an empirical study, towards a general and solid approach to fast detect fire and flame in videos, with the applications in video surveillance and event retrieval. Our system consists of three cascaded steps: (1) candidate regions proposing by a background model, (2) fire region classifying with color-texture features and a dictionary of visual words, and (3) temporal verifying. The experimental evaluation and analysis are done for each step. We believe that it is a useful service to both academic research and real-world application. In addition, we release the software of the proposed system with the source code, as well as a public benchmark and data set, including 64 video clips covered both indoor and outdoor scenes under different conditions. We achieve an 82% Recall with 93% Precision on the data set, and greatly improve the performance by state-of-the-arts methods. version:1
arxiv-1407-0731 | Info-Greedy sequential adaptive compressed sensing | http://arxiv.org/abs/1407.0731 | id:1407.0731 author:Gabor Braun, Sebastian Pokutta, Yao Xie category:cs.IT math.IT math.ST stat.ML stat.TH  published:2014-07-02 summary:We present an information-theoretic framework for sequential adaptive compressed sensing, Info-Greedy Sensing, where measurements are chosen to maximize the extracted information conditioned on the previous measurements. We show that the widely used bisection approach is Info-Greedy for a family of $k$-sparse signals by connecting compressed sensing and blackbox complexity of sequential query algorithms, and present Info-Greedy algorithms for Gaussian and Gaussian Mixture Model (GMM) signals, as well as ways to design sparse Info-Greedy measurements. Numerical examples demonstrate the good performance of the proposed algorithms using simulated and real data: Info-Greedy Sensing shows significant improvement over random projection for signals with sparse and low-rank covariance matrices, and adaptivity brings robustness when there is a mismatch between the assumed and the true distributions. version:4
arxiv-1502-00377 | Integrating Graph Partitioning and Matching for Trajectory Analysis in Video Surveillance | http://arxiv.org/abs/1502.00377 | id:1502.00377 author:Liang Lin, Yongyi Lu, Yan Pan, Xiaowu Chen category:cs.CV 68U01  published:2015-02-02 summary:In order to track the moving objects in long range against occlusion, interruption, and background clutter, this paper proposes a unified approach for global trajectory analysis. Instead of the traditional frame-by-frame tracking, our method recovers target trajectories based on a short sequence of video frames, e.g. $15$ frames. We initially calculate a foreground map at each frame, as obtained from a state-of-the-art background model. An attribute graph is then extracted from the foreground map, where the graph vertices are image primitives represented by the composite features. With this graph representation, we pose trajectory analysis as a joint task of spatial graph partitioning and temporal graph matching. The task can be formulated by maximizing a posteriori under the Bayesian framework, in which we integrate the spatio-temporal contexts and the appearance models. The probabilistic inference is achieved by a data-driven Markov Chain Monte Carlo (MCMC) algorithm. Given a peroid of observed frames, the algorithm simulates a ergodic and aperiodic Markov Chain, and it visits a sequence of solution states in the joint space of spatial graph partitioning and temporal graph matching. In the experiments, our method is tested on several challenging videos from the public datasets of visual surveillance, and it outperforms the state-of-the-art methods. version:1
arxiv-1502-00374 | Adaptive Scene Category Discovery with Generative Learning and Compositional Sampling | http://arxiv.org/abs/1502.00374 | id:1502.00374 author:Liang Lin, Ruimao Zhang, Xiaohua Duan category:cs.CV 68U01  published:2015-02-02 summary:This paper investigates a general framework to discover categories of unlabeled scene images according to their appearances (i.e., textures and structures). We jointly solve the two coupled tasks in an unsupervised manner: (i) classifying images without pre-determining the number of categories, and (ii) pursuing generative model for each category. In our method, each image is represented by two types of image descriptors that are effective to capture image appearances from different aspects. By treating each image as a graph vertex, we build up an graph, and pose the image categorization as a graph partition process. Specifically, a partitioned sub-graph can be regarded as a category of scenes, and we define the probabilistic model of graph partition by accumulating the generative models of all separated categories. For efficient inference with the graph, we employ a stochastic cluster sampling algorithm, which is designed based on the Metropolis-Hasting mechanism. During the iterations of inference, the model of each category is analytically updated by a generative learning algorithm. In the experiments, our approach is validated on several challenging databases, and it outperforms other popular state-of-the-art methods. The implementation details and empirical analysis are presented as well. version:1
arxiv-1502-00363 | Iterated Support Vector Machines for Distance Metric Learning | http://arxiv.org/abs/1502.00363 | id:1502.00363 author:Wangmeng Zuo, Faqiang Wang, David Zhang, Liang Lin, Yuchi Huang, Deyu Meng, Lei Zhang category:cs.LG cs.CV  published:2015-02-02 summary:Distance metric learning aims to learn from the given training data a valid distance metric, with which the similarity between data samples can be more effectively evaluated for classification. Metric learning is often formulated as a convex or nonconvex optimization problem, while many existing metric learning algorithms become inefficient for large scale problems. In this paper, we formulate metric learning as a kernel classification problem, and solve it by iterated training of support vector machines (SVM). The new formulation is easy to implement, efficient in training, and tractable for large-scale problems. Two novel metric learning models, namely Positive-semidefinite Constrained Metric Learning (PCML) and Nonnegative-coefficient Constrained Metric Learning (NCML), are developed. Both PCML and NCML can guarantee the global optimality of their solutions. Experimental results on UCI dataset classification, handwritten digit recognition, face verification and person re-identification demonstrate that the proposed metric learning methods achieve higher classification accuracy than state-of-the-art methods and they are significantly more efficient in training. version:1
arxiv-1502-00354 | A Web-based Interactive Visual Graph Analytics Platform | http://arxiv.org/abs/1502.00354 | id:1502.00354 author:Nesreen K. Ahmed, Ryan A. Rossi category:cs.SI cs.HC stat.ML  published:2015-02-02 summary:This paper proposes a web-based visual graph analytics platform for interactive graph mining, visualization, and real-time exploration of networks. GraphVis is fast, intuitive, and flexible, combining interactive visualizations with analytic techniques to reveal important patterns and insights for sense making, reasoning, and decision making. Networks can be visualized and explored within seconds by simply drag-and-dropping a graph file into the web browser. The structure, properties, and patterns of the network are computed automatically and can be instantly explored in real-time. At the heart of GraphVis lies a multi-level interactive network visualization and analytics engine that allows for real-time graph mining and exploration across multiple levels of granularity simultaneously. Both the graph analytic and visualization techniques (at each level of granularity) are dynamic and interactive, with immediate and continuous visual feedback upon every user interaction (e.g., change of a slider for filtering). Furthermore, nodes, edges, and subgraphs are easily inserted, deleted or exported via a number of novel techniques and tools that make it extremely easy and flexible for exploring, testing hypothesis, and understanding networks in real-time over the web. A number of interactive visual graph analytic techniques are also proposed including interactive role discovery methods, community detection, as well as a number of novel block models for generating graphs with community structure. Finally, we also highlight other key aspects including filtering, querying, ranking, manipulating, exporting, partitioning, as well as tools for dynamic network analysis and visualization, interactive graph generators, and a variety of multi-level network analysis, summarization, and statistical techniques. version:1
arxiv-1502-00344 | Complex Background Subtraction by Pursuing Dynamic Spatio-Temporal Models | http://arxiv.org/abs/1502.00344 | id:1502.00344 author:Liang Lin, Yuanlu Xu, Xiaodan Liang, Jianhuang Lai category:cs.CV 68U01  published:2015-02-02 summary:Although it has been widely discussed in video surveillance, background subtraction is still an open problem in the context of complex scenarios, e.g., dynamic backgrounds, illumination variations, and indistinct foreground objects. To address these challenges, we propose an effective background subtraction method by learning and maintaining an array of dynamic texture models within the spatio-temporal representations. At any location of the scene, we extract a sequence of regular video bricks, i.e. video volumes spanning over both spatial and temporal domain. The background modeling is thus posed as pursuing subspaces within the video bricks while adapting the scene variations. For each sequence of video bricks, we pursue the subspace by employing the ARMA (Auto Regressive Moving Average) Model that jointly characterizes the appearance consistency and temporal coherence of the observations. During online processing, we incrementally update the subspaces to cope with disturbances from foreground objects and scene changes. In the experiments, we validate the proposed method in several complex scenarios, and show superior performances over other state-of-the-art approaches of background subtraction. The empirical studies of parameter setting and component analysis are presented as well. version:1
arxiv-1502-00341 | Discriminatively Trained And-Or Graph Models for Object Shape Detection | http://arxiv.org/abs/1502.00341 | id:1502.00341 author:Liang Lin, Xiaolong Wang, Wei Yang, Jian-Huang Lai category:cs.CV 68U01  published:2015-02-02 summary:In this paper, we investigate a novel reconfigurable part-based model, namely And-Or graph model, to recognize object shapes in images. Our proposed model consists of four layers: leaf-nodes at the bottom are local classifiers for detecting contour fragments; or-nodes above the leaf-nodes function as the switches to activate their child leaf-nodes, making the model reconfigurable during inference; and-nodes in a higher layer capture holistic shape deformations; one root-node on the top, which is also an or-node, activates one of its child and-nodes to deal with large global variations (e.g. different poses and views). We propose a novel structural optimization algorithm to discriminatively train the And-Or model from weakly annotated data. This algorithm iteratively determines the model structures (e.g. the nodes and their layouts) along with the parameter learning. On several challenging datasets, our model demonstrates the effectiveness to perform robust shape-based object detection against background clutter and outperforms the other state-of-the-art approaches. We also release a new shape database with annotations, which includes more than 1500 challenging shape instances, for recognition and detection. version:1
arxiv-1502-00324 | Modified Fast Fractal Image Compression Algorithm in spatial domain | http://arxiv.org/abs/1502.00324 | id:1502.00324 author:M. Salarian, H. Miar Naimi category:cs.CV  published:2015-02-01 summary:In this paper a new fractal image compression algorithm is proposed in which the time of encoding process is considerably reduced. The algorithm exploits a domain pool reduction approach, along with using innovative predefined values for contrast scaling factor, S, instead of searching it across [0,1]. Only the domain blocks with entropy greater than a threshold are considered as domain pool. As a novel point, it is assumed that in each step of the encoding process, the domain block with small enough distance shall be found only for the range blocks with low activity (equivalently low entropy). This novel point is used to find reasonable estimations of S, and use them in the encoding process as predefined values, mentioned above, the remaining range blocks are split into four new smaller range blocks and the algorithm must be iterated for them, considered as the other step of encoding process. The algorithm has been examined for some of the well-known images and the results have been compared with the state-of-the-art algorithms. The experiments show that our proposed algorithm has considerably lower encoding time than the other where the encoded images are approximately the same in quality. version:1
arxiv-1502-00303 | Dynamic texture and scene classification by transferring deep image features | http://arxiv.org/abs/1502.00303 | id:1502.00303 author:Xianbiao Qi, Chun-Guang Li, Guoying Zhao, Xiaopeng Hong, Matti Pietikäinen category:cs.CV  published:2015-02-01 summary:Dynamic texture and scene classification are two fundamental problems in understanding natural video content. Extracting robust and effective features is a crucial step towards solving these problems. However the existing approaches suffer from the sensitivity to either varying illumination, or viewpoint changing, or even camera motion, and/or the lack of spatial information. Inspired by the success of deep structures in image classification, we attempt to leverage a deep structure to extract feature for dynamic texture and scene classification. To tackle with the challenges in training a deep structure, we propose to transfer some prior knowledge from image domain to video domain. To be specific, we propose to apply a well-trained Convolutional Neural Network (ConvNet) as a mid-level feature extractor to extract features from each frame, and then form a representation of a video by concatenating the first and the second order statistics over the mid-level features. We term this two-level feature extraction scheme as a Transferred ConvNet Feature (TCoF). Moreover we explore two different implementations of the TCoF scheme, i.e., the \textit{spatial} TCoF and the \textit{temporal} TCoF, in which the mean-removed frames and the difference between two adjacent frames are used as the inputs of the ConvNet, respectively. We evaluate systematically the proposed spatial TCoF and the temporal TCoF schemes on three benchmark data sets, including DynTex, YUPENN, and Maryland, and demonstrate that the proposed approach yields superior performance. version:1
arxiv-1501-06262 | 3D Human Activity Recognition with Reconfigurable Convolutional Neural Networks | http://arxiv.org/abs/1501.06262 | id:1501.06262 author:Keze Wang, Xiaolong Wang, Liang Lin, Meng Wang, Wangmeng Zuo category:cs.CV 68U01 I.4  published:2015-01-26 summary:Human activity understanding with 3D/depth sensors has received increasing attention in multimedia processing and interactions. This work targets on developing a novel deep model for automatic activity recognition from RGB-D videos. We represent each human activity as an ensemble of cubic-like video segments, and learn to discover the temporal structures for a category of activities, i.e. how the activities to be decomposed in terms of classification. Our model can be regarded as a structured deep architecture, as it extends the convolutional neural networks (CNNs) by incorporating structure alternatives. Specifically, we build the network consisting of 3D convolutions and max-pooling operators over the video segments, and introduce the latent variables in each convolutional layer manipulating the activation of neurons. Our model thus advances existing approaches in two aspects: (i) it acts directly on the raw inputs (grayscale-depth data) to conduct recognition instead of relying on hand-crafted features, and (ii) the model structure can be dynamically adjusted accounting for the temporal variations of human activities, i.e. the network configuration is allowed to be partially activated during inference. For model training, we propose an EM-type optimization method that iteratively (i) discovers the latent structure by determining the decomposed actions for each training example, and (ii) learns the network parameters by using the back-propagation algorithm. Our approach is validated in challenging scenarios, and outperforms state-of-the-art methods. A large human activity database of RGB-D videos is presented in addition. version:3
arxiv-1502-00258 | Learning Latent Spatio-Temporal Compositional Model for Human Action Recognition | http://arxiv.org/abs/1502.00258 | id:1502.00258 author:Xiaodan Liang, Liang Lin, Liangliang Cao category:cs.CV 68U01 I.5; I.4  published:2015-02-01 summary:Action recognition is an important problem in multimedia understanding. This paper addresses this problem by building an expressive compositional action model. We model one action instance in the video with an ensemble of spatio-temporal compositions: a number of discrete temporal anchor frames, each of which is further decomposed to a layout of deformable parts. In this way, our model can identify a Spatio-Temporal And-Or Graph (STAOG) to represent the latent structure of actions e.g. triple jumping, swinging and high jumping. The STAOG model comprises four layers: (i) a batch of leaf-nodes in bottom for detecting various action parts within video patches; (ii) the or-nodes over bottom, i.e. switch variables to activate their children leaf-nodes for structural variability; (iii) the and-nodes within an anchor frame for verifying spatial composition; and (iv) the root-node at top for aggregating scores over temporal anchor frames. Moreover, the contextual interactions are defined between leaf-nodes in both spatial and temporal domains. For model training, we develop a novel weakly supervised learning algorithm which iteratively determines the structural configuration (e.g. the production of leaf-nodes associated with the or-nodes) along with the optimization of multi-layer parameters. By fully exploiting spatio-temporal compositions and interactions, our approach handles well large intra-class action variance (e.g. different views, individual appearances, spatio-temporal structures). The experimental results on the challenging databases demonstrate superior performance of our approach over other competing methods. version:1
arxiv-1502-00256 | Human Re-identification by Matching Compositional Template with Cluster Sampling | http://arxiv.org/abs/1502.00256 | id:1502.00256 author:Yuanlu Xu, Liang Lin, Wei-Shi Zheng, Xiaobai Liu category:cs.CV 68U01  published:2015-02-01 summary:This paper aims at a newly raising task in visual surveillance: re-identifying people at a distance by matching body information, given several reference examples. Most of existing works solve this task by matching a reference template with the target individual, but often suffer from large human appearance variability (e.g. different poses/views, illumination) and high false positives in matching caused by conjunctions, occlusions or surrounding clutters. Addressing these problems, we construct a simple yet expressive template from a few reference images of a certain individual, which represents the body as an articulated assembly of compositional and alternative parts, and propose an effective matching algorithm with cluster sampling. This algorithm is designed within a candidacy graph whose vertices are matching candidates (i.e. a pair of source and target body parts), and iterates in two steps for convergence. (i) It generates possible partial matches based on compatible and competitive relations among body parts. (ii) It confirms the partial matches to generate a new matching solution, which is accepted by the Markov Chain Monte Carlo (MCMC) mechanism. In the experiments, we demonstrate the superior performance of our approach on three public databases compared to existing methods. version:1
arxiv-1502-00250 | Driver distraction detection and recognition using RGB-D sensor | http://arxiv.org/abs/1502.00250 | id:1502.00250 author:Céline Craye, Fakhri Karray category:cs.CV  published:2015-02-01 summary:Driver inattention assessment has become a very active field in intelligent transportation systems. Based on active sensor Kinect and computer vision tools, we have built an efficient module for detecting driver distraction and recognizing the type of distraction. Based on color and depth map data from the Kinect, our system is composed of four sub-modules. We call them eye behavior (detecting gaze and blinking), arm position (is the right arm up, down, right of forward), head orientation, and facial expressions. Each module produces relevant information for assessing driver inattention. They are merged together later on using two different classification strategies: AdaBoost classifier and Hidden Markov Model. Evaluation is done using a driving simulator and 8 drivers of different gender, age and nationality for a total of more than 8 hours of recording. Qualitative and quantitative results show strong and accurate detection and recognition capacity (85% accuracy for the type of distraction and 90% for distraction detection). Moreover, each module is obtained independently and could be used for other types of inference, such as fatigue detection, and could be implemented for real cars systems. version:1
arxiv-1502-00245 | Injury risk prediction for traffic accidents in Porto Alegre/RS, Brazil | http://arxiv.org/abs/1502.00245 | id:1502.00245 author:Christian S. Perone category:cs.LG cs.AI  published:2015-02-01 summary:This study describes the experimental application of Machine Learning techniques to build prediction models that can assess the injury risk associated with traffic accidents. This work uses an freely available data set of traffic accident records that took place in the city of Porto Alegre/RS (Brazil) during the year of 2013. This study also provides an analysis of the most important attributes of a traffic accident that could produce an outcome of injury to the people involved in the accident. version:1
arxiv-1405-1119 | Feature selection for classification with class-separability strategy and data envelopment analysis | http://arxiv.org/abs/1405.1119 | id:1405.1119 author:Yishi Zhang, Chao Yang, Anrong Yang, Chan Xiong, Xingchi Zhou, Zigang Zhang category:cs.LG cs.IT math.IT stat.ML I.5.2; G.1.6; H.1.1  published:2014-05-06 summary:In this paper, a novel feature selection method is presented, which is based on Class-Separability (CS) strategy and Data Envelopment Analysis (DEA). To better capture the relationship between features and the class, class labels are separated into individual variables and relevance and redundancy are explicitly handled on each class label. Super-efficiency DEA is employed to evaluate and rank features via their conditional dependence scores on all class labels, and the feature with maximum super-efficiency score is then added in the conditioning set for conditional dependence estimation in the next iteration, in such a way as to iteratively select features and get the final selected features. Eventually, experiments are conducted to evaluate the effectiveness of proposed method comparing with four state-of-the-art methods from the viewpoint of classification accuracy. Empirical results verify the feasibility and the superiority of proposed feature selection method. version:2
arxiv-1502-00231 | Feature Selection with Redundancy-complementariness Dispersion | http://arxiv.org/abs/1502.00231 | id:1502.00231 author:Zhijun Chen, Chaozhong Wu, Yishi Zhang, Zhen Huang, Bin Ran, Ming Zhong, Nengchao Lyu category:cs.LG stat.ML I.5.2; H.1.1  published:2015-02-01 summary:Feature selection has attracted significant attention in data mining and machine learning in the past decades. Many existing feature selection methods eliminate redundancy by measuring pairwise inter-correlation of features, whereas the complementariness of features and higher inter-correlation among more than two features are ignored. In this study, a modification item concerning the complementariness of features is introduced in the evaluation criterion of features. Additionally, in order to identify the interference effect of already-selected False Positives (FPs), the redundancy-complementariness dispersion is also taken into account to adjust the measurement of pairwise inter-correlation of features. To illustrate the effectiveness of proposed method, classification experiments are applied with four frequently used classifiers on ten datasets. Classification results verify the superiority of proposed method compared with five representative feature selection methods. version:1
arxiv-1402-4419 | Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning | http://arxiv.org/abs/1402.4419 | id:1402.4419 author:Julien Mairal category:math.OC cs.LG stat.ML  published:2014-02-18 summary:Majorization-minimization algorithms consist of successively minimizing a sequence of upper bounds of the objective function. These upper bounds are tight at the current estimate, and each iteration monotonically drives the objective function downhill. Such a simple principle is widely applicable and has been very popular in various scientific fields, especially in signal processing and statistics. In this paper, we propose an incremental majorization-minimization scheme for minimizing a large sum of continuous functions, a problem of utmost importance in machine learning. We present convergence guarantees for non-convex and convex optimization when the upper bounds approximate the objective up to a smooth error; we call such upper bounds "first-order surrogate functions". More precisely, we study asymptotic stationary point guarantees for non-convex problems, and for convex ones, we provide convergence rates for the expected objective function value. We apply our scheme to composite optimization and obtain a new incremental proximal gradient algorithm with linear convergence rate for strongly convex functions. In our experiments, we show that our method is competitive with the state of the art for solving machine learning problems such as logistic regression when the number of training samples is large enough, and we demonstrate its usefulness for sparse estimation with non-convex penalties. version:3
arxiv-1411-5899 | Falling Rule Lists | http://arxiv.org/abs/1411.5899 | id:1411.5899 author:Fulton Wang, Cynthia Rudin category:cs.AI cs.LG  published:2014-11-21 summary:Falling rule lists are classification models consisting of an ordered list of if-then rules, where (i) the order of rules determines which example should be classified by each rule, and (ii) the estimated probability of success decreases monotonically down the list. These kinds of rule lists are inspired by healthcare applications where patients would be stratified into risk sets and the highest at-risk patients should be considered first. We provide a Bayesian framework for learning falling rule lists that does not rely on traditional greedy decision tree learning methods. version:3
arxiv-1502-00199 | Chemical Reaction Optimization for the Set Covering Problem | http://arxiv.org/abs/1502.00199 | id:1502.00199 author:James J. Q. Yu, Albert Y. S. Lam, Victor O. K. Li category:cs.NE  published:2015-02-01 summary:The set covering problem (SCP) is one of the representative combinatorial optimization problems, having many practical applications. This paper investigates the development of an algorithm to solve SCP by employing chemical reaction optimization (CRO), a general-purpose metaheuristic. It is tested on a wide range of benchmark instances of SCP. The simulation results indicate that this algorithm gives outstanding performance compared with other heuristics and metaheuristics in solving SCP. version:1
arxiv-1502-00197 | An Inter-molecular Adaptive Collision Scheme for Chemical Reaction Optimization | http://arxiv.org/abs/1502.00197 | id:1502.00197 author:James J. Q. Yu, Victor O. K. Li, Albert Y. S. Lam category:cs.NE  published:2015-02-01 summary:Optimization techniques are frequently applied in science and engineering research and development. Evolutionary algorithms, as a kind of general-purpose metaheuristic, have been shown to be very effective in solving a wide range of optimization problems. A recently proposed chemical-reaction-inspired metaheuristic, Chemical Reaction Optimization (CRO), has been applied to solve many global optimization problems. However, the functionality of the inter-molecular ineffective collision operator in the canonical CRO design overlaps that of the on-wall ineffective collision operator, which can potential impair the overall performance. In this paper we propose a new inter-molecular ineffective collision operator for CRO for global optimization. To fully utilize our newly proposed operator, we also design a scheme to adapt the algorithm to optimization problems with different search space characteristics. We analyze the performance of our proposed algorithm with a number of widely used benchmark functions. The simulation results indicate that the new algorithm has superior performance over the canonical CRO. version:1
arxiv-1502-00196 | Optimal V2G Scheduling of Electric Vehicles and Unit Commitment using Chemical Reaction Optimization | http://arxiv.org/abs/1502.00196 | id:1502.00196 author:James J. Q. Yu, Victor O. K. Li, Albert Y. S. Lam category:cs.NE  published:2015-02-01 summary:An electric vehicle (EV) may be used as energy storage which allows the bi-directional electricity flow between the vehicle's battery and the electric power grid. In order to flatten the load profile of the electricity system, EV scheduling has become a hot research topic in recent years. In this paper, we propose a new formulation of the joint scheduling of EV and Unit Commitment (UC), called EVUC. Our formulation considers the characteristics of EVs while optimizing the system total running cost. We employ Chemical Reaction Optimization (CRO), a general-purpose optimization algorithm to solve this problem and the simulation results on a widely used set of instances indicate that CRO can effectively optimize this problem. version:1
arxiv-1502-00195 | Sensor Deployment for Air Pollution Monitoring Using Public Transportation System | http://arxiv.org/abs/1502.00195 | id:1502.00195 author:James J. Q. Yu, Victor O. K. Li, Albert Y. S. Lam category:cs.NE  published:2015-02-01 summary:Air pollution monitoring is a very popular research topic and many monitoring systems have been developed. In this paper, we formulate the Bus Sensor Deployment Problem (BSDP) to select the bus routes on which sensors are deployed, and we use Chemical Reaction Optimization (CRO) to solve BSDP. CRO is a recently proposed metaheuristic designed to solve a wide range of optimization problems. Using the real world data, namely Hong Kong Island bus route data, we perform a series of simulations and the results show that CRO is capable of solving this optimization problem efficiently. version:1
arxiv-1502-00194 | Real-Coded Chemical Reaction Optimization with Different Perturbation Functions | http://arxiv.org/abs/1502.00194 | id:1502.00194 author:James J. Q. Yu, Albert Y. S. Lam, Victor O. K. Li category:cs.NE  published:2015-02-01 summary:Chemical Reaction Optimization (CRO) is a powerful metaheuristic which mimics the interactions of molecules in chemical reactions to search for the global optimum. The perturbation function greatly influences the performance of CRO on solving different continuous problems. In this paper, we study four different probability distributions, namely, the Gaussian distribution, the Cauchy distribution, the exponential distribution, and a modified Rayleigh distribution, for the perturbation function of CRO. Different distributions have different impacts on the solutions. The distributions are tested by a set of well-known benchmark functions and simulation results show that problems with different characteristics have different preference on the distribution function. Our study gives guidelines to design CRO for different types of optimization problems. version:1
arxiv-1502-00193 | Evolutionary Artificial Neural Network Based on Chemical Reaction Optimization | http://arxiv.org/abs/1502.00193 | id:1502.00193 author:James J. Q. Yu, Albert Y. S. Lam, Victor O. K. Li category:cs.NE  published:2015-02-01 summary:Evolutionary algorithms (EAs) are very popular tools to design and evolve artificial neural networks (ANNs), especially to train them. These methods have advantages over the conventional backpropagation (BP) method because of their low computational requirement when searching in a large solution space. In this paper, we employ Chemical Reaction Optimization (CRO), a newly developed global optimization method, to replace BP in training neural networks. CRO is a population-based metaheuristics mimicking the transition of molecules and their interactions in a chemical reaction. Simulation results show that CRO outperforms many EA strategies commonly used to train neural networks. version:1
arxiv-1502-00192 | Pose and Shape Estimation with Discriminatively Learned Parts | http://arxiv.org/abs/1502.00192 | id:1502.00192 author:Menglong Zhu, Xiaowei Zhou, Kostas Daniilidis category:cs.CV  published:2015-02-01 summary:We introduce a new approach for estimating the 3D pose and the 3D shape of an object from a single image. Given a training set of view exemplars, we learn and select appearance-based discriminative parts which are mapped onto the 3D model from the training set through a facil- ity location optimization. The training set of 3D models is summarized into a sparse set of shapes from which we can generalize by linear combination. Given a test picture, we detect hypotheses for each part. The main challenge is to select from these hypotheses and compute the 3D pose and shape coefficients at the same time. To achieve this, we optimize a function that minimizes simultaneously the geometric reprojection error as well as the appearance matching of the parts. We apply the alternating direction method of multipliers (ADMM) to minimize the resulting convex function. We evaluate our approach on the Fine Grained 3D Car dataset with superior performance in shape and pose errors. Our main and novel contribution is the simultaneous solution for part localization, 3D pose and shape by maximizing both geometric and appearance compatibility. version:1
arxiv-1501-04691 | Tracing the boundaries of materials in transparent vessels using computer vision | http://arxiv.org/abs/1501.04691 | id:1501.04691 author:Sagi Eppel category:cs.CV  published:2015-01-20 summary:Visual recognition of material boundaries in transparent vessels is valuable for numerous applications. Such recognition is essential for estimation of fill-level, volume and phase-boundaries as well as for tracking of such chemical processes as precipitation, crystallization, condensation, evaporation and phase-separation. The problem of material boundary recognition in images is particularly complex for materials with non-flat surfaces, i.e., solids, powders and viscous fluids, in which the material interfaces have unpredictable shapes. This work demonstrates a general method for finding the boundaries of materials inside transparent containers in images. The method uses an image of the transparent vessel containing the material and the boundary of the vessel in this image. The recognition is based on the assumption that the material boundary appears in the image in the form of a curve (with various constraints) whose endpoints are both positioned on the vessel contour. The probability that a curve matches the material boundary in the image is evaluated using a cost function based on some image properties along this curve. Several image properties were examined as indicators for the material boundary. The optimal boundary curve was found using Dijkstra's algorithm. The method was successfully examined for recognition of various types of phase-boundaries, including liquid-air, solid-air and solid-liquid interfaces, as well as for various types of glassware containers from everyday life and the chemistry laboratory (i.e., bottles, beakers, flasks, jars, columns, vials and separation-funnels). In addition, the method can be easily extended to materials carried on top of carrier vessels (i.e., plates, spoons, spatulas). version:3
arxiv-1502-00141 | An evaluation framework for event detection using a morphological model of acoustic scenes | http://arxiv.org/abs/1502.00141 | id:1502.00141 author:Mathieu Lagrange, Grégoire Lafay, Mathias Rossignol, Emmanouil Benetos, Axel Roebel category:stat.ML cs.SD  published:2015-01-31 summary:This paper introduces a model of environmental acoustic scenes which adopts a morphological approach by ab-stracting temporal structures of acoustic scenes. To demonstrate its potential, this model is employed to evaluate the performance of a large set of acoustic events detection systems. This model allows us to explicitly control key morphological aspects of the acoustic scene and isolate their impact on the performance of the system under evaluation. Thus, more information can be gained on the behavior of evaluated systems, providing guidance for further improvements. The proposed model is validated using submitted systems from the IEEE DCASE Challenge; results indicate that the proposed scheme is able to successfully build datasets useful for evaluating some aspects the performance of event detection systems, more particularly their robustness to new listening conditions and the increasing level of background sounds. version:1
arxiv-1502-00133 | Sparse Dueling Bandits | http://arxiv.org/abs/1502.00133 | id:1502.00133 author:Kevin Jamieson, Sumeet Katariya, Atul Deshpande, Robert Nowak category:stat.ML cs.LG  published:2015-01-31 summary:The dueling bandit problem is a variation of the classical multi-armed bandit in which the allowable actions are noisy comparisons between pairs of arms. This paper focuses on a new approach for finding the "best" arm according to the Borda criterion using noisy comparisons. We prove that in the absence of structural assumptions, the sample complexity of this problem is proportional to the sum of the inverse squared gaps between the Borda scores of each suboptimal arm and the best arm. We explore this dependence further and consider structural constraints on the pairwise comparison matrix (a particular form of sparsity natural to this problem) that can significantly reduce the sample complexity. This motivates a new algorithm called Successive Elimination with Comparison Sparsity (SECS) that exploits sparsity to find the Borda winner using fewer samples than standard algorithms. We also evaluate the new algorithm experimentally with synthetic and real data. The results show that the sparsity model and the new algorithm can provide significant improvements over standard approaches. version:1
arxiv-1502-00130 | The Search for Computational Intelligence | http://arxiv.org/abs/1502.00130 | id:1502.00130 author:Joseph Corneli, Ewen Maclean category:cs.NE cs.AI F.1.1; I.6.3; J.4  published:2015-01-31 summary:We define and explore in simulation several rules for the local evolution of generative rules for 1D and 2D cellular automata. Our implementation uses strategies from conceptual blending. We discuss potential applications to modelling social dynamics. version:1
arxiv-1502-00115 | Optimized Projection for Sparse Representation Based Classification | http://arxiv.org/abs/1502.00115 | id:1502.00115 author:Can-Yi Lu, De-Shuang Huang category:cs.CV  published:2015-01-31 summary:Dimensionality reduction (DR) methods have been commonly used as a principled way to understand the high-dimensional data such as facial images. In this paper, we propose a new supervised DR method called Optimized Projection for Sparse Representation based Classification (OP-SRC), which is based on the recent face recognition method, Sparse Representation based Classification (SRC). SRC seeks a sparse linear combination on all the training data for a given query image, and make the decision by the minimal reconstruction residual. OP-SRC is designed on the decision rule of SRC, it aims to reduce the within-class reconstruction residual and simultaneously increase the between-class reconstruction residual on the training data. The projections are optimized and match well with the mechanism of SRC. Therefore, SRC performs well in the OP-SRC transformed space. The feasibility and effectiveness of the proposed method is verified on the Yale, ORL and UMIST databases with promising results. version:1
arxiv-1502-00094 | Twitter Hash Tag Recommendation | http://arxiv.org/abs/1502.00094 | id:1502.00094 author:Roman Dovgopol, Matt Nohelty category:cs.IR cs.LG  published:2015-01-31 summary:The rise in popularity of microblogging services like Twitter has led to increased use of content annotation strategies like the hashtag. Hashtags provide users with a tagging mechanism to help organize, group, and create visibility for their posts. This is a simple idea but can be challenging for the user in practice which leads to infrequent usage. In this paper, we will investigate various methods of recommending hashtags as new posts are created to encourage more widespread adoption and usage. Hashtag recommendation comes with numerous challenges including processing huge volumes of streaming data and content which is small and noisy. We will investigate preprocessing methods to reduce noise in the data and determine an effective method of hashtag recommendation based on the popular classification algorithms. version:1
arxiv-1502-00093 | Deep learning of fMRI big data: a novel approach to subject-transfer decoding | http://arxiv.org/abs/1502.00093 | id:1502.00093 author:Sotetsu Koyamada, Yumi Shikauchi, Ken Nakae, Masanori Koyama, Shin Ishii category:stat.ML cs.LG q-bio.NC  published:2015-01-31 summary:As a technology to read brain states from measurable brain activities, brain decoding are widely applied in industries and medical sciences. In spite of high demands in these applications for a universal decoder that can be applied to all individuals simultaneously, large variation in brain activities across individuals has limited the scope of many studies to the development of individual-specific decoders. In this study, we used deep neural network (DNN), a nonlinear hierarchical model, to construct a subject-transfer decoder. Our decoder is the first successful DNN-based subject-transfer decoder. When applied to a large-scale functional magnetic resonance imaging (fMRI) database, our DNN-based decoder achieved higher decoding accuracy than other baseline methods, including support vector machine (SVM). In order to analyze the knowledge acquired by this decoder, we applied principal sensitivity analysis (PSA) to the decoder and visualized the discriminative features that are common to all subjects in the dataset. Our PSA successfully visualized the subject-independent features contributing to the subject-transferability of the trained decoder. version:1
arxiv-1502-00082 | Category-Epitomes : Discriminatively Minimalist Representations for Object Categories | http://arxiv.org/abs/1502.00082 | id:1502.00082 author:Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu category:cs.CV  published:2015-01-31 summary:Freehand line sketches are an interesting and unique form of visual representation. Typically, such sketches are studied and utilized as an end product of the sketching process. However, we have found it instructive to study the sketches as sequentially accumulated composition of drawing strokes added over time. Studying sketches in this manner has enabled us to create novel sparse yet discriminative sketch-based representations for object categories which we term category-epitomes. Our procedure for obtaining these epitomes concurrently provides a natural measure for quantifying the sparseness underlying the original sketch, which we term epitome-score. We construct and analyze category-epitomes and epitome-scores for freehand sketches belonging to various object categories. Our analysis provides a novel viewpoint for studying the semantic nature of object categories. version:1
arxiv-1502-00064 | A Batchwise Monotone Algorithm for Dictionary Learning | http://arxiv.org/abs/1502.00064 | id:1502.00064 author:Huan Wang, John Wright, Daniel Spielman category:cs.LG  published:2015-01-31 summary:We propose a batchwise monotone algorithm for dictionary learning. Unlike the state-of-the-art dictionary learning algorithms which impose sparsity constraints on a sample-by-sample basis, we instead treat the samples as a batch, and impose the sparsity constraint on the whole. The benefit of batchwise optimization is that the non-zeros can be better allocated across the samples, leading to a better approximation of the whole. To accomplish this, we propose procedures to switch non-zeros in both rows and columns in the support of the coefficient matrix to reduce the reconstruction error. We prove in the proposed support switching procedure the objective of the algorithm, i.e., the reconstruction error, decreases monotonically and converges. Furthermore, we introduce a block orthogonal matching pursuit algorithm that also operates on sample batches to provide a warm start. Experiments on both natural image patches and UCI data sets show that the proposed algorithm produces a better approximation with the same sparsity levels compared to the state-of-the-art algorithms. version:1
arxiv-1501-07093 | Novel Approaches for Predicting Risk Factors of Atherosclerosis | http://arxiv.org/abs/1501.07093 | id:1501.07093 author:V. Sree Hari Rao, M. Naresh Kumar category:cs.LG  published:2015-01-28 summary:Coronary heart disease (CHD) caused by hardening of artery walls due to cholesterol known as atherosclerosis is responsible for large number of deaths world-wide. The disease progression is slow, asymptomatic and may lead to sudden cardiac arrest, stroke or myocardial infraction. Presently, imaging techniques are being employed to understand the molecular and metabolic activity of atherosclerotic plaques to estimate the risk. Though imaging methods are able to provide some information on plaque metabolism they lack the required resolution and sensitivity for detection. In this paper we consider the clinical observations and habits of individuals for predicting the risk factors of CHD. The identification of risk factors helps in stratifying patients for further intensive tests such as nuclear imaging or coronary angiography. We present a novel approach for predicting the risk factors of atherosclerosis with an in-built imputation algorithm and particle swarm optimization (PSO). We compare the performance of our methodology with other machine learning techniques on STULONG dataset which is based on longitudinal study of middle aged individuals lasting for twenty years. Our methodology powered by PSO search has identified physical inactivity as one of the risk factor for the onset of atherosclerosis in addition to other already known factors. The decision rules extracted by our methodology are able to predict the risk factors with an accuracy of $99.73%$ which is higher than the accuracies obtained by application of the state-of-the-art machine learning techniques presently being employed in the identification of atherosclerosis risk studies. version:2
arxiv-1502-00062 | A New Intelligence Based Approach for Computer-Aided Diagnosis of Dengue Fever | http://arxiv.org/abs/1502.00062 | id:1502.00062 author:Vadrevu Sree Hari Rao, Mallenahalli Naresh Kumar category:stat.ML cs.AI cs.LG  published:2015-01-31 summary:Identification of the influential clinical symptoms and laboratory features that help in the diagnosis of dengue fever in early phase of the illness would aid in designing effective public health management and virological surveillance strategies. Keeping this as our main objective we develop in this paper, a new computational intelligence based methodology that predicts the diagnosis in real time, minimizing the number of false positives and false negatives. Our methodology consists of three major components (i) a novel missing value imputation procedure that can be applied on any data set consisting of categorical (nominal) and/or numeric (real or integer) (ii) a wrapper based features selection method with genetic search for extracting a subset of most influential symptoms that can diagnose the illness and (iii) an alternating decision tree method that employs boosting for generating highly accurate decision rules. The predictive models developed using our methodology are found to be more accurate than the state-of-the-art methodologies used in the diagnosis of the dengue fever. version:1
arxiv-1410-7404 | Maximally Informative Hierarchical Representations of High-Dimensional Data | http://arxiv.org/abs/1410.7404 | id:1410.7404 author:Greg Ver Steeg, Aram Galstyan category:stat.ML cs.LG physics.data-an  published:2014-10-27 summary:We consider a set of probabilistic functions of some input variables as a representation of the inputs. We present bounds on how informative a representation is about input data. We extend these bounds to hierarchical representations so that we can quantify the contribution of each layer towards capturing the information in the original data. The special form of these bounds leads to a simple, bottom-up optimization procedure to construct hierarchical representations that are also maximally informative about the data. This optimization has linear computational complexity and constant sample complexity in the number of variables. These results establish a new approach to unsupervised learning of deep representations that is both principled and practical. We demonstrate the usefulness of the approach on both synthetic and real-world data. version:2
arxiv-1502-00046 | Max-Margin Object Detection | http://arxiv.org/abs/1502.00046 | id:1502.00046 author:Davis E. King category:cs.CV  published:2015-01-31 summary:Most object detection methods operate by applying a binary classifier to sub-windows of an image, followed by a non-maximum suppression step where detections on overlapping sub-windows are removed. Since the number of possible sub-windows in even moderately sized image datasets is extremely large, the classifier is typically learned from only a subset of the windows. This avoids the computational difficulty of dealing with the entire set of sub-windows, however, as we will show in this paper, it leads to sub-optimal detector performance. In particular, the main contribution of this paper is the introduction of a new method, Max-Margin Object Detection (MMOD), for learning to detect objects in images. This method does not perform any sub-sampling, but instead optimizes over all sub-windows. MMOD can be used to improve any object detection method which is linear in the learned parameters, such as HOG or bag-of-visual-word models. Using this approach we show substantial performance gains on three publicly available datasets. Strikingly, we show that a single rigid HOG filter can outperform a state-of-the-art deformable part model on the Face Detection Data Set and Benchmark when the HOG filter is learned via MMOD. version:1
arxiv-1409-3870 | Text mixing shapes the anatomy of rank-frequency distributions: A modern Zipfian mechanics for natural language | http://arxiv.org/abs/1409.3870 | id:1409.3870 author:Jake Ryland Williams, James P. Bagrow, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL physics.soc-ph  published:2014-09-12 summary:Natural languages are full of rules and exceptions. One of the most famous quantitative rules is Zipf's law which states that the frequency of occurrence of a word is approximately inversely proportional to its rank. Though this `law' of ranks has been found to hold across disparate texts and forms of data, analyses of increasingly large corpora over the last 15 years have revealed the existence of two scaling regimes. These regimes have thus far been explained by a hypothesis suggesting a separability of languages into core and non-core lexica. Here, we present and defend an alternative hypothesis, that the two scaling regimes result from the act of aggregating texts. We observe that text mixing leads to an effective decay of word introduction, which we show provides accurate predictions of the location and severity of breaks in scaling. Upon examining large corpora from 10 languages in the Project Gutenberg eBooks collection (eBooks), we find emphatic empirical support for the universality of our claim. version:3
arxiv-1501-06633 | maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell GPUs | http://arxiv.org/abs/1501.06633 | id:1501.06633 author:Andrew Lavin category:cs.NE cs.DC cs.LG  published:2015-01-27 summary:This paper describes maxDNN, a computationally efficient convolution kernel for deep learning with the NVIDIA Maxwell GPU. maxDNN reaches 96.3% computational efficiency on typical deep learning network architectures. The design combines ideas from cuda-convnet2 with the Maxas SGEMM assembly code. We only address forward propagation (FPROP) operation of the network, but we believe that the same techniques used here will be effective for backward propagation (BPROP) as well. version:3
arxiv-1502-00030 | SHOE: Supervised Hashing with Output Embeddings | http://arxiv.org/abs/1502.00030 | id:1502.00030 author:Sravanthi Bondugula, Varun Manjunatha, Larry S. Davis, David Doermann category:cs.CV  published:2015-01-30 summary:We present a supervised binary encoding scheme for image retrieval that learns projections by taking into account similarity between classes obtained from output embeddings. Our motivation is that binary hash codes learned in this way improve both the visual quality of retrieval results and existing supervised hashing schemes. We employ a sequential greedy optimization that learns relationship aware projections by minimizing the difference between inner products of binary codes and output embedding vectors. We develop a joint optimization framework to learn projections which improve the accuracy of supervised hashing over the current state of the art with respect to standard and sibling evaluation metrics. We further boost performance by applying the supervised dimensionality reduction technique on kernelized input CNN features. Experiments are performed on three datasets: CUB-2011, SUN-Attribute and ImageNet ILSVRC 2010. As a by-product of our method, we show that using a simple k-nn pooling classifier with our discriminative codes improves over the complex classification models on fine grained datasets like CUB and offer an impressive compression ratio of 1024 on CNN features. version:1
arxiv-1501-04878 | A Light Transport Model for Mitigating Multipath Interference in TOF Sensors | http://arxiv.org/abs/1501.04878 | id:1501.04878 author:Nikhil Naik, Achuta Kadambi, Christoph Rhemann, Shahram Izadi, Ramesh Raskar, Sing Bing Kang category:cs.CV  published:2015-01-20 summary:Continuous-wave Time-of-flight (TOF) range imaging has become a commercially viable technology with many applications in computer vision and graphics. However, the depth images obtained from TOF cameras contain scene dependent errors due to multipath interference (MPI). Specifically, MPI occurs when multiple optical reflections return to a single spatial location on the imaging sensor. Many prior approaches to rectifying MPI rely on sparsity in optical reflections, which is an extreme simplification. In this paper, we correct MPI by combining the standard measurements from a TOF camera with information from direct and global light transport. We report results on both simulated experiments and physical experiments (using the Kinect sensor). Our results, evaluated against ground truth, demonstrate a quantitative improvement in depth accuracy. version:2
arxiv-1501-05703 | Beyond Frontal Faces: Improving Person Recognition Using Multiple Cues | http://arxiv.org/abs/1501.05703 | id:1501.05703 author:Ning Zhang, Manohar Paluri, Yaniv Taigman, Rob Fergus, Lubomir Bourdev category:cs.CV  published:2015-01-23 summary:We explore the task of recognizing peoples' identities in photo albums in an unconstrained setting. To facilitate this, we introduce the new People In Photo Albums (PIPA) dataset, consisting of over 60000 instances of 2000 individuals collected from public Flickr photo albums. With only about half of the person images containing a frontal face, the recognition task is very challenging due to the large variations in pose, clothing, camera viewpoint, image resolution and illumination. We propose the Pose Invariant PErson Recognition (PIPER) method, which accumulates the cues of poselet-level person recognizers trained by deep convolutional networks to discount for the pose variations, combined with a face recognizer and a global recognizer. Experiments on three different settings confirm that in our unconstrained setup PIPER significantly improves on the performance of DeepFace, which is one of the best face recognizers as measured on the LFW dataset. version:2
arxiv-1501-07867 | Multi-task Image Classification via Collaborative, Hierarchical Spike-and-Slab Priors | http://arxiv.org/abs/1501.07867 | id:1501.07867 author:Hojjat Seyed Mousavi, Umamahesh Srinivas, Vishal Monga, Yuanming Suo, Minh Dao, Trac. D. Tran category:cs.CV  published:2015-01-30 summary:Promising results have been achieved in image classification problems by exploiting the discriminative power of sparse representations for classification (SRC). Recently, it has been shown that the use of \emph{class-specific} spike-and-slab priors in conjunction with the class-specific dictionaries from SRC is particularly effective in low training scenarios. As a logical extension, we build on this framework for multitask scenarios, wherein multiple representations of the same physical phenomena are available. We experimentally demonstrate the benefits of mining joint information from different camera views for multi-view face recognition. version:1
arxiv-1501-07862 | An Analytical Study of different Document Image Binarization Methods | http://arxiv.org/abs/1501.07862 | id:1501.07862 author:Mahua Nandy, Satadal Saha category:cs.CV  published:2015-01-30 summary:Document image has been the area of research for a couple of decades because of its potential application in the area of text recognition, line recognition or any other shape recognition from the image. For most of these purposes binarization of image becomes mandatory as far as recognition is concerned. Throughout couple decades standard algorithms have already been developed for this purpose. Some of these algorithms are applicable to degraded image also. Our objective behind this work is to study the existing techniques, compare them in view of advantages and disadvantages and modify some of these algorithms to optimize time or performance. version:1
arxiv-1412-0494 | Orthogonal Matrix Retrieval in Cryo-Electron Microscopy | http://arxiv.org/abs/1412.0494 | id:1412.0494 author:Tejal Bhamre, Teng Zhang, Amit Singer category:cs.CV  published:2014-12-01 summary:In single particle reconstruction (SPR) from cryo-electron microscopy (cryo-EM), the 3D structure of a molecule needs to be determined from its 2D projection images taken at unknown viewing directions. Zvi Kam showed already in 1980 that the autocorrelation function of the 3D molecule over the rotation group SO(3) can be estimated from 2D projection images whose viewing directions are uniformly distributed over the sphere. The autocorrelation function determines the expansion coefficients of the 3D molecule in spherical harmonics up to an orthogonal matrix of size $(2l+1)\times (2l+1)$ for each $l=0,1,2,...$. In this paper we show how techniques for solving the phase retrieval problem in X-ray crystallography can be modified for the cryo-EM setup for retrieving the missing orthogonal matrices. Specifically, we present two new approaches that we term Orthogonal Extension and Orthogonal Replacement, in which the main algorithmic components are the singular value decomposition and semidefinite programming. We demonstrate the utility of these approaches through numerical experiments on simulated data. version:2
arxiv-1501-07844 | A Proximal Bregman Projection Approach to Continuous Max-Flow Problems Using Entropic Distances | http://arxiv.org/abs/1501.07844 | id:1501.07844 author:John S. H. Baxter, Martin Rajchl, Jing Yuan, Terry M. Peters category:cs.CV  published:2015-01-30 summary:One issue limiting the adaption of large-scale multi-region segmentation is the sometimes prohibitive memory requirements. This is especially troubling considering advances in massively parallel computing and commercial graphics processing units because of their already limited memory compared to the current random access memory used in more traditional computation. To address this issue in the field of continuous max-flow segmentation, we have developed a \textit{pseudo-flow} framework using the theory of Bregman proximal projections and entropic distances which implicitly represents flow variables between labels and designated source and sink nodes. This reduces the memory requirements for max-flow segmentation by approximately 20\% for Potts models and approximately 30\% for hierarchical max-flow (HMF) and directed acyclic graph max-flow (DAGMF) models. This represents a great improvement in the state-of-the-art in max-flow segmentation, allowing for much larger problems to be addressed and accelerated using commercially available graphics processing hardware. version:1
arxiv-1407-0316 | Significant Subgraph Mining with Multiple Testing Correction | http://arxiv.org/abs/1407.0316 | id:1407.0316 author:Mahito Sugiyama, Felipe Llinares López, Niklas Kasenburg, Karsten M. Borgwardt category:stat.ME cs.LG stat.ML  published:2014-07-01 summary:The problem of finding itemsets that are statistically significantly enriched in a class of transactions is complicated by the need to correct for multiple hypothesis testing. Pruning untestable hypotheses was recently proposed as a strategy for this task of significant itemset mining. It was shown to lead to greater statistical power, the discovery of more truly significant itemsets, than the standard Bonferroni correction on real-world datasets. An open question, however, is whether this strategy of excluding untestable hypotheses also leads to greater statistical power in subgraph mining, in which the number of hypotheses is much larger than in itemset mining. Here we answer this question by an empirical investigation on eight popular graph benchmark datasets. We propose a new efficient search strategy, which always returns the same solution as the state-of-the-art approach and is approximately two orders of magnitude faster. Moreover, we exploit the dependence between subgraphs by considering the effective number of tests and thereby further increase the statistical power. version:3
arxiv-1503-07460 | RANSAC based three points algorithm for ellipse fitting of spherical object's projection | http://arxiv.org/abs/1503.07460 | id:1503.07460 author:Shenghui Xu category:cs.CV  published:2015-01-30 summary:As the spherical object can be seen everywhere, we should extract the ellipse image accurately and fit it by implicit algebraic curve in order to finish the 3D reconstruction. In this paper, we propose a new ellipse fitting algorithm which only needs three points to fit the projection of spherical object and is different from the traditional algorithms that need at least five point. The fitting procedure is just similar as the estimation of Fundamental Matrix estimation by seven points, and the RANSAC algorithm has also been used to exclude the interference of noise and scattered points. version:1
arxiv-1501-07768 | Confidence intervals for AB-test | http://arxiv.org/abs/1501.07768 | id:1501.07768 author:Cyrille Dubarry category:stat.ML  published:2015-01-30 summary:AB-testing is a very popular technique in web companies since it makes it possible to accurately predict the impact of a modification with the simplicity of a random split across users. One of the critical aspects of an AB-test is its duration and it is important to reliably compute confidence intervals associated with the metric of interest to know when to stop the test. In this paper, we define a clean mathematical framework to model the AB-test process. We then propose three algorithms based on bootstrapping and on the central limit theorem to compute reliable confidence intervals which extend to other metrics than the common probabilities of success. They apply to both absolute and relative increments of the most used comparison metrics, including the number of occurrences of a particular event and a click-through rate implying a ratio. version:1
arxiv-1501-07758 | Gibbs-Ringing Artifact Removal Based on Local Subvoxel-shifts | http://arxiv.org/abs/1501.07758 | id:1501.07758 author:Elias Kellner, Bibek Dhital, Marco Reisert category:physics.med-ph cs.CV  published:2015-01-30 summary:Gibbs-ringing is a well known artifact which manifests itself as spurious oscillations in the vicinity of sharp image transients, e.g. at tissue boundaries. The origin can be seen in the truncation of k-space during MRI data-acquisition. Consequently, correction techniques like Gegenbauer reconstruction or extrapolation methods aim at recovering these missing data. Here, we present a simple and robust method which exploits a different view on the Gibbs-phenomena. The truncation in k-space can be interpreted as a convolution with a sinc-function in image space. Hence, the severity of the artifacts depends on how the sinc-function is sampled. We propose to re-interpolate the image based on local, subvoxel shifts to sample the ringing pattern at the zero-crossings of the oscillating sinc-function. With this, the artifact can effectively and robustly be removed with a minimal amount of smoothing. version:1
arxiv-1501-07738 | Co-Regularized Deep Representations for Video Summarization | http://arxiv.org/abs/1501.07738 | id:1501.07738 author:Olivier Morère, Hanlin Goh, Antoine Veillard, Vijay Chandrasekhar, Jie Lin category:cs.CV  published:2015-01-30 summary:Compact keyframe-based video summaries are a popular way of generating viewership on video sharing platforms. Yet, creating relevant and compelling summaries for arbitrarily long videos with a small number of keyframes is a challenging task. We propose a comprehensive keyframe-based summarization framework combining deep convolutional neural networks and restricted Boltzmann machines. An original co-regularization scheme is used to discover meaningful subject-scene associations. The resulting multimodal representations are then used to select highly-relevant keyframes. A comprehensive user study is conducted comparing our proposed method to a variety of schemes, including the summarization currently in use by one of the most popular video sharing websites. The results show that our method consistently outperforms the baseline schemes for any given amount of keyframes both in terms of attractiveness and informativeness. The lead is even more significant for smaller summaries. version:1
arxiv-1501-07692 | Blob indentation identification via curvature measurement | http://arxiv.org/abs/1501.07692 | id:1501.07692 author:Matthew Sottile category:cs.CV  published:2015-01-30 summary:This paper presents a novel method for identifying indentations on the boundary of solid 2D shape. It uses the signed curvature at a set of points along the boundary to identify indentations and provides one parameter for tuning the selection mechanism for discriminating indentations from other boundary irregularities. An efficient implementation is described based on the Fourier transform for calculating curvature from a sequence of points obtained from the boundary of a binary blob. version:1
arxiv-1411-6243 | Structure Regularization for Structured Prediction: Theories and Experiments | http://arxiv.org/abs/1411.6243 | id:1411.6243 author:Xu Sun category:cs.LG  published:2014-11-23 summary:While there are many studies on weight regularization, the study on structure regularization is rare. Many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. However, this trend could have been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. To control structure-based overfitting, we propose a structure regularization framework via \emph{structure decomposition}, which decomposes training samples into mini-samples with simpler structures, deriving a model with better generalization power. We show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead to better accuracy. As a by-product, the proposed method can also substantially accelerate the training speed. The method and the theoretical results can apply to general graphical models with arbitrary structures. Experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks, achieving state-of-the-art accuracies yet with substantially faster training speed. version:2
arxiv-1501-07683 | Downscaling Microwave Brightness Temperatures Using Self Regularized Regressive Models | http://arxiv.org/abs/1501.07683 | id:1501.07683 author:Subit Chakrabarti, Jasmeet Judge, Anand Rangarajan, Sanjay Ranka category:cs.CV 68  published:2015-01-30 summary:A novel algorithm is proposed to downscale microwave brightness temperatures ($\mathrm{T_B}$), at scales of 10-40 km such as those from the Soil Moisture Active Passive mission to a resolution meaningful for hydrological and agricultural applications. This algorithm, called Self-Regularized Regressive Models (SRRM), uses auxiliary variables correlated to $\mathrm{T_B}$ along-with a limited set of \textit{in-situ} SM observations, which are converted to high resolution $\mathrm{T_B}$ observations using biophysical models. It includes an information-theoretic clustering step based on all auxiliary variables to identify areas of similarity, followed by a kernel regression step that produces downscaled $\mathrm{T_B}$. This was implemented on a multi-scale synthetic data-set over NC-Florida for one year. An RMSE of 5.76~K with standard deviation of 2.8~k was achieved during the vegetated season and an RMSE of 1.2~K with a standard deviation of 0.9~K during periods of no vegetation. version:1
arxiv-1501-07681 | Vector Quantization by Minimizing Kullback-Leibler Divergence | http://arxiv.org/abs/1501.07681 | id:1501.07681 author:Lan Yang, Jingbin Wang, Yujin Tu, Prarthana Mahapatra, Nelson Cardoso category:cs.CV  published:2015-01-30 summary:This paper proposes a new method for vector quantization by minimizing the Kullback-Leibler Divergence between the class label distributions over the quantization inputs, which are original vectors, and the output, which is the quantization subsets of the vector set. In this way, the vector quantization output can keep as much information of the class label as possible. An objective function is constructed and we also developed an iterative algorithm to minimize it. The new method is evaluated on bag-of-features based image classification problem. version:1
arxiv-1501-07676 | Towards Resolving Software Quality-in-Use Measurement Challenges | http://arxiv.org/abs/1501.07676 | id:1501.07676 author:Issa Atoum, Chih How Bong, Narayanan Kulathuramaiyer category:cs.SE cs.CL  published:2015-01-30 summary:Software quality-in-use comprehends the quality from user's perspectives. It has gained its importance in e-learning applications, mobile service based applications and project management tools. User's decisions on software acquisitions are often ad hoc or based on preference due to difficulty in quantitatively measure software quality-in-use. However, why quality-in-use measurement is difficult? Although there are many software quality models to our knowledge, no works surveys the challenges related to software quality-in-use measurement. This paper has two main contributions; 1) presents major issues and challenges in measuring software quality-in-use in the context of the ISO SQuaRE series and related software quality models, 2) Presents a novel framework that can be used to predict software quality-in-use, and 3) presents preliminary results of quality-in-use topic prediction. Concisely, the issues are related to the complexity of the current standard models and the limitations and incompleteness of the customized software quality models. The proposed framework employs sentiment analysis techniques to predict software quality-in-use. version:1
arxiv-1409-0575 | ImageNet Large Scale Visual Recognition Challenge | http://arxiv.org/abs/1409.0575 | id:1409.0575 author:Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei category:cs.CV I.4.8; I.5.2  published:2014-09-01 summary:The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements. version:3
arxiv-1501-07627 | Representing Objects, Relations, and Sequences | http://arxiv.org/abs/1501.07627 | id:1501.07627 author:Stephen I. Gallant, T. Wendy Okaywe category:cs.LG  published:2015-01-29 summary:Vector Symbolic Architectures (VSAs) are high-dimensional vector representations of objects (eg., words, image parts), relations (eg., sentence structures), and sequences for use with machine learning algorithms. They consist of a vector addition operator for representing a collection of unordered objects, a Binding operator for associating groups of objects, and a methodology for encoding complex structures. We first develop Constraints that machine learning imposes upon VSAs: for example, similar structures must be represented by similar vectors. The constraints suggest that current VSAs should represent phrases ("The smart Brazilian girl") by binding sums of terms, in addition to simply binding the terms directly. We show that matrix multiplication can be used as the binding operator for a VSA, and that matrix elements can be chosen at random. A consequence for living systems is that binding is mathematically possible without the need to specify, in advance, precise neuron-to-neuron connection properties for large numbers of synapses. A VSA that incorporates these ideas, MBAT (Matrix Binding of Additive Terms), is described that satisfies all Constraints. With respect to machine learning, for some types of problems appropriate VSA representations permit us to prove learnability, rather than relying on simulations. We also propose dividing machine (and neural) learning and representation into three Stages, with differing roles for learning in each stage. For neural modeling, we give "representational reasons" for nervous systems to have many recurrent connections, as well as for the importance of phrases in language processing. Sizing simulations and analyses suggest that VSAs in general, and MBAT in particular, are ready for real-world applications. version:1
arxiv-1410-1980 | Deep Representations for Iris, Face, and Fingerprint Spoofing Detection | http://arxiv.org/abs/1410.1980 | id:1410.1980 author:David Menotti, Giovani Chiachia, Allan Pinto, William Robson Schwartz, Helio Pedrini, Alexandre Xavier Falcao, Anderson Rocha category:cs.CV  published:2014-10-08 summary:Biometrics systems have significantly improved person identification and authentication, playing an important role in personal, national, and global security. However, these systems might be deceived (or "spoofed") and, despite the recent advances in spoofing detection, current solutions often rely on domain knowledge, specific biometric reading systems, and attack types. We assume a very limited knowledge about biometric spoofing at the sensor to derive outstanding spoofing detection systems for iris, face, and fingerprint modalities based on two deep learning approaches. The first approach consists of learning suitable convolutional network architectures for each domain, while the second approach focuses on learning the weights of the network via back-propagation. We consider nine biometric spoofing benchmarks --- each one containing real and fake samples of a given biometric modality and attack type --- and learn deep representations for each benchmark by combining and contrasting the two learning approaches. This strategy not only provides better comprehension of how these approaches interplay, but also creates systems that exceed the best known results in eight out of the nine benchmarks. The results strongly indicate that spoofing detection systems based on convolutional networks can be robust to attacks already known and possibly adapted, with little effort, to image-based attacks that are yet to come. version:3
arxiv-1501-07584 | Efficient Divide-And-Conquer Classification Based on Feature-Space Decomposition | http://arxiv.org/abs/1501.07584 | id:1501.07584 author:Qi Guo, Bo-Wei Chen, Feng Jiang, Xiangyang Ji, Sun-Yuan Kung category:cs.LG  published:2015-01-29 summary:This study presents a divide-and-conquer (DC) approach based on feature space decomposition for classification. When large-scale datasets are present, typical approaches usually employed truncated kernel methods on the feature space or DC approaches on the sample space. However, this did not guarantee separability between classes, owing to overfitting. To overcome such problems, this work proposes a novel DC approach on feature spaces consisting of three steps. Firstly, we divide the feature space into several subspaces using the decomposition method proposed in this paper. Subsequently, these feature subspaces are sent into individual local classifiers for training. Finally, the outcomes of local classifiers are fused together to generate the final classification results. Experiments on large-scale datasets are carried out for performance evaluation. The results show that the error rates of the proposed DC method decreased comparing with the state-of-the-art fast SVM solvers, e.g., reducing error rates by 10.53% and 7.53% on RCV1 and covtype datasets respectively. version:1
arxiv-1501-07518 | High-Dimensional Longitudinal Classification with the Multinomial Fused Lasso | http://arxiv.org/abs/1501.07518 | id:1501.07518 author:Samrachana Adhikari, Fabrizio Lecci, James T. Becker, Brian W. Junker, Lewis H. Kuller, Oscar L. Lopez, Ryan J. Tibshirani category:stat.AP stat.ML  published:2015-01-29 summary:We study regularized estimation in high-dimensional longitudinal classification problems, using the lasso and fused lasso regularizers. The constructed coefficient estimates are piecewise constant across the time dimension in the longitudinal problem, with adaptively selected change points (break points). We present an efficient algorithm for computing such estimates, based on proximal gradient descent. We apply our proposed technique to a longitudinal data set on Alzheimer's disease from the Cardiovascular Health Study Cognition Study, and use this data set to motivate and demonstrate several practical considerations such as the selection of tuning parameters, and the assessment of model stability. version:1
arxiv-1501-07496 | Implementation of an Automatic Syllabic Division Algorithm from Speech Files in Portuguese Language | http://arxiv.org/abs/1501.07496 | id:1501.07496 author:E. L. F. Da Silva, H. M. de Oliveira category:cs.SD cs.CL cs.DS  published:2015-01-29 summary:A new algorithm for voice automatic syllabic splitting in the Portuguese language is proposed, which is based on the envelope of the speech signal of the input audio file. A computational implementation in MatlabTM is presented and made available at the URL http://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its straightforwardness, the proposed method is very attractive for embedded systems (e.g. i-phones). It can also be used as a screen to assist more sophisticated methods. Voice excerpts containing more than one syllable and identified by the same envelope are named as super-syllables and they are subsequently separated. The results indicate which samples corresponds to the beginning and end of each detected syllable. Preliminary tests were performed to fifty words at an identification rate circa 70% (further improvements may be incorporated to treat particular phonemes). This algorithm is also useful in voice command systems, as a tool in the teaching of Portuguese language or even for patients with speech pathology. version:1
arxiv-1501-07467 | Regression and Learning to Rank Aggregation for User Engagement Evaluation | http://arxiv.org/abs/1501.07467 | id:1501.07467 author:Hamed Zamani, Azadeh Shakery, Pooya Moradi category:cs.IR cs.LG H.2.8; J.4  published:2015-01-29 summary:User engagement refers to the amount of interaction an instance (e.g., tweet, news, and forum post) achieves. Ranking the items in social media websites based on the amount of user participation in them, can be used in different applications, such as recommender systems. In this paper, we consider a tweet containing a rating for a movie as an instance and focus on ranking the instances of each user based on their engagement, i.e., the total number of retweets and favorites it will gain. For this task, we define several features which can be extracted from the meta-data of each tweet. The features are partitioned into three categories: user-based, movie-based, and tweet-based. We show that in order to obtain good results, features from all categories should be considered. We exploit regression and learning to rank methods to rank the tweets and propose to aggregate the results of regression and learning to rank methods to achieve better performance. We have run our experiments on an extended version of MovieTweeting dataset provided by ACM RecSys Challenge 2014. The results show that learning to rank approach outperforms most of the regression models and the combination can improve the performance significantly. version:1
arxiv-1501-07422 | Pairwise Rotation Hashing for High-dimensional Features | http://arxiv.org/abs/1501.07422 | id:1501.07422 author:Kohta Ishikawa, Ikuro Sato, Mitsuru Ambai category:cs.CV stat.ML  published:2015-01-29 summary:Binary Hashing is widely used for effective approximate nearest neighbors search. Even though various binary hashing methods have been proposed, very few methods are feasible for extremely high-dimensional features often used in visual tasks today. We propose a novel highly sparse linear hashing method based on pairwise rotations. The encoding cost of the proposed algorithm is $\mathrm{O}(n \log n)$ for n-dimensional features, whereas that of the existing state-of-the-art method is typically $\mathrm{O}(n^2)$. The proposed method is also remarkably faster in the learning phase. Along with the efficiency, the retrieval accuracy is comparable to or slightly outperforming the state-of-the-art. Pairwise rotations used in our method are formulated from an analytical study of the trade-off relationship between quantization error and entropy of binary codes. Although these hashing criteria are widely used in previous researches, its analytical behavior is rarely studied. All building blocks of our algorithm are based on the analytical solution, and it thus provides a fairly simple and efficient procedure. version:1
arxiv-1501-07399 | Particle swarm optimization for time series motif discovery | http://arxiv.org/abs/1501.07399 | id:1501.07399 author:Joan Serrà, Josep Lluis Arcos category:cs.LG cs.NE  published:2015-01-29 summary:Efficiently finding similar segments or motifs in time series data is a fundamental task that, due to the ubiquity of these data, is present in a wide range of domains and situations. Because of this, countless solutions have been devised but, to date, none of them seems to be fully satisfactory and flexible. In this article, we propose an innovative standpoint and present a solution coming from it: an anytime multimodal optimization algorithm for time series motif discovery based on particle swarms. By considering data from a variety of domains, we show that this solution is extremely competitive when compared to the state-of-the-art, obtaining comparable motifs in considerably less time using minimal memory. In addition, we show that it is robust to different implementation choices and see that it offers an unprecedented degree of flexibility with regard to the task. All these qualities make the presented solution stand out as one of the most prominent candidates for motif discovery in long time series streams. Besides, we believe the proposed standpoint can be exploited in further time series analysis and mining tasks, widening the scope of research and potentially yielding novel effective solutions. version:1
arxiv-1107-4042 | Optimal Adaptive Learning in Uncontrolled Restless Bandit Problems | http://arxiv.org/abs/1107.4042 | id:1107.4042 author:Cem Tekin, Mingyan Liu category:math.OC cs.LG  published:2011-07-20 summary:In this paper we consider the problem of learning the optimal policy for uncontrolled restless bandit problems. In an uncontrolled restless bandit problem, there is a finite set of arms, each of which when pulled yields a positive reward. There is a player who sequentially selects one of the arms at each time step. The goal of the player is to maximize its undiscounted reward over a time horizon T. The reward process of each arm is a finite state Markov chain, whose transition probabilities are unknown by the player. State transitions of each arm is independent of the selection of the player. We propose a learning algorithm with logarithmic regret uniformly over time with respect to the optimal finite horizon policy. Our results extend the optimal adaptive learning of MDPs to POMDPs. version:3
arxiv-1307-0473 | Online discrete optimization in social networks in the presence of Knightian uncertainty | http://arxiv.org/abs/1307.0473 | id:1307.0473 author:Maxim Raginsky, Angelia Nedić category:math.OC cs.DC cs.LG  published:2013-07-01 summary:We study a model of collective real-time decision-making (or learning) in a social network operating in an uncertain environment, for which no a priori probabilistic model is available. Instead, the environment's impact on the agents in the network is seen through a sequence of cost functions, revealed to the agents in a causal manner only after all the relevant actions are taken. There are two kinds of costs: individual costs incurred by each agent and local-interaction costs incurred by each agent and its neighbors in the social network. Moreover, agents have inertia: each agent has a default mixed strategy that stays fixed regardless of the state of the environment, and must expend effort to deviate from this strategy in order to respond to cost signals coming from the environment. We construct a decentralized strategy, wherein each agent selects its action based only on the costs directly affecting it and on the decisions made by its neighbors in the network. In this setting, we quantify social learning in terms of regret, which is given by the difference between the realized network performance over a given time horizon and the best performance that could have been achieved in hindsight by a fictitious centralized entity with full knowledge of the environment's evolution. We show that our strategy achieves the regret that scales polylogarithmically with the time horizon and polynomially with the number of agents and the maximum number of neighbors of any agent in the social network. version:2
arxiv-1501-07340 | Sequential Probability Assignment with Binary Alphabets and Large Classes of Experts | http://arxiv.org/abs/1501.07340 | id:1501.07340 author:Alexander Rakhlin, Karthik Sridharan category:cs.IT cs.LG math.IT stat.ML  published:2015-01-29 summary:We analyze the problem of sequential probability assignment for binary outcomes with side information and logarithmic loss, where regret---or, redundancy---is measured with respect to a (possibly infinite) class of experts. We provide upper and lower bounds for minimax regret in terms of sequential complexities of the class. These complexities were recently shown to give matching (up to logarithmic factors) upper and lower bounds for sequential prediction with general convex Lipschitz loss functions (Rakhlin and Sridharan, 2015). To deal with unbounded gradients of the logarithmic loss, we present a new analysis that employs a sequential chaining technique with a Bernstein-type bound. The introduced complexities are intrinsic to the problem of sequential probability assignment, as illustrated by our lower bound. We also consider an example of a large class of experts parametrized by vectors in a high-dimensional Euclidean ball (or a Hilbert ball). The typical discretization approach fails, while our techniques give a non-trivial bound. For this problem we also present an algorithm based on regularization with a self-concordant barrier. This algorithm is of an independent interest, as it requires a bound on the function values rather than gradients. version:1
arxiv-1407-0010 | Pixel-wise Orthogonal Decomposition for Color Illumination Invariant and Shadow-free Image | http://arxiv.org/abs/1407.0010 | id:1407.0010 author:Liangqiong Qu, Jiandong Tian, Zhi Han, Yandong Tang category:cs.CV  published:2014-06-30 summary:In this paper, we propose a novel, effective and fast method to obtain a color illumination invariant and shadow-free image from a single outdoor image. Different from state-of-the-art methods for shadow-free image that either need shadow detection or statistical learning, we set up a linear equation set for each pixel value vector based on physically-based shadow invariants, deduce a pixel-wise orthogonal decomposition for its solutions, and then get an illumination invariant vector for each pixel value vector on an image. The illumination invariant vector is the unique particular solution of the linear equation set, which is orthogonal to its free solutions. With this illumination invariant vector and Lab color space, we propose an algorithm to generate a shadow-free image which well preserves the texture and color information of the original image. A series of experiments on a diverse set of outdoor images and the comparisons with the state-of-the-art methods validate our method. version:2
arxiv-1501-07338 | On Vectorization of Deep Convolutional Neural Networks for Vision Tasks | http://arxiv.org/abs/1501.07338 | id:1501.07338 author:Jimmy SJ. Ren, Li Xu category:cs.CV  published:2015-01-29 summary:We recently have witnessed many ground-breaking results in machine learning and computer vision, generated by using deep convolutional neural networks (CNN). While the success mainly stems from the large volume of training data and the deep network architectures, the vector processing hardware (e.g. GPU) undisputedly plays a vital role in modern CNN implementations to support massive computation. Though much attention was paid in the extent literature to understand the algorithmic side of deep CNN, little research was dedicated to the vectorization for scaling up CNNs. In this paper, we studied the vectorization process of key building blocks in deep CNNs, in order to better understand and facilitate parallel implementation. Key steps in training and testing deep CNNs are abstracted as matrix and vector operators, upon which parallelism can be easily achieved. We developed and compared six implementations with various degrees of vectorization with which we illustrated the impact of vectorization on the speed of model training and testing. Besides, a unified CNN framework for both high-level and low-level vision tasks is provided, along with a vectorized Matlab implementation with state-of-the-art speed performance. version:1
arxiv-1501-07315 | Per-Block-Convex Data Modeling by Accelerated Stochastic Approximation | http://arxiv.org/abs/1501.07315 | id:1501.07315 author:Konstantinos Slavakis, Georgios B. Giannakis category:cs.LG  published:2015-01-29 summary:Applications involving dictionary learning, non-negative matrix factorization, subspace clustering, and parallel factor tensor decomposition tasks motivate well algorithms for per-block-convex and non-smooth optimization problems. By leveraging the stochastic approximation paradigm and first-order acceleration schemes, this paper develops an online and modular learning algorithm for a large class of non-convex data models, where convexity is manifested only per-block of variables whenever the rest of them are held fixed. The advocated algorithm incurs computational complexity that scales linearly with the number of unknowns. Under minimal assumptions on the cost functions of the composite optimization task, without bounding constraints on the optimization variables, or any explicit information on bounds of Lipschitz coefficients, the expected cost evaluated online at the resultant iterates is provably convergent with quadratic rate to an accumulation point of the (per-block) minima, while subgradients of the expected cost asymptotically vanish in the mean-squared sense. The merits of the general approach are demonstrated in two online learning setups: (i) Robust linear regression using a sparsity-cognizant total least-squares criterion; and (ii) semi-supervised dictionary learning for network-wide link load tracking and imputation with missing entries. Numerical tests on synthetic and real data highlight the potential of the proposed framework for streaming data analytics by demonstrating superior performance over block coordinate descent, and reduced complexity relative to the popular alternating-direction method of multipliers. version:1
arxiv-1411-5404 | Stochastic Block Transition Models for Dynamic Networks | http://arxiv.org/abs/1411.5404 | id:1411.5404 author:Kevin S. Xu category:cs.SI cs.LG physics.soc-ph stat.ME  published:2014-11-19 summary:There has been great interest in recent years on statistical models for dynamic networks. In this paper, I propose a stochastic block transition model (SBTM) for dynamic networks that is inspired by the well-known stochastic block model (SBM) for static networks and previous dynamic extensions of the SBM. Unlike most existing dynamic network models, it does not make a hidden Markov assumption on the edge-level dynamics, allowing the presence or absence of edges to directly influence future edge probabilities while retaining the interpretability of the SBM. I derive an approximate inference procedure for the SBTM and demonstrate that it is significantly better at reproducing durations of edges in real social network data. version:2
arxiv-1501-07304 | The Beauty of Capturing Faces: Rating the Quality of Digital Portraits | http://arxiv.org/abs/1501.07304 | id:1501.07304 author:Miriam Redi, Nikhil Rasiwasia, Gaurav Aggarwal, Alejandro Jaimes category:cs.CV cs.CY cs.MM  published:2015-01-28 summary:Digital portrait photographs are everywhere, and while the number of face pictures keeps growing, not much work has been done to on automatic portrait beauty assessment. In this paper, we design a specific framework to automatically evaluate the beauty of digital portraits. To this end, we procure a large dataset of face images annotated not only with aesthetic scores but also with information about the traits of the subject portrayed. We design a set of visual features based on portrait photography literature, and extensively analyze their relation with portrait beauty, exposing interesting findings about what makes a portrait beautiful. We find that the beauty of a portrait is linked to its artistic value, and independent from age, race and gender of the subject. We also show that a classifier trained with our features to separate beautiful portraits from non-beautiful portraits outperforms generic aesthetic classifiers. version:1
arxiv-0901-4180 | Google distance between words | http://arxiv.org/abs/0901.4180 | id:0901.4180 author:Bjørn Kjos-Hanssen, Alberto J. Evangelista category:cs.CL I.2.7  published:2009-01-27 summary:Cilibrasi and Vitanyi have demonstrated that it is possible to extract the meaning of words from the world-wide web. To achieve this, they rely on the number of webpages that are found through a Google search containing a given word and they associate the page count to the probability that the word appears on a webpage. Thus, conditional probabilities allow them to correlate one word with another word's meaning. Furthermore, they have developed a similarity distance function that gauges how closely related a pair of words is. We present a specific counterexample to the triangle inequality for this similarity distance function. version:2
arxiv-1412-1271 | Deep Distributed Random Samplings for Supervised Learning: An Alternative to Random Forests? | http://arxiv.org/abs/1412.1271 | id:1412.1271 author:Xiao-Lei Zhang category:cs.LG stat.ML  published:2014-12-03 summary:In (\cite{zhang2014nonlinear,zhang2014nonlinear2}), we have viewed machine learning as a coding and dimensionality reduction problem, and further proposed a simple unsupervised dimensionality reduction method, entitled deep distributed random samplings (DDRS). In this paper, we further extend it to supervised learning incrementally. The key idea here is to incorporate label information into the coding process by reformulating that each center in DDRS has multiple output units indicating which class the center belongs to. The supervised learning method seems somewhat similar with random forests (\cite{breiman2001random}), here we emphasize their differences as follows. (i) Each layer of our method considers the relationship between part of the data points in training data with all training data points, while random forests focus on building each decision tree on only part of training data points independently. (ii) Our method builds gradually-narrowed network by sampling less and less data points, while random forests builds gradually-narrowed network by merging subclasses. (iii) Our method is trained more straightforward from bottom layer to top layer, while random forests build each tree from top layer to bottom layer by splitting. (iv) Our method encodes output targets implicitly in sparse codes, while random forests encode output targets by remembering the class attributes of the activated nodes. Therefore, our method is a simpler, more straightforward, and maybe a better alternative choice, though both methods use two very basic elements---randomization and nearest neighbor optimization---as the core. This preprint is used to protect the incremental idea from (\cite{zhang2014nonlinear,zhang2014nonlinear2}). Full empirical evaluation will be announced carefully later. version:2
arxiv-1502-00555 | A Discrete Tchebichef Transform Approximation for Image and Video Coding | http://arxiv.org/abs/1502.00555 | id:1502.00555 author:P. A. M. Oliveira, R. J. Cintra, F. M. Bayer, S. Kulasekera, A. Madanayake category:stat.ME cs.CV cs.MM cs.NA stat.CO  published:2015-01-28 summary:In this paper, we introduce a low-complexity approximation for the discrete Tchebichef transform (DTT). The proposed forward and inverse transforms are multiplication-free and require a reduced number of additions and bit-shifting operations. Numerical compression simulations demonstrate the efficiency of the proposed transform for image and video coding. Furthermore, Xilinx Virtex-6 FPGA based hardware realization shows 44.9% reduction in dynamic power consumption and 64.7% lower area when compared to the literature. version:1
arxiv-1501-03854 | Understanding Kernel Ridge Regression: Common behaviors from simple functions to density functionals | http://arxiv.org/abs/1501.03854 | id:1501.03854 author:Kevin Vu, John Snyder, Li Li, Matthias Rupp, Brandon F. Chen, Tarek Khelif, Klaus-Robert Müller, Kieron Burke category:physics.comp-ph cs.LG stat.ML  published:2015-01-16 summary:Accurate approximations to density functionals have recently been obtained via machine learning (ML). By applying ML to a simple function of one variable without any random sampling, we extract the qualitative dependence of errors on hyperparameters. We find universal features of the behavior in extreme limits, including both very small and very large length scales, and the noise-free limit. We show how such features arise in ML models of density functionals. version:2
arxiv-1501-05382 | Enhanced Mixtures of Part Model for Human Pose Estimation | http://arxiv.org/abs/1501.05382 | id:1501.05382 author:Wenjuan Gong, Yongzhen Huang, Jordi Gonzalez, and Liang Wang category:cs.CV  published:2015-01-22 summary:Mixture of parts model has been successfully applied to 2D human pose estimation problem either as explicitly trained body part model or as latent variables for the whole human body model. Mixture of parts model usually utilize tree structure for representing relations between body parts. Tree structures facilitate training and referencing of the model but could not deal with double counting problems, which hinder its applications in 3D pose estimation. While most of work targeted to solve these problems tend to modify the tree models or the optimization target. We incorporate other cues from input features. For example, in surveillance environments, human silhouettes can be extracted relative easily although not flawlessly. In this condition, we can combine extracted human blobs with histogram of gradient feature, which is commonly used in mixture of parts model for training body part templates. The method can be easily extend to other candidate features under our generalized framework. We show 2D body part detection results on a public available dataset: HumanEva dataset. Furthermore, a 2D to 3D pose estimator is trained with Gaussian process regression model and 2D body part detections from the proposed method is fed to the estimator, thus 3D poses are predictable given new 2D body part detections. We also show results of 3D pose estimation on HumanEva dataset. version:2
arxiv-1501-07005 | Survey:Natural Language Parsing For Indian Languages | http://arxiv.org/abs/1501.07005 | id:1501.07005 author:Monika T. Makwana, Deepak C. Vegda category:cs.CL  published:2015-01-28 summary:Syntactic parsing is a necessary task which is required for NLP applications including machine translation. It is a challenging task to develop a qualitative parser for morphological rich and agglutinative languages. Syntactic analysis is used to understand the grammatical structure of a natural language sentence. It outputs all the grammatical information of each word and its constituent. Also issues related to it help us to understand the language in a more detailed way. This literature survey is groundwork to understand the different parser development for Indian languages and various approaches that are used to develop such tools and techniques. This paper provides a survey of research papers from well known journals and conferences. version:1
arxiv-1501-06993 | Feature Sampling Strategies for Action Recognition | http://arxiv.org/abs/1501.06993 | id:1501.06993 author:Youjie Zhou, Hongkai Yu, Song Wang category:cs.CV  published:2015-01-28 summary:Although dense local spatial-temporal features with bag-of-features representation achieve state-of-the-art performance for action recognition, the huge feature number and feature size prevent current methods from scaling up to real size problems. In this work, we investigate different types of feature sampling strategies for action recognition, namely dense sampling, uniformly random sampling and selective sampling. We propose two effective selective sampling methods using object proposal techniques. Experiments conducted on a large video dataset show that we are able to achieve better average recognition accuracy using 25% less features, through one of proposed selective sampling methods, and even remain comparable accuracy while discarding 70% features. version:1
arxiv-1501-06929 | A Probabilistic Least-Mean-Squares Filter | http://arxiv.org/abs/1501.06929 | id:1501.06929 author:Jesus Fernandez-Bes, Víctor Elvira, Steven Van Vaerenbergh category:stat.ML cs.SY stat.AP  published:2015-01-27 summary:We introduce a probabilistic approach to the LMS filter. By means of an efficient approximation, this approach provides an adaptable step-size LMS algorithm together with a measure of uncertainty about the estimation. In addition, the proposed approximation preserves the linear complexity of the standard LMS. Numerical results show the improved performance of the algorithm with respect to standard LMS and state-of-the-art algorithms with similar complexity. The goal of this work, therefore, is to open the door to bring some more Bayesian machine learning techniques to adaptive filtering. version:1
arxiv-1501-04656 | Microscopic Advances with Large-Scale Learning: Stochastic Optimization for Cryo-EM | http://arxiv.org/abs/1501.04656 | id:1501.04656 author:Ali Punjani, Marcus A. Brubaker category:stat.ML cs.CV cs.LG q-bio.QM  published:2015-01-19 summary:Determining the 3D structures of biological molecules is a key problem for both biology and medicine. Electron Cryomicroscopy (Cryo-EM) is a promising technique for structure estimation which relies heavily on computational methods to reconstruct 3D structures from 2D images. This paper introduces the challenging Cryo-EM density estimation problem as a novel application for stochastic optimization techniques. Structure discovery is formulated as MAP estimation in a probabilistic latent-variable model, resulting in an optimization problem to which an array of seven stochastic optimization methods are applied. The methods are tested on both real and synthetic data, with some methods recovering reasonable structures in less than one epoch from a random initialization. Complex quasi-Newton methods are found to converge more slowly than simple gradient-based methods, but all stochastic methods are found to converge to similar optima. This method represents a major improvement over existing methods as it is significantly faster and is able to converge from a random initialization. version:2
arxiv-1411-2674 | The Bayesian Echo Chamber: Modeling Social Influence via Linguistic Accommodation | http://arxiv.org/abs/1411.2674 | id:1411.2674 author:Fangjian Guo, Charles Blundell, Hanna Wallach, Katherine Heller category:stat.ML cs.CL cs.LG cs.SI  published:2014-11-11 summary:We present the Bayesian Echo Chamber, a new Bayesian generative model for social interaction data. By modeling the evolution of people's language usage over time, this model discovers latent influence relationships between them. Unlike previous work on inferring influence, which has primarily focused on simple temporal dynamics evidenced via turn-taking behavior, our model captures more nuanced influence relationships, evidenced via linguistic accommodation patterns in interaction content. The model, which is based on a discrete analog of the multivariate Hawkes process, permits a fully Bayesian inference algorithm. We validate our model's ability to discover latent influence patterns using transcripts of arguments heard by the US Supreme Court and the movie "12 Angry Men." We showcase our model's capabilities by using it to infer latent influence patterns from Federal Open Market Committee meeting transcripts, demonstrating state-of-the-art performance at uncovering social dynamics in group discussions. version:3
arxiv-1501-06794 | Computing Functions of Random Variables via Reproducing Kernel Hilbert Space Representations | http://arxiv.org/abs/1501.06794 | id:1501.06794 author:Bernhard Schölkopf, Krikamol Muandet, Kenji Fukumizu, Jonas Peters category:stat.ML cs.DS cs.LG G.3; I.2.6; D.3.3  published:2015-01-27 summary:We describe a method to perform functional operations on probability distributions of random variables. The method uses reproducing kernel Hilbert space representations of probability distributions, and it is applicable to all operations which can be applied to points drawn from the respective distributions. We refer to our approach as {\em kernel probabilistic programming}. We illustrate it on synthetic data, and show how it can be used for nonparametric structural equation models, with an application to causal inference. version:1
arxiv-1501-06751 | A Cheap System for Vehicle Speed Detection | http://arxiv.org/abs/1501.06751 | id:1501.06751 author:Chaim Ginzburg, Amit Raphael, Daphna Weinshall category:cs.CV  published:2015-01-27 summary:The reliable detection of speed of moving vehicles is considered key to traffic law enforcement in most countries, and is seen by many as an important tool to reduce the number of traffic accidents and fatalities. Many automatic systems and different methods are employed in different countries, but as a rule they tend to be expensive and/or labor intensive, often employing outdated technology due to the long development time. Here we describe a speed detection system that relies on simple everyday equipment - a laptop and a consumer web camera. Our method is based on tracking the license plates of cars, which gives the relative movement of the cars in the image. This image displacement is translated to actual motion by using the method of projection to a reference plane, where the reference plane is the road itself. However, since license plates do not touch the road, we must compensate for the entailed distortion in speed measurement. We show how to compute the compensation factor using knowledge of the license plate standard dimensions. Consequently our system computes the true speed of moving vehicles fast and accurately. We show promising results on videos obtained in a number of scenes and with different car models. version:1
arxiv-1412-3409 | Teaching Deep Convolutional Neural Networks to Play Go | http://arxiv.org/abs/1412.3409 | id:1412.3409 author:Christopher Clark, Amos Storkey category:cs.AI cs.LG cs.NE  published:2014-12-10 summary:Mastering the game of Go has remained a long standing challenge to the field of AI. Modern computer Go systems rely on processing millions of possible future positions to play well, but intuitively a stronger and more 'humanlike' way to play the game would be to rely on pattern recognition abilities rather then brute force computation. Following this sentiment, we train deep convolutional neural networks to play Go by training them to predict the moves made by expert Go players. To solve this problem we introduce a number of novel techniques, including a method of tying weights in the network to 'hard code' symmetries that are expect to exist in the target function, and demonstrate in an ablation study they considerably improve performance. Our final networks are able to achieve move prediction accuracies of 41.1% and 44.4% on two different Go datasets, surpassing previous state of the art on this task by significant margins. Additionally, while previous move prediction programs have not yielded strong Go playing programs, we show that the networks trained in this work acquired high levels of skill. Our convolutional neural networks can consistently defeat the well known Go program GNU Go, indicating it is state of the art among programs that do not use Monte Carlo Tree Search. It is also able to win some games against state of the art Go playing program Fuego while using a fraction of the play time. This success at playing Go indicates high level principles of the game were learned. version:2
arxiv-1501-06722 | Parametric Image Segmentation of Humans with Structural Shape Priors | http://arxiv.org/abs/1501.06722 | id:1501.06722 author:Alin-Ionut Popa, Cristian Sminchisescu category:cs.CV  published:2015-01-27 summary:The figure-ground segmentation of humans in images captured in natural environments is an outstanding open problem due to the presence of complex backgrounds, articulation, varying body proportions, partial views and viewpoint changes. In this work we propose class-specific segmentation models that leverage parametric max-flow image segmentation and a large dataset of human shapes. Our contributions are as follows: (1) formulation of a sub-modular energy model that combines class-specific structural constraints and data-driven shape priors, within a parametric max-flow optimization methodology that systematically computes all breakpoints of the model in polynomial time; (2) design of a data-driven class-specific fusion methodology, based on matching against a large training set of exemplar human shapes (100,000 in our experiments), that allows the shape prior to be constructed on-the-fly, for arbitrary viewpoints and partial views. (3) demonstration of state of the art results, in two challenging datasets, H3D and MPII (where figure-ground segmentation annotations have been added by us), where we substantially improve on the first ranked hypothesis estimates of mid-level segmentation methods, by 20%, with hypothesis set sizes that are up to one order of magnitude smaller. version:1
arxiv-1501-06716 | A General Preprocessing Method for Improved Performance of Epipolar Geometry Estimation Algorithms | http://arxiv.org/abs/1501.06716 | id:1501.06716 author:Maria Kushnir, Ilan Shimshoni category:cs.CV  published:2015-01-27 summary:In this paper a deterministic preprocessing algorithm is presented, whose output can be given as input to most state-of-the-art epipolar geometry estimation algorithms, improving their results considerably. They are now able to succeed on hard cases for which they failed before. The algorithm consists of three steps, whose scope changes from local to global. In the local step it extracts from a pair of images local features (e.g. SIFT). Similar features from each image are clustered and the clusters are matched yielding a large number of putative matches. In the second step pairs of spatially close features (called 2keypoints) are matched and ranked by a classifier. The 2keypoint matches with the highest ranks are selected. In the global step, from each two 2keypoint matches a fundamental matrix is computed. As quite a few of the matrices are generated from correct matches they are used to rank the putative matches found in the first step. For each match the number of fundamental matrices, for which it approximately satisfies the epipolar constraint, is calculated. This set of matches is combined with the putative matches generated by standard methods and their probabilities to be correct are estimated by a classifier. These are then given as input to state-of-the-art epipolar geometry estimation algorithms such as BEEM, BLOGS and USAC yielding much better results than the original algorithms. This was shown in extensive testing performed on almost 900 image pairs from six publicly available data-sets. version:1
arxiv-1412-2210 | Risk Estimation Without Using Stein's Lemma -- Application to Image Denoising | http://arxiv.org/abs/1412.2210 | id:1412.2210 author:Sagar Venkatesh Gubbi, Chandra Sekhar Seelamantula category:cs.CV  published:2014-12-06 summary:We address the problem of image denoising in additive white noise without placing restrictive assumptions on its statistical distribution. In the recent literature, specific noise distributions have been considered and correspondingly, optimal denoising techniques have been developed. One of the successful approaches for denoising relies on the notion of unbiased risk estimation, which enables one to obtain a useful substitute for the mean-square error. For the case of additive white Gaussian noise contamination, the risk estimation procedure relies on Stein's lemma. Sophisticated wavelet-based denoising techniques, which are essentially nonlinear, have been developed with the help of the lemma. We show that, for linear, shift-invariant denoisers, it is possible to obtain unbiased risk estimates of the mean-square error without using Stein's lemma. An interesting consequence of this development is that the unbiased risk estimator becomes agnostic to the statistical distribution of the noise. As a proof of principle, we show how the new methodology can be used to optimize the parameters of a simple Gaussian smoother. By locally adapting the parameters of the Gaussian smoother, we obtain a shift-variant smoother, which has a denoising performance (quantified by the improvement in peak signal-to-noise ratio (PSNR)) that is competitive to far more sophisticated methods reported in the literature. The proposed solution exhibits considerable parallelism, which we exploit in a Graphics Processing Unit (GPU) implementation. version:2
arxiv-1410-0949 | Tight Regret Bounds for Stochastic Combinatorial Semi-Bandits | http://arxiv.org/abs/1410.0949 | id:1410.0949 author:Branislav Kveton, Zheng Wen, Azin Ashkan, Csaba Szepesvari category:cs.LG cs.AI stat.ML  published:2014-10-03 summary:A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we close the problem of computationally and sample efficient learning in stochastic combinatorial semi-bandits. In particular, we analyze a UCB-like algorithm for solving the problem, which is known to be computationally efficient; and prove $O(K L (1 / \Delta) \log n)$ and $O(\sqrt{K L n \log n})$ upper bounds on its $n$-step regret, where $L$ is the number of ground items, $K$ is the maximum number of chosen items, and $\Delta$ is the gap between the expected returns of the optimal and best suboptimal solutions. The gap-dependent bound is tight up to a constant factor and the gap-free bound is tight up to a polylogarithmic factor. version:3
arxiv-1412-8729 | High Dimensional Expectation-Maximization Algorithm: Statistical Optimization and Asymptotic Normality | http://arxiv.org/abs/1412.8729 | id:1412.8729 author:Zhaoran Wang, Quanquan Gu, Yang Ning, Han Liu category:stat.ML  published:2014-12-30 summary:We provide a general theory of the expectation-maximization (EM) algorithm for inferring high dimensional latent variable models. In particular, we make two contributions: (i) For parameter estimation, we propose a novel high dimensional EM algorithm which naturally incorporates sparsity structure into parameter estimation. With an appropriate initialization, this algorithm converges at a geometric rate and attains an estimator with the (near-)optimal statistical rate of convergence. (ii) Based on the obtained estimator, we propose new inferential procedures for testing hypotheses and constructing confidence intervals for low dimensional components of high dimensional parameters. For a broad family of statistical models, our framework establishes the first computationally feasible approach for optimal estimation and asymptotic inference in high dimensions. Our theory is supported by thorough numerical results. version:2
arxiv-1306-4960 | Optimal computational and statistical rates of convergence for sparse nonconvex learning problems | http://arxiv.org/abs/1306.4960 | id:1306.4960 author:Zhaoran Wang, Han Liu, Tong Zhang category:stat.ML  published:2013-06-20 summary:We provide theoretical analysis of the statistical and computational properties of penalized $M$-estimators that can be formulated as the solution to a possibly nonconvex optimization problem. Many important estimators fall in this category, including least squares regression with nonconvex regularization, generalized linear models with nonconvex regularization and sparse elliptical random design regression. For these problems, it is intractable to calculate the global solution due to the nonconvex formulation. In this paper, we propose an approximate regularization path-following method for solving a variety of learning problems with nonconvex objective functions. Under a unified analytic framework, we simultaneously provide explicit statistical and computational rates of convergence for any local solution attained by the algorithm. Computationally, our algorithm attains a global geometric rate of convergence for calculating the full regularization path, which is optimal among all first-order algorithms. Unlike most existing methods that only attain geometric rates of convergence for one single regularization parameter, our algorithm calculates the full regularization path with the same iteration complexity. In particular, we provide a refined iteration complexity bound to sharply characterize the performance of each stage along the regularization path. Statistically, we provide sharp sample complexity analysis for all the approximate local solutions along the regularization path. In particular, our analysis improves upon existing results by providing a more refined sample complexity bound as well as an exact support recovery result for the final estimator. These results show that the final estimator attains an oracle statistical property due to the usage of nonconvex penalty. version:5
arxiv-1402-1754 | Two-stage Sampled Learning Theory on Distributions | http://arxiv.org/abs/1402.1754 | id:1402.1754 author:Zoltan Szabo, Arthur Gretton, Barnabas Poczos, Bharath Sriperumbudur category:math.ST cs.LG math.FA stat.ML stat.TH 62G08  46E22  47B32 G.3; I.2.6  published:2014-02-07 summary:We focus on the distribution regression problem: regressing to a real-valued response from a probability distribution. Although there exist a large number of similarity measures between distributions, very little is known about their generalization performance in specific learning tasks. Learning problems formulated on distributions have an inherent two-stage sampled difficulty: in practice only samples from sampled distributions are observable, and one has to build an estimate on similarities computed between sets of points. To the best of our knowledge, the only existing method with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which suffers from slow convergence issues in high dimensions), and the domain of the distributions to be compact Euclidean. In this paper, we provide theoretical guarantees for a remarkably simple algorithmic alternative to solve the distribution regression problem: embed the distributions to a reproducing kernel Hilbert space, and learn a ridge regressor from the embeddings to the outputs. Our main contribution is to prove the consistency of this technique in the two-stage sampled setting under mild conditions (on separable, topological domains endowed with kernels). For a given total number of observations, we derive convergence rates as an explicit function of the problem difficulty. As a special case, we answer a 15-year-old open question: we establish the consistency of the classical set kernel [Haussler, 1999; Gartner et. al, 2002] in regression, and cover more recent kernels on distributions, including those due to [Christmann and Steinwart, 2010]. version:6
arxiv-1501-06598 | Online Nonparametric Regression with General Loss Functions | http://arxiv.org/abs/1501.06598 | id:1501.06598 author:Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.IT cs.LG math.IT  published:2015-01-26 summary:This paper establishes minimax rates for online regression with arbitrary classes of functions and general losses. We show that below a certain threshold for the complexity of the function class, the minimax rates depend on both the curvature of the loss function and the sequential complexities of the class. Above this threshold, the curvature of the loss does not affect the rates. Furthermore, for the case of square loss, our results point to the interesting phenomenon: whenever sequential and i.i.d. empirical entropies match, the rates for statistical and online learning are the same. In addition to the study of minimax regret, we derive a generic forecaster that enjoys the established optimal rates. We also provide a recipe for designing online prediction algorithms that can be computationally efficient for certain problems. We illustrate the techniques by deriving existing and new forecasters for the case of finite experts and for online linear regression. version:1
arxiv-1410-7452 | Consensus Message Passing for Layered Graphical Models | http://arxiv.org/abs/1410.7452 | id:1410.7452 author:Varun Jampani, S. M. Ali Eslami, Daniel Tarlow, Pushmeet Kohli, John Winn category:cs.CV cs.AI cs.LG  published:2014-10-27 summary:Generative models provide a powerful framework for probabilistic reasoning. However, in many domains their use has been hampered by the practical difficulties of inference. This is particularly the case in computer vision, where models of the imaging process tend to be large, loopy and layered. For this reason bottom-up conditional models have traditionally dominated in such domains. We find that widely-used, general-purpose message passing inference algorithms such as Expectation Propagation (EP) and Variational Message Passing (VMP) fail on the simplest of vision models. With these models in mind, we introduce a modification to message passing that learns to exploit their layered structure by passing 'consensus' messages that guide inference towards good solutions. Experiments on a variety of problems show that the proposed technique leads to significantly more accurate inference results, not only when compared to standard EP and VMP, but also when compared to competitive bottom-up conditional models. version:2
arxiv-1501-06587 | Measuring academic influence: Not all citations are equal | http://arxiv.org/abs/1501.06587 | id:1501.06587 author:Xiaodan Zhu, Peter Turney, Daniel Lemire, André Vellino category:cs.DL cs.CL cs.LG  published:2015-01-26 summary:The importance of a research article is routinely measured by counting how many times it has been cited. However, treating all citations with equal weight ignores the wide variety of functions that citations perform. We want to automatically identify the subset of references in a bibliography that have a central academic influence on the citing paper. For this purpose, we examine the effectiveness of a variety of features for determining the academic influence of a citation. By asking authors to identify the key references in their own work, we created a data set in which citations were labeled according to their academic influence. Using automatic feature selection with supervised machine learning, we found a model for predicting academic influence that achieves good performance on this data set using only four features. The best features, among those we evaluated, were those based on the number of times a reference is mentioned in the body of a citing paper. The performance of these features inspired us to design an influence-primed h-index (the hip-index). Unlike the conventional h-index, it weights citations by how many times a reference is mentioned. According to our experiments, the hip-index is a better indicator of researcher performance than the conventional h-index. version:1
arxiv-1409-2080 | Multiscale statistical testing for connectome-wide association studies in fMRI | http://arxiv.org/abs/1409.2080 | id:1409.2080 author:P. Bellec, Y. Benhajali, F. Carbonell, C. Dansereau, G. Albouy, M. Pelland, C. Craddock, O. Collignon, J. Doyon, E. Stip, P. Orban category:q-bio.QM cs.CV stat.AP  published:2014-09-07 summary:Alterations in brain connectivity have been associated with a variety of clinical disorders using functional magnetic resonance imaging (fMRI). We investigated empirically how the number of brain parcels (or scale) impacted the results of a mass univariate general linear model (GLM) on connectomes. The brain parcels used as nodes in the connectome analysis were functionnally defined by a group cluster analysis. We first validated that a classic Benjamini-Hochberg procedure with parametric GLM tests did control appropriately the false-discovery rate (FDR) at a given scale. We then observed on realistic simulations that there was no substantial inflation of the FDR across scales, as long as the FDR was controlled independently within each scale, and the presence of true associations could be established using an omnibus permutation test combining all scales. Second, we observed both on simulations and on three real resting-state fMRI datasets (schizophrenia, congenital blindness, motor practice) that the rate of discovery varied markedly as a function of scales, and was relatively higher for low scales, below 25. Despite the differences in discovery rate, the statistical maps derived at different scales were generally very consistent in the three real datasets. Some seeds still showed effects better observed around 50, illustrating the potential benefits of multiscale analysis. On real data, the statistical maps agreed well with the existing literature. Overall, our results support that the multiscale GLM connectome analysis with FDR is statistically valid and can capture biologically meaningful effects in a variety of experimental conditions. version:2
arxiv-1312-2967 | Every LWF and AMP chain graph originates from a set of causal models | http://arxiv.org/abs/1312.2967 | id:1312.2967 author:Jose M. Peña category:stat.ML  published:2013-12-10 summary:This paper aims at justifying LWF and AMP chain graphs by showing that they do not represent arbitrary independence models. Specifically, we show that every chain graph is inclusion optimal wrt the intersection of the independence models represented by a set of directed and acyclic graphs under conditioning. This implies that the independence model represented by the chain graph can be accounted for by a set of causal models that are subject to selection bias, which in turn can be accounted for by a system that switches between different regimes or configurations. version:3
arxiv-1411-0030 | A* Sampling | http://arxiv.org/abs/1411.0030 | id:1411.0030 author:Chris J. Maddison, Daniel Tarlow, Tom Minka category:stat.CO stat.ML  published:2014-10-31 summary:The problem of drawing samples from a discrete distribution can be converted into a discrete optimization problem. In this work, we show how sampling from a continuous distribution can be converted into an optimization problem over continuous space. Central to the method is a stochastic process recently described in mathematical statistics that we call the Gumbel process. We present a new construction of the Gumbel process and A* sampling, a practical generic sampling algorithm that searches for the maximum of a Gumbel process using A* search. We analyze the correctness and convergence time of A* sampling and demonstrate empirically that it makes more efficient use of bound and likelihood evaluations than the most closely related adaptive rejection sampling-based algorithms. version:2
arxiv-1404-4797 | Parallel Graph Partitioning for Complex Networks | http://arxiv.org/abs/1404.4797 | id:1404.4797 author:Henning Meyerhenke, Peter Sanders, Christian Schulz category:cs.DC cs.DS cs.NE cs.SI physics.soc-ph  published:2014-04-18 summary:Processing large complex networks like social networks or web graphs has recently attracted considerable interest. In order to do this in parallel, we need to partition them into pieces of about equal size. Unfortunately, previous parallel graph partitioners originally developed for more regular mesh-like networks do not work well for these networks. This paper addresses this problem by parallelizing and adapting the label propagation technique originally developed for graph clustering. By introducing size constraints, label propagation becomes applicable for both the coarsening and the refinement phase of multilevel graph partitioning. We obtain very high quality by applying a highly parallel evolutionary algorithm to the coarsened graph. The resulting system is both more scalable and achieves higher quality than state-of-the-art systems like ParMetis or PT-Scotch. For large complex networks the performance differences are very big. For example, our algorithm can partition a web graph with 3.3 billion edges in less than sixteen seconds using 512 cores of a high performance cluster while producing a high quality partition -- none of the competing systems can handle this graph on our system. version:3
arxiv-1501-06284 | On a Family of Decomposable Kernels on Sequences | http://arxiv.org/abs/1501.06284 | id:1501.06284 author:Andrea Baisero, Florian T. Pokorny, Carl Henrik Ek category:cs.LG  published:2015-01-26 summary:In many applications data is naturally presented in terms of orderings of some basic elements or symbols. Reasoning about such data requires a notion of similarity capable of handling sequences of different lengths. In this paper we describe a family of Mercer kernel functions for such sequentially structured data. The family is characterized by a decomposable structure in terms of symbol-level and structure-level similarities, representing a specific combination of kernels which allows for efficient computation. We provide an experimental evaluation on sequential classification tasks comparing kernels from our family of kernels to a state of the art sequence kernel called the Global Alignment kernel which has been shown to outperform Dynamic Time Warping version:1
arxiv-1501-06237 | Deep Transductive Semi-supervised Maximum Margin Clustering | http://arxiv.org/abs/1501.06237 | id:1501.06237 author:Gang Chen category:cs.LG 68T10 I.2.6  published:2015-01-26 summary:Semi-supervised clustering is an very important topic in machine learning and computer vision. The key challenge of this problem is how to learn a metric, such that the instances sharing the same label are more likely close to each other on the embedded space. However, little attention has been paid to learn better representations when the data lie on non-linear manifold. Fortunately, deep learning has led to great success on feature learning recently. Inspired by the advances of deep learning, we propose a deep transductive semi-supervised maximum margin clustering approach. More specifically, given pairwise constraints, we exploit both labeled and unlabeled data to learn a non-linear mapping under maximum margin framework for clustering analysis. Thus, our model unifies transductive learning, feature learning and maximum margin techniques in the semi-supervised clustering framework. We pretrain the deep network structure with restricted Boltzmann machines (RBMs) layer by layer greedily, and optimize our objective function with gradient descent. By checking the most violated constraints, our approach updates the model parameters through error backpropagation, in which deep features are learned automatically. The experimental results shows that our model is significantly better than the state of the art on semi-supervised clustering. version:1
arxiv-1501-06225 | Online Optimization : Competing with Dynamic Comparators | http://arxiv.org/abs/1501.06225 | id:1501.06225 author:Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, Karthik Sridharan category:cs.LG math.OC stat.ML  published:2015-01-26 summary:Recent literature on online learning has focused on developing adaptive algorithms that take advantage of a regularity of the sequence of observations, yet retain worst-case performance guarantees. A complementary direction is to develop prediction methods that perform well against complex benchmarks. In this paper, we address these two directions together. We present a fully adaptive method that competes with dynamic benchmarks in which regret guarantee scales with regularity of the sequence of cost functions and comparators. Notably, the regret bound adapts to the smaller complexity measure in the problem environment. Finally, we apply our results to drifting zero-sum, two-player games where both players achieve no regret guarantees against best sequences of actions in hindsight. version:1
arxiv-1412-3705 | A Topic Modeling Approach to Ranking | http://arxiv.org/abs/1412.3705 | id:1412.3705 author:Weicong Ding, Prakash Ishwar, Venkatesh Saligrama category:cs.LG stat.ML  published:2014-12-11 summary:We propose a topic modeling approach to the prediction of preferences in pairwise comparisons. We develop a new generative model for pairwise comparisons that accounts for multiple shared latent rankings that are prevalent in a population of users. This new model also captures inconsistent user behavior in a natural way. We show how the estimation of latent rankings in the new generative model can be formally reduced to the estimation of topics in a statistically equivalent topic modeling problem. We leverage recent advances in the topic modeling literature to develop an algorithm that can learn shared latent rankings with provable consistency as well as sample and computational complexity guarantees. We demonstrate that the new approach is empirically competitive with the current state-of-the-art approaches in predicting preferences on some semi-synthetic and real world datasets. version:3
arxiv-1501-06195 | Randomized sketches for kernels: Fast and optimal non-parametric regression | http://arxiv.org/abs/1501.06195 | id:1501.06195 author:Yun Yang, Mert Pilanci, Martin J. Wainwright category:stat.ML cs.DS cs.LG stat.CO  published:2015-01-25 summary:Kernel ridge regression (KRR) is a standard method for performing non-parametric regression over reproducing kernel Hilbert spaces. Given $n$ samples, the time and space complexity of computing the KRR estimate scale as $\mathcal{O}(n^3)$ and $\mathcal{O}(n^2)$ respectively, and so is prohibitive in many cases. We propose approximations of KRR based on $m$-dimensional randomized sketches of the kernel matrix, and study how small the projection dimension $m$ can be chosen while still preserving minimax optimality of the approximate KRR estimate. For various classes of randomized sketches, including those based on Gaussian and randomized Hadamard matrices, we prove that it suffices to choose the sketch dimension $m$ proportional to the statistical dimension (modulo logarithmic factors). Thus, we obtain fast and minimax optimal approximations to the KRR estimate for non-parametric regression. version:1
arxiv-1501-06180 | Exploring Human Vision Driven Features for Pedestrian Detection | http://arxiv.org/abs/1501.06180 | id:1501.06180 author:Shanshan Zhang, Christian Bauckhage, Dominik A. Klein, Armin B. Cremers category:cs.CV  published:2015-01-25 summary:Motivated by the center-surround mechanism in the human visual attention system, we propose to use average contrast maps for the challenge of pedestrian detection in street scenes due to the observation that pedestrians indeed exhibit discriminative contrast texture. Our main contributions are first to design a local, statistical multi-channel descriptorin order to incorporate both color and gradient information. Second, we introduce a multi-direction and multi-scale contrast scheme based on grid-cells in order to integrate expressive local variations. Contributing to the issue of selecting most discriminative features for assessing and classification, we perform extensive comparisons w.r.t. statistical descriptors, contrast measurements, and scale structures. This way, we obtain reasonable results under various configurations. Empirical findings from applying our optimized detector on the INRIA and Caltech pedestrian datasets show that our features yield state-of-the-art performance in pedestrian detection. version:1
arxiv-1201-5604 | Discrete and fuzzy dynamical genetic programming in the XCSF learning classifier system | http://arxiv.org/abs/1201.5604 | id:1201.5604 author:Richard J. Preen, Larry Bull category:cs.AI cs.LG cs.NE cs.SY math.OC  published:2012-01-26 summary:A number of representation schemes have been presented for use within learning classifier systems, ranging from binary encodings to neural networks. This paper presents results from an investigation into using discrete and fuzzy dynamical system representations within the XCSF learning classifier system. In particular, asynchronous random Boolean networks are used to represent the traditional condition-action production system rules in the discrete case and asynchronous fuzzy logic networks in the continuous-valued case. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such dynamical systems within XCSF to solve a number of well-known test problems. version:2
arxiv-1501-06129 | An Occlusion Reasoning Scheme for Monocular Pedestrian Tracking in Dynamic Scenes | http://arxiv.org/abs/1501.06129 | id:1501.06129 author:Sourav Garg, Swagat Kumar, Rajesh Ratnakaram, Prithwijit Guha category:cs.CV  published:2015-01-25 summary:This paper looks into the problem of pedestrian tracking using a monocular, potentially moving, uncalibrated camera. The pedestrians are located in each frame using a standard human detector, which are then tracked in subsequent frames. This is a challenging problem as one has to deal with complex situations like changing background, partial or full occlusion and camera motion. In order to carry out successful tracking, it is necessary to resolve associations between the detected windows in the current frame with those obtained from the previous frame. Compared to methods that use temporal windows incorporating past as well as future information, we attempt to make decision on a frame-by-frame basis. An occlusion reasoning scheme is proposed to resolve the association problem between a pair of consecutive frames by using an affinity matrix that defines the closeness between a pair of windows and then, uses a binary integer programming to obtain unique association between them. A second stage of verification based on SURF matching is used to deal with those cases where the above optimization scheme might yield wrong associations. The efficacy of the approach is demonstrated through experiments on several standard pedestrian datasets. version:1
arxiv-1501-06116 | Prediction Error Reduction Function as a Variable Importance Score | http://arxiv.org/abs/1501.06116 | id:1501.06116 author:Ernest Fokoué category:stat.ML 62H25  62H30  published:2015-01-25 summary:This paper introduces and develops a novel variable importance score function in the context of ensemble learning and demonstrates its appeal both theoretically and empirically. Our proposed score function is simple and more straightforward than its counterpart proposed in the context of random forest, and by avoiding permutations, it is by design computationally more efficient than the random forest variable importance function. Just like the random forest variable importance function, our score handles both regression and classification seamlessly. One of the distinct advantage of our proposed score is the fact that it offers a natural cut off at zero, with all the positive scores indicating importance and significance, while the negative scores are deemed indications of insignificance. An extra advantage of our proposed score lies in the fact it works very well beyond ensemble of trees and can seamlessly be used with any base learners in the random subspace learning context. Our examples, both simulated and real, demonstrate that our proposed score does compete mostly favorably with the random forest score. version:1
arxiv-1212-0451 | Semi-blind Source Separation via Sparse Representations and Online Dictionary Learning | http://arxiv.org/abs/1212.0451 | id:1212.0451 author:Sirisha Rambhatla, Jarvis D. Haupt category:cs.SD stat.AP stat.ML  published:2012-12-03 summary:This work examines a semi-blind single-channel source separation problem. Our specific aim is to separate one source whose local structure is approximately known, from another a priori unspecified background source, given only a single linear combination of the two sources. We propose a separation technique based on local sparse approximations along the lines of recent efforts in sparse representations and dictionary learning. A key feature of our procedure is the online learning of dictionaries (using only the data itself) to sparsely model the background source, which facilitates its separation from the partially-known source. Our approach is applicable to source separation problems in various application domains; here, we demonstrate the performance of our proposed approach via simulation on a stylized audio source separation task. version:2
arxiv-1501-06103 | A simpler condition for consistency of a kernel independence test | http://arxiv.org/abs/1501.06103 | id:1501.06103 author:Arthur Gretton category:stat.ML  published:2015-01-25 summary:A statistical test of independence may be constructed using the Hilbert-Schmidt Independence Criterion (HSIC) as a test statistic. The HSIC is defined as the distance between the embedding of the joint distribution, and the embedding of the product of the marginals, in a Reproducing Kernel Hilbert Space (RKHS). It has previously been shown that when the kernel used in defining the joint embedding is characteristic (that is, the embedding of the joint distribution to the feature space is injective), then the HSIC-based test is consistent. In particular, it is sufficient for the product of kernels on the individual domains to be characteristic on the joint domain. In this note, it is established via a result of Lyons (2013) that HSIC-based independence tests are consistent when kernels on the marginals are characteristic on their respective domains, even when the product of kernels is not characteristic on the joint domain. version:1
arxiv-1501-06095 | Between Pure and Approximate Differential Privacy | http://arxiv.org/abs/1501.06095 | id:1501.06095 author:Thomas Steinke, Jonathan Ullman category:cs.DS cs.CR cs.LG  published:2015-01-24 summary:We show a new lower bound on the sample complexity of $(\varepsilon, \delta)$-differentially private algorithms that accurately answer statistical queries on high-dimensional databases. The novelty of our bound is that it depends optimally on the parameter $\delta$, which loosely corresponds to the probability that the algorithm fails to be private, and is the first to smoothly interpolate between approximate differential privacy ($\delta > 0$) and pure differential privacy ($\delta = 0$). Specifically, we consider a database $D \in \{\pm1\}^{n \times d}$ and its \emph{one-way marginals}, which are the $d$ queries of the form "What fraction of individual records have the $i$-th bit set to $+1$?" We show that in order to answer all of these queries to within error $\pm \alpha$ (on average) while satisfying $(\varepsilon, \delta)$-differential privacy, it is necessary that $$ n \geq \Omega\left( \frac{\sqrt{d \log(1/\delta)}}{\alpha \varepsilon} \right), $$ which is optimal up to constant factors. To prove our lower bound, we build on the connection between \emph{fingerprinting codes} and lower bounds in differential privacy (Bun, Ullman, and Vadhan, STOC'14). In addition to our lower bound, we give new purely and approximately differentially private algorithms for answering arbitrary statistical queries that improve on the sample complexity of the standard Laplace and Gaussian mechanisms for achieving worst-case accuracy guarantees by a logarithmic factor. version:1
arxiv-1411-7399 | Fisher Vectors Derived from Hybrid Gaussian-Laplacian Mixture Models for Image Annotation | http://arxiv.org/abs/1411.7399 | id:1411.7399 author:Benjamin Klein, Guy Lev, Gil Sadeh, Lior Wolf category:cs.CV  published:2014-11-26 summary:In the traditional object recognition pipeline, descriptors are densely sampled over an image, pooled into a high dimensional non-linear representation and then passed to a classifier. In recent years, Fisher Vectors have proven empirically to be the leading representation for a large variety of applications. The Fisher Vector is typically taken as the gradients of the log-likelihood of descriptors, with respect to the parameters of a Gaussian Mixture Model (GMM). Motivated by the assumption that different distributions should be applied for different datasets, we present two other Mixture Models and derive their Expectation-Maximization and Fisher Vector expressions. The first is a Laplacian Mixture Model (LMM), which is based on the Laplacian distribution. The second Mixture Model presented is a Hybrid Gaussian-Laplacian Mixture Model (HGLMM) which is based on a weighted geometric mean of the Gaussian and Laplacian distribution. An interesting property of the Expectation-Maximization algorithm for the latter is that in the maximization step, each dimension in each component is chosen to be either a Gaussian or a Laplacian. Finally, by using the new Fisher Vectors derived from HGLMMs, we achieve state-of-the-art results for both the image annotation and the image search by a sentence tasks. version:2
arxiv-1501-06066 | Sparse Distance Weighted Discrimination | http://arxiv.org/abs/1501.06066 | id:1501.06066 author:Boxiang Wang, Hui Zou category:stat.ML stat.CO  published:2015-01-24 summary:Distance weighted discrimination (DWD) was originally proposed to handle the data piling issue in the support vector machine. In this paper, we consider the sparse penalized DWD for high-dimensional classification. The state-of-the-art algorithm for solving the standard DWD is based on second-order cone programming, however such an algorithm does not work well for the sparse penalized DWD with high-dimensional data. In order to overcome the challenging computation difficulty, we develop a very efficient algorithm to compute the solution path of the sparse DWD at a given fine grid of regularization parameters. We implement the algorithm in a publicly available R package sdwd. We conduct extensive numerical experiments to demonstrate the computational efficiency and classification performance of our method. version:1
arxiv-1501-06060 | Consistency Analysis of Nearest Subspace Classifier | http://arxiv.org/abs/1501.06060 | id:1501.06060 author:Yi Wang category:stat.ML cs.LG  published:2015-01-24 summary:The Nearest subspace classifier (NSS) finds an estimation of the underlying subspace within each class and assigns data points to the class that corresponds to its nearest subspace. This paper mainly studies how well NSS can be generalized to new samples. It is proved that NSS is strongly consistent under certain assumptions. For completeness, NSS is evaluated through experiments on various simulated and real data sets, in comparison with some other linear model based classifiers. It is also shown that NSS can obtain effective classification results and is very efficient, especially for large scale data sets. version:1
arxiv-1408-2714 | Learning From Non-iid Data: Fast Rates for the One-vs-All Multiclass Plug-in Classifiers | http://arxiv.org/abs/1408.2714 | id:1408.2714 author:Vu Dinh, Lam Si Tung Ho, Nguyen Viet Cuong, Duy Nguyen, Binh T. Nguyen category:stat.ML  published:2014-08-12 summary:We prove new fast learning rates for the one-vs-all multiclass plug-in classifiers trained either from exponentially strongly mixing data or from data generated by a converging drifting distribution. These are two typical scenarios where training data are not iid. The learning rates are obtained under a multiclass version of Tsybakov's margin assumption, a type of low-noise assumption, and do not depend on the number of classes. Our results are general and include a previous result for binary-class plug-in classifiers with iid data as a special case. In contrast to previous works for least squares SVMs under the binary-class setting, our results retain the optimal learning rate in the iid case. version:2
arxiv-1401-7077 | Quantifying literature quality using complexity criteria | http://arxiv.org/abs/1401.7077 | id:1401.7077 author:Gerardo Febres, Klaus Jaffe category:cs.CL  published:2014-01-28 summary:We measured entropy and symbolic diversity for English and Spanish texts including literature Nobel laureates and other famous authors. Entropy, symbol diversity and symbol frequency profiles were compared for these four groups. We also built a scale sensitive to the quality of writing and evaluated its relationship with the Flesch's readability index for English and the Szigriszt's perspicuity index for Spanish. Results suggest a correlation between entropy and word diversity with quality of writing. Text genre also influences the resulting entropy and diversity of the text. Results suggest the plausibility of automated quality assessment of texts. version:3
arxiv-1501-05970 | Automatic Objects Removal for Scene Completion | http://arxiv.org/abs/1501.05970 | id:1501.05970 author:Jianjun Yang, Yin Wang, Honggang Wang, Kun Hua, Wei Wang, Ju Shen category:cs.CV  published:2015-01-23 summary:With the explosive growth of web-based cameras and mobile devices, billions of photographs are uploaded to the internet. We can trivially collect a huge number of photo streams for various goals, such as 3D scene reconstruction and other big data applications. However, this is not an easy task due to the fact the retrieved photos are neither aligned nor calibrated. Furthermore, with the occlusion of unexpected foreground objects like people, vehicles, it is even more challenging to find feature correspondences and reconstruct realistic scenes. In this paper, we propose a structure based image completion algorithm for object removal that produces visually plausible content with consistent structure and scene texture. We use an edge matching technique to infer the potential structure of the unknown region. Driven by the estimated structure, texture synthesis is performed automatically along the estimated curves. We evaluate the proposed method on different types of images: from highly structured indoor environment to the natural scenes. Our experimental results demonstrate satisfactory performance that can be potentially used for subsequent big data processing: 3D scene reconstruction and location recognition. version:1
arxiv-1501-05964 | Advances in Human Action Recognition: A Survey | http://arxiv.org/abs/1501.05964 | id:1501.05964 author:Guangchun Cheng, Yiwen Wan, Abdullah N. Saudagar, Kamesh Namuduri, Bill P. Buckles category:cs.CV  published:2015-01-23 summary:Human action recognition has been an important topic in computer vision due to its many applications such as video surveillance, human machine interaction and video retrieval. One core problem behind these applications is automatically recognizing low-level actions and high-level activities of interest. The former is usually the basis for the latter. This survey gives an overview of the most recent advances in human action recognition during the past several years, following a well-formed taxonomy proposed by a previous survey. From this state-of-the-art survey, researchers can view a panorama of progress in this area for future research. version:1
arxiv-1403-7752 | Auto-encoders: reconstruction versus compression | http://arxiv.org/abs/1403.7752 | id:1403.7752 author:Yann Ollivier category:cs.NE cs.IT cs.LG math.IT  published:2014-03-30 summary:We discuss the similarities and differences between training an auto-encoder to minimize the reconstruction error, and training the same auto-encoder to compress the data via a generative model. Minimizing a codelength for the data using an auto-encoder is equivalent to minimizing the reconstruction error plus some correcting terms which have an interpretation as either a denoising or contractive property of the decoding function. These terms are related but not identical to those used in denoising or contractive auto-encoders [Vincent et al. 2010, Rifai et al. 2011]. In particular, the codelength viewpoint fully determines an optimal noise level for the denoising criterion. version:2
arxiv-1406-7321 | Proximal Quasi-Newton for Computationally Intensive L1-regularized M-estimators | http://arxiv.org/abs/1406.7321 | id:1406.7321 author:Kai Zhong, Ian E. H. Yen, Inderjit S. Dhillon, Pradeep Ravikumar category:stat.ML  published:2014-06-27 summary:We consider the class of optimization problems arising from computationally intensive L1-regularized M-estimators, where the function or gradient values are very expensive to compute. A particular instance of interest is the L1-regularized MLE for learning Conditional Random Fields (CRFs), which are a popular class of statistical models for varied structured prediction problems such as sequence labeling, alignment, and classification with label taxonomy. L1-regularized MLEs for CRFs are particularly expensive to optimize since computing the gradient values requires an expensive inference step. In this work, we propose the use of a carefully constructed proximal quasi-Newton algorithm for such computationally intensive M-estimation problems, where we employ an aggressive active set selection technique. In a key contribution of the paper, we show that the proximal quasi-Newton method is provably super-linearly convergent, even in the absence of strong convexity, by leveraging a restricted variant of strong convexity. In our experiments, the proposed algorithm converges considerably faster than current state-of-the-art on the problems of sequence labeling and hierarchical classification. version:2
arxiv-1406-7424 | Complexity Measures and Concept Learning | http://arxiv.org/abs/1406.7424 | id:1406.7424 author:Andreas D. Pape, Kenneth J. Kurtz, Hiroki Sayama category:cs.IT cs.LG math.IT  published:2014-06-28 summary:The nature of concept learning is a core question in cognitive science. Theories must account for the relative difficulty of acquiring different concepts by supervised learners. For a canonical set of six category types, two distinct orderings of classification difficulty have been found. One ordering, which we call paradigm-specific, occurs when adult human learners classify objects with easily distinguishable characteristics such as size, shape, and shading. The general order occurs in all other known cases: when adult humans classify objects with characteristics that are not readily distinguished (e.g., brightness, saturation, hue); for children and monkeys; and when categorization difficulty is extrapolated from errors in identification learning. The paradigm-specific order was found to be predictable mathematically by measuring the logical complexity of tasks, i.e., how concisely the solution can be represented by logical rules. However, logical complexity explains only the paradigm-specific order but not the general order. Here we propose a new difficulty measurement, information complexity, that calculates the amount of uncertainty remaining when a subset of the dimensions are specified. This measurement is based on Shannon entropy. We show that, when the metric extracts minimal uncertainties, this new measurement predicts the paradigm-specific order for the canonical six category types, and when the metric extracts average uncertainties, this new measurement predicts the general order. Moreover, for learning category types beyond the canonical six, we find that the minimal-uncertainty formulation correctly predicts the paradigm-specific order as well or better than existing metrics (Boolean complexity and GIST) in most cases. version:3
arxiv-1501-05854 | Unsupervised Segmentation of Multispectral Images with Cellular Automata | http://arxiv.org/abs/1501.05854 | id:1501.05854 author:Wuilian Torres, Antonio Rueda-Toicen category:cs.CV  published:2015-01-23 summary:Multispectral images acquired by satellites are used to study phenomena on the Earth's surface. Unsupervised classification techniques analyze multispectral image content without considering prior knowledge of the observed terrain; this is done using techniques which group pixels that have similar statistics of digital level distribution in the various image channels. In this paper, we propose a methodology for unsupervised classification based on a deterministic cellular automaton. The automaton is initialized in an unsupervised manner by setting seed cells, selected according to two criteria: to be representative of the spatial distribution of the dominant elements in the image, and to take into account the diversity of spectral signatures in the image. The automaton's evolution is based on an attack rule that is applied simultaneously to all its cells. Among the noteworthy advantages of deterministic cellular automata for multispectral processing of satellite imagery is the consideration of topological information in the image via seed positioning, and the ability to modify the scale of the study. version:1
arxiv-1501-05192 | A Graph Theoretic Approach for Object Shape Representation in Compositional Hierarchies Using a Hybrid Generative-Descriptive Model | http://arxiv.org/abs/1501.05192 | id:1501.05192 author:Umit Rusen Aktas, Mete Ozay, Ales Leonardis, Jeremy L. Wyatt category:cs.CV  published:2015-01-21 summary:A graph theoretic approach is proposed for object shape representation in a hierarchical compositional architecture called Compositional Hierarchy of Parts (CHOP). In the proposed approach, vocabulary learning is performed using a hybrid generative-descriptive model. First, statistical relationships between parts are learned using a Minimum Conditional Entropy Clustering algorithm. Then, selection of descriptive parts is defined as a frequent subgraph discovery problem, and solved using a Minimum Description Length (MDL) principle. Finally, part compositions are constructed by compressing the internal data representation with discovered substructures. Shape representation and computational complexity properties of the proposed approach and algorithms are examined using six benchmark two-dimensional shape image datasets. Experiments show that CHOP can employ part shareability and indexing mechanisms for fast inference of part compositions using learned shape vocabularies. Additionally, CHOP provides better shape retrieval performance than the state-of-the-art shape retrieval methods. version:2
arxiv-1501-05790 | Taking a Deeper Look at Pedestrians | http://arxiv.org/abs/1501.05790 | id:1501.05790 author:Jan Hosang, Mohamed Omran, Rodrigo Benenson, Bernt Schiele category:cs.CV  published:2015-01-23 summary:In this paper we study the use of convolutional neural networks (convnets) for the task of pedestrian detection. Despite their recent diverse successes, convnets historically underperform compared to other pedestrian detectors. We deliberately omit explicitly modelling the problem into the network (e.g. parts or occlusion modelling) and show that we can reach competitive performance without bells and whistles. In a wide range of experiments we analyse small and big convnets, their architectural choices, parameters, and the influence of different training data, including pre-training on surrogate tasks. We present the best convnet detectors on the Caltech and KITTI dataset. On Caltech our convnets reach top performance both for the Caltech1x and Caltech10x training setup. Using additional data at training time our strongest convnet model is competitive even to detectors that use additional data (optical flow) at test time. version:1
arxiv-1501-05759 | Filtered Channel Features for Pedestrian Detection | http://arxiv.org/abs/1501.05759 | id:1501.05759 author:Shanshan Zhang, Rodrigo Benenson, Bernt Schiele category:cs.CV  published:2015-01-23 summary:This paper starts from the observation that multiple top performing pedestrian detectors can be modelled by using an intermediate layer filtering low-level features in combination with a boosted decision forest. Based on this observation we propose a unifying framework and experimentally explore different filter families. We report extensive results enabling a systematic analysis. Using filtered channel features we obtain top performance on the challenging Caltech and KITTI datasets, while using only HOG+LUV as low-level features. When adding optical flow features we further improve detection quality and report the best known results on the Caltech dataset, reaching 93% recall at 1 FPPI. version:1
arxiv-1303-2071 | Application of the SP theory of intelligence to the understanding of natural vision and the development of computer vision | http://arxiv.org/abs/1303.2071 | id:1303.2071 author:J. Gerard Wolff category:cs.CV cs.AI  published:2013-03-08 summary:The SP theory of intelligence aims to simplify and integrate concepts in computing and cognition, with information compression as a unifying theme. This article discusses how it may be applied to the understanding of natural vision and the development of computer vision. The theory, which is described quite fully elsewhere, is described here in outline but with enough detail to ensure that the rest of the article makes sense. Low level perceptual features such as edges or corners may be identified by the extraction of redundancy in uniform areas in a manner that is comparable with the run-length encoding technique for information compression. The concept of multiple alignment in the SP theory may be applied to the recognition of objects, and to scene analysis, with a hierarchy of parts and sub-parts, and at multiple levels of abstraction. The theory has potential for the unsupervised learning of visual objects and classes of objects, and suggests how coherent concepts may be derived from fragments. As in natural vision, both recognition and learning in the SP system is robust in the face of errors of omission, commission and substitution. The theory suggests how, via vision, we may piece together a knowledge of the three-dimensional structure of objects and of our environment, it provides an account of how we may see things that are not objectively present in an image, and how we recognise something despite variations in the size of its retinal image. And it has things to say about the phenomena of lightness constancy and colour constancy, the role of context in recognition, and ambiguities in visual perception. A strength of the SP theory is that it provides for the integration of vision with other sensory modalities and with other aspects of intelligence. version:2
arxiv-1501-05740 | Bayesian Learning for Low-Rank matrix reconstruction | http://arxiv.org/abs/1501.05740 | id:1501.05740 author:Martin Sundin, Cristian R. Rojas, Magnus Jansson, Saikat Chatterjee category:stat.ML cs.LG cs.NA  published:2015-01-23 summary:We develop latent variable models for Bayesian learning based low-rank matrix completion and reconstruction from linear measurements. For under-determined systems, the developed methods are shown to reconstruct low-rank matrices when neither the rank nor the noise power is known a-priori. We derive relations between the latent variable models and several low-rank promoting penalty functions. The relations justify the use of Kronecker structured covariance matrices in a Gaussian based prior. In the methods, we use evidence approximation and expectation-maximization to learn the model parameters. The performance of the methods is evaluated through extensive numerical simulations. version:1
arxiv-1411-7717 | On the Expressive Efficiency of Sum Product Networks | http://arxiv.org/abs/1411.7717 | id:1411.7717 author:James Martens, Venkatesh Medabalimi category:cs.LG stat.ML  published:2014-11-27 summary:Sum Product Networks (SPNs) are a recently developed class of deep generative models which compute their associated unnormalized density functions using a special type of arithmetic circuit. When certain sufficient conditions, called the decomposability and completeness conditions (or "D&C" conditions), are imposed on the structure of these circuits, marginal densities and other useful quantities, which are typically intractable for other deep generative models, can be computed by what amounts to a single evaluation of the network (which is a property known as "validity"). However, the effect that the D&C conditions have on the capabilities of D&C SPNs is not well understood. In this work we analyze the D&C conditions, expose the various connections that D&C SPNs have with multilinear arithmetic circuits, and consider the question of how well they can capture various distributions as a function of their size and depth. Among our various contributions is a result which establishes the existence of a relatively simple distribution with fully tractable marginal densities which cannot be efficiently captured by D&C SPNs of any depth, but which can be efficiently captured by various other deep generative models. We also show that with each additional layer of depth permitted, the set of distributions which can be efficiently captured by D&C SPNs grows in size. This kind of "depth hierarchy" property has been widely conjectured to hold for various deep models, but has never been proven for any of them. Some of our other contributions include a new characterization of the D&C conditions as sufficient and necessary ones for a slightly strengthened notion of validity, and various state-machine characterizations of the types of computations that can be performed efficiently by D&C SPNs. version:3
arxiv-1501-05684 | Bi-Objective Nonnegative Matrix Factorization: Linear Versus Kernel-Based Models | http://arxiv.org/abs/1501.05684 | id:1501.05684 author:Paul Honeine, Fei Zhu category:stat.ML cs.CV cs.LG math.OC  published:2015-01-22 summary:Nonnegative matrix factorization (NMF) is a powerful class of feature extraction techniques that has been successfully applied in many fields, namely in signal and image processing. Current NMF techniques have been limited to a single-objective problem in either its linear or nonlinear kernel-based formulation. In this paper, we propose to revisit the NMF as a multi-objective problem, in particular a bi-objective one, where the objective functions defined in both input and feature spaces are taken into account. By taking the advantage of the sum-weighted method from the literature of multi-objective optimization, the proposed bi-objective NMF determines a set of nondominated, Pareto optimal, solutions instead of a single optimal decomposition. Moreover, the corresponding Pareto front is studied and approximated. Experimental results on unmixing real hyperspectral images confirm the efficiency of the proposed bi-objective NMF compared with the state-of-the-art methods. version:1
arxiv-1501-05680 | Active Mean Fields for Probabilistic Image Segmentation: Connections with Chan-Vese and Rudin-Osher-Fatemi Models | http://arxiv.org/abs/1501.05680 | id:1501.05680 author:Marc Niethammer, Kilian M. Pohl, Firdaus Janoos, William M. Wells III category:cs.CV  published:2015-01-22 summary:Image segmentation is a fundamental task for extracting semantically meaningful regions from an image. The goal is to assign object labels to each image location. Due to image-noise, shortcomings of algorithms and other ambiguities in the images, there is uncertainty in the assigned labels. In multiple application domains, estimates of this uncertainty are important. For example, object segmentation and uncertainty quantification is essential for many medical application, including tumor segmentation for radiation treatment planning. While a Bayesian characterization of the label posterior provides estimates of segmentation uncertainty, Bayesian approaches can be computationally prohibitive for practical applications. On the other hand, typical optimization based algorithms are computationally very efficient, but only provide maximum a-posteriori solutions and hence no estimates of label uncertainty. In this paper, we propose Active Mean Fields (AMF), a Bayesian technique that uses a mean-field approximation to derive an efficient segmentation and uncertainty quantification algorithm. This model, which allows combining any label-likelihood measure with a boundary length prior, yields a variational formulation that is convex. A specific implementation of that model is the Chan--Vese segmentation model (CV), which formulates the binary segmentation problem through Gaussian likelihoods combined with a boundary-length regularizer. Furthermore, the Euler--Lagrange equations derived from the AMF model are equivalent to those of the popular Rudin-Osher-Fatemi (ROF) model for image de-noising. Solutions to the AMF model can thus be implemented by directly utilizing highly-efficient ROF solvers on log-likelihood ratio fields. We demonstrate the approach using synthetic data, as well as real medical images (for heart and prostate segmentations), and on standard computer vision test images. version:1
arxiv-1501-05940 | A New Efficient Method for Calculating Similarity Between Web Services | http://arxiv.org/abs/1501.05940 | id:1501.05940 author:T. Rachad, J. Boutahar, S. El ghazi category:cs.AI cs.CL cs.IR cs.SE  published:2015-01-22 summary:Web services allow communication between heterogeneous systems in a distributed environment. Their enormous success and their increased use led to the fact that thousands of Web services are present on the Internet. This significant number of Web services which not cease to increase has led to problems of the difficulty in locating and classifying web services, these problems are encountered mainly during the operations of web services discovery and substitution. Traditional ways of search based on keywords are not successful in this context, their results do not support the structure of Web services and they consider in their search only the identifiers of the web service description language (WSDL) interface elements. The methods based on semantics (WSDLS, OWLS, SAWSDL...) which increase the WSDL description of a Web service with a semantic description allow raising partially this problem, but their complexity and difficulty delays their adoption in real cases. Measuring the similarity between the web services interfaces is the most suitable solution for this kind of problems, it will classify available web services so as to know those that best match the searched profile and those that do not match. Thus, the main goal of this work is to study the degree of similarity between any two web services by offering a new method that is more effective than existing works. version:1
arxiv-1003-1072 | An Offline Technique for Localization of License Plates for Indian Commercial Vehicles | http://arxiv.org/abs/1003.1072 | id:1003.1072 author:Satadal Saha, Subhadip Basu, Mita Nasipuri, Dipak Kumar Basu category:cs.CV  published:2010-03-04 summary:Automatic License Plate Recognition (ALPR) is a challenging area of research due to its importance to variety of commercial applications. The overall problem may be subdivided into two key modules, firstly, localization of license plates from vehicle images, and secondly, optical character recognition of extracted license plates. In the current work, we have concentrated on the first part of the problem, i.e., localization of license plate regions from Indian commercial vehicles as a significant step towards development of a complete ALPR system for Indian vehicles. The technique is based on color based segmentation of vehicle images and identification of potential license plate regions. True license plates are finally localized based on four spatial and horizontal contrast features. The technique successfully localizes the actual license plates in 73.4% images. version:2
arxiv-1003-6052 | Development of an automated Red Light Violation Detection System (RLVDS) for Indian vehicles | http://arxiv.org/abs/1003.6052 | id:1003.6052 author:Satadal Saha, Subhadip Basu, Mita Nasipuri, Dipak Kumar Basu category:cs.CV  published:2010-03-31 summary:Integrated Traffic Management Systems (ITMS) are now implemented in different cities in India to primarily address the concerns of road-safety and security. An automated Red Light Violation Detection System (RLVDS) is an integral part of the ITMS. In our present work we have designed and developed a complete system for generating the list of all stop-line violating vehicle images automatically from video snapshots of road-side surveillance cameras. The system first generates adaptive background images for each camera view, subtracts captured images from the corresponding background images and analyses potential occlusions over the stop-line in a traffic signal. Considering round-the-clock operations in a real-life test environment, the developed system could successfully track 92% images of vehicles with violations on the stop-line in a "Red" traffic signal. version:2
arxiv-1003-6059 | A novel scheme for binarization of vehicle images using hierarchical histogram equalization technique | http://arxiv.org/abs/1003.6059 | id:1003.6059 author:Satadal Saha, Subhadip Basu, Mita Nasipuri, Dipak Kumar Basu category:cs.CV  published:2010-03-31 summary:Automatic License Plate Recognition system is a challenging area of research now-a-days and binarization is an integral and most important part of it. In case of a real life scenario, most of existing methods fail to properly binarize the image of a vehicle in a congested road, captured through a CCD camera. In the current work we have applied histogram equalization technique over the complete image and also over different hierarchy of image partitioning. A novel scheme is formulated for giving the membership value to each pixel for each hierarchy of histogram equalization. Then the image is binarized depending on the net membership value of each pixel. The technique is exhaustively evaluated on the vehicle image dataset as well as the license plate dataset, giving satisfactory performances. version:2
arxiv-1501-05624 | A Collaborative Kalman Filter for Time-Evolving Dyadic Processes | http://arxiv.org/abs/1501.05624 | id:1501.05624 author:San Gultekin, John Paisley category:stat.ML cs.LG  published:2015-01-22 summary:We present the collaborative Kalman filter (CKF), a dynamic model for collaborative filtering and related factorization models. Using the matrix factorization approach to collaborative filtering, the CKF accounts for time evolution by modeling each low-dimensional latent embedding as a multidimensional Brownian motion. Each observation is a random variable whose distribution is parameterized by the dot product of the relevant Brownian motions at that moment in time. This is naturally interpreted as a Kalman filter with multiple interacting state space vectors. We also present a method for learning a dynamically evolving drift parameter for each location by modeling it as a geometric Brownian motion. We handle posterior intractability via a mean-field variational approximation, which also preserves tractability for downstream calculations in a manner similar to the Kalman filter. We evaluate the model on several large datasets, providing quantitative evaluation on the 10 million Movielens and 100 million Netflix datasets and qualitative evaluation on a set of 39 million stock returns divided across roughly 6,500 companies from the years 1962-2014. version:1
arxiv-1501-05617 | Unsupervised image segmentation by Global and local Criteria Optimization Based on Bayesian Networks | http://arxiv.org/abs/1501.05617 | id:1501.05617 author:Mohamed Ali Mahjoub, Mohamed Mhiri category:cs.CV  published:2015-01-22 summary:Today Bayesian networks are more used in many areas of decision support and image processing. In this way, our proposed approach uses Bayesian Network to modelize the segmented image quality. This quality is calculated on a set of attributes that represent local evaluation measures. The idea is to have these local levels chosen in a way to be intersected into them to keep the overall appearance of segmentation. The approach operates in two phases: the first phase is to make an over-segmentation which gives superpixels card. In the second phase, we model the superpixels by a Bayesian Network. To find the segmented image with the best overall quality we used two approximate inference methods, the first using ICM algorithm which is widely used in Markov Models and a second is a recursive method called algorithm of model decomposition based on max-product algorithm which is very popular in the recent works of image segmentation. For our model, we have shown that the composition of these two algorithms leads to good segmentation performance. version:1
arxiv-1501-05590 | Sketch and Validate for Big Data Clustering | http://arxiv.org/abs/1501.05590 | id:1501.05590 author:Panagiotis A. Traganitis, Konstantinos Slavakis, Georgios B. Giannakis category:stat.ML cs.LG  published:2015-01-22 summary:In response to the need for learning tools tuned to big data analytics, the present paper introduces a framework for efficient clustering of huge sets of (possibly high-dimensional) data. Building on random sampling and consensus (RANSAC) ideas pursued earlier in a different (computer vision) context for robust regression, a suite of novel dimensionality and set-reduction algorithms is developed. The advocated sketch-and-validate (SkeVa) family includes two algorithms that rely on K-means clustering per iteration on reduced number of dimensions and/or feature vectors: The first operates in a batch fashion, while the second sequential one offers computational efficiency and suitability with streaming modes of operation. For clustering even nonlinearly separable vectors, the SkeVa family offers also a member based on user-selected kernel functions. Further trading off performance for reduced complexity, a fourth member of the SkeVa family is based on a divergence criterion for selecting proper minimal subsets of feature variables and vectors, thus bypassing the need for K-means clustering per iteration. Extensive numerical tests on synthetic and real data sets highlight the potential of the proposed algorithms, and demonstrate their competitive performance relative to state-of-the-art random projection alternatives. version:1
arxiv-1501-05552 | Estimating the Intrinsic Dimension of Hyperspectral Images Using an Eigen-Gap Approach | http://arxiv.org/abs/1501.05552 | id:1501.05552 author:A. Halimi, P. Honeine, M. Kharouf, C. Richard, J. -Y. Tourneret category:stat.AP cs.CV  published:2015-01-22 summary:Linear mixture models are commonly used to represent hyperspectral datacube as a linear combinations of endmember spectra. However, determining of the number of endmembers for images embedded in noise is a crucial task. This paper proposes a fully automatic approach for estimating the number of endmembers in hyperspectral images. The estimation is based on recent results of random matrix theory related to the so-called spiked population model. More precisely, we study the gap between successive eigenvalues of the sample covariance matrix constructed from high dimensional noisy samples. The resulting estimation strategy is unsupervised and robust to correlated noise. This strategy is validated on both synthetic and real images. The experimental results are very promising and show the accuracy of this algorithm with respect to state-of-the-art algorithms. version:1
arxiv-1408-4712 | Bi-l0-l2-Norm Regularization for Blind Motion Deblurring | http://arxiv.org/abs/1408.4712 | id:1408.4712 author:Wen-Ze Shao, Hai-Bo Li, Michael Elad category:cs.CV  published:2014-08-20 summary:In blind motion deblurring, leading methods today tend towards highly non-convex approximations of the l0-norm, especially in the image regularization term. In this paper, we propose a simple, effective and fast approach for the estimation of the motion blur-kernel, through a bi-l0-l2-norm regularization imposed on both the intermediate sharp image and the blur-kernel. Compared with existing methods, the proposed regularization is shown to be more effective and robust, leading to a more accurate motion blur-kernel and a better final restored image. A fast numerical scheme is deployed for alternatingly computing the sharp image and the blur-kernel, by coupling the operator splitting and augmented Lagrangian methods. Experimental results on both a benchmark image dataset and real-world motion blurred images show that the proposed approach is highly competitive with state-of-the- art methods in both deblurring effectiveness and computational efficiency. version:3
arxiv-1501-05497 | An Improved Feature Descriptor for Recognition of Handwritten Bangla Alphabet | http://arxiv.org/abs/1501.05497 | id:1501.05497 author:Nibaran Das, Subhadip Basu, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri, Dipak kumar Basu category:cs.CV  published:2015-01-22 summary:Appropriate feature set for representation of pattern classes is one of the most important aspects of handwritten character recognition. The effectiveness of features depends on the discriminating power of the features chosen to represent patterns of different classes. However, discriminatory features are not easily measurable. Investigative experimentation is necessary for identifying discriminatory features. In the present work we have identified a new variation of feature set which significantly outperforms on handwritten Bangla alphabet from the previously used feature set. 132 number of features in all viz. modified shadow features, octant and centroid features, distance based features, quad tree based longest run features are used here. Using this feature set the recognition performance increases sharply from the 75.05% observed in our previous work [7], to 85.40% on 50 character classes with MLP based classifier on the same dataset. version:1
arxiv-1501-05495 | A GA Based approach for selection of local features for recognition of handwritten Bangla numerals | http://arxiv.org/abs/1501.05495 | id:1501.05495 author:Nibaran Das, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu, Mita Nasipuri category:cs.CV  published:2015-01-22 summary:Soft computing approaches are mainly designed to address the real world ill-defined, imprecisely formulated problems, combining different kind of novel models of computation, such as neural networks, genetic algorithms (GAs. Handwritten digit recognition is a typical example of one such problem. In the current work we have developed a two-pass approach where the first pass classifier performs a coarse classification, based on some global features of the input pattern by restricting the possibility of classification decisions within a group of classes, smaller than the number of classes considered initially. In the second pass, the group specific classifiers concentrate on the features extracted from the selected local regions, and refine the earlier decision by combining the local and the global features for selecting the true class of the input pattern from the group of candidate classes selected in the first pass. To optimize the selection of local regions a GA based approach has been developed here. The maximum recognition performance on Bangla digit samples as achieved on the test set, during the first pass of the two pass approach is 93.35%. After combining the results of the two stage classifiers, an overall success rate of 95.25% is achieved. version:1
arxiv-1501-05494 | Design of a novel convex hull based feature set for recognition of isolated handwritten Roman numerals | http://arxiv.org/abs/1501.05494 | id:1501.05494 author:Nibaran Das, Sandip Pramanik, Subhadip Basu, Punam Kumar Saha, Ram Sarkar, Mahantapas Kundu category:cs.CV  published:2015-01-22 summary:In this paper, convex hull based features are used for recognition of isolated Roman numerals using a Multi Layer Perceptron (MLP) based classifier. Experiments of convex hull based features for handwritten character recognition are few in numbers. Convex hull of a pattern and the centroid of the convex hull both are affine invariant attributes. In this work, 25 features are extracted based on different bays attributes of the convex hull of the digit patterns. Then these patterns are divided into four sub-images with respect to the centroid of the convex hull boundary. From each such sub-image 25 bays features are also calculated. In all 125 convex hull based features are extracted for each numeric digit patterns under the current experiment. The performance of the designed feature set is tested on the standard MNIST data set, consisting of 60000 training and 10000 test images of handwritten Roman using an MLP based classifier a maximum success rate of 97.44% is achieved on the test data. version:1
arxiv-1501-05141 | An Algebra to Merge Heterogeneous Classifiers | http://arxiv.org/abs/1501.05141 | id:1501.05141 author:Philippe J. Giabbanelli, Joseph G. Peters category:cs.DM cs.LG 97R50  08Axx H.2.8; I.1.2; I.5.2  published:2015-01-21 summary:In distributed classification, each learner observes its environment and deduces a classifier. As a learner has only a local view of its environment, classifiers can be exchanged among the learners and integrated, or merged, to improve accuracy. However, the operation of merging is not defined for most classifiers. Furthermore, the classifiers that have to be merged may be of different types in settings such as ad-hoc networks in which several generations of sensors may be creating classifiers. We introduce decision spaces as a framework for merging possibly different classifiers. We formally study the merging operation as an algebra, and prove that it satisfies a desirable set of properties. The impact of time is discussed for the two main data mining settings. Firstly, decision spaces can naturally be used with non-stationary distributions, such as the data collected by sensor networks, as the impact of a model decays over time. Secondly, we introduce an approach for stationary distributions, such as homogeneous databases partitioned over different learners, which ensures that all models have the same impact. We also present a method that uses storage flexibly to achieve different types of decay for non-stationary distributions. Finally, we show that the algebraic approach developed for merging can also be used to analyze the behaviour of other operators. version:2
arxiv-1501-05472 | Handwritten Devanagari Script Segmentation: A non-linear Fuzzy Approach | http://arxiv.org/abs/1501.05472 | id:1501.05472 author:Ram Sarkar, Bibhash Sen, Nibaran Das, Subhadip Basu category:cs.CV  published:2015-01-22 summary:The paper concentrates on improvement of segmentation accuracy by addressing some of the key challenges of handwritten Devanagari word image segmentation technique. In the present work, we have developed a new feature based approach for identification of Matra pixels from a word image, design of a non-linear fuzzy membership functions for headline estimation and finally design of a non-linear fuzzy functions for identifying segmentation points on the Matra. The segmentation accuracy achieved by the current technique is 94.8%. This shows an improvement of performance by 1.8% over the previous technique [1] on a 300-word dataset, used for the current experiment. version:1
arxiv-1402-0480 | Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets | http://arxiv.org/abs/1402.0480 | id:1402.0480 author:Diederik P. Kingma, Max Welling category:cs.LG stat.ML  published:2014-02-03 summary:Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the non-centered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments. version:5
arxiv-1501-05432 | Point Context: An Effective Shape Descriptor for RST-invariant Trajectory Recognition | http://arxiv.org/abs/1501.05432 | id:1501.05432 author:Xingyu Wu, Xia Mao, Lijiang Chen, Yuli Xue, Angelo Compare category:cs.CV 51A05 I.2.10; I.5.4  published:2015-01-22 summary:Motion trajectory recognition is important for characterizing the moving property of an object. The speed and accuracy of trajectory recognition rely on a compact and discriminative feature representation, and the situations of varying rotation, scaling and translation has to be specially considered. In this paper we propose a novel feature extraction method for trajectories. Firstly a trajectory is represented by a proposed point context, which is a rotation-scale-translation (RST) invariant shape descriptor with a flexible tradeoff between computational complexity and discrimination, yet we prove that it is a complete shape descriptor. Secondly, the shape context is nonlinearly mapped to a subspace by kernel nonparametric discriminant analysis (KNDA) to get a compact feature representation, and thus a trajectory is projected to a single point in a low-dimensional feature space. Experimental results show that, the proposed trajectory feature shows encouraging improvement than state-of-art methods. version:1
arxiv-1309-1233 | Noisy Sparse Subspace Clustering | http://arxiv.org/abs/1309.1233 | id:1309.1233 author:Yu-Xiang Wang, Huan Xu category:stat.ML  published:2013-09-05 summary:This paper considers the problem of subspace clustering under noise. Specifically, we study the behavior of Sparse Subspace Clustering (SSC) when either adversarial or random noise is added to the unlabelled input data points, which are assumed to be in a union of low-dimensional subspaces. We show that a modified version of SSC is \emph{provably effective} in correctly identifying the underlying subspaces, even with noisy data. This extends theoretical guarantee of this algorithm to more practical settings and provides justification to the success of SSC in a class of real applications. version:2
arxiv-1501-05396 | Deep Multimodal Learning for Audio-Visual Speech Recognition | http://arxiv.org/abs/1501.05396 | id:1501.05396 author:Youssef Mroueh, Etienne Marcheret, Vaibhava Goel category:cs.CL cs.LG  published:2015-01-22 summary:In this paper, we present methods in deep multimodal learning for fusing speech and visual modalities for Audio-Visual Automatic Speech Recognition (AV-ASR). First, we study an approach where uni-modal deep networks are trained separately and their final hidden layers fused to obtain a joint feature space in which another deep network is built. While the audio network alone achieves a phone error rate (PER) of $41\%$ under clean condition on the IBM large vocabulary audio-visual studio dataset, this fusion model achieves a PER of $35.83\%$ demonstrating the tremendous value of the visual channel in phone classification even in audio with high signal to noise ratio. Second, we present a new deep network architecture that uses a bilinear softmax layer to account for class specific correlations between modalities. We show that combining the posteriors from the bilinear networks with those from the fused model mentioned above results in a further significant phone error rate reduction, yielding a final PER of $34.03\%$. version:1
arxiv-1501-05349 | Exploiting Big Data in Logistics Risk Assessment via Bayesian Nonparametrics | http://arxiv.org/abs/1501.05349 | id:1501.05349 author:Yan Shang, David B. Dunson, Jing-Sheng Song category:stat.AP stat.ML 62-07  published:2015-01-21 summary:In cargo logistics, a key performance measure is transport risk, defined as the deviation of the actual arrival time from the planned arrival time. Neither earliness nor tardiness is desirable for customer and freight forwarders. In this paper, we investigate ways to assess and forecast transport risks using a half-year of air cargo data, provided by a leading forwarder on 1336 routes served by 20 airlines. Interestingly, our preliminary data analysis shows a strong multimodal feature in the transport risks, driven by unobserved events, such as cargo missing flights. To accommodate this feature, we introduce a Bayesian nonparametric model -- the probit stick-breaking process (PSBP) mixture model -- for flexible estimation of the conditional (i.e., state-dependent) density function of transport risk. We demonstrate that using simpler methods, such as OLS linear regression, can lead to misleading inferences. Our model provides a tool for the forwarder to offer customized price and service quotes. It can also generate baseline airline performance to enable fair supplier evaluation. Furthermore, the method allows us to separate recurrent risks from disruption risks. This is important, because hedging strategies for these two kinds of risks are often drastically different. version:1
arxiv-1412-0233 | The Loss Surfaces of Multilayer Networks | http://arxiv.org/abs/1412.0233 | id:1412.0233 author:Anna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, Yann LeCun category:cs.LG  published:2014-11-30 summary:We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting. version:3
arxiv-1409-1576 | Machine Learning Etudes in Astrophysics: Selection Functions for Mock Cluster Catalogs | http://arxiv.org/abs/1409.1576 | id:1409.1576 author:Amir Hajian, Marcelo Alvarez, J. Richard Bond category:astro-ph.CO astro-ph.IM cs.LG stat.ML  published:2014-09-04 summary:Making mock simulated catalogs is an important component of astrophysical data analysis. Selection criteria for observed astronomical objects are often too complicated to be derived from first principles. However the existence of an observed group of objects is a well-suited problem for machine learning classification. In this paper we use one-class classifiers to learn the properties of an observed catalog of clusters of galaxies from ROSAT and to pick clusters from mock simulations that resemble the observed ROSAT catalog. We show how this method can be used to study the cross-correlations of thermal Sunya'ev-Zeldovich signals with number density maps of X-ray selected cluster catalogs. The method reduces the bias due to hand-tuning the selection function and is readily scalable to large catalogs with a high-dimensional space of astrophysical features. version:2
arxiv-1501-04659 | On the impact of topological properties of smart grids in power losses optimization problems | http://arxiv.org/abs/1501.04659 | id:1501.04659 author:Francesca Possemato, Maurizio Paschero, Lorenzo Livi, Antonello Rizzi, Alireza Sadeghian category:cs.CE cs.NE  published:2015-01-19 summary:Power losses reduction is one of the main targets for any electrical energy distribution company. In this paper, we face the problem of joint optimization of both topology and network parameters in a real smart grid. We consider a portion of the Italian electric distribution network managed by the ACEA Distribuzione S.p.A. located in Rome. We perform both the power factor correction (PFC) for tuning the generators and the distributed feeder reconfiguration (DFR) to set the state of the breakers. This joint optimization problem is faced considering a suitable objective function and by adopting genetic algorithms as global optimization strategy. We analyze admissible network configurations, showing that some of these violate constraints on current and voltage at branches and nodes. Such violations depend only on pure topological properties of the configurations. We perform tests by feeding the simulation environment with real data concerning hourly samples of dissipated and generated active and reactive power values of the ACEA smart grid. Results show that removing the configurations violating the electrical constraints from the solution space leads to interesting improvements in terms of power loss reduction. To conclude, we provide also an electrical interpretation of the phenomenon using graph-based pattern analysis techniques. version:2
arxiv-1501-05279 | Extreme Entropy Machines: Robust information theoretic classification | http://arxiv.org/abs/1501.05279 | id:1501.05279 author:Wojciech Marian Czarnecki, Jacek Tabor category:cs.LG  published:2015-01-21 summary:Most of the existing classification methods are aimed at minimization of empirical risk (through some simple point-based error measured with loss function) with added regularization. We propose to approach this problem in a more information theoretic way by investigating applicability of entropy measures as a classification model objective function. We focus on quadratic Renyi's entropy and connected Cauchy-Schwarz Divergence which leads to the construction of Extreme Entropy Machines (EEM). The main contribution of this paper is proposing a model based on the information theoretic concepts which on the one hand shows new, entropic perspective on known linear classifiers and on the other leads to a construction of very robust method competetitive with the state of the art non-information theoretic ones (including Support Vector Machines and Extreme Learning Machines). Evaluation on numerous problems spanning from small, simple ones from UCI repository to the large (hundreads of thousands of samples) extremely unbalanced (up to 100:1 classes' ratios) datasets shows wide applicability of the EEM in real life problems and that it scales well. version:1
arxiv-1501-05222 | Plug-and-play dual-tree algorithm runtime analysis | http://arxiv.org/abs/1501.05222 | id:1501.05222 author:Ryan R. Curtin, Dongryeol Lee, William B. March, Parikshit Ram category:cs.DS cs.LG  published:2015-01-21 summary:Numerous machine learning algorithms contain pairwise statistical problems at their core---that is, tasks that require computations over all pairs of input points if implemented naively. Often, tree structures are used to solve these problems efficiently. Dual-tree algorithms can efficiently solve or approximate many of these problems. Using cover trees, rigorous worst-case runtime guarantees have been proven for some of these algorithms. In this paper, we present a problem-independent runtime guarantee for any dual-tree algorithm using the cover tree, separating out the problem-dependent and the problem-independent elements. This allows us to just plug in bounds for the problem-dependent elements to get runtime guarantees for dual-tree algorithms for any pairwise statistical problem without re-deriving the entire proof. We demonstrate this plug-and-play procedure for nearest-neighbor search and approximate kernel density estimation to get improved runtime guarantees. Under mild assumptions, we also present the first linear runtime guarantee for dual-tree based range search. version:1
arxiv-1501-05200 | Minimax Optimal Sparse Signal Recovery with Poisson Statistics | http://arxiv.org/abs/1501.05200 | id:1501.05200 author:Mohammad H. Rohban, Delaram Motamedvaziri, Venkatesh Saligrama category:stat.ML  published:2015-01-21 summary:We are motivated by problems that arise in a number of applications such as Online Marketing and Explosives detection, where the observations are usually modeled using Poisson statistics. We model each observation as a Poisson random variable whose mean is a sparse linear superposition of known patterns. Unlike many conventional problems observations here are not identically distributed since they are associated with different sensing modalities. We analyze the performance of a Maximum Likelihood (ML) decoder, which for our Poisson setting involves a non-linear optimization but yet is computationally tractable. We derive fundamental sample complexity bounds for sparse recovery when the measurements are contaminated with Poisson noise. In contrast to the least-squares linear regression setting with Gaussian noise, we observe that in addition to sparsity, the scale of the parameters also fundamentally impacts $\ell_2$ error in the Poisson setting. We show tightness of our upper bounds both theoretically and experimentally. In particular, we derive a minimax matching lower bound on the mean-squared error and show that our constrained ML decoder is minimax optimal for this regime. version:1
arxiv-1501-05152 | Mirror, mirror on the wall, tell me, is the error small? | http://arxiv.org/abs/1501.05152 | id:1501.05152 author:Heng Yang, Ioannis Patras category:cs.CV  published:2015-01-21 summary:Do object part localization methods produce bilaterally symmetric results on mirror images? Surprisingly not, even though state of the art methods augment the training set with mirrored images. In this paper we take a closer look into this issue. We first introduce the concept of mirrorability as the ability of a model to produce symmetric results in mirrored images and introduce a corresponding measure, namely the \textit{mirror error} that is defined as the difference between the detection result on an image and the mirror of the detection result on its mirror image. We evaluate the mirrorability of several state of the art algorithms in two of the most intensively studied problems, namely human pose estimation and face alignment. Our experiments lead to several interesting findings: 1) Surprisingly, most of state of the art methods struggle to preserve the mirror symmetry, despite the fact that they do have very similar overall performance on the original and mirror images; 2) the low mirrorability is not caused by training or testing sample bias - all algorithms are trained on both the original images and their mirrored versions; 3) the mirror error is strongly correlated to the localization/alignment error (with correlation coefficients around 0.7). Since the mirror error is calculated without knowledge of the ground truth, we show two interesting applications - in the first it is used to guide the selection of difficult samples and in the second to give feedback in a popular Cascaded Pose Regression method for face alignment. version:1
arxiv-1501-05144 | Lazier ABC | http://arxiv.org/abs/1501.05144 | id:1501.05144 author:Dennis Prangle category:stat.CO stat.ML  published:2015-01-21 summary:ABC algorithms involve a large number of simulations from the model of interest, which can be very computationally costly. This paper summarises the lazy ABC algorithm of Prangle (2015), which reduces the computational demand by abandoning many unpromising simulations before completion. By using a random stopping decision and reweighting the output sample appropriately, the target distribution is the same as for standard ABC. Lazy ABC is also extended here to the case of non-uniform ABC kernels, which is shown to simplify the process of tuning the algorithm effectively. version:1
arxiv-1412-8765 | A General Theory of Hypothesis Tests and Confidence Regions for Sparse High Dimensional Models | http://arxiv.org/abs/1412.8765 | id:1412.8765 author:Yang Ning, Han Liu category:stat.ML  published:2014-12-30 summary:We consider the problem of uncertainty assessment for low dimensional components in high dimensional models. Specifically, we propose a decorrelated score function to handle the impact of high dimensional nuisance parameters. We consider both hypothesis tests and confidence regions for generic penalized M-estimators. Unlike most existing inferential methods which are tailored for individual models, our approach provides a general framework for high dimensional inference and is applicable to a wide range of applications. From the testing perspective, we develop general theorems to characterize the limiting distributions of the decorrelated score test statistic under both null hypothesis and local alternatives. These results provide asymptotic guarantees on the type I errors and local powers of the proposed test. Furthermore, we show that the decorrelated score function can be used to construct point and confidence region estimators that are semiparametrically efficient. We also generalize this framework to broaden its applications. First, we extend it to handle high dimensional null hypothesis, where the number of parameters of interest can increase exponentially fast with the sample size. Second, we establish the theory for model misspecification. Third, we go beyond the likelihood framework, by introducing the generalized score test based on general loss functions. Thorough numerical studies are conducted to back up the developed theoretical results. version:2
arxiv-1502-07743 | Tracking an Object with Unknown Accelerations using a Shadowing Filter | http://arxiv.org/abs/1502.07743 | id:1502.07743 author:Kevin Judd category:cs.SY cs.CV math.OC  published:2015-01-21 summary:A commonly encountered problem is the tracking of a physical object, like a maneuvering ship, aircraft, land vehicle, spacecraft or animate creature carrying a wireless device. The sensor data is often limited and inaccurate observations of range or bearing. This problem is more difficult than tracking a ballistic trajectory, because an operative affects unknown and arbitrarily changing accelerations. Although stochastic methods of filtering or state estimation (Kalman filters and particle filters) are widely used, out of vogue variational methods are more appropriate in this tracking context, because the objects do not typically display any significant random motions at the length and time scales of interest. This leads us to propose a rather elegant approach based on a \emph{shadowing filter}. The resulting filter is efficient (reduces to the solution of linear equations) and robust (uneffected by missing data and singular correlations that would cause catastrophic failure of Bayesian filters.) The tracking is so robust, that in some common situations it actually performs better by ignoring error correlations that are so vital to Kalman filters. version:1
arxiv-1501-05069 | Convergent Bayesian formulations of blind source separation and electromagnetic source estimation | http://arxiv.org/abs/1501.05069 | id:1501.05069 author:Kevin H. Knuth, Herbert G. Vaughan Jr category:physics.data-an physics.med-ph stat.ML  published:2015-01-21 summary:We consider two areas of research that have been developing in parallel over the last decade: blind source separation (BSS) and electromagnetic source estimation (ESE). BSS deals with the recovery of source signals when only mixtures of signals can be obtained from an array of detectors and the only prior knowledge consists of some information about the nature of the source signals. On the other hand, ESE utilizes knowledge of the electromagnetic forward problem to assign source signals to their respective generators, while information about the signals themselves is typically ignored. We demonstrate that these two techniques can be derived from the same starting point using the Bayesian formalism. This suggests a means by which new algorithms can be developed that utilize as much relevant information as possible. We also briefly mention some preliminary work that supports the value of integrating information used by these two techniques and review the kinds of information that may be useful in addressing the ESE problem. version:1
arxiv-1501-05068 | Difficulties applying recent blind source separation techniques to EEG and MEG | http://arxiv.org/abs/1501.05068 | id:1501.05068 author:Kevin H. Knuth category:physics.data-an physics.med-ph stat.ML  published:2015-01-21 summary:High temporal resolution measurements of human brain activity can be performed by recording the electric potentials on the scalp surface (electroencephalography, EEG), or by recording the magnetic fields near the surface of the head (magnetoencephalography, MEG). The analysis of the data is problematic due to the fact that multiple neural generators may be simultaneously active and the potentials and magnetic fields from these sources are superimposed on the detectors. It is highly desirable to un-mix the data into signals representing the behaviors of the original individual generators. This general problem is called blind source separation and several recent techniques utilizing maximum entropy, minimum mutual information, and maximum likelihood estimation have been applied. These techniques have had much success in separating signals such as natural sounds or speech, but appear to be ineffective when applied to EEG or MEG signals. Many of these techniques implicitly assume that the source distributions have a large kurtosis, whereas an analysis of EEG/MEG signals reveals that the distributions are multimodal. This suggests that more effective separation techniques could be designed for EEG and MEG signals. version:1
arxiv-1410-7484 | Abrupt Motion Tracking via Nearest Neighbor Field Driven Stochastic Sampling | http://arxiv.org/abs/1410.7484 | id:1410.7484 author:Tianfei Zhou, Yao Lu, Feng Lv, Huijun Di, Qingjie Zhao, Jian Zhang category:cs.CV  published:2014-10-28 summary:Stochastic sampling based trackers have shown good performance for abrupt motion tracking so that they have gained popularity in recent years. However, conventional methods tend to use a two-stage sampling paradigm, in which the search space needs to be uniformly explored with an inefficient preliminary sampling phase. In this paper, we propose a novel sampling-based method in the Bayesian filtering framework to address the problem. Within the framework, nearest neighbor field estimation is utilized to compute the importance proposal probabilities, which guide the Markov chain search towards promising regions and thus enhance the sampling efficiency; given the motion priors, a smoothing stochastic sampling Monte Carlo algorithm is proposed to approximate the posterior distribution through a smoothing weight-updating scheme. Moreover, to track the abrupt and the smooth motions simultaneously, we develop an abrupt-motion detection scheme which can discover the presence of abrupt motions during online tracking. Extensive experiments on challenging image sequences demonstrate the effectiveness and the robustness of our algorithm in handling the abrupt motions. version:2
arxiv-1501-04920 | Regroupement sémantique de définitions en espagnol | http://arxiv.org/abs/1501.04920 | id:1501.04920 author:Gerardo Sierra, Juan-Manuel Torres-Moreno, Alejandro Molina category:cs.IR cs.CL  published:2015-01-20 summary:This article focuses on the description and evaluation of a new unsupervised learning method of clustering of definitions in Spanish according to their semantic. Textual Energy was used as a clustering measure, and we study an adaptation of the Precision and Recall to evaluate our method. version:1
arxiv-1501-04870 | Scalable Multi-Output Label Prediction: From Classifier Chains to Classifier Trellises | http://arxiv.org/abs/1501.04870 | id:1501.04870 author:J. Read, L. Martino, P. Olmos, D. Luengo category:stat.ML cs.CV cs.DS cs.LG stat.CO  published:2015-01-20 summary:Multi-output inference tasks, such as multi-label classification, have become increasingly important in recent years. A popular method for multi-label classification is classifier chains, in which the predictions of individual classifiers are cascaded along a chain, thus taking into account inter-label dependencies and improving the overall performance. Several varieties of classifier chain methods have been introduced, and many of them perform very competitively across a wide range of benchmark datasets. However, scalability limitations become apparent on larger datasets when modeling a fully-cascaded chain. In particular, the methods' strategies for discovering and modeling a good chain structure constitutes a mayor computational bottleneck. In this paper, we present the classifier trellis (CT) method for scalable multi-label classification. We compare CT with several recently proposed classifier chain methods to show that it occupies an important niche: it is highly competitive on standard multi-label problems, yet it can also scale up to thousands or even tens of thousands of labels. version:1
arxiv-1501-04819 | Separation of undersampled composite signals using the Dantzig selector with overcomplete dictionaries | http://arxiv.org/abs/1501.04819 | id:1501.04819 author:Ashley Prater, Lixin Shen category:math.NA stat.ML  published:2015-01-20 summary:In many applications one may acquire a composition of several signals that may be corrupted by noise, and it is a challenging problem to reliably separate the components from one another without sacrificing significant details. Adding to the challenge, in a compressive sensing framework, one is given only an undersampled set of linear projections of the composite signal. In this paper, we propose using the Dantzig selector model incorporating an overcomplete dictionary to separate a noisy undersampled collection of composite signals, and present an algorithm to efficiently solve the model. The Dantzig selector is a statistical approach to finding a solution to a noisy linear regression problem by minimizing the $\ell_1$ norm of candidate coefficient vectors while constraining the scope of the residuals. If the underlying coefficient vector is sparse, then the Dantzig selector performs well in the recovery and separation of the unknown composite signal. In the following, we propose a proximity operator based algorithm to recover and separate unknown noisy undersampled composite signals through the Dantzig selector. We present numerical simulations comparing the proposed algorithm with the competing Alternating Direction Method, and the proposed algorithm is found to be faster, while producing similar quality results. Additionally, we demonstrate the utility of the proposed algorithm in several experiments by applying it in various domain applications including the recovery of complex-valued coefficient vectors, the removal of impulse noise from smooth signals, and the separation and classification of a composition of handwritten digits. version:1
arxiv-1311-1911 | Visualizing the Effects of a Changing Distance on Data Using Continuous Embeddings | http://arxiv.org/abs/1311.1911 | id:1311.1911 author:Gina Gruenhage, Manfred Opper, Simon Barthelme category:stat.ML  published:2013-11-08 summary:Most ML methods, from clustering to classification, rely on a distance function to describe relationships between datapoints. For complex datasets it is hard to avoid making some arbitrary choices when defining a distance function. To compare images, one must choose a spatial scale, for signals, a temporal scale. The right scale is hard to pin down and it is preferable when results do not depend too tightly on the exact value one picked. Topological data analysis seeks to address this issue by focusing on the notion of neighbourhood instead of distance. Here, we show that in some cases a simpler solution is available. One can check how strongly distance relationships depend on a hyperparameter using dimensionality reduction. We formulate a variant of dynamical multi-dimensional scaling (MDS), which embeds datapoints as curves. The resulting algorithm is based on the Concave-Convex Procedure (CCCP) and provides a simple and efficient way of visualizing changes and invariances in distance patterns as a hyperparameter is varied. We also present a variant to analyze the dependence on multiple hyperparameters. We provide a cMDS algorithm that is straightforward to implement, use and extend. To illustrate the possibilities of cMDS, we apply cMDS to several real-world data sets. version:2
arxiv-1403-6888 | Fast Localization of Facial Landmark Points | http://arxiv.org/abs/1403.6888 | id:1403.6888 author:Nenad Markuš, Miroslav Frljak, Igor S. Pandžić, Jörgen Ahlberg, Robert Forchheimer category:cs.CV  published:2014-03-26 summary:Localization of salient facial landmark points, such as eye corners or the tip of the nose, is still considered a challenging computer vision problem despite recent efforts. This is especially evident in unconstrained environments, i.e., in the presence of background clutter and large head pose variations. Most methods that achieve state-of-the-art accuracy are slow, and, thus, have limited applications. We describe a method that can accurately estimate the positions of relevant facial landmarks in real-time even on hardware with limited processing power, such as mobile devices. This is achieved with a sequence of estimators based on ensembles of regression trees. The trees use simple pixel intensity comparisons in their internal nodes and this makes them able to process image regions very fast. We test the developed system on several publicly available datasets and analyse its processing speed on various devices. Experimental results show that our method has practical value. version:2
arxiv-1501-04754 | Distributed Data Association in Smart Camera Networks via Dual Decomposition | http://arxiv.org/abs/1501.04754 | id:1501.04754 author:Jiuqing Wan, Yuting Nie, Li Liu category:cs.CV  published:2015-01-20 summary:One of the fundamental requirements for visual surveillance using smart camera networks is the correct association of each persons observations generated on different cameras. Recently, distributed data association that involves only local information processing on each camera node and mutual information exchanging between neighboring cameras has attracted many research interests due to its superiority in large scale applications. In this paper, we formulate the problem of data association in smart camera networks as an Integer Programming problem by introducing a set of linking variables, and propose two distributed algorithms, namely L-DD and Q-DD, to solve the Integer Programming problem using dual decomposition technique. In our algorithms, the original IP problem is decomposed into several sub-problems, which can be solved locally and efficiently on each smart camera, and then different sub-problems reach consensus on their solutions in a rigorous way by adjusting their parameters based on projected sub-gradient optimization. The proposed methods are simple and flexible, in that (i) we can incorporate any feature extraction and matching technique into our framework to measure the similarity between two observations, which is used to define the cost of each link, and (ii) we can decompose the original problem in any way as long as the resulting sub-problem can be solved independently on individual camera. We show the competitiveness of our methods in both accuracy and speed by theoretical analysis and experimental comparison with state of the art algorithms on two real data sets collected by camera networks in our campus garden and office building. version:1
arxiv-1406-7806 | Building DNN Acoustic Models for Large Vocabulary Speech Recognition | http://arxiv.org/abs/1406.7806 | id:1406.7806 author:Andrew L. Maas, Peng Qi, Ziang Xie, Awni Y. Hannun, Christopher T. Lengerich, Daniel Jurafsky, Andrew Y. Ng category:cs.CL cs.LG cs.NE stat.ML  published:2014-06-30 summary:Deep neural networks (DNNs) are now a central component of nearly all state-of-the-art speech recognition systems. Building neural network acoustic models requires several design decisions including network architecture, size, and training loss function. This paper offers an empirical investigation on which aspects of DNN acoustic model design are most important for speech recognition system performance. We report DNN classifier performance and final speech recognizer word error rates, and compare DNNs using several metrics to quantify factors influencing differences in task performance. Our first set of experiments use the standard Switchboard benchmark corpus, which contains approximately 300 hours of conversational telephone speech. We compare standard DNNs to convolutional networks, and present the first experiments using locally-connected, untied neural networks for acoustic modeling. We additionally build systems on a corpus of 2,100 hours of training data by combining the Switchboard and Fisher corpora. This larger corpus allows us to more thoroughly examine performance of large DNN models -- with up to ten times more parameters than those typically used in speech recognition systems. Our results suggest that a relatively simple DNN architecture and optimization technique produces strong results. These findings, along with previous work, help establish a set of best practices for building DNN hybrid speech recognition systems with maximum likelihood training. Our experiments in DNN optimization additionally serve as a case study for training DNNs with discriminative loss functions for speech tasks, as well as DNN classifiers more generally. version:2
arxiv-1501-04725 | Learning Invariants using Decision Trees | http://arxiv.org/abs/1501.04725 | id:1501.04725 author:Siddharth Krishna, Christian Puhrsch, Thomas Wies category:cs.PL cs.LG  published:2015-01-20 summary:The problem of inferring an inductive invariant for verifying program safety can be formulated in terms of binary classification. This is a standard problem in machine learning: given a sample of good and bad points, one is asked to find a classifier that generalizes from the sample and separates the two sets. Here, the good points are the reachable states of the program, and the bad points are those that reach a safety property violation. Thus, a learned classifier is a candidate invariant. In this paper, we propose a new algorithm that uses decision trees to learn candidate invariants in the form of arbitrary Boolean combinations of numerical inequalities. We have used our algorithm to verify C programs taken from the literature. The algorithm is able to infer safe invariants for a range of challenging benchmarks and compares favorably to other ML-based invariant inference techniques. In particular, it scales well to large sample sets. version:1
