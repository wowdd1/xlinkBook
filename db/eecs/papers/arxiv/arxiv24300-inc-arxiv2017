arxiv-1701-00757 | Clustering Signed Networks with the Geometric Mean of Laplacians | http://arxiv.org/abs/1701.00757 | id:1701.00757 author:Pedro Mercado, Francesco Tudisco, Matthias Hein category:stat.ML  published:2017-01-03 summary:Signed networks allow to model positive and negative relationships. We analyze existing extensions of spectral clustering to signed networks. It turns out that existing approaches do not recover the ground truth clustering in several situations where either the positive or the negative network structures contain no noise. Our analysis shows that these problems arise as existing approaches take some form of arithmetic mean of the Laplacians of the positive and negative part. As a solution we propose to use the geometric mean of the Laplacians of positive and negative part and show that it outperforms the existing approaches. While the geometric mean of matrices is computationally expensive, we show that eigenvectors of the geometric mean can be computed efficiently, leading to a numerical scheme for sparse matrices which is of independent interest. version:1
arxiv-1612-09030 | Meta-Unsupervised-Learning: A supervised approach to unsupervised learning | http://arxiv.org/abs/1612.09030 | id:1612.09030 author:Vikas K. Garg, Adam Tauman Kalai category:cs.LG cs.AI cs.CV  published:2016-12-29 summary:We introduce a new paradigm to investigate unsupervised learning, reducing unsupervised learning to supervised learning. Specifically, we mitigate the subjectivity in unsupervised decision-making by leveraging knowledge acquired from prior, possibly heterogeneous, supervised learning tasks. We demonstrate the versatility of our framework via comprehensive expositions and detailed experiments on several unsupervised problems such as (a) clustering, (b) outlier detection, and (c) similarity prediction under a common umbrella of meta-unsupervised-learning. We also provide rigorous PAC-agnostic bounds to establish the theoretical foundations of our framework, and show that our framing of meta-clustering circumvents Kleinberg's impossibility theorem for clustering. version:2
arxiv-1701-00749 | Pyndri: a Python Interface to the Indri Search Engine | http://arxiv.org/abs/1701.00749 | id:1701.00749 author:Christophe Van Gysel, Evangelos Kanoulas, Maarten de Rijke category:cs.IR cs.CL  published:2017-01-03 summary:We introduce pyndri, a Python interface to the Indri search engine. Pyndri allows to access Indri indexes from Python at two levels: (1) dictionary and tokenized document collection, (2) evaluating queries on the index. We hope that with the release of pyndri, we will stimulate reproducible, open and fast-paced IR research. version:1
arxiv-1701-00737 | Deterministic and Probabilistic Conditions for Finite Completability of Low-rank Multi-View Data | http://arxiv.org/abs/1701.00737 | id:1701.00737 author:Morteza Ashraphijuo, Xiaodong Wang, Vaneet Aggarwal category:cs.IT cs.LG math.AG math.IT  published:2017-01-03 summary:We consider the multi-view data completion problem, i.e., to complete a matrix $\mathbf{U}=[\mathbf{U}_1 \mathbf{U}_2]$ where the ranks of $\mathbf{U},\mathbf{U}_1$, and $\mathbf{U}_2$ are given. In particular, we investigate the fundamental conditions on the sampling pattern, i.e., locations of the sampled entries for finite completability of such a multi-view data given the corresponding rank constraints. In contrast with the existing analysis on Grassmannian manifold for a single-view matrix, i.e., conventional matrix completion, we propose a geometric analysis on the manifold structure for multi-view data to incorporate more than one rank constraint. We provide a deterministic necessary and sufficient condition on the sampling pattern for finite completability. We also give a probabilistic condition in terms of the number of samples per column that guarantees finite completability with high probability. Finally, using the developed tools, we derive the deterministic and probabilistic guarantees for unique completability. version:1
arxiv-1701-00728 | On (Commercial) Benefits of Automatic Text Summarization Systems in the News Domain: A Case of Media Monitoring and Media Response Analysis | http://arxiv.org/abs/1701.00728 | id:1701.00728 author:Pashutan Modaresi, Philipp Gross, Siavash Sefidrodi, Mirja Eckhof, Stefan Conrad category:cs.CL  published:2017-01-03 summary:In this work, we present the results of a systematic study to investigate the (commercial) benefits of automatic text summarization systems in a real world scenario. More specifically, we define a use case in the context of media monitoring and media response analysis and claim that even using a simple query-based extractive approach can dramatically save the processing time of the employees without significantly reducing the quality of their work. version:1
arxiv-1701-00723 | Image denoising using group sparsity residual and external nonlocal self-similarity prior | http://arxiv.org/abs/1701.00723 | id:1701.00723 author:Zhiyuan Zha, Xinggan Zhang, Qiong Wang, Yechao Bai, Lan Tang category:cs.CV  published:2017-01-03 summary:Nonlocal image representation has been successfully used in many image-related inverse problems including denoising, deblurring and deblocking. However, a majority of reconstruction methods only exploit the nonlocal self-similarity (NSS) prior of the degraded observation image, it is very challenging to reconstruct the latent clean image. In this paper we propose a novel model for image denoising via group sparsity residual and external NSS prior. To boost the performance of image denoising, the concept of group sparsity residual is proposed, and thus the problem of image denoising is transformed into one that reduces the group sparsity residual. Due to the fact that the groups contain a large amount of NSS information of natural images, we obtain a good estimation of the group sparse coefficients of the original image by the external NSS prior based on Gaussian Mixture model (GMM) learning and the group sparse coefficients of noisy image is used to approximate the estimation. Experimental results have demonstrated that the proposed method not only outperforms many state-of-the-art methods, but also delivers the best qualitative denoising results with finer details and less ringing artifacts. version:1
arxiv-1701-00694 | Mixed one-bit compressive sensing with applications to overexposure correction for CT reconstruction | http://arxiv.org/abs/1701.00694 | id:1701.00694 author:Xiaolin Huang, Yan Xia, Lei Shi, Yixing Huang, Ming Yan, Joachim Hornegger, Andreas Maier category:cs.CV cs.IR cs.NA math.NA  published:2017-01-03 summary:When a measurement falls outside the quantization or measurable range, it becomes saturated and cannot be used in classical reconstruction methods. For example, in C-arm angiography systems, which provide projection radiography, fluoroscopy, digital subtraction angiography, and are widely used for medical diagnoses and interventions, the limited dynamic range of C-arm flat detectors leads to overexposure in some projections during an acquisition, such as imaging relatively thin body parts (e.g., the knee). Aiming at overexposure correction for computed tomography (CT) reconstruction, we in this paper propose a mixed one-bit compressive sensing (M1bit-CS) to acquire information from both regular and saturated measurements. This method is inspired by the recent progress on one-bit compressive sensing, which deals with only sign observations. Its successful applications imply that information carried by saturated measurements is useful to improve recovery quality. For the proposed M1bit-CS model, alternating direction methods of multipliers is developed and an iterative saturation detection scheme is established. Then we evaluate M1bit-CS on one-dimensional signal recovery tasks. In some experiments, the performance of the proposed algorithms on mixed measurements is almost the same as recovery on unsaturated ones with the same amount of measurements. Finally, we apply the proposed method to overexposure correction for CT reconstruction on a phantom and a simulated clinical image. The results are promising, as the typical streaking artifacts and capping artifacts introduced by saturated projection data are effectively reduced, yielding significant error reduction compared with existing algorithms based on extrapolation. version:1
arxiv-1701-00677 | New Methods of Enhancing Prediction Accuracy in Linear Models with Missing Data | http://arxiv.org/abs/1701.00677 | id:1701.00677 author:Mohammad Amin Fakharian, Ashkan Esmaeili, Farokh Marvasti category:stat.ML cs.LG  published:2017-01-03 summary:In this paper, prediction for linear systems with missing information is investigated. New methods are introduced to improve the Mean Squared Error (MSE) on the test set in comparison to state-of-the-art methods, through appropriate tuning of Bias-Variance trade-off. First, the use of proposed Soft Weighted Prediction (SWP) algorithm and its efficacy are depicted and compared to previous works for non-missing scenarios. The algorithm is then modified and optimized for missing scenarios. It is shown that controlled over-fitting by suggested algorithms will improve prediction accuracy in various cases. Simulation results approve our heuristics in enhancing the prediction accuracy. version:1
arxiv-1701-00669 | Product Manifold Filter: Non-Rigid Shape Correspondence via Kernel Density Estimation in the Product Space | http://arxiv.org/abs/1701.00669 | id:1701.00669 author:Matthias Vestner, Roee Litman, Emanuele Rodolà, Alex Bronstein, Daniel Cremers category:cs.CV  published:2017-01-03 summary:Many algorithms for the computation of correspondences between deformable shapes rely on some variant of nearest neighbor matching in a descriptor space. Such are, for example, various point-wise correspondence recovery algorithms used as a post-processing stage in the functional correspondence framework. Such frequently used techniques implicitly make restrictive assumptions (e.g., near-isometry) on the considered shapes and in practice suffer from lack of accuracy and result in poor surjectivity. We propose an alternative recovery technique capable of guaranteeing a bijective correspondence and producing significantly higher accuracy and smoothness. Unlike other methods our approach does not depend on the assumption that the analyzed shapes are isometric. We derive the proposed method from the statistical framework of kernel density estimation and demonstrate its performance on several challenging deformable 3D shape matching datasets. version:1
arxiv-1701-00660 | Ambiguity and Incomplete Information in Categorical Models of Language | http://arxiv.org/abs/1701.00660 | id:1701.00660 author:Dan Marsden category:cs.LO cs.CL math.CT  published:2017-01-03 summary:We investigate notions of ambiguity and partial information in categorical distributional models of natural language. Probabilistic ambiguity has previously been studied using Selinger's CPM construction. This construction works well for models built upon vector spaces, as has been shown in quantum computational applications. Unfortunately, it doesn't seem to provide a satisfactory method for introducing mixing in other compact closed categories such as the category of sets and binary relations. We therefore lack a uniform strategy for extending a category to model imprecise linguistic information. In this work we adopt a different approach. We analyze different forms of ambiguous and incomplete information, both with and without quantitative probabilistic data. Each scheme then corresponds to a suitable enrichment of the category in which we model language. We view different monads as encapsulating the informational behaviour of interest, by analogy with their use in modelling side effects in computation. Previous results of Jacobs then allow us to systematically construct suitable bases for enrichment. We show that we can freely enrich arbitrary dagger compact closed categories in order to capture all the phenomena of interest, whilst retaining the important dagger compact closed structure. This allows us to construct a model with real convex combination of binary relations that makes non-trivial use of the scalars. Finally we relate our various different enrichments, showing that finite subconvex algebra enrichment covers all the effects under consideration. version:1
arxiv-1701-00652 | Semidefinite tests for latent causal structures | http://arxiv.org/abs/1701.00652 | id:1701.00652 author:Aditya Kela, Kai von Prillwitz, Johan Aberg, Rafael Chaves, David Gross category:stat.ML quant-ph  published:2017-01-03 summary:Testing whether a probability distribution is compatible with a given Bayesian network is a fundamental task in the field of causal inference, where Bayesian networks model causal relations. Here we consider the class of causal structures where all correlations between observed quantities are solely due to the influence from latent variables. We show that each model of this type imposes a certain signature on the observable covariance matrix in terms of a particular decomposition into positive semidefinite components. This signature, and thus the underlying hypothetical latent structure, can be tested in a computationally efficient manner via semidefinite programming. This stands in stark contrast with the algebraic geometric tools required if the full observable probability distribution is taken into account. The semidefinite test is compared with tests based on entropic inequalities. version:1
arxiv-1701-00609 | Akid: A Library for Neural Network Research and Production from a Dataism Approach | http://arxiv.org/abs/1701.00609 | id:1701.00609 author:Shuai Li category:cs.LG cs.DC  published:2017-01-03 summary:Neural networks are a revolutionary but immature technique that is fast evolving and heavily relies on data. To benefit from the newest development and newly available data, we want the gap between research and production as small as possibly. On the other hand, differing from traditional machine learning models, neural network is not just yet another statistic model, but a model for the natural processing engine --- the brain. In this work, we describe a neural network library named {\texttt akid}. It provides higher level of abstraction for entities (abstracted as blocks) in nature upon the abstraction done on signals (abstracted as tensors) by Tensorflow, characterizing the dataism observation that all entities in nature processes input and emit out in some ways. It includes a full stack of software that provides abstraction to let researchers focus on research instead of implementation, while at the same time the developed program can also be put into production seamlessly in a distributed environment, and be production ready. At the top application stack, it provides out-of-box tools for neural network applications. Lower down, akid provides a programming paradigm that lets user easily build customized models. The distributed computing stack handles the concurrency and communication, thus letting models be trained or deployed to a single GPU, multiple GPUs, or a distributed environment without affecting how a model is specified in the programming paradigm stack. Lastly, the distributed deployment stack handles how the distributed computing is deployed, thus decoupling the research prototype environment with the actual production environment, and is able to dynamically allocate computing resources, so development (Devs) and operations (Ops) could be separated. Please refer to http://akid.readthedocs.io/en/latest/ for documentation. version:1
arxiv-1701-00422 | Towards multiple kernel principal component analysis for integrative analysis of tumor samples | http://arxiv.org/abs/1701.00422 | id:1701.00422 author:Nora K. Speicher, Nico Pfeifer category:stat.ML  published:2017-01-02 summary:Personalized treatment of patients based on tissue-specific cancer subtypes has strongly increased the efficacy of the chosen therapies. Even though the amount of data measured for cancer patients has increased over the last years, most cancer subtypes are still diagnosed based on individual data sources (e.g. gene expression data). We propose an unsupervised data integration method based on kernel principal component analysis. Principal component analysis is one of the most widely used techniques in data analysis. Unfortunately, the straight-forward multiple-kernel extension of this method leads to the use of only one of the input matrices, which does not fit the goal of gaining information from all data sources. Therefore, we present a scoring function to determine the impact of each input matrix. The approach enables visualizing the integrated data and subsequent clustering for cancer subtype identification. Due to the nature of the method, no free parameters have to be set. We apply the methodology to five different cancer data sets and demonstrate its advantages in terms of results and usability. version:2
arxiv-1701-00599 | AENet: Learning Deep Audio Features for Video Analysis | http://arxiv.org/abs/1701.00599 | id:1701.00599 author:Naoya Takahashi, Michael Gygli, Luc Van Gool category:cs.MM cs.CV cs.SD  published:2017-01-03 summary:We propose a new deep network for audio event recognition, called AENet. In contrast to speech, sounds coming from audio events may be produced by a wide variety of sources. Furthermore, distinguishing them often requires analyzing an extended time period due to the lack of clear sub-word units that are present in speech. In order to incorporate this long-time frequency structure of audio events, we introduce a convolutional neural network (CNN) operating on a large temporal input. In contrast to previous works this allows us to train an audio event detection system end-to-end. The combination of our network architecture and a novel data augmentation outperforms previous methods for audio event detection by 16%. Furthermore, we perform transfer learning and show that our model learnt generic audio features, similar to the way CNNs learn generic features on vision tasks. In video analysis, combining visual features and traditional audio features such as MFCC typically only leads to marginal improvements. Instead, combining visual features with our AENet features, which can be computed efficiently on a GPU, leads to significant performance improvements on action recognition and video highlight detection. In video highlight detection, our audio features improve the performance by more than 8% over visual features alone. version:1
arxiv-1701-00597 | Deep Convolutional Neural Networks for Pairwise Causality | http://arxiv.org/abs/1701.00597 | id:1701.00597 author:Karamjit Singh, Garima Gupta, Lovekesh Vig, Gautam Shroff, Puneet Agarwal category:cs.LG  published:2017-01-03 summary:Discovering causal models from observational and interventional data is an important first step preceding what-if analysis or counterfactual reasoning. As has been shown before, the direction of pairwise causal relations can, under certain conditions, be inferred from observational data via standard gradient-boosted classifiers (GBC) using carefully engineered statistical features. In this paper we apply deep convolutional neural networks (CNNs) to this problem by plotting attribute pairs as 2-D scatter plots that are fed to the CNN as images. We evaluate our approach on the 'Cause- Effect Pairs' NIPS 2013 Data Challenge. We observe that a weighted ensemble of CNN with the earlier GBC approach yields significant improvement. Further, we observe that when less training data is available, our approach performs better than the GBC based approach suggesting that CNN models pre-trained to determine the direction of pairwise causal direction could have wider applicability in causal discovery and enabling what-if or counterfactual analysis. version:1
arxiv-1701-00593 | HLA class I binding prediction via convolutional neural networks | http://arxiv.org/abs/1701.00593 | id:1701.00593 author:Yeeleng Scott Vang, Xiaohui Xie category:cs.CE cs.LG  published:2017-01-03 summary:Many biological processes are governed by protein-ligand interactions. Of such is the recognition of self and nonself cells by the immune system. This immune response process is regulated by the major histocompatibility complex (MHC) protein which is encoded by the human leukocyte antigen (HLA) complex. Understanding the binding potential between MHC and peptides is crucial to our understanding of the functioning of the immune system, which in turns will broaden our understanding of autoimmune diseases and vaccine design. We introduce a new distributed representation of amino acids, named HLA-Vec, that can be used for a variety of downstream proteomic machine learning tasks. We then propose a deep convolutional neurerror can be used only in preambleal network architecture, named HLA-CNN, for the task of HLA class I-peptide binding prediction. Experimental results show combining the new distributed representation with our HLA-CNN architecture acheives state-of-the-art results in the vast majority of the latest two Immune Epitope Database (IEDB) weekly automated benchmark datasets. version:1
arxiv-1701-00576 | Shortcut Sequence Tagging | http://arxiv.org/abs/1701.00576 | id:1701.00576 author:Huijia Wu, Jiajun Zhang, Chengqing Zong category:cs.CL  published:2017-01-03 summary:Deep stacked RNNs are usually hard to train. Adding shortcut connections across different layers is a common way to ease the training of stacked networks. However, extra shortcuts make the recurrent step more complicated. To simply the stacked architecture, we propose a framework called shortcut block, which is a marriage of the gating mechanism and shortcuts, while discarding the self-connected part in LSTM cell. We present extensive empirical experiments showing that this design makes training easy and improves generalization. We propose various shortcut block topologies and compositions to explore its effectiveness. Based on this architecture, we obtain a 6% relatively improvement over the state-of-the-art on CCGbank supertagging dataset. We also get comparable results on POS tagging task. version:1
arxiv-1701-00573 | Robust method for finding sparse solutions to linear inverse problems using an L2 regularization | http://arxiv.org/abs/1701.00573 | id:1701.00573 author:Gonzalo H Otazu category:cs.NA cs.LG stat.ML  published:2017-01-03 summary:We analyzed the performance of a biologically inspired algorithm called the Corrected Projections Algorithm (CPA) when a sparseness constraint is required to unambiguously reconstruct an observed signal using atoms from an overcomplete dictionary. By changing the geometry of the estimation problem, CPA gives an analytical expression for a binary variable that indicates the presence or absence of a dictionary atom using an L2 regularizer. The regularized solution can be implemented using an efficient real-time Kalman-filter type of algorithm. The smoother L2 regularization of CPA makes it very robust to noise, and CPA outperforms other methods in identifying known atoms in the presence of strong novel atoms in the signal. version:1
arxiv-1701-00562 | End-to-End Attention based Text-Dependent Speaker Verification | http://arxiv.org/abs/1701.00562 | id:1701.00562 author:Shi-Xiong Zhang, Zhuo Chen, Yong Zhao, Jinyu Li, Yifan Gong category:cs.CL stat.ML  published:2017-01-03 summary:A new type of End-to-End system for text-dependent speaker verification is presented in this paper. Previously, using the phonetically discriminative/speaker discriminative DNNs as feature extractors for speaker verification has shown promising results. The extracted frame-level (DNN bottleneck, posterior or d-vector) features are equally weighted and aggregated to compute an utterance-level speaker representation (d-vector or i-vector). In this work we use speaker discriminative CNNs to extract the noise-robust frame-level features. These features are smartly combined to form an utterance-level speaker vector through an attention mechanism. The proposed attention model takes the speaker discriminative information and the phonetic information to learn the weights. The whole system, including the CNN and attention model, is joint optimized using an end-to-end criterion. The training algorithm imitates exactly the evaluation process --- directly mapping a test utterance and a few target speaker utterances into a single verification score. The algorithm can automatically select the most similar impostor for each target speaker to train the network. We demonstrated the effectiveness of the proposed end-to-end system on Windows $10$ "Hey Cortana" speaker verification task. version:1
arxiv-1701-00561 | Robust and Real-time Deep Tracking Via Multi-Scale Domain Adaptation | http://arxiv.org/abs/1701.00561 | id:1701.00561 author:Xinyu Wang, Hanxi Li, Yi Li, Fumin Shen, Fatih Porikli category:cs.CV  published:2017-01-03 summary:Visual tracking is a fundamental problem in computer vision. Recently, some deep-learning-based tracking algorithms have been achieving record-breaking performances. However, due to the high complexity of deep learning, most deep trackers suffer from low tracking speed, and thus are impractical in many real-world applications. Some new deep trackers with smaller network structure achieve high efficiency while at the cost of significant decrease on precision. In this paper, we propose to transfer the feature for image classification to the visual tracking domain via convolutional channel reductions. The channel reduction could be simply viewed as an additional convolutional layer with the specific task. It not only extracts useful information for object tracking but also significantly increases the tracking speed. To better accommodate the useful feature of the target in different scales, the adaptation filters are designed with different sizes. The yielded visual tracker is real-time and also illustrates the state-of-the-art accuracies in the experiment involving two well-adopted benchmarks with more than 100 test videos. version:1
arxiv-1612-09413 | Permuted and Augmented Stick-Breaking Bayesian Multinomial Regression | http://arxiv.org/abs/1612.09413 | id:1612.09413 author:Quan Zhang, Mingyuan Zhou category:stat.ME stat.ML  published:2016-12-30 summary:To model categorical response variables given their covariates, we propose a permuted and augmented stick-breaking (paSB) construction that one-to-one maps the observed categories to randomly permuted latent sticks. This new construction transforms multinomial regression into regression analysis of stick-specific binary random variables that are mutually independent given their covariate-dependent stick success probabilities, which are parameterized by the regression coefficients of their corresponding categories. The paSB construction allows transforming an arbitrary cross-entropy-loss binary classifier into a Bayesian multinomial one. Specifically, we parameterize the negative logarithms of the stick failure probabilities with a family of covariate-dependent softplus functions to construct nonparametric Bayesian multinomial softplus regression, and transform Bayesian support vector machine (SVM) into Bayesian multinomial SVM. These Bayesian multinomial regression models are not only capable of providing probability estimates, quantifying uncertainty, and producing nonlinear classification decision boundaries, but also amenable to posterior simulation. Example results demonstrate their attractive properties and appealing performance. version:2
arxiv-1701-00504 | Stance detection in online discussions | http://arxiv.org/abs/1701.00504 | id:1701.00504 author:Peter Krejzl, Barbora Hourová, Josef Steinberger category:cs.CL  published:2017-01-02 summary:This paper describes our system created to detect stance in online discussions. The goal is to identify whether the author of a comment is in favor of the given target or against. Our approach is based on a maximum entropy classifier, which uses surface-level, sentiment and domain-specific features. The system was originally developed to detect stance in English tweets. We adapted it to process Czech news commentaries. version:1
arxiv-1701-00495 | Vid2speech: Speech Reconstruction from Silent Video | http://arxiv.org/abs/1701.00495 | id:1701.00495 author:Ariel Ephrat, Shmuel Peleg category:cs.CV cs.SD  published:2017-01-02 summary:Speechreading is a notoriously difficult task for humans to perform. In this paper we present an end-to-end model based on a convolutional neural network (CNN) for generating an intelligible acoustic speech signal from silent video frames of a speaking person. The proposed CNN generates sound features for each frame based on its neighboring frames. Waveforms are then synthesized from the learned speech features to produce intelligible speech. We show that by leveraging the automatic feature learning capabilities of a CNN, we can obtain state-of-the-art word intelligibility on the GRID dataset, and show promising results for learning out-of-vocabulary (OOV) words. version:1
arxiv-1701-00481 | Stochastic Variance-reduced Gradient Descent for Low-rank Matrix Recovery from Linear Measurements | http://arxiv.org/abs/1701.00481 | id:1701.00481 author:Xiao Zhang, Lingxiao Wang, Quanquan Gu category:stat.ML  published:2017-01-02 summary:We study the problem of estimating low-rank matrices from linear measurements (a.k.a., matrix sensing) through nonconvex optimization. We propose an efficient stochastic variance reduced gradient descent algorithm to solve a nonconvex optimization problem of matrix sensing. Our algorithm can be applied to both noisy and noiseless observations. In the case with noisy observations, we prove that our algorithm converges to the unknown low-rank matrix at a linear rate up to the minimax optimal statistical error. While in the noiseless setting, our algorithm is guaranteed to linearly converge to the unknown low-rank matrix and achieves exact recovery with optimal sample complexity. Most notably, the overall computational complexity of our proposed algorithm, which is defined as the iteration complexity times per iteration time complexity, is lower than the state-of-the-art algorithms based on gradient descent. Experiments on synthetic data corroborate the superiority of the proposed algorithm over the state-of-the-art algorithms. version:1
arxiv-1701-00458 | Deep-HiTS: Rotation Invariant Convolutional Neural Network for Transient Detection | http://arxiv.org/abs/1701.00458 | id:1701.00458 author:Guillermo Cabrera-Vives, Ignacio Reyes, Francisco Förster, Pablo A. Estévez, Juan-Carlos Maureira category:astro-ph.IM cs.CV  published:2017-01-02 summary:We introduce Deep-HiTS, a rotation invariant convolutional neural network (CNN) model for classifying images of transients candidates into artifacts or real sources for the High cadence Transient Survey (HiTS). CNNs have the advantage of learning the features automatically from the data while achieving high performance. We compare our CNN model against a feature engineering approach using random forests (RF). We show that our CNN significantly outperforms the RF model reducing the error by almost half. Furthermore, for a fixed number of approximately 2,000 allowed false transient candidates per night we are able to reduce the miss-classified real transients by approximately 1/5. To the best of our knowledge, this is the first time CNNs have been used to detect astronomical transient events. Our approach will be very useful when processing images from next generation instruments such as the Large Synoptic Survey Telescope (LSST). We have made all our code and data available to the community for the sake of allowing further developments and comparisons at https://github.com/guille-c/Deep-HiTS. version:1
arxiv-1701-00449 | Retrieving Similar X-Ray Images from Big Image Data Using Radon Barcodes with Single Projections | http://arxiv.org/abs/1701.00449 | id:1701.00449 author:Morteza Babaie, H. R. Tizhoosh, Shujin Zhu, M. E. Shiri category:cs.CV  published:2017-01-02 summary:The idea of Radon barcodes (RBC) has been introduced recently. In this paper, we propose a content-based image retrieval approach for big datasets based on Radon barcodes. Our method (Single Projection Radon Barcode, or SP-RBC) uses only a few Radon single projections for each image as global features that can serve as a basis for weak learners. This is our most important contribution in this work, which improves the results of the RBC considerably. As a matter of fact, only one projection of an image, as short as a single SURF feature vector, can already achieve acceptable results. Nevertheless, using multiple projections in a long vector will not deliver anticipated improvements. To exploit the information inherent in each projection, our method uses the outcome of each projection separately and then applies more precise local search on the small subset of retrieved images. We have tested our method using IRMA 2009 dataset a with 14,400 x-ray images as part of imageCLEF initiative. Our approach leads to a substantial decrease in the error rate in comparison with other non-learning methods. version:1
arxiv-1701-00405 | Adversarially Tuned Scene Generation | http://arxiv.org/abs/1701.00405 | id:1701.00405 author:V S R Veeravasarapu, Constantin Rothkopf, Ramesh Visvanathan category:cs.CV  published:2017-01-02 summary:Generalization performance of trained computer vision systems that use computer graphics (CG) generated data is not yet effective due to the concept of 'domain-shift' between virtual and real data. Although simulated data augmented with a few real world samples has been shown to mitigate domain shift and improve transferability of trained models, guiding or bootstrapping the virtual data generation with the distributions learnt from target real world domain is desired, especially in the fields where annotating even few real images is laborious (such as semantic labeling, and intrinsic images etc.). In order to address this problem in an unsupervised manner, our work combines recent advances in CG (which aims to generate stochastic scene layouts coupled with large collections of 3D object models) and generative adversarial training (which aims train generative models by measuring discrepancy between generated and real data in terms of their separability in the space of a deep discriminatively-trained classifier). Our method uses iterative estimation of the posterior density of prior distributions for a generative graphical model. This is done within a rejection sampling framework. Initially, we assume uniform distributions as priors on the parameters of a scene described by a generative graphical model. As iterations proceed the prior distributions get updated to distributions that are closer to the (unknown) distributions of target data. We demonstrate the utility of adversarially tuned scene generation on two real-world benchmark datasets (CityScapes and CamVid) for traffic scene semantic labeling with a deep convolutional net (DeepLab). We realized performance improvements by 2.28 and 3.14 points (using the IoU metric) between the DeepLab models trained on simulated sets prepared from the scene generation models before and after tuning to CityScapes and CamVid respectively. version:1
arxiv-1701-00352 | Weakly Supervised Semantic Segmentation using Web-Crawled Videos | http://arxiv.org/abs/1701.00352 | id:1701.00352 author:Seunghoon Hong, Donghun Yeo, Suha Kwak, Honglak Lee, Bohyung Han category:cs.CV  published:2017-01-02 summary:We propose a novel algorithm for weakly supervised semantic segmentation based on image-level class labels only. In weakly supervised setting, it is commonly observed that trained model overly focuses on discriminative parts rather than the entire object area. Our goal is to overcome this limitation with no additional human intervention by retrieving videos relevant to target class labels from web repository, and generating segmentation labels from the retrieved videos to simulate strong supervision for semantic segmentation. During this process, we take advantage of image classification with discriminative localization technique to reject false alarms in retrieved videos and identify relevant spatio-temporal volumes within retrieved videos. Although the entire procedure does not require any additional supervision, the segmentation annotations obtained from videos are sufficiently strong to learn a model for semantic segmentation. The proposed algorithm substantially outperforms existing methods based on the same level of supervision and is even as competitive as the approaches relying on extra annotations. version:1
arxiv-1701-00338 | Assessing Uncertainties in X-ray Single-particle Three-dimensional reconstructions | http://arxiv.org/abs/1701.00338 | id:1701.00338 author:Stefan Engblom, Carl Nettelblad, Jing Liu category:stat.ME cs.CV physics.data-an  published:2017-01-02 summary:Modern technology for producing extremely bright and coherent X-ray laser pulses provides the possibility to acquire a large number of diffraction patterns from individual biological nanoparticles, including proteins, viruses, and DNA. These two-dimensional diffraction patterns can be practically reconstructed and retrieved down to a resolution of a few \angstrom. In principle, a sufficiently large collection of diffraction patterns will contain the required information for a full three-dimensional reconstruction of the biomolecule. The computational methodology for this reconstruction task is still under development and highly resolved reconstructions have not yet been produced. We analyze the Expansion-Maximization-Compression scheme, the current state of the art approach for this very challenging application, by isolating different sources of uncertainty. Through numerical experiments on synthetic data we evaluate their respective impact. We reach conclusions of relevance for handling actual experimental data, as well as pointing out certain improvements to the underlying estimation algorithm. We also introduce a practically applicable computational methodology in the form of bootstrap procedures for assessing reconstruction uncertainty in the real data case. We evaluate the sharpness of this approach and argue that this type of procedure will be critical in the near future when handling the increasing amount of data. version:1
arxiv-1701-00326 | Challenges ahead Electron Microscopy for Structural Biology from the Image Processing point of view | http://arxiv.org/abs/1701.00326 | id:1701.00326 author:Carlos Oscar S. Sorzano, Jose Maria Carazo category:cs.CV q-bio.QM I.4  published:2017-01-02 summary:Since the introduction of Direct Electron Detectors (DEDs), the resolution and range of macromolecules amenable to this technique has significantly widened, generating a broad interest that explains the well over a dozen reviews in top journal in the last two years. Similarly, the number of job offers to lead EM groups and/or coordinate EM facilities has exploded, and FEI (the main microscope manufacturer for Life Sciences) has received more than 100 orders of high-end electron microscopes by summer 2016. Strategic corporate movements are also happening, with very big players entering the market through key acquisitions (Thermo Fisher has recently bought FEI for \$4.2B), partly attracted by new Pharma interest in the field, now perceived to be in a position to impact structure-based drug design. The scientific perspectives are indeed extremely positive but, in these moments of well-founded generalized optimists, we want to make a reflection on some of the hurdles ahead us, since they certainly exist and they indeed limit the informational content of cryoEM projects. Here we focus on image processing aspects, particularly in the so-called area of Single Particle Analysis, discussing some of the current resolution and high-throughput limiting factors. version:1
arxiv-1701-00322 | Deep learning for plasma tomography using the bolometer system at JET | http://arxiv.org/abs/1701.00322 | id:1701.00322 author:Francisco A. Matos, Diogo R. Ferreira, Pedro J. Carvalho, JET Contributors category:stat.ML physics.plasm-ph  published:2017-01-02 summary:Deep learning is having a profound impact in many fields, especially those that involve some form of image processing. Deep neural networks excel in turning an input image into a set of high-level features. On the other hand, tomography deals with the inverse problem of recreating an image from a number of projections. In plasma diagnostics, tomography aims at reconstructing the cross-section of the plasma from radiation measurements. This reconstruction can be computed with neural networks. However, previous attempts have focused on learning a parametric model of the plasma profile. In this work, we use a deep neural network to produce a full, pixel-by-pixel reconstruction of the plasma profile. For this purpose, we use the overview bolometer system at JET, and we introduce an up-convolutional network that has been trained and tested on a large set of sample tomograms. We show that this network is able to reproduce existing reconstructions with a high level of accuracy, as measured by several metrics. version:1
arxiv-1701-00485 | Two-Bit Networks for Deep Learning on Resource-Constrained Embedded Devices | http://arxiv.org/abs/1701.00485 | id:1701.00485 author:Wenjia Meng, Zonghua Gu, Ming Zhang, Zhaohui Wu category:cs.LG cs.CV  published:2017-01-02 summary:With the rapid proliferation of Internet of Things and intelligent edge devices, there is an increasing need for implementing machine learning algorithms, including deep learning, on resource-constrained mobile embedded devices with limited memory and computation power. Typical large Convolutional Neural Networks (CNNs) need large amounts of memory and computational power, and cannot be deployed on embedded devices efficiently. We present Two-Bit Networks (TBNs) for model compression of CNNs with edge weights constrained to (-2, -1, 1, 2), which can be encoded with two bits. Our approach can reduce the memory usage and improve computational efficiency significantly while achieving good performance in terms of classification accuracy, thus representing a reasonable tradeoff between model size and performance. version:1
arxiv-1701-00311 | Bayesian model selection consistency and oracle inequality with intractable marginal likelihood | http://arxiv.org/abs/1701.00311 | id:1701.00311 author:Yun Yang, Debdeep Pati category:math.ST stat.ME stat.ML stat.TH  published:2017-01-02 summary:In this article, we investigate large sample properties of model selection procedures in a general Bayesian framework when a closed form expression of the marginal likelihood function is not available or a local asymptotic quadratic approximation of the log-likelihood function does not exist. Under appropriate identifiability assumptions on the true model, we provide sufficient conditions for a Bayesian model selection procedure to be consistent and obey the Occam's razor phenomenon, i.e., the probability of selecting the "smallest" model that contains the truth tends to one as the sample size goes to infinity. In order to show that a Bayesian model selection procedure selects the smallest model containing the truth, we impose a prior anti-concentration condition, requiring the prior mass assigned by large models to a neighborhood of the truth to be sufficiently small. In a more general setting where the strong model identifiability assumption may not hold, we introduce the notion of local Bayesian complexity and develop oracle inequalities for Bayesian model selection procedures. Our Bayesian oracle inequality characterizes a trade-off between the approximation error and a Bayesian characterization of the local complexity of the model, illustrating the adaptive nature of averaging-based Bayesian procedures towards achieving an optimal rate of posterior convergence. Specific applications of the model selection theory are discussed in the context of high-dimensional nonparametric regression and density regression where the regression function or the conditional density is assumed to depend on a fixed subset of predictors. As a result of independent interest, we propose a general technique for obtaining upper bounds of certain small ball probability of stationary Gaussian processes. version:1
arxiv-1701-00299 | Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution | http://arxiv.org/abs/1701.00299 | id:1701.00299 author:Lanlan Liu, Jia Deng category:cs.LG stat.ML  published:2017-01-02 summary:We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward deep neural network that allow selective execution. Given an input, only a subset of D2NN neurons are executed, and the particular subset is determined by the D2NN itself. By pruning unnecessary computation depending on input, D2NNs provide a way to improve computational efficiency. To achieve dynamic selective execution, a D2NN augments a regular feed-forward deep neural network (directed acyclic graph of differentiable modules) with one or more controller modules. Each controller module is a sub-network whose output is a decision that controls whether other modules can execute. A D2NN is trained end to end. Both regular modules and controller modules in a D2NN are learnable and are jointly trained to optimize both accuracy and efficiency. Such training is achieved by integrating backpropagation with reinforcement learning. With extensive experiments of various D2NN architectures on image classification tasks, we demonstrate that D2NNs are general and flexible, and can effectively optimize accuracy-efficiency trade-offs. version:1
arxiv-1701-00295 | Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image | http://arxiv.org/abs/1701.00295 | id:1701.00295 author:Denis Tome, Chris Russell, Lourdes Agapito category:cs.CV  published:2017-01-01 summary:We propose a unified formulation for the problem of 3D human pose estimation from a single raw RGB image that reasons jointly about 2D joint estimation and 3D pose reconstruction to improve both tasks. We take an integrated approach that fuses probabilistic knowledge of 3D human pose with a multi-stage CNN architecture and uses the knowledge of plausible 3D landmark locations to refine the search for better 2D locations. The entire process is trained end-to-end, is extremely efficient and obtains state- of-the-art results on Human3.6M outperforming previous approaches both on 2D and 3D errors. version:1
arxiv-1701-00294 | The Geodesic Distance between $\mathcal{G}_I^0$ Models and its Application to Region Discrimination | http://arxiv.org/abs/1701.00294 | id:1701.00294 author:José Naranjo-Torres, Juliana Gambini, Alejandro C. Frery category:cs.CV stat.ML  published:2017-01-01 summary:The $\mathcal{G}_I^0$ distribution is able to characterize different regions in monopolarized SAR imagery. It is indexed by three parameters: the number of looks (which can be estimated in the whole image), a scale parameter and a texture parameter. This paper presents a new proposal for feature extraction and region discrimination in SAR imagery, using the geodesic distance as a measure of dissimilarity between $\mathcal{G}_I^0$ models. We derive geodesic distances between models that describe several practical situations, assuming the number of looks known, for same and different texture and for same and different scale. We then apply this new tool to the problems of (i)~identifying edges between regions with different texture, and (ii)~quantify the dissimilarity between pairs of samples in actual SAR data. We analyze the advantages of using the geodesic distance when compared to stochastic distances. version:1
arxiv-1701-00289 | Integrating sentiment and social structure to determine preference alignments: The Irish Marriage Referendum | http://arxiv.org/abs/1701.00289 | id:1701.00289 author:David J. P. O'Sullivan, Guillermo Garduño-Hernández, James P. Gleeson, Mariano Beguerisse-Díaz category:cs.SI cs.CL cs.IR physics.soc-ph  published:2017-01-01 summary:We investigate the relationship between social structure and sentiment through the analysis of half a million tweets about the Irish Marriage Referendum of 2015. We obtain the sentiment of every tweet with the hashtags #marref and #marriageref posted in the days leading to the referendum, and construct networks to aggregate sentiment and study the interactions among users. The sentiment of the mention tweets that a user sends is correlated with the sentiment of the mentions received, and there are significantly more connections between users with similar sentiment scores than among users with opposite scores. We combine the community structure of the follower and mention networks, the activity level of the users, and sentiment scores to find groups of users who support voting 'yes' or 'no' on the referendum. We find that many conversations between users on opposing sides of the debate occurred in the absence of follower connections, suggesting that there were efforts by some users to establish dialogue and debate across ideological divisions. These results show that social structures can be successfully integrated with sentiment to analyse and understand the disposition of social media users. We discuss the implications of our work for the integration of data and meta-data, opinion dynamics, public opinion modelling and polling. version:1
arxiv-1701-00285 | High Dimensional Multi-Level Covariance Estimation and Kriging | http://arxiv.org/abs/1701.00285 | id:1701.00285 author:Julio E. Castrillon-Candas category:stat.CO stat.ML  published:2017-01-01 summary:With the advent of big data sets much of the computational science and engineering communities have been moving toward data-driven approaches to regression and classification. However, they present a significant challenge due to the increasing size, complexity and dimensionality of the problems. In this paper a multi-level kriging method that scales well with dimensions is developed. A multi-level basis is constructed that is adapted to a random projection tree (or kD-tree) partitioning of the observations and a sparse grid approximation. This approach identifies the high dimensional underlying phenomena from the noise in an accurate and numerically stable manner. Furthermore, numerically unstable covariance matrices are transformed into well conditioned multi-level matrices without compromising accuracy. A-posteriori error estimates are derived, such as the sub-exponential decay of the coefficients of the multi-level covariance matrix. The multi-level method is tested on numerically unstable problems of up to 50 dimensions. Accurate solutions with feasible computational cost are obtained. version:1
arxiv-1612-09296 | Symmetry, Saddle Points, and Global Geometry of Nonconvex Matrix Factorization | http://arxiv.org/abs/1612.09296 | id:1612.09296 author:Xingguo Li, Zhaoran Wang, Junwei Lu, Raman Arora, Jarvis Haupt, Han Liu, Tuo Zhao category:cs.LG math.OC stat.ML  published:2016-12-29 summary:We propose a general theory for studying the geometry of nonconvex objective functions with underlying symmetric structures. In specific, we characterize the locations of stationary points and the null space of the associated Hessian matrices via the lens of invariant groups. As a major motivating example, we apply the proposed general theory to characterize the global geometry of the low-rank matrix factorization problem. In particular, we illustrate how the rotational symmetry group gives rise to infinitely many non-isolated strict saddle points and equivalent global minima of the objective function. By explicitly identifying all stationary points, we divide the entire parameter space into three regions: ($\cR_1$) the region containing the neighborhoods of all strict saddle points, where the objective has negative curvatures; ($\cR_2$) the region containing neighborhoods of all global minima, where the objective enjoys strong convexity along certain directions; and ($\cR_3$) the complement of the above regions, where the gradient has sufficiently large magnitudes. We further extend our result to the matrix sensing problem. This allows us to establish strong global convergence guarantees for popular iterative algorithms with arbitrary initial solutions. version:2
arxiv-1701-00251 | Outlier Robust Online Learning | http://arxiv.org/abs/1701.00251 | id:1701.00251 author:Jiashi Feng, Huan Xu, Shie Mannor category:cs.LG stat.ML  published:2017-01-01 summary:We consider the problem of learning from noisy data in practical settings where the size of data is too large to store on a single machine. More challenging, the data coming from the wild may contain malicious outliers. To address the scalability and robustness issues, we present an online robust learning (ORL) approach. ORL is simple to implement and has provable robustness guarantee -- in stark contrast to existing online learning approaches that are generally fragile to outliers. We specialize the ORL approach for two concrete cases: online robust principal component analysis and online linear regression. We demonstrate the efficiency and robustness advantages of ORL through comprehensive simulations and predicting image tags on a large-scale data set. We also discuss extension of the ORL to distributed learning and provide experimental evaluations. version:1
arxiv-1612-07139 | Deep-learning in Mobile Robotics - from Perception to Control Systems: A Survey on Why and Why not | http://arxiv.org/abs/1612.07139 | id:1612.07139 author:Lei Tai, Ming Liu category:cs.RO cs.AI cs.LG cs.SY  published:2016-12-21 summary:Deep-learning has dramatically changed the world overnight. It greatly boosted the development of visual perception, object detection, and speech recognition, etc. That was attributed to the multiple convolutional processing layers for abstraction of learning representations from massive data. The advantages of deep convolutional structures in data processing motivated the applications of artificial intelligence methods in robotic problems, especially perception and control system, the two typical and challenging problems in robotics. This paper presents a survey of the deep-learning research landscape in mobile robotics. We start with introducing the definition and development of deep-learning in related fields, especially the essential distinctions between image processing and robotic tasks. We described and discussed several typical applications and related works in this domain, followed by the benefits from deep-learning, and related existing frameworks. Besides, operation in the complex dynamic environment is regarded as a critical bottleneck for mobile robots, such as that for autonomous driving. We thus further emphasize the recent achievement on how deep-learning contributes to navigation and control systems for mobile robots. At the end, we discuss the open challenges and research frontiers. version:3
arxiv-1701-00220 | Classification of Smartphone Users Using Internet Traffic | http://arxiv.org/abs/1701.00220 | id:1701.00220 author:Andrey Finkelstein, Ron Biton, Rami Puzis, Asaf Shabtai category:cs.LG cs.CR  published:2017-01-01 summary:Today, smartphone devices are owned by a large portion of the population and have become a very popular platform for accessing the Internet. Smartphones provide the user with immediate access to information and services. However, they can easily expose the user to many privacy risks. Applications that are installed on the device and entities with access to the device's Internet traffic can reveal private information about the smartphone user and steal sensitive content stored on the device or transmitted by the device over the Internet. In this paper, we present a method to reveal various demographics and technical computer skills of smartphone users by their Internet traffic records, using machine learning classification models. We implement and evaluate the method on real life data of smartphone users and show that smartphone users can be classified by their gender, smoking habits, software programming experience, and other characteristics. version:1
arxiv-1701-00754 | Using Artificial Neural Networks (ANN) to Control Chaos | http://arxiv.org/abs/1701.00754 | id:1701.00754 author:Ibrahim Ighneiwaa, Salwa Hamidatoua, Fadia Ben Ismaela category:cs.LG nlin.CD  published:2017-01-01 summary:Controlling Chaos could be a big factor in getting great stable amounts of energy out of small amounts of not necessarily stable resources. By definition, Chaos is getting huge changes in the system's output due to unpredictable small changes in initial conditions, and that means we could take advantage of this fact and select the proper control system to manipulate system's initial conditions and inputs in general and get a desirable output out of otherwise a Chaotic system. That was accomplished by first building some known chaotic circuit (Chua circuit) and the NI's MultiSim was used to simulate the ANN control system. It was shown that this technique can also be used to stabilize some hard to stabilize electronic systems. version:1
arxiv-1701-00198 | A robust approach for tree segmentation in deciduous forests using small-footprint airborne LiDAR data | http://arxiv.org/abs/1701.00198 | id:1701.00198 author:Hamid Hamraz, Marco A. Contreras, Jun Zhang category:cs.CV cs.CE cs.CG  published:2017-01-01 summary:This paper presents a non-parametric approach for segmenting trees from airborne LiDAR data in deciduous forests. Based on the LiDAR point cloud, the approach collects crown information such as steepness and height on-the-fly to delineate crown boundaries, and most importantly, does not require a priori assumptions of crown shape and size. The approach segments trees iteratively starting from the tallest within a given area to the smallest until all trees have been segmented. To evaluate its performance, the approach was applied to the University of Kentucky Robinson Forest, a deciduous closed-canopy forest with complex terrain and vegetation conditions. The approach identified 94% of dominant and co-dominant trees with a false detection rate of 13%. About 62% of intermediate, overtopped, and dead trees were also detected with a false detection rate of 15%. The overall segmentation accuracy was 77%. Correlations of the segmentation scores of the proposed approach with local terrain and stand metrics was not significant, which is likely an indication of the robustness of the approach as results are not sensitive to the differences in terrain and stand structures. version:1
arxiv-1701-00193 | Video-based Person Re-identification with Accumulative Motion Context | http://arxiv.org/abs/1701.00193 | id:1701.00193 author:Hao Liu, Zequn Jie, Karlekar Jayashree, Meibin Qi, Jianguo Jiang, Shuicheng Yan, Jiashi Feng category:cs.CV  published:2017-01-01 summary:Video based person re-identification plays a central role in realistic security and video surveillance. In this paper we propose a novel Accumulative Motion Context (AMOC) network for addressing this important problem, which effectively exploits the long-range motion context for robustly identifying the same person under challenging conditions. Given a video sequence of the same or different persons, the proposed AMOC network jointly learns appearance representation and motion context from a collection of adjacent frames using a two-stream convolutional architecture. Then AMOC accumulates clues from motion context by recurrent aggregation, allowing effective information flow among adjacent frames and capturing dynamic gist of the persons. The architecture of AMOC is end-to-end trainable and thus motion context can be adapted to complement appearance clues under unfavorable conditions (\textit{e.g.}, occlusions). Extensive experiments are conduced on two public benchmark datasets, \textit{i.e.}, the iLIDS-VID and PRID-2011 datasets, to investigate the performance of AMOC. The experimental results demonstrate that the proposed AMOC network outperforms state-of-the-arts for video-based re-identification significantly and confirm the advantage of exploiting long-range motion context for video based person re-identification, validating our motivation evidently. version:1
arxiv-1701-00188 | Aspect-augmented Adversarial Networks for Domain Adaptation | http://arxiv.org/abs/1701.00188 | id:1701.00188 author:Yuan Zhang, Regina Barzilay, Tommi Jaakkola category:cs.CL  published:2017-01-01 summary:We introduce a neural method for transfer learning between two (source and target) classification tasks or aspects over the same domain. Instead of target labels, we assume a few keywords pertaining to source and target aspects indicating sentence relevance rather than document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant. Experimental results demonstrate that our approach outperforms different baselines and model variants on two datasets, yielding an improvement of 24% on a pathology dataset and 5% on a review dataset. version:1
arxiv-1701-00185 | Self-Taught Convolutional Neural Networks for Short Text Clustering | http://arxiv.org/abs/1701.00185 | id:1701.00185 author:Jiaming Xu, Bo XuPeng Wang, Suncong Zheng, Guanhua Tian, Jun Zhao, Bo Xu category:cs.IR cs.CL  published:2017-01-01 summary:Short text clustering is a challenging problem due to its sparseness of text representation. Here we propose a flexible Self-Taught Convolutional neural network framework for Short Text Clustering (dubbed STC^2), which can flexibly and successfully incorporate more useful semantic features and learn non-biased deep text representation in an unsupervised manner. In our framework, the original raw text features are firstly embedded into compact binary codes by using one existing unsupervised dimensionality reduction methods. Then, word embeddings are explored and fed into convolutional neural networks to learn deep feature representations, meanwhile the output units are used to fit the pre-trained binary codes in the training process. Finally, we get the optimal clusters by employing K-means to cluster the learned representations. Extensive experimental results demonstrate that the proposed framework is effective, flexible and outperform several popular clustering methods when tested on three public short text datasets. version:1
arxiv-1701-00178 | Lazily Adapted Constant Kinky Inference for Nonparametric Regression and Model-Reference Adaptive Control | http://arxiv.org/abs/1701.00178 | id:1701.00178 author:Jan-Peter Calliess category:math.OC cs.AI cs.LG cs.SY stat.ML  published:2016-12-31 summary:Techniques known as Nonlinear Set Membership prediction, Lipschitz Interpolation or Kinky Inference are approaches to machine learning that utilise presupposed Lipschitz properties to compute inferences over unobserved function values. Provided a bound on the true best Lipschitz constant of the target function is known a priori they offer convergence guarantees as well as bounds around the predictions. Considering a more general setting that builds on Hoelder continuity relative to pseudo-metrics, we propose an online method for estimating the Hoelder constant online from function value observations that possibly are corrupted by bounded observational errors. Utilising this to compute adaptive parameters within a kinky inference rule gives rise to a nonparametric machine learning method, for which we establish strong universal approximation guarantees. That is, we show that our prediction rule can learn any continuous function in the limit of increasingly dense data to within a worst-case error bound that depends on the level of observational uncertainty. We apply our method in the context of nonparametric model-reference adaptive control (MRAC). Across a range of simulated aircraft roll-dynamics and performance metrics our approach outperforms recently proposed alternatives that were based on Gaussian processes and RBF-neural networks. For discrete-time systems, we provide stability guarantees for our learning-based controllers both for the batch and the online learning setting. version:1
arxiv-1701-00169 | Tree segmentation in multi-story stands within small-footprint airborne LiDAR data | http://arxiv.org/abs/1701.00169 | id:1701.00169 author:Hamid Hamraz, Marco A. Contreras, Jun Zhang category:cs.CV cs.CE cs.CG  published:2016-12-31 summary:Airborne LiDAR point cloud of a forest contains three dimensional data, from which vertical stand structure (including information about under-story trees) can be derived. This paper presents a segmentation approach for multi-story stands that strips the point cloud to its canopy layers, identifies individual tree segments within each layer using a DSM-based tree identification method as a building block, and combines the segments of immediate layers in order to fix potential over-segmentation of tree crowns across the layers. We introduce local layering that analyzes the vertical distributions of LiDAR points within their local neighborhoods in order to locally determine the height thresholds for layering the canopy. Unlike the previous work that stripped stiff layers within constrained areas, the local layering method strips flexible (in thickness and elevation) and narrower canopy layers within unconstrained areas. Statistical analyses showed that layering in general strongly improves identifying (specifically under-story) trees for the cost of moderately increasing over-segmentation rate of the identified trees. Combining tree segments across the immediate layers did not seem to improve tree identification accuracy remarkably, suggesting that layers separated canopy layers rather precisely. version:1
arxiv-1701-00168 | Social Media Argumentation Mining: The Quest for Deliberateness in Raucousness | http://arxiv.org/abs/1701.00168 | id:1701.00168 author:Jan Šnajder category:cs.CL  published:2016-12-31 summary:Argumentation mining from social media content has attracted increasing attention. The task is both challenging and rewarding. The informal nature of user-generated content makes the task dauntingly difficult. On the other hand, the insights that could be gained by a large-scale analysis of social media argumentation make it a very worthwhile task. In this position paper I discuss the motivation for social media argumentation mining, as well as the tasks and challenges involved. version:1
arxiv-1701-00167 | Very Fast Kernel SVM under Budget Constraints | http://arxiv.org/abs/1701.00167 | id:1701.00167 author:David Picard category:stat.ML cs.LG  published:2016-12-31 summary:In this paper we propose a fast online Kernel SVM algorithm under tight budget constraints. We propose to split the input space using LVQ and train a Kernel SVM in each cluster. To allow for online training, we propose to limit the size of the support vector set of each cluster using different strategies. We show in the experiment that our algorithm is able to achieve high accuracy while having a very high number of samples processed per second both in training and in the evaluation. version:1
arxiv-1701-00165 | Improved Stereo Matching with Constant Highway Networks and Reflective Confidence Learning | http://arxiv.org/abs/1701.00165 | id:1701.00165 author:Amit Shaked, Lior Wolf category:cs.CV  published:2016-12-31 summary:We present an improved three-step pipeline for the stereo matching problem and introduce multiple novelties at each stage. We propose a new highway network architecture for computing the matching cost at each possible disparity, based on multilevel weighted residual shortcuts, trained with a hybrid loss that supports multilevel comparison of image patches. A novel post-processing step is then introduced, which employs a second deep convolutional neural network for pooling global information from multiple disparities. This network outputs both the image disparity map, which replaces the conventional "winner takes all" strategy, and a confidence in the prediction. The confidence score is achieved by training the network with a new technique that we call the reflective loss. Lastly, the learned confidence is employed in order to better detect outliers in the refinement step. The proposed pipeline achieves state of the art accuracy on the largest and most competitive stereo benchmarks, and the learned confidence is shown to outperform all existing alternatives. version:1
arxiv-1701-00160 | NIPS 2016 Tutorial: Generative Adversarial Networks | http://arxiv.org/abs/1701.00160 | id:1701.00160 author:Ian Goodfellow category:cs.LG  published:2016-12-31 summary:This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises. version:1
arxiv-1701-00145 | Expanding Subjective Lexicons for Social Media Mining with Embedding Subspaces | http://arxiv.org/abs/1701.00145 | id:1701.00145 author:Silvio Amir, Rámon Astudillo, Wang Ling, Paula C. Carvalho, Mário J. Silva category:cs.CL  published:2016-12-31 summary:Recent approaches for sentiment lexicon induction have capitalized on pre-trained word embeddings that capture latent semantic properties. However, embeddings obtained by optimizing performance of a given task (e.g. predicting contextual words) are sub-optimal for other applications. In this paper, we address this problem by exploiting task-specific representations, induced via embedding sub-space projection. This allows us to expand lexicons describing multiple semantic properties. For each property, our model jointly learns suitable representations and the concomitant predictor. Experiments conducted over multiple subjective lexicons, show that our model outperforms previous work and other baselines; even in low training data regimes. Furthermore, lexicon-based sentiment classifiers built on top of our lexicons outperform similar resources and yield performances comparable to those of supervised models. version:1
arxiv-1701-00142 | EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras (Extended Abstract) | http://arxiv.org/abs/1701.00142 | id:1701.00142 author:Helge Rhodin, Christian Richardt, Dan Casas, Eldar Insafutdinov, Mohammad Shafiei, Hans-Peter Seidel, Bernt Schiele, Christian Theobalt category:cs.CV  published:2016-12-31 summary:Marker-based and marker-less optical skeletal motion-capture methods use an outside-in arrangement of cameras placed around a scene, with viewpoints converging on the center. They often create discomfort by possibly needed marker suits, and their recording volume is severely restricted and often constrained to indoor scenes with controlled backgrounds. We therefore propose a new method for real-time, marker-less and egocentric motion capture which estimates the full-body skeleton pose from a lightweight stereo pair of fisheye cameras that are attached to a helmet or virtual-reality headset. It combines the strength of a new generative pose estimation framework for fisheye views with a ConvNet-based body-part detector trained on a new automatically annotated and augmented dataset. Our inside-in method captures full-body motion in general indoor and outdoor scenes, and also crowded scenes. version:1
arxiv-1701-00138 | RNN-based Encoder-decoder Approach with Word Frequency Estimation | http://arxiv.org/abs/1701.00138 | id:1701.00138 author:Jun Suzuki, Masaaki Nagata category:cs.CL cs.AI stat.ML  published:2016-12-31 summary:This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark. version:1
arxiv-1701-00736 | Simulated Tornado Optimization | http://arxiv.org/abs/1701.00736 | id:1701.00736 author:S. Hossein Hosseini, Tohid Nouri, Afshin Ebrahimi, S. Ali Hosseini category:math.OC cs.AI cs.NE  published:2016-12-31 summary:We propose a swarm-based optimization algorithm inspired by air currents of a tornado. Two main air currents - spiral and updraft - are mimicked. Spiral motion is designed for exploration of new search areas and updraft movements is deployed for exploitation of a promising candidate solution. Assignment of just one search direction to each particle at each iteration, leads to low computational complexity of the proposed algorithm respect to the conventional algorithms. Regardless of the step size parameters, the only parameter of the proposed algorithm, called tornado diameter, can be efficiently adjusted by randomization. Numerical results over six different benchmark cost functions indicate comparable and, in some cases, better performance of the proposed algorithm respect to some other metaheuristics. version:1
arxiv-1701-00066 | A POS Tagger for Code Mixed Indian Social Media Text - ICON-2016 NLP Tools Contest Entry from Surukam | http://arxiv.org/abs/1701.00066 | id:1701.00066 author:Sree Harsha Ramesh, Raveena R Kumar category:cs.CL  published:2016-12-31 summary:Building Part-of-Speech (POS) taggers for code-mixed Indian languages is a particularly challenging problem in computational linguistics due to a dearth of accurately annotated training corpora. ICON, as part of its NLP tools contest has organized this challenge as a shared task for the second consecutive year to improve the state-of-the-art. This paper describes the POS tagger built at Surukam to predict the coarse-grained and fine-grained POS tags for three language pairs - Bengali-English, Telugu-English and Hindi-English, with the text spanning three popular social media platforms - Facebook, WhatsApp and Twitter. We employed Conditional Random Fields as the sequence tagging algorithm and used a library called sklearn-crfsuite - a thin wrapper around CRFsuite for training our model. Among the features we used include - character n-grams, language information and patterns for emoji, number, punctuation and web-address. Our submissions in the constrained environment,i.e., without making any use of monolingual POS taggers or the like, obtained an overall average F1-score of 76.45%, which is comparable to the 2015 winning score of 76.79%. version:1
arxiv-1701-00040 | p-DLA: A Predictive System Model for Onshore Oil and Gas Pipeline Dataset Classification and Monitoring - Part 1 | http://arxiv.org/abs/1701.00040 | id:1701.00040 author:E. N. Osegi category:cs.CV  published:2016-12-31 summary:With the rise in militant activity and rogue behaviour in oil and gas regions around the world, oil pipeline disturbances is on the increase leading to huge losses to multinational operators and the countries where such facilities exist. However, this situation can be averted if adequate predictive monitoring schemes are put in place. We propose in the first part of this paper, an artificial intelligence predictive monitoring system capable of predictive classification and pattern recognition of pipeline datasets. The predictive system is based on a highly sparse predictive Deviant Learning Algorithm (p-DLA) designed to synthesize a sequence of memory predictive clusters for eventual monitoring, control and decision making. The DLA (p-DLA) is compared with a popular machine learning algorithm, the Long Short-Term Memory (LSTM) which is based on a temporal version of the standard feed-forward back-propagation trained artificial neural networks (ANNs). The results of simulations study show impressive results and validates the sparse memory predictive approach which favours the sub-synthesis of a highly compressed and low dimensional knowledge discovery and information prediction scheme. It also shows that the proposed new approach is competitive with a well-known and proven AI approach such as the LSTM. version:1
arxiv-1701-00031 | Super-Resolution Reconstruction of Electrical Impedance Tomography Images | http://arxiv.org/abs/1701.00031 | id:1701.00031 author:R. A. Borsoi, J. C. C. Aya, G. H. Costa, J. C. M. Bermudez category:cs.CV  published:2016-12-30 summary:Electrical Impedance Tomography (EIT) systems are becoming popular because they present several advantages over competing systems. However, EIT leads to images with very low resolution. Moreover, the nonuniform sampling characteristic of EIT precludes the straightforward application of traditional image ruper-resolution techniques. In this work, we propose a resampling based Super-Resolution method for EIT image quality improvement. Preliminary results show that the proposed technique can lead to substantial improvements in EIT image resolution, making it more competitive with other technologies. version:1
arxiv-1701-00008 | Deep Neural Networks to Enable Real-time Multimessenger Astrophysics | http://arxiv.org/abs/1701.00008 | id:1701.00008 author:Daniel George, E. A. Huerta category:astro-ph.IM astro-ph.GA astro-ph.HE cs.LG gr-qc  published:2016-12-30 summary:We introduce a new methodology for time-domain signal processing, based on deep learning neural networks, which has the potential to revolutionize data analysis in science. To illustrate how this enables real-time multimessenger astrophysics, we designed two deep convolutional neural networks that can analyze time-series data from observatories including advanced LIGO. The first neural network recognizes the presence of gravitational waves from binary black hole mergers, and the second one estimates the mass of each black hole, given weak signals hidden in extremely noisy time-series inputs. We highlight the advantages offered by this novel method, which outperforms matched-filtering or conventional machine learning techniques, and propose strategies to extend our implementation for simultaneously targeting different classes of gravitational wave sources while ignoring anomalous noise transients. Our results strongly indicate that deep neural networks are highly efficient and versatile tools for directly processing raw noisy data streams. Furthermore, we pioneer a new paradigm to accelerate scientific discovery by combining high-performance simulations on traditional supercomputers and artificial intelligence algorithms that exploit innovative hardware architectures such as deep-learning-optimized GPUs. This unique approach immediately provides a natural framework to unify multi-spectrum observations in real-time, thus enabling coincident detection campaigns of gravitational waves sources and their electromagnetic counterparts. version:1
arxiv-1612-09596 | Counterfactual Prediction with Deep Instrumental Variables Networks | http://arxiv.org/abs/1612.09596 | id:1612.09596 author:Jason Hartford, Greg Lewis, Kevin Leyton-Brown, Matt Taddy category:stat.AP cs.LG stat.ML  published:2016-12-30 summary:We are in the middle of a remarkable rise in the use and capability of artificial intelligence. Much of this growth has been fueled by the success of deep learning architectures: models that map from observables to outputs via multiple layers of latent representations. These deep learning algorithms are effective tools for unstructured prediction, and they can be combined in AI systems to solve complex automated reasoning problems. This paper provides a recipe for combining ML algorithms to solve for causal effects in the presence of instrumental variables -- sources of treatment randomization that are conditionally independent from the response. We show that a flexible IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework imposes some specific structure on the stochastic gradient descent routine used for training, but it is general enough that we can take advantage of off-the-shelf ML capabilities and avoid extensive algorithm customization. We outline how to obtain out-of-sample causal validation in order to avoid over-fit. We also introduce schemes for both Bayesian and frequentist inference: the former via a novel adaptation of dropout training, and the latter via a data splitting routine. version:1
arxiv-1612-09574 | Automatic Data Deformation Analysis on Evolving Folksonomy Driven Environment | http://arxiv.org/abs/1612.09574 | id:1612.09574 author:Massimiliano Dal Mas category:cs.IR cs.CL cs.CY cs.SI  published:2016-12-30 summary:The Folksodriven framework makes it possible for data scientists to define an ontology environment where searching for buried patterns that have some kind of predictive power to build predictive models more effectively. It accomplishes this through an abstractions that isolate parameters of the predictive modeling process searching for patterns and designing the feature set, too. To reflect the evolving knowledge, this paper considers ontologies based on folksonomies according to a new concept structure called "Folksodriven" to represent folksonomies. So, the studies on the transformational regulation of the Folksodriven tags are regarded to be important for adaptive folksonomies classifications in an evolving environment used by Intelligent Systems to represent the knowledge sharing. Folksodriven tags are used to categorize salient data points so they can be fed to a machine-learning system and "featurizing" the data. version:1
arxiv-1612-09548 | A Unified Tensor-based Active Appearance Face Model | http://arxiv.org/abs/1612.09548 | id:1612.09548 author:Zhen-Hua Feng, Josef Kittler, William Christmas, Xiao-Jun Wu category:cs.CV  published:2016-12-30 summary:Appearance variations result in many difficulties in face image analysis. To deal with this challenge, we present a Unified Tensor-based Active Appearance Model (UT-AAM) for jointly modelling the geometry and texture information of 2D faces. In contrast with the classical Tensor-based AAM (T-AAM), the proposed UT-AAM has four advantages: First, for each type of face information, namely shape and texture, we construct a tensor model capturing all relevant appearance variations. This unified tensor model contrasts with the variation-specific models of T-AAM. Second, a strategy for dealing with self-occluded faces is proposed to obtain consistent shape and texture representations of faces across large pose variations. Third, our UT-AAM is capable of constructing the model from an incomplete training dataset, using tensor completion methods. Last, we use an effective cascaded-regression-based method for UT-AAM fitting. With these improvements, the utility of UT-AAM in practice is considerably enhanced in comparison with the classical T-AAM. As an example, we demonstrate the improvements in training facial landmark detectors through the use of UT-AAM to synthesise a large number of virtual samples. Experimental results obtained using the Multi-PIE and 300-W face datasets demonstrate the merits of the proposed approach. version:1
arxiv-1612-08714 | Clustering with Confidence: Finding Clusters with Statistical Guarantees | http://arxiv.org/abs/1612.08714 | id:1612.08714 author:Andreas Henelius, Kai Puolamäki, Henrik Boström, Panagiotis Papapetrou category:stat.ML cs.LG  published:2016-12-27 summary:Clustering is a widely used unsupervised learning method for finding structure in the data. However, the resulting clusters are typically presented without any guarantees on their robustness; slightly changing the used data sample or re-running a clustering algorithm involving some stochastic component may lead to completely different clusters. There is, hence, a need for techniques that can quantify the instability of the generated clusters. In this study, we propose a technique for quantifying the instability of a clustering solution and for finding robust clusters, termed core clusters, which correspond to clusters where the co-occurrence probability of each data item within a cluster is at least $1 - \alpha$. We demonstrate how solving the core clustering problem is linked to finding the largest maximal cliques in a graph. We show that the method can be used with both clustering and classification algorithms. The proposed method is tested on both simulated and real datasets. The results show that the obtained clusters indeed meet the guarantees on robustness. version:2
arxiv-1612-09542 | A Joint Speaker-Listener-Reinforcer Model for Referring Expressions | http://arxiv.org/abs/1612.09542 | id:1612.09542 author:Licheng Yu, Hao Tan, Mohit Bansal, Tamara L. Berg category:cs.CV cs.AI cs.CL  published:2016-12-30 summary:Referring expressions are natural language constructions used to identify particular objects within a scene. In this paper, we propose a unified framework for the tasks of referring expression comprehension and generation. Our model is composed of three modules: speaker, listener, and reinforcer. The speaker generates referring expressions, the listener comprehends referring expressions, and the reinforcer introduces a reward function to guide sampling of more discriminative expressions. The listener-speaker modules are trained jointly in an end-to-end learning framework, allowing the modules to be aware of one another during learning while also benefiting from the discriminative reinforcer's feedback. We demonstrate that this unified framework and training achieves state-of-the-art results for both comprehension and generation on three referring expression datasets. Project and demo page: https://vision.cs.unc.edu/refer version:1
arxiv-1612-09535 | PAMPO: using pattern matching and pos-tagging for effective Named Entities recognition in Portuguese | http://arxiv.org/abs/1612.09535 | id:1612.09535 author:Conceição Rocha, Alípio Jorge, Roberta Sionara, Paula Brito, Carlos Pimenta, Solange Rezende category:cs.IR cs.CL  published:2016-12-30 summary:This paper deals with the entity extraction task (named entity recognition) of a text mining process that aims at unveiling non-trivial semantic structures, such as relationships and interaction between entities or communities. In this paper we present a simple and efficient named entity extraction algorithm. The method, named PAMPO (PAttern Matching and POs tagging based algorithm for NER), relies on flexible pattern matching, part-of-speech tagging and lexical-based rules. It was developed to process texts written in Portuguese, however it is potentially applicable to other languages as well. We compare our approach with current alternatives that support Named Entity Recognition (NER) for content written in Portuguese. These are Alchemy, Zemanta and Rembrandt. Evaluation of the efficacy of the entity extraction method on several texts written in Portuguese indicates a considerable improvement on $recall$ and $F_1$ measures. version:1
arxiv-1612-09508 | Feedback Networks | http://arxiv.org/abs/1612.09508 | id:1612.09508 author:Amir R. Zamir, Te-Lin Wu, Lin Sun, William Shen, Jitendra Malik, Silvio Savarese category:cs.CV  published:2016-12-30 summary:Currently, the most successful learning models in computer vision are based on learning successive representations followed by a decision layer. This is usually actualized through feedforward multilayer neural networks, e.g. ConvNets, where each layer forms one of such successive representations. However, an alternative that can achieve the same goal is a feedback based approach in which the representation is formed in an iterative manner based on a feedback received from previous iteration's output. We establish that a feedback based approach has several fundamental advantages over feedforward: it enables making early predictions at the query time, its output naturally conforms to a hierarchical structure in the label space (e.g. a taxonomy), and it provides a new basis for Curriculum Learning. We observe that feedback networks develop a considerably different representation compared to feedforward counterparts, in line with the aforementioned advantages. We put forth a general feedback based learning architecture with the endpoint results on par or better than existing feedforward networks with the addition of the above advantages. We also investigate several mechanisms in feedback architectures (e.g. skip connections in time) and design choices (e.g. feedback length). We hope this study offers new perspectives in quest for more natural and practical learning models. version:1
arxiv-1612-09506 | Adult Content Recognition from Images Using a Mixture of Convolutional Neural Networks | http://arxiv.org/abs/1612.09506 | id:1612.09506 author:Mundher Al-Shabi, Tee Connie, Andrew Beng Jin Teoh category:stat.ML cs.CV cs.NE  published:2016-12-30 summary:With rapid development of the Internet, the web contents become huge. Most of the websites are publicly available and anyone can access the contents everywhere such as workplace, home and even schools. Nev-ertheless, not all the web contents are appropriate for all users, especially children. An example of these contents is pornography images which should be restricted to certain age group. Besides, these images are not safe for work (NSFW) in which employees should not be seen accessing such contents. Recently, convolutional neural networks have been successfully applied to many computer vision problems. Inspired by these successes, we propose a mixture of convolutional neural networks for adult content recognition. Unlike other works, our method is formulated on a weighted sum of multiple deep neural network models. The weights of each CNN models are expressed as a linear regression problem learnt using Ordinary Least Squares (OLS). Experimental results demonstrate that the proposed model outperforms both single CNN model and the average sum of CNN models in adult content recognition. version:1
arxiv-1612-08185 | Deep Probabilistic Modeling of Natural Images using a Pyramid Decomposition | http://arxiv.org/abs/1612.08185 | id:1612.08185 author:Alexander Kolesnikov, Christoph H. Lampert category:cs.CV  published:2016-12-24 summary:We introduce a new technique for probabilistic modeling of natural images that combines the advantages of classic multi-scale and modern deep learning models. By explicitly representing natural images at different scales we derive a model that can capture high level image structure in a computationally efficient way. We show experimentally that our model achieves new state-of-the-art image modeling performance on the CIFAR-10 dataset and at the same time is much faster than competitive models. We also evaluate the proposed technique on a human faces dataset and demonstrate the potential of our model to generate nearly photorealistic face samples. version:2
arxiv-1612-09466 | Double Coupled Canonical Polyadic Decomposition for Joint Blind Source Separation | http://arxiv.org/abs/1612.09466 | id:1612.09466 author:Xiao-Feng Gong, Qiu-Hua Lin, Feng-Yu Cong, Lieven De Lathauwer category:stat.ML  published:2016-12-30 summary:Joint blind source separation (J-BSS) is an emerging data-driven technique for multi-set data-fusion. In this paper, J-BSS is addressed from a tensorial perspective. We show how, by using second-order multi-set statistics in J-BSS, a specific double coupled canonical polyadic decomposition (DC-CPD) problem can be formulated. We propose an algebraic DC-CPD algorithm based on a coupled rank-1 detection mapping. This algorithm converts a possibly underdetermined DC-CPD to a set of overdetermined CPDs. The latter can be solved algebraically via a generalized eigenvalue decomposition based scheme. Therefore, this algorithm is deterministic and returns the exact solution in the noiseless case. In the noisy case, it can be used to effectively initialize optimization based DC-CPD algorithms. In addition, we obtain the determini- stic and generic uniqueness conditions for DC-CPD, which are shown to be more relaxed than their CPD counterparts. Experiment results are given to illustrate the superiority of DC- CPD over standard CPD with regards to uniqueness and accuracy. version:1
arxiv-1612-09465 | Adaptive Lambda Least-Squares Temporal Difference Learning | http://arxiv.org/abs/1612.09465 | id:1612.09465 author:Timothy A. Mann, Hugo Penedones, Shie Mannor, Todd Hester category:cs.LG cs.AI stat.ML  published:2016-12-30 summary:Temporal Difference learning or TD($\lambda$) is a fundamental algorithm in the field of reinforcement learning. However, setting TD's $\lambda$ parameter, which controls the timescale of TD updates, is generally left up to the practitioner. We formalize the $\lambda$ selection problem as a bias-variance trade-off where the solution is the value of $\lambda$ that leads to the smallest Mean Squared Value Error (MSVE). To solve this trade-off we suggest applying Leave-One-Trajectory-Out Cross-Validation (LOTO-CV) to search the space of $\lambda$ values. Unfortunately, this approach is too computationally expensive for most practical applications. For Least Squares TD (LSTD) we show that LOTO-CV can be implemented efficiently to automatically tune $\lambda$ and apply function optimization methods to efficiently search the space of $\lambda$ values. The resulting algorithm, ALLSTD, is parameter free and our experiments demonstrate that ALLSTD is significantly computationally faster than the na\"{i}ve LOTO-CV implementation while achieving similar performance. version:1
arxiv-1612-09438 | Automatic Discoveries of Physical and Semantic Concepts via Association Priors of Neuron Groups | http://arxiv.org/abs/1612.09438 | id:1612.09438 author:Shuai Li, Kui Jia, Xiaogang Wang category:cs.LG  published:2016-12-30 summary:The recent successful deep neural networks are largely trained in a supervised manner. It {\it associates} complex patterns of input samples with neurons in the last layer, which form representations of {\it concepts}. In spite of their successes, the properties of complex patterns associated a learned concept remain elusive. In this work, by analyzing how neurons are associated with concepts in supervised networks, we hypothesize that with proper priors to regulate learning, neural networks can automatically associate neurons in the intermediate layers with concepts that are aligned with real world concepts, when trained only with labels that associate concepts with top level neurons, which is a plausible way for unsupervised learning. We develop a prior to verify the hypothesis and experimentally find the proposed priors help neural networks automatically learn both basic physical concepts at the lower layers, e.g., rotation of filters, and highly semantic concepts at the higher layers, e.g., fine-grained categories of an entry-level category. version:1
arxiv-1612-09434 | Data driven estimation of Laplace-Beltrami operator | http://arxiv.org/abs/1612.09434 | id:1612.09434 author:Frédéric Chazal, Ilaria Giulini, Bertrand Michel category:cs.CG cs.LG math.ST stat.TH  published:2016-12-30 summary:Approximations of Laplace-Beltrami operators on manifolds through graph Lapla-cians have become popular tools in data analysis and machine learning. These discretized operators usually depend on bandwidth parameters whose tuning remains a theoretical and practical problem. In this paper, we address this problem for the unnormalized graph Laplacian by establishing an oracle inequality that opens the door to a well-founded data-driven procedure for the bandwidth selection. Our approach relies on recent results by Lacour and Massart [LM15] on the so-called Lepski's method. version:1
arxiv-1612-09420 | Automatic labeling of molecular biomarkers of whole slide immunohistochemistry images using fully convolutional networks | http://arxiv.org/abs/1612.09420 | id:1612.09420 author:Fahime Sheikhzadeh, Martial Guillaud, Rabab K. Ward category:q-bio.TO cs.CV  published:2016-12-30 summary:This paper addresses the problem of quantifying biomarkers in multi-stained tissues, based on color and spatial information. A deep learning based method that can automatically localize and quantify the cells expressing biomarker(s) in a whole slide image is proposed. The deep learning network is a fully convolutional network (FCN) whose input is the true RGB color image of a tissue and output is a map of the different biomarkers. The FCN relies on a convolutional neural network (CNN) that classifies each cell separately according to the biomarker it expresses. In this study, images of immunohistochemistry (IHC) stained slides were collected and used. More than 4,500 RGB images of cells were manually labeled based on the expressing biomarkers. The labeled cell images were used to train the CNN (obtaining an accuracy of 92% in a test set). The trained CNN is then extended to an FCN that generates a map of all biomarkers in the whole slide image acquired by the scanner (instead of classifying every cell image). To evaluate our method, we manually labeled all nuclei expressing different biomarkers in two whole slide images and used theses as the ground truth. Our proposed method for immunohistochemical analysis compares well with the manual labeling by humans (average F-score of 0.96). version:1
arxiv-1612-08843 | FastMask: Segment Multi-scale Object Candidates in One Shot | http://arxiv.org/abs/1612.08843 | id:1612.08843 author:Hexiang Hu, Shiyi Lan, Yuning Jiang, Zhimin Cao, Fei Sha category:cs.CV cs.AI  published:2016-12-28 summary:Objects appear to scale differently in natural images. This fact requires methods dealing with object-centric tasks e.g. object proposal to have robust performance over scale variances of objects. In the paper we present a novel segment proposal framework, namely FastMask, which takes advantage of the hierarchical structure in deep convolutional neural network to segment multi-scale objects in one shot. Innovatively, we generalize segment proposal network into three different functional components (body, neck and head). We further propose a weight-shared residual neck module as well as a scale-tolerant attentional head module for multi-scale training and efficient one-shot inference. On MS COCO benchmark, the proposed FastMask outperforms all state-of-the-art segment proposal methods in average recall while keeping 2~5 times faster. More impressively, with a slight trade-off in accuracy, FastMask can segment objects in near real time (~13 fps) at 800$\times$600 resolution images, highlighting its potential in practical applications. Our implementation is available on https://github.com/voidrank/FastMask. version:2
arxiv-1612-09411 | Shape Estimation from Defocus Cue for Microscopy Images via Belief Propagation | http://arxiv.org/abs/1612.09411 | id:1612.09411 author:Arnav Bhavsar category:cs.CV  published:2016-12-30 summary:In recent years, the usefulness of 3D shape estimation is being realized in microscopic or close-range imaging, as the 3D information can further be used in various applications. Due to limited depth of field at such small distances, the defocus blur induced in images can provide information about the 3D shape of the object. The task of `shape from defocus' (SFD), involves the problem of estimating good quality 3D shape estimates from images with depth-dependent defocus blur. While the research area of SFD is quite well-established, the approaches have largely demonstrated results on objects with bulk/coarse shape variation. However, in many cases, objects studied under microscopes often involve fine/detailed structures, which have not been explicitly considered in most methods. In addition, given that, in recent years, large data volumes are typically associated with microscopy related applications, it is also important for such SFD methods to be efficient. In this work, we provide an indication of the usefulness of the Belief Propagation (BP) approach in addressing these concerns for SFD. BP has been known to be an efficient combinatorial optimization approach, and has been empirically demonstrated to yield good quality solutions in low-level vision problems such as image restoration, stereo disparity estimation etc. For exploiting the efficiency of BP in SFD, we assume local space-invariance of the defocus blur, which enables the application of BP in a straightforward manner. Even with such an assumption, the ability of BP to provide good quality solutions while using non-convex priors, reflects in yielding plausible shape estimates in presence of fine structures on the objects under microscopy imaging. version:1
arxiv-1612-09401 | Action Recognition Based on Joint Trajectory Maps with Convolutional Neural Networks | http://arxiv.org/abs/1612.09401 | id:1612.09401 author:Pichao Wang, Wanqing Li, Chuankun Li, Yonghong Hou category:cs.CV  published:2016-12-30 summary:Convolutional Neural Networks (ConvNets) have recently shown promising performance in many computer vision tasks, especially image-based recognition. How to effectively apply ConvNets to sequence-based data is still an open problem. This paper proposes an effective yet simple method to represent spatio-temporal information carried in $3D$ skeleton sequences into three $2D$ images by encoding the joint trajectories and their dynamics into color distribution in the images, referred to as Joint Trajectory Maps (JTM), and adopts ConvNets to learn the discriminative features for human action recognition. Such an image-based representation enables us to fine-tune existing ConvNets models for the classification of skeleton sequences without training the networks afresh. The three JTMs are generated in three orthogonal planes and provide complimentary information to each other. The final recognition is further improved through multiply score fusion of the three JTMs. The proposed method was evaluated on four public benchmark datasets, the large NTU RGB+D Dataset, MSRC-12 Kinect Gesture Dataset (MSRC-12), G3D Dataset and UTD Multimodal Human Action Dataset (UTD-MHAD) and achieved the state-of-the-art results. version:1
arxiv-1612-07429 | Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks | http://arxiv.org/abs/1612.07429 | id:1612.07429 author:Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee, Hailin Jin, Thomas Funkhouser category:cs.CV  published:2016-12-22 summary:Indoor scene understanding is central to applications such as robot navigation and human companion assistance. Over the last years, data-driven deep neural networks have outperformed many traditional approaches thanks to their representation learning capabilities. One of the bottlenecks in training for better representations is the amount of available per-pixel ground truth data that is required for core scene understanding tasks such as semantic segmentation, normal prediction, and object edge detection. To address this problem, a number of works proposed using synthetic data. However, a systematic study of how such synthetic data is generated is missing. In this work, we introduce a large-scale synthetic dataset with 400K physically-based rendered images from 45K realistic 3D indoor scenes. We study the effects of rendering methods and scene lighting on training for three computer vision tasks: surface normal prediction, semantic segmentation, and object boundary detection. This study provides insights into the best practices for training with synthetic data (more realistic rendering is worth it) and shows that pretraining with our new synthetic dataset can improve results beyond the current state of the art on all three tasks. version:2
arxiv-1612-09346 | Rotation equivariant vector field networks | http://arxiv.org/abs/1612.09346 | id:1612.09346 author:Diego Marcos, Michele Volpi, Nikos Komodakis, Devis Tuia category:cs.CV  published:2016-12-29 summary:We propose a method to encode rotation equivariance or invariance into convolutional neural networks (CNNs). Each convolutional filter is applied with several orientations and returns a vector field that represents the magnitude and angle of the highest scoring rotation at the given spatial location. To propagate information about the main orientation of the different features to each layer in the network, we propose an enriched orientation pooling, i.e. max and argmax operators over the orientation space, allowing to keep the dimensionality of the feature maps low and to propagate only useful information. We name this approach RotEqNet. We apply RotEqNet to three datasets: first, a rotation invariant classification problem, the MNIST-rot benchmark, in which we improve over the state-of-the-art results. Then, a neuron membrane segmentation benchmark, where we show that RotEqNet can be applied successfully to obtain equivariance to rotation with a simple fully convolutional architecture. Finally, we improve significantly the state-of-the-art on the problem of estimating cars' absolute orientation in aerial images, a problem where the output is required to be covariant with respect to the object's orientation. version:1
arxiv-1612-09328 | The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process | http://arxiv.org/abs/1612.09328 | id:1612.09328 author:Hongyuan Mei, Jason Eisner category:cs.LG stat.ML  published:2016-12-29 summary:Many events occur in the world. Some event types are stochastically excited or inhibited---in the sense of having their probabilities elevated or decreased---by patterns in the sequence of previous events. Discovering such patterns can help us predict which type of event will happen next and when. Learning such structure should benefit various applications, including medical prognosis, consumer behavior, and social media activity prediction. We propose to model streams of discrete events in continuous time, by constructing a neurally self-modulating multivariate point process. This generative model allows past events to influence the future in complex ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events. We evaluate our model on multiple datasets and show that it significantly outperforms other strong baselines. version:1
arxiv-1612-09322 | Deep Learning Logo Detection with Data Expansion by Synthesising Context | http://arxiv.org/abs/1612.09322 | id:1612.09322 author:Hang Su, Xiatian Zhu, Shaogang Gong category:cs.CV  published:2016-12-29 summary:Logo detection in unconstrained images is challenging, particularly when only very sparse labelled training images are accessible due to high labelling costs. In this work, we describe a model training image synthesising method capable of improving significantly logo detection performance when only a handful of (e.g., 10) labelled training images captured in realistic context are available, avoiding extensive manual labelling costs. Specifically, we design a novel algorithm for generating Synthetic Context Logo (SCL) training images to increase model robustness against unknown background clutters, resulting in superior logo detection performance. For benchmarking model performance, we introduce a new logo detection dataset TopLogo-10 collected from top 10 most popular clothing/wearable brandname logos captured in rich visual context. Extensive comparisons show the advantages of our proposed SCL model over the state-of-the-art alternatives for logo detection using two real-world logo benchmark datasets: FlickrLogo-32 and our new TopLogo-10. version:1
arxiv-1612-09297 | Communication-efficient Distributed Estimation and Inference for Transelliptical Graphical Models | http://arxiv.org/abs/1612.09297 | id:1612.09297 author:Pan Xu, Lu Tian, Quanquan Gu category:stat.ML  published:2016-12-29 summary:We propose communication-efficient distributed estimation and inference methods for the transelliptical graphical model, a semiparametric extension of the elliptical distribution in the high dimensional regime. In detail, the proposed method distributes the $d$-dimensional data of size $N$ generated from a transelliptical graphical model into $m$ worker machines, and estimates the latent precision matrix on each worker machine based on the data of size $n=N/m$. It then debiases the local estimators on the worker machines and send them back to the master machine. Finally, on the master machine, it aggregates the debiased local estimators by averaging and hard thresholding. We show that the aggregated estimator attains the same statistical rate as the centralized estimator based on all the data, provided that the number of machines satisfies $m \lesssim \min\{N\log d/d,\sqrt{N/(s^2\log d)}\}$, where $s$ is the maximum number of nonzero entries in each column of the latent precision matrix. It is worth noting that our algorithm and theory can be directly applied to Gaussian graphical models, Gaussian copula graphical models and elliptical graphical models, since they are all special cases of transelliptical graphical models. Thorough experiments on synthetic data back up our theory. version:1
arxiv-1612-09283 | Generalized Intersection Kernel | http://arxiv.org/abs/1612.09283 | id:1612.09283 author:Ping Li category:stat.ML cs.LG  published:2016-12-29 summary:Following the very recent line of work on the ``generalized min-max'' (GMM) kernel, this study proposes the ``generalized intersection'' (GInt) kernel and the related ``normalized generalized min-max'' (NGMM) kernel. In computer vision, the (histogram) intersection kernel has been popular, and the GInt kernel generalizes it to data which can have both negative and positive entries. Through an extensive empirical classification study on 40 datasets from the UCI repository, we are able to show that this (tuning-free) GInt kernel performs fairly well. The empirical results also demonstrate that the NGMM kernel typically outperforms the GInt kernel. Interestingly, the NGMM kernel has another interpretation --- it is the ``asymmetrically transformed'' version of the GInt kernel, based on the idea of ``asymmetric hashing''. Just like the GMM kernel, the NGMM kernel can be efficiently linearized through (e.g.,) generalized consistent weighted sampling (GCWS), as empirically validated in our study. Owing to the discrete nature of hashed values, it also provides a scheme for approximate near neighbor search. version:1
arxiv-1701-00705 | Using Big Data to Enhance the Bosch Production Line Performance: A Kaggle Challenge | http://arxiv.org/abs/1701.00705 | id:1701.00705 author:Ankita Mangal, Nishant Kumar category:cs.LG  published:2016-12-29 summary:This paper describes our approach to the Bosch production line performance challenge run by Kaggle.com. Maximizing the production yield is at the heart of the manufacturing industry. At the Bosch assembly line, data is recorded for products as they progress through each stage. Data science methods are applied to this huge data repository consisting records of tests and measurements made for each component along the assembly line to predict internal failures. We found that it is possible to train a model that predicts which parts are most likely to fail. Thus a smarter failure detection system can be built and the parts tagged likely to fail can be salvaged to decrease operating costs and increase the profit margins. version:1
arxiv-1612-09259 | Motifs in Temporal Networks | http://arxiv.org/abs/1612.09259 | id:1612.09259 author:Ashwin Paranjape, Austin R. Benson, Jure Leskovec category:cs.SI physics.soc-ph stat.ML  published:2016-12-29 summary:Networks are a fundamental tool for modeling complex systems in a variety of domains including social and communication networks as well as biology and neuroscience. Small subgraph patterns in networks, called network motifs, are crucial to understanding the structure and function of these systems. However, the role of network motifs in temporal networks, which contain many timestamped links between the nodes, is not yet well understood. Here we develop a notion of a temporal network motif as an elementary unit of temporal networks and provide a general methodology for counting such motifs. We define temporal network motifs as induced subgraphs on sequences of temporal edges, design fast algorithms for counting temporal motifs, and prove their runtime complexity. Our fast algorithms achieve up to 56.5x speedup compared to a baseline method. Furthermore, we use our algorithms to count temporal motifs in a variety of networks. Results show that networks from different domains have significantly different motif counts, whereas networks from the same domain tend to have similar motif counts. We also find that different motifs occur at different time scales, which provides further insights into structure and function of temporal networks. version:1
arxiv-1612-09213 | Verifying Heaps' law using Google Books Ngram data | http://arxiv.org/abs/1612.09213 | id:1612.09213 author:Vladimir V. Bochkarev, Eduard Yu. Lerner, Anna V. Shevlyakova category:cs.CL physics.soc-ph 91F20 J.5; I.2.7; G.3  published:2016-12-29 summary:This article is devoted to the verification of the empirical Heaps law in European languages using Google Books Ngram corpus data. The connection between word distribution frequency and expected dependence of individual word number on text size is analysed in terms of a simple probability model of text generation. It is shown that the Heaps exponent varies significantly within characteristic time intervals of 60-100 years. version:1
arxiv-1612-09205 | Deep neural heart rate variability analysis | http://arxiv.org/abs/1612.09205 | id:1612.09205 author:Tamas Madl category:cs.NE cs.AI cs.LG I.2  published:2016-12-29 summary:Despite of the pain and limited accuracy of blood tests for early recognition of cardiovascular disease, they dominate risk screening and triage. On the other hand, heart rate variability is non-invasive and cheap, but not considered accurate enough for clinical practice. Here, we tackle heart beat interval based classification with deep learning. We introduce an end to end differentiable hybrid architecture, consisting of a layer of biological neuron models of cardiac dynamics (modified FitzHugh Nagumo neurons) and several layers of a standard feed-forward neural network. The proposed model is evaluated on ECGs from 474 stable at-risk (coronary artery disease) patients, and 1172 chest pain patients of an emergency department. We show that it can significantly outperform models based on traditional heart rate variability predictors, as well as approaching or in some cases outperforming clinical blood tests, based only on 60 seconds of inter-beat intervals. version:1
arxiv-1612-09199 | Quantum Clustering and Gaussian Mixtures | http://arxiv.org/abs/1612.09199 | id:1612.09199 author:Mahajabin Rahman, Davi Geiger category:stat.ML cs.CV  published:2016-12-29 summary:The mixture of Gaussian distributions, a soft version of k-means , is considered a state-of-the-art clustering algorithm. It is widely used in computer vision for selecting classes, e.g., color, texture, and shapes. In this algorithm, each class is described by a Gaussian distribution, defined by its mean and covariance. The data is described by a weighted sum of these Gaussian distributions. We propose a new method, inspired by quantum interference in physics. Instead of modeling each class distribution directly, we model a class wave function such that its magnitude square is the class Gaussian distribution. We then mix the class wave functions to create the mixture wave function. The final mixture distribution is then the magnitude square of the mixture wave function. As a result, we observe the quantum class interference phenomena, not present in the Gaussian mixture model. We show that the quantum method outperforms the Gaussian mixture method in every aspect of the estimations. It provides more accurate estimations of all distribution parameters, with much less fluctuations, and it is also more robust to data deformations from the Gaussian assumptions. We illustrate our method for color segmentation as an example application. version:1
arxiv-1612-08425 | Unsupervised Learning for Computational Phenotyping | http://arxiv.org/abs/1612.08425 | id:1612.08425 author:Chris Hodapp category:stat.ML cs.LG  published:2016-12-26 summary:With large volumes of health care data comes the research area of computational phenotyping, making use of techniques such as machine learning to describe illnesses and other clinical concepts from the data itself. The "traditional" approach of using supervised learning relies on a domain expert, and has two main limitations: requiring skilled humans to supply correct labels limits its scalability and accuracy, and relying on existing clinical descriptions limits the sorts of patterns that can be found. For instance, it may fail to acknowledge that a disease treated as a single condition may really have several subtypes with different phenotypes, as seems to be the case with asthma and heart disease. Some recent papers cite successes instead using unsupervised learning. This shows great potential for finding patterns in Electronic Health Records that would otherwise be hidden and that can lead to greater understanding of conditions and treatments. This work implements a method derived strongly from Lasko et al., but implements it in Apache Spark and Python and generalizes it to laboratory time-series data in MIMIC-III. It is released as an open-source tool for exploration, analysis, and visualization, available at https://github.com/Hodapp87/mimic3_phenotyping version:2
arxiv-1612-09162 | High-dimensional Filtering using Nested Sequential Monte Carlo | http://arxiv.org/abs/1612.09162 | id:1612.09162 author:Christian A. Naesseth, Fredrik Lindsten, Thomas B. Schön category:stat.CO stat.ML  published:2016-12-29 summary:Sequential Monte Carlo (SMC) methods comprise one of the most successful approaches to approximate Bayesian filtering. However, SMC without good proposal distributions struggle in high dimensions. We propose nested sequential Monte Carlo (NSMC), a methodology that generalises the SMC framework by requiring only approximate, properly weighted, samples from the SMC proposal distribution, while still resulting in a correct SMC algorithm. This way we can exactly approximate the locally optimal proposal, and extend the class of models for which we can perform efficient inference using SMC. We show improved accuracy over other state-of-the-art methods on several spatio-temporal state space models. version:1
arxiv-1612-09161 | Learning Visual N-Grams from Web Data | http://arxiv.org/abs/1612.09161 | id:1612.09161 author:Ang Li, Allan Jabri, Armand Joulin, Laurens van der Maaten category:cs.CV  published:2016-12-29 summary:Real-world image recognition systems need to recognize tens of thousands of classes that constitute a plethora of visual concepts. The traditional approach of annotating thousands of images per class for training is infeasible in such a scenario, prompting the use of webly supervised data. This paper explores the training of image-recognition systems on large numbers of images and associated user comments. In particular, we develop visual n-gram models that can predict arbitrary phrases that are relevant to the content of an image. Our visual n-gram models are feed-forward convolutional networks trained using new loss functions that are inspired by n-gram models commonly used in language modeling. We demonstrate the merits of our models in phrase prediction, phrase-based image retrieval, relating images and captions, and zero-shot transfer. version:1
arxiv-1612-09158 | The interplay between system identification and machine learning | http://arxiv.org/abs/1612.09158 | id:1612.09158 author:Gianluigi Pillonetto category:cs.SY cs.LG stat.ML  published:2016-12-29 summary:Learning from examples is one of the key problems in science and engineering. It deals with function reconstruction from a finite set of direct and noisy samples. Regularization in reproducing kernel Hilbert spaces (RKHSs) is widely used to solve this task and includes powerful estimators such as regularization networks. Recent achievements include the proof of the statistical consistency of these kernel- based approaches. Parallel to this, many different system identification techniques have been developed but the interaction with machine learning does not appear so strong yet. One reason is that the RKHSs usually employed in machine learning do not embed the information available on dynamic systems, e.g. BIBO stability. In addition, in system identification the independent data assumptions routinely adopted in machine learning are never satisfied in practice. This paper provides new results which strengthen the connection between system identification and machine learning. Our starting point is the introduction of RKHSs of dynamic systems. They contain functionals over spaces defined by system inputs and allow to interpret system identification as learning from examples. In both linear and nonlinear settings, it is shown that this perspective permits to derive in a relatively simple way conditions on RKHS stability (i.e. the property of containing only BIBO stable systems or predictors), also facilitating the design of new kernels for system identification. Furthermore, we prove the convergence of the regularized estimator to the optimal predictor under conditions typical of dynamic systems. version:1
arxiv-1612-09147 | Linear Learning with Sparse Data | http://arxiv.org/abs/1612.09147 | id:1612.09147 author:Ofer Dekel category:cs.LG  published:2016-12-29 summary:Linear predictors are especially useful when the data is high-dimensional and sparse. One of the standard techniques used to train a linear predictor is the Averaged Stochastic Gradient Descent (ASGD) algorithm. We present an efficient implementation of ASGD that avoids dense vector operations. We also describe a translation invariant extension called Centered Averaged Stochastic Gradient Descent (CASGD). version:1
arxiv-1612-09134 | From Virtual to Real World Visual Perception using Domain Adaptation -- The DPM as Example | http://arxiv.org/abs/1612.09134 | id:1612.09134 author:Antonio M. Lopez, Jiaolong Xu, Jose L. Gomez, David Vazquez, German Ros category:cs.CV cs.AI  published:2016-12-29 summary:Supervised learning tends to produce more accurate classifiers than unsupervised learning in general. This implies that training data is preferred with annotations. When addressing visual perception challenges, such as localizing certain object classes within an image, the learning of the involved classifiers turns out to be a practical bottleneck. The reason is that, at least, we have to frame object examples with bounding boxes in thousands of images. A priori, the more complex the model is regarding its number of parameters, the more annotated examples are required. This annotation task is performed by human oracles, which ends up in inaccuracies and errors in the annotations (aka ground truth) since the task is inherently very cumbersome and sometimes ambiguous. As an alternative we have pioneered the use of virtual worlds for collecting such annotations automatically and with high precision. However, since the models learned with virtual data must operate in the real world, we still need to perform domain adaptation (DA). In this chapter we revisit the DA of a deformable part-based model (DPM) as an exemplifying case of virtual- to-real-world DA. As a use case, we address the challenge of vehicle detection for driver assistance, using different publicly available virtual-world data. While doing so, we investigate questions such as: how does the domain gap behave due to virtual-vs-real data with respect to dominant object appearance per domain, as well as the role of photo-realism in the virtual world. version:1
arxiv-1612-09122 | Modeling documents with Generative Adversarial Networks | http://arxiv.org/abs/1612.09122 | id:1612.09122 author:John Glover category:cs.LG  published:2016-12-29 summary:This paper describes a method for using Generative Adversarial Networks to learn distributed representations of natural language documents. We propose a model that is based on the recently proposed Energy-Based GAN, but instead uses a Denoising Autoencoder as the discriminator network. Document representations are extracted from the hidden layer of the discriminator and evaluated both quantitatively and qualitatively. version:1
arxiv-1612-09113 | Deep Semi-Supervised Learning with Linguistically Motivated Sequence Labeling Task Hierarchies | http://arxiv.org/abs/1612.09113 | id:1612.09113 author:Jonathan Godwin, Pontus Stenetorp, Sebastian Riedel category:cs.CL  published:2016-12-29 summary:In this paper we present a novel Neural Network algorithm for conducting semi-supervised learning for sequence labeling tasks arranged in a linguistically motivated hierarchy. This relationship is exploited to regularise the representations of supervised tasks by backpropagating the error of the unsupervised task through the supervised tasks. We introduce a neural network where lower layers are supervised by junior downstream tasks and the final layer task is an auxiliary unsupervised task. The architecture shows improvements of up to two percentage points F1 for Chunking compared to a plausible baseline. version:1
arxiv-1612-09106 | Sequence-to-point learning with neural networks for nonintrusive load monitoring | http://arxiv.org/abs/1612.09106 | id:1612.09106 author:Chaoyun Zhang, Mingjun Zhong, Zongzuo Wang, Nigel Goddard, Charles Sutton category:stat.AP cs.LG  published:2016-12-29 summary:Energy disaggregation (a.k.a nonintrusive load monitoring, NILM), a single-channel blind source separation problem, aims to decompose the mains which records the whole electricity consumption into appliance-wise readings. This problem is difficult because it is inherently unidentifiable. Recent approaches have shown that the identifiability problem could be reduced by introducing domain knowledge into the model. Deep neural networks have been shown to be promising to tackle this problem in literatures. However, it is not clear why and how the neural networks could work for this problem. In this paper, we propose sequence-to-point learning for NILM, where the input is a window of the mains and the output is a single point of the target appliance. We use convolutional neural networks to train the model. Interestingly, we systematically show that the convolutional neural networks can inherently learn the signatures of the target appliances, which are automatically added into the model to reduce the identifiability problem. We applied the proposed neural network approaches to a real-world household energy data, and show that the methods achieve the state-of-the-art performance. version:1
arxiv-1612-09076 | Selecting Bases in Spectral learning of Predictive State Representations via Model Entropy | http://arxiv.org/abs/1612.09076 | id:1612.09076 author:Yunlong Liu, Hexing Zhu category:cs.LG stat.ML  published:2016-12-29 summary:Predictive State Representations (PSRs) are powerful techniques for modelling dynamical systems, which represent a state as a vector of predictions about future observable events (tests). In PSRs, one of the fundamental problems is the learning of the PSR model of the underlying system. Recently, spectral methods have been successfully used to address this issue by treating the learning problem as the task of computing an singular value decomposition (SVD) over a submatrix of a special type of matrix called the Hankel matrix. Under the assumptions that the rows and columns of the submatrix of the Hankel Matrix are sufficient~(which usually means a very large number of rows and columns, and almost fails in practice) and the entries of the matrix can be estimated accurately, it has been proven that the spectral approach for learning PSRs is statistically consistent and the learned parameters can converge to the true parameters. However, in practice, due to the limit of the computation ability, only a finite set of rows or columns can be chosen to be used for the spectral learning. While different sets of columns usually lead to variant accuracy of the learned model, in this paper, we propose an approach for selecting the set of columns, namely basis selection, by adopting a concept of model entropy to measure the accuracy of the learned model. Experimental results are shown to demonstrate the effectiveness of the proposed approach. version:1
arxiv-1612-09057 | Deep Learning and Hierarchal Generative Models | http://arxiv.org/abs/1612.09057 | id:1612.09057 author:Elchanan Mossel category:cs.LG  published:2016-12-29 summary:In this paper we propose a new prism for studying deep learning motivated by connections between deep learning and evolution. Our main contributions are: 1, We introduce of a sequence of increasingly complex hierarchal generative models which interpolate between standard Markov models on trees (phylogenetic models) and deep learning models. 2. Formal definitions of classes of algorithms that are not deep. 3. Rigorous proofs showing that such classes are information theoretically much weaker than optimal "deep" learning algorithms. In our models, deep learning is performed efficiently and proven to classify correctly with high probability. All of the models and results are in the semi-supervised setting. Many open problems and future directions are presented. version:1
arxiv-1612-09529 | Linking the Neural Machine Translation and the Prediction of Organic Chemistry Reactions | http://arxiv.org/abs/1612.09529 | id:1612.09529 author:Juno Nam, Jurae Kim category:cs.LG  published:2016-12-29 summary:Finding the main product of a chemical reaction is one of the important problems of organic chemistry. This paper describes a method of applying a neural machine translation model to the prediction of organic chemical reactions. In order to translate 'reactants and reagents' to 'products', a gated recurrent unit based sequence-to-sequence model and a parser to generate input tokens for model from reaction SMILES strings were built. Training sets are composed of reactions from the patent databases, and reactions manually generated applying the elementary reactions in an organic chemistry textbook of Wade. The trained models were tested by examples and problems in the textbook. The prediction process does not need manual encoding of rules (e.g., SMARTS transformations) to predict products, hence it only needs sufficient training reaction sets to learn new types of reactions. version:1
arxiv-1612-09034 | Geometric descent method for convex composite minimization | http://arxiv.org/abs/1612.09034 | id:1612.09034 author:Shixiang Chen, Shiqian Ma category:math.OC cs.LG stat.ML  published:2016-12-29 summary:In this paper, we extend the geometric descent method recently proposed by Bubeck, Lee and Singh to solving nonsmooth and strongly convex composite problems. We prove that the resulting algorithm, GeoPG, converges with a linear rate $(1-1/\sqrt{\kappa})$, thus achieves the optimal rate among first-order methods, where $\kappa$ is the condition number of the problem. Numerical results on linear regression and logistic regression with elastic net regularization show that GeoPG compares favorably with Nesterov's accelerated proximal gradient method. version:1
arxiv-1612-09022 | A Basic Recurrent Neural Network Model | http://arxiv.org/abs/1612.09022 | id:1612.09022 author:Fathi M. Salem category:cs.NE stat.ML  published:2016-12-29 summary:We present a model of a basic recurrent neural network (or bRNN) that includes a separate linear term with a slightly "stable" fixed matrix to guarantee bounded solutions and fast dynamic response. We formulate a state space viewpoint and adapt the constrained optimization Lagrange Multiplier (CLM) technique and the vector Calculus of Variations (CoV) to derive the (stochastic) gradient descent. In this process, one avoids the commonly used re-application of the circular chain-rule and identifies the error back-propagation with the co-state backward dynamic equations. We assert that this bRNN can successfully perform regression tracking of time-series. Moreover, the "vanishing and exploding" gradients are explicitly quantified and explained through the co-state dynamics and the update laws. The adapted CoV framework, in addition, can correctly and principally integrate new loss functions in the network on any variable and for varied goals, e.g., for supervised learning on the outputs and unsupervised learning on the internal (hidden) states. version:1
arxiv-1612-09007 | A Deep Learning Approach To Multiple Kernel Fusion | http://arxiv.org/abs/1612.09007 | id:1612.09007 author:Huan Song, Jayaraman J. Thiagarajan, Prasanna Sattigeri, Karthikeyan Natesan Ramamurthy, Andreas Spanias category:stat.ML cs.LG  published:2016-12-28 summary:Kernel fusion is a popular and effective approach for combining multiple features that characterize different aspects of data. Traditional approaches for Multiple Kernel Learning (MKL) attempt to learn the parameters for combining the kernels through sophisticated optimization procedures. In this paper, we propose an alternative approach that creates dense embeddings for data using the kernel similarities and adopts a deep neural network architecture for fusing the embeddings. In order to improve the effectiveness of this network, we introduce the kernel dropout regularization strategy coupled with the use of an expanded set of composition kernels. Experiment results on a real-world activity recognition dataset show that the proposed architecture is effective in fusing kernels and achieves state-of-the-art performance. version:1
arxiv-1612-08994 | Here's My Point: Argumentation Mining with Pointer Networks | http://arxiv.org/abs/1612.08994 | id:1612.08994 author:Peter Potash, Alexey Romanov, Anna Rumshisky category:cs.CL  published:2016-12-28 summary:One of the major goals in automated argumentation mining is to uncover the argument structure present in argumentative text. In order to determine this structure, one must understand how different individual components of the overall argument are linked. General consensus in this field dictates that the argument components form a hierarchy of persuasion, which manifests itself in a tree structure. This work provides the first neural network-based approach to argumentation mining, focusing on the two tasks of extracting links between argument components, and classifying types of argument components. In order to solve this problem, we propose to use a joint model that is based on a Pointer Network architecture. A Pointer Network is appealing for this task for the following reasons: 1) It takes into account the sequential nature of argument components; 2) By construction, it enforces certain properties of the tree structure present in argument relations; 3) The hidden representations can be applied to auxiliary tasks. In order to extend the contribution of the original Pointer Network model, we construct a joint model that simultaneously attempts to learn the type of argument component, as well as continuing to predict links between argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, achieving far superior performance than a regular Pointer Network model. Our results show that optimizing for both tasks, and adding a fully-connected layer prior to recurrent neural network input, is crucial for high performance. version:1
arxiv-1612-08989 | Shamela: A Large-Scale Historical Arabic Corpus | http://arxiv.org/abs/1612.08989 | id:1612.08989 author:Yonatan Belinkov, Alexander Magidow, Maxim Romanov, Avi Shmidman, Moshe Koppel category:cs.CL I.2.7  published:2016-12-28 summary:Arabic is a widely-spoken language with a rich and long history spanning more than fourteen centuries. Yet existing Arabic corpora largely focus on the modern period or lack sufficient diachronic information. We develop a large-scale, historical corpus of Arabic of about 1 billion words from diverse periods of time. We clean this corpus, process it with a morphological analyzer, and enhance it by detecting parallel passages and automatically dating undated texts. We demonstrate its utility with selected case-studies in which we show its application to the digital humanities. version:1
arxiv-1612-08974 | ggRandomForests: Exploring Random Forest Survival | http://arxiv.org/abs/1612.08974 | id:1612.08974 author:John Ehrlinger category:stat.CO stat.ML  published:2016-12-28 summary:Random forest (Leo Breiman 2001a) (RF) is a non-parametric statistical method requiring no distributional assumptions on covariate relation to the response. RF is a robust, nonlinear technique that optimizes predictive accuracy by fitting an ensemble of trees to stabilize model estimates. Random survival forests (RSF) (Ishwaran and Kogalur 2007; Ishwaran et al. 2008) are an extension of Breimans RF techniques allowing efficient nonparametric analysis of time to event data. The randomForestSRC package (Ishwaran and Kogalur 2014) is a unified treatment of Breimans random forest for survival, regression and classification problems. Predictive accuracy makes RF an attractive alternative to parametric models, though complexity and interpretability of the forest hinder wider application of the method. We introduce the ggRandomForests package, tools for visually understand random forest models grown in R (R Core Team 2014) with the randomForestSRC package. The ggRandomForests package is structured to extract intermediate data objects from randomForestSRC objects and generate figures using the ggplot2 (Wickham 2009) graphics package. This document is structured as a tutorial for building random forest for survival with the randomForestSRC package and using the ggRandomForests package for investigating how the forest is constructed. We analyse the Primary Biliary Cirrhosis of the liver data from a clinical trial at the Mayo Clinic (Fleming and Harrington 1991). Our aim is to demonstrate the strength of using Random Forest methods for both prediction and information retrieval, specifically in time to event data settings. version:1
arxiv-1612-08967 | Efficient iterative policy optimization | http://arxiv.org/abs/1612.08967 | id:1612.08967 author:Nicolas Le Roux category:cs.AI cs.LG cs.RO  published:2016-12-28 summary:We tackle the issue of finding a good policy when the number of policy updates is limited. This is done by approximating the expected policy reward as a sequence of concave lower bounds which can be efficiently maximized, drastically reducing the number of policy updates required to achieve good performance. We also extend existing methods to negative rewards, enabling the use of control variates. version:1
arxiv-1612-08936 | Partial Membership Latent Dirichlet Allocation | http://arxiv.org/abs/1612.08936 | id:1612.08936 author:Chao Chen, Alina Zare, Huy Trinh, Gbeng Omotara, J. Tory Cobb, Timotius Lagaunne category:cs.CV stat.ML  published:2016-12-28 summary:Topic models (e.g., pLSA, LDA, sLDA) have been widely used for segmenting imagery. However, these models are confined to crisp segmentation, forcing a visual word (i.e., an image patch) to belong to one and only one topic. Yet, there are many images in which some regions cannot be assigned a crisp categorical label (e.g., transition regions between a foggy sky and the ground or between sand and water at a beach). In these cases, a visual word is best represented with partial memberships across multiple topics. To address this, we present a partial membership latent Dirichlet allocation (PM-LDA) model and an associated parameter estimation algorithm. This model can be useful for imagery where a visual word may be a mixture of multiple topics. Experimental results on visual and sonar imagery show that PM-LDA can produce both crisp and soft semantic image segmentations; a capability previous topic modeling methods do not have. version:1
arxiv-1612-08220 | Understanding Neural Networks through Representation Erasure | http://arxiv.org/abs/1612.08220 | id:1612.08220 author:Jiwei Li, Will Monroe, Dan Jurafsky category:cs.CL  published:2016-12-24 summary:While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models. version:2
arxiv-1612-08932 | Optimal bandwidth estimation for a fast manifold learning algorithm to detect circular structure in high-dimensional data | http://arxiv.org/abs/1612.08932 | id:1612.08932 author:Susovan Pal, Praneeth Vepakomma category:stat.ML  published:2016-12-28 summary:We provide a way to infer about existence of topological circularity in high-dimensional data sets in $\mathbb{R}^d$ from its projection in $\mathbb{R}^2$ obtained through a fast manifold learning map as a function of the high-dimensional dataset $\mathbb{X}$ and a particular choice of a positive real $\sigma$ known as bandwidth parameter. At the same time we also provide a way to estimate the optimal bandwidth for fast manifold learning in this setting through minimization of these functions of bandwidth. We also provide limit theorems to characterize the behavior of our proposed functions of bandwidth. version:1
arxiv-1612-08927 | Fast color transfer from multiple images | http://arxiv.org/abs/1612.08927 | id:1612.08927 author:Asad Khan, Luo Jiang, Wei Li, Ligang Liu category:cs.CV cs.GR  published:2016-12-28 summary:Color transfer between images uses the statistics information of image effectively. We present a novel approach of local color transfer between images based on the simple statistics and locally linear embedding. A sketching interface is proposed for quickly and easily specifying the color correspondences between target and source image. The user can specify the correspondences of local region using scribes, which more accurately transfers the target color to the source image while smoothly preserving the boundaries, and exhibits more natural output results. Our algorithm is not restricted to one-to-one image color transfer and can make use of more than one target images to transfer the color in different regions in the source image. Moreover, our algorithm does not require to choose the same color style and image size between source and target images. We propose the sub-sampling to reduce the computational load. Comparing with other approaches, our algorithm is much better in color blending in the input data. Our approach preserves the other color details in the source image. Various experimental results show that our approach specifies the correspondences of local color region in source and target images. And it expresses the intention of users and generates more actual and natural results of visual effect. version:1
arxiv-1612-08915 | Bayesian Optimization with Shape Constraints | http://arxiv.org/abs/1612.08915 | id:1612.08915 author:Michael Jauch, Víctor Peña category:stat.ML  published:2016-12-28 summary:In typical applications of Bayesian optimization, minimal assumptions are made about the objective function being optimized. This is true even when researchers have prior information about the shape of the function with respect to one or more argument. We make the case that shape constraints are often appropriate in at least two important application areas of Bayesian optimization: (1) hyperparameter tuning of machine learning algorithms and (2) decision analysis with utility functions. We describe a methodology for incorporating a variety of shape constraints within the usual Bayesian optimization framework and present positive results from simple applications which suggest that Bayesian optimization with shape constraints is a promising topic for further research. version:1
arxiv-1612-08894 | Unsupervised domain adaptation in brain lesion segmentation with adversarial networks | http://arxiv.org/abs/1612.08894 | id:1612.08894 author:Konstantinos Kamnitsas, Christian Baumgartner, Christian Ledig, Virginia F. J. Newcombe, Joanna P. Simpson, Andrew D. Kane, David K. Menon, Aditya Nori, Antonio Criminisi, Daniel Rueckert, Ben Glocker category:cs.CV  published:2016-12-28 summary:Significant advances have been made towards building accurate automatic segmentation systems for a variety of biomedical applications using machine learning. However, the performance of these systems often degrades when they are applied on new data that differ from the training data, for example, due to variations in imaging protocols. Manually annotating new data for each test domain is not a feasible solution. In this work we investigate unsupervised domain adaptation using adversarial neural networks to train a segmentation method which is more invariant to differences in the input data, and which does not require any annotations on the test domain. Specifically, we learn domain-invariant features by learning to counter an adversarial network, which attempts to classify the domain of the input data by observing the activations of the segmentation network. Furthermore, we propose a multi-connected domain discriminator for improved adversarial training. Our system is evaluated using two MR databases of subjects with traumatic brain injuries, acquired using different scanners and imaging protocols. Using our unsupervised approach, we obtain segmentation accuracies which are close to the upper bound of supervised domain adaptation. version:1
arxiv-1612-08879 | Deep Unsupervised Representation Learning for Remote Sensing Images | http://arxiv.org/abs/1612.08879 | id:1612.08879 author:DaoYu Lin category:cs.CV  published:2016-12-28 summary:Scene classification plays a key role in interpreting the remotely sensed high-resolution images. With the development of deep learning, supervised learning in classification of Remote Sensing with convolutional networks (CNNs) has been frequently adopted. However, researchers paid less attention to unsupervised learning in remote sensing with CNNs. In order to filling the gap, this paper proposes a set of CNNs called \textbf{M}ultiple l\textbf{A}ye\textbf{R} fea\textbf{T}ure m\textbf{A}tching(MARTA) generative adversarial networks (GANs) to learn representation using only unlabeled data. There will be two models of MARTA GANs involved: (1) a generative model $G$ that captures the data distribution and provides more training data; (2) a discriminative model $D$ that estimates the possibility that a sample came from the training data rather than $G$ and in this way a well-formed representation of dataset can be learned. Therefore, MARTA GANs obtain the state-of-the-art results which outperform the results got from UC-Merced Land-use dataset and Brazilian Coffee Scenes dataset. version:1
arxiv-1612-08875 | The Pessimistic Limits of Margin-based Losses in Semi-supervised Learning | http://arxiv.org/abs/1612.08875 | id:1612.08875 author:Jesse H. Krijthe, Marco Loog category:stat.ML cs.LG  published:2016-12-28 summary:We show that for linear classifiers defined by convex margin-based surrogate losses that are monotonically decreasing, it is impossible to construct any semi-supervised approach that is able to guarantee an improvement over the supervised classifier measured by this surrogate loss. For non-monotonically decreasing loss functions, we demonstrate safe improvements are possible. version:1
arxiv-1612-08871 | Semantic Video Segmentation by Gated Recurrent Flow Propagation | http://arxiv.org/abs/1612.08871 | id:1612.08871 author:David Nilsson, Cristian Sminchisescu category:cs.CV  published:2016-12-28 summary:Semantic video segmentation is challenging due to the sheer amount of data that needs to be processed and labeled in order to construct accurate models. In this paper we present a deep, end-to-end trainable methodology to video segmentation that is capable of leveraging information present in unlabeled data in order to improve semantic estimates. Our model combines a convolutional architecture and a spatial transformer recurrent layer that are able to temporally propagate labeling information by means of optical flow, adaptively gated based on its locally estimated uncertainty. The flow, the recogition and the gated propagation modules can be trained jointly, end-to-end. The gated recurrent flow propagation component of our model can be plugged-in any static semantic segmentation architecture and turn it into a weakly supervised video processing one. Our extensive experiments in the challenging CityScapes dataset indicate that the resulting model can leverage unlabeled temporal frames next to a labeled one in order to improve both the video segmentation accuracy and the consistency of its temporal labeling, at no additional annotation cost. version:1
arxiv-1612-08825 | Accelerated Convolutions for Efficient Multi-Scale Time to Contact Computation in Julia | http://arxiv.org/abs/1612.08825 | id:1612.08825 author:Alexander Amini, Berthold Horn, Alan Edelman category:cs.CV cs.AI cs.PF  published:2016-12-28 summary:Convolutions have long been regarded as fundamental to applied mathematics, physics and engineering. Their mathematical elegance allows for common tasks such as numerical differentiation to be computed efficiently on large data sets. Efficient computation of convolutions is critical to artificial intelligence in real-time applications, like machine vision, where convolutions must be continuously and efficiently computed on tens to hundreds of kilobytes per second. In this paper, we explore how convolutions are used in fundamental machine vision applications. We present an accelerated n-dimensional convolution package in the high performance computing language, Julia, and demonstrate its efficacy in solving the time to contact problem for machine vision. Results are measured against synthetically generated videos and quantitatively assessed according to their mean squared error from the ground truth. We achieve over an order of magnitude decrease in compute time and allocated memory for comparable machine vision applications. All code is packaged and integrated into the official Julia Package Manager to be used in various other scenarios. version:1
arxiv-1612-08820 | Multivariate mixture model for myocardium segmentation combining multi-source images | http://arxiv.org/abs/1612.08820 | id:1612.08820 author:Xiahai Zhuang category:cs.CV  published:2016-12-28 summary:This paper proposes a method for simultaneous segmentation of multi-source images, using the multivariate mixture model (MvMM) and maximum of log-likelihood (LL) framework. The segmentation is a procedure of texture classification, and the MvMM is used to model the joint intensity distribution of the images. Specifically, the method is applied to the myocardial segmentation combining the complementary texture information from multi-sequence (MS) cardiac magnetic resonance (CMR) images. Furthermore, there exist inter-image mis-registration and intra-image misalignment of slices in the MS CMR images. Hence, the MvMM is formulated with transformations, which are embedded into the LL framework and optimized simultaneously with the segmentation parameters. The proposed method is able to correct the inter- and intra-image misalignment by registering each slice of the MS CMR to a virtual common space, as well as to delineate the indistinguishable boundaries of myocardium consisting of pathologies. Results have shown statistically significant improvement in the segmentation performance of the proposed method with respect to the conventional approaches which can solely segment each image separately. The proposed method has also demonstrated better robustness in the incongruent data, where some images may not fully cover the region of interest and the full coverage can only be reconstructed combining the images from multiple sources. version:1
arxiv-1612-08813 | Optimization of Test Case Generation using Genetic Algorithm (GA) | http://arxiv.org/abs/1612.08813 | id:1612.08813 author:Ahmed Mateen, Marriam Nazir, Salman Afsar Awan category:cs.SE cs.NE  published:2016-12-28 summary:Testing provides means pertaining to assuring software performance. The total aim of software industry is actually to make a certain start associated with high quality software for the end user. However, associated with software testing has quite a few underlying concerns, which are very important and need to pay attention on these issues. These issues are effectively generating, prioritization of test cases, etc. These issues can be overcome by paying attention and focus. Solitary of the greatest Problems in the software testing area is usually how to acquire a great proper set associated with cases to confirm software. Some other strategies and also methodologies are proposed pertaining to shipping care of most of these issues. Genetic Algorithm (GA) belongs to evolutionary algorithms. Evolutionary algorithms have a significant role in the automatic test generation and many researchers are focusing on it. In this study explored software testing related issues by using the GA approach. In addition to right after applying some analysis, better solution produced, that is feasible and reliable. The particular research presents the implementation of GAs because of its generation of optimized test cases. Along these lines, this paper gives proficient system for the optimization of test case generation using genetic algorithm. version:1
arxiv-1612-08810 | The Predictron: End-To-End Learning and Planning | http://arxiv.org/abs/1612.08810 | id:1612.08810 author:David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, Thomas Degris category:cs.LG cs.AI cs.NE  published:2016-12-28 summary:One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple "imagined" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures. version:1
arxiv-1612-08796 | Symbolic Representation and Classification of Logos | http://arxiv.org/abs/1612.08796 | id:1612.08796 author:D. S. Guru, N. Vinay Kumar category:cs.CV 68U10  published:2016-12-28 summary:In this paper, a model for classification of logos based on symbolic representation of features is presented. The proposed model makes use of global features of logo images such as color, texture, and shape features for classification. The logo images are broadly classified into three different classes, viz., logo image containing only text, an image with only symbol, and an image with both text and a symbol. In each class, the similar looking logo images are clustered using K-means clustering algorithm. The intra-cluster variations present in each cluster corresponding to each class are then preserved using symbolic interval data. Thus referenced logo images are represented in the form of interval data. A sample logo image is then classified using suitable symbolic classifier. For experimentation purpose, relatively large amount of color logo images is created consisting of 5044 logo images. The classification results are validated with the help of accuracy, precision, recall, F-measure, and time. To check the efficacy of the proposed model, the comparative analyses are given against the other models. The results show that the proposed model outperforms the other models with respect to time and F-measure. version:1
arxiv-1612-08795 | Provable learning of Noisy-or Networks | http://arxiv.org/abs/1612.08795 | id:1612.08795 author:Sanjeev Arora, Rong Ge, Tengyu Ma, Andrej Risteski category:cs.LG cs.DS stat.ML  published:2016-12-28 summary:Many machine learning applications use latent variable models to explain structure in data, whereby visible variables (= coordinates of the given datapoint) are explained as a probabilistic function of some hidden variables. Finding parameters with the maximum likelihood is NP-hard even in very simple settings. In recent years, provably efficient algorithms were nevertheless developed for models with linear structures: topic models, mixture models, hidden markov models, etc. These algorithms use matrix or tensor decomposition, and make some reasonable assumptions about the parameters of the underlying model. But matrix or tensor decomposition seems of little use when the latent variable model has nonlinearities. The current paper shows how to make progress: tensor decomposition is applied for learning the single-layer {\em noisy or} network, which is a textbook example of a Bayes net, and used for example in the classic QMR-DT software for diagnosing which disease(s) a patient may have by observing the symptoms he/she exhibits. The technical novelty here, which should be useful in other settings in future, is analysis of tensor decomposition in presence of systematic error (i.e., where the noise/error is correlated with the signal, and doesn't decrease as number of samples goes to infinity). This requires rethinking all steps of tensor decomposition methods from the ground up. For simplicity our analysis is stated assuming that the network parameters were chosen from a probability distribution but the method seems more generally applicable. version:1
arxiv-1612-08792 | A novel Gaussian mixture model for superpixel segmentation | http://arxiv.org/abs/1612.08792 | id:1612.08792 author:Zhihua Ban, Jianguo Liu, Li Cao category:cs.CV  published:2016-12-28 summary:Superpixel segmentation is used to partition an image into perceptually coherence atomic regions. As a preprocessing step of computer vision applications, it can enormously reduce the number of entries of subsequent algorithms. With each superpixel associated with a Gaussian distribution, we assume that a pixel is generated by first randomly choosing one of the superpixels, and then the pixel is drawn from the corresponding Gaussian density. Unlike most applications of Gaussian mixture model in clustering, data points in our model are assumed to be non-identically distributed. Given an image, a log-likelihood function is constructed for maximizing. Based on a solution derived from the expectation-maximization method, a well designed algorithm is proposed. Our method is of linear complexity with respect to the number of pixels, and it can be implemented using parallel techniques. To the best of our knowledge, our algorithm outperforms the state-of-the-art in accuracy and presents a competitive performance in computational efficiency. version:1
arxiv-1612-08789 | Automatic composition and optimisation of multicomponent predictive systems | http://arxiv.org/abs/1612.08789 | id:1612.08789 author:Manuel Martin Salvador, Marcin Budka, Bogdan Gabrys category:cs.LG  published:2016-12-28 summary:Composition and parametrisation of multicomponent predictive systems (MCPSs) consisting of chains of data transformation steps is a challenging task. This paper is concerned with theoretical considerations and extensive experimental analysis for automating the task of building such predictive systems. In the theoretical part of the paper, we first propose to adopt the Well-handled and Acyclic Workflow (WA-WF) Petri net as a formal representation of MCPSs. We then define the optimisation problem in which the search space consists of suitably parametrised directed acyclic graphs (i.e. WA-WFs) forming the sought MCPS solutions. In the experimental analysis we focus on examining the impact of considerably extending the search space resulting from incorporating multiple sequential data cleaning and preprocessing steps in the process of composing optimised MCPSs, and the quality of the solutions found. In a range of extensive experiments three different optimisation strategies are used to automatically compose MCPSs for 21 publicly available datasets and 7 datasets from real chemical processes. The diversity of the composed MCPSs found is an indication that fully and automatically exploiting different combinations of data cleaning and preprocessing techniques is possible and highly beneficial for different predictive models. Our findings can have a major impact on development of high quality predictive models as well as their maintenance and scalability aspects needed in modern applications and deployment scenarios. version:1
arxiv-1612-07976 | DeMIAN: Deep Modality Invariant Adversarial Network | http://arxiv.org/abs/1612.07976 | id:1612.07976 author:Kuniaki Saito, Yusuke Mukuta, Yoshitaka Ushiku, Tatsuya Harada category:cs.LG stat.ML  published:2016-12-23 summary:Obtaining common representations from different modalities is important in that they are interchangeable with each other in a classification problem. For example, we can train a classifier on image features in the common representations and apply it to the testing of the text features in the representations. Existing multi-modal representation learning methods mainly aim to extract rich information from paired samples and train a classifier by the corresponding labels; however, collecting paired samples and their labels simultaneously involves high labor costs. Addressing paired modal samples without their labels and single modal data with their labels independently is much easier than addressing labeled multi-modal data. To obtain the common representations under such a situation, we propose to make the distributions over different modalities similar in the learned representations, namely modality-invariant representations. In particular, we propose a novel algorithm for modality-invariant representation learning, named Deep Modality Invariant Adversarial Network (DeMIAN), which utilizes the idea of Domain Adaptation (DA). Using the modality-invariant representations learned by DeMIAN, we achieved better classification accuracy than with the state-of-the-art methods, especially for some benchmark datasets of zero-shot learning. version:2
arxiv-1612-08780 | An FFT-based Synchronization Approach to Recognize Human Behaviors using STN-LFP Signal | http://arxiv.org/abs/1612.08780 | id:1612.08780 author:Hosein M. Golshan, Adam O. Hebb, Sara J. Hanrahan, Joshua Nedrud, Mohammad H. Mahoor category:cs.CV q-bio.NC  published:2016-12-28 summary:Classification of human behavior is key to developing closed-loop Deep Brain Stimulation (DBS) systems, which may be able to decrease the power consumption and side effects of the existing systems. Recent studies have shown that the Local Field Potential (LFP) signals from both Subthalamic Nuclei (STN) of the brain can be used to recognize human behavior. Since the DBS leads implanted in each STN can collect three bipolar signals, the selection of a suitable pair of LFPs that achieves optimal recognition performance is still an open problem to address. Considering the presence of synchronized aggregate activity in the basal ganglia, this paper presents an FFT-based synchronization approach to automatically select a relevant pair of LFPs and use the pair together with an SVM-based MKL classifier for behavior recognition purposes. Our experiments on five subjects show the superiority of the proposed approach compared to other methods used for behavior classification. version:1
arxiv-1612-09268 | The ontogeny of discourse structure mimics the development of literature | http://arxiv.org/abs/1612.09268 | id:1612.09268 author:Natalia Bezerra Mota, Sylvia Pinheiro, Mariano Sigman, Diego Fernandez Slezak, Guillermo Cecchi, Mauro Copelli, Sidarta Ribeiro category:q-bio.NC cs.CL physics.soc-ph  published:2016-12-27 summary:Discourse varies with age, education, psychiatric state and historical epoch, but the ontogenetic and cultural dynamics of discourse structure remain to be quantitatively characterized. To this end we investigated word graphs obtained from verbal reports of 200 subjects ages 2-58, and 676 literary texts spanning ~5,000 years. In healthy subjects, lexical diversity, graph size, and long-range recurrence departed from initial near-random levels through a monotonic asymptotic increase across ages, while short-range recurrence showed a corresponding decrease. These changes were explained by education and suggest a hierarchical development of discourse structure: short-range recurrence and lexical diversity stabilize after elementary school, but graph size and long-range recurrence only stabilize after high school. This gradual maturation was blurred in psychotic subjects, who maintained in adulthood a near-random structure. In literature, monotonic asymptotic changes over time were remarkable: While lexical diversity, long-range recurrence and graph size increased away from near-randomness, short-range recurrence declined, from above to below random levels. Bronze Age texts are structurally similar to childish or psychotic discourses, but subsequent texts converge abruptly to the healthy adult pattern around the onset of the Axial Age (800-200 BC), a period of pivotal cultural change. Thus, individually as well as historically, discourse maturation increases the range of word recurrence away from randomness. version:1
arxiv-1612-08712 | Semantic Perceptual Image Compression using Deep Convolution Networks | http://arxiv.org/abs/1612.08712 | id:1612.08712 author:Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, James Storer category:cs.MM cs.CV  published:2016-12-27 summary:It has long been considered a significant problem to improve the visual quality of lossy image and video compression. Recent advances in computing power together with the availability of large training data sets has increased interest in the application of deep learning cnns to address image recognition and image processing tasks. Here, we present a powerful cnn tailored to the specific task of semantic image understanding to achieve higher visual quality in lossy compression. A modest increase in complexity is incorporated to the encoder which allows a standard, off-the-shelf jpeg decoder to be used. While jpeg encoding may be optimized for generic images, the process is ultimately unaware of the specific content of the image to be compressed. Our technique makes jpeg content-aware by designing and training a model to identify multiple semantic regions in a given image. Unlike object detection techniques, our model does not require labeling of object positions and is able to identify objects in a single pass. We present a new cnn architecture directed specifically to image compression, which generates a map that highlights semantically-salient regions so that they can be encoded at higher quality as compared to background regions. By adding a complete set of features for every class, and then taking a threshold over the sum of all feature activations, we generate a map that highlights semantically-salient regions so that they can be encoded at a better quality compared to background regions. Experiments are presented on the Kodak PhotoCD dataset and the MIT Saliency Benchmark dataset, in which our algorithm achieves higher visual quality for the same compressed size. version:1
arxiv-1612-08669 | A Hybrid Both Filter and Wrapper Feature Selection Method for Microarray Classification | http://arxiv.org/abs/1612.08669 | id:1612.08669 author:Li-Yeh Chuang, Chao-Hsuan Ke, Cheng-Hong Yang category:cs.LG  published:2016-12-27 summary:Gene expression data is widely used in disease analysis and cancer diagnosis. However, since gene expression data could contain thousands of genes simultaneously, successful microarray classification is rather difficult. Feature selection is an important pre-treatment for any classification process. Selecting a useful gene subset as a classifier not only decreases the computational time and cost, but also increases classification accuracy. In this study, we applied the information gain method as a filter approach, and an improved binary particle swarm optimization as a wrapper approach to implement feature selection; selected gene subsets were used to evaluate the performance of classification. Experimental results show that by employing the proposed method fewer gene subsets needed to be selected and better classification accuracy could be obtained. version:1
arxiv-1612-08650 | Reproducible Pattern Recognition Research: The Case of Optimistic SSL | http://arxiv.org/abs/1612.08650 | id:1612.08650 author:Jesse H. Krijthe, Marco Loog category:stat.ML cs.LG  published:2016-12-27 summary:In this paper, we discuss the approaches we took and trade-offs involved in making a paper on a conceptual topic in pattern recognition research fully reproducible. We discuss our definition of reproducibility, the tools used, how the analysis was set up, show some examples of alternative analyses the code enables and discuss our views on reproducibility. version:1
arxiv-1612-08642 | Bayesian Nonparametric Models for Synchronous Brain-Computer Interfaces | http://arxiv.org/abs/1612.08642 | id:1612.08642 author:Jaime Fernando Delgado Saa, Mujdat Cetin category:cs.CV q-bio.NC  published:2016-12-27 summary:A brain-computer interface (BCI) is a system that aims for establishing a non-muscular communication path for subjects who had suffer from a neurodegenerative disease. Many BCI systems make use of the phenomena of event-related synchronization and de-synchronization of brain waves as a main feature for classification of different cognitive tasks. However, the temporal dynamics of the electroencephalographic (EEG) signals contain additional information that can be incorporated into the inference engine in order to improve the performance of the BCIs. This information about the dynamics of the signals have been exploited previously in BCIs by means of generative and discriminative methods. In particular, hidden Markov models (HMMs) have been used in previous works. These methods have the disadvantage that the model parameters such as the number of hidden states and the number of Gaussian mixtures need to be fix "a priori". In this work, we propose a Bayesian nonparametric model for brain signal classification that does not require "a priori" selection of the number of hidden states and the number of Gaussian mixtures of a HMM. The results show that the proposed model outperform other methods based on HMM as well as the winner algorithm of the BCI competition IV. version:1
arxiv-1612-08633 | A Sparse Nonlinear Classifier Design Using AUC Optimization | http://arxiv.org/abs/1612.08633 | id:1612.08633 author:Vishal Kakkar, Shirish K. Shevade, S Sundararajan, Dinesh Garg category:cs.AI cs.LG stat.ML  published:2016-12-27 summary:AUC (Area under the ROC curve) is an important performance measure for applications where the data is highly imbalanced. Learning to maximize AUC performance is thus an important research problem. Using a max-margin based surrogate loss function, AUC optimization problem can be approximated as a pairwise rankSVM learning problem. Batch learning methods for solving the kernelized version of this problem suffer from scalability and may not result in sparse classifiers. Recent years have witnessed an increased interest in the development of online or single-pass online learning algorithms that design a classifier by maximizing the AUC performance. The AUC performance of nonlinear classifiers, designed using online methods, is not comparable with that of nonlinear classifiers designed using batch learning algorithms on many real-world datasets. Motivated by these observations, we design a scalable algorithm for maximizing AUC performance by greedily adding the required number of basis functions into the classifier model. The resulting sparse classifiers perform faster inference. Our experimental results show that the level of sparsity achievable can be order of magnitude smaller than the Kernel RankSVM model without affecting the AUC performance much. version:1
arxiv-1612-06007 | A Hidden Absorbing Semi-Markov Model for Informatively Censored Temporal Data: Learning and Inference | http://arxiv.org/abs/1612.06007 | id:1612.06007 author:Ahmed M. Alaa, Mihaela van der Schaar category:cs.AI stat.ML  published:2016-12-18 summary:Modeling continuous-time physiological processes that manifest a patient's evolving clinical states is a key step in approaching many problems in healthcare. In this paper, we develop the Hidden Absorbing Semi-Markov Model (HASMM): a versatile probabilistic model that is capable of capturing the modern electronic health record (EHR) data. Unlike exist- ing models, an HASMM accommodates irregularly sampled, temporally correlated, and informatively censored physiological data, and can describe non-stationary clinical state transitions. Learning an HASMM from the EHR data is achieved via a novel forward- filtering backward-sampling Monte-Carlo EM algorithm that exploits the knowledge of the end-point clinical outcomes (informative censoring) in the EHR data, and implements the E-step by sequentially sampling the patients' clinical states in the reverse-time direction while conditioning on the future states. Real-time inferences are drawn via a forward- filtering algorithm that operates on a virtually constructed discrete-time embedded Markov chain that mirrors the patient's continuous-time state trajectory. We demonstrate the di- agnostic and prognostic utility of the HASMM in a critical care prognosis setting using a real-world dataset for patients admitted to the Ronald Reagan UCLA Medical Center. version:2
arxiv-1612-08608 | ASAP: Asynchronous Approximate Data-Parallel Computation | http://arxiv.org/abs/1612.08608 | id:1612.08608 author:Asim Kadav, Erik Kruus category:cs.DC cs.LG  published:2016-12-27 summary:Emerging workloads, such as graph processing and machine learning are approximate because of the scale of data involved and the stochastic nature of the underlying algorithms. These algorithms are often distributed over multiple machines using bulk-synchronous processing (BSP) or other synchronous processing paradigms such as map-reduce. However, data parallel processing primitives such as repeated barrier and reduce operations introduce high synchronization overheads. Hence, many existing data-processing platforms use asynchrony and staleness to improve data-parallel job performance. Often, these systems simply change the synchronous communication to asynchronous between the worker nodes in the cluster. This improves the throughput of data processing but results in poor accuracy of the final output since different workers may progress at different speeds and process inconsistent intermediate outputs. In this paper, we present ASAP, a model that provides asynchronous and approximate processing semantics for data-parallel computation. ASAP provides fine-grained worker synchronization using NOTIFY-ACK semantics that allows independent workers to run asynchronously. ASAP also provides stochastic reduce that provides approximate but guaranteed convergence to the same result as an aggregated all-reduce. In our results, we show that ASAP can reduce synchronization costs and provides 2-10X speedups in convergence and up to 10X savings in network costs for distributed machine learning applications and provides strong convergence guarantees. version:1
arxiv-1612-08549 | Relative Error Bounds for Nonnegative Matrix Factorization under a Geometric Assumption | http://arxiv.org/abs/1612.08549 | id:1612.08549 author:Zhaoqiang Liu, Vincent Y. F. Tan category:stat.ML stat.ME  published:2016-12-27 summary:We propose a geometric assumption on nonnegative data matrices such that under this assumption, we are able to provide upper bounds (both deterministic and probabilistic) on the relative error of nonnegative matrix factorization (NMF). The algorithm we propose first uses the geometric assumption to obtain an exact clustering of the columns of the data matrix; subsequently, it employs several rank-one NMFs to obtain the final decomposition. When applied to data matrices generated from our statistical model, we observe that our proposed algorithm produces factor matrices with comparable relative errors vis-a`-vis classical NMF algorithms but with much faster speeds. On face image and hyperspectral imaging datasets, we demonstrate that our algorithm provides an excellent initialization for applying other NMF algorithms at a low computational cost. Finally, we show on face and text datasets that the combinations of our algorithm and several classical NMF algorithms outperform other algorithms in terms of clustering performance. version:1
arxiv-1612-08544 | Theory-guided Data Science: A New Paradigm for Scientific Discovery | http://arxiv.org/abs/1612.08544 | id:1612.08544 author:Anuj Karpatne, Gowtham Atluri, James Faghmous, Michael Steinbach, Arindam Banerjee, Auroop Ganguly, Shashi Shekhar, Nagiza Samatova, Vipin Kumar category:cs.LG cs.AI stat.ML  published:2016-12-27 summary:Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science. version:1
arxiv-1612-08543 | Distributed Real-Time Sentiment Analysis for Big Data Social Streams | http://arxiv.org/abs/1612.08543 | id:1612.08543 author:Amir Hossein Akhavan Rahnama category:stat.ML cs.CL cs.DB cs.DC cs.IR  published:2016-12-27 summary:Big data trend has enforced the data-centric systems to have continuous fast data streams. In recent years, real-time analytics on stream data has formed into a new research field, which aims to answer queries about what-is-happening-now with a negligible delay. The real challenge with real-time stream data processing is that it is impossible to store instances of data, and therefore online analytical algorithms are utilized. To perform real-time analytics, pre-processing of data should be performed in a way that only a short summary of stream is stored in main memory. In addition, due to high speed of arrival, average processing time for each instance of data should be in such a way that incoming instances are not lost without being captured. Lastly, the learner needs to provide high analytical accuracy measures. Sentinel is a distributed system written in Java that aims to solve this challenge by enforcing both the processing and learning process to be done in distributed form. Sentinel is built on top of Apache Storm, a distributed computing platform. Sentinels learner, Vertical Hoeffding Tree, is a parallel decision tree-learning algorithm based on the VFDT, with ability of enabling parallel classification in distributed environments. Sentinel also uses SpaceSaving to keep a summary of the data stream and stores its summary in a synopsis data structure. Application of Sentinel on Twitter Public Stream API is shown and the results are discussed. version:1
arxiv-1612-08534 | Robust LSTM-Autoencoders for Face De-Occlusion in the Wild | http://arxiv.org/abs/1612.08534 | id:1612.08534 author:Fang Zhao, Jiashi Feng, Jian Zhao, Wenhan Yang, Shuicheng Yan category:cs.CV  published:2016-12-27 summary:Face recognition techniques have been developed significantly in recent years. However, recognizing faces with partial occlusion is still challenging for existing face recognizers which is heavily desired in real-world applications concerning surveillance and security. Although much research effort has been devoted to developing face de-occlusion methods, most of them can only work well under constrained conditions, such as all the faces are from a pre-defined closed set. In this paper, we propose a robust LSTM-Autoencoders (RLA) model to effectively restore partially occluded faces even in the wild. The RLA model consists of two LSTM components, which aims at occlusion-robust face encoding and recurrent occlusion removal respectively. The first one, named multi-scale spatial LSTM encoder, reads facial patches of various scales sequentially to output a latent representation, and occlusion-robustness is achieved owing to the fact that the influence of occlusion is only upon some of the patches. Receiving the representation learned by the encoder, the LSTM decoder with a dual channel architecture reconstructs the overall face and detects occlusion simultaneously, and by feat of LSTM, the decoder breaks down the task of face de-occlusion into restoring the occluded part step by step. Moreover, to minimize identify information loss and guarantee face recognition accuracy over recovered faces, we introduce an identity-preserving adversarial training scheme to further improve RLA. Extensive experiments on both synthetic and real datasets of faces with occlusion clearly demonstrate the effectiveness of our proposed RLA in removing different types of facial occlusion at various locations. The proposed method also provides significantly larger performance gain than other de-occlusion methods in promoting recognition performance over partially-occluded faces. version:1
arxiv-1612-08510 | Learning Non-Lambertian Object Intrinsics across ShapeNet Categories | http://arxiv.org/abs/1612.08510 | id:1612.08510 author:Jian Shi, Yue Dong, Hao Su, Stella X. Yu category:cs.CV  published:2016-12-27 summary:We consider the non-Lambertian object intrinsic problem of recovering diffuse albedo, shading, and specular highlights from a single image of an object. We build a large-scale object intrinsics database based on existing 3D models in the ShapeNet database. Rendered with realistic environment maps, millions of synthetic images of objects and their corresponding albedo, shading, and specular ground-truth images are used to train an encoder-decoder CNN. Once trained, the network can decompose an image into the product of albedo and shading components, along with an additive specular component. Our CNN delivers accurate and sharp results in this classical inverse problem of computer vision, sharp details attributed to skip layer connections at corresponding resolutions from the encoder to the decoder. Benchmarked on our ShapeNet and MIT intrinsics datasets, our model consistently outperforms the state-of-the-art by a large margin. We train and test our CNN on different object categories. Perhaps surprising especially from the CNN classification perspective, our intrinsics CNN generalizes very well across categories. Our analysis shows that feature learning at the encoder stage is more crucial for developing a universal representation across categories. We apply our synthetic data trained model to images and videos downloaded from the internet, and observe robust and realistic intrinsics results. Quality non-Lambertian intrinsics could open up many interesting applications such as image-based albedo and specular editing. version:1
arxiv-1612-08504 | Classifying Patents Based on their Semantic Content | http://arxiv.org/abs/1612.08504 | id:1612.08504 author:Antonin Bergeaud, Yoann Potiron, Juste Raimbault category:physics.soc-ph cs.CL  published:2016-12-27 summary:In this paper, we extend some usual techniques of classification resulting from a large-scale data-mining and network approach. This new technology, which in particular is designed to be suitable to big data, is used to construct an open consolidated database from raw data on 4 million patents taken from the US patent office from 1976 onward. To build the pattern network, not only do we look at each patent title, but we also examine their full abstract and extract the relevant keywords accordingly. We refer to this classification as semantic approach in contrast with the more common technological approach which consists in taking the topology when considering US Patent office technological classes. Moreover, we document that both approaches have highly different topological measures and strong statistical evidence that they feature a different model. This suggests that our method is a useful tool to extract endogenous information. version:1
arxiv-1612-08499 | End-to-End Data Visualization by Metric Learning and Coordinate Transformation | http://arxiv.org/abs/1612.08499 | id:1612.08499 author:Lilei Zheng, Ying Zhang, Stefan Duffner, Khalid Idrissi, Christophe Garcia, Atilla Baskurt category:cs.CV  published:2016-12-27 summary:This paper presents a deep nonlinear metric learning framework for data visualization on an image dataset. We propose the Triangular Similarity and prove its equivalence to the Cosine Similarity in measuring a data pair. Based on this novel similarity, a geometrically motivated loss function - the triangular loss - is then developed for optimizing a metric learning system comprising two identical CNNs. It is shown that this deep nonlinear system can be efficiently trained by a hybrid algorithm based on the conventional backpropagation algorithm. More interestingly, benefiting from classical manifold learning theories, the proposed system offers two different views to visualize the outputs, the second of which provides better classification results than the state-of-the-art methods in the visualizable spaces. version:1
arxiv-1612-08498 | Steerable CNNs | http://arxiv.org/abs/1612.08498 | id:1612.08498 author:Taco S. Cohen, Max Welling category:cs.LG stat.ML  published:2016-12-27 summary:It has long been recognized that the invariance and equivariance properties of a representation are critically important for success in many vision tasks. In this paper we present Steerable Convolutional Neural Networks, an efficient and flexible class of equivariant convolutional networks. We show that steerable CNNs achieve state of the art results on the CIFAR image classification benchmark. The mathematical theory of steerable representations reveals a type system in which any steerable representation is a composition of elementary feature types, each one associated with a particular kind of symmetry. We show how the parameter cost of a steerable filter bank depends on the types of the input and output features, and show how to use this knowledge to construct CNNs that utilize parameters effectively. version:1
arxiv-1612-07545 | A Revisit of Hashing Algorithms for Approximate Nearest Neighbor Search | http://arxiv.org/abs/1612.07545 | id:1612.07545 author:Deng Cai category:cs.CV  published:2016-12-22 summary:Approximate Nearest Neighbor (ANN) search is a fundamental problem in many areas of machine learning and data mining. During the past decade, numerous hashing algorithms are proposed to solve this problem. Every proposed algorithm claims outperforms other state-of-the-art methods. However, there are serious drawbacks in the evaluation of existing hashing papers and most of the claims in these papers should be re-examined. 1) Most of the existing papers failed to correctly measure the search time which is essential for the ANN search problem. 2) As a result, most of the papers report the performance increases as the code length increases, which is wrong if we measure the search time correctly. 3) The performance of some hashing algorithms (e.g., LSH) can easily be boosted if one uses multiple hash tables, which is an important factor should be considered in the evaluation while most of the papers failed to do so. In this paper, we carefully revisit many popular hashing algorithms and suggest one possible promising direction. For the sake of reproducibility, all the codes used in the paper are released on Github, which can be used as a testing platform to fairly compare various hashing algorithms. version:2
arxiv-1612-07562 | A note on the function approximation error bound for risk-sensitive reinforcement learning | http://arxiv.org/abs/1612.07562 | id:1612.07562 author:Prasenjit Karmakar, Shalabh Bhatnagar category:cs.LG  published:2016-12-22 summary:In this paper we improve the existing function approximation error bound for the policy evaluation algorithm when the aim is to find the risk-sensitive cost represented using exponential utility. version:2
arxiv-1612-08484 | An Automated CNN Recommendation System for Image Classification Tasks | http://arxiv.org/abs/1612.08484 | id:1612.08484 author:Song Wang, Li Sun, Wei Fan, Jun Sun, Satoshi Naoi, Koichi Shirahata, Takuya Fukagai, Yasumoto Tomita, Atsushi Ike category:cs.CV  published:2016-12-27 summary:Nowadays the CNN is widely used in practical applications for image classification task. However the design of the CNN model is very professional work and which is very difficult for ordinary users. Besides, even for experts of CNN, to select an optimal model for specific task may still need a lot of time (to train many different models). In order to solve this problem, we proposed an automated CNN recommendation system for image classification task. Our system is able to evaluate the complexity of the classification task and the classification ability of the CNN model precisely. By using the evaluation results, the system can recommend the optimal CNN model and which can match the task perfectly. The recommendation process of the system is very fast since we don't need any model training. The experiment results proved that the evaluation methods are very accurate and reliable. version:1
arxiv-1612-08461 | Randomized Block Frank-Wolfe for Convergent Large-Scale Learning | http://arxiv.org/abs/1612.08461 | id:1612.08461 author:Liang Zhang, Gang Wang, Daniel Romero, Georgios B. Giannakis category:math.OC cs.LG cs.NA  published:2016-12-27 summary:Owing to their low-complexity iterations, Frank-Wolfe (FW) solvers are well suited for various large-scale learning tasks. When block-separable constraints are also present, randomized FW has been shown to further reduce complexity by updating only a fraction of coordinate blocks per iteration. In this context, the present work develops feasibility-ensuring step sizes, and provably convergent randomized block Frank-Wolfe (RB-FW) solvers that are flexible in selecting the number of blocks to update per iteration. Convergence rates of RB-FW are established through computational bounds on a primal sub-optimality measure, and on the duality gap. Different from existing convergence analysis, which only applies to a step-size sequence that does not generally lead to feasible iterates, the analysis here includes two classes of step-size sequences that not only guarantee feasibility of the iterates, but also enhance flexibility in choosing decay rates. The novel convergence results are markedly broadened to encompass also nonconvex objectives, and further assert that RB-FW with exact line-search reaches a stationary point at rate $\mathcal{O}(1/\sqrt{t})$. Performance of RB-FW with different step sizes and number of blocks is demonstrated in two applications, namely charging of electrical vehicles and structural support vector machines. Simulated tests demonstrate the impressive performance improvement of RB-FW relative to existing randomized single-block FW methods. version:1
arxiv-1612-08408 | Signature of Geometric Centroids for 3D Local Shape Description and Partial Shape Matching | http://arxiv.org/abs/1612.08408 | id:1612.08408 author:Keke Tang, Peng Song, Xiaoping Chen category:cs.CV  published:2016-12-26 summary:Depth scans acquired from different views may contain nuisances such as noise, occlusion, and varying point density. We propose a novel Signature of Geometric Centroids descriptor, supporting direct shape matching on the scans, without requiring any preprocessing such as scan denoising or converting into a mesh. First, we construct the descriptor by voxelizing the local shape within a uniquely defined local reference frame and concatenating geometric centroid and point density features extracted from each voxel. Second, we compare two descriptors by employing only corresponding voxels that are both non-empty, thus supporting matching incomplete local shape such as those close to scan boundary. Third, we propose a descriptor saliency measure and compute it from a descriptor-graph to improve shape matching performance. We demonstrate the descriptor's robustness and effectiveness for shape matching by comparing it with three state-of-the-art descriptors, and applying it to object/scene reconstruction and 3D object recognition. version:1
arxiv-1612-08406 | Correlated signal inference by free energy exploration | http://arxiv.org/abs/1612.08406 | id:1612.08406 author:Torsten A. Enßlin, Jakob Knollmüller category:stat.ML astro-ph.IM cs.IT cs.LG math.IT 62F15  published:2016-12-26 summary:The inference of correlated signal fields with unknown correlation structures is of high scientific and technological relevance, but poses significant conceptual and numerical challenges. To address these, we develop the correlated signal inference (CSI) algorithm within information field theory (IFT) and discuss its numerical implementation. To this end, we introduce the free energy exploration (FrEE) strategy for numerical information field theory (NIFTy) applications. The FrEE strategy is to let the mathematical structure of the inference problem determine the dynamics of the numerical solver. FrEE uses the Gibbs free energy formalism for all involved unknown fields and correlation structures without marginalization of nuisance quantities. It thereby avoids the complexity marginalization often impose to IFT equations. FrEE simultaneously solves for the mean and the uncertainties of signal, nuisance, and auxiliary fields, while exploiting any analytically calculable quantity. Finally, FrEE uses a problem specific and self-tuning exploration strategy to swiftly identify the optimal field estimates as well as their uncertainty maps. For all estimated fields, properly weighted posterior samples drawn from their exact, fully non-Gaussian distributions can be generated. Here, we develop the FrEE strategies for the CSI of a normal, a log-normal, and a Poisson log-normal IFT signal inference problem and demonstrate their performances via their NIFTy implementations. version:1
arxiv-1612-08392 | Multi-Region Neural Representation: A novel model for decoding visual stimuli in human brains | http://arxiv.org/abs/1612.08392 | id:1612.08392 author:Muhammad Yousefnezhad, Daoqiang Zhang category:stat.ML cs.LG q-bio.NC  published:2016-12-26 summary:Multivariate Pattern (MVP) classification holds enormous potential for decoding visual stimuli in the human brain by employing task-based fMRI data sets. There is a wide range of challenges in the MVP techniques, i.e. decreasing noise and sparsity, defining effective regions of interest (ROIs), visualizing results, and the cost of brain studies. In overcoming these challenges, this paper proposes a novel model of neural representation, which can automatically detect the active regions for each visual stimulus and then utilize these anatomical regions for visualizing and analyzing the functional activities. Therefore, this model provides an opportunity for neuroscientists to ask this question: what is the effect of a stimulus on each of the detected regions instead of just study the fluctuation of voxels in the manually selected ROIs. Moreover, our method introduces analyzing snapshots of brain image for decreasing sparsity rather than using the whole of fMRI time series. Further, a new Gaussian smoothing method is proposed for removing noise of voxels in the level of ROIs. The proposed method enables us to combine different fMRI data sets for reducing the cost of brain studies. Experimental studies on 4 visual categories (words, consonants, objects and nonsense photos) confirm that the proposed method achieves superior performance to state-of-the-art methods. version:1
arxiv-1612-08388 | Clustering Algorithms: A Comparative Approach | http://arxiv.org/abs/1612.08388 | id:1612.08388 author:Mayra Z. Rodriguez, Cesar H. Comin, Dalcimar Casanova, Odemir M. Bruno, Diego R. Amancio, Francisco A. Rodrigues, Luciano da F. Costa category:cs.LG stat.ML  published:2016-12-26 summary:Many real-world systems can be studied in terms of pattern recognition tasks, so that proper use (and understanding) of machine learning methods in practical applications becomes essential. While a myriad of classification methods have been proposed, there is no consensus on which methods are more suitable for a given dataset. As a consequence, it is important to comprehensively compare methods in many possible scenarios. In this context, we performed a systematic comparison of 7 well-known clustering methods available in the R language. In order to account for the many possible variations of data, we considered artificial datasets with several tunable properties (number of classes, separation between classes, etc). In addition, we also evaluated the sensitivity of the clustering methods with regard to their parameters configuration. The results revealed that, when considering the default configurations of the adopted methods, the spectral approach usually outperformed the other clustering algorithms. We also found that the default configuration of the adopted implementations was not accurate. In these cases, a simple approach based on random selection of parameters values proved to be a good alternative to improve the performance. All in all, the reported approach provides subsidies guiding the choice of clustering algorithms. version:1
arxiv-1612-06699 | Unsupervised Perceptual Rewards for Imitation Learning | http://arxiv.org/abs/1612.06699 | id:1612.06699 author:Pierre Sermanet, Kelvin Xu, Sergey Levine category:cs.CV cs.RO  published:2016-12-20 summary:Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a suitable reward function takes considerable manual engineering and often requires additional and potentially visible sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide useful feedback on these implicit intermediate steps or sub-goals. To address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify the key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit sub-goal supervision. The resulting reward functions can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward functions, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also demonstrate that our method can be used to learn a complex real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task. version:2
arxiv-1612-08375 | Abstractive Headline Generation for Spoken Content by Attentive Recurrent Neural Networks with ASR Error Modeling | http://arxiv.org/abs/1612.08375 | id:1612.08375 author:Lang-Chi Yu, Hung-yi Lee, Lin-shan Lee category:cs.CL  published:2016-12-26 summary:Headline generation for spoken content is important since spoken content is difficult to be shown on the screen and browsed by the user. It is a special type of abstractive summarization, for which the summaries are generated word by word from scratch without using any part of the original content. Many deep learning approaches for headline generation from text document have been proposed recently, all requiring huge quantities of training data, which is difficult for spoken document summarization. In this paper, we propose an ASR error modeling approach to learn the underlying structure of ASR error patterns and incorporate this model in an Attentive Recurrent Neural Network (ARNN) architecture. In this way, the model for abstractive headline generation for spoken content can be learned from abundant text data and the ASR data for some recognizers. Experiments showed very encouraging results and verified that the proposed ASR error model works well even when the input spoken content is recognized by a recognizer very different from the one the model learned from. version:1
arxiv-1612-06676 | Multivariate Industrial Time Series with Cyber-Attack Simulation: Fault Detection Using an LSTM-based Predictive Data Model | http://arxiv.org/abs/1612.06676 | id:1612.06676 author:Pavel Filonov, Andrey Lavrentyev, Artem Vorontsov category:cs.LG stat.ML  published:2016-12-20 summary:We adopted an approach based on an LSTM neural network to monitor and detect faults in industrial multivariate time series data. To validate the approach we created a Modelica model of part of a real gasoil plant. By introducing hacks into the logic of the Modelica model, we were able to generate both the roots and causes of fault behavior in the plant. Having a self-consistent data set with labeled faults, we used an LSTM architecture with a forecasting error threshold to obtain precision and recall quality metrics. The dependency of the quality metric on the threshold level is considered. An appropriate mechanism such as "one handle" was introduced for filtering faults that are outside of the plant operator field of interest. version:2
arxiv-1612-08359 | Extracting Sub-Exposure Images from a Single Capture Through Fourier-based Optical Modulation | http://arxiv.org/abs/1612.08359 | id:1612.08359 author:Shah Rez Khan, Martin Feldman, Bahadir K. Gunturk category:cs.CV  published:2016-12-26 summary:Through pixel-wise optical coding of images during exposure time, it is possible to extract sub-exposure images from a single capture. Such a capability can be used for different purposes, including high-speed imaging, high-dynamic-range imaging and compressed sensing. In this paper, we demonstrate a sub-exposure image extraction method, where the exposure coding pattern is inspired from frequency division multiplexing idea of communication systems. The coding masks modulate sub-exposure images in such a way that they are placed in non-overlapping regions in Fourier domain. The sub-exposure image extraction process involves digital filtering of the captured signal with proper band-pass filters. The prototype imaging system incorporates a Liquid Crystal over Silicon (LCoS) based spatial light modulator synchronized with a camera for pixel-wise exposure coding. version:1
arxiv-1612-08354 | Image-Text Multi-Modal Representation Learning by Adversarial Backpropagation | http://arxiv.org/abs/1612.08354 | id:1612.08354 author:Gwangbeen Park, Woobin Im category:cs.CV cs.CL cs.LG  published:2016-12-26 summary:We present novel method for image-text multi-modal representation learning. In our knowledge, this work is the first approach of applying adversarial learning concept to multi-modal learning and not exploiting image-text pair information to learn multi-modal feature. We only use category information in contrast with most previous methods using image-text pair information for multi-modal embedding. In this paper, we show that multi-modal feature can be achieved without image-text pair information and our method makes more similar distribution with image and text in multi-modal feature space than other methods which use image-text pair information. And we show our multi-modal feature has universal semantic information, even though it was trained for category prediction. Our model is end-to-end backpropagation, intuitive and easily extended to other multi-modal learning work. version:1
arxiv-1612-08333 | Text Summarization using Deep Learning and Ridge Regression | http://arxiv.org/abs/1612.08333 | id:1612.08333 author:Karthik Bangalore Mani category:cs.CL  published:2016-12-26 summary:We develop models and extract relevant features for automatic text summarization and investigate the performance of different models on the DUC 2001 dataset. Two different models were developed, one being a ridge regressor and the other one was a multi-layer perceptron. The hyperparameters were varied and their performance were noted. We segregated the summarization task into 2 main steps, the first being sentence ranking and the second step being sentence selection. In the first step, given a document, we sort the sentences based on their Importance, and in the second step, in order to obtain non-redundant sentences, we weed out the sentences that are have high similarity with the previously selected sentences. version:1
arxiv-1612-06027 | Neural Multi-Source Morphological Reinflection | http://arxiv.org/abs/1612.06027 | id:1612.06027 author:Katharina Kann, Ryan Cotterell, Hinrich Schütze category:cs.CL  published:2016-12-19 summary:We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one source form since different source forms can provide complementary information, e.g., different stems. We further present a novel extension to the encoder- decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research. version:2
arxiv-1612-08321 | Generalized Optimal Matching Methods for Causal Inference | http://arxiv.org/abs/1612.08321 | id:1612.08321 author:Nathan Kallus category:stat.ML math.OC math.ST stat.ME stat.TH  published:2016-12-26 summary:We develop an encompassing framework and theory for matching and related methods for causal inference that reveal the connections and motivations behind various existing methods and give rise to new and improved ones. The framework is given by generalizing a new functional analytical characterization of optimal matching as minimizing worst-case conditional mean squared error given the observed data based on specific restrictions and assumptions. By generalizing these, we obtain a new class of generalized optimal matching (GOM) methods, for which we provide a single theory for tractability and consistency that applies generally to GOM. Many commonly used existing methods are included in GOM and using their GOM interpretation we extend these to new methods that judiciously and automatically trade off balance for variance and outperform their standard counterparts. As a subclass of GOM, we develop kernel optimal matching, which, as supported by new theory, is notable for combining the interpretability of matching methods, the non-parametric model-free consistency of optimal matching, the efficiency of well-specified regression, the judicious sample size selection of monotonic imbalance bounding methods, the double robustness of augmented inverse propensity weight estimators, and the model-selection flexibility of Gaussian-process regression. We discuss connections to and non-linear generalizations of equal percent bias reduction and its ramifications. version:1
arxiv-1612-06457 | High Performance Software in Multidimensional Reduction Methods for Image Processing with Application to Ancient Manuscripts | http://arxiv.org/abs/1612.06457 | id:1612.06457 author:Corneliu T. C. Arsene, Peter E. Pormann, Naima Afif, Stephen Church, Mark Dickinson category:cs.CV  published:2016-12-19 summary:Multispectral imaging is an important technique for improving the readability of written or printed text where the letters have faded, either due to deliberate erasing or simply due to the ravages of time. Often the text can be read simply by looking at individual wavelengths, but in some cases the images need further enhancement to maximise the chances of reading the text. There are many possible enhancement techniques and this paper assesses and compares an extended set of dimensionality reduction methods for image processing. We assess 15 dimensionality reduction methods in two different manuscripts. This assessment was performed both subjectively by asking the opinions of scholars who were experts in the languages used in the manuscripts which of the techniques they preferred and also by using the Davies-Bouldin and Dunn indexes for assessing the quality of the resulted image clusters. We found that the Canonical Variates Analysis (CVA) method which was using a Matlab implementation and we have used previously to enhance multispectral images, it was indeed superior to all the other tested methods. However it is very likely that other approaches will be more suitable in specific circumstance so we would still recommend that a range of these techniques are tried. In particular, CVA is a supervised clustering technique so it requires considerably more user time and effort than a non-supervised technique such as the much more commonly used Principle Component Analysis Approach (PCA). If the results from PCA are adequate to allow a text to be read then the added effort required for CVA may not be justified. For the purposes of comparing the computational times and the image results, a CVA method is also implemented in C programming language and using the GNU (GNUs Not Unix) Scientific Library (GSL) and the OpenCV (OPEN source Computer Vision) computer vision programming library. version:2
arxiv-1612-08274 | Globally Optimal Object Tracking with Fully Convolutional Networks | http://arxiv.org/abs/1612.08274 | id:1612.08274 author:Jinho Lee, Brian Kenji Iwana, Shouta Ide, Seiichi Uchida category:cs.CV  published:2016-12-25 summary:Tracking is one of the most important but still difficult tasks in computer vision and pattern recognition. The main difficulties in the tracking field are appearance variation and occlusion. Most traditional tracking methods set the parameters or templates to track target objects in advance and should be modified accordingly. Thus, we propose a new and robust tracking method using a Fully Convolutional Network (FCN) to obtain an object probability map and Dynamic Programming (DP) to seek the globally optimal path through all frames of video. Our proposed method solves the object appearance variation problem with the use of a FCN and deals with occlusion by DP. We show that our method is effective in tracking various single objects through video frames. version:1
arxiv-1612-05695 | Reinforcement Learning Using Quantum Boltzmann Machines | http://arxiv.org/abs/1612.05695 | id:1612.05695 author:Daniel Crawford, Anna Levit, Navid Ghadermarzy, Jaspreet S. Oberoi, Pooya Ronagh category:quant-ph cs.AI cs.LG cs.NE math.OC  published:2016-12-17 summary:We investigate whether quantum annealers with select chip layouts can outperform classical computers in reinforcement learning tasks. We associate a transverse field Ising spin Hamiltonian with a layout of qubits similar to that of a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to numerically simulate quantum sampling from this system. We design a reinforcement learning algorithm in which the set of visible nodes representing the states and actions of an optimal policy are the first and last layers of the deep network. In absence of a transverse field, our simulations show that DBMs train more effectively than restricted Boltzmann machines (RBM) with the same number of weights. Since sampling from Boltzmann distributions of a DBM is not classically feasible, this is evidence of advantage of a non-Turing sampling oracle. We then develop a framework for training the network as a quantum Boltzmann machine (QBM) in the presence of a significant transverse field for reinforcement learning. This further improves the reinforcement learning method using DBMs. version:2
arxiv-1612-08242 | YOLO9000: Better, Faster, Stronger | http://arxiv.org/abs/1612.08242 | id:1612.08242 author:Joseph Redmon, Ali Farhadi category:cs.CV  published:2016-12-25 summary:We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time. version:1
arxiv-1612-08230 | Pancreas Segmentation in Abdominal CT Scan: A Coarse-to-Fine Approach | http://arxiv.org/abs/1612.08230 | id:1612.08230 author:Yuyin Zhou, Lingxi Xie, Wei Shen, Elliot Fishman, Alan Yuille category:cs.CV  published:2016-12-25 summary:Deep neural networks have been widely adopted for automatic organ segmentation from CT-scanned images. However, the segmentation accuracy on some small organs (e.g., the pancreas) is sometimes below satisfaction, arguably because deep networks are easily distracted by the complex and variable background region which occupies a large fraction of the input volume. In this paper, we propose a coarse-to-fine approach to deal with this problem. We train two deep neural networks using different regions of the input volume. The first one, the coarse-scaled model, takes the entire volume as its input. It is used for roughly locating the spatial position of the pancreas. The second one, the fine-scaled model, only sees a small input region covering the pancreas, thus eliminating the background noise and providing more accurate segmentation especially around the boundary areas. At the testing stage, we first use the coarse-scaled model to roughly locate the pancreas, then adopt the fine-scaled model to refine the initial segmentation in an iterative manner to obtain increasingly better segmentation. We evaluate our algorithm on the NIH pancreas segmentation dataset with 82 volumes, and outperform the state-of-the-art [18] by more than 4% measured by the Dice-Sorensen Coefficient (DSC). In addition, we report 62.43% DSC for our worst case, significantly better than 34.11% reported in [18]. version:1
arxiv-1612-08205 | Predicting the Industry of Users on Social Media | http://arxiv.org/abs/1612.08205 | id:1612.08205 author:Konstantinos Pappas, Rada Mihalcea category:cs.CL cs.SI  published:2016-12-24 summary:Automatic profiling of social media users is an important task for supporting a multitude of downstream applications. While a number of studies have used social media content to extract and study collective social attributes, there is a lack of substantial research that addresses the detection of a user's industry. We frame this task as classification using both feature engineering and ensemble learning. Our industry-detection system uses both posted content and profile information to detect a user's industry with 64.3% accuracy, significantly outperforming the majority baseline in a taxonomy of fourteen industry classes. Our qualitative analysis suggests that a person's industry not only affects the words used and their perceived meanings, but also the number and type of emotions being expressed. version:1
arxiv-1612-08178 | JU_KS_Group@FIRE 2016: Consumer Health Information Search | http://arxiv.org/abs/1612.08178 | id:1612.08178 author:Kamal Sarkar, Debanjan Das, Indra Banerjee, Mamta Kumari, Prasenjit Biswas category:cs.IR cs.CL  published:2016-12-24 summary:In this paper, we describe the methodology used and the results obtained by us for completing the tasks given under the shared task on Consumer Health Information Search (CHIS) collocated with the Forum for Information Retrieval Evaluation (FIRE) 2016, ISI Kolkata. The shared task consists of two sub-tasks - (1) task1: given a query and a document/set of documents associated with that query, the task is to classify the sentences in the document as relevant to the query or not and (2) task 2: the relevant sentences need to be further classified as supporting the claim made in the query, or opposing the claim made in the query. We have participated in both the sub-tasks. The percentage accuracy obtained by our developed system for task1 was 73.39 which is third highest among the 9 teams participated in the shared task. version:1
arxiv-1612-08171 | KS_JU@DPIL-FIRE2016:Detecting Paraphrases in Indian Languages Using Multinomial Logistic Regression Model | http://arxiv.org/abs/1612.08171 | id:1612.08171 author:Kamal Sarkar category:cs.CL  published:2016-12-24 summary:In this work, we describe a system that detects paraphrases in Indian Languages as part of our participation in the shared Task on detecting paraphrases in Indian Languages (DPIL) organized by Forum for Information Retrieval Evaluation (FIRE) in 2016. Our paraphrase detection method uses a multinomial logistic regression model trained with a variety of features which are basically lexical and semantic level similarities between two sentences in a pair. The performance of the system has been evaluated against the test set released for the FIRE 2016 shared task on DPIL. Our system achieves the highest f-measure of 0.95 on task1 in Punjabi language.The performance of our system on task1 in Hindi language is f-measure of 0.90. Out of 11 teams participated in the shared task, only four teams participated in all four languages, Hindi, Punjabi, Malayalam and Tamil, but the remaining 7 teams participated in one of the four languages. We also participated in task1 and task2 both for all four Indian Languages. The overall average performance of our system including task1 and task2 overall four languages is F1-score of 0.81 which is the second highest score among the four systems that participated in all four languages. version:1
arxiv-1612-08170 | Joint denoising and distortion correction of atomic scale scanning transmission electron microscopy images | http://arxiv.org/abs/1612.08170 | id:1612.08170 author:Benjamin Berkels, Benedikt Wirth category:cs.CV physics.data-an  published:2016-12-24 summary:Nowadays, modern electron microscopes deliver images at atomic scale. The precise atomic structure encodes information about material properties. Thus, an important ingredient in the image analysis is to locate the centers of the atoms shown in micrographs as precisely as possible. Here, we consider scanning transmission electron microscopy (STEM), which acquires data in a rastering pattern, pixel by pixel. Due to this rastering combined with the magnification to atomic scale, movements of the specimen even at the nanometer scale lead to random image distortions that make precise atom localization difficult. Given a series of STEM images, we derive a Bayesian method that jointly estimates the distortion in each image and reconstructs the underlying atomic grid of the material by fitting the atom bumps with suitable bump functions. The resulting highly non-convex minimization problems are solved numerically with a trust region approach. Well-posedness of the reconstruction method and the model behavior for faster and faster rastering are investigated using variational techniques. The performance of the method is finally evaluated on both synthetic and real experimental data. version:1
arxiv-1612-08169 | Unsupervised Video Segmentation via Spatio-Temporally Nonlocal Appearance Learning | http://arxiv.org/abs/1612.08169 | id:1612.08169 author:Kaihua Zhang, Xuejun Li, Qingshan Liu category:cs.CV  published:2016-12-24 summary:Video object segmentation is challenging due to the factors like rapidly fast motion, cluttered backgrounds, arbitrary object appearance variation and shape deformation. Most existing methods only explore appearance information between two consecutive frames, which do not make full use of the usefully long-term nonlocal information that is helpful to make the learned appearance stable, and hence they tend to fail when the targets suffer from large viewpoint changes and significant non-rigid deformations. In this paper, we propose a simple yet effective approach to mine the long-term sptatio-temporally nonlocal appearance information for unsupervised video segmentation. The motivation of our algorithm comes from the spatio-temporal nonlocality of the region appearance reoccurrence in a video. Specifically, we first generate a set of superpixels to represent the foreground and background, and then update the appearance of each superpixel with its long-term sptatio-temporally nonlocal counterparts generated by the approximate nearest neighbor search method with the efficient KD-tree algorithm. Then, with the updated appearances, we formulate a spatio-temporal graphical model comprised of the superpixel label consistency potentials. Finally, we generate the segmentation by optimizing the graphical model via iteratively updating the appearance model and estimating the labels. Extensive evaluations on the SegTrack and Youtube-Objects datasets demonstrate the effectiveness of the proposed method, which performs favorably against some state-of-art methods. version:1
arxiv-1612-08153 | EgoReID: Cross-view Self-Identification and Human Re-identification in Egocentric and Surveillance Videos | http://arxiv.org/abs/1612.08153 | id:1612.08153 author:Shervin Ardeshir, Sandesh Sharma, Ali Broji category:cs.CV cs.CG  published:2016-12-24 summary:Human identification remains to be one of the challenging tasks in computer vision community due to drastic changes in visual features across different viewpoints, lighting conditions, occlusion, etc. Most of the literature has been focused on exploring human re-identification across viewpoints that are not too drastically different in nature. Cameras usually capture oblique or side views of humans, leaving room for a lot of geometric and visual reasoning. Given the recent popularity of egocentric and top-view vision, re-identification across these two drastically different views can now be explored. Having an egocentric and a top view video, our goal is to identify the cameraman in the content of the top-view video, and also re-identify the people visible in the egocentric video, by matching them to the identities present in the top-view video. We propose a CRF-based method to address the two problems. Our experimental results demonstrates the efficiency of the proposed approach over a variety of video recorded from two views. version:1
arxiv-1507-08726 | Robustness in sparse linear models: relative efficiency based on robust approximate message passing | http://arxiv.org/abs/1507.08726 | id:1507.08726 author:Jelena Bradic category:math.ST stat.ME stat.ML stat.TH  published:2015-07-31 summary:Understanding efficiency in high dimensional linear models is a longstanding problem of interest. Classical work with smaller dimensional problems dating back to Huber and Bickel has illustrated the benefits of efficient loss functions. When the number of parameters $p$ is of the same order as the sample size $n$, $p \approx n$, an efficiency pattern different from the one of Huber was recently established. In this work, we consider the effects of model selection on the estimation efficiency of penalized methods. In particular, we explore whether sparsity, results in new efficiency patterns when $p > n$. In the interest of deriving the asymptotic mean squared error for regularized M-estimators, we use the powerful framework of approximate message passing. We propose a novel, robust and sparse approximate message passing algorithm (RAMP), that is adaptive to the error distribution. Our algorithm includes many non-quadratic and non-differentiable loss functions. We derive its asymptotic mean squared error and show its convergence, while allowing $p, n, s \to \infty$, with $n/p \in (0,1)$ and $n/s \in (1,\infty)$. We identify new patterns of relative efficiency regarding a number of penalized $M$ estimators, when $p$ is much larger than $n$. We show that the classical information bound is no longer reachable, even for light--tailed error distributions. We show that the penalized least absolute deviation estimator dominates the penalized least square estimator, in cases of heavy--tailed distributions. We observe this pattern for all choices of the number of non-zero parameters $s$, both $s \leq n$ and $s \approx n$. In non-penalized problems where $s =p \approx n$, the opposite regime holds. Therefore, we discover that the presence of model selection significantly changes the efficiency patterns. version:2
arxiv-1612-08117 | Improving Human-Machine Cooperative Visual Search With Soft Highlighting | http://arxiv.org/abs/1612.08117 | id:1612.08117 author:Ronald T. Kneusel, Michael C. Mozer category:cs.HC cs.NE  published:2016-12-24 summary:Advances in machine learning have produced systems that attain human-level performance on certain visual tasks, e.g., object identification. Nonetheless, other tasks requiring visual expertise are unlikely to be entrusted to machines for some time, e.g., satellite and medical imagery analysis. We describe a human-machine cooperative approach to visual search, the aim of which is to outperform either human or machine acting alone. The traditional route to augmenting human performance with automatic classifiers is to draw boxes around regions of an image deemed likely to contain a target. Human experts typically reject this type of hard highlighting. We propose instead a soft highlighting technique in which the saliency of regions of the visual field is modulated in a graded fashion based on classifier confidence level. We report on experiments with both synthetic and natural images showing that soft highlighting achieves a performance synergy surpassing that attained by hard highlighting. version:1
arxiv-1612-08116 | Tensors and algebra give interpretable groups for crosstalk mechanisms in breast cancer | http://arxiv.org/abs/1612.08116 | id:1612.08116 author:Anna Seigal, Mariano Beguerisse-Díaz, Birgit Schoeberl, Mario Niepel, Heather A. Harrington category:q-bio.QM math.OC physics.soc-ph q-bio.MN stat.ML  published:2016-12-24 summary:We introduce a tensor-based algebraic clustering method to extract sparse, low-dimensional structure from multidimensional arrays of experimental data. Our methodology is applicable to high dimensional data structures that arise across the sciences. Specifically we introduce a new way to cluster data subject to multi-indexed structural constraints via integer programming. The method can work as a stand-alone clustering tool or in combination with established methods. We implement this approach on a dataset consisting of genetically diverse breast cancer cell lines exposed to a range of signaling molecules, where each experiment is labelled by its combination of cell line and ligand. The data consist of time-course measurements of the immediate-early signaling of mitogen activated protein kinase (MAPK), and phosphoinositide 3-kinase (PI3K)/Protein kinase B (AKT). By respecting the multi-indexed structure of the experimental data, the analysis can be optimized for biological interpretation and therapeutic understanding. We quantify the heterogeneity of breast cancer cell subtypes and systematically explore mechanistic models of MAP Kinase and PI3K (phosphoinositide 3-kinase)/AKT crosstalk based on the results of our method. version:1
arxiv-1612-08109 | Solving Combinatorial Optimization problems with Quantum inspired Evolutionary Algorithm Tuned using a Novel Heuristic Method | http://arxiv.org/abs/1612.08109 | id:1612.08109 author:Nija Mani, Gursaran, Ashish Mani category:cs.AI cs.NE  published:2016-12-23 summary:Quantum inspired Evolutionary Algorithms were proposed more than a decade ago and have been employed for solving a wide range of difficult search and optimization problems. A number of changes have been proposed to improve performance of canonical QEA. However, canonical QEA is one of the few evolutionary algorithms, which uses a search operator with relatively large number of parameters. It is well known that performance of evolutionary algorithms is dependent on specific value of parameters for a given problem. The advantage of having large number of parameters in an operator is that the search process can be made more powerful even with a single operator without requiring a combination of other operators for exploration and exploitation. However, the tuning of operators with large number of parameters is complex and computationally expensive. This paper proposes a novel heuristic method for tuning parameters of canonical QEA. The tuned QEA outperforms canonical QEA on a class of discrete combinatorial optimization problems which, validates the design of the proposed parameter tuning framework. The proposed framework can be used for tuning other algorithms with both large and small number of tunable parameters. version:1
arxiv-1612-08102 | On Spectral Analysis of Directed Signed Graphs | http://arxiv.org/abs/1612.08102 | id:1612.08102 author:Yuemeng Li, Xintao Wu, Aidong Lu category:cs.SI cs.LG physics.soc-ph  published:2016-12-23 summary:It has been shown that the adjacency eigenspace of a network contains key information of its underlying structure. However, there has been no study on spectral analysis of the adjacency matrices of directed signed graphs. In this paper, we derive theoretical approximations of spectral projections from such directed signed networks using matrix perturbation theory. We use the derived theoretical results to study the influences of negative intra cluster and inter cluster directed edges on node spectral projections. We then develop a spectral clustering based graph partition algorithm, SC-DSG, and conduct evaluations on both synthetic and real datasets. Both theoretical analysis and empirical evaluation demonstrate the effectiveness of the proposed algorithm. version:1
arxiv-1612-08083 | Language Modeling with Gated Convolutional Networks | http://arxiv.org/abs/1612.08083 | id:1612.08083 author:Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier category:cs.CL  published:2016-12-23 summary:The pre-dominant approach to language modeling to date is based on recurrent neural networks. In this paper we present a convolutional approach to language modeling. We introduce a novel gating mechanism that eases gradient propagation and which performs better than the LSTM-style gating of (Oord et al, 2016) despite being simpler. We achieve a new state of the art on WikiText-103 as well as a new best single-GPU result on the Google Billion Word benchmark. In settings where latency is important, our model achieves an order of magnitude speed-up compared to a recurrent baseline since computation can be parallelized over time. To our knowledge, this is the first time a non-recurrent approach outperforms strong recurrent models on these tasks. version:1
arxiv-1612-08082 | Constructing Effective Personalized Policies Using Counterfactual Inference from Biased Data Sets with Many Features | http://arxiv.org/abs/1612.08082 | id:1612.08082 author:Onur Atan, William R. Zame, Qiaojun Feng, Mihaela van der Schaar category:stat.ML cs.LG  published:2016-12-23 summary:This paper proposes a novel approach for constructing effective personalized policies when the observed data lacks counter-factual information, is biased and possesses many features. The approach is applicable in a wide variety of settings from healthcare to advertising to education to finance. These settings have in common that the decision maker can observe, for each previous instance, an array of features of the instance, the action taken in that instance, and the reward realized -- but not the rewards of actions that were not taken: the counterfactual information. Learning in such settings is made even more difficult because the observed data is typically biased by the existing policy (that generated the data) and because the array of features that might affect the reward in a particular instance -- and hence should be taken into account in deciding on an action in each particular instance -- is often vast. The approach presented here estimates propensity scores for the observed data, infers counterfactuals, identifies a (relatively small) number of features that are (most) relevant for each possible action and instance, and prescribes a policy to be followed. Comparison of the proposed algorithm against the state-of-art algorithm on actual datasets demonstrates that the proposed algorithm achieves a significant improvement in performance. version:1
arxiv-1612-08049 | Correlation Preserving Sparse Coding Over Multi-level Dictionaries for Image Denoising | http://arxiv.org/abs/1612.08049 | id:1612.08049 author:Rui Chen, Huizhu Jia, Xiaodong Xie, Wen Gao category:cs.CV  published:2016-12-23 summary:In this letter, we propose a novel image denoising method based on correlation preserving sparse coding. Because the instable and unreliable correlations among basis set can limit the performance of the dictionary-driven denoising methods, two effective regularized strategies are employed in the coding process. Specifically, a graph-based regularizer is built for preserving the global similarity correlations, which can adaptively capture both the geometrical structures and discriminative features of textured patches. In particular, edge weights in the graph are obtained by seeking a nonnegative low-rank construction. Besides, a robust locality-constrained coding can automatically preserve not only spatial neighborhood information but also internal consistency present in noisy patches while learning overcomplete dictionary. Experimental results demonstrate that our proposed method achieves state-of-the-art denoising performance in terms of both PSNR and subjective visual quality. version:1
arxiv-1612-08037 | Blind restoration for non-uniform aerial images using non-local Retinex model and shearlet-based higher-order regularization | http://arxiv.org/abs/1612.08037 | id:1612.08037 author:Rui Chen, Huizhu Jia, Xiaodong Xie, Wen Gao category:cs.CV  published:2016-12-23 summary:Aerial images are often degraded by space-varying motion blur and simultaneous uneven illumination. To recover high-quality aerial image from its non-uniform version, we propose a novel patch-wise restoration approach based on a key observation that the degree of blurring is inevitably affected by the illuminated conditions. A non-local Retinex model is developed to accurately estimate the reflectance component from the degraded aerial image. Thereafter the uneven illumination is corrected well. And then non-uniform coupled blurring in the enhanced reflectance image is alleviated and transformed towards uniform distribution, which will facilitate the subsequent deblurring. For constructing the multi-scale sparsified regularizer, the discrete shearlet transform is improved to better represent anisotropic image features in term of directional sensitivity and selectivity. In addition, a new adaptive variant of total generalized variation is proposed for the structure-preserving regularizer. These complementary regularizers are elegantly integrated into an objective function. The final deblurred image with uniform illumination can be extracted by applying the fast alternating direction scheme to solve the derived function. The experimental results demonstrate that our algorithm can not only remove both the space-varying illumination and motion blur in the aerial image effectively but also recover the abundant details of aerial scenes with top-level objective and subjective quality, and outperforms other state-of-the-art restoration methods. version:1
arxiv-1612-08036 | Focused Learning and Proofreading for Delineation of Curvilinear Structures | http://arxiv.org/abs/1612.08036 | id:1612.08036 author:Agata Mosinska, Carlos Becker, Pascal Fua category:cs.CV  published:2016-12-23 summary:Many state-of-the-art delineation methods rely on supervised machine learning algorithms. As a result, they require manually annotated training data, which is tedious to obtain. Furthermore, even minor classification errors may significantly affect the topology of the final result. In this paper we propose a unified approach to address both of these problems by taking into account the influence of a potential misclassification on the resulting delineation. In an Active Learning context, we find parts of linear structures that should be annotated first in order to train an effective classifier. In a proofreading context, we similarly find regions of the resulting reconstruction that should be verified in priority to obtain a nearly-perfect result. In both cases, by focusing the attention of the human expert on the potential classification mistakes, which are the most critical parts of the delineation, we reduce the amount of annotation effort. We demonstrate the effectiveness of our approach on a variety of datasets that comprise both biomedical and aerial imagery. version:1
arxiv-1612-08012 | Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the LUNA16 challenge | http://arxiv.org/abs/1612.08012 | id:1612.08012 author:Arnaud Arindra Adiyoso Setio, Alberto Traverso, Thomas de Bel, Moira S. N. Berens, Cas van den Bogaard, Piergiorgio Cerello, Hao Chen, Qi Dou, Maria Evelina Fantacci, Bram Geurts, Robbert van der Gugten, Pheng Ann Heng, Bart Jansen, Michael M. J. de Kaste, Valentin Kotov, Jack Yu-Hung Lin, Jeroen T. M. C. Manders, Alexander Sónora-Mengana, Juan Carlos García-Naranjo, Mathias Prokop, Marco Saletta, Cornelia M Schaefer-Prokop, Ernst T. Scholten, Luuk Scholten, Miranda M. Snoeren, Ernesto Lopez Torres, Jef Vandemeulebroucke, Nicole Walasek, Guido C. A. Zuidhof, Bram van Ginneken, Colin Jacobs category:cs.CV  published:2016-12-23 summary:Automatic detection of pulmonary nodules in thoracic computed tomography (CT) scans has been an active area of research for the last two decades. However, there have only been few studies that provide a comparative performance evaluation of different systems on a common database. We have therefore set up the LUNA16 challenge, an objective evaluation framework for automatic nodule detection algorithms using the largest publicly available reference database of chest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their algorithm and upload their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates should be classified. This paper describes the setup of LUNA16 and presents the results of the challenge so far. Moreover, the impact of combining individual systems on the detection performance was also investigated. It was observed that the leading solutions employed convolutional networks and used the provided set of nodule candidates. The combination of these solutions achieved an excellent sensitivity of over 95% at fewer than 1.0 false positives per scan. This highlights the potential of combining algorithms to improve the detection performance. Our observer study with four expert readers has shown that the best system detects nodules that were missed by expert readers who originally annotated the LIDC-IDRI data. We released this set of additional nodules for further development of CAD systems. version:1
arxiv-1612-07993 | RSSL: Semi-supervised Learning in R | http://arxiv.org/abs/1612.07993 | id:1612.07993 author:Jesse H. Krijthe category:stat.ML cs.LG  published:2016-12-23 summary:In this paper, we introduce a package for semi-supervised learning research in the R programming language called RSSL. We cover the purpose of the package, the methods it includes and comment on their use and implementation. We then show, using several code examples, how the package can be used to replicate well-known results from the semi-supervised learning literature. version:1
arxiv-1612-07978 | Two-stream convolutional neural network for accurate RGB-D fingertip detection using depth and edge information | http://arxiv.org/abs/1612.07978 | id:1612.07978 author:Hengkai Guo, Guijin Wang, Xinghao Chen category:cs.CV  published:2016-12-23 summary:Accurate detection of fingertips in depth image is critical for human-computer interaction. In this paper, we present a novel two-stream convolutional neural network (CNN) for RGB-D fingertip detection. Firstly edge image is extracted from raw depth image using random forest. Then the edge information is combined with depth information in our CNN structure. We study several fusion approaches and suggest a slow fusion strategy as a promising way of fingertip detection. As shown in our experiments, our real-time algorithm outperforms state-of-the-art fingertip detection methods on the public dataset HandNet with an average 3D error of 9.9mm, and shows comparable accuracy of fingertip estimation on NYU hand dataset. version:1
arxiv-1612-07528 | Cohort of LSTM and lexicon verification for handwriting recognition with gigantic lexicon | http://arxiv.org/abs/1612.07528 | id:1612.07528 author:Bruno Stuner, Clément Chatelain, Thierry Paquet category:cs.CV  published:2016-12-22 summary:Handwriting recognition state of the art methods are based on Long Short Term Memory (LSTM) recurrent neural networks (RNN) coupled with the use of linguistic knowledge. LSTM RNN presents high raw performance and interesting training properties that allow us to break with the standard method at the state of the art. We present a simple and efficient way to extract from a single training a large number of complementary LSTM RNN, called cohort, combined in a cascade architecture with a lexical verification. This process does not require fine tuning, making it easy to use. Our verification allow to deal quickly and efficiently with gigantic lexicon (over 3 million words). We achieve state of the art results for isolated word recognition with very large lexicon and present novel results for an unprecedented gigantic lexicon. version:2
arxiv-1612-07956 | A CRF Based POS Tagger for Code-mixed Indian Social Media Text | http://arxiv.org/abs/1612.07956 | id:1612.07956 author:Kamal Sarkar category:cs.CL  published:2016-12-23 summary:In this work, we describe a conditional random fields (CRF) based system for Part-Of- Speech (POS) tagging of code-mixed Indian social media text as part of our participation in the tool contest on POS tagging for codemixed Indian social media text, held in conjunction with the 2016 International Conference on Natural Language Processing, IIT(BHU), India. We participated only in constrained mode contest for all three language pairs, Bengali-English, Hindi-English and Telegu-English. Our system achieves the overall average F1 score of 79.99, which is the highest overall average F1 score among all 16 systems participated in constrained mode contest. version:1
arxiv-1612-06505 | Parallelized Tensor Train Learning of Polynomial Classifiers | http://arxiv.org/abs/1612.06505 | id:1612.06505 author:Zhongming Chen, Kim Batselier, Johan A. K. Suykens, Ngai Wong category:cs.LG cs.AI  published:2016-12-20 summary:In pattern classification, polynomial classifiers are well-studied methods as they are capable of generating complex decision surfaces. Unfortunately, the use of multivariate polynomials is limited to kernels as in support vector machines, because polynomials quickly become impractical for high-dimensional problems. In this paper, we effectively overcome the curse of dimensionality by employing the tensor train format to represent a polynomial classifier. Based on the structure of tensor trains, two learning algorithms are proposed which involve solving different optimization problems of low computational complexity. Furthermore, we show how both regularization to prevent overfitting and parallelization, which enables the use of large training sets, are incorporated into these methods. Both the efficiency and efficacy of our tensor-based polynomial classifier are then demonstrated on the two popular datasets USPS and MNIST. version:3
arxiv-1612-07940 | Supervised Opinion Aspect Extraction by Exploiting Past Extraction Results | http://arxiv.org/abs/1612.07940 | id:1612.07940 author:Lei Shu, Bing Liu, Hu Xu, Annice Kim category:cs.CL cs.LG  published:2016-12-23 summary:One of the key tasks of sentiment analysis of product reviews is to extract product aspects or features that users have expressed opinions on. In this work, we focus on using supervised sequence labeling as the base approach to performing the task. Although several extraction methods using sequence labeling methods such as Conditional Random Fields (CRF) and Hidden Markov Models (HMM) have been proposed, we show that this supervised approach can be significantly improved by exploiting the idea of concept sharing across multiple domains. For example, "screen" is an aspect in iPhone, but not only iPhone has a screen, many electronic devices have screens too. When "screen" appears in a review of a new domain (or product), it is likely to be an aspect too. Knowing this information enables us to do much better extraction in the new domain. This paper proposes a novel extraction method exploiting this idea in the context of supervised sequence labeling. Experimental results show that it produces markedly better results than without using the past information. version:1
arxiv-1612-07921 | Understanding Non-optical Remote-sensed Images: Needs, Challenges and Ways Forward | http://arxiv.org/abs/1612.07921 | id:1612.07921 author:Amit Kumar Mishra category:cs.CV  published:2016-12-23 summary:Non-optical remote-sensed images are going to be used more often in man- aging disaster, crime and precision agriculture. With more small satellites and unmanned air vehicles planning to carry radar and hyperspectral image sensors there is going to be an abundance of such data in the recent future. Understanding these data in real-time will be crucial in attaining some of the important sustain- able development goals. Processing non-optical images is, in many ways, different from that of optical images. Most of the recent advances in the domain of image understanding has been using optical images. In this article we shall explain the needs for image understanding in non-optical domain and the typical challenges. Then we shall describe the existing approaches and how we can move from there to the desired goal of a reliable real-time image understanding system. version:1
arxiv-1612-07919 | EnhanceNet: Single Image Super-Resolution through Automated Texture Synthesis | http://arxiv.org/abs/1612.07919 | id:1612.07919 author:Mehdi S. M. Sajjadi, Bernhard Schölkopf, Michael Hirsch category:cs.CV  published:2016-12-23 summary:Single image super-resolution is the task of inferring a high-resolution image from a single low-resolution input. Traditionally, the performance of algorithms for this task is measured using pixel-wise reconstruction measures such as peak signal-to-noise ratio (PSNR) which have been shown to correlate poorly with the human perception of image quality. As a result, algorithms minimizing these metrics tend to produce oversmoothed images that lack high-frequency textures and do not look natural despite yielding high PSNR values. We propose a novel combination of automated texture synthesis with a perceptual loss focusing on creating realistic textures rather than optimizing for a pixel-accurate reproduction of ground truth images during training. By using feed-forward fully convolutional neural networks in an adversarial training setting, we achieve a significant boost in image quality at high magnification ratios. Extensive experiments on a number of datasets show the effectiveness of our approach, yielding state-of-the-art results in both quantitative and qualitative benchmarks. version:1
arxiv-1612-07899 | DARN: a Deep Adversial Residual Network for Intrinsic Image Decomposition | http://arxiv.org/abs/1612.07899 | id:1612.07899 author:Louis Lettry, Kenneth Vanhoey, Luc Van Gool category:cs.CV  published:2016-12-23 summary:We present a new deep supervised learning method for intrinsic decomposition of a single image into its albedo and shading components. Our contributions are based on a new fully convolutional neural network that estimates absolute albedo and shading jointly. As opposed to classical intrinsic image decomposition work, it is fully data-driven, hence does not require any physical priors like shading smoothness or albedo sparsity, nor does it rely on geometric information such as depth. Compared to recent deep learning techniques, we simplify the architecture, making it easier to build and train. It relies on a single end-to-end deep sequence of residual blocks and a perceptually-motivated metric formed by two discriminator networks. We train and demonstrate our architecture on the publicly available MPI Sintel dataset and its intrinsic image decomposition augmentation. We additionally discuss and augment the set of quantitative metrics so as to account for the more challenging recovery of non scale-invariant quantities. Results show that our work outperforms the state of the art algorithms both on the qualitative and quantitative aspect, while training convergence time is reduced. version:1
arxiv-1612-07896 | A Base Camp for Scaling AI | http://arxiv.org/abs/1612.07896 | id:1612.07896 author:C. J. C. Burges, T. Hart, Z. Yang, S. Cucerzan, R. W. White, A. Pastusiak, J. Lewis category:cs.AI cs.LG  published:2016-12-23 summary:Modern statistical machine learning (SML) methods share a major limitation with the early approaches to AI: there is no scalable way to adapt them to new domains. Human learning solves this in part by leveraging a rich, shared, updateable world model. Such scalability requires modularity: updating part of the world model should not impact unrelated parts. We have argued that such modularity will require both "correctability" (so that errors can be corrected without introducing new errors) and "interpretability" (so that we can understand what components need correcting). To achieve this, one could attempt to adapt state of the art SML systems to be interpretable and correctable; or one could see how far the simplest possible interpretable, correctable learning methods can take us, and try to control the limitations of SML methods by applying them only where needed. Here we focus on the latter approach and we investigate two main ideas: "Teacher Assisted Learning", which leverages crowd sourcing to learn language; and "Factored Dialog Learning", which factors the process of application development into roles where the language competencies needed are isolated, enabling non-experts to quickly create new applications. We test these ideas in an "Automated Personal Assistant" (APA) setting, with two scenarios: that of detecting user intent from a user-APA dialog; and that of creating a class of event reminder applications, where a non-expert "teacher" can then create specific apps. For the intent detection task, we use a dataset of a thousand labeled utterances from user dialogs with Cortana, and we show that our approach matches state of the art SML methods, but in addition provides full transparency: the whole (editable) model can be summarized on one human-readable page. For the reminder app task, we ran small user studies to verify the efficacy of the approach. version:1
arxiv-1612-07866 | Spectral algorithms for tensor completion | http://arxiv.org/abs/1612.07866 | id:1612.07866 author:Andrea Montanari, Nike Sun category:cs.DS math.ST stat.ML stat.TH  published:2016-12-23 summary:In the tensor completion problem, one seeks to estimate a low-rank tensor based on a random sample of revealed entries. In terms of the required sample size, earlier work revealed a large gap between estimation with unbounded computational resources (using, for instance, tensor nuclear norm minimization) and polynomial-time algorithms. Among the latter, the best statistical guarantees have been proved, for third-order tensors, using the sixth level of the sum-of-squares (SOS) semidefinite programming hierarchy (Barak and Moitra, 2014). However, the SOS approach does not scale well to large problem instances. By contrast, spectral methods --- based on unfolding or matricizing the tensor --- are attractive for their low complexity, but have been believed to require a much larger sample size. This paper presents two main contributions. First, we propose a new unfolding-based method, which outperforms naive ones for symmetric $k$-th order tensors of rank $r$. For this result we make a study of singular space estimation for partially revealed matrices of large aspect ratio, which may be of independent interest. For third-order tensors, our algorithm matches the SOS method in terms of sample size (requiring about $rd^{3/2}$ revealed entries), subject to a worse rank condition ($r\ll d^{3/4}$ rather than $r\ll d^{3/2}$). We complement this result with a different spectral algorithm for third-order tensors in the overcomplete ($r\ge d$) regime. Under a random model, this second approach succeeds in estimating tensors of rank $d\le r \ll d^{3/2}$ from about $rd^{3/2}$ revealed entries. version:1
arxiv-1612-07857 | Human Action Attribute Learning From Video Data Using Low-Rank Representations | http://arxiv.org/abs/1612.07857 | id:1612.07857 author:Tong Wu, Prudhvi Gurram, Raghuveer M. Rao, Waheed U. Bajwa category:stat.ML cs.LG  published:2016-12-23 summary:Representation of human actions as a sequence of human body movements or action attributes enables the development of models for human activity recognition and summarization. We present an extension of the low-rank representation (LRR) model, termed the clustering-aware structure-constrained low-rank representation (CS-LRR) model, for unsupervised learning of human action attributes from video data. Our model is based on the union-of-subspaces (UoS) framework, and integrates spectral clustering into the LRR optimization problem for better subspace clustering results. We lay out an efficient linear alternating direction method to solve the CS-LRR optimization problem. We also introduce a hierarchical subspace clustering approach, termed hierarchical CS-LRR, to learn the attributes without the need for a priori specification of their number. By visualizing and labeling these action attributes, the hierarchical model can be used to semantically summarize long video sequences of human actions at multiple resolutions. A human action or activity can also be uniquely represented as a sequence of transitions from one action attribute to another, which can then be used for human action recognition. We demonstrate the effectiveness of the proposed model for semantic summarization and action recognition through comprehensive experiments on five real-world human action datasets. version:1
arxiv-1612-07850 | Automatic Interpretation of Unordered Point Cloud Data for UAV Navigation in Construction | http://arxiv.org/abs/1612.07850 | id:1612.07850 author:M. D. Phung, C. H. Quach, D. T. Chu, N. Q. Nguyen, T. H. Dinh, Q. P. Ha category:cs.RO cs.CV cs.SY  published:2016-12-23 summary:The objective of this work is to develop a data processing system that can automatically generate waypoints for navigation of an unmanned aerial vehicle (UAV) to inspect surfaces of structures like buildings and bridges. The input includes data recorded by two 2D laser scanners, orthogonally mounted on the UAV, and an inertial measurement unit (IMU). To achieve the goal, algorithms are developed to process the data collected. They are separated into three major groups: (i) the data registration and filtering to generate a 3D model of the structure and control the density of point clouds for data completeness enhancement; (ii) the surface and obstacle detection to assist the UAV in monitoring tasks; and (iii) the waypoint generation to set the flight path. Experiments on different data sets show that the developed system is able to reconstruct a 3D point cloud of the structure, extract its surfaces and objects, and generate waypoints for the UAV to accomplish inspection tasks. version:1
arxiv-1612-07846 | A State Space Approach for Piecewise-Linear Recurrent Neural Networks for Reconstructing Nonlinear Dynamics from Neural Measurements | http://arxiv.org/abs/1612.07846 | id:1612.07846 author:Daniel Durstewitz category:q-bio.NC cs.NE q-bio.QM stat.ML  published:2016-12-23 summary:The computational properties of neural systems are often thought to be implemented in terms of their network dynamics. Hence, recovering the system dynamics from experimentally observed neuronal time series, like multiple single-unit (MSU) recordings or neuroimaging data, is an important step toward understanding its computations. Ideally, one would not only seek a state space representation of the dynamics, but would wish to have access to its governing equations for in-depth analysis. Recurrent neural networks (RNNs) are a computationally powerful and dynamically universal formal framework which has been extensively studied from both the computational and the dynamical systems perspective. Here we develop a semi-analytical maximum-likelihood estimation scheme for piecewise-linear RNNs (PLRNNs) within the statistical framework of state space models, which accounts for noise in both the underlying latent dynamics and the observation process. The Expectation-Maximization algorithm is used to infer the latent state distribution, through a global Laplace approximation, and the PLRNN parameters iteratively. After validating the procedure on toy examples, the approach is applied to MSU recordings from the rodent anterior cingulate cortex obtained during performance of a classical working memory task, delayed alternation. A model with 5 states turned out to be sufficient to capture the essential computational dynamics underlying task performance, including stimulus-selective delay activity. The estimated models were rarely multi-stable, but rather were tuned to exhibit slow dynamics in the vicinity of a bifurcation point. In summary, the present work advances a semi-analytical (thus reasonably fast) maximum-likelihood estimation framework for PLRNNs that may enable to recover the relevant dynamics underlying observed neuronal time series, and directly link them to computational properties. version:1
arxiv-1612-07843 | "What is Relevant in a Text Document?": An Interpretable Machine Learning Approach | http://arxiv.org/abs/1612.07843 | id:1612.07843 author:Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, Wojciech Samek category:cs.CL cs.IR cs.LG stat.ML  published:2016-12-23 summary:Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications. version:1
arxiv-1612-07833 | Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task | http://arxiv.org/abs/1612.07833 | id:1612.07833 author:Nan Ding, Sebastian Goodman, Fei Sha, Radu Soricut category:cs.CL cs.CV  published:2016-12-22 summary:We introduce a new multi-modal task for computer systems, posed as a combined vision-language comprehension challenge: identifying the most suitable text describing a scene, given several similar options. Accomplishing the task entails demonstrating comprehension beyond just recognizing "keywords" (or key-phrases) and their corresponding visual concepts. Instead, it requires an alignment between the representations of the two modalities that achieves a visually-grounded "understanding" of various linguistic elements and their dependencies. This new task also admits an easy-to-compute and well-studied metric: the accuracy in detecting the true target among the decoys. The paper makes several contributions: an effective and extensible mechanism for generating decoys from (human-created) image captions; an instance of applying this mechanism, yielding a large-scale machine comprehension dataset (based on the COCO images and captions) that we make publicly available; human evaluation results on this dataset, informing a performance upper-bound; and several baseline and competitive learning approaches that illustrate the utility of the proposed task and dataset in advancing both image and language comprehension. We also show that, in a multi-task learning setting, the performance on the proposed task is positively correlated with the end-to-end task of image captioning. version:1
arxiv-1612-07828 | Learning from Simulated and Unsupervised Images through Adversarial Training | http://arxiv.org/abs/1612.07828 | id:1612.07828 author:Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda Wang, Russ Webb category:cs.CV cs.LG cs.NE  published:2016-12-22 summary:With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data. version:1
